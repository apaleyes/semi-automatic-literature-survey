doi,title,publisher,content_type,abstract,html_url,publication_title,publication_date,database,query_name,query_value
10.1109/IS.2018.8710526,A Digital Twin-based Privacy Enhancement Mechanism for the Automotive Industry,IEEE,Conferences,"This paper discusses a Digital Twin demonstrator for privacy enhancement in the automotive industry. Here, the Digital Twin demonstrator is presented as a method for the design and implementation of privacy enhancement mechanisms, and is used to detect privacy concerns and minimize breaches and associated risks to which smart car drivers can be exposed through connected infotainment applications and services. The Digital Twin-based privacy enhancement demonstrator is designed to simulate variety of conditions that can occur in the smart car ecosystem. We firstly identify the core stakeholders (actors) in the smart car ecosystem, their roles and exposure to privacy vulnerabilities and associated risks. Secondly, we identify assets that consume and generate sensitive privacy data in smart cars, their functionalities, and relevant privacy concerns and risks. Thirdly, we design an infrastructure for collecting (i) real-time sensor data from smart cars and their assets, and (ii) environmental data, road and traffic data, generated through operational driving lifecycle. In order to ensure compliance of the collected data with privacy policies and regulations, e.g. with GDPR requirements for enforcement of the data subject's rights, we design methods for the Digital Twin-based privacy enhancement demonstrator that are based on behavioural analytics informed by GDPR. We also perform data anonymization to minimize privacy risks and enable actions such as sending an automatic informed consent to the stakeholders.",https://ieeexplore.ieee.org/document/8710526/,2018 International Conference on Intelligent Systems (IS),25-27 Sept. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SCC53864.2021.00047,A Framework for Enabling Cyber-Twins based Industry 4.0 Application Development,IEEE,Conferences,"Industry 4.0 applications aim to improve industrial efficiency, product consistency, and related supply chains by providing smart, data-driven solutions that involve machines, processes, workers, and products. This paper proposes Cyber Twins (CTs) for Industry 4.0 application development and a CT- based framework that reduces the cost/effort of Industry 4.0 application development by providing: 1) a CT ontology for describing industrial machines, including their sensors, actuators, settings and automation/control functions, 2) machine emulation/simulation for testing, and 3) services for generating CTs from their semantic descriptions, CT-based communication with machines, and CT integration across Industry 4.0 applications. We present a proof-of-concept CT framework implementation and introduce a cost model to compute the costs of CT-based Industry 4.0 application development and compare this with the cost of applications that use other existing digital twins. Via these, we measure the benefits of CT-based application development in a real-world use case from the milk processing industry.",https://ieeexplore.ieee.org/document/9592482/,2021 IEEE International Conference on Services Computing (SCC),5-10 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMCCE.2018.00050,A Smart Manufacturing Compliance Architecture of Electronic Batch Recording System (eBRS) for Life Sciences Industry,IEEE,Conferences,"The paradigm shift brought about by smart manufacturing or Industrie 4.0 has posed threefold challenges to electronic batch recording system (eBRS) in Life Sciences Industry: 1) the structure of the data should be informative and standard for interoperate using information models, 2) administration of synchronization between physical world and cyber world for smart decision making and optimization using cyber physical system (CPS) and 3) so-called digital manufacturing operations management (digital MOM) characterized by decentralization, comprehensive collaboration and servitization shall be implemented. Under the new situations of smart manufacturing or Industrie 4.0, the requirements from information models, CPS and digital MOM will become the most principal criteria to be considered for future eBRS/MES and other operations management information system in shop floor. To fulfill these demands, an approach combining ISA95/88 hybrid model with activities ontology and variant domain-driven design for SOA-based eBRS development has been presented. An eBRS software platform has been developed on the theoretical basis and applied to a specific application scenario of Lyophilized Injection Production for verifying its feasibility purpose.",https://ieeexplore.ieee.org/document/8537548/,"2018 3rd International Conference on Mechanical, Control and Computer Engineering (ICMCCE)",14-16 Sept. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PacificVis.2018.00026,A Visual Analytics Approach for Equipment Condition Monitoring in Smart Factories of Process Industry,IEEE,Conferences,"Monitoring equipment conditions is of great value in manufacturing, which can not only reduce unplanned downtime by early detecting anomalies of equipment but also avoid unnecessary routine maintenance. With the coming era of Industry 4.0 (or industrial internet), more and more assets and machines in plants are equipped with various sensors and information systems, which brings an unprecedented opportunity to capture large-scale and fine-grained data for effective on-line equipment condition monitoring. However, due to the lack of systematic methods, analysts still find it challenging to carry out efficient analyses and extract valuable information from the mass volume of data collected, especially for process industry (e.g., a petrochemical plant) with complex manufacturing procedures. In this paper, we report the design and implementation of an interactive visual analytics system, which helps managers and operators at manufacturing sites leverage their domain knowledge and apply substantial human judgements to guide the automated analytical approaches, thus generating understandable and trustable results for real-world applications. Our system integrates advanced analytical algorithms (e.g., Gaussian mixture model with a Bayesian framework) and intuitive visualization designs to provide a comprehensive and adaptive semi-supervised solution to equipment condition monitoring. The example use cases based on a real-world manufacturing dataset and interviews with domain experts demonstrate the effectiveness of our system.",https://ieeexplore.ieee.org/document/8365986/,2018 IEEE Pacific Visualization Symposium (PacificVis),10-13 April 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSSS.2011.5974714,A data prediction algorithm based on BP neural network in telecom industry,IEEE,Conferences,"The data mining technology is more and more widely used in the telecom industry. But telecom data set always includes instances with missing values. Besides, many data mining models are sensitive for the missing value and distortion. Estimating missing values becomes an inherent problem. To address the problem, A prediction method is proposed for the missing value based on the BP neural network and K-means algorithm. The algorithm first clusters the dataset, and selects a certain cluster for the instance with missing values, then we train the BP net using the cluster and get the architecture parameters. When a test instance has a missing value of a certain attribute, we regard the attribute as the aiming attribute and apply the instance to the network, it will produce a prediction value . This paper uses real datasets from the telecom industry as the test datasets. The result shows that the algorithm can be used to predict the missing value of telecom industry with good performance.",https://ieeexplore.ieee.org/document/5974714/,2011 International Conference on Computer Science and Service System (CSSS),27-29 June 2011,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPDWinter52325.2021.00056,A study on Medical Device Security Status in Medical Convergence Industry,IEEE,Conferences,"Beyond informatization, the development of new technologies and the introduction of innovative technologies in the Fourth Industrial Revolution are the evolution of various industries. In the early information age, the introduction of various information systems has made it possible to manage and utilize large amounts of data in high quality, thus expanding the business of other industries faster and more reliably. However, in certain industries, various devices and equipments are dependent on the existing information system, and thus, time and cost difficulties are introduced to introduce a new system. Accordingly, existing security problems rather than new security flaws are coming to reality. The purpose of this study is to investigate and analyze the security status of existing medical devices according to the arrival of convergence environment in the medical industry.",https://ieeexplore.ieee.org/document/9403515/,"2021 21st ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)",28-30 Jan. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISPASS51385.2021.00014,AIBench Training: Balanced Industry-Standard AI Training Benchmarking,IEEE,Conferences,"Earlier-stage evaluations of a new AI architecture/system need affordable AI benchmarks. Only using a few AI component benchmarks like MLPerf alone in the other stages may lead to misleading conclusions. Moreover, the learning dynamics are not well understood, and the benchmarks' shelf-life is short. This paper proposes a balanced benchmarking methodology. We use real-world benchmarks to cover the factors space that impacts the learning dynamics to the most considerable extent. After performing an exhaustive survey on Internet service AI domains, we identify and implement nineteen representative AI tasks with state-of-the-art models. For repeatable performance ranking (RPR subset) and workload characterization (WC subset), we keep two subsets to a minimum for affordability. We contribute by far the most comprehensive AI training benchmark suite. The evaluations show: (1) AIBench Training (v1.1) outperforms MLPerf Training (v0.7) in terms of diversity and representativeness of model complexity, computational cost, convergent rate, computation, and memory access patterns, and hotspot functions; (2) Against the AIBench full benchmarks, its RPR subset shortens the benchmarking cost by 64%, while maintaining the primary workload characteristics; (3) The performance ranking shows the single-purpose AI accelerator like TPU with the optimized TensorFlow framework performs better than that of GPUs while losing the latter's general support for various AI models. The specification, source code, and performance numbers are available from the AIBench homepage https://www.benchcouncil.org/aibench-training/index.html.",https://ieeexplore.ieee.org/document/9408170/,2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),28-30 March 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2014.7004408,Advanced planning and control of manufacturing processes in steel industry through big data analytics: Case study and architecture proposal,IEEE,Conferences,"Enterprises in today's globalized world are compelled to react on threats and opportunities in a highly flexible manner. Hence, companies that are able to analyze the current state of their business processes, forecast their most optimal progresses and with this proactively control them will have a decisive competitive advantage. Technological progress in sensor technology has boosted real-time situation awareness, especially in manufacturing operations. The paper at hands examines, based on a case study stemming from the steel manufacturing industry, which production-related data is collectable using state of the art sensors forming a basis for a detailed situation awareness and for deriving accurate forecasts. However, analyses of this data point out that dedicated big data analytics approaches are required to utilize the full potential out of it. By proposing an architecture for predictive process planning and control systems, the paper intends to form a working and discussion basis for further research and implementation efforts in big data analytics.",https://ieeexplore.ieee.org/document/7004408/,2014 IEEE International Conference on Big Data (Big Data),27-30 Oct. 2014,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HICSS.2007.52,Agent-based Human-computer-interaction for Real-time Monitoring Systems in the Trucking Industry,IEEE,Conferences,"Auto ID systems can replace time-consuming, costly and error-prone processes of human data entry and produce detailed real time information. However, they would add value only to the extent that data is presented in a user-friendly manner. As model-based decision support is not always adequate, an agent-based approach is often chosen. Real life entities such as orders and trucks are represented by agents, which negotiate in order to solve planning problems. For the respective data representation at least two forms can be distinguished, focusing either on (1) resources (account-based) or (2) orders (order-centric). Applying cognitive fit theory we describe how the different interfaces affect decision making. The hypotheses would be tested in a laboratory experiment. The intended contribution should support that order-centric interfaces have higher user-friendliness and are especially beneficial to low-analytics and planners working under time pressure",https://ieeexplore.ieee.org/document/4076424/,2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07),3-6 Jan. 2007,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BDEIM52318.2020.00043,An Empirical Study on the Correlation between Operating Capital Management and Operating Performance-Taking the Communication Terminal and Accessories Industry as an example,IEEE,Conferences,"This article uses Stata16.0 software tools to analyze and research the 2015-2019 company annual reports of listed companies in the communication terminal and accessories industry collected in the RESSET financial research database, and summarizes the impact of short-term operating funds on operating performance in the company's supply, production and sales scenarios, provides reference suggestions and valuable experience for companies of different sizes in the communication terminal and accessories industry and other industries in the use of short-term operating funds, and promotes the sustainable and healthy development of enterprises.",https://ieeexplore.ieee.org/document/9407126/,2020 International Conference on Big Data Economy and Information Management (BDEIM),11-13 Dec. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COASE.2018.8560557,An Industry 4.0 Cyber-Physical Framework for Micro Devices Assembly,IEEE,Conferences,"An advanced cyber manufacturing framework to support the collaborative assembly of micro devices is presented based on Industry 4.0 principles. The distributed cyber and physical components work together to plan, assemble and monitor micro assembly related tasks; micro assembly refers to the assembly of micron sized devices which cannot be manufactured by MEMS technologies. The collaborative framework proposed includes assembly planning and path planning modules, Virtual Reality based assembly simulation environments and physical assembly work cells. An ontology based approach was implemented to address semantic interoperability issues to support formation of temporary partnerships in a Virtual Enterprise context. The key to the design and implementation of this complex framework is an information centric process modeling approach which provides a data/information oriented basis for collaboration. A collaborative cyber physical test bed has been built to demonstrate feasibility of proposed framework and approach.",https://ieeexplore.ieee.org/document/8560557/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMSEC.2011.6010784,An improved B&B technique applying to telecom industry,IEEE,Conferences,"The data mining technology is more and more widely used in the telecom industry. When we construct Bayesian Belief Network, the branch and bound technique based on the minimum description length principle (B&B technique) is one of the classical algorithm. To telecom data, high dimensionality and huge volume set obstacle when constructing Bayesian Belief Network. But we utilize the inconspicuous correlation between telecom attributes, improve the process of B&B technique and simplify the structure to solve complexity rooting from telecom data's feature. The algorithm first construct a dependence ordering, then a simplified B&B technique suitable for telecom data is applied. We compare the result and complexity with the original B&B technique. This paper uses real datasets from the telecom industry. The result shows that the new algorithm can construct the network almost the same as the original one, but with good performance.",https://ieeexplore.ieee.org/document/6010784/,"2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",8-10 Aug. 2011,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SYSCON.2018.8369547,An interactive architecture for industrial scale prediction: Industry 4.0 adaptation of machine learning,IEEE,Conferences,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",https://ieeexplore.ieee.org/document/8369547/,2018 Annual IEEE International Systems Conference (SysCon),23-26 April 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIT.2016.18,Analysis of Complex Data in Telecommunications Industry,IEEE,Conferences,"In this paper, we report an application of data analytics in a real world business case of the telecom industry. This work has been tied up with an IT company in India with a large data set of telecom customers. As part of data analytics, the first task was to perform cleansing of bad and missing data, transforming heterogeneous formats into a unified format, semantic analysis on the data (semantics of attributes and their relationships), and then perform unsupervised learning using BIRCH technique of clustering. We describe and discuss the approach and results of these tasks on a sample data in this work.",https://ieeexplore.ieee.org/document/7876322/,2016 IEEE International Conference on Computer and Information Technology (CIT),8-10 Dec. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SHARK-ADI.2007.3,Architectural Knowlege Management Strategies: Approaches in Research and Industry,IEEE,Conferences,"The software architecture community has recently gained an increasing interest in managing architectural knowledge. However, up until now there have been no attempts to obtain an overview of the work in the field. In this paper we present a preliminary review on current approaches to architectural knowledge management. To this end, we compare approaches known from literature and encountered in industry with knowledge management theory. We found that in reports from research and practice there appears to be a preference to use the codification strategy. However, our observations of the software architecture industry show that organizations in general tend to use a personalization strategy unintentionally. This paper serves as a call for awareness of this gap between intention and reality, and questions the biased focus on intentional codification alone. We suggest to close this gap through focusing on hybrid approaches.",https://ieeexplore.ieee.org/document/4273342/,"Second Workshop on Sharing and Reusing Architectural Knowledge - Architecture, Rationale, and Design Intent (SHARK/ADI'07: ICSE Workshops 2007)",20-26 May 2007,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PerComWorkshops51409.2021.9431009,Architecture and pervasive platform for machine learning services in Industry 4.0,IEEE,Conferences,"Pervasive computing promotes the integration of smart electronic devices in our living and working spaces in order to provide new, advanced services. Recently many prototype services based on machine learning techniques have been proposed in a number of domains like smart homes, smart buildings or smart plants. However, the number of applications effectively deployed in the real world is still limited. We believe that architectural principles and integrated frameworks are still missing today to successfully and repetitively support application developers and operators. In this paper, we present a novel architecture and a pervasive platform allowing the development of machine learning based applications in smart buildings.",https://ieeexplore.ieee.org/document/9431009/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIPTM54933.2022.9753944,Artificial Intelligence led Industry 4.0 Application for Sustainable Development,IEEE,Conferences,"In recent times we can observe technological growth in respect of new expansions, innovations, and improvements in every industry. This brings a great movement in this era in the field of military, healthcare, industries, education, or daily households. Technology 4.0 has boomed into the current market with the name of Industry 4.0 which deals with the information change in automation and manufacturing technologies. It includes the concept of IoT, cloud computing, and artificial intelligence for generating smart factories and industries. Industry 4.0 knots the digital and physical progressions together to create these smart industries. It also optimizes and manages the real-time requirements and supply chain management in the factories. It makes a profit to the industries and organizations by quick decision making and its efficiency. The backbone of this technology is Artificial intelligence which gives a substantial contribution to Industry 4.0. AI deals with all the technologies we use in our daily lives. The environment in which we are living has a great impact on AI as all the physical data is associated with digital data. The dynamic nature of AI shows its efficiency in the field of sports, security, the military, industries, production, and many more organizations. Now this combination of Industry4.0 and AI together equips the application with enormous solutions and services. Industry 4.0 is still at its early stage of expansion, implementation, and employment even though it has a significant performance in every domain. In this paper, we have done a study on AI &#x0026; Industry 4.0 and discussed its challenges, importance, techniques used, and outcomes. Lastly, its effects and benefits and the importance of invention in the area of industries and day-to-day lives have been discussed.",https://ieeexplore.ieee.org/document/9753944/,2022 2nd International Conference on Innovative Practices in Technology and Management (ICIPTM),23-25 Feb. 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCA51647.2021.00071,Ascend: a Scalable and Unified Architecture for Ubiquitous Deep Neural Network Computing : Industry Track Paper,IEEE,Conferences,"Deep neural networks (DNNs) have been successfully applied to a great variety of applications, ranging from small IoT devices to large scale services in a data center. In order to improve the efficiency of processing these DNN models, dedicated hardware accelerators are required for all these scenarios. Theoretically, there exists an optimized acceleration architecture for each application. However, considering the cost of chip design and corresponding tool-chain development, researchers need to trade off between efficiency and generality. In this work, we demonstrate that it is practical to use a unified architecture, called Ascend, to support those applications, ranging from IoT devices to data-center services. We provide a lot of design details to explain that the success of Ascend relies on contributions from different levels. First, heterogeneous computing units are employed to support various DNN models. And the datapath is adapted according to the requirement of computing and data access. Second, when scaling the Ascend architecture from a single core to a cluster containing thousands of cores, it involves design efforts, such as memory hierarchy and system level integration. Third, a multi-tier compiler, which provides flexible choices for developers, is the last critical piece. Experimental results show that using accelerators based on the Ascend architecture can achieve comparable or even better performance in different applications. In addition, various chips based on the Ascend architecture have been successfully commercialized. More than 100 million chips have been used in real products.",https://ieeexplore.ieee.org/document/9407221/,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),27 Feb.-3 March 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV47402.2020.9304778,"Autonomous Driving Vehicle Control Auto-Calibration System: An Industry-Level, Data-Driven and Learning-based Vehicle Longitudinal Dynamic Calibrating Algorithm",IEEE,Conferences,"The control module is a crucial part for autonomous driving systems, a typical control algorithm often requires vehicle dynamics (such as longitudinal dynamics) as inputs, which, unfortunately are difficult to calibrate in real time. Further, it is also a challenge to reflect instantaneous changes in longitudinal dynamics (e.g. load changes) using a calibration table. As a result, control performance may deteriorate when load changes considerably (especially for small cargoes). In this paper, we will show how we build a data-driven longitudinal calibration procedure using machine learning techniques to adapt load changes in real time. We first generated offline calibration tables from human driving data. The offline table serves as an initial guess for later uses, and it only requires twenty minutes of data collection and processing. We then used an online learning algorithm to appropriately update the initial table (the offline table) based on real-time performance analysis. Experiments indicated (a) offline auto-calibration leads to a better control accuracy, compared with manual calibration; (b) online auto-calibration is capable to handle load changes and significantly reduce real time control error. This system has been deployed to more than one hundred Baidu self-driving vehicles (both hybrid and electronic vehicles) since April 2018. By January 2019, the system had been tested for more than 2,000 hours and over 10,000 kilometers (6,213 miles) and was still proven to be effective.",https://ieeexplore.ieee.org/document/9304778/,2020 IEEE Intelligent Vehicles Symposium (IV),19 Oct.-13 Nov. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/USSEC53120.2021.9655742,Averaged Errors as a Risk Factor for Intelligent Forecasting Systems Operation in the Power Industry,IEEE,Conferences,"The paper discusses the operational risk in intelligent systems for forecasting time series. Typically, when developing and testing regression models based on machine learning, their accuracy is calculated over a long time interval, from several months to several years, and then is averaged. However, in the real-life operation of such systems, the customer is likely to draw a conclusion about the system efficiency based on the results of the first 2–4 weeks of operation. If one or several large errors appear on this short interval, they will not be averaged as it happens over a long one. As a result, there is a risk of failure in the intelligent forecasting system implementation due to the discrepancy between the calculated mean error and that obtained over a short time period at the start of operation. This study considers the problem of solar power plant generation short-term forecasting, analyzes the distribution of errors over short time periods, and substantiates the need to use more detailed accuracy metrics of machine learning models than the error values averaged over a long interval.",https://ieeexplore.ieee.org/document/9655742/,2021 Ural-Siberian Smart Energy Conference (USSEC),13-15 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTAS52030.2021.00041,Brief Industry Paper: Workload-Aware GPU Performance Estimation in the Airborne Embedded System,IEEE,Conferences,"New generation airborne embedded system has deployed Graphical Processing Units (GPUs) to raise processing capability to meet growing computational demands. Applications in the airborne embedded system have strict real-time constraints. Therefore, it is necessary to accurately predict timing behaviors of those applications. Many previous work propose GPU performance models to estimate the execution time of applications. However, most of those models do not consider the impact of co-execution on the GPU performance. In this paper, we propose a workload-aware GPU performance model to predict the execution time of applications executed concurrently on a single GPU. Experimental results illustrate that the proposed model can achieve a 5.1%-11.6% prediction error in a real airborne embedded hardware platform.",https://ieeexplore.ieee.org/document/9470486/,2021 IEEE 27th Real-Time and Embedded Technology and Applications Symposium (RTAS),18-21 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTAS52030.2021.00048,Brief Industry Paper: optimizing Memory Efficiency of Graph Neural Networks on Edge Computing Platforms,IEEE,Conferences,"Graph neural networks (GNN) have achieved state-of-the-art performance on various industrial tasks. However, the poor efficiency of GNN inference and frequent Out-of-Memory (OOM) problem limit the successful application of GNN on edge computing platforms. To tackle these problems, a feature decomposition approach is proposed for memory efficiency optimization of GNN inference. The proposed approach could achieve outstanding optimization on various GNN models, covering a wide range of datasets, which speeds up the inference by up to 3×. Furthermore, the proposed feature decomposition could significantly reduce the peak memory usage (up to 5× in memory efficiency improvement) and mitigate OOM problems during GNN inference.",https://ieeexplore.ieee.org/document/9470441/,2021 IEEE 27th Real-Time and Embedded Technology and Applications Symposium (RTAS),18-21 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MED.2017.7984310,Cloud computing for big data analytics in the Process Control Industry,IEEE,Conferences,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0.",https://ieeexplore.ieee.org/document/7984310/,2017 25th Mediterranean Conference on Control and Automation (MED),3-6 July 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCNC45663.2020.9120761,Collaborative Learning Model for Cyberattack Detection Systems in IoT Industry 4.0,IEEE,Conferences,"Although the development of IoT Industry 4.0 has brought breakthrough achievements in many sectors, e.g., manufacturing, healthcare, and agriculture, it also raises many security issues to human beings due to a huge of emerging cybersecurity threats recently. In this paper, we propose a novel collaborative learning-based intrusion detection system which can be efficiently implemented in IoT Industry 4.0. In the system under consideration, we develop smart “filters” which can be deployed at the IoT gateways to promptly detect and prevent cyberattacks. In particular, each filter uses the collected data in its network to train its cyberattack detection model based on the deep learning algorithm. After that, the trained model will be shared with other IoT gateways to improve the accuracy in detecting intrusions in the whole system. In this way, not only the detection accuracy is improved, but our proposed system also can significantly reduce the information disclosure as well as network traffic in exchanging data among the IoT gateways. Through thorough simulations on real datasets, we show that the performance obtained by our proposed method can outperform those of the conventional machine learning methods.",https://ieeexplore.ieee.org/document/9120761/,2020 IEEE Wireless Communications and Networking Conference (WCNC),25-28 May 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EAEEIE.2013.6576535,Cooperation between industry and university based on the evaluation of the industrial research results in the academic environment,IEEE,Conferences,"Based on the European Structural Funds it was developed the Intelligent Mobile Box, Intelligent Panel Controller with intelligent adaptive controllers within the industrial research and experimental development in the company Kybernetes, s.r.o. Within the frame of the academic-industry cooperation, the intelligent adaptive controller was tested at the Department of Cybernetics and Artificial Intelligence, Technical university of Kosice, Slovakia. The tests of the mobile intelligent adaptive controller were performed on two levels of university study, on the Bachelor level on the exercises from the subject “Control of Technological Processes” and on the Engineering level the exercises from the subject “Intelligent Control Networks” and on one Diploma project. Goals of students of the Control of the technological processes course had two goals, firstly to connect the intelligent adaptive controller to pre-defined controlled system (real plant, real model or simulated model) and next to validate the control results. Students of the Diploma project on the Engineering level had more advanced goals. Tasks defined for engineering students were to connect the intelligent adaptive controller to non-defined controlled system, setup the adaptivity process of the controller regarding the learning error, parameterize the control system, observe and validate the control results. Both sides concluded this cooperation as very valuable. Main contributions for students were (U1) the challenge to apply studied theoretical knowledge on the real industrial controllers, (U2) experience with new research results and technologies deployed in industry and (U3) the implementation of the control and adaptive algorithms from abstract mathematical area to real PLC controller. On side of industry research company the main contributions were (C1) testing of designed algorithms and (C2) user feedback from students to make the application HMI interface more understandable a native.",https://ieeexplore.ieee.org/document/6576535/,2013 24th EAEEIE Annual Conference (EAEEIE 2013),30-31 May 2013,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
,Corporate Social Responsibility Challenges and Risks of Industry 4.0 technologies: A review,VDE,Conferences,"The fourth industrial revolution arrived with many enabling technologies that would impact important sociological aspects in the industry. Some of the Industry 4.0 technologies are already running in different industrial application, and other are still as a paradigm state. The social, economic, and environmental acceptance of Industry 4.0 technologies is still under discussion, which open new opportunities to execute various analysis about the possible implications of the implementation of such technologies. This article refers to an exploratory analysis and identification of the different challenges and risks of this new Industry 4.0 paradigm and its related technologies. The technologies under review were Internet of Things, Artificial Intelligence, Cloud Computing, cybersecurity, bid data, blockchain, 5G, robotics, adding manufacturing, unmanned systems, autonomous vehicles, virtual reality, and augmented reality. As a result, different social challenges and risks were identified for each technology, starting from vulnerability, implementation cost, until social aspects such as education and unemployment caused by those new technologies. In conclusion, Industry 4.0 arrived with a lot of benefits to the industry business, but companies should not stop thinking about sustainable development.",https://ieeexplore.ieee.org/document/8835964/,"Smart SysTech 2019; European Conference on Smart Objects, Systems and Technologies",4-5 June 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AINS47559.2019.8968698,Cyber Security Risk Assessment on Industry 4.0 using ICS testbed with AI and Cloud,IEEE,Conferences,"Industry 4.0 is a new concept, thus risk assessment is necessary. Several risk assessment methods for Industrial Control System (ICS) and Industry 4.0 have been proposed, however, it is difficult to identify impacts on the physical world caused by cyber attacks against ICS since many of these are based on tabletop analysis or software simulations. Therefore, we focus on the risk assessment using actual machines (ICS testbed) which can help to solve the above problems. In Industry 4.0, autonomous judgment and execution are required for the cyber-physical system, it is based on information exchange using Artificial Intelligence (AI) and cloud technologies. In this research, we evaluate cyber risks through attacks against ICS with AI and cloud using ICS testbed. The proposed method can clarify cyber risks and impacts on the real world, and corresponding countermeasures.",https://ieeexplore.ieee.org/document/8968698/,"2019 IEEE Conference on Application, Information and Network Security (AINS)",19-21 Nov. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COINS51742.2021.9524088,Cyber Security Risks of Technical Components in Industry 4.0,IEEE,Conferences,"Industry 4.0 is a new concept of automation data exchange in manufacturing. Industry 4.0 consists of various technical components such as Artificial Intelligence (AI), IoT, OPC UA, and cloud. There could be cyber risks when these technical components are used in Industrial Control System (ICS) without security consideration. Therefore cyber risk assessment, reconsideration of implementation, operation are necessary to adopt Industry 4.0. Problems of previous research for ICS risk assessments are some of them are theoretical and conceptual methods such as simulations. Moreover, they do not focus on Industry 4.0 components.The objective of this research is to prove the concrete cyber risks and introduces corresponding secure implementations for Industry 4.0 components such as Artificial Intelligence (AI), the Internet of Things (IoT), OPC Unified Architecture (OPC UA). The proposed method has benefits for clarifying impacts on the real world by cyber-attacks through penetration tests against ICS testbed with actual machines.",https://ieeexplore.ieee.org/document/9524088/,2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS),23-25 Aug. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PTC.2001.964653,DPlan: a case study on the cooperation between university and industry,IEEE,Conferences,"The aim of this paper is to explain how the research interests of a university group have been matched with the real needs of a distribution company team working in the field. It describes a case study of technology transfer, from research, through prototyping, to the market. This paper also addresses the formulation and the main evolutionary computation techniques used to deal with the problem of optimal distribution planning. The integration of all systems currently in operation is a priority of EDP distribution. The strategy is to arrive at a situation where those systems become modules of an integration platform. The infrastructure for this integration platform is a geographic information system. In the paper we describe the integration of DPlan in this platform.",https://ieeexplore.ieee.org/document/964653/,2001 IEEE Porto Power Tech Proceedings (Cat. No.01EX502),10-13 Sept. 2001,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CASE48305.2020.9216961,Decentralized coordination of autonomous AGVs for flexible factory automation in the context of Industry 4.0,IEEE,Conferences,"Future smart factories feature flexible systems that can dynamically reconFigure manufacturing systems via near real-time system monitoring and learning-based self-optimization. Automated guided vehicles (AGVs), as a critical method of transporting goods and material within a factory, is vital for flexible automation in a smart factory. However, there is an urgent gap in the ability to dynamically schedule and assign tasks for AGVs in a dynamic environment. In this research, we propose a decentralized AGV fleet architecture and task allocation method to enable dynamic allocation/reallocation of tasks in an AGV fleet. The developed algorithm can also reconFigure AGV task allocations to adapt to system changes, such as AGV failure and new AGVs joining the system. The system modeling, setup and algorithms are presented with a case study in a lab environment that demonstrates flexible collaboration between an AGV fleet and a robotic assembly cell.",https://ieeexplore.ieee.org/document/9216961/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICESC48915.2020.9155712,Demand Forecasting in Retail Industry for Liquor Consumption using LSTM,IEEE,Conferences,"This paper focuses on forecasting demand of sales stock. Retailers face problems of stock out or over stock due to improper management or estimation skills. So, to help retailers overcome these loss causing problems, our system predicts the stock requirements based on the previous data. This proposed work has implemented various algorithms like Long Short-Term Memory (LSTM), Regressions and Support Vector Machine (SVM) to predict sales. Various methodologies are used for demand forecasting and an appropriate one is to be selected which is capable of providing a high estimation accuracy. Our experimental results on real data set suggests to follow LSTM as it considerably improves the forecasting performance.",https://ieeexplore.ieee.org/document/9155712/,2020 International Conference on Electronics and Sustainable Communication Systems (ICESC),2-4 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPFA49335.2020.9260582,"Detection and prevention of assembly defects, by machine learning algorithms, in semiconductor industry for automotive",IEEE,Conferences,"Years of experience within semiconductor manufacturing facilities have led to optimize processes to serve both quality and cost. The solution to achieve next generational levels requires a new approach: this one is fitting with implementation of advanced analytics and machine learning algorithms. Applied to manufacturing data which corresponds with a real big data context, these algorithms can provide insights and automate responses to detect, prevent and ultimately eliminate the most severe failure modes. The project described in this paper targets a wafer sawing process. Various challenges that are raised in such a project are of different natures. A first one is the need for a high level of technical expertise in the manufacturing process of focus: this is essential to define the meaningful dataset that represents comprehensively the desired output of the process. Another component is the data collection aspect: many data have to be collected, stored and parsed, and some small signals found will become the leading indicator to an upcoming process degradation and capability of capturing them is essential. Another key data is traceability of the processed material. Additionally, ensuring an informatic technology architecture to support collection, storage, parsing and computation of the datasets is a significant challenge. Lastly, project success is related to the data scientist expertise to build adequate machine learning algorithms. Optimization of the models can take several iterations with back and forth communication between data scientists and process technical experts. This paper describes issues revealed, some solutions found, and future expectations.",https://ieeexplore.ieee.org/document/9260582/,2020 IEEE International Symposium on the Physical and Failure Analysis of Integrated Circuits (IPFA),20-23 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EuCNC/6GSummit51104.2021.9482590,Empowering Industry 4.0 and Autonomous Drone Scouting use cases through 5G-DIVE Solution,IEEE,Conferences,"The 5G Edge Intelligence for Vertical Experimentation (5G-DIVE) project aims at demonstrating the technical merits and business value proposition of 5G technologies in two vertical pilots, namely the Industry 4.0 (I4.0) and Autonomous Drones Scout (ADS) pilots. This paper presents an overview of the overall 5G-DIVE solution and reports the results of the initial validation campaign of the selected use case, featuring 5G connectivity, distributed Edge computing, and artificial intelligence. The initial results for the I4.0 provide a baseline for next step validation campaign targeting a broader scale 5G implementation, while the ADS results provides promising results for enhancing the autonomous navigation in real-time.",https://ieeexplore.ieee.org/document/9482590/,2021 Joint European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit),8-11 June 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2019.8913901,Explainable Machine Learning in Industry 4.0: Evaluating Feature Importance in Anomaly Detection to Enable Root Cause Analysis,IEEE,Conferences,"In the past recent years, Machine Learning methodologies have been applied in countless application areas. In particular, they play a key role in enabling Industry 4.0. However, one of the main obstacles to the diffusion of Machine Learning-based applications is related to the lack of interpretability of most of these methods. In this work, we propose an approach for defining a `feature importance' in Anomaly Detection problems. Anomaly Detection is an important Machine Learning task that has an enormous applicability in industrial scenarios. Indeed, it is extremely relevant for the purpose of quality monitoring. Moreover, it is often the first step towards the design of a Machine Learning-based smart monitoring solution because Anomaly Detection can be implemented without the need of labelled data. The proposed feature importance evaluation approach is designed for Isolation Forest, one of the most commonly used algorithm for Anomaly Detection. The efficacy of the proposed method is tested on synthetic and real industrial datasets.",https://ieeexplore.ieee.org/document/8913901/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CTIT.2018.8649493,Game Theoretic Approach for Applying Artificial Intelligence in the Credit Industry,IEEE,Conferences,"The law of accelerating returns can be viewed as a concept that describes acceleration of technological progress. The idea is that tools are used for developing more advanced tools that are applied for creating even more advanced tools etc. A similar idea has been implemented in algorithms for advancing artificial intelligence. In this paper, the results of applying these algorithms in games are discussed. Nevertheless, real life tasks seem more complicated. The game theoretic approach can be applied for transition from theoretical and unrealistic games to more complex and practical tasks. Applications of the game theoretic approach to advance artificial intelligence in solving tasks in the credit industry are proposed.",https://ieeexplore.ieee.org/document/8649493/,2018 Fifth HCT Information Technology Trends (ITT),28-29 Nov. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/APCA.1999.805015,IEEE Industry Applications Society. 1999 Advanced Process Control Applications for Industry Workshop. Record of Workshop Papers,IEEE,Conferences,Presents the front cover of the proceedings record.,https://ieeexplore.ieee.org/document/805015/,IEEE Industry Applications Society Advanced Process Control Applications for Industry Workshop,29-30 April 1999,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC.2018.10204,Indoor Augmented Reality Using Deep Learning for Industry 4.0 Smart Factories,IEEE,Conferences,"This paper proposes to design, develop and implement a fast and markerless mobile augmented reality system to achieve the registration for, the visualization of, and the interaction with machines in indoor smart factories with Industry 4.0 vision. A lightweight deep-learning image detection module based on MobileNets running on mobile devices is used to detect/recognize different machines and different portions of machines. Internet of Things (IoT) networking allows machines and sensors in machines to report data, such as machine settings and machine states, to the cloud-side server. Thus, augmented information associated with a machine portion can be derived from the server and superimposed with the portion image shown on the device display. Furthermore, interaction methods based on touch gestures and distance calculation are also implemented. A prototype system is developed and tested in a mechanical workshop for the purpose of validation and evaluation. The system is shown to achieve high detection accuracy, intuitive visualization, and unique interaction modes.",https://ieeexplore.ieee.org/document/8377831/,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),23-27 July 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF53024.2021.9733776,Industry 4.0 and International Collaborative Online Learning in a Higher Education Course on Machine Learning,IEEE,Conferences,"The need for more efficient online learning strategies surged due to the global pandemic, providing opportunities to use global classrooms such as the COIL (Collaborative Online International Learning) model. COIL facilitates faculty members&#x0027; interactions at two different universities in different countries and creates virtual learning communities. Additionally, besides the pandemic, the advent of Industry 4.0 confronts graduates with the need to develop competencies in Machine Learning (ML), which are applied to resolve many industrial problems requiring prediction and classification and the availability and management of large amounts of data. This paper describes a global classroom in ML designed and implemented by professors at Tecnologico de Monterrey (Tec) in Mexico and the University of Applied Sciences W&#x00FC;rzburg-Schweinfurt (FHWS) in Germany<sup>1</sup><sup>1</sup>The collaboration was funded by the German Academic Exchange Service (DAAD). The global classroom&#x0027;s goal was to implement a joint international experience to develop machine learning competencies among students working in teams to resolve real problems with data-driven methods using an online digital platform called Remote Virtual Lab 4.0 (vLab).",https://ieeexplore.ieee.org/document/9733776/,2021 Machine Learning-Driven Digital Technologies for Educational Innovation Workshop,15-17 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DASA53625.2021.9682254,Industry 4.0 in maintenance: Using condition monitoring in electric machines,IEEE,Conferences,"Industry traditionally considered maintenance as a cost and a necessity to replace equipment and machines, but the path has changed to better focus on maintenance to prevent faults and it was designated as predictive. The ones motivated to take these advantages are faced with two of the biggest barriers: the investment it requires and the difficulty to develop algorithms. The costs of installation are still high, but the avoided costs surpass it. Also, Internet of Things (IoT) has brought a big shift, which is been known as the industry 4.0. One of the potentials in maintenance is the conditioning monitoring. Condition monitoring sensors and devices are now linked to maintenance platforms, providing real-time data. This new connectivity is both more affordable and easier to implement than predictive maintenance. Real-time data allows managers to adjust preventive maintenance plans while providing greater reliability. At the same time, artificial intelligence manages this data to recognize patterns, which is one of the most promising advances in digital reliability. So, regardless of the ability to immediately implement a preventive maintenance plan, condition monitoring is an asset itself. The present paper presents the common faults on electric machines, their effects, their impact on the industry and the main techniques on condition control to prevent them. It is also added the reflection on the use of IoT to enhance the potential of condition control maintenance. The implementation of continuous improvement actions throughout the life of the equipment allows to increase efficiency, either by overcoming weaknesses or by adapting production or operational capacities to processes, production or maintenance, avoiding under maintenance or over maintenance and minimizing operating costs.",https://ieeexplore.ieee.org/document/9682254/,2021 International Conference on Decision Aid Sciences and Application (DASA),7-8 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/MIPRO48935.2020.9245232,Integrating Industry Seminars within a Software Engineering Module to Enhance Student Motivation,IEEE,Conferences,"Engineering students increasingly demand and require coverage of emerging technologies to prepare themselves for subsequent research and employment. Industry and professional bodies are also concerned that engineering education doesn't always prepare students adequately for the world of work. The software engineering postgraduate professional practice module at University College London is designed to provide real-world experience, before students commence their industry research projects. Industry speakers are invited from a range of organizations, including ThoughtWorks, IBM, Form3, Verne Global, and Fujitsu. Seminars include: DevOps, microservices, cloud-native architectures, machine learning and quantum technologies. Before each topic is covered, students are asked their understanding of the subject matter, via questionnaires. This information is shared with industry speakers to ensure the content of presentations is compatible with students' prior knowledge. It has proved valuable to allow time for discussions to facilitate professional networking, which particularly benefits female students. Students have indicated they highly value the real-world project examples delivered by industry experts. This suggests that integrating industry seminars can enhance engineering education and motivate students by covering leading-edge technologies and practices. However, this requires considerable time in coordinating and codeveloping seminars, and such initiatives need to be adequately resourced to be effective.",https://ieeexplore.ieee.org/document/9245232/,"2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO)",28 Sept.-2 Oct. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII52469.2022.9708896,Integration of a reconfigurable robotic workcell for assembly operations in automotive industry,IEEE,Conferences,"This paper deals with the integration of a flexible, reconfigurable work cell performing assembly of parts in the automotive industry. The unique feature of the developed cell is that it can function in two modes: a) entirely autonomously or b) in cooperation with a human, where the operation of the robot dynamically adapts to human actions. We have implemented technologies for online recognition of human intention and for real-time learning of robust assembly policies to achieve the desired outcome. This challenging goals dictate the integration of modern deep learning algorithms, statistical learning, and compliant robot control into a unique ROS-based robot control system.",https://ieeexplore.ieee.org/document/9708896/,2022 IEEE/SICE International Symposium on System Integration (SII),9-12 Jan. 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTSI.2019.8895598,Intelligent Embedded Load Detection at the Edge on Industry 4.0 Powertrains Applications,IEEE,Conferences,"In the context of Industry 4.0, there has been great focus in developing intelligent sensors. Deploying them, condition monitoring and predictive maintenance have become feasible solutions to minimize operating and maintenance costs while also increasing safety. A critical aspect is the applied load to the supervised machinery system. Vibration data can be used to determine the current condition, but this needs signal processing specially developed and adapted to the monitored machine part for feature extraction. Artificial intelligence (AI) can, on one hand, simplify the development of such special purpose processing and on another hand be used to monitor and classify machine conditions by learning features directly from data. By bringing the AI computation as close as possible to the sensor (Edge-AI), data bandwidth can be minimized, system scalability and responsiveness can be improved and real-time requirements can be fulfilled. This work describes how Edge-AI on a STM32-bit microcontroller can be implemented. Our experimental setup demonstrates how AI can be effectively used to detect and classify the load on a powertrain. In order to do this, we use a MEMS capacity accelerometer to sense vibrations of the system. Also, this work demonstrates how Deep Neural Networks (DNN) for signal classification are build and trained by using an open-source deep learning framework and how the code library for the microcontroller is automatically generated afterwards by using STM32Cube. AI toolchain. We compare the classification accuracy of a memory compressed DNN against an uncompressed DNN.",https://ieeexplore.ieee.org/document/8895598/,2019 IEEE 5th International forum on Research and Technology for Society and Industry (RTSI),9-12 Sept. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIM46487.2021.9517377,Introducing adaptive mechatronic designs in bulk handling industry,IEEE,Conferences,"The advances of mechatronic system design and system integration have shown improvements in functionality, performance and energy efficiency in many applications across industries, from autonomous ground vehicles and drones to conveyor belts. This trend has been adopted in some industries more than others. The design of equipment to handle granular or bulk material is commonly based on traditional approaches. Therefore, introducing mechatronic concepts in the design procedure can enable new possibilities, such as sensor integration and data analyses, adaptability and control. The efficiency of bulk material handling equipment in ports, agriculture and food processing is heavily influenced by the operational conditions. Typically, a piece of equipment is designed for defined operational conditions when the maximum performance can be achieved. In this work the concept of adaptability to varying operational conditions is explored by understanding the technologies implemented in other industries and the feasibility to be implemented in the bulk handling equipment design. Sensing technology, actuation and adaptability are systematically presented in this work to support the design process of the next generation of bulk handling equipment. This will pave the way for incorporating the technological trends in the design, such as: sustainability, “smartness”, Internet of Things, Industry 4.0, digital twin and machine learning. Adaptive mechatronic solutions will play a crucial role in generating and implementing innovative sustainable solutions for bulk handling equipment.",https://ieeexplore.ieee.org/document/9517377/,2021 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM),12-16 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/icp.2022.0311,Introducing self-sustainable cloud platform for data management and extraction of actionable knowledge for smart healthcare industry: a COVID-19 case study,IET,Conferences,"The novel COVID-19 is a highly contagious disease. Data scientists worldwide are attempting to respond to the pandemic by building Artificial Intelligence (AI) solutions like forecasting pandemic growth, speculating possible mutations, identifying the symptoms caused, and many more. The models require vast quantities of data to make predictions. For a newly identified virus, it may take many months or sometimes years to collect related data and prepare it for data analysis purposes, which can further delay the process of making AI solutions. Hence, there is a need for a pipeline system which can facilitate a quick transmission of medical data from healthcare providers to data scientists. This paper proposes a cloud computing platform that allows smart cities to respond to the pandemic faster, with the collaboration of public health centers and data scientists. The platform provides a structured way of identifying and utilizing collaboration opportunities between health centers and the data science community, generating actionable knowledge. The system consists of two parts: 1) The software on the hospital's side, allowing real-time data gathering and automated uploading to cloud servers, 2) The cloud system to facilitate data storing along with model building and deploying. Customers can use the deployed models on a prepaid basis, the money collected will be divided among data scientists and data providers. This unique feature ensures the healthy participation of data providers in the process of making healthcare solutions for the smart cities. A sponsor can also sponsor a project. Hence, the system will sustain on its own with the involvement of stakeholders.",https://ieeexplore.ieee.org/document/9770607/,4th Smart Cities Symposium (SCS 2021),21-23 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IDITR54676.2022.9796493,KOOPI: Keypoint-Oriented Object Positioning in Industry,IEEE,Conferences,"In manufacturing, object detection via industrial images enables many typical applications such as positioning of screw holes, electronic components, and other devices. The conventional method based on classical image processing generally consists of two steps: image feature extraction and object classification, so that it usually results in a low detection speed and poor accuracy due to its complicated procedure. In recent years, thanks to the rapid development of deep learning networks, higher classification accuracy and less computing performance requirement can be achieved in many typical applications, compared to using the conventional schemes. In this paper, by investigating object detection based on deep learning, a new idea utilizing some keypoint-oriented deep learning networks to the workpiece positioning area is proposed and verified by collecting dataset from practical workpieces. Our novel method performs competitively with existing schemes and runs in real-time. As for the simulation of screw hole positioning, the proposed network can effectively detect the screw hole in the image and accurately locate it. By comparing with commercial software such as Halcon&#x00AE; or VisionPro&#x00AE;, the feasibility of applying keypoint-oriented deep learning networks to intelligent manufacturing is validated.",https://ieeexplore.ieee.org/document/9796493/,2022 International Conference on Innovations and Development of Information Technologies and Robotics (IDITR),27-29 May 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDE51399.2021.00283,Learning to Optimize Industry-Scale Dynamic Pickup and Delivery Problems,IEEE,Conferences,"The Dynamic Pickup and Delivery Problem (DPDP) is aimed at dynamically scheduling vehicles among multiple sites in order to minimize the cost when delivery orders are not known a priori. Although DPDP plays an important role in modern logistics and supply chain management, state-of-the-art DPDP algorithms are still limited on their solution quality and efficiency. In practice, they fail to provide a scalable solution as the numbers of vehicles and sites become large. In this paper, we propose a data-driven approach, Spatial-Temporal Aided Double Deep Graph Network (ST-DDGN), to solve industry-scale DPDP. In our method, the delivery demands are first forecast using spatial-temporal prediction method, which guides the neural network to perceive spatial-temporal distribution of delivery demand when dispatching vehicles. Besides, the relationships of individuals such as vehicles are modelled by establishing a graph-based value function. ST-DDGN incorporates attention-based graph embedding with Double DQN (DDQN). As such, it can make the inference across vehicles more efficiently compared with traditional methods. Our method is entirely data driven and thus adaptive, i.e., the relational representation of adjacent vehicles can be learned and corrected by ST-DDGN from data periodically. We have conducted extensive experiments over real-world data to evaluate our solution. The results show that ST-DDGN reduces 11.27% number of the used vehicles and decreases 13.12% total transportation cost on average over the strong baselines, including the heuristic algorithm deployed in our UAT (User Acceptance Test) environment and a variety of vanilla DRL methods. We are due to fully deploy our solution into our online logistics system and it is estimated that millions of USD logistics cost can be saved per year.",https://ieeexplore.ieee.org/document/9458860/,2021 IEEE 37th International Conference on Data Engineering (ICDE),19-22 April 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCO53203.2021.9663861,MAS agents development for mining industry,IEEE,Conferences,"The essence of multi-agent technology is a fundamentally new method of solving problems. In contrast to the classical method, when a search is carried out a well-defined (deterministic) algorithm, which allows find the best solution to the problem in multi-technology solution is obtained automatically as a result of the interaction of many self-parking enforcement targeted software modules - the so-called software agents ants. Often classical methods for solving problems are not applicable in real life. There are various fields where MAS could be implemented, for the research of this paper the mining industry where taken.This paper explains MAS, its classification, shows the possibility of use of agent modeling in real industry. The article describes the steps of the development of agents, and the testing of them on a build-up layout of quarry and on working prototypes of the dumper and excavator robots.",https://ieeexplore.ieee.org/document/9663861/,2021 16th International Conference on Electronics Computer and Computation (ICECCO),25-26 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SCC47175.2019.9116104,Modeling and management of human resources in the reconfiguration of production system in industry 4.0 by neural networks,IEEE,Conferences,"In Industry 4.0, the role of employees changes significantly. Real-time production line control transforms job content. Work processes affect working conditions. The implementation of a socio-technical approach to the organization of work gives workers the opportunity to adapt their skills. Indeed, production work will become more and more multi-factor, especially with regard to control and decision-making tasks. In this paper, a proposal for an intelligent system for modeling skills and human resource management in the production system chain through the use of two artificial neural networks. The first NN1 network allows for the identification of the human factor, as well as the second NN2 network is reserved for valuing the human skills needed in Industry 4.0.",https://ieeexplore.ieee.org/document/9116104/,"2019 International Conference on Signal, Control and Communication (SCC)",16-18 Dec. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2019.8702575,Multi-View Fusion Neural Network with Application in the Manufacturing Industry,IEEE,Conferences,"In recent years the research community and industry have paid high attention to the field of machine learning, especially deep learning. Nowadays many real-world classification or rather prediction applications are implemented by neural network models. We propose a multi-view fusion neural network with application in the manufacturing industry. Image information of multiple cameras is fused and used by the proposed model to predict the state of a manufacturing machine. Experiments show that the overall classification performance is increased from a baseline of 92.7% to 99.5% by the fusion model.",https://ieeexplore.ieee.org/document/8702575/,2019 IEEE International Symposium on Circuits and Systems (ISCAS),26-29 May 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/DATE48585.2020.9116407,Network Synthesis for Industry 4.0,IEEE,Conferences,"Today's factory machines are ever more connected with SCADA, MES, ERP applications as well as external systems for data analysis. Different types of network architectures must be used for this purpose. For instance, control applications at the lowest level are susceptible to delays and errors while data analysis with machine learning procedures requires to move a large amount of data without real-time constraints. Standard data formats, like Automation Markup Language (AML), have been established to document factory environment, machine placement and network deployment, however, no automatic technique is currently available in the context of Industry 4.0 to choose the best mix of network architectures according to spacial constraints, cost, and performance. We propose to fill this gap by formulating an optimization problem. First of all, spatial and communication requirements are extracted from the AML description. Then, the optimal interconnection of wired or wireless channels is obtained according to application objectives. Finally, this result is back-annotated to AML to be used in the life cycle of the production system. The proposed methodology is described through a small, but complete, smart production plant.",https://ieeexplore.ieee.org/document/9116407/,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",9-13 March 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEIC54506.2022.9748822,Object detection based on deep learning techniques in resource-constrained environment for healthcare industry,IEEE,Conferences,"Advanced technologies and algorithms such as the Internet of Things (IoT), computer vision (CV), and deep learning are widely used in the healthcare industry to enhance global med-ical care. Internet of Things (IoT) has the potential to be limitless due to increased network agility, integrated artificial intelligence (AI), and the ability to deploy and automate systems. Embedded systems playa vital role in IoT due to real-time computing, low power consumption, and low maintenance cost. Object detection is a computer vision technique that aims to process and identify certain objects such as people, cars, animals, or buildings in digital images or videos. The goal of object detection is to develop computational models for computer vision applications. Recently, rapid advancement in deep learning techniques accelerated the momentum of object detection. In this study, we proposed a mechanism to perform object detection based on deep learning techniques in resource-constrained IoT devices. Due to limited computational powers in embedded systems, the performance of deep learning algorithms is not good enough. To achieve this, we compressed the video using a codec and streamed it to the amazon cloud for object detection. Video codec was used to uncompress the video in its original format so that there is no loss of video quality. A pre-trained YOLO model was deployed for object detection in medical images. The output is sent to the client using a lightweight protocol for data communication. Results indicate that the proposed mechanism worked well in a resource-constrained environment without compromising accuracy and time.",https://ieeexplore.ieee.org/document/9748822/,"2022 International Conference on Electronics, Information, and Communication (ICEIC)",6-9 Feb. 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UPEC.2015.7339883,Optimising the use of electrical energy within the waste water industry through improved utilization of process control and automation,IEEE,Conferences,"Active system control, real time control and active system management are some of the new concepts the water utilities are investigating as a way of improving asset energy usage. The assets under most scrutiny are those with high energy burdens such as pumps and blowers. Fuzzy logic, genetic algorithms and artificial neural networks are an increasing form of control employed within software systems. The increase in availability of faster processors permits engineers to run larger, more complex models with higher precision and reliability. By reviewing the already active forms of control variants, the current work aims to integrate a block wise hydraulic model with live process data, captured from field sensors, with a view to deploying a combined optimisation technique that in turn focuses on reducing demand side energy consumption. This paper presents an initial review of these systems and an incipient proposed approach.",https://ieeexplore.ieee.org/document/7339883/,2015 50th International Universities Power Engineering Conference (UPEC),1-4 Sept. 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSME.2017.41,Predicting and Evaluating Software Model Growth in the Automotive Industry,IEEE,Conferences,"The size of a software artifact influences the software quality and impacts the development process. In industry, when software size exceeds certain thresholds, memory errors accumulate and development tools might not be able to cope anymore, resulting in a lengthy program start up times, failing builds, or memory problems at unpredictable times. Thus, foreseeing critical growth in software modules meets a high demand in industrial practice. Predicting the time when the size grows to the level where maintenance is needed prevents unexpected efforts and helps to spot problematic artifacts before they become critical.Although the amount of prediction approaches in literature is vast, it is unclear how well they fit with prerequisites and expectations from practice. In this paper, we perform an industrial case study at an automotive manufacturer to explore applicability and usability of prediction approaches in practice. In a first step, we collect the most relevant prediction approaches from literature, including both, approaches using statistics and machine learning. Furthermore, we elicit expectations towards predictions from practitioners using a survey and stakeholder workshops. At the same time, we measure software size of 48 software artifacts by mining four years of revision history, resulting in 4,547 data points. In the last step, we assess the applicability of state-of-the-art prediction approaches using the collected data by systematically analyzing how well they fulfill the practitioners' expectations.Our main contribution is a comparison of commonly used prediction approaches in a real world industrial setting while considering stakeholder expectations. We show that the approaches provide significantly different results regarding prediction accuracy and that the statistical approaches fit our data best.",https://ieeexplore.ieee.org/document/8094464/,2017 IEEE International Conference on Software Maintenance and Evolution (ICSME),17-22 Sept. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSAA53316.2021.9564181,Predictive maintenance based on anomaly detection using deep learning for air production unit in the railway industry,IEEE,Conferences,"Predictive maintenance methods assist early detection of failures and errors in machinery before they reach critical stages. This study proposes a data-driven predictive maintenance framework for the air production unit (APU) system of a train of Metro do Porto by deep learning based on a sparse autoencoder (SAE) network that efficiently detects abnormal data and considerably reduces the false alarm rate. Several analog and digital sensors installed on the APU system allow the detection of behavioral changes and deviations from the normal pattern by analyzing the collected data. We implemented two versions of the SAE network in which we inputted analog sensors data and digital sensors data, and the experimental results show that the failures due to air leakage problems are predicted by analog sensors data while other types of failures are identified by digital sensors data. A low pass filter is applied to the output of the SAE network, and a sequence of abnormal data is used as an alarm for the APU system failure. Performance indicators of the SAE network with digital sensors data, in terms of F1 Score, Recall, and Precision, are respectively, about 33.6&#x0025;, 42&#x0025;, and 28&#x0025; better than those of the SAE network with analog sensors data. For comparison purposes, we also implemented a variational autoencoder (VAE). The results show that SAE performance is better than that of VAE by 14&#x0025;, 77&#x0025;, and 37&#x0025; respectively, for Recall, Precision and F1 Score.",https://ieeexplore.ieee.org/document/9564181/,2021 IEEE 8th International Conference on Data Science and Advanced Analytics (DSAA),6-9 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC45102.2020.9294450,Predictive maintenance leveraging machine learning for time-series forecasting in the maritime industry,IEEE,Conferences,"One of the key challenges in the maritime industry refers to minimizing the time a vessel cannot be utilized, which has multiple effects. The latter is addressed through maintenance approaches that however in many cases are not efficient in terms of cost and downtime. Predictive maintenance provides optimized maintenance scheduling offering extended vessel lifespan, coupled with reduced maintenance costs. As in several industries, including the maritime domain, an increasing amount of data is made available through the deployment and exploitation of data sources, such as on board sensors that provide real-time information. These data provide the required ground for analysis and thus support for various types of data-driven decision making. In the maritime domain, sensors are deployed on vessels to monitor their engines and data analysis tools are needed to assist engineers towards reduced operational risk through predictive maintenance solutions that are put in place. In this paper, we present an approach for anomaly detection on time-series data, utilizing machine learning on the vessels sensor data, in order to predict the condition of specific parts of the vessel's main engine and thus facilitate predictive maintenance. The novel characteristic of the proposed approach refers both to the inclusion of new innovative models to address the case of predictive maintenance in maritime and the combination of those different models, highlighting an improved result in terms of evaluation metrics.",https://ieeexplore.ieee.org/document/9294450/,2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),20-23 Sept. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AQTR49680.2020.9129934,Rapid Prototyping of IoT Applications for the Industry,IEEE,Conferences,"In this article a novel approach to rapid IoT application prototype design and development is presented using an existing experimental dataset and a functional model. Using the existing data, we populate our NoSQL Apache Cassandra database cluster with legacy data and generate similar data using a python code, considering the Mosquitto MQTT protocol implementation and Node-RED node.js development environment. Using Node-RED, we display the data already collected, and dynamically create new data that can be monitored in real-time in the provided dashboard. The possibilities and utility of this approach are explored in the article, and a simple prototype application for modeling the open access Combined Cycle Power Plant (CCPP) dataset provided by the UCI Machine Learning Repository is presented to prove the efficiency and rapidity of IoT application development. The presented system development approach can be used in industrial environment for rapid development of IIoT applications.",https://ieeexplore.ieee.org/document/9129934/,"2020 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",21-23 May 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IMITEC50163.2020.9334129,Real Time Customer Churn Scoring Model for the Telecommunications Industry,IEEE,Conferences,"There are two types of customers in the telecommunication industry; the pre-paid and the contract customers. In South Africa it is the pre-paid customers that keep telcos constantly worried because such customers do not have anything binding them to the company, they can leave and join a competitor at any time. To retain such customers, telcos need to customise suitable solutions especially for those customers that are agitating and can churn at any time. This needs customer churn prediction models that would take advantage of big data analytics and provide the telco industry with a real time solution. The purpose of this study was to develop a real time customer churn prediction model. The study used the CRISP-DM methodology and the three machine learning algorithms for implementation. Watson Studio software was used for the model prototype deployment. The study used the confusion matrix to unpack a number of performance measures. The results showed that all the models had some degree of misclassification, however the misclassification rate of the Logistic Regression was very minimal (2.2%) as differentiated from the Random Forest and the Decision Tree, which had misclassification rates of 20.8% and 21.7% respectively. The results further showed that both Random Forest and the Decision Tree had good accuracy rates of 78.3% and 79.2% respectively, although they were still not better than that of the Logistic Regression. Despite the two having good accuracy rates, they had the highest rates of misclassification of class events. The conclusion we drew from this was that, accuracy is not a dependable measure for determining model performance.",https://ieeexplore.ieee.org/document/9334129/,2020 2nd International Multidisciplinary Information Technology and Engineering Conference (IMITEC),25-27 Nov. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZY.2008.4630459,Real time Takagi-Sugeno fuzzy model based pattern recognition in the batch chemical industry,IEEE,Conferences,"This contribution describes the real time pressure check pattern recognition of an industrial batch dryer. The goal is to identify the start of the drying process and to calculate the time elapsed between two consequent batch starts (batch time) right after the batch has completed. The presented pattern recognition method implements a supervised learning approach based on Takagi-Sugeno fuzzy (TS) models. The decision maker design is based on plant data compressed by the PI algorithm (OSI Software, Inc). It is concluded that the developed classifier is able to perform real time classification and the compressed PI data can be used in order to design data analysis tools which are useful for chemical batch plant operation investigations.",https://ieeexplore.ieee.org/document/4630459/,2008 IEEE International Conference on Fuzzy Systems (IEEE World Congress on Computational Intelligence),1-6 June 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSIMA47653.2019.9057343,Real-Time Wireless Monitoring for Three Phase Motors in Industry: A Cost-Effective Solution using IoT,IEEE,Conferences,"In recent days modern environment industries are facing rapid flourishing for performance capabilities and their requirements for corporate clients and industrial sector. Internet of Things (IoT) is an innovative and rapidly growing field for automation and evaluation in networks, Artificial Intelligence, data sensing, data mining, and big data. These systems have a great tendency to monitor and control different process used in industries. IoT systems have been implemented and have applications in different industries due to their cost-effectiveness and flexibility In this paper we have developed a system which includes real-time monitoring of current reading of three-phase motor through a wireless network. With the help of this system, data can be saved and monitored and then transmitted to cloud storage. This system contains Arduino-UNO board, ACS-712 current sensor, ESP-8266 Wi-Fi module which sends information to an IoT API service THING-SPEAK that behave like a cloud for various sensors to monitor data. The proposed system was successfully deployed in Aisha Steel Mills, Karachi, Pakistan.",https://ieeexplore.ieee.org/document/9057343/,"2019 IEEE International Conference on Smart Instrumentation, Measurement and Application (ICSIMA)",27-29 Aug. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIBA52610.2021.9688007,Research and Implementation of User Behavior Simulation Technology Based on Power Industry Cyber Range,IEEE,Conferences,"The current cyber range of the electric power industry has background traffic simulation problems, prospect behavior generation problems, and target simulation diversity problems in large-scale network environments. In response to these problems, this paper proposes a user behavior simulation technology suitable for power industry cyber range. We perform large-scale, high-fidelity network user behavior simulation by combining real traffic playback, inject user behavior traffic in the real network into the virtual target network, then model the user behavior in multiple scenarios in the power network space and generate it based on the method of sequence prediction, and finally realize the diversity simulation of user behavior.",https://ieeexplore.ieee.org/document/9688007/,"2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)",17-19 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2009.5212284,SVM optimized scheme based PSO in application of engineering industry process,IEEE,Conferences,"Aimed to the problem that it is hardship to get real-time and on-line measuring parameters in wood drying process, a novel PSO-SVM model that hybridized the particle swarm optimization (PSO) and support vector machines (SVM) to improve the nonlinearity caused by ambient temperature and other disturbance factors is presented. Support vector machines (SVM) based on statistical learning theory and structural risk minimization is proposed to deal with these problems. However, the model complexity and generalization performance of support vector machines (SVM) depend on a good setting of the three parameters (ε,c,γ). In this paper, the particle swarm optimization is applied to optimize the parameters (ε,c,γ) at the same time. Based on the proposed method, both PSO-SVM and SVM models are established and implemented to estimate lumber moisture content value in wood drying process. The result of comparative analysis is given. Experimental results show that solutions obtained by PSO-SVM training seem to be more robust and better generalization performance compared to SVM training.",https://ieeexplore.ieee.org/document/5212284/,2009 International Conference on Machine Learning and Cybernetics,12-15 July 2009,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I-SMAC49090.2020.9243544,Scalable IoT Solution using Cloud Services – An Automobile Industry Use Case,IEEE,Conferences,"The role of IoT and related internet-based applications in otherwise mechanical devices to monitor, manage and enhance the performance of the same is quite widespread now. Almost all public cloud service providers provide scalable, fully managed and elastic IoT related services. The data flows from these services are essentially streaming and can be consumed for further use in various predictive, descriptive and visualization modules. The cloud platforms enable ingestion, transformation and usage of the data by providing streaming, machine learning and sharable visualization services. This ecosystem greatly reduces the time to create IoT based minimum viable product creation which in turn enhances the business value realization cycle. The effect of cycle time reduction to design, architect and develop IoT solutions leads to a rapid improvement of business lead time and makes it easier for businesses to gain from the data insights and plan the next course of action. In this paper, one such enterprise graded use case is explored, in which the Azure IoT platform in terms of the offerings and associated ecosystem of Azure Stream Analytics and Azure Machine learning services are explained. This paper covers design, architecture, development and deployment of the solution prepared and how the same is monitored once in production. Security is a very important aspect of the same and here the security architecture is being explored. A conclusion is presented with the scope of future enhancements using auto ML services in serverless platforms to enable real-time automated decision making augmented with human expertise and intelligence.",https://ieeexplore.ieee.org/document/9243544/,"2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)",7-9 Oct. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IDAP.2018.8620887,Smart Grids and Industry 4.0,IEEE,Conferences,"Since the beginning of the Industrial Revolution, the concept of Industrial Revolution in the 4th/ Industry 4.0 which is a new turning point in the technologies used, is in every field as well as in the energy field which is considered as the key point of all revolutions. It is aimed that energy generation, transmission and distribution are more efficient and highly reliable together with the next generation software and hardware which is the result of this concept.It has been seen that existing networks can not be enough to achieve this goal.At this point, flexible, reliable, clean, sustainable and highly efficient electric energy due to smart grids come into play. However, due to the increase in demand for electricity due to increase in population and industrialization, the dependence of electric energy on production resources, the availability of renewable energy sources in a dispersed geographical structure and the loss and leakage are becoming more important in our country. The Cyber-Physical System, the Internet of Things, M2M (machine to machine), etc components are used in the energy field, especially in intelligent network systems, resulting in intensive use of technology at every stage from power generation to end-user distribution point. This includes better system operation, reduced costs, increased energy efficiency, reduced greenhouse effect, reduced downtime with reduced downtime, reduced loss/leakage rates, improved energy quality, better management of production and storage systems, intelligent meter reading and load management and real-time supply-demand management. Industry 4.0/4.Industrial Revolution and smart grids have many common aspects such as optimization, automation, efficient use and management of energy, intelligent production, everything internet. In this study, the development process of the Industrial 4.0 revolution in the world and in our country will be examined and the interaction with these process smart grids will be examined.",https://ieeexplore.ieee.org/document/8620887/,2018 International Conference on Artificial Intelligence and Data Processing (IDAP),28-30 Sept. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WRC-SARA.2019.8931920,Software-defined Cloud Manufacturing in the Context of Industry 4.0,IEEE,Conferences,"In the practice of &#x201C;Cloud Manufacturing (CMfg)&#x201D; or &#x201C;Industrial Internet&#x201D;, there still exist key problems, including: 1) big data analytics and decision-making in the cloud could not meet the requirements of time-sensitive manufacturing applications, moreover uploading ZettaBytes of future device data to the cloud may cause serious network congestion, 2) the manufacturing system lacks openness and evolvability, thus restricting the rapid optimization and transformation of the system, 3) big data from the shop-floor IoT devices and the internet has not been effectively utilized to guide the optimization and upgrade of the manufacturing system. In view of these key practical problems, we propose an open evolutionary architecture of intelligent CMfg system with collaborative edge and cloud processing capability. Hierarchical gateways near shop-floor things are introduced to enable fast processing for time-sensitive applications. Big data in another dimension from the software defined perspective will be used to decide the efficient operations and highly dynamic upgrade of the system. From the software system view, we also propose a new mode - AI-Mfg-Ops (AI-enabled Cloud Manufacturing Operations) with a supporting framework, which can promote the fast operation and upgrading of CMfg systems with AI enabled monitoring-analysis-planning-execution close loop. This work can improve the universality of CMfg for real-time fast response and operation &#x0026; upgrading.",https://ieeexplore.ieee.org/document/8931920/,2019 WRC Symposium on Advanced Robotics and Automation (WRC SARA),21-22 Aug. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRITO48877.2020.9198036,State of Art: Energy Efficient Protocols for Self-Powered Wireless Sensor Network in IIoT to Support Industry 4.0,IEEE,Conferences,"Up gradation of manufacturing systems in industries by means of implementing innovative manufacturing techniques that captures real time data, applies machine learning algorithms, makes entire system self-decisive and provides inter connectivity to the whole system is the prime focus of the Industry 4.0. It is aimed at bringing new industrial revolution with the help of internet of things technology due to its considerable influence in the industrial manufacturing process. Though impact of internet of things in industrial sector is huge, a practical implementation incorporates challenges in energy efficiency, self-powered sensor nodes and security. For processing of gathered data self-powered sensor nodes may sinks energy from ambient energy sources. A considerable amount of efforts has been put by researchers to address the challenges for development of energy efficient routing protocol for such nodes. The presented survey is aimed at analyzing the protocols for contribution towards the goals of industry 4.0 Energy efficient protocol will support the system to consume least energy for its operation. Henceforth combination of self-powered wireless sensor network and energy efficient protocol will be useful to power up many industrial IoT applications.",https://ieeexplore.ieee.org/document/9198036/,"2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",4-5 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE-SEIP55303.2022.9793981,Testing Machine Learning Systems in Industry: An Empirical Study,IEEE,Conferences,"Machine learning becomes increasingly prevalent and integrated into a wide range of software systems. These systems, named ML systems, must be adequately tested to gain confidence that they behave correctly. Although many research efforts have been devoted to testing technologies for ML systems, the industrial teams are faced with new challenges on testing the ML systems in real-world settings. To absorb inspirations from the industry on the problems in ML testing, we conducted an empirical study including a survey with 87 responses and interviews with 7 senior ML practitioners from well-known IT companies. Our study uncovers significant industrial concerns on major testing activities, i.e., test data collection, test execution, and test result analysis, and also the good practices and open challenges from the perspective of the industry. (1) Test data collection is conducted in different ways on ML model, data, and code and faced with different challenges. (2) Test execution in ML systems suffers from two major problems: entanglement among the components and the regression on model performance. (3) Test result analysis centers on quantitative methods, e.g., metric-based evaluation, and is combined with some qualitative methods based on practitioners&#x2019; experience. Based on our findings, we highlight the research opportunities and also provide some implications for practitioners.",https://ieeexplore.ieee.org/document/9793981/,2022 IEEE/ACM 44th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),22-24 May 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPDWinter52325.2021.00068,The Case Study on Use of Bigdata and AI in Distribution Industry,IEEE,Conferences,"With the development of telecommunication technology, a commerce platform centered on mobile devices is developing. Global mobile commerce companies such as Amazon are trying to recommend optimized products to customers and provide optimized logistics services through real-time big data. As online-oriented commerce develops, rival local offline stores are making changes for survival. In Korea, the government is trying to find a balance with offline stores by controlling big data-oriented online commerce, as is the regulation that sought to protect the local market by limiting the business hours of Super Supermarket (SSM) to balance local offline stores and Supermarket. However, in the data-driven fourth industrial revolution, we will have to find ways to develop by utilizing data, which is the main raw material. This paper tried to investigate cases of applying and operating big data in the distribution industry and seek ways to promote focused on big-data sharing that can develop with offline stores by conducting a Delphi survey to experts.",https://ieeexplore.ieee.org/document/9403518/,"2021 21st ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)",28-30 Jan. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MLISE54096.2021.00090,The Design and Implementation of Beidou Ground-based Augmentation System in Natural Resources Industry,IEEE,Conferences,"The Beidou Ground-based Augmentation System is an important part of the Beidou Navigation Satellite System (BDS). It can provide real-time centimeter-level positioning services, effectively solving the problem of low positioning accuracy and poor real-time performance of BDS in natural resource industry. This issue introduces the design and implementation of Beidou Ground-based Augmentation System in natural resources industry, and verifies the system&#x0027;s performance in the practical applications of mine law-enforcement monitoring.",https://ieeexplore.ieee.org/document/9611663/,2021 International Conference on Machine Learning and Intelligent Systems Engineering (MLISE),9-11 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/icABCD49160.2020.9183853,The Impact of Smart Manufacturing Approach On The South African Manufacturing Industry,IEEE,Conferences,"SM is a technology-driven approach that mainly utilises machines to monitor the entire production of an organisation. The objective of SM in an organisation is to identify ways to automatize the manufacturing process while using data analytics to optimize the manufacturing performance. This research mitigates the impact of technology, in this case expressed as SM in South African industries. The research followed a quantitative approach whereby 42 respondents from low, medium low and high technology industries took part in the study. Data has been amassed from first-hand experience by mean of an adapted questionnaire constituted of three sections: The first section was about the general demographic information of the respondents. Section two investigates the respondent's awareness on SM. Finally, section three assessed the impact that SM had on the performance of the organisation. The findings of this study revealed that Smart Manufacturing has a positive impact in South African manufacturing organisations as it allows effective operations, fast response to customers demand, real time operations optimisation. Nevertheless, Smart Manufacturing is a new concept under the fourth industrial revolution in South Africa and will need time before being totally implemented in all organisations as it is costly.",https://ieeexplore.ieee.org/document/9183853/,"2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)",6-7 Aug. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PICMET.2015.7273077,The effects of disruption on different types of tile manufacturing Industry-layouts: An empirical investigation on tile manufacturing industry,IEEE,Conferences,"Almost all manufacturing facilities need to use production scheduling systems to increase productivity and reduce production costs. Most manufacturing industries invest huge amount of money to manufacture and supply products on time in order to meet customers demand and objectives but due to unforeseen disruptions, these objectives are difficult to achieve. In Real-life, production operations are subject to a large number of unexpected disruptions that may invalidate an original schedule. This work considered effects of disruption on different types of industry-layouts that are seldom been used in manufacturing industries namely: fixed, product, production and process industry-layouts. Questionnaires were used for data collection from a number of companies in Cameroon and reliably theory, simulation software's were using to analyze the data. The study reveals that, disruptions varies per industry-layout and the leading sources of disruptions are machine breakdown, power failure, employee's absenteeism and material shortage. It is concluded that disruption of one type may not greatly affect productivity of a certain industry-layout whilst similar disruptions can have devastating effects on another type and also, the impacts of disruption are dependent on the Industry-layouts.",https://ieeexplore.ieee.org/document/7273077/,2015 Portland International Conference on Management of Engineering and Technology (PICMET),2-6 Aug. 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIRE.2014.6894851,Transferring research into the real world: How to improve RE with AI in the automotive industry,IEEE,Conferences,"For specifications, people use natural language. We show that processing natural language and combining this with intelligent deduction and reasoning with ontologies can possibly replace some manual processes associated with requirements engineering (RE). Our prior research shows that the software tools we developed can indeed solve problems in the RE process. This paper shows this does not only work in the software engineering domain, but also for embedded software in the automotive industry. We use artificial intelligence in the sense of combining semantic knowledge from ontologies and natural language processing. This enables computer systems to “understand” requirement texts and process these with “common sense”. Our specification improver RESI detects flaws in texts such as ambiguous words, incomplete process words, and erroneous quantifiers and determiners.",https://ieeexplore.ieee.org/document/6894851/,2014 IEEE 1st International Workshop on Artificial Intelligence for Requirements Engineering (AIRE),26-26 Aug. 2014,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMAE.2017.8038685,Using artificial intelligence based expert system for selection of design subcontractors: A case study in aerospace industry,IEEE,Conferences,"As one of the top expectations for type certification of an aircraft, Aviation Authorities (AA) regulate design organization to establish Design Assurance System (DAS). DAS is composed of design, independent monitoring and airworthiness functions in which these functions are specialized for aerospace industry. Besides, Design Organization Approval (DOA) is a milestone to establish a rigid Design Assurance System. By this way, design organization assures aircraft development life cycle by complying with aviation regulations. To meet requirements of Design Organization Approval, Design Organization transfers its authority and technical signatories to its subcontractors to improve effectiveness of the system. So, performance of design subcontractors shall be traceable and measurable to match capability requirements of main contractor. Thus, subcontractor evaluation is a long and complicated process; survey implementation could be misleading in some cases. The purpose of this study is to propose a novel tool to measure performance of a design subcontractor according to necessities of Design Assurance System. Up to now, there is no tool to evaluate aviation design subcontractors. With this tool, contractor firm can evaluate multiple criteria in a single run. AHP is used to prioritize criteria relative to each other one-by-one. Then, for subcontractor selection and subcontractor monitoring, Artificial Neural Network (ANN) is applied to optimize decision making process. Annual Actual Data is applied in AHP model to assess current performance score of subcontractor. To have a long term judgment of this system, the model shall be applied to a design subcontractor for more than once on fixed periods such as quarterly, yearly etc.",https://ieeexplore.ieee.org/document/8038685/,2017 8th International Conference on Mechanical and Aerospace Engineering (ICMAE),22-25 July 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISWCS.2019.8877305,Visible Light Positioning for Location-Based Services in Industry 4.0,IEEE,Conferences,"Industry 4.0 refers to the evolution in manufacturing from computerization to fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts. In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0 applications. We also explore sensing techniques, including cameras (and depth sensors), and other light-based solutions for object positioning and detection along with their respective limitations. We then demonstrate an application of positioning for real time robot control in an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience with this cyberphysical-virtual application, we propose Ray-Surface Positioning (RSP), a novel VLP technique, as a low cost positioning system for Industry 4.0.",https://ieeexplore.ieee.org/document/8877305/,2019 16th International Symposium on Wireless Communication Systems (ISWCS),27-30 Aug. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BDCloud.2018.00136,"iSTEP, an Integrated Self-Tuning Engine for Predictive Maintenance in Industry 4.0",IEEE,Conferences,"The recent expansion of IoT-enabled (Internet of Things) devices in manufacturing contexts and their subsequent data-driven exploitation paved the way to the advent of the Industry 4.0, promoting a full integration of IT services, smart devices, and control systems with physical objects, their electronics and sensors. The real-time transmission and analysis of collected data from factories has the potential to create manufacturing intelligence, of which predictive maintenance is an expression. Hence the need to design new approaches able to manage not only the data volume, but also the variety and velocity, extracting actual value from the humongous amounts of collected data. To this aim, we present iSTEP, an integrated Self-Tuning Engine for Predictive maintenance, based on Big Data technologies and designed for Industry 4.0 applications. The proposed approach targets some of the most common needs of manufacturing enterprises: compatibility with both the on-premises and the in-the-cloud environments, exploitation of reliable and largely supported Big Data platforms, easy deployment through containerized software modules, virtually unlimited horizontal scalability, fault-tolerant self-reconfiguration, flexible yet friendly streaming-KPI computations, and above all, the integrated provisioning of self-tuning machine learning techniques for predictive maintenance. The current implementation of iSTEP exploits a distributed architecture based on Apache Kafka, Spark Streaming, MLlib, and Cassandra; iSTEP provides (i) a specific feature engineering block aimed at automatically extracting metrics from the production monitoring time series, which improves the predictive performance by 77% on average, and (ii) a self-tuning approach that dynamically selects the best prediction algorithm, which improves the predictive performance up to 60%. The iSTEP engine provides transparent predictive models, able to provide end users with insights into the knowledge learned, and it has been experimentally evaluated on a public unbalanced failure dataset, whose extensive results are discussed in the paper.",https://ieeexplore.ieee.org/document/8672266/,"2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom)",11-13 Dec. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TLA.2021.9480156,A Data Governance Framework for Industry 4.0,IEEE,Journals,"The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This digitization of the industrial environment is characterized by the connection of Information Technologies (IT) and Operations Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT). One of the main consequences of this integration is the increasing amount and variety of data generated in real time from different sources. In this environment of intensive generation of actionable information, data becomes a critical asset for Industry 4.0, at all stages of the value chain. However, in order to data become a competitive advantage for the company, it must be managed and governed like any other strategic asset, and therefore it is necessary to rely on a Data Governance system. Industry 4.0 requires a reformulation of governance since the data is a key element and the backbone of the processes of the organization. This paper proposes a Reference Framework for the implementation of Data Governance Systems for Industry 4.0. Previously, it contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, cloud and edge computing, artificial intelligence and current regulations.",https://ieeexplore.ieee.org/document/9480156/,IEEE Latin America Transactions,Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3121302,A Survey on Industry 4.0 for the Oil and Gas Industry: Upstream Sector,IEEE,Journals,"The market volatility in the oil and gas (O&G) sector, the dwindling demand for oil due to the impact of COVID-19, and the push for alternative greener energy are driving the need for innovation and digitization in the O&G industry. This has attracted research interest from academia and the industry in the application of industry 4.0 (I4.0) technologies in the O&G sector. The application of some of these I4.0 technologies has been presented in the literature, but the domain still lacks a comprehensive survey of the application of I4.0 in the O&G upstream sector. This paper investigates the state-of-the-art efforts directed toward I4.0 technologies in the O&G upstream sector. To achieve this, first, an overview of the I4.0 is discussed followed by a systematic literature review from an integrative perspective for publications between 2012–2021 with 223 analyzed documents. The benefits and challenges of the adoption of I4.0 have been identified. Moreover, the paper adds value by proposing a framework for the implementation of I4.0 in the O&G upstream sector. Finally, future directions and research opportunities such as framework, edge computing, quantum computing, communication technologies, standardization, and innovative areas related to the implementation of I4.0 in the upstream sector are presented. The findings from this review show that I4.0 technologies are currently being explored and deployed for various aspects of the upstream sector. However, some of the I4.0 technologies like additive manufacturing and virtual reality are least explored.",https://ieeexplore.ieee.org/document/9579415/,IEEE Access,2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TII.2019.2962029,An Entropy-Based Approach to Real-Time Information Extraction for Industry 4.0,IEEE,Journals,"Industry 4.0 has drawn considerable attention from industry and academic research communities. The recent advances in Internet of Things (IoT), Big Data analytics, sensor technology, and artificial intelligence have led to the design and implementation of novel approaches to take full advantage of data-driven solutions applicable to Industry 4.0. With the availability of large datasets, it has become crucially important to identify the appropriate amount of relevant information, which would optimize the overall analysis of the corresponding systems. In this article, specific properties of dynamically evolving data systems are introduced and investigated, which provide framework to assess the appropriate amount of representative information.",https://ieeexplore.ieee.org/document/8941297/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCPMT.2020.3047089,Automatic Industry PCB Board DIP Process Defect Detection System Based on Deep Ensemble Self-Adaption Method,IEEE,Journals,"A deep ensemble convolutional neural network (CNN) model to inspect printed circuit board (PCB) board dual in-line package (DIP) soldering defects with Hybrid-YOLOv2 (YOLOv2 as a foreground detector and ResNet-101 as a classifier) and Faster RCNN with ResNet-101 and Feature Pyramid Network (FPN) (FRRF) achieved a detection rate of 97.45% and a false alarm rate (FAR) of 20%-30% in the previous study [34]. However, applying the method to other production lines, environmental variations, such as lighting, orientations of the sample feeds, and mechanical deviations, led to the degradation in detection performance. This article proposes an effective self-adaption method that collects “exception data” like the samples with which the Artificial Intelligent (AI) model made mistakes from the automated optical inspection inference edge to the training server, retraining with exceptions on the server and deploying back to the edge. The proposed defect detection system has been verified with real tests that achieved a detection rate of 99.99% with an FAR 20%-30% and less than 15 s of inspection time on a resolution $7296 \times 6000$ PCB image. The proposed system has proven capable of shortening inspection and repair time for online operators, where a 33% efficiency boost from the three production lines of the collaborated factory has been reported [6]. The contribution of the proposed retraining mechanism is threefold: 1) because the retraining process directly learns from the exceptions, the model can quickly adapt to the characteristic of each production line, leading to a fast and reliable mass deployment; 2) the proposed retraining mechanism is a necessary self-service for conventional users as it incrementally improves the detection performance without professional guidance or fine-tuning; and 3) the semiautomatic exception data collection method helps to reduce the time-consuming manual labeling during the retraining process.",https://ieeexplore.ieee.org/document/9306873/,"IEEE Transactions on Components, Packaging and Manufacturing Technology",Feb. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCAD.2021.3075422,Bridging the Pragmatic Gaps for Mixed-Criticality Systems in the Automotive Industry,IEEE,Journals,"An increasingly important trend in the design of safety-critical systems is the integration of components with different levels of criticality onto a common hardware platform. Mixed-criticality systems (MCSs) have been well researched in academia, but can be difficult to implement in industrial scenarios as the theoretical models underpinning the research do not sufficiently consider industrial safety practice and safety standards. In this article, we make the first attempt toward the implementation of the MCS theoretical model in industrial settings. To this end, we identify the pragmatic gaps between theory and practice, and then propose a generic industrial MCS architecture, termed P-MCS (Practical-MCS). P-MCS is built upon the conventional theoretical MCS model with additional considerations of industrial safety requirements: 1) runtime safety analysis, determining preserved applications in each system mode and 2) correct partitioning and isolation of different critical elements. We introduce three implementing methods for P-MCS. Corresponding to the new system architecture, we present a theoretical model and schedulability analysis (with consideration of shared resources) to ensure system predictability. Finally, we evaluate and demonstrate P-MCS in terms of system schedulability, overheads, throughput, and predictability, along with a real-world case study. As shown in the evaluation, the considerations of industrial requirements lead to extra overheads and performance reduction in P-MCS. Such weaknesses can be considerably mitigated by hardware assistance and acceleration.",https://ieeexplore.ieee.org/document/9415640/,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,April 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TII.2018.2807797,Deep Endoscope: Intelligent Duct Inspection for the Avionic Industry,IEEE,Journals,"We present the first autonomous endoscope for the visual inspection of very small ducts and cavities, up to a 6-mm diameter. The system has been designed, implemented, and tested in a challenging industrial scenario and in strict collaboration with an avionic industry partner. The inspected objects are metallic gearboxes eventually presenting different residuals (e.g., sand, machining swarfs, and metallic dust) inside the oil ducts. The automatic system is actuated by a robotic arm that moves the endoscope with a microcamera inside the gearbox duct, while a deep-learning-based spatio-temporal image analysis module detects, classifies, and localizes defects in real time. Feedback is given to the robotic arm in order to move or extract the endoscope given the detected anomalies. Evaluation provides a detection rate of nearly 98% given different tests with different types of residuals and duct structures.",https://ieeexplore.ieee.org/document/8295126/,IEEE Transactions on Industrial Informatics,April 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.2998723,"Digital Twin for the Oil and Gas Industry: Overview, Research Trends, Opportunities, and Challenges",IEEE,Journals,"With the emergence of industry 4.0, the oil and gas (O&G) industry is now considering a range of digital technologies to enhance productivity, efficiency, and safety of their operations while minimizing capital and operating costs, health and environment risks, and variability in the O&G project life cycles. The deployment of emerging technologies allows O&G companies to construct digital twins (DT) of their assets. Considering DT adoption, the O&G industry is still at an early stage with implementations limited to isolated and selective applications instead of industry-wide implementation, limiting the benefits from DT implementation. To gain the full potential of DT and related technological adoption, a comprehensive understanding of DT technology, the current status of O&G-related DT research activities, and the opportunities and challenges associated with the deployment of DT in the O&G industry are of paramount importance. In order to develop this understanding, this paper presents a literature review of DT within the context of the O&G industry. The paper follows a systematic approach to select articles for the literature review. First, a keywords-based publication search was performed on the scientific databases such as Elsevier, IEEE Xplore, OnePetro, Scopus, and Springer. The filtered articles were then analyzed using online text analytic software (Voyant Tools) followed by a manual review of the abstract, introduction and conclusion sections to select the most relevant articles for our study. These articles and the industrial publications cited by them were thoroughly reviewed to present a comprehensive overview of DT technology and to identify current research status, opportunities and challenges of DT deployment in the O&G industry. From this literature review, it was found that asset integrity monitoring, project planning, and life cycle management are the key application areas of digital twin in the O&G industry while cyber security, lack of standardization, and uncertainty in scope and focus are the key challenges of DT deployment in the O&G industry. When considering the geographical distribution for the DT related research in the O&G industry, the United States (US) is the leading country, followed by Norway, United Kingdom (UK), Canada, China, Italy, Netherland, Brazil, Germany, and Saudi Arabia. The overall publication rate was less than ten articles (approximately) per year until 2017, and a significant increase occurred in 2018 and 2019. The number of journal publications was noticeably lower than the number of conference publications, and the majority of the publications presented theoretical concepts rather than the industrial implementations. Both these observations suggest that the DT implementation in the O&G industry is still at an early stage.",https://ieeexplore.ieee.org/document/9104682/,IEEE Access,2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3103680,Human Centric Digital Transformation and Operator 4.0 for the Oil and Gas Industry,IEEE,Journals,"Working at an oil and gas facility, such as a drilling rig, production facility, processing facility, or storage facility, involves various challenges, including health and safety risks. It is possible to leverage emerging digital technologies such as smart sensors, wearable or mobile devices, big data analytics, cloud computing, extended reality technologies, robotic systems, and drones to mitigate the challenges faced by oil and gas workers. While these technologies are not new to the oil and gas industry, most of its existing digital transformation initiatives follow business or process-centric approaches, in which the critical driver of the technology adoption is the enhancement of production, efficiency, and revenue. As a result, they may not address the challenges faced by the workers. As oil and gas workers are among the essential assets in the oil and gas industry, it is vital to address the challenges faced by these workers. This paper proposes a human-centric digital transformational framework for the oil and gas industry to deploy existing digital technologies to enhance their workers' health, safety, and working conditions. The paper outlines the critical challenges faced by oilfield workers, introduces a system architecture to implements a human-centric digital transformation, discusses the opportunities of the proposed framework, and summarizes the key impediment for the proposed framework.",https://ieeexplore.ieee.org/document/9509417/,IEEE Access,2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JETCAS.2021.3097699,Machine-Learning-Based Microwave Sensing: A Case Study for the Food Industry,IEEE,Journals,"Despite the meticulous attention of food industries to prevent hazards in packaged goods, some contaminants may still elude the controls. Indeed, standard methods, like X-rays, metal detectors and near-infrared imaging, cannot detect low-density materials. Microwave sensing is an alternative method that, combined with machine learning classifiers, can tackle these deficiencies. In this paper we present a design methodology applied to a case study in the food sector. Specifically, we offer a complete flow from microwave dataset acquisition to deployment of the classifiers on real-time hardware and we show the effectiveness of this method in terms of detection accuracy. In the case study, we apply the machine-learning based microwave sensing approach to the case of food jars flowing at high speed on a conveyor belt. First, we collected a dataset from hazelnut-cocoa spread jars which were uncontaminated or contaminated with various intrusions, including low-density plastics. Then, we performed a design space exploration to choose the best MLPs as binary classifiers, which resulted to be exceptionally accurate. Finally, we selected the two most light-weight models for implementation on both an ARM-based CPU and an FPGA SoC, to cover a wide range of possible latency requirements, from loose to strict, to detect contaminants in real-time. The proposed design flow facilitates the design of the FPGA accelerator that might be required to meet the timing requirements by using a high-level approach, which might be suited for the microwave domain experts without specific digital hardware skills.",https://ieeexplore.ieee.org/document/9489295/,IEEE Journal on Emerging and Selected Topics in Circuits and Systems,Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/KAMW.2008.4810712,2008 IEEE International Symposium on Knowledge Acquisition and Modeling Workshop proceedings,IEEE,Conferences,"The following concepts are discussed: advanced knowledge modeling languages and tools; knowledge capture through machine learning and knowledge discovery in data bases; specific knowledge modeling issues for CBR systems, cooperative KBS, training applications; knowledge acquisition from texts and WWW; evaluation of methods, techniques and tools for knowledge acquisition; knowledge engineering and software engineering; uncertainty and vagueness aspects of knowledge modeling; knowledge acquisition applications such as library, industry, commerce, government, education and so on; knowledge acquisition in intelligent system; inclusive learning; formal and informal learning; HCI for educational systems; transforming learning through technologies; new generations of educational technologies; Web 2.0 and social computing for learning; educational technologies for new generations; real-time assessment of learning and performance; mobile computing for learning and instruction; personalized educational systems; interdisciplinary programs for educational technologists; CSCL technologies; content authoring technologies; e-pedagogy and instructional design; knowledge management technologies in education; organizational management of e-learning in universities; e- testing and new test theories; data mining, text mining, and web mining in education.",https://ieeexplore.ieee.org/document/4810712/,2008 IEEE International Symposium on Knowledge Acquisition and Modeling Workshop,21-22 Dec. 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITCA52113.2020.00084,5G Enabling Technologies in Rail,IEEE,Conferences,"Leveraging recent advances in IoT, blockchain, big data, artificial intelligence, and others, these state-of-art technologies still have difficulty in massive deployment and real fruition of working together in the industry. 5G brings new opportunities through enabling these technologies and thus leads to new developments. This paper introduces 5G and analyzes how 5G enable other technologies. Besides, it slices complicated railway scenarios into three aspects, and then discusses applications and innovations 5G and technologies can bring to rail.",https://ieeexplore.ieee.org/document/9422090/,2020 2nd International Conference on Information Technology and Computer Application (ITCA),18-20 Dec. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITAET47105.2019.9170214,A Basic Permanent Magnets Array Interaction Project for Teaching Artificial Intelligence as a Complementary Model,IEEE,Conferences,"There are new algorithms such as artificial intelligence (AI) methodologies that have achieved accurate representation of experimental systems. On the other hand, undergraduate freshmen students must understand AI methodologies since the industry has developed several products based on those and some academic problems also can be solved using AI. If those students do not learn how to model real systems using AI, they will be losing the opportunity of applying this powerful tool for solving several real problems in their professional life. Since the AI model can be a representation for forecasting the performance of the real model, this model can help the design process and provide information during its operation. This paper proposes an engineering project to teach artificial intelligence algorithms using real systems that are non-linear. Since permanent magnets are used in several applications, they can be attractive for modeling those when they are interacting between them; hence, this paper shows the interaction among them when they are deployed as an electrical power source. Moreover, this source could be classified as a renewable energy source. The basic generation of electrical energy is based on changing the magnetic field. Although the operation principle is basic, the electrical source has a non-liner description that is extremely complex so AI could be applied to create a model that represents those non-linear relationships in a precise manner. The main goal of this work is to describe an undergraduate project that can be used for teaching how to model a real system using AI algorithms. The main characteristics and properties of the permanent magnets are studied for the comprehension of how magnets can be implemented. It is also examined the viability for the construction of an electric motor using only permanent magnets, based on the analysis of different designs and materials and finally an AI model is created.",https://ieeexplore.ieee.org/document/9170214/,2019 International Conference on Innovative Trends and Advances in Engineering and Technology (ICITAET),27-28 Dec. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UIC-ATC.2017.8397649,A CNN based bagging learning approach to short-term load forecasting in smart grid,IEEE,Conferences,"Short-term load forecasting in smart grid is key to electricity dispatch scheduling, reliability analysis, and maintenance planning for the generators. In this paper, we present a convolutional neural networks (CNN) based bagging model for forecasting hourly loads. We employ CNN to train forecasting models on big load data sets. Then, we segment a real industry load data set into many subsets, fine-tune the forecasting models on these subsets to learn weak forecasting models, and assemble these weak forecasting models to conduct a bagging forecasting model, where the learning and assembling procedures are implemented on Spark. Specifically, all load samples in those data sets are reorganized as images with respect to similarities between relations of pixels in images and those of features in load samples. Experimental results indicate the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/8397649/,"2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",4-8 Aug. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GIOTS.2018.8534533,A Car as a Semantic Web Thing: Motivation and Demonstration,IEEE,Conferences,"Car signal data is usually hard to access, understand and integrate for non automotive domain experts. In this paper, we use semantic technologies for enriching signal data in the automotive industry and access it through Web of Things interactions. This combination allows the access and integration of car data from the web. We built VSSo, a Vehicle Signal ontology based on SOSA/SSN Observations and Actuations, and generated WoT Actions, Events and Properties, enriched with domain metadata. We mapped VSSo to a Web of Things ontology and we developed a Web of Things protocol binding with LwM2M, and made an implementation in a real car. This implementation resulted in a first working prototype, and a number of future improvements required in order to be compliant with automotive standards.",https://ieeexplore.ieee.org/document/8534533/,2018 Global Internet of Things Summit (GIoTS),4-7 June 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PHM-Paris.2019.00052,A Common Service Middleware for Intelligent Complex Software System,IEEE,Conferences,"With the rapid development of the Internet of Things (IoT) and artificial intelligence (AI) technology, various intelligent complex software systems (i-CSS) are increasingly popular, becoming one of the most important software system development paradigms. Its inherent growth construction and adaptive evolution properties pose new challenges to existing software design and development methods. Especially, how to achieve growth construction by quickly reusing existing excellent software resources, and how to establish data flow across system boundaries around the business flow to achieve adaptive evolution based on data intelligence. Facing the above challenges, this paper proposes novel data-oriented analysis and design method (DOAD), microservice and container-based mashup development method (SCMD). On this basis, the paper implements i-CSS common service middleware to support the above methods in engineering. In a real cloud-based PHM system and the other three industry projects, the proposed methods and middleware are used for application verification, the results show that they can greatly reduce the complexity of i-CSS design and development, reduce the ability threshold of the i-CSS development team, improve the development efficiency of the development team, reduce the team development workload by 31.5% on average, and help the i-CSS team effectively cope with the challenges of growth construction and adaptive evolution.",https://ieeexplore.ieee.org/document/8756426/,2019 Prognostics and System Health Management Conference (PHM-Paris),2-5 May 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/icp.2021.2184,A Comparison of Artificial Intelligence Time-Series Data Analysis Techniques for Load Demand Forecasting,IET,Conferences,"Distributed energy resources (DER) have seen substantial increases in deployment in recent years driven by, for example, government targets for reducing carbon emissions. However, the widespread deployment of DER also provides challenges in both real-time operational and long-term planning situations, some of which can be mitigated using time-series data collection and forecasting. For example, unmasking the effect of DER (particularly generation or storage) on underlying load demand data when considering the aggregate loading at a substation level. In this investigation, three artificial intelligence forecasting methods, AutoRegressive Integrated Moving Average (ARIMA), Holt-Winters Exponential Smoothing (HWES) and Box-Jenkins Adjusted ARIMA (BOXJ), are evaluated for their relative effectiveness in real-time and long-term situations using various datasets from UK electricity industry innovation projects. The results of this investigation suggest that HWES is more suited for operational use, providing a substantially less computationally intensive route for forecasting than the other methods, but simultaneously providing forecasts that perform with similar levels of accuracy as other methods in the short term. The results also suggest that BOXJ and ARIMA are substantially more accurate in long-term forecasts, BOXJ being the most accurate, but with run-times that makes these methods suitable for planning horizons but unsuitable for operational use.",https://ieeexplore.ieee.org/document/9692081/,CIRED 2021 - The 26th International Conference and Exhibition on Electricity Distribution,20-23 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RE48521.2020.00029,A Deep Context-wise Method for Coreference Detection in Natural Language Requirements,IEEE,Conferences,"Requirements are usually written by different stakeholders with diverse backgrounds and skills and evolve continuously. Therefore inconsistency caused by specialized jargons and different domains, is inevitable. In particular, entity coreference in Requirement Engineering (RE) is that different linguistic expressions refer to the same real-world entity. It leads to misconception about technical terminologies, and impacts the readability and understandability of requirements negatively. Manual detection entity coreference is labor-intensive and time-consuming. In this paper, we propose a DEEP context-wise semantic method named DeepCoref to entity COREFerence detection. It consists of one fine-tuning BERT model for context representation and a Word2Vec-based network for entity representation. We use a multi-layer perception in the end to fuse and make a trade-off between two representations for obtaining a better representation of entities. The input of the network is requirement contextual text and related entities, and the output is the predictive label to infer whether two entities are coreferent. The evaluation on industry data shows that our approach significantly outperforms three baselines with average precision and recall of 96.10% and 96.06% respectively. We also compare DeepCoref with three variants to demonstrate the performance enhancement from different components.",https://ieeexplore.ieee.org/document/9218208/,2020 IEEE 28th International Requirements Engineering Conference (RE),31 Aug.-4 Sept. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCNC49033.2022.9700522,A Deep Reinforcement Learning-based Resource Management Scheme for SDN-MEC-supported XR Applications,IEEE,Conferences,"The Multi-Access Edge Computing (MEC) paradigm provides a promising solution for efficient computing services at edge nodes, such as base stations (BS), access points (AP), etc. By offloading highly intensive computational tasks to MEC servers, critical benefits in terms of reducing energy consumption at mobile devices and lowering processing latency can be achieved to support high Quality of Service (QoS) to many applications. Among the services which would benefit from MEC deployments are eXtended Reality (XR) applications which are receiving increasing attention from both academia and industry. XR applications have high resource requirements, mostly in terms of network bandwidth, computation and storage. Often these resources are not available in classic network architectures and especially not when XR applications are run by mobile devices. This paper leverages the concepts of Software Defined Networking (SDN) and Network Function Virtualization (NFV) to propose an innovative resource management scheme considering heterogeneous QoS requirements at the MEC server level. The resource assignment is formulated by employing a Deep Reinforcement Learning (DRL) technique to support high quality of XR services. The simulation results show how our proposed solution outperforms other state-of-the-art resource management-based schemes.",https://ieeexplore.ieee.org/document/9700522/,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),8-11 Jan. 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData47090.2019.9005598,A Dynamic Neural Network Model for Click-Through Rate Prediction in Real-Time Bidding,IEEE,Conferences,"Real-time bidding (RTB) that features perimpression-level real-time ad auctions has become a popular practice in today's digital advertising industry. In RTB, click-through rate (CTR) prediction is a fundamental problem to ensure the success of an ad campaign and boost revenue. In this paper, we present a dynamic CTR prediction model designed for the Samsung demand-side platform (DSP). From our production data, we identify two key technical challenges that have not been fully addressed by the existing solutions: the dynamic nature of RTB and user information scarcity. To address both challenges, we develop a Dynamic Neural Network model. Our model effectively captures the dynamic evolutions of both users and ads and integrates auxiliary data sources (e.g., installed apps) to better model users' preferences. We put forward a novel interaction layer that fuses both explicit user responses (e.g., clicks on ads) and auxiliary data sources to generate consolidated user preference representations. We evaluate our model using a large amount of data collected from the Samsung advertising platform and compare our method against several state-of-the-art methods that are likely suitable for real-world deployment. The evaluation results demonstrate the effectiveness of our method and the potential for production. In addition, we discuss how to address a few practical engineering challenges caused by big data toward making our model in readiness for deployment.",https://ieeexplore.ieee.org/document/9005598/,2019 IEEE International Conference on Big Data (Big Data),9-12 Dec. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCBDA51879.2021.9442522,A FPGA Deployment System Based on Convolutional Neural Network for Rolling Bearing Diagnosis,IEEE,Conferences,"Real-time fault diagnosis of rolling bearing is a challenging issue for industry. Although artificial intelligence-based technologies could be well used for fault diagnosis of rolling bearing, the factories may not take into account the deployment of diagnosis algorithms. To tackle the issue, this work proposes a flexible deployment system of diagnosis algorithm for rolling bearing, where the required Convolutional Neural Network (CNN) model is deployed on the Field Programmable Gate Array (FPGA) to identify the working conditions of rolling bearing using vibration signals. The experimental results show that the deployed system performs accurately and efficiently on the test set, while a real-time prediction of FPGA could be guaranteed, indicating its potential as a powerful auxiliary system of rotating machinery.",https://ieeexplore.ieee.org/document/9442522/,2021 IEEE 6th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA),24-26 April 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRV.2015.42,A Hidden Markov Model for Vehicle Detection and Counting,IEEE,Conferences,"To reduce roadway congestion and improve traffic safety, accurate traffic metrics, such as number of vehicles travelling through lane-ways, are required. Unfortunately most existing infrastructure, such as loop-detectors and many video detectors, do not feasibly provide accurate vehicle counts. Consequently, a novel method is proposed which models vehicle motion using hidden Markov models (HMM). The proposed method represents a specified small region of the roadway as 'empty', 'vehicle entering', 'vehicle inside', and 'vehicle exiting', and then applies a modified Viterbi algorithm to the HMM sequential estimation framework to initialize and track vehicles. Vehicle observations are obtained using an Adaboost trained Haar-like feature detector. When tested on 88 hours of video, from three distinct locations, the proposed method proved to be robust to changes in lighting conditions, moving shadows, and camera motion, and consistently out-performed Multiple Target Tracking (MTT) and Virtual Detection Line(VDL) implementations. The median vehicle count error of the proposed method is lower than MTT and VDL by 28%, and 70% respectively. As future work, this algorithm will be implemented to provide the traffic industry with improved automated vehicle counting, with the intent to eventually provide real-time counts.",https://ieeexplore.ieee.org/document/7158929/,2015 12th Conference on Computer and Robot Vision,3-5 June 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SiPS47522.2019.9020540,A Hybrid GPU + FPGA System Design for Autonomous Driving Cars,IEEE,Conferences,"Autonomous driving cars need highly complex hardware and software systems, which require high performance computing platforms in order to enable a real time AI-based perception and decision making pipeline. The industry has been exploring various in-vehicle accelerators such as GPUs, ASICs and FPGAs. Yet the autonomous driving platform design is far from mature when taking into account of system reliability, redundancy and higher level of autonomy. In this work, we propose a hybrid computing system design, which integrates a GPU as the primary computing system and a FPGA as a secondary system. This hybrid system architecture has multiple advantages: 1) The FPGA can be constantly running as a complementary system with very short latency, helping to detect main system failure and anomalous behavior, contributing to system functionality verification and reliability. 2) If the primary system fails (mostly from sensor or interconnection error), the FPGA will quickly detect the failure and run a safe-mode task with a subset of sensors. 3) The FPGA can be used as an independent computing system to run extra algorithm components to improve the overall system autonomy. For example, FPGA can handle driver monitoring tasks while GPU focuses on driving functions. Together they can boost the driving function from L2 (constantly requires driver's attention) to L3 (allows driver to mind off for 10 seconds). This paper defines how such a system works, discusses various use cases and potential design challenges, and shares some initial results and insights about how to make such a system deliver the maximum value for autonomous driving.",https://ieeexplore.ieee.org/document/9020540/,2019 IEEE International Workshop on Signal Processing Systems (SiPS),20-23 Oct. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCC.and.EUC.2013.124,A Hypervisor for MIPS-Based Architecture Processors - A Case Study in Loongson Processors,IEEE,Conferences,"Loongson is a family of general purpose processors based on MIPS architecture designed and manufactured in Mainland China. With the maturity of Loongson CPUs, applications are widely available with the increasing development of software tools and hardware platforms by research teams in academia and industry. In recent years, products based on Loongson have been mainly used in education, personal computers and server systems. Meanwhile, it is not yet popularly used in industrial real-time control fields, so such products have large room and potential to further development and deployment. The M-Hyper visor discussed in this paper is a real-time hyper visor designed for MIPS architecture and implemented in Loongson2F processor. It is based on the management program of para-virtualization whilst multiple partitions are scheduled to execute according to their priorities. The design and implementation of M-Hyper visor is discussed, along with details as timer, interrupts, memory management, partition loading and scheduling, to enrich real-time virtualized applications for MIPS architecture. Evaluation results show the performance and viability of proposed design, being promising to new deployments.",https://ieeexplore.ieee.org/document/6832006/,2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing,13-15 Nov. 2013,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITOEC53115.2022.9734540,A Lightweight YOLOv5 Transmission Line Defect Detection Method Based on Coordinate Attention,IEEE,Conferences,"At present, in the power industry, there has always been a demand for intelligent computing and real-time feedback on the edge side using embedded devices. Due to the number of parameters, calculations, and memory usage of the deep learning model, its deployment on edge devices is severely affected. Based on this, this paper proposes a lightweight object detection network based on coordinate attention. The network is based on YOLOv5, decouples the large convolution kernels in the network in channel and space, reduces the parameters of the convolution kernel and the calculation amount of convolution operations, and realizes the lightweight processing of the network. In addition, a lightweight coordinate attention module is introduced into the network, and the model can obtain a larger area of information by embedding position information into the attention map without introducing large overheads, so that the model can increase a small amount of calculation while being significant improve the mAP of the model. The lightweight YOLOv5 model based on coordinate attention makes it possible to deploy on embedded devices with limited resources and achieve better detection results. Lightweight YOLOv5l, YOLOv5m, YOLOv5s, and YOLOv5n reduce FLOPs by about 60.94&#x0025;, 55.69&#x0025;, 46.25&#x0025;, and 46.51&#x0025;, respectively.",https://ieeexplore.ieee.org/document/9734540/,2022 IEEE 6th Information Technology and Mechatronics Engineering Conference (ITOEC),4-6 March 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2018.8592763,A Machine Learning Approach Applied to Energy Prediction in Job Shop Environments,IEEE,Conferences,"Energy efficiency has become a great challenge for manufacturing companies. Although it is possible to improve efficiency applying new and more efficient machines, decision makers tend to look for some less expensive alternatives. In this context, the adoption of more efficient strategies during the production planning can allow the reduction in energy consumption and associated emissions. Furthermore, the current reality of manufacturing companies, brought by Industry 4.0 concepts, requires more flexibility of production systems, thus, increasing complexity for machine rescheduling without compromising sustainable requirements. In this paper, we propose a method to predict total energy consumption in job shop systems applying machine learning techniques. Different schedules may result in different consumption rates. However, there is a nonlinear relationship between these targets. Therefore, an Artificial Neural Network (ANN) is applied for a quick estimation of total energy consumption. In order to validate the model, computational experiments, using digital manufacturing software tools, are performed on different job shop configurations to show the efficiency of the proposed model.",https://ieeexplore.ieee.org/document/8592763/,IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society,21-23 Oct. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSITSS54238.2021.9683768,A Machine Learning Approach for DDoS Prevention System in Cloud Computing Environment,IEEE,Conferences,"Cloud computing is a novel and rapidly expanding way for the IT industry to expand its business using resources available on a pay-as-you-go basis. What if the server goes down due to cyber-attacks? It’s a tremendous loss for the company because every minute counts, and if the data is lost or misused, the ramifications are a major issue for the cloud service provider. As a result, we offered a solution in this paper that, if implemented in the real world, will be able to mitigate Distributed Denial of Service (DDoS) attacks in cloud environments. According to the design, we have trained three machine learning classification models, including random forest, support vector machine, and logistic regression, on a real-world dataset called CICDDoS2019 from the Canadian Institute for Cyber Security, so that the proposed algorithm can choose the best classifier from the three to prevent an attack based on the traffic rate.",https://ieeexplore.ieee.org/document/9683768/,2021 IEEE International Conference on Computation System and Information Technology for Sustainable Solutions (CSITSS),16-18 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/DATE48585.2020.9116539,A Machine Learning Based Write Policy for SSD Cache in Cloud Block Storage,IEEE,Conferences,"Nowadays, SSD cache plays an important role in cloud storage systems. The associated write policy, which enforces an admission control policy regarding filling data into the cache, has a significant impact on the performance of the cache system and the amount of write traffic to SSD caches. Based on our analysis on a typical cloud block storage system, approximately 47.09% writes are write-only, i.e., writes to the blocks which have not been read during a certain time window. Naively writing the write-only data to the SSD cache unnecessarily introduces a large number of harmful writes to the SSD cache without any contribution to cache performance. On the other hand, it is a challenging task to identify and filter out those write-only data in a real-time manner, especially in a cloud environment running changing and diverse workloads.In this paper, to alleviate the above cache problem, we propose an ML-WP, Machine Learning Based Write Policy, which reduces write traffic to SSDs by avoiding writing write-only data. The main challenge in this approach is to identify write-only data in a real-time manner. To realize ML-WP and achieve accurate write-only data identification, we use machine learning methods to classify data into two groups (i.e., write-only and normal data). Based on this classification, the write-only data is directly written to backend storage without being cached. Experimental results show that, compared with the industry widely deployed write-back policy, ML-WP decreases write traffic to SSD cache by 41.52%, while improving the hit ratio by 2.61% and reducing the average read latency by 37.52%.",https://ieeexplore.ieee.org/document/9116539/,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",9-13 March 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2018.8351785,A Machine Learning-Based Strategy for Efficient Resource Management of Video Encoding on Heterogeneous MPSoCs,IEEE,Conferences,"The design of new streaming systems is becoming a major area of research to deploy services targeted in the Internet-of-Things (IoT) era. In this context, the new High Efficiency Video Coding (HEVC) standard provides high efficiency and scalability of quality at the cost of increased computational complexity for edge nodes, which is a new challenge for the design of IoT systems. The usage of hardware acceleration in conjunction with general-purpose cores in Multiprocessor Systems-on-Chip (MP-SoCs) is a promising solution to create heterogeneous computing systems to manage the complexity of real-time streaming for high-end IoT systems, achieving higher throughput and power efficiency when compared to conventional processors alone. Furthermore, Machine Learning (ML) provides a promising solution to efficiently use this next-generation of heterogeneous MPSoC designs that the EDA industry is developing by dynamically optimizing system performance under diverse requirements such as frame resolution, search area, operating frequency and stream allocation. In this work, we propose an ML-based approach for stream allocation and Dynamic Voltage and Frequency Scaling (DVFS) management on a heterogeneous MPSoC composed of ARM cores and FPGA fabric containing hardware accelerators for the motion estimation of HEVC encoding. Our experiments on a Zynq7000 SoC outline 20% higher throughput when compared to the state-of-the-art streaming systems for next-generation IoT devices.",https://ieeexplore.ieee.org/document/8351785/,2018 IEEE International Symposium on Circuits and Systems (ISCAS),27-30 May 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNS48642.2020.9162309,A Machine Learning-based Approach for Automated Vulnerability Remediation Analysis,IEEE,Conferences,"Security vulnerabilities in firmware/software pose an important threat ton power grid security, and thus electric utility companies should quickly decide how to remediate vulnerabilities after they are discovered. Making remediation decisions is a challenging task in the electric industry due to the many factors to consider, the balance to maintain between patching and service reliability, and the large amount of vulnerabilities to deal with. Unfortunately, remediation decisions are current manually made which take a long time. This increases security risks and incurs high cost of vulnerability management. In this paper, we propose a machine learning-based automation framework to automate remediation decision analysis for electric utilities. We apply it to an electric utility and conduct extensive experiments over two real operation datasets obtained from the utility. Results show the high effectiveness of the solution.",https://ieeexplore.ieee.org/document/9162309/,2020 IEEE Conference on Communications and Network Security (CNS),29 June-1 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCEAI52939.2021.00004,A Method for Designing and Analyzing Automotive Software Architecture: A Case Study for an Autonomous Electric Vehicle,IEEE,Conferences,"Software complexity is increased in automotive systems because many software functions are required for autonomous driving, electrified vehicles, and connected cars. In addition, autonomous driving requires centralized software that generally decreases evolvability with many connections. Thus, the automotive industry adopted the microservice architecture within the service-oriented architecture (SOA), which was already being used in distributed computing environments in the information and communication technology (ICT) industry. However, the software characteristics of an automotive system are different from those of an ICT system. Automotive software generally fulfills safety and real-time requirements that are not required in ICT software. Another challenge is integrating electric control units (ECUs) because software platforms supporting SOA require relatively high computational power and network bandwidth, which increases ECU cost. Thus, the deployment of software functions must be considered before integrating ECUs to find an optimal design solution for evolvability, dependability, real-time performance, cost, etc. However, many OEMs integrate ECUs based on deploying vehicular features without software architecture. It causes optimality problems during integrating ECUs. We propose component-based sensor-process-actuator architectural style for high-level architecture to handle quality attributes. Software architecture for an autonomous electrified vehicle will be presented with the proposed architectural style. The architecture is used to deploy software components and integrated ECUs with empirical quantitative analysis. Four design patterns for dependability with the architectural style will also be introduced.",https://ieeexplore.ieee.org/document/9544320/,2021 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI),27-29 Aug. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDUCON46332.2021.9454147,"A Mixed Reality Approach Enriching the Agricultural Engineering Education Paradigm, against the COVID-19 Constraints",IEEE,Conferences,"Since the very early beginning of the mankind history, any great difficulty, like wars or diseases, had to be a challenge for progress and innovation, otherwise the game was lost. In this regard, the recent COVID-19 pandemic provides to the learners’ and teachers’ community a great opportunity to better adapt and enrich their educational practices. Initially, aiming to assist students of agricultural engineering to demystify the innovative technologies of their scientific area, a remotely programmed and controlled robotic arm platform for fruit-picking purposes is deployed. This is just the excuse behind which a colorful bouquet of modern and software and hardware components are glued together to provide the potential for supporting a wide range of modern engineering applications. In an era that the speed of the technological achievements makes difficult to categorize their impact in industry, society or education, the proposed approach can be classified as containing mainly mixed reality, mobile, blended and project-based learning characteristics. A first set of results indicate that the discussed platform can greatly assist the students to tackle the lack of physical presence in the laboratory/classroom providing a quite interesting alternative to full in-vitro educational practices.",https://ieeexplore.ieee.org/document/9454147/,2021 IEEE Global Engineering Education Conference (EDUCON),21-23 April 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC48688.2020.0-202,A Modular Edge-/Cloud-Solution for Automated Error Detection of Industrial Hairpin Weldings using Convolutional Neural Networks,IEEE,Conferences,"The traction battery and the electric motor are the most important components of the electrified powertrain. To increase the energy efficiency of the electric motor, wound copper wires are being replaced by coated rectangular copper wires, so-called hairpins. Hence, to connect the hairpins conductively, they must be welded together. However, such new production processes are unknown compared with classic motor production. Therefore, this research aims to integrate Industry 4.0 techniques, such as cloud and edge computing, and advanced data analysis in the production process to better understand and optimize the manufacturing processes. Welding defects are classified with the help of a convolutional neural network (CNN) (predictive analysis) and, depending on the defect, a recommended course of action for reworking (prescriptive analysis) is given. However, the application of such complex algorithms as neural networks to large amounts of data requires huge computing resources. Therefore, a modular combination of an edge and cloud architecture is proposed in this paper. Furthermore, a pure cloud solution is compared with the edge solution.",https://ieeexplore.ieee.org/document/9202655/,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",13-17 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2008.4635950,A Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities - Part 1: Prototype Design and Development,IEEE,Conferences,"This three-part paper thoroughly addresses the design and development of multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work [1], [2], [3], defined its autonomy, communications, and artificial intelligence (AI) requirements [4], [5], and initiated the preliminary design of a simple system prototype [6], we are extending the build of a system prototype and simulate it in real-time to validate its logical behavior in normal and abnormal process situations and analyze its performance.",https://ieeexplore.ieee.org/document/4635950/,2008 IEEE International Symposium on Intelligent Control,3-5 Sept. 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2008.4635951,A Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities - Part 2: Prototype Design Verification,IEEE,Conferences,"This three-part paper thoroughly addresses the design and development of multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system defined its autonomy, communications, and artificial intelligence (AI) requirements, and initiated the preliminary design of a simple system prototype, we are extending the build of a system prototype and simulate it in real-time to validate its logical behavior in normal and abnormal process situations and analyze its performance. The second-part paper addresses the ICAM system prototype design verification and its logical behavior during sensor faults in the plant.",https://ieeexplore.ieee.org/document/4635951/,2008 IEEE International Symposium on Intelligent Control,3-5 Sept. 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2008.4635952,A Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities - Part 3: Performance Analysis and System Limitations,IEEE,Conferences,"This three-part paper thoroughly addresses the design and development of multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work (J.H. Taylor and A.F. Sayda, 2005), (A.F. Sayda and J.H. Taylor, 2006), defined its autonomy, communications, and artificial intelligence (AI) requirements and initiated the preliminary design of a simple system prototype (J.H. Taylor and A.F. Sayda, 2008), we are extending the build of a system prototype and simulate it in real-time to validate its logical behavior in normal and abnormal process situations and analyze its performance. The third-part paper addresses the ICAM system prototype validation in terms of system performance analysis and system behavior during unexpected situations.",https://ieeexplore.ieee.org/document/4635952/,2008 IEEE International Symposium on Intelligent Control,3-5 Sept. 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2019.8852094,A Music Recommendation System Based on logistic regression and eXtreme Gradient Boosting,IEEE,Conferences,"With the rapid growth of music industry data, it is difficult for people to find their favorite songs in the music library. Therefore, people urgently need an efficient music recommendation system to help them retrieve music. Traditional collaborative filtering algorithms are applied to the field of music recommendation. However, collaborative filtering does not handle data sparse problems very well when new items are introduced. To solve this problem, some people use the logistic regression method as a classifier to predict the user's music preferences to recommend songs. Logistic regression is a linear model that does not handle complex non-linear data features. In this paper, we propose a hybrid LX recommendation algorithm by integrating logistic regression and eXtreme Gradient Boosting(xgboost). A series of experiments are conducted on a real music dataset to evaluate the effectiveness of our proposed LX model. Our results show that the error and AUC of our LX model are better than other methods.",https://ieeexplore.ieee.org/document/8852094/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I-SMAC52330.2021.9640935,A Novel Approach for Detecting Traffic Signs using Deep Learning,IEEE,Conferences,"Nowadays, the demand for automatic driving assistance system is growing rapidly because it is reducing risk on drivers and helping to reduce the road accidents. In existing systems, many technological solutions are there but they are failing to produce promising accuracy. This paper has implemented a deep learning model for recognizing traffic signs that are present in the real world. The dataset that is utilized here is GTSRB which consists of 50,000 images of variable sizes. Due to modern technology and improvement in the automobile industry, numerous problems are encountering due to the huge number of vehicles. As a result, there is an increase in the number of accidents that happen due to the misreading of data by humans. To overcome the problem of misinterpretation, Traffic Sign Recognition is developed. Traffic Sign Recognition system capable of extracting traffic signs in real-time and can recognize the sign associated with the image. This model is being developed by using CNN. Our model producing 99.99% accuracy on training as well as validation data set. Traffic Sign Recognition is also a great contribution to the driver-less car technology that is being developed by Tesla. For a car to be driven without the help of a human, it should be able to detect traffic signs and act accordingly. The proposed model works effectively in different illuminating conditions and directions, where existing systems fail to produce promising results. This model helps to provide high accurate driver assisting system which can help to reduce accidents due traffic signal identification.",https://ieeexplore.ieee.org/document/9640935/,"2021 Fifth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)",11-13 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMCR47492.2019.8955729,A Novel Architecture for Condition Based Machinery Health Monitoring on Marine Vessels Using Deep Learning and Edge Computing,IEEE,Conferences,"Condition based machinery health monitoring on marine vessels involves collecting operational sensor data on the vessel using a robust data acquisition system and determining asset health using anomaly detection analytics. Automation and digitalization of marine vessels involve smart digital technologies such as the Internet of Things (IoT) to collect ships' health data and send it over to a central processing location where this data is analyzed. However, it is difficult to apply this to the shipping industry due to offshore data transmission bandwidth challenges. Deep Learning, a technology that can be used to conduct Machinery Health Monitoring (MHM) holds the key to solve the bandwidth problems. In this paper, we investigate the use of Convolutional Neural Networks (CNN) as a practical solution for deploying Smart Health Monitoring on Marine Vessels using the example of electric induction motors. We show a mechanism to develop data-driven deep learning model that can classify if the motor is in a healthy or faulty condition, and propose an architecture to deploy this model on the Marine vessel in real time on an edge computing hardware. While in operation, sensor data from the motor will be fed into the DL Model, and the resulting predictions will be presented in the Vessel Alarm Monitoring System.",https://ieeexplore.ieee.org/document/8955729/,2019 IEEE International Symposium on Measurement and Control in Robotics (ISMCR),19-21 Sept. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICESS.Symposia.2008.101,A Novel Embedded Maintenance Unit of Hydro Generator Excitation System,IEEE,Conferences,"Hydro generator excitation system is one of the most maintenance intensive subsystem in hydro power plants. A novel embedded maintenance unit of the excitation system is studied. A knowledge bank, which summarizes both the expert knowledge and the analytical knowledge on the failure modes and their manifestations as well as their effects, is established. An expert system is developed to optimize the control parameters, to detect and diagnose the equipment degradations or/and failures, and to give both control and maintenance remedy on base of the on-line state monitoring and performance evaluation of the regulating system. This expert system is integrated into the powerful microcomputer of the regulator and runs in parallel with the regular regulating tasks. A successful application in industry is presented.",https://ieeexplore.ieee.org/document/4627125/,2008 International Conference on Embedded Software and Systems Symposia,29-31 July 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC52423.2021.9658901,A Novel Fault Diagnosis Method Based on Ensemble Feature Selection in The Industrial IoT Scenario,IEEE,Conferences,"Fault diagnosis as a research hotspot in the field of prognostics and health management (PHM) has attracted the attention of academia and industry. Deep learning is widely used in academia due to its strong self-learning ability. However, as a ""black-box"" model, the features extracted by deep learning have poor interpretability which can’t be understood by IoT devices. In this paper, we propose a novel fault diagnosis method based on ensemble feature selection. We separately evaluate our method on the four-fault hydraulic components, which are derived from the real-world hydraulic time series data sets. The results show that compared with the deep learning method the proposed method can greatly reduce the complexity of the model without much reduction accuracy. Meanwhile, the balanced ensemble feature selection method we proposed is better than traditional ensemble methods such as union, intersection, and weighted linear aggregation. After testing, the method we proposed can be applied to the industrial IoT fault diagnosis.",https://ieeexplore.ieee.org/document/9658901/,"2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",17-20 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISDA.2008.252,A Novel Fitness Function in Genetic Algorithms to Optimize Neural Networks for Imbalanced Data Sets,IEEE,Conferences,"The imbalanced data sets are often encountered in business, industry and real life applications. In this paper, the novel fitness function in genetic algorithms to optimize neural networks is proposed for solving the classification problems in imbalanced data sets. Not only the parameters of neural networks but also the links-pruning between neurons are regarded as an optimization problem in this study. The fitness function consists of the mean square error, the classification error rate for each class, the distances between the examples and the boundary of classification. The artificial data set and the UCI data sets are used to verify the classifier we proposed. The experimental results showed that the classifier performs better than the conventional back-propagation neural network.",https://ieeexplore.ieee.org/document/4696407/,2008 Eighth International Conference on Intelligent Systems Design and Applications,26-28 Nov. 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITNEC52019.2021.9586993,A Novel Method for Color Forged Image Detection,IEEE,Conferences,"Digital images permeate almost all areas of our lives, mainly in news media, scientific discoveries, medical imaging, and judicial evidence. However, in recent years, due to the wide application of deep learning in image processing technology, these technologies or software have not only brought convenience to people, but also made it easier for people to forge or tamper with digital images without leaving any traces. The authenticity of digital images has been affected. A serious threat to modify and threaten. These forged or tampered images will bring serious threats to judicial justice, social stability, and the medical industry, and cause huge negative effects. Therefore, this article proposes an authenticity detection algorithm for generating color forged images based on deep learning. The corresponding color channel features of the real and forged image datasets are extracted and FIsher encoded, respectively, and the encoded color channel features are used to train the SVM model. Experiments prove that our proposed method achieves better results in detecting image color tampering.",https://ieeexplore.ieee.org/document/9586993/,"2021 IEEE 5th Information Technology,Networking,Electronic and Automation Control Conference (ITNEC)",15-17 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICESC51422.2021.9532599,A Novel System for Real Time Drowsiness Warning and Engine Ignition Authorization using Face Recognition,IEEE,Conferences,"With the advancement of technology, there is huge progress in automotive industry. This paper proposes two automotive solutions to enhance safety and security features in vehicles. The first proposed solution is Real Time Driver Drowsiness Warning System implemented on Raspberry Pi 4B platform, with Raspbian operating system. This system is developed by applying transfer learning on CNN based pretrained model MobileNetV2. OpenCV libraries are used to detect drowsiness. The second proposed solution is authorized engine ignition using face recognition. Face recognition is achieved by using Caffe based detector and FaceNet algorithm. The recognized face of the user will act as key to enable engine ignition. Thus, this system will be able to reduce private vehicle theft. Driver drowsiness warning system is trained using MRL Eye dataset and achieved 98.75% accuracy.",https://ieeexplore.ieee.org/document/9532599/,2021 Second International Conference on Electronics and Sustainable Communication Systems (ICESC),4-6 Aug. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIEM51511.2021.9445281,A Novel approach of GUI Mapping with image based widget detection and classification,IEEE,Conferences,"Software testing is vital for the intellectual benefits of software reliability and quality. At present, Graphical user interfaces are the most common and widely used interfaces in the software industry. Furthermore, GUI Testing is an important approach to ensure the quality of software. Automated software testing is a GUI front end applications similar to APP, and WEB, etc and is a vastly time and resource-consuming task. Therefore, this will become even more complex in rapidly updated GUI applications such as Ex: Patches/Version updates of a mobile App, or the product and offer updates in marketing websites like Flipkart, Amazon, etc., in which the GUI components are continuously updated infinitely. Developing a test case scenario whenever a new GUI component is updated will affect the productivity of the application. In our experiment, we found a better way of improving GUI testing by consequently detecting and classifying GUI widgets using machine learning techniques. Additionally, we also found that detecting and classifying GUI objects in screenshots and reports with a position of the widgets (x, y coordinates) and type of the widgets, matches with trained samples, URL links, and screen links. Hence, we in this paper will analyze and devise an efficient automated testing strategy for Web Applications. This is a unique way of web Graphical user interface testing with a computer vision. This paper will also present the parameters used for object detection, classification, and evaluation with image processing using machine learning algorithms with better accuracy.",https://ieeexplore.ieee.org/document/9445281/,2021 2nd International Conference on Intelligent Engineering and Management (ICIEM),28-30 April 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RE51729.2021.00036,A Pipeline for Automating Labeling to Prediction in Classification of NFRs,IEEE,Conferences,"Non-Functional Requirements (NFRs) focus on the operational constraints of the software system. Early detection of NFRs enables their incorporation into the architectural design at an initial stage, a practice obviously preferable to expensive refactoring at a later stage. Automated identification and classification of NFRs has therefore seen numerous efforts using rule-based, machine learning and deep learning-based approaches. One of the major challenges for such an automation is the manual effort that needs to be invested into labeling of training data. This is a concern for large software vendors who typically work on a variety of applications in diverse domains. We address this challenge by designing a pipeline that facilitates classification of NFRs using only a limited amount (~ 20% of an available new dataset) of labeled data for training. We (1) employed Snorkel to automatically label a dataset comprising NFRs from various Software Requirement Specification documents, (2) trained several classifiers using it, and (3) reused these pre-trained classifiers using a Transfer Learning approach to classify NFRs in industry-specific datasets. From among the various language model classifiers, the best results have been obtained for a BERT based classifier fine-tuned to learn the linguistic intricacies of three different domain-specific datasets from real-life projects.",https://ieeexplore.ieee.org/document/9604524/,2021 IEEE 29th International Requirements Engineering Conference (RE),20-24 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITC-Egypt52936.2021.9513888,A Proposed end to end Telemedicine System based on embedded system and mobile application using CMOS wearable sensors,IEEE,Conferences,"Internet of things (IoT) and Embedded systems have extensive applications in healthcare markets. Integration of IoT with healthcare started with wearable smartwatches monitoring some signals and storing this data in the cloud. With 4G/5G and WiFi 6 networks. Healthcare data can be analyzed with Artificial Intelligence providing new era Internet of Medical Things (IoMT) that encompass an array of internet-capable medical devices that are in constant communication with each other or with the cloud; Internet of Healthcare Things (IoHT) that is the digital transformation of the healthcare industry. This article presents an end-to-end architecture with realization of three modules for key IoT aspects for healthcare and telemedicine. Results from a real implementation of application Platform for Data Processing including patient and doctor data base-based web site, MySQL data base, Android based mobile App, and PHP webserver.",https://ieeexplore.ieee.org/document/9513888/,2021 International Telecommunications Conference (ITC-Egypt),13-15 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00071,A QoE and Visual Attention Evaluation on the Influence of Spatial Audio in 360 Videos,IEEE,Conferences,"Recently, there has been growing interest from academia and industry on the application of immersive technologies across a range of domains. Once such technology, 360° video, can be captured using an omnidirectional multi-camera arrangement. These 360° videos can then be rendered via Virtual Reality (VR) Head Mounted Displays (HMD). Viewers then have the freedom to look around the scene in any direction they wish. Whereas a body of work exists that focused on modeling visual attention (VA) in VR, little research has considered the impact of the audio modality on VA in VR. It is well accepted that audio has an important role in VR experiences. High quality spatial audio offers listeners the opportunity to experience sound in all directions. One such technique, Ambisonics or 3D audio, offers a complete 360° soundscape. This paper reports the results of an empirical study that looked at understanding how (if at all) spatial audio influences visual attention in 360° videos. It also assessed the impact of spatial audio on the user’s Quality of Experience (QoE) by capturing implicit, explicit, and objective metrics. The results suggest surprisingly similar explicit QoE ratings for both the spatial and non-spatial audio environments. The implicit metrics indicate that users integrated with the spatial environment more quickly than the non-spatial environment. Users who experienced the spatial audio environment had a higher maximum mean head pose pitch value and were found to be more focused towards the sound-emitting regions in the spatial audio environment experiences.",https://ieeexplore.ieee.org/document/9319084/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCI49370.2019.00084,A Real-Time Based Intelligent System for Predicting Equipment Status,IEEE,Conferences,"In manufacturing industry, significant productivity losses arise due to equipment failures. Therefore, it is an important task to prevent the equipment from failure by monitoring each machine's sensor data in advance. However, most of the current developed systems have been only focused on monitoring the sensor data and have a difficulty in applying advanced algorithms to the real-time stream data. To address issues, we implemented an intelligent system that employs real-time streaming engine loaded with the machine learning libraries for predictive maintenance analysis. By applying a deep-learning based model to the real-time streaming data, we can provide not only trends of raw sensor data but also give an indicator representing an equipment's status in real-time. We anticipate that our system contributes to recognize the equipment's status by monitoring the indicator for productivity improvement in manufacturing industry in real-time.",https://ieeexplore.ieee.org/document/9071016/,2019 International Conference on Computational Science and Computational Intelligence (CSCI),5-7 Dec. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData52589.2021.9671800,A Real-World Implementation of Unbiased Lift-based Bidding System,IEEE,Conferences,"In display ad auctions of Real-Time Bidding (RTB), a typical Demand-Side Platform (DSP) bids based on the predicted probability of click and conversion right after an ad impression. Recent studies find such a strategy is suboptimal and propose a better bidding strategy named lift-based bidding. Lift-based bidding simply bids the price according to the lift effect of the ad impression and achieves maximization of target metrics such as sales. Despite its superiority, lift-based bidding has not yet been widely accepted in the avertising industry. For one reason, lift-based bidding is less profitable f or DSP providers under the current billing rule. Second, the practical usefulness of lift-based bidding is not widely understood in the online advertising industry due to the lack of a comprehensive investigation of its impact.We here propose a practically-implementable lift-based bidding system that perfectly fits the current billing rules. We conduct extensive experiments using a real-world advertising campaign and examine the performance under various settings. We find that lift-based bidding, especially unbiased lift-based bidding is most profitable for both DSP providers and advertisers. Our ablation study highlights that lift-based bidding has a good property for currently dominant first price auctions. The results will motivate the online advertising industry to consider lift-based advertising.",https://ieeexplore.ieee.org/document/9671800/,2021 IEEE International Conference on Big Data (Big Data),15-18 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC.2019.10205,A Scalable Framework for Multilevel Streaming Data Analytics using Deep Learning,IEEE,Conferences,"The rapid growth of data in velocity, volume, value, variety, and veracity has enabled exciting new opportunities and presented big challenges for businesses of all types. Recently, there has been considerable interest in developing systems for processing continuous data streams with the increasing need for real-time analytics for decision support in the business, healthcare, manufacturing, and security. The analytics of streaming data usually relies on the output of offline analytics on static or archived data. However, businesses and organizations like our industry partner Gnowit, strive to provide their customers with real time market information and continuously look for a unified analytics framework that can integrate both streaming and offline analytics in a seamless fashion to extract knowledge from large volumes of hybrid streaming data. We present our study on designing a multilevel streaming text data analytics framework by comparing leading edge scalable open-source, distributed, and in-memory technologies. We demonstrate the functionality of the framework for a use case of multilevel text analytics using deep learning for language understanding and sentiment analysis including data indexing and query processing. Our framework combines Spark streaming for real time text processing, the Long Short Term Memory (LSTM) deep learning model for higher level sentiment analysis, and other tools for SQL-based analytical processing to provide a scalable solution for multilevel streaming text analytics.",https://ieeexplore.ieee.org/document/8754149/,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),15-19 Jul 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISIS.2012.78,A Semantic Model for Utility Driven Discovery of Cloud Resources,IEEE,Conferences,"Cloud computing has reached a high level of acceptance, both in academia and in industry. The maturity of the technology, along with the considerable business opportunity that has been promised, is the main responsible for its success. Nevertheless, today only few, very big players dominate the commercial panorama and take over market shares. From that position, they impose rigid pricing policies and quite inflexible negotiation schemes. When the ongoing cloud standardization process will complete, and thus full interoperability among clouds will be accomplished, new players will come into play. A real competition among cloud providers will then start based on key factors like the capability of providing flexible services tailored to specific, fine-grained customers' requirements. In this market scenario a mechanism must be devised to support the matchmaking between what providers offer and what customers demand. In this work we define a semantic model to help customers and providers to characterize their demands/offers, and propose the use of semantic tools to perform the matchmaking in such a way to maximize both the provider's and the customer's satisfaction.",https://ieeexplore.ieee.org/document/6245782/,"2012 Sixth International Conference on Complex, Intelligent, and Software Intensive Systems",4-6 July 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMARTCOMP52413.2021.00025,A Semi-Supervised Bayesian Anomaly Detection Technique for Diagnosing Faults in Industrial IoT Systems,IEEE,Conferences,"The Industry 4.0 paradigm has changed the way industrial systems with hundreds of sensor-actuator enabled devices, including industrial internet of things (IIoT), cooperate and communicate with the physical and human worlds. Given the intricacy, the diagnostics of such systems is extremely important. While anomaly detection is a valid approach to avoid unplanned maintenance or even complete breakdown, its effective realization in IIoT requires the design and implementation of frameworks for efficient monitoring, data collection, and analysis. Most of the existing anomaly detection techniques provide only a diagnosis of the fault without taking into account the uncertainty. Moreover, the lack of ground truth data (which is a typical problem in the industrial context), make their implementation even more challenging. This paper proposes an anomaly detection technique built on top of an industrial framework for the data collection and monitoring. Specifically, we address the lack of labeled data by designing a semi-supervised anomaly detection algorithm that exploits Bayesian Gaussian Mixtures to assess the working condition of the plant while measuring the uncertainty during the diagnosis process and we implement the proposed framework on a real-life IIoT testbed, namely a scale replica assembly plant. Experimental results demonstrate that our anomaly detection algorithm is able to detect the plant working conditions with 99.8% of accuracy, and the semi-supervised approach performs better than a supervised one.",https://ieeexplore.ieee.org/document/9556269/,2021 IEEE International Conference on Smart Computing (SMARTCOMP),23-27 Aug. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DAS.2012.10,A Signature Verification Framework for Digital Pen Applications,IEEE,Conferences,"In this paper we present a framework for real-time online signature verification scenarios. The proposed framework is based on state-of-the-art feature extraction and Gaussian Mixture Model (GMM) classification. While our signature verification library is generally applicable to any input device using digital pens, we have implemented verification scenarios using the Anoto digital pen. As such our automated signature verification framework becomes an interesting commodity for industry, because the Anoto SDK is easy to apply and the GMM-based classification can be seamlessly integrated. The novelty of this work is the application of our framework that takes real-time online signature verification to every scenario where digital pens may potentially be used. In this paper we describe several scenarios where our framework has been applied, including signatures in financial contracts or ordering processes. We also propose a general approach to integrate the GMM-descriptions into electronic ID-cards in order to also store behavioral biometrics on these cards. In experiments we have measured the performance of the signature verification system when skilled forgeries were present. The interest shown by our partner financial institutions and the results of our initial evaluations indicate that our signature verification framework suits exactly the demands of our clients.",https://ieeexplore.ieee.org/document/6195406/,2012 10th IAPR International Workshop on Document Analysis Systems,27-29 March 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PCIC31437.2018.9080444,"A Smart Condition Monitoring System for HV Networks with Artificial Intelligence, Augmented Reality and Virtual Reality: Copyright Material IEEE, Paper No. PCIC-2018-37",IEEE,Conferences,"The authors present a conceptual design for a SMART asset monitoring solution for high voltage (HV) networks in the petrochemical industry. The paper discusses the potential for incorporating artificial intelligence (AI), augmented reality (AR) and virtual reality (VR) into an application of the Industrial Internet of Things (IIoT) for condition monitoring. The paper is a continuation of the work presented by the authors at the IEEE-PCIC 2017 conference in Calgary. The proposed asset management system analyses condition monitoring (CM) data and assesses the risk of failure data across complete HV networks. Knowledge of deteriorating asset condition provides the operator with an advanced, early warning of incipient mechanical and electrical faults. With knowledge of the severity and source of such faults, pinpointed preventative maintenance interventions can then made during planned maintenance outages. The complete HV network asset monitoring solution described includes permanent sensors and monitoring nodes deployed at strategic locations across the network. Processed data is passed via a local area network to local servers and then via secure data cloud transmission to a centralized monitoring server located at the CM headquarters. This central server operates a CM database that logs, displays, benchmarks and trends the condition data with comparison to a statistically-significant database of measurements. It is proposed in the IIoT solution proposed that this database will be downloadable to a smartphone/tablet for use by the field engineer. The monitoring technology will likely also incorporate a number of AI machine learning software modules for the de-noising of raw signals and the diagnosis of different types of defects within different types of HV plant items. The proposed SMART CM system includes an advanced graphical user interface (GUI) for viewing HV asset CM data along with operational and maintenance (O&M) data. The GUI will also be able to display both condition criticality and operational criticality (on a color-coded range of 0-100%) for individual HV plant items on a digitized mimic of the HV network's single-line diagram (SLD). This could also be combined with geometric positioning data of assets across the facility (including HV cable routes and lengths) to provide a fully digitized SMART network diagram for use in the IIoT asset management solution. Asset management data, combined with the application of the developing techniques of AI, AR and VR, will greatly help the user to visualize the plant items in 3-D, their position within the network, their condition and operational criticality along with all related asset management information together on one dashboard screen, downloaded onto smartphone/tablet. The paper concludes with a case study showing the development of a specification for a SMART IIoT asset condition monitoring solution suitable for a large petrochemical refining facility.",https://ieeexplore.ieee.org/document/9080444/,2018 IEEE Petroleum and Chemical Industry Technical Conference (PCIC),24-26 Sept. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/CCC50068.2020.9188783,A Smartphone-Based Networked Control Platform: Design and Implementation,IEEE,Conferences,"The rapid development of embedded systems and IT makes electrical devices functional and tiny. This progress is especially reflected in the smartphone industry. In this paper, a novel applicable wireless control platform based on smartphones is proposed. The system is of high value due to the advantages of smartphones such as mobility, resourcefulness, tiny and etc. Mechanisms are designed in this paper to guarantee the real-timeness of the controller. To ease the controller implementation, code auto-generation technique is integrated to the platform, which is able to convert Simulink block diagrams to executable files for Android smartphones. In addition, protocols are designed to transfer data to remote workstations such that the control system can be supervised and monitored on-line. At last, the control platform is tested on a spacecraft simulator. The experimental results validate the effectiveness and reliability of the designed platform.",https://ieeexplore.ieee.org/document/9188783/,2020 39th Chinese Control Conference (CCC),27-29 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IOLTS52814.2021.9486704,A Suitability Analysis of Software Based Testing Strategies for the On-line Testing of Artificial Neural Networks Applications in Embedded Devices,IEEE,Conferences,"Electronic devices based on artificial intelligence solutions are pervading our everyday life. Nowadays, human decision processes are supported by real-time data gathered from intelligent systems. Artificial Neural Networks (ANNs) are one of the most used deep learning predictive models due to their outstanding computational capabilities. However, assessing their reliability is still an open issue faced by both the academic and industrial worlds, especially when ANNs are deployed on safety-critical systems, such as self-driving cars in the automotive world. In these systems, a strategy for identifying hardware faults is required by industry standards (e.g., ISO26262 for automotive, and DO254 for avionics). Among the existing in-field test strategies, the periodic scheduling of on-line Software Test Library (STL) is a wide strategy adopted; STL allows to reach an acceptable fault coverage without the need for additional hardware. However, when dealing with ANN-based applications, the execution of on-line tests interleaving the ANN inferences may jeopardise the strive for performance maximization. The paper presents a comprehensive analysis of six possible scenarios concerning the execution of on-line self-test programs in embedded devices running ANN-based applications. In the proposed scenarios, the impact of the STL execution on the ANN performance is analyzed; in particular, the execution times of an inference and the Fault Detection Time (FDT) of the STL are discussed and compared. Experimental analyses are provided by relying on: an open-source RISC-V platform running two different convolutional neural networks; a STL for RISC-V cores with a maximum achievable fault coverage of 90%.",https://ieeexplore.ieee.org/document/9486704/,2021 IEEE 27th International Symposium on On-Line Testing and Robust System Design (IOLTS),28-30 June 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSE46696.2019.8984442,A Systems Approach to Validating and Analyzing Improvements to Real-Time Image Classification Utilizing Machine Learning,IEEE,Conferences,"This paper delves into the generation and use of image classification models in a real-time environment utilizing machine learning. The ImageAI framework is used to generate a list of models from a set of training images and also for classifying new images using the generated models. Through this paper, previous research projects and industry programs are analyzed for design and operation. The basic implementation results in models that classify new images correctly the majority of the time with a high level of confidence. However, almost a quarter of the time the models classify images incorrectly. This paper attempts to improve the classification accuracy and improve the operational efficiency of the overall system as well.",https://ieeexplore.ieee.org/document/8984442/,2019 International Symposium on Systems Engineering (ISSE),1-3 Oct. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAAI.2013.73,A Tourist Navigation System in Which a Historical Character Guides to Related Spots by Hide-and-Seek,IEEE,Conferences,"Tourism is an important industry, and various initiatives for the discovery of sightseeing spots are being implemented. Most current sightseeing navigation systems, which are very efficient at enabling immediate acquisition of desired information, lack a sense of fun. Moreover, although some researches have been conducted on the creation of new encounters and discoveries in tourist areas, there is a problem in that the high degree of user freedom does not lead users to spots which tourist area authorities hope to promote. Thus, in this study we propose a system that stimulates rediscovery of sightseeing spots through hide-and-seek with CG characters using augmented reality technology. We intend to verify the anticipated effects of the proposed system in upcoming evaluation experiments.",https://ieeexplore.ieee.org/document/6783892/,2013 Conference on Technologies and Applications of Artificial Intelligence,6-8 Dec. 2013,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIEVicIVPR52578.2021.9564229,A Vision-Based Lane Detection Approach for Autonomous Vehicles Using a Convolutional Neural Network Architecture,IEEE,Conferences,"Autonomous vehicles no longer belong to the realm of science fiction. They have become a prominent area of research in the last two decades because of the integration of Artificial Intelligence in the automobile industry. Apart from the development of various complex learning algorithms, the advancement of cameras, sensors, and geolocation technology as well as the escalation in the capacity of machines have played a crucial role in bringing this technology into reality. We have had significant breakthroughs in the development of autonomous cars within the last ten years. However, despite the success of multiple prototypes in navigating within the borders of a delimited area, researchers are yet to overcome several drawbacks before embodying them in the transport system; and one of those hurdles lies in the lane detection system of the cars. Therefore, in this article, we present an intelligent lane detection algorithm incorporating fully-connected Neural Networks with a secondary layer protection scheme to detect the borders of a lane. We achieved over 98% classification accuracy using the proposed lane detection model. We also implemented the model in a small prototype to take a look at its performance. Experimental results infer that the algorithm is capable of lane detection and ready for practical use.",https://ieeexplore.ieee.org/document/9564229/,"2021 Joint 10th International Conference on Informatics, Electronics & Vision (ICIEV) and 2021 5th International Conference on Imaging, Vision & Pattern Recognition (icIVPR)",16-20 Aug. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDOC49727.2020.00017,A Zone Reference Model for Enterprise-Grade Data Lake Management,IEEE,Conferences,"Data lakes are on the rise as data platforms for any kind of analytics, from data exploration to machine learning. They achieve the required flexibility by storing heterogeneous data in their raw format, and by avoiding the need for pre-defined use cases. However, storing only raw data is inefficient, as for many applications, the same data processing has to be applied repeatedly. To foster the reuse of processing steps, literature proposes to store data in different degrees of processing in addition to their raw format. To this end, data lakes are typically structured in zones. There exists various zone models, but they are varied, vague, and no assessments are given. It is unclear which of these zone models is applicable in a practical data lake implementation in enterprises. In this work, we assess existing zone models using requirements derived from multiple representative data analytics use cases of a real-world industry case. We identify the shortcomings of existing work and develop a zone reference model for enterprise-grade data lake management in a detailed manner. We assess the reference model's applicability through a prototypical implementation for a real-world enterprise data lake use case. This assessment shows that the zone reference model meets the requirements relevant in practice and is ready for industry use.",https://ieeexplore.ieee.org/document/9233155/,2020 IEEE 24th International Enterprise Distributed Object Computing Conference (EDOC),5-8 Oct. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI.2017.8280935,A benchmark environment motivated by industrial control problems,IEEE,Conferences,"In the research area of reinforcement learning (RL), frequently novel and promising methods are developed and introduced to the RL community. However, although many researchers are keen to apply their methods on real-world problems, implementing such methods in real industry environments often is a frustrating and tedious process. Generally, academic research groups have only limited access to real industrial data and applications. For this reason, new methods are usually developed, evaluated and compared by using artificial software benchmarks. On one hand, these benchmarks are designed to provide interpretable RL training scenarios and detailed insight into the learning process of the method on hand. On the other hand, they usually do not share much similarity with industrial real-world applications. For this reason we used our industry experience to design a benchmark which bridges the gap between freely available, documented, and motivated artificial benchmarks and properties of real industrial problems. The resulting industrial benchmark (IB) has been made publicly available to the RL community by publishing its Java and Python code, including an OpenAI Gym wrapper, on Github. In this paper we motivate and describe in detail the IB's dynamics and identify prototypic experimental settings that capture common situations in real-world industry control problems.",https://ieeexplore.ieee.org/document/8280935/,2017 IEEE Symposium Series on Computational Intelligence (SSCI),27 Nov.-1 Dec. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTEST.2018.8397165,A classifier-based test oracle for embedded software,IEEE,Conferences,"Despite great advances in different software testing areas, one important challenge, achieving an automated test oracle, has been overlooked by academia and industry. Among various approaches for constructing a test oracle, machine learning techniques have been successful in recent years. However, there are some situations in which the existing machine learning based oracles have deficiencies. These situations include testing of applications with low observability, such as embedded software and multimedia software programs. There are also cases in testing embedded software in which explicit historical data in form of input-output relationships is not available, and situations in which the comparison between expected results and actual outputs is impossible or hard. Addressing these deficiencies, this paper proposes a new black box solution to construct automated oracles which can be applied to embedded software and other programs with low observability. To achieve this, we have employed an Artificial Neural Network (ANN) algorithm to build a model which merely requires program's input values as well as corresponding pass/fail outcome, as the training set. We have conducted extensive experiments on several benchmarks. The results manifest the applicability of the proposed approach to software systems with low observability as well as its higher accuracy in comparison to a well-known machine learning based method.",https://ieeexplore.ieee.org/document/8397165/,2018 Real-Time and Embedded Systems and Technologies (RTEST),9-10 May 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2012.6301363,A device configuration management tool for context-aware system,IEEE,Conferences,"Automation industry is moving towards more complex systems which are posing new challenges for operation from both machine and human perspectives. A group of challenges is related to management of the overwhelming information flow and to usability of the systems, and context-aware solutions have been recently introduced to the automation field in order to cope with challenges of this kind. The context awareness is seen as a solution which would allow to both the technological system and the human operator to infer the optimal decisions and to behave in the most effective way. In order to reach this capability, the external physical world and the system must be described in a way both interprétable for humans and machines. Ambition of this paper is to contribute to the context-aware (re)configuration of the system with a tool, which is designed to improve the efficiency of the configuration phase of a context-aware system. The tool provides a solution to configure and model the field devices of a system via automatically generated ontologies. This research is a part of a device management framework for a building automation use case, which is targeting to support controlling decisions of dwellers, technical support and social services.",https://ieeexplore.ieee.org/document/6301363/,IEEE 10th International Conference on Industrial Informatics,25-27 July 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICE/ITMC49519.2020.9198625,A digital twin model of a pasteurization system for food beverages: tools and architecture,IEEE,Conferences,"Many enabling technologies of Industry 4.0 (Internet of Things “IoT”, Cloud systems, Big Data Analytics) contribute to the creation of what is the Digital Twin or virtual twin of a physical process, that is a mathematical model capable of describing the process, product or service in a precise way in order to carry out analyses and apply strategies. Digital Twin models integrate artificial intelligence, machine learning and analytics software with the data collected from the production plants to create digital simulation models that update when the parameters of the production processes or the working conditions change. This is a self-learning mechanism, which makes use of data collected from various sources (sensors that transmit operating conditions; experts, such as engineers with deep knowledge of the industrial domain; other similar machines or fleets of similar machines) and integrates also historical data relating to the past use of the machine. Starting from the virtual twin vision, simulation plays a key role within the Industry 4.0 transformation. Creating a virtual prototype has become necessary and strategic to raise the safety levels of the operators engaged in the maintenance phases, but above all the integration of the digital model with the IoT has become particularly effective, as the advent of software platforms offers the possibility of integrating real-time data with all the digital information that a company owns on a given process, ensuring the realization of the Digital Twin. In this context, this work aims at developing optimized solutions for application in a beverage pasteurization system using the Digital Twin approach, capable of creating a virtual modelling of the process and preventing high-risk events for operators.",https://ieeexplore.ieee.org/document/9198625/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSCS.2013.6651227,A face recognition system based on a Kinect sensor and Windows Azure cloud technology,IEEE,Conferences,"The aim of this paper is to build a system for human detection based on facial recognition. The state-of-the-art face recognition algorithms obtain high recognition rates base on demanding costs - computational, energy and memory. The use of these classical algorithms on an embedded system cannot achieve such performances due to the existing constrains: computational power and memory. Our objective is to develop a cheap, real time embedded system able to recognize faces without any compromise on system's accuracy. The system is designed for automotive industry, smart house application and security systems. To achieve superior performance (higher recognition rates) in real time, an optimum combination of new technologies was used for detection and classification of faces. The face detection system uses skeletal-tracking feature of Microsoft Kinect sensor. The face recognition, more precisely - the training of neural network, the most computing-intensive part of the software, is achieved based on the Windows Azures cloud technology.",https://ieeexplore.ieee.org/document/6651227/,"International Symposium on Signals, Circuits and Systems ISSCS2013",11-12 July 2013,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1998.687153,A hybrid structure for adaptive fixed weight recurrent networks,IEEE,Conferences,"Due to the evolution of the underlying physical process, a correct model can transform into an erroneous one. We therefore propose a method which overcomes this problem by adapting the network along the way. Our method (clustered error injection) is based (a) on the ability of real-time recurrent learning networks to form clustered network structures and (b) on the error injection principle. The actual model error is fed back into the network as an input. This improves the model performance by adapting it to a changing environment. This technique is tested on two examples, a mathematical modelling problem and a real-life problem from the chemical process industry.",https://ieeexplore.ieee.org/document/687153/,1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98CH36227),4-9 May 1998,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSST.1991.138548,A neural network based histogramic procedure for fast image segmentation,IEEE,Conferences,"The determination of the dimension of a lumber board, the location and extent of surface defects on it, are essential in the construction of a visual inspection station for the lumber industry. The paper presents a neural network based histogramic procedure that performs on the image of a board and can be used to determine the board dimension, the location and extent of surface defects on it, in near real time. The method is based on segmentation of the image based on multiple threshold information derived from a multi-layered neural network. Such a scheme can be applied in general to image analysis and the implementation shows fast processing requiring very little control over the environment. The construction of the network and its training are also discussed.<>",https://ieeexplore.ieee.org/document/138548/,[1991 Proceedings] The Twenty-Third Southeastern Symposium on System Theory,10-12 March 1991,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IAS.2005.1518384,A neural network based optimal wide area control scheme for a power system,IEEE,Conferences,"With deregulation of the power industry, many tie lines between control areas are driven to operate near their maximum capacity, especially those serving heavy load centers. Wide area control systems (WACSs) using wide-area or global signals can provide remote auxiliary control signals to local controllers such as automatic voltage regulators, power system stabilizers, etc to damp out inter-area oscillations. This paper presents the design and the DSP implementation of a nonlinear optimal wide area controller based on adaptive critic designs and neural networks for a power system on the real-time digital simulator (RTDS/spl reg/). The performance of the WACS as a power system stability agent is studied using the Kundur's two area power system example. The WACS provides better damping of power system oscillations under small and large disturbances even with the inclusion of local power system stabilizers.",https://ieeexplore.ieee.org/document/1518384/,"Fourtieth IAS Annual Meeting. Conference Record of the 2005 Industry Applications Conference, 2005.",2-6 Oct. 2005,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WINCOM50532.2020.9272477,A new middleware for managing heterogeneous robot in ubiquitous environments,IEEE,Conferences,"Heterogeneity is one of the main issues for the deployment of the Industry 4.0. This is due to the diversity in the available robots and the IIoT devices. These equipments use different programming languages and communication protocols. To make the integration of such equipments easy, we propose TalkRoBots, a middleware that allows heterogeneous robots and IIoT devices to communicate together and exchange data in a transparent way. The middleware was experimented in a real scenario with different robots that demonstrate its efficiency.",https://ieeexplore.ieee.org/document/9272477/,2020 8th International Conference on Wireless Networks and Mobile Communications (WINCOM),27-29 Oct. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITIIT49094.2020.9071531,A novel approach to detect IoT malware by system calls using Deep learning techniques,IEEE,Conferences,"Recently, IoT devices or smart objects widely utilized in all kinds of fields such as medical, defense, automobile industry etc. Due to its intelligence and popularity, attacker seeks its help to launch malicious attack in high speed at low cost. In this regard, Researchers have turned their interest towards improving the security for the Internet of things devices. In this model, the malware were detected based on their behavior in terms of system calls sequence arise during its execution. The system calls of IoT malware are gathered using Strace tool in Ubuntu. The generated malicious system calls are preprocessed by n-gram techniques to retrieve required features. The extracted system calls were classified into two class i.e normal and malicious sequence using Recurrent neural network(RNN). The efficiency of this deep learning is tested using various performance metrics. The real time IoT malware samples were collected from IOTPOT honeypot which emulates different CPU architecture of IoT devices.",https://ieeexplore.ieee.org/document/9071531/,2020 International Conference on Innovative Trends in Information Technology (ICITIIT),13-14 Feb. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2004.1308024,A real-time monitoring and diagnosis system for manufacturing automation,IEEE,Conferences,"Condition monitoring and fault diagnosis in modern engineering practices is of great practical significance for improving the quality and productivity, preventing the machinery from damages. In general, this practice consists of two parts: extracting appropriate features from sensor signals and recognizing possible faulty patterns from the features. In order to cope with the complex manufacturing operations and develop a feasible system for real-time application, we proposed three approaches. By defining the marginal energy, a new feature representation emerged, while by real-time learning algorithms with support vector techniques and hidden Markov model representations, a modular software architecture and a new similarity measure were developed for comparison, monitoring, and diagnosis. A novel intelligent computer-based system has been developed and evaluated in over 30 factories and numerous metal stamping processes as an example of manufacturing operations. The real-time operation of this system demonstrated that the proposed system is able to detect abnormal conditions efficiently and effectively resulting in a low-cost, effective approach to real-time monitoring in manufacturing. The related technologies have been transferred to industry, presenting a tremendous impact in current automation practice in Asia and the world.",https://ieeexplore.ieee.org/document/1308024/,"IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004",26 April-1 May 2004,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MFI-2003.2003.1232590,A robust real time position and force (hybrid) control of a robot manipulator in presence of uncertainties,IEEE,Conferences,"We examine the living intelligent biological systems and model the computational system components. We consider the situation of a kind of ""blind-tracking"" with constant force/torque by a human hand. The problem involves hand kinematics, hand motor control, and an adaptive judgment method from the position and force/torque reflection of the uncertain hyper plane. In this study, these control levels were designed using neural networks and fuzzy logic technologies. The control levels are coordinated amongst themselves forming the distributed artificial intelligent (DAI) system. The conclusive characteristic of the proposed controller was a one-step-ahead feedback control. This DAI-based control systems was implemented in the RX-90 industrial robot. Certainly these types of control system will help an industry to be autonomous and increase the productivity as well.",https://ieeexplore.ieee.org/document/1232590/,"Proceedings of IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, MFI2003.",1-1 Aug. 2003,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICISC.2018.8398982,A rule-based classification of short message service type,IEEE,Conferences,"Short message service (SMS) is one of the most popular means of communication due to its type of usages like one-time passwords(OTPs), banking transaction alerts, and other promotional messages. SMS is the most time-sensitive channel of communication that demands service providers to send any information globally without any delay in respective time zone. For successful implementation of this service, the Telecom Regulatory of India (TRAI) has recommended certain guidelines that must be followed for the delivery of short messages. The prevalent practice in the industry is to store rules or raw string in a database and to match with incoming SMSs in synchronous mode. However, there are various shortcomings in this traditional approach that have been also discussed in this paper. Further, to address these issues, this research work proposes a novel approach for automated classification of SMSs in real time by the SMS service using a rule-based system of database template matching. Further to validate the proposed approach, SMS database of Netcore Solutions Pvt Limited, India, has been used for training the proposed algorithm. Proposed rule-based classifier keeps learning and updating the training dataset as more SMSs are accumulated and processed through the server. Other advantages, in terms of performance metrics, of the classification using the proposed rule-based database template matching over the traditional database matching approach have been reported in this work as evaluation measures. Reported rule-based SMS classification algorithm shows highest average classification accuracy of 100% which is better as compared to the similar research works already available in the literature.",https://ieeexplore.ieee.org/document/8398982/,2018 2nd International Conference on Inventive Systems and Control (ICISC),19-20 Jan. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNSC.2014.6906645,A survey of water distribution system and new approach to intelligent water distribution system,IEEE,Conferences,"This paper reviews the challenges in water distribution systems (WDS), development and deployment of intelligent systems to manage WDS efficiently. Recognizing that intelligent systems has the potential to revolutionize management of precious natural resource by utilities, this paper provides a summary of the research work and practices for researchers and industry practitioners to ensure that the technology cultivates sustainable drinking water management. At present a lot technological developments ensures reduction of labor cost for various tasks in WDS. The technical sophistications of intelligent systems to ensure optimal use of precious resources have increased noticeably in recent decades. This paper addresses all concerned issues with WDS such as real time data collection, forecast of future water consumption, labor cost, recovery cost of water treatment and distribution, power supply requirement, operational time, water leakage, remote capturing of meter reading etc. We proposed a new intelligent water distribution system, which overcomes most of the problems in WDS.",https://ieeexplore.ieee.org/document/6906645/,2014 First International Conference on Networks & Soft Computing (ICNSC2014),19-20 Aug. 2014,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2008.4618263,A system scheme of 3D object reconstruction from single 2D graphics based on neural networks,IEEE,Conferences,"3D reconstruction has turned out to be a significant need in computer aided design (CAD) technology. With the rapid development of virtual reality and industry design, more and more 3D models are required in practical needs. So an artificial neural network based system scheme is presented in this paper, which can be used in 3D object reconstruction from single 2D image. BP neural network (BP NN) is applied in our system due to its non-linear mapping and self-adaptive ability. The 3D objects are represented by Open Inventor, which is a 3D software development kit based on OpenGL. Experimental results show that neural network is a promising approach for reconstruction and representation of 3D objects.",https://ieeexplore.ieee.org/document/4618263/,2008 6th IEEE International Conference on Industrial Informatics,13-16 July 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CLEI.2017.8226440,A visualization tool to detect refactoring opportunities in SOA applications,IEEE,Conferences,"Service-oriented computing (SOC) has been widely used by software industry for building distributed software applications that can be run in heterogeneous environments. It has also been required that these applications should be both high-quality and adaptable to market changes. However, a major problem in this type of applications is its growth; as the size and complexity of applications increase, the probability of duplicity of code increases. This problem could have a negative impact on quality attributes, such as performance, maintenance and evolution, among others. This paper presents a web tool called VizSOC to assist software developers in detecting refactoring opportunities in service-oriented applications. The tool receives WSDL (Web Service Description Language) documents, detects anti-patterns and suggests how to resolve them, and delivers a list of refactoring suggestions to start working on the refactoring process. To visualize the results in an orderly and comprehensible way, we use the Hierarchical Edge Bundles (HEB) visualization technique. The experimentation of the tool has been supported using two real-life case-studies, where we measured the amount of anti-patterns detected and the performance of clustering algorithms by using internal validity criteria. The results indicate that VizSOC is an effective aid to detect refactoring opportunities, and also allows developers to reduce effort along the detection process.",https://ieeexplore.ieee.org/document/8226440/,2017 XLIII Latin American Computer Conference (CLEI),4-8 Sept. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMITCon.2019.8862212,ACT Testbot and 4S Quality Metrics in XAAS Framework,IEEE,Conferences,"The purpose of this paper is to analyze all Cloud based Service Models, Continuous Integration, Deployment and Delivery process and propose an Automated Continuous Testing and testing as a service based TestBot and metrics dashboard which will be integrated with all existing automation, bug logging, build management, configuration and test management tools. Recently cloud is being used by organizations to save time, money and efforts required to setup and maintain infrastructure and platform. Continuous Integration and Delivery is in practice nowadays within Agile methodology to give capability of multiple software releases on daily basis and ensuring all the development, test and Production environments could be synched up quickly. In such an agile environment there is need to ramp up testing tools and processes so that overall regression testing including functional, performance and security testing could be done along with build deployments at real time. To support this phenomenon, we researched on Continuous Testing and worked with industry professionals who are involved in architecting, developing and testing the software products. A lot of research has been done towards automating software testing so that testing of software product could be done quickly and overall testing process could be optimized. As part of this paper we have proposed ACT TestBot tool, metrics dashboard and coined 4S quality metrics term to quantify quality of the software product. ACT testbot and metrics dashboard will be integrated with Continuous Integration tools, Bug reporting tools, test management tools and Data Analytics tools to trigger automation scripts, continuously analyze application logs, open defects automatically and generate metrics reports. Defect pattern report will be created to support root cause analysis and to take preventive action.",https://ieeexplore.ieee.org/document/8862212/,"2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)",14-16 Feb. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAE.2009.47,AI Based Framework for Dynamic Modeling of Software Maintenance Effort Estimation,IEEE,Conferences,"With the growth of IT and software industry there has been a shift in the software development paradigm from traditional techniques based to object-oriented and component-based development techniques. Due to this shift dynamic changes occur in the technology, environment and many other qualitative/quantitative factors, leading to increased maintenance effort. The challenge then lies inaccurately modeling and estimating the software maintenance effort under such dynamically emerging circumstances. This paper summarizes some of the recent advances in the field of dynamic software maintenance effort estimation. Various new static and dynamic factors and their roles in effort prediction are discussed, and based on them a dynamic modeling framework is proposed. Artificial intelligence is contemplated in this research, mainly functional networks based. Further, the proposed framework needs to be validated with real life project data.",https://ieeexplore.ieee.org/document/4804539/,2009 International Conference on Computer and Automation Engineering,8-10 March 2009,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC48688.2020.0-227,AI and ML-Driving and Exponentiating Sustainable and Quantifiable Digital Transformation,IEEE,Conferences,"AI is a major transforming technology impacting every sector of life. AI is not a force to deprive humans and take over the control, rather a real enabler and lever for digital transformation. The former view aligns with Hollywood movies, and need to be undressed with the latter, which is realistic and becoming tangible over time as more organizations, and communities are leveraging AI's potential. Developing a practical understanding of AI, its capabilities, the challenges, and opportunities that it brings is fundamental to get the maximum out of its envisaged potential. The objective of this paper is to highlight how technology and industry have developed, discuss the role of AI in driving intelligent transformation concentrating on an applicable understanding of AI and related technologies. We introduce a new conceptual framework: AI's multi-dimensional role, to highlight its transformative power in multiple aspects. We also introduce an AI based Information and Model Governance Framework.",https://ieeexplore.ieee.org/document/9202502/,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",13-17 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA45728.2021.9613359,AI environment for predictive maintenance in a manufacturing scenario,IEEE,Conferences,"Industries generate and collect a huge amount of data about their processes. Usually such data contain relevant information, which can be used to monitor and analyze the processes, but also improve them, applying optimization techniques that allow to enhance different aspects, such as machine maintenance scheduling, product quality, use of resources and so on and so forth. The Digital Twin (DT) concept has been recently applied in the context of Industry 4.0, to exploit this data, leveraging advanced physical modelling, data analysis, Artificial Intelligence (AI) algorithms for optimization and prediction, which are two key concepts in the Industry 4.0 paradigm. Currently, the major challenge in this field is to make these techniques available for the industries and ensure their ease of use for the final users. This paper presents an holistic solution that combining two open-source software - i.e., REclaim oPtimization and simuLatIon Cooperation in digitAl twin (REPLICA) and Optimization Platform for Refurbishment and Re-manufacturing (OPR2) - provides a flexible, open and easy-to-use AI environment that allows data scientists to create, test, connect and deploy their algorithms and to optimize them. This work presents a first prototype of this solution, then describes how it has been tested and validated with real industry data and finally provides the results obtained with these tests.",https://ieeexplore.ieee.org/document/9613359/,2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA ),7-10 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CloudCom.2019.00037,APEX: Adaptive Ext4 File System for Enhanced Data Recoverability in Edge Devices,IEEE,Conferences,"Recently Edge Computing paradigm has gained significant popularity both in industry and academia. With its increased usage in real-life scenarios, security, privacy and integrity of data in such environments have become critical. Malicious deletion of mission-critical data due to ransomware, trojans and viruses has been a huge menace and recovering such lost data is an active field of research. As most of Edge computing devices have compute and storage limitations, difficult constraints arise in providing an optimal scheme for data protection. These devices mostly use Linux/Unix based operating systems. Hence, this work focuses on extending the Ext4 file system to APEX (Adaptive Ext4): a file system based on novel on-the-fly learning model that provides an Adaptive Recover-ability Aware file allocation platform for efficient post-deletion data recovery and therefore maintaining data integrity. Our recovery model and its lightweight implementation allow significant improvement in recover-ability of lost data with lower compute, space, time, and cost overheads compared to other methods. We demonstrate the effectiveness of APEX through a case study of overwriting surveillance videos by CryPy malware on Raspberry-Pi based Edge deployment and show 678% and 32% higher recovery than Ext4 and current state-of-the-art File Systems. We also evaluate the overhead characteristics and experimentally show that they are lower than other related works.",https://ieeexplore.ieee.org/document/8968863/,2019 IEEE International Conference on Cloud Computing Technology and Science (CloudCom),11-13 Dec. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR46125.2019.00036,"AR Tracking with Hybrid, Agnostic And Browser Based Approach",IEEE,Conferences,"Mobile platform tools are desirable when it comes to practical augmented reality applications. With the convenience and portability that the form factor has to offer, it lays an ideal basic foundation for a feasible use case in industry and commercial applications. Here, we present a novel approach of using the monocular Simultaneous Localization and Mapping (SLAM) information provided by a Cross-Reality (XR) device to augment the linked 3D CAD models. The main objective is to use the tracking technology for an augmented and mixed reality experience by tracking a 3D model and superimposing its respective 3D CAD model data over the images we receive from the camera feed of the XR device without any scene preparation (e.g markers or feature maps). The intent is to conduct a visual analysis and evaluations based on the intrinsic and extrinsic of the model in the visualization system that instant3Dhub has to offer. To achieve this we make use of the Apple's ARKit to obtain the images, sensor data and SLAM heuristic of client XR device, remote marker-less model based 3D object tracking from monocular RGB image data and hybrid client server architecture. Our approach is agnostic of any SLAM system or Augmented Reality (AR) framework. We make use of the Apple's ARKit because of the its ease of use, affordability, stability and maturity as a platform and as an integrated system.",https://ieeexplore.ieee.org/document/8942252/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS45731.2020.9180670,AVAC: A Machine Learning Based Adaptive RRAM Variability-Aware Controller for Edge Devices,IEEE,Conferences,"Recently, the Edge Computing paradigm has gained significant popularity both in industry and academia. Researchers now increasingly target to improve performance and reduce energy consumption of such devices. Some recent efforts focus on using emerging RRAM technologies for improving energy efficiency, thanks to their no leakage property and high integration density. As the complexity and dynamism of applications supported by such devices escalate, it has become difficult to maintain ideal performance by static RRAM controllers. Machine Learning provides a promising solution for this, and hence, this work focuses on extending such controllers to allow dynamic parameter updates. In this work we propose an Adaptive RRAM Variability-Aware Controller, AVAC, which periodically updates Wait Buffer and batch sizes using on-the-fly learning models and gradient ascent. AVAC allows Edge devices to adapt to different applications and their stages, to improve computation performance and reduce energy consumption. Simulations demonstrate that the proposed model can provide up to 29% increase in performance and 19% decrease in energy, compared to static controllers, using traces of real-life healthcare applications on a Raspberry-Pi based Edge deployment.",https://ieeexplore.ieee.org/document/9180670/,2020 IEEE International Symposium on Circuits and Systems (ISCAS),12-14 Oct 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IDT.2013.6727080,Accurate and efficient identification of worst-case execution time for multicore processors: A survey,IEEE,Conferences,"Parallel systems were for a long time confined to high-performance computing. However, with the increasing popularity of multicore processors, parallelization has also become important for other computing domains, such as desktops and embedded systems. Mission-critical embedded software, like that used in avionics and automotive industry, also needs to guarantee real time behavior. For that purpose, tools are needed to calculate the worst-case execution time (WCET) of tasks running on a processor, so that the real time system can make sure that real time guarantees are met. However, due to the shared resources present in a multicore system, this task is made much more difficult as compared to finding WCET for a single core processor. In this paper, we will discuss how recent research has tried to solve this problem and what the open research problems are.",https://ieeexplore.ieee.org/document/6727080/,2013 8th IEEE Design and Test Symposium,16-18 Dec. 2013,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSC.2008.33,Activity Recognition Using a Web 3.0 Database,IEEE,Conferences,"Web 3.0 envisages software agents that know how to reason over activities, events, locations, people, companies, and their inter-relationships. Learning more about customers through behavioral and activity recognition is here today through currently available Semantic Technologies and is a showcase for how these technologies will evolve. The demonstration shows real world examples of activity recognition using a combination of industry standard RDF and OWL, reasoning with basic Geotemporal primitives and some well-known Social Network Analytics.",https://ieeexplore.ieee.org/document/4597233/,2008 IEEE International Conference on Semantic Computing,4-7 Aug. 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2017.8104789,Addressing security challenges in industrial augmented reality systems,IEEE,Conferences,"In context of Industry 4.0 Augmented Reality (AR) is frequently mentioned as the upcoming interface technology for human-machine communication and collaboration. Many prototypes have already arisen in both the consumer market and in the industrial sector. According to numerous experts it will take only few years until AR will reach the maturity level to be deployed in productive applications. Especially for industrial usage it is required to assess security risks and challenges this new technology implicates. Thereby we focus on plant operators, Original Equipment Manufacturers (OEMs) and component vendors as stakeholders. Starting from several industrial AR use cases and the structure of contemporary AR applications, in this paper we identify security assets worthy of protection and derive the corresponding security goals. Afterwards we elaborate the threats industrial AR applications are exposed to and develop an edge computing architecture for future AR applications which encompasses various measures to reduce security risks for our stakeholders.",https://ieeexplore.ieee.org/document/8104789/,2017 IEEE 15th International Conference on Industrial Informatics (INDIN),24-26 July 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTer51097.2020.9325474,Adjusting The Hard Level on Game by a Prediction for Improving Attraction and Business Value,IEEE,Conferences,"The computer gaming industry has become more popular among the young generation over the years. It became a sport and According to past studies, people choose computer games over physical games (outdoor games) due to their busy lifestyles which crack social relationships and fear of injuries. There are around 2.2 billion gamers around the world. Out of the assessed 7.6 billion individuals living on earth, as of July 2018, that implies nearly 33% of individuals on this planet are gamers. Out of those 2.2 billion gamers, 1.2 billion of the players play PC game. An online survey was conducted to collect information from random game players and non-game players scattered throughout Sri Lanka to identify the problems faced by them when playing the games and why others do not play the game or are discouraged to play games. 354 participants participated in this survey. In which 127 of the participants were Female and 227 Male. The participation age range was identified as 15 to 49 Years. The research team was able to summarize that 80.04% of the participants play games while 19.06% don't play games. Out of the participants who play games, 9.08% are pro players while the rest of the 78% of players are intermediate players. We identified that the main problems faced by most of the players are as follows: Some missions are too easy and boring or players getting stuck in the middle of the level because the level is too hard or because the mission is too hard to complete. The Popularity of games can be varied according to the players' skill level. Tasks of a game can be boring if it's too easy for a skilled player. On the other hand, if it's too hard for a beginner level player, it may tend to give up the game. The other problem identified was the inability of game character customization as per the preference of 92.07% of gamers who participated in the survey. According to their responses, we were able to understand the problems that they faced while playing the game and what they expect in a game. Therefore, the research team proposes a game that changes the level of hardness/complexity of the level according to the players' mood automatically. In simple terms, the game will `adjust and adopt' based on the user's emotions. The next level will be automatically designed based on the player record of the previous level using an algorithm that takes the emotions as inputs. To achieve this, the gamer's heart pulse rate, facial expression, and speed of using the controls are captured and analysed to obtain the most accurate emotion level of the player using machine learning. The level of emotions is taken along with the parameters such as age, gender, and haptic feedback to adjust the hardness level of the level.It is important to identify the player skill level to understand the user's experience as a gamer. To determine a player's skill level in the background, a keylogger captures the pressed key and the time it takes to press the key. It is also necessary to identify the emotional state of the player to provide a unique user experience to the player and it can be done in two ways which are facial expression and heart-pulse. Facial expression detection results in identifying and predicting the basic emotions of an individual such as happiness, anger, and neutral which will be necessary for the adaptive game, and also the emotional range of a player is predicted. Additionally, age, gender, and face-shape are determined. The dataset is trained using machine learning to detect the facial expression and identify the emotion, age, gender, and face-shape of the individual. The Facial Expression of the player will be captured in real-time from the web camera of the computer and the emotion will be identified automatically as well as the age gender and face-shape. The other method is through Heart pulse, Heart pulse also can be used to detect the emotions of the player. There are 3 types of emotion levels Happy, Fear, Neutral. To do this, the heart rate sensor is used to capture the player's heart rate. According to the output received after training the dataset using machine learning algorithms and the skill level output is used to adjust the level of the game while the gamer is playing the game. To obtain a much more accurate emotional level, the heartbeat level, and the facial emotional level are analysed together. We have created a new model for analysing the above data using linear regression. Using the said model, the gender, age, the skill level of the player as well as the difficulty in the level of the game can be identified. The next level of the game will be designed using the data from the previous level and this process will continue until the player quits the game. A model is trained by using machine learning technology to predict character characteristics (moving speed, health regenerating) of the game character. The difficulty level of the player is taken as input for the training process. The output characteristics are given by an in-game module and adjust the characteristics of the game character until the player quits playing. The AI algorithm performs the process of reflecting the characteristics of the player to the in-game character. Age, gender, and face-shape are used as input data. The algorithm process input data to generate a unique set of variables. The variables manipulate the appearance of the in-game character until the player quits playing. Our game is developed within three months; therefore, it is difficult to predict the emotions or obtain a skill level like in an existing commercialized game because the demo game doesn't evoke any emotions or measure the skill level of the player. So, we have tested our application with the other similar existing games and we were able to obtain an accurate emotion and skill level. Even though the game is designed in a way to change the levels and character abilities based on the inputs obtained, we can't do so in other existing games. Therefore, the player records obtained from the games we tested were taken into a text record and analysed separately. Based on the analysed data we were able to obtain an accuracy level of 68%.In the future, the demo game will be further developed and tested with our system to obtain an emotion. The option of changing the game character based on the player's appearance is 75% successful, in which face-shape and gender were used. In the future, we will be using age as well. In the future a profile can be designed based on skill level, therefore making it possible to adjust the game based on a player's past record.This system provides a solution to problems faced by the game players such as the game being too easy and boring or hard and stressful. This can also prevent health issues that can be caused when a person gets too angry or violent and stress.",https://ieeexplore.ieee.org/document/9325474/,2020 20th International Conference on Advances in ICT for Emerging Regions (ICTer),4-7 Nov. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISCT.2019.8777408,Adoption of VR influencing AI on 3D objects,IEEE,Conferences,"The advent of new technologies has made it possible for new areas to be explored in the gaming industry. Virtual Reality has progressed at an exponential rate which has allowed it to be used in many industries. Virtual Reality allows users to be immersed in a highly detailed environment which allows for a more realistic experience. This paper outlines the development of a game on adoption of VR influencing AI on 3D objects, and the integration of virtual reality gaming Oculus and Leap Motion device in the gameplay, The Leap Motion is used to recognize hand gestures while Oculus will serve as the visual medium for the user. The game itself has been designed and developed on the Unity 3D gaming engine. This project endeavors to highlight the importance and diverse application of VR in various industries. The distinctive integration of Oculus, Leap and PC leads to a realistic gaming experience.",https://ieeexplore.ieee.org/document/8777408/,2019 International Conference on Information Science and Communication Technology (ICISCT),9-10 March 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEARS53579.2022.9752368,Advanced Machine Learning Scenarios for Real World Applications using Weka Platform,IEEE,Conferences,"the most significant predictive Analysis, Machine and Deep Learning in a wide range of fields, including engineering, finance, economics, and real-time imaging. To obtain greater accuracy, scientists are experimenting with a various customs frame works and including tools and technologies. According to Market Research News US, machine learning-based implementations would have a global market value of nearly 50 billion dollars by year 2025.Government and social services are currently implementing Machine and Deep learning in order to reach barest minimum of mistakes reductions. There are several companies in the industry working on cutting-edge algorithms and implementation views for machine learning.",https://ieeexplore.ieee.org/document/9752368/,2022 International Conference on Electronics and Renewable Systems (ICEARS),16-18 March 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SoutheastCon42311.2019.9113415,Advanced Signal Processing for Decision Making and Decision-Fusion Software Systems for Aircraft Structural Health Monitoring,IEEE,Conferences,"Tracking the structure health status of the aircraft fleet is one of the important factors for improving safety in the aviation industry. US Airforce Research Lab is working hard to develop technology to monitor the structural health status of aircraft before and after takeoff. This technology is called Structural Health Monitoring or Structural Health Management (SHM), and uses advanced signal processing techniques. The advantages of this include improving the technology and safety factors of the aircraft by increasing the robustness of the aircraft structure as well as reduce the cost of the labor maintenance. This research paper focuses on developing a software system that is capable of detecting a crack in the aircraft structure at an early stage. Also, the developed software will be used to estimate the crack lengths within 90% accuracy. The developed software uses the Matlab environment for all algorithms developed: calculating and finding the crack, and estimating the crack length. Artificial Intelligent (AI) techniques such as fuzzy logic and neural networks are used to support the decision regarding the length of the crack. Also, developed decision fusion frameworks are designed to increase the accurate percentage rate of the decision making results. The results obtained from this research are compared with the baseline developed by The Air Force Research Laboratory (AFRL). Finally, the results of this research along with the capabilities of using advanced signal processing integrated with AI, show that the calculations for the crack and the crack length produce very acceptable results using a real data gathered by AFRL. Also, the percentage average error performance analysis of the developed software's system algorithms is provided in the research paper.",https://ieeexplore.ieee.org/document/9113415/,2019 SoutheastCon,11-14 April 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EISIC.2017.21,Adversarial Machine Learning in Malware Detection: Arms Race between Evasion Attack and Defense,IEEE,Conferences,"Since malware has caused serious damages and evolving threats to computer and Internet users, its detection is of great interest to both anti-malware industry and researchers. In recent years, machine learning-based systems have been successfully deployed in malware detection, in which different kinds of classifiers are built based on the training samples using different feature representations. Unfortunately, as classifiers become more widely deployed, the incentive for defeating them increases. In this paper, we explore the adversarial machine learning in malware detection. In particular, on the basis of a learning-based classifier with the input of Windows Application Programming Interface (API) calls extracted from the Portable Executable (PE) files, we present an effective evasion attack model (named EvnAttack) by considering different contributions of the features to the classification problem. To be resilient against the evasion attack, we further propose a secure-learning paradigm for malware detection (named SecDefender), which not only adopts classifier retraining technique but also introduces the security regularization term which considers the evasion cost of feature manipulations by attackers to enhance the system security. Comprehensive experimental results on the real sample collections from Comodo Cloud Security Center demonstrate the effectiveness of our proposed methods.",https://ieeexplore.ieee.org/document/8240775/,2017 European Intelligence and Security Informatics Conference (EISIC),11-13 Sept. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EEEI.2014.7005849,Adware detection and privacy control in mobile devices,IEEE,Conferences,"In this paper we propose a system and algorithms for detection of Adware in mobile devices that are based on machine learning algorithms and are capable of adapting to the ongoing transformation of Adware and Malware. The system is based on static and dynamic analysis of mobile applications, extraction of useful features and real-time classification. This classification is based on supervised machine learning algorithms with emphasis on fast, linear operations and efficient implementation on mobile devices. The system presented in this paper enables identification of relevant features that are salient in Adware and Malware, useful for further analysis by security researchers. The proposed system exhibits a detection rate of 97%. The system has been tested and verified on known, industry standard, datasets and is superior to state of the art solutions available in the market. These result have been verified by 3rd party evaluators.",https://ieeexplore.ieee.org/document/7005849/,2014 IEEE 28th Convention of Electrical & Electronics Engineers in Israel (IEEEI),3-5 Dec. 2014,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEWIT.2013.6713745,Agent-based planning and control for groupage traffic,IEEE,Conferences,"In this research and technology transfer project, the planning and control processes of the industrial partner Hellmann Worldwide Logistics GmbH & Co. KG are analyzed. An agent-based approach is presented to model current processes and to exploit the identified optimization potential. The developed system directly connects the information flow and the material flow as well as their interdependencies in order to optimize the planning and control in groupage traffic. The software system maps current processes to agents as system components and improves the efficiency by intelligent objects. To handle the high complexity and dynamics of logistics autonomous intelligent agents plan and control the way of represented objects through the logistic network by themselves and induce a flexible and reactive system behavior. We evaluate the implemented dispatching application by simulating the groupage traffic processes using effectively transported orders and process data provided by our industrial partner. Moreover, we modeled real world infrastructures and considered also the dynamics by the simulation of unexpected events and process disturbances. The results show that the system significantly decreases daily cost by reducing the required number of transport providers and shifting conventional orders to next days, which need no immediate delivery. Thus the system increases the efficiency and meets the special challenges and requirements of groupage traffic. Moreover, the system supports freight carriers and dispatchers with adequate tour and routing proposals. Computed tours were successfully validated by human dispatchers. Due to the promising results, Hellmann is highly interested in transferring the prototype to an application that optimizes the daily operations in numerous distribution centers. Finally, provide further research perspectives, and emphasize the advantages of the developed system in Industry 4.0 applications.",https://ieeexplore.ieee.org/document/6713745/,2013 10th International Conference and Expo on Emerging Technologies for a Smarter World (CEWIT),21-22 Oct. 2013,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCON52037.2021.9702502,Airlines based Twitter Sentiment Analysis Using Deep Learning,IEEE,Conferences,"The airlines industry has been a competitive marketplace that has grown rapidly over the last few decades. Mostly, customers like family members, businessman, sportsman and youngsters are traveling through Airlines. If people are participating, their feedback is extremely important. Customer direct feedback may be favorable or negative, but understanding their Tweets is critical for improvement. This serves as a conduit for open-source inactive communication between promoters and customers coincidentally exercising their time and duties on the same platform for a variety of reasons. Sentiment Analysis on the social networking sites like Twitter or Facebook that bridge the gap between information and real time feedback, has become an amazing method for finding out about a user's feelings and has a wide scope of utilizations. It focuses on polarity (positive, negative, and neutral), sentiments and emotions (urgent, not urgent), and even intents (interested not interested). In this paper, the idea is to analyses the tweets emerging from social site such as Twitter, necessarily focused around the airline industry, its customers and employees, current as well as imminent. So ultimately, the objective is to deploy the deep learning algorithms on dataset of 14641 total tweets regarding U.S airlines, collected from Kaggle repository. The similar data set is utilized for both training and testing because there is more possibility for errors, which raises the likelihood of inaccurate predictions. Therefore, train_test_split function of scikit-learn python library has been used. It allows to breaking a dataset with ease while pursuing an ideal model. To prevent over fitting associated with the co-adaptation of feature detectors, the dropout learning algorithm has been called on to remarkable results.",https://ieeexplore.ieee.org/document/9702502/,2021 5th International Conference on Information Systems and Computer Networks (ISCON),22-23 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NOMS.1998.655214,Alarm correlation engine (ACE),IEEE,Conferences,"Networks are growing in size and complexity, resulting in increased alarm volume and number of unfamiliar alarms. Often, there is no proportional increase in monitoring personnel and response time to faults suffers. GTE deployed Telephone Operations Network Integrated Control System (TONICS) in 1993 to support its network management operations. To stay competitive in the face of continued staff reductions, increase in network size, and monitoring complications related to deregulation of the telephone industry, GTE is introducing artificial intelligence techniques into TONICS. Alarm Correlation Engine (ACE), the system described in this paper, is part of the effort. ACE aids network management by correlating alarms on the basis of common cause to provide alarm compression, filtering, and suppression. In conjunction with its ability to carry out prescribed responses, it improves response times and increases productivity. ACE was developed with the following requirements: reliability, speed, versatility (handle alarms from different switches and networks), ease of knowledge engineering (field technicians must be able to construct, test, and modify correlation patterns), handle in real time multiple network problems, and finally, interface smoothly with GTE's TONICS system. ACE's strength lies in its domain specific correlation language which facilitates knowledge engineering and in its asynchronous processing core that enables integration into a real-time monitoring system.",https://ieeexplore.ieee.org/document/655214/,NOMS 98 1998 IEEE Network Operations and Management Symposium,15-20 Feb. 1998,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSC53577.2021.00101,An APT Malware Classification Method Based on Adaboost Feature Selection and LightGBM,IEEE,Conferences,"Advanced Persistent Threat (APT) attack activities with the theme of COVID-19 and vaccine are also growing rapidly. The target of APT attack has gradually expanded from government agencies to vaccine manufacturers, medical industry and so on. What&#x0027;s more, APT groups have a strict organizational structure and professional division of labor and malware delivered by the same APT groups are similar. Classifying malware samples into known APT groups in time can minimize losses as soon as possible and keep relevant industries vigilant. In our paper, we proposed a multi-classification method of APT malware based on Adaboost and LightGBM. We collect real APT malware samples that have been delivered by 12 known APT groups. The API call sequence of each APT malware is obtained through the sandbox. For the relationship between adjacent APIs, we use TF-IDF algorithm combined with bi-gram. Then, Adaboost algorithm is used to select out the important API features, which form the target feature subset. Finally, we use the above subset combined with LightGBM ensemble algorithm to train multiple classifiers, named Ada-LightGBM. The experimental results show that our method is superior to the single Adaboost and LightGBM method. The classifier has good recognition performance for the test samples.",https://ieeexplore.ieee.org/document/9750513/,2021 IEEE Sixth International Conference on Data Science in Cyberspace (DSC),9-11 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DCOSS49796.2020.00046,An Agnostic Data-Driven Approach to Predict Stoppages of Industrial Packing Machine in Near,IEEE,Conferences,"As data awareness in manufacturing companies increases with the deployment of sensors and Internet of Things (IoT) devices, data-driven maintenance and prediction have become quite popular in the Industry 4.0 paradigm. Machine Learning (ML) has been recognised as a promising, efficient and reliable tool for fault detection use cases, as it allows to export important knowledge from monitored assets. Scientists deal with issues such as the small amount of data that indicate potential problems, or the imbalance which exists between the standard process data and the data inadequacy of the systems to make a high precision forecast. Currently, in this context, even large industries are not able to effectively predict abnormal behaviors in their tools, processes and equipment, when adopting strategies to anticipate crucial events. In this paper, we propose a methodology to enable prediction of a packing machine's stoppages in manufacturing process of a large industry, by using forecasting techniques based on univariate time series data. There are more than 100 reasons that cause the machine to stop, in a quite big production line length. However, we use a single signal, concerning the machines operational status to make our prediction, without considering other fault or warning signals, hence its characterization as ""agnostic"". A workflow is presented for cleaning and preprocessing the data, and for training and evaluating a predictive model. Two predictive models, namely ARIMA and Prophet, are applied and evaluated on real data from an advanced machining process used for packing. Training and evaluation tests indicate that the results of the applied methods perform well on a daily basis. Our work can be further extended and act as reference for future research activities that could lead to more robust and accurate prediction frameworks.",https://ieeexplore.ieee.org/document/9183540/,2020 16th International Conference on Distributed Computing in Sensor Systems (DCOSS),25-27 May 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISEFS.2006.251169,An Approach to Real-time Color-based Object Tracking,IEEE,Conferences,"Object tracking is of great interest in different areas of industry, security and defense. Tracking moving objects based on color information is more robust than systems utilizing motion cues. In order to maintain the lock on the object as the surrounding conditions vary, the color model needs to be adapted in real-time. In this paper an on-line learning method for the color model is implemented using fuzzy adaptive resonance theory (ART). Fuzzy ART is a type of neural network that is trained based on competitive learning principle. The color model of the target region is regularly updated based on the vigilance criteria (which is a threshold) applied to the pixel color information. The target location in the next frame is predicted using evolving extended Takagi-Sugeno (exTS) model to improve the tracking performance. The results of applying exTS for prediction of the position of the moving target were compared with the usually used solution based on Kalman filter. The experiments with real footage demonstrate over a variety of scenarios the superiority of the exTS as a predictor comparing to the Kalman filter. Further investigation concentrates on using evolving clustering for realizing computationally efficient simultaneous tracking of different segments in the object",https://ieeexplore.ieee.org/document/4016733/,2006 International Symposium on Evolving Fuzzy Systems,7-9 Sept. 2006,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCC.2018.00010,An Edge Based Smart Parking Solution Using Camera Networks and Deep Learning,IEEE,Conferences,"The smart parking industry continues to evolve as an increasing number of cities struggle with traffic congestion and inadequate parking availability. For urban dwellers, few things are more irritating than anxiously searching for a parking space. Research results show that as much as 30% of traffic is caused by drivers driving around looking for parking spaces in congested city areas. There has been considerable activity among researchers to develop smart technologies that can help drivers find a parking spot with greater ease, not only reducing traffic congestion but also the subsequent air pollution. Many existing solutions deploy sensors in every parking spot to address the automatic parking spot detection problems. However, the device and deployment costs are very high, especially for some large and old parking structures. A wide variety of other technological innovations are beginning to enable more adaptable systems-including license plate number detection, smart parking meter, and vision-based parking spot detection. In this paper, we propose to design a more adaptable and affordable smart parking system via distributed cameras, edge computing, data analytics, and advanced deep learning algorithms. Specifically, we deploy cameras with zoom-lens and motorized head to capture license plate numbers by tracking the vehicles when they enter or leave the parking lot; cameras with wide angle fish-eye lens will monitor the large parking lot via our custom designed deep neural network. We further optimize the algorithm and enable the real-time deep learning inference in an edge device. Through the intelligent algorithm, we can significantly reduce the cost of existing systems, while achieving a more adaptable solution. For example, our system can automatically detect when a car enters the parking space, the location of the parking spot, and precisely charge the parking fee and associate this with the license plate number.",https://ieeexplore.ieee.org/document/8457691/,2018 IEEE International Conference on Cognitive Computing (ICCC),2-7 July 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2019.00115,An Edge Computing Visual System for Vegetable Categorization,IEEE,Conferences,"In self-service supermarket and retail industry, efforts to reduce customer wait time using automatic grocery item identification are challenged by low recognition accuracy, long response time and substantial requirement for equipment. In this paper, we propose a novel edge computing system named EdgeVegfru for vegetable and fruit image classification. While existing work on Vegfru dataset shows excellent performance, few of them have been deployed in real-world applications. We adopt an edge computing paradigm, design, implement and evaluate the whole system on the Android devices. The proposed deep learning model and quantization algorithm reduce the model size and inference time significantly. Our system has shown out-standing accuracy within limited time and computation resources, compared with other machine learning methods(such as Support Vector Machine(SVM), Random Forest(RF)), thus providing the potential path for automatic recognition and pricing in self-service retail stores.",https://ieeexplore.ieee.org/document/8999203/,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),16-19 Dec. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/QRS54544.2021.00098,An Effective Crowdsourced Test Report Clustering Model Based on Sentence Embedding,IEEE,Conferences,"In the crowdsourced testing industry, efficient and automated classification of true bugs from test reports can greatly reduce the cost of software testing. Most of the existing methods are based on TF-IDF or machine learning methods to vectorize the test report and then construct a classifier. However, the document vector constructed by keywords more or less ignores the description information in the document, which affects the performance of classification and detection of real defects. In order to use the description information to construct an effective report clustering model, we propose a model called RCSE to encode test report description information at the sentence level, calculate the similarity between the test reports from the feature similarity of the description sentence, then cluster the test report. We evaluated the model on 3,442 reports. The experimental results show that the clustering model based on sentence embedding has an average purity of 12.3 &#x0025; and an ARI of 22.0&#x0025; higher than the keyword-based model on three report datasets.",https://ieeexplore.ieee.org/document/9724806/,"2021 IEEE 21st International Conference on Software Quality, Reliability and Security (QRS)",6-10 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIC53490.2021.9692984,An Effective Technique for Detecting Defecting Parts of Fabric in Digital Image,IEEE,Conferences,"Inspection and detection of fault parts of the fabric are one of the major issues in the textile industry. Currently, detection of the faulty parts of fabric by the human vision system, which detect very less percentage and decreases the quality of the fabric. So, there is a need to develop a new real-time technique that consists of a computer vision system that works automatically to detect all fault parts of fabric and increase the quality of the fabric. In this paper, we proposed a method to detect the defects in the fabric. To achieve this objective, Fast Fourier Transform and Cross-correlation techniques, i.e. linear processes are first realized to scrutinize the building uniformity land-escapes of the fabric picture in the incidence area. To progress the competence of the system and speechless the tricky of uncovering errors, an extra thresholding process is realized using an equal collection filter. Through this sieve, the system is able to detect only the definite or real defects and high point their meticulous proportions. Also, Recurrent Neural Network and Artificial Neural Network are used to classify the faulty and non-faulty parts of the image. Results shows that proposed method is performed better as compare to baseline methods.",https://ieeexplore.ieee.org/document/9692984/,2021 International Conference on Innovative Computing (ICIC),9-10 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2CT45611.2019.9033957,An Efficient Approach to Fruit Classification and Grading using Deep Convolutional Neural Network,IEEE,Conferences,"In India, the agricultural industry has seen a boom in recent years, demanding an increased inclusion of automation in it. An important aspect of this agro-automation is grading and classification of agricultural produce. These labor intensive tasks can be automated by use of Computer Vision and Machine Learning. This paper focuses on developing a standalone system capable of classifying 3 types of fruit and taking apple as test case of grading. The fruit types include apple, orange, pear and lemon. Further, apples have been graded into four grades, Grade 1 being the best quality apple and Grade 4 consisting of the spoilt ones. Input is given in the form of fruit image. The involved methodology is dataset formation, preprocessing, software as well as hardware implementations and classification. Preprocessing consists of background removal and segmentation techniques in order to extract fruit area. Deep Convolutional Neural Network has been chosen for the real time implementation of system and applied on fruit 360 dataset. For that purpose, the Inception V3 model is trained using the transfer training approach, thus enabling it to distinguish fruit images. The results after experimentation show that the Top 5 accuracy on the dataset used is 90% and the Top 1 accuracy is 85% which targets accuracy limitation of previous attempts.",https://ieeexplore.ieee.org/document/9033957/,2019 IEEE 5th International Conference for Convergence in Technology (I2CT),29-31 March 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IDAP.2018.8620812,An Embedded Real-Time Object Detection and Measurement of its Size,IEEE,Conferences,"In these days, real-time object detection and dimensioning of objects is an important issue from many areas of industry. This is a vital topic of computer vision problems. This study presents an enhanced technique for detecting objects and computing their measurements in real time from video streams. We suggested an object measurement technique for real-time video by utilizing OpenCV libraries and includes the canny edge detection, dilation, and erosion algorithms. The suggested technique comprises of four stages: (1) identifying an object to be measured by using canny edge detection algorithm, (2) using morphological operators includes dilation and erosion algorithm to close gaps between edges, (3) find and sort contours, (4) measuring the dimensions of objects. In the implementation of the proposed technique, we designed a system that used OpenCV software library, Raspberry Pi 3 and Raspberry Camera. The proposed technique was nearly achieved 98% success in determines the size of the objects.",https://ieeexplore.ieee.org/document/8620812/,2018 International Conference on Artificial Intelligence and Data Processing (IDAP),28-30 Sept. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSME.2017.71,An Experience Report on Applying Passive Learning in a Large-Scale Payment Company,IEEE,Conferences,"Passive learning techniques infer graph models on the behavior of a system from large trace logs. The research community has been dedicating great effort in making passive learning techniques more scalable and ready to use by industry. However, there is still a lack of empirical knowledge on the usefulness and applicability of such techniques in large scale real systems. To that aim, we conducted action research over nine months in a large payment company. Throughout this period, we iteratively applied passive learning techniques with the goal of revealing useful information to the development team. In each iteration, we discussed the findings and challenges to the expert developer of the company, and we improved our tools accordingly. In this paper, we present evidence that passive learning can indeed support development teams, a set of lessons we learned during our experience, a proposed guide to facilitate its adoption, and current research challenges.",https://ieeexplore.ieee.org/document/8094462/,2017 IEEE International Conference on Software Maintenance and Evolution (ICSME),17-22 Sept. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAPAI49758.2021.9462061,An Industrial Assistance System with Manual Assembly Step Recognition in Virtual Reality,IEEE,Conferences,"In the era of Industry 4.0, worker assistance systems are becoming more and more important. In order to assist shop floor workers in manual assembly tasks, we implemented an assistance system in virtual reality. A deep neural network was trained to recognize the current work step in real-time during an assembly process, thus giving the assistance system context-awareness. We defined the problem of assembly step recognition as a multivariate time series classification using the poses of the workers head, both hands and all relevant tools and objects. With this definition, the VR environment's output can also be replaced with data from the real world. For our proof-of-concept assembly step recognition system, we created an assembly process consisting of six different work steps, five movable assembly parts and one tool. We showed that we can train an activity recognition model for assembly steps with only 10 assembly recordings. To achieve this, we used multiple data augmentation techniques and proposed a novel method of synthesizing new training data, which we call Path Joining. With only 10 training recordings, we attain a categorical classification accuracy of 81 percent and with 60 recordings we achieve an accuracy of 89 percent.",https://ieeexplore.ieee.org/document/9462061/,2021 International Conference on Applied Artificial Intelligence (ICAPAI),19-21 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDCS.2019.00139,An Industrial IoT Solution for Evaluating Workers' Performance Via Activity Recognition,IEEE,Conferences,"The Industrial Internet of Things (IIoT) is a key pillar of the Fourth Industrial Evolution or Industry 4.0. It aims to achieve direct information exchange between industrial machines, people, and processes. By tapping and analysing such data, IIoT can more importantly provide for significant improvements in productivity, product quality, and safety via proactive detection of problems in the performance and reliability of production machines, workers, and industrial processes. While the majority of existing IIoT research is currently focusing on the predictive maintenance of industrial machines (unplanned production stoppages lead to significant increases in costs and lost plant productivity), this paper focuses on monitoring and assessing worker productivity. This IIoT research is particularly important for large manufacturing plants where most production activities are performed by workers using tools and operating machines. With this aim, this paper introduces a novel industrial IoT solution for monitoring, evaluating, and improving worker and related plant productivity based on workers activity recognition using a distributed platform and wearable sensors. More specifically, this IIoT solution captures acceleration and gyroscopic data from wearable sensors in edge computers and analyses them in powerful processing servers in the cloud to provide a timely evaluation of the performance and productivity of each individual worker in the production line. These are achieved by classifying worker production activities and computing Key Performance Indicators (KPIs) from the captured sensor data. We present a real-world case study that utilises our IIoT solution in a large meat processing plant (MPP). We illustrate the design of the IIoT solution, describe the in-plant data collection during normal operation, and present the sensor data analysis and related KPI computation, as well as the outcomes and lessons learnt.",https://ieeexplore.ieee.org/document/8884821/,2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS),7-10 July 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AITEST52744.2021.00024,An Industrial Workbench for Test Scenario Identification for Autonomous Driving Software,IEEE,Conferences,"Testing of autonomous vehicles involves enormous challenges for the automotive industry. The number of real-world driving scenarios is extremely large, and choosing effective test scenarios is essential, as well as combining simulated and real world testing. We present an industrial workbench of tools and workflows to generate efficient and effective test scenarios for active safety and autonomous driving functions. The workbench is based on existing engineering tools, and helps smoothly integrate simulated testing, with real vehicle parameters and software. We aim to validate the workbench with real cases and further refine the input model parameters and distributions.",https://ieeexplore.ieee.org/document/9564354/,2021 IEEE International Conference on Artificial Intelligence Testing (AITest),23-26 Aug. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASI55125.2022.9774445,An Innovative Method to Monitor and Control an Injection Molding Process Condition using Artificial Intelligence based Edge Computing System,IEEE,Conferences,"High precision injection molding process is in high demand among the polymer industrialist to maintain a sustainable and consistent production of the plastic product parts, and it is hard to estimate and judge the early detection of the defective product parts from the machine parameter and processing condition. However, the real-time variation in the process condition is reflected in the polymer melt flow pressure and temperature variation, and in the specific volume of the product part built in the mold cavity. Accordingly, in this objective, this paper proposed a cost-effective, embedded edge computing system using temperature and pressure sensors interfaced with Arduino Mega and ESP 32D for both real-time monitoring, and a data acquisition unit to train and develop an artificial model (AI). Thereby, an AI model with low mean absolute error and root mean squared error is developed using TensorFlow Lite Micro and loaded into the edge device to detect the variation and predict the specific volume of the molded product part in real-time from the obtained pressure and temperature sensor data. The experimental study reveals that the proposed approach has a lot of potential for practical applications in an industrial process to analyze and predict an insight in advance and for the successful implementation of smart sensor application, intelligent manufacturing constituting Industry 4.0.",https://ieeexplore.ieee.org/document/9774445/,2022 8th International Conference on Applied System Innovation (ICASI),22-23 April 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ic-ETITE47903.2020.450,An IoT Based Smart Water Quality Monitoring System using Cloud,IEEE,Conferences,"The Internet of Things (IoT) is the network of physical devices, vehicles, home appliances, and other items embedded with electronics, software, sensors, actuators and connectivity which enables these things to connect and exchange data. The number of IoT devices has increased 31% year-over-year to 8.4 billion in 2017 and it is estimated that there will be 30 billion devices by 2020. Water pollution is a major environmental problem in India. The largest source of water pollution in India is untreated sewage. Other sources of pollution include agricultural runoff and unregulated small scale industry that results in polluting, most of the rivers, lakes and surface water in India. In this paper, An IoT Based Smart Water Quality Monitoring System using Cloud and Deep Learning is proposed to monitor the quality of the water in water-bodies. In conventional systems, the monitoring process involves the manual collection of sample water from various regions, followed by laboratory testing and analysis. This process is ineffective, as this process is arduous and time-consuming and it does not provide real-time results. The quality of water should be monitored continuously, to ensure the safe supply of water from any water bodies and water resources. Hence, the design and development of a low-cost system for real-time monitoring of water quality using the Internet of Things (IoT) is essential. Monitoring water quality in water bodies using Internet of Things (IoT) helps in combating environmental issues and improving the health and living standards of all living things. The proposed system monitors the quality of water relentlessly with the help of IoT devices, such as, NodeMCU. The in-built Wi-Fi module is attached in NodeMCU which enables internet connectivity transfers the measured data from sensors to the Cloud. The prototype is designed in such a way that it can monitor the number of pollutants in the water. Multiple sensors are used to measure various parameters to assess the quality of water from water bodies. The results are stored in the Cloud, deep learning techniques are used to predict whether the water suitable or not.",https://ieeexplore.ieee.org/document/9077792/,2020 International Conference on Emerging Trends in Information Technology and Engineering (ic-ETITE),24-25 Feb. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSE46696.2019.8984462,An IoT Reconfigurable SoC Platform for Computer Vision Applications,IEEE,Conferences,"The field of Internet of Things (IoT) and smart sensors has expanded rapidly in various fields of research and industrial applications. The area of IoT robotics has become a critical component in the evolution of Industry 4.0 standard. In this paper, we developed an IoT based reconfigurable System on Chip (SoC) robot that is fast and efficient for computer vision applications. It can be deployed in other IoT robotics applications and achieve its intended function. A Terasic Hexapod Spider Robot (TSR) was used with its DE0-Nano SoC board to implement our IoT robotics system. The TSR was designed to provide a competent computer vision application to recognize different shapes using a machine learning classifier. The data processing for image detection was divided into two parts, the first part involves hardware implementation on the SoC board and to provide real-time interaction of the robot with the surrounding environment. The second part of implementation is based on the cloud processing technique, where further data analysis was performed. The image detection algorithm for the computer vision component was tested and successfully implemented to recognize shapes. The TSR moves or reacts based on the detected image. The Field Programmable Gate Array (FPGA) part is programmed to handle the movement of the robot and the Hard Processor System (HPS) handles the shape recognition, Wi-Fi connectivity, and Bluetooth communication. This design is implemented, tested and can be used in real-time applications in harsh environments where movements of other robots are restricted.",https://ieeexplore.ieee.org/document/8984462/,2019 International Symposium on Systems Engineering (ISSE),1-3 Oct. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCNCW.2019.8902853,An Open 5G NFV Platform for Smart City Applications Using Network Softwarization,IEEE,Conferences,"Advanced wireless communication network testbeds are now widely being deployed around European and cross-continental. This represents an interesting opportunity for vertical industry and academia to perform experimentation and validation before a real deployment. In this paper, we present 5GinFIRE as a suitably flexible platform towards open 5G (Network Function Virtualization (NFV) ecosystem and playground. On top of this platform, we designed and deployed a smart city safety system as a vertical use case, exploring 5G capabilities through a combination of NFV and machine learning to provide end-to-end communication and low latency smart city service. This safety system helps detecting criminals along the city and sending a notification to the security center. A Virtual Network Function (VNF) has been developed to enable video transcoding, face detection and recognition at the cloud or the edge of the network. The validation of the overall system is performed through the deployment of the use case indoor (Smart Internet Lab) and outdoor (Millennium Square Bristol). We show the VNF specification and present a quantitative analysis in terms of bandwidth, response time, processing time and transmission speed in terms of Quality of Experience (QoE).",https://ieeexplore.ieee.org/document/8902853/,2019 IEEE Wireless Communications and Networking Conference Workshop (WCNCW),15-18 April 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSC47296.2019.8937650,An Optimized Positive-Unlabeled Learning Method for Detecting a Large Scale of Malware Variants,IEEE,Conferences,"Malicious softwares (Malware) are able to quickly evolve into many different variants and evade existing detection mechanisms, rendering the ineffectiveness of traditional signature-based malware detection systems. Many researchers have proposed advanced malware detection techniques by using Machine Learning. Although the machine learning based techniques perform well in detecting a wide range of malware variants, there still remain some problems when meeting the real scene in the industry. Since the volume of new malware variants grows fast and labelling data is expensive and takes a lot of labor, companies cannot label every one of those samples. They tend to label a small part of the malware samples and treat the rest of the unlabeled samples as benign samples in which the original malware samples are treated as mislabeled. This causes a bias of decision boundary which severely limits the accuracy. To address such a problem, in this paper, we propose a cost-sensitive boosting method to train an unbiased detection model with the malicious-unlabeled executables to improve the accuracy. Along with that, in order to detect malware variants efficiently, we propose a byte co-occurrence matrix as a representation of byte streams of executables to detect malware variants directly. Experimental results show that the machine learning methods optimized by our approach can achieve 80% to 90% accuracy while the original machine learning methods can only achieve 50% to 85% accuracy when the unlabeled data contain different rates of mislabeled positive data.",https://ieeexplore.ieee.org/document/8937650/,2019 IEEE Conference on Dependable and Secure Computing (DSC),18-20 Nov. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPHYS.2018.8390779,An approach for implementing key performance indicators of a discrete manufacturing simulator based on the ISO 22400 standard,IEEE,Conferences,"Performance measurement tools and techniques have become very significant in today's industries for increasing the efficiency of their processes in order to face the competitive market. The first step towards performance measurement is the real-time monitoring and gathering of the data from the manufacturing system. Applying these performance measurement techniques on real-world industry in a way that is more general and efficient is the next challenge. This paper presents a methodology for implementing the key performance indicators defined in the ISO 22400 standard-Automation systems and integration, Key performance indicators (KPIs) for manufacturing operations management. The proposed methodology is implemented on a multi robot line simulator for measuring its performance at runtime. The approach implements a knowledge-based system within an ontology model which describes the environment, the system and the KPIs. In fact, the KPIs semantic descriptions are based on the data models presented in the Key Performance Indicators Markup Language (KPIML), which is an XML implementation of models developed by the Manufacturing Enterprise Solutions Association (MESA) international organization.",https://ieeexplore.ieee.org/document/8390779/,2018 IEEE Industrial Cyber-Physical Systems (ICPS),15-18 May 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1992.220085,An optimal scheduling of pick place operations of a robot-vision-tracking system by using back-propagation and Hamming networks,IEEE,Conferences,"The authors present a neural network approach to solve the dynamic scheduling problem for pick-place operations of a robot-vision-tracking system. An optimal scheduling problem is formulated to minimize robot processing time without constraint violations. This is a real-time optimization problem which must be repeated for each group of objects. A scheme which uses neural networks to learn the mapping from object pattern space to optimal order space offline and to recall online what has been learned is presented. The idea was implemented in a real system to solve a problem in large commercial dishwashing operations. Experimental results have been shown that with four different objects, time savings of up to 21% are possible over first-come, first-served schemes currently used in industry.<>",https://ieeexplore.ieee.org/document/220085/,Proceedings 1992 IEEE International Conference on Robotics and Automation,12-14 May 1992,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ESSCIRC.1992.5468456,Analog Electronic Neural Networks,IEEE,Conferences,"The interest in analog circuit techniques for implementing neural nets is undiminished, as is indicated by a large number of recent designs, coming from universities as well as from industry. One group of circuits are networks containing the ""multiply-accumulate"" neurons with a large interconnectivity. The main motivation for using analog circuit techniques is the fact that the multiply-accumulate operation can be implemented compactly, if only a moderate precision of the computation is required. Other types of networks are more algorithm-specific, hard-wired for one function, for example Kohonen networks, or neuromorphic designs implementing functions found in the visual or the auditory system. Most neural nets are built with standard CMOS technology, except for a few designs in CCD technology. A few analog neural net chips are now commercially available, and more and more reports of applications are appearing. This is a significant step in the development of analog neural nets, as now their usefulness is being put to the test in ""real-world"" applications.",https://ieeexplore.ieee.org/document/5468456/,ESSCIRC '92: Eighteenth European Solid-State Circuits conference,21-23 Sept. 1992,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCT52121.2021.9616738,Analysis of Edge Detection for Road Lanes through Hardware Implementation,IEEE,Conferences,"Recent years have witnessed the rapid advancements and commercial success of autonomous vehicles due to the developments in Image Processing, Machine Learning and AI. This has attracted researchers from both academia and industry to conduct various research on driverless or self-driving cars. One of the significant aspects of self-driving vehicles is the detection of lanes through Edge Detection to correct the vehicle from inadvertent road departure. As real-time Edge detection is a fundamental step of Complex Image processing that handles lane detection and autonomous driving, it is crucial for engineers to select optimum edge detection technique. This paper, presents a practical approach to compare various edge detection algorithms to figure out the most ideal method for lane detection, considering processing time and efficiency. Though, multiple research works are available regarding edge detection method, all of these comparisons are either software simulation based or was not focused primarily on the task of lane detection. Thus, creating an absence of hardware-based practical implementation and comparison of the edge detection methods. In this work, we have discussed the working of four most commonly used edge detection technologies: Sobel, Roberts, Laplacian and Canny, followed by the results from implementing them on a scaled autonomous electric car using Raspberry Pi, Arduino and Raspberry Pi Camera Module, under similar test conditions. Furthermore, a detailed literature review is also presented in this article.",https://ieeexplore.ieee.org/document/9616738/,"2021 Fourth International Conference on Electrical, Computer and Communication Technologies (ICECCT)",15-17 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONISOFT50191.2020.00025,Analysis of automated estimation models using machine learning,IEEE,Conferences,"Plenty of practice based on software estimation has been developed in software industry. Algorithmic models represent the most formal approach that have provided the most reliable results. However, the use of informal practice is still prevalent just like the expert judgment which will not allow Software Engineering grow up.An important activity in big and small companies is to generate reliable estimation models. The development of these models is usually based on information obtained from past projects and requires a deep and precise analysis.This paper presents the application of the automated estimation-model generator system that uses machine learning techniques whit the objective of analysing the accuracy of these models comparing them to the traditional estimation methods using an international database and the internal database of a company.",https://ieeexplore.ieee.org/document/9307790/,2020 8th International Conference in Software Engineering Research and Innovation (CONISOFT),4-6 Nov. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CICT.2013.6558219,Analysis of rice granules using image processing and neural network,IEEE,Conferences,"In food handling industry, grading of granular food materials is necessary because samples of material are subjected to adulteration. In the past, food products in the form of particles or granules were passed through sieves or other mechanical means for grading purposes. In this paper, analysis is performed on basmati rice granules; to evaluate the performance using image processing and Neural Network is implemented based on the features extracted from rice granules for classification grades of granules. Digital imaging is recognized as an efficient technique, to extract the features from rice granules in a non-contact manner. Images are acquired for rice using camera. Conversion to gray scale, Median smoothing, Adaptive thresholding, Canny edge detection, Sobel edge Detection, morphological operations, extraction of quantitative information are the checks that are performed on the acquired image using image processing technique through Open source Computer Vision (Open CV) which is a library of functions that aids image processing in real time. The morphological features acquired from the image are given to Neural Network. This work has been done to identify the relevant quality category for a given rice sample based on its parameters. The performance of image processing reduced the time of operation and improved the crop recognition greatly. Grading results obtained from Neural Network system shows greater accuracy when compared with the outputs from human experts.",https://ieeexplore.ieee.org/document/6558219/,2013 IEEE Conference on Information & Communication Technologies,11-12 April 2013,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRITO51393.2021.9596185,Analyzing Psychological Gamers Profile through Progressive Gaming and Artificial Intelligence,IEEE,Conferences,"Game development has evolved to a point of being a method where it is being used to improve the day-to-day lifestyle of the person playing the game (henceforth referred to as a ‘player’). With these advancements, this paper represents the Integration of Artificial Intelligence (AI) in Game Development. AI has already been introduced in Game Development, ranging from Computer Generated Non-Player Characters (NPC), randomly generated maps or level, or even the difficulty of the level. In Gaming, AI is based on responsive and adaptive video game experiences. Using NPC to power interactive experience to simulate a human Game-player. One of the new Frontier of development and the method games are played, AI uses the behaviour of the player to control the game experience. Games refer to data that is made systematic using algorithms and helps to develop games with human interaction. As a developer, the experience needs to be delivered and AI makes the process easier. AI needs to store information to be able to reference which can respond to various stimuli. The mass of information required is the reason why AI has not been adapted into the Industry. Using Voice Intelligence is changing the inputs of games, and assist to increase intelligence genre games. Creating frameworks using design acknowledgment and reinforcement realizing is an important part of the future of Game development. By making the AI intelligent makes the games more real. Since Players pay attention to details, developers can use AI to help develop the game. In this paper, we will be focusing on the ways to identify the behaviour of the player and then using the information to develop the game further. By creating a profile, we can arrange and suit various reinforcements. Using the 16 personality tests, we can identify and rate the player based on the factor of their Mind, Energy, Nature, Tactics, and Identity.",https://ieeexplore.ieee.org/document/9596185/,"2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",3-4 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SITA.2015.7358402,Analyzing the non-functional requirements to improve accuracy of software effort estimation through case based reasoning,IEEE,Conferences,"Producing accurate software effort estimation is essential for effective software project management that remains a considerable challenge to software engineering and software industry in general. Many methods have been proposed to increase the accuracy of estimating the software project size, effort, or cost. However, the primary focus has been on functional requirements FRs. We are convinced that the rigorous estimation requires a thorough knowledge of all the requirements of the software to be measured. Consequently, a clear identification of the FRs and NFRs as well as a strong understanding of the relationships existing between them is crucial to get measurements closer to reality. In this paper, we propose an early software size and effort estimation method based on a combination of COSMIC and case based reasoning that uses individual requirement measurements as a solution to improve the performance of CBR and to increase the precision of the estimations. This hybrid technique consists in adjusting the FRs measurements by the effect of NFRs with which they are connected. A new link requirements model is proposed in which the possible relationships existing between FRs and NFRs are expressed. This combination will help to efficiently include NFRs, and their relations with FRs, earlier in the measurement process and throughout the life cycle of the software development project.",https://ieeexplore.ieee.org/document/7358402/,2015 10th International Conference on Intelligent Systems: Theories and Applications (SITA),20-21 Oct. 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CASE49439.2021.9551267,Anomaly Detection Based on Selection and Weighting in Latent Space,IEEE,Conferences,"With the high requirements of automation in the era of Industry 4.0, anomaly detection plays an increasingly important role in high safety and reliability in the production and manufacturing industry. Recently, autoencoders have been widely used as a backend algorithm for anomaly detection. Different techniques have been developed to improve the anomaly detection performance of autoencoders. Nonetheless, little attention has been paid to the latent representations learned by autoencoders. In this paper, we propose a novel selection-and-weighting-based anomaly detection framework called SWAD. In particular, the learned latent representations are individually selected and weighted. Experiments on both benchmark and real-world datasets have shown the effectiveness and superiority of SWAD. On the benchmark datasets, the SWAD framework has reached comparable or even better performance than the state-of-the-art approaches.",https://ieeexplore.ieee.org/document/9551267/,2021 IEEE 17th International Conference on Automation Science and Engineering (CASE),23-27 Aug. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LCN52139.2021.9524931,Anomaly Detection for Discovering Performance Degradation in Cellular IoT Services,IEEE,Conferences,"Connected and automated vehicles (CAVs) are envisioned to revolutionize the transportation industry, enabling autonomous processes and real-time exchange of information among vehicles and infrastructure. To safely navigate the roadways, CAVs rely on sensor readings and data from the surrounding vehicles. Hence, a fault or anomaly arising from the hardware, software, or the network can lead into devastating consequences regarding safety. This study investigates potential performance degradation caused by anomalies, by analyzing real-life vehicles’ sensory and network-related data. The aim is to utilize unsupervised learning for anomaly detection, with a goal to describe the cause and effect of the detected anomalies from a performance perspective. The results show around 93% F1-score when detecting anomalies imposed by the cellular network and the vehicle’s sensors. Moreover, with approximately 90% F1-score we can detect anomalous predictions from a deployed network-related ML model predicting cellular throughput and describe the root-causes behind the detected anomalies.",https://ieeexplore.ieee.org/document/9524931/,2021 IEEE 46th Conference on Local Computer Networks (LCN),4-7 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNSURV.2016.7486356,Anomaly detection in aircraft data using Recurrent Neural Networks (RNN),IEEE,Conferences,"Anomaly Detection in multivariate, time-series data collected from aircraft's Flight Data Recorder (FDR) or Flight Operational Quality Assurance (FOQA) data provide a powerful means for identifying events and trends that reduce safety margins. The industry standard “Exceedance Detection” algorithm uses a list of specified parameters and their thresholds to identify known deviations. In contrast, Machine Learning algorithms detect unknown unusual patterns in the data either through semi-supervised or unsupervised learning. The Multiple Kernel Anomaly Detection (MKAD) algorithm based on One-class SVM identified 6 of 11 canonical anomalies in a large dataset but is limited by the need for dimensionality reduction, poor sensitivity to short term anomalies, and inability to detect anomalies in latent features. This paper describes the application of Recurrent Neural Networks (RNN) with Long Term Short Term Memory (LTSM) and Gated Recurrent Units (GRU) architectures which can overcome the limitations described above. The RNN algorithms detected 9 out the 11 anomalies in the test dataset with Precision = 1, Recall = 0.818 and F1 score = 0.89. RNN architectures, designed for time-series data, are suited for implementation on the flight deck to provide real-time anomaly detection. The implications of these results are discussed.",https://ieeexplore.ieee.org/document/7486356/,2016 Integrated Communications Navigation and Surveillance (ICNS),19-21 April 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/SOFTCOM.2019.8903672,Anomaly-based Intrusion Detection in Industrial Data with SVM and Random Forests,IEEE,Conferences,"Attacks on industrial enterprises are increasing in number as well as in effect. Since the introduction of industrial control systems in the 1970' s, industrial networks have been the target of malicious actors. More recently, the political and warfare-aspects of attacks on industrial and critical infrastructure are becoming more relevant. In contrast to classic home and office IT systems, industrial IT, so-called OT systems, have an effect on the physical world. Furthermore, industrial devices have long operation times, sometimes several decades. Updates and fixes are tedious and often not possible. The threats on industry with the legacy requirements of industrial environments creates the need for efficient intrusion detection that can be integrated into existing systems. In this work, the network data containing industrial operation is analysed with machine learning- and time series-based anomaly detection algorithms in order to discover the attacks introduced to the data. Two different data sets are used, one Modbus-based gas pipeline control traffic and one OPC UA-based batch processing traffic. In order to detect attacks, two machine learning-based algorithms are used, namely SVM and Random Forest. Both perform well, with Random Forest slightly outperforming SVM. Furthermore, extracting and selecting features as well as handling missing data is addressed in this work.",https://ieeexplore.ieee.org/document/8903672/,"2019 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",19-21 Sept. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICE2T.2017.8215979,Application framework for forest surveillance and data acquisition using unmanned aerial vehicle system,IEEE,Conferences,"An application framework is proposed in this paper that considers low cost surveillance mechanism and data acquisition in the forest. An application is developed as proof of concept with detailed design that can take advantage of unmanned urban vehicle to be directly configured and controlled in real-time. The advantages are numerous; it can be used for many purposes. For example, it can be used for observing critical and important area for intruder activities or to know the current state of any object of interest. We considered using machine learning and image processing and can be used for species of trees in the forest by color and size detection. A separate service running on separate remote server will be responsible for this. We have proposed a application framework particularly to be cheap and targeted to be easy to handle by non-technical persons and that it does not require large software system knowledge like Pix4D or DroneDeploy. This system will be useful for operations and research specially the forestry and palm oil plantation surveillance, and sustainable timber industry that specially needs carefully collected imageries and data from objects. Collection of raw data from sensor networks is also proposed in the system architecture.",https://ieeexplore.ieee.org/document/8215979/,2017 International Conference on Engineering Technology and Technopreneurship (ICE2T),18-20 Sept. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DASA54658.2022.9765294,"Application of AI, IOT and ML for Business Transformation of The Automotive Sector",IEEE,Conferences,"Automotive industry is essential in human lives. It is not possible to imagine a day without driving or some public transport. Today, digital technologies are making motor vehicles and the industry more intelligent. The entire value chain of automotive business is transforming. A better connect with customers is needed. All this is possible through advanced digital technologies. Automotive companies are overhauling business processes and relationships. Legacy IT systems for manufacturing, engineering, supply chain etc. are being reinvented. This transformation encompasses software, robotics, connected devices, and artificial intelligence. Artificial intelligence (AI) made the dream of self-driving cars possible. AI will soon transform every device. Tesla, Google Waymo, and Nvidia are examples of machine learning algorithms used to detect how far different objects are, from the car. Augmented reality (AR) and virtual reality (VR) analysis enables users to watch blind spots. AI enhances security by simultaneous coordination with many sensors. With AR, VR and mixed reality (MR), automotive companies have a personalized retail platform and a competitive edge. This paper studies AI applications in the automotive sector. It studies the recent developments, and applications of AI. It discusses how companies use AI for cost reduction, market strategies, sales promotion, and even funding.",https://ieeexplore.ieee.org/document/9765294/,2022 International Conference on Decision Aid Sciences and Applications (DASA),23-25 March 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/Confluence51648.2021.9377160,Application of Artificial Intelligence in Human Resource Management Practices,IEEE,Conferences,"In the dynamic and competitive world, technology has changed the pace of all the industry. Artificial intelligence is a technology which enables the industry to grow at faster pace and efficiently finishing their work. This technology has entered into various departments such as finance department, human resource department, marketing, production etc. AI system has enabled the organization to enhance their existing performance and efficiently performing functions on a day-to-day basis. Currently, due to dynamic and competitive environment people working at different managerial level are working under pressure and understanding the need of artificial intelligence at workplace. Authors have used quantitative research to conduct the research and regression methods has been used to analyse the data. AI as a technology has a role in the different HR practices starting from talent acquisition and extending it to the assessing the performance of the people at work place. This research will study the relation of artificial intelligence and HR functions and different functions performed by HR department. The objective is to understand the factor like innovativeness and how use of HR operations. To conduct the study HR professionals from different IT companies were considered. Through the analysis the result indicated the positive linkage between different factors such ease of use and innovativeness which clearly indicates that AI has a influence on both the factors. This research paper will provide indepth knowledge about artificial intelligence which is at present a new revolution in industry and referred as industry 4.0.",https://ieeexplore.ieee.org/document/9377160/,"2021 11th International Conference on Cloud Computing, Data Science & Engineering (Confluence)",28-29 Jan. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/Inforino53888.2022.9783011,Application of Gradient Boosting Algorithm for Predicting Equipment Failures,IEEE,Conferences,"This article discusses the process of predicting failures of deep-pumping equipment in the oil industry. A procedure for predicting failures of oil well equipment is proposed, which is based on the application of a gradient boosting algorithm and includes three stages. At the first stage, a set of features necessary for the analysis is formed, as well as a sample of data. At the second stage of data preprocessing, the following are performed: processing of missing values; coding of categorical features; getting rid of outliers by the interquartile range method. At the third stage, the gradient boosting algorithm is applied to predict failures of deeppumping equipment. The peculiarity of the algorithm implementation consists in the proposed scheme of actions, which means sequential fixing first the number of decision trees, and then the learning rate, involving the analysis of the dependencies of the root-mean-square error on both the learning rate and the number of trees. The result of predicting failures of deep-pumping equipment of an oil well is presented and the results of experimental studies using the developed models on real data are described. The predicted value of the number of failures of the deep-pumping equipment of the oil well for the subsidiary of the company Rosneft Nyaganneftegaz was calculated. The proposed procedure for solving the problem of predicting equipment failures based on the application of the gradient boosting algorithm is advisable to use in the process of obtaining an engineering education both when studying special disciplines and when performing final qualifying works.",https://ieeexplore.ieee.org/document/9783011/,2022 VI International Conference on Information Technologies in Engineering Education (Inforino),12-15 April 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSME.2019.00057,Application of Philosophical Principles in Linux Kernel Customization,IEEE,Conferences,"Philosophical principles are very useful in customization of Linux kernel, e.g., the answer for the question: ""For the pointer to the start address of page table, is it a physical address or a virtual address?"" can be derived by one simple philosophical principle: the depth of recursion is limited. This is because if the pointer were a virtual address, there would be another new page table to store the translation information of this virtual address, but who was responsible for storing the translation information of the start address of this new page table? This would result an infinite recursion. So the pointer definitely is a physical address. In fact, the usefulness of philosophical principles comes from the reduction of searching space. And this reduction is very important in customization of Linux kernel, for it could cut down the size of the new code needed to be read. This is especially valuable when considering that Linux kernel is continuously updating and huge now. Another example to further demonstrate the reduction of searching space in customization is showed in the following: in customization of file system in kernel version 3.10, the question: ""Does the Linux kernel itself maintain the consistency between the buffer cache and the page cache?"". This is a hard problem in practice, for without any guidance of philosophical principle, a developer has to read all of the code in Linux kernel to get a precise answer. The tricky part of this question is that if the developer only read a part of the codes and doesn't find any mechanisms for maintenance of cache consistency, the conclusion of non-existence of such mechanisms still can not be drawn, for there's still a possibility that such mechanisms exist in the codes not explored. Besides, if the developer search internet to find the answer, assume that the developer is lucky enough, he/she finally finds one program example on a web page shows that the inconsistency may raise between buffer cache and page cache. He/she still can not get the conclusion that Linux kernel does not maintain such consistency, because that program example maybe is only valid in a specific scenario, e.g. in kernel version 2.26, not 3.10. But we can get a satisfied answer by using the philosophical principle: the cost of management process should be far less than the value created by being managed process. By this principle, it can be drawn that Linux kernel doesn't maintain the consistency between the buffer cache and page cache in kernel 3.10. This is because that the data in buffer cache and page cache is highly dependent on application logic, so if Linux kernel wanted to maintain such consistency, it would have to track all these applications, which cost was much higher than the benefits that these applications could produce. However, the successful application of philosophical principles depends on two factors: firstly, establishment of a mapping between concepts in Linux system and well-known concepts in human society. This is not a new idea, e.g. the word of ""cost"" is a concept first appeared in human society, not in computer science, but nowadays, developers establish a mapping between this concept and concepts in computer science. Although the idea is very old, it is still very effective. Since well-known concepts in human society are familiar to most developers and are what they have in common, the cost of applying philosophical principles is reduced. Besides this, already existing cause-effect relations among concepts in human society can be highly possible to be reused in philosophical deduction in Linux kernel. E.g, in the mapping we established, process is treated as a human and since in religion of human society, God creates humankind, it is natural to derive that there's one process that creates all other processes in Linux system with high probability. Secondly, a concrete model with many qualitative and quantitative details should be the basis of philosophical deduction. We build such model according to our past experiences and the construction of the model follows the philosophical principle: unfold the complexity only when it is necessary. E.g., in this model, for a specific detail, it is covered only when it is required in practice. This is to lower down the cost of modelling huge and continuously evolving Linux kernel. This model is very important, without it, philosophical deduction is impossible. But it is really a hard work, according to our experiences, it needs at least 6-years of work on Linux kernel for one developer to build it. Although philosophical principles are very useful in practice, there's a big gap on the recognition of philosophical principles between academic researchers and industry practioners. E.g., some academic researcher seriously doubts whether the mapping above, which mentioned God, is helpful. In fact, it is, for by this mapping, a developer will know that the existence of the process, which is the origin of all other processes, is highly possible and also that process maybe is not easily observed. This is true, for that process is the process which PID is zero and that process can not be observed by Linux command: ""ps -e"". That process is a very valuable point of customization, e.g., by modifying that process, all processes in the Linux will be affected. Why does this big gap exist? We believe there're at least three reasons: i. The bias on philosophical principles. This usually comes from the observation that some developers establish wrong mapping between the philosophical principles and the objects in real world. But is that true for those that has been verified many times in practice? ii. Wrong expectations. E.g., hope to get the precise answer when applying philosophical principles, instead of reducing the searching space. iii. Some academic researchers do not realize that a good philosophical principle usually is the result of a deep learning process of many years by human brain. Finally, we suggest that more efforts should be put on the studying of philosophical principles in program understanding and we believe that in the near future, the philosophical principles plus AI will be a trend in program understanding.",https://ieeexplore.ieee.org/document/8919057/,2019 IEEE International Conference on Software Maintenance and Evolution (ICSME),29 Sept.-4 Oct. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CMI.2016.7413757,Application of close loop expert system for heating control of rolling mill furnaces in a steel plant,IEEE,Conferences,"One of the critical impediment faced in hierarchical control in an process industry is unavailability of exact mathematical co-relations, which can precisely define the process behavior. These are primarily due to variable, complex and un-measurable factors and noises influencing the process behavior. However, such cases are most appropriate application areas of Expert Systems. In process industry, Expert Systems are one of the successful application areas of Artificial Intelligence, where expertise and knowledge of a Process Expert or a group of Experts are embedded as computer inference software and database. In a real time situation, these systems can take intelligent decisions as would have been taken by process Expert on a similar situation. Determination of exact Set Process Temperatures or thermal regime on different parts of Rolling Mill Furnaces like Annealing Furnace in a Steel Industry is an intriguing problem. However, this decision is very crucial as final mechanical and metallurgical quality of steel stock significantly depends on fixing and accurate control of these temperatures. But as a irony, no well defined mathematical co-relations are available, which can predict exact thermal regime to be followed to achieve desired quality and properties of steel coils/sheets under heating inside such furnaces. The aforesaid intriguing issue has been successfully resolved by development and implementation of Expert System guided heating control system through prediction and control of optimum furnace temperatures inside Annealing Furnaces at Cold Rolling Mill of Bokaro Steel Plant and Decarburization-Annealing Furnace at Silicon Steel Mill of Rourkela Steel Plant. In both the cases, concepts of hierarchical automation has been used, wherein Expert System comprising Level-II tier of automation predicts most appropriate thermal regime to obtain desired product quality for a given set of steel sheet. A seamlessly dovetailed PLC constitutes Level-I automation layer. PLC monitors and controls the plant as per advice from Expert System. Both the systems have enhanced plants efficiency by improving production, quality and energy conservation.",https://ieeexplore.ieee.org/document/7413757/,"2016 IEEE First International Conference on Control, Measurement and Instrumentation (CMI)",8-10 Jan. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2001.939567,Application of neural networks in online oil content monitors,IEEE,Conferences,"Four major types of oils were examined to obtain both fluorescence and light scattering spectra as a function of oil concentrations. The large variations in fluorescence and scattering intensity with oil types and sub-types make it difficult to calibrate the analytical instrument using traditional methods. We implemented a multivariate, nonlinear calibration of instrumental response through an artificial neural network. We demonstrated that the combined use of fluorescence and scattering data significantly improves the quantitative prediction accuracy. The trained backpropagation neural network was used successfully to predict the concentrations of single oils and their mixtures, and appears well suited for the calibration of an online oil content monitor. The newly developed technique permits the online monitoring of oil concentrations in wastewater discharged from ships and oil refinery industry.",https://ieeexplore.ieee.org/document/939567/,IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222),15-19 July 2001,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAIBDA53561.2021.00014,Applications of Smart Energy Integrated Service Platform in Optimization of Energy Utilization of Customers,IEEE,Conferences,"In the new era, comprehensive energy service has gradually become an important development direction to promote the high-quality development of energy industry and boost the development of real economy. The construction of smart integrated energy service platform is the internal demand for energy service enterprises to transform into integrated energy service providers, and is a solution to provide comprehensive energy support services for power users. The platform is divided into perception layer, acquisition layer, network layer, platform layer, application layer and display layer from bottom to top. Its main functions include energy regulation, energy efficiency analysis, smart operation, energy trading and smart dispatching. The construction of smart energy comprehensive service platform can provide customers with more high-quality, more convenient and more comprehensive energy value-added services, and constantly create more value for customers. Practice has proved that the implementation of smart energy integrated service platform can greatly reduce the monthly energy cost, monthly power load of the park and monthly coal consumption in park and enhance the indexes of monthly active users and customer satisfaction.",https://ieeexplore.ieee.org/document/9545981/,"2021 International Conference on Artificial Intelligence, Big Data and Algorithms (CAIBDA)",28-30 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMICND.2003.1252463,Applications of bio-inspired systems as function approximators,IEEE,Conferences,"In this paper we would like to show how the bio-inspired systems topologies can be applied as function approximators (nonlinear regressors) in a wealth of practical applications. We selected one application in the financial industry and another in real estate. The goal is to discover nonlinear mappings between input variables and important outcomes. In the financial arena the outcome is to predict the value of the S&P 500 (one of the most important Standard&Poor's indexes), using several financial indicators, while in the real estate application the problem is to estimate the price of a house based on several indicators. We will see that bio-inspired systems provide a very powerful analysis tool. The software tool used was NeuroSolutions, from NeuroDimension Inc., a leading edge neural network development software.",https://ieeexplore.ieee.org/document/1252463/,2003 International Semiconductor Conference. CAS 2003 Proceedings (IEEE Cat. No.03TH8676),28 Sept.-2 Oct. 2003,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV.2003.1217948,Applied visual user interface technique in knowledge management,IEEE,Conferences,"Extracting actionable insight from large high-dimensional data sets, and its use for more effective decision-making, has become a pervasive problem across many application fields in both research and industry. The objective of our presentation is to report on some investigations of this problem covering both these areas. Taking as the problem domain the area of ""unsupervised learning"", we show that by tightly coupling statistical analysis technique with combinations of visualization components and techniques for interactivity, real-time analysis of multidimensional data can be efficiently made. We give particular attention to the ways in which dynamic visual representations can be used in these contexts to facilitate shared understanding. Our system is implemented and validated in the context of 3D medical imaging knowledge construction, knowledge management and geovisualisation.",https://ieeexplore.ieee.org/document/1217948/,"Proceedings on Seventh International Conference on Information Visualization, 2003. IV 2003.",18-18 July 2003,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NetSoft48620.2020.9165317,Applying Machine Learning to End-to-end Slice SLA Decomposition,IEEE,Conferences,"5G is set to revolutionize the network service industry with unprecedented use-cases in industrial automation, augmented reality, virtual reality and many other domains. Network slicing is a key enabler to realize this concept, and comes with various SLA requirements in terms of latency, throughput, and reliability. Network slicing is typically performed in an end-to-end (e2e) manner across multiple domains, for example, in mobile networks, a slice can span access, transport and core networks. Thus, if an SLA requirement is specified for e2e services, we need to ensure that the total SLA budget is appropriately proportioned to each participating domain in an adaptive manner. Such an SLA decomposition can be extremely useful for network service operators as they can plan accordingly for actual deployment. In this paper we design and implement an SLA decomposition planner for network slicing using supervised machine learning algorithms. Traditional optimization based approaches cannot deal with the dynamic nature of such services. We design machine learning models for SLA decomposition, based on random forest, gradient boosting and neural network. We then evaluate each class of algorithms in terms of accuracy, sample complexity, and model explainability. Our experiments reveal that, in terms of these three requirements, the gradient boosting and neural network algorithms for SLA decomposition out-perform random forest algorithms, given emulated data sets.",https://ieeexplore.ieee.org/document/9165317/,2020 6th IEEE Conference on Network Softwarization (NetSoft),29 June-3 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCC53858.2021.00012,Applying the Experience of Artificial Intelligence Methods for Information Systems Cyber Protection at Industrial Control Systems,IEEE,Conferences,"The rapid development of the Industry 4.0 initiative highlights the problems of Cyber-security of Industrial Computer Systems and, following global trends in Cyber Defense, the implementation of Artificial Intelligence instruments. The authors, having certain achievement in the implementation of Artificial Intelligence tools in Cyber Protection of Information Systems and, more precisely, creating and successfully experimenting with a hybrid model of Intrusion Detection and Prevention System (IDPS), decided to study and experiment with the possibility of applying a similar model to Industrial Control Systems. This raises the question: can the experience of applying Artificial Intelligence methods in Information Systems, where this development went beyond the experimental phase and has entered into the real implementation phase, be useful for experimenting with these methods in Industrial Systems.",https://ieeexplore.ieee.org/document/9668825/,"2021 25th International Conference on Circuits, Systems, Communications and Computers (CSCC)",19-22 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/CISTI49556.2020.9141124,Artificial Intelligence Applied to Software Testing: A Literature Review,IEEE,Conferences,"In the last few years Artificial Intelligence (AI) algorithms and Machine Learning (ML) approaches have been successfully applied in real-world scenarios like commerce, industry and digital services, but they are not a widespread reality in Software Testing. Due to the complexity of software testing, most of the work of AI/ML applied to it is still academic. This paper briefly presents the state of the art in the field of software testing, applying ML approaches and AI algorithms. The progress analysis of the AI and ML methods used for this purpose during the last three years is based on the Scopus Elsevier, web of Science and Google Scholar databases. Algorithms used in software testing have been grouped by test types. The paper also tries to create relations between the main AI approaches and which type of tests they are applied to, in particular white-box, grey-box and black-box software testing types. We conclude that black-box testing is, by far, the preferred method of software testing, when AI is applied, and all three methods of ML (supervised, unsupervised and reinforcement) are commonly used in black-box testing being the “clustering” technique, Artificial Neural Networks and Genetic Algorithms applied to “fuzzing” and regression testing.",https://ieeexplore.ieee.org/document/9141124/,2020 15th Iberian Conference on Information Systems and Technologies (CISTI),24-27 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICE/ITMC52061.2021.9570215,Artificial Intelligence as Enabler for Sustainable Development,IEEE,Conferences,"When the UN published the 17 Sustainable Development Goals (SDGs) in 2015, emerging technologies like Artificial Intelligence (AI) were not yet mature. However, through its deployment across industry sectors and verticals, issues related to sustainability, fairness, inclusiveness, efficiency, and usability of these technologies are now priorities for global consumers and producers. This paper discusses what needs to be considered by both policy makers and ‘managers’ in order to exploit the use of AI for SDG achievement. AI can act as a real and meaningful enabler to achieve sustainability goals; however, it may also have negative impacts. Therefore, a carefully balanced approach is required to ensure that Artificial Intelligence systems are employed to help solve sustainability issues without inadvertently affecting other goals.",https://ieeexplore.ieee.org/document/9570215/,"2021 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",21-23 June 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAACCA51523.2021.9465297,Assessing Machine learning-based approaches for Silica concentration estimation in Iron Froth flotation,IEEE,Conferences,"In the mining industry, specifically in the flotation process, there is a challenge associated to noninvasive, real-time contaminant and impurities estimation. Achieving predictions on contaminant levels has a high impact on quality insurance and it can help technicians and engineers to make adjustments in advance to improve the quality of the final product, and thus profits. In this paper, exploratory research is performed on the problem of silica concentrate estimation for iron ore froth flotation using machine learning techniques, with the goal to identify algorithms that may be suitable for industry soft sensor development. For this purpose, a public, real process dataset is used and three different machine learning techniques are implemented: Random Forest (RF), Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). The techniques were implemented, tested and compared in terms of their error percentage, mean absolute error, mean square error, and root mean square error. Obtained results show acceptable performance for LTSM and GRU implementations, being LSTM network the out-performer with errors below 9%.",https://ieeexplore.ieee.org/document/9465297/,2021 IEEE International Conference on Automation/XXIV Congress of the Chilean Association of Automatic Control (ICA-ACCA),22-26 March 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSPCC.2018.8567797,Attack Detection for Wireless Enterprise Network: a Machine Learning Approach,IEEE,Conferences,"An increasing number of enterprises are adopting wireless technology to deploy networks. However, wireless enterprise networks are more vulnerable than wired networks because of the broadcast feature. Thus, illegal attacks such as data theft and information forgery seriously threaten the property and information security of users and enterprises; these phenomena are attracting increasing attention from both academia and industry. Additionally, effectively detecting the attacks in the wireless enterprise networks is one of todays most important and challenging problems, especially in Wi-Fi networks, as attacks become increasingly covert and diverse. Fortunately, WiFi networks produce large amounts of data, providing copious big data for researchers. In this paper, using the Aegean Wi-Fi Intrusion Dataset (AWID), which is derived from the real-world Wi-Fi network, we introduce machine learning to detect network attacks. To significantly increase the training and convergence speeds, we deploy two-dimensional data cleaning and select 18 useful attributes from the original set of 154. Then, we introduce support vector machine (SVM) to detect attacks based on the cleaned dataset. The detection accuracy for flooding attacks, injection attacks, and normal data reached 89.18%, 87.34%, and 99.88% respectively. To the best of our knowledge, this is the first study to introduce a two-dimensional data cleaning method with an SVM to improve the detection accuracy for attacks. Finally, our detection results are comparable with the existing studies; however, our method operates with simpler data attributes with faster and more efficient training speed.",https://ieeexplore.ieee.org/document/8567797/,"2018 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)",14-16 Sept. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2017.7915547,Augmented reality based on edge computing using the example of remote live support,IEEE,Conferences,"Augmented Reality (AR) introduces vast opportunities to the industry in terms of time and therefore cost reduction when utilized in various tasks. The biggest obstacle for a comprehensive deployment of mobile AR is that current devices still leave much to be desired concerning computational and graphical performance. To improve this situation in this paper we introduce an AR Edge Computing architecture with the aim to offload the demanding AR algorithms over the local network to a high-end PC considering the real-time requirements of AR. As an example use case we implemented an AR Remote Live Support application. Applications like this on the one hand are strongly demanded in the industry at present, on the other hand by now mostly do not implement a satisfying tracking algorithm lacking computational resources. In our work we lay the focus on both, the possibilities our architecture offers regarding improvements of tracking and the challenges it implies in respect of real-time. We found that offloading AR algorithms in real-time is possible with available WiFi making use of standard compression techniques like JPEG. However it can be improved by future radio solutions offering higher bandwidth to avoid additional latency contributed by the coding.",https://ieeexplore.ieee.org/document/7915547/,2017 IEEE International Conference on Industrial Technology (ICIT),22-25 March 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2016.7727278,Augmenting adaptation with retrospective model correction for non-stationary regression problems,IEEE,Conferences,"Existing adaptive predictive methods often use multiple adaptive mechanisms as part of their coping strategy in non-stationary environments. We address a scenario when selective deployment of these adaptive mechanisms is possible. In this case, deploying each adaptive mechanism results in different candidate models, and only one of these candidates is chosen to make predictions on the subsequent data. After observing the error of each of candidate, it is possible to revert the current model to the one which had the least error. We call this strategy retrospective model correction. In this work we aim to investigate the benefits of such approach. As a vehicle for the investigation we use an adaptive ensemble method for regression in batch learning mode which employs several adaptive mechanisms to react to changes in the data. Using real world data from the process industry we show empirically that the retrospective model correction is indeed beneficial for the predictive accuracy, especially for the weaker adaptive mechanisms.",https://ieeexplore.ieee.org/document/7727278/,2016 International Joint Conference on Neural Networks (IJCNN),24-29 July 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WSAI49636.2020.9143279,Automated Analysis of Seizure Behavior in Video: Methods and Challenges,IEEE,Conferences,"Automated analysis of seizure behavior in video using intelligent video analytics technology has significant applications in healthcare industry, since it can provide accurate and quantitative measurement of human seizure behavior for assisting diagnosis. This paper presents a brief survey on intelligent video analytics for automated seizure behavior analysis, including both conventional motion analysis based approaches and the state-of-the-art machine learning based approaches. Furthermore, a new automated video analytics framework is proposed in this paper, by exploiting the machine learning approach to build a seizure motion model and performing automatic detection of seizure events in the surveillance video in real time. This paper also discusses the preliminary experimental results and deployment of the proposed framework, as well as the future research challenges in this area.",https://ieeexplore.ieee.org/document/9143279/,2020 2nd World Symposium on Artificial Intelligence (WSAI),27-29 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NOMS.2018.8406234,Automated diagnostic of virtualized service performance degradation,IEEE,Conferences,"Service assurance for cloud applications is a challenging task and is an active area of research for academia and industry. One promising approach is to utilize machine learning for service quality prediction and fault detection so that suitable mitigation actions can be executed. In our previous work, we have shown how to predict service-level metrics in real-time just from operational data gathered at the server side. This gives the service provider early indications on whether the platform can support the current load demand. This paper provides the logical next step where we extend our work by proposing an automated detection and diagnostic capability for the performance faults manifesting themselves in cloud and datacenter environments. This is a crucial task to maintain the smooth operation of running services and minimizing downtime. We demonstrate the effectiveness of our approach which exploits the interpretative capabilities of Self- Organizing Maps (SOMs) to automatically detect and localize different performance faults for cloud services.",https://ieeexplore.ieee.org/document/8406234/,NOMS 2018 - 2018 IEEE/IFIP Network Operations and Management Symposium,23-27 April 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASEW52652.2021.00024,Automated game testing using computer vision methods,IEEE,Conferences,"Video game development is a growing industry nowadays with high revenues. However, even if there are many resources invested in the software development process, many games still contain bugs or performance issues that affect the user experience. This paper presents ideas on how computer vision methods can be used to automate the process of game testing. The goal is to replace the parts of the testing process that require human users (testers) with machines as much as possible, in order to reduce costs and perform more tests in less time by scaling with hardware resources. The focus is on solving existing real-world problems that have emerged from several discussions with industry partners. We base our methods on previous work in this area using intelligent agents playing video games and deep learning methods that interpret feedback from their actions based on visual output. The paper proposes several methods and a set of open-source tools, independent of the operating system or deployment platform, to evaluate the efficiency of the presented methods.",https://ieeexplore.ieee.org/document/9680292/,2021 36th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),15-19 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CICommS.2013.6582853,Automated network application classification: A competitive learning approach,IEEE,Conferences,"The design of a sustainable application level classification system has, over the past few years, been the subject of much research by academics and industry alike. The methodologies proposed rely predominantly on predefined signatures for each protocol, applied to each passing flow in order to classify them. These signatures are often static, resulting in inaccuracies during the classification process. This problem is compounded by delays in signature update releases. This paper presents an approach toward automated signature generation, mitigating classification problems experienced with existing systems. A hierarchical system is proposed, where signatures are developed and deployed in real-time. The ideas set forth in this research are evaluated by experimentation in a live network environment. Discriminators of both encrypted and plain-text application protocol samples were recorded and automatically annotated by a Hierarchical Self-Organizing Map (HSOM). The clusters identified by the HSOM were used in a supervised training process that correctly identified protocols with an almost perfect (99% percent) success rate.",https://ieeexplore.ieee.org/document/6582853/,2013 IEEE Symposium on Computational Intelligence for Communication Systems and Networks (CIComms),16-19 April 2013,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2010.5596494,Automated texture classification of marble shades with real-time PLC neural network implementation,IEEE,Conferences,"The subjective evaluation of marbles based on their visual appearance could be replaced by an automated texture classification system, intending to achieve high classification accuracy and production effectiveness. The existing marble classification methods from a computational point of view are either too complex or very expensive. Nowadays some inspection systems in marble industry that automates the quality-control tasks and shade classification are too expensive and are compatible only with specific technological equipment. In this paper a new approach for classification of marble tiles with similar shades is proposed. It is based on simple image preprocessing, on training a MLP neural network (MLP NN) with marble histograms and implementation of the algorithm in a Programmable Logic Controller (PLC) for real-time execution. A method for training the MLP NN aiming optimization of MLP parameters and topology is proposed. The designed automated system uses only standard PLC modules and communication interfaces. The experimental test results when recognizing marble textures with added motion blur are represented and discussed. The performance of the modeling technique is assessed with different training and test sets. The classification accuracy results are compared to other results obtained by similar approaches.",https://ieeexplore.ieee.org/document/5596494/,The 2010 International Joint Conference on Neural Networks (IJCNN),18-23 July 2010,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VTC2021-Spring51267.2021.9448686,Automatic Generation of Critical Test Cases for the Development of Highly Automated Driving Functions,IEEE,Conferences,"The development of highly automated driving functions is currently one of the key drivers for the automotive industry and research. In addition to the technical constraints in the implementation of these functions, a major challenge is the verification of functional safety. Conventional approaches aiming at statistical validation in the sense of real test drives are reaching their economic limits. On the other hand, there are simulation methods that allow a lot of freedom in test case design, but whose representativeness and relevance must be proven separately. In this paper an approach is presented that allows to generate critical concrete scenarios and test cases for automated driving functions by means of a reinforcement learning based optimization using here the example of an overtaking assistant. For this purpose, a Q-Learning approach is used that automates the parameter generation for the test cases. While pure combinatorics of the variable parameters leads to an unmanageable amount of test cases, the percentage of actually relevant critical test cases is very low. In this work we show how the share of critical and thus relevant test cases can be increased significantly by using the presented method compared to a purely combinatorial parameter variation.",https://ieeexplore.ieee.org/document/9448686/,2021 IEEE 93rd Vehicular Technology Conference (VTC2021-Spring),25-28 April 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACPEE53904.2022.9783875,Automatic Inspection of Power System Operations Based on Lightweight Neural Network,IEEE,Conferences,"In the automatic inspection task of the power system industry, it is of great significance to employ artificial intelligence algorithms to automatically detect the illegal operation action of workers. Although many existing object detection algorithms can achieve good detection rate, due to the large scale and large amounts of parameters of the model, it can hardly be directly used in edge devices for real-time detection. Based on two frequency employed baseline lightweight models, this paper presents a novel deep learning algorithm for illegal operation action detection which uses the 5&#x00D7;5 deep detachable convolution kernel. The proposed lightweight network could not only extracts discriminative features by the two special convolutional layers, but also better detect small targets in the scene. The proposed network can be implemented on edge devices such as Huawei Atlas 200 DK. Experimental results on our collected dataset of the surveillance videos shot during the project of Ningxia Electric Power Co., LTD. Show that the proposed method could achieve comparable accuracy with only very few number of parameters, thus can be applied for real-time automatic inspection.",https://ieeexplore.ieee.org/document/9783875/,2022 7th Asia Conference on Power and Electrical Engineering (ACPEE),15-17 April 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSREW53611.2021.00113,Automatic Issue Classifier: A Transfer Learning Framework for Classifying Issue Reports,IEEE,Conferences,"Issue tracking systems are used in the software industry for the facilitation of maintenance activities that keep the software robust and up to date with ever-changing industry requirements. Usually, users report issues that can be categorized into different labels such as bug reports, enhancement requests, and questions related to the software. Most of the issue tracking systems make the labelling of these issue reports optional for the issue submitter, which leads to a large number of unlabeled issue reports. In this paper, we present a state-of-the-art method to classify the issue reports into their respective categories i.e. bug, enhancement, and question. This is a challenging task because of the common use of informal language in the issue reports. Existing studies use traditional natural language processing approaches adopting key-word based features, which fail to incorporate the contextual relationship between words and therefore result in a high rate of false positives and false negatives. Moreover, previous works utilize a uni-label approach to classify the issue reports however, in reality, an issue-submitter can tag one issue report with more than one label at a time. This paper presents our approach to classify the issue reports in a multi-label setting. We use an off-the-shelf neural network called RoBERTa and fine-tune it to classify the issue reports. We validate our approach on issue reports belonging to numerous industrial projects from GitHub. We were able to achieve promising F-1 scores of 81 &#x0025;, 74&#x0025;, and 80&#x0025; for bug reports, enhancements, and questions, respectively. We also develop an industry tool called Automatic Issue Classifier (AIC), which automatically assigns labels to newly reported issues on GitHub repositories with high accuracy.",https://ieeexplore.ieee.org/document/9700173/,2021 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),25-28 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VTCSpring.2019.8746507,Autonomous Driving without a Burden: View from Outside with Elevated LiDAR,IEEE,Conferences,"The current autonomous driving architecture places a heavy burden in signal processing for the graphics processing units (GPUs) in the car. This directly translates into battery drain and lower energy efficiency, crucial factors in electric vehicles. This is due to the high bit rate of the captured video and other sensing inputs, mainly due to Light Detection and Ranging (LiDAR) sensor at the top of the car which is an essential feature in autonomous vehicles. LiDAR is needed to obtain a high precision map for the vehicle AI to make relevant decisions. However, this is still a quite restricted view from the car. This is the same even in the case of cars without a LiDAR such as Tesla. The existing LiDARs and the cameras have limited horizontal and vertical fields of visions. In all cases it can be argued that precision is lower, given the smaller map generated. This also results in the accumulation of a large amount of data in the order of several TBs in a day, the storage of which becomes challenging. If we are to reduce the effort for the processing units inside the car, we need to uplink the data to edge or an appropriately placed cloud. However, the required data rates in the order of several Gbps are difficult to be met even with the advent of 5G. Therefore, we propose to have a coordinated set of LiDAR's outside at an elevation which can provide an integrated view with a much larger field of vision (FoV) to a centralized decision making body which then sends the required control actions to the vehicles with a lower bit rate in the downlink and with the required latency. The calculations we have based on industry standard equipment from several manufacturers show that this is not just a concept but a feasible system which can be implemented.The proposed system can play a supportive role with existing autonomous vehicle architecture and it is easily applicable in an urban area.",https://ieeexplore.ieee.org/document/8746507/,2019 IEEE 89th Vehicular Technology Conference (VTC2019-Spring),28 April-1 May 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EIT.2018.8500102,Behavioral Cloning for Lateral Motion Control of Autonomous Vehicles Using Deep Learning,IEEE,Conferences,"Current trend of the automotive industry combined with research by the major tech companies has proved that self-driving vehicles are the future. With successful demonstration of neural network based autonomous driving, NVIDIA has introduced a new paradigm for autonomous driving software. The biggest challenge for self-driving cars is autonomous lateral control. An end-to-end model seems very promising in providing a complete software stack for autonomous driving. Although this system is not ready to be provided as a feature in the market today, it is one of the many steps in the right direction to make self-driving cars a reality. The work described in this paper focusses on how an end-to-end model is implemented. The subtleties of training a successful end-to-end model are highlighted with the aim of providing an insight on deep learning and software required for neural network training. Detailed analyses of data acquisition and training systems are provided and installation procedures for all required tools and software discussed. TORCS is used for developing and testing the end-to-end model. Approximately ten hours of driving data was collected from two different tracks. Using four hours of data from a track, we trained a deep neural network to steer a car inside simulation. Even with such a small training set, the end-to-end model developed demonstrated capabilities to maintain lanes and complete laps in different tracks. For a multilane track, like the one used for training, the model demonstrated an autonomy of 96.62%. For single lane unknown tracks, the model steered the vehicle successfully for 89.02% of the time. The results indicate that end-to-end learning and behavioral cloning can be used to drive autonomously in new and unknown scenarios.",https://ieeexplore.ieee.org/document/8500102/,2018 IEEE International Conference on Electro/Information Technology (EIT),3-5 May 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NetSoft48620.2020.9165393,Benchmarking and Profiling 5G Verticals' Applications: An Industrial IoT Use Case,IEEE,Conferences,"The Industry 4.0 sector is evolving in a tremendous pace by introducing a set of industrial automation mechanisms tightly coupled with the exploitation of Internet of Things (IoT), 5G and Artificial Intelligence (AI) technologies. By combining such emerging technologies, interconnected sensors, instruments, and other industrial devices are networked together with industrial applications, formulating the Industrial IoT (IIoT) and aiming to improve the efficiency and reliability of the deployed applications and provide Quality of Service (QoS) guarantees. However, in a 5G era, efficient, reliable and highly performant applications' provision has to be combined with exploitation of capabilities offered by 5G networks. Optimal usage of the available resources has to be realised, while guaranteeing strict QoS requirements such as high data rates, ultra-low latency and jitter. The first step towards this direction is based on the accurate profiling of vertical industries' applications in terms of resources usage, capacity limits and reliability characteristics. To achieve so, in this paper we provide an integrated methodology and approach for benchmarking and profiling 5G vertical industries' applications. This approach covers the realisation of benchmarking experiments and the extraction of insights based on the analysis of the collected data. Such insights are considered the cornerstones for the development of AI models that can lead to optimal infrastructure usage along with assurance of high QoS provision. The detailed approach is applied in a real IIoT use case, leading to profiling of a set of 5G network functions.",https://ieeexplore.ieee.org/document/9165393/,2020 6th IEEE Conference on Network Softwarization (NetSoft),29 June-3 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIIC51459.2021.9415189,Big Data Platform for Intelligence Industrial IoT Sensor Monitoring System Based on Edge Computing and AI,IEEE,Conferences,"The cutting edge of Industry 4.0 has driven everything to be converted to disruptive innovation and digitalized. This digital revolution is imprinted by modern and advanced technology that takes advantage of Big Data and Artificial Intelligence (AI) to nurture from automatic learning systems, smart city, smart energy, smart factory to the edge computing technology, and so on. To harness an appealing, noteworthy, and leading development in smart manufacturing industry, the modern industrial sciences and technologies such as Big Data, Artificial Intelligence, Internet of things, and Edge Computing have to be integrated cooperatively. Accordingly, a suggestion on the integration is presented in this paper. This proposed paper describes the design and implementation of big data platform for intelligence industrial internet of things sensor monitoring system and conveys a prediction of any upcoming errors beforehand. The architecture design is based on edge computing and artificial intelligence. To extend more precisely, industrial internet of things sensor here is about the condition monitoring sensor data - vibration, temperature, related humidity, and barometric pressure inside facility manufacturing factory.",https://ieeexplore.ieee.org/document/9415189/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCEM.2018.00025,Bodhisattva - Rapid Deployment of AI on Containers,IEEE,Conferences,"Cloud-based machine learning is becoming increasingly important in all verticals of the industry as all organizations want to leverage ML and AI to solve real-world problems of emerging markets. But, incorporating these services into business solutions is a goliath task, mainly due to the sheer effort necessary to go from development to deployment. We present a novel idea that enables users to easily specify, create, train and rapidly deploy machine learning models through a scalable Machine-Learning-as-a-Service (MLaaS) offering. The MLaaS is provided as an end-to-end microservice suite in a container-based PaaS environment for web applications on the cloud. Our implementation provides an intuitive web-based GUI for tenants to consume these services in a few quick steps. The utility of our service is demonstrated by training ML models for various use cases and comparing them on factors like time-to-deploy, resource usage and training metrics.",https://ieeexplore.ieee.org/document/8648632/,2018 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM),23-24 Nov. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SBESC49506.2019.9046091,Brazilian Mercosur License Plate Detection: a Deep Learning Approach Relying on Synthetic Imagery,IEEE,Conferences,"Automated license plate recognition (ALPR) technology is a powerful technology enabling more efficient and effective law enforcement, security, payment collection, and research. A common license plate standard was adopted by the member states of the Mercosur trading bloc (Argentina, Brazil, Paraguay and Uruguay) and consequently requires an upgrade to the ALPR software used by law enforcement and industry. Due to the scarcity of real license plate images, training state-of-the-art supervised detectors is unfeasible unless data augmentation techniques and synthetic training data are used. This paper presents an accurate and efficient automated Mercosur license plate detector using a Convolutional Neural Network (CNN) trained exclusively with synthetic imagery. In order to obtain the synthetic training data, Mercosur license plates were faithfully reproduced. Digital image processing techniques were employed to reduce the domain gap and a CNN with basic image manipulation was used to embed the artificial licensed plates in to realistic contexts. The trained model was then validated on real images captured from a parking lot and a publicly available traffic monitoring video stream. The results of experiments suggest detection accuracy of about 95% and an average running time of 40 milliseconds.",https://ieeexplore.ieee.org/document/9046091/,2019 IX Brazilian Symposium on Computing Systems Engineering (SBESC),19-22 Nov. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2016.7840859,Building a research data science platform from industrial machines,IEEE,Conferences,"Data Science research has a long history in academia which spans from large-scale data management, to data mining and data analysis using technologies from database management systems (DBMS's). While traditional HPC offers tools on leveraging existing technologies with data processing needs, the large volume of data and the speed of data generation pose significant challenges. Using the Hadoop platform and tools built on top of it drew immense interest from academia after it gained success in industry. Georgia Institute of Technology received a donation of 200 compute nodes from Yahoo. Turning these industrial machines into a research Data Science Platform (DSP) poses unique challenges, such as: nontrivial hardware design decisions, configuration tool choices, node integration into existing HPC infrastructure, partitioning resource to meet different application needs, software stack choices, etc. We have 40 nodes up and running, 24 running as a Hadoop and Spark cluster, 12 running as a HBase and OpenTSDB cluster, the others running as service nodes. We successfully tested it against Spark Machine Learning algorithms using a 88GB image dataset, Spark DataFrame and GraphFrame with a Wikipedia dataset, and Hadoop MapReduce wordcount on a 300GB dataset. The OpenTSDB cluster is for real-time time series data ingestion and storage for sensor data. We are working on bringing up more nodes. We share our first-hand experience gained in our journey, which we believe will benefit and inspire other academic institutions.",https://ieeexplore.ieee.org/document/7840859/,2016 IEEE International Conference on Big Data (Big Data),5-8 Dec. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TEMSCON-EUR52034.2021.9488593,Business Forum Panel: Perspectives and real-life application of AI in Croatia 20-May 2021 9:00-10:30 CET,IEEE,Conferences,"Organized by IEEE Croatia Section and Technology and Engineering Management Chapter Artificial Intelligence is one of the hottest topics at the moment. In recent years it has been attracting the equal attention of the academia, industry and policy makers. The topic itself is far from new, especially scientifically and engineering wise, but its true deployment raises many questions: technological, legal/ethical, economic. AI is without a doubt a technology for the future, but it is reasonable to ask what are its true perspective and application in a foreseeable future in Croatia and world wide. These and many other questions will be discussed on this round table with our distinguished guests.",https://ieeexplore.ieee.org/document/9488593/,2021 IEEE Technology & Engineering Management Conference - Europe (TEMSCON-EUR),17-20 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CBI.2013.34,Business and Information Systems Engineering -- In Quest for Research and Education Agenda in Europe,IEEE,Conferences,"The new complex digital and information services systems and industries are calling for new transdisciplinary approaches on how to achieve technical, social, and cultural knowledge and skills to serve future needs of the industry and society. The leading digital industry is in demand of engineer's with knowledge and capabilities to collaborate and move between highly complex digital business and technical systems domains. The new reality is under constant transformation, highly intangible and nonlinear interconnected system. Transformative digital business innovations, rapidly evolving business models, architectures enabled business model scalability and ultra large-scale of systems are the new characteristics of this software-dominant-logic. This paper aims at covering contemporary challenges of the interdisciplinary in business, service, software and systems engineering by analyzing different research ontologies and curriculum models. The paper analyses recent Computer Science (CS) and Information Systems (IS) curriculum developments and reflects through different ontologies and recent research. Paper uses Service Design and Engineering (SDE) as a didactic and curriculum model for future Information Systems Engineering (ISE) and Business and Information Systems Engineering (BISE). The new curriculums serve the needs of global information start-ups, businesses, governments, and societies. This paper aims at describing the ontological foundations and conflicts, the axiology of the new curriculum model and proposes an integrated multi-ontology as the foundation for BISE new curriculum.",https://ieeexplore.ieee.org/document/6642875/,2013 IEEE 15th Conference on Business Informatics,15-18 July 2013,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9413181,CARRADA Dataset: Camera and Automotive Radar with Range- Angle- Doppler Annotations,IEEE,Conferences,"High quality perception is essential for autonomous driving (AD) systems. To reach the accuracy and robustness thatare required by such systems, several types of sensors must be combined. Currently, mostly cameras and laser scanners (lidar) are deployed to build a representation of the world around the vehicle. While radar sensors have been used fora long time in the automotive industry, they are still under-used for AD despite their appealing characteristics (notably, their ability to measure the relative speed of obstacles and to operate even in adverse weather conditions). To alarge extent, this situation is due to the relative lack of automotive datasets with real radar signals that are both raw and annotated. In this work, we introduce CARRADA, a dataset of synchronized camera and radar recordings with range-angle-Doppler annotations. We also present a semi-automatic annotation approach, which was used to annotate the dataset, and a radar semantic segmentation baseline, which we evaluate on several metrics. Both our code and dataset are available online.",https://ieeexplore.ieee.org/document/9413181/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISVLSI49217.2020.00-12,CPSoSaware: Cross-Layer Cognitive Optimization Tools & Methods for the Lifecycle Support of Dependable CPSoS,IEEE,Conferences,"Cyber-physical Systems of Systems (CPSoS) are large complex systems where physical elements interact with and are controlled by a large number of distributed and networked computing elements as well as human users. Their increasingly stringent demands on efficient use of resources, high service and product quality levels and, of course low cost and competitiveness on the world market introduce big challenges related to the design operation continuum of dependable connected CPSs. The CPSoSaware project aims at developing the models and software tools to allocate computational power/resources to the CPS end devices and autonomously determining what cyber-physical processes will be handled by the devices' heterogeneous components (CPUs, GPUs, FPGA fabric, software stacks). The project relies on Artificial Intelligence (AI) support to strengthen reliability, fault tolerance and security at system level and also to lead to CPS designs that work in a decentralized way, collaboratively, in an equilibrium, by sharing tasks and data with minimal central intervention. The CPSoSaware system will interact with the human users/operators through extended reality visual and touchable interfaces increasing situational awareness. The CPSoSaware system will be evaluated: i) in the automotive sector, in mixed traffic environments with semi-autonomous connected vehicles and ii) in the manufacturing industry where inspection and repair scenarios are employed using collaborative robots.",https://ieeexplore.ieee.org/document/9155036/,2020 IEEE Computer Society Annual Symposium on VLSI (ISVLSI),6-8 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AEMCSE51986.2021.00098,CSF: convolution sequential fusion model for session recommender system,IEEE,Conferences,"Capturing users’ specific preferences is a fundamental problem in a large-scale recommender system. Currently, Sequence-based session recommendation has become a hot research direction in the industry. However, the current approach is somewhat inadequate in capturing user’s long-term preferences. We propose a session model based on convolution sequential fusion (CSF) to capture users' specific preferences by fusing users' long-term behaviors and short-term sessions. Compared with existing sequential recommendations, we address two inherent weaknesses of session recommendations: 1) The interaction of long-term interest features is not sufficiently extracted. 2) The combination of long-term interest and short-term interest is too simplistic. The long-term interests of users are varied and complex. simultaneously, users' long-term interests are closely related to their short-term interests, so they should be better integrated. We propose to encode the behavior sequence with two corresponding components: the convolutional network for interactive extraction of users' long-term interests and the long-short term gated fusion module for better combination of long-short preferences Our entire model has been test on multiple real-world data sets, and the results demonstrate that our model is more effective than other recent models on multiple evaluation benchmarks.",https://ieeexplore.ieee.org/document/9513159/,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",26-28 March 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IISWC53511.2021.00026,Cactus: Top-Down GPU-Compute Benchmarking using Real-Life Applications,IEEE,Conferences,"Benchmarking is the de facto standard for evaluating hardware architectures in academia and industry. While several benchmark suites targeting different application domains have been developed for CPU processors over many decades, benchmarking GPU architectures is not as mature. Since the introduction of GPUs for general-purpose computing, the purpose has been to accelerate (a) specific part(s) of the code, called (a) kernel(s). The initial GPU-compute benchmark suites, which are still widely used today, hence consist of relatively simple workloads that are composed of one or few kernels with specific unambiguous execution characteristics. In contrast, we find that modern-day real-life GPU-compute applications are much more complex consisting of many more kernels with differing characteristics. A fundamental question can hence be raised: are current benchmark suites still representative for modern real-life applications? In this paper, we introduce Cactus, a collection of widely used real-life open-source GPU-compute applications. The aim of this work is to offer a new perspective on GPU-compute benchmarking: while existing benchmark suites are designed in a bottom-up fashion (i.e., starting from kernels that are likely to perform well on GPUs), we perform GPU-compute benchmarking in a top-down fashion, starting from complex real-life applications that are composed of multiple kernels. We characterize the Cactus benchmarks by quantifying their kernel execution time distribution, by analyzing the workloads using the roofline model, by performing a performance metrics correlation analysis, and by classifying their constituent kernels through multi-dimensional data analysis. The overall conclusion is that the Cactus workloads execute many more kernels, include more diverse and more complex execution behavior, and cover a broader range of the workload space compared to the prevalently used benchmark suites. We hence believe that Cactus is a useful complement to the existing GPU-compute benchmarking toolbox.",https://ieeexplore.ieee.org/document/9668300/,2021 IEEE International Symposium on Workload Characterization (IISWC),7-9 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EUVIP.2018.8611701,Classification of Building Information Model (BIM) Structures with Deep Learning,IEEE,Conferences,"In this work we study an application of machine learning to the construction industry and we use classical and modern machine learning methods to categorize images of building designs into three classes: Apartment building, Industrial building or Other. No real images are used, but only images extracted from Building Information Model (BIM) software, as these are used by the construction industry to store building designs. For this task, we compared four different methods: the first is based on classical machine learning, where Histogram of Oriented Gradients (HOG) was used for feature extraction and a Support Vector Machine (SVM) for classification; the other three methods are based on deep learning, covering common pre-trained networks as well as one designed from scratch. To validate the accuracy of the models, a database of 240 images was used. The accuracy achieved is 57% for the HOG + SVM model, and above 89% for the neural networks.",https://ieeexplore.ieee.org/document/8611701/,2018 7th European Workshop on Visual Information Processing (EUVIP),26-28 Nov. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2MTC.2018.8409769,Classification of eye gestures using machine learning for use in embedded switch controller,IEEE,Conferences,"The classification of signals captured by sampling devices through analysis is a powerful tool with application spanning nearly every industry. Using such classification on real time signals to detect different events or anomalies provides a fast and reliable way of implementing monitoring or control systems. Machine learning classification models include support vector machines (SVM), K-nearest neighbors (KNN), decision trees, and many more. These algorithms separate labelled datasets based on features extracted from the inputs. In the assistive technology field, the use of eye tracking technology provides patients of quadriplegia, ALS, or other neurodegenerative disease with the ability to control speech devices using their eyes. To provide a low-cost alternative to the existing costly devices, an electrooculography (EOG) controller was utilized for obtaining signal data and classifying gestures. A dataset consisting of these gestures was collected over several trials and classified using an SVM, KNN, and decision tree's algorithm with a moving window buffer suitable for an embedded device with peak overall accuracies of 96.8%, 96.9%, and 95.4% respectively. The trained models are converted to C code and uploaded to an ATmega328p AVR microcontroller. Using a decision tree implementation, the intentional blink signal classification is successfully predicted with an accuracy of 97.33% accuracy and filters out unintentional blinks with 100% accuracy on the embedded device.",https://ieeexplore.ieee.org/document/8409769/,2018 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),14-17 May 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCNC49032.2021.9369589,Closed Loop Paging Optimization for Efficient Mobility Management,IEEE,Conferences,"The 4G/5G networks deploy conventional Tracking Area Update and multi-step paging procedures for mobility management. The paging procedure consumes significant amount of licensed spectrum resources. The signaling overhead is going to worsen further with the increasing use of small cells and higher user mobility speed. To address this challenge, the telecommunication industry is embracing closed loop approaches to predict user mobility patterns. Though the mobility pattern prediction is a known problem, most of the existing solutions apply it for enhancing the handover management and use academic dataset. Furthermore, limited research has been done on idle-state users. In this paper, we propose a Closed Loop Paging (CLOP) optimization solution using semi-supervised learning model to reduce paging overhead. We harness the real network dataset to analyze the location trail of users in the network to predict a subset of Base Stations for paging to locate idle-state users. Our empirical results demonstrate that Linear Support Vector Machine (L-SVM) classification method excels when compared to other supervised learning models. The L-SVM Classifier saves nearly 43% of the paging overhead with a marginal increase in connection establishment delay by around 7.3%.",https://ieeexplore.ieee.org/document/9369589/,2021 IEEE 18th Annual Consumer Communications & Networking Conference (CCNC),9-12 Jan. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COINS49042.2020.9191386,Closing the loop: Real-time Error Detection and Correction in automotive production using Edge-/Cloud-Architecture and a CNN,IEEE,Conferences,"One challenge faced by the automotive industry is the shift from combustion to electrically powered vehicles. This change strongly impacts on components such as the electric motor and the battery, and hence on production. In this context, the low level of expert knowledge is especially problematic. To meet these new challenges, this paper introduces a data-driven optimization of the production process by integrating a modular edge and cloud computing layer, and advanced data analysis. Defects are classified by a convolutional neural network (CNN) (predictive analytics) and corrected (depending on the defect type) by an automated rework (prescriptive analytics). The architecture of the CNN achieves an accuracy of 99.21% to predict the defect class. The automated rework process is selected through an implemented decision tree. The edge device communicates with a programmable logic controller (PLC) through a cyber physical interface. As an example of its practical application, the method is applied to hairpin welding of the stator of an electric motor with real production data.",https://ieeexplore.ieee.org/document/9191386/,2020 International Conference on Omni-layer Intelligent Systems (COINS),31 Aug.-2 Sept. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCWD54268.2022.9776267,Cloud-edge-device Collaboration Mechanisms of Cloud Manufacturing for Customized and Personalized Products,IEEE,Conferences,"With the increasingly developed industry and more comprehensive product offerings, customized and personalized products (CPPs) gradually become a main business model of many enterprises. However, the characteristics of CPPs, such as large differences in product modules and short product delivery cycles, put forward very high demands for the intelligence, flexibility and real-time performance of cloud manufacturing (CMfg). To satisfy the above typical demands, a cloud-edge-device collaborative framework of CMfg is proposed to support distributed data processing and fast decision-making. In the context of Cloud-edge-device collaboration, the vertically and horizontally distributed deployment and update mechanisms of deep learning models (DLMs) are brought forward and analyzed in detail to provide rapid response and high-performance decision-making services for CPPs. In addition, related key technologies are presented to provide references for the technical research direction.",https://ieeexplore.ieee.org/document/9776267/,2022 IEEE 25th International Conference on Computer Supported Cooperative Work in Design (CSCWD),4-6 May 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCME52200.2021.9591113,Cobots for FinTech,IEEE,Conferences,"Embedded devices enabling payments transaction processing in Financial Services industry cannot have any margin for error. These devices need to be tested & validated by replicating production like environment to the extent possible. This means literally handling payments related events like swiping a credit card, tapping a mobile phone or pressing buttons amongst many other things like in real world. Embedded Software development is time consuming as it involves multiple man-machine interactions and dependencies such as managing and handling embedded devices, operating devices (Push buttons, interpret display panels, read receipt printouts etc.) and sharing devices for collaboration within team. During the current pandemic, it was impossible for software teams to travel to office, share devices or even procure necessary devices on time for project related tasks. This caused delay to project delivery and increased Time to market. The paper describes how the team used Capgemini's flexible Robotics as a Service (RaaS) platform that helped during pandemic to automate feasible man-machine interactions using Robotic arms. The paper provides details of the work done by the team that involves internet of things (IoT), Artificial Intelligence (AI) to remotely handle and operate hardware and devices thereby completing embedded software development life cycles faster and well within budget while ensuring superior product quality and importantly ensuring team's health and safety. This is novel in Financial Services space.",https://ieeexplore.ieee.org/document/9591113/,"2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)",7-8 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.2017.8247730,Communication middleware technologies for industrial distributed control systems: A literature review,IEEE,Conferences,"Industry 4.0 is the German vision for the future of manufacturing, where smart factories use information and communication technologies to digitise their processes to achieve improved quality, lower costs, and increased efficiency. It is likely to bring a massive change to the way control systems function today. Future distributed control systems are expected to have an increased connectivity to the Internet, in order to capitalize on new offers and research findings related to digitalization, such as cloud, big data, and machine learning. A key technology in the realization of distributed control systems is middleware, which is usually described as a reusable software layer between operating system and distributed applications. Various middleware technologies have been proposed to facilitate communication in industrial control systems and hide the heterogeneity amongst the subsystems, such as OPC UA, DDS, and RT-CORBA. These technologies can significantly simplify the system design and integration of devices despite their heterogeneity. However, each of these technologies has its own characteristics that may work better for particular applications. Selection of the best middleware for a specific application is a critical issue for system designers. In this paper, we conduct a survey on available standard middleware technologies, including OPC UA, DDS, and RT-CORBA, and show new trends for different industrial domains.",https://ieeexplore.ieee.org/document/8247730/,2017 22nd IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),12-15 Sept. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECAI.2018.8679004,Comparative Analysis of E-Learning Platforms on The Market,IEEE,Conferences,"The E-Learning concept came gradually in the public eye, having an ascendant exponential evolution, directly dependent on the technologies and methodologies involved. Its history is not a long one, stretched over two decades, the term being introduced in 1999 by Jay Cross. At the end of the 1990s, the term LMS, Learning Management System, was introduced, and e-Learning applications are widely used, both in education and in the private environment, as a support for continuous personal development. The emergence and extensive use of applications or program packages (eg Microsoft Office), dedicated dynamic content websites (eg Youtube) as well as access to electronic resources (eg Email, chat) led to the construction of communities within the context of online learning, facilitating communication, access and transfer of resources. After 2010, the E-Learning implications in the education system are recognized and heavily exploited. Access to huge treasures and resources, unlimited by geographical boundaries, a diverse range of ways to customize teaching and assessment techniques, an independent competitive market, ongoing development, and quality deliverables tailored to needs or purposes are just a few of the features of the current E-Learning domain. Educational programs attach great importance to both content, appearance and organization, and to the interaction between the teacher and the learner. The learner is provided with a variety of media resources, from slides, books and bibliographic materials grouped in one place to the real-time, to video-demonstrative teaching content in real time complete the learning experience and give it a practical note. This paper aims to present the characteristics that define an E-Learning platform and how they are assimilated by the educational systems, as well as to perform a comparative analysis of the existing E-learning platforms on the market, referring to the statistics on the E-learning industry in the world. The conclusions of the paper will attempt to outline some perspectives in this field.",https://ieeexplore.ieee.org/document/8679004/,"2018 10th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",28-30 June 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIS53314.2022.9743129,Comparative Analysis of Software Reliability Prediction Using Machine Learning and Deep Learning,IEEE,Conferences,"Software Reliability is an integral part to determine Software Quality. Software is considered to be of high quality if its reliability is high. There exist many statistical models that can help in predicting Software Reliability, but it is very difficult to consider all the real-world factors and hence it makes the task of reliability prediction very difficult. Therefore, it becomes more challenging for the IT industry to predict if a software is dependable or not. Machine Learning and Deep Learning can be used for the prediction of Software Reliability by programming a model that assesses reliability by fault prediction in a more meticulous manner. Therefore, in this study the use of predefined Artificial Intelligence algorithms, mainly Artificial Neural Network (ANN), Recurrent Neural Network (RNN), Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM) are intended for predicting software reliability on a time series software failure dataset and are compared on the basis of selected performance metrics. Each of the algorithm trained on software failure dataset will be used to predict the software failure time after a certain number of corrective modifications are performed on the software. Based on the result of the studies, it is discovered that LSTM produces superior outcomes in predicting the software failure trend as it can capture long and short-term trends in the software failure dataset.",https://ieeexplore.ieee.org/document/9743129/,2022 Second International Conference on Artificial Intelligence and Smart Energy (ICAIS),23-25 Feb. 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iSAI-NLP.2018.8692924,"Comparative Assessment of Indoor Positioning Technologies, Techniques, and Algorithms",IEEE,Conferences,"Indoor positioning systems (IPS) are used to locate the position of objects in indoor environments. Due to its many real-world applications, IPS has garnered interest from both academia and industry. Each IPS is made up of three major components: sensor technology, position-finding technique, and operating algorithm. The goal of this paper is to examine and independently compare different types of components with the aim to understand and make useful suggestions for improvement. This paper also presents the past and current trends in IPS and predict the future trends in approaches to the design and implementation of IPS.",https://ieeexplore.ieee.org/document/8692924/,2018 International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP),15-17 Nov. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2002.1189341,Computer based robot training in a virtual environment,IEEE,Conferences,"As more market segments are welcoming automation, the robotic field continues to expand. With the accepted breadth of viable industrial robotic applications increasing, the need for flexible robotic training also grows. In the area of simulation and offline programming there have been innovative developments to Computer Aided Robotics (CAR) Systems. New and notable releases have been introduced to the public, especially among the small, affordable, and easy to use systems. These CAR-Systems are mainly aimed at system integrators in general industry business fields to whom the complex, powerful software tools used by the automotive industry (and its suppliers) are oversized. In general, CAR-Systems are used to design robot cells and to create the offline programs necessary to reduce start-up time and to achieve a considerable degree of planning reliability. Another potential yet to be fully considered, is the use of such CAR-Systems as an inexpensive and user-friendly tool for robotics training. This paper will show the educational potential and possibility inherent in simulation and introduce a successful example of this new method of training. Finally, this presentation should be seen as an attempt to outline novel methods for future education in an industrial environment characterized by the increased occurrence and implementation of the virtual factory.",https://ieeexplore.ieee.org/document/1189341/,"2002 IEEE International Conference on Industrial Technology, 2002. IEEE ICIT '02.",11-14 Dec. 2002,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMSEC.2011.6010807,Constructing dependence ordering for B&B technique in learning Bayesian belief network,IEEE,Conferences,"The data mining technology is more and more widely used. How to construct a Bayesian belief network has been discussed in many different ways. As one of the classical algorithms, the branch and bound technique based on the minimum description length principle has been proposed by Joe Suzuki in 1998. But one of the most important premises of the B&B Technique is that an attributes' dependence ordering has been prepared. To address the problem, a new method is proposed for attributes' dependence ordering. The algorithm first constructs a dependence tree using training dataset, then we use breadth first searching and get the dependence ordering. That results in a raw ordering. Then we order the nodes that are in the same layer of the dependence tree in order to make the result more accurate. This paper uses real datasets from the telecom industry as the test datasets. The result shows that the algorithm can construct the dependence ordering with good performance.",https://ieeexplore.ieee.org/document/6010807/,"2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",8-10 Aug. 2011,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASE.2019.00042,Continuous Incident Triage for Large-Scale Online Service Systems,IEEE,Conferences,"In recent years, online service systems have become increasingly popular. Incidents of these systems could cause significant economic loss and customer dissatisfaction. Incident triage, which is the process of assigning a new incident to the responsible team, is vitally important for quick recovery of the affected service. Our industry experience shows that in practice, incident triage is not conducted only once in the beginning, but is a continuous process, in which engineers from different teams have to discuss intensively among themselves about an incident, and continuously refine the incident-triage result until the correct assignment is reached. In particular, our empirical study on 8 real online service systems shows that the percentage of incidents that were reassigned ranges from 5.43% to 68.26% and the number of discussion items before achieving the correct assignment is up to 11.32 on average. To improve the existing incident triage process, in this paper, we propose DeepCT, a Deep learning based approach to automated Continuous incident Triage. DeepCT incorporates a novel GRU-based (Gated Recurrent Unit) model with an attention-based mask strategy and a revised loss function, which can incrementally learn knowledge from discussions and update incident-triage results. Using DeepCT, the correct incident assignment can be achieved with fewer discussions. We conducted an extensive evaluation of DeepCT on 14 large-scale online service systems in Microsoft. The results show that DeepCT is able to achieve more accurate and efficient incident triage, e.g., the average accuracy identifying the responsible team precisely is 0.641~0.729 with the number of discussion items increasing from 1 to 5. Also, DeepCT statistically significantly outperforms the state-of-the-art bug triage approach.",https://ieeexplore.ieee.org/document/8952483/,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),11-15 Nov. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IGSC51522.2020.9291232,Conversion of an Unsupervised Anomaly Detection System to Spiking Neural Network for Car Hacking Identification,IEEE,Conferences,"Across industry, there is an increasing availability of streaming, time-varying data, where it is important to detect anomalous behavior. These data are found in an enormous number of sensor-based applications, in cybersecurity (where anomalous behavior could indicate an attack), and in finance. Spiking Neural Networks (SNNs) have come under the spotlight for machine learning applications due to the extreme energy efficiency of their implementation on neuromorphic processors like the Intel Loihi research chip. In this paper we explore the applicability of spiking neural networks for in vehicle cyberattack detection. We show exemplary results by converting an autoencoder model to spiking form. We present a learning model comparison that shows the proposed SNN autoencoder outperforms a One Class Support Vector Machine and an Isolation Forest. Furthermore, only a slight reduction in accuracy is observed when compared to a traditional autoencoder.",https://ieeexplore.ieee.org/document/9291232/,2020 11th International Green and Sustainable Computing Workshops (IGSC),19-22 Oct. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AISP53593.2022.9760616,Convolutional GRU Networks based Singing Voice Separation,IEEE,Conferences,"Toned voice study is gaining importance due to advancement in the music industry. The breaking down of toned voice and its backtracking is similar to carrying images from the source domain to the target domain while preserving its content representation. For our case, the mixed voice prints were transformed into their constituent component. The drawback of U-Net convolutional architecture is that the learning rate may come down in the middle layers for deeper models, so there is some risk if the network learning is ignored in some cases where the abstract features are represented in those layers. In this work, we proclaim the methodology CGRUN for the task of singing voice division. It leads to a causal system that is naturally suitable for real-time processing applications. The speech processing application is the segregation of toned voices for voice mixing. Through software evaluation, this experiment confirms the use of CGRUN for toned voice separation. The technical term used for toned voice segregation and its backtracking is Music Information Retrieval (MIR).",https://ieeexplore.ieee.org/document/9760616/,2022 2nd International Conference on Artificial Intelligence and Signal Processing (AISP),12-14 Feb. 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEM45057.2020.9309978,Creating Transparency in the Finished Vehicles Transportation Process Through the Implementation of a Real-Time Decision Support System,IEEE,Conferences,"The complexity of global distribution networks in the automotive industry and likewise the number of disruptions significantly increased throughout the last years. In order to monitor relevant processes and to optimize decision-making in case of disruptions, a concept for a decision support system (DSS) was introduced. For this purpose, the distribution process weaknesses of the German premium automotive company BMW were identified. The method used was a Failure Mode and Effect Analysis with operational managers and relevant process partners interviews. Based on the findings, performance indicators, thresholds, early warnings and options for action were specified. A big data platform supports the processing of the growing number of relevant data in real-time. In the long-term decision-making can be automated using machine learning algorithms. This paper proves that negative impacts of disruptions can be minimized, and the robustness of the process improved by anticipating and identifying deviations beforehand and in real-time. Hence, companies save money while strengthening customer satisfaction. The DSS can be seen as a necessary precursor of a digital twin.",https://ieeexplore.ieee.org/document/9309978/,2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM),14-17 Dec. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/cp.2018.1731,Current trend in planning and scheduling of construction project using artificial in telligence,IET,Conferences,"During digitalization of Architecture, Engineering and Construction (AEC) industry, one of the influential technologies, Computer-Aided Design can present the state of these digital technologies that assist practitioners to achieve better work. Similar with CAD, digital project planning and schedule control approaches are still mainly relayed on expert experiences which leads to high expert cost in construction project. Researchers and construction-related project participants develops technologies related to Artificial Intelligence (AI) field to decrease the dependence level of expert in construction planning and schedule control. This paper focuses on 1) teasing out the historical trend in planning and scheduling of construction project of AI technologies; 2) using published construction projects and interview data collected by authors, introducing the current trend of implementation of AI technologies in construction planning and schedule control area. Then, this paper identified gaps of AI technologies adoption between academic perspective and real world in both construction scheduling and planning areas. This paper collected totally 1120 journal papers, by keywords as: Artificial Intelligence, Construction. Then the papers are selected following criterion that after 2000 and only related to relevant research fields. After paper selection, totally 383 papers are analysed to identify the historical trend.",https://ieeexplore.ieee.org/document/8826800/,"IET Doctoral Forum on Biomedical Engineering, Healthcare, Robotics and Artificial Intelligence 2018 (BRAIN 2018)",4-4 Nov. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GUCON50781.2021.9573994,Cyber Warfare Threat Categorization on CPS by Dark Web Terrorist,IEEE,Conferences,"The Industrial Internet of Things (IIoT) also referred as Cyber Physical Systems (CPS) as critical elements, expected to play a key role in Industry 4.0 and always been vulnerable to cyber-attacks and vulnerabilities. Terrorists use cyber vulnerability as weapons for mass destruction. The dark web's strong transparency and hard-to-track systems offer a safe haven for criminal activity. On the dark web (DW), there is a wide variety of illicit material that is posted regularly. For supervised training, large-scale web pages are used in traditional DW categorization. However, new study is being hampered by the impossibility of gathering sufficiently illicit DW material and the time spent manually tagging web pages. We suggest a system for accurately classifying criminal activity on the DW in this article. Rather than depending on the vast DW training package, we used authorized regulatory to various types of illicit activity for training Machine Learning (ML) classifiers and get appreciable categorization results. Espionage, Sabotage, Electrical power grid, Propaganda and Economic disruption are the cyber warfare motivations and We choose appropriate data from the open source links for supervised Learning and run a categorization experiment on the illicit material obtained from the actual DW. The results shows that in the experimental setting, using TF-IDF function extraction and a AdaBoost classifier, we were able to achieve an accuracy of 0.942. Our method enables the researchers and System authoritarian agency to verify if their DW corpus includes such illicit activity depending on the applicable rules of the illicit categories they are interested in, allowing them to identify and track possible illicit websites in real time. Because broad training set and expert-supplied seed keywords are not required, this categorization approach offers another option for defining illicit activities on the DW.",https://ieeexplore.ieee.org/document/9573994/,"2021 IEEE 4th International Conference on Computing, Power and Communication Technologies (GUCON)",24-26 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITHET.2012.6246060,Cybernetic Education Centre: Monitoring and control of learner's e-learning study in the field of Cybernetics and Automation by Coloured Petri Nets model,IEEE,Conferences,"This paper presents the Cybernetic Education Centre (CEDUC) as a hybrid e-learning and training centre for higher education of Cybernetics and Automation fields. If we consider Cybernetics we consider basically (1) controlled systems and (2) control systems. In case of controlled systems learner is focused on the process of analyze, identification, design of the mathematical model and simulation of the controlled system. Therefore this paper deals with controlled models in the laboratories of our department (a) real laboratory models, (b) simulation models and (c) virtual models which creates one integrated hybrid architecture what represents one of new ideas in the paper (Fig. 1). Learner of control systems is focused mainly on design of control parameters, design of control algorithms, design of hardware, software and communication architectures of control systems. Overall control system is represented by Distributed Control System (DCS) which serves for learners to verify designed control systems. The verification of the control systems is very important from safety point of view to prepare learners for real production conditions. Second new idea of the paper is implementation of the Coloured Petri Nets as automata to control access to the resources (not only typical study resources but also access to the components of hybrid DCS architecture) as well as to monitor the learner activities during the study. CEDUC is supported by intensive industry-university partnership. Conclusion of the paper summarizes the results of the study process of learners in DCS environment.",https://ieeexplore.ieee.org/document/6246060/,2012 International Conference on Information Technology Based Higher Education and Training (ITHET),21-23 June 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECCE44975.2020.9235814,Data-Driven Nonparametric Li-Ion Battery Ageing Model Aiming At Learning From Real Operation Data: Holistic Validation With Ev Driving Profiles,IEEE,Conferences,"Conventional Li-ion battery ageing models require a significant amount of time and experimental resources to provide accurate predictions under realistic operating conditions. Furthermore, there is still an uncertainty on the validity of purely laboratory data-based ageing models for the accurate ageing prediction of battery systems deployed in field.At the same time, there is significant interest from industry in the introduction of new data collection telemetry technology. This implies the forthcoming availability of a significant amount of in-field battery operation data. In this context, the development of ageing models able to learn from in-field battery operation data is an interesting solution to mitigate the need for exhaustive laboratory testing, reduce the development cost of ageing models and at the same time ensure the validity of the model for prediction under real operating conditions.In this paper, a holistic data-driven ageing model developed under the Gaussian Process framework is validated with experimental battery ageing data. Both calendar and cycle ageing are considered, to predict the capacity loss within real EV driving scenarios. The model can learn from the driving data progressively observed, improving continuously its performances and providing more accurate and confident predictions.",https://ieeexplore.ieee.org/document/9235814/,2020 IEEE Energy Conversion Congress and Exposition (ECCE),11-15 Oct. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO.2018.8396547,Data-driven quality prognostics for automated riveting processes,IEEE,Conferences,"Technologies based in robotics and automatics are reshaping the aerospace industry. Aircraft manufacturers and top-tier suppliers now rely on robotics to perform most of its operational tasks. Over the years, a succession of implemented mobile robots has been developed with the mission of automating important industrial processes such as welding, material handling or assembly procedures. However, despite the progress achieved, a major limitation is that the process still requires human supervision and an extensive quality control process. An approach to address this limitation is to integrate machine learning methods within the quality control process. The idea is to develop algorithms that can direct manufacturing experts towards critical areas requiring human supervision and quality control. In this paper we present an application of machine learning to a concrete industrial problem involving the quality control of a riveting machine. The proposal consists of an intelligent predictive model that can be integrated within the existing real time sensing and pre-processing sub-systems at the equipment level. The framework makes use of several data-driven techniques for pre-processing and feature engineering, combined with the most accurate algorithms, validated through k-folds cross validation technique which also estimates prediction errors. The model is able to classify the manufacturing process of the machine as nominal or anomalous according to a real-world data set of design requirements and operational data. Several machine learning algorithms are compared such as linear regression, nearest neighbor, support vector machines, decision trees, random forests and extreme gradient boost. Results obtained from the case study suggest that the proposed model produces accurate predictions which meet industrial standards.",https://ieeexplore.ieee.org/document/8396547/,2018 IEEE Aerospace Conference,3-10 March 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IMCEC51613.2021.9482089,Data-driven scheduling for smart shop floor via reinforcement learning with model-based clustering algorithm,IEEE,Conferences,"Various information technologies provide the manufacturing system massive data, which can support the decision optimization of product lifecycle management. However, the lack of effective use for advanced technologies prevents manufacturing industry from being automated and intelligent. Therefore, this paper proposes the smart shop floor and implementation mechanism. Meanwhile, based on the clustering and reinforcement learning, the brain agent and scheduling agent are designed to determine the approriate rule according to the shop floor state information, thus realizing the data-driven real-time scheduling. Experimental results show that the smart shop floor can effectively deal with disturbance events and has better performance compared with composite dispatching rules.",https://ieeexplore.ieee.org/document/9482089/,"2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)",18-20 June 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2019.8851923,Deep Domain Adaptation for Vulnerable Code Function Identification,IEEE,Conferences,"Due to the ubiquity of computer software, software vulnerability detection (SVD) has become crucial in the software industry and in the field of computer security. Two significant issues in SVD arise when using machine learning, namely: i) how to learn automatic features that can help improve the predictive performance of vulnerability detection and ii) how to overcome the scarcity of labeled vulnerabilities in projects that require the laborious labeling of code by software security experts. In this paper, we address these two crucial concerns by proposing a novel architecture which leverages deep domain adaptation with automatic feature learning for software vulnerability identification. Based on this architecture, we keep the principles and reapply the state-of-the-art deep domain adaptation methods to indicate that deep domain adaptation for SVD is plausible and promising. Moreover, we further propose a novel method named Semi-supervised Code Domain Adaptation Network (SCDAN) that can efficiently utilize and exploit information carried in unlabeled target data by considering them as the unlabeled portion in a semi-supervised learning context. The proposed SCDAN method enforces the clustering assumption, which is a key principle in semi-supervised learning. The experimental results using six real-world software project datasets show that our SCDAN method and the baselines using our architecture have better predictive performance by a wide margin compared with the Deep Code Network (VulDeePecker) method without domain adaptation. Also, the proposed SCDAN significantly outperforms the DIRT-T which to the best of our knowledge is currently the-state-of-the-art method in deep domain adaptation and other baselines.",https://ieeexplore.ieee.org/document/8851923/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIAfS52090.2021.9605823,Deep Learning & Computer Vision for IoT based Intelligent Driver Assistant System,IEEE,Conferences,"With the exponential increment of vehicles, roadside accidents also have been increasing rapidly where approximately 80% of these accidents were caused by human error. Therefore, the automobile industry and the government authorities are more focused on accident prevention by introducing improved road safety systems for the general public. Driver assistance system (DAS) is an intelligent road safety development where it senses the surrounding of a moving vehicle which will assist the driver to avoid the dangers and also warn the drivers of immediate dangers. With the current technological advancements, the automobile industry is equipped with the internet of things (IoT) based data transfer mechanisms, wherewith the concept of ‘connected car’ the passengers and the other interconnected vehicles with the internet can share data with back end applications. The data is consisting of the current location, the distance travelled by the vehicle, whether the vehicle requires urgent service. This study is mainly focused on the development of an intelligent driver assistance system based on computer vision and deep learning, which can prevent accidents with early detection of drowsiness, harmful objects and also by alerting the driver with the signboards and the road lines. The system is capable of passing the emergency messages to the driver as well as the other interconnected vehicles through the website by communicating the real-time road map generated within the system. The proposed system has been implemented and tested with multiple detection scenarios where machine learning has been employed to improve the accuracy of the results.",https://ieeexplore.ieee.org/document/9605823/,2021 10th International Conference on Information and Automation for Sustainability (ICIAfS),11-13 Aug. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CASE48305.2020.9216881,Deep Learning for Early Damage Detection of Tailing Pipes Joints with a Robotic Device,IEEE,Conferences,"In the mining industry, it is usual to employ several kilometers of pipes to carry tailing from the plant to a dam. Only in the Salobo Mine, a copper operation in the Amazon forest from Vale S.A., there are more than three and a half kilometers of tailing pipes. Since the material passing through the tailing pipe causes an abrasion effect that could lead to failures, regular inspections are needed. However, given the risky environment to perform manual inspections, a teleoperated or autonomous robot is a crucial tool to keep track of the pipe health. In this work, we propose a deep-learning methodology to process the data stream of images from the robot, aiming to detect early failures directly on the onboard computer of the device in real-time. Multiple architectures of deep-learning image classification were evaluated to detect the anomalies. We validated the early damage detection accuracy and pinpointed the approximate location of the anomalies using the Class Activation Mapping of the networks. Then, we tested the runtime for the network architectures that obtained the best results on different hardware to analyze the need for a GPU onboard in the robot. Moreover, we also trained a Single Shot object Detector to find the boundaries of the pipe joints, which means that the anomaly classification is performed only when a joint is detected. Our results show that it is possible to build an automatic anomaly detection system in the software of the robot.",https://ieeexplore.ieee.org/document/9216881/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SCC.2017.51,Deep and Shallow Model for Insurance Churn Prediction Service,IEEE,Conferences,"Churn prediction is very important to the insurance industry. Therefore, there is a big value to investigate how to improve its performance. More importantly, a good model can be used by a common service provider and benefit many companies. State-of-the-art methods either use 1) shallow models such as logistic regression, with sophisticated feature engineering, or 2) deep models that learn features and classification models simultaneously. In terms of performance, shallow models can memorize better while deep models can generalize better but may under-generalize with insufficient data. Therefore, we propose a combined Deep &, Shallow model (DSM) to take the strengths of both memorization and generalization in one model by jointly training shallow models and deep models. The experiment results show that for insurance churn prediction, joint training can significantly improve the performance and the DSM earns better performance than both shallow-only and deep-only models. In our real-life dataset, the DSM performs better than CNN, LSTM, Stochastic Gradient Descent, Linear Discriminant Analysis, Quadratic Discriminant Analysis, Gaussian Naive Bayes, AdaBoost, Random Forest, and Gradient Tree Boosting. In addition, the DSM can also be applied to other prediction services.",https://ieeexplore.ieee.org/document/8035004/,2017 IEEE International Conference on Services Computing (SCC),25-30 June 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE.2018.8326053,Deep learning networks in CE,IEEE,Conferences,"I could probably offer a half-way answer to “what is the next challenge for CE Imaging?”. Reality is that I don't know for sure (there are many), but I know how to solve it: neural networks. The real question becomes how to enable low cost NN implementations and deployments in CE devices without the fear of losing years of work invested in training and optimizing the networks designed to solve specific problems (sound enhancement, better imaging, understanding of the surroundings, easier cooking, better coffee, etc.). This talk will detail the challenges and industry proven solutions for computer vision and computational imaging using a hybrid traditional imaging and deep learning approach. IP protection against intellectual theft once the solutions are deployed is also briefly discussed.",https://ieeexplore.ieee.org/document/8326053/,2018 IEEE International Conference on Consumer Electronics (ICCE),12-14 Jan. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CBMS.2019.00040,Deep-Learning and HPC to Boost Biomedical Applications for Health (DeepHealth),IEEE,Conferences,"This document introduces the DeepHealth project: ""Deep-Learning and HPC to Boost Biomedical Applications for Health"". This project is funded by the European Commission under the H2020 framework program and aims to reduce the gap between the availability of mature enough AI-solutions and their deployment in real scenarios. Several existing software platforms provided by industrial partners will integrate state-of-the-art machine-learning algorithms and will be used for giving support to doctors in diagnosis, increasing their capabilities and efficiency. The DeepHealth consortium is composed by 21 partners from 9 European countries including hospitals, universities, large industry and SMEs.",https://ieeexplore.ieee.org/document/8787438/,2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS),5-7 June 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SANER.2019.8668044,DeepCT: Tomographic Combinatorial Testing for Deep Learning Systems,IEEE,Conferences,"Deep learning (DL) has achieved remarkable progress over the past decade and has been widely applied to many industry domains. However, the robustness of DL systems recently becomes great concerns, where minor perturbation on the input might cause the DL malfunction. These robustness issues could potentially result in severe consequences when a DL system is deployed to safety-critical applications and hinder the real-world deployment of DL systems. Testing techniques enable the robustness evaluation and vulnerable issue detection of a DL system at an early stage. The main challenge of testing a DL system attributes to the high dimensionality of its inputs and large internal latent feature space, which makes testing each state almost impossible. For traditional software, combinatorial testing (CT) is an effective testing technique to balance the testing exploration effort and defect detection capabilities. In this paper, we perform an exploratory study of CT on DL systems. We propose a set of combinatorial testing criteria specialized for DL systems, as well as a CT coverage guided test generation technique. Our evaluation demonstrates that CT provides a promising avenue for testing DL systems.",https://ieeexplore.ieee.org/document/8668044/,"2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)",24-27 Feb. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM42981.2021.9488784,DeepLoRa: Learning Accurate Path Loss Model for Long Distance Links in LPWAN,IEEE,Conferences,"LoRa (Long Range) is an emerging wireless technology that enables long-distance communication and keeps low power consumption. Therefore, LoRa plays a more and more important role in Low-Power Wide-Area Networks (LPWANs), which easily extend many large-scale Internet of Things (IoT) applications in diverse scenarios (e.g., industry, agriculture, city). In lots of environments where various types of land-covers usually exist, it is challenging to precisely predict a LoRa link's path loss. As a result, how to deploy LoRa gateways to ensure reliable coverage and develop precise fingerprint-based localization becomes a difficult issue in practice. In this paper, we propose DeepLoRa, a deep learning-based approach to accurately estimate the path loss of long-distance links in complex environments. Specifically, DeepLoRa relies on remote sensing to automatically recognize land-cover types along a LoRa link. Then, DeepLoRa utilizes Bi-LSTM (Bidirectional Long Short Term Memory) to develop a land-cover aware path loss model. We implement DeepLoRa and use the data gathered from a real LoRaWAN deployment on campus to evaluate its performance extensively in terms of estimation accuracy and model transferability. The results show that DeepLoRa reduces the estimation error to less than 4 dB, which is 2× smaller than state-of-the-art models.",https://ieeexplore.ieee.org/document/9488784/,IEEE INFOCOM 2021 - IEEE Conference on Computer Communications,10-13 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DASC-PICom-DataCom-CyberSciTec.2017.200,DeepSim: Cluster Level Behavioral Simulation Model for Deep Learning,IEEE,Conferences,"We are witnessing an explosion of AI based use cases driving the computer industry, and especially datacenter and server architectures. As Intel faces fierce competition in this emerging technology space, it is important that architecture definitions and directions are driven with data from proper tools and methodologies, and insights are drawn from end-to-end holistic analysis at the datacenter levels. In this paper, we introduce DeepSim, a cluster-level behavioral simulation model for deep learning. DeepSim, which is based on the Intel CoFluent simulation framework, uses timed behavioral models to simulate complex interworking between compute nodes, networking, and storage at the datacenter level, providing a realistic performance model of a real-world image recognition applications based on the popular Deep Learning Framework Caffe. The end-to-end simulation data from DeepSim provides insight which can be used for architecture analysis driving future datacenter architecture directions. DeepSim enables scalable system design, deployment, and capacity planning through accurate performance insights. Results from preliminary scaling studies (e.g. node scaling and network scaling) and what-if analyses (e.g., Xeon with HBM and Xeon Phi with dual OPA) are presented in this paper. The simulation results are correlated well with empirical measurements, achieving an accuracy of 95%.",https://ieeexplore.ieee.org/document/8328544/,"2017 IEEE 15th Intl Conf on Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)",6-10 Nov. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFCOMW.2017.8116515,Demo abstract: Telco localization techniques,IEEE,Conferences,"When a mobile device connects to a telecom network (e.g., GSM 2G, CDMA 3G and LTE 4G), the network generates measurement report (MR) data containing signal conditions to support communication services. Using MR data to predict the accurate position of a mobile device is an important problem in Telco industry, called Telco localization problem. Although the literatures have proposed various MR-based localization algorithms, it is unclear how such algorithms perform in terms of localization precision. We develop a tool named STLT, which supports comprehensive performance study of a broad category of the state-of-art localization algorithms. STLT provides flexible training/testing data division, deep parameter turning as well as three visualization modes to display localization results. Through the demonstration of STLT on three real MR datasets provided by one of the largest Telco operators in China, we have three interesting findings that could inspire and enhance future research in Telco location.",https://ieeexplore.ieee.org/document/8116515/,2017 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),1-4 May 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2019.00089,Denoising Internet Delay Measurements using Weak Supervision,IEEE,Conferences,"To understand the delay characteristics of the Internet, a myriad of measurement tools and techniques are proposed by the researchers in academia and industry. Datasets from such measurement tools are curated to facilitate analyses at a later time. Despite the benefits of these tools and datasets, the systematic interpretation of measurements in the face of measurement noise. Unfortunately, state-of-the-art denoising techniques are labor-intensive and ineffective. To tackle this problem, we develop NoMoNoise, an open-source framework for denoising latency measurements by leveraging the recent advancements in weak-supervised learning. NoMoNoise can generate measurement noise labels that could be integrated into the inference and control logic to remove and/or repair noisy measurements in an automated and rapid fashion. We evaluate the efficacy of NoMoNoise in a lab-based setting and a real-world setting by applying it on CAIDA's Ark dataset and show that NoMoNoise can remove noisy measurements effectively with high accuracy.",https://ieeexplore.ieee.org/document/8999330/,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),16-19 Dec. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPEC.2019.8916576,Deploying AI Frameworks on Secure HPC Systems with Containers,IEEE,Conferences,"The increasing interest in the usage of Artificial Intelligence (AI) techniques from the research community and industry to tackle “real world” problems, requires High Performance Computing (HPC) resources to efficiently compute and scale complex algorithms across thousands of nodes. Unfortunately, typical data scientists are not familiar with the unique requirements and characteristics of HPC environments. They usually develop their applications with high level scripting languages or frameworks such as TensorFlow and the installation processes often require connection to external systems to download open source software during the build. HPC environments, on the other hand, are often based on closed source applications that incorporate parallel and distributed computing API’s such as MPI and OpenMP, while users have restricted administrator privileges, and face security restrictions such as not allowing access to external systems. In this paper we discuss the issues associated with the deployment of AI frameworks in a secure HPC environment and how we successfully deploy AI frameworks on SuperMUC-NG with Charliecloud.",https://ieeexplore.ieee.org/document/8916576/,2019 IEEE High Performance Extreme Computing Conference (HPEC),24-26 Sept. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSN.2019.00043,Deploying Intrusion-Tolerant SCADA for the Power Grid,IEEE,Conferences,"While there has been considerable research on making power grid Supervisory Control and Data Acquisition (SCADA) systems resilient to attacks, the problem of transitioning these technologies into deployed SCADA systems remains largely unaddressed. We describe our experience and lessons learned in deploying an intrusion-tolerant SCADA system in two realistic environments: a red team experiment in 2017 and a power plant test deployment in 2018. These experiences resulted in technical lessons related to developing an intrusion-tolerant system with a real deployable application, preparing a system for deployment in a hostile environment, and supporting protocol assumptions in that hostile environment. We also discuss some meta-lessons regarding the cultural aspects of transitioning academic research into practice in the power industry.",https://ieeexplore.ieee.org/document/8809554/,2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN),24-27 June 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBAIE49996.2020.00085,Design and Implementation of Intelligent Tour Guide System in Large Scenic Area Based on Fog Computing,IEEE,Conferences,"In recent years, the tourism industry has developed rapidly, and there are more and more large-scale tourist parks. There is an urgent need for intelligent guide systems that take tourists as the center and integrate multiple functions into one. Based on the basic requirements of smart tourism, this paper proposes an intelligent guide system based on fog computing. The overall architecture and system functions of the system are designed. The tasks undertaken by each level and the implementation methods of each function are analyzed in detail. This intelligent tour guide system can provide high-quality services such as location-based and real-scenery commentary, intelligent tour guides, program reminders, and consulting exchanges to bring tourists a more satisfactory travel experience.",https://ieeexplore.ieee.org/document/9196285/,"2020 International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",12-14 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIID51893.2021.9456521,Design and Implementation of Online Monitoring System for Soil Salinity and Alkalinity in Yangtze River Delta Tideland,IEEE,Conferences,"Soil salinity and alkalinity is an important index concerned by planting industry. In order to meet the demand of long-term observation of soil salinity and alkalinity in precision agriculture and eco-environmental protection, and to solve the current pain points of long sampling period and high cost of soil salinity measurement, this paper designs and implements an online monitoring system for soil salinity alkalinity in tideland in the Yangtze River Delta for crop planting and soil remediation. This system uses solar power supply system and maintenance-free digital sensor, which can be arranged in monitoring area for a long time to collect soil temperature, humidity and salinity data. The collected data can be stored in SD card locally and transmitted to cloud server in real time through 4G network. Up to now, the system has been running stably for nearly two years under the condition of unattended and maintenance free. More than 30000 soil salinity data have been collected from 5 monitoring points, which can be used for long-term observation of the interaction between salinity and plant growth, so as to improve the soil and improve the quality of agriculture products.",https://ieeexplore.ieee.org/document/9456521/,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),28-30 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIID51893.2021.9456537,Design and Implementation of Regional Food Distribution Platform Based on Big Data,IEEE,Conferences,"In recent years, the rapid development of big data has made people's daily life very convenient, and at the same time, tasting all kinds of food has become an important activity for people to travel. Due to the vast territory of China, there are many types of cuisines with great differences, it is very important for travelers to understand the special regional cuisines in the area. This project aggregates regional food data based on big data, and provides tourists with efficient, stable and professional data retrieval and analysis services through a visual data interface, and provides intuitive, accurate, and real-time data support for the decision-making of finding characteristic regional food. This thesis first conducted a relevant understanding of big data and the overall situation of regional cuisine, analyzed the distribution of food in the sub-provincial city of Xi'an, explored the research methods and implementation methods of related projects at home and abroad, based on this, summarized the research of this project the goal. At the same time, the focus of this project is to analyze the price, score and popularity of regional food data analysis in my country, and to summarize, proofread and organize the data obtained before into standard and standardized data. Through the classification of basic information, the visual analysis and display of data is realized, and the key data urgently needed by decision makers are extracted from it. To analyze the needs of decision makers, establish corresponding strategies and measures to improve the quality of data services. The establishment of this system provides a useful supplement and improvement to the existing industry data analysis system. The system is mainly divided into six modules, which are data collection, data review, data summary, and visual data display modules. Among them, data collection includes crawling relevant data from the Internet and retrieving key data. The data audit function includes classifying the crawled data and reviewing its effectiveness. Data aggregation includes summarizing the filtered data in an excel table and passing Excel generates a visual chart that you want to know, and at the same time generates a word cloud by associating certain two related items in the data in the excel table. The visual data display module can more clearly show the results of data analysis to users, so that users can make better decisions. The visualization data uses AmChart Flash charts. The implementation of this system adopts Django Python Web framework, the development language chooses Python, the development tool adopted is PyCharm, and the database tool adopts MySQL.",https://ieeexplore.ieee.org/document/9456537/,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),28-30 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBAR55169.2021.00014,Design and application of real estate market monitoring platform based on spatio-temporal big data,IEEE,Conferences,"When the traditional visualization application platform visually analyzes large-scale real estate spatio-temporal data, there are problems such as slow loading and poor functional scalability. In order to solve these problems, this paper proposes a design scheme of a visual analysis platform for real estate market monitoring. First, carry out conceptual analysis and feature induction of spatio-temporal big data to realize the collection processing and organizational structure analysis of real estate spatio-temporal big data. Then a four-tier platform architecture consisting of data sources layer, data resource management layer, interface service engine layer and visual application layer is designed. The visualization platform designed by this scheme has good scalability, and can provide technical guidance for the construction of a visualization analysis platform in the field of industry spatio-temporal big data. Finally, a visual application design is carried out based on the real estate market data of Hubei Province, which proves the validity and feasibility of the research scheme in this paper.",https://ieeexplore.ieee.org/document/9727001/,"2021 International Conference on Big Data, Artificial Intelligence and Risk Management (ICBAR)",5-7 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2017.8108946,Design and implementation of smoke early warning system based on video analysis technology,IEEE,Conferences,"In order to obtain real-time accurate detection of smoke in the panoramic area, a visual -based smoke early warning system is designed. A 180-degree holographic and high speed camera is equipped as the video capture part of the system, and the video acquisition and target lock functions are implemented. The intelligent recognition function is separated into three steps. Firstly, the suspected smoke area is extracted by Vibe algorithm. Secondly, the Camshift method is used to track the suspected area of moving smoke. Thirdly, the smoke is detected by the proposed fusion dynamic features. The interface of system is developed by the software of QT. The designed system can not only realize sensitive area monitoring function in manual mode, but also the intelligent smoke early warning and detection function are achieved in automatic mode. The experimental results showed the designed system obtained the advantages of simple operation, rapid smoke alarm and the satisfied accuracy for industry requirement.",https://ieeexplore.ieee.org/document/8108946/,2017 International Conference on Machine Learning and Cybernetics (ICMLC),9-12 July 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SYNASC.2016.045,Detecting Malicious URLs: A Semi-Supervised Machine Learning System Approach,IEEE,Conferences,"As malware industry grows, so does the means of infecting a computer or device evolve. One of the most common infection vector is to use the Internet as an entry point. Not only that this method is easy to use, but due to the fact that URLs come in different forms and shapes, it is really difficult to distinguish a malicious URL from a benign one. Furthermore, every system that tries to classify or detect URLs must work on a real time stream and needs to provide a fast response for every URL that is submitted for analysis (in our context a fast response means less than 300-400 milliseconds/URL). From a malware creator point of view, it is really easy to change such URLs multiple times in one day. As a general observation, malicious URLs tend to have a short life (they appear, serve malicious content for several hours and then they are shut down usually by the ISP where they reside in). This paper aims to present a system that analyzes URLs in network traffic that is also capable of adjusting its detection models to adapt to new malicious content. Every correctly classified URL is reused as part of a new dataset that acts as the backbone for new detection models. The system also uses different clustering techniques in order to identify the lack of features on malicious URLs, thus creating a way to improve detection for this kind of threats.",https://ieeexplore.ieee.org/document/7829617/,2016 18th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC),24-27 Sept. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASE51524.2021.9678891,Detecting TensorFlow Program Bugs in Real-World Industrial Environment,IEEE,Conferences,"Deep learning has been widely adopted in industry and has achieved great success in a wide range of application areas. Bugs in deep learning programs can cause catastrophic failures, in addition to a serious waste of resources and time.This paper aims at detecting industrial TensorFlow program bugs. We report an extensive empirical study on 12,289 failed TensorFlow jobs, showing that existing static tools can effectively detect 72.55% of the top three types of Python bugs in industrial TensorFlow programs. In addition, we propose (for the first time) a constraint-based approach for detecting TensorFlow shape-related errors (one of the most common TensorFlow-specific bugs), together with an associated tool, ShapeTracer. Our evaluation on a set of 60 industrial TensorFlow programs shows that ShapeTracer is efficient and effective: it analyzes each program in at most 3 seconds and detects effectively 40 out of 60 industrial TensorFlow program bugs, with no false positives. ShapeTracer has been deployed in the platform-X platform and will be released soon.",https://ieeexplore.ieee.org/document/9678891/,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),15-19 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCI51800.2020.00019,Detection and Defense from False Data Injection Attacks In Aviation Cyber-Physical Systems Using Artificial Immune Systems,IEEE,Conferences,"In recent years, there has been a rapid expansion in the development of Cyber-Physical Systems (CPS), which allows the physical components and the cyber components of a system to be fully integrated and interacted with each other and with the physical world. The commercial aviation industry is shifting towards Aviation Cyber-Physical Systems (ACPS) framework because it allows real-time monitoring and diagnostics, real-time data analytics, and the use of Artificial Intelligent technologies in decision making. Inevitably, ACPS is not immune to cyber-attacks due to integrating a network system, which introduces serious security threats. False Data Injection (FDI) attack is widely used against CPS. It is a serious threat to the integrity of the connected physical components. In this paper, we propose a novel security algorithm for detecting FDI attacks in the communication network of ACPS using Artificial Immune System (AIS). The algorithm was developed based on the negative selection approach. The negative selection algorithm is used to detect malicious network packets and drop them. Then, a Nonlinear Autoregressive Exogenous (NARX) network is used to predict packets that dropped by the negative selection algorithm. The developed algorithm was implemented and tested on a networked control system of commercial aircraft as an Aviation Cyber-physical system.",https://ieeexplore.ieee.org/document/9457964/,2020 International Conference on Computational Science and Computational Intelligence (CSCI),16-18 Dec. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSSA.2016.19,Detection of Man-in-the-Middle Attacks on Industrial Control Networks,IEEE,Conferences,"In this paper we present a method to detect Man-in-the-Middle attacks on industrial control systems. The approach uses anomaly detection by developing a model of normal behaviour of the industrial control system network. To come as close as possible to reality a simple industrial system, a conveyor belt with sensors and actuators, was set up with controllers widely used in industry. A machine learning approach based on the k-Nearest Neighbors algorithm with Bregman divergence was used to define a model of normal (valid) behaviour. Afterwards Man-in-the-Middle attacks were launched against the system and its behaviour during the attack was compared to the valid behaviour model. The results show that the approach taken was able to detect such attacks with satisfactory accuracy.",https://ieeexplore.ieee.org/document/7861654/,2016 International Conference on Software Security and Assurance (ICSSA),24-25 Aug. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RAAI52226.2021.9508033,Development of Gasoline-Electric Hybrid Propulsion Surveillance and Reconnaissance VTOL UAV,IEEE,Conferences,"Vertical Take-Off and Landing (VTOL) Unmanned Aerial Vehicles (UAV) have been a high potential topic in the aerospace industry during the last decades due to its multirotor and fixed-wing nature of the aircraft. Besides, having the ability to rapidly deploy from a tight airstrip and gathering Intelligence, Surveillance, and Reconnaissance (ISR) information is the best way to be one step ahead of the enemy. In this paper, we present the implementation and development of gasoline-electric hybrid propulsion VTOL Unmanned Aerial vehicle respectively. The Hybrid propulsion VTOL UAV offers image and real-time video transmission to the ground station with fully autonomous control to get the best view of the enemy from the sky. The gasoline-electric hybrid propulsion system provides long flight endurance with efficient power consumption. The fundamentals of the multirotor and the conventional fixed-wing aircraft present the theoretical background of the aircraft. The accomplished design consists of high-performance multirotor motors with an efficient gasoline engine. Furthermore, the control system architecture, avionics, and power distribution system presented with addressing cost-effective trending design techniques. The performance of the system has been improved using commercially off-the-shelf (COTS) hardware.",https://ieeexplore.ieee.org/document/9508033/,"2021 IEEE International Conference on Robotics, Automation and Artificial Intelligence (RAAI)",21-23 April 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GCAT52182.2021.9587584,Development of System for Detection and Prevention of Cyber Attacks Using Artifıcial Intelligence Methods,IEEE,Conferences,"Artificial intelligence (AI) technologies have given the cyber security industry a huge leverage with the possibility of having significantly autonomous models that can detect and prevent cyberattacks – even though there still exist some degree of human interventions. AI technologies have been utilized in gathering data which can then be processed into information that are valuable in the prevention of cyberattacks. These AI-based cybersecurity frameworks have commendable scalability about them and are able to detect malicious activities within the cyberspace in a prompter and more efficient manner than conventional security architectures. However, our one or two completed studies did not provide a complete and clear analyses to apply different machine learning algorithms on different media systems. Because of the existing methods of attack and the dynamic nature of malware or other unwanted software (adware etc.) it is important to automatically and systematically create, update and approve malicious packages that can be available to the public. Some of Complex tests have shown that DNN performs maybe can better than conventional machine learning classification. Finally, we present a multiple, large and hybrid DNN torrent structure called Scale-Hybrid-IDS-AlertNet, which can be used to effectively monitor to detect and review the impact of network traffic and host-level events to warn directly or indirectly about cyber-attacks. Besides this, they are also highly adaptable and flexible, with commensurate efficiency and accuracy when it comes to the detection and prevention of cyberattacks.There has been a multiplicity of AI-based cyber security architectures in recent years, and each of these has been found to show varying degree of effectiveness. Deep Neural Networks, which tend to be more complex and even more efficient, have been the major focus of research studies in recent times. In light of the foregoing, the objective of this paper is to discuss the use of AI methods in fighting cyberattacks like malware and DDoS attacks, with attention on DNN-based models.",https://ieeexplore.ieee.org/document/9587584/,2021 2nd Global Conference for Advancement in Technology (GCAT),1-3 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDPC51184.2020.9388192,Development of a Cloud- and Edge-Architecture for adaptive model weight optimization of a CNN exemplified by optical detection of hairpin welding,IEEE,Conferences,"The beginning of a global reorientation towards an increasingly conscientious approach to nature and the human habitat has been accompanied by changes in industry and society. The automotive industry, where a transition from combustion to electrically powered vehicles is currently underway, is also concerned with this change. In addition to increasing the capacity of the battery, improving the efficiency of the electric motor is essential. To achieve these goals, however, new technologies such as hairpins for the stator are needed. An important process step involves the welding of two pairs of hairpins, which often leads to welding defects. Nevertheless, expert knowledge in this field is limited. Optical monitoring of the welding process with the help of a convolutional neural network (CNN) is a good approach. This approach can compensate for the low level of expert knowledge and detects and classifies welding defects directly in the production line. However, the disadvantage of optical monitoring is that production conditions and the surrounding environment change over time. This has an impact on optical detection and can negatively affect the accuracy of a CNN. For example, the camera perspective can change, which has a negative effect on optical quality monitoring. Therefore, this paper presents an approach for monitoring and evaluating the quality of a CNN in a cloud instance online. If a deteriorating quality is detected, the CNN in the cloud is re-trained by continuously collected data and then automatically deployed to the production line. This allows the CNN to adapt to the changing environmental conditions. The present approach is demonstrated and validated with real data of the stator production process. Compared with the current state-of-the-art, this control loop is highly automated and requires a minimum of human intervention.",https://ieeexplore.ieee.org/document/9388192/,2020 10th International Electric Drives Production Conference (EDPC),8-9 Dec. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2010.5580600,Development of a distributed control system for PLC-based applications,IEEE,Conferences,"This paper presents experiments aims to built a platform to monitor and control PLC-based processes over PROFIBUS-DP network. The platform is built using industry-standard off-the-shelf PLCs. Integrated with each PLC are communication processors that can be used for connectivity to the network and to a DP modem. The communication processor module used in this work provides an industrial compatible protocol over PROFIBUS-DP. The mode of communication of the industrial automation control system base on the PROFIBUS agreement where variable frequency drives and motor control centers can incorporate bus technologies, and developed a variable-frequency speed base on PROFIBUS-DP. The system can real-time control and collect data from diversified equipment, and the data get across the PROFIBUS is carried to industrial control computer. After data analysis, system carry on the real-time monitoring to the scene, and realize the automation control. This system is very important to study bus, distributed system, the frequency conversion velocity and PLC. It is channel to research on automation control system.",https://ieeexplore.ieee.org/document/5580600/,2010 International Conference on Machine Learning and Cybernetics,11-14 July 2010,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DASA53625.2021.9682299,Development of a novel Decision-Making tool for vessel efficiency optimization using IoT and DL,IEEE,Conferences,"The era of the Internet of Things and big data has completely changed the landscape of many scientific disciplines. Embracing the big data revolution, machine learning techniques seem to have substituted traditional physics-based models due to their effectiveness and flexibility. According to Rolls-Royce [1], the smart ship will revolutionize the landscape of ship design and operations, the same way smartphones did our daily life. These enhanced data-driven analytics yield substantial benefits in terms of cost savings by limiting the degree to which human factor intervenes to perform certain operations, and also by providing reliable and accurate information in real-time information regarding the operational health of the machinery on board, ensuring the safety of the crew as well as the reliability of the equipment. [2]. Despite its importance, the majority of efforts towards this direction are limited to academic efforts or black-box approaches. In the current work, a distributed software platform that augments the decision-making process within the shipping industry is critically presented. Key technological pillars are Big Data technologies, data fusion, and large-scale deep learning-based processing techniques. Evaluation of the platform, based on actual case data, provides evidence of its capacity for diagnosis and anomaly detection on the operation of critical subsystems.",https://ieeexplore.ieee.org/document/9682299/,2021 International Conference on Decision Aid Sciences and Application (DASA),7-8 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMBC.2016.7591089,Development of a real time activity monitoring Android application utilizing SmartStep,IEEE,Conferences,"Footwear based activity monitoring systems are becoming popular in academic research as well as consumer industry segments. In our previous work, we had presented developmental aspects of an insole based activity and gait monitoring system-SmartStep, which is a socially acceptable, fully wireless and versatile insole. The present work describes the development of an Android application that captures the SmartStep data wirelessly over Bluetooth Low energy (BLE), computes features on the received data, runs activity classification algorithms and provides real time feedback. The development of activity classification methods was based on the the data from a human study involving 4 participants. Participants were asked to perform activities of sitting, standing, walking, and cycling while they wore SmartStep insole system. Multinomial Logistic Discrimination (MLD) was utilized in the development of machine learning model for activity prediction. The resulting classification model was implemented in an Android Smartphone. The Android application was benchmarked for power consumption and CPU loading. Leave one out cross validation resulted in average accuracy of 96.9% during model training phase. The Android application for real time activity classification was tested on a human subject wearing SmartStep resulting in testing accuracy of 95.4%.",https://ieeexplore.ieee.org/document/7591089/,2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),16-20 Aug. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AISP53593.2022.9760604,Development of an Inspection Software towards Detection and Location of Cracks and Foreign Objects in Boiler header or Pipes,IEEE,Conferences,"Industry 4.0 offers a radical transformation to increase cost-effective, flexible, and efficient production of higher-quality fully automated systems by collecting and analyzing data across machines. From the last few decades, power industry has started to focus on real-time systems instead of using static methodology in periodical boiler inspection. The power plant undergoes sudden break down due to cracks and foreign bodies causing huge economic loss to the plant as well as the country. To avoid such unforeseen breakdown, most of the power plants has adopted inspection and monitoring system as a regular solution. Visual inspection is one of the most popular techniques for such inspections using a tiny camera with high-power LEDs (Known as Borescope). But it has several limitations for circumferential (360&#x00B0;) and longitudinal (2000mm) coverage and also equidistance inspection from the center of the header is not possible using a conventional Borescope. A specific Digital Video Recorder (DVR) used for the inspection and monitoring is not sufficient to resolve multipurpose requirements such as position of the foreign body and crack, feature of magnification, and more important is data log including plant information and crack details with images. A real-time inspection module has been developed integrated with robotic (AI) based on computer vision to make the inspection dynamic and fully automated.",https://ieeexplore.ieee.org/document/9760604/,2022 2nd International Conference on Artificial Intelligence and Signal Processing (AISP),12-14 Feb. 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONECCT.2014.6740278,Development of biochip arryer and imaging system for making biochip,IEEE,Conferences,"A bio-chip arrayer system has been developed indigenously for producing bio-chips. The bio-chips are based on use of track etched membranes (TEM) as a novel solid support. TEM are extremely thin (10 micron) and highly microporous membranes. We have tested polycarbonate (PC) TEM by manual spotting of antibodies for the development of the antibody bio-chip for simultaneous estimation of the thyroid related hormones in sera of patients with thyroid disorders. However, manual spotting is a labor-intensive and error prone process. The arrayer has been developed using piezoelectric based pin head for non contact spotting of solution on the porous TEM. This is a computer controlled three axes robotic arm to automate and streamline different processes. It precisely and accurately dispenses few picolitre to nanolitre solution resulting in excellent spot reproducibility and gives uniform spot morphology. It uses real time pressure control and excellent circuit for voltage and pulse width for piezo-dispenser. It has been developed according to the flexible industry standard (25 × 75 mm) glass slide microarray format and can be used with any 3-D (membranes or gel pads) or 2-D (glass) substrates. Real time machine vision system has also been developed to monitor the dispenser performance. The system has been modeled using the neural networks to dispense the different viscous samples to maintain the same spot size. This system includes user friendly control software to define all critical parameters for printing operation. The performance, accuracy and repeatability of the system are evaluated by printing test patterns and quantifying various spot parameters using statistical methods.",https://ieeexplore.ieee.org/document/6740278/,"2014 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)",6-7 Jan. 2014,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TELFOR48224.2019.8971360,Development of intelligent systems and application of gamification in artificial intelligent learning,IEEE,Conferences,"CToday, intelligent systems are used in many fields - medicine, agriculture, transport, telecommunications, industrial process management and control, finance, commerce, the computer game industry and many others. This paper describes a complete way to develop an intelligent software system for simulation and visualization of artificial intelligence algorithms. The system includes artificial intelligence algorithms from basic search strategies and game theory, inference algorithms and knowledge representation models, to advanced search techniques, machine and inductive learning. The application of these algorithms to real everyday problems and the application of this software system as an auxiliary tool for analysis of input data and inference in various fields are presented. The system is also applicable to education in an introductory artificial intelligence course at the university, so the last phase of the research involved the transition of the software system into a game-based tool and application of gamification.",https://ieeexplore.ieee.org/document/8971360/,2019 27th Telecommunications Forum (TELFOR),26-27 Nov. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP48927.2020.9367369,Development of simulator for efficient aquaculture of Sillago japonica using reinforcement learning,IEEE,Conferences,"Recently, the situation in the Japanese fishing industry has become critical, resulting in one of the most significant food issues in Japan. Aquaculture technology is expected to be a solution to this problem. Sillago japonica is a fish that inhabits shallow waters in parts of Asia, and large Sillago japonica is very expensive. Therefore, we believe that aquaculture of this fish would help revitalize the fishing industry in Japan. However, aquaculture requires considerable manual labor. Hence, we need to introduce new technologies for the aquaculture of Sillago japonica. Specifically, we have been developing two systems to improve efficiency and reduce costs of this aquaculture: an environment control system and an automatic feeding system. The former is to maintain favorable environment conditions for the fish in the aquaculture tank. The latter is for optimal feeding of the fish. In this paper, we describe the development of an automatic feeding system using artificial intelligence (AI). This system includes four processes: image input, image recognition, feeder control, and feeding action. We have adopted AI technologies to assist in the second and third processes. Although these two processes can be implemented together, it is easier for the AI to learn them as two separate processes. In particular, the second (image recognition) process uses supervised learning, and the third (feeder control) process uses reinforcement learning. However, it is impractical to train the AI in the third process in a real-world aquaculture environment that sustains many failures. Therefore, we have developed an aquaculture simulator to facilitate AI learning of the feeder control process. Additionally, we performed an experiment to validate our simulator using the number of feeders and the number of fish as a parameter.",https://ieeexplore.ieee.org/document/9367369/,2020 International Conference on Image Processing and Robotics (ICIP),6-8 March 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OCEANS-Genova.2015.7271691,DexROV: Enabling effective dexterous ROV operations in presence of communication latency,IEEE,Conferences,"Subsea interventions in the oil & gas industry as well as in other domains such as archaeology or geological surveys are demanding and costly activities for which robotic solutions are often deployed in addition or in substitution to human divers - contributing to risks and costs cutting. The operation of ROVs (Remotely Operated Vehicles) nevertheless requires significant off-shore dedicated manpower to handle and operate the robotic platform and the supporting vessel. In order to reduce the footprint of operations, DexROV proposes to implement and evaluate novel operation paradigms with safer, more cost effective and time efficient ROV operations. As a keystone of the proposed approach, manned support will in a large extent be delocalized within an onshore ROV control center, possibly at a large distance from the actual operations, relying on satellite communications. The proposed scheme also makes provision for advanced dexterous manipulation and semi-autonomous capabilities, leveraging human expertise when deemed useful. The outcomes of the project will be integrated and evaluated in a series of tests and evaluation campaigns, culminating with a realistic deep sea (1,300 meters) trial in the Mediterranean sea.",https://ieeexplore.ieee.org/document/7271691/,OCEANS 2015 - Genova,18-21 May 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICT52872.2021.00047,Digital Knowledge Base for Industrial Augmented Reality Systems Based on Semantic Technologies,IEEE,Conferences,"Augmented Reality is a technology that offers enormous potential in the industry. Due to a lack of expertise, however, companies are facing various challenges in exploiting this potential. This includes the demand-oriented configuration of the AR system. Depending on the planned use case and company-specific influencing factors and general conditions, suitable AR devices are to be identified and functionalities need to be purposefully selected. The necessary knowledge is implicitly available in numerous sources and difficult to access for companies. In the scope of this work, therefore, a machine-readable knowledge base is developed, in which the knowledge relevant for the AR system configuration is consolidated and formalized. Four aspects of knowledge are considered: The AR system structure including technical and functional components; Potential influences through ambient factors; Experience and application knowledge; The specification of specific AR devices. The knowledge base is realized based on the Web Ontology Language (OWL) and thus enables a digital and partially automated processing of the knowledge using software tools and algorithms. In combination with Artificial Intelligence solutions, the developed knowledge base can be transferred to other systems in the future and provide a powerful tool for system configuration.",https://ieeexplore.ieee.org/document/9476944/,2021 4th International Conference on Information and Computer Technologies (ICICT),11-14 March 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCT53315.2021.9711888,Digital Triplet of a Industrial Multistage Centrifugal Pump,IEEE,Conferences,"As industries are increasing worldwide it is desired to combine existing technology with Machine Learning Features in order to enhance the functionality to higher cognitive level. Digital Twins places a major role in such an integration process to meet the requirement of industrial revolution. Digital Triplet is one such improvement of digital twin methodology which is obtained by introducing an intelligent activity layer, thereby meeting standard requirement of Industry 4.0. Many research activities are being carried out based on development, implementation and testing for involving the adopted technique in the industrial applications. This paper explains the classification model of combined ML and System Digital Twin. Also, a security feature was enabled by introducing a sensor value and machine status. Performance measures are carried out to test the advantages of ML module implemented and also real time applications of this process also validated.",https://ieeexplore.ieee.org/document/9711888/,2021 4th International Conference on Computing and Communications Technologies (ICCCT),16-17 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IAI50351.2020.9262203,Digital Twin Enabled Smart Control Engineering as an Industrial AI: A New Framework and Case Study,IEEE,Conferences,"In Industry 4.0, the increasing complexity of industrial systems introduces unknown dynamics that affect the performance of manufacturing processes. Thus, Digital Twin appears as a breaking technology to develop virtual representations of any complex system design, analysis, and behavior prediction tasks to enhance the system understanding via enabling capabilities like real-time analytics, or Smart Control Engineering. In this paper, a novel framework is proposed for the design and implementation of Digital Twin applications to the development of Smart Control Engineering. The framework involve the steps of system documentation, Multidomain Simulation, Behavioral Matching, and real-time monitoring, which is applied to develop the Digital Twin for a real-time vision feedback temperature uniformity control. The obtained results show that Digital Twin is a fundamental part of the transformation into Industry 4.0.",https://ieeexplore.ieee.org/document/9262203/,2020 2nd International Conference on Industrial Artificial Intelligence (IAI),23-25 Oct. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DESSERT.2018.8409186,Digitization of the economy of Ukraine: Strategic challenges and implementation technologies,IEEE,Conferences,"The main directions, challenges, threats of digitization of the national economy of Ukraine have been considered in the paper. The attention is focused on the found weaknesses and the imperfection of the strategy and the state policy of digitization of Ukraine's economy. The authors have proven the potential and new possibilities of solving public finance management problems with the usage of blockchain technology. It has been justified that activation of transformation processes in the real economy sector due to the introduction of Industry 4.0 concept is important for Ukraine. The paper reveals basic principles and technologies, the experience of the European Union, and characterizes Industry 4.0 view in Ukraine. The development of the latest financial technologies - FinTech - has been recognized as the driver of digital transformation of financial services. The types of FinTech innovations, the features of increasing competition between FinTech companies and traditional financial intermediaries, the tendencies of FinTech development in Ukraine have been characterized.",https://ieeexplore.ieee.org/document/8409186/,"2018 IEEE 9th International Conference on Dependable Systems, Services and Technologies (DESSERT)",24-27 May 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICST.2019.00029,Directing a Search Towards Execution Properties with a Learned Fitness Function,IEEE,Conferences,"Search based software testing is a popular and successful approach both in academia and industry. SBST methods typically aim to increase coverage whereas searching for executions with specific properties is largely unresearched. Fitness functions for execution properties often possess search landscapes that are difficult or intractable. We demonstrate how machine learning techniques can convert a property that is not searchable, in this case crashes, into one that is. Through experimentation on 6000 C programs drawn from the Codeflaws repository, we demonstrate a strong, program independent correlation between crashing executions and library function call patterns within those executions as discovered by a neural net. We then exploit the correlation to produce a searchable fitness landscape to modify American Fuzzy Lop, a widely used fuzz testing tool. On a test set of previously unseen programs drawn from Codeflaws, a search strategy based on a crash targeting fitness function outperformed a baseline in 80.1% of cases. The experiments were then repeated on three real world programs: the VLC media player, and the libjpeg and mpg321 libraries. The correlation between library call traces and crashes generalises as indicated by ROC AUC scores of 0.91, 0.88 and 0.61. The produced search landscape however is not convenient due to plateaus. This is likely because these programs do not use standard C libraries as often as do those in Codeflaws. This limitation can be overcome by considering a more powerful observation domain and a broader training corpus in future work. Despite limited generalisability of the experimental setup, this research opens new possibilities in the intersection of machine learning, fitness functions, and search based testing in general.",https://ieeexplore.ieee.org/document/8730160/,"2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)",22-27 April 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCECE.2019.8861718,Distributed Optimal Power Flow for Electric Power Systems with High Penetration of Distributed Energy Resources,IEEE,Conferences,"Optimization technology is developing to the point of becoming a cost-effective enabler of increased power transfer asset utilization. This paper presents a smart decomposition technique for the traditional optimal power flow (OPF) algorithm to allow distributed optimal power flow (DOPF) calculations without relying on a centralized controller. Hence, it develops a feasible distributed architectures for the electric power industry. The proposed method is implemented using Monte Carlo Tree Search based reinforcement learning (MCTS-RL) algorithm. This reduces computational complexity and allows to avoid difficulties associated with stochastic modeling often used to capture the random nature of distributed energy resources (DER) units and loads. The efficiency of the optimization process is improved when the DOPF reflects the fast response capability of the optimal solution. This contribution provides results for a real-time dispatchable resource and demonstrates the flexibility of RL to adapt to changes of system states, ultimately reducing the generation cost while maintaining the system security constraints.",https://ieeexplore.ieee.org/document/8861718/,2019 IEEE Canadian Conference of Electrical and Computer Engineering (CCECE),5-8 May 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPDPS.2018.00068,Do Developers Understand IEEE Floating Point?,IEEE,Conferences,"Floating point arithmetic, as specified in the IEEE standard, is used extensively in programs for science and engineering. This use is expanding rapidly into other domains, for example with the growing application of machine learning everywhere. While floating point arithmetic often appears to be arithmetic using real numbers, or at least numbers in scientific notation, it actually has a wide range of gotchas. Compiler and hardware implementations of floating point inject additional surprises. This complexity is only increasing as different levels of precision are becoming more common and there are even proposals to automatically reduce program precision (reducing power/energy and increasing performance) when results are deemed """"good enough.'"""" Are software developers who depend on floating point aware of these issues? Do they understand how floating point can bite them? To find out, we conducted an anonymous study of different groups from academia, national labs, and industry. The participants in our sample did only slightly better than chance in correctly identifying key unusual behaviors of the floating point standard, and poorly understood which compiler and architectural optimizations were non-standard. These surprising results and others strongly suggest caution in the face of the expanding complexity and use of floating point arithmetic.",https://ieeexplore.ieee.org/document/8425212/,2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS),21-25 May 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSE.2014.273,Dolphin: Ultrasonic-Based Gesture Recognition on Smartphone Platform,IEEE,Conferences,"User experience of smart mobile devices can be improved in numerous scenarios with the assist of in-air gesture recognition. Most existing methods proposed by industry and academia are based on special sensors. On the contrary, a special sensor-independent in-air gesture recognition method named Dolphin is proposed in this paper which can be applied to off-the-shelf smart devices directly. The only sensors Dolphin needs are the loudspeaker and microphone embedded in the device. Dolphin emits a continuous 21 KHz tone by the loudspeaker and receive the gesture-reflecting ultrasonic wave by the microphone. The gesture performed is encoded into the reflected ultrasonic in the form of Doppler shift. By combining manual recognition and machine leaning methods, Dolphin extracts features from Doppler shift and recognizes a rich set of pre-defined gestures with high accuracy in real time. Parameter selection strategy and gesture recognition under several scenarios are discussed and evaluated in detail. Dolphin can be adapted to multiple devices and users by training using machine learning methods.",https://ieeexplore.ieee.org/document/7023784/,2014 IEEE 17th International Conference on Computational Science and Engineering,19-21 Dec. 2014,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EHB50910.2020.9280165,Drivers’ Drowsiness Detection and Warning Systems for Critical Infrastructures,IEEE,Conferences,"Road traffic accidents, due to driver fatigue, tend to inflict high mortality rates comparing with accidents involving rested drivers. Currently there is an emerging automotive industry trend towards equipping vehicles with various driver-assistance technologies. Third parties also started producing complementary systems, including ones that can detect the driver's degree of fatigue, but this growing field requires further research and development. The main purpose of this paper is the development and implementation of a system capable to detecting and alert, in real-time, the driver's level of fatigue. A system like this is expected to make the driver aware of the assumed danger when his level of driving and taking decisions are reduced and is indicating a sleep break as the necessary approach. By monitoring the state of the human eyes, it is assumed that the signs of driver fatigue can be detected early enough to prevent a possible road accident, which could result in severe injuries or ultimately, in fatalities. Hence, in this work the authors are focused on the video monitoring of the driver face, especially on his eyes position in time, when open or closed, using a machine learning object detection algorithm, the Haar Cascade. Two pretrained Haar classifiers, a face cascade, and an eye cascade were imported from the OpenCV GitHub repository. The OpenCV library, as well as other required packages, were installed on a BeagleBone Black Wireless development board. The software implementation, in order to achieve the driver's drowsiness detection, was made through the Python software program. The proposed system manages to alert if the eyes of the driver are being kept closed for more than a certain amount of time by triggering a set of warning lights and sounds. The large-scale implementation of this type of system will drop the number of road accidents caused by the drivers' fatigue, thus saving countless lives and bringing a reduction of the socio-economic costs associated with these tragic events.",https://ieeexplore.ieee.org/document/9280165/,2020 International Conference on e-Health and Bioengineering (EHB),29-30 Oct. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigDataCongress.2017.44,Drowsy Driving Warning System Based on GS1 Standards with Machine Learning,IEEE,Conferences,"Drowsy driving is the primary cause of motor vehicle accidents and is a risk factor that leads to the loss of human life, remaining as a challenge for the global automotive industry. Recently, drowsy monitoring system has been actively studied for prediction system based machine learning. However, the challenges of automotive real-time constraints and flexibility should be taken into consideration against a large amount of heterogeneous data from vehicle network and other device. To solve this problem, we propose drowsy monitoring system based machine learning using GS1 standard. First, vehicle motion data is defined and modeled using the GS1 standard language for drowsy predict. Second, we propose an optimal algorithm selection and detail architecture for automotive real-time environments through machine learning algorithms (KNN, Naïve Bayes, Logistic Regression) and deep learning algorithms (RNN-LSTM). Finally, we describe system-wide integration and implementation through the open source hardware Raspberry Pi and the machine learning SW framework. We provide optimal LSTM architecture and implementation that takes into account the real-time environmental conditions and how to improve the readability and usability of the vehicle motion data. We also share the rapid prototyping methodology case of connected car systems without other sensor devices.",https://ieeexplore.ieee.org/document/8029337/,2017 IEEE International Congress on Big Data (BigData Congress),25-30 June 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSAA53316.2021.9564245,Dynamic Graph Convolutional LSTM application for traffic flow estimation from error-prone measurements: results and transferability analysis,IEEE,Conferences,"The technological advances in the transportation and automotive industry led to the use of new types of sensing systems more cost-effective and adapted to large-scale dense deployment. Those sensing techniques allow continuously gathering traffic measurements times series in different geospatial locations. The accuracy of the obtained raw measurements is often hindered by different factors related to the sensing environment and the sensing process itself and thus fail to capture the short-term traffic variations crucial for real-time traffic monitoring. In this paper, we propose the DGC-LSTM model for area-wide traffic estimation from error-prone measurements time series. The backbone of the DGC-LSTM model is a graph convolutional Long Short Term Memory model with a dynamic adjacency matrix. The adjacency matrix is learned and optimized during the model training. The adjacency matrix values are estimated from the set of contextual features that impact the dynamicity of the dependencies in both the spatial and temporal dimensions. Experiments on a realistic synthetic labelled Bluetooth counts dataset is used for model evaluation. Lastly, we highlight the importance of transfer learning methods to improve the model applicability by ensuring model adaptation to the new deployment site while avoiding the extensive data-labelling effort.",https://ieeexplore.ieee.org/document/9564245/,2021 IEEE 8th International Conference on Data Science and Advanced Analytics (DSAA),6-9 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA46521.2020.9211946,Dynamic Process Planning using Digital Twins and Reinforcement Learning,IEEE,Conferences,"In order to enable changeable production of Industry 4.0 applications, a production system should respond to unpredictable changes quickly and adequately. This requires process planning to be performed based on the real time operating conditions and dynamic changes to be handled with cognitive skills. To meet this demand, we present a process planning approach using digital twins and reinforcement learning to derive near-optimal process plans. The digital twins enable access to real-time information about the production system. They also constitute the environment for training the agent of the reinforcement learning method. The environment works as a virtual plant, containing the attributes of the product and resources, and uses simulation models of the resources to calculate the reward for an action in terms of reinforcement learning. Reinforcement learning enables our approach to derive process plans via trial and error. Besides the virtual plant, our approach has a planner, which plays the role of the agent to derive near-optimal plans by trying different actions in the virtual plant, and observes the rewards. We apply the Q-learning algorithm to derive near optimal process plans. The evaluation results show that our approach is able to derive near-optimal process plans for different problem sizes. The evaluation also demonstrated the planner’s ability to identify by itself which action to take in which situation. Consequently, no modeling of the preconditions and effects of the actions is necessary.",https://ieeexplore.ieee.org/document/9211946/,2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),8-11 Sept. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CloudNet.2014.6968974,Dynamic allocation and efficient distribution of data among multiple clouds using network coding,IEEE,Conferences,"Distributed storage has attracted large interest lately from both industry and researchers as a flexible, cost-efficient, high performance, and potentially secure solution for geographically distributed data centers, edge caching or sharing storage among users. This paper studies the benefits of random linear network coding to exploit multiple commercially available cloud storage providers simultaneously with the possibility to constantly adapt to changing cloud performance in order to optimize data retrieval times. The main contribution of this paper is a new data distribution mechanisms that cleverly stores and moves data among different clouds in order to optimize performance. Furthermore, we investigate the trade-offs among storage space, reliability and data retrieval speed for our proposed scheme. By means of real-world implementation and measurements using well-known and publicly accessible cloud service providers, we can show close to 9x less network use for the adaptation compared to more conventional dense recoding approaches, while maintaining similar download time performance and the same reliability.",https://ieeexplore.ieee.org/document/6968974/,2014 IEEE 3rd International Conference on Cloud Networking (CloudNet),8-10 Oct. 2014,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAC353642.2021.9697306,E-fresh : Computer Vision and IOT Framework for Fruit Freshness Detection,IEEE,Conferences,"The food industry is expanding every day and it is crucial to maintain the required standards which impact their market value. To maintain these standards, manpower is used which is inconsistent, expensive, and time-consuming. With the help of automation of classification, we can speed up this process with less expensive resources using Computer Vision along with the Internet of Things(IoT). The evolution of the Internet of Things(IoT) has played an important role in making the devices smarter and more connected. The large amount of data collected from these devices can be used for data analysis which helps the industries to plan their future decisions. This paper proposes the idea of implementing an infrastructure having a micro-controller that would accurately segregate three kinds of fruits into two categories i.e. Fresh and Rotten. The classification will be done with the help of the Deep Learning algorithm, Convolutional Neural Network(CNN) by using a dataset containing images of those three fruits and also considering the input from the sensors which include sensors such as alcohol sensor, methane sensor, etc. The infrastructure proposed in the paper considers the standards of Industry 4.0 which implies the real-world implementation of the infrastructure.",https://ieeexplore.ieee.org/document/9697306/,"2021 International Conference on Advances in Computing, Communication, and Control (ICAC3)",3-4 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCNT51525.2021.9580072,Early Detection of Disease in Rice Paddy: A Deep Learning based Convolution Neural Networks Approach,IEEE,Conferences,"The agriculture industry faces huge economic losses due to bacterial, viral or fungal infections in the crops due to which farmers lose 15 to 20% of their total profit every year. India is the second largest producer of rice and a leading exporter of the same in the global market. Thus, early detection of diseases in essential crops is a significant area of research in order to prevent further damage to them. The widespread development of Deep Learning makes it possible to achieve the goal of disease detection in crops. The novelty of this work is early detection of Brown spot disease in rice paddy using Convolution Neural Networks. The area of the disease affected was also found to optimize the usage of fertilizers. This work makes use of Image recognition and preprocessing algorithm based on real time data. Data preprocessing and feature extraction has been done using a self-designed image-processing tool. Tensor flow and Keras framework has been implemented on both training and testing data which was collected manually from rice fields. The proposed model achieved an accuracy of 97.32%.",https://ieeexplore.ieee.org/document/9580072/,2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT),6-8 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSIT49958.2020.9321954,Eco-friendly Home Automation System Implemented Using Machine Learning Algorithms,IEEE,Conferences,"This paper presents the exemplary system of house automation implemented with the use of Industry 4.0 inventions. The proposed system tries to benefit from weather conditions to heat or cool the house without any electrical heaters or air conditioners. It is implemented with the aid of Machine Learning algorithms, the Internet of Things, and Cloud technology. The paper contains a technical and practical description of the system, the results of the real use, and proposed extensions that can improve the presented solution.",https://ieeexplore.ieee.org/document/9321954/,2020 IEEE 15th International Conference on Computer Sciences and Information Technologies (CSIT),23-26 Sept. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCloud-EdgeCom49738.2020.00050,Edge Computing-based 3D Pose Estimation and Calibration for Robot Arms,IEEE,Conferences,"Industrial robots are widely used in current production lines, and complex pipeline processes, especially those with different assembly requirements, are designed for intelligent manufacturing in the era of industry 4.0. During the new crown epidemic, a large number of car companies used the production line to transform production of medical materials such as masks and protective clothing, which provided a strong guarantee for fighting the epidemic. In this scenario, a pipeline is often assembled from robotic arms from multiple suppliers. The traditional methods is complex and takes a lot of time. In this paper, we propose a novel deep learning based robot arm 3D pose estimation and calibration model with simple Kinect stereo cameras which can be deployed on light-weight edge computing systems. The light-weight deep CNN model can detection 5 predefined key points based on RGB-D data. In this way, when the assembly line composed of different robot arms needs to be reassembled, our model can quickly provide the robot’s pose information without additional tuning processes. Testing in Webots with Rokae xb4 robot arm model shows that our model can quickly estimate the key point of the robot arm.",https://ieeexplore.ieee.org/document/9170983/,2020 7th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2020 6th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom),1-3 Aug. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISAECT50560.2020.9523700,Edge-Cloud Architectures Using UAVs Dedicated To Industrial IoT Monitoring And Control Applications,IEEE,Conferences,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud.",https://ieeexplore.ieee.org/document/9523700/,2020 International Symposium on Advanced Electrical and Communication Technologies (ISAECT),25-27 Nov. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIIoT52608.2021.9454247,"Education System for Bangladesh Using Augmented Reality, Virtual Reality and Artificial Intelligence",IEEE,Conferences,"This paper presents an innovative application for students to study and understand their coursework without any external help from a private tutor. The system uses Augmented Reality (AR) to provide hands on experience for the students. The presented system also supports Virtual Reality (VR) that enriches this process and immerses the users into a fun and productive learning experience. Moreover, the system introduces an industry first Artificial intelligence (AI) based study guide that directs students towards necessary topics and advises them on what to improve on. All the core system features are implemented and are accessible via two mediums. First, a standalone mobile phone application. Second, a dedicated web portal.",https://ieeexplore.ieee.org/document/9454247/,2021 IEEE World AI IoT Congress (AIIoT),10-13 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEETS.2016.7583860,Efficiency optimization of induction motor drive using Artificial Neural Network,IEEE,Conferences,"Induction motors are the workhorse of industry, have good efficiency at rated load, but long duration usage of IM at partial load shows poor efficiency which leads to waste in energy and revenue as well. These motors are reliable, robust, high power/mass ratio and economic, hence replaced all other motors in the industry, so even minute increment in induction motor efficiency can have a major impact on consumption of electricity and saving of revenue, globally. This paper utilizes, a combination of two key concepts of efficiency optimization-loss model control (LMC) and search control (SC) for efficient operation of induction motors used in various industrial applications, in aforesaid load condition. At first, to estimate optimal Ids values for various load conditions, an optimal Ids expression in terms of machine parameters and load parameters, based on machine loss model in d-q frame along with classical optimization technique, is utilized. Secondly, an offline trained artificial neural network (ANN) controller is used to reproduce the optimal Ids values, in run-time load condition. This eliminates run-time computations and perturbation for optimal flux, as in conventional SC method. The (ANN) optimal controller is designed for optimal Ids as output, while providing load torque and speed information as inputs. The training is performed in MATLAB and good accuracy of the training model is seen. Dynamic and steady-state performances are compared for proposed optimal (optimal Ids) operations and conventional vector operations (constant Ids), with the help of a simulation model, developed in MATLAB. Excellent dynamic response in load transients as well as superior efficiency performance (1- 18%) at steady-state, for a wide range of speed and torque in simulation is attained. Assimilated with similar earlier work, the proposed methodology offers effortless implementation in real-time industrial facilities, ripple free operations, fast response and higher energy savings.",https://ieeexplore.ieee.org/document/7583860/,2016 International Conference on Energy Efficient Technologies for Sustainability (ICEETS),7-8 April 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSME46990.2020.00082,Efficient Bug Triage For Industrial Environments,IEEE,Conferences,"Bug triage is an important task for software maintenance, especially in the industrial environment, where timely bug fixing is critical for customer experience. This process is usually done manually and often takes significant time. In this paper, we propose a machine-learning-based solution to address the problem efficiently. We argue that in the industrial environment, it is more suitable to assign bugs to software components (then to responsible developers) than to developers directly. Because developers can change their roles in industry, they may not oversee the same software module as before. We also demonstrate experimentally that assigning bugs to components rather than developers leads to much higher accuracy. Our solution is based on text-projection features extracted from bug descriptions. We use a Deep Neural Network to train the classification model. The proposed solution achieves state-of-the-art performance based on extensive experiments using multiple data sets. Moreover, our solution is computationally efficient and runs in near real-time.",https://ieeexplore.ieee.org/document/9240673/,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),28 Sept.-2 Oct. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/eScience.2019.00047,Efficient Runtime Capture of Multiworkflow Data Using Provenance,IEEE,Conferences,"Computational Science and Engineering (CSE) projects are typically developed by multidisciplinary teams. Despite being part of the same project, each team manages its own workflows, using specific execution environments and data processing tools. Analyzing the data processed by all workflows globally is a core task in a CSE project. However, this analysis is hard because the data generated by these workflows are not integrated. In addition, since these workflows may take a long time to execute, data analysis needs to be done at runtime to reduce cost and time of the CSE project. A typical solution in scientific data analysis is to capture and relate the data in a provenance database while the workflows run, thus allowing for data analysis at runtime. However, the main problem is that such data capture competes with the running workflows, adding significant overhead to their execution. To mitigate this problem, we introduce in this paper a system called ProvLake, which adopts design principles for providing efficient distributed data capture from the workflows. While capturing the data, ProvLake logically integrates and ingests them into a provenance database ready for analyses at runtime. We validated ProvLake in a real use case in the O&G industry encompassing four workflows that process 5 TB datasets for a deep learning classifier. Compared with Komadu, the closest solution that meets our goals, our approach enables runtime multiworkflow data analysis with much smaller overhead, such as 0.1%.",https://ieeexplore.ieee.org/document/9041720/,2019 15th International Conference on eScience (eScience),24-27 Sept. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICWS49710.2020.00067,Efficient Search for Moving Object Devices in Internet of Things Networks,IEEE,Conferences,"IoT search engines have attracted increasing attention from both academia and industry, since they are capable of crawling heterogeneous data sources in highly dynamic environment. To process tens of thousands of spatial-temporal-keyword queries per second, query efficiency and communication cost in IoT search engines become critical issues. To address these challenges, caching mechanisms in collaborative edge-cloud computing architecture, which can implement the caching paradigm in cloud for frequent n-hop neighboring activity regions, is proposed in this paper. Thereafter, frequent query results can be achieved quickly leveraging the spatial-temporal-keyword filtering index of n-hop neighbor regions through modeling keywords relevance and uncertain traveling time. Besides, we adopt STK-tree proposed previously to directly answer non-frequent queries. Extensive experiments on real-life dataset demonstrate that our method outperforms the state-of-the-art's techniques in terms of the reduction of the query time and the number of transmitted messages.",https://ieeexplore.ieee.org/document/9284140/,2020 IEEE International Conference on Web Services (ICWS),19-23 Oct. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SC.2008.5213940,Efficient management of data center resources for Massively Multiplayer Online Games,IEEE,Conferences,"Today's massively multiplayer online games (MMOGs) can include millions of concurrent players spread across the world. To keep these highly-interactive virtual environments online, a MMOG operator may need to provision tens of thousands of computing resources from various data centers. Faced with large resource demand variability, and with misfit resource renting policies, the current industry practice is to maintain for each game tens of self-owned data centers. In this work we investigate the dynamic resource provisioning from external data centers for MMOG operation. We introduce a novel MMOG workload model that represents the dynamics of both the player population and the player interactions. We evaluate several algorithms, including a novel neural network predictor, for predicting the resource demand. Using trace-based simulation, we evaluate the impact of the data center policies on the resource provisioning efficiency; we show that dynamic provisioning can be much more efficient than its static alternative.",https://ieeexplore.ieee.org/document/5213940/,SC '08: Proceedings of the 2008 ACM/IEEE Conference on Supercomputing,15-21 Nov. 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CASE48305.2020.9249228,Efficiently Learning a Distributed Control Policy in Cyber-Physical Production Systems Via Simulation Optimization,IEEE,Conferences,"The manufacturing industry is becoming more dynamic than ever. The limitations of non-deterministic network delays and real-time requirements call for decentralized control. For such dynamic and complex systems, learning methods stand out as a transformational technology to have a more flexible control solution. Using simulation for learning enables the description of highly dynamic systems and provides samples without occupying a real facility. However, it requires prohibitively expensive computation. In this paper, we argue that simulation optimization is a powerful tool that can be applied to various simulation-based learning processes for tremendous effects. We proposed an efficient policy learning framework, ROSA (Reinforcement-learning enhanced by Optimal Simulation Allocation), with unprecedented integration of learning, control, and simulation optimization techniques, which can drastically improve the efficiency of policy learning in a cyber-physical system. A proof-of-concept is implemented on a conveyer-switch network, demonstrating how ROSA can be applied for efficient policy learning, with an emphasis on the industrial distributed control system.",https://ieeexplore.ieee.org/document/9249228/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VLSI-DAT.2019.8741637,Embedded Memories for Silicon-In-Package: Optimization of Memory Subsystem from IoT to Machine Learning,IEEE,Conferences,"Traditional memory subsystem consisting of SRAM, DRAM and SSD/HDD has served the needs of electronics industry for decades. With the rapid increase in Graphics, IoT and Machine Learning applications, several new memories have been innovated to optimize the memory hierarchy. Optane memory is deployed to close the performance/area gap between DRAM and SSD. Similarly, embedded DRAM inserted in products as L4 Cache to close the gap between SRAM and DRAM. Lately, MRAM and ReRAM are also brought to reality to target wide range of applications, covering embedded Non-Volatile Memory and Flash buffer at platform level. This talk will go through these innovate memories and how one needs to optimize the memory subsystem for the best application. There's no memory that fits all, but design and architecture opportunities exist for targeted applications.",https://ieeexplore.ieee.org/document/8741637/,"2019 International Symposium on VLSI Design, Automation and Test (VLSI-DAT)",22-25 April 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VLSI-TSA.2019.8804633,Embedded Memories for Silicon-ln-Package: Optimization of Memory Subsystem from loT to Machine Learning,IEEE,Conferences,"Traditional memory subsystem consisting of SRAM, DRAM and SSD/HDD has served the needs of electronics industry for decades. With the rapid increase in Graphics, loT and Machine Learning applications, several new memories have been innovated to optimize the memory hierarchy. Optane memory is deployed to close the performance/area gap between DRAM and SSD. Similarly, embedded DRAM inserted in products as L4 Cache to close the gap between SRAM and DRAM. Lately, MRAM and ReRAM are also brought to reality to target wide range ofapplications, covering embedded Non-Volatile Memory and Flash buffer at platform level. This talk will go through these innovate memories and how one needs to optimize the memory subsystem for the best application. There's no memory that fits all, but design and architecture opportunities exist for targeted applications.",https://ieeexplore.ieee.org/document/8804633/,"2019 International Symposium on VLSI Technology, Systems and Application (VLSI-TSA)",22-25 April 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2017.8258076,Empirical evaluations of active learning strategies in legal document review,IEEE,Conferences,"One type of machine learning, text classification, is now regularly applied in the legal matters involving voluminous document populations because it can reduce the time and expense associated with the review of those documents. One form of machine learning - Active Learning - has drawn attention from the legal community because it offers the potential to make the machine learning process even more effective. Active Learning, applied to legal documents, is considered a new technology in the legal domain and is continuously applied to all documents in a legal matter until an insignificant number of relevant documents are left for review. This implementation is slightly different than traditional implementations of Active Learning where the process stops once achieving acceptable model performance. The purpose of this paper is twofold: (i) to question whether Active Learning actually is a superior learning methodology and (ii) to highlight the ways that Active Learning can be most effectively applied to real legal industry data. Unlike other studies, our experiments were performed against large data sets taken from recent, real-world legal matters covering a variety of areas. We conclude that, although these experiments show the Active Learning strategy popularly used in legal document review can quickly identify informative training documents, it becomes less effective over time. In particular, our findings suggest this most popular form of Active Learning in the legal arena, where the highest-scoring documents are selected as training examples, is in fact not the most efficient approach in most instances. Ultimately, a different Active Learning strategy may be best suited to initiate the predictive modeling process but not to continue through the entire document review.",https://ieeexplore.ieee.org/document/8258076/,2017 IEEE International Conference on Big Data (Big Data),11-14 Dec. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VLSITechnology18217.2020.9265031,Empowering Next-Generation Applications through FLASH Innovation,IEEE,Conferences,"The flash industry has continuously produced game-changing innovations in density, latency, and form factors resulting in large cost-performance benefits. To address the wide spectrum of storage demands coming from phone/IoT devices, mobile compute, up to data centers, new flash architectures are essential to handle these next generation applications. Future technology must include not only new architectures and more layers in flash chip designs, but also a roadmap for QLC flash and beyond, new memories, new classes of SSDs, and new software technologies. They must all come together to enable and accelerate the next wave of applications including the real-time analytics, AI (Artificial Intelligence)/ML (Machine Learning), high-performance computing, IoT, and virtual and augmented reality.",https://ieeexplore.ieee.org/document/9265031/,2020 IEEE Symposium on VLSI Technology,16-19 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCGRID.2017.22,Energy Model for Low-Power Cluster,IEEE,Conferences,"Energy efficiency in high performance computing (HPC) systems is a relevant issue nowadays, which is approached from multiple edges and components (network, I/O, resource management, etc). HPC industry turned its focus towards embedded and low-power computational infrastructures (of RISC architecture processors) to improve energy efficiency, therefore, we use an ARM-based cluster, known as millicluster, designed to achieve high energy efficiency with low power. We provide a model for energy consumption estimation based on experimental data, obtained of measurements performed during a benchmarking process that represents a real-world workload, such as scientific computing algorithms of artificial intelligence. The energy model enables power prediction of tasks in low-power nodes with high accuracy, and its implementation in a job scheduling algorithm of HPC, facilitates the optimization of energy consumption and performance metrics at the same time.",https://ieeexplore.ieee.org/document/7973809/,"2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)",14-17 May 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICE/ITMC49519.2020.9198492,Enhancing Cognition for Digital Twins,IEEE,Conferences,"In the era of Industry 4.0, Digital Twins (DTs) pave the way for the creation of the Cognitive Factory. By virtualizing and twinning information stemming from the real and the digital world, it is now possible to connect all parts of the production process by having virtual copies of physical elements interacting with each other in the digital and physical realms. However, this alone does not imply cognition. Cognition requires modelling not only the physical characteristics but also the behavior of production elements and processes. The latter can be founded upon data-driven models produced via Data Analytics and Machine Learning techniques, giving rise to the so-called Cognitive (Digital) Twin. To further enable the Cognitive Factory, a novel concept, dubbed as Enhanced Cognitive Twin (ECT), is proposed in this paper as a way to introduce advanced cognitive capabilities to the DT artefact that enable supporting decisions, with the end goal to enable DTs to react to inner or outer stimuli. The Enhanced Cognitive Twin can be deployed at different hierarchical levels of the production process, i.e., at sensor-, machine-, process-, employee- or even factory-level, aggregated to allow both horizontal and vertical interplay. The ECT notion is proposed in the context of process industries, where cognition is particularly important due to the continuous, non-linear, and varied nature of the respective production processes.",https://ieeexplore.ieee.org/document/9198492/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECS53924.2021.9665527,Enhancing Security in the Industrial IoT Sector using Quantum Computing,IEEE,Conferences,"The development of edge computing and machine learning technologies have led to the growth of Industrial IoT systems. Autonomous decision making and smart manufacturing are flourishing in the current age of Industry 4.0. By providing more compute power to edge devices and connecting them to the internet, the so-called Cyber Physical Systems are prone to security threats like never before. Security in the current industry is based on cryptographic techniques that use pseudorandom number keys. Keys generated by a pseudo-random number generator pose a security threat as they can be predicted by a malicious third party. In this work, we propose a secure Industrial IoT Architecture that makes use of true random numbers generated by a quantum random number generator (QRNG). CITRIOT's FireConnect IoT node is used to show the proof of concept in a quantum-safe network where the random keys are generated by a cloud based quantum device. We provide an implementation of QRNG on both real quantum computer and quantum simulator. Then, we compare the results with pseudorandom numbers generated by a classical computer.",https://ieeexplore.ieee.org/document/9665527/,"2021 28th IEEE International Conference on Electronics, Circuits, and Systems (ICECS)",28 Nov.-1 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TrustCom53373.2021.00034,Enhancing Trust-based Medical Smartphone Networks via Blockchain-based Traffic Sampling,IEEE,Conferences,"With more devices being inter- or intra-connected, Internet of Things (IoT) has gradually been adopted in many disciplines, such as healthcare industry, coined as Internet of Medical Things (IoMT). The purpose of IoMT is to facilitate the efficiency and effectiveness of medical operations, i.e., remotely monitoring the status of patients. In such healthcare environments, smartphones have become an important device to communicate with others and update the information of patients, resulting in a special type of IoMT called Medical Smartphone Networks (MSNs). To reinforce the distributed architecture, trust management schemes are often implemented to defend against insider attacks. However, how to maintain the robustness of trust management in heavy traffic networks still remains a challenge, i.e., COVID-19 incident would cause excessive traffic for healthcare organizations and increase the difficulty of validating trustworthiness among MSN nodes. In this work, we focus on this issue and propose a blockchain-enabled adaptive traffic sampling method to help enhance the robustness of trust management under high traffic environments. The use of blockchain technology aims to build a verified database of malicious traffic among all nodes. The evaluation in a real healthcare environment demonstrates the viability and effectiveness of our approach.",https://ieeexplore.ieee.org/document/9724473/,"2021 IEEE 20th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)",20-22 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2018.8622583,Ensemble Machine Learning Systems for the Estimation of Steel Quality Control,IEEE,Conferences,"Recent advances in the steel industry have encountered challenges in soliciting decision making solutions for quality control of products based on data mining techniques. In this paper, we present a steel quality control prediction system encompassing with real-world data as well as comprehensive data analysis results. The core process is cautiously designed as a regression problem, which is then best handled by grouping various learning algorithms with their massive resource of historical production datasets. The characteristics of the currently most popular learning models used in regression problem analysis are as well investigated and compared. The performance indicates our steel quality control prediction system based on ensemble machine learning model can offer promising result whilst delivering high usability for local manufacturers to address the production problem by aid of development of machine learning techniques. Furthermore, real-world deployment of this system is demonstrated and discussed. Finally, future directions and the performance expectation are pointed out.",https://ieeexplore.ieee.org/document/8622583/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/21CW48944.2021.9532522,Ergonomics of Human Machine Integration in Variable Autonomy,IEEE,Conferences,"“Human technologies are made by humans, for humans”. In recent days pairing people with the system is getting easier. The systems and tools we use are becoming increasingly intelligent and more interconnected with autonomous behavior giving birth to cyber physical systems. The advances in the miniaturization of computation makes our tool behave intelligent using Artificial Intelligence. This intelligence in the form of a software where the inputs are taken from entities of real-time systems. The ultimate goal of the future research should be to emulate the functions of human-human and human-autonomy teams directly and evaluate their joint performance and contributions. Armed with this approach and existing technologies we can uncover novel approaches in Industry 4.0 and this paper ends with the overview of human-machine autonomy and ergonomics at variable autonomy.",https://ieeexplore.ieee.org/document/9532522/,2021 IEEE Conference on Norbert Wiener in the 21st Century (21CW),22-25 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCWC51732.2021.9375834,Evaluation of Machine Learning Architectures in Healthcare,IEEE,Conferences,"Machine Learning (ML) is now influencing every part of the industry. From detecting objects from an image to recommending different items while doing online shopping based on someone's recent browsing history. Now, the ML is touching the healthcare sector. It is now an area of interest for more doctors and scientists to implement different techniques and harvest the power of ML. Over the past few years, there is a race to implement Artificial Intelligence (AI) and ML in this sector. Multiple scholars have presented their approach. Currently, the proposed models are nowhere near the actual implementation of these models in the real world. However, these models are laying down the path to do so in the future. Here is a review of some of the papers discussing different techniques for targeting various diseases using AI/ML. Each paper introduces a method developed based on the separate datasets created from organically collected data.",https://ieeexplore.ieee.org/document/9375834/,2021 IEEE 11th Annual Computing and Communication Workshop and Conference (CCWC),27-30 Jan. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSECS52883.2021.00011,Existing Semantic Ontology and its Challenges for Enhancing Interoperability in IoT Environment,IEEE,Conferences,"Internet of Things technology is widely used in several domains including industry, society, and the environment. Heterogeneous data is a critical problem in the Internet of Things technology; this heterogeneity leads to a lack of interoperability which limits the possibility of sharing and reusing data for supporting decision-makers in different fields. The aim of this paper is to review the current IoT ontologies and their main challenges. Ontology has been used to solve the heterogeneity problem and enhance interoperability for the Internet of Things by using common core ontology. The main feature and success factor of ontology as a representation of knowledge is that the features being flexible, clearable, shareable, and reusable. This study is probable for contributing to developing common core ontology for the Internet of Things with a comprehensive view for solving the lack of sharing information and offer complete recommendations for supporting the interoperability processes for IoT application domains.",https://ieeexplore.ieee.org/document/9537077/,2021 International Conference on Software Engineering & Computer Systems and 4th International Conference on Computational Science and Information Management (ICSECS-ICOCSIM),24-26 Aug. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSEC53205.2021.9684633,Experimental Piano &#x2013; Playing Robot Hand,IEEE,Conferences,"The robotic and Artificial Intelligent (AI) have been introduced as a key factor for industry revolution4.0. Many industries such as manufacturing, agriculture, logistic and supply chain and so on, are transformed and applied robotic and AI in order to enhance the productivities and reduce cost. In addition, the automation system based on modern AI are highly adopted according to lacking of labors in ageing society era. The AI in creative work is very challenging, especially in music. This paper presents the experiments of making processing unit to understand music source separation, automatic music transcription and optical music recognition is our first experiments. Music source separation (MSS) is the separation of music into different sound sources (for example, vocal, drum, or piano). Automatic music transcription (AMT) is a transcription of instrument sound as a musical note. Optical music recognition (OMR) is a task that involves reading and interpreting music notes and creating a machine-readable version of the sheet music. From these three basic knowledges, our first prototype of a robotic hand is implemented successfully and is able to play piano from musical notes, songs and piano sound properly in real-time. The design and experimental results are explained in this paper.",https://ieeexplore.ieee.org/document/9684633/,2021 25th International Computer Science and Engineering Conference (ICSEC),18-20 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MDM.2017.48,Experimental Study of Telco Localization Methods,IEEE,Conferences,"Telecommunication (Telco) localization is a technique to accurately locate mobile devices (MDs) using measurement report (MR) data, and has been widely used in Telco industry. Many techniques have been proposed, including measurement-based statistical algorithms, fingerprinting algorithms and different machine learning-based algorithms. However, it has not been well studied yet on how these algorithms perform on various Telco MR data sets. In this paper, we conduct a comprehensive experimental study of five state-of-art algorithms for Telco localization. Based on real data sets from two Telco networks, we study the localization performance of such algorithms. We find that a Random Forest-based machine learning algorithm performs best in most experiments due to high localization accuracy and insensitivity to data volume. The experimental result and observation in this paper may inspire and enhance future research in Telco localization.",https://ieeexplore.ieee.org/document/7962466/,2017 18th IEEE International Conference on Mobile Data Management (MDM),29 May-1 June 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WSC.1989.718775,Expert Simulation For On-line Scheduling,IEEE,Conferences,"In recent years, the automotive industry has realized the importance of speed of new products to market and has mounted efforts for improving it. The Expert System Scheduler (ESS) facilitates these efforts by enabling manufacturing plants to generate viable schedules under increasing constraints and demands for flexibility. The scheduler takes advantage of the Computer Integrated Manufacturing (CIM) environment by utilizing the real-time information from the factory for responsive scheduling. The Expert System Scheduler uses heuristics developed by an experiences factory scheduler. It uses simulation concepts and these heuristics to generate schedules. Forward and ""backward"" simulation are used at different stages of the schedule generation process. The system is used to control parts flow on the factory floor at one automated facility. This highly automated facility is a testbed for implementation of CIM concepts. The scheduler runs on a Texas Instruments (TI) Explorer II computer using software developed inhouse utilizing IntelliCorp's Knowledge Engineering Environment (KEE) shell and the LISP language. The scheduling computer is networked to the factory control computer, which actually controls the plant floor. The TI Explorer II acquires current plant floor information from the factory control system, generates a new schedule and sends it back within a short time. The configuration allows fast response to changes in requirements and plant floor conditions.",https://ieeexplore.ieee.org/document/718775/,1989 Winter Simulation Conference Proceedings,4-6 Dec. 1989,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS48674.2020.9214045,Extensions of the open-source framework Aerostack 3.0 for the development of more interactive flights between UAVs,IEEE,Conferences,"The basis for properly verified R&D works is to provide reliable prototyping tools at three most important stages: computer simulation, laboratory tests and real-world experiments. In the laboratory-limited conditions, particular importance is attributed to the first two stages, especially in the context of the safety development of autonomous flights of unmanned aerial vehicle (UAV) groups in various missions. The open-source framework Aerostack support those needs and its effectiveness has been proven in the International Micro Air Vehicle Indoor Competitions (IMAV 2013, 2016, 2017) and Mohammed Bin Zayed International Robotics Challenge (MBZIRC 2020). In the paper, the exemplary functionalities for the new version of Aerostack Version 3.0 Distribution Sirocco (Aerial robotics framework for the industry), extended additionally with a library of new behaviors, are presented. The mission of UAVs can be developed fast and effectively in order to conduct test flights with real drones in lab, before one will decide to fly autonomously outdoor. The representative results obtained for low-cost AR.Drone 2.0 UAV models in two missions, are presented. The first mission is autonomous patrolling the area by a pair of UAVs, the second - intercepting the intruder in guarded area by the guard UAV.",https://ieeexplore.ieee.org/document/9214045/,2020 International Conference on Unmanned Aircraft Systems (ICUAS),1-4 Sept. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCWD54268.2022.9776062,FPoL: Federated Learning-Enabled Collaborative Packing Leakage Detection System,IEEE,Conferences,"Leaking oil from a stuffing box (packing) during the process of oil extraction may lead to serious economic as well as environmental problems. In recent years, a number of deep learning based methods have been proposed to build a real-time oil leakage detection system by analyzing data collected from different sensors. However, deep learning is a data hungry technology. To train a leakage detection model with satisfactory accuracy, numerous diversified training data are needed, which is hard to be collected by a single enterprise. Building collaboration among different enterprises by sharing sensor data for training can yield a better oil leakage detection model. But such collaboration is usually limited by concerns on sensitive information contained in sensor data being leaked. To handle these concerns and exploit benefits from collaboration training, in this paper, we leverage federated learning (FL), which is an emerging decentralized deep learning paradigm, to build an oil leakage detection model in a collaborative but privacy-protecting manner. In addition, considering data across enterprises may be labeled in different manners, we also carefully design an FL personalized strategy so that our collaboration paradigm can be conducted on data with label shift problems. We evaluate our methods on a real-world industry dataset and demonstrate that 1) Models yielded by our FL system can achieve very competitive results compared with models yielded by data-sharing collaboration; 2) Our proposed personalized strategy can properly handle the label shift problems under traditional FL setup.",https://ieeexplore.ieee.org/document/9776062/,2022 IEEE 25th International Conference on Computer Supported Cooperative Work in Design (CSCWD),4-6 May 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICT50521.2020.00032,Face Recognition Techniques using Statistical and Artificial Neural Network: A Comparative Study,IEEE,Conferences,"Face recognition is the process of identifying a person by their facial characteristics from a digital image or a video frame. Face recognition has extensive applications and there will be a massive development in future technologies. The main contribution of this research is to perform a comparative study between different statistical-based face recognition techniques, namely: Eigen-faces, Fisher-faces, and Local Binary Patterns Histograms (LBPH) to measure their effectiveness and efficiency using real-database images. These recognizers still used on top of commercial face recognition products. Additionally, this research is comprehensively comparing 17 face-recognition techniques adopted in research and industry that use artificial-neural network, criticize and categories them into an understandable category. Also, this research provides some directions and suggestions to overcome the direct and indirect issues for face recognition. It has found that there is no existing recognition method that the community of face recognition has agreed on and solves all the issues that face the recognition, such as different pose variation, illumination, blurry and low-resolution images. This study is important to the recognition communities, software companies, and government security officials. It has a direct impact on drawing clear path for new face recognition propositions. This study is one of the studies with respect to the size of its reviewed approaches and techniques.",https://ieeexplore.ieee.org/document/9092128/,2020 3rd International Conference on Information and Computer Technologies (ICICT),9-12 March 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICS51289.2020.00088,Feature Selection for Malicious Traffic Detection with Machine Learning,IEEE,Conferences,"The network technology plays an important role in the emerging industry 4.0. Industrial control systems (ICS) are related to all aspects of human life and have become the target of cyber-attackers. Attacks on ICS may not only cause economic loss, but also damage equipment and hurt staff. The biggest challenges in establishing a secure network communication system is how to effectively detect and prevent malicious network behavior. A Network Intrusion Detection System (NIDS) can be deployed as a defense mechanism for cyberattacks. However, for industrial internet-of-things (IIoT) applications with limited computing resources, designing an effective NIDS is challenging. In this paper, we propose to use machine learning as the core technology to build a compact and effective NIDS for IIoT. The proposed method is validated by using the more recent UNSW-NB 15 dataset to improve the detection capability against new types of attacks in the real world. Furthermore, we demonstrate that the method is also valid for traditional KDD-CUP-99 dataset. Experimental results show that the proposed method achieves better performance than previous methods.",https://ieeexplore.ieee.org/document/9359069/,2020 International Computer Symposium (ICS),17-19 Dec. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/APSEC51365.2020.00047,Federated Learning Systems: Architecture Alternatives,IEEE,Conferences,"Machine Learning (ML) and Artificial Intelligence (AI) have increasingly gained attention in research and industry. Federated Learning, as an approach to distributed learning, shows its potential with the increasing number of devices on the edge and the development of computing power. However, most of the current Federated Learning systems apply a single-server centralized architecture, which may cause several critical problems, such as the single-point of failure as well as scaling and performance problems. In this paper, we propose and compare four architecture alternatives for a Federated Learning system, i.e. centralized, hierarchical, regional and decentralized architectures. We conduct the study by using two well-known data sets and measuring several system performance metrics for all four alternatives. Our results suggest scenarios and use cases which are suitable for each alternative. In addition, we investigate the trade-off between communication latency, model evolution time and the model classification performance, which is crucial to applying the results into real-world industrial systems.",https://ieeexplore.ieee.org/document/9359305/,2020 27th Asia-Pacific Software Engineering Conference (APSEC),1-4 Dec. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCST50977.2020.00021,Food object recognition and intelligent billing system based on Cascade R-CNN,IEEE,Conferences,"With the development of information technology and artificial intelligence, using science and technology to change the low efficiency of the catering industry is a very effective means. The existing system of food identification and intelligent billing in the market includes artificial billing, RFID induction, photo recognition, etc. Based on the Cascade R-CNN algorithm and computer vision technology, this paper proposes an intelligent food identification and intelligent billing system. First, the database is created for algorithm training, then the mobile device is used to collect the food image, and the depth neural network is used to identify the food in the image. Finally, the price calculation result of each is found and returned to the user. In this paper, the basic principle and implementation method of the system are described in detail, and the experimental phenomenon is analyzed. The experimental results show that the system has good accuracy.",https://ieeexplore.ieee.org/document/9262826/,2020 International Conference on Culture-oriented Science & Technology (ICCST),28-31 Oct. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/R10-HTC.2018.8629835,Foody - Smart Restaurant Management and Ordering System,IEEE,Conferences,"Customers play a vital role in the contemporary food industry when determining the quality of the restaurant and its food. Restaurants give considerable attention to customers’ feedback about their service, since the reputation of the business depends on it. Key factors of evaluating customer satisfaction are, being able to deliver the services effectively to lessen the time of consumption, as well as maintaining a high quality of service. In most cases of selecting a prominent restaurant, customers focus on their choice of favorite food in addition to available seating and space options. Long waiting times and serving the wrong order is a common mistake that happens in every restaurant that eventually leads to customer dissatisfaction. Objectives of this online application “Foody” is to address these deficiencies and provide efficient and accurate services to the customer, by providing unique menus to each customer considering their taste. This concept is implemented as a mobile application using latest IT concepts such as Business Intelligence, Data Mining, Predictive Analysis and Artificial Intelligence. This includes graphics and 3D modeling that provide existent physical information related to food such as colors, sizes and further user can view the ingredients of the meal as well as the available tables. In addition, the app shows the real-time map to the restaurant. Current table reservation status is indicated by the color change of the table. Unique food recommendation and it’s order for each customer is generated by analyzing their social media information and the system notifies the customer the wait time by calculating it. Preparation of food and allocation is done subjectively. The expected outcome of the research is to develop a fully automated restaurant management system with the mentioned features as well as to avoid confusions between orders, provide better view of food and allow the customer to choose the menu according to their taste in a minimum time.",https://ieeexplore.ieee.org/document/8629835/,2018 IEEE Region 10 Humanitarian Technology Conference (R10-HTC),6-8 Dec. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UIC-ATC.2017.8397484,Forecasting car rental demand based temporal and spatial travel patterns,IEEE,Conferences,"Recent years, shared mobility services have gained momentum across the world. Meanwhile, rental car industry has seen great developments in China and has reached a scale of economy. Knowing the rental behavior pattern and forecasting the demand become more important for rental businesses. To this end, in this paper, we aim to analyze the rental mobility pattern by examining multiple factors in a holistic manner. A special goal is to predict the demand of a given region. Specifically, we first analyze regular mobility based on real trips of rental cars. Then, we extract key features from multiple types of rental-related data, such as rental behavior profiles and geo-social information of regions. Next, based on these features, we develop a multi-task learning based regression approach for predicting rental cars' demand. This approach can effectively learn not only fundamental features but also relationships between regions by considering multiple factors. Finally, we conduct extensive experiments on real-world rental trip data collected in Beijing. The experimental results validate the effectiveness of the proposed approach for forecasting rental demand in the real world.",https://ieeexplore.ieee.org/document/8397484/,"2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",4-8 Aug. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIID51893.2021.9456549,Freight positioning technology of high speed railway carriage based on UWB,IEEE,Conferences,"With the rapid development of railway freight transportation industry, it is imperative for information and intelligent logistics management to replace the traditional logistics management technology. This paper proposes the design of high-speed railway material positioning system based on UWB technology. Firstly, the difficulties of UWB implementation in high-speed railway freight positioning are analyzed. In order to solve the characteristics of nonlinear and non Gaussian distribution of multi-path interference, this paper presents a particle filter ranging calibration algorithm for serious multipath interference in train compartment, which has serious multipath interference, processes 25 sets of ranging data with particle filter. Finally, by arranging 6 base stations in the high-speed railway carriage, the multi-base station weighted least square method is used to locate the information of the material location in the carriage. The feasibility of the system is verified by comparing with the real location.",https://ieeexplore.ieee.org/document/9456549/,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),28-30 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/EuRAD50154.2022.9784513,Fruit Sorting with Amplitude-only Measurements,IEEE,Conferences,"The food industry is in constant demand of performing and easy to implement Non-Destructive Evaluation NDE techniques. In this paper, we tackle the problem of the automatic inspection of fruits and more specifically, the sorting of healthy and damaged fruits, taking apples and peaches as examples. In a recent work, we have explained how to proceed by combining mm-Wave measurements processed with a 2D-FFT and Machine Learning algorithm. The accuracy reaches at least 80&#x0025;. Although the 2D-FFT is a real-time processing and thus interesting for an industrial implementation, it requires complex measurements, i.e amplitude and phase, which makes the acquisition system more complex. Here we aim to overcome this difficulty by processing amplitude-only measurements. We make use of an image processing based on the direct conversion of the measured amplitude into images. The images form the dataset for the classifier that we choose as a non-linear SVM with a RBF kernel. The advantage of the SVM is that the computational burden is moved to the training phase where we compute the optimal hyper-parameters C&#x002A; and &#x03D2;*, while the test is very fast. First, we describe the complete workflow and use a set of apples measured in W-band in Autumn 2019 for validation purpose. We then extend the validation to measurements of peaches conducted over a long time period (summer 2019 and 2020). Finally, we investigate the robustness of the method over frequency while moving to the D-band. For all tests the accuracy is of 100&#x0025;",https://ieeexplore.ieee.org/document/9784513/,2021 18th European Radar Conference (EuRAD),5-7 April 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCMEIM52463.2020.00032,Garbage Classification and Recognition Based on SqueezeNet,IEEE,Conferences,"In this study, an intelligent garbage classification and recognition system was deployed on the industry integrated computer with the I3-7100U processor and 2G memory. Considering the unit prediction time and prediction accuracy, SqueezeNet was selected as the classification network training model among ResNet, InceptionV3, and SqueezeNet. The pretraining SqueezeNet network model on the ImageNet-1000 dataset was used for transfer learning, and the model predication accuracy was improved by using image enhancement and Adam optimizer. The comparison between the comprehensive test set and the real garbage image showed that the model predication accuracy reached 87.7% after training, and the prediction time in the industrial integrated machine was less than 2 seconds, which met the needs of practical applications.",https://ieeexplore.ieee.org/document/9409502/,2020 3rd World Conference on Mechanical Engineering and Intelligent Manufacturing (WCMEIM),4-6 Dec. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNSC.2006.1673254,General Methodology for Action-Oriented Industrial Ecology Complex Systems Approach Applied to the Rotterdam Industrial Cluster,IEEE,Conferences,"A new approach for the understanding and shaping of the evolution of large scale socio-technical systems is presented. A proof-of-concept knowledge application has been developed, based on the industrial Rotterdam-Rijnmond case. The knowledge application includes the design of a model of industry-infrastructure evolution. Such networks are modeled via a system decomposition, formalization in an ontology and implementation of an agent based model. In simulation runs several network metrics are presented. The results provide insights in real world system behavior and show the validity and potential of the approach",https://ieeexplore.ieee.org/document/1673254/,"2006 IEEE International Conference on Networking, Sensing and Control",23-25 April 2006,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AI4I.2018.8665690,Genetic Algorithm Based Parallelization Planning for Legacy Real-Time Embedded Programs,IEEE,Conferences,"Multicore platforms are pervasively deployed in many different sectors of industry. Hence, it is appealing to accelerate the execution through adapting the sequential programs to the underlying architecture to efficiently utilize the hardware resources, e.g., the multi-cores. However, the parallelization of legacy sequential programs remains a grand challenge due to the complexity of the program analysis and dynamics of the runtime environment. This paper focuses on parallelization planning in that the best parallelization candidates would be determined after the parallelism discovery in the target large sequential programs. In this endeavor, a genetic algorithm based method is deployed to help find an optimal solution considering different aspects from the task decomposition to solution evaluation while achieving the maximized speedup. We have experimented the proposed approach on industrial real time embedded application to reveal excellent speedup results.",https://ieeexplore.ieee.org/document/8665690/,2018 First International Conference on Artificial Intelligence for Industries (AI4I),26-28 Sept. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCI.2018.8441350,Genetic Approach based Bug Triage for Sequencing the Instance and Features,IEEE,Conferences,"In software industry analyzing bug by various tester and developer is a costly approach. So collecting these bug reports and triage is done manually which consume time with high rate of error. Here proposed work has focus on this triage of the bug reports by reducing the dataset size. In order to reduce cost of bug triage proper sequencing of the instance and feature selection is done. Here instance and feature selection are clustered by using list of words, keywords and bug id as fitness function parameters. Two stage learning genetic algorithm named as teacher learning based optimization was used for clustering. As genetic algorithms are unsupervised learning approach, so new set bug report triage is adopt by the proposed work. Experiment is done on real dataset of bug reports. Result shows that proposed work is better on precision value by 38.5% while execution time was reduce by 29.2% as compared with existing procedures.",https://ieeexplore.ieee.org/document/8441350/,2018 International Conference on Computer Communication and Informatics (ICCCI),4-6 Jan. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CDC.2004.1428748,Grey-box modelling of a motorcycle shock absorber,IEEE,Conferences,"There is an increasing use of virtual prototyping tools in the motorcycle industry, aimed at reducing the development time of new models and speeding up performance optimization, by providing the designer with an in-laboratory virtual test track. Virtual prototyping software is essentially multi body simulation software that requires the availability of models of all the vehicle components. The choice of the model is then of paramount importance, since it heavily affects the accuracy and reliability of the simulation results. Conventional models (like linear models) are often inadequate to describe the behavior of complex nonlinear components, so that it is necessary to appeal to different modelling approaches. This is actually the case when dealing with motorcycle suspension systems, given that their most critical part, the shock absorber, exhibits nonlinear and time-variant behavior. In this paper, a grey-box model of a racing motorcycle mono tube shock absorber is proposed. It consists of a nonlinear parametric model and a black-box, neural network based model. The absorber model has been implemented in a numerical simulation environment, and it has been validated against experimental test data. The results of the validation show that the model is able to reproduce the real behavior of the shock absorber with an accuracy that matches or even beats that of other models previously presented in the literature. The interfacing of the proposed model to the ADAMS virtual prototyping environment is also discussed.",https://ieeexplore.ieee.org/document/1428748/,2004 43rd IEEE Conference on Decision and Control (CDC) (IEEE Cat. No.04CH37601),14-17 Dec. 2004,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSCC51209.2021.9528256,Helmet Detection Using Faster Region-Based Convolutional Neural Networks and Single-Shot MultiBox Detector,IEEE,Conferences,"In a country like India, with excessive population density in all big cities, motorcycles have become dominant modes of transport. It is observed that most motorcyclists avoid wearing helmets despite it being an indispensable safety equipment, whose use can significantly reduce the risk of severe head and brain injuries during accidents. Due to violations of most of the traffic and safety rules, motorcycle accidents have been skyrocketing in the recent years. Hence, it’s the need of the hour to build an effective and scalable system capable of automatic helmet detection by analyzing the surveillance camera’s traffic videos. Although several theoretical deep learning-based models have been proposed to detect helmets for the traffic surveillance aspect, an optimal solution for the industry application is less discussed. This paper demonstrates a novel implementation of the Faster R-CNN and SSD framework for accurate helmet detection in real-time low-quality surveillance videos. The experimental results claim that there is a trade-off between accuracy and execution speed. We also present a comprehensive comparative analysis of the two algorithms and determine the best real-time use case scenarios for each of them.",https://ieeexplore.ieee.org/document/9528256/,2021 8th International Conference on Smart Computing and Communications (ICSCC),1-3 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASPDAC.2011.5722294,High performance lithographic hotspot detection using hierarchically refined machine learning,IEEE,Conferences,"Under real and continuously improving manufacturing conditions, lithography hotspot detection faces several key challenges. First, real hotspots become less but harder to fix at post-layout stages; second, false alarm rate must be kept low to avoid excessive and expensive post-processing hotspot removal; third, full chip physical verification and optimization require fast turn-around time. To address these issues, we propose a high performance lithographic hotspot detection flow with ultra-fast speed and high fidelity. It consists of a novel set of hotspot signature definitions and a hierarchically refined detection flow with powerful machine learning kernels, ANN (artificial neural network) and SVM (support vector machine). We have implemented our algorithm with industry-strength engine under real manufacturing conditions in 45nm process, and showed that it significantly outperforms previous state-of-the-art algorithms in hotspot detection false alarm rate (2.4X to 2300X reduction) and simulation run-time (5X to 237X reduction), meanwhile archiving similar or slightly better hotspot detection accuracies. Such high performance lithographic hotspot detection under real manufacturing conditions is especially suitable for guiding lithography friendly physical design.",https://ieeexplore.ieee.org/document/5722294/,16th Asia and South Pacific Design Automation Conference (ASP-DAC 2011),25-28 Jan. 2011,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.15439/2017F253,Human machine synergies in intra-logistics: Creating a hybrid network for research and technologies,IEEE,Conferences,"The purpose of the article is to outline the futuristic vision of Industry 4.0 in intra-logistics by creating a hybrid network for research and technologies thereby providing a detailed account on the research centre, available technologies and their possibilities for collaboration. Scientific challenges in the field of Industry 4.0 and intra-logistics are identified due to the new form of interaction between humans and machines. This kind of collaboration provides new possibilities of materials handling that can be developed with the support of real-time motion data tracking and virtual reality systems. These services will be provided by a new research centre for flexible human-machine cooperation networks in Dortmund. By the use of various reference and experiment systems various real-time scenarios can be emulated including digital twin simulation concepts. Big data emerges as an important paradigm in this research project where all systems are made flexible in terms of networking for all the systems to consume the data produced and also to combine all the data to arrive at new insights using concepts from machine learning and deep learning networks. This leads to the challenge of finding a common syntax for inter-operating systems. This paper describes the design and deployment strategies of research centre with the possibilities and the design insights for a futuristic Industry 4.0 material handling facility.",https://ieeexplore.ieee.org/document/8104684/,2017 Federated Conference on Computer Science and Information Systems (FedCSIS),3-6 Sept. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC42975.2020.9283392,Human-in-the-Loop Error Precursor Detection using Language Translation Modeling of HMI States,IEEE,Conferences,"Situational Awareness (SA) is paramount to ensuring operational safety in Nuclear Power Plant (NPP) and Commercial aviation industry. An increase in Human-in-the-loop (HITL) error rate may be indicative of reduced operator SA while undermining safety. In this paper, natural language processing (NLP) is applied for modelling industrial Human Machine Interface (HMI) state transitions as a means to detect operator HITL error precursors in real-time. A custom seq2seq encode-decoder deep-learning model design is implemented and evaluated using real-plant scenario dataset obtained from a NPP Operator training simulator. Results support NLP HMI state model may be employed to detect HITL error precursor within the desired N time-steps prior to an accident event.",https://ieeexplore.ieee.org/document/9283392/,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",11-14 Oct. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICISCE48695.2019.00071,Hydropower Generation Forecasting via Deep Neural Network,IEEE,Conferences,"With the advances of deep learning, its applications in our daily life have attracted considerable attention from both academics and industry. However, most of the existing works focus on computer vision and natural language processing, while few studies in the real industrial manufactures. The main reasons are that the data resources are difficult to obtain and the relationships between industrial data are too complex to be modeled. In this paper, we propose a deep neural network based approach for hydroelectric power generation prediction, which, to our knowledge, is the first attempt modeling power generation data with the combination of residual neural networks and recurrent neural networks. Furthermore, we consider different grains of the hydropower generation by dividing the data into four-levels, i.e. recent, daily, weekly, and time-series sequences, which can greatly improve the prediction performance. To this end, we employ a multi-information fusion method to fuse the four components (i.e. closeness, period, trend, long-period) predicted results, among which different component is learned with different weights to determine their influence on final hydropower prediction. Experiments conducting on the real hydropower generation prove the effectiveness of the proposed model, which significantly outperform the baselines. We hope this research will open a new perspective of improving data usage in the industry, especially in power generation areas.",https://ieeexplore.ieee.org/document/9107696/,2019 6th International Conference on Information Science and Control Engineering (ICISCE),20-22 Dec. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PerComWorkshops48775.2020.9156250,ID Sequence Analysis for Intrusion Detection in the CAN bus using Long Short Term Memory Networks,IEEE,Conferences,"The number of computer controlled vehicles throughout the world is rising at a staggering speed. Even though this enhances the driving experience, it opens a new security hole in the automotive industry. To alleviate this issue, we are proposing an intrusion detection system (IDS) to the controller area network (CAN), which is the de facto communication standard of present-day vehicles. We implemented an IDS based on the analysis of ID sequences. The IDS uses a trained Long-Short Term Memory (LSTM) to predict an arbitration ID that will appear in the future by looking back to the last 20 packet arbitration IDs. The output from the LSTM network is a softmax probability of all the 42 arbitration IDs in our test car. The softmax probability is used in two approaches for IDS. In the first approach, a single arbitration ID is predicted by taking the class which has the highest softmax probability. This method only gave us an accuracy of 0.6. Applying this result in a real vehicle would give us a lot of false negatives, hence we devised a second approach that uses log loss as an anomaly signal. The evaluated log loss is compared with a predefined threshold to see if the result is in the anomaly boundary. Furthermore, We have tested our approach using insertion, drop and illegal ID attacks which greatly outperform the conventional method with practical F1 scores of 0.9, 0.84, and 1.0 respectively.",https://ieeexplore.ieee.org/document/9156250/,2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),23-27 March 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICETAS.2018.8629202,IOS Mobile Application for Food and Location Image Prediction using Convolutional Neural Networks,IEEE,Conferences,"Machine Learning is a popular research area in software industry alongside with big data, micro services, virtual reality, and augmented reality. With the recent developments in improving computing capacity, deep learning approaches such as Convolutional Neural Networks (CNN) has become the trendiest topic in machine learning for image recognition. In this paper, we have developed an IOS application for food image recognition using modified CNN models. In particular, we developed an IOS mobile application by converting the models to CoreML and then using them within the IOS application for food image recognition. After fine-tuning a pre-trained Google InceptionV3 model, we were able to achieve 82.03% Top-1 accuracy on the test set using a single crop per item. Using 10 crops per example and taking the most frequent predicted class(es), we were able to achieve 86.97% Top-1 Accuracy and 97.42% Top-5 Accuracy.",https://ieeexplore.ieee.org/document/8629202/,2018 IEEE 5th International Conference on Engineering Technologies and Applied Sciences (ICETAS),22-23 Nov. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC.2018.00073,IT Professional 20th Anniversary Panel: The New Realities of AI,IEEE,Conferences,"Summary form only given, as follows. A complete record of the panel discussion was not made available for publication as part of the conference proceedings. Artificial intelligence (AI) is now creating a lot of excitement and hype among professionals and across all kinds of business and industry, as well as among individuals. It is no longer just the theme of science fiction essays and movies. It is emerging as new, innovative approach for solving challenging problems that we encounter in practice, and as an enabler of disruptive innovations and smarter world. AI’s renaissance is driven by recent complementary developments, including major advances in the AI arena, realistic expectations, and recent success in its applications. AI is also raising some major concerns, real and perceived. Nevertheless, AI is trending to become a new normal and is increasingly being adopted in many applications despite its concerns and limitations. This panel will examine the new realities of AI and offer its perspectives and recommendations. It’ll deliberate on: Where is AI headed? What new applications and innovations will emerge, and how they might impact? What are the risks and concerns? How we can leverage AI for good, and address its major risks and concerns? How do we get prepared for the new AI age?",https://ieeexplore.ieee.org/document/8377698/,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),23-27 July 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIIMA50553.2020.9290302,Implementación de SCADA a través del protocolo MQTT,IEEE,Conferences,"This document describes an implementation of a SCADA system powered by MQTT & OPC-UA protocols and hosted within the Google Cloud Platform system. This combination allows to have integrated, scalable, secure and reliable industrial communications while allowing real-time data acquisition and sensor feed that can then be used in real-time OEE tracking or predictive maintenance models, to name some examples. This in line with the Industry 4.0 initiatives mainly fueled by data and machine learning autonomous systems.",https://ieeexplore.ieee.org/document/9290302/,2020 IX International Congress of Mechatronics Engineering and Automation (CIIMA),4-6 Nov. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COASE.2007.4341740,Implementation Considerations of Various Virtual Metrology Algorithms,IEEE,Conferences,"In the semiconductor industry, run-to-run (R2R) control is an important technique to improve process capability and further enhance the production yield. As the dimension of electronic device shrinks increasingly, wafer-to-wafer (W2W) advanced process control (APC) becomes essential for critical stages. W2W APC needs to obtain the metrology value of each wafer; however, it will be highly time and cost consuming for obtaining actual metrology value of each wafer by physical measurement. Recently, an efficient and cost-effective approach denoted virtual metrology (VM) was proposed to substitute the actual metrology. To implement VM in W2W APC, both conjecture-accuracy and real-time requirements need to be considered. In this paper, various VM algorithms of back-propagation neural network (BPNN), simple recurrent neural network (SRNN) and multiple regression (MR) are evaluated to see whether they can meet the accuracy and real-time requirements of W2W APC or not. The fifth-generation TFT-LCD CVD process is used to test and verify the requirements. Test results show that both one-hidden-layered BPNN and SRNN VM algorithms can achieve acceptable conjecture accuracy and meet the real-time requirements of semiconductor and TFT-LCD W2W APC applications.",https://ieeexplore.ieee.org/document/4341740/,2007 IEEE International Conference on Automation Science and Engineering,22-25 Sept. 2007,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICC.2016.7511226,Implementation and evaluation of adaptive video streaming based on Markov decision process,IEEE,Conferences,"In HTTP-based adaptive streaming systems, media server simply stores video content segmented into a series of small chunks coded in different qualities and sizes. The decision for next chunk's quality level to achieve a high quality viewing experience is left to the client which is a challenging task, especially in mobile environment due to unexpected changes in network bandwidth. Using computer simulations, previous work has demonstrated that Markov decision process (MDP) is very effective for such decision making and that it can reduce video freezing or re-buffering events drastically compared to other methods of adaptation. However, to date there has been no practical implementation and evaluation of MDP-based DASH players. In this work, we extend a publicly available DASH player recently released by DASH industry forum to realise a real DASH player that implements MDP-based video adaptation. We implement two alternative MDP optimisation algorithms, value iteration and Q learning and evaluate their performances in real driving conditions under 300 minutes of video streaming. Our results show that value iteration and Q learning reduce video freezing by a factor of 8 and 11, respectively, compared to the default decision making algorithm implemented in the public DASH player.",https://ieeexplore.ieee.org/document/7511226/,2016 IEEE International Conference on Communications (ICC),22-27 May 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS52415.2021.9466068,Implementation of Cloud Based Action Recognition Backend Platform,IEEE,Conferences,"The Internet of Things (IoT) growth are rapidly in various fields such as industry 4.0, smart cities, and smart homes. Implementation of IoT for electronic assistance had been researched to increase the longevity of human life. However, not all IoT implementation as human life assistance provides action recognition monitoring on multiple elderly people, provide information such as real-time action monitoring, and real-time streaming in a mobile application. Therefore, this research intends to create a system that can receive and provide information on each elderly people who registered. The Action Recognition Backend Platform will be working as cloud computing to receive and manage input data from Edge Computing Action Recognition. This platform integrated Deep Learning, Data Analytics, Big Data Warehouse that implemented Extract, Transform, and Load (ETL) methods, communication services with MQTT, and Kafka Streaming Processor. The test result showed that the edge computing action recognition got better model accuracy performance from our last model [1], which can predict with 50,7% accuracy in 0.5 confidence threshold. Moreover, the backend platform had been successfully implemented a simple IoT paradigm and got an average delivery time of MQTT communication at 204ms, for streaming data process took an average delay of 680ms.",https://ieeexplore.ieee.org/document/9466068/,2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS),28-30 April 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPHYS.2019.8780271,Implementation of Industrial Cyber Physical System: Challenges and Solutions,IEEE,Conferences,"The Industry Internet of Things (IIoT) and the Industry Cyber-Physical System (ICPS) for real industry are becoming vitally necessary in the smart manufacturing environment. Very large number of intelligent sensors are being available generating an exploding amount of data. Several issues come with the big data in real industry, including the a grand-scale connected network construction with the data security and access protocol issues, data quality with considerable noise when gathered from industrial factories, efficient data storage, smart interconnection with cloud services, and real-time analytics requirements. This paper proposes an integrated CPS based architecture for smart manufacturing and provides the deployment details, addressing all the potential problems in an appropriate way. It has been successfully implemented in a real industry environment, and won the Best Industry Application of IoT at the BigInsights Data & AI Innovation Awards.",https://ieeexplore.ieee.org/document/8780271/,2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS),6-9 May 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IES53407.2021.9594001,Implementation of SUMO Simulation for Comparison of CVRP,IEEE,Conferences,"With the rapid increase in human density, development, and mobility in urban areas, the need for logistics distribution systems is increasing which is an important part of connecting industries with their consumers. Thus, route planning is an important thing for the industry. Therefore, this paper proposes a comparison of several vehicle routing problems algorithms and test the routes that have been obtained on a simulation system based on real conditions. Our proposed algorithm consists of Mixed Integer Linear Programming (MILP), Clarke-Wright and Reinforcement Learning algorithm using Markov Decision Process. Digital maps, customer data and route planning results will be converted into a SUMO simulation. We compare the performance of the algorithm with parameters consisting of the number of routes, distance traveled, computation time and simulation time. The experimental results show that the MILP algorithm has the best performance with the most optimal route results, but other algorithms have a lower computation time.",https://ieeexplore.ieee.org/document/9594001/,2021 International Electronics Symposium (IES),29-30 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCWorkshops52231.2021.9538856,Implementation of an intelligent target detection system for edge node,IEEE,Conferences,"With the rapid development of the artificial intelligence industry, the application of object detection technology in real life is becoming increasingly widespread, such as intelligent monitoring, autonomous driving, and augmented reality. In this paper, object detection system is deployed on edge devices which is low complexity and low-cost device such as the Raspberry Pi. Implement Mobilenet-SSD based on the deep learning framework and deploy on edge devices, such as image acquisition, object detection and result display. The results show that the object detection technology can also be achieved on devices with scarce computing resources such as the Raspberry Pi and satisfies actual business requirements.",https://ieeexplore.ieee.org/document/9538856/,2021 IEEE/CIC International Conference on Communications in China (ICCC Workshops),28-30 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE-TW46550.2019.8991771,Implementation of ransomware prediction system based on weighted-KNN and real-time isolation architecture on SDN Networks,IEEE,Conferences,"In May 2017, hackers used the ransomware WannaCry to launch large-scale attacks on 150 countries, affecting every industry. Therefore, detection and control of the ransomware virus has become an important issue for security experts in recent years. Recently, machine learning, deep learning, and artificial intelligence technologies have become increasingly mature. Many companies (such as Google) have introduced software-defined networking (SDN) to replace the original network architecture, traffic routing, and network configuration control management. Therefore, this paper proposes a ransomware prediction system based on weighted-K-Nearest-Neighbor. This system includes the detection and prediction of ransomware packet traffic and design and the implementation of a dynamic isolation system integrated SDN. The experimental results show that the precision of detecting normal flow and abnormal flow is 99.7 and 97.7, respectively.",https://ieeexplore.ieee.org/document/8991771/,2019 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW),20-22 May 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTC.2012.6418168,Implementation of the disruption predictor APODIS in JET real time network using the MARTe framework,IEEE,Conferences,"The evolution in the past years of Machine learning techniques, as well as the technological evolution of computer architectures and operating systems, are enabling new approaches for complex problems in different areas of industry and research, where a classical approach is nonviable due to lack of knowledge of the problem's nature. A typical example of this situation is the prediction of plasma disruptions in Tokamak devices. This paper shows the implementation of a real time disruption predictor. The predictor is based on a support vector machine (SVM). The implementation was done under the MARTe framework on a six core x86 architecture. The system is connected in JET's Real time Data Network (RTDN). Online results show a high degree of successful predictions and a low rate of false alarms thus, confirming its usefulness in a disruption mitigation scheme. The implementation shows a low computational load, which in an immediate future will be exploited to increase the prediction's temporal resolution.",https://ieeexplore.ieee.org/document/6418168/,2012 18th IEEE-NPSS Real Time Conference,9-15 June 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAML48257.2019.00042,Importance of Cloud Deployment Model and Security Issues of Software as a Service (SaaS) for Cloud Computing,IEEE,Conferences,"Cloud computing, now-a-days has become the jargon in the industry of IT. It is a model that gives worldwide access to shared pools of configurable resources over the internet. That means it run several applications or programs at the same time on more than one computer. The nature and usage of the cloud computing are developing rapidly both virtually and in reality. It is making things easier for the user of internet by many of its attractive features but these features, have not just only challenged the existing security system, but have also revealed new security issues. In this paper the deployment of cloud models are discussed . The cloud models are deployed as public cloud, private cloud and hybrid cloud. Each of these models have their own advantages and disadvantages. Cloud also provides some services like software as a service (SaaS), Platform as a service (PaaS), Infrastructure as a service (IaaS) . This paper is an insightful analysis of the features as well as the securities challenges of Software as a Service (SaaS) for cloud computing.",https://ieeexplore.ieee.org/document/8989204/,2019 International Conference on Applied Machine Learning (ICAML),25-26 May 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISDA.2007.368136,Improved Missile Route Planning and Targeting using Game-Based Computational Intelligence,IEEE,Conferences,"This paper discusses a research project that employs computational intelligence (CI) to improve the ability of military planners to route sensors and weapons to effectively engage mobile targets. Future target motion is predicted through the use of multiple software agents employing goal oriented action planning (GOAP). Derived from the Stanford Research Institute Planning System (STRIPS), GOAP is a relatively new class of CI that is ideally suited to dynamic real-time environments such as military operations. The project is unusual in its adaptation of computer gaming industry technology for use in real-time, tactical military applications",https://ieeexplore.ieee.org/document/4219083/,2007 IEEE Symposium on Computational Intelligence in Security and Defense Applications,1-5 April 2007,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT52682.2021.9491636,Improvement of personal loans granting methods in banks using machine learning methods and approaches in Palestine,IEEE,Conferences,"For banking organizations, loan approval and risk assessment which is related is a very complex and significant process which is needs a high effort for relevant employee or manager to take a decision, because of manual or traditional methods that used in banks. The banking industry still needs a more precise method of predictive modeling for several problems. In general, for financial institutions and especially for banks forecasting credit defaulters is a hard challenge. The primary role of the current systems is to accept, or sending loan application to a specific level of approval to be studied and it is very difficult to foresee the probability of the borrower for paying the due dues amount without using methods to predict. Machine learning (ML) techniques and the algorithm that belongs to are a very amazing and promising technique in predicting for a large amount of data. Our research proposed to study three machine learning algorithms [1], Decision Tree (DT), Logistic Regression (LR), and Random Forest (RF), by using real data collected from Quds Bank with a variables that cover credit restriction and regulator instructions. The algorithm has been implemented to predict the loan approval of customers and the output tested in terms of the predicted accuracy.",https://ieeexplore.ieee.org/document/9491636/,2021 International Conference on Information Technology (ICIT),14-15 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE-SEIP55303.2022.9793983,Improving Code Autocompletion with Transfer Learning,IEEE,Conferences,"Software language models have achieved promising results predicting code completion usages, and several industry studies have described successful IDE integration. Recently, accuracy in autocompletion prediction improved 12.8%[2] from training on a real-world dataset collected from programmers&#x2019; IDE activities. But what if the number of examples of IDE autocompletion in the target programming language is inadequate for model training? In this paper, we highlight practical reasons for this inadequacy, and make a call to action in using transfer learning to overcome the issue.",https://ieeexplore.ieee.org/document/9793983/,2022 IEEE/ACM 44th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),22-24 May 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2001.938518,Improving prediction of customer behavior in nonstationary environments,IEEE,Conferences,"Customer churn, switching from one service provider to another, costs the wireless telecommunications industry $4 billion each year in North America and Europe. To proactively build lasting relationships with customers, it is thus crucial to predict customer behavior. Machine learning has been applied to churn prediction, using historical data such as usage, billing, customer service, and demographics. However, because customer behavior is often nonstationary, training a model based on data extracted from a window of time in the past yields poor performance on the present. We propose two distinct approaches, using more historical data or new, unlabeled data, to improve the results for this real-world, large-scale, nonstationary problem. A new ensemble classification method, with combination weights learned from both labeled and unlabeled data, is also proposed, and it outperforms bagging and mixture of experts.",https://ieeexplore.ieee.org/document/938518/,IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222),15-19 July 2001,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GIOTS49054.2020.9119497,Industrial IoT and Digital Twins for a Smart Factory : An open source toolkit for application design and benchmarking,IEEE,Conferences,"The rapid evolution of digital technology and designed intelligence, such as the Internet of Things (IoT), Big data analytics, Artificial Intelligence (AI), Cyber Physical Systems (CPS), has been a catalyst for the 4th industrial revolution (known as industry 4.0). Among other, the two key state-of-the-art concepts in Industry 4.0, are Industrial IoT (IIoT) and digital twins. IIoT facilitates real-time data acquisition, processing and analytics over large amount of sensor data streams produced by sensors installed within a smart factory, while the ‘digital twin’ concept aims to enable smart factories via the digital replication or representation of physical machines, processes, people in cyber-space. This paper explores the capability of present-state open-source platforms to collectively achieve digital twin capabilities, including IoT real-time data acquisition, virtual representation, analytics, and visualisation. The aim of this work is to ‘close the gap’ between research and implementation, through a collective open source IoT and Digital Twin architecture. The performance of the open-source architecture in this work, is demonstrated in a use-case utilising industry ‘open data’, and is bench-marked with universal testing tools.",https://ieeexplore.ieee.org/document/9119497/,2020 Global Internet of Things Summit (GIoTS),3-3 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN52387.2021.9533907,Information-theoretic Source Code Vulnerability Highlighting,IEEE,Conferences,"Software vulnerabilities are a crucial and serious concern in the software industry and computer security. A variety of methods have been proposed to detect vulnerabilities in real-world software. Recent methods based on deep learning approaches for automatic feature extraction have improved software vulnerability identification compared with machine learning approaches based on hand-crafted feature extraction. However, these methods can usually only detect software vulnerabilities at a function or program level, which is much less informative because, out of hundreds (thousands) of code statements in a program or function, only a few core statements contribute to a software vulnerability. This requires us to find a way to detect software vulnerabilities at a fine-grained level. In this paper, we propose a novel method based on the concept of mutual information that can help us to detect and isolate software vulnerabilities at a fine-grained level (i.e., several statements that are highly relevant to a software vulnerability that include the core vulnerable statements) in both unsupervised and semi-supervised contexts. We conduct comprehensive experiments on real-world software projects to demonstrate that our proposed method can detect vulnerabilities at a fine-grained level by identifying several statements that mostly contribute to the vulnerability detection decision.",https://ieeexplore.ieee.org/document/9533907/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR.2018.00054,"Integrating Biomechanical and Animation Motion Capture Methods in the Production of Participant Specific, Scaled Avatars",IEEE,Conferences,"3D motion capture of human movement in animation and biomechanics has developed in relatively separate and parallel domains. The two disciplines use different language, software, computational models and have different aims. As a result, in the life sciences, human movement is predominantly analyzed as non-visual biomechanical data. Whereas human movement visualization in animation typically lacks the accuracy outside of that required in the entertainment industry. This project draws from both disciplines to develop a novel approach in the creation of participant specific, motion capture skeletons which are retargeted onto participant specific, anatomically scaled, humanoid avatars. The customized motion capture marker placement, skeleton and character scaling used in this new approach aims to retain a high level of movement fidelity and minimize discrepancies between participant and avatar movement. This process has been used in the visualization of aesthetic movement such as dance and provides a step towards the generation of a digital double which can facilitate full body immersion into digital environments.",https://ieeexplore.ieee.org/document/8613673/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I-ESA.2009.60,Integrating Process and Ontology for Supply Chain Modelling,IEEE,Conferences,This paper introduces an ontology model developed to support supply chain process modelling. Supply chain provides the business context for achieving interoperability of enterprise systems. It is observed that the emphasis on ontology development for enterprise interoperability could result in information models that are not relevant to real business needs. This work explicitly defines the generic business processes relevant to supply chain operations and develops the ontology that was tested in the creation of the information model to support the information exchange needs three industry case studies. It demonstrated that prior identification of processes the ontology is supposed to support facilitates its development and also its subsequent validation. This paper introduces the overall ontology development approach together with some of the findings that summarizes our experiences in developing the ontology model to support supply chain process modelling.,https://ieeexplore.ieee.org/document/5260822/,2009 International Conference on Interoperability for Enterprise Software and Applications China,21-22 April 2009,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZY.2006.1681917,Intelligent Constant Current Control for Resistance Spot Welding,IEEE,Conferences,"Resistance spot welding is one of the primary means of joining sheet metal in the automotive industry and other industries. The demand for improved corrosion resistance has led the automotive industry to increasingly use zinc coated steel in auto body construction. One of the major concerns associated with welding coated steel is the mushrooming effect (the increase in the electrode diameter due to deposition of copper into the spot surface) resulting in reduced current density and undersized welds (cold welds). The most common approach to this problem is based on the use of simple unconditional incremental algorithms (steppers) for preprogrammed current scheduling. In this paper, an intelligent algorithm is proposed for adjusting the amount of current to compensate for the electrodes degradation. The algorithm works as a fuzzy logic controller using a set of engineering rules with fuzzy predicates that dynamically adapt the secondary current to the state of the weld process. The state is identified by indirectly estimating two of the main process characteristics - weld quality and expulsion rate. A soft sensor for indirect estimation of the weld quality employing a learning vector quantization (LVQ) type classifier is designed to provide a real time approximate assessment of the weld nugget diameter. Another soft sensing algorithm is applied to predict the impact of changes in current on the expulsion rate of the weld process. By maintaining the expulsion rate just below a minimal acceptable level, robust process control performance and satisfactory weld quality are achieved. The intelligent constant current control for resistance spot welding is implemented and validated on a medium frequency direct current (MFDC) constant current weld controller. Results demonstrate a substantial improvement of weld quality and reduction of process variability due to the proposed new control algorithm.",https://ieeexplore.ieee.org/document/1681917/,2006 IEEE International Conference on Fuzzy Systems,16-21 July 2006,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPRECON52453.2021.9641000,Intelligent Fault Diagnosis Mechanism for Industrial Robot Actuators using Digital Twin Technology,IEEE,Conferences,"Intelligent fault detection is a mechanism’s competency to distinguish between healthy and faulty machine signals for smart and efficient diagnosis. The modelling and analysis of the parameters that contribute to the system’s fundamental operation form the crux of the framework. A heuristic technology to enable real-time intelligent fault detection is digital twin technology. Digital twin technology allows a tandem establishment between real-world machines and the virtual domain, allowing for the inclusion of optimization and maintenance frameworks. Sparsely represented machines in the digital twin domain are linear actuators, which form essential parts of various industrial and commercial machines. Therefore, this study has modelled a data-driven and multi-physics robotic linear actuator digital twin, and integrated it with a custom designed fault detection mechanism using Naïve Bayes classifier. This architecture can autonomously be deployed in tandem to the physical machine to alarm and diagnose electrical faults as soon as they occur in the machine. As compared with conventional diagnostics this will reduce machine down-time and expedite repairs. The resultant model built on MATLAB, Simulink gave an accuracy of 96% and required minimal processing capability to operate. Widespread commercial utilization of the proposed model can pave the path for Industry 4.0 utilization of linear actuators as well as technologies including industrial robots that utilize them.",https://ieeexplore.ieee.org/document/9641000/,2021 IEEE International Power and Renewable Energy Conference (IPRECON),24-26 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITST.2007.4295849,Intelligent Fleet Management System with Concurrent GPS & GSM Real-Time Positioning Technology,IEEE,Conferences,"Fleet management system is a rapid growing industry. This system helps institutions to manage vehicle fleet efficiently and effectively through smart allocation of resources. In this project, an intelligent fleet management system which incorporates the power of concurrent Global Positioning System (GPS) and Global System for Mobile Communications (GSM) real-time positioning, front-end intelligent and web-based management software is proposed. In contrast to systems which depend solely on GPS positioning, the proposed system provides higher positioning accuracy and is capable to track the target at areas where GPS signals are weak or unavailable. The terminal is powered by Front-End Intelligent Technology (FEI), a comprehensive embedded technology that is equipped with necessary artificial intelligence to mimic human intelligence in decision-making for quicker response, better accuracy and less dependence on a backend server. With less dependency on the backend, large scale fleet management system can be implemented more effectively. The proposed system is successfully implemented and evaluated on twenty vehicles including buses and cars in Universiti Teknologi Malaysia (UTM). Results from the test-bed shown that user can monitor and track the real-time physical location and conditions of their vehicles via Internet or Short Message Service (SMS). The web-based fleet management software also helped the user to manage fleets more effectively.",https://ieeexplore.ieee.org/document/4295849/,2007 7th International Conference on ITS Telecommunications,6-8 June 2007,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ARIS50834.2020.9205772,Intelligent Robot for Worker Safety Surveillance: Deep Learning Perception and Visual Navigation,IEEE,Conferences,"The fatal injury rate for the construction industry is higher than the average for all industries. Recently, researchers have shown an increased interest in occupational safety in the construction industry. However, all the current methods using conventional machine learning with stationary cameras suffer from some severe limitations, perceptual aliasing (e.g., different places/objects can appear identical), occlusion (e.g., place/object appearance changes between visits), seasonal / illumination changes, significant viewpoint changes, etc. This paper proposes a perception module using end-to-end deep-learning and visual SLAM (Simultaneous Localization and Mapping) for an effective and efficient object recognition and navigation using a differential-drive mobile robot. Various deep-learning frameworks and visual navigation strategies with evaluation metrics are implemented and validated for the selection of the best model. The deep-learning model's predictions are evaluated via the metrics (model speed, accuracy, complexity, precision, recall, P-R curve, F1 score). The YOLOv3 shows the best trade-off among all algorithms, 57.9% mean average precision (mAP), in real-world settings, and can process 45 frames per second (FPS) on NVIDIA Jetson TX2 which makes it suitable for real-time detection, as well as a right candidate for deploying the neural network on a mobile robot. The evaluation metrics used for the comparison of laser SLAM are Root Mean Square Error (RMSE). The Google Cartographer SLAM shows the lowest RMSE and acceptable processing time. The experimental results demonstrate that the perception module can meet the requirements of head protection criteria in Occupational Safety and Health Administration (OSHA) standards for construction. To be more precise, this module can effectively detect construction worker's non-hardhat-use in different construction site conditions and can facilitate improved safety inspection and supervision.",https://ieeexplore.ieee.org/document/9205772/,2020 International Conference on Advanced Robotics and Intelligent Systems (ARIS),19-21 Aug. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI50451.2021.9659994,Intelligent Strategies to Combine Move Heuristics in Selection Hyper-heuristics for Real-World Fibre Network Design Optimisation,IEEE,Conferences,"Increasing competition in today's telecommunication industry drives the need for more cost effective services. In order to reduce the cost of designing a fibre network with low capital expenditure, automation and optimisation of network design has become crucial. British Telecom's network design software, BT NetDesign, has been developed for the purpose of network design and optimisation using a rich set of network/graph-based heuristics and the simulated annealing (SA) search method. Although NetDesign provides several different ways of navigating the search space via different move heuristics, the existing search method (SA) does not consistently reach the near-global optimum as the size of network increases. To deal with larger networks, this study utilises an intelligent approach based on the well-known Luby sequence to combine move heuristics, using two separate learning schemes: frequency based and bigram statistics. These two strategies are rigorously evaluated on network instances of different sizes. Experimental results on real-world case studies indicate that a bigram scheme with a longer warm-up period to learn heuristic combinations can reach high quality solutions for large networks.",https://ieeexplore.ieee.org/document/9659994/,2021 IEEE Symposium Series on Computational Intelligence (SSCI),5-7 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2006.372422,Intelligent Tuned PID Controllers for PMSM Drive - A Critical Analysis,IEEE,Conferences,"This paper presents a critical analysis of intelligent tuned PID controllers for the Permanent Magnet Synchronous Motor drive. The PID proportional-integral-derivative (PID) controller is the most popular controller in process industry. The PID algorithm is simple, reliable and robust for the control of first and second order processes and even high order processes with well damped modes. In this paper, PID control strategies, based on fuzzy logic, neural network and genetic algorithms are reviewed and implemented. A mathematical model of the drive is developed to study steady state and transient performance of current regulated voltage source Inverter (VSI) fed PMSM in the field oriented mode under different conditions. The drive includes PID controller, vector control structure, the inverter and the machine. Different tuning algorithms and their effect on dynamic performance of PMSM drive in the real time frame are illustrated in terms of starting and speed reversal time, steady state error in speed, overshoot, performance parameters and speed and torque ripple. Good agreements between different methods has been observed and presented.",https://ieeexplore.ieee.org/document/4237744/,2006 IEEE International Conference on Industrial Technology,15-17 Dec. 2006,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISDA.2010.5687225,Intelligent online case-based planning agent model for real-time strategy games,IEEE,Conferences,"Research in learning and planning in real-time strategy (RTS) games is very interesting in several industries such as military industry, robotics, and most importantly game industry. A recent published work on online case-based planning in RTS Games does not include the capability of online learning from experience, so the knowledge certainty remains constant, which leads to inefficient decisions. In this paper, an intelligent agent model based on both online case-based planning (OLCBP) and reinforcement learning (RL) techniques is proposed. In addition, the proposed model has been evaluated using empirical simulation on Wargus (an open-source clone of the well known RTS game Warcraft 2). This evaluation shows that the proposed model increases the certainty of the case base by learning from experience, and hence the process of decision making for selecting more efficient, effective and successful plans.",https://ieeexplore.ieee.org/document/5687225/,2010 10th International Conference on Intelligent Systems Design and Applications,29 Nov.-1 Dec. 2010,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI52525.2021.00200,Interactive Explainable Case-Based Reasoning for Behavior Modelling in Videogames,IEEE,Conferences,"Creation of believable characters is one of the most challenging problems in the video game industry. Although there are different authoring tools available for designers and programmers to create the behavior of non-player characters, it remains a complex and error prone process that requires a high level of technical knowledge. Considering how powerful Learning from Demonstration is for building intelligent agents that are able to replicate human player behaviors, we are currently using an online case-based reasoning agent that learns how to imitate real players of Ms. Pac-Man using an interactive approach in which both the human player and the computational agent take turns controlling the character. In this paper, we describe the new explanation subsystem and how it helps game designers to understand the agent’s learning process. We also present an evaluation of the system performed by five professional video game designers.",https://ieeexplore.ieee.org/document/9643387/,2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI),1-3 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDUCON52537.2022.9766555,Internet of Things Meets Machine Learning: A Water Usage Alert Example,IEEE,Conferences,"The rapid growth of the electronics industry resulted in numerous, amazing and cheap devices, while fluent documentation and user-friendly programming environments are available for them. Modern educational systems worldwide have exploited this dynamic by including in their didactic curricula innovative practices that are usually called STEM actions. Added to this, enriching educational methods with real-world problem solving techniques increases students&#x2019; interest and prepares them for their future role in the society. Apparently, such challenging problems are not missing, with the depletion of natural resources to be one of the most intense ones. In this context, promising modern technological flavors like Internet of Things (IoT) and Machine Learning (ML) can join their potential to form educationally fruitful and also practically important activities targeted at increasing the social awareness for the water misuse problem, like the ones proposed herein. These activities also encourage the deployment of low-cost appliances that, only with minor modifications, can respond to a wide variety real problems in either urban or rural environments.",https://ieeexplore.ieee.org/document/9766555/,2022 IEEE Global Engineering Education Conference (EDUCON),28-31 March 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCMST54943.2021.00014,Internet of Things-Based Middleware Against Cyber-Attacks on Smart Homes using Software-Defined Networking and Deep Learning,IEEE,Conferences,"Internet of Things (IoT) devices are expected to number about 3.5 billion by 2023.; a tremendous amount of Internet of Things (IoT) data that is generating (IoT) devices is estimated to exceed 79.4 zettabytes by 2025. Security challenges will become an increasingly significant issue, especially in smart homes that depend entirely on IoT devices. Due to the weak infrastructure of the IoT, it is vulnerable to various types of cyber-attacks. The most common IoT attacks are distributed denial of service (DDoS). Most traditional security solutions, like intrusion detection systems (IDS), cannot detect most attacks. Complexity is being hidden by a new paradigm that recently arose, called the software-defined system that is brings a significant change to the networking industry, a great solution for mitigation of attacks that can adopt deep learning technique to encounter cyber-attacks based on the attack behavior and by filtering normal and attack traffic by using well-defined rules. This paper proposed a system by suggesting middleware that can help mitigate or prevent various attacks on IoT on smart home environment. Machine learning has included in the middleware to provide automatic protection against cyber-attacks on IoT networks. A promising approach to protecting real-time, highly accurate attacks on SDN-managed IoT networks has been proposed. This middleware allows IoT devices to efficiently handle evolving security threats dynamically and adaptively without impacting the IoT devices.",https://ieeexplore.ieee.org/document/9784593/,2021 2nd International Conference on Computational Methods in Science & Technology (ICCMST),17-18 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MetroInd4.0IoT48571.2020.9138288,Introducing a cloud based architecture for the distributed analysis of Real-Time Ethernet traffic,IEEE,Conferences,"The use of industrial communication protocols based on Real-Time Ethernet (RTE) standards is completely replacing traditional industrial fieldbuses. As usual, when a technology becomes mature, the need of efficient diagnostic and maintenance tools quickly raises. Very often, following the paradigm of Industry 4.0, the most effective diagnostic systems are today based on distributed, cloud-centric, architectures and artificial intelligence. However, the distributed analysis of RTE systems is challenging, considering the plurality of protocols and the stringent cost constrains which are common in industry. In this paper, a new architecture for the distributed analysis of RTE networks is proposed, leveraging on distributed probes that send traffic samples to Matlab cloud for remote analysis. The paper also proposes a software conversion tool to adapt general PCAP files captured by popular sniffers (e.g. Wireshark) into MAT file for easier Matlab elaboration. Last, a test bench for characterization (in terms of transfer delays) of the first part of the chain for RTE traffic sampling is described. The results show that in less than 10 seconds it is possible to transfer chunks of RTE traffic data (captured on industrial networks with hundreds of real-time devices) directly to the cloud and to have them converted in Matlab format.",https://ieeexplore.ieee.org/document/9138288/,2020 IEEE International Workshop on Metrology for Industry 4.0 & IoT,3-5 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PerComWorkshops48775.2020.9156201,"Invited Talk: Software Engineering, AI and autonomous vehicles: Security assurance",IEEE,Conferences,"In this talk, I will first walk through some real industrial requirements and research challenges in autonomous vehicles. I will then talk about research works which can potentially solve these issues, mainly covering training, testing and anomaly detection for autonomous systems and driver behaviour detection. The talk will be broad but covering some state of the art interesting research questions and directions in autonomous vehicles safety and security assurance, and human vehicle interaction which shall be suitable both for researchers and industry practitioners for in-depth enquiry and collaboration.",https://ieeexplore.ieee.org/document/9156201/,2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),23-27 March 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSNW.2012.6264658,Invited talk: Challenges in Medical Cyber-Physical Systems,IEEE,Conferences,"Summary form only given. As computers and communication bandwidth become ever faster and cheaper, computing and communication capabilities are embedded in all types of objects and structures in the physical environment. Harnessing these capabilities to bridge the cyber-world with the physical world will allow the development of applications with great societal impact and economic benefit. At the heart of these applications are cyber-physical systems consisting of integrated computational and communication cores that interact with the physical world, with intelligence provided by embedded software. Cyber Physical Systems (CPS) are engineered systems that provide tight integration of and coordination between the cyber world of computing and communications and the physical world. CPS are to meet the needs of the new generation of engineered systems that are highly dependable, efficiently produced and certified, and capable of advanced performance in computation, communication, and control. CPS will transform how we interact with and control the physical world around us, as the Internet transformed how we interact and communicate with one another and revolutionized how and where we access information. One application domain of CPS is Medical Cyber-Physical Systems (MCPS), which are life-critical, context-aware, networked systems of medical devices. Medical device industry is undergoing a transformation, embracing the potential of embedded software and network connectivity. Instead of stand-alone devices that can be designed, certified, and used to treat patients independent of each other, distributed systems that simultaneously control multiple aspects of the patient's physiology are increasingly used in hospitals to provide high-quality continuous care for patients. The combination of embedded software controlling medical devices, networking capabilities, and complicated physiological dynamics of patient bodies makes MCPS complex. The need to design complex MCPS that are both safe and effective presents numerous challenges, including achieving high assurance in system software, interoperability, context-aware intelligence, autonomy, security and privacy, and device certification. In this talk, I will discuss these challenges in developing MCPS and present some of our work in addressing them, and several open research and development issues.",https://ieeexplore.ieee.org/document/6264658/,IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN 2012),25-28 June 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DASA51403.2020.9317177,"IoT Driven Resiliency with Artificial Intelligence, Machine Learning and Analytics for Digital Transformation",IEEE,Conferences,"A new manufacturing era “Industry 4.0” is emerging with two unique characteristics: intelligent manufacturing and integrated manufacturing. This pattern rationale with the progress of digital transformation, in which efficient manufacturing and production systems is being continuously pursued. Digital transformation initiatives generate large data sets due to massive integration of devices in internet of things (IoT) environment. This scenario demands the fastest insights to respond on time considering three key pillars: communication network evolution, digital business, and customer experience. IoT driven resiliency with traditional analytics has limited value without artificial intelligence (AI), and machine learning (ML). This study aims to explore this phenomenon of interest by conducting group discussion with software vendors. The results will helpful to utilize the power of AI and ML with analytics to leverage a large amount of data which would contribute to the success of digital transformation of organizations with real-time decision-making.",https://ieeexplore.ieee.org/document/9317177/,2020 International Conference on Decision Aid Sciences and Application (DASA),8-9 Nov. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIAICT.2019.8784859,IoT-NDN: An IoT Architecture via Named Data Netwoking (NDN),IEEE,Conferences,"Internet of Things (IoT) systems have become the central part of future internet research. In the IoT, heterogeneous devices are connected to sense the environment or to observe individual tasks. Many research fields use the seamless IoT infrastructure to interact with the integrated devices and diverse services. Furthermore, IoT is a promising technology to increase the comfort and quality of life and opens new ways of interaction between people and things. Real life applications in healthcare sectors, home automation, industry, smart cities, monitoring scenarios, etc. benefit from the low-cost wireless technology in IoT on one hand. On the contrary, IoT system has many challenging features: many devices are resource-constrained with energy and memory, are highly heterogeneous and their applications continuously transmit transient information. Furthermore, Requesting, delivering and updating the information in IoT are challenging because of the resource limitation. Named Data Networking (NDN) is one of the latest and the most important Information-Centric Networking (ICN) approaches which uses named data to deliver data in the network. Based on hierarchically structured names, NDN matches the application pattern of IoT systems and uses its communication concept to optimize the power supply and distribute the data efficiently in the network. This paper discusses the main concepts of NDN including naming, routing, forwarding and caching in IoT infrastructure. To achieve an efficient system in different applications scenario in future IoT, an IoT architecture is proposed via NDN called IoT-NDN. The objective of this research work is the design and development of IoT-NDN for different fields in IoT systems. The deployment of IoT-NDN is challenging and requires proper design choices. The proposed solution and challenges of all mentioned issues are discussed in this paper.",https://ieeexplore.ieee.org/document/8784859/,"2019 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)",1-3 July 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FMEC49853.2020.9144776,IoT-WLAN Proximity Network for Potentiostats,IEEE,Conferences,"The implementation of potentiostats as portable and communicated devices has reached significant progress to benefit research, industry, and education. The Internet of Things (IoT) is a good opportunity to interconnect devices such as the potentiostats together with electronics, communication technologies, and chemistry into a single system. This work proposes a network for potentiostats using machine-to-machine (M2M) protocols, modifying its functioning mechanism in the broker to check the payload of the message that passes through it and synchronize the sensors depending on its content. Although one sensor can be synchronized directly to another, the broker decides which sensor to pair. This modification was made in the M2M protocol algorithm, both in the Broker and in the Client (sensor). In addition to this, the network uses an interconnection architecture of IoT smart networks of proximity with centralized management. The results of the tests carried out showed that the use of a modified M2M such as the one proposed in the architecture allows synchronization and comparison of the measurements of several sensors in real-time.",https://ieeexplore.ieee.org/document/9144776/,2020 Fifth International Conference on Fog and Mobile Edge Computing (FMEC),20-23 April 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IGSC48788.2019.8957164,IoT/CPS Ecosystem for Efficient Electricity Consumption : Invited Paper,IEEE,Conferences,"Modern society relies on smart systems like internet of things (IoT) and cyber physical systems (CPS) to monitor and control physical processes. The widespread deployment of IoT and CPSs result in fast growth of sensor data as physical processes are constantly monitored by billions of IP-enabled sensors (44 zettabytes by 2020). Hence, fog nodes are deployed to make network edge rich in computing resources to enable real-time data analytics using artificial intelligence/machine learning (AI/ML) for Big data generated from IoT and CPSs. This paper proposes IoT/CPS ecosystem for smart grid (SG) utilizing industry 4.0 concept to manage and control the loads using an intelligent predictive controller based on artificial neural network (ANN). The ANN is trained to predict the loads in certain districts based on previous smart meter readings installed at consumers and substations. This is a novel approach which integrates IoT/CPSs ecosystem into electric power system to deliver energy to consumers with high efficiency, reduce the cost, optimize the energy consumption, improve the reliability and enable real-time monitoring of power consumption.",https://ieeexplore.ieee.org/document/8957164/,2019 Tenth International Green and Sustainable Computing Conference (IGSC),21-24 Oct. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC52423.2021.9658712,Is Timing Critical to Trace Reconstruction?,IEEE,Conferences,"Dynamic analysis of real-world software systems is challenging due to imperfections, noise and data loss. Moreover, these systems evolve with time and their requirements are usually either not clearly specified or unknown, which makes it hard to analyze them. Therefore, it is important to create models that can learn to behave similarly to these systems to enable us to predict their actions, recover missing data, or detect potential failures ahead of time.Several models have been proposed to model sequential data, but the vast majority of them only have a qualitative notion of time or no notion of it at all. In this paper, we extend the work on incorporating a quantitative notion of time to RNN and introduce Time GRU. This modified GRU can learn the behaviour of complex software systems to a very high degree of accuracy. Our approach is scalable and has shown state-of-the-art performance on industry-strength software with real operating logs from Blackberry’s QNX real-time operating system. The proposed model can predict upcoming sequences of events more than 100 timesteps ahead in time with more than 90% accuracy. This allows for significant improvement in trace reconstruction and failure explainability.",https://ieeexplore.ieee.org/document/9658712/,"2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",17-20 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCA.2010.5416648,Is hardware innovation over?,IEEE,Conferences,"My colleagues, promotion committees, research funding agencies and business people often wonder if there is need for any architecture research. There seems to be no room to dislodge Intel IA-32. Even the number of new Application-Specific Integrated Circuits (ASICs) seems to be declining each year, because of the ever-increasing development cost. This viewpoint ignores another reality which is that the future will be dominated by mobile devices such as smart phones and the infrastructure needed to support consumer services on these devices. This is already restructuring the IT industry. To the first-order, in the mobile world functionality is determined by what can be supported within a 3W power budget. The only way to reduce power by one to two orders of magnitude is via functionally specialized hardware blocks. A fundamental shift is needed in the current design flow of systems-on-a-chip (SoCs) to produce them in a less-risky and cost-effective manner. In this talk we will present, via examples, a method of designing systems that facilitates the synthesis of complex SoCs from reusable ¿IP¿ modules. The technical challenge is to provide a method for connecting modules in a parallel setting so that the functionality and the performance of the composite are predictable.",https://ieeexplore.ieee.org/document/5416648/,HPCA - 16 2010 The Sixteenth International Symposium on High-Performance Computer Architecture,9-14 Jan. 2010,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2017.8022721,Issues with conducting controlled on-line experiments for E-Commerce,IEEE,Conferences,"More and more on-line experiments have been done in E-Commerce in order to understand the behavior of users or customers and then apply the data analysis technique to provide business guidance. One of the techniques is A/B testing. However, there is not clear guidance on the sample size in order for us to have valuable, trustable discovery. The purpose of this work is to find out a way to group customers in the data sample in order to achieve an optimal difference between the buckets. Based on the analysis result of real data collected during joining an industry project, we think the problem is complex and the meaningful conclusions have to be drawn with caution from business experiments such as A/B testing, due to the vast variation in the data. Moreover, if we don't allocate enough samples in the treatment group, the experiment could be inconclusive even if the testing lasts for a longer enough time, such as one month.",https://ieeexplore.ieee.org/document/8022721/,"2017 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",26-28 June 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSS53909.2021.9721979,Kernel-based Class-specific Broad Learning System for software defect prediction,IEEE,Conferences,"With the continuous expansion of the software industry, the problem of software defects is receiving more and more attention. There has been a series of machine learning methods applied to the field of software defect prediction (SDP) as a way to ensure the stability of software. However, SDP suffers from the imbalance problem. To solve this problem, we first propose a class-specific broad learning system (CSBLS), which assigns a specific penalty factor to each class in accordance with the data distribution. Then we design a class-specific kernel-based broad learning system (CSKBLS), which adopts kernel mapping instead of random projection. This additive kernel scheme takes into account both outliers and noise in the data set. Extensive experiments on the real-world NASA datasets show that CSKBLS outperforms the comparison methods on the tasks of software defect prediction.",https://ieeexplore.ieee.org/document/9721979/,"2021 8th International Conference on Information, Cybernetics, and Computational Social Systems (ICCSS)",10-12 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICIS52592.2021.9694131,Keynote Speech 2: Data-driven Machine Learning Precision Livestock Farming Technologies and Applications,IEEE,Conferences,"Summary form only given. The complete presentation was not made available for publication as part of the conference proceedings. The paper details the features of a platform that implements PLF management strategies for the dairy industry. The platform elements comprise robust, high node count sensor networks gathering data from individual animals and a cloud based software environment that manages on-farm data and pro-actively alerts the farmer, real time, of key operational and management interventions. The principle is that if the needs of animals at the individual level are properly defined and met, then the needs of farmers and downstream stakeholders including consumers follow. The more precisely that needs are met, the less waste there is in the system, resulting in greater economic and environmental benefits. In turn, the creation of new business models based on provisioning a range of services to livestock farmers becomes possible, promoting the easy uptake of technology to all in the supply chain. The platform is scalable in terms of handling multiple data streams, able to manage farms increasing in size including hybrid environments and support remote farms harnessing the growth of the connected world. The platform captures data from cows located in different areas, collates this information, and presents it to a mix including the farmers/herdsmen, veterinarians, feed specialists over various mediums, such as smartphone, home computer or within the parlor.",https://ieeexplore.ieee.org/document/9694131/,2021 Tenth International Conference on Intelligent Computing and Information Systems (ICICIS),5-7 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCTA48790.2019.9478839,Keynote Speech II: Readiness for the Impact of Emerging Technologies,IEEE,Conferences,"Summary form only given. The complete presentation was not made available for publication as part of the conference proceedings. The digital world is becoming increasingly intertwined with the physical world of machines, to which it is bringing ubiquitous intelligence and a perpetual flow of information. These trends are driving us towards a very different future. That future has already started. A new wave of social, economic, and psychological changes is expected to abruptly affect almost everything we do. With change, many opportunities come along. Those who anticipate the course of the future, and prepare for it, will be ready to seize these opportunities and will come out winners. Those who chose to ignore the signs of change, will risk losing their livelihood and eventually hurting their families, businesses, and societies. Those who see the storm coming but react by standing still in panic, disgruntlement, and lamentation will be defenseless when the inevitable waves hit their shores. This presentation overviews the trends in technology and applications, including Artificial Intelligence, Big Data Analytics, Robotics, Internet of Things, Industry 4.0, etc. The impact that such advances are likely to have on the high-tech as well as the low-tech job markets is outlined. Some actions and initiatives are proposed and discussed, with the purpose of triggering a larger debate on how individuals, businesses, academic institutions, and governments should prepare for the anticipated massive changes that are already beginning to affect our world.",https://ieeexplore.ieee.org/document/9478839/,2019 29th International Conference on Computer Theory and Applications (ICCTA),29-31 Oct. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ReConFig48160.2019.8994747,Keynote2: Global-Scale FPGA-Accelerated Deep Learning Inference with Microsoft's Project Brainwave,IEEE,Conferences,"The computational challenges involved in accelerating deep learning have attracted the attention of computer architects across academia and industry. While many deep learning accelerators are currently theoretical or exist only in prototype form, Microsoft's Project Brainwave is in massive-scale production in data centers across the world. Project Brainwave runs on our Catapult networked FPGAs, which provide latency that is low enough to enable ""Real-time Al"" - deep learning inference that is fast enough for interactive services and achieves peak throughput at a batch size of 1. Project Brainwave powers the latest version of Microsoft's Bing search engine, which uses cutting-edge neural network models that are much larger than the neural networks used in typical benchmarks. In this talk, I'll discuss how Project Brainwave's FPGA-based and software components work together to accelerate both first-party workloads - like Bing search - and third-party applications using neural network models, like high energy physics and manufacturing quality control. I'll also talk about how FPGAs are the perfect platform for the fast-changing world of deep neural networks, since their reconfigurability allows us to update our accelerator in place to keep up with the state of the art.",https://ieeexplore.ieee.org/document/8994747/,2019 International Conference on ReConFigurable Computing and FPGAs (ReConFig),9-11 Dec. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PerComWorkshops51409.2021.9431114,Keynote: Explainable-by-design Deep Learning,IEEE,Conferences,"MACHINE and AI justifiably attract the attention and interest not only of the wider scientific community and industry, but also society and policy makers. However, even the most powerful (in terms of accuracy) algorithms such as deep learning (DL) can give a wrong output, which may be fatal. Due to the opaque and cumbersome model structure used by DL, some authors started to talk about a dystopian “black box” society. Despite the success in this area, the way computers learn is still principally different from the way people acquire new knowledge, recognise objects and make decisions. People do not need a huge amount of annotated data. They learn by example, using similarities to previously acquired prototypes, not by using parametric analytical models. Current ML approaches are focused primarily on accuracy and overlook explainability, the semantic meaning of the internal model representation, reasoning and its link with the problem domain. They also overlook the efforts to collect and label training data and rely on assumptions about the data distribution that are often not satisfied. The ability to detect the unseen and unexpected and start learning this new class/es in real time with no or very little supervision is critically important and is something that no currently existing classifier can offer. The challenge is to fill this gap between high level of accuracy and the semantically meaningful solutions. The most efficient algorithms that have fuelled interest towards ML and AI recently are also computationally very hungry - they require specific hardware accelerators such as GPU, huge amounts of labeled data and time. They produce parametrised models with hundreds of millions of coefficients, which are also impossible to interpret or be manipulated by a human. Once trained, such models are inflexible to new knowledge. They cannot dynamically evolve their internal structure to start recognising new classes. They are good only for what they were originally trained for. They also lack robustness, formal guarantees about their behaviour and explanatory and normative transparency. This makes problematic use of such algorithms in high stake complex problems such as aviation, health, bailing from jail, etc. where the clear rationale for a particular decision is very important and the errors are very costly. All these challenges and identified gaps require a dramatic paradigm shift and a radical new approach. In this talk the speaker will present such a new approach towards the next generation of computationally lean ML and AI algorithms that can learn in real-time using normal CPUs on computers, laptops, smartphones or even be implemented on chip that will change dramatically the way these new technologies are being applied. It is explainable-by-design. It focuses on addressing the open research challenge of developing highly efficient, accurate ML algorithms and AI models that are transparent, interpretable, explainable and fair by design. Such systems are able to self-learn lifelong, and continuously improve without the need for complete retraining, can start learning from few training data samples, explore the data space, detect and learn from unseen data patterns, collaborate with humans or other such algorithms seamlessly.",https://ieeexplore.ieee.org/document/9431114/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC.2008.188,Keynote: Towards An Ontological Foundation for Evolving Agent Communities,IEEE,Conferences,"Research in ontology management has reached a certain level of maturity, however, there is still little  understanding of, and technological support for, the methodological and evolutionary aspects of ontologies as resources. Yet these are crucial in distributed and collaborative worlds such as the Semantic  Web, where ontologies and their communities of use naturally and mutually co-evolve. Through a deep understanding of the real-time, community-driven, evolution of so-called ontologies, a semantic agent system can be made operationally relevant and sustainable over long periods of time. Such a paradigm shift in knowledge-intensive and community-driven systems would affect knowledge sharing and communication across diverse communities in business, industry, and society. In this paper, we give an overview of the practical and theoretical challenges and limitations, and based on that introduce an ontological and methodological foundation for community evolution processes.",https://ieeexplore.ieee.org/document/4591613/,2008 32nd Annual IEEE International Computer Software and Applications Conference,28 July-1 Aug. 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2017.8122711,Knowledge extracted from recurrent deep belief network for real time deterministic control,IEEE,Conferences,"Recently, the market on deep learning including not only software but also hardware is developing rapidly. Big data is collected through IoT devices and the industry world will analyze them to improve their manufacturing process. Deep Learning has the hierarchical network architecture to represent the complicated features of input patterns. Although deep learning can show the high capability of classification, prediction, and so on, the implementation on GPU devices are required. We may meet the trade-off between the higher precision by deep learning and the higher cost with GPU devices. We can success the knowledge extraction from the trained deep learning with high classification capability. The knowledge that can realize faster inference of pre-trained deep network is extracted as IF-THEN rules from the network signal flow given input data. Some experiment results with benchmark tests for time series data sets showed the effectiveness of our proposed method related to the computational speed.",https://ieeexplore.ieee.org/document/8122711/,"2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",5-8 Oct. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SC41405.2020.00025,Kraken: Memory-Efficient Continual Learning for Large-Scale Real-Time Recommendations,IEEE,Conferences,"Modern recommendation systems in industry often use deep learning (DL) models that achieve better model accuracy with more data and model parameters. However, current opensource DL frameworks, such as TensorFlow and PyTorch, show relatively low scalability on training recommendation models with terabytes of parameters. To efficiently learn large-scale recommendation models from data streams that generate hundreds of terabytes training data daily, we introduce a continual learning system called Kraken. Kraken contains a special parameter server implementation that dynamically adapts to the rapidly changing set of sparse features for the continual training and serving of recommendation models. Kraken provides a sparsity-aware training system that uses different learning optimizers for dense and sparse parameters to reduce memory overhead. Extensive experiments using real-world datasets confirm the effectiveness and scalability of Kraken. Kraken can benefit the accuracy of recommendation tasks with the same memory resources, or trisect the memory usage while keeping model performance.",https://ieeexplore.ieee.org/document/9355295/,"SC20: International Conference for High Performance Computing, Networking, Storage and Analysis",9-19 Nov. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCNC.2019.8651681,Lameness Detection as a Service: Application of Machine Learning to an Internet of Cattle,IEEE,Conferences,"Lameness is a big problem in the dairy industry, farmers are not yet able to adequately solve it because of the high initial setup costs and complex equipment in currently available solutions, and as a result, we propose an end-to-end IoT application that leverages advanced machine learning and data analytics techniques to identify lame dairy cattle. As part of a real world trial in Waterford, Ireland, 150 dairy cows were each fitted with a long range pedometer. The mobility data from the sensors attached to the front leg of each cow is aggregated at the fog node to form time series of behavioral activities (e.g., step count, lying time and swaps per hour). These are analyzed in the cloud and lameness anomalies are sent to farmer's mobile device using push notifications. The application and model automatically measure and can gather data continuously such that cows can be monitored daily. This means there is no need for herding the cows, furthermore the clustering technique employed proposes a new approach of having a different model for subsets of animals with similar activity levels as opposed to a one size fits all approach. It also ensures that the custom models dynamically adjust as weather and farm condition change as the application scales. The initial results indicate that we can predict lameness 3 days before it can be visually captured by the farmer with an overall accuracy of 87%. This means that the animal can either be isolated or treated immediately to avoid any further effects of lameness.",https://ieeexplore.ieee.org/document/8651681/,2019 16th IEEE Annual Consumer Communications & Networking Conference (CCNC),11-14 Jan. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CarpathianCC.2017.7970404,Landing area recognition by image applied to an autonomous control landing of VTOL aircraft,IEEE,Conferences,"The pattern recognition aims to classify objects on different categories based on characteristics analysis. The usage of pattern recognition shows itself more and more frequent and widely used, covering different areas both in industry and research and development of new technologies. With that in mind, this work aims to compare two nonlinear classifiers, the Adaptive Boosting method and the Artificial Neural Network method, applied to the identification of a certain landmark, where the more profitable is inserted in a Vertical Take-Off and Landing (VTOL) aircraft real model to trigger the land action after a demanded mission in the trained pattern presence. It is used as sensing method, computer vision technique, from camera's acquired images the characteristics are extracted by a proceeding based on Viola-Jones technique. To optimize the classification, it is also used the Principal Component Analysis method to uncouple the amount of data in the training stage and optimize the results in both classifiers. To prove the efficiency of the classifier when the aircraft is flying, it is used to test a scenario where it is possible to simulate the landing action with different altitudes. The Adaptive Boosting method proved itself to be more advantageous due to its simple implementation and less computational processing effort, despite the slightly lower performance when it comes to classifying compared to the Artificial Neural Network. The Principal Component Analysis method also shows itself to be a good improvement when applied to both techniques, raising the success rate of the classifiers in all the tested cases. The results obtained in the simulation tests were considered satisfactory as the aircraft lands with great precision over the determined landmark after identifying the landing area used for training.",https://ieeexplore.ieee.org/document/7970404/,2017 18th International Carpathian Control Conference (ICCC),28-31 May 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISGT.2016.7781159,Large-scale detection of non-technical losses in imbalanced data sets,IEEE,Conferences,"Non-technical losses (NTL) such as electricity theft cause significant harm to our economies, as in some countries they may range up to 40% of the total electricity distributed. Detecting NTLs requires costly on-site inspections. Accurate prediction of NTLs for customers using machine learning is therefore crucial. To date, related research largely ignore that the two classes of regular and non-regular customers are highly imbalanced, that NTL proportions may change and mostly consider small data sets, often not allowing to deploy the results in production. In this paper, we present a comprehensive approach to assess three NTL detection models for different NTL proportions in large real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and Support Vector Machine. This work has resulted in appreciable results that are about to be deployed in a leading industry solution. We believe that the considerations and observations made in this contribution are necessary for future smart meter research in order to report their effectiveness on imbalanced and large real world data sets.",https://ieeexplore.ieee.org/document/7781159/,2016 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT),6-9 Sept. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICT4DA53266.2021.9672224,Latency optimized architectures for a real-time inference pipeline for control tasks,IEEE,Conferences,"With the increasing development of GPUs, the inference time of CNNs continues to decrease. This enables new AI applications in manufacturing that have a direct impact on the control of a process. For this, a GPU is integrated into a real-time system so that the CNN can be executed in real-time. However, it is not sufficient to consider the inference process only, but also to minimize the latency of the whole pipeline. For this purpose, execution strategies of the inference pipeline are presented and evaluated in this paper. The presented architectures are compared using criteria for latency, implementation effort, and exchangeability. The latencies are quantified with measurements on a demonstrator. As a result, the most synchronous architecture has the lowest latency but is not suitable for the use in a service-oriented architecture as targeted by the Industry 4.0. For this, another architecture is presented, providing a good balance between latency and service orientation.",https://ieeexplore.ieee.org/document/9672224/,2021 International Conference on Information and Communication Technology for Development for Africa (ICT4DA),22-24 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NICS.2018.8606862,Lead Engagement by Automated Real Estate Chatbot,IEEE,Conferences,"Recently, automated chatbot has been increasingly applied in real estate industry. Even though chatbots cannot fully replace the traditional relation between agents and home buyers, they can help to engage potential clients (or leads) in meaningful conversations, which is highly useful for lead capture. In this paper, we present an intelligent chatbot for this purpose. Various machine learning techniques, including multi-task deep learning technique for intent identification and frequent itemsets for conversation elaboration, have been employed in our system. Our chatbot has been deployed by CEO K35 GROUP JSC with daily updated data of real estate information at Hanoi and Ho Chi Minh cities, Vietnam.",https://ieeexplore.ieee.org/document/8606862/,2018 5th NAFOSTED Conference on Information and Computer Science (NICS),23-24 Nov. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VPPC49601.2020.9330855,Learning Based Energy Management Strategy Offline Trainers Comparison for Plug-in Hybrid Electric Buses,IEEE,Conferences,"The automotive industry is facing a transformation towards the massive digitalization and data-acquisition of the vehicles operation. The exploitation of operational data opens up new opportunities in the energy efficiency improvement of the vehicles. In this regard, the combination of optimization techniques with neural networks and fuzzy systems in one unified framework, known as learning-based energy management strategies, have been identified as promising methods. These learning-based techniques combine the optimized operation with the IF-THEN human-type reasoning simplicity of a fuzzy system through neural-type of learning. Therefore, fuzzy-neural networks are the bridge that allows to learn offline from the optimal operation and design energy management strategy for real time implementation. In this regard, the main contribution of this paper lies on the comparison of a previously developed ANFIS approach with a simpler Neo-Fuzzy neuron based, with the aim to evaluate the tradeoff between accuracy and computational and structural efficiency. The proposed approach represents a fuzzy-neural structure with less parameters for training that is expected to facilitate its future real time application for energy management strategies for each bus from a fleet operating on a predefined route.",https://ieeexplore.ieee.org/document/9330855/,2020 IEEE Vehicle Power and Propulsion Conference (VPPC),18 Nov.-16 Dec. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2018.8460502,Learning Motion Predictors for Smart Wheelchair Using Autoregressive Sparse Gaussian Process,IEEE,Conferences,"Constructing a smart wheelchair on a commercially available powered wheelchair (PWC) platform avoids a host of seating, mechanical design and reliability issues but requires methods of predicting and controlling the motion of a device never intended for robotics. Analog joystick inputs are subject to black-box transformations which may produce intuitive and adaptable motion control for human operators, but complicate robotic control approaches; furthermore, installation of standard axle mounted odometers on a commercial PWC is difficult. In this work, we present an integrated hardware and software system for predicting the motion of a commercial PWC platform that does not require any physical or electronic modification of the chair beyond plugging into an industry standard auxiliary input port. This system uses an RGB-D camera and an Arduino interface board to capture motion data, including visual odometry and joystick signals, via ROS communication. Future motion is predicted using an autoregressive sparse Gaussian process model. We evaluate the proposed system on real-world short-term path prediction experiments. Experimental results demonstrate the system's efficacy when compared to a baseline neural network model.",https://ieeexplore.ieee.org/document/8460502/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDUCON.2018.8363365,Learning analytics for location-based serious games,IEEE,Conferences,"Pervasive gaming has experimented a huge commercial growth with location-based game successes such as Pokémon GO or Ingress. The serious game industry has an opportunity to take advantage of location-based mechanics to better connect games with the real world, creating more authentic immersive learning environments. Games such as historical tours, story-based exploration, laboratories, or flora explorations can greatly benefit from location-based mechanics. Location-based games usually include an augmented map to provide game context, which combines both traditional game-dependent elements such as avatars, and location-based elements such as areas or points of interest. For players, interacting with location-based elements may involve entering or exiting specific areas; or reaching a certain location and looking in a specific direction. To include standards-based learning analytics for location-based serious games (SGs), we have added support for player movement and location-based interactions to the pre-existing xAPI serious game profile. We have validated this approach through a case-study example that guided players through different sports-related facilities within a large outdoor area. This work have been carried out and it is available as part of the analytics infrastructure used in EU H2020 RAGE and BEACONING serious game projects.",https://ieeexplore.ieee.org/document/8363365/,2018 IEEE Global Engineering Education Conference (EDUCON),17-20 April 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CITS.2015.7297720,Learning automaton based context oriented middleware architecture for precision agriculture,IEEE,Conferences,"The continuous alteration of living landscape in earth due to the greatest impact of information technology has converted the whole earth into a small digital village. That means anyone could share information in any form with any other person residing anywhere in the earth. With the availability of information and communication technology like cloud computing, heterogeneous networking, crowd sensing, web services and data mining, anywhere and anytime information sharing is possible, but this will bring out lot of challenges like incompatibility in standards, data portability, data aggregation, data dissemination, differential context and communication overhead. The ICT has changed many aspects of human lifestyle, work places and living spaces. However, there is one sector namely agriculture which has been deprived of the real advantages of ICT. Hence, there exists a digital divide between farming industry and other industries. Farming industry consumes large amount of natural resources such as water, energy and fertilizers that could escalate issues such as global warming, soil degradation and depletion of ground water to the next level. To reduce the global warming effect, farming industry needs to be integrated with relevant technologies. This paper proposes ubiquitous context oriented middleware architecture for precision agriculture (LA-COMPa) to solve major issues such as waste of water, improper application of fertilizer, choice of wrong crops and season, poor yield and lack of marketing.",https://ieeexplore.ieee.org/document/7297720/,"2015 International Conference on Computer, Information and Telecommunication Systems (CITS)",15-17 July 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR-Adjunct54149.2021.00036,Learning to Perceive: Perceptual Resolution Enhancement for VR Display with Efficient Neural Network Processing,IEEE,Conferences,"Even though the Virtual Reality (VR) industry is experiencing a rapid growth with ever-expanding demands today, VR applications have yet to provide a fully immersive experience. The insufficient resolution of the VR head-mounted display (HMD) hinders the user from further immersion into the virtual world. In this work, we attempt to enhance the immersive experience by improving the perceptual resolution of VR HMDs. We employ an efficient neural-network-based approach with the proposed temporal integration loss function. By taking the temporal integration mechanism of the Human Visual System (HVS) into account, our network learns the perception process of the human eye, and temporally upsamples a sequence that in turn improves its perceived resolution. Specifically, we discuss a possible scenario where we deploy our approach on a VR system equipped with the eye-tracking technology, which could save up to 75% of the computational load. Compared with the state-of-the-art in terms of the inference time analysis and a user experiment, it shows that our approach runs around 1.89× faster and produces more favorable results.",https://ieeexplore.ieee.org/document/9585895/,2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),4-8 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/DATE.2019.8714959,Learning to infer: RL-based search for DNN primitive selection on Heterogeneous Embedded Systems,IEEE,Conferences,"Deep Learning is increasingly being adopted by industry for computer vision applications running on embedded devices. While Convolutional Neural Networks' accuracy has achieved a mature and remarkable state, inference latency and throughput are a major concern especially when targeting low-cost and low-power embedded platforms. CNNs' inference latency may become a bottleneck for Deep Learning adoption by industry, as it is a crucial specification for many real-time processes. Furthermore, deployment of CNNs across heterogeneous platforms presents major compatibility issues due to vendor-specific technology and acceleration libraries.In this work, we present QS-DNN, a fully automatic search based on Reinforcement Learning which, combined with an inference engine optimizer, efficiently explores through the design space and empirically finds the optimal combinations of libraries and primitives to speed up the inference of CNNs on heterogeneous embedded devices. We show that, an optimized combination can achieve 45x speedup in inference latency on CPU compared to a dependency-free baseline and 2x on average on GPGPU compared to the best vendor library. Further, we demonstrate that, the quality of results and time ""to-solution"" is much better than with Random Search and achieves up to 15x better results for a short-time search.",https://ieeexplore.ieee.org/document/8714959/,"2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)",25-29 March 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PDP50117.2020.00041,Lessons learned from comparing C-CUDA and Python-Numba for GPU-Computing,IEEE,Conferences,"Python as programming language is increasingly gaining importance, especially in data science, scientific, and parallel programming. It is faster and easier to learn than classical programming languages such as C. However, usability often comes at the cost of performance and applications written in Python are considered to be much slower than applications written in C or FORTRAN. Further, it does not allow the usage of GPUs-besides of pre-compiled libraries.However, the Numba package promises performance similar to C code for compute intensive parts of a Python application and it supports CUDA, which allows the use of GPUs inside a Python application.In this paper we compare the performance of Numba-CUDA and C -CUDA for different kinds of applications. For compute intensive benchmarks, the performance of the Numba version only reaches between 50% and 85% performance of the CCUDA version, despite the reduction operation, where the Numba version outperforms CUDA. Analyzing the PTX code and CUDA performance counters revealed that index-calculation is one limiting factor in Numba. Another problem is the type interference for single precision computations, as some values are computed in double precision. By optimizing this within the Numba package, the performance of Numba improves. However, C-CUDA applications still outperform the Numba versions. Further analysis with the CloverLeav Mini App shows that Numba performance further decreases for applications with multiple different compute kernels. The non-GPU part slows down these applications, due to the slow Python interpreter. This leads to a worse GPU utilization.Today Python is widely used in industry and academia and has been the first choice of coding languages among software programmers in the last years. Currently, according to the TIOBE index [5], it is the 3rd most popular programming language and the number one in IEEE Spectrum's fifth annual interactive ranking of the top programming languages [4]. One reason for this is that is easier to learn than classical programming languages like C. However, the other reason is the increasing popularity of Data Science, where Python is the most used language. A collection of libraries such as NumPy [22], and Matplotlib [1] or Scipy [8] provide a rich set of functions for scientific computing [16]. Packages like Dask [19], PyCompss [21] and MPI for Python [6] allow running Python applications on large, parallel machines, promising high performance. However, the performance of Python is considered slow compared to compiled languages such as C, C++, and FORTRAN, especially for heavy computations. In recent years, more and more tools have been developed to counter this prejudice. Numpy [22], for example, uses C-like arrays to store data and offers fast functions implemented in C to speed up calculations. The CuPy [14] package provides a similar set of functions, but these functions are implemented for GPUs using CUDA. The SciPy library is based on NumPy and provides a rich set on functionalities for scientific computing. Still, the high performance of these libraries is provided by the underling C-implementations. Internally, they use libraries like OpenBlas or IntelMKL to reach high performance and therefore, they are limited by the functions which are provided by theses libraries. Therefore, a performance problem always arises when the required functionality is not implemented within these libraries. In this case, the application falls back to the Python interpreter. Compared to ""bare metal"" code, interpreted code is slow. In addition, in Python it is not possible to use GPUs or other accelerators directly, as the Python interpreter cannot execute code on these machines. Therefore, the usage is only possible with precompiled libraries. To overcome this limitation, different approaches where developed to mix C, CUDA or OpenCL with Python. Cython [2] allows integrating C-code in Python applications to improve performance of critical sections. It also allows an easy development of wrappers for C-libraries. Similar, packages such as PyCuda and PyOpenCL [9] support wrappers for CUDA or OpenCL code within a Python script. Both approaches require the mixture of different programming languages.Numba [10] follows a different approach. Instead of merging C/CUDA code with Python, it allows the development of efficient applications for both, CPUs and GPUs in Python style. When a Python script using Numba is executed, marked functions are compiled just-in-time (JIT) using the LLVM framework. Using Python for GPU programming can mean a considerable simplification in the development of parallel applications.But often a simplification of comes at the expense of performance, and one expects a performance loss from Python compared to pure C code. In this paper, we want to understand the differences between native C-CUDA code and CUDA-code written in Python with Numba. We also want to share some basic tips how to improve the performance of applications written in Numba.We will first analyse a few micro benchmarks in detail. We are using these simple benchmarks, as it is easier to understand the differences with small code examples. We will use the collected information to derive some optimization for Numba. Finally, we evaluate and compare the performance of a more application like mini-app, written in C-CUDA and Numba accelerated Python. We will evaluate if our insights from the microbenchmarks to real applications.",https://ieeexplore.ieee.org/document/9092407/,"2020 28th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)",11-13 March 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPP.2011.40,Location-Aware MapReduce in Virtual Cloud,IEEE,Conferences,"MapReduce is an important programming model for processing and generating large data sets in parallel. It is commonly applied in applications such as web indexing, data mining, machine learning, etc. As an open-source implementation of MapReduce, Hadoop is now widely used in industry. Virtualization, which is easy to configure and economical to use, shows great potential for cloud computing. With the increasing core number in a CPU and involving of virtualization technique, one physical machine can hosts more and more virtual machines, but I/O devices normally do not increase so rapidly. As MapReduce system is often used to running I/O intensive applications, decreasing of data redundancy and load unbalance, which increase I/O interference in virtual cloud, come to be serious problems. This paper builds a model and defines metrics to analyze the data allocation problem in virtual environment theoretically. And we design a location-aware file block allocation strategy that retains compatibility with the native Hadoop. Our model simulation and experiment in real system shows our new strategy can achieve better data redundancy and load balance to reduce I/O interference. Execution time of applications such as RandomWriter, Text Sort and Word Count are reduced by up to 33% and 10% on average.",https://ieeexplore.ieee.org/document/6047196/,2011 International Conference on Parallel Processing,13-16 Sept. 2011,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2017.8123012,Long short term memory networks for short-term electric load forecasting,IEEE,Conferences,"Short-term electricity demand forecasting is critical to utility companies. It plays a key role in the operation of power industry. It becomes all the more important and critical with increasing penetration of renewable energy sources. Short-term load forecasting enables power companies to make informed business decisions in real-time. Demand patterns are extremely complex due to market deregulation and other environmental factors. Although there has been extensive research in the area of short-term electrical load forecasting, difficulties in implementation and lack of transparency in results has been cited as a main challenge. Deep neural architectures have recently shown their ability to mine complex underlying patterns in various domains. In our work, we present a deep recurrent neural architecture to unearth the complex patterns underlying the regional demand profiles without specific insights from the utilities. The model learns from historical data patterns. We show that deep recurrent neural network with long-short term memory architecture presents a robust methodology for accurate short term load forecasting with the ability to adapt and learn the underlying complex features over time. In most cases it matches the performance of the latest state-of-the-art techniques and even supercedes it in a few cases.",https://ieeexplore.ieee.org/document/8123012/,"2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",5-8 Oct. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSRE.2016.46,MHCP Model for Quality Evaluation for Software Structure Based on Software Complex Network,IEEE,Conferences,"Accidents caused by defective software systems have long been a nightmare. Though engineers utilize advanced techniques and rigorous quality control procedures, we still have to admit that the increasing complexity and expanding scale of software systems make it extremely difficult to guarantee high quality deliverables. Since large-scale software systems exhibit the characteristics of complex networks, applying the principles of complex networks to evaluate the quality of software systems has attracted attention from both academia and industry. Unfortunately, most current research studies focus only on one or a limited number of attributes of software structures which makes them ineffective in providing comprehensive and insightful quality evaluation for software structures. To overcome this problem, we propose an approach based on various software structural characteristics to evaluate software structures from modularity, hierarchy, complexity, and fault propagation points of view. A model based on these four aspects is proposed to better understand software structural quality. A prediction model is also proposed to provide insights on the nature of software evolution and its current status. Experiments using two software projects were performed against the thresholds obtained by evaluating more than 5,000 versions of open source projects. Our results suggest that the approach described in this paper can help us analyze real-world software projects for better quality evaluation.",https://ieeexplore.ieee.org/document/7774529/,2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE),23-27 Oct. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/QRS54544.2021.00093,MINTS: Unsupervised Temporal Specifications Miner,IEEE,Conferences,"Specifications for software systems are quite often missing or are obsolete given the evolutionary nature of these systems. Lack of precise software specifications makes the task of debugging and detecting a malfunction of system behavior challenging. Prior works have primarily focused on extracting system specifications in the form of template-based mining frameworks or interactive simulation models. In safety-critical systems where the time of occurrence of events is of prime importance extracting specifications with a quantitative notion of time seems a daunting task. This work presents an unsupervised approach to mine timed temporal properties in the form of deterministic finite state machines with a custom-designed trie data structure. Our frame-work, MINTS learns dominant system specifications from their system traces that are represented as a timed deterministic finite state machine. MINTS is shown to be sound and complete. MINTS scalability and correctness is validated using real-world industry strength traces.",https://ieeexplore.ieee.org/document/9724772/,"2021 IEEE 21st International Conference on Software Quality, Reliability and Security (QRS)",6-10 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCA45697.2020.00045,MLPerf Inference Benchmark,IEEE,Conferences,"Machine-learning (ML) hardware and software system demand is burgeoning. Driven by ML applications, the number of different ML inference systems has exploded. Over 100 organizations are building ML inference chips, and the systems that incorporate existing models span at least three orders of magnitude in power consumption and five orders of magnitude in performance; they range from embedded devices to data-center solutions. Fueling the hardware are a dozen or more software frameworks and libraries. The myriad combinations of ML hardware and ML software make assessing ML-system performance in an architecture-neutral, representative, and reproducible manner challenging. There is a clear need for industry-wide standard ML benchmarking and evaluation criteria. MLPerf Inference answers that call. In this paper, we present our benchmarking method for evaluating ML inference systems. Driven by more than 30 organizations as well as more than 200 ML engineers and practitioners, MLPerf prescribes a set of rules and best practices to ensure comparability across systems with wildly differing architectures. The first call for submissions garnered more than 600 reproducible inference-performance measurements from 14 organizations, representing over 30 systems that showcase a wide range of capabilities. The submissions attest to the benchmark&#x2019;s flexibility and adaptability.",https://ieeexplore.ieee.org/document/9138989/,2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA),30 May-3 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM.2018.8485910,MV-Sports: A Motion and Vision Sensor Integration-Based Sports Analysis System,IEEE,Conferences,"Recently, intelligent sports analytics is becoming a hot area in both industry and academia for coaching, practicing tactic and technical analysis. With the growing trend of bringing sports analytics to live broadcasting, sports robots and common playfield, a low cost system that is easy to deploy and performs real-time and accurate sports analytics is very desirable. However, existing systems, such as Hawk-Eye, cannot satisfy these requirements due to various factors. In this paper, we present MV-Sports, a cost-effective system for real-time sports analysis based on motion and vision sensor integration. Taking tennis as a case study, we aim to recognize player shot types and measure ball states. For fine-grained player action recognition, we leverage motion signal for fast action highlighting and propose a long short term memory (LSTM)-based framework to integrate MV data for training and classification. For ball state measurement, we compute the initial ball state via motion sensing and devise an extended kalman filter (EKF)-based approach to combine ball motion physics-based tracking and vision positioning-based tracking to get more accurate ball state. We implement MV-Sports on commercial off-the-shelf (COTS) devices and conduct real-world experiments to evaluate the performance of our system. The results show our approach can achieve accurate player action recognition and ball state measurement with sub-second latency.",https://ieeexplore.ieee.org/document/8485910/,IEEE INFOCOM 2018 - IEEE Conference on Computer Communications,16-19 April 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INCET49848.2020.9154161,Machine Learning Approach to Estimation of Internal Parameters of a Single Phase Transformer,IEEE,Conferences,"Transformer is the heart of power industry. So, it is necessary to track its performance every instant. Internal parameters of any transformer helps evaluate the performance and improve output waveform, raising efficiency[1]. They also state the condition of the internal windings[2]. It requires offloading of transformer to evaluate its internal parameters which is very costly since transformer needs to operate always to fully recover the investment. Many iterative analysis techniques have been proposed by many researchers in the past. Machine Learning is the current state of the art technology whose application is proposed in this paper. Various statistical methods are deployed in the training of such algorithms which comprises of Linear Regression, Stochastic Gradient Decent, Support Vector Machine and many more[3]. But, real time applications require minimum latency so Levenberg-Marquardt algorithm has been proposed for evaluation with higher accuracy.",https://ieeexplore.ieee.org/document/9154161/,2020 International Conference for Emerging Technology (INCET),5-7 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICoDT255437.2022.9787424,"Machine Learning based Theoretical Framework for Failure Prediction, Detection, and Correction of Mission-Critical Flight Software",IEEE,Conferences,"Mission-critical flight software acts as the control mechanism for autonomous flights and lies at the heart of next-generation developments in the aviation industry. Most state-of-the-art technological evolution is realized through the use of contemporary software which implements the essentially required, novel, innovative, and featuring value additions. Real-time physical exposure and the data-driven flying nature of aerial vehicles make them vulnerable to an ever-evolving new threat spectrum of cyber security. Nation or state-sponsored cyber attacks through sensors&#x2019; data corruption, hardware Trojans, or counterfeit wireless signals may exploit dormant and residual software vulnerabilities. It may lead to severe and catastrophic consequences including but not limited to serious injury or death of the crew, extreme damage or loss to equipment and environment. We have proposed a machine learning based theoretical framework for real-time monitoring and failure analysis of autonomous flight software. It has been introduced to protect the mission-critical flight software from run-time data-driven semantic bugs and exploitation that may be caused by missing, jammed, or spoofed data values, due to malicious online cyber activities. The effectiveness of the proposed framework has been demonstrated by the evaluation of a real-world incident of grounding an aerial vehicle by the actors in their vicinity without the intent of the original equipment manufacturer (OEM). The results show that the reported undesired but successful cyber attack may has been avoided by the effective utilization of our proposed cyber defense approach, which is targeted at software failure prediction, detection, and correction for autonomous aerial vehicles.",https://ieeexplore.ieee.org/document/9787424/,2022 2nd International Conference on Digital Futures and Transformative Technologies (ICoDT2),24-26 May 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MECO49872.2020.9134073,Machine Learning for Network Routing,IEEE,Conferences,"Though currently a &#x201C;hot topic&#x201D;, over the past fifteen years [1][2], there has been significant work on the use of machine learning to design large scale computer-communication networks, motivated by the complexity of the systems that are being considered and the unpredictability of their workloads. A topic of great concern has been security [3] and novel techniques for detecting network attacks have been developed based on Machine Learning [8]. However the main challenge with Machine Learning methods in networks has concerned their compatibility with the Internet Protocol and with legacy systems, and a major step forward has come from the establishment of Software Defined Networks (SDN) [4] which delegate network routing to specific SDN routers [4]. SDN has become an industry standard for concentrating network management and routing decisions within specific SDN routers that download the selected paths periodically to network routers, which operate otherwise under the IP protocol. In this paper we describe our work on real-time control of Security and Privacy [7], Energy Consumption and QoS [6] of packet networks using Machine Learning based on the Cognitive Packet Network [9] principles and their application to the H2020 SerIoT Project [5].",https://ieeexplore.ieee.org/document/9134073/,2020 9th Mediterranean Conference on Embedded Computing (MECO),8-11 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICCS53718.2022.9788210,Machine Learning in 6G: The Future of Wireless Communication,IEEE,Conferences,"The 6<sup>th</sup> Generation of wireless communication, 6G, is the future of wireless communication technology. 6G has the potential to support various pioneering applications. Post Covid-19 pandemic, the necessity of remote data-connectivity is recognized as a socio-economic requirement. Data-centric communication has been one of the key-features in 5G and the trend is expected to continue in near future. 6G proposes to support various cutting-edge applications, including remote healthcare, holographic transportation, twin body area network, unmanned vehicle, smart infrastructures and augmented and virtual reality. These applications require a superior data-rate and infrastructure beyond the current 5G facilities. Hence, currently, both industry and academia are exploring the possible technology solutions that can support the advanced communication requirements of near future. This research work intends to provide a comprehensive summary of the Key Performance Indicators (KPI) of 6G, possible 6G network architectures, use cases and enabling technologies. Additionally, an extensive summary of the Machine Learning (ML) algorithms implemented in 6G are provided to achieve various technology goals.",https://ieeexplore.ieee.org/document/9788210/,2022 6th International Conference on Intelligent Computing and Control Systems (ICICCS),25-27 May 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2019.8935703,Machine Learning in Failure Regions Detection and Parameters Analysis,IEEE,Conferences,"Testing automation is one of the challenges facing the software development industry, especially for large complex products. This paper proposes a mechanism called multi stage failure detector (MSFD) for automating black box testing using different machine learning algorithms. The input to MSFD is the tool's set of parameters and their value ranges. The parameter values are randomly sampled to produce a large number of parameter combinations that are fed into the software under test. Using neural networks, the resulting logs from the tool are classified into passing and failing logs and the failing logs are then clustered (using mean shift clustering) into different failure types. MSFD provides visualization of the failures along with the responsible parameters. Experiments on and results for two real-world complex software products are provided, showing the ability of MSFD to detect all failures and cluster them into the correct failure types, thus reducing the analysis time of failures, improving coverage, and increasing productivity.",https://ieeexplore.ieee.org/document/8935703/,"2019 20th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",8-11 July 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIMS.2018.8355297,Machine learning algorithms for estimating powder blend composition using near infrared spectroscopy,IEEE,Conferences,"This paper presents a NIRS based real time continuous monitoring of powder blend composition which has widespread applications such as the pharmaceutical industry. The paper extends the implementation of several machine learning methodologies applied to sensor data collected using an NIR spectrometer for a model powder blending process. Several techniques were examined for the development of chemometric models of the multi-sensor data, including Principal Component Analysis (PCA), Partial Least Squares Regression (PLSR), Support Vector Machines (SVM) and Artificial Neural Networks (ANN). The performances of each of the models were compared in terms of accuracy (MSE) in predicting blend composition. The results obtained show that machine learning-based approaches produce process models of similar accuracy and robustness compared to models developed by PLSR while requiring minimal pre-processing and also being more adaptable to new data. The paper also discusses the prospect of using Convolutional Neural Networks (CNN) for NIRS data analysis.",https://ieeexplore.ieee.org/document/8355297/,2018 2nd International Symposium on Small-scale Intelligent Manufacturing Systems (SIMS),16-18 April 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICSS.2016.7882607,Machine learning algorithms for process analytical technology,IEEE,Conferences,"Increased globalisation and competition are drivers for process analytical technologies (PAT) that enable seamless process control, greater flexibility and cost efficiency in the process industries. The paper will discuss process modelling and control for industrial applications with an emphasis on solutions enabling the real-time data analytics of sensor measurements that PAT demands. This research aims to introduce an integrated process control approach, embedding novel sensors for monitoring in real time the critical control parameters of key processes in the minerals, ceramics, non-ferrous metals, and chemical process industries. The paper presents a comparison of machine learning algorithms applied to sensor data collected for a polymerisation process. Several machine learning algorithms including Adaptive Neuro-Fuzzy Inference Systems, Neural Networks and Genetic Algorithms were implemented using MATLAB® Software and compared in terms of accuracy (MSE) and robustness in modelling process progression. The results obtained show that machine learning-based approaches produce significantly more accurate and robust process models compared to models developed manually while also being more adaptable to new data. The paper presents perspectives on the potential benefits of machine learning algorithms with a view to their future in the industrial process industry.",https://ieeexplore.ieee.org/document/7882607/,2016 World Congress on Industrial Control Systems Security (WCICSS),12-14 Dec. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IntelliSys.2017.8324372,Machine learning and deep neural network — Artificial intelligence core for lab and real-world test and validation for ADAS and autonomous vehicles: AI for efficient and quality test and validation,IEEE,Conferences,"Autonomous vehicles are now the future of automobile industry. Human drivers can be completely taken out of the loop through the implementation of safe and intelligent autonomous vehicles. Although we can say that HW and SW development continues to play a large role in the automotive industry, test and validation of these systems is a must. The ability to test these vehicles thoroughly and efficiently will ensure their proper and flawless operation. When a large number of people with heterogeneous knowledge and skills try to develop an autonomous vehicle together, it is important to use a sensible engineering process. State of the art techniques for such development include Waterfall, Agile & V-model, where test & validation (T&V) process is an integral part of such a development cycle. This paper will propose a new methodology using machine learning & deep neural network (AI-core) for lab & real-world T&V for ADAS (Advanced driver assistance system) and autonomous vehicles. The methodology will initially connect T&V of individual systems in each level of development and that of complete system efficiently, by using the proposed phase methodology, in which autonomous driving functions are grouped under categories, special T&V processes are carried on simulation as well as in HIL systems. The complete transition towards AI in the field of T&V will be a sequence of steps. Initially the AI-core is fed with available test scenarios, boundary conditions for the test cases and scenarios, and examples, the AI-core will conduct virtual tests on simulation environment using available test scenarios and further generates new test cases and scenarios for efficient and precise tests. These test cases and scenarios are meant to cover all available cases and concentrate on the area where bugs or failures occur. The complete surrounding environment in the simulation is also controlled by the AI-core which means that the system can attain endless/all-possible combinations of the surrounding environment which is necessary. Results of the tests are sorted and stored, critical and important tests are again repeated in the real-world environment using automated cars with other real subsystems to depict the surrounding environment, which are all controlled by the AI-core, and meanwhile the AI-core is always in the loop and learning from each and every executed test case and its results/outcomes. The main goal is to achieve efficient and high quality test and validation of systems for automated driving, which can save precious time in the development process. As a future scope of this methodology, we can step-up to make most parts of test and validation completely autonomous.",https://ieeexplore.ieee.org/document/8324372/,2017 Intelligent Systems Conference (IntelliSys),7-8 Sept. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/cp.2020.0049,Machine learning based anomaly-based intrusion detection system in a full digital substation,IET,Conferences,"The cyberattacks that occurred in recent years have raised concerns in critical infrastructures, including power system networks. Identifying ongoing attacks is essential to enable the energy industry to respond to adversaries. Many commercial products and research projects include machine learning based intrusion detection systems but there is still a need for understanding the data training requirements for those systems in order to successfully deploy them to protect power systems. This paper presents the development of an anomaly-based Intrusion Detection System (IDS) based on a machine learning methodology to create a whitelist. The system was implemented using GNU Octave. It was trained using traffic flow from real devices generated from a Virtual Site Acceptance Testing and Training (VSATT) platform where multi-vendor secondary devices were set up and communicated to each other. The system was then tested using different datasets which were also generated from the VSATT platform. Results show that the implemented IDS performed correctly under different case studies. The results also indicate that the learned traffic identifies GOOSE and MMS messages based on the normal behaviours from those protocols, but the presence of other messages might require manual inputs to be incorporated in the training dataset.",https://ieeexplore.ieee.org/document/9449350/,15th International Conference on Developments in Power System Protection (DPSP 2020),9-12 March 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CompComm.2017.8322600,Machine learning to detect anomalies in web log analysis,IEEE,Conferences,"As the information technology develops rapidly, Web servers are easily to be attacked because of their high value. Therefore, Web security has aroused great concern in both academia and industry. Anomaly detection plays a significant role in the field of Web security, and log messages recording detailed system runtime information has become an important data analysis object accordingly. Traditional log anomaly detection relies on programmers to manually inspect by keyword search and regular expression match. Although the programmers can use intrusion detection system to reduce their workload, yet the log system data is huge, attack types are various, and hacking skills are improving, which make the traditional detection not efficient enough. To improve the traditional detection technology, many of anomaly detection mechanisms have been proposed in recent years, especially the machine learning method. In this paper, an anomaly detection system for web log files has been proposed, which adopts a two-level machine learning algorithm. The decision tree model classifies normal and anomalous data sets. The normal data set is manually checked for the establishment of multiple HMMs. The experimental data comes from the real industrial environment where log files have been collected, which contain many true intrusion messages. After comparing with three types of machine learning algorithms used in anomaly detection, the experimental results on this data set suggest that this system achieves higher detection accuracy and can detect unknown anomaly data.",https://ieeexplore.ieee.org/document/8322600/,2017 3rd IEEE International Conference on Computer and Communications (ICCC),13-16 Dec. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSTC.2017.8011854,Machinery equipment early fault detection using Artificial Neural Network based Autoencoder,IEEE,Conferences,"Machinery equipment early fault detection is still in an open challenge. The objective of this paper is to introduce a parametric method Artificial Neural Network based Autoencoder implemented to perform early fault detection of a machinery equipment. The performance of this method is then compared to one of the industry state of the art nonparametric methods called Similarity Based Modeling. The comparison is done by analyzing the implementation result on both artificial and real case dataset. Root Mean Square Error (RMSE) is applied to measure the performance. Based on the result of the research, both of these methods are effective to do pattern recognition and able to identify data anomaly or in this case is fault identification.",https://ieeexplore.ieee.org/document/8011854/,2017 3rd International Conference on Science and Technology - Computer (ICST),11-12 July 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DISCOVER.2018.8673981,MalDetect: A Framework to detect Fast Flux Domains,IEEE,Conferences,"Performing passive or active attacks through malware-infected systems (bots) by hiding the identity of an attacker, referred to as fast flux is one of the common threat in the context of security. With the connectivity of millions of unsecured systems to networks, detecting fast-flux based attack is one of the major challenge to the industry. This paper presents, a framework which discriminates fast flux domains from Content Distribution Network (CDN) in real-time. The proposed framework embeds features such as, DNS query response, Geographical-location of IP addresses, network distinction and delay in our framework to detect the fast flux domains. Our model has been evaluated using five different machine learning algorithms and out of which, the Random Forest (RF) algorithm performed the best with an F1 score of 0.9915 and Matthews Correlation Coefficient of 0.9672. We also did experimentation on different feature sets individually to identify the best performing feature set in detecting the fast flux domains. We observed Geographical location-based feature set outperformed than the other feature set with a significant accuracy and precision.",https://ieeexplore.ieee.org/document/8673981/,"2018 IEEE Distributed Computing, VLSI, Electrical Circuits and Robotics (DISCOVER)",13-14 Aug. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DTPI52967.2021.9540077,Mechanical Design Paradigm based on ACP Method in Parallel Manufacturing,IEEE,Conferences,"Parallel Manufacturing is a new manufacturing paradigm in industry, deeply integrating informalization, automation, and artificial intelligence. In this paper we propose a new mechanical design paradigm in Parallel Manufacturing based on ACP method. The key is to regard the design procedure based on artificial design and emulation method as two independent procedures, which can be modeled as a parallel system. The design procedure based on ACP method does not include a real system, which is an inventive extension of the traditional parallel system. This method can be implemented with social information by introducing the definition of SDV, SDM, and Intelligent Design Manager, making it highly adaptive for social manufacturing and Parallel Manufacturing.",https://ieeexplore.ieee.org/document/9540077/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2014.6888682,Microcredit risk assessment using crowdsourcing and social networks,IEEE,Conferences,"The task of automated risk assessment is attracting significant attention in the light of the recent microloan popularity growth. The industry requires a real time method for the timely processing of the extensive number of applicants for short-term small loans. Owing to the vast number of applications, manual verification is not a viable option. In cooperation with a microloan company in Azerbaijan, we have researched automated risk assessment using crowdsourcing. The principal concept behind this approach is the fact that a significant amount of information relating to a particular applicant can be retrieved from the social networks. The suggested approach can be divided into three parts: First, applicant information is collected on social networks such as LinkedIn and Facebook. This can only occur with the applicant's permission. Then, this data is processed using a program that extracts the relevant information segments. Finally, these information segments are evaluated using crowdsourcing. We attempted to evaluate the information segments using social networks. To that end, we automatically posted requests on the social networks regarding certain information segments and evaluated the community response by counting “likes” and “shares”. For example, we posted the status, “Do you think that a person who has worked at ABC Company is more likely to repay a loan? Please “like” this post if you agree.” From the results, we were able to estimate public opinion. Once evaluated, each information segment was then given a weight factor that was optimized using available loan-repay test data provided to us by a company. We then tested the proposed system on a set of 400 applicants. Using a second crowdsourcing approach, we were able to confirm that the resulting solution provided a 92.5% correct assessment, with 6.45% false positives and 11.11% false negatives, with an assessment duration of 24 hours.",https://ieeexplore.ieee.org/document/6888682/,"15th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",30 June-2 July 2014,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECAI.2017.8166463,Mitigating DoS attacks in publish-subscribe IoT networks,IEEE,Conferences,"Internet of Things (IoT) is an emerging subject which enables multiple applications and requires robust security solutions. IoT architectures contribute to critical aspects of our society like transportation, health-care, industry, telecommunications and many others. Security is difficult to be implemented in the IoT context because the embedded devices are resource constrained and deployed in uncontrolled areas, thus being targeted by various security attacks. In scenarios like smart city, IoT networks are populated by both trusted and untrusted transient devices, thus mechanisms for mitigating complex security attacks must be implemented. Embedded networks with near real-time constraints are subject to denial-of-service attacks which can easily affect the applications availability by injecting malicious packets into the network. This paper proposes a lightweight security mechanism which addresses DoS attacks in IoT publish-subscribe applications.",https://ieeexplore.ieee.org/document/8166463/,"2017 9th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",29 June-1 July 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISADS.1999.838444,Mobile agents in autonomous decentralized systems,IEEE,Conferences,"In the last years mobile agents have gained much attention in the industry and academic community. Many prototype systems and some products providing mobile agent capability have been developed and mobile agents have been proposed for being used in a set of application areas, such as electronic commerce, workflow systems, network management, among others. In the last years we also experienced an enormous development in the direction of network centric computing. Complex distributed environments emerge that enable and support the integration of systems and that allow new forms of automated cooperation. For an effective integration to take place, the autonomy of participants in the environment must be taken into consideration. Such environments represent then examples of autonomous decentralised systems (ADSs). In this paper the authors present their view on the convenience of applying mobile agents to ADSs. Firstly, some benefits of mobile agents that can be explored in applications are presented. Afterwards, a discussion of some issues that shall enable more effective use of mobile agents in real applications and that have impact on the definition of the scope of applicability of mobile agents to ADSs is presented.",https://ieeexplore.ieee.org/document/838444/,Proceedings. Fourth International Symposium on Autonomous Decentralized Systems. - Integration of Heterogeneous Systems -,23-23 March 1999,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SCC.2018.00014,Modeling Sentiment Polarity in Support Ticket Data for Predicting Cloud Service Subscription Renewal,IEEE,Conferences,"In the cloud based service provisioning industry, one of the main challenges that providers face involves keeping existing tenants engaged while attracting new ones. To address this, providers need to gain insights about customer satisfaction. In that regards, support ticket data, understood as the main way of communication between both parties, can be mined to obtain an estimation of customer satisfaction by means of the polarity of the sentiment extracted from the report descriptions. To that end, in this work we propose a model which can learn a feature representation for sentiment polarity changes, from the sequence of tickets emitted by a given customer during the period associated with the service subscription term. Then, that resulting feature representation, combined with other handcrafted features related to contract and ticket data, is passed to a classifier which estimates the likelihood of service subscription renewal by the customer. Experiment results using real data from a service provider shows that learned representation of sentiment polarity changes from support ticket data in combination with other handcrafted features improves the accuracy in predicting subscription renewals. Moreover, our architecture is flexible enough incorporate and integrate several feature representations and give more expressive power to the prediction.",https://ieeexplore.ieee.org/document/8456400/,2018 IEEE International Conference on Services Computing (SCC),2-7 July 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAT.2006.85,Modeling and Application of Virtual Machine Tool,IEEE,Conferences,"The recent years of the 21th Century are associated with the advent of virtual reality technologies for modern industry and manufacturing engineering. Virtual Machine Tool Technology is given to design, test, control and machine parts in a virtual reality environment. This paper presents the methods to model and simulate the virtual machine tools in response to change in the machining requirements. Specifically, a set of module combination rules and a modeling method of the structure of machine tools using connectivity graph are developed. By this way virtual machine tool is implemented. The developed virtual machine tool can be efficiently used for industry training and machine leaning and operating.",https://ieeexplore.ieee.org/document/4089203/,16th International Conference on Artificial Reality and Telexistence--Workshops (ICAT'06),29 Nov.-1 Dec. 2006,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE45063.2020.9152407,Modeling and Predicting an Industrial Process Using a Neural Network and Automation Data,IEEE,Conferences,"Production optimization and prevention of faults and unplanned production halts are areas of particular interest in industry. Predictive analysis is commonly implemented with data analytics and machine learning techniques. Usually, the usage of such tools requires knowledge of the machine learning theory and the subject to be studied, e.g. a pumping process. This paper presents a case study on modeling of a pumping process using stored automation data. The model is trained to predict the performance percentage of the process with minimal background knowledge of the process and data analytics. The proposed model is built with IBM SPSS Modeler, a data analysis tool not usually used in real-time industrial predictive analysis as it is not often considered the best tool when working with time series data. The model is deployed in a cloud service to implement a real-time, visualized predictive analysis system. The case study shows that Modeler can be used for data analysis, modeling, and production purposes. Depending on the case, Modeler can provide an alternative tool compared with typical machine learning tools, as models built with Modeler can be deployed into a cloud service for production use. The findings indicate that industrial automation data are a valuable resource, and data analysis can be conducted on various platforms and tools.",https://ieeexplore.ieee.org/document/9152407/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ENABL.1996.555181,Modelling project coordination in a multi-agent framework,IEEE,Conferences,"In current engineering practice often traditional management structures and virtual organisations are combined. In addition to formal structures, informal, dynamic organisational structures emerge in which engineers are personally responsible for effective interaction. They decide when to exchange information, and with whom, when to question requirements, when to acknowledge conflicts, et cetera. In such virtual organisations, project coordination may become quite complicated. Communication and coordination in a real-life case of concurrent design in the aircraft industry has been modelled and specified within the modelling framework DESIRE, on the basis of Jennings' (1995) informal, multi-agent model of cooperative problem solving.",https://ieeexplore.ieee.org/document/555181/,Proceedings of WET ICE '96. IEEE 5th Workshop on Enabling Technologies; Infrastucture for Collaborative Enterprises,19-21 June 1996,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCSCT.2008.104,Monitoring and Early-Warning of the Supply Chain by Using System Dynamics and Neural Networks,IEEE,Conferences,"Effective management of a supply chain requires the ability to detect unexpected variations at an early stage, which brings the possibility of taking preventive decisions to mitigate the variations. This paper proposes a methodology that monitors the dynamic trends of supply chain performance indicators, and gives early-warnings for potential risks. Initially, a supply chain model is built using system dynamics. Then, based on this model, a neural network which can be trained to adapt to the real supply chain is developed. Acting as the kernel of monitoring and early-warning module, the neural network can make online predictions of dynamic trend of indicators so that an enterprise would have enough time to respond to any unwanted situations. The architecture of monitoring and early-warning module is proposed and a case study of the manufacturing industry is presented to illustrate the methodology and architecture.",https://ieeexplore.ieee.org/document/4731437/,2008 International Symposium on Computer Science and Computational Technology,20-22 Dec. 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WF-IoT.2019.8767291,Mountain Pine Beetle Monitoring with IoT,IEEE,Conferences,"Outbreaks of forest pests cause large-scale damages, which lead to significant impact on the ecosystem as well as the forestry industry. Current methods of monitoring pest outbreaks involve field, aerial and remote sensing surveys. These methods only provide partial spatial coverage and can detect outbreaks only after they have substantially progressed across wide geographic areas. This paper presents an IoT system for real-time insect infestation detection using bioacoustic recognition via machine learning techniques. Specifically, we focus on detecting the Mountain Pine Beetle (MPB), which is the most destructive insect of mature pines in western North American forests. We present the design of the system and describe its various hardware and software components. Experimental results collected from a prototype implementation of the system are presented, which show that the system can detect MPB with 82% accuracy. We also demonstrate the applicability of our system in other noise monitoring applications, and report our experimental results on urban noise detection and classification.",https://ieeexplore.ieee.org/document/8767291/,2019 IEEE 5th World Forum on Internet of Things (WF-IoT),15-18 April 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636717,Multi-Scale Aggregation with Self-Attention Network for Modeling Electrical Motor Dynamics,IEEE,Conferences,"Modeling induction motor dynamics is a crucial problem in the industry. The previous works mainly model the dynamics based on the physical model assumption and state equation. However, due to the complex internal structure of motors, the traditional methods cannot estimate dynamics precisely. To address this issue, we adopt a deep learning-based approach that takes the time-series motor data measured from the sensor to estimate the dynamics without making any assumptions about the motor’s interior. In this paper, we propose a multi-scale feature aggregation with self-attention network (MASNet) to deal with modeling motor dynamics. First, our model extracts multi-scale features from motor signals by various convolutional kernel sizes. Then, the proposed adaptive feature aggregation module fuses different receptive signals effectively. To further refine these high-level motor features, two-stream components, containing Bidirectional LSTM and self-attention module, are applied to improve motor contextual information. Moreover, we present a novel temporal relative loss to enhance consecutive signal consistency, which improves the performance of modeling dynamics. To deploy the service in the real-world scenario, our network is very lightweight and reduces the number of parameters from 0.62M to 0.09M (around 85% reduction) but outperforms state-of-the-art algorithms by extensive experiments on simulation and real-world motor datasets.",https://ieeexplore.ieee.org/document/9636717/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/MIPRO52101.2021.9596665,Multi-Stakeholder Engagement in Agile Service Platform Co-Creation,IEEE,Conferences,"In the recent decade, the concept of Smart Tourism Destination (STD) has emerged to represent both a strategic aim to develop sustainable competitive advantages for tourism destinations or wider regions, and a managerial approach and dimensions to enhance data-driven development to measure and apply smart technologies for competitive and inclusive change in a tourism ecosystem. In tourism destination development globally, the aim for sustainable digital transformation in low-productivity business sectors is one of the key drivers for smart tourism and smart destination development. More recently, the global Covid-19 pandemic has given a boost to touchless, digital and green tourism development initiatives in the EU and worldwide. To advance sustainable digital transformation with agile methods in STD development, a systemic service platform approach and agile prototyping with smart emerging technologies, (eg. with AI, IoT, Augmented & Virtual Reality, 5G and Robotics), and multi-stakeholder co-creation is needed to engage key tourism industry ecosystem representatives. This conceptual paper advocates that open-source solutions combined with comprehensive multi-stakeholder co-creation aid in prototyping a systemic digital platform solution for smart tourism destinations. The paper concludes with an illustration of a conceptual model of service platform development for smart destinations, utilizing network co-creation with quadruple-helix stakeholders for sustainable regional impacts.",https://ieeexplore.ieee.org/document/9596665/,"2021 44th International Convention on Information, Communication and Electronic Technology (MIPRO)",27 Sept.-1 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCSIC54682.2021.00059,Near-Real Time Quality Prediction in a Plastic Injection Molding Process Using Apache Spark,IEEE,Conferences,"The automotive industry is undergoing wide scope transformation. Industry 4.0 has both expanded the possibilities of digital transformation in automotive, increased its importance to all mobility ecosystem and being driven by continued digitization of the entire value chain. Manufacturing data which is unceasingly flow during serial production is one of the great sources towards Industry 4.0 goal to fully automatizing complex human dependent processes. However, there are few challenges to consider such as collecting and filtering various data from shop floor in given production cycle time range and make them ready for real time analytics as well as constructing efficient data pipeline to reach useful outcomes which is reliable enough to meet customer expectations. In this study, we will extract meaningful relation between injection machine parameters from Farplas Automotive Company's shop floor and describe their effects on the product quality. We will train and test machine learning models with different hyperparameters and test model performance to identify defected products. Finally, we will show implementation of streaming data pipeline using Kafka and Spark to be able to analyze injection machine data and effectively predict plastic injection product's OK-NOK condition real time even before human operator reaches the product itself. Consequently, detecting defected products will be independent from human attention which makes production areas one step closer to dark factory.",https://ieeexplore.ieee.org/document/9644395/,2021 International Symposium on Computer Science and Intelligent Controls (ISCSIC),12-14 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UralCon52005.2021.9559619,Neural Network for Real-Time Signal Processing: the Nonlinear Distortions Filtering,IEEE,Conferences,"Artificial neural networks, after their training and testing, allow, in the ""if then"" mode, to process signals in real time. This is relevant for signal processing tasks in electrical engineering and the electric power industry, especially for the analysis of nonlinear signal distortions, transients in electrical networks, during switching, emergency modes, and so on. The paper shows the neuro algorithm possibilities based on an elementary perceptron for filtering nonlinear distortions of industrial frequency signals of 50 Hz over a time interval of units of milliseconds. At the same time, the accuracy for the signals’ amplitude, phase, and frequency determining is units of percent. Examples of the fundamental frequency signal selection in the presence of harmonics, the determination of the signal parameters during transients, and the correction of the transformer saturation current are given. It is shown that real-time neural network processing can be carried out in a ""sliding window"" with the duration of units of milliseconds. The estimates made during the implementation of the neuro algorithm in a microprocessor device showed its application possibility in the electric power industry secondary equipment.",https://ieeexplore.ieee.org/document/9559619/,2021 International Ural Conference on Electrical Power Engineering (UralCon),24-26 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AFRCON.1996.562998,New solutions in the control of the Hungarian power system,IEEE,Conferences,"In the frame of a reconstruction project on the Hungarian Power Control System a lot of intelligent control tools are investigated. Some are accepted, others are rejected. Since 1993 the first Dispatcher Training Simulator of the Hungarian Electric Power Industry has been working in the regional dispatcher center of the North Hungarian Electricity Distribution Company (EMASZ KDSZ). The simulator performs discrete, event-driven simulation of the network, the protection and the telemetry system. A new continuous approach to the power system restoration problem is proposed. The continuous restoration means an on-line function that supports all the dispatcher actions. It is active and alerts not only the restorative network state, but also in the normal state. This real time expert system is strongly based on the co-operation with the SCADA-EMS functions and the different off-line network calculations. The idea is proposed to be implemented in the Hungarian National Control Centre. In 1994-95 a common project ('A Neural Network Application to Power System's Signal Processing') was performed in the institute KFKI MSZKI. We sought for the appropriate neural network structure, which fits the best the problem of event recognition. The research was extended for a sequential pattern matching algorithm too. On the base of the results we are working on an event recognizer application.",https://ieeexplore.ieee.org/document/562998/,Proceedings of IEEE. AFRICON '96,27-27 Sept. 1996,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE45063.2020.9152527,Next generation control units simplifying industrial machine learning,IEEE,Conferences,"The increasing amount of sensor data creates new possibilities through data-driven projects in industry. The demands on the final solution architecture are different and typically include at least one controller (e.g. PLC). In this paper, we focus on these industrial controllers and identify their possible roles in smart factories: Collection, distribution and preprocessing of field data, execution of intelligence and functionalities to ease rapid prototyping approaches. Furthermore, we specify the capabilities leading to the fulfillment of these roles and present an application example of drive anomaly detection to demonstrate how to setup an industrial controller for machine learning projects.",https://ieeexplore.ieee.org/document/9152527/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MED.2016.7535908,Nonlinear model predictive control hardware implementation with custom-precision floating point operations,IEEE,Conferences,"Model predictive control (MPC) based techniques have found many applications both in academia and in industry. Its reach, however, may not be compared to classical control techniques due to e.g. the difficulty of solving an optimization problem at each sampling interval with real-time requirements. Most of the efforts to make the application of MPC viable address this problem with more efficient solvers. This paper, in contrast, proposes a new approach for a real-time MPC solution by mapping an approximate off-line solution into an artificial neural network in a FPGA (Field Programmable Gate Array). We implemented a radial basis function artificial neural network on a low cost FPGA using custom precision floating point operations and tested the control on a single-link robotic manipulator. The amount of time used to calculate the control action at each time instant is in around one microsecond. The comparison between the offline and the approximate solution shows the soundness of the idea. We provide an analysis of hardware usage and execution time in order to achieve the best compromise considering the precision for a given application.",https://ieeexplore.ieee.org/document/7535908/,2016 24th Mediterranean Conference on Control and Automation (MED),21-24 June 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCNT49239.2020.9225680,"Notice of Violation of IEEE Publication Principles: 6G Wireless Communication: Its Vision, Viability, Application, Requirement, Technologies, Encounters and Research",IEEE,Conferences,"The fast development of multiband ultrafast seamless network and super reliable data transmission system to support heavy traffic applications such as artificial intelligence, machine learning, deep learning, augmented reality, virtual reality, 3D media, Internet of Things, Enterprise Internet of Thing and the Internet of Nano-things that involves with the real time transfer of data, voice and video in terabytes per second (Tb/s), the current cellular network (5G Network is insufficient to meet the growth of usage of triple play services in fraction of time). To meet the expectation of heavy data users is a big challenge in today's generation. To handle the situation of drastic demand of data, the sixth generation of mobile technology (6G) should be deeply studied along with its potential in terms of bandwidth, low latency, channel capacity, channel modeling techniques, loss propagation models, energy spectrum efficiency, faster network connectivity and data security. In this paper the vision in terms intelligent computing and wireless massive connectivity, feasibility, requirement in terms of modifying the existing 5G network, technologies in terms of artificial intelligence, 3D networking, SM-MIMO and optical computing, challenges after deployment, research to promote good health for 6G and application of 6G in the field of industry, automation sector, health, and transport has been studied and presented.",https://ieeexplore.ieee.org/document/9225680/,"2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",1-3 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2016.7515945,Notice of Violation of IEEE Publication Principles: Adaptive Fuzzy PID speed control of DC belt conveyor system,IEEE,Conferences,Conveyor belt system is one of the most common transfer systems used in industry to transfer goods from one point to another in a limited distance. It is used in industries such as the electromechanical /mechanical assembly manufacturing to transfer work piece from one station to another or one process to another in food industries. The belt conveyor system discussed in this paper is driven by a DC motor and two speed controllers. The PID speed controller is designed to provide comparison to the main controller which is the Adaptive Fuzzy PID Speed controller. Both controllers are implemented in a real hardware where the algorithm will be written in PLC using SCL language. The experimental result shows that Adaptive Fuzzy PID controller performs better and adapted to the changes in load much faster than the conventional PID controller. This project has also proved that PLC is capable of performing high level control system tasks..,https://ieeexplore.ieee.org/document/7515945/,"2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",30 May-1 June 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTSI.2017.8065980,OPEB: Open physical environment benchmark for artificial intelligence,IEEE,Conferences,"Artificial Intelligence methods to solve continuous-control tasks have made significant progress in recent years. However, these algorithms have important limitations and still need significant improvement to be used in industry and real-world applications. This means that this area is still in an active research phase. To involve a large number of research groups, standard benchmarks are needed to evaluate and compare proposed algorithms. In this paper, we propose a physical environment benchmark framework to facilitate collaborative research in this area by enabling different research groups to integrate their designed benchmarks in a unified cloud-based repository and also share their actual implemented benchmarks via the cloud. We demonstrate the proposed framework using an actual implementation of the classical mountain-car example and present the results obtained using a Reinforcement Learning algorithm.",https://ieeexplore.ieee.org/document/8065980/,2017 IEEE 3rd International Forum on Research and Technologies for Society and Industry (RTSI),11-13 Sept. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMART52563.2021.9675310,Object-Text Detection and Recognition System,IEEE,Conferences,"The object recognition system based on deep learning has been applied in different domains, e.g. in the Intelligent transportation system, Autonomous driving system, etc. Along with object detection, in numerous scenes, text detection and recognition have conjointly brought abundant attention and analysis application.Real-time object detection and dimensioning as well as text recognition are important topics for many branches of the industry today.The projected system consists of object – text, detection and recognition module, and a dimension measuring module. This system offers an improved method of classifying objects and calculating their measures in real-time from video sequences. The proposed system uses OpenCV libraries, which comprise erosion algorithms, canny edge detection, dilation, and contour detection. To accomplish the task of the text recognition Tesseract OCR engine is employed.",https://ieeexplore.ieee.org/document/9675310/,2021 10th International Conference on System Modeling & Advancement in Research Trends (SMART),10-11 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCIS48116.2019.9073682,On Improving Text Generation Via Integrating Text Coherence,IEEE,Conferences,"Automatic text generation techniques with either extractive-based or generative-based methods are becoming increasingly popular and widely used in industry. In contrast to existing extractive-based text generation approaches that ignore the text coherence, our proposed approach integrates both keyword coverage and text coherence into our optimization framework. In this paper, we employ the semantics-based coherence and syntax-based coherence metrics to evaluate the text coherence. Extensive experiments on a real corpus demonstrate that our method outperforms baselines overall regrading ROUGE and Human evaluation metrics. Our model provides new insights on how to utilize coherence measures to arrange the sentences extracted by keyword covering method. The proposed method has been deployed on a real system to help generate coherent text.",https://ieeexplore.ieee.org/document/9073682/,2019 IEEE 6th International Conference on Cloud Computing and Intelligence Systems (CCIS),19-21 Dec. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NCC52529.2021.9530062,On Traffic Classification in Enterprise Wireless Networks,IEEE,Conferences,"Enterprises today are quickly adopting intelligent, adaptive, and flexible wireless communication technologies in order to become compliant with Industry 4.0. One of the technological challenges related to this is to provide Quality of Services (QoS)-enabled network connectivity to the applications. Diverse QoS demands from the applications intimidate the underlying wireless networks to be agile and adaptive. Since the applications are diverse in nature, there must be a mechanism to learn the application types in near real-time so that the network can be provisioned accordingly. In this paper, we propose a Machine Learning (ML) based method to classify the application traffic. Our method is different from the existing port based and Deep Packet Inspection (DPI) based methods and uses statistical features of the network traffic related to the applications. We validate the performance of the proposed model in a lab based SDNized WiFi set-up. SDNization ensures that the proposed model can be deployed in practice.",https://ieeexplore.ieee.org/document/9530062/,2021 National Conference on Communications (NCC),27-30 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2015.7280779,On sequences of different adaptive mechanisms in non-stationary regression problems,IEEE,Conferences,"Existing adaptive predictive methods often use multiple adaptive mechanisms as part of their coping strategy in non-stationary environments. These mechanisms are usually deployed in a prescribed order which does not change. In this work we investigate and provide a comparative analysis of the effects of using a flexible order of adaptive mechanisms' deployment resulting in varying adaptation sequences. As a vehicle for this comparison, we use an adaptive ensemble method for regression in batch learning mode which employs several adaptive mechanisms to react to the changes in data. Using real world data from the process industry we demonstrate that such flexible deployment of available adaptive methods embedded in a cross-validatory framework can benefit the predictive accuracy over time.",https://ieeexplore.ieee.org/document/7280779/,2015 International Joint Conference on Neural Networks (IJCNN),12-17 July 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SITIS.2019.00053,On the Utility of Machine Learning for Service Capacity Management of Enterprise Applications,IEEE,Conferences,"The IT industry drives the digital transformation and, at the same time, is affected itself by related trends of automation and computerization. This paper examines the applicability of machine-learning techniques to the process of service capacity management for Commercial-off-the-shelf enterprise applications. We use real monitoring data from more than 18.000 SAP application and database instances which are running on more than 16.000 different servers in order to train performance models for standard business functions. A learning algorithm which is based on Boosted trees achieves sufficient accuracy to predict mean response times for ten frequently used transactions. To evaluate utility, models are successfully applied as part of a scenario-based demonstration in the fields of server sizing, load testing, and server consolidation with the objective to identify cost-effective designs. Results strongly emphasize the need to integrate monitoring data from uniform business applications in order to allow for novel and cost-effective capacity management service offerings.",https://ieeexplore.ieee.org/document/9067964/,2019 15th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS),26-29 Nov. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AUTEST.1989.81113,On the evaluation of real life test expert systems,IEEE,Conferences,"The authors consider the factors affecting the successful integration of expert systems (ESs) in testing and maintenance environments. The first factor considered has to do with the communication between the ES and the UUT (unit under test) expert and between the ES and the test technician. The authors discuss the importance of embedding in the expert system basic understanding of electronic terms and universal knowledge bases, i.e. knowledge which is not unique to a specific UUT. The second factor considered has to do with the communication between the ES and the ATE (automatic test equipment). It is claimed that artificial intelligence software will not penetrate the real-world test industry, unless it offers very smooth interfaces with test instrumentation and ATE. Several specific requirements are discussed. This work is based on extensive experience in introducing the AITEST expert system for electronic troubleshooting and service management to a wide variety of fields; including aerospace and military, automotive, computers and peripherals, communication and general instrumentation.<>",https://ieeexplore.ieee.org/document/81113/,IEEE Automatic Testing Conference.The Systems Readiness Technology Conference. Automatic Testing in the Next Decade and the 21st Century. Conference Record.,25-28 Sept. 1989,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ChiCC.2018.8483784,On-line Detection and Analysis of Alloy Steel Elements Based on the LIBS Technology and Random Forest Regression,IEEE,Conferences,"The Laser Induced Breakdown Spectroscopy (LIBS) technology can be used to detect the elements in the alloy steel in real time. Quantitative analysis method of the traditional LIBS technology mainly has the calibration method and calibration free method, but there are two shortcomings: low prediction accuracy and over fitting. Random Forest Regression (RFR) algorithm can be used for classification and regression, can effectively avoid “overfitting” phenomenon. Therefore, in this paper, we combine the random forest regression algorithm with laser induced breakdown spectroscopy applied to the detection of the concentration of alloy steel elements in the metallurgy industry. At the same time, compared with partial least squares method based on the LIBS, the results show that the random forest algorithm combined with the LIBS technology has the higher prediction accuracy, lower root mean square error and better robustness.",https://ieeexplore.ieee.org/document/8483784/,2018 37th Chinese Control Conference (CCC),25-27 July 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASCC.2015.7244597,Online sequential extreme learning machine algorithm based human activity recognition using inertial data,IEEE,Conferences,"Human activity recognition (HAR) is the basis for many real world applications concerning health care, sports and gaming industry. Different methodological perspectives have been proposed to perform HAR. One appealing methodology is to take an advantage of data that are collected from inertial sensors which are embedded in the individual's smartphone. These data contain rich amount of information about daily activities of the user. However, there is no straightforward analytical mapping between a performed activity and its corresponding data. Besides, online training for the classification in these types of applications is a concern. This paper aims at classifying human activities based on the inertial data collected from a user's smartphone. An Online Sequential Extreme Learning Machine (OSELM) method is implemented to train a single hidden layer feed-forward network (SLFN). Experimental results with an average accuracy of 82.05% are achieved.",https://ieeexplore.ieee.org/document/7244597/,2015 10th Asian Control Conference (ASCC),31 May-3 June 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2004.1417325,OntoSmartResource: an industrial resource generation in semantic Web,IEEE,Conferences,"Semantic Web is a logical evolution of the existing Web. It was meant to serve for machines as today's Web does for humans. The term ""machines"" according to the existing semantic Web's vocabulary mostly means ""computers"". However industry needs such applications, which consider machines also as embedded computational entities within field devices, personal devices, microwave ovens, etc. In other words, now we should involve the real (industrial) world objects as resources into semantic Web. Still the main object of such a world will be a human, which becoming a resource (not just a user of resources) in the distributed environment. We introduce an extension of the semantic Web resources to a new generation of the enhanced smart semantic Web resources (OntoSmartResources). We consider following aspects as: agent-driven resource behavior mechanism, resource semantic description and maintenance, and some aspects of the human resources related to representation and adaptation.",https://ieeexplore.ieee.org/document/1417325/,"2nd IEEE International Conference on Industrial Informatics, 2004. INDIN '04. 2004",24-26 June 2004,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2006.275808,"Ontology for Cognitics, Closed-Loop Agility Constraint, and Case Study - a Mobile Robot with Industrial-Grade Components",IEEE,Conferences,"The paper refers to intelligent industrial automation. The objective is to present key elements and methods for best practice, as well as some results obtained. The first part presents an ontology for automated cognition (cognitics), where, based on information and time, the main cognitive concepts, including those of complexity, knowledge, expertise, learning, intelligence abstraction, and concretization are rigorously defined, along with corresponding metrics and specific units. Among important conclusions at this point are the fact that reality is much too complex to be approached better than through much simplified models, in very restricted contexts. Another conclusion is the necessity to be focused on goal. Extensions are made here for group behavior. The second part briefly presents a basic law governing the choice of overall control architecture: achievable performance level of control system in terms of agility, relative to process dynamics, dictates the type of approaches which is suitable, in a spectrum which ranges from simple threshold-based switching, to classical closed-loop calculus (PID, state space multivariable systems, etc.), up to ""impossible"" cases where additional controllers must be considered, leading to cascaded, hierarchical control structures. For complex cases such as latter ones, new tools and methodologies must be designed, as is typical in O3NEIDA initiative, at least for software components. Finally, a large part of the paper presents a case study, a mobile robot, i.e. an embedded autonomous system with distributed, networked control, featuring industry-grade components, designed with the main goal of robust functionality. The case illustrates several of the concepts introduced earlier in the paper.",https://ieeexplore.ieee.org/document/4053562/,2006 4th IEEE International Conference on Industrial Informatics,16-18 Aug. 2006,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FITME.2010.5654850,Ontology-based dynamic data gathering of geographic information services,IEEE,Conferences,"The geographic information service applications are getting more and more closely interconnected with the real-time industries and businesses recently. It becomes an urgent problem to gather business information from various industries directly. In this paper, we present an ontology-based dynamic data gathering method of geographic information services, which can extract instant information from MIS or similar systems of different commercial companies and organizations. We advanced the dynamic information ontology standard for industries to support the interoperability of the distributed and heterogeneous MIS and GIS databases. Then we collected information from various industries on-line by using Web Service technology, and integrated it with urban digital maps. In addition, the industry information is interconnected and aggregated through searching and data aggregation technologies. Based on these works, a multi-layers service framework is given and its detail functions are discussed.",https://ieeexplore.ieee.org/document/5654850/,2010 International Conference on Future Information Technology and Management Engineering,9-10 Oct. 2010,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIECA.2005.1644370,Optimal approach for enhancement of large and small scale near-infrared and infrared imaging,IEEE,Conferences,"In a broad area of industry such as remote sensing and medical diagnosing, imaging enhancement technology takes a leading role, where energy distribution of the light source depends not only on image coordinate but also on wavelength. Both infrared (IR) and near-infrared (NIR) imaging techniques have a variety of applications in these fields. For instance, satellite images are taken via IR or NIR spectrometer and laser Doppler medical scanning is collaborated with NIR spectrometer. Matrix functions of any image correspond to brightness or energy at each image pixel. The actual decision making must rely on detailed investigation of images being obtained. Therefore, image processing should be taken into account so as to enhance the results from real world. Segmentation is an image analysis approach to clarify feature ambiguity and information noise, which divides an image into separate parts that correlate with the objects or areas of the particular object involved. This procedure can be conducted by clustering, which is a process of partitioning a set of pattern vectors into subsets. Being a simple unsupervised learning algorithm, k-means clustering algorithm has the potential to both simplify the computation and accelerate the convergence. In most cases optimization is closely related to clustering, which gives rise to the best way of problem solving. In this article, optimal approach is proposed to be implemented along with image segmentation. This methodology is to enhance both large scale and small scale IR and NIR image processing.",https://ieeexplore.ieee.org/document/1644370/,2005 International Conference on Industrial Electronics and Control Applications,29 Nov.-2 Dec. 2005,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA45728.2021.9613590,Optimized Deep Learning Object Recognition for Drones using Embedded GPU,IEEE,Conferences,"Nowadays, drones can be seen in various applications in industry like surveillance and transportation. Industrial drones leverage fully-fledged computer vision techniques, such as object detection based on Deep Learning Neural Networks (DNN), to efficiently perform these objectives. Those techniques come with a high computational effort and are implemented on distributed schemes using ground devices with high performance and power consumption. This limits a drone's operational range since it has to communicate with the ground devices constantly. To alleviate such constraints, an optimized, low-power perception system on the drone is desirable. This work improves a trained DNN architecture to navigate a UAV introduced by the University of Zurich called DroNet. DroNet is computationally expensive and has a high power consumption, making it unsuitable for embedded platforms because of low memory and computational power. In this paper, a ROS-based architecture is first designed to port DroNet on a low-power Jetson Nano board, which conducts the drone's perception and control tasks. Secondly, tuning parameters and various schemes have been carried out to run the inference of the DNN efficiently. To implement the different layers in DNNs, Nvidia's TensorRT SDK is used to compile a high-performance inference engine for the Jetson Nano. Results showed that the Jetson Nano can achieve real-time performance, with 47 frames per second using a Winograd convolution and well-tuned parallelization parameters. The implementation can also achieve a speedup of 2× as compared with the Jetson Nanos ARM CPU while increasing the power consumption by 54%. Finally, the Jetson Nano's usability for drone inference algorithm is shown, achieving real-time response using the DroNet DNN without losing detection accuracy.",https://ieeexplore.ieee.org/document/9613590/,2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA ),7-10 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/SpliTech49282.2020.9243735,PADL: a Language for the Operationalization of Distributed Analytical Pipelines over Edge/Fog Computing Environments,IEEE,Conferences,"In this paper we introduce PADL, a language for modeling and deploying data-based analytical pipelines. The novelty of this language relies on its independence from both the infrastructure and the technologies used on it. Specifically, this descriptive language aims at embracing all the particularities and constraints of high-demanding deployment models, such as critical restrictions regarding latency, privacy and performance, by providing fully-compliant schemas for implementing data analytical workloads. The adoption of PADL provides means for the operationalization of these pipelines in a reproducible and resilient fashion. In addition, PADL is able to fully utilize the benefits of Edge and Fog computing layers. The feasibility of the language has been validated with an analytical pipeline deployed over an Edge computing environment to solve an Industry 4.0 use case. The promising results obtained therefrom pave the way towards the widespread adoption of our proposed language when deploying data analytical pipelines over real application scenarios.",https://ieeexplore.ieee.org/document/9243735/,2020 5th International Conference on Smart and Sustainable Technologies (SpliTech),23-26 Sept. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAAI.2015.7407079,PCBA demand forecasting using an evolving Takagi-Sugeno system,IEEE,Conferences,"This paper investigates the use of using an evolving fuzzy system for printed circuit board (PCBA) demand forecasting. The algorithm is based on the evolving Takagi-Sugeno (eTS) fuzzy system, which has the ability to incorporate new patterns by changing its internal structure in an on-line fashion. We argue that these capabilities could aid in forecasting dynamic demand patterns such as those experienced in the electronic manufacturing (EMS) industry. An eTS fuzzy system is implemented in the R statistical programming language and is tested on both synthetic and real-world data. To our knowledge, this is one of the first applications of an evolving fuzzy system to forecast product demand. The results indicate that the evolving fuzzy system outperforms competing approaches for the application considered.",https://ieeexplore.ieee.org/document/7407079/,2015 Conference on Technologies and Applications of Artificial Intelligence (TAAI),20-22 Nov. 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIIMA50553.2020.9290289,PI tuning based on Bacterial Foraging Algorithm for flow control,IEEE,Conferences,"In the industrial field, the need has arisen to use more efficient and robust controllers using artificial intelligence techniques that optimize the operation of processes within the industry. In this way, the need arises to employ adaptive controllers such as the BFOA and implement it in real systems in which its functionality can be analyzed. This article presents the implementation and analysis in a fully instrumented functional prototype with industrial sensors. The work methodology is documented from the acquisition of the physical variables through the OPC client-server communication; the synchronization of the excitation of the input variable (variable speed drive) and obtaining the evolution of the flow in time; with the experimental data, the identification methodology by relative least squares was used to obtain the transfer function. Later, the BFOA algorithm was implemented to adjust the constants of a PI controller (Kp and Ki) and analyze the response through simulation using Matlab software, in which satisfactory results were observed based on the analysis of response to disturbances and as an end final part, the controller and the BFOA algorithm were implemented in a PLC-S7-1500 controller in SCL language, and the functionality was validated with the functional prototype, changing the flow setpoints at certain times, observing a behavior according to the simulations carried out. with a minimum overshoot of approximately 5 % and an establishment time of 20s.",https://ieeexplore.ieee.org/document/9290289/,2020 IX International Congress of Mechatronics Engineering and Automation (CIIMA),4-6 Nov. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigDataCongress.2019.00032,"PREMISES, a Scalable Data-Driven Service to Predict Alarms in Slowly-Degrading Multi-Cycle Industrial Processes",IEEE,Conferences,"In recent years, the number of industry-4.0-enabled manufacturing sites has been continuously growing, and both the quantity and variety of signals and data collected in plants are increasing at an unprecedented rate. At the same time, the demand of Big Data processing platforms and analytical tools tailored to manufacturing environments has become more and more prominent. Manufacturing companies are collecting huge amounts of information during the production process through a plethora of sensors and networks. To extract value and actionable knowledge from such precious repositories, suitable data-driven approaches are required. They are expected to improve the production processes by reducing maintenance costs, reliably predicting equipment failures, and avoiding quality degradation. To this aim, Machine Learning techniques tailored for predictive maintenance analysis have been adopted in PREMISES (PREdictive Maintenance service for Industrial procesSES), an innovative framework providing a scalable Big Data service able to predict alarming conditions in slowly-degrading processes characterized by cyclic procedures. PREMISES has been experimentally tested and validated on a real industrial use case, resulting efficient and effective in predicting alarms. The framework has been designed to address the main Big Data and industrial requirements, by being developed on a solid and scalable processing framework, Apache Spark, and supporting the deployment on modularized containers, specifically upon the Docker technology stack.",https://ieeexplore.ieee.org/document/8818217/,2019 IEEE International Congress on Big Data (BigDataCongress),8-13 July 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HOTI51249.2020.00011,"Panel: SmartNIC or DPU, Who Wins?",IEEE,Conferences," Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. SmartNICs and DPUs are surging in popularity to meet the new urgent requirements for media processing; neural networks, ML, and AI; and real-time automation, especially at the Network Edge. Yet they represent divergent visions of how to achieve the right combination of processing and throughput. In this discussion, we’ll highlight the fundamental differences between SmartNICs and DPUs and drill down into the hardware architectures that enable these massive compute engines, calling out the advantages and weaknesses of each approach. This panel brings together technology leaders from the six most relevant SmartNIC and DPU companies today to discuss their architecture and vision of the future. We have industry bedrocks AWS, Broadcom, NVIDIA, and Xilinx, as well as the newest kids on the block Fungible and Pensando. Eventually, it all comes down to software; we’ll touch on the underlying language and model each company chose, their path to providing solutions on their platforms, and their plan to enable thirdparty developers and perhaps even App stores. We'll close with 60 seconds from each company on the three things we should expect to see in the next 24 months.",https://ieeexplore.ieee.org/document/9188293/,2020 IEEE Symposium on High-Performance Interconnects (HOTI),19-21 Aug. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2018.8622065,Parallel Large-Scale Neural Network Training For Online Advertising,IEEE,Conferences,"Neural networks have shown great successes in many fields. Due to the complexity of the training pipeline, however, using them in an industrial setting is challenging. In online advertising, the complexity arises from the immense size of the training data, and the dimensionality of the sparse feature space (both can be hundreds of billions). To tackle these challenges, we built TrainSparse (TS), a system that parallelizes the training of neural networks with a focus on efficiently handling large-scale sparse features. In this paper, we present the design and implementation of TS, and show the effectiveness of the system by applying it to predict the ad conversion rate (pCVR), one of the key problems in online advertising. We also compare several methods for dimensionality reduction on sparse features in the pCVR task. Experiments on real-world industry data show that TS achieves outstanding performance and scalability.",https://ieeexplore.ieee.org/document/8622065/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAMI48414.2020.9108758,Patient assessment using computer games in rehabilitation,IEEE,Conferences,"This article focuses on the issue of rehabilitation of patients with motor impairment using computer games. The rehabilitation industry has been experiencing a boom in recent years, coupled with the growing popularity of virtual reality technology, a drop in prices for these technologies and the expansion the entertainment industry in the form of computer games. We are trying to create a computer game system designed for home rehabilitation of patients. The aim of this article is to devise with a subset of motor impairment assessment of patients with the intention of adapting the demands of computer games based on the physical abilities of a particular patient. We are all trying to keep this system at the level available to a regular home user.",https://ieeexplore.ieee.org/document/9108758/,2020 IEEE 18th World Symposium on Applied Machine Intelligence and Informatics (SAMI),23-25 Jan. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FPA.1994.636094,Perception systems implemented in analog VLSI for real-time applications,IEEE,Conferences,"We point out that analog VLSI can now be considered as the ideal medium to implement computational systems intended to carry out real time perceptive or even cognitive tasks that are not well handled by traditional computers. By exploiting the analog features of the transistors, only a few devices are needed to realise most of the elementary functions required to implement perceptive systems, resulting in very dense, sophisticated circuits and low power consumption. Elementary artificial retinas in silicon based on their biological counterparts have already been successfully used in industrial applications. Artificial cochleas and noses are also under development. This new enabling technology is of great interest over a wide range of industrial sectors, including robotics, automotive, surveillance and food industry.",https://ieeexplore.ieee.org/document/636094/,Proceedings of PerAc '94. From Perception to Action,7-9 Sept. 1994,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CloudCom.2019.00066,Performance Analysis of Data Parallelism Technique in Machine Learning for Human Activity Recognition Using LSTM,IEEE,Conferences,"Human activity recognition (HAR), driven by large deep learning models, has received a lot of attention in recent years due to its high applicability in diverse application domains, manipulate time-series data to speculate on activities. Meanwhile, the cloud term ""as-a-service"" has essentially revolutionized the information technology industry market over the last ten years. These two trends somehow are incorporating to inspire a new model for the assistive living application: HAR as a service in the cloud. However, with frequently updates deep learning frameworks in open source communities as well as various new hardware features release, which make a significant software management challenge for deep learning model developers. To address this problem, container techniques are widely employed to facilitate the deep learning software development cycle. In addition, models and the available datasets are being larger and more complicated, and so, an expanding amount of computing resources is desired so that these models are trained in a feasible amount of time. This requires an emerging distributed training approach, called data parallelism, to achieve low resource utilization and faster execution in training time. Therefore, in this paper, we apply the data parallelism to build an assistive living HAR application using LSTM model, deploying in containers within a Kubernetes cluster to enable the real-time recognition as well as prediction of changes in human activity patterns. We then systematically measure the influence of this technique on the performance of the HAR application. Firstly, we evaluate our system performance with regard to CPU and GPU when deployed in containers and host environment, then analyze the outcomes to verify the difference in terms of the model learning performance. Through the experiments, we figure out that data parallelism strategy is efficient for improving model learning performance. In addition, this technique helps to increase the scaling efficiency in our system.",https://ieeexplore.ieee.org/document/8968845/,2019 IEEE International Conference on Cloud Computing Technology and Science (CloudCom),11-13 Dec. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICT50816.2021.9358747,Performance Analysis of Frequency Variation System using Drives (VT240s and Axpert Eazy) for Industrial Application,IEEE,Conferences,"The proposed research work describes the general system management and real-time applicability of VFD i.e. Variable frequency drive. The project embodies the automation of 2 styles of VFD series namely VT240S and Axpert eazy. These VFD are going to be managed through PLC. Further, VFD is employed for control. The speed of the motor is often serially and domestically managed by VFD. In domestic management, there will be a switch affiliation within the drive and in serial management, the motor is managed through PLC. The motor will stop by each ways. For serial management, the programs should style in software package and it will be downloaded in PLC and by giving external wire affiliation like Modbus 485 from PLC to VFD, the operation is completed. Recently, the speed of the motor control is emerging as a big issue in industrial automation. Machine control with accurate result is required in industry applications for the design and development of the tools in various domains. Machine speed is used to control and vary the frequency parameter. So, the Variable Frequency Drive [VFD] that has been used to control the speed of the motor by varying the range of frequency is proposed to design a VFD machine system with number of tools and machine learning techniques in the paper.",https://ieeexplore.ieee.org/document/9358747/,2021 6th International Conference on Inventive Computation Technologies (ICICT),20-22 Jan. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2018.8622389,Performance and Memory Trade-offs of Deep Learning Object Detection in Fast Streaming High-Definition Images,IEEE,Conferences,"Deep learning models are associated with various deployment challenges. Inference of such models is typically very compute-intensive and memory-intensive. In this paper, we investigate the performance of deep learning models for a computer vision application used in the automotive manufacturing industry. This application has demanding requirements that are characteristic of Big Data systems, including high volume and high velocity. The application has to process a very large set of high-definition images in real-time with appropriate accuracy requirements using a deep learning-based object detection model. Meeting the run time, accuracy, and resource requirements require a careful consideration of the choice of model, model parameters, hardware, and environmental support. In this paper, we investigate the trade-offs of the most popular deep neural network-based object detection models on four hardware platforms. We report the trade-offs of resource consumption, run time, and accuracy for a realistic real-time application environment.",https://ieeexplore.ieee.org/document/8622389/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/METROI4.2019.8792860,Performance evaluation of full-cloud and edge-cloud architectures for Industrial IoT anomaly detection based on deep learning,IEEE,Conferences,"One of the most interesting application of data analysis to industry is the real-time detection of anomalies during production. Industrial IoT paradigm includes all the components to realize predictive systems, like the anomaly detection ones. In this case, the goal is to discover patterns, in a given dataset, that do not resemble the “normal” behavior, to identify faults, malfunctions or the effects of bad maintenance. The use of complex neural networks to implement deep learning algorithm for anomaly detection is very common. The position of the deep learning algorithm is one of the main problem: this kind of algorithm requires both high computational power and data transfer bandwidth, rising serious questions on the system scalability. Data elaboration in the edge domain (i.e. close to the machine) usually reduce data transfer but requires to instantiate expensive physical assets. Cloud computing is usually cheaper but Cloud data transfer is expensive. In this paper a test methodology for the comparison of the two architectures for anomaly detection system is proposed. A real use case is described in order to demonstrate the feasibility. The experimental results show that, by means of the proposed methodology, edge and Cloud solutions implementing deep learning algorithms for industrial applications can be easily evaluated. In details, for the considered use case (with Siemens controller and Microsoft Azure platform) the tradeoff between scalability, communication delay, and bandwidth usage, has been studied. The results show that the full-cloud architecture can outperform the edge-cloud architecture when Cloud computation power is scaled.",https://ieeexplore.ieee.org/document/8792860/,2019 II Workshop on Metrology for Industry 4.0 and IoT (MetroInd4.0&IoT),4-6 June 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMARTCOMP.2017.7946997,PhD Forum: Deep Learning-Based Real-Time Malware Detection with Multi-Stage Analysis,IEEE,Conferences,"Protecting computer systems is a critical and ongoing problem, given that real-time malware detection is hard. The state-of-the-art for defense cannot keep pace with the increasing level of sophistication of malware. The industry, for instance, relies heavily on anti-virus technology for threat, which is effective for malware with known signatures, but not sustainable given the massive amount of malware samples released daily, as well as and its inefficacy in dealing with zero-day and polymorphic/metamorphic malware (practical detection rates range from 25% to 50%). Behavior-based approaches attempt to identify malware behaviors using instruction sequences, computation trace logic, and system (or API) call sequences. These solutions have been mostly based on conventional machine learning (ML) models with hand-craft features, such as K-nearest neighbor, SVM, and decision tree algorithms. However, current solutions based on ML suffer from high false-positive rates, mainly because of (i) the complexity and diversity of current software and malware, which are hard to capture during the learning phase of thealgorithms, (ii) sub-optimal feature extraction, and (iii) limited/outdated dataset. Since malware has been continuously evolving, existing protection mechanisms do not cope well with the increasedsophistication and complexity of these attacks, especially those performed by advanced persistent threats (APT), which are multi-module, stealthy, and target- focused. Furthermore, malware campaigns are not homogeneous--malware sophistication varies depending on the target, the type of service exploited as part of the attack (e.g., Internet Banking, relationship sites), the attack spreading source (e.g., phishing, drive-by downloads), and the location of the target. The accuracy of malware classification depends on gaining sufficient context information and extracting meaningful abstraction of behaviors. In problems about detecting malicious behavior based on sequence of system calls, longer sequences likely contain more information. However, classical ML- based detectors (i.e., Random Forest, Naive Bayes) often use short windows of system calls during the decision process and may not be able to extract enough features for accurate detection in a long term window. Thus, the main drawback of such approaches is to accomplish accurate detection, since it is difficult to analyze complex and longer sequences of malicious behaviors with limited window sizes, especially when malicious and benign behaviors are interposed. In contrast, Deep Learning models are capable of analyzing longer sequences of system calls and making better decisions through higher level information extraction and semantic knowledge learning. However, Deep Learning requires more computation time to estimate the probability of detection when the model needs to be retrained incrementally, a common requirement for malware detection when new variants and samples are frequently added to the training set. The trade-off is challenging: fast and not-so-accurate (classical ML methods) versus time-consuming and accurate detection (emerging Deep Learning methods). Our proposal is to leverage the best of the two worlds with Spectrum, a practical multi-stage malware- detection system operating in collaboration with the operating system (OS).",https://ieeexplore.ieee.org/document/7946997/,2017 IEEE International Conference on Smart Computing (SMARTCOMP),29-31 May 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISI49825.2020.9280509,Phishcasting: Deep Learning for Time Series Forecasting of Phishing Attacks,IEEE,Conferences,"Phishing attacks remain pervasive and continue to be a source of significant monetary loss, identity theft, and malware. One of the challenges is that in most organizational settings, the detection paradigm is inherently about identifying and reacting to threats in real-time, as they are unfolding. As a way to complement these efforts with greater foresight, we introduce the idea of phishcasting — forecasting of phishing threat levels weeks or months into the future. Given that phishing attack volume time series data is noisy and devoid of traditional seasonal and cyclical trends, we extend the time series forecasting framework to utilize multiple time series, auxiliary information and alternate representations. We also introduce CoT-Net, a flexible, end-to-end CNN-LSTM based deep learning method for forecasting of complex phishing attack volume time series. CoT-Net uses time series embeddings to uncover correlations between organizational attack patterns within and across industry sectors. Using a publicly available test bed featuring multiple organizations’ attack volume over time, we find CoT-Net to outperform most state-of-the-art time series forecasting methods. By showing that phishcasting might be possible and practical, our work has important proactive implications for cybersecurity.",https://ieeexplore.ieee.org/document/9280509/,2020 IEEE International Conference on Intelligence and Security Informatics (ISI),9-10 Nov. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTON.2019.8840155,Photonics-Supported 5G Test Facilities for Low Latency Applications,IEEE,Conferences,"Berlin has become one of the EU's strategic spot for the evolution of 5G networks due in part to several ongoing activities supporting the effort. So far, there has been a huge investment from 5G Berlin Innovation Cluster to deliver a unique infrastructure providing opportunities for research centres, SMEs, and start-ups to develop novel solutions and products [1]. In this regard, there has been a complementary activity from Berlin Centre for Digital Transformation to develop a three-node optical network, which connects three Fraunhofer institutes, HHI, FOKUS, and IPK in a metro ring structure [2]. Additionally, there is a recently funded German national project (OTB-5G+) to develop an Open Testbed Berlin for 5G and Beyond, which will deliver a three-node ROADM network. These activities along with some other ones, like the presence of a Berlin node for two of the EU 5G trial projects (i.e., 5Genesis [3] and 5Gvinni [4]), have made Berlin a strategic spot in the evolution of the EU 5G ecosystem. Fraunhofer HHI hosts several aspects of the above-mentioned infrastructure including the 3-node ROADM metro network testbed, which provide a great opportunity to perform real-field experiments with 5G-ready RAN infrastructure and edge compute capability for realizing low-latency end-to-end use-cases. This metro network is SDN controlled and hosts edge compute node capabilities. Additionally, it could offer NFV services and will eventually support network slicing. Regarding the RAN infrastructure, currently, there are LTE base stations installed, which operate at 2.6 GHz. They will be upgraded to 5G base stations operating at 3.7 GHz as soon as they become available. The access segment, which is based on a passive WDM network, is expected to be extended by sophisticated mm-wave and optical-wireless communication links as bridges demonstrating the network extensions where optical fibers cannot be deployed. This network will be an SDN-enabled allowing the inclusion of several street 5G access points to the infrastructure. The three-node ROADM test-bed described above and shown in Fig. 1 will also host several demonstrations, including the final demonstration of metro-haul project [5]. Two of the ROADM nodes will be realized as AMEN (i.e., Access-Metro Edge Node) and the last one is MCEN (i.e., Metro-Core Edge Node), both developed in the framework of Metro-Haul project. This network will be also connected to German metro ring. The German metro ring will provide connectivity to our partners' premises, including ADVA, NOKIA, and Infinera enabling field trials and optical transmission tests.Figure 1.A part of the 5G test facilities hosted in Berlin.In this talk, we will present in detail the facilities hosted in Berlin, covering its geographical expansion as well as the hosted optical and wireless technologies. We then explore several demonstrations planned to run over this infrastructure showcasing the benefits brought about by different technology pillars in the context of high capacity 5G test facilities in Berlin, including network slicing, network function virtualization, distributed computing over edge compute nodes to eventually address the execution of latency-critical and bandwidth- hungry applications in an intelligent, distributed, and resource-efficient way. We will conclude the talk by providing several ways for future collaboration through which research institutes and industry sector across Europe can get access to the infrastructure in order to perform collaborative field-trials, measurement campaign, autonomous networking solutions validation, etc.",https://ieeexplore.ieee.org/document/8840155/,2019 21st International Conference on Transparent Optical Networks (ICTON),9-13 July 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFCOMW.2019.8845276,Poster Abstract: Deep Learning Workloads Scheduling with Reinforcement Learning on GPU Clusters,IEEE,Conferences,"With the recent widespread adoption of deep learning (DL) in academia and industry, more attention are attracted by DL platform, which can support research and development (R&D) of AI firms, institutes and universities. Towards an off-the-shelf distributed GPU cluster, prior work propose prediction-based schedulers to allocate resources for diverse DL workloads. However, the prediction-based schedulers have disadvantages on prediction accuracy and offline-profiling costs. In this paper, we propose a learning-based scheduler, which models the scheduling problem as a reinforcement learning problem, achieving minimum average job completion time and maximum system utilization. The scheduler contains the designs of state space, action space, reward function and update scheme. Furthermore, we will evaluate our proposed scheduler implemented as a plugin of Tensorflow on real cluster and large-scale simulation.",https://ieeexplore.ieee.org/document/8845276/,IEEE INFOCOM 2019 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),29 April-2 May 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEC50012.2020.00019,Poster: Lambda architecture for robust condition based maintenance with simulated failure modes,IEEE,Conferences,"Condition based maintenance (CBM) is increasingly seen as a promising approach for addressing downtime issues which are a common occurrence in the manufacturing industry and are a major cause of lost productivity. However, it has been a challenge to develop a generic CBM solution that works for all assets since each asset has unique sources of noise. This mandates use of manual diagnostics to custom tailor a solution for each asset for accurate failure mode identification (FMI). This problem is further compounded by the scarcity of failure data. In this paper, we propose a lambda architecture for FMI of industrial assets that achieves low initial deployment cost while securing a reasonable classification accuracy. The lambda architecture consists of a light-compute edge node, such as Raspberry Pi, that processes high-speed vibration data in real-time to extract useful features and applies a deep-learning (DL) engine which is trained in a cloud platform, such as AWS. In addition, we also incorporate a failure modes' feature simulator so that DL models can adapt to different industrial assets without costly failure data collection. Finally, experimental results are provided using the bearings' failures dataset validating the proposed cost-effective CBM architecture with high accuracy and scalability.",https://ieeexplore.ieee.org/document/9355694/,2020 IEEE/ACM Symposium on Edge Computing (SEC),12-14 Nov. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AFRICON46755.2019.9133822,Predicting Gold Mine Surface Cooling Systems Energy Consumption,IEEE,Conferences,"An artificial neural network (ANN) was utilised to predict the energy consumption of the fridge plants of a mine's surface cooling system. Predictive accuracy of 96, 89% was achieved. The maximum and minimum predicted energy consumption on the fridge plants was found to be 17 MW, 12 MW, respectively, which is fairly close to the real-time energy consumption of the machines. This model was implemented under automated load shift conditions to reinforce a hypothesis of this research, which is that demand side management (DSM) initiatives can be augmented by accurate predictive models. Accurate predictive models will ensure effective cooling system planning, sufficient machine maintenance, effective cooling system operation, optimal mine energy allocation, and energy management on the mine cooling systems, particularly its fridge plants/chillers. As the mining industry traverses towards automation of its DSM initiatives, intelligent systems have to be implemented for full automation to be achieved, and this research sought to make a contribution to that aspect. An ANN was found to outclass multiple linear regression, thus, ANNs were found to be better models for integration into DSM projects. Finally, the number of fridge plants that need to operate were determined based on the predicted energy consumption. The number of fridge plants that operated during Eskom's morning and evening peak periods was 4 and 1, respectively. This was found to be better than the traditional mode of operation whereby the entire number of fridge plants (6) operate all day.",https://ieeexplore.ieee.org/document/9133822/,2019 IEEE AFRICON,25-27 Sept. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTC.2016.7763364,Predicting churn in mobile free-to-play games,IEEE,Conferences,"In the mobile game industry, Free-to-Play games are dominantly released, and therefore player retention and purchases have become important issues. In this paper, we propose a game player model for predicting when players will leave a game. Firstly, we define player churn in the game and extract features that contain the properties of the player churn from the player logs. And then we tackle the problem of imbalanced datasets. Finally, we exploit classification algorithms from machine learning and evaluate the performance of the proposed prediction model using cross-validation. Experimental results show that the proposed model has high accuracy enough to predict churn for real-world application.",https://ieeexplore.ieee.org/document/7763364/,2016 International Conference on Information and Communication Technology Convergence (ICTC),19-21 Oct. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCE52056.2021.9514218,Predicting “Maintenance Priority” with AI,IEEE,Conferences,"In Tiipras oil refineries, an average of 100 thousand maintenance requests are created annually for more than 140 thousand pieces of equipment. These requests are prioritized manually by chief experts with over 25 years of experience and classified as urgent or planned. If maintenance requests that need to be solved urgently in the refining industry are mislabeled and delayed, they may cause process upsets leading to health & safety hazards, environment problems or big asset damage. To minimize this risk, we think that supporting the decision mechanism with algorithms and cross checking/replacing human decisions by using today's AI technologies is the right approach that reduces the possibility of human error. In this study, our main goal is to automate maintenance prioritization process with supervised and unsupervised ML algorithms, deploy an AI system and achieve high accuracy. Our study was carried out basically in 4 main steps: • Exploratory Data Analysis • Clustering - Feature Addition - Feature Selection • Model Selection and Results • Additional Studies With this study, we aim to explain our AI study, share our experience with other partners that have similar needs and provide them an effective tool and systematic approach about management of transition from human to machine with a real industry case. We believe that the transfer of priority selection process from human to algorithms ensure consistent decisions, reduce costs and tolerate experience losses.",https://ieeexplore.ieee.org/document/9514218/,"2021 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",12-13 June 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAECT54875.2022.9808060,Prediction of Maintenance Time and IoT Device Failures using Artificial Intelligence,IEEE,Conferences,"The real-time mechatronic system is critical in today's industry for increasing productivity and product quality to meet consumer demands. The quality of a product is largely determined by the quality of the machines employed in the manufacturing process. The reliability prediction model's accuracy is improved by sorting the submodules systematically and feeding the qualitative and quantitative fault data acquired into it. Fault detection and reliability forecasting modules are included in this model. Predictive maintenance aims to reduce equipment downtime and lessen the impact of failures by scheduling maintenance activities prior to the commencement of failures. This hastened the implementation of Genetic algorithms based on artificial intelligence and machine learning to predict equipment problems. For software fault prediction, a Bayes Decision classifier is used in this study to find error probabilities and integrals using feature and classifier data, this work explains how to make basic predictions about software errors. Chernoff Bound and Bhattacharyya Bound are also discussed in the suggested software error prediction using fault-predictable zone. Software errors can be predicted using a new Bayesian decision procedure that incorporates error probabilities and integrals from a machine learning model.",https://ieeexplore.ieee.org/document/9808060/,"2022 Second International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)",21-22 April 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRITO51393.2021.9596436,Prediction of Sensor Faults and Outliers in IoT Devices,IEEE,Conferences,"Internet of Things (IoT) is tremendously growing and interacting with the physical world in the era of Industry 4.0. In near future, billions of companies will have advanced communication technology and it will increase the growth of critical systems. The accuracy measurement of the functionality of a critical system is a challenging job. Fault Tolerance (FT) is a major concern to ensure the dependability, availability and reliability of critical systems. Faults should be predicted and controlled proactively to lessen failure impact on the critical systems. To predict these failures and use the relevant procedure to avoid it before it actually occurs, FT techniques are used. These techniques are implemented in critical systems to avoid failures as the security of systems is more important than the reliability of systems. It minimizes the effect of faults that are being investigated. FT techniques work on a concept that if the system is built differently then it should fail differently. If a redundant variant fails then atleast the other one should give a satisfactory result. This study exhibits an analysis of existing FT techniques like N-version programming, Recovery blocks and N-self-checking programming. A critical study of sensor faults and outliers prediction models in IoT is presented. A bibliometric analysis is also carried out on 716 Scopus indexed publications to analyze the current research trends in this domain.",https://ieeexplore.ieee.org/document/9596436/,"2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",3-4 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEEIE52663.2021.9616687,Predictive Monitoring of Wirebond Ultrasonic Signal on Electrical Test Result Using Machine Learning,IEEE,Conferences,"This study presents the development of a cost-effective predictive monitoring system interfaced to an existing wirebonding machine to determine its corresponding electrical test result. The system was tested on one production lot with the data gathered subjected to a machine learning process to predict if the gathered signal will be rejected and will become a possible failure at later electrical testing. Three classification algorithms: Logistic Regression, Decision Tree, and Support Vector Machine were used to evaluate which prediction algorithm provides the expected electrical results. The study shows that 98% to 99 % accuracy was achieved in order to predict whether the lot will produce high production yield or not. Its embedded design approach lend itself well to real-time operation. More importantly this study would provide a way for legacy manufacturing equipment to be upgraded and thus be integrated into other system that are designed for “Industry 4. 0” implementation.",https://ieeexplore.ieee.org/document/9616687/,"2021 7th International Conference on Electrical, Electronics and Information Engineering (ICEEIE)",2-2 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ARITH.2019.00047,Privacy-Preserving Deep Learning via Additively Homomorphic Encryption,IEEE,Conferences,"We aim at creating a society where we can resolve various social challenges by incorporating the innovations of the fourth industrial revolution (e.g. IoT, big data, AI, robot, and the sharing economy) into every industry and social life. By doing so the society of the future will be one in which new values and services are created continuously, making people's lives more conformable and sustainable. This is Society 5.0, a super-smart society. Security and privacy are key issues to be addressed to realize Society 5.0. Privacy-preserving data analytics will play an important role. In this talk we show our recent works on privacy-preserving data analytics such as privacy-preserving logistic regression and privacy-preserving deep learning. Finally, we show our ongoing research project under JST CREST “AI”. In this project we are developing privacy-preserving financial data analytics systems that can detect fraud with high security and accuracy. To validate the systems, we will perform demonstration tests with several financial institutions and solve the problems necessary for their implementation in the real world.",https://ieeexplore.ieee.org/document/8877418/,2019 IEEE 26th Symposium on Computer Arithmetic (ARITH),10-12 June 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GloSIC.2018.8570124,Probabilistic Estimations of Increasing Expected Reliability and Safety for Intelligent Manufacturing,IEEE,Conferences,"In the near future the possibilities of the modern probabilistic models, artificial intelligence and machine learning methods can provide an intelligent support of making decisions by an operator in real time. An agile recovery of intelligent manufacturing integrity can be implemented owing to the development of industrial robotics. For intelligent manufacturing it means the expected reliability and safety may be in the near future at the expense of intelligent support of decision making and the agile recovery of integrity. To answer the question “How much essential may be this increasing?” here are proposed: general analytical approaches for a probabilistic estimation of the expected reliability and safety for every monitored element or the system of intelligent manufacturing on a level of probability distribution functions (PDF) of the time between the losses of system integrity; estimations of increasing the expected reliability and safety for intelligent manufacturing at the expense of the intelligent support of decision making and agile recovery of integrity; the comparisons of the estimations on a prognostic period up to 10 years using the identical model in applications to expected reliability and safety. The applications of the proposed approaches allow the customers, designers, developers, users and experts of Industry 4.0 intelligent manufacturing to be guided by the proposed probabilistic estimations for solving problems of reliability and safety in the system life cycle. The results are demonstrated by examples.",https://ieeexplore.ieee.org/document/8570124/,2018 Global Smart Industry Conference (GloSIC),13-15 Nov. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UPCON52273.2021.9667555,Probabilistic Modeling of Human Locomotion for Biped Robot Trajectory Generation,IEEE,Conferences,"The wheel-type robot has found numerous applications in hospitals, restaurants, entertainment, the automation industry, etc., and shows its applicability in solving the tasks efficiently. However, it failed to achieve the same efficiency in an unstructured environment that is mostly found in the real world. Thus, a biped robot can replace the wheel-type robot for better performance. The biped robot has many joints which make it a complex higher degree of freedom system. Hence, the designing of the controller, reference trajectory generation, state estimation and, filter design for feedback signal is a very cumbersome task. This paper focuses on the generation of the reference trajectories. Since human locomotion is optimal naturally, therefore, the human data is used for this study, which is collected at Robotics and Machine ANalytics (RAMAN) Lab, MNIT, Jaipur, India. In the literature, various authors have implemented model-based learning methods to develop a model based on data. However, these models suffer from model bias i.e., it is assumed that learned model accurately define the real system. Therefore, in this paper, the authors have proposed probabilistic models to model the human locomotion data. The reference trajectory is generated using the Bayesian ridge regression, Automatic relevance determination regression, and Gaussian process regression. The performance evaluation of developed models are based on average error, maximum error, root mean square error, and percentage normalized root mean square error.",https://ieeexplore.ieee.org/document/9667555/,"2021 IEEE 8th Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)",11-13 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2016.7759479,Probabilistic approaches for self-tuning path tracking controllers using prior knowledge of the terrain,IEEE,Conferences,"Nowadays, agricultural and mining industry applications require saving energy in mobile robotic tasks. This critical issue encouraged us to enhance the performance of path tracking controllers during manoeuvring over slippery and rough terrains. In this scenario, we propose probabilistic approaches under machine learning schemes in order to optimally self-tune the controller. The approaches are real time implemented and tested in a mining machinery skid steer loader Cat® 262C under gravel and muddy terrains (and their transitions). Finally, experimental results presented in this work show that the performance of the controller enhances up to 20% (average) without compromising saturations in the actuators.",https://ieeexplore.ieee.org/document/7759479/,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),9-14 Oct. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2018.8622491,Profiling Driver Behavior for Personalized Insurance Pricing and Maximal Profit,IEEE,Conferences,"Profiling driver behaviors and designing appropriate pricing models are essential for auto insurance companies to gain profits and attract customers (drivers). The existing approaches either rely on static demographic information like age, or model only coarse-grained driving behaviors. They are therefore ineffective to yield accurate risk predictions over time for appropriate pricing, resulting in profit decline or even financial loss. Moreover, existing pricing strategies seldom take profit maximization into consideration, especially under the enterprise constraints. The recent growth of vehicle telematics data (vehicle sensing data) brings new opportunities to auto insurance industry, because of its sheer size and fine-grained mobility for profiling drivers. But, how to fuse these sparse, inconsistent and heterogeneous data is still not well addressed. To tackle these problems, we propose a unified PPP (Profile-Price-Profit) framework, working on the real-world large-scale vehicle telematics data and insurance data. PPP profiles drivers' fine-grained behaviors by considering various driving features from the trajectory perspective. Then, to predict drivers' risk probabilities, PPP leverages the group-level insight and categorizes drivers' different temporal risk change patterns into groups by ensemble learning. Next, the pricing model in PPP incorporates both the demographic analysis and the mobility factors of driving risk and mileage, to generate personalized insurance price for supporting flexible premium periods. Finally, the maximal profit problem proves to be NP-Complete. Then, an efficient heuristic-based dynamic programming is proposed. Extensive experimental results demonstrated that, PPP effectively predicts the driver's risk and outperforms the current company's pricing strategy (in industry) and the state-of-the-art approach. PPP also achieves near the maximal profit (difference by only 3%) for the company, and lowers the total price for the drivers.",https://ieeexplore.ieee.org/document/8622491/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSREW.2016.23,Programming the Network: Application Software Faults in Software-Defined Networks,IEEE,Conferences,"Software-defined networking (SDN) is a key new paradigm emerging in the industry, in which networks can be dynamically reconfigured in real-time through software. SDN networks are also being used in conjunction with cloud computing to extend virtualization and elasticity to the network level and as a foundation for the Internet of Things (IoT). A key concept in SDN is the separation of the network control and data planes, together with an application plane that supports the programming of network applications in general-purpose languages such as Java and Python. These network applications can be developed by an enterprise, service provider or vendor, or purchased from third-parties through SDN application stores. While the programmability of SDN provides tremendous flexibility and adaptability to changing network conditions and demands, it also exposes networks to significant vulnerabilities through software faults in network applications, as well as in the control and data planes. In this paper, we demonstrate how faulty SDN applications can compromise other SDN applications or even crash an entire SDN network, and describe relationships between software faults in SDN applications and design faults in SDN controllers. We also show how machine-learning based anomaly detection and analytics can be used to identify SDN software faults and help guide real-time network response, through a proof-of-concept case study.",https://ieeexplore.ieee.org/document/7789391/,2016 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),23-27 Oct. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSEET49119.2020.9206229,Project-Based Learning in a Machine Learning Course with Differentiated Industrial Projects for Various Computer Science Master Programs,IEEE,Conferences,"Graduating computer science students with skills sufficient for industrial needs is a priority in higher education teaching. Project-based approaches are promising to develop practical and social skills, needed to address real-world problems in teams. However, rapid technological transition makes an initial training of contemporary methods challenging. This affects the currently much-discussed machine learning domain as well. The study at hand describes a re-framed teaching approach for a machine learning course, offered to various computer science master programs. Project-based learning is introduced with differentiated projects provided by industrial partners that address the diverse study programs. Course attendees are supported with manuals, tools, and tutoring, passing through the Cross Industry Standard Process for Data Mining (CRISP-DM). Observations made during two iterations are reported, accompanied by a first empiric evaluation of student experiences.",https://ieeexplore.ieee.org/document/9206229/,2020 IEEE 32nd Conference on Software Engineering Education and Training (CSEE&T),9-12 Nov. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWOBI47054.2019.9114431,Proof of Concept: Using Reinforcement Learning agent as an adversary in Serious Games,IEEE,Conferences,"This article focuses on simple rehabilitation video-game called Flying with Friends. The rehabilitation industry has been experiencing a boom in recent years, coupled with the growing popularity of virtual reality technology, a drop in prices for these technologies and the expansion entertainment industry in the form of computer games. The goal of this experiment was to provide a proof that such systems are viable option when it comes to artificial intelligence systems in serious video-games, but not limited to only serious ones. The solution described in this article, in cooperation with experts, is going to be deployed in a real rehabilitation environment.",https://ieeexplore.ieee.org/document/9114431/,2019 IEEE International Work Conference on Bioinspired Intelligence (IWOBI),3-5 July 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PC.2017.7976254,Proposal of system for automatic weld evaluation,IEEE,Conferences,"The paper deals with the development of a system for automatic weld recognition using new information technologies based on cloud computing and single-board computer in the context of Industry 4.0. The proposed system is based on a visual system for weld recognition, and a neural network based on cloud computing for real-time weld evaluation, both implemented on a single-board low-cost computer. The proposed system was successfully verified on welding samples which correspond to a real welding process in the car production process. The system considerably contributes to the welds diagnostics in industrial processes of small- and medium-sized enterprises.",https://ieeexplore.ieee.org/document/7976254/,2017 21st International Conference on Process Control (PC),6-9 June 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACC.2008.4587179,Prototype design of a multi-agent system for integrated control and asset management of petroleum production facilities,IEEE,Conferences,"This paper addresses a practical intelligent multi- agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work and defined its autonomy, communications, and artificial intelligence (AI) requirements, we are proceeding to build a system prototype and simulate it in real time to validate its logical behavior in normal and abnormal process situations. We also conducted a thorough system performance analysis to detect any computational bottlenecks. Although the preliminary system prototype design has limitations, simulation results have demonstrated an effective system logical behavior and performance.",https://ieeexplore.ieee.org/document/4587179/,2008 American Control Conference,11-13 June 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WORKS49585.2019.00006,Provenance Data in the Machine Learning Lifecycle in Computational Science and Engineering,IEEE,Conferences,"Machine Learning (ML) has become essential in several industries. In Computational Science and Engineering (CSE), the complexity of the ML lifecycle comes from the large variety of data, scientists' expertise, tools, and workflows. If data are not tracked properly during the lifecycle, it becomes unfeasible to recreate a ML model from scratch or to explain to stackholders how it was created. The main limitation of provenance tracking solutions is that they cannot cope with provenance capture and integration of domain and ML data processed in the multiple workflows in the lifecycle, while keeping the provenance capture overhead low. To handle this problem, in this paper we contribute with a detailed characterization of provenance data in the ML lifecycle in CSE; a new provenance data representation, called PROV-ML, built on top of W3C PROV and ML Schema; and extensions to a system that tracks provenance from multiple workflows to address the characteristics of ML and CSE, and to allow for provenance queries with a standard vocabulary. We show a practical use in a real case in the O&G industry, along with its evaluation using 239,616 CUDA cores in parallel.",https://ieeexplore.ieee.org/document/8943505/,2019 IEEE/ACM Workflows in Support of Large-Scale Science (WORKS),17-17 Nov. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCChinaW.2018.8674519,R-CNN Object Detection Inference With Deep Learning Accelerator,IEEE,Conferences,"The explosively increasing demands of high-speed data applications have brought massive access requirements to various mobile devices. As integrating with artificial intelligence and neural network, the mobile device industry is often more concerned with faster inference with lower power consumption, bringing deep learning inference acceleration to the spotlight. In this paper, we perform a neural network inference merging R-CNN, an object detection model, into a deep learning accelerator architecture. It is a brand new implementation of neural network on embedded system hardware IP-cores of edge computation. On one hand, based on the embedded system environment, we implement the image pre-processing with region proposal algorithm and image post-processing with NMS method. On the other hand, we perform the feature computation with the deep learning accelerator through optimized software and hardware configurations. Through this method, we solve the problem of time-consuming in the computation of neural network layers and give a precise and real-time prediction of object detection. Our R-CNN inference achieves impressive results with 1.9 to 2.6 times higher performance compared with other inference processors.",https://ieeexplore.ieee.org/document/8674519/,2018 IEEE/CIC International Conference on Communications in China (ICCC Workshops),16-18 Aug. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HONET53078.2021.9615449,R-IDPS: Real time SDN based IDPS system for IoT security,IEEE,Conferences,"Internet of things increases the automation pace of the world but at the same time, IoT poses many security challenges for the industry. Intrusion detection and prevention systems have dominated the market for security in conventional networks. The challenge to IDPS is huge resource utilization and imparting performance penalties. Also, real-time training of detection machine learning models has been an issue. In this research, we develop an agent-based IDPS system using software-defined networking (SDN) technology at its core. The system develops a baseline profile for the IoT network by analyzing data from all the devices under normal conditions. Based on this profile, we extract the network traffic features. Using these features, we construct our dataset for anomaly detection in the network. For detection, we use a support vector machine to detect ICMP flood and TCP SYN flood attacks. The R-IDPS machine learning model is capable of real-time training. The proposed model (R-IDPS) is fully capable of mitigating attacks using SDN technology. The main objective of the research is to analyze the accuracy of the proposed SDN-based intrusion detection system especially under the stress conditions of DDoS attacks. Simulation results show 97 &#x0025; to 99 &#x0025; of attack detection accuracy with no false positives. The R-IDPS is scalable for both large and heterogeneous IoT networks.",https://ieeexplore.ieee.org/document/9615449/,"2021 IEEE 18th International Conference on Smart Communities: Improving Quality of Life Using ICT, IoT and AI (HONET)",11-13 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCICC46617.2019.9146036,RTPA-based Software Generation by AI Programming,IEEE,Conferences,"AI programming toward autonomous software generation is not only a highly demanded technology by the software industry, but also a hard challenge to the theories of software engineering and computational intelligence. A methodology and tool for autonomous program generation (APG) are recently developed based on Real-Time Process Algebra (RTPA). This paper demonstrates an experimental result of autonomous code generation on a digital clock system by the APG tool. The experimental results indicate a novel approach towards AI programming for machine-enabled software generation theories and technologies.",https://ieeexplore.ieee.org/document/9146036/,2019 IEEE 18th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),23-25 July 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCI51800.2020.00085,Radically Simplifying Game Engines: AI Emotions & Game Self-Evolution,IEEE,Conferences,"Today, video games are a multi-billion-dollar industry, continuously evolving through the incorporation of new technologies and innovative design. However, current video game software content creation requires extensive and often-times ambiguous planning phases for developing aesthetics, online capabilities, and gameplay mechanics. Design elements can vary significantly relative to the expertise of artists, designers, budget, and overall game engine/software features and capabilities. Game development processes are often extensively long coding sessions, usually involving a highly iterative creative process, where user requirements are rarely provided. Therefore, we propose significantly simplifying game design and development with novel Artificial Cognition Architecture real-time scalability and dynamic emotion core. Rather than utilizing more static emotion state weighting emotion engines (e.g. ExAI), we leverage significant ACA research in successful implementation of analog neural learning bots with Maslowan objective function algorithms. We also leverage AI- based Artificial Psychology software which utilizes ACA's fine grained self-evolving emotion modeling in humanistic avatar patients for Psychologist training. An ACA common cognitive core provides the gaming industry with wider applications across video game genres. A modular, scalable, and cognitive emotion game architecture implements Non-Playable Character (NPC) learning and self-evolution. ACA models NPC's with fine grained emotions, providing interactive dynamic personality traits for a more realistic game environment and enables NPC self-evolution under the influence of both other NPC's and players. Furthermore, we explore current video game design engine architecture (e.g. Unity, Unreal Engine) and propose an ACA integration approach. We apply artificial cognition and emotion intelligence modeling to engender video games with more distinct, realistic consumer gaming experiences, while simultaneously minimizing software gaming development efforts and costs.",https://ieeexplore.ieee.org/document/9457899/,2020 International Conference on Computational Science and Computational Intelligence (CSCI),16-18 Dec. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RSP.2018.8631995,Rapid Prototyping of Embedded Vision Systems: Embedding Computer Vision Applications into Low-Power Heterogeneous Architectures,IEEE,Conferences,"Embedded vision is a disruptive new technology in the vision industry. It is a revolutionary concept with far reaching implications, and it is opening up new applications and shaping the future of entire industries. It is applied in self-driving cars, autonomous vehicles in agriculture, digital dermascopes that help specialists make more accurate diagnoses, among many other unique and cutting-edge applications. The design of such systems gives rise to new challenges for embedded Software developers. Embedded vision applications are characterized by stringent performance constraints to guarantee real-time behaviours and, at the same time, energy constraints to save battery on the mobile platforms. In this paper, we address such challenges by proposing an overall view of the problem and by analysing current solutions. We present our last results on embedded vision design automation over two main aspects: the adoption of the model-based paradigm for the embedded vision rapid prototyping, and the application of heterogeneous programming languages to improve the system performance. The paper presents our recent results on the design of a localization and mapping application combined with image recognition based on deep learning optimized for an NVIDIA Jetson TX2.",https://ieeexplore.ieee.org/document/8631995/,2018 International Symposium on Rapid System Prototyping (RSP),4-5 Oct. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM.2019.8737649,ReLeS: A Neural Adaptive Multipath Scheduler based on Deep Reinforcement Learning,IEEE,Conferences,"The Multipath TCP (MPTCP) protocol, featured by its ability of capacity aggregation across multiple links and connectivity maintenance against single-path failure, has been attracting increasing attention from the industry and academy. Multipath packet scheduling is a unique and fundamental mechanism for the design and implementation of MPTCP, which is responsible for distributing the traffic over multiple subflows. The existing multipath schedulers are facing the challenges of network heterogeneities, comprehensive QoS goals, and dynamic environments, etc. To address these challenges, we propose ReLeS, a Reinforcement Learning based Scheduler for MPTCP. ReLeS uses modern deep reinforcement learning (DRL) techniques to learn a neural network to generate the control policy for packet scheduling. It adopts a comprehensive reward function that takes diverse QoS characteristics into consideration to optimize packet scheduling. To support real-time scheduling, we propose an asynchronous training algorithm that enables parallel execution of packet scheduling, data collecting, and neural network training. We implement ReLeS in the Linux kernel and evaluate it over both emulated and real network conditions. Extensive experiments show that ReLeS significantly outperforms the state-of-the-art schedulers.",https://ieeexplore.ieee.org/document/8737649/,IEEE INFOCOM 2019 - IEEE Conference on Computer Communications,29 April-2 May 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FTDCS.2007.29,Reaching Semantic Interoperability through Semantic Association of Domain Standards,IEEE,Conferences,"The vision of semantic interoperability has led much research on ontology matching. Research in this area primarily focuses on discovering similarity between entities of ontologies. The performance of proposed approaches relies on the existence of such similarity relationship and sufficient data for inferring it. However, in reality, many distributed systems do not have such presumptions. This paper addresses this challenge by associating the entities through affinity semantic (to what degree they are related in their application context). Through the analysis of a motivating example in building construction industry, this paper formally defines semantic association based on multiple-perspective domain standards. This paper hypothesizes that the establishment and the use of such standards can practically serve as a framework for reaching semantic interoperability between autonomous information systems. This paper also shows that such framework has the potential to make revolutionary impacts on workflow automation, information retrieval, and ontology matching research areas",https://ieeexplore.ieee.org/document/4144609/,11th IEEE International Workshop on Future Trends of Distributed Computing Systems (FTDCS'07),21-23 March 2007,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSITSS54238.2021.9682881,Real Time Cardiac Diagnosis for Arrhythmia Using Machine Learning and IOT,IEEE,Conferences,"Cardiac arrhythmia is a perilous infection that leads to extreme medical issues in people. An ideal analysis of arrhythmia can be valuable in preventing deaths due to cardiac conditions. Through persistent, automated, and noninvasive diagnosis of cardiac arrhythmia illnesses, the Internet of Things (IoT) aims to revolutionize the medical care industry. A system to predict cardiac arrhythmia using an IoT device which reads the ECG signal, analyses it, and informs the physician in the event of an emergency. The aim of the proposed project is to build an IoT -empowered ECG reading device to extract the ECG signal. ECG signal is examined using Pan Tompkins Q R S detection algorithm to acquire its unique features. The features are then used to classify the cardiac arrhythmia illness in the classification method. Even at home, users may monitor their cardiovascular health by using an ECG signal. The system is small; thus, it requires less support and is less expensive to run. It is beneficial for the doctor to be able to assess the heart condition as quickly and precisely as possible.",https://ieeexplore.ieee.org/document/9682881/,2021 IEEE International Conference on Computation System and Information Technology for Sustainable Solutions (CSITSS),16-18 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1993.714095,Real World Computing program - overview,IEEE,Conferences,"In 1992 the Real World Computing (RWC) program was launched by MITI (Ministry of International Trade and Industry, Japan). This is a Japanese new national project on information processing as the successor to the Fifth Generation Computer project which formally ended in 1992. The primary objective of the new program is to lay the theoretical foundation and to pursue the technological realization of human-like flexible information processing as a new paradigm of information processing towards the highly information-based society of the 21st century. Contrary to that the Fifth Generation project pursued the aspect of logical (symbol-based) information processing of humans, the RWC program is rather pursuing another aspect of intuitive (pattern-based) information processing as a new framework and foundation of information processing, and also aiming at unifying both aspects in a bottom-up manner.",https://ieeexplore.ieee.org/document/714095/,"Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)",25-29 Oct. 1993,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FABS52071.2021.9702559,Real time alert system to prevent Car Accident,IEEE,Conferences,"The use of artificial intelligence to solve a complex problem and to solve a complex issue such as foreseeing and avoiding accidents on roads implies a robust and proven procedure. An effective and proven method is needed to predict and prevent accidents on the road. Artificial vision is an area of artificial intelligence to explain and understand the visual world. With machine vision and Intelligent Transportation Systems (ITS), we can provide drivers with a safety net. These technologies help reduce human error in the automotive industry and provide riders with tools and features to help them avoid serious mistakes and accidents. In this paper we present a Drowsiness Detection System which indicates the drowsiness of the car driver by detecting the drivers face in real time video and considering the various eye landmarks indicates the ratio of each eye telling how much the driver&#x2019;s eyes are open or close. This system is developed using the state-of-art Computer Vision technology making it easy to implement highly effective. This system is implemented as an Android Application which uses android mobile back camera for recording car driver&#x2019;s live video and implementing the above developed model to calculate the eye aspect ratio and indicate it along with the eye in the video.",https://ieeexplore.ieee.org/document/9702559/,"2021 International Conference on Forensics, Analytics, Big Data, Security (FABS)",21-22 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCICT.2018.8325873,Real time control of induction motor using neural network,IEEE,Conferences,"Induction Machine being simple in operation and highly reliable equipment with low cost involving minimal maintenance requirements it has become the most popular equipment in industry. With the development of power electronic technology, low cost DSP, micro-controllers and parameter estimation techniques the induction motor an attractive component for the future high performance drives induction motors have many applications in the industries. The PWM technique which drives a Voltage Source Inverter (VSI) in order to apply v/f control is used to control a 3 ph induction motor. In industrial applications the most widely used controllers are PI controllers because of their simple structure and their capability of delivering good performance over a large band of operating condition. PI and ANN controllers have been designed and developed using MATLAB/SIMULINK. Prototype model is developed to validate the effectiveness of the PI and ANN control of induction motor drive using dSPACE DS1104 controller. The performance of the SVPWM based induction motor in open loop and closed loop is presented with simulation. Artificial Neural Network and Conventional PI controllers have been practically implemented using SVPWM based VSI fed induction motor in open loop mode. Hardware set up has been developed using Inverter and dSPACE controller. The real time performance of ANN based induction motor is presented by validating simulation results with the hardware results.",https://ieeexplore.ieee.org/document/8325873/,2018 International Conference on Communication information and Computing Technology (ICCICT),2-3 Feb. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CBI.2013.63,Real-Time Collaboration and Experience Reuse for Cloud-Based Workflow Management Systems,IEEE,Conferences,"In this paper we explore the concept of a real-time workflow collaboration platform. The work presents how a cloud-based Workflow Management System (WfMS) combines the technologic features which are offered by the cloud computing paradigm with a developed resource model for collaboration and reuse of experiential knowledge in workflows. It is based on a prototypical generic software system for integrated process and knowledge management and addresses the concept for collaborative workflow modeling. The concept for the reuse of workflows combines the ability of the resource model to share workflows with Case-Based Reasoning, a specific field of Artificial Intelligence. In particular, a sample workflow of a process in the financial industry is discussed. By enabling collaborative workflow modeling and by providing expert knowledge to large group of users, we aim at the improvement of the quality of workflows. Consequently, workload can be reduced, thus facilitating the work of all process stakeholders.",https://ieeexplore.ieee.org/document/6642905/,2013 IEEE 15th Conference on Business Informatics,15-18 July 2013,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCRD54409.2022.9730142,Real-Time Distortion Prediction and Optimization of Weld Sequence Using Artificial Neural Network,IEEE,Conferences,"Distortion is one of the predominant challenges concerning the quality and efficiency of a welded joint. It has been shown that distortion is significantly influenced by the weld sequence. In this context, Weld Sequence Optimization (WSO) is ideal for avoiding bottlenecks in the design stage, repairing, and overall capital expenditure in a manufacturing industry. Generally, the weld procedures are determined through experienced welders and in some cases, a plan of experimentation may be necessary. The current research is based on practical testing, computational simulations, and Artificial Neural Networks (ANN). Experiments are carried out using Gas Metal Arc Welding and the Finite Element Model (FEM) has been performed by MSc Simufact Welding software as well as it is validated by high-intensity experiments. The objective of the present research is to create and evaluate a useful method providing real-time predictions of distortion as well as WSO using hot-encoding. The generated optimal sequence from Neural network (NN) models is evaluated by performing the confirmatory test. The findings revealed that the proposed ANN method can significantly predict and optimize weld sequences for reducing distortion.",https://ieeexplore.ieee.org/document/9730142/,2022 14th International Conference on Computer Research and Development (ICCRD),7-9 Jan. 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDUSCON51756.2021.9529474,Real-Time Downhole Geosteering Data Processing Using Deep Neural Networks On FPGA,IEEE,Conferences,"The success of machine learning has spread the deployment of Deep neural Networks (DNNs) in numerous industrial applications. As an essential technique in today’s oilfield industry, geosteering requires performing DNN inference on the hardware devices that operates under the severe down-hole environments. However, it can produce massive power dissipation and cause long delays to execute the computation-intensive DNN inference on the current hardware platforms, e.g., CPU and GPU. In this paper, we propose an FPGA-based hardware design to efficiently conduct the DNN inference for geosteering tasks in downhole environments. At first, a comprehensive analysis is presented to choose the optimal computation mapping method for the target DNN model. A detailed description of the customized hardware implementation is then proposed to accomplish a complete DNN inference on the FPGA board. The experimental results shows that the proposed design achieves 7× (1.4×) improvement on performance and 82× (1.3×) reduction on power consumption compared with CPU(GPU).",https://ieeexplore.ieee.org/document/9529474/,2021 14th IEEE International Conference on Industry Applications (INDUSCON),15-18 Aug. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UEMCON53757.2021.9666543,Real-Time Dynamic Object Recognition and Grasp Detection for Robotic Arm using Streaming Video: A Design for Visually Impaired Persons,IEEE,Conferences,"The use of robotic arms in industrial applications such as manufacturing, medical and aerospace industry has been steadily gaining popularity. Research is also ongoing into how to improve the lives of ordinary citizens using robotic arms. Often, it is difficult for physically impaired individuals to complete tasks such as obtaining an object from a shelf, opening a refrigerator door, obtaining food or drink from the refrigerator, and picking a needed item from a drawer. Thus, they must rely on a caregiver to aid them with such tasks or complete these tasks for them. In such cases, a robotic arm could potentially be used to assist the visually impaired individual to pick up items. Prior work has described a system that takes in audio commands from an individual, applies natural language processing to identify the action required by the user, and combines this with object recognition and grasp detection to identify the object in the fridge. However, the limitation in that work was the different components were not integrated and there were inefficiencies in the process of grasp point detection from images.This work builds on existing research by integrating the object recognition and grasp detection components. The work describes a technique to dynamically adjust the size of the object image sent to the grasp point detection module by extracting only the portion necessary for grasp detection from the stream of camera images. This approach of separately performing the object recognition, identifying the portion of the object suitable for grasping (i.e., the handle), and dynamically determining the buffer zone around the handle for use in the grasp detection module reduces the amount of data transmitted between steps and enables real-time object recognition and grasp identification. The integrated framework with object recognition using a pre-trained model and dynamic grasp point detection program was successfully tested with an Intel RealSense D455 camera.",https://ieeexplore.ieee.org/document/9666543/,"2021 IEEE 12th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",1-4 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEAA.2019.00035,Real-Time Estimated Time of Arrival Prediction System using Historical Surveillance Data,IEEE,Conferences,"Prediction of Estimated Times of Arrival (ETA) is a challenging problem for the aviation industry. Flights recurrently deviate from their scheduled time of arrival, which has negative downstream consequences that affect the efficiency of operations. Therefore, accurate and up-to-date ETA estimations prior to its landing can help in optimizing the actions to be taken by the different air transportation agents whenever schedule deviations are incurred, and thus reduce the economic, logistic and environmental impact that they cause. This presentation exposes an infrastructure for high-fidelity computation of accurate ETA in real-time, based on a data-driven approach that leverages the use of recorded aircraft trajectories. This infrastructure is composed of different elements: (1) a live ADS-B tracks gathering system embedded in a lambda-architecture cluster, with capabilities for real-time distribution and data lake storage (2) an ETA prediction machine learning model, employing the actual 4D aircraft position as input; and (3) a hybrid cloud architecture to support real-time visualization and distributions of ETA predictions. The proposed infrastructure has been successfully validated in a real environment (Transforming Transport, an European Commission funded project). This infrastructure enables real-time computation and distribution of accurate ETA for any arrival operation of interest. Results supported the envisioned benefits of getting such accurate ETA, which basically turn into a reduction of associated costs for airport authorities and airlines.",https://ieeexplore.ieee.org/document/8906713/,2019 45th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),28-30 Aug. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCE52056.2021.9514169,Real-Time Fraud Prediction On Streaming Customer-Behaviour Data,IEEE,Conferences,"The field of detection and analysis of fraud situations continues to be an essential topic in the telecom industry. This study proposes a methodology that can predict abnormal behavior situations on real-time streaming customer behavior data in the telecommunication domain. The prototype implementation of the proposed method is designed, developed, and applied to the telecommunications dataset. We perform performance tests for the method's prediction success and scalability metrics on the designed prototype application. The results indicate that the proposed method proves to be a promising approach in the telecommunication sector.",https://ieeexplore.ieee.org/document/9514169/,"2021 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",12-13 June 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MobServ.2016.24,Real-Time Privacy Preserving Crowd Estimation Based on Sensor Data,IEEE,Conferences,"As one of the popular topics to ensure public safety, crowd estimation has attracted lots of attentions from both industry and academia. Most of traditional crowd estimation approaches rely on sophisticated computer vision algorithms to estimate crowd based on camera data, therefore suffering from privacy issues and high deployment and data processing cost. In this paper we present a sensor fusion based approach to real-time crowd estimation based on privacy-conscious and inexpensive sensors. The approach has been implemented and verified first by a small scale deployment at our lab, and then tested based on a 3-month trial at a shopping mall in Singapore. A deep analysis has been carried out based on the data sets collected from the trial, showing promising results: (1) the data from CO2, sound pressure and infrared sensors are influential in estimating crowd levels for indoor environments, (2) Random Forest and C4.5 are identified as the more suitable supervised learning models, (3) an accuracy of 95% can be achieved by our crowd estimation system in a real scenario. In contrast to the state of the art, our approach is privacy preserving and can provide comparable estimation accuracy with lower deployment and processing cost and better applicability for large scale setups. It can be used either as an alternative solution when user privacy must be enforced or as a complementary solution to camera-based crowd estimation when privacy is less concerned because of pubic safety.",https://ieeexplore.ieee.org/document/7787060/,2016 IEEE International Conference on Mobile Services (MS),27 June-2 July 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARCV.2018.8581288,Real-Time Robot Vision on Low-Performance Computing Hardware,IEEE,Conferences,"Small robots have numerous interesting applications in domains like industry, education, scientific research, and services. For most applications vision is important, however, the limitations of the computing hardware make this a challenging task. In this paper, we address the problem of real-time object recognition and propose the Fast Regions of Interest Search (FROIS) algorithm to quickly find the ROIs of the objects in small robots with low-performance hardware. Subsequently, we use two methods to analyze the ROIs. First, we develop a Convolutional Neural Network on a desktop and deploy it onto the low-performance hardware for object recognition. Second, we adopt the Histogram of Oriented Gradients descriptor and linear Support Vector Machines classifier and optimize the HOG component for faster speed. The experimental results show that the methods work well on our small robots with Raspberry Pi 3 embedded 1.2 GHz ARM CPUs to recognize the objects. Furthermore, we obtain valuable insights about the trade-offs between speed and accuracy.",https://ieeexplore.ieee.org/document/8581288/,"2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV)",18-21 Nov. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIS.2018.8466525,Real-Time Tracing Of A Weld Line Using Artificial Neural Networks,IEEE,Conferences,"Robotic Manipulators are becoming increasingly popular nowadays with applications in almost every industry and production line. It is difficult but essential to create a common algorithm for the different types of manipulators present in todays market so that automation can be achieved at a faster rate. This paper aims to present a real time implementation of a method to control a Tal Brabo! Robotic manipulator to move along a given weld line in order to be utilized in factories for increasing production capacity and decreasing production time. The controller used here is provided by Trio, whose ActiveX component is interfaced to MATLAB. Images were captured to identify weld lines in every possible alignment to find points of interest and the neural network was trained in order to follow a given weld line once the work-piece was placed on the work-table.",https://ieeexplore.ieee.org/document/8466525/,2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS),6-8 June 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AI4I46381.2019.00022,Real-World Conversational AI for Hotel Bookings,IEEE,Conferences,"In this paper, we present a real-world conversational AI system to search for and book hotels through text messaging. Our architecture consists of a frame-based dialogue management system, which calls machine learning models for intent classification, named entity recognition, and information retrieval subtasks. Our chatbot has been deployed on a commercial scale, handling tens of thousands of hotel searches every day. We describe the various opportunities and challenges of developing a chatbot in the travel industry.",https://ieeexplore.ieee.org/document/9027787/,2019 Second International Conference on Artificial Intelligence for Industries (AI4I),25-27 Sept. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAC53003.2021.9727507,Real-time Object Detection Algorithm For Underwater Robots,IEEE,Conferences,"The marine aquaculture fishing industry has caused many problems such as low work efficiency and hidden safety hazards due to low automation. Research on marine fishing robots that can replace humans for autonomous operations has excellent significance and prospects. Aiming at the problems of uneven lighting, poor visibility, and many magazines in the underwater environment, the underwater fishing robot can quickly and autonomously detect marine products and other targets. This paper implements an underwater image enhancement algorithm that can be deployed on a small AI system and an object detection algorithm based on YOLOv5 (You Only Look Once Version 5). The algorithm includes using the limited contrast adaptive histogram equalization image enhancement technology to solve the image quality problems of blue-green and blurry underwater images and then using the YOLOv5 object detection network to detect and locate underwater creatures. Experimental results show that the algorithm can effectively solve poor underwater image quality and unclear targets and rapidly detect seafood targets. The detection accuracy of this algorithm can reach 85%. It has been applied to the underwater fishing robot independently developed by the team and deployed on the Jetson Xavier NX small AI system. The detection accuracy can reach 80%, and the detection speed can reach 30FPS.",https://ieeexplore.ieee.org/document/9727507/,2021 China Automation Congress (CAC),22-24 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IMCEC46724.2019.8983886,Real-time Optimization of Continuous Catalytic Reforming Process with Hybrid Model,IEEE,Conferences,"A real-time optimization method with the combination of simplified mechanism and deep belief neural network model for continuous catalytic reforming (CCR) process is presented this paper. The basic model of reforming reaction is established based on the simplified mechanism as well as the kinetic parameters are estimated with deep belief neural network(DBNN), and the objective functuon together the maximization of aromatics yield with the minimization of energy consumption is solved by the non-line optimizer. The hybrid mode, can not only reflect the physical characteristics of the CCR process, but also improve the model extrapolation ability, which is greatly guaranteed the efficiency and stability for the real-time optimization of CCR. Finally, the effectiveness of method proposed is validated through a case study on a industry CCR process.",https://ieeexplore.ieee.org/document/8983886/,"2019 IEEE 3rd Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)",11-13 Oct. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2012.6359610,Real-time dynamic vehicle detection on resource-limited mobile platform,IEEE,Conferences,"Given the rapid expansion of car ownership worldwide, vehicle safety is an increasingly critical issue in the automobile industry. The reduced cost of cameras and optical devices has made it economically feasible to deploy front-mounted intelligent systems for visual-based event detection. Prior to vehicle event detection, detecting vehicles robustly in real time is challenging, especially conducting detection process in images captured by a dynamic camera. Therefore, in this paper, a robust vehicle detector is developed. Our contribution is three-fold. Road modeling is first proposed to confine detection area for maintaining low computation complexity and reducing false alarms as well. Haar-like features and eigencolors are then employed for the vehicle detector. To tackle the occlusion problem, chamfer distance is used to estimate the probability of each individual vehicle. AdaBoost algorithm is used to select critical features from a combined high dimensional feature set. Experiments on an extensive dataset show that our proposed system can effectively detect vehicles under different lighting and traffic conditions, and thus demonstrates its feasibility in real-world environments.",https://ieeexplore.ieee.org/document/6359610/,2012 International Conference on Machine Learning and Cybernetics,15-17 July 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SACI51354.2021.9465544,Real-time locating system and digital twin in Lean 4.0,IEEE,Conferences,"Digital twin plays a key role in the current development of smart manufacturing systems. Through simulation in the cyber world, real phenomena in the physical world can be predicted and optimized before the final implementation. The usage of the digital twin is enhanced along with the uprising of Industry 4.0, in which data availability supports the further insight of system status, helping the operation managers understand their system and perform resources adjustment more easily. Based on this digitization mature, Lean 4.0, a new concept elaborated from Lean manufacturing, has been interested recently. There are several technologies constituted digital twin that provide a favourable condition for Lean 4.0, such as augmented reality, cloud computing. In this paper, the integration of the Real-time Locating System (RTLS) into digital twin is proposed, which facilitates the performance of Lean 4.0 in manufacturing operation. Not only gain effective control over the facility's assets, but this integration also enhances the resources utilization, cut down operational wastes, thus brings a better turnover for industrial systems. A case study of successful implementation is shown, which proved the possible advantages of this approach.",https://ieeexplore.ieee.org/document/9465544/,2021 IEEE 15th International Symposium on Applied Computational Intelligence and Informatics (SACI),19-21 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSensT.2017.8304431,Real-time monitoring of powder blend composition using near infrared spectroscopy,IEEE,Conferences,"Near Infrared Spectroscopy (NIRS) is a very powerful utility in a Process Analytical Technology (PAT) system because it can be used to monitor a multitude of process parameters non-invasively, non-destructively in real time and in hazardous environments. A catch to the versatility of NIRS is the requirement for Multi-Variate Data Analysis (MVDA) to calibrate the measurement of the parameter of interest. This paper presents a NIRS based real time continuous monitoring of powder blend composition which has widespread applications such as the pharmaceutical industry. The proposed system design enables reduction of optical path length so that the sensors can be successfully installed into powder conveyance systems. Sensor signal processing techniques were developed in this work to improve accuracy while minimizing pre-processing steps. The paper presents the implementation of several parameter estimation methodologies applied to sensor data collected using MATLAB® software for a model powder blending process. Several techniques were examined for the development of chemometric models of the multi-sensor data, including Principal Component Analysis (PCA), Partial Least Squares Regression (PLSR), Support Vector Machines (SVM) and Artificial Neural Networks (ANN). The performances of each of the models were compared in terms of accuracy (MSE) in predicting blend composition. The results obtained show that machine learning-based approaches produce process models of similar accuracy and robustness compared to models developed by PLSR while requiring minimal pre-processing and also being more adaptable to new data.",https://ieeexplore.ieee.org/document/8304431/,2017 Eleventh International Conference on Sensing Technology (ICST),4-6 Dec. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCN52240.2021.9522281,Realization of an Intrusion Detection use-case in ONAP with Acumos,IEEE,Conferences,"With Software-Defined Networking and Machine Learning/Artificial Intelligence (ML/AI) reaching new paradigms in their corresponding fields, both academia and industry have exhibited interests in discovering unique aspects of intelligent and autonomous communication networks. Transforming such intentions and interests to reality involves software development and deployment, which has its own story of significant evolution. There has been a notable shift in the strategies and approaches to software development. Today, the divergence of tools and technologies as per demand is so substantial that adapting a software application from one environment to another could involve tedious redesign and redevelopment. This implies enormous effort in migrating existing applications and research works to a modern industrial setup. Additionally, the struggles with sustainability maintenance of such applications could be painful. Concerning ML/AI, the capabilities to train, deploy, retrain, and re-deploy AI models as quickly as possible will be crucial for AI-driven network systems. An end-to-end workflow using unified open-source frameworks is the need of the hour to facilitate the integration of ML/AI models into the modern software-driven virtualized communication networks. Hence, in our paper, we present such a prototype by demonstrating the journey of a sample SVM classifier from being a python script to be deployed as a micro-service using ONAP and Acumos. While illustrating various features of Acumos and ONAP, this paper intends to make readers familiar with an end-to-end workflow taking advantage of the integration of both open-source platforms.",https://ieeexplore.ieee.org/document/9522281/,2021 International Conference on Computer Communications and Networks (ICCCN),19-22 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2019.8848119,Realtime Adaptive Virtual Reality for Pain Reduction,IEEE,Conferences,"Recent years have seen digital game mediums taking conventional amusement, entertainment and leisure industries by storm. They have revolutionized the system to the extent that the industry cannot now even dream to do without this overwhelming reality. The same game mediums that have capitalized on intrinsic leisure aspects have simultaneously focused with equal vigor on other equally, if not more, important collateral objectives. This paper builds on this concept and discusses a work in progress currently being carried out at the University of Malta. It proposes the use of games as a means of distraction therapy for individuals undergoing painful clinical treatment procedures. The creation of an adaptive Virtual Reality (VR) game within an Artificial Intelligence framework will without doubt be of a significantly greater benefit to the community than mere entertainment applications.",https://ieeexplore.ieee.org/document/8848119/,2019 IEEE Conference on Games (CoG),20-23 Aug. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSCDS53736.2022.9760886,Recent Trends and Challenges of Diverless Vehicles in Real World Application,IEEE,Conferences,"Different countries have mostly been missing in recent conversations about AI-powered autonomous autos. Several worldwide polls and other research on the acceptability, popularity, and trust in autonomous cars were also eliminated. Accordingly, this is in view of its many advantages, such as the reduction of road accidents, the implementation of an efficient car sharing and transportation system, and precise navigation without regard to distractions. In certain circumstances, self driving technology may be used in commercial vehicles as well as passenger cars, as they improve in sophistication and accessibility. Extreme and unexpected situations, including their moral dimensions, will be handled by technology in the future. Human mistake or omission is considered to be the primary cause of most road accidents. These incidents are expected to decrease as autonomous technology become more widely available gravity, but the limited tests that are now available have not been able to properly prove underlie the problem at hand. Vehicle automation is on the rise, and this development will have far-reaching consequences for society, auto industry companies, since mechatronics will not only be a complementing component but also it&#x0027;s an essential aspect of the vehicle business.",https://ieeexplore.ieee.org/document/9760886/,2022 International Conference on Sustainable Computing and Data Communication Systems (ICSCDS),7-9 April 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISAMSR53229.2021.9567891,Recent and Future Innovative Artificial Intelligence Services and Fields,IEEE,Conferences,"Recent innovative Artificial Intelligence (AI) solutions accelerate digital transformations in different fields. It is important to highlight and explore this innovative AI service in different domains so that digital transformation can be planned, designed, and implemented for maximum society benefits. This paper investigates the different fields of AI services that can be utilized towards achieving highly beneficial digital transformations in different societal domains. This includes marketing, finance and banking, healthcare industry, emotion, and creative AI, as well as recent AI fields as explainable and responsible AI. Exploring and understanding the innovative AI services in these domains widen researcher capabilities to achieve effective and highly beneficial digital transformations.",https://ieeexplore.ieee.org/document/9567891/,"2021 4th International Symposium on Agents, Multi-Agent Systems and Robotics (ISAMSR)",6-8 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPAIR.2011.5976889,Recognition of bolt and nut using artificial neural network,IEEE,Conferences,"This paper focuses on the recognition system of bolt and nut in real time for application in various industries, particularly the automotive industry. The objective of this study is to develop the image processing algorithm to get the normalized cropping images which would be suitable inputs for the learning process using Backpropagation Neural Network. Testing is done using a real-time visual recognition system. The Matlab software version 7.6 is used to integrate all algorithms, whereas the stepper motor differentiates the final result of bolt and nut in separate places. The result shows that the system can detect moving object accurately on the belt conveyor at a speed of 9 cm/sec. with an accuracy 92 %.",https://ieeexplore.ieee.org/document/5976889/,2011 International Conference on Pattern Analysis and Intelligence Robotics,28-29 June 2011,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RAMS.2002.981623,Reliability improvement of airport ground transportation vehicles using neural networks to anticipate system failure,IEEE,Conferences,"This paper describes a joint industry/university collaboration to develop a prototype system to provide real time monitoring of an airport ground transportation vehicle with the objectives of improving availability and minimizing field failures by estimating the proper timing for condition-based maintenance. Hardware for the vehicle was designed, developed and tested to monitor door characteristics (voltage and current through the motor that opens and closes the doors and door movement time and position), to quickly predict degraded performance, and to anticipate failures. A combined statistical and neural network approach was implemented. The neural network ""learns"" the differences among door sets and can be tuned quite easily through this learning. Signals are processed in real time and combined with previous monitoring data to estimate, using the neural network, the condition of the door set relative to maintenance needs. The prototype system was installed on several vehicle door sets at the Pittsburgh International Airport and successfully tested for several months under simulated and actual operating conditions. Preliminary results indicate that improved operational reliability and availability can be achieved.",https://ieeexplore.ieee.org/document/981623/,Annual Reliability and Maintainability Symposium. 2002 Proceedings (Cat. No.02CH37318),28-31 Jan. 2002,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WINCOM.2018.8629594,"Reliable and cost-effective communication at high seas, for a safe operation of autonomous ship",IEEE,Conferences,"Nowadays, the high automation of ships has resulted in an ever -increasing need for real time data exchange to monitor ships, cargo, and ensure safety and security on board. This evolution in ship technology and management, must be supported by reliable, and cost-effective communication carriers. The satellite communication services can deliver vital real time capability to keep the ship continuously connected to the maritime players while it is trading worldwide. At high seas, the only communication service that can meet the ship need in term of data exchange is the mobile satellite communication. The reliability of these communication services will contribute significantly in the success of the future implementation of the autonomous ship in the maritime industry. This paper consists of the presentation of an overview of the current satellite services and its comparison in term of reliability, cost-effectiveness, and its capability to meet the autonomous ship communication need for its safe operation. This study results in a proposal of the suitable satellite communication services that can support the implementation of autonomous ship.",https://ieeexplore.ieee.org/document/8629594/,2018 6th International Conference on Wireless Networks and Mobile Communications (WINCOM),16-19 Oct. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DIS.2006.63,Remote Programming of Multirobot Systems within the UPC-UJI Telelaboratories: System Architecture and Agent-Based Multirobot Control,IEEE,Conferences,"One of the areas that needs more improvement within the E-Learning environments via Internet (in fact they suppose a very big effort to be accomplished) is allowing students to access and practice real experiments is a real laboratory, instead of using simulations [1]. Real laboratories allow students to acquire methods, skills and experience related to real equipment, in a manner that is very close to the way they are being used in industry. The purpose of the project is the study, development and implementation of an E-Learning environment to allow undergraduate students to practice subjects related to Robotics and Artificial Intelligence. The system, which is now at a preliminary stage, will allow the remote experimentation with real robotic devices (i.e. robots, cameras, etc.). It will enable the student to learn in a collaborative manner (remote participation with other students) where it will be possible to combine the onsite activities (performed ""in-situ"" within the real lab during the normal practical sessions), with the ""online"" one (performed remotely from home via the Internet). Moreover, the remote experiments within the E-Laboratory to control the real robots can be performed by both, students and even scientist. This project is under development and it is carried out jointly by two Universities (UPC and UJI). In this article we present the system architecture and the way students and researchers have been able to perform a Remote Programming of Multirobot Systems via web.",https://ieeexplore.ieee.org/document/1633445/,IEEE Workshop on Distributed Intelligent Systems: Collective Intelligence and Its Applications (DIS'06),15-16 June 2006,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIBA52610.2021.9688233,Research and Implementation of Small Object Detection Algorithm for Power Embedded Devices,IEEE,Conferences,"Currently in the power industry, there has been a demand for intelligent computing and real-time feedback at the edge using embedded devices. However, in some scenarios, the excessive computation still seriously affects the practicality of intelligent edge devices. Take transmission inspection drone aerial images as an example, the requirement to identify some small objects at large resolution makes the inference model can only be deployed on servers or desktops with strong computing power support. In order to realize the practicality of edge intelligence devices for small object detection, this paper adopts the strategy of hierarchical object detection. This paper first analyzes the problems faced by small object detection under constrained computation and proposes the strategy of using hierarchical object detection, then refines the design and validation of the sliding window selection for hierarchical object detection, and finally selects the base model to be used considering the accuracy, inference speed, and other factors. This enables fast detection of small objects such as missing pins on resource-limited embedded devices with good detection results.",https://ieeexplore.ieee.org/document/9688233/,"2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)",17-19 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DTPI52967.2021.9540104,Research and practice of lightweight digital twin speeding up the implementation of flexible manufacturing systems,IEEE,Conferences,"Parallel manufacturing in Industry 5.0 requires digital twin to digitize physical systems, building virtual models to open up channels connecting physical systems, information systems, and social systems, and transforming the physical models of the existing production environment to achieve two-way feedback of virtual and real is the current research direction. This paper proposes the modeling idea of lightweight digital twin, extracts core dimensions and performs digital virtual simulation, so as to quickly realize the complete process of two-way feedback, and realize a set of chess flexible parallel manufacturing production lines as a practice for the design of complete lightweight digital twin.",https://ieeexplore.ieee.org/document/9540104/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2006.258822,Research of SDSS Based on Spatial DW and Comgis,IEEE,Conferences,"With the continuously expanded means of obtaining spatial data, many real life applications would substantially benefit from introducing spatial representations of the data. More and more researchers pay attention to SDSS (spatial decision support system). Spatial data warehouse provides an efficient analysis environment for both spatial data and non-spatial data, and ComGis can simplify the processing of spatial data in complex system. By combining these two technologies, a framework of spatial DSS is proposed. Finally, a prototype used in field of administration for industry and commerce is introduced",https://ieeexplore.ieee.org/document/4028306/,2006 International Conference on Machine Learning and Cybernetics,13-16 Aug. 2006,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICET51757.2021.9450991,Research on Artificial Intelligence Industrial Big Data Platform for Industrial Internet Applications,IEEE,Conferences,"With the development of industry, enterprises have put forward higher requirements for real-time data collection and analysis. According to this, a distributed industrial big data platform for Industrial Internet applications is designed, which can effectively support the industrial field big data collection and storage. The system contains a distributed database, real-time database and offline big data platform to implement the collection and storage of all data in the system. Thus, the industrial big data platform architecture and related data hierarchical processing flow are proposed, which can effectively meet the requirements of the multi-source data and real-time processing of smart factory. The proposed data platform technical architecture will have important reference value for the realization of smart manufacturing and smart factory.",https://ieeexplore.ieee.org/document/9450991/,2021 IEEE 4th International Conference on Electronics Technology (ICET),7-10 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNSC52481.2021.9702244,Research on Digital Twin Model and Visualization of Power Transformer,IEEE,Conferences,"With the introduction of national-level strategic concepts such as intelligent manufacturing and Industry 4.0, more and more people are beginning to pay attention to information, sensing, artificial intelligence and other technologies. Digital twins are the key to achieving digitalization in various fields. Compared with On-line monitoring or fault diagnosis technology, the obvious advantages of digital twins are timeliness, two-way prediction, and high visibility. The model is also the key to the application of digital twins. The creation of models is related to the accuracy and visualization of digital twins. This article summarizes the development of digital twins and establishes the physical model of the transformer. Three-dimensional model of crystal alloy transformer, no-load test and short-circuit test are performed on the model, and the real-time change graph of the magnetic flux density in the iron core during no-load simulation operation is given. Finally, the development of digital twins in the field of power systems is prospected.",https://ieeexplore.ieee.org/document/9702244/,"2021 IEEE International Conference on Networking, Sensing and Control (ICNSC)",3-5 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPBDIS53214.2021.9658461,Research on Graph Network Recommendation Algorithm Based on Random Walk and Convolutional Neural Network,IEEE,Conferences,"As a general form of describing and modeling complex systems, networks widely exist in different scenes and tasks in various fields of the real world. Therefore, how to effectively calculate (graph Computing) and analyze (graph mining) data based on network structure has always been the core basic research direction in the field of computing science and data mining, and has been continuously studied by scholars from computer, sociology, biology and other disciplines. Network representation learning can better analyze the information hidden in complex networks, and with the help of graph neural network, it provides a universal method to solve various practical problems under the background of network structure data, which has attracted the common attention of academia and industry. At the same time, traditional recommendation algorithms generally analyze the user's rating data on items and then make preference recommendations. These methods have some problems, such as difficulty in extracting deep features, single data processing method, etc. These problems will lead to low prediction accuracy and unreasonable results. Therefore, in this paper. The recommendation algorithm for heterogeneous networks based on feature embedding is improved (RW-CNN), and the random walk algorithm of convolution and graph networks in deep learning is used to process the text feature of item names. Firstly, the network topology similarity calculation module is used to measure the consistency and complementarity of each view network, and then the word vector learning technology is used to generate the representation vector of multi view network: the word vector module is used to distinguish the complementarity between views, and the hierarchical hidden state based on multi views is used to extract these information, To retain the unique complementary information of each view; The word vector module is used to recommend multiple views with stronger consistency. Through the middle hidden state shared by multiple views, the graph convolution technology is used to aggregate information and realize the fusion of consistent information. Finally, the learned representation vector is used for link prediction and node classification tasks. The experimental results on a variety of real data sets show that the effect of this model is significantly improved.",https://ieeexplore.ieee.org/document/9658461/,2021 International Conference on High Performance Big Data and Intelligent Systems (HPBD&IS),5-7 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIBA52610.2021.9688295,Research on Intelligent Supervision and Application System of Food Traceability Based on Blockchain and Artificial intelligence,IEEE,Conferences,"The lack of transparency in the production and circulation of commodities and the lack of corresponding supervision have led to endless problems such as food safety, counterfeit and shoddy products, loss and damage of commodities, and damage to the rights and interests of consumers. The traditional centralized database traceability monitoring system has serious problems of data trust, data fragmentation, difficulty in accountability, and low enthusiasm of merchants. In order to solve the traditional system problems, it is proposed to build an intelligent supervision system model for food traceability based on blockchain and artificial intelligence. Blockchain technology can effectively make up and improve the shortcomings of the existing commodity traceability technology, and achieve full process control and real-time storage. Forensic forensics, increase transparency, prevent counterfeiting, and increase consumer trust; AI uses industry-sharing data to perform big data analysis to guide companies in business decisions; at the same time, in order to increase user stickiness and increase the ecological environment of the platform, the article proposes to increase Blockchain and artificial intelligence and application ecology form a more practical and complete integrated system model of traceability, supervision and application. Finally, using FISCO BCOS as a blockchain platform development platform, the validity of the model is verified, and it can provide a certain reference for food traceability companies, software R&D companies, and government regulatory agencies.",https://ieeexplore.ieee.org/document/9688295/,"2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)",17-19 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE53849.2021.00112,Research on the Application of Artificial Intelligence in Taekwondo Sport,IEEE,Conferences,"As an emerging discipline integrating computer, mathematics, linguistics, psychology, brain science, physics, computer, software, and philosophy, the field of artificial intelligence has grown to be the hottest technology field in China. The Country has been paying great attention to the innovation and development of AI, AI technology has been integrated into various aspects and scenarios such as manufacturing, life and leisure, medical and recreation, road traffic, security and surveillance. Premier Li Keqiang clearly proposed in the Government Work Report to ""expand and strengthen new industrial clusters, implement big data development action, strengthen the research and application of a new generation of artificial intelligence, promote the 'Internet +' in sports and other fields, develop intelligent industries, and expand intelligent life. The national leaders have been promoting a strong sports nation from the height of national strategy, attaching importance to the development of sports industry and emphasizing the importance of Internet + sports and artificial intelligence + sports. Taekwondo, as a popular sport worldwide, has been practiced by more than 200 countries and nearly 80 million people worldwide. However, in the process of rapid development, taekwondo also has some urgent problems that need to be solved, such as the decline in the attractiveness of the event, the difficulty of improving the performance of competition and training. Therefore, it is meaningful to contemplate on how to apply artificial intelligence technology in taekwondo for the healthy development and popularization of the sport in the new era.",https://ieeexplore.ieee.org/document/9696042/,2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),24-26 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE53849.2021.00120,Research on the Application of Virtual Reality Technology in English Classroom,IEEE,Conferences,"With the continuous development of computer hardware and network technology, many emerging technologies are also constantly moving towards the road of popularization from behind the scenes. In the past two years, virtual reality technology has become a hot topic in the industry, education and research fields. With the development of software and the reduction of related hardware costs, coupled with the great potential of virtual reality technology in the field of training and education, the development and design of related courses and supporting materials are imperative. This paper combines the research of some Chinese scholars on the problems in college English teaching, and combines virtual reality technology with college English teaching to form a new way of college English teaching.",https://ieeexplore.ieee.org/document/9696118/,2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),24-26 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSAA.2016.32,Reserve Price Optimization at Scale,IEEE,Conferences,"Online advertising is a multi-billion dollar industry largely responsible for keeping most online content free and content creators (""publishers"") in business. In one aspect of advertising sales, impressions are auctioned off in second price auctions on an auction-by-auction basis through what is known as real-time bidding (RTB). An important mechanism through which publishers can influence how much revenue they earn is reserve pricing in RTB auctions. The optimal reserve price problem is well studied in both applied and academic literatures. However, few solutions are suited to RTB, where billions of auctions for ad space on millions of different sites and Internet users are conducted each day among bidders with heterogenous valuations. In particular, existing solutions are not robust to violations of assumptions common in auction theory and do not scale to processing terabytes of data each hour, a high dimensional feature space, and a fast changing demand landscape. In this paper, we describe a scalable, online, real-time, incrementally updated reserve price optimizer for RTB that is currently implemented as part of the AppNexus Publisher Suite. Our solution applies an online learning approach, maximizing a custom cost function suited to reserve price optimization. We demonstrate the scalability and feasibility with the results from the reserve price optimizer deployed in a production environment. In the production deployed optimizer, the average revenue lift was 34.4% with 95% confidence intervals (33.2%, 35.6%) from more than 8 billion auctions over 46 days, a substantial increase over non-optimized and often manually set rule based reserve prices.",https://ieeexplore.ieee.org/document/7796939/,2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA),17-19 Oct. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICE/ITMC52061.2021.9570221,Resilient Manufacturing Systems enabled by AI support to AR equipped operator,IEEE,Conferences,"Supply chains and manufacturing systems robustness and resilience are, for many years, but especially nowadays, key features requested to ensure reliable and efficient production processes. Two domains are crucial to achieve such purpose: the former is fast and comprehensive monitoring, efficient and reliable condition detection and effective and explicable support for decision making. The latter refers to the intervention by operators, able to better identify problems and to put in place effective operations aimed at fixing it or, better, to prevent such circumstances. This paper presents an integrated approach encompassing a sophisticated IoT and AI-based approach to monitor and detect critical situations, fully integrated with an AR (Augmented Reality) system supporting operators in the field to take informed actions in bi-directional continuous connection. Activities in the context of EC funded project Qu4lity developed in Politecnico di Milano Industry 4.0 Lab, a test environment implementing the proposed approach and demonstrating in an automated production line the effectiveness of the approach, significantly improving performances. Analysis of performance indicators demonstrates the soundness of the proposed solution and implementation methodology to make the overall production process more resilient, efficient and with product defects reduction.",https://ieeexplore.ieee.org/document/9570221/,"2021 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",21-23 June 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ELMAR.2019.8918680,Review on Text Detection Methods on Scene Images,IEEE,Conferences,"Recently, a lot of effort has been put into developing text detection methods on natural scene images in academic research and industry. In general, text detection refers to localizing all text instances in an image which can be further processed with an Optical Character Recognition (OCR) software in order to obtain machine-readable characters. The amount of published methods is constantly growing which makes it very challenging to be up-to-date with all approaches and state-of-the-art methods. Review papers become outdated in a less than a year from being published. Deep learning, a fast-growing field by itself, has become a mainstream approach in developing text detection methods. In this paper we present the up-to-date state-of-the-art methods in this challenging field. The methods are compared by their accuracy and real-time performance. We also present the most popular evaluation datasets for scene text detection.",https://ieeexplore.ieee.org/document/8918680/,2019 International Symposium ELMAR,23-25 Sept. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICST53961.2022.00048,RiverGame - a game testing tool using artificial intelligence,IEEE,Conferences,"As is the case with any very complex and interactive software, many video games are released with various minor or major issues that can potentially affect the user experience, cause security issues for players, or exploit the companies that deliver the products. To test their games, companies invest important resources in quality assurance personnel who usually perform the testing mostly manually. The main goal of our work is to automate various parts of the testing process that involve human users (testers) and thus to reduce costs and run more tests in less time. The secondary goal is to provide mechanisms to make test specification writing easier and more efficient. We focus on solving initial real-world problems that have emerged from several discussions with industry partners. In this paper, we present RiverGame, a tool that allows game developers to automatically test their products from different points of view: the rendered output, the sound played by the game, the animation and movement of the entities, the performance and various statistical analyses. We also address the problem of input priorities, scheduling, and directing the testing effort towards custom and dynamic directions. At the core of our methods, we use state-of-the-art artificial intelligence methods for analysis and a behavior-driven development (BDD) methodology for test specifications. Our technical solution is open-source, independent of game engine, platform, and programming language.",https://ieeexplore.ieee.org/document/9787838/,"2022 IEEE Conference on Software Testing, Verification and Validation (ICST)",4-14 April 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDUCON52537.2022.9766659,Roadmap for development of skills in Artificial Intelligence by means of a Reinforcement Learning model using a DeepRacer autonomous vehicle,IEEE,Conferences,"Using Deepracer, through experimentation and simulation, theoretical concepts can be applied to a practical application of reinforcement learning (RL) in a real-life problem. It was considered as a highly useful tool to develop many direct and transversal competences that students need to work within the field of artificial intelligence (AI). Nowadays the combination of Hardware, Computer Vision methods and Machine Learning (ML) algorithms for the development of controllers for vehicle driving automation have facilitated the development of solutions for this problem. The intention of this work is to show a Roadmap that was formulated to learn AI, ML and RL competencies required to prepare undergraduate students for the industry of this area, following a structured mostly practical learning plan using Deepracer AWS platform and local alternatives for training; and a physical vehicle as primary tools that have made an incredibly compact setup process and reduced complexity in the educational researching field related to learning autonomous vehicles (AV) software development process. The AWS DeepRacer framework includes all needed hardware based on a two front-camera vehicle for stereo vision and a LiDAR sensor, besides it provides a powerful computation computer for high performance in scale autonomous vehicles. The roadmap was executed testing and comparing different RL models over the modalities that Deepracer provides (by comparing both local and AWS console training) documentation of the execution of the generated Roadmap is shown to ensure that should be considered as a stable learning system that could be followed by college programs. Finally, models were tested on a physical track built, and limitations, considerations and improvements for this Roadmap are explained as a contribution for future work.",https://ieeexplore.ieee.org/document/9766659/,2022 IEEE Global Engineering Education Conference (EDUCON),28-31 March 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CloudIntelligence52565.2021.00013,Robust and Transferable Anomaly Detection in Log Data using Pre-Trained Language Models,IEEE,Conferences,"Anomalies or failures in large computer systems, such as the cloud, have an impact on a large number of users that communicate, compute, and store information. Therefore, timely and accurate anomaly detection is necessary for reliability, security, safe operation, and mitigation of losses in these increasingly important systems. Recently, the evolution of the software industry opens up several problems that need to be tackled including (1) addressing the software evolution due software upgrades, and (2) solving the cold-start problem, where data from the system of interest is not available. In this paper, we propose a framework for anomaly detection in log data, as a major troubleshooting source of system information. To that end, we utilize pre-trained general-purpose language models to preserve the semantics of log messages and map them into log vector embeddings. The key idea is that these representations for the logs are robust and less invariant to changes in the logs, and therefore, result in a better generalization of the anomaly detection models. We perform several experiments on a cloud dataset evaluating different language models for obtaining numerical log representations such as BERT, GPT-2, and XL. The robustness is evaluated by gradually altering log messages, to simulate a change in semantics. Our results show that the proposed approach achieves high performance and robustness, which opens up possibilities for future research in this direction.",https://ieeexplore.ieee.org/document/9527018/,2021 IEEE/ACM International Workshop on Cloud Intelligence (CloudIntelligence),29-29 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCE49384.2020.9179241,Role of Ubiquitous Computing and Mobile WSN Technologies and Implementation,IEEE,Conferences,"Computing capabilities such as real time data, unlimited connection, data from sensors, environmental analysis, automated decisions (machine learning) are demanded by many areas like industry for example decision making, machine learning, by research and military, for example GPS, sensor data collection. The possibility to make these features compatible with each domain that demands them is known as ubiquitous computing. Ubiquitous computing includes network topologies such as wireless sensor networks (WSN) which can help further improving the existing communication, for example the Internet. Also, ubiquitous computing is included in the Internet of Things (IoT) applications. In this article, it is discussed the mobility of WSN and its advantages and innovations, which make possible implementations for smart home and office. Knowing the growing number of mobile users, we place the mobile phone as the key factor of the future ubiquitous wireless networks. With secure computing, communicating, and storage capacities of mobile devices, they can be taken advantage of in terms of architecture in the sense of scalability, energy efficiency, packet delay, etc. Our work targets to present a structure from a ubiquitous computing point of view for researchers who have an interest in ubiquitous computing and want to research on the analysis, to implement a novel method structure for the ubiquitous computing system in military sectors. Also, this paper presents security and privacy issues in ubiquitous sensor networks (USN).",https://ieeexplore.ieee.org/document/9179241/,"2020 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",12-13 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SPAC53836.2021.9539961,SDN-based Industrial Internet Security Gateway,IEEE,Conferences,"Industrial Internet is widely used in the production field. As the openness of networks increases, industrial networks facing increasing security risks. Information and communication technologies are now available for most industrial manufacturing. This industry-oriented evolution has driven the emergence of cloud systems, the Internet of Things (IoT), Big Data, and Industry 4.0. However, new technologies are always accompanied by security vulnerabilities, which often expose unpredictable risks. Industrial safety has become one of the most essential and challenging requirements. In this article, we highlight the serious challenges facing Industry 4.0, introduce industrial security issues and present the current awareness of security within the industry. In this paper, we propose solutions for the anomaly detection and defense of the industrial Internet based on the demand characteristics of network security, the main types of intrusions and their vulnerability characteristics. The main work is as follows: This paper first analyzes the basic network security issues, including the network security needs, the security threats and the solutions. Secondly, the security requirements of the industrial Internet are analyzed with the characteristics of industrial sites. Then, the threats and attacks on the network are analyzed, i.e., system-related threats and process-related threats; finally, the current research status is introduced from the perspective of network protection, and the research angle of this paper, i.e., network anomaly detection and network defense, is proposed in conjunction with relevant standards. This paper proposes a software-defined network (SDN)-based industrial Internet security gateway for the security protection of the industrial Internet. Since there are some known types of attacks in the industrial network, in order to fully exploit the effective information, we combine the ExtratreesClassifier to enhance the detection rate of anomaly detection. In order to verify the effectiveness of the algorithm, this paper simulates an industrial network attack, using the acquired training data for testing. The test data are industrial network traffic datasets, and the experimental results show that the algorithm is suitable for anomaly detection in industrial networks.",https://ieeexplore.ieee.org/document/9539961/,"2021 International Conference on Security, Pattern Analysis, and Cybernetics（SPAC)",18-20 June 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigDataSecurity-HPSC-IDS49724.2020.00050,SLA as a mechanism to manage risks related to chatbot services,IEEE,Conferences,Intelligent Chatbot services become one of the mainstream applications in user help and many other areas. Apart from bringing numerous benefits to users these services may bring additional risks to the companies that employ them. The study starts with the review of the scale of chatbot industry and common use cases by focusing on their applications & industry tendencies. Review of functionality and architecture of typical chatbot services shows the potential risks associated with chatbots. Analysis of such risks in the paper helped to build a checklist that security managers can use to assess risks prior to chatbot implementation. The proposed checklist was tested by reviewing a number of Service Level Agreements (SLA) of real chatbot providers.,https://ieeexplore.ieee.org/document/9123051/,"2020 IEEE 6th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)",25-27 May 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WF-IoT.2016.7845468,SQenloT: Semantic query engine for industrial Internet-of-Things gateways,IEEE,Conferences,"The Advent of Internet-of-Things (IoT) paradigm has brought exciting opportunities to solve many real-world problems. IoT in industries is poised to play an important role not only to increase productivity and efficiency but also to improve customer experiences. Two main challenges that are of particular interest to industry include: handling device heterogeneity and getting contextual information to make informed decisions. These challenges can be addressed by IoT along with proven technologies like the Semantic Web. In this paper, we present our work, SQenIoT: a Semantic Query Engine for Industrial IoT. SQenIoT resides on a commercial product and offers query capabilities to retrieve information regarding the connected things in a given facility. We also propose a things query language, targeted for resource-constrained gateways and non-technical personnel such as facility managers. Two other contributions include multi-level ontologies and mechanisms for semantic tagging in our commercial products. The implementation details of SQenIoT and its performance results are also presented.",https://ieeexplore.ieee.org/document/7845468/,2016 IEEE 3rd World Forum on Internet of Things (WF-IoT),12-14 Dec. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9811576,SafetyNet: Safe Planning for Real-World Self-Driving Vehicles Using Machine-Learned Policies,IEEE,Conferences,"In this paper we present the first safe system for full control of self-driving vehicles trained from human demonstrations and deployed in challenging, real-world, urban environments. Current industry-standard solutions use rule-based systems for planning. Although they perform reasonably well in common scenarios, the engineering complexity renders this approach incompatible with human-level performance. On the other hand, the performance of machine-learned (ML) planning solutions can be improved by simply adding more exemplar data. However, ML methods cannot offer safety guarantees and sometimes behave unpredictably. To combat this, our approach uses a simple yet effective rule-based fallback layer that performs sanity checks on an ML planner&#x0027;s decisions (e.g. avoiding collision, assuring physical feasibility). This allows us to leverage ML to handle complex situations while still assuring the safety, reducing ML planner-only collisions by 95%. We train our ML planner on 300 hours of expert driving demonstrations using imitation learning and deploy it along with the fallback layer in downtown San Francisco, where it takes complete control of a real vehicle and navigates a wide variety of challenging urban driving scenarios.",https://ieeexplore.ieee.org/document/9811576/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FCCM48280.2020.00067,Scalable Full Hardware Logic Architecture for Gradient Boosted Tree Training,IEEE,Conferences,"Gradient Boosted Tree is most effective and standard machine learning algorithm in many fields especially with various type of tabular dataset. Besides, recent industry field and robotics field require high-speed, power efficient and real-time training with enormous data. FPGA is effective device which enable custom domain specific approach to give acceleration as well as power efficiency. We introduce a scalable full hardware implementation of Gradient Boosted Tree training with high performance and flexibility of hyper parameterization. Experimental work shows that our hardware implementation achieved 11–33 times faster than state-of-art GPU acceleration even with small gates and low power FPGA device.",https://ieeexplore.ieee.org/document/9114741/,2020 IEEE 28th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM),3-6 May 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IS3C50286.2020.00134,Screw defect detection system based on AI image recognition technology,IEEE,Conferences,"In the past ten years, smart manufacturing has been widely discussed and gradually introduced into various manufacturing fields. Since Germany proposed the concept of “Industry 4.0” in 2011, it has been spreading and feverish all over the world. For Industry 4.0, information digitization, intelligent defect detection and database platform management are their main core technologies. Aiming at a large screw industry manufacturing field in central and southern Taiwan, this paper proposes a screw defect detection system based on AI image recognition technology to detect damage to the nut during the “molding” process in the screw production process, and it is determined whether the inspected screw passes the inspection. The recognition result is given as shown in Figure 1. This paper uses 500 non-defective screw samples and 20 defective screw samples provided by the screw factory. The above samples collected real-time images through the sampling structure designed in this article, and we adopt Microsoft Corporation's ML.NET suite to model AI images, and uses the following four deep learning models: ResNetV2 50, ResNetV2 101, InceptionV3, MoblieNetV2 for learning; in the process of learning, this article divides the data set into three types of data sets (one is the unknown set that is not used for training but mixed with correct and defective samples, and the other is used for post-training verification of mixed samples with correct and defective samples. The third is a training set for training a mixture of correct and defective samples) This arrangement is used for subsequent verification models; after training, a PC-based screw defect detection system is implemented as shown in Figure 2; finally, with Detect screw defects in the form of instant photography. After the experiment, in 1,000 repeated tests, the success rate of defect detection reached 97%, while the false positive rate was only 2%.",https://ieeexplore.ieee.org/document/9394116/,"2020 International Symposium on Computer, Consumer and Control (IS3C)",13-16 Nov. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNS48642.2020.9162320,SecureAIS - Securing Pairwise Vessels Communications,IEEE,Conferences,"Modern vessels increasingly rely on the Automatic Identification System (AIS) digital technology to wirelessly broadcast identification and position information to neighboring vessels and ports. AIS is a time-slotted protocol that also provides unicast messages-usually employed to manage self-separation and to exchange safety information. However, AIS does not provide any security feature. The messages are exchanged in clear-text, and they are not authenticated, being vulnerable to several attacks, including forging and replay. Despite the existing contributions in the literature propose some strategies to overcome the exposed weaknesses, some are insecure, others do not comply with the standard, while the remaining few ones would introduce an unacceptable overhead-that is, they would require a relevant number of AIS-time slots, because of the limited payload available for each time-slot. In this paper, we propose SecureAIS, a key agreement scheme that allows any pair of vessels in reach of an AIS radio to agree on a shared session key of the desired length, by requiring just a fraction of the AIS time-slots of competing solutions. Further, the scheme is fully standard compliant and does not require any modification to the available hardware. The proposed scheme integrates the Elliptic Curve Qu-Vanstone (ECQV) implicit certification scheme and the Elliptic Curve Diffie Hellman (ECDH) key agreement technique, requiring moderate computational overhead while enjoying an optimal usage of the available bandwidth. When configured with the highest security level of 256 bits, SecureAIS allows two AIS transceivers to agree on a shared session key in 20 time-slots, against the 96 time-slots required by the competing solution based on traditional X.509 certificates. The proposed solution has been implemented and tested in a real scenario, while its security has been formally evaluated through the ProVerif tool. Finally, the source code of our Proof-of-concept using GNURadio and Ettus Research X310 has been also released as open-source to pave the way to further research by both Industry and Academia in maritime communication security.",https://ieeexplore.ieee.org/document/9162320/,2020 IEEE Conference on Communications and Network Security (CNS),29 June-1 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iSAI-NLP48611.2019.9045404,Semantic Enhancement and Multi-level Label Embedding for Chinese News Headline Classification,IEEE,Conferences,"News headline classification is a specific example of short text classification, which aims to extract semantic information from the short text and classify it accurately. It can provide a fast classification method for data of various kinds of news media, thus arousing the common concern of academia and industry. Most short text classification methods are based on the semantic expansion of external knowledge, which is unable to expansion dynamically in real time and make full use of label information. To overcome these problems, we propose a novel method which consists of three parts: semantic enhancement, multi-dimensional feature fusion network and multi-level label embedding. Firstly, the word-level semantic information are embedded into the character encoding from pre-train model to enhance semantic features. Secondly, both of Bi-GRU and multi-scale CNN are used to extract sequence and local features of text to enhance the semantic representation of the sentence. Furthermore, the multi-level label embedding is used to filter textual vector and assist classification in the word and sentence level respectively. Experimental results on NLPCC 2017 Chinese news headline classification task show that our model achieves 84.74% of accuracy and 84.75% of F1, improves over the best baseline model by 1.5% and 1.6%, respectively, and reaches the state-of-the-art performance.",https://ieeexplore.ieee.org/document/9045404/,2019 14th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP),30 Oct.-1 Nov. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.2014.7005195,Semantic repository for case-based reasoning in CBM services,IEEE,Conferences,"Condition-based maintenance (CBM) has been implemented in industry to arrange the maintenance work as efficiently as possible. Case-based reasoning (CBR) can be used to automate part of the CBM decision process. However, in complex situations the final decisions have to be made by domain maintenance experts based on information gathered from several sources. This paper presents an approach for utilizing Semantic Web technologies and CBR in a knowledge base system supporting CBM services. The case knowledge base (CKB) is built over a semantic repository with an inference engine supporting ontology based information integration and data access using SPARQL queries. The knowledge base model developed for the system contains CBR task ontology and domain ontology for industrial control valves. Feasibility of the prototype CKB system was evaluated in experiments with real industrial case data.",https://ieeexplore.ieee.org/document/7005195/,Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA),16-19 Sept. 2014,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCICC46617.2019.9146103,Sequence Learning for Images Recognition in Videos with Differential Neural Networks,IEEE,Conferences,"Sequence learning from real-time videos is one of the hard challenges to current machine learning technologies and classic neural networks. Since existing supervised learning technologies are heavily dependent on intensive data and prior training, new methodologies for learning temporal sequences by unsupervised learning theories and technologies are yet to be developed. This paper presents the design and implementation of a novel Differential Neural Network (∇NN) for unsupervised sequence learning. The methodology is developed with a set of fundamental theories and enabling technologies for solving the problems of visual object recognition, motion detection, and visual semantic analysis in video sequence. A set of experiments on ∇NN for sequence learning is demonstrated. This work has not only led to a theoretical breakthrough to novel machine sequence learning, but also applicable to a wide range of challenging problems in computational intelligence and the AI industry.",https://ieeexplore.ieee.org/document/9146103/,2019 IEEE 18th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),23-25 July 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCGrid.2015.41,Service Clustering for Autonomic Clouds Using Random Forest,IEEE,Conferences,"Managing and optimising cloud services is one of the main challenges faced by industry and academia. A possible solution is resorting to self-management, as fostered by autonomic computing. However, the abstraction layer provided by cloud computing obfuscates several details of the provided services, which, in turn, hinders the effectiveness of autonomic managers. Data-driven approaches, particularly those relying on service clustering based on machine learning techniques, can assist the autonomic management and support decisions concerning, for example, the scheduling and deployment of services. One aspect that complicates this approach is that the information provided by the monitoring contains both continuous (e.g. CPU load) and categorical (e.g. VM instance type) data. Current approaches treat this problem in a heuristic fashion. This paper, instead, proposes an approach, which uses all kinds of data and learns in a data-driven fashion the similarities and resource usage patterns among the services. In particular, we use an unsupervised formulation of the Random Forest algorithm to calculate similarities and provide them as input to a clustering algorithm. For the sake of efficiency and meeting the dynamism requirement of autonomic clouds, our methodology consists of two steps: (i) off-line clustering and (ii) on-line prediction. Using datasets from real-world clouds, we demonstrate the superiority of our solution with respect to others and validate the accuracy of the on-line prediction. Moreover, to show the applicability of our approach, we devise a service scheduler that uses the notion of similarity among services and evaluate it in a cloud test-bed.",https://ieeexplore.ieee.org/document/7152517/,"2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing",4-7 May 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EEEIC.2019.8783979,Short-term load forecasting for Jordan’s Power System Using Neural Network based Different,IEEE,Conferences,"In recent times, there is a burst of interest in load forecasting. It is considered as a backbone activity in the electric power industry. Load forecasting influence decision both from an engineering perspective as well as a financial perspective. The key success in load forecasting is a partnership with the industry and how much the suitable load forecasting method has been widely used by the industry. These methods should be a simple solution to the real world problem. Although some level of sophistication is necessary to achieve high accuracy, so the successful methods typically find a good balance between simplicity and accuracy. Hence neural networks (NN) has received wide concern because of its understandable model, simple and direct implementation and satisfactory performance. In this paper, a short-term load forecasting is demonstrated based on neural networks (NN) adaption in Jordan's power system. Updated parameters are op-timized using different techniques; particle swarm optimization (PSO), genetic algorithm (GA) and elephant herding optimization (EHO). In addition, the error is calculated before and after optimization techniques. As the initial experimental result, the analysis shows that the prediction obtained using metaheuristic optimization based NNs is competitively suitable for the load forecasting. Whereas, two-layer NN gives better prediction with the least error. Therefore, it is clear from the above studies that two-layer NN is the best for load forecasting. Finally, this work is implemented using neural network toolbox and optimization Matlab codes in MathWorks for developing a new and foreseeable future forecasting model.",https://ieeexplore.ieee.org/document/8783979/,2019 IEEE International Conference on Environment and Electrical Engineering and 2019 IEEE Industrial and Commercial Power Systems Europe (EEEIC / I&CPS Europe),11-14 June 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCEA53728.2021.00060,ShuffleNet2MC: A method of light weight fault diagnosis,IEEE,Conferences,"Bearing fault diagnosis plays an important role in the field of modern industry. Although convolution neural network achieves good results, large amount of parameters costs a lot of calculation, which brings challenges to the deployment of fault diagnosis tasks in low computational power equipments. To solve the problems, an novel CNN model ShuffleNet2MC based on improved Shufflenetv2 network is proposed. Firstly, Depthwise convolution and Channel Shuffle are used to reduce the computational cost while ensuring the accuracy of diagnosis computation; Secondly, mixed convolution is used to extract the features of different resolutions through multi-scale and multi-channel method, which improves the accuracy of the model; Finally, K-means quantization is applied to the model, which greatly reduces the GFLOPS of the model and further improves the performance of the model while ensuring that the accuracy is basically unchanged. A large number of experiments on the bearing fault dataset of Western Reserve University show that: The times of floating point operation and classification accuracy of ShufflenetV2 are 0.001GFLOPS and 97.9&#x0025; respectively in the task of fault diagnosis. Compared with other models, it not only reduces the model parameters and compresses the model, but also gets better classification accuracy.",https://ieeexplore.ieee.org/document/9581143/,2021 International Conference on Computer Engineering and Application (ICCEA),25-27 June 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIEM51511.2021.9445285,Simulation based vehicle movement tracking using Kalman Filter algorithm for Autonomous vehicles,IEEE,Conferences,"In the domain of Software automotive industry, one of the most widely used algorithms for performing analysis of driving operations is the Kalman filter algorithm. In today's world of advanced machine learning, the Kalman filter remains an important tool to fuse measurements from several sensors to estimate the real time state of robotics systems such as a self-driving vehicle. Kalman filter is able to update an estimate of evolving nature of continuously changing states of the common filters to take a probabilistic estimate. The driving scenario results are updated in real time using 2-steps update and correction method. In this paper, we have described the process of Kalman filter and its variant to estimate about the detection of moving object in a given traffic scenario using advance toolboxes of MATLAB. Results have been shown for multiple changing parameters.",https://ieeexplore.ieee.org/document/9445285/,2021 2nd International Conference on Intelligent Engineering and Management (ICIEM),28-30 April 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WSC52266.2021.9715412,Simulation-Optimization of Digital Twin,IEEE,Conferences,"With rapid advancements in Cyber-Physical manufacturing, the Internet of Things, Simulation software, and Machine Learning algorithms, the applicability of Industry 4.0 is gaining momentum. The demand for real-time decision-making in the manufacturing industry has given significant attention to the field of Digital Twin (DT). The whole idea revolves around creating a digital counterpart of the physical system based on enterprise data to exploit the effects of numerous parameters and make informed decisions. Based on that, this paper proposes a simulation-optimization framework for the DT model of a Beverage Manufacturing Plant. A data-driven simulation model developed in Simio is integrated with Python to perform Multi-Objective optimization. The framework explores optimal solutions by simulating multiple scenarios by altering the availability of operators and dispatching/scheduling rules. The results show that simulation optimization can be integrated into the Digital-Twin models as part of real-time production planning and scheduling.",https://ieeexplore.ieee.org/document/9715412/,2021 Winter Simulation Conference (WSC),12-15 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636018,Smart Pointers and Shared Memory Synchronisation for Efficient Inter-process Communication in ROS on an Autonomous Vehicle,IEEE,Conferences,"Despite the stringent requirements of a real-time system, the reliance of the Robot Operating System (ROS) on the loopback network interface imposes a considerable overhead on the transport of high bandwidth data, while the nodelet package, which is an efficient mechanism for intra-process communication, does not address the problem of efficient local inter-process communication (IPC). To remedy this, we propose a novel integration into ROS of smart pointers and synchronisation primitives stored in shared memory. These obey the same semantics and, more importantly, exhibit the same performance as their C++ standard library counterparts, making them preferable to other local IPC mechanisms. We present a series of benchmarks for our mechanism - which we call LOT (Low Overhead Transport) - and use them to assess its performance on realistic data loads based on Five’s Autonomous Vehicle (AV) system, and extend our analysis to the case where multiple ROS nodes are running in Docker containers. We find that our mechanism performs up to two orders of magnitude better than the standard IPC via local loopback. Finally, we apply industry-standard profiling techniques to explore the hotspots of code running in both user and kernel space, comparing our implementation against alternatives.",https://ieeexplore.ieee.org/document/9636018/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WICT.2012.6409060,Software effort prediction using unsupervised learning (clustering) and functional link artificial neural networks,IEEE,Conferences,"Software cost estimation continues to be an area of concern for managing of software development industry. We use unsupervised learning (e.g., clustering algorithms) combined with functional link artificial neural networks for software effort prediction. The unsupervised learning (clustering) indigenously divide the input space into the required number of partitions thus eliminating the need of ad-hoc selection of number of clusters. Functional link artificial neural networks (FLANNs), on the other hand is a powerful computational model. Chebyshev polynomial has been used in the FLANN as a choice for functional expansion to exhaustively study the performance. Three real life datasets related to software cost estimation have been considered for empirical evaluation of this proposed method. The experimental results show that our method could significantly improve prediction accuracy of conventional FLANN and has the potential to become an effective method for software cost estimation.",https://ieeexplore.ieee.org/document/6409060/,2012 World Congress on Information and Communication Technologies,30 Oct.-2 Nov. 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSI.1992.217250,Software engineering environment development (SEED): an integration project of ROC,IEEE,Conferences,"The SEED development environment composes a software developer's service center, workstation environment (user site), and a communications network. Applications emphasis is on business software written in Cobol, scientific and engineering software written in Fortran 77, and system software written in C. Some AI or real-time and military-usable languages such as Lisp, Prolog, and Ada are also provided for software developers. Instead of having a heavy economic reliance on manufacturing, Taiwan must achieve even greater success in the services and information industries. In these industries, there is no more important component than software. It is no exaggeration to state that the continued elevation of the living standard in Taiwan depends on the growth and near term future domestic dominance and international competitiveness of its information industry. This growth demands sincere and focused efforts on the software industry. The government, related institutes, education institutions, and private industries will continue to plan and work together to make Taiwan software industry world-class in size and technology.<>",https://ieeexplore.ieee.org/document/217250/,Proceedings of the Second International Conference on Systems Integration,15-18 June 1992,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI50451.2021.9660133,Space and Time Efficiency Analysis of Data-Driven Methods Applied to Embedded Systems,IEEE,Conferences,"One of the applications of data-driven methods in the industry is the creation of real-time, embedded measurements, whether to monitor or replace sensor signals. As the number of embedded systems in products raises over time, the energy efficiency of such systems must be considered in the design. The time (processor) efficiency of the embedded software is directly related to the energy efficiency of the embedded system. Therefore, when considering some embedded software solutions, such as data-driven methods, time efficiency must be taken into account to improve energy efficiency. In this work, the energy efficiency of three data-driven methods: the Sparse Identification of Nonlinear Dynamics (SINDy), the Extreme Learning Machine (ELM), and the Random-Vector Functional Link (RVFL) network were assessed by using the creation of a real-time in-cylinder pressure sensor for diesel engines as a task. The three methods were kept with equivalent performances, whereas their relative execution time was tested and classified by their statistical rankings. Additionally, the space (memory) efficiency of the methods was assessed. The contribution of this work is to provide a guide to choose the best data-driven method to be used in an embedded system in terms of efficiency.",https://ieeexplore.ieee.org/document/9660133/,2021 IEEE Symposium Series on Computational Intelligence (SSCI),5-7 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VLSID.2018.26,Special Session: Design of Energy-Efficient and Reliable VLSI Systems: A Data-Driven Perspective,IEEE,Conferences,"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. The amount of data generated and collected across computing platforms every day is not only enormous, but growing at an exponential rate. Advanced data analytics and machine learning techniques have become increasingly essential to analyze and extract meaning from such “Big Data”. These techniques can be very useful to detect patterns and trends to improve the operational behavior of computing systems, but they also introduce a number of outstanding challenges: (1) How can we design and deploy data analytics mechanisms to improve energy-efficiency and reliability in IoT and mobile devices, without introducing significant software overheads? (2) How to leverage emerging technologies (e.g.,3D integration) to design energy-efficient and reliable manycore systems for big data computing? (3) How to use machine learning and data mining techniques for effective design space exploration of computing systems, and enable adaptive control to improve energy-efficiency? (4) How can data analytics detect anomalies and increase robustness in the network backbone of emerging large-scale networking systems? To address these outstanding challenges, out-of-the-box approaches need to be explored. In this special session, we will discuss these outstanding problems and describe far-reaching solutions applicable across the interconnected ecosystem of IoT and mobile devices, manycore chips, datacenters, and networks. The special session brings together speakers with unique insights on applying data analytics and machine learning to real-world problems to achieve the most sought after features on multi-scale computing platforms, viz. intelligent data mining, energyefficiency, and robustness. By integrating data analytics and machine learning algorithms, statistical modeling, embedded hardware and software design, and cloud computing content, this session will engage a broad section of Embedded and VLSI Design conference attendees. This special session is targeted towards university researchers/professors, students, industry professionals, and embedded/VLSI system designers. This session will attract newcomers who want to learn how to apply data analytics to solve problems in computing systems, as well as experienced researchers looking for exciting new directions in embedded systems, VLSI design, EDA algorithms, and multi-scale computing.",https://ieeexplore.ieee.org/document/8326889/,2018 31st International Conference on VLSI Design and 2018 17th International Conference on Embedded Systems (VLSID),6-10 Jan. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACMI53878.2021.9528185,Speed Bump & Pothole Detection with Single Shot MultiBox Detector Algorithm & Speed Control for Autonomous Vehicle,IEEE,Conferences,"The development of self-driving cars has always been an extensive research field for the automobile industry. To make a capable self-driving car, many challenges need to be resolved. Detection of the road condition is one of them. This paper focuses on a particular part-detection of speed bumps and potholes using a camera and analyzing the video feed with the help of artificial intelligence. To solve this problem a popular and lightweight algorithm, SSD (Single Shot Multibox Detector) is used. This is an optimal choice because of being lightweight and also accurate enough to run on mobile devices and to use in real-life situations. For detecting speed bumps and potholes, a dataset has been created based on the road structure of Bangladesh as the main priority of this system is to work on the local environment. Raspberry Pi has been used as the main processing unit because of being small but powerful. A warning system has been implemented so that it can warn the onboard driver about the upcoming pothole or speed bump. This system can also send a signal to the speed controller unit of the car to reduce the speed on detection to avoid accidents or damages to the car. The speed control unit is a microcontroller-based system that uses an ATmega328 microcontroller and L298 motor driver. This paper summarizes the combination of an artificial intelligence-based detection system injunction with a microcontroller-based speed control system in a cost-effective way that can be used in building self-driving cars.",https://ieeexplore.ieee.org/document/9528185/,"2021 International Conference on Automation, Control and Mechatronics for Industry 4.0 (ACMI)",8-9 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCA45697.2020.00038,SpinalFlow: An Architecture and Dataflow Tailored for Spiking Neural Networks,IEEE,Conferences,"Spiking neural networks (SNNs) are expected to be part of the future AI portfolio, with heavy investment from industry and government, e.g., IBM TrueNorth, Intel Loihi. While Artificial Neural Network (ANN) architectures have taken large strides, few works have targeted SNN hardware efficiency. Our analysis of SNN baselines shows that at modest spike rates, SNN implementations exhibit significantly lower efficiency than accelerators for ANNs. This is primarily because SNN dataflows must consider neuron potentials for several ticks, introducing a new data structure and a new dimension to the reuse pattern. We introduce a novel SNN architecture, SpinalFlow, that processes a compressed, time-stamped, sorted sequence of input spikes. It adopts an ordering of computations such that the outputs of a network layer are also compressed, time-stamped, and sorted. All relevant computations for a neuron are performed in consecutive steps to eliminate neuron potential storage overheads. Thus, with better data reuse, we advance the energy efficiency of SNN accelerators by an order of magnitude. Even though the temporal aspect in SNNs prevents the exploitation of some reuse patterns that are more easily exploited in ANNs, at 4-bit input resolution and 90% input sparsity, SpinalFlow reduces average energy by 1.8×, compared to a 4-bit Eyeriss baseline. These improvements are seen for a range of networks and sparsity/resolution levels; SpinalFlow consumes 5× less energy and 5.4× less time than an 8-bit version of Eyeriss. We thus show that, depending on the level of observed sparsity, SNN architectures can be competitive with ANN architectures in terms of latency and energy for inference, thus lowering the barrier for practical deployment in scenarios demanding real-time learning.",https://ieeexplore.ieee.org/document/9138926/,2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA),30 May-3 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASRU.2009.5372951,"Spoken dialogue systems: Challenges, and opportunities for research",IEEE,Conferences,"Summary form only given. Research into spoken dialog systems has yielded some interesting results recently, such as statistical models for improved robustness, and machine learning for optimal control, among others. What are the basic ideas behind these techniques? What opportunities do they exploit? Are they ready to be deployed in real systems? What remains to be done? This talk aims to tackle these questions. First, the task of dialog management and its challenges will be reviewed, including the effects of ASR errors; the curse of history; the lack of a single optimization metric; and the theory of mind problem. Next, current solutions to these problems will be addressed, focusing on learnings from real deployed systems, particularly in industry. Though relatively pervasive, current practices in industry still yield systems with important flaws. Recently, the research community has attempted to advance the state-of-the-art with techniques such as statistical models, machine learning, simulation, and incremental processing. This talk will present the basic ideas of some of these techniques, and examine their prospects for success in real applications - in light of both pragmatic commercial constraints, and also more fundamental properties of dialog. Finally, opportunities for further progress will be suggested.",https://ieeexplore.ieee.org/document/5372951/,2009 IEEE Workshop on Automatic Speech Recognition & Understanding,13 Nov.-17 Dec. 2009,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPDPS.2019.00113,Stochastic Gradient Descent on Modern Hardware: Multi-core CPU or GPU? Synchronous or Asynchronous?,IEEE,Conferences,"There is an increased interest in building data analytics frameworks with advanced algebraic capabilities both in industry and academia. Many of these frameworks, e.g., TensorFlow, implement their compute-intensive primitives in two flavors-as multi-thread routines for multi-core CPUs and as highly-parallel kernels executed on GPU. Stochastic gradient descent (SGD) is the most popular optimization method for model training implemented extensively on modern data analytics platforms. While the data-intensive properties of SGD are well-known, there is an intense debate on which of the many SGD variants is better in practice. In this paper, we perform a comprehensive experimental study of parallel SGD for training machine learning models. We consider the impact of three factors - computing architecture (multi-core CPU or GPU), synchronous or asynchronous model updates, and data sparsity - on three measures-hardware efficiency, statistical efficiency, and time to convergence. We draw several interesting findings from our experiments with logistic regression (LR), support vector machines (SVM), and deep neural nets (MLP) on five real datasets. As expected, GPU always outperforms parallel CPU for synchronous SGD. The gap is, however, only 2-5X for simple models, and below 7X even for fully-connected deep nets. For asynchronous SGD, CPU is undoubtedly the optimal solution, outperforming GPU in time to convergence even when the GPU has a speedup of 10X or more. The choice between synchronous GPU and asynchronous CPU is not straightforward and depends on the task and the characteristics of the data. Thus, CPU should not be easily discarded for machine learning workloads. We hope that our insights provide a useful guide for applying parallel SGD in practice and - more importantly - choosing the appropriate computing architecture",https://ieeexplore.ieee.org/document/8820776/,2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS),20-24 May 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC.2019.8917416,Strengthening the Case for a Bayesian Approach to Car-following Model Calibration and Validation using Probabilistic Programming,IEEE,Conferences,"Compute and memory constraints have historically prevented traffic simulation software users from fully utilizing the predictive models underlying them. When calibrating car-following models, particularly, accommodations have included 1) using sensitivity analysis to limit the number of parameters to be calibrated, and 2) identifying only one set of parameter values using data collected from multiple car-following instances across multiple drivers. Shortcuts are further motivated by insufficient data set sizes, for which a driver may have too few instances to fully account for the variation in their driving behavior. In this paper, we demonstrate that recent technological advances can enable transportation researchers and engineers to overcome these constraints and produce calibration results that 1) outperform industry standard approaches, and 2) allow for a unique set of parameters to be estimated for each driver in a data set, even given a small amount of data. We propose a novel calibration procedure for car-following models based on Bayesian machine learning and probabilistic programming, and apply it to real-world data from a naturalistic driving study. We also discuss how this combination of mathematical and software tools can offer additional benefits such as more informative model validation and the incorporation of true-to-data uncertainty into simulation traces.",https://ieeexplore.ieee.org/document/8917416/,2019 IEEE Intelligent Transportation Systems Conference (ITSC),27-30 Oct. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WiCom.2008.1203,Study on the Mobile Water Conservancy Service Platform Based on Ontology,IEEE,Conferences,"Traditional application water conservancy software takes the full use of modern information technology and resources as a premise, some researches have been achieved on technical problems, such as collection, transmission and processing of water conservancy information. But there are problems as single mode and rough real-time of information issue, inflexibility and weak extension ability in software design. Based on the ontology theory, framework, component, WAP, Portal, we developed a mobile water conservancy service platform, constructed mobile applications of flood control on the platform, improved the situation of pattern, efficiency of information issue, and reusability of software. Platform provides reliable sustain of information collection, processing in events of water conservancy emergency and disasters, the application prospect and practical value of mobile service platform in water conservancy industry.",https://ieeexplore.ieee.org/document/4679111/,"2008 4th International Conference on Wireless Communications, Networking and Mobile Computing",12-14 Oct. 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICE/ITMC49519.2020.9198430,Supporting SMEs in the Lake Constance Region in the Implementation of Cyber-Physical-Systems: Framework and Demonstrator,IEEE,Conferences,"With the emergence of the recent Industry 4.0 movement, data integration is now also being driven along the production line, made possible primarily by the use of established concepts of intelligent supply chains, such as the digital avatars. Digital avatars - sometimes also called Digital Twins or more broadly Cyber-Physical Systems (CPS) - are already successfully used in holistic systems for intelligent transport ecosystems, similar to the use of Big Data and artificial intelligence technologies interwoven with modern production and supply chains. The goal of this paper is to describe how data from interwoven, autonomous and intelligent supply chains can be integrated into the diverse data ecosystems of the Industry 4.0, influenced by a multitude of data exchange formats and varied data schemas. In this paper, we describe how a framework for supporting SMEs was established in the Lake Constance region and describe a demonstrator sprung from the framework. The demonstrator project's goal is to exhibit and compare two different approaches towards optimisation of manufacturing lines. The first approach is based upon static optimisation of production demand, i.e. exact or heuristic algorithms are used to plan and optimise the assignment of orders to individual machines. In the second scenario, we use real-time situational awareness - implemented as digital avatar - to assign local intelligence to jobs and raw materials in order to compare the results to the traditional planning methods of scenario one. The results are generated using event-discrete simulation and are compared to common (heuristic) job scheduling algorithms.",https://ieeexplore.ieee.org/document/9198430/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCES45898.2019.9002132,Symptom Based Health Prediction using Data Mining,IEEE,Conferences,"The general day to day health of a person is vital for the efficient functioning of the human body. Taking certain prominent symptoms and their diseases to build a Machine learning model to predict common diseases based on real symptoms is the objective of this research. With the dataset of the most commonly exhibited diseases, we built a relation to predicting the possible disease based on the input of symptoms. The proposed model utilizes the capability of different Machine learning algorithms combined with text processing to achieve accurate prediction. Text processing has been implemented using Tokenization and, is combined with various algorithms to test the similarities and the outputs. In health industry, it provides several benefits such as pre-emptive detection of diseases, faster diagnosis, medical history for review of patients etc.",https://ieeexplore.ieee.org/document/9002132/,2019 International Conference on Communication and Electronics Systems (ICCES),17-19 July 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OCIT53463.2021.00091,TLO-based Neural Discrete Predictive Approach for DTFC Induction Motor Drive,IEEE,Conferences,"This paper represents a neural predictive (NP) approach optimized with the teaching learning-based optimization (NP-TLO) technique employed to a direct torque and flux controlled (DTFC) induction motor (IM) drive to extensively reduce the torque and flux ripple along with the enhanced dynamic as well as steady-state performance. In order to have a fair comparison and superior performance, a comparative evaluation is also illustrated here for NP-TLO with that of the conventional model predictive (MP) approach optimized with TLO (MP-TLO). NP approach has been proposed as a superior control technique as the MP approach has the disadvantage of the large computational burden imposed due to its long iteration process. Further, the TLO approach has been applied proficiently with the neural predictive approach to fix the solution for nonlinear optimization problems keeping in view the system constraints like output and states. The DTFC model incorporated with both predictive approaches is designed and developed with the MATLAB software and with a real-time validation as well with a 5 HP IM drive to show its potentiality in real-world industry applications. Moreover, the computational involvedness in NN-TLO is declined compared to MP-TLO and thus, the produced optimized control signal of NN-TLO contributes better performance and robust operation by retaining all characteristics of the DTFC drive.",https://ieeexplore.ieee.org/document/9719371/,2021 19th OITS International Conference on Information Technology (OCIT),16-18 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIST.2018.8426166,Table Transformation Rule Learner,IEEE,Conferences,"As we know, table data is a popular data form in industry and scientific research fields. However, sometimes the original table data could not meet updating requirements in real applications, so we need to convert them into required form. In this paper we propose an approach to learn the transformation rules that convert original table data to target form. Based on Inductive Logic Programming(ILP), we design a learning system called Table Transformation Rule Learner (TTRL). It uses specific predicates and background knowledge for this task to generate table transformation rules. We implement a unique heuristic function (HF) in TTRL to accelerate searching process for rule generation, and we use semi-supervised learning (SSL) in order to obtain more information especially from small set of sample data. We also address the problem like over-generalization which may occur when having only positive training examples in ILP learning process. We test our program in several kinds of table data, and the result shows that the transformation rules can be learned correctly. Moreover, our designed searching strategy can greatly reduce the time cost of searching rules.",https://ieeexplore.ieee.org/document/8426166/,2018 Eighth International Conference on Information Science and Technology (ICIST),30 June-6 July 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS.2013.4,Table of contents,IEEE,Conferences,"The following topics are dealt with: artificial intelligence; neural networks and fuzzy systems; evolutionary computation; bioinformatics and bioengineering; data and semantic mining; games, VR and visualization; intelligent systems and applications; systems intelligence; control intelligence; e-science and e-systems; robotics, cybernetics, engineering, and manufacturing; operations research; discrete-event and real-time systems; image, speech and signal processing; industry, business, management, human factors and social issues; energy, power, transport, logistics, harbour, shipping and marine simulation; parallel, distributed, and software architectures and systems; mobile-ad hoc wireless networks, Mobicast, sensor placement, and target tracking; performance engineering of computer and communications systems; and circuits and devices.",https://ieeexplore.ieee.org/document/6959881/,"2013 1st International Conference on Artificial Intelligence, Modelling and Simulation",3-5 Dec. 2013,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEET53442.2021.9659667,Tailored Software Design for Remote Communication Skill Development in Software Engineering Curriculums,IEEE,Conferences,"Communication skill is one of the most crucial skills that Software Engineering (SE) industry demands. According to literature, it is evident that oral communication, feedback, and presentation skills are the most crucial among all. SE curriculums need to be structured to prepare its undergraduates to meet these demands. Due to Covid-19 pandemic, most universities opted for online education. Therefore, communication skill development inevitably needed to be conducted remotely. Current mechanisms used in these remote sessions do not cater to unique needs of SE industry. This paper proposes a design of a plugin to be used with Zoom to analyze and improve student&#x0027;s communication skills real time. The proposed design is inclusive of two phases. In phase one, variables which determine communication skills are identified. In relation to these variables, student communication data will be collected for a period of a semester. The data collection will occur via Zoom sessions, for a selected project-based learning (PBL) module. In phase two, a machine learning model will be created using the gathered data in phase 1, and the plugin will be implemented. The plugin will categorize the students&#x0027; communication skills into 3 categories such as &#x201C;weak&#x201D;, &#x201C;average&#x201D; and &#x201C;good&#x201D;. The proposed plugin generates a comprehensive report on student&#x0027;s communication skills which could be downloaded at the end of each session. The expected accuracy rate of the plugin is 80&#x0025;.",https://ieeexplore.ieee.org/document/9659667/,2021 International Conference on Engineering and Emerging Technologies (ICEET),27-28 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE-SEET.2019.00014,Teaching Internet of Things (IoT) Literacy: A Systems Engineering Approach,IEEE,Conferences,"The Internet of Things (IoT) invades our world with billions of smart, interconnected devices, all programmed to make our lives easier. For educators, teaching such a vast and dynamic field is both a necessity and a challenge. IoT-relevant topics such as programming, hardware, networking and artificial intelligence are already covered in core computing curricula. Does this mean that fresh graduates are well prepared to tackle complex IoT problems? Unfortunately, nothing could be further from the truth. The problem is that IoT devices are complex systems, where software, hardware, and humans interact with each other. From this interaction, unique behavior and hazardous situations can emerge that might easily stay undetected, unless systems are analyzed as a whole. This paper presents two differently flavored courses that teach IoT using a holistic, system-centric approach. The first is a broad introduction to Pervasive Computing, focused on the intelligence of ""Things"". The second is an advanced course that zooms on the process of testing a software-intensive system. The key characteristics of our approach are : (1) teaching only the bare essentials (topics needed for end-to-end engineering of a smart system), (2) a strong, hands-on project component, using microcontroller-based miniature systems, inspired by real-life, and (3) a rich partnership with industry and academic idea incubators. Positive student evaluations gathered during the last five years demonstrate that such an approach brings engagement, self-confidence and realism in IoT classrooms. We believe that this success can be replicated in other courses, by shifting the focus on different IoT-relevant aspects.",https://ieeexplore.ieee.org/document/8802112/,2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET),25-31 May 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRTAC.2018.8679129,TensorFlow Based Website Click through Rate (CTR) Prediction Using Heat maps,IEEE,Conferences,"Web Heat Maps are used to identify the click patterns and activities visited by the users of the website. Using heat maps, one can make a manual decision based on the user's click activity. This paper proposes a framework using TensorFlow to identify and detect the users click activity in real time. Tensor Flow also suggest or take business decisions predicted through users clicks. This paper models Tensor Flow's machine learning library to take automated decisions like placement of suitable products, placement of advertisements and others based on the highest clicks recorded by the users. The results predict that the future businesses like e-commerce, fashion and retail industry can benefit more if this framework is deployed in such applications.",https://ieeexplore.ieee.org/document/8679129/,2018 International Conference on Recent Trends in Advance Computing (ICRTAC),10-11 Sept. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE-Companion.2019.00131,Testing Untestable Neural Machine Translation: An Industrial Case,IEEE,Conferences,"Neural Machine Translation (NMT) has shown great advantages and is becoming increasingly popular. However, in practice, NMT often produces unexpected translation failures in its translations. While reference-based black-box system testing has been a common practice for NMT quality assurance during development, an increasingly critical industrial practice, named in-vivo testing, exposes unseen types or instances of translation failures when real users are using a deployed industrial NMT system. To fill the gap of lacking test oracles for in-vivo testing of NMT systems, we propose a new methodology for automatically identifying translation failures without reference translations. Our evaluation conducted on real-world datasets shows that our methodology effectively detects several targeted types of translation failures. Our experiences on deploying our methodology in both production and development environments of WeChat (a messenger app with over one billion monthly active users) demonstrate high effectiveness of our methodology along with high industry impact.",https://ieeexplore.ieee.org/document/8802818/,2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),25-31 May 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSE51940.2021.9569551,The Integration Development of Artificial Intelligence and Education,IEEE,Conferences,"With the rapid progress and development of modern information science and technology, artificial intelligence technology has become more and more extensive in many fields. How to incorporate artificial intelligence into education has become a hot topic of the whole society. In this paper, analysis of artificial intelligence used to extract application potential and value of intelligent correction, real-time monitoring, education fairness and campus safety. But there are also challenges in personality education, safety ethics, teaching efficiency, etc. In order to make artificial intelligence better serve the education industry, it is necessary to increase the infrastructure construction and environment configuration of artificial intelligence equipment. And then improving the education practitioners’ awareness and correct cognition of the relationship between intelligent machine safety ethics and artificial intelligence.",https://ieeexplore.ieee.org/document/9569551/,2021 16th International Conference on Computer Science & Education (ICCSE),17-21 Aug. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITISEE53823.2021.9655968,The Internet of Things: Real-Time Monitoring System for Production Machine,IEEE,Conferences,"The importance of optimization in the manufacturing industry to achieve efficiency and effectiveness, especially in production machines. This research focuses on developing a real-time monitoring system based on the internet to monitor power consumption and production machines' speed accurately. The data taken by the real-time monitoring system is integrated into the ERP System, which is stored as time series data of power consumption and engine speed. The importance of making production cost estimates, especially on the power consumption of a production machine for the future, and functioning as a tool for real-time monitoring, the data obtained is processed by regression to get an estimate of the total power consumption for several expected outputs. The data is processed using a linear regression algorithm to predict power consumption at the desired speed on the intended production machine. To get a better tool than the one that existed before, the method in its development uses PIECES analysis. In its implementation, this real-time monitoring system runs according to the objectives, and the data obtained is as expected. This model was developed on production machines in the flexible packaging industry that uses rotogravure machines. It is possible that it can be used on production machines in other industries that use electrical power correlated with the speed measured based on the rotation of a motor.",https://ieeexplore.ieee.org/document/9655968/,"2021 IEEE 5th International Conference on Information Technology, Information Systems and Electrical Engineering (ICITISEE)",24-25 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISDFS55398.2022.9800837,The Necessity of Emotion Recognition from Speech Signals for Natural and Effective Human-Robot Interaction in Society 5.0,IEEE,Conferences,"The history of humanity has reached Industry 4.0 that aims to the integration of information technologies and especially artificial intelligence with all life-sustaining mechanisms in the 21st century, and consecutively, the transformation of Society 5.0 has begun. Society 5.0 means a smart society in which humans share life with physical robots and software robots as well as smart devices based on augmented reality. Industry 4.0 contains main structures such as the internet of things, big data analytics, digital transformation, cyber-physical systems, artificial intelligence, and business processes optimization. It is impossible to consider the machines to be without emotions and emotional intelligence within the transformation of smart tools and artificial intelligence, in addition, while it is planned to give most of the commands with voice and speaking, it became more important to develop algorithms that can detect emotions. In the smart society, new and rapid methods are needed for speech recognition, emotion recognition, and speech emotion recognition areas to maximize human-computer (HCI) or human-robot interaction (HRI) and collaboration. In this study, speech recognition and speech emotion recognition studies in robot technology are investigated and developments are revealed.",https://ieeexplore.ieee.org/document/9800837/,2022 10th International Symposium on Digital Forensics and Security (ISDFS),6-7 June 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEEICT53905.2021.9667893,The Role of IoT in Digitalizing Mining Sector of Bangladesh,IEEE,Conferences,"Mining is expected to play an important part in Bangladesh’s economic development, regardless of the country’s economy, with potential outcomes in terms of business, manufacturing, and employment. Recent developments in technologies such as the Internet of things (IoT) and Artificial Intelligence (AI) have paved the way for digitalizing the mining sector. When these technologies are deployed in the mining industry, the results acquired may be used immediately to support process optimization, machine health, worker safety, and asset management. Similarly, mine digitization is expected to provide significant possibilities and benefit Bangladesh’s Industrial growth. However, digital mining is difficult to implement in reality owing to constraints in communication, data management, and storage infrastructure. Furthermore, the mining companies’ inclination to remain with outdated methods rather than depending on experimental novel technologies hinders development. This paper briefly describes the architecture of mine IoT, different applications, and potential challenges to enable mine digitalization from the perspective of Bangladesh.",https://ieeexplore.ieee.org/document/9667893/,2021 5th International Conference on Electrical Engineering and Information Communication Technology (ICEEICT),18-20 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICC40277.2020.9148964,The Scalability Analysis of Machine Learning Based Models in Road Traffic Flow Prediction,IEEE,Conferences,"Nowadays, traffic flow prediction, as a vital part of the Intelligent Transportation System (ITS), has attracted considerable attention from both academia and industry. Many prediction methods have been proposed and can be categorized into parametric methods and non-parametric methods. Nonparametric methods, especially Machine Learning (ML)-based methods, compared to parametric methods, need less prior knowledge about the relationship among different traffic patterns and can better fit the non-linear features of traffic data. However, we notice that, due to the complex structure, ML-models require a higher cost of implementation regarding time consumption of training and predicting. Therefore, in this paper, we evaluate not only the accuracy but also the efficiency and scalability of some state-of-the-art ML-models, which is the key to apply a prediction model into the real world. Furthermore, we design an off-line optimization method, Desensitization, to improve the scalability of a given model.",https://ieeexplore.ieee.org/document/9148964/,ICC 2020 - 2020 IEEE International Conference on Communications (ICC),7-11 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MIPRO.2016.7522210,The challenge of cellular cooperative ITS services based on 5G communications technology,IEEE,Conferences,"We live at a time when the automotive industry is going through a technological revolution with the development of vehicles changing into autonomous moving objects having the properties of artificial intelligence. Additionally, cellular communications networks introduce new technologies and concepts: SDN (Software-defined networking), NFV (Network Functions Virtualization). These advanced software-defined communications networks virtualize network functions, allowing a new way of configuration, control and management. Increasing the speed and automation are key requirements to support the most demanding services such as mobile payment of contextual services, as well as the introduction of new “Machine” users. SDN also assists in the implementation of new infrastructure for the dynamic services that are based on the concepts of IoT, Big Data and Everything-as-a-Service. Using NFV enables the Internet of Things services that provide a new way of connecting people, processes, data and devices. It is precisely thes5G communications networkse requirements that are essential for the introduction of C-ITS systems, i.e., the integration of passengers, drivers, vehicles and transport infrastructure, as well as information, statistics, predictive traffic analytics, all this in real-time. Due to this trend of development of cellular communications networks in the next 5G communications networks, automotive and ITS traffic systems are becoming the most important market for business expansion of telecom operators. Such revolutionary technological changes, together with the integration of various industries entails a series of challenges. The aim of this paper is to define important information, challenges and opportunities for the telecom industry to provide mobility for people and goods.",https://ieeexplore.ieee.org/document/7522210/,"2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",30 May-3 June 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IMTC.2001.928200,The development of an artificial neural network embedded automated inspection quality management system,IEEE,Conferences,This paper describes in detail the development of an innovative artificial neural network embedded automated inspection scheme for the manufacturing industry employing digital image processing techniques. Such a system is capable of performing real-time image processing tasks and identifies the size and location of the finished components on manufactured products as well as the flaws and scratches on surface of products during the manufacturing process. The proposed artificial neural network embedded quality management system provides a user-friendly user interface that has been implemented and tested on a case study from a printed circuit board manufacture. The experimental results have demonstrated the functionality and superiority of the developed artificial neural network embedded inspection system.,https://ieeexplore.ieee.org/document/928200/,IMTC 2001. Proceedings of the 18th IEEE Instrumentation and Measurement Technology Conference. Rediscovering Measurement in the Age of Informatics (Cat. No.01CH 37188),21-23 May 2001,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIAWS.1991.236599,The strategic significance of expert systems,IEEE,Conferences,"Expert or knowledge-based systems are being deployed in virtually every industry because of their power to bring strategic competitive advantage to the firms who successfully employ them. The strategic significance of expert systems lies in their ability to improve the effectiveness and efficiency of the decision-making process and, in the long run, improved decision-making leads to growth and increased profitability. This is especially true of organizations, such as brokerage firms, whose 'life's blood' is fruitful, consistent, and reliable advice. Expert system technology is advancing at a great pace as new methods for improved reasoning and automated learning are emerging from research to reality. If firms in the business of offering advice do not continually upgrade their expert systems with the latest conceptual tools, they will find it difficult to grow, even survive, in the 'knowledge age'.<>",https://ieeexplore.ieee.org/document/236599/,Proceedings First International Conference on Artificial Intelligence Applications on Wall Street,9-11 Oct. 1991,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCIS51004.2021.9397068,To Detect the Distributed Denial-of-Service Attacks in SDN using Machine Learning Algorithms,IEEE,Conferences,"The reason for Software Defined Network (SDN) to gain importance in both the academics and industry as a new emerging way of network management, is its architecture which decouples the data plane (forwarding devices) and the control plane (controller) making it possible to upgrade and update into newer versions. SDN is a new or novel way of `programmable networks' which has led to growth of innovative technologies and scenarios in terms of network virtualization, flexibility, enhanced growth control, dynamic network policy, reduced operational cost. Despite, these advantages; it is also one of main reasons for cyber threats. Amongst them the most vulnerable is the DDoS attacks. DDoS attack in SDN is quite a threat to the security in SDN network. It attacks at the network layer or application layer of the infrastructure. It can cause problems as simple as inability to refresh a particular page to as severe as failure of an entire server. In this paper, DDoS is taken into consideration with SDN and proposed a IDS which studied for detection of the attackers in the real time incoming traffic. Machine Learning algorithms such as Naïve Bayes, KNN, K-Means clustering, and Linear Regression are used to form the module 1 of the IDS (the Signature IDS) and Module 2 form for uses three way handshake to identify the exact host which is an intruder. On finding the intruder it is being placed in the Access Control List (ACL). Also, analysis of efficiency of different machine learning algorithm is performed to understand the effectiveness.",https://ieeexplore.ieee.org/document/9397068/,"2021 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)",19-20 Feb. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2007.4450938,Toward A Practical Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities,IEEE,Conferences,"This paper addresses a practical intelligent multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work [1], [2], [3], we define the autonomy, communications, and artificial intelligence (AI) requirements of the component agents of such a system. We also discuss the software implementation of such agents. Furthermore, we describe a simple system prototype, and conduct a real time simulation experiment to analyze the prototype performance. Simulation results reveal that MATLAB can be used to build high performance real-time multi-agent systems, which can be used for many applications.",https://ieeexplore.ieee.org/document/4450938/,2007 IEEE 22nd International Symposium on Intelligent Control,1-3 Oct. 2007,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSESS.2016.7883200,Toward an intelligent solution for perishable food cold chain management,IEEE,Conferences,"With the continuous development of Internet of Things (IoT), cloud computing, and artificial intelligence, many cold chain logistics providers have updated their IT solutions to enhance the quality control of perishable food products in the cold chain industry. Based on RFID, WSN, GPS, and cloud computing, a typical cloud-based IoT platform empowers a logistics company to monitor the real-time status of supervised food products and give decision support intelligently in a cost-effective way. Furthermore, complex problems such as load planning and route planning can be solved by using the artificial intelligence. Therefore, in this paper we attempt to propose and present an intelligent solution from the perspective of key enabling technologies and system framework.",https://ieeexplore.ieee.org/document/7883200/,2016 7th IEEE International Conference on Software Engineering and Service Science (ICSESS),26-28 Aug. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MELCON.2018.8379089,Toward energy saving and environmental protection by implementation of autonomous ship,IEEE,Conferences,"The energy saving and environmental protection remain the main pillars of the maritime industry's sustainability enhancement. This sustainability will become a reality by implementation of autonomous ship (AS), which is considered as an alternative and may carry the potential to increase the profitability of shipping company. The elimination of crew, the introduction of innovative technology and adoption of new ship design on board of this kind of ship will participate positively in energy saving and environmental protection. For an in-situ study, a hundred of conventional ships (CS) have been visited, their energy consumption and environmental impact are assessed taking into consideration the trading area, navigation scenarios, power capacity and number of crew members. In this paper, we assess and quantify the positive impact on energy saving and environmental pollution prevention resulting from the implementation of AS. For this the facilities related to crew living are enumerated and the impact of their elimination on energy saving and environmental pollution prevention is presented. Other ship design concepts are proposed to enhance the energy-saving and environmental protection are proposed. A benchmarking of CS and AS in term of energy saving and environmental pollution prevention is presented.",https://ieeexplore.ieee.org/document/8379089/,2018 19th IEEE Mediterranean Electrotechnical Conference (MELECON),2-7 May 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3377811.3380368,Towards Characterizing Adversarial Defects of Deep Learning Software from the Lens of Uncertainty,IEEE,Conferences,"Over the past decade, deep learning (DL) has been successfully applied to many industrial domain-specific tasks. However, the current state-of-the-art DL software still suffers from quality issues, which raises great concern especially in the context of safety- and security-critical scenarios. Adversarial examples (AEs) represent a typical and important type of defects needed to be urgently addressed, on which a DL software makes incorrect decisions. Such defects occur through either intentional attack or physical-world noise perceived by input sensors, potentially hindering further industry deployment. The intrinsic uncertainty nature of deep learning decisions can be a fundamental reason for its incorrect behavior. Although some testing, adversarial attack and defense techniques have been recently proposed, it still lacks a systematic study to uncover the relationship between AEs and DL uncertainty. In this paper, we conduct a large-scale study towards bridging this gap. We first investigate the capability of multiple uncertainty metrics in differentiating benign examples (BEs) and AEs, which enables to characterize the uncertainty patterns of input data. Then, we identify and categorize the uncertainty patterns of BEs and AEs, and find that while BEs and AEs generated by existing methods do follow common uncertainty patterns, some other uncertainty patterns are largely missed. Based on this, we propose an automated testing technique to generate multiple types of uncommon AEs and BEs that are largely missed by existing techniques. Our further evaluation reveals that the uncommon data generated by ourmethod is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35%. Our results call for attention and necessity to generate more diverse data for evaluating quality assurance solutions of DL software.",https://ieeexplore.ieee.org/document/9284139/,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),5-11 Oct. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA46521.2020.9212097,Towards Real-time Process Monitoring and Machine Learning for Manufacturing Composite Structures,IEEE,Conferences,"Components made from carbon fiber reinforced plastics (CFRP) offer attractive stability properties for the automotive or aerospace industry despite their light weight. To automate CFRP production, resin transfer molding (RTM) based on thermoset plastics is commonly applied. However, this manufacturing process has its shortcomings in quality and costs. The project CosiMo aims for a highly automated and cost-attractive manufacturing process using cheaper thermoplastic materials. In a thermoplastic RTM (T-RTM) process, the polymerization of ϵ-caprolactam to polyamide 6 is investigated using an intelligent mold tooling. Multiple sensor types integrated into the mold allow for tracking of process-relevant variables, such as material flow and polymerization state. In addition to monitoring the T-RTM process, a digital twin visualizes progress and makes predictions about issues and countermeasures based on machine learning.",https://ieeexplore.ieee.org/document/9212097/,2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),8-11 Sept. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GreenTech46478.2020.9289758,"Towards Smart e-Infrastructures, A Community Driven Approach Based on Real Datasets",IEEE,Conferences,"e-Infrastructures have powered the successful penetration of e-services across domains, and form the backbone of the modern computing landscape. e-Infrastructure is a broad term used for large, medium and small scale computing environments. The increasing sophistication and complexity of applications have led to even small-scale data centers consisting of thousands of interconnects. However, efficient utilization of resources in data centers remains a challenging task, mainly due to the complexity of managing physical nodes, network equipment, cooling systems, electricity, etc. This results in a very strong carbon footprint of this industry. In recent years, efforts based on machine learning approaches have shown promising results towards reducing energy consumption of data centers. Yet, practical solutions that can help data center operators in offering energy efficient services are lacking. This problem is more visible in the context of medium and small scale data center operators (the long tail of e-infrastructure providers). Additionally, a disconnect between solution providers (machine learning experts) and data center operators has been observed. This article presents a community-driven open source software framework that allows community members to develop better understanding of various aspects of resource utilization. The framework leverages machine learning models for forecasting and optimizing various parameters of data center operations, enabling improved efficiency, quality of service and lower energy consumption. Also, the proposed framework does not require datasets to be shared, which alleviates the extra effort of organizing, describing and anonymizing data in an appropriate format.",https://ieeexplore.ieee.org/document/9289758/,2020 IEEE Green Technologies Conference(GreenTech),1-3 April 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2009.106,Towards a Technology Platform for Building Corporate Radar Applications that Mine the Web for Business Insight,IEEE,Conferences,"In this paper, we give a progress report on an ongoing effort at Accenture to develop a technology platform for building a wide range of corporate radar applications,which can turn the Web into a systematic source of business insight. Our goal is to share the platform we have developed and the lessons we have learned, so others can leverage this knowledge when building similar applications. We give an overview of this platform, which integrates a combination of established AI technologies - i.e. semantic models, natural language processing, and inference engines - in a novel way. We then illustrate the kinds of corporate radars that can be built with our platform through two applications we developed at Accenture: the technology lifecycle tracker, which assesses the maturity of technologies from the wireless industry, and the technology trend tracker, which measures hype versus reality for emerging technology trends such as cloud computing, software-as-a-service, and more. Finally, we discuss our experiences in using this platform to build these applications and the lessons learned.",https://ieeexplore.ieee.org/document/5363824/,2009 21st IEEE International Conference on Tools with Artificial Intelligence,2-4 Nov. 2009,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2016.7474875,Towards a fully automated 3D printability checker,IEEE,Conferences,"3D printing has become one of the most popular evolutionary techniques with diverse application, even as normal people's hobby. As printing target, enormous 3D virtual models from game industry and virtual reality flood the internet, shared in various online forums, such as thingiverse. But which of them can really be printed? In this paper we propose the 3D Printability Checker, which can be used to automatically answer this non-trivial question. One of the major novelties of this paper is the process of dependable software engineering we use to build this Printability Checker. Firstly, we prove that this question is decidable with a given 3D object and a list of printer profiles. Secondly, we design and implement such a checker. Finally, we show our experimental results and use them further for a machine learning approach to improve our system in an automatic way. The generic framework provides a useful basis of automatic self-improvement of the software by combining current techniques in the area of formal method, geometry modelling and machine learning.",https://ieeexplore.ieee.org/document/7474875/,2016 IEEE International Conference on Industrial Technology (ICIT),14-17 March 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISEF.2017.8090700,Towards an IOT-enabled intelligent energy management system,IEEE,Conferences,"Vehicles and billion other physical devices are today enriched with electronics, sensors and software that enable data collection and exchange. At the same time, Internet-of-Things initiates a new era where the world will change deeply and decisively in many ways. In this context, both industry and research community attempts to merge engineering and artificial intelligence. This direction considers objects, vehicles, devices and even buildings, as the driving force for autonomous IoT that enables intelligent management for crucial issues. To this end, this article proposes a novel approach that combines theoretical and scientific knowledge, related to IoT and Artificial Intelligence to real-world needs as they are reported by an engineering perspective. The article presents the first steps towards a theoretical model that will lead to an intelligent platform which will enable efficient energy management in real world paradigms, moving from simulations to real-life applications.",https://ieeexplore.ieee.org/document/8090700/,"2017 18th International Symposium on Electromagnetic Fields in Mechatronics, Electrical and Electronic Engineering (ISEF) Book of Abstracts",14-16 Sept. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JCSSE53117.2021.9493822,Towards robust Machine Learning models for grape ripeness assessment,IEEE,Conferences,"Artificial intelligence methods need to be more transparent for wider acceptance by the industry. In particular deep neural networks (DNN) are not explainable, due to the complex processes the input undergo. The present work addresses model explainability for wine grapes quality assessment through 1D-CNN, using regression activation maps (RAM) to show the contribution score of each wavelength for the prediction of sugar content. This way we identify the relevant regions related to this enological parameter. The results obtained indicate that the proposed approach can successfully highlight important spectral regions related to sugars absorption, improving the current state of the art, and opening way to dimensionality reduction methods and further model interpretation.",https://ieeexplore.ieee.org/document/9493822/,2021 18th International Joint Conference on Computer Science and Software Engineering (JCSSE),30 June-2 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSES52305.2021.9633959,Traffic Sign Detection using Deep Learning Techniques in Autonomous Vehicles,IEEE,Conferences,"Autonomous vehicle is an emerging topic for both researchers and the automobile industry as companies are still struggling to make fully functional autonomous vehicles. Driving a safe vehicle in a real world depends on different conditions, such as distance from other vehicles, pedestrians, animals, speed-breakers, traffic signals and other unpredictable dynamic environments. Autonomous vehicle can decrease vehicle crashes because software installed in the vehicle instructs the control system of the autonomous vehicle rather than human, and Software makes less error compare to human beings. Automated Traffic Sign Detection and Recognition (ATSDR) is an important task for a safe driving by an autonomous vehicle. Many researchers have used various deep learning-based models for in real-time ATSDR. Here in the present review, we have studied various deep learning models used for in real-time ATSDR. Our study suggested that YOLO and SSD can detect the traffic sign in real time and are superior models for ATSDR as compared to other deep learning methods as CNN, R-CNN, Fast R-CNN and Faster RCNN.",https://ieeexplore.ieee.org/document/9633959/,"2021 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)",24-25 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDUCON52537.2022.9766804,Training of Engineers: Approaches to Customization of Educational Programs,IEEE,Conferences,"Since the field of information technology (IT) is constantly and rapidly developing, the training of engineering personnel couldn&#x2019;t be behind this global digital transformation. The new digital reality focuses on the mandatory formation of competencies which are end-to-end technology-oriented: Big Data, machine learning and artificial intelligence, augmented and virtual reality, robotics, blockchain, Internet of Things, 5G technologies, quantum technologies and others. Considering the speed of the development of these technologies, concerns have arisen about the relevance of the content of educational programs of higher education, as well as the degree of flexibility of educational trajectories of engineering graduates. To ensure the relevance and flexibility, constant revision of training programs regarding the study and development of new paradigms and solutions is required. This paper calls into question how to meet the requirements of the labor market at the time of graduation, considering that the educational programs are compiled at the time of the beginning of education and in accordance with the established federal standards of higher education. The goal is to adjust the results of the educational program or even to introduce completely new results in accordance with the changes that have occurred in the industry at the time of the implementation of the program to ensure students form the most relevant and in-demand competencies. The article describes the approaches of the Institute of Computer Technologies and Information Security (ICTIS) of the Southern Federal University to the creation of flexible, interdisciplinary bachelor&#x2019;s and master&#x2019;s degree educational programs. The content of these programs directly corresponds to the current and promising demands of the labor market. The presented approaches to the customization of educational programs meet the needs of students in personal and professional development. These approaches contribute to the development of a student-centered learning system that is specific for each level of education. Project-based learning is a key tool for the formation of relevant professional competencies within bachelor&#x2019;s degree programs. The master&#x2019;s degree focuses on the formation of unique research competencies in the context of the current agenda. For this purpose, the ICTIS has opened special research programs that involve a grant system for undergraduates. What is also important is the inclusion of undergraduates in research groups led by postdocs of the Institute. The key principle uniting these approaches into a single system is the introduction of its own educational standards in engineering areas in the ICTIS. The standards of the ICTIS regulate the possibility for students to choose variable professional competencies after mastering the basic educational component. These competencies are formed annually based on the analysis of prospective labor market demands and can be included in the program even after the start of its implementation. The analysis on the results of the ICTIS educational standards implementation has shown the effectiveness of the concept of these standards in the field of computer technology and information security. The effectiveness of this concept of the educational standard allows gradually being implemented in other fields of knowledge.",https://ieeexplore.ieee.org/document/9766804/,2022 IEEE Global Engineering Education Conference (EDUCON),28-31 March 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TDC.2002.1177707,Transformer monitoring moving forward from monitoring to diagnostics,IEEE,Conferences,"The technologies employed for monitoring transformers have been evolving over the last 10 or more years to the point where they are now commonly accepted, and have been demonstrated to provide useful data on the key parameters and components of critical power transformers. A major aspect of these technologies has been the accumulation of copious amounts of data, and the associated problem, of what to do with it all. With time and resources in short supply to do a proper analysis of this data, to turn it into useful transformer information, there needs to be a new set of technologies and techniques implemented. The advent of new methods of data modeling and interpretation using statistical analysis, rules based, and artificial intelligence systems is now moving from the research stage to practical field implementation. The industry has a very real need to move from ""just monitoring"" equipment to the point of being able to have the knowledge of the operating condition of the equipment and when things begin to go wrong, diagnose the problem to provide a recommended course of action.",https://ieeexplore.ieee.org/document/1177707/,IEEE/PES Transmission and Distribution Conference and Exhibition,6-10 Oct. 2002,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAIRO.2017.61,Tuning Software Based on Genetic Algorithm Applied to Industrial PID Loops,IEEE,Conferences,"Time-delay processes are frequently found in industry and the most common representations are first order plus delay time (FOPDT) and integrator plus delay time (IPDT) transfer functions. The identification of time delay systems is a challenging task and usually, the performance of its control is not optimal. This work presents a software based on a real coded genetic algorithm to identify the system, using open or close loop information, and to tune the PID controller using several methods. Results on simulation and real industrial loops are presented.",https://ieeexplore.ieee.org/document/8253004/,"2017 International Conference on Control, Artificial Intelligence, Robotics & Optimization (ICCAIRO)",20-22 May 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR.2015.73,Tutorial 4: AR Implementations in Informal Learning,IEEE,Conferences,"A variety of cases uses of AR in informal learning environments. The cases uses are drawn from a variety of different contexts. There will be examples of AR use in education, tourism, event organizing, and others. This is mainly geared to people creating learning environments in any industry a foundation to start implementation AR. The featured case use will be how AR was used at TEDxKyoto to engage participants. There will also be several student projects that use AR presented and available for demo.",https://ieeexplore.ieee.org/document/7328050/,2015 IEEE International Symposium on Mixed and Augmented Reality,29 Sept.-3 Oct. 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CICA.2009.4982774,Tutorial CICA-T Computing with intelligence for identification and control of nonlinear systems,IEEE,Conferences,"System characterization and identification are fundamental problems in systems theory and play a major role in the design of controllers. System identification and nonlinear control has been proposed and implemented using intelligent systems such as neural networks, fuzzy logic, reinforcement learning, artificial immune system and many others using inverse models, direct/indirect adaptive, or cloning a linear controller. Adaptive Critic Designs (ACDs) are neural networks capable of optimization over time under conditions of noise and uncertainty. The ACD technique develops optimal control laws using two networks - critic and action. There are merits for each approach adopted will be presented. The primary aim of this tutorial is to provide control and system engineers/researchers from industry/academia, new to the field of computational intelligence with the fundamentals required to benefit from and contribute to the rapidly growing field of computational intelligence and its real world applications, including identification and control of power and energy systems, unmanned vehicle navigation, signal and image processing, and evolvable and adaptive hardware systems.",https://ieeexplore.ieee.org/document/4982774/,2009 IEEE Symposium on Computational Intelligence in Control and Automation,30 March-2 April 2009,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VTCFall.2018.8690788,Ultra-Low Power IoT Traffic Monitoring System,IEEE,Conferences,"Given the sizable anticipated proliferation of Internet of Things (IoT) devices, Forrester Research forecasts that the fleet management and transportation industry sectors will enjoy more growth than others. This may come as no surprise, since infrastructure (e.g., roadways, bridges, airports) is a prime candidate for sensor integration to provide real-time measurements and to support intelligent decisions. The predicted increase of deployed devices makes it difficult to calculate the amount of energy required for these functions. Current estimates suggest that 2 to 4% of worldwide carbon emissions can be attributed to the information and communication industry. This paper presents novel algorithms designed to optimize power consumption of an intelligent vehicle counter and classifier sensors. Each was based on an event-driven methodology wherein a control block orchestrates the work of different components and subsystems. System duty-cycle is reduced through several techniques, and a reinforcement learning algorithm is introduced to control the system power policy, according to the traffic environment. Battery life for a sensor supported by a 2300 mAh battery was extended from 48-hour, adopted all-on policy to more than 400 days when leveraging the algorithms and techniques presented in this work.",https://ieeexplore.ieee.org/document/8690788/,2018 IEEE 88th Vehicular Technology Conference (VTC-Fall),27-30 Aug. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF53456.2022.9744314,Uncertainty Processing by Tensor Algebra Means in Condition of Movement Along Complex Roads,IEEE,Conferences,"The article is devoted to research related to the processing of uncertainty by means of tensor algebra in the conditions of the road traffic control based on wireless sensor networks determine complex controlling road infrastructure. The organization of traffic on difficult roads is one of the key challenges of the transportation industry. The solution of this problem occurs under conditions of uncertainty, which may appear at different levels of the traffic control process. The article offers a classification of uncertainty in a complex dynamic system. By the example of organizing the interaction of smart controllers in an electronic coupling, the author shows the results of applying the methods of tensor network analysis to obtain the computational base of electronic coupling network interaction of intelligent (smart) vehicle controllers. Examples of processing uncertainty of different types using the apparatus of fuzzy sets and tensor basis are considered. Tensor equations provide efficient processing of big data, obtaining information in-real-time mode, the stability of &#x201C;Intelligent electronic hitch&#x201D; system to changes in the topology of the connection of controllers and changes in soft- and hard- components of the connection. The use of tensors in the computational basis of the intelligent electronic hitch makes it possible successful implementation of embedded AI at the level of an intelligent (smart) controller. The results obtained in the article show that the computational basis on the example of the electronic hitch algorithm can be used in smart controller systems of any level of complexity based on wireless sensor networks.",https://ieeexplore.ieee.org/document/9744314/,2022 Systems of Signals Generating and Processing in the Field of on Board Communications,15-17 March 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECA52323.2021.9676126,Understanding Customer Reviews using Facial Expression Recognition System,IEEE,Conferences,"The 21st century marks the beginning of Industry4.0 with data as primary driving force. The abundant availability of high-resolution videos, images of numerous industry processes, human activities coupled with sophisticated data driven technologies stands a base for evolution of latest technologies like image processing, industrial process automation. Here, the aim is to adapt the image processing technology to understand human behaviors based on the facial expressions drawn from various persons and apply that to evaluate customer review in retail market. The objective is to build a deep learning facial expression recognition classification model using OpenCV, Convolutional Neural Networks (CNN) libraries backed by TensorFlow framework and the trained model is deployed to a web interface using Flask API and generate customer review sheet. Attempts are made to analyze and classify data into 3 customer reviews as Satisfied, Not Satisfied and Neutral. These classification algorithms are deployed to find the real-time facial expression on videos and images to find the probability that an image belong to specific class data and generate customer review sheet.",https://ieeexplore.ieee.org/document/9676126/,"2021 5th International Conference on Electronics, Communication and Aerospace Technology (ICECA)",2-4 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSTIC52283.2021.9461259,Universal Semiconductor ATPG Solutions for ATE Platform under the Trend of AI and ADAS,IEEE,Conferences,"This article introduces a universal semiconductor Automatic Test Pattern Generation (ATPG) solution for Automated Test Equipment (ATE) platform. With the increasing trend of Artificial Intelligence (AI) and Advanced Driving Assistance System (ADAS) the communication between semiconductor devices requires advanced protocols such as Mobile Industry Processor Interface (MIPI) and Point-to-point (P2P) protocols. A designer-based solution is developed to provide a one-click software approach to create test vectors for common protocols and customized protocols. As a result, the silicon debug cycle can be massively reduced, comparing with converting waveform files generated from traditional Electronic Design Automation (EDA) tools. This solution can dramatically reduce the workload of test engineers and enable IC designers to participate in the debugging process of the device directly with an intuitive way. Such workflow can rise the efficiency of semiconductor test process and further decrease the Time to Market (TTM) of new product. This solution is designed as a comparable tool towards traditional EDA tools and will be another choice for ATPG solution. Up to now, this solution can generate test vectors for advanced protocols like MIPI D-PHY/C-PHY as well as basic protocols such as Inter-Integrated Circuit (I2C) and Serial Peripheral Interface (SPI) and complete evaluation on real device.",https://ieeexplore.ieee.org/document/9461259/,2021 China Semiconductor Technology International Conference (CSTIC),14-15 March 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTC.2017.8190968,Unmanned aerial vehicle surveillance system (UAVSS) for forest surveillance and data acquisition,IEEE,Conferences,"An application framework is proposed in this paper that considers low cost surveillance mechanism and data acquisition in the forest. An application is developed as proof of concept with detailed design that can take advantage of unmanned urban vehicle to be directly configured and controlled in real-time. The advantages are numerous; it can be used for many purposes. For example, it can be used for observing critical and important area for intruder activities or to know the current state of any object of interest. We considered using machine learning and image processing and can be used for species of trees in the forest by color and size detection. A separate service running on separate remote server will be responsible for this. We have proposed a application framework particularly to be cheap and easy to handle by non-technical persons and that it does not require large software system knowledge like Pix4D or DroneDeploy. This system will be useful for operations and research specially the forestry and palm oil plantation surveillance, and sustainable timber industry that specially needs carefully collected imageries and data from objects. Collection of raw data from sensor networks is also proposed in the system architecture.",https://ieeexplore.ieee.org/document/8190968/,2017 International Conference on Information and Communication Technology Convergence (ICTC),18-20 Oct. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2016.7840903,Unravelling the Myth of big data and artificial intelligence in sustainable natural resource development,IEEE,Conferences,"Natural Resources businesses span one or more of the following verticals: Explore & Discover, Develop, Extract & Transport, Market & Trade. Natural Gas (NG) is a key component of the Natural Resources industry, which contributes both directly and indirectly to sustain and support the day to day energy needs of humans. Various forums drive key initiatives to maintain the sustainability of natural resources for decades to come. The recent evolution of advanced Information Technology (IT) fields, termed as `Big Data and Artificial Intelligence (BD&AI),' fortify these initiatives by providing anytime, anywhere access of data (for example in remote offshore assets or facilities) from a vast corpus base, in addition to enabling natural human interaction. In this paper we intend to explore the key primitives of BD&AI and cite the method of real-world implementation in the Natural Gas industry to further enable sustainable development.",https://ieeexplore.ieee.org/document/7840903/,2016 IEEE International Conference on Big Data (Big Data),5-8 Dec. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2MTC43012.2020.9128596,Unsupervised Flight Phase Recognition with Flight Data Clustering based on GMM,IEEE,Conferences,"Currently, with the rapid development of the aviation industry, researchers are paying more attention to the improvement of aviation safety. Aviation safety mainly includes flight safety, aviation ground safety, and air defense safety. In terms of flight safety, the analysis of large amounts of flight data has gradually become a useful tool for timely detection of potential dangers at various stages of flight. As a result, flight data analysis has been one of the hot topics in aviation. However, due to the complexity of the aircraft operating conditions throughout the aircraft, if the data is analyzed at the entire flight phase, it is very difficult and time consuming to identify the problematic fight phase. Therefore, flight phase recognition for civil aircraft is implemented in this study. A flight phase recognition method based on Gaussian Mixture Model (GMM) is proposed in this work, which is the important foundation for timely detecting the abnormal event and improving the system safety and reliability. Firstly, the FDR data are preprocessed by spline interpolation and normalization, and then a GMM-based flight phase clustering is realized. In addition, a set of evaluation method is developed to evaluate the quality of flight phase recognition result. Finally, the effectiveness of the method is verified by using real FDR data from NASA's open database.",https://ieeexplore.ieee.org/document/9128596/,2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),25-28 May 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RoEduNet51892.2020.9324883,Usage of Asymetric Small Binning to Compute Histogram of Oriented Gradients for Edge Computing Image Sensors,IEEE,Conferences,"In case of multiple imaging sensors used in different networks (home security, surveillance, automotive, industrial), there is a challenge to perform object detection algorithms in real time, even on the cloud, for a large number of sensors. This is why there is an intensive effort in the industry to move object detection processing on the edge, with the benefits of reducing the bandwidth needs and allowing for scalability in large networks. In this paper we present a hardware friendly optimization technique to compute Histogram of Oriented Gradients (HOG) on the edge, by approximating the HOG orientation as a multitude of small bins. The technique is implemented in RTL for FPGA or ASIC and serves as the first step in a standard object detection algorithm (using Histogram of Oriented Gradients as feature extractor and Support Vector Machine as the detection algorithm). We verified the results of the proposed optimizations for errors by comparison to a reference method and the overall object detection algorithm for robustness.",https://ieeexplore.ieee.org/document/9324883/,2020 19th RoEduNet Conference: Networking in Education and Research (RoEduNet),11-12 Dec. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iCECE.2010.1483,Use of Accurate GPS Timing Based on Radial Basis Probabilistic Neural Network in Electric Systems,IEEE,Conferences,"GPS based time reference provides inexpensive but highly-accurate timing and synchronization capability and meets requirements in power system fault location, monitoring, and control. In the present era of restructuring and modernization of electric power utilities, the applications of GIS/GPS technology in power industry are growing and covering several technical and management activities. Because of GPS receivers error sources are time variant, it is necessary to remove the GPS measurement noise. In this paper a Radial Basis Probabilistic Neural Network (RBPNN) has been applied to GPS receivers timing data for modeling of their timing error sources. This method estimates GPS receivers timing errors from their previous values. The efficiency of this algorithm is illustrated by experiments. For real-time restitution of GPS timing accuracy, the proposed method is implemented on designed hardwares. The experimental tests results on collected real data emphasizes that GPS timing RMS errors can be reduced from 300 nsec and 200 nsec to less than 160 nsec and 41 nsec, with and without SA, respectively.",https://ieeexplore.ieee.org/document/5630774/,2010 International Conference on Electrical and Control Engineering,25-27 June 2010,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICST.2017.20,Using Semantic Similarity in Crawling-Based Web Application Testing,IEEE,Conferences,"To automatically test web applications, crawling-based techniques are usually adopted to mine the behavior models, explore the state spaces or detect the violated invariants of the applications. However, their broad use is limited by the required manual configurations for input value selection, GUI state comparison and clickable detection. In existing crawlers, the configurations are usually string-matching based rules looking for tags or attributes of DOM elements, and often application-specific. Moreover, in input topic identification, it can be difficult to determine which rule suggests a better match when several rules match an input field to more than one topic. This paper presents a natural-language approach based on semantic similarity to address the above issues. The proposed approach represents DOM elements as vectors in a vector space formed by the words used in the elements. The topics of encountered input fields during crawling can then be inferred by their similarities with ones in a labeled corpus. Semantic similarity can also be applied to suggest if a GUI state is newly discovered and a DOM element is clickable under an unsupervised learning paradigm. We evaluated the proposed approach in input topic identification with 100 real-world forms and GUI state comparison with real data from industry. Our evaluation shows that the proposed approach has comparable or better performance to the conventional techniques. Experiments in input topic identification also show that the accuracy of the rule-based approach can be improved by up to 22% when integrated with our approach.",https://ieeexplore.ieee.org/document/7927970/,"2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)",13-17 March 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FSKD.2015.7382048,Using hidden markov model for dynamic malware analysis: First impressions,IEEE,Conferences,"Malware developers are coming up with new techniques to escape malware detection. Furthermore, with the common availability of malware construction kits and metamorphic virus generators, creation of obfuscated malware has become a child's play. This has made the task of anti-malware industry a challenging one, who need to analyze tens of thousands of new malware samples everyday in order to provide defense against the malware threat. The silver lining is that most of the malware generated by such means is different only syntactically, and hence techniques employing dynamic analysis and behavior modeling can be effectively used for classifying malware. In this paper we have proposed a malware classification scheme based on Hidden Markov Models using system calls as observed symbols. Our approach combines the powerful statistical pattern analysis capability of Hidden Markov Models with the proven capacity of system calls as discriminating dynamic features for countering malware obfuscation. Testing the proposed technique on system call logs of real malware shows that it has the potential of effectively classifying unknown malware into known classes.",https://ieeexplore.ieee.org/document/7382048/,2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD),15-17 Aug. 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR.2019.00-22,VR Props: An End-to-End Pipeline for Transporting Real Objects Into Virtual and Augmented Environments,IEEE,Conferences,"Improvements in both software and hardware, as well as an increase in consumer suitable equipment, have resulted in great advances in the fields of virtual and augmented reality. Typically, systems use controllers or hand gestures to interact with virtual objects. However, these motions are often unnatural and diminish the immersion of the experience. Moreover, these approaches offer limited tactile feedback. There does not currently exist a platform to bring an arbitrary physical object into the virtual world without additional peripherals or the use of expensive motion capture systems. Such a system could be used for immersive experiences within the entertainment industry as well as being applied to VR or AR training experiences, in the fields of health and engineering. We propose an end-to-end pipeline for creating an interactive virtual prop from rigid and non-rigid physical objects. This includes a novel method for tracking the deformations of rigid and non-rigid objects at interactive rates using a single RGBD camera. We scan our physical object and process the point cloud to produce a triangular mesh. A range of possible deformations can be obtained by using a finite element method simulation and these are reduced to a low dimensional basis using principal component analysis. Machine learning approaches, in particular neural networks, have become key tools in computer vision and have been used on a range of tasks. Moreover, there has been an increased trend in training networks on synthetic data. To this end, we use a convolutional neural network, trained on synthetic data, to track the movement and potential deformations of an object in unlabelled RGB images from a single RGBD camera. We demonstrate our results for several objects with different sizes and appearances.",https://ieeexplore.ieee.org/document/8943647/,2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR),14-18 Oct. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCECE47787.2020.9255806,Vehicle Damage Classification and Fraudulent Image Detection Including Moiré Effect Using Deep Learning,IEEE,Conferences,"Image-based vehicle insurance processing and loan management has large scope for automation in automotive industry. In this paper we consider the problem of car damage classification, where categories include medium damage, huge damage and no damage. Based on deep learning techniques, MobileNet model is proposed with transfer learning for classification. Moreover, moving towards automation also comes with diverse hurdles; users can upload fake images like screenshots or taking pictures from computer screens, etc. To tackle this problem a hybrid approach is proposed to provide only authentic images to algorithm for damage classification as input. In this regard, moiré effect detection and metadata analysis is performed to detect fraudulent images. For damage classification 95% and for moiré effect detection 99% accuracy is achieved.",https://ieeexplore.ieee.org/document/9255806/,2020 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE),30 Aug.-2 Sept. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVIDL51233.2020.00-85,Vehicle automatic driving system based on embedded and machine learning,IEEE,Conferences,"With the rapid development of Chinnes highway transportation industry, the problem of road traffic safety has become increasingly prominent. Highway passenger transport accidents are generally fatal and fatal accidents. Traffic accidents not only cause enormous economic losses to transport enterprises, but also have a very bad social impact on local highway transport management departments, which has even become a new social instability factor. This is mainly because there are many problems in autopilot technology, such as low recognition accuracy, poor real-time performance, weak anti-interference ability and so on. However, embedded technology and machine learning can solve these problems well, so autopilot technology will become the mainstream in the future. Firstly, this paper analyses the importance of autopilot technology. Then this paper analyses the machine learning target recognition, vehicle automatic driving system model and vehicle automatic driving system flow. Finally, this paper designs the function of autopilot system.",https://ieeexplore.ieee.org/document/9270523/,"2020 International Conference on Computer Vision, Image and Deep Learning (CVIDL)",10-12 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAC54203.2021.9671198,Virtual Dressing Room: Smart Approach to Select and Buy Clothes,IEEE,Conferences,"The clothing industry portrays a major part of a respective country`s economy. Due to the predilection for clothing items of the people have led to the increasing of physical and online clothing stores in all around the world. Most of the people are used to go to the physical shopping and purchase their desired clothing items. But, as a consequence of the current pandemic situation, most of the people are unable to step out from their homes. This application is intended to cater an opportunity to the customers, who are not able to reach the physical clothing stores due to a pandemic situation and mobility difficulties. In addition, this application diminishes the time wastage, clothing size mismatches and the lesser user satisfaction ratio inside a physical clothing store. A customized 3D model has featured in the application to cater the virtual fitting experience to the customer. And the AI chatbot assistant in the application interacts with the user while catering virtual assistance for a better cloth selection process. In addition to that, this application has concentrated on the clothing shop by providing a future sales prediction component utilizing the K-Nearest Neighbors algorithm to provide an aid to their business commitments.",https://ieeexplore.ieee.org/document/9671198/,2021 3rd International Conference on Advancements in Computing (ICAC),9-11 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SERVICES.2010.125,Virtual Services in Cloud Computing,IEEE,Conferences,"Cloud computing has aroused wide research interests and has been accepted by industry. Services are playing the essential role in cloud computing as cloud computing refers to “both the applications delivered as services over the Internet and the hardware and systems software in the datacenters that provide those services”. Therefore, service-oriented architecture should play an important role in cloud computing. In addition, one of the characteristics of cloud computing is to make services available on demand. Given a group of services, different demands may involve different set of services and in different order. This is related to services reuse and composition. However, existing methods only solve service composition in a binary manner: Either a solution exists or no solutions at all. In this paper, we propose the concept of virtual services, which physically do not exist, but are conceptually treated in the same way as physical services. Virtual services are useful when traditional service compositions fail. Virtual services can connect the existing physical services and enable the composition process to succeed. The specifications of virtual services will provide valuable information about how to develop real services to meet the requirements of the given demand. In this paper, we present algorithms that can help identify the virtual services in the case of composition failure and provide the specification of virtual services for further analysis and development.",https://ieeexplore.ieee.org/document/5575563/,2010 6th World Congress on Services,5-10 July 2010,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITST.2017.7972192,Virtual assistants and self-driving cars,IEEE,Conferences,"Self-driving cars are technologically a reality and in the next decade they are expected to reach the highest level of automation. While there is general agreement that an advanced human-autonomous vehicle (HAV) interaction is key to achieve the benefits of self-driving cars, it is less clear what role artificial intelligence (AI) should play in this context. While the scientific community is debating on the role and intersections of AI, autonomous vehicles and related issues, above all ethics, the automotive industry is already presenting AI-based products and services that may influence, in a direction or in another, our technological and societal futures. This paper focuses on virtual assistants, the personification of the car intelligence incorporating, among others, an algorithmic “brain”, a synthetic human “voice” and powerful sensor-based “senses”. Should virtual assistants just assist humans or replace them whenever necessary? Should their scope of action be limited to safety-related driving tasks or to any activity performed in the car or controlled from the car? Although at a very early stage of commercial development, the paper will review the state-of-the-art of in-car virtual assistants underlining their role and functions in the connected and automated driving ecosystem. By drawing from earlier reflections on automation, robots and intelligent agents, it will then identify a series of issues to be addressed by the scientific community, policy-makers and the automotive industry stakeholders.",https://ieeexplore.ieee.org/document/7972192/,2017 15th International Conference on ITS Telecommunications (ITST),29-31 May 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITHET.2012.6246058,Virtual industrial training: Joining innovative interfaces with plant modeling,IEEE,Conferences,"Training in industry is one of the most critical and expensive tasks to be faced by the management. Furthermore, in some cases, it is dangerous or even impossible to directly train operators on the real plants where security and safety problems may arise, making it very difficult to start training programs at low cost. For these reasons, the field of training in industry is rapidly developing using software or hardware solutions coming mainly from the following research areas: i) Human-Computer interaction, i.e., the use of complex and interactive human-machine interfaces, ii) plant simulators, i.e., software systems which are delivered with the plant itself to test and to learn complex tasks and processes, iii) Intelligent Training Systems, i.e., the availability of intelligent and personalized training systems where a virtual tutor guides users through a personalized learning path. In this paper we present the overall architecture of a system for industrial training, embedded into an Intelligent Tutoring System that can provide more effective and personalized training and learning in a context where working directly on real plants can be difficult and very expensive. In particular we present a simulator for training operators in using power plants, based on a multimedia and on interactive interface. This system is particularly suitable to be used for training in industrial electric and oil plants. Moreover, the system allows operators for collaborative problem solving. Currently the system is under delivery to an Italian Electric industry.",https://ieeexplore.ieee.org/document/6246058/,2012 International Conference on Information Technology Based Higher Education and Training (ITHET),21-23 June 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData52589.2021.9671295,Virtual-SRE For Monitoring Large Scale Time-series Data,IEEE,Conferences,"Monitoring online time-series data and providing real-time alerts are crucial for a variety of industry applications. But doing it at a large scale is a challenging task. In this paper, we introduce Virtual-SRE, an anomaly detection framework which utilizes human knowledge and is suitable for large-scale applications. The proposed approach starts without any human labeling. Instead, human knowledge is obtained through feedbacks. A joint data and model optimization process is proposed to ensure that accurate models can be built. This makes it easily applicable to large-scale and heterogeneous tasks while enjoying the benefits of supervised learning. The proposed framework has been deployed to monitor thousands of time-series data streams and obtained favorable results.",https://ieeexplore.ieee.org/document/9671295/,2021 IEEE International Conference on Big Data (Big Data),15-18 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSTCC.2019.8885611,Visual Analytics Framework for Condition Monitoring in Cyber-Physical Systems,IEEE,Conferences,"One of the biggest challenges facing the factory of the future today is to reduce the time-to-market access and increase through the improvement of competitiveness and efficiency. In order to achieve this target, data analytics in Industrial Cyber-Physical System becomes a feasible option. In this paper, a visual analytics framework for condition monitoring of the machine tool is presented with the aim to manage events and alarms at factory level. The framework is assessed in a particular use case that consists in a multi-threaded cloud-based solution for the global analysis of the behaviour of variables acquired from PLC, CNC and robot manipulator. A human-machine interface is also designed for the real-time visualization of the key performance indicators according to the user's criteria. This tool implemented is a great solution for condition monitoring and decision-making process based on data analytics from simple statistics to complex machine learning methods. The results achieved are part of the vision and implementation of the industrial test bed of “Industry and Society 5.0” platform.",https://ieeexplore.ieee.org/document/8885611/,"2019 23rd International Conference on System Theory, Control and Computing (ICSTCC)",9-11 Oct. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMV53313.2021.9670978,Voice Controlled Augmented Reality For Real Estate,IEEE,Conferences,"As technology advances, augmented reality is becoming more prevalent in every business. The most frequent usage of AR is to project real things onto the user, which is usually done via an image target. In the real estate industry, no one can deny AR's capacity to improve the buying and selling experience. AR can help real estate developers expand their marketing methods and give clients a more memorable home experience. It has already been used in apps for house design and land hunting, and the industry's strike proves that Augmented Reality has a lot more to give. Everyone nowadays has a smartphone or tablet with which to access the internet, and the technology utilized in these devices is improving every day. As a result, using AR tools in everyday life and having a comfortable AR experience on mobile devices is becoming more convenient. The amount of time spent touring each site with consumers and not having appropriate resources to impress them is a common challenge that real estate developers confront. Augmented reality software is frequently the seal of approval that realtors receive in order to grow their business and overcome these obstacles. This paper proposes a method for projecting a home onto an image target and allowing the user to explore the interior of the house. Voice controllers incorporated into AR can control the interior.",https://ieeexplore.ieee.org/document/9670978/,2021 International Conference on Artificial Intelligence and Machine Vision (AIMV),24-26 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CASE48305.2020.9216781,Weak Scratch Detection of Optical Components Using Attention Fusion Network,IEEE,Conferences,"Scratches on the optical surface can directly affect the reliability of the optical system. Machine vision-based methods have been widely applied in various industrial surface defect inspection scenarios. Since weak scratches imaging in the dark field has an ambiguous edge and low contrast, which brings difficulty in automatic defect detection. Recently, many existing visual inspection methods based on deep learning cannot effectively inspect weak scratches due to the lack of attention-aware features. To address the problems arising from industry-specific characteristics, this paper proposes “Attention Fusion Network;”, a convolutional neural network using attention mechanism built by hard and soft attention modules to generate attention-aware features. The hard attention module is implemented by integrating the brightness adjustment operation in the network, and the soft attention module is composed of scale attention and channel attention. The proposed model is trained on a real-world industrial scratch dataset and compared with other defect inspection methods. The proposed method can achieve the best performance to detect the weak scratch inspection of optical components compared to the traditional scratch detection methods and other deep learning-based methods.",https://ieeexplore.ieee.org/document/9216781/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SenSysML50931.2020.00007,Welcome Message from SenSys-ML'20 Chairs,IEEE,Conferences,"We are very excited to welcome you to the second ACM/IEEE International Workshop on Machine Learning on Edge in Sensor Systems (SenSys-ML 2020). SenSys-ML’20 will be held in conjunction with the CPS-IoT Week 2020 and focuses on work that combines sensor signals from the physical world with machine learning, particularly in ways that are distributed to the device or use edge and fog computing. The development and deployment of ML at the very edge remains a technological challenge constrained by computing, memory, energy, network bandwidth, and data privacy and security limitations. This is especially true for battery-operated devices and always-on use cases and applications. In recent years this has gained attention from both academia and industry and many TinyML initiatives have been started focusing both in hardware and software advancements. This workshop will provide a forum for sensing, networking and machine learning researchers to present and share their latest research on building machine learning-enabled sensor systems. Sensys-ML focuses on providing extensive feedback on Work-In-Progress papers involving machine learning (TinyML/ UltraML) on sensor systems. Many papers were submitted from multiple countries, and four papers were selected for publication. This year our articles had themes including techniques for collecting low-resolution images from thermal cameras for human activity recognition, VAE-based approach for privacy preservation of sensor data, deep model compression techniques, and novel GRU based shallow Neural Networks. We see advancement and contributions made by research work will have a significant impact on real-world applications and future research directions.",https://ieeexplore.ieee.org/document/9111711/,2020 IEEE Second Workshop on Machine Learning on Edge in Sensor Systems (SenSys-ML),21-21 April 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/YAC.2018.8406429,Wheel classification using convolutional neural networks,IEEE,Conferences,"With the fast development of automobile wheel industry, many methods of wheel classification have been proposed. The appearance of wheel changes a lot during the process of production, but few of these conventional methods is designed to handle the many challenges of wheel classification for different appearances. This paper studies on wheel classification for different appearances during the process of production, and proposes a wheel classification method using convolutional neural network. Firstly, we implement circle detection, gray value analysis and size normalization on wheel images and collect ten common types of wheel to build a dataset. Based on the features of the wheel images, a convolutional neural network is proposed. Then data augmentation is implemented on the dataset and local response normalization layer is added in the convolutional neural network. The experiment results show that the presented method has a better performance both in classification accuracy and real-time requirement than conventional methods and achieves 95.6% accuracy on wheel classification in the dataset.",https://ieeexplore.ieee.org/document/8406429/,2018 33rd Youth Academic Annual Conference of Chinese Association of Automation (YAC),18-20 May 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFCOM.2009.5062082,"Wi-Sh: A Simple, Robust Credit Based Wi-Fi Community Network",IEEE,Conferences,"Wireless community networks, where users share wireless bandwidth is attracting tremendous interest from academia and industry. Companies such as FON have been successful in attracting large communities of users. However, solutions such as FON either require users to buy specialized FON routers or firmware modifications to existing routers. In this paper we propose a solution which requires no such sophisticated hardware. An alternative is to provide a solution which requires users to download a client software on to their PCs. While the solution appears simple it raises several issues of incentivizing users to share their bandwidth and also issues of preventing users from cheating behaviors which give them an unfair advantage. In this paper, we propose a system and solution which (i) requires only software downloads on PCs, (ii) is robust to tampering of the software, and intermittent monitoring of an access point by the owner, (iii) a credit based mechanism whereby users earn credits for sharing bandwidth and punishment and pricing mechanism whereby users are charged at a higher price whenever they are caught misbehaving. By making simple but plausible assumptions about user behavior, we show via analysis and extensive simulations that the system converges to a Pareto optimal Nash equilibrium. We further validate our system model, by running trace driven simulations on real world data. We believe that the solution provided by Wi-Sh is an attractive and more credible alternative to solutions such as FON.",https://ieeexplore.ieee.org/document/5062082/,IEEE INFOCOM 2009,19-25 April 2009,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTSS.2018.00029,Work-in-Progress: Making Machine Learning Real-Time Predictable,IEEE,Conferences,"Machine learning (ML) on edge computing devices is becoming popular in the industry as a means to make control systems more intelligent and autonomous. The new trend is to utilize embedded edge devices, as they boast higher computational power and larger memories than before, to perform ML tasks that had previously been limited to cloud-hosted deployments. In this work, we assess the real-time predictability and consider data privacy concerns by comparing traditional cloud services with edge-based ones for certain data analytics tasks. We identify the subset of ML problems appropriate for edge devices by investigating if they result in real-time predictable services for a set of widely used ML libraries. We specifically enhance the Caffe library to make it more suitable for real-time predictability. We then deploy ML models with high accuracy scores on an embedded system, exposing it to industry sensor data from the field, to demonstrates its efficacy and suitability for real-time processing.",https://ieeexplore.ieee.org/document/8603205/,2018 IEEE Real-Time Systems Symposium (RTSS),11-14 Dec. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASE.2019.00077,Wuji: Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning,IEEE,Conferences,"Game testing has been long recognized as a notoriously challenging task, which mainly relies on manual playing and scripting based testing in game industry. Even until recently, automated game testing still remains to be largely untouched niche. A key challenge is that game testing often requires to play the game as a sequential decision process. A bug may only be triggered until completing certain difficult intermediate tasks, which requires a certain level of intelligence. The recent success of deep reinforcement learning (DRL) sheds light on advancing automated game testing, without human competitive intelligent support. However, the existing DRLs mostly focus on winning the game rather than game testing. To bridge the gap, in this paper, we first perform an in-depth analysis of 1349 real bugs from four real-world commercial game products. Based on this, we propose four oracles to support automated game testing, and further propose Wuji, an on-the-fly game testing framework, which leverages evolutionary algorithms, DRL and multi-objective optimization to perform automatic game testing. Wuji balances between winning the game and exploring the space of the game. Winning the game allows the agent to make progress in the game, while space exploration increases the possibility of discovering bugs. We conduct a large-scale evaluation on a simple game and two popular commercial games. The results demonstrate the effectiveness of Wuji in exploring space and detecting bugs. Moreover, Wuji found 3 previously unknown bugs, which have been confirmed by the developers, in the commercial games.",https://ieeexplore.ieee.org/document/8952543/,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),11-15 Nov. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/DATE54114.2022.9774534,XANDAR: Exploiting the X-by-Construction Paradigm in Model-based Development of Safety-critical Systems,IEEE,Conferences,"Realizing desired properties &#x201C;by construction&#x201D; is a highly appealing goal in the design of safety-critical embedded systems. As verification and validation tasks in this domain are often both challenging and time-consuming, the by-construction paradigm is a promising solution to increase design productivity and reduce design errors. In the XANDAR project, partners from industry and academia develop a toolchain that will advance current development processes by employing a modelbased X-by-Construction (XbC) approach. XANDAR defines a development process, metamodel extensions, a library of safety and security patterns, and investigates many further techniques for design automation, verification, and validation. The developed toolchain will use a hypervisor-based platform, targeting future centralized, AI-capable high-performance embedded processing systems. It is co-developed and validated in both an avionics use case for situation perception and pilot assistance as well as an automotive use case for autonomous driving.",https://ieeexplore.ieee.org/document/9774534/,"2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)",14-23 March 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FPL53798.2021.00075,XANDAR: X-by-Construction Design framework for Engineering Autonomous & Distributed Real-time Embedded Software Systems,IEEE,Conferences,"The next generation of networked embedded systems (ES) necessitates rapid prototyping and high performance while maintaining key qualities like trustworthiness and safety. However, development of safety-critical ES suffers from complex software (SW) toolchains and engineering processes. Moreover, the current trend in autonomous systems, which relies on Machine Learning (ML) and AI applications when combined with fail-operational requirements renders the Verification and Validation (V&V) of these new systems a challenging endeavor. Prime examples are Advanced Driver-Assistance Systems (ADAS) that are prone to various safety/security vulnerabilities. The XANDAR project aims at developing a mature SW toolchain (from requirements analysis to the actual code integration on target including V&V) fulfilling the needs of industry for rapid prototyping of interoperable and autonomous ES. Starting from a model-based system architecture, XANDAR will leverage automatic model synthesis and software parallelization techniques to achieve specific non-functional requirements setting the foundation for a novel (real-time, safety-, and security)-by-Construction paradigm.",https://ieeexplore.ieee.org/document/9556412/,2021 31st International Conference on Field-Programmable Logic and Applications (FPL),30 Aug.-3 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UKSim.2012.123,[Cover art],IEEE,Conferences,The following topics are dealt with: neural networks; evolutionary computation; adaptive dynamic programming; re-enforcement learning; bio-informatics; bio-engineering; computational finance; economics; semantic mining; data mining; virtual reality; data visualization; intelligent systems; soft computing; hybrid computing; e-science; e-systems; robotics; cybernetics; manufacturing; engineering; operations research; discrete event systems; real time systems; image processing; speech processing; signal processing; industry; business; social issues; human factors; marine simulation; power systems; logistics;parallel systems; distributed systems; software architectures;Internet modelling; semantic Web; ontologies; mobile ad hoc wireless networks; Mobicast; sensor placement; target tracking; circuits; sensors and devices.,https://ieeexplore.ieee.org/document/6205540/,2012 UKSim 14th International Conference on Computer Modelling and Simulation,28-30 March 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRIS.2017.101,[Front cover],IEEE,Conferences,The following topics are dealt with: education; Web technology; hospital; ZigBee; feature extraction; mobile smart tourism; marketing system; digital privacy; cement production; virtual reality; fire protection; rescue training; sports information dissemination model; lightning damage; electromechanical equipment fault signals; generalized regression neural network; cloud security intrusion detection; urban cultural landscape heritage protection; suspended centrifuge drum; VPN isolation gateway; Internet of Things; IBMS; service-oriented architecture; electronic product information; household service robot; power management chip; control system; PLC; pumped storage power station; fuzzy clustering algorithm; high performance CQRS architecture; association rule mining algorithm; electronic manufacturing industry; micro-blog; SVM; network anomaly traffic detection algorithm; parking lot; die mold process; moving target detection; tor anonymous network; IPv6 NetStream; domain ontology; machine tool; logistics; smart factory; SPSS analysis; agricultural monitoring data; computer software development; digital watermarking algorithm; and Big Data.,https://ieeexplore.ieee.org/document/8101323/,2017 International Conference on Robots & Intelligent System (ICRIS),15-16 Oct. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RAIT.2016.7507858,[Title page],IEEE,Conferences,The following topics are dealt with: wireless communication; signal processing; artificial intelligence; knowledge discovery; soft computing; cryptography; network security; image processing; video processing; pattern recognition; machine learning; VLSI design; embedded systems; real time systems; geosciences; mining industry; software engineering; information retrieval; e-govemance; information communication technology; and medical computing.,https://ieeexplore.ieee.org/document/7507858/,2016 3rd International Conference on Recent Advances in Information Technology (RAIT),3-5 March 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DCOSS.2019.00126,Virtual Light Sensors in Industrial Environment Based on Machine Learning Algorithms,IEEE,Conferences,"Internet of Things (IoT) has become the backbone of current and future emerging applications both in the public and the private, industrial sector. The IoT paradigm, enhanced with intelligence and big data analytics, has found applications in a wide range of solutions such as smart home, smart city, industrial IoT etc. Even though IoT implies that cheap motes can conduct a specific task, thus a large number of them can be deployed, we aim to minimize the installed hardware while we still have a high level of quality of service. Machine Learning algorithms can support this challenge by generating virtual data via utilization of real data from a smaller subset of sensors. The generated data can replicate sensor behavior which would otherwise be difficult or impossible to track. It is also possible to use simulation models for data analysis model validation, by generating new data under varying conditions. In this paper, we propose a concept of an IoT testbed which allows virtual IoT resources to be immersed and tested in real life conditions, which are met in everyday life. Additionally, the features of the implemented testbed prototype are discussed while taking into account specific use cases, regarding luminosity scenarios in industrial environments.",https://ieeexplore.ieee.org/document/8804746/,2019 15th International Conference on Distributed Computing in Sensor Systems (DCOSS),29-31 May 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CLEOE-EQEC.2019.8872523,Wavelength Independent Image Classification through a Multimode Fiber using Deep Neural Networks,IEEE,Conferences,"Deep Neural Networks (DNNs) have been increasingly implemented in different research fields or industrial applications. Large amounts of data are processed daily in order to extract useful information using machine learning techniques. Many research groups have shown impressive results on improving resolution in microscopy and quantitative phase retrieval by training DNNs on real datasets [1,2]. Recently, recovery and reconstruction of images after they have propagated through multimode optical fibers (MMFs) have also been achieved using DNNs [3,4]. When images propagate through MMFs they suffer severe scrambling because the information gets distributed among the different spatial modes that the fiber supports. Furthermore, since the fiber modes propagate with different velocities, the local information of the input decorrelates after a few millimeters along the MMF, thus resulting in the formation of a speckle pattern at the output. Recovery of information from such speckle patterns is of practical interest for integrating the MMFs for endoscopic applications in medicine or for signal recovery in telecommunications.",https://ieeexplore.ieee.org/document/8872523/,2019 Conference on Lasers and Electro-Optics Europe & European Quantum Electronics Conference (CLEO/Europe-EQEC),23-27 June 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCA.2000.897405,Winner take all experts network for sensor validation,IEEE,Conferences,"The validation of sensor measurements has become an integral part of the operation and control of modern industrial equipment. The sensor under a harsh environment must be shown to consistently provide the correct measurements. Analysis of the validation hardware or software should trigger an alarm when the sensor signals deviate appreciably from the correct values. Neural network based models can be used to estimate critical sensor values when neighboring sensor measurements are used as inputs. The discrepancy between the measured and predicted sensor values may then be used as an indicator for sensor health. The proposed winner take all experts (WTAE) network is based on a 'divide and conquer' strategy. It employs a growing fuzzy clustering algorithm to divide a complicated problem into a series of simpler sub-problems and assigns an expert to each of them locally. After the sensor approximation, the outputs from the estimator and the real sensor value are compared both in the time domain and the frequency domain. Three fault indicators are used to provide analytical redundancy to detect the sensor failure. In the decision stage, the intersection of three fuzzy sets accomplishes a decision level fusion, which indicates the confidence level of the sensor health. Two data sets, the Spectra Quest Machinery Fault Simulator data set and the Westland vibration data set, were used in simulations to demonstrate the performance of the proposed WTAE network. The simulation results show the proposed WTAE is competitive with or even superior to the existing approaches.",https://ieeexplore.ieee.org/document/897405/,Proceedings of the 2000. IEEE International Conference on Control Applications. Conference Proceedings (Cat. No.00CH37162),27-27 Sept. 2000,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2012.6389251,ZnO crystalline nanowires array for application in gas ionization sensor,IEEE,Conferences,"The air monitoring becomes daily necessity not only in industrial environment and in aerospace applications but also in a living milieu as a consequence of the gas pollution. For detection of gaseous pollutants gas sensors are employed. In this work the successful p-type and n-type ZnO nanowires (NWs) were accomplished during electrochemical deposition. P-type ZnO NWs doped with Ag dopant were achieved with omitted post annealing procedure. Also, the novel gas ionization sensor (GIS) with integrated p-type ZnO NWs as the anode is proposed. P-type ZnO NWs-based gas sensor's characteristics compared with the gold NWs-based GIS, which was developed and reported by our group previously. It showed the comparable breakdown voltages in inert gas (Ar) atmosphere. P-type ZnO NWs-based GIS demonstrated good repeatability. The practical and low cost p-type ZnO NWs-based gas sensor presented in this article shows potential for future implementation in real world gas sensors' applications.",https://ieeexplore.ieee.org/document/6389251/,IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society,25-28 Oct. 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR47638.2019.9043946,libSmart: an Open-Source Tool for Simple Integration of Deep Learning into Intelligent Robotic Systems,IEEE,Conferences,"Intelligent robotic systems can be empowered by advanced deep learning techniques. Robotic operations such as object recognition are well investigated by researchers involved in machine learning. However, these solutions have often led to ad-hoc implementation in experimental settings. Less reported is systematic implementation of deep learning models in industrial robots. The lack of standard implementation platforms has impeded widespread use of deep learning modules in industrial robots. It is of great importance to have development platforms that can coordinate several deep learning modules of a complex system. In this paper, a scalable deep-learning friendly robot task organization system named libSmart is introduced. Similar to ROS, the architecture of the proposed system allows users to plug and play various devices but the proposed architecture is also highly compatible with deep learning modules. Specifically, the deployment of deep learning models is handled using a novel data graph method with distributed computing. In this way, the computationally expensive training and inferencing processes of deep learning models can be handled with isolated accelerating hardware to reduce the overall system latency. Successful implementation of simultaneous object recognition and pose estimation by an industrial robot has been presented as a case study. The proposed system is open source for all users to build their own intelligent systems with customized deep-learning models. (https://github.com/RustIron/libSmart.git).",https://ieeexplore.ieee.org/document/9043946/,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),4-9 Aug. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9340956,robo-gym – An Open Source Toolkit for Distributed Deep Reinforcement Learning on Real and Simulated Robots,IEEE,Conferences,"Applying Deep Reinforcement Learning (DRL) to complex tasks in the field of robotics has proven to be very successful in the recent years. However, most of the publications focus either on applying it to a task in simulation or to a task in a real world setup. Although there are great examples of combining the two worlds with the help of transfer learning, it often requires a lot of additional work and fine-tuning to make the setup work effectively. In order to increase the use of DRL with real robots and reduce the gap between simulation and real world robotics, we propose an open source toolkit: robo-gym1. We demonstrate a unified setup for simulation and real environments which enables a seamless transfer from training in simulation to application on the robot. We showcase the capabilities and the effectiveness of the framework with two real world applications featuring industrial robots: a mobile robot and a robot arm. The distributed capabilities of the framework enable several advantages like using distributed algorithms, separating the workload of simulation and training on different physical machines as well as enabling the future opportunity to train in simulation and real world at the same time. Finally, we offer an overview and comparison of robo-gym with other frequently used state-of-the-art DRL frameworks.",https://ieeexplore.ieee.org/document/9340956/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE45063.2020.9152441,Deployment of a Smart and Predictive Maintenance System in an Industrial Case Study,IEEE,Conferences,"Industrial manufacturing environments are often characterized as being stochastic, dynamic and chaotic, being crucial the implementation of proper maintenance strategies to ensure the production efficiency, since the machines' breakdown leads to a degradation of the system performance, causing the loss of productivity and business opportunities. In this context, the use of emergent ICT technologies, such as Internet of Things (IoT), machine learning and augmented reality, allows to develop smart and predictive maintenance systems, contributing for the reduction of unplanned machines' downtime by predicting possible failures and recovering faster when they occur. This paper describes the deployment of a smart and predictive maintenance system in an industrial case study, that considers IoT and machine learning technologies to support the online and real-time data collection and analysis for the earlier detection of machine failures, allowing the visualization, monitoring and schedule of maintenance interventions to mitigate the occurrence of such failures. The deployed system also integrates machine learning and augmented reality technologies to support the technicians during the execution of maintenance interventions.",https://ieeexplore.ieee.org/document/9152441/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPDPSW.2012.178,Deriving a Methodology for Code Deployment on Multi-Core Platforms via Iterative Manual Optimizations,IEEE,Conferences,"In recent years, there has been what can only be described as an explosion in the types of processing devices one can expect to find within a given computer system. These include the multi-core CPU, the General Purpose Graphics Processing Unit (GPGPU) and the Accelerated Processing Unit (APU), to name but a few. The widespread uptake of these systems presents would-be users with at least two problems. Firstly, each device exposes a complex underlying architecture which must be appreciated in order to attain optimal performance. This is coupled with the fact that a single system can support an arbitrary number of such devices. Consequently, fully leveraging the performance capabilities of such a system must come at a cost -- increasingly prolonged development times. Adhering to a methodology will have the significant industrial impact of reducing these development times. This paper describes the continued formulation of such a novel methodology. Two real world scientific programs are optimized for execution on the CUDA platform. Double precision accuracy and optimized speedups (which include PCI-E transfer times) of 15x and 17x are achieved.",https://ieeexplore.ieee.org/document/6270808/,2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum,21-25 May 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iSAI-NLP48611.2019.9045302,Design and Development of a Low-Cost Indigenous Solar Powered 4-DOF Robotic Manipulator on an Unmanned Ground Vehicle,IEEE,Conferences,"We present in this paper the design, control and implementation of a versatile low cost manipulator with an arm gripper configured on an existing unmanned ground vehicle (UGV) for lifting payload (PL) and performing real world tasks. The major development in this work is the robust and efficient stable customized design of manipulator having four degrees of freedom (DOF) capable of lifting up to 1.5 kg weight for various industrial and non-industrial applications. The communication link is established using two human supervisory controlled wireless four channel 2. 4GHz remote controllers, which are separately used for UGV and manipulator for effective maneuvering and control of a 6-DOF system (UGV and Manipulator) overall. The controlling of RC servo motors is made using Arduino Uno controller board. An on board solar panel is used for charging batteries run time during the day. A50W, 1SV standard solar panel is used to enhance the maneuvering time of UGV and manipulator. The unique feature of the selected UGV is its two rotating head on flippers capable of controlled maneuvering especially in uneven terrain surfaces, stair climbing etc. Trial run experiments have shown the developed system is capable to perform future tasks in human unapproachable situations like contaminated or hazardous areas in several industrial and military applications.",https://ieeexplore.ieee.org/document/9045302/,2019 14th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP),30 Oct.-1 Nov. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO54168.2021.9739376,Design and Experimental Testing of a Compact High-Precision Magnetic Tracking System,IEEE,Conferences,"Magnetic field-based localization and tracking technology has been widely used in medical and industrial fields. This paper presents the hardware and software design of a compact and portable magnetic localization system to achieve highly accurate and stable tracking. Experimental results show that a mean systematic error of 0.24 &#x03BC;&#x03A4; is achieved among the 27 channels of the magnetic sensor array in 300 measurements. The average correlation coefficient between the measured signals and the simulation signals of the 27 channels is 0.999. The repeated localization errors on the position and orientation are 0.81mm and 0.77&#x00B0;, respectively. When the working distance is 68 mm, the system reaches the best localization performance. The root mean square errors of position and orientation are 0.53mm and 0.88&#x00B0;, respectively. The system has the advantages of portability, high precision, and low system noise.",https://ieeexplore.ieee.org/document/9739376/,2021 IEEE International Conference on Robotics and Biomimetics (ROBIO),27-31 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIAI.2019.8850785,Design and Implementation of Intelligent Building Control System Based on Real-time Database,IEEE,Conferences,"In this paper, we propose an intelligent building control system based on BrowserIServer(B/S) architecture that allows intelligent control of construction equipment and integrate multiple subsystems. The system is compatible with three communication protocols and intellectualized control is realized by logic control based on Event-Condition-Action(ECA) rules. The system adopts Agilor distributed real-time database system to ensure high real-time performance. The design of distributed system ensures the stability of system by ensuring that the service failure in a single area of system does not affect normal operation of other areas. The system provides six kinds of interfaces, which makes system have strong integration. The research of system architecture provides a general model in the field of intelligent building for enabling communication and computing infrastructure for industrial Artificial Intelligence(AI).",https://ieeexplore.ieee.org/document/8850785/,2019 1st International Conference on Industrial Artificial Intelligence (IAI),23-27 July 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INCOS45849.2019.8951312,Design and Implementation of Non-Intrusive Load Monitoring using Machine Learning Algorithm for Appliance Monitoring,IEEE,Conferences,"Energy Conservation and management is gaining popularity in research area due to the increase in energy demand. It also plays a vital role in industrial/commercial/domestic/power sectors for reducing the carbon emission, the energy bill. Thus, increase in conservation would change the economy of the world energy crisis. To conserve energy, it is very important to have a better monitoring and identification system. The existing monitoring technique has conventional energy meter or smart energy meter that gives total energy consumption only. To enhance a better quality of services in existing monitoring technique, there is a need to monitor energy consumption of individual appliances and hence one meter/sensor for each appliance are necessary. Due to more sensors and its associated installation cost, this technique is not a cost effective in nature. To overcome Non-intrusive Load monitoring technique was introduced to disaggregate the total energy consumption from a single meter using Machine learning disaggregation algorithm. Thus, to identify the appliances malfunctioning Non-intrusive load monitoring (NILM) technique can be used as a Real time Monitoring technique. In this paper, it is proposed to use single energy meter for the set of appliances to monitor the status of the individual appliances. Non-intrusive Load Monitoring technique using machine learning algorithms has been discussed for appliances identification and monitoring for energy conservation. The MATLAB/Simulink Software has been used for designing and mathematical modeling of each appliance. The NILM technique mainly involves the three stages via; Data acquisition, feature extraction and training of data under different classification algorithm for appliance identification. Data acquisition used for acquiring the voltage and current from a single phase system. Using the features extracted like active, reactive power the different load patterns of individual appliances can be studied. Training of data under DT and K-NN which are supervised learning techniques are used as disaggregation algorithm. Moreover, the algorithms are compared using the Confusion matrix and ROC curve for the prediction of accuracy. The result shows that the K-NN algorithm is having a better accuracy of performance compared with DT algorithm.",https://ieeexplore.ieee.org/document/8951312/,"2019 IEEE International Conference on Intelligent Techniques in Control, Optimization and Signal Processing (INCOS)",11-13 April 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICETET.2009.71,Design and Implementation of Real Time Neurofuzzy Based pH Controller,IEEE,Conferences,"The quality of an intelligent control system is the ability to control the process with a certain degree of autonomy. These requirements, as an autonomous process controller are ever increasing. Considering the fact that existing algorithm based controllers such as the adaptive PID have their own limitations/are inadequate, the controllers are designed to emulate human mental faculties such as adaptation, learning, and planning under uncertainties and also coping up with large amounts of data by reducing the complexity of their representation. The family of intelligent controllers includes those based on neural nets, fuzzy logic, classic artificial intelligence and the genetic algorithms. This paper describes design and development of real time neurofuzzy based pH controller, which can be used in water treatment processes, laboratory studies and other industrial applications.",https://ieeexplore.ieee.org/document/5395042/,2009 Second International Conference on Emerging Trends in Engineering & Technology,16-18 Dec. 2009,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSPIS.2018.8700563,Design and Implementation of a Parcel Sorter Using Deep Learning,IEEE,Conferences,"Automation in industrial environment reduces the cost of the operation while increasing the overall performance. Having an automation mechanism in the e-commerce warehouses to sort the parcels based on their destinations or shipping method will reduce the parcel processing time significantly. To automate parcel processing in Digikala's warehouse, a parcel sorter system is designed and implemented. In this system shipment method of the parcel is indicated by a set of markers. A computer vision system is developed to identify these markers using deep learning algorithms. The parcels are identified while they are moving on the conveyor belt in a relatively high speed (1 m/s). The computer vision system is capable of processing 1.3MP pictures in real-time with a rate of 100FPS. To sort the parcels an omni wheel roller mechanism is designed and utilized. To achieve the best results in a practical environment, a gap optimization mechanism and pack positioning conveyor are implemented and placed before the sorter. This system is successfully installed in the Digikala's warehouse.",https://ieeexplore.ieee.org/document/8700563/,2018 4th Iranian Conference on Signal Processing and Intelligent Systems (ICSPIS),25-27 Dec. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF53024.2021.9733780,Design and Implementation of an Educational Technology Kit Aligned to the Conceptual Framework of Educational Mechatronics,IEEE,Conferences,"Educational systems continuously seek new educational technologies that improve their students&#x0027; knowledge and help them acquire the necessary skills for the new industrial era. Studies indicate that technology plays a significant role in student performance when implemented correctly, stimulates better interactions between instructors and the students, and encourages cooperative learning, collaboration, problem-solving, and communication competencies. A necessary skill for a good learning process is spatial thinking. Studies show that students who do not have good spatial thinking will have more trouble passing science, technology, engineering, and mathematics (STEM) courses. This paper presents the design and implementation of a novel educational kit to aid in the development of spatial thinking, specifically applied to the two-dimensional Cartesian Coordinate Systems (2D-CACSET) designed, from its conception, to apply the Educational Mechatronics Conceptual Framework (EMCF). This innovative 2D-CACSET comprises two Tags, four Anchors, and a Listener that use a real-time location system based on Ultra-Wide Band (UWB) technology as a cornerstone of operation. In addition, it has a 2D dashboard and a Graphical User Interface that allow mechatronic concepts from the concrete, graphic and abstract levels of thinking. Thus, in this practice, students acquired knowledge and developed skills to apply when working with more complex prototypes throughout their lives.",https://ieeexplore.ieee.org/document/9733780/,2021 Machine Learning-Driven Digital Technologies for Educational Innovation Workshop,15-17 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIEA53260.2021.00021,Design and Implementation of the Prototype for Hybrid Production of Multi-Type Products,IEEE,Conferences,"A smart manufacturing prototype called iCandy Box, used for hybrid packing of assorted candies, was designed to study and verify the cyber-physical control methods. The prototype is aimed to provide personalized consumption, as it can perform flexible and customized production. The prototype is powered by a cloud-edge-end enabled collaborative information framework, which can support both industrial big data and artificial intelligence applications. Furthermore, it is characterized by modularization and interdisciplinarity; therefore, it can be used to carry out both experiments and training in several major fields, including smart manufacturing and IoT. The experimental results have shown that the prototype can carry out hybrid production, paving the way for the study and verification of cyber-physical control methods.",https://ieeexplore.ieee.org/document/9525542/,2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA),14-16 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CDC.2001.980681,Design and implementation of industrial neural network controller using backstepping,IEEE,Conferences,"A novel neural network (NN) backstepping controller is modified for application to an industrial motor drive system. A control system structure and NN tuning algorithms are presented that are shown to guarantee the stability and performance of the closed-loop system. The NN backstepping controller is implemented on an actual motor drive system using a two-PC control system developed at the authors' university. The implementation results show that the NN backstepping controller is highly effective in controlling the industrial motor drive system. It is also shown that the NN controller gives better results on actual systems than a standard backstepping controller developed assuming full knowledge of the dynamics. Moreover, the NN controller does not require the linear-in-the-parameters assumption or the computation of regression matrices required by standard backstepping.",https://ieeexplore.ieee.org/document/980681/,Proceedings of the 40th IEEE Conference on Decision and Control (Cat. No.01CH37228),4-7 Dec. 2001,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCNEA.2017.25,Design of Multi-channel Temperature Control Inspection System Based on PLC,IEEE,Conferences,"The temperature control system is widely used in the field of industrial control, such as the boiler's temperature control system in Steel, chemical plants and thermal power plants. For the requirements of remote centralized management and security monitor in temperature control system, a temperature control inspection system consisted by down-computer clew and up-computer, is designed in this paper. In this system, a programmable logic controller (PLC) is use as up-computer, multiple AI smart meters are use as down-computer clew. The structure of the system hardware and the interconnection of the various parts are introduced simply, the design and implementation of communication system of down-computer is elaborated in detail, and the part of the communication system program is given. The actual operation shows that the remote monitoring function can be realized and design requirements be satisfied by the application of intelligent instruments of real-time collection, processing and feedback on the site temperature, and high efficiency, high universality and reliable stability are the advantages of the system.",https://ieeexplore.ieee.org/document/8128601/,"2017 International Conference on Computer Network, Electronic and Automation (ICCNEA)",23-25 Sept. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IAEAC50856.2021.9390638,Design of Wireless Temperature Acquisition System,IEEE,Conferences,"With the development of production technology, temperature is particularly important in modern industrial control. Measuring temperature parameters and analyzing them can play a role in real-time monitoring of equipment operation status, production environment and other external environment, so as to ensure equipment safety and efficient production of the whole production line. As an indispensable element of automated production lines, sensor is developed while also facing challenges. For demand of industrial automation persecution, sensors continuously improve measurement capability. This paper presents an idea, that designing correction algorithm in software to improve the accuracy of the temperature sensor, which is reduce the updated hardware investment.",https://ieeexplore.ieee.org/document/9390638/,"2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)",12-14 March 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSNT.2017.8418538,Detection of attacks in IoT based on ontology using SPARQL,IEEE,Conferences,"Nowadays, the concept of Internet of Things (IoT) has been manifested into reality with the help of latest developments or transformations in hardware circuitry, devices and protocols. IoT is such a diversified field in which a lot of challenges are faced during implementations of IoT applications including smart cities, smart homes, industrial sectors etc. The current scenario is highly demanding for deployment of smart sensors into existing applications to deliver a fully automated system. The major issue faced by IoT's existing system is security issue. In this paper, various attacks in IoT systems has discussed and focuses on ontology based model to deal with various attacks.",https://ieeexplore.ieee.org/document/8418538/,2017 7th International Conference on Communication Systems and Network Technologies (CSNT),11-13 Nov. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAMechS49982.2020.9310079,Developing Robotic System for Harvesting Pineapples,IEEE,Conferences,"This paper develops a robotic system to harvest pineapple autonomously. The system contains a machine vision unit, two robotic manipulators mounted on a platform, custom end-effectors, and an image-based harvesting control unit. The manipulators with Gantry 3DOF PPP configuration are geometrically optimized to move the end-effectors approaching pineapples. Each end-effector is actuated by pneumatic actuator and equipped with a cage-shaped gripper to fix the selective pineapple inside and a cutting device to cut its stalk. YOLOv3 approach is implemented for detecting and recognizing pineapple fruits that meet requirements for harvest. The experiment results demonstrate the success of pineapple recognition with 90.82% mAP. The 3D position of the recognized pineapples will be calculated and sent to the control system. The control system, including an industrial computer communicating with PLCs to conducts the manipulators and end-effectors to approach and d the recognized pineapples. The complete system has been tested on the experimental field-model. The success rate of pineapple harvesting is 95.55% and the average time is 12 seconds per one fruit. In the future, this system will be improved for automatic harvesting in real pineapple fields.",https://ieeexplore.ieee.org/document/9310079/,2020 International Conference on Advanced Mechatronic Systems (ICAMechS),10-13 Dec. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBAIE52039.2021.9389863,Development and application of low-latency edge IoT agent device for ubiquitous power Internet of Things,IEEE,Conferences,"With the development of smart grid, IOT devices in the field of power control are widely used. The diversity of types, features and functions of IOT devices not only integrates detailed big data, but also challenges the real-time, compatibility and big data processing capability of system control. In this paper, the low-latency edge IoT agent device for ubiquitous power Internet of Things is studied, which performs nearby processing of massive intelligent electricity data to avoid the bottleneck of communication channel caused by massive data transmission, and meet the special requirements of energy management and control for quick response and accurate execution. The software and hardware decoupling and deep programmable ideas are proposed. In addition, adopting a common hardware platform and resource virtualization function platform to realize low latency guarantee mechanism and mode from three levels. Moreover, the low latency energy industrial application scenarios are selected to demonstrate the application, and the proposed scheme is proved to provide safe and fast control methods, and also make the system have stronger big data collection capability of electricity consumption information.",https://ieeexplore.ieee.org/document/9389863/,"2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",26-28 March 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FarEastCon.2018.8602651,Development of a Transport Robot for Automated Warehouses,IEEE,Conferences,"Industrial robots and manipulators are widely used as transport-loading devices in automated production. It is possible to combine equipment into coordinated production complexes of various sizes with the help of robots and they will not be bound by rigid planning and the number of installed units. Transport robots have proven themselves as flexible automated means of realizing intra-shop and interoperation material connections. More and more companies are developing technologies for vehicles through which they can communicate with each other and use real-time data from production infrastructure facilities. Electric vehicles and unmanned vehicles have become a new technological trend. In this regard, the paper deals with a prototype of an innovative transport robot, created for automated warehouses. It is proposed to use a computer vision system with image recognition based on the embedded software for the transport robot positioning inside the production facilities. The algorithm of deep machine learning was adapted to solve this problem. Using this algorithm, the prototype tests were performed successfully.",https://ieeexplore.ieee.org/document/8602651/,2018 International Multi-Conference on Industrial Engineering and Modern Technologies (FarEastCon),3-4 Oct. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IAS.2004.1348846,Development of a self-tuned neuro-fuzzy controller for induction motor drives,IEEE,Conferences,"In This work a novel adaptive neuro-fuzzy (NF) based speed control of an induction motor (IM) is presented. The proposed neuro-fuzzy controller (NFC) incorporates fuzzy logic laws with a five-layer artificial neural network (ANN) scheme. In this controller only three membership functions are used for each input keeping in mind for low computational burden, which will be suitable for real-time implementation. Furthermore, for the proposed NFC an improved self-tuning method is developed based on the IM theory and its high performance requirements. The main task of the tuning method is to adjust the parameters of the fuzzy logic controller (FLC) in order to minimize the square of the error between actual and reference outputs. This work also demonstrates how the proposed NFC can easily be adjusted to work with different size of induction motors. A complete simulation model for indirect field oriented control of IM incorporating the proposed NFC is developed. The performance of the proposed NFC based IM drive is investigated extensively at different operating conditions in simulation. In order to prove the superiority of the proposed NFC, the results for the proposed controller are also compared to those obtained by a conventional PI controller. The proposed NFC based IM drive is found to be more robust as compared to conventional PI controller based drive and hence found suitable for high performance industrial drive applications.",https://ieeexplore.ieee.org/document/1348846/,"Conference Record of the 2004 IEEE Industry Applications Conference, 2004. 39th IAS Annual Meeting.",3-7 Oct. 2004,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2018.8761037,Developmental Bayesian Optimization of Black-Box with Visual Similarity-Based Transfer Learning,IEEE,Conferences,"We present a developmental framework based on a long-term memory and reasoning mechanisms (Vision Similarity and Bayesian Optimisation). This architecture allows a robot to optimize autonomously hyper-parameters that need to be tuned from any action and/or vision module, treated as a black-box. The learning can take advantage of past experiences (stored in the episodic and procedural memories) in order to warm-start the exploration using a set of hyper-parameters previously optimized from objects similar to the new unknown one (stored in a semantic memory). As example, the system has been used to optimized 9 continuous hyper-parameters of a professional software (Kamido) both in simulation and with a real robot (industrial robotic arm Fanuc) with a total of 13 different objects. The robot is able to find a good object-specific optimization in 68 (simulation) or 40 (real) trials. In simulation, we demonstrate the benefit of the transfer learning based on visual similarity, as opposed to an amnesic learning (i.e. learning from scratch all the time). Moreover, with the real robot, we show that the method consistently outperforms the manual optimization from an expert with less than 2 hours of training time to achieve more than 88% of success.",https://ieeexplore.ieee.org/document/8761037/,2018 Joint IEEE 8th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob),17-20 Sept. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SPW.2019.00040,Devil in the Detail: Attack Scenarios in Industrial Applications,IEEE,Conferences,"In the past years, industrial networks have become increasingly interconnected and opened to private or public networks. This leads to an increase in efficiency and manageability, but also increases the attack surface. Industrial networks often consist of legacy systems that have not been designed with security in mind. In the last decade, an increase in attacks on cyber-physical systems was observed, with drastic consequences on the physical work. In this work, attack vectors on industrial networks are categorised. A real-world process is simulated, attacks are then introduced. Finally, two machine learning-based methods for time series anomaly detection are employed to detect the attacks. Matrix Profiles are employed more successfully than a predictor Long Short-Term Memory network, a class of neural networks.",https://ieeexplore.ieee.org/document/8844618/,2019 IEEE Security and Privacy Workshops (SPW),19-23 May 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BIGCOMP.2019.8679267,Diagnosis of Corporate Insolvency Using Massive News Articles for Credit Management,IEEE,Conferences,"In the aftermath of the 4th Industrial Revolution, AI and Big data technology have been used in various fields in South Korea, and the techniques are being applied to and complemented in various service fields which were implemented without them before. Especially, in order to secure credit stability for borrowed companies from financial institutions and to preemptively respond to the risks about-by means of online news articles and SNS data-the attempts to forecast the possibility of insolvency and adopt them into actual business are actively conducted by major domestic banks. In this study, we describe several analytical methods, outputs, and problems that are encountered during the processes of developing the unstructured text-based prediction system to detect the possibility of corporate insolvency-which ordered by a national government bank and discuss related issues with a real case. As a result, we have implemented an automatic tagger program for labeling largely unlabeled articles, and newly devised a prediction algorithm of the possibility of corporate insolvency. We achieved the accuracy of 92% (AUC 0.96) in aspect of performance and the hit ratio of 50% among the number of predicted 26 candidates that have the possibility of insolvency. Thus, the result of our study is revealed to be complementary to the financial data analysis sufficiently in performance, but yet have several limitations such as data coverage, reliability, and the characteristics of Korean language.",https://ieeexplore.ieee.org/document/8679267/,2019 IEEE International Conference on Big Data and Smart Computing (BigComp),27 Feb.-2 March 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UPCON47278.2019.8980279,Diagnosis of Induction Motor Faults Using Frequency Occurrence Image Plots—A Deep Learning Approach,IEEE,Conferences,"Accurate diagnosis of induction motor faults is important for reliable and safe operation of industrial processes. Majority of the faults which occur in induction motors are mainly diagnosed using motor current signature analysis. However, the accuracy of fault detection depends on selection of suitable features from motor current, the failure of which may result in incorrect interpretation. Considering the aforesaid fact, this paper presents an image processing aided deep learning framework for reliable diagnosis of induction motor faults, which eliminates the need of separate feature extraction stage. To this end, the motor current signals under different types of fault conditions were procured and were subsequently processed into frequency occurrence plots. The frequency occurrence image plots for different fault scenarios were finally used as inputs to a deep convolution neural network for the purpose of classification. Transfer learning technique was adopted to reduce the computation time of Convolution Neural Network and classification of motor faults was done at five different loading conditions. Four types of classification tasks have been addressed here and comprehensive analysis was done using a variety of CNN architectures. It has been observed that the proposed method returns a highest mean classification accuracy of 96.67% in segregating different types of faults which can be implemented in real-life for condition monitoring of induction motors.",https://ieeexplore.ieee.org/document/8980279/,"2019 International Conference on Electrical, Electronics and Computer Engineering (UPCON)",8-10 Nov. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PerComWorkshops53856.2022.9767492,Digital Twin-based Intrusion Detection for Industrial Control Systems,IEEE,Conferences,"Digital twins have recently gained significant interest in simulation, optimization, and predictive maintenance of Industrial Control Systems (ICS). Recent studies discuss the possibility of using digital twins for intrusion detection in industrial systems. Accordingly, this study contributes to a digital twin-based security framework for industrial control systems, extending its capabilities for simulation of attacks and defense mechanisms. Four types of process-aware attack scenarios are implemented on a standalone open-source digital twin of an industrial filling plant: command injection, network Denial of Service (DoS), calculated measurement modification, and naive measurement modification. A stacked ensemble classifier is proposed as the real-time intrusion detection, based on the offline evaluation of eight supervised machine learning algorithms. The designed stacked model outperforms previous methods in terms of F1Score and accuracy, by combining the predictions of various algorithms, while it can detect and classify intrusions in near real-time (0.1 seconds). This study also discusses the practicality and benefits of the proposed digital twin-based security framework.",https://ieeexplore.ieee.org/document/9767492/,2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),21-25 March 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICVES.2009.5400189,Digital implementation of fuzzy logic controller for wide range speed control of brushless DC motor,IEEE,Conferences,"The brushless DC motors find wide applications such as in battery operated vehicles, wheel chairs, automotive fuel pumps, robotics, machine tools, aerospace and in many industrial applications due to their superior electrical and mechanical characteristics and its capability to operate in hazardous environment. Conventional controllers fail to yield desired performance in BLDC motor control systems due to the non-linearity arising out of variation in the system parameters and change in load. The main focus is now on the application of artificial intelligent techniques such as fuzzy logic to solve this problem. Another great challenge is to reduce the size and cost of the drive system without compromising the performance. In this paper, the design and digital implementation of fuzzy logic controller using a versatile ADUC812 microcontroller, and low-cost, compact, superior performance components are used in order to reduce the cost and size of the drive system. The experimental results are presented to prove the flexibility of the control scheme in real time.",https://ieeexplore.ieee.org/document/5400189/,2009 IEEE International Conference on Vehicular Electronics and Safety (ICVES),11-12 Nov. 2009,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIKE.2018.00042,Distributed Osmotic Computing Approach to Implementation of Explainable Predictive Deep Learning at Industrial IoT Network Edges with Real-Time Adaptive Wavelet Graphs,IEEE,Conferences,"Challenges associated with developing analytics solutions at the edge of large scale Industrial Internet of Things (IIoT) networks close to where data is being generated in most cases involves developing analytics solutions from ground up. However, this approach increases IoT development costs and system complexities, delay time to market, and ultimately lowers competitive advantages associated with delivering next-generation IoT designs. To overcome these challenges, existing, widely available, hardware can be utilized to successfully participate in distributed edge computing for IIoT systems. In this paper, an osmotic computing approach is used to illustrate how distributed osmotic computing and existing low-cost hardware may be utilized to solve complex, compute-intensive Explainable Artificial Intelligence (XAI) deep learning problem from the edge, through the fog, to the network cloud layer of IIoT systems. At the edge layer, the C28x digital signal processor (DSP), an existing low-cost, embedded, real-time DSP that has very wide deployment and integration in several IoT industries is used as a case study for constructing real-time graph-based Coiflet wavelets that could be used for several analytic applications including deep learning pre-processing applications at the edge and fog layers of IIoT networks. Our implementation is the first known application of the fixed-point C28x DSP to construct Coiflet wavelets. Coiflet Wavelets are constructed in the form of an osmotic microservice, using embedded low-level machine language to program the C28x at the network edge. With the graph-based approach, it is shown that an entire Coiflet wavelet distribution could be generated from only one wavelet stored in the C28x based edge device, and this could lead to significant savings in memory at the edge of IoT networks. Pearson correlation coefficient is used to select an edge generated Coiflet wavelet and the selected wavelet is used at the fog layer for pre-processing and denoising IIoT data to improve data quality for fog layer based deep learning application. Parameters for implementing deep learning at the fog layer using LSTM networks have been determined in the cloud. For XAI, communication network noise is shown to have significant impact on results of predictive deep learning at IIoT network fog layer.",https://ieeexplore.ieee.org/document/8527474/,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),26-28 Sept. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARCV.2014.7064599,Distributed signature analysis of induction motors using Artificial Neural Networks,IEEE,Conferences,Motor current signature analysis is a modern approach to fault diagnose and classification for induction motors. Many studies reported successful implementation of MCSA in laboratory situations whereas the method was not so successful in real industrial situation due to propagation of neighbor faults and unwanted noise signals. This paper investigate the correlation between different observations of events in order to provide a more accurate estimation of behavior of electrical motors at a given site. An analytical framework has been implemented to correlate and classify independent fault observations and diagnose the type and identify the origin of fault symptoms. The fault diagnosis algorithm has two layers. Initially outputs of all sensors are processed to generate fault indicators. These fault indicators then are to be classified using an Artificial Neural Network. A typical industrial site is taken as a case study and simulated to evaluate the concept of distributed fault analysis.,https://ieeexplore.ieee.org/document/7064599/,2014 13th International Conference on Control Automation Robotics & Vision (ICARCV),10-12 Dec. 2014,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE-SEIP.2017.8,Domain Adaptation for Test Report Classification in Crowdsourced Testing,IEEE,Conferences,"In crowdsourced testing, it is beneficial to automatically classify the test reports that actually reveal a fault - a true fault, from the large number of test reports submitted by crowd workers. Most of the existing approaches toward this task simply leverage historical data to train a machine learning classifier and classify the new incoming reports. However, our observation on real industrial data reveals that projects under crowdsourced testing come from various domains, and the submitted reports usually contain different technical terms to describe the software behavior for each domain. The different data distribution across domains could significantly degrade the performance of classification models when utilized for cross-domain report classification. To build an effective cross-domain classification model, we leverage deep learning to discover the intermediate representation that is shared across domains, through the co-occurrence between domain-specific terms and domain-unaware terms. Specifically, we use the Stacked Denoising Autoencoders to automatically learn the high-level features from raw textual terms, and utilize these features for classification. Our evaluation on 58 commercial projects of 10 domains from one of the Chinese largest crowdsourced testing platforms shows that our approach can generate promising results, compared to three commonly-used and state-of-the-art baselines. Moreover, we also evaluate its usefulness using real-world case studies. The feedback from real-world testers demonstrates its practical value.",https://ieeexplore.ieee.org/document/7965432/,2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track (ICSE-SEIP),20-28 May 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PHM-Paris.2019.00054,Domain Adaptive Transfer Learning for Fault Diagnosis,IEEE,Conferences,"Thanks to digitization of industrial assets in fleets, the ambitious goal of transferring fault diagnosis models from one machine to the other has raised great interest. Solving these domain adaptive transfer learning tasks has the potential to save large efforts on manually labeling data and modifying models for new machines in the same fleet. Although data-driven methods have shown great potential in fault diagnosis applications, their ability to generalize on new machines and new working conditions are limited because of their tendency to overfit to the training set in reality. One promising solution to this problem is to use domain adaptation techniques. It aims to improve model performance on the target new machine. Inspired by its successful implementation in computer vision, we introduced Domain-Adversarial Neural Networks (DANN) to our context, along with two other popular methods existing in previous fault diagnosis research. We then carefully justify the applicability of these methods in realistic fault diagnosis settings, and offer a unified experimental protocol for a fair comparison between domain adaptation methods for fault diagnosis problems.",https://ieeexplore.ieee.org/document/8756463/,2019 Prognostics and System Health Management Conference (PHM-Paris),2-5 May 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIAFS.2007.4544783,Dynamic power management of an embedded sensor network based on actor-critic reinforcement based learning,IEEE,Conferences,"Wireless sensor networks (WSNs) have gained tremendous popularity in recent years due to the wide range of applications envisioned - ranging from aerospace and defense to industrial and commercial. Although limited by communication and energy constraints, the low cost, small sensor nodes lend themselves to be deployed in large numbers to form a network with high spatial distribution. The overall effectiveness of the sensor network depends on how well the mutually contradicting objectives of conserving the limited on-board battery power and keeping the sensors awake for stimuli, are managed. In this paper, we have proposed an actor-critic based reinforcement learning mechanism that can be practically implemented on an embedded sensor with limited memory and processing power. Specifically, the contribution of this paper is the development of the value function (or critic/reinforcement function) that is implemented on each sensor node which aids in dynamic power scheduling based on different situations. The effectiveness of the proposed method has been demonstrated with real world experiments.",https://ieeexplore.ieee.org/document/4544783/,2007 Third International Conference on Information and Automation for Sustainability,4-6 Dec. 2007,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAI52893.2021.9639679,E-Learning - the practice in industrial enterprises,IEEE,Conferences,"The dynamic development of technology in today's digital world entails changes in the business models, thus predetermining a radical turn in human resource management (HRM). We witness the constant readjustment of the functions of HRM professionals due to the creation and implementation of new technologies. Among the most currently used technologies are: E-Learning, social networks, mobile technologies, learning management systems, virtual reality, chat bots, gamification and artificial intelligence. The authors explore in this report the E-learning practices in the industrial enterprises. Based on their survey, they draft conclusions and provide methodological guidelines for overcoming the weaknesses in the process of online training of the personnel in the industrial enterprises.",https://ieeexplore.ieee.org/document/9639679/,2021 International Conference Automatics and Informatics (ICAI),30 Sept.-2 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCT46805.2019.8947193,EDGE AI for Heterogeneous and Massive IoT Networks,IEEE,Conferences,"By combining multiple sensing and wireless access technologies, the Internet of Things (IoT) shall exhibit features with large-scale, massive, and heterogeneous sensors and data. To integrate diverse radio access technologies, we present the architecture of heterogeneous IoT system for smart industrial parks and build an IoT experimental platform. Various sensors are installed on the IoT devices deployed on the experimental platform. To efficiently process the raw sensor data and realize edge artificial intelligence (AI), we describe four statistical features of the raw sensor data that can be effectively extracted and processed at the network edge in real time. The statistical features are calculated and fed into a back-propagation neural network (BPNN) for sensor data classification. By comparing to the k-nearest neighbor classification algorithm, we examine the BPNN-based classification method with a great amount of raw data gathered from various sensors. We evaluate the system performance according to the classification accuracy of BPNN and the performance indicators of the cloud server, which shows that the proposed approach can effectively enable the edge-AI-based heterogeneous IoT system to process the sensor data at the network edge in real time while reducing the demand for computing and network resources of the cloud.",https://ieeexplore.ieee.org/document/8947193/,2019 IEEE 19th International Conference on Communication Technology (ICCT),16-19 Oct. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SECON52354.2021.9491609,EFCam: Configuration-Adaptive Fog-Assisted Wireless Cameras with Reinforcement Learning,IEEE,Conferences,"Visual sensing has been increasingly employed in industrial processes. This paper presents the design and implementation of an industrial wireless camera system, namely, EFCam, which uses low-power wireless communications and edge-fog computing to achieve cordless and energy-efficient visual sensing. The camera performs image pre-processing (i.e., compression or feature extraction) and transmits the data to a resourceful fog node for advanced processing using deep models. EFCam admits dynamic configurations of several parameters that form a configuration space. It aims to adapt the configuration to maintain desired visual sensing performance of the deep model at the fog node with minimum energy consumption of the camera in image capture, pre-processing, and data communications, under dynamic variations of application requirement and wireless channel conditions. However, the adaptation is challenging due primarily to the complex relationships among the involved factors. To address the complexity, we apply deep reinforcement learning to learn the optimal adaptation policy. Extensive evaluation based on trace-driven simulations and experiments show that EFCam complies with the accuracy and latency requirements with lower energy consumption for a real industrial product object tracking application, compared with four baseline approaches incorporating hysteresis-based adaptation.",https://ieeexplore.ieee.org/document/9491609/,"2021 18th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)",6-9 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/QUATIC.2016.043,Early Diagnostics on Team Communication: Experience-Based Forecasts on Student Software Projects,IEEE,Conferences,"Effective team communication is a prerequisite for software quality and project success. It implies correctly elicited customer requirements, conduction of occurring change requests and to adhere releases. Team communication is a complex construct that consists of numerous characteristics, individual styles, influencing factors and dynamic intensities during a project. These elements are complicated to be measured or scheduled, especially in newly formed teams. According to software developers with few experiences in teams, it would be highly desirable to recognize dysfunctional or underestimated communication behaviors already in early project phases. Otherwise, negative affects may cause delay of releases or even endanger software quality. We introduce an approach on the feasibility of forecasting team's communication behavior in student software projects. We build a very first forecasting model that involves software engineering and industrial psychological terms to extract multi week communication forecasts with accurate results. The model consists of a k-nearest neighbor machine learning algorithm and is trained and evaluated with 34 student software projects from a previously taken field study. This study is an encouraging first step towards forecasting team communication to reveal potential miscommunications during a project. It is our aim to give young software developing teams an experience-based assistance about their information flow and enable adjustment for dysfunctional communication, to avoid fire fighting situation or even risks of alternating software qualities.",https://ieeexplore.ieee.org/document/7814540/,2016 10th International Conference on the Quality of Information and Communications Technology (QUATIC),6-9 Sept. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2008.97,Early-Life Cycle Reuse Approach for Component-Based Software of Autonomous Mobile Robot System,IEEE,Conferences,"Applying software reuse to many embedded realtime systems, such as autonomous mobile robot system poses significant challenges to industrial software processes due to the resource-constrained and realtime requirements of the systems. An approach for early life-cycle systematic reuse for component-based software engineering (ELCRA) of autonomous mobile robot software is developed. The approach allows reuse at the early stage of software development process by integrating analysis patterns, component model, and component-oriented programming framework. The results of applying the approach in developing software for real robots show that the strategies and processes proposed in the approach can fulfill requirements for self-contained, platform-independent and real-time predictable mobile robot.",https://ieeexplore.ieee.org/document/4617381/,"2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing",6-8 Aug. 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AEMCSE50948.2020.00057,Effective Risk Prediction of Tailings Ponds Using Machine Learning,IEEE,Conferences,"A tailings pond is a place for storing industrial waste, which is a major hazard source with high potential energy. The stability of the tailings dam is usually evaluated by saturation line height. The measuring sensors for saturation line are very expensive and have poor lightning protection abilities. To solve this problem, this paper takes Jiande tailings pond as the study area. Machine learning models are built after integrating various sensor monitoring data, relying on two sensors to accurately predict the saturation line height to predict the safety of tailings pond. The average R2 of real-time regression of saturation line is 99.16%, and average accuracy of real-time warning is 99.89%.",https://ieeexplore.ieee.org/document/9131337/,"2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",24-26 April 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICT50816.2021.9358469,Efficient Fault Isolation Method to Monitor Industrial Batch Processes,IEEE,Conferences,Industrial batch processes are very popular manufacturing system with large number of process variables involved. Monitoring of batch processes using statistical process monitoring becomes very difficult in view of the complex correlations between the process variables. This paper focuses on a fault isolation based process monitoring method without prior information of fault where fault isolation problem is converted into a variable selection. Variable selection is a learning algorithm used here to solve the problem of selection and isolation of variables from a model. The method discussed here uses a sparse coefficient based dissimilarity analysis algorithm known as Sparse Dissimilarity Algorithm(SDISSIM) which checks a calculated D-index for identifying fault in the process. A sparse coefficient is tabulated to verify the process variables contributing to the fault and an absolute variance difference is calculated to select the variables for fault isolation. Finally SDISSIM method is explained by successful implementation in MATLAB with real time industrial process data.,https://ieeexplore.ieee.org/document/9358469/,2021 6th International Conference on Inventive Computation Technologies (ICICT),20-22 Jan. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIMSA.2003.1227220,Embedded e-diagnostic for distributed industrial machinery,IEEE,Conferences,"Industrial process machine failure often causes severe financial implications. This is compounded by the lack of availability of experts and the complications of getting them to site. One solution is to give the expert access to the machine remotely with the addition of an Artificial Intelligence (AI) based diagnostics software to assist with the decision making process. Our research is based on such a system, which combines modern communications with intelligent diagnostics software. Accessibility to process machines can now be global with the promise of predictability to the diagnosis. It is felt the importance of this research work cannot be overstated with the constantly moving worldwide manufacturing base and the real situation of the machine designers being based in a different country to their customer. The most vulnerable areas of a machine are its parts that consist of electro-mechanical actuation. The author utilises conventional Newtonian physics and differential calculus to model these and an AI technique of fault prediction and detection.",https://ieeexplore.ieee.org/document/1227220/,"The 3rd International Workshop on Scientific Use of Submarine Cables and Related Technologies, 2003.",31-31 July 2003,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MARK.2011.6046555,Enabling hazard identification from requirements and reuse-oriented HAZOP analysis,IEEE,Conferences,"The capability to identify potential system hazards and operability problems, and to recommend appropriate mitigation mechanisms is vital to the development of safety critical embedded systems. Hazard and Operability (HAZOP) analysis which is mostly used to achieve these objectives is a complex and largely human-centred process, and increased tool support could reduce costs and improve quality. This work presents a framework and tool prototype that facilitates the early identification of potential system hazards from requirements and the reuse of previous experience for conducting HAZOP. The results from the preliminary evaluation of the tool suggest its potential viability for application in real industrial context.",https://ieeexplore.ieee.org/document/6046555/,2011 4th International Workshop on Managing Requirements Knowledge,30-30 Aug. 2011,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN52387.2021.9533808,End-to-End Federated Learning for Autonomous Driving Vehicles,IEEE,Conferences,"In recent years, with the development of computation capability in devices, companies are eager to investigate and utilize suitable ML/DL methods to improve their service quality. However, with the traditional learning strategy, companies need to first build up a powerful data center to collect and analyze data from the edge and then perform centralized model training, which turns out to be inefficient. Federated Learning has been introduced to solve this challenge. Because of its characteristics such as model-only exchange and parallel training, the technique can not only preserve user data privacy but also accelerate model training speed. The method can easily handle real-time data generated from the edge without taking up a lot of valuable network transmission resources. In this paper, we introduce an approach to end-to-end on-device Machine Learning by utilizing Federated Learning. We validate our approach with an important industrial use case in the field of autonomous driving vehicles, the wheel steering angle prediction. Our results show that Federated Learning can significantly improve the quality of local edge models and also reach the same accuracy level as compared to the traditional centralized Machine Learning approach without its negative effects. Furthermore, Federated Learning can accelerate model training speed and reduce the communication overhead, which proves that this approach has great strength when deploying ML/DL components to various real-world embedded systems.",https://ieeexplore.ieee.org/document/9533808/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA45728.2021.9613213,Energy Efficient-based Sensor Data Prediction using Deep Concatenate MLP,IEEE,Conferences,"This paper proposes a system to reduce sensor energy consumption by predicting the next sensor value. The current implementation of the smart factory utilizes wireless sensor network nodes to monitor the environmental condition in real-time. Instead of periodically exploiting those nodes, a deep learning prediction-based algorithm is proposed in the cluster head to reduce sensing times and increase sensor lifetime. The cluster head can learn the behavior of each sensor nodes based on its previous value. The proposed scenario can be combined with existing solutions in sensor failure detection and recovery to provide a robust solution in the industrial environment.",https://ieeexplore.ieee.org/document/9613213/,2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA ),7-10 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCO48375.2019.9043233,Enhanced Decision Tree-J48 With SMOTE Machine Learning Algorithm for Effective Botnet Detection in Imbalance Dataset,IEEE,Conferences,"Botnet is one of the major security threats in the field of information technology today (IT). The increase in the rate of attack on industrial IT infrastructures, theft of personal data and attacks on financial information is becoming critical. Majority of available dataset for botnet detection are very old and may not be able to stand the present reality in this research area. One of the latest dataset from Canadian Institute of Cyber Security labeled “CICIDS2017” was noted as an imbalance data distribution ratio of 99% to 1%. This distribution represents majority to minority class ratio. This may pose a challenge of over-fitting in majority class to the research and create a bias in the analysis of results. This research work has adopted J48 decision tree machine learning algorithm with application of SMOTE technique in solving the problem of imbalance dataset, thereby leading to an improved detection of botnets. The accuracy of the highest scenario was 99.95%. This is a significant improvement in detection rate compare to the previous research work.",https://ieeexplore.ieee.org/document/9043233/,"2019 15th International Conference on Electronics, Computer and Computation (ICECCO)",10-12 Dec. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE43902.2021.00120,Enhancing Genetic Improvement of Software with Regression Test Selection,IEEE,Conferences,"Genetic improvement uses artificial intelligence to automatically improve software with respect to non-functional properties (AI for SE). In this paper, we propose the use of existing software engineering best practice to enhance Genetic Improvement (SE for AI). We conjecture that existing Regression Test Selection (RTS) techniques (which have been proven to be efficient and effective) can and should be used as a core component of the GI search process for maximising its effectiveness. To assess our idea, we have carried out a thorough empirical study assessing the use of both dynamic and static RTS techniques with GI to improve seven real-world software programs. The results of our empirical evaluation show that incorporation of RTS within GI significantly speeds up the whole GI process, making it up to 78% faster on our benchmark set, being still able to produce valid software improvements. Our findings are significant in that they can save hours to days of computational time, and can facilitate the uptake of GI in an industrial setting, by significantly reducing the time for the developer to receive feedback from such an automated technique. Therefore, we recommend the use of RTS in future test-based automated software improvement work. Finally, we hope this successful application of SE for AI will encourage other researchers to investigate further applications in this area.",https://ieeexplore.ieee.org/document/9401972/,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),22-30 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECS.2010.5724589,Estimating design quality of digital systems via machine learning,IEEE,Conferences,"Although the term design quality of digital systems can be assessed from many aspects, the distribution and density of bugs are two decisive factors. This paper presents the application of machine learning techniques to model the relationship between specified metrics of high-level design and its associated bug information. By employing the project repository (i.e., high level design and bug repository), the resultant models can be used to estimate the quality of associated designs, which is very beneficial for design, verification and even maintenance processes of digital systems. A real industrial microprocessor is employed to validate our approach. We hope that our work can shed some light on the application of software techniques to help improve the reliability of various digital designs.",https://ieeexplore.ieee.org/document/5724589/,"2010 17th IEEE International Conference on Electronics, Circuits and Systems",12-15 Dec. 2010,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPIN.2018.8533862,Evaluation Criteria for Inside-Out Indoor Positioning Systems Based on Machine Learning,IEEE,Conferences,"Real-time tracking allows to trace goods and enables the optimization of logistics processes in many application areas. Camera-based inside-out tracking that uses an infrastructure of fixed and known markers is costly as the markers need to be installed and maintained in the environment. Instead, systems that use natural markers suffer from changes in the physical environment. Recently a number of approaches based on machine learning (ML) aim to address such issues. This paper proposes evaluation criteria that consider algorithmic properties of ML-based positioning schemes and introduces a dataset from an indoor warehouse scenario to evaluate for them. Our dataset consists of images labeled with millimeter precise positions that allows for a better development and performance evaluation of learning algorithms. This allows an evaluation of machine learning algorithms for monocular optical positioning in a realistic indoor position application for the first time. We also show the feasibility of ML-based positioning schemes for an industrial deployment.",https://ieeexplore.ieee.org/document/8533862/,2018 International Conference on Indoor Positioning and Indoor Navigation (IPIN),24-27 Sept. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICSS.2015.7420323,Evolving decision trees to detect anomalies in recurrent ICS networks,IEEE,Conferences,"Researchers have previously attempted to apply machine learning techniques to network anomaly detection problems. Due to the staggering amount of variety that can occur in normal networks, as well as the difficulty in capturing realistic data sets for supervised learning or testing, the results have often been underwhelming. These challenges are far less pronounced when considering industrial control system (ICS) networks. The recurrent nature of these networks results in less noise and more consistent patterns for a machine learning algorithm to recognize. We propose a method of evolving decision trees through genetic programming (GP) in order to detect network anomalies, such as device outages. Our approach extracts over a dozen features from network packet captures and netflows, normalizes them, and relates them in decision trees using fuzzy logic operators. We used the trees to detect three specific network events from three different points on the network across a statistically significant number of runs and achieved 100% accuracy on five of the nine experiments. When the trees attempted to detect more challenging events at points of presence further from the occurrence, the accuracy averaged to above 98%. On cases where the trees were many hops away and not enough information was available, the accuracy dipped to roughly 50%, or that of a random search. Using our method, all of the evolutionary cycles of the GP algorithm are computed a-priori, allowing the best resultant trees to be deployed as semi-real-time sensors with little overhead. In order for the trees to perform optimally, buffered packets and flows need to be ingested at twenty minute intervals.",https://ieeexplore.ieee.org/document/7420323/,2015 World Congress on Industrial Control Systems Security (WCICSS),14-16 Dec. 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/QUATIC.2012.29,Experimental Software Engineering in Educational Context,IEEE,Conferences,"Empirical studies are important in software engineering to evaluate new tools, techniques, methods and technologies in a structured way before they are introduced in the industrial (real) software process. Within this PhD thesis we will develop a framework of a consistent process for involving students as subjects of empirical studies of software engineering. In concrete, our experiences with software development teams composed of students will analyze how RUP (Rational Unified Process) processes can be compliant with the CMMI (Capability Maturity Model Integration), namely in the context of MLs (maturity levels) 2 and 3. Additionally, we will also analyze the influence of project management tools to improve the process maturity of the teams. Our final goal of carrying out empirical studies with students is to understand its validity when compared with the corresponding studies in real industrial settings.",https://ieeexplore.ieee.org/document/6511839/,2012 Eighth International Conference on the Quality of Information and Communications Technology,3-6 Sept. 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EUROSIM.2013.39,Experimental and Computational Materials Defects Investigation,IEEE,Conferences,"Production of railway axles (i.e., one of the basic material of the modern train) is an elaborate process unfree from faults and problems. Errors during the manufacturing or the plies' overlapping, in fact, can cause particular flaws in the resulting material, so compromising its same integrity. Within this framework, ultrasonic tests could be useful to characterize the presence of defect, depending on its dimensions. On the contrary, the requirement of a perfect state for used materials is unavoidable in order to assure both transport reliability and passenger safety. Therefore, a real-time approach able to recognize and classify the defect starting from the finite element simulated ultrasonic echoes could be very useful in industrial applications. The ill-posedness of the so defined process induces a regularization method. In this paper, a finite element and a heuristic approach are proposed. Particularly, the proposed method is based on the use of a Neural Network approach, the so called ""learning by sample techniques"", and on the use of Support Vector Machines in order to classify the kind of defect. Results assure good performances of the implemented approach, with very interesting applications.",https://ieeexplore.ieee.org/document/7004937/,2013 8th EUROSIM Congress on Modelling and Simulation,10-13 Sept. 2013,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IAS.2011.6074350,Experimental performance of a model reference adaptive flux observer based NFC for IM drive,IEEE,Conferences,"This paper presents a model reference adaptive flux (MRAF) observer based neuro-fuzzy controller (NFC) for an induction motor (IM) drive. An improved observer model is developed based on a reference flux model and a closed-loop Gopinath model flux observer which combines current and voltage model flux observers. The d-axis reference flux linkage of the indirect field oriented control is provided by flux weakening method. Furthermore, a proportional-integral (PI) based flux controller is used to provide the compensation for the reference flux model by comparing the flux reference and the observed flux from Gopinath model flux observer. An improved self-tuned NFC is utilized as a speed controller for IM drive. The proposed NFC incorporates fuzzy logic laws with a five-layer artificial neural network (ANN) scheme. In the proposed NFC, parameters of the 4th layer are tuning online for the purpose of minimizing the square of the error. Furthermore, the design of normalized inputs makes the proposed NFC suitable for variant size of IM with a little adjusting. A complete simulation model for indirect field oriented control of IM incorporating the proposed MRAF observer based NFC is developed in Matlab/Simulink. The performances of the proposed IM drive is investigated extensively at different dynamic operating conditions such as step change in load, step change in change in speed, parameter variations, etc. The proposed IM drive is also implemented in real-time using DSP board DS1104 for a laboratory 1 HP IM. The performance of the proposed MRAF observer based NFC controller is found robust and potential candidate for high performance industrial drive applications.",https://ieeexplore.ieee.org/document/6074350/,2011 IEEE Industry Applications Society Annual Meeting,9-13 Oct. 2011,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MELCON.1994.380920,Expert system for control purpose based on CLIPS,IEEE,Conferences,"The use of AI techniques in complex process control of industrial environments, introduces some problems. The first is related to which kind of tools can be used and their match to system requirements. The next one is how to incorporate new features if needed and how to integrate them. The last one is related to the final implementation. Expert systems available in public domain source code seems a good solution in order to reach the advantages of an ES developed specially for the application without spending time developing low level ES features. An ES in source code allows us to select some parts of an ES kernel useful for the particular application. Actually we can find commercial ES in source code like CLIPS. We describe the problem from the point of view of developing ES with CLIPS. This tool is a complete ES language written in C code, it uses the RETE fast pattern matching algorithm to implement forward chaining inference engine. The availability of the source code allows us to cut the parts not used in a particular application and recompile it in a specific machine for control purposes. Therefore we can link the modified code with real time run-time libraries, obtaining applications dealing with real time constraints. Based on this tool, an intelligent controller has been developed which uses fuzzy logic for uncertainty handling.<>",https://ieeexplore.ieee.org/document/380920/,Proceedings of MELECON '94. Mediterranean Electrotechnical Conference,12-14 April 1994,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IS.2018.8710554,Exploiting the Digital Twin in the Assessment and Optimization of Sustainability Performances,IEEE,Conferences,"Digitalization has shown the potential to disrupt industrial value chains by supporting real-time, risk-free and inexpensive inputs to decision making towards enhanced companies' productivity and value networks flexibility. Developing a reliable and robust digital replica of the physical systems of the value chain is one of the most advanced (and challenging) approaches to digitalization, condensed in the concept of Digital Twin (DT). DT plays a fundamental role in creating a data-rich environment where simulation and optimization procedures can be run. With DT expected to become a commodity in the coming years, simulation and optimization become therefore a more accessible instrument for the improvement of manufacturing and business processes also in small enterprises with limited investment capacity. While scientific literature has analysed the adoption of DT in the optimization of products lifecycle, no contributions have yet focused on the exploitation of DT to improve the sustainability performances of whole value chains. In this paper we propose a reference framework where DTs built upon process and system data gathered from the field, allow to quickly assess the sustainability performances of both existing and planned production mixes and to compare achievable impacts with changing processes and technologies, thus enabling advisory features for sustainability-aware decision making in structured, multi-entity value networks. Internal validation will be deployed referring to real case studies.",https://ieeexplore.ieee.org/document/8710554/,2018 International Conference on Intelligent Systems (IS),25-27 Sept. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/Agro-Geoinformatics.2019.8820424,Farming on the edge: Architectural Goals,IEEE,Conferences,"This research investigates how advances in Internet of Things (IoT) and availability of internet connection would enable Edge Solutions to promote smart utilization of existing machines at the edge. The presented results are based on experiments performed in real scenarios using the proposed solution. Whereas scenarios were cloned from real environments it is important to have in mind that experiments were performed with low load in terms of data and small number of devices in terms of distribution. As result of extensive architecture investigation for an optimal edge solution and its possible correlation to industrial applications, this paper will provide evidences supporting the use of edge solutions in challenging conditions which arise at the edge, including smart factories and smart agriculture. The present work assumes that the reader has some exposition to Edge computing, Cloud computing and software development. The paper will present some important findings on this area, compare main architectural aspects and will provide a broad view of how edge solutions might be built for this particular scenario. Having discussed how the ideal architecture works and having provided an overview about how it may be applied to industrial plants, the final section of this paper addresses how artificial intelligence will fit into edge solutions, forming a new source of “smart capabilities” to existing environments.",https://ieeexplore.ieee.org/document/8820424/,2019 8th International Conference on Agro-Geoinformatics (Agro-Geoinformatics),16-19 July 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITOEC.2018.8740558,Fast Inter Mode Decision Algorithms for x265,IEEE,Conferences,"The latest High-Efficiency Video Coding (HEVC) standard achieves nearly 50% bit rates reduction for similar quality relative to H.264/Advanced Video Coding(AVC) . However, its complexity is enormously increased ,which becomes one of the most challenges for its deployment in real time applications. The only solution to decrease the coding complexity is to set up different settings by adjusting various coding parameters. Among them, low complexity settings are suitable for industrial applications and conducive to the popularization of HEVC. Traditional fast mode decision algorithms mainly aim at decreasing coding complexity for high complexity settings. In this paper, we propose a fast mode decision method for HEVC with low complexity settings according to machine learning. A decision tree is constructed to decide whether to check 2N×2N mode or the SKIP/MERGE mode by exploiting relevant information from spatiotemporal adjacent Coding Units(CUs). Further mode skipping is performed based on the result of the first step. Experiments show that the proposed scheme can only increase by 1.42% Bjotegaard Delta Bit rate(BDBR) with an average time reduction of 22.45% for HEVC with low complexity settings.",https://ieeexplore.ieee.org/document/8740558/,2018 IEEE 4th Information Technology and Mechatronics Engineering Conference (ITOEC),14-16 Dec. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE43902.2021.00085,Fast Outage Analysis of Large-Scale Production Clouds with Service Correlation Mining,IEEE,Conferences,"Cloud-based services are surging into popularity in recent years. However, outages, i.e., severe incidents that always impact multiple services, can dramatically affect user experience and incur severe economic losses. Locating the root-cause service, i.e., the service that contains the root cause of the outage, is a crucial step to mitigate the impact of the outage. In current industrial practice, this is generally performed in a bootstrap manner and largely depends on human efforts: the service that directly causes the outage is identified first, and the suspected root cause is traced back manually from service to service during diagnosis until the actual root cause is found. Unfortunately, production cloud systems typically contain a large number of interdependent services. Such a manual root cause analysis is often time-consuming and labor-intensive. In this work, we propose COT, the first outage triage approach that considers the global view of service correlations. COT mines the correlations among services from outage diagnosis data. After learning from historical outages, COT can infer the root cause of emerging ones accurately. We implement COT and evaluate it on a real-world dataset containing one year of data collected from Microsoft Azure, one of the representative cloud computing platforms in the world. Our experimental results show that COT can reach a triage accuracy of 82.1%-83.5%, which outperforms the state-of-the-art triage approach by 28.0%-29.7%.",https://ieeexplore.ieee.org/document/9402074/,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),22-30 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCT.2018.8356831,Fault class prediction in unsupervised learning using model-based clustering approach,IEEE,Conferences,"Manufacturing industries have been on a steady path considering for new methods to achieve near-zero downtime to have flexibility in the manufacturing process and being economical. In the last decade with the availability of industrial internet of things (IIoT) devices, this has made it possible to monitor the machine continuously using wireless sensors, assess the degradation and predict the failures of time. Condition-based predictive maintenance has made a significant influence in monitoring the asset and predicting the failure of time. This has minimized the impact on production, quality, and maintenance cost. Numerous approaches have been in proposed over the years and implemented in supervised learning. In this paper, challenges of supervised learning such as need for historical data and incapable of classifying new faults accurately will be overcome with a new methodology using unsupervised learning for rapid implementation of predictive maintenance activity which includes fault prediction and fault class detection for known and unknown faults using density estimation via Gaussian Mixture Model Clustering and K-means algorithm and compare their results with a real case vibration data.",https://ieeexplore.ieee.org/document/8356831/,2018 International Conference on Information and Computer Technologies (ICICT),23-25 March 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2MTC43012.2020.9129595,Feature Ranking under Industrial Constraints in Continuous Monitoring Applications based on Machine Learning Techniques,IEEE,Conferences,"The design work-flow of machine learning techniques for continuous monitoring or predictive maintenance in an industrial context is usually a two step procedure: the selection of features to be computed from the observed signals and training of a suitable algorithm with real-life meaningful data, that will be next deployed in the second step. Feature selection is a relevant task since it provides a powerful optimisation of the deployed algorithm performance, for the given training data-set. The paper provides a method for feature ranking and selection that embeds constraints coming from real-life applications, including sensing device specifications, environmental noise, available processing resources, being all these latter aspects not considered in the currently available literature methods for feature selection. A practical case-study in the field on anomaly detection of machines is reported and discussed, in order to show the good properties of the provided method.",https://ieeexplore.ieee.org/document/9129595/,2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),25-28 May 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DAS.2018.29,Field Extraction by Hybrid Incremental and A-Priori Structural Templates,IEEE,Conferences,"In this paper, we present an incremental frame-work for extracting information fields from administrative documents. First, we demonstrate some limits of the existing state-of-the-art methods such as the delay of the system efficiency. This is a concern in industrial context when we have only few samples of each document class. Based on this analysis, we propose a hybrid system combining incremental learning by means of itf-df statistics and a-priori generic models. We report in the experimental section our results obtained with a dataset of real invoices.",https://ieeexplore.ieee.org/document/8395204/,2018 13th IAPR International Workshop on Document Analysis Systems (DAS),24-27 April 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTTA.2008.4530058,Filters Bank Derived from the Wavelet Transform for Real Time Change Detection in Signal,IEEE,Conferences,"The aim of this paper is to detect the faults in industrial systems, through on-line monitoring. The faults that are concerned correspond to changes in frequency components of the signal. Thus, early fault detection, which reduces the possibility of catastrophic damage, is possible by detecting the changes of characteristic features of the signal. This approach combines the filters bank technique, for extracting frequency and energy characteristic features, and the dynamic cumulative sum method (DCS), which is a recursive calculation of the logarithm of the likelihood ratio between two local hypotheses. The main contribution is to derive the filters coefficients from the wavelet in order to use the filters bank as a wavelet transform. The advantage of our approach is that the filters bank can be hardware implemented and can be used for online detection.",https://ieeexplore.ieee.org/document/4530058/,2008 3rd International Conference on Information and Communication Technologies: From Theory to Applications,7-11 April 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICESC48915.2020.9155625,Fire and Gun Violence based Anomaly Detection System Using Deep Neural Networks,IEEE,Conferences,"Real-time object detection to improve surveillance methods is one of the sought-after applications of Convolutional Neural Networks (CNNs). This research work has approached the detection of fire and handguns in areas monitored by cameras. Home fires, industrial explosions, and wildfires are a huge problem that cause adverse effects on the environment. Gun violence and mass shootings are also on the rise in certain parts of the world. Such incidents are time-sensitive and can cause a huge loss to life and property. Hence, the proposed work has built a deep learning model based on the YOLOv3 algorithm that processes a video frame-by-frame to detect such anomalies in real-time and generate an alert for the concerned authorities. The final model has a validation loss of 0.2864, with a detection rate of 45 frames per second and has been benchmarked on datasets like IMFDB, UGR, and FireNet with accuracies of 89.3%, 82.6% and 86.5% respectively. Experimental result satisfies the goal of the proposed model and also shows a fast detection rate that can be deployed indoor as well as outdoors.",https://ieeexplore.ieee.org/document/9155625/,2020 International Conference on Electronics and Sustainable Communication Systems (ICESC),2-4 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2000.973216,Force control in robotic assembly under extreme uncertainty using ANN,IEEE,Conferences,"Robotic assembly operations can be performed by specifying an exact model of the operation. However, the uncertainties involved during assembly make it difficult to conceive such a model In these cases, the use of a connectionist model may be advantageous. In this paper, the design of a robotic cell based on the adaptive resonance theory artificial neural network and a PC host-slave architecture that overcame these uncertainties is presented. Different sources of uncertainty under real conditions are identified and their contribution in a typical assembly operation evaluated. The robotic system is implemented using a PUMA 761 industrial robot with six degrees of freedom (DOF) and a force/torque (F/T) sensor attached to its wrist which conveys force information to the neural network controller (NNC). Results during assembly operations are presented which validate the approach. Furthermore, the method is generic and can be implemented onto other manipulators.",https://ieeexplore.ieee.org/document/973216/,"2000 26th Annual Conference of the IEEE Industrial Electronics Society. IECON 2000. 2000 IEEE International Conference on Industrial Electronics, Control and Instrumentation. 21st Century Technologies",22-28 Oct. 2000,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PerComWorkshops51409.2021.9430942,Forecasting Parking Lots Availability: Analysis from a Real-World Deployment,IEEE,Conferences,"Smart parking technologies are rapidly being deployed in cities and public/private places around the world for the sake of enabling users to know in real time the occupancy of parking lots and offer applications and services on top of that information. In this work, we detail a real-world deployment of a full-stack smart parking system based on industrial-grade components. We also propose innovative forecasting models (based on CNN-LSTM) to analyze and predict parking occupancy ahead of time. Experimental results show that our model can predict the number of available parking lots in a ±3% range with about 80% accuracy over the next 1-8 hours. Finally, we describe novel applications and services that can be developed given such forecasts and associated analysis.",https://ieeexplore.ieee.org/document/9430942/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDICON52576.2021.9691655,Forecasting of Load and Solar PV Power to Assess Demand Response Potential,IEEE,Conferences,"Estimating demand response potential is important for the power utility in planning and management of the energy resources and power infrastructure. In this work, we have developed a machine learning model using a long short-term memory neural network for multivariate time series load forecasting using Python. The response of the load forecasting model with the different network configuration parameters is also analyzed in order to improve the accuracy of the model. Real-time electrical load data of the 11 kV feeder from one of the substations of the Goa state electricity board is used for training and developing the short-term forecasting model. The result yields model capable of accurate electrical load forecasting. Energy forecasting of SPV system of 100 kWp capacity is also done for a similar period by using PV<sup>&#x002A;</sup> SOL software. The data of the scheduled power reserved for the 11 kV feeder feeding an industrial area is analyzed. From the forecasted load, SPV energy prediction, and the scheduled power data the demand response potential is estimated. This work will help the state power utility to plan and coordinate demand response programs along with scheduling renewable energy resources for maintaining the reliability and security of the power system network.",https://ieeexplore.ieee.org/document/9691655/,2021 IEEE 18th India Council International Conference (INDICON),19-21 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AiDAS47888.2019.8970881,"Framework Of Malay Intelligent Autonomous Helper (Min@H): Text, Speech And Knowledge Dimension Towards Artificial Wisdom For Future Military Training System",IEEE,Conferences,"Industrial Revolution 4.0 is expected to improve the way of military training system. Most of the assistant systems use English for their Human Machine Interaction (HMI) such `SARA' a virtual socially aware robot assistant which exclude Malay socio-emotional aspects. This scenario opens a suggestion, to internalize socio-emotional aspects based on Malay culture, custom and beliefs to military autonomous training systems (i.e. MIN@H) that can improve the `collaborative' skills between Malaysian military personnel and the systems. Therefore, to increase the wisdom of the systems, they must have feature to capture information for their human users or helping human users to learn new knowledge and ensure the interaction is comfortable and engaging. For that reason, the systems must understand Malay language and be able to interpret emotion and expression behavior according to the Malay culture and custom, furthermore, the systems able to differentiate the level of user's understanding and build a good rapport or feeling of harmony that makes communication possible or easy between the systems and users. This concept of the systems is referred as Malay Artificial Wisdom System (AWS). There are three fundamental aspects to achieve the AWS. First, to computationally model the conversational strategies and rapport between the system and human users based-on user's understanding and system's articulation. Second, to computationally model, recognize and synthesize the emotion and expression behavior according to the Malay culture, custom and beliefs. Third, the AWS can do analytical reasoning and responding in relation to falsehood analysis and users' understanding level. Knowledge discovery and inference technique as well as HMI that cater the inputs and output of the MIN@H will be developed to accomplish the AWS concept. This program could embrace military training system in Malaysia to enhance military personnel skills and experts in various areas.",https://ieeexplore.ieee.org/document/8970881/,2019 1st International Conference on Artificial Intelligence and Data Sciences (AiDAS),19-19 Sept. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEST.2018.8495711,From M&V to M&T: An artificial intelligence-based framework for real-time performance verification of demand-side energy savings,IEEE,Conferences,"The European Union’s Energy Efficiency Directive is placing an increased focus on the measurement and verification (M&V) of demand side energy savings. The objective of M&V is to quantify energy savings with minimum uncertainty. M&V is currently undergoing a transition to practices, known as M&V 2.0, that employ automated advanced analytics to verify performance. This offers the opportunity to effectively manage the transition from short-term M&V to long-term monitoring and targeting (M&T) in industrial facilities. The original contribution of this paper consists of a novel, robust and technology agnostic framework that not only satisfies the requirements of M&V 2.0, but also bridges the gap between M&V and M&T by ensuring persistence of savings. The approach features a unique machine learning-based energy modelling methodology, model deployment and an exception reporting system that ensures early identification of performance degradation. A case study demonstrates the effectiveness of the approach. Savings from a real-world project are found to be 177,962 +/- 12,334 kWh with a 90% confidence interval. The uncertainty associated with the savings is 8.6% of the allowable uncertainty, thus highlighting the viability of the framework as a reliable and effective tool.",https://ieeexplore.ieee.org/document/8495711/,2018 International Conference on Smart Energy Systems and Technologies (SEST),10-12 Sept. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCI-CC.2014.6921432,From information revolution to intelligence revolution: Big data science vs. intelligence science,IEEE,Conferences,"The hierarchy of human knowledge is categorized at the levels of data, information, knowledge, and intelligence. For instance, given an AND-gate with 1,000-input pins, it may be described very much differently at various levels of perceptions in the knowledge hierarchy. At the data level on the bottom, it represents a 21,000 state space, known as `big data' in recent terms, which appears to be a big issue in engineering. However, at the information level, it just represents 1,000 bit information that is equivalent to the numbers of inputs. Further, at the knowledge level, it expresses only two rules that if all inputs are one, the output is one; and if any input is zero, the output is zero. Ultimately, at the intelligence level, it is simply an instance of the logical model of an AND-gate with arbitrary inputs. This problem reveals that human intelligence and wisdom are an extremely efficient and a fast convergent induction mechanism for knowledge and wisdom elicitation and abstraction where data are merely factual materials and arbitrary instances in the almost infinite state space of the real world. Although data and information processing have been relatively well studied, the nature, theories, and suitable mathematics underpinning knowledge and intelligence are yet to be systematically studied in cognitive informatics and cognitive computing. This will leads to a new era of human intelligence revolution following the industrial, computational, and information revolutions. This is also in accordance with the driving force of the hierarchical human needs from low-level material requirements to high-level ones such as knowledge, wisdom, and intelligence. The trend to the emerging intelligent revolution is to meet the ultimate human needs. The basic approach to intelligent revolution is to invent and embody cognitive computers, cognitive robots, and cognitive systems that extend human memory capacity, learning ability, wisdom, and creativity. Via intelligence revolution, an interconnected cognitive intelligent Internet will enable ordinary people to access highly intelligent systems created based on the latest development of human knowledge and wisdom. Highly professional systems may help people to solve typical everyday problems. Towards these objectives, the latest advances in abstract intelligence and intelligence science investigated in cognitive informatics and cognitive computing are well positioned at the center of intelligence revolution. A wide range of applications of cognitive computers have been developing in ICIC [http://www.ucalgary.ca/icic/] such as, inter alia, cognitive computers, cognitive robots, cognitive learning engines, cognitive Internet, cognitive agents, cognitive search engines, cognitive translators, cognitive control systems, cognitive communications systems, and cognitive automobiles.",https://ieeexplore.ieee.org/document/6921432/,2014 IEEE 13th International Conference on Cognitive Informatics and Cognitive Computing,18-20 Aug. 2014,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE.2013.6606744,From models to code and back: Correct-by-construction code from UML and ALF,IEEE,Conferences,"Ever increasing complexity of modern software systems demands new powerful development mechanisms. Model-driven engineering (MDE) can ease the development process through problem abstraction and automated code generation from models. In order for MDE solutions to be trusted, such generation should preserve the system's properties defined at modelling level, both functional and extra-functional, all the way down to the target code. The outcome of our research is an approach that aids the preservation of system's properties in MDE of embedded systems. More specifically, we provide generation of full source code from design models defined using the CHESS-ML, monitoring of selected extra-functional properties at code level, and back-propagation of observed values to design models. The approach is validated against industrial case-studies in the telecommunications applicative domain.",https://ieeexplore.ieee.org/document/6606744/,2013 35th International Conference on Software Engineering (ICSE),18-26 May 2013,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2013.6699377,Fuel Cells prognostics using echo state network,IEEE,Conferences,"One remaining technological bottleneck to develop industrial Fuel Cell (FC) applications resides in the system limited useful lifetime. Consequently, it is important to develop failure diagnostic and prognostic tools enabling the optimization of the FC. Among all the existing prognostics approaches, datamining methods such as artificial neural networks aim at estimating the process' behavior without huge knowledge about the underlying physical phenomena. Nevertheless, this kind of approach needs huge learning dataset. Also, the deployment of such an approach can be long (trial and error method), which represents a real problem for industrial applications where real-time complying algorithms must be developed. According to this, the aim of this paper is to study the application of a reservoir computing tool (the Echo State Network) as a prognostics system enabling the estimation of the Remaining Useful Life of a Proton Exchange Membrane Fuel Cell. Developments emphasize on the prediction of the mean voltage cells of a degrading FC. Accuracy and time consumption of the approach are studied, as well as sensitivity of several parameters of the ESN. Results appear to be very promising.",https://ieeexplore.ieee.org/document/6699377/,IECON 2013 - 39th Annual Conference of the IEEE Industrial Electronics Society,10-13 Nov. 2013,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COASE.2017.8256157,Full automatic path planning of cooperating robots in industrial applications,IEEE,Conferences,"Parts made of carbon fiber reinforced plastics (CFRP) for airplane components can be so huge that a single industrial robot is no longer able to handle them, and cooperating robots are required. Manual programming of cooperating robots is difficult, but with large numbers of different sized and shaped cut-pieces, it is almost impossible. This paper presents an automated production system consisting of a camera for the precise detection of the position of each cut-piece and a collision-free path planner which can dynamically react to different positions for the transfer motions. The path is planned for multiple robots adhering to motion constrains, such as the requirement that the textile cut-piece must form a catenary which can change during transport. Additionally a technique based on machine learning has been implemented which correctly resolves redundancy for a linear axis during planning. Finally, all components are tested on a real robot system in industrial scale.",https://ieeexplore.ieee.org/document/8256157/,2017 13th IEEE Conference on Automation Science and Engineering (CASE),20-23 Aug. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2017.8107749,Fuzzy logic based solar panel and battery control system design,IEEE,Conferences,"Photovoltaic systems, whether they are domestic, commercial or industrial often incorporate some forms of system protection. However, elaborate real-time fault detection is not defined for most such systems. To address this shortfall, a comprehensive photovoltaic installation system fault detection and control strategy is presented in this paper. The designed system is made up of fault detection in any of one of the photovoltaic system components that include the solar panel, charge controller, battery and inverter. The system also includes battery and user load current control. Fuzzy logic principle, due to its powerful non-linear problem solving capabilities is used in formulation of the fault detection and control algorithms as opposed to the classical method. This results in simpler, cheaper and faster hardware, which in this case is implemented on the PIC18F4550 microcontroller.",https://ieeexplore.ieee.org/document/8107749/,2017 International Conference on Machine Learning and Cybernetics (ICMLC),9-12 July 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZ-IEEE.2014.6891715,Fuzzy uncertainty assessment in RBF Neural Networks using neutrosophic sets for multiclass classification,IEEE,Conferences,"In this paper we introduce a fuzzy uncertainty assessment methodology based on Neutrosophic Sets (NS). This is achieved via the implementation of a Radial Basis Function Neural-Network (RBF-NN) for multiclass classification that is functionally equivalent to a class of Fuzzy Logic Systems (FLS). Two types of uncertainties are considered: a) fuzziness and b) ambiguity, with both uncertainty types measured in each receptive unit (RU) of the hidden layer of the RBF-NN. The use of NS assists in the quantification of the uncertainty and formation of the rulebase; the resulting RBF-NN modelling structure proves to have enhanced transparency features to interpretation that enables us to understand the influence of each system parameter thorughout the parameter identification. The presented methodology is based on firstly constructing a neutrosophic set by calculating the associated fuzziness in each rule - and then use this information to train the RBF-NN; and secondly, an ambiguity measure that is defined via the truth and falsity measures related to each normalised consequence of the fuzzy rules within the RUs. In order to evaluate the individual ambiguity in the RUs and then the average ambiguity of the whole system, a neutrosophic set is constructed. Finally, the proposed methodology is tested against two case studies: a benchmark dataset problem and a real industrial case study. On both cases we demonstrate the effectiveness of the developed methodology in automatically creating uncertainty measures and utilising this new information to improve the quality of the trained model.",https://ieeexplore.ieee.org/document/6891715/,2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),6-11 July 2014,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCO.2018.8634719,Gas Turbine Fault Classification Based on Machine Learning Supervised Techniques,IEEE,Conferences,"Nowadays Machinery Diagnostic becomes a major part for many industrial applications. It allows to predict and prevent of breakages. An analysis of the trends in the development of power machines show that the most advanced installations can be created using gas turbine technologies. Quite justified, many energy specialists consider the XXI century - the century of gas turbine technologies. It is very important to prevent gas turbine failure. In this paper investigated machine learning classification techniques with further implementation for fault detection in gas turbine running data trends. Investigation was done for real gas compression station running parameters.",https://ieeexplore.ieee.org/document/8634719/,2018 14th International Conference on Electronics Computer and Computation (ICECCO),29 Nov.-1 Dec. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MERCon.2019.8818945,Generalised Framework for Automated Conversational Agent Design via QFD,IEEE,Conferences,"Automated conversational agents are often regarded as the most promising method of responding to customer queries with minimum human intervention. Focus of the existing literature is mostly on technological innovations such as artificial intelligence and learning. A generalized text based real-time conversational agent or a chatbot development framework can be both conceptually and practically appealing, as a way to develop technologies to improve the responsiveness and customer friendliness of the chatbots. Different forms of technological advancements could help the firms to deploy right chatbot technology with regard to the user requirements in their organizations. This paper explores some product design ideas such as Analytic Hierarchy Process (AHP) and Quality Function Deployment (QFD) drawn from industrial engineering literature for chatbot development.",https://ieeexplore.ieee.org/document/8818945/,2019 Moratuwa Engineering Research Conference (MERCon),3-5 July 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AI4I51902.2021.00022,Generating Reinforcement Learning Environments for Industrial Communication Protocols,IEEE,Conferences,"An important part of any reinforcement learning application is interfacing the agent to its environment. To enable an easier use of reinforcement learning agents in manufacturing and automation-related real-world environments, we propose an environment generator which acts as an adapter between the interface of the agent and existing industrial communication protocols. This paper describes the functionality and architecture of such an environment generator.",https://ieeexplore.ieee.org/document/9565507/,2021 4th International Conference on Artificial Intelligence for Industries (AI4I),20-22 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2019.8790171,Genetic Algorithm for Topology Optimization of an Artificial Neural Network Applied to Aircraft Turbojet Engine Identification,IEEE,Conferences,"Artificial neural networks (ANN) has attracted attention of the academic community by the current progress that this technique has provided in speech recognition and digital media such as as image, video, audio, and signal processing. Some fields, as industrial process control and product development can be highly benefited by the development of techniques based on the proven potentialities of ANN models, allowing more accurate simulation, better adaptation to changing environments, and greater robustness in model-based fault diagnosis. Along with the advance of ANNs, there is a trend of open-source softwares use for soft computing which facilitates the access of the interested readers to implement their own codes and to explore other applications. Historically evolutionary algorithms such as the Genetic Algorithm (GA) have been implemented to evolve the architectures to search for solutions, in order to solve this fundamental issue that is still an open problem in the general case. Therefore, the present paper investigates the application of ANN to model the nonlinear aircraft turbojet engine through black-box approach. For that purpose it was used real-world measurements of aircraft engine's fuel and rotation as input and output, respectively. In order to facilitate the design, the ANN was optimized aiming to determine the best topology according to the one-step-ahead and free-run simulation. The results obtained encourage the use of automatically generated ANN architectures for dynamic system modeling.",https://ieeexplore.ieee.org/document/8790171/,2019 IEEE Congress on Evolutionary Computation (CEC),10-13 June 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSP52935.2021.9522588,Genetic Programming based Identification of an Industrial Process,IEEE,Conferences,"In the field of industrial automation, it is essential to develop and improve mathematical methods that assist in obtaining more accurate models of real-world systems. In the following paper, a machine learning tool is applied to the problem of identifying a model of an industrial process. Symbolic regression and genetic programming are a successful combination of methods using which one can identify a nonlinear model in analytical form based on data collected from a process during routine operation. In this paper, a detailed description of the method implementation as well as necessary data preprocessing steps are presented. Then, the resulting models are validated on an industrial data set and compared on the basis of performance metrics with more classical methods and previous results achieved by the authors. Finally, the encountered problems in the realization of the methods are reflected upon.",https://ieeexplore.ieee.org/document/9522588/,2021 44th International Conference on Telecommunications and Signal Processing (TSP),26-28 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZY.2006.1681714,Granular Auto-regressive Moving Average (grARMA) Model for Predicting a Distribution from Other Distributions. Real-world Applications,IEEE,Conferences,"Industrial products are often output in batches at discrete times. A batch gives rise to distributions of measurements, one distribution per variable of interest. There may be a need for modeling to predict a distribution from other distributions. This work represents a distribution by a fuzzy interval number (FIN) interpreted as an information granule. Based on vector lattice theory it is shown that the lattice F+ of positive FINs is a cone in a non-linearly tunable, metric, linear space. In conclusion, a multivariate granular autoregressive moving average (grARMA) model is proposed for predicting a distribution from other distributions. A recursive neural network implementation is shown. We report preliminary results regarding two real-world applications including, first, industrial fertilizer production and, second, environmental pollution monitoring along seashore in northern Greece. The far-reaching potential of novel techniques is discussed.",https://ieeexplore.ieee.org/document/1681714/,2006 IEEE International Conference on Fuzzy Systems,16-21 July 2006,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASE51524.2021.9678746,Graph-based Incident Aggregation for Large-Scale Online Service Systems,IEEE,Conferences,"As online service systems continue to grow in terms of complexity and volume, how service incidents are managed will significantly impact company revenue and user trust. Due to the cascading effect, cloud failures often come with an overwhelming number of incidents from dependent services and devices. To pursue efficient incident management, related incidents should be quickly aggregated to narrow down the problem scope. To this end, in this paper, we propose GRLIA, an incident aggregation framework based on graph representation learning over the cascading graph of cloud failures. A representation vector is learned for each unique type of incident in an unsupervised and unified manner, which is able to simultaneously encode the topological and temporal correlations among incidents. Thus, it can be easily employed for online incident aggregation. In particular, to learn the correlations more accurately, we try to recover the complete scope of failures&#x2019; cascading impact by leveraging fine-grained system monitoring data, i.e., Key Performance Indicators (KPIs). The proposed framework is evaluated with real-world incident data collected from a large-scale online service system of Huawei Cloud. The experimental results demonstrate that GRLIA is effective and outperforms existing methods. Furthermore, our framework has been successfully deployed in industrial practice.",https://ieeexplore.ieee.org/document/9678746/,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),15-19 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VETECS.2000.851509,"Graphical control of autonomous, virtual vehicles",IEEE,Conferences,"This paper presents some of the developments we made with the goal of allowing a friendly control and simulation of a large number of autonomous agents based in behavior in interactive real-time systems. Our work has been specially oriented to the simulation and control of autonomous vehicles and pedestrians in the preparation of scenarios to driving simulation experiments in the DriS simulator. Because every element is intrinsically autonomous, only a few of them are usually addressed to implement the desired study event. Also, because our model is autonomous and controllable, we can use the same model in the implementation of both environment traffic and controlled vehicles. Our scripting language is based in Grafcet, a well known graphical language used in the specification and programming of industrial controllers. Our technique allows the imposition of both short time orders and long time goals to each autonomous element. Orders can be triggered reactively using sensors that monitor the state of virtual traffic and configurable timers that generate all the necessary fixed and variable time events.",https://ieeexplore.ieee.org/document/851509/,VTC2000-Spring. 2000 IEEE 51st Vehicular Technology Conference Proceedings (Cat. No.00CH37026),15-18 May 2000,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASE51524.2021.9678708,Groot: An Event-graph-based Approach for Root Cause Analysis in Industrial Settings,IEEE,Conferences,"For large-scale distributed systems, it is crucial to efficiently diagnose the root causes of incidents to maintain high system availability. The recent development of microservice architecture brings three major challenges (i.e., complexities of operation, system scale, and monitoring) to root cause analysis (RCA) in industrial settings. To tackle these challenges, in this paper, we present Groot, an event-graph-based approach for RCA. Groot constructs a real-time causality graph based on events that summarize various types of metrics, logs, and activities in the system under analysis. Moreover, to incorporate domain knowledge from site reliability engineering (SRE) engineers, Groot can be customized with user-defined events and domain-specific rules. Currently, Groot supports RCA among 5,000 real production services and is actively used by the SRE teams in eBay, a global e-commerce system serving more than 159 million active buyers per year. Over 15 months, we collect a data set containing labeled root causes of 952 real production incidents for evaluation. The evaluation results show that Groot is able to achieve 95% top-3 accuracy and 78% top-1 accuracy. To share our experience in deploying and adopting RCA in industrial settings, we conduct a survey to show that users of Groot find it helpful and easy to use. We also share the lessons learned from deploying and adopting Groot to solve RCA problems in production environments.",https://ieeexplore.ieee.org/document/9678708/,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),15-19 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593709,"Hands and Faces, Fast: Mono-Camera User Detection Robust Enough to Directly Control a UAV in Flight",IEEE,Conferences,"We present a robust real-time system for simultaneous detection of hands and faces in RGB and gray-scale images, and a novel dataset used for training. Our goal is to provide a robust sensor front-end suitable for real-time human-robot interaction using face-engagement and gestures. Using hand-labelled videos obtained from real human-UAV interaction experiments, we re-trained the YOLOv2 Deep Convolutional Neural Network to detect only hands and faces. This model was then used to automatically label several much larger third-party datasets. After manual correction of these results, we modified and re-trained the model on all this labelled data. We obtain qualitatively good detection results at 60Hz on a commodity GPU: our simultaneous hand-and-face detector gives state of the art accuracy and speed in a hand detection benchmark and competitive results in a face detection benchmark. To demonstrate its effectiveness for human-robot interaction we describe its use as the input to a simple but practical gestural human-UAV interface for entertainment or industrial applications. All software, training and test data are freely available.",https://ieeexplore.ieee.org/document/8593709/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.2019.8868968,Hard Real-Time Capable OPC UA Server as Hardware Peripheral for Single Chip IoT Systems,IEEE,Conferences,"The fast semantics project examines the use of the OPC Unified Architecture (OPC UA) in embedded industrial systems and proposes the design of a customizable, hard real time capable OPC UA Intellectual Property Core (IP Core) for single chip computing plattforms. This allows using OPC UA in both novel energy efficient sensor applications and in state of the art field devices. These single chip OPC UA servers form the semantic data sources for future applications such as cloud based added value services or machine learning applications. This article presents the design alternatives and first synthesis results for the implementation of OPC UA servers in embedded systems.",https://ieeexplore.ieee.org/document/8868968/,2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),10-13 Sept. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/QRS54544.2021.00034,Historical Information Stability based Reward for Reinforcement Learning in Continuous Integration Testing,IEEE,Conferences,"In the continuous integration, test case prioritization can effectively alleviate the resource-intensive problems associated with frequent integration commits. Test case prioritization in continuous integration is a sequential decision problem from which reinforcement learning is applied and can effectively adapt and learn from a changing environment. However, continuous integration testing brings new problems of sparse rewards to reinforcement learning because of frequent integration with low test failure and this problem can be addressed by increasing the number of rewarded test cases. In this paper, we propose a reinforcement learning reward object selection strategy based on Test Case Synchronization and Diversity (TCSD) that rewards failed test cases and with an additional selection of passed test cases with potential failure ability. The experiments on six real-world industrial data sets show that TCSD improves the learning efficiency and fault detection ability of reinforcement learning 6.35&#x0025; in average NAPFD compared with the traditional strategies.",https://ieeexplore.ieee.org/document/9724758/,"2021 IEEE 21st International Conference on Software Quality, Reliability and Security (QRS)",6-10 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.2018.8502527,Holo Pick'n'Place,IEEE,Conferences,"In this paper we contribute to the research on facilitating industrial robot programming by presenting a concept for intuitive drag and drop like programming of pick and place tasks with Augmented Reality (AR). We propose a service-oriented architecture to achieve easy exchangeability of components and scalability with respect to AR devices and robot workplaces. Our implementation uses a HoloLens and a UR5 robot, which are integrated into a framework of RESTful web services. The user can drag recognized objects and drop them at a desired position to initiate a pick and place task. Although the positioning accuracy is unsatisfactory yet, our implemented prototype achieves most of the desired advantages to proof the concept.",https://ieeexplore.ieee.org/document/8502527/,2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA),4-7 Sept. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA45728.2021.9613331,Human Action Recognition as part of a Natural Machine Operation Framework,IEEE,Conferences,"The reliability of systems that use machine learning to recognize the human working in an industrial environment is of high importance for the employee safety. we present a framework which is capable of recognizing the person's natural interaction with an industrial machine. We focus on the application of human action recognition in the context of machine operation by skilled workers in industrial or commercial environments. We propose a framework that includes action recognition as part of a software component for understanding behavior. For our use case, we defined an exemplary machine operation workflow which we use to compare five different neural networks in terms of prediction accuracy and real-time capabilities. Moreover, we compare different input shapes as the resolution of input images and the size of the possible 3D-volume in order to study the robustness of the models. For our evaluation, we created our own custom dataset containing six action classes. Our analysis shows that the best model is the I3D with color images, a resolution of 112 × 112 pixels and 16 consecutive frames. The I3D also exhibited the best run-time performance for real-time applications.",https://ieeexplore.ieee.org/document/9613331/,2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA ),7-10 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IDAACS-SWS.2018.8525503,Hybrid MAC for Low Latency Wireless Communication Enabling Industrial HMI Applications,IEEE,Conferences,"Wireless technologies are one of the core components of the future industrial applications. They provide flexibility and scalability to the factory floor in parallel with deployment cost reduction. In our paper, we concentrate on future-oriented human-machine interaction (HMI) applications such as augmented reality (AR) or mobile control. Based on their requirements, we provide an investigation of IEEE 802.11 channel access techniques with respect to their suitability for industrial applications.",https://ieeexplore.ieee.org/document/8525503/,2018 IEEE 4th International Symposium on Wireless Systems within the International Conferences on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS-SWS),20-21 Sept. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.1992.253866,Hybrid architectures for intelligent robotic systems,IEEE,Conferences,"Hybrid architectures, based on combinations of analogic, symbolic, and neural methods, are well suited for real-time applications in advanced robotics. Real-time industrial applications are mainly based on the correction of preplanned programs. So far, the planning and control modules of these kind of applications are often unable to react and/or classify un-expected events. The approach described attempts to integrate the sensor-based analogic method and the neural method into a multiple-level architecture that operates on an analogic world model, so that the action planning can be performed in a smart, reactive way. Given the task, the system builds the world model of the scenario. The reasoning and planning modules act both at the strategic as well as reactive levels, and the activated sensor-based motor strategies handle the sensorial data inputs and drive the robot controller module in the execution of the stream of motor commands. The interaction between the different levels is mainly based on the idea of maintaining and updating in real-time the world model, so that each module can locally operate on specific parts of the whole world model.<>",https://ieeexplore.ieee.org/document/253866/,[1992] Proceedings IEEE International Workshop on Robot and Human Communication, 1992,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCE50029.2021.9467162,ICS Cyber Attack Detection with Ensemble Machine Learning and DPI using Cyber-kit Datasets,IEEE,Conferences,"Digitization has pioneered to drive exceptional changes across all industries in the advancement of analytics, automation, and Artificial Intelligence (AI) and Machine Learning (ML). However, new business requirements associated with the efficiency benefits of digitalization are forcing increased connectivity between IT and OT networks, thereby increasing the attack surface and hence the cyber risk. Cyber threats are on the rise and securing industrial networks are challenging with the shortage of human resource in OT field, with more inclination to IT/OT convergence and the attackers deploy various hi-tech methods to intrude the control systems nowadays. We have developed an innovative real-time ICS cyber test kit to obtain the OT industrial network traffic data with various industrial attack vectors. In this paper, we have introduced the industrial datasets generated from ICS test kit, which incorporate the cyber-physical system of industrial operations. These datasets with a normal baseline along with different industrial hacking scenarios are analyzed for research purposes. Metadata is obtained from Deep packet inspection (DPI) of flow properties of network packets. DPI analysis provides more visibility into the contents of OT traffic based on communication protocols. The advancement in technology has led to the utilization of machine learning/artificial intelligence capability in IDS ICS SCADA. The industrial datasets are pre-processed, profiled and the abnormality is analyzed with DPI. The processed metadata is normalized for the easiness of algorithm analysis and modelled with machine learning-based latest deep learning ensemble LSTM algorithms for anomaly detection. The deep learning approach has been used nowadays for enhanced OT IDS performances.",https://ieeexplore.ieee.org/document/9467162/,2021 8th International Conference on Computer and Communication Engineering (ICCCE),22-23 June 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MetroInd4.0IoT51437.2021.9488447,IOT data-driven experimental process optimisation for kevlar fiberglass components for aeronautic,IEEE,Conferences,"This paper describes the work carried out during the PROOF experiment (IOT data-driven experimental PROcess Optimization for kevlar Fiberglass components for aeronautic), winner of the second open call of the MIDIH EU project. The main objectives of the experiment are the integration of smart sensing devices with the Energy@Work IoT gateways and the development of cloudified innovative data-driven methodologies and data analytics tools to support process optimization in the production of hybrid composite material parts for the aeronautical sector. Collection of real-time production-data from multiple sensors with several industrial protocols and data transfer to the MIDIH project platform has been performed adopting the IoT gateway developed by Energy@Work, following MIDIH reference architecture for advanced data processing and visualization (e.g., Fiware Orion Context Broker, Apache Flink and Fiware Knowage) by using MQTT protocol. Then, historical and new acquired data has been analysed using advanced clustering techniques and trends, with the purpose to allow a novel CPS-based predictive system on the production process. Machine-Learning algorithms and visualisations (GUI based on Fiware Knowage) in real operating conditions have been used to validate the performance and assess the outcome. Finally, thanks to the implementation of specific optimization rules, able to process data gathered from the sensor network, a framework for distributed processing engine has been exploited by (i) generating tips for energy efficiency and process optimization and (ii) providing different type of alarms based on expected consumptions, resulting in concrete support to production managers for the improvement of the whole production value chain.",https://ieeexplore.ieee.org/document/9488447/,2021 IEEE International Workshop on Metrology for Industry 4.0 & IoT (MetroInd4.0&IoT),7-9 June 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPADS.2016.0155,IRMD: Malware Variant Detection Using Opcode Image Recognition,IEEE,Conferences,"Malware detection becomes mission critical as its threats spread from personal computers to industrial control systems. Modern malware generally equips with sophisticated anti-detection mechanisms such as code-morphism, which allows the malware to evolve into many variants and bypass traditional code feature based detection systems. In this paper, we propose to disassemble binary executables into opcodes sequences, and then convert the opcodes into images. By using convolutional neural network to compare the opcode images generated from binary targets with the opcode images generated from known malware sample codes, we can detect if the target binary executables is malicious. Theoretical analysis and real-life experiments results show that malware detection using visualized analysis is comparable in terms of accuracy, our approach can significantly improve 15% of detection accuracy when the detection set contains a large quantity of binaries and the training set is much smaller.",https://ieeexplore.ieee.org/document/7823870/,2016 IEEE 22nd International Conference on Parallel and Distributed Systems (ICPADS),13-16 Dec. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDMW.2017.40,Identifying Irregular Power Usage by Turning Predictions into Holographic Spatial Visualizations,IEEE,Conferences,"Power grids are critical infrastructure assets that face non-technical losses (NTL) such as electricity theft or faulty meters. NTL may range up to 40% of the total electricity distributed in emerging countries. Industrial NTL detection systems are still largely based on expert knowledge when deciding whether to carry out costly on-site inspections of customers. Electricity providers are reluctant to move to large-scale deployments of automated systems that learn NTL profiles from data due to the latter's propensity to suggest a large number of unnecessary inspections. In this paper, we propose a novel system that combines automated statistical decision making with expert knowledge. First, we propose a machine learning framework that classifies customers into NTL or non-NTL using a variety of features derived from the customers' consumption data. The methodology used is specifically tailored to the level of noise in the data. Second, in order to allow human experts to feed their knowledge in the decision loop, we propose a method for visualizing prediction results at various granularity levels in a spatial hologram. Our approach allows domain experts to put the classification results into the context of the data and to incorporate their knowledge for making the final decisions of which customers to inspect. This work has resulted in appreciable results on a real-world data set of 3.6M customers. Our system is being deployed in a commercial NTL detection software.",https://ieeexplore.ieee.org/document/8215672/,2017 IEEE International Conference on Data Mining Workshops (ICDMW),18-21 Nov. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NDS.2017.8070626,"Image-driven, model-free control of repetitive processes based on machine learning",IEEE,Conferences,"An image-driven, model-free approach to design control systems for a large class of industrial process is proposed. A mathematical model of the process is replaced by sequences of subsequent images which play the role of the process (plant) states. The length of this sequences depends on the speed of the process dynamics and on the frame rate. Firstly, a learning sequence of the system states is collected and then, it is used for classifying (clustering) its states. A decision of the control system is attached by an expert to each class (cluster) of the states. At the implementation stage images from a camera in the loop are collected into subsequences corresponding to the system states, then they are classified and a control action corresponding to a class at hand is undertaken. This general idea is exemplified by the case study of a laser power control in an additive manufacturing, which is a repetitive process. A tree-like, hierarchical classifier is proposed in order to recognize the process states, each consisting of three consecutive images. Its performance is tested on real-life images from the process of laser cladding additive manufacturing.",https://ieeexplore.ieee.org/document/8070626/,2017 10th International Workshop on Multidimensional (nD) Systems (nDS),13-15 Sept. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCWAMTIP53232.2021.9674087,Immersive 4D Intelligent Interactive Platform Based on Deep Learning,IEEE,Conferences,"With the advent of the Internet era, the cost of realizing virtual characters has been greatly reduced. The high cost and low efficiency exhibited by traditional virtual technology can&#x0027;t meet social needs. People are seeking fast, convenient and accurate virtual character reproduction technology. Through researching the characteristics of the characters appearance, language habits, voice tone and so on, the research direction of this paper is to simulate and reshape voice and images, construct a cloud platform-based on user-side and client-side, and integrate deep learning, natural language processing, digital twins and other technologies to an immersive 4D intelligent interactive platform. The platform under in the form of application software provides integrated services of intelligent voice interaction and virtual character interaction. In the industrial diagnosis mode, the transition from traditional video retention and voice retention to a new intelligent voice recognition and simulation mode is realized.",https://ieeexplore.ieee.org/document/9674087/,2021 18th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP),17-19 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2MTC43012.2020.9129119,Impact of Noise on Machine Learning-based Condition Monitoring Applications: a Case Study,IEEE,Conferences,"In the paper, application of Machine Learning (ML) techniques for the continuous monitoring of the health status of mild mission-critical industrial equipment is considered. A meaningful real-life case-study is presented in order to show how acquisition conditions may severely impact on the performance of the system. In particular, it is shown that a wrong estimate of noise effects in the deployed system may induce a wrong choice of the best features feeding the ML monitoring algorithm, hence affecting accuracy of the target devices. The discussed results may provide an useful guidance to the practitioner in the field during the design phase of ML-based devices depending of the equipment specifications and environmental conditions.",https://ieeexplore.ieee.org/document/9129119/,2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),25-28 May 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.2005.1612698,Implementation of neural networks in foundation fieldbus environment using standard function blocks,IEEE,Conferences,"This paper presents an implementation approach of artificial neural networks in industrial network environment, through the use of function blocks standardized by field-bus foundation (FF). This enables the implementation of a wide range of applications that involve this mathematical tool, such as intelligent control, failure detection, etc in standard FF system. For validation propose, some examples are presented from of real experiments",https://ieeexplore.ieee.org/document/1612698/,2005 IEEE Conference on Emerging Technologies and Factory Automation,19-22 Sept. 2005,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9412842,Improved anomaly detection by training an autoencoder with skip connections on images corrupted with Stain-shaped noise,IEEE,Conferences,"In industrial vision, the anomaly detection problem can be addressed with an autoencoder trained to map an arbitrary image, i.e. with or without any defect, to a clean image, i.e. without any defect. In this approach, anomaly detection relies conventionally on the reconstruction residual or, alternatively, on the reconstruction uncertainty. To improve the sharpness of the reconstruction, we consider an autoencoder architecture with skip connections. In the common scenario where only clean images are available for training, we propose to corrupt them with a synthetic noise model to prevent the convergence of the network towards the identity mapping, and introduce an original Stain noise model for that purpose. We show that this model favors the reconstruction of clean images from arbitrary real-world images, regardless of the actual defects appearance. In addition to demonstrating the relevance of our approach, our validation provides the first consistent assessment of reconstruction-based methods, by comparing their performance over the MVTec AD dataset [1], both for pixel- and image-wise anomaly detection. Our implementation is available at https://github.com/anncollin/AnomalyDetection-Keras.",https://ieeexplore.ieee.org/document/9412842/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAICA50127.2020.9182590,Improvement of Model Simplification Algorithm Based on LOD and Implementation of WebGL,IEEE,Conferences,"With the continuous progress of computer graphics, virtual reality and other technologies, 3D models in the field of industrial production have become more and more sophisticated, and the triangular surfaces need to be rendered are more than one million, which poses a great challenge to the storage, transmission and computing power of computers. Therefore, in order to adapt to the current performance of the computer while taking into account the rendering effect of the model, the Level-of-Detail(LOD) technology has been spawned. Industrial models tend to have complex structures and need to be displayed accurately in the rendering process, and in the case of a large number of holes in the model, the common algorithm is difficult to maintain the topology of the model well. Therefore, the article uses the edge collapse algorithm. To improve it, an algorithm that uses the mean deviation to guide the simplification process of edge collapse is proposed. While ensuring sufficient triangular mesh simplification, the topology of complex industrial models is maintained.",https://ieeexplore.ieee.org/document/9182590/,2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),27-29 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2015.7125235,Improving accuracy of long-term prognostics of PEMFC stack to estimate remaining useful life,IEEE,Conferences,"Proton Exchange Membrane Fuel cells (PEMFC) are energy systems that facilitate electrochemical reactions to create electrical energy from chemical energy of hydrogen. PEMFC are promising source of renewable energy that can operate on low temperature and have the advantages of high power density and low pollutant emissions. However, PEMFC technology is still in the developing phase, and its large-scale industrial deployment requires increasing the life span of fuel cells and decreasing their exploitation costs. In this context, Prognostics and Health Management of fuel cells is an emerging field, which aims at identifying degradation at early stages and estimating the Remaining Useful Life (RUL) for life cycle management. Indeed, due to prognostics capability, the accurate estimates of RUL enables safe operation of the equipment and timely decisions to prolong its life span. This paper contributes data-driven prognostics of PEMFC by an ensemble of constraint based Summation Wavelet-Extreme Learning Machine (SW-ELM) algorithm to improve accuracy and robustness of long-term prognostics. The SW-ELM is used for ensemble modeling due to its enhanced applicability for real applications as compared to conventional data-driven algorithms. The proposed prognostics model is validated on run-to-failure data of PEMFC stack, which had the life span of 1750 hours. The results confirm capability of the prognostics model to achieve accurate RUL estimates.",https://ieeexplore.ieee.org/document/7125235/,2015 IEEE International Conference on Industrial Technology (ICIT),17-19 March 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/FPL.2017.8056761,In-network online data analytics with FPGAs,IEEE,Conferences,"The growth of sensor technology, communication systems and computation have led to vast quantities of data being available for relevant parties to utilise. Applications such as the monitoring and analysis of industrial equipment, smart surveillance, and fraud detection rely on the `real-time' analysis of time sensitive data gathered from distributed sources. A variety of processing tasks, such as filtering, aggregation, machine learning algorithms, or other transformations to be carried out on this data in order to extract value from it. Centralised computation strategies are often deployed in these scenarios, with the majority of the data being forwarded though the network to a datacenter environment, typically due to the lack of required computational or storage resources at the leaves of the network, and data from other sources or historical data being required. This approach has also traditionally been viewed as more scalable, as resources can be augmented through the addition of extra compute hardware and cloud services.",https://ieeexplore.ieee.org/document/8056761/,2017 27th International Conference on Field Programmable Logic and Applications (FPL),4-8 Sept. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2MTC.2013.6555470,Increase of PLC computability with neural network for recovery of faults in electrical distribution substation,IEEE,Conferences,"Due to the increasing in technological development and modernization of industrial process, control techniques to address high performance are being developed. These not only solve new problems in more complicated plants, but also improve the performance of existing controllers. To improve the control performance of electrical distribution substations, embedded optimization techniques have been developed utilizing the programmable logic controllers (PLC). In this work an industrial artificial neural controller, that is the union between industrial controller and neural network, in associated with an intelligent electronic device (IED) for data acquisition is developed. The PLC execute operational tasks. The neural network controller performs data processing in a MATLAB environment that communicates with the PLC, to receive and to send data, via OPC protocol. The proposed methodology is evaluated in virtual electrical power substations, where the automation devices are: 1)Smart Relays, Remote Transmission Units (RTU) and PLC are real devices and 2) transformers, circuit breakers and capacitor banks simulated with software or hardware.",https://ieeexplore.ieee.org/document/6555470/,2013 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),6-9 May 2013,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRCE50905.2020.9199256,Industrial Implementation and Performance Evaluation of LSD-SLAM and Map Filtering Algorithms for Obstacles Avoidance in a Cooperative Fleet of Unmanned Aerial Vehicles,IEEE,Conferences,"In this paper we present an industrial implementation and performance evaluation of the problem of obstacles detection by drones using autonomous navigation systems. The software module that has been developed as well as the tests conducted are part of a large industrial R&D Vitrociset project called SWARM: an AI-Enabled Command and Control (C&C) system, able to execute and review ISR missions for mini/micro cooperative fleets of heterogeneous UAVs. The presented software module, that is currently under test, has been developed to recognize obstacles and drive correctly the drones, using images acquired by low cost RGB video cameras, whose features of lightness and reduced size allow them to be installed on mini/micro UAVs. Moreover, this setup does not require special calibration and preconfiguration processes like the ones necessary for example using stereo video camera systems. The real-time recognition of obstacles in the surrounding environment has been obtained and evaluated through the implementation, performance evaluation and tests of the LSD-SLAM and map filtering algorithms; the core of the study has been realized starting from the integration of these algorithms with a simulated drone in a synthetic environment. The areas of interest have been identified through the filtering of a computer generated map: the module was then integrated into the SWARM project platform, allowing the control of a single drone's movement and making it ready for use in a cooperative fleet environment.",https://ieeexplore.ieee.org/document/9199256/,2020 3rd International Conference on Intelligent Robotic and Control Engineering (IRCE),10-12 Aug. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAICA52286.2021.9497973,Industrial Internet Security Protection Based on an Industrial Firewall,IEEE,Conferences,"A crucial step in the development of a security system for the industrial Internet is the implementation of an industrial firewall as the first line of defense for the multi-layer defense-in-depth system and an important safeguard for industrial network security. In the design, development, deployment, application, and maintenance of industrial firewalls, the firewall performance and architecture are vital aspects. This thesis focuses on the analysis and discussion of the requirements and abilities of an industrial firewall in terms of adaptability, network isolation, industrial communication protocol identification, filtering and analysis, real-time performance and reliability, and self-protection.",https://ieeexplore.ieee.org/document/9497973/,2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),28-30 June 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CASE48305.2020.9216902,Industrial Robot Grasping with Deep Learning using a Programmable Logic Controller (PLC),IEEE,Conferences,"Universal grasping of a diverse range of previously unseen objects from heaps is a grand challenge in e-commerce order fulfillment, manufacturing, and home service robotics. Recently, deep learning based grasping approaches have demonstrated results that make them increasingly interesting for industrial deployments. This paper explores the problem from an automation systems point-of-view. We develop a robotics grasping system using Dex-Net, which is fully integrated at the controller level. Two neural networks are deployed on a novel industrial AI hardware acceleration module close to a PLC with a power footprint of less than 10 W for the overall system. The software is tightly integrated with the hardware allowing for fast and efficient data processing and real-time communication. The success rate of grasping an object form a bin is up to 95% with more than 350 picks per hour, if object and receptive bins are in close proximity. The system was presented at the Hannover Fair 2019 (world's largest industrial trade fair) and other events, where it performed over 5,000 grasps per event.",https://ieeexplore.ieee.org/document/9216902/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IIHMSP.2010.149,Industrial Workflows Recognition by Computer Vision and AI Technologies,IEEE,Conferences,"The introduction of the Gigabit-Ethernet for Machine Vision opened a new road for workflows recognition in industrial and SMEs environments. The network centric approach to running a production level vision system are coming on stream, which provide distributed computing across industrial developments. Humans and machines interaction can be exploited and surveyed using artificial intelligence algorithms applied on signals from multiple standalone agents on camera networks and giving to the end user, a full view of abnormal or alarm events conditions during the workflows execution. This article is presenting the architecture of such a system, where the designer of the workflows, plus the engineer of the workflow surveillance system should consider, in order either to simulate the industrial process offline, or during the real time workflow execution in the SMEs or in the industrial environment. The related methodology is based on Java technologies, which are presented and latest innovations from the multi agents and workflow processes composition.",https://ieeexplore.ieee.org/document/5636265/,2010 Sixth International Conference on Intelligent Information Hiding and Multimedia Signal Processing,15-17 Oct. 2010,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MetroInd4.0IoT48571.2020.9138310,Infrared Thermography and Image Processing applied on Weldings Quality Monitoring,IEEE,Conferences,"The present paper proposes a methodological approach to evaluate welding quality in the context of tank production. In particular, infrared thermography was adopted to control the structural homogeneity achieved after welding. The analysis was implemented by applying the K-Means algorithm, morphological image processing and artificial neural network based on the Long Short Term Memory - LSTM - technique. The adopted approach was chosen to perform different image post-processing analysis and therefore highlights, identifies, and quantifies welding inhomogeneities, as cracks and defects. The work was developed within the research framework of an industrial project. The proposed approach could be implemented in inline production systems which integrate an artificial intelligence processor for real time quality monitoring.",https://ieeexplore.ieee.org/document/9138310/,2020 IEEE International Workshop on Metrology for Industry 4.0 & IoT,3-5 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPHYS.2018.8387672,Integrated IPC for data-driven fault detection,IEEE,Conferences,"Condition monitoring and fault detection are of critical importance in modern industrial processes for efficient and productive machine operation. Early and accurate detection of faults facilitates in the efficient and secure operation of the plant by preventing losses due to machinery breakdown. In this study, we present a novel and rapid fault detection methodology using an integrated Industrial PC. The approach is based on integrating existing hardware components and software libraries for efficient application of machine learning algorithms to an industrial process. The Industrial PC is integrated within the fieldbus system of a Bulk Good Laboratory Plant providing superior network security as compared to an external connection to a cloud platform. Additionally, the Industrial PC provides predominantly better numerical computation functionality as compared to traditional PLC environment due to the availability of machine learning libraries in high-level languages. The fault detection procedure is accomplished with the Principal Component Analysis dimensionality reduction algorithm along with Hotelling's T2 statistic. A variety of sensor fault cases pertinent to the Bulk Good Laboratory Plant are analysed and experimental results show that all fault cases were detected with low detection delays. The time required to detect faults reflects the real-time capabilities of the system in an industrial scenario.",https://ieeexplore.ieee.org/document/8387672/,2018 IEEE Industrial Cyber-Physical Systems (ICPS),15-18 May 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/APPEEC.2010.5448235,Integrated Modeling and Simulating of the Three-axis Turbine Power Generation based on the Neural Network Identification,IEEE,Conferences,"Gas turbine units are used in industrial applications more and more widely, and many people have researched widely in the fault diagnosis technology and the control technology for the gas turbine units. Simulation of Gas Turbine technology is the basis and the mathematical model of real-time is the basis of all relevant digital simulation. Using the neural network recognition technology of the radial basis function and the curve-fitting technology of part-characteristics, the mathematical model of three-axis gas turbine is established. At the same time the simulation model is established based on Matlab/simulink software. The dynamic simulation of the gas turbine for the burden loading and reducing has been researched and the response of for the output speed and fuel capacity is obtained. It is shown that the model of the gas turbine has the fast learning speed, high sampling rate and high accuracy.",https://ieeexplore.ieee.org/document/5448235/,2010 Asia-Pacific Power and Energy Engineering Conference,28-31 March 2010,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.1988.65409,Integrated architecture for intelligent control,IEEE,Conferences,"A novel architecture for controlling and managing large-scale intelligent systems is presented, in which the different expert systems and numerical computation routines are coordinated by a metasystem. These expert systems and numerical routines may be written in different languages or programming tools, debugged and used separately. In this way, it is possible to easily add new programs and reduce the scope of rule search to enhance efficiency. Moreover, the optimal solution can be selected among the conflict results, and parallel processing become feasible in the integrated intelligent system. It is concluded that this architecture can serve as a universal configuration to develop high-performance intelligent systems for many complicated industrial applications in real-world domains.<>",https://ieeexplore.ieee.org/document/65409/,Proceedings IEEE International Symposium on Intelligent Control 1988,24-26 Aug. 1988,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MoRSE48060.2019.8998694,Integration of Blockchains with Management Information Systems,IEEE,Conferences,"In the era of the fourth industrial revolution (In-dustry 4.0), many Management Information Systems (MIS) integrate real-time data collection and use technologies such as big data, machine learning, and cloud computing, to foster a wide range of creative innovations, business improvements, and new business models and processes. However, the integration of blockchain with MIS offers the blockchain trilemma of security, decentralisation and scalability. MIS are usually Web 2.0 client-server applications that include the front end web systems and back end databases; while blockchain systems are Web 3.0 decentralised applications. MIS are usually private systems that a single party controls and manages; while blockchain systems are usually public, and any party can join and participate. This paper clarifies the key concepts and illustrates with figures, the implementation of public, private and consortium blockchains on the Ethereum platform. Ultimately, the paper presents a framework for building a private blockchain system on the public Ethereum blockchain. Then, integrating the Web 2.0 client-server applications that are commonly used in MIS with Web 3.0 decentralised blockchain applications.",https://ieeexplore.ieee.org/document/8998694/,"2019 International Conference on Mechatronics, Robotics and Systems Engineering (MoRSE)",4-6 Dec. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SDS.2018.8370412,Intel Optane™ technology as differentiator for internet of everything and fog computing,IEEE,Conferences,"Traditional network and cloud solutions cannot address effectively the infrastructure and data architecture challenges introduced by the emergence of the Internet of Everything (IoE). IoE creates unprecedented volume of complex data which must be stored, transferred, processed and analyzed in real time for time sensitive applications. Indeed, most of the applications in IoE domain (autonomous driving, health monitoring, banking, industrial control systems ...) are time and quality of service sensitive and need new solutions for data integrity and data availability for successful decision. At Intel, we are working on new computer paradigm and storage technologies such as Intel® Optane™ SSDs and Intel® 3D NAND SSDs that can address some of these challenges. With Intel® Optane™, it is now possible to create a high performance and real time available Software-Defined Infrastructure (SDI) by enabling full dis-aggregate and pool of the underlying hardware resources, creating distributed memory/storage imperatives and giving research community and enterprise the performance and capabilities to benefit from revolutionary technologies such as Fog Computing and Artificial Intelligence by dynamically assigning compute, storage and network resource in real-time time sensitive workloads. In this keynote, we will discuss data storage, availability, integrity, quality of service and movement in this compute paradigm shift. We will also touch on how it impacts mobile edge, fog computing and cloud infrastructures. We will then conclude with the challenges and opportunities this new solution will bring.",https://ieeexplore.ieee.org/document/8370412/,2018 Fifth International Conference on Software Defined Systems (SDS),23-26 April 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MSN48538.2019.00085,Intelligent Log Analysis System for Massive and Multi-Source Security Logs: MMSLAS Design and Implementation Plan,IEEE,Conferences,"In the Internet of Things and industrial controlnetwork servers, a large number of logs will be formed everymoment. This log information, as an important basis for eventrecording and security auditing, provides important informa-tion for identifying threat sources, identifying threat degreeand judging threat impact. However, the current security loganalysis system usually only standardizes the logs separately, and lacks the correlation analysis of the information fromvarious sources. Thus, this paper presents an intelligent loganalysis system for massive and multi-source security logs-MMSLAS(Massive and Multi-Source Security Log AnalysisSystem). In the log analysis module, the system integratesbusiness rule analysis and behavior analysis and additionallyadopts a machine learning-based analysis method, which fullyexploits the correlation between security logs and realizes thecomprehensive analysis of multi-source security logs. At thesame time, the distributed architecture scheme is also sufficientto cope with the system load caused by a large amount ofdata. The final implementation results show that MMSLAScan quickly locate the improper behavior in the log, and detectthe abnormal requests in advance according to the analysis ofthe behavior trajectory.",https://ieeexplore.ieee.org/document/9066044/,2019 15th International Conference on Mobile Ad-Hoc and Sensor Networks (MSN),11-13 Dec. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIN51074.2021.9385543,Intelligent Monitoring of IoT Devices using Neural Networks,IEEE,Conferences,"The Internet of Things (IoT) has seen expeditious growth in recent times with 7 billion connected devices in 2020, thus leading to the vital importance of real-time monitoring of IoT devices. Through this paper, we demonstrate the idea of building a cloud-native application to monitor smart home devices. The application intends to provide valuable performance metrics from the perspective of end-users and react to anomalies in real-time. In this demo paper, we conduct the demonstration using Autoencoder (an unsupervised technique) based Deep Neural Networks (DNNs) to learn the normal operating conditions of power consumption of smart devices. When an anomaly is detected, the DNNs take proactive action and send appropriate commands back to the device. In addition, the users are provided with a real-time graphical representation of power consumption. This will help to save electricity on a domestic as well as industrial level. Finally, we discuss the future prospects of monitoring IoT devices.",https://ieeexplore.ieee.org/document/9385543/,"2021 24th Conference on Innovation in Clouds, Internet and Networks and Workshops (ICIN)",1-4 March 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GCCE.2014.7031179,Intelligent PID control based on PSO type NN for USM,IEEE,Conferences,"Ultrasonic Motors (USMs) are a kind of actuators with attractive features. Owing to the features, they are expected to be applied widely in industrial fields. Especially, since they work well in or near MRI environment, they are expected to play more important roles in medical and welfare area. In this research, for the control of USM, an intelligent PID control method using Neural Network (NN) combined with type Particle Swarm Optimization (PSO) is developed. In the method, the intelligent controller is designed based on variable gain type PID control using NN. The learning of the NN unit is implemented by the PSO. The PID gains are adjusted by the intelligent controller in real-time environment. The effectiveness of the method is confirmed by experiments.",https://ieeexplore.ieee.org/document/7031179/,2014 IEEE 3rd Global Conference on Consumer Electronics (GCCE),7-10 Oct. 2014,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCECE.2009.5090245,Intelligent speed controllers for IPM motor drives,IEEE,Conferences,"In this paper the comparative performances of the interior permanent magnet synchronous motor (IPMSM) drive system using proportional integral (PI) controller, proportional integral derivative (PID) controller, adaptive neural network (NN) controller, and wavelet based multiresolution proportional integral derivative (MRPID) controller are presented. In the proposed wavelet based MRPID controller, the discrete wavelet transform is used to decompose the error between actual and command speeds into different frequency components at various scales. The wavelet transformed coefficients of different scales are scaled by their respective gains, and then are added together to generate the control signal. The performances of the IPMSM drive system are investigated in simulation and experiments at different dynamic operating conditions. The vector control scheme of the conventional and proposed speed controllers based IPMSM drive system is successfully implemented in real-time using the digital signal processor board ds1102 on the laboratory 1-hp IPMSM. The simulation and laboratory test results confirm the superiority of the proposed wavelet based MRPID controller over the conventional speed controllers for wide spread applications in high performance industrial motor drive systems.",https://ieeexplore.ieee.org/document/5090245/,2009 Canadian Conference on Electrical and Computer Engineering,3-6 May 2009,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIMCA.2006.133,International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce - Title,IEEE,Conferences,"The following topics are dealt with: intelligent agents and ontologies; data mining, knowledge discovery and decision making; intelligent systems; Web technologies and Web services; virtual reality and games; image processing and image understanding techniques; adaptive control and automation; modelling, prediction and control; multi-agent systems and computational intelligence; agent systems, personal assistant agents and profiling; fuzzy systems for industrial automation; control strategies; neural network applications; clustering, classification, data mining and risk analysis; dynamics systems; innovative control systems, hardware design and implementation; robotics and automation; e-business, e-commerce, innovative Web applications; Web databases; diagnosis and medical applications; learning systems; optimization, hybrid systems, genetic algorithms and evolutionary computation control applications; online learning and ERP; knowledge acquisition and classification; nanomechatronics; simulation and control; mobile network applications; information retrieval; Bayesian networks; human computer interaction; cognitive science; mobile agents; knowledge management; intelligent control; e-search and navigation; security.",https://ieeexplore.ieee.org/document/4052645/,2006 International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce (CIMCA'06),28 Nov.-1 Dec. 2006,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WF-IoT51360.2021.9595035,Interpretable Multi-Step Production Optimization Utilizing IoT Sensor Data,IEEE,Conferences,"In an industrial manufacturing process, such as petroleum, chemical, and food processing, with the deployment of thousands of sensors in the plants, we have the chance to provide real-time onsite management for the processes. Beyond the real-time status update, utilizing vast IoT data and creating machine learning and optimization models provide us with intelligent business recommendations. Those are used by the site engineers and managers to make real-time decisions in a situation with multiple conflicting operational and business goals. Those goals include maximizing financial gain, minimizing costs, limiting the usage of certain raw materials or additives, decreasing environmental impact, and more. When formalizing these decision-making tasks, often there is no prior knowledge of compromise between the conflicting goals. That poses a challenge to generate a proper objective function. In this paper, we create a Multi-Step optimization process to address this uncertainty of selecting proper objectives and their preferences. Instead of using an explicit trade-off to create a single weighted objective function (as a traditional approach) and rely on a single attempt to find the optimal solution, we decompose this problem into multiple steps. In each step, we optimize only one objective from one KPI with an exact semantic meaning. We demonstrate the usability of the approach using a practical application from an oil sands processing facility, provide modeling results focusing on the response to business priorities, performance, and interpretability. The multi-step approach presents the convergence of the target goal with an outcome KPI with comparison for each step to illustrate the enhanced interpretability.",https://ieeexplore.ieee.org/document/9595035/,2021 IEEE 7th World Forum on Internet of Things (WF-IoT),14 June-31 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSCS52333.2021.9497411,Inverted Pendulum Control with a Robotic Arm using Deep Reinforcement Learning,IEEE,Conferences,"Inverted pendulum control is a benchmark control problem that researchers have used to test the new control strategies over the past 50 years. Deep Reinforcement Learning Algorithm is used recently on the inverted pendulum on a straightforward form. The inverted pendulum had only one degree of freedom and was moving on a plane. This paper demonstrates a successful implementation of a deep reinforcement learning algorithm on an inverted pendulum that rotates freely on a spherical joint with an industrial 6 degrees freedom robot arm. This research used the Deep Reinforcement Learning algorithm in Robot Operating System (ROS) and Gazebo Simulation. Experimental results show that the proposed method achieved promising outputs and reaches the control objectives. We were able to control the inverted pendulum upward for 30 and 20 seconds in two case studies. Two other significant novelties in this research are using an inertial measurement unit (IMU) on the tip of the pendulum, that will facilitate implementation on the real robot for future work and different reward functions in comparing to past publications that enable continuous learning and mastering control in a vertical position",https://ieeexplore.ieee.org/document/9497411/,"2021 International Symposium on Signals, Circuits and Systems (ISSCS)",15-16 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PARC49193.2020.236616,IoT and Cloud Computing based Smart Water Metering System,IEEE,Conferences,"This paper focuses on the developmental and implementation methodology of smart water meter based on Internet of Things (IoT) and Cloud computing equipped with machine learning algorithms, to differentiate between normal and excessive water usage at industrial, domestic and all other sectors having an abundance of water usage, both for Indian and worldwide context. Recognizing that intelligent metering of water has the potential to alter customer engagement of water usage in urban and rural water supplies, this paper fosters for sustainable water management, a need of the present. With shrinking reserves of clean water resources worldwide, it is becoming cumbersome to cater for this resource to masses in the coming years on a consistent basis. Using our smart water meter, water resources can be managed efficiently and an optimum use could save water for the future generations. Sensors will provide for real time monitoring of hydraulic data, automated control and alarming from Cloud platform in case of events such as water leakages, excessive usage, etc. Analysis of the same will help in taking meaningful actions. Thus we do propose for a smart water metering technology that can be utilized by Indian citizens, and worldwide, to curb wastage of water. With an ease of monitoring and visualization of the data through the Cloud platform combined with machine learning based tools to detect excess water consumption, the server-less architecture we propose can be easily adopted and implemented in a large scale.",https://ieeexplore.ieee.org/document/9087024/,2020 International Conference on Power Electronics & IoT Applications in Renewable Energy and its Control (PARC),28-29 Feb. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCGrid51090.2021.00075,IoTwins: Design and Implementation of a Platform for the Management of Digital Twins in Industrial Scenarios,IEEE,Conferences,"With the increase of the volume of data produced by IoT devices, there is a growing demand of applications capable of elaborating data anywhere along the IoT-to-Cloud path (Edge/Fog). In industrial environments, strict real-time constraints require computation to run as close to the data origin as possible (e.g., IoT Gateway or Edge nodes), whilst batch-wise tasks such as Big Data analytics and Machine Learning model training are advised to run on the Cloud, where computing resources are abundant. The H2020 IoTwins project leverages the digital twin concept to implement virtual representation of physical assets (e.g., machine parts, machines, production/control processes) and deliver a software platform that will help enterprises, and in particular SMEs, to build highly innovative, AI-based services that exploit the potential of IoT/Edge/Cloud computing paradigms. In this paper, we discuss the design principles of the IoTwins reference architecture, delving into technical details of its components and offered functionalities, and propose an exemplary software implementation.",https://ieeexplore.ieee.org/document/9499575/,"2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)",10-13 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PerComWorkshops51409.2021.9430865,Keyword Spotting for Industrial Control using Deep Learning on Edge Devices,IEEE,Conferences,"Spoken commands promise unique advantages for the control of industrial machinery. Operators are enabled to keep their eyes on safety critical aspects of the process at all times and are free to use their hands in other parts of the process, instead of remote control. Current keyword spotting systems are prone to misunderstanding spoken utterances, especially in noisy environments, and are commonly deployed as non-realtime cloud services. Consequently, these systems can not be trusted with safety critical industrial control. We adapt a DS-CNN and a CNN for keyword spotting and use augmented training data, including real industrial noise, to increase their robustness. Furthermore, we apply post-training quantization and analyze the performance of both networks using multiple embedded systems, including a Google Edge TPU. We carry out a systematic analysis of accuracies, memory footprint and inference times using different combinations of data augmentations, hardware platforms, and quantizations. We show that augmented training data increases the inference accuracy in noisy environments by up to 20 %. Among others, this is demonstrated using an integer quantized network with a memory footprint of 0.57 MByte, reaching inference speeds of less than 5 ms on an embedded CPU and less than 1 ms on the Edge TPU. The results show that keyword spotting for industrial control is feasible on embedded systems and that the training data augmentation has a significant impact on the robustness in challenging environments.",https://ieeexplore.ieee.org/document/9430865/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ACC.1988.4789792,Knowledge-Based Approach to Real-Time Supervisory Control,IEEE,Conferences,"Increasing complexity of industrial plants necessitates the usage of advanced programming techniques in process control. Artificial Intelligence programming offers new opportunities in constructing complex software systems. This paper describes an approach which has been used to build Intelligent Process Control System (IPCS). IPCS expand the conventional process control systems with an additional, ""knowledge-based layer"", where declarative programming methods are used extensively. The knowledge-based components represent a model of the process from multiple viewpoints and the simulator, monitor, control, diagnostics and operator interface subsystems are coupled to a symbolic process database. A special architecture, the Multigraph Architecture has been used as a general framework for integrating symbolic and numeric programming techniques in a distributed environment. The knowledge-based components have been implemented as ""Autonomous Communicating Objects"". The application of concurrent programming techniques made possible the introduction of new approaches in the design of the monitoring and diagnostic system. A prototype IPCS has been implemented on a network of VAX computers.",https://ieeexplore.ieee.org/document/4789792/,1988 American Control Conference,15-17 June 1988,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2015.7281881,Knowledge-driven finite-state machines. Study case in monitoring industrial equipment,IEEE,Conferences,Traditionally state machines are implemented by coding the desired behavior of a given system. This work proposes the use of ontological models to describe and perform computations on state machines by using SPARQL queries. This approach represents a paradigm shift relating to the customary manner in which state machines are stored and computed. The main contribution of the work is an ontological model to represent state machines and a set of generic queries that can be used in any knowledge-driven state machine to compute valuable information. The approach was tested in a study case were the state machines of industrial robots in a manufacturing line were modeled as ontological models and used for monitoring the behavior of these devices on real time.,https://ieeexplore.ieee.org/document/7281881/,2015 IEEE 13th International Conference on Industrial Informatics (INDIN),22-24 July 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2019.00655,L3-Net: Towards Learning Based LiDAR Localization for Autonomous Driving,IEEE,Conferences,"We present L3-Net - a novel learning-based LiDAR localization system that achieves centimeter-level localization accuracy, comparable to prior state-of-the-art systems with hand-crafted pipelines. Rather than relying on these hand-crafted modules, we innovatively implement the use of various deep neural network structures to establish a learning-based approach. L3-Net learns local descriptors specifically optimized for matching in different real-world driving scenarios. 3D convolutions over a cost volume built in the solution space significantly boosts the localization accuracy. RNNs are demonstrated to be effective in modeling the vehicle's dynamics, yielding better temporal smoothness and accuracy. We comprehensively validate the effectiveness of our approach using freshly collected datasets. Multiple trials of repetitive data collection over the same road and areas make our dataset ideal for testing localization systems. The SunnyvaleBigLoop sequences, with a year's time interval between the collected mapping and testing data, made it quite challenging, but the low localization error of our method in these datasets demonstrates its maturity for real industrial implementation.",https://ieeexplore.ieee.org/document/8954371/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TELE52840.2021.9482484,Learning Activities for Mastering Flexible Roof Constructions,IEEE,Conferences,"Flexible roofs become an integral part of modern building constructions. Being widely used for public buildings, they have great potential in the field of industrial construction as well. Flexible roofs, however, possess complex structural behavior. They require deep understanding of the key concepts by designers and project managers, who implement them into a real life. Civil engineering and architectural specializations need to incorporate flexible constructions into the academic activity. It will ensure competitiveness of prospective specialists in the job market. Active learning approach is considered for mastering flexible roof structures. Prototyping is an essential tool for long-term memorization of the key concepts. Modern computer technologies and specialized software packages, however, gradually displace laboratory modeling. Structural deformability, geometrically nonlinear behavior, preliminary stressing and equilibrium structural shape can be easily grasped by means of computer simulation and numerical investigation of exemplary constructions.",https://ieeexplore.ieee.org/document/9482484/,2021 1st International Conference on Technology Enhanced Learning in Higher Education (TELE),24-25 June 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636547,Learning Contact-Rich Assembly Skills Using Residual Admittance Policy,IEEE,Conferences,"Contact-rich assembly tasks may result in large and unpredictable forces and torques when the locations of the contacting parts are uncertain. The ability to correct the trajectory in response to haptic feedback and accomplish the task despite location uncertainties is an important skill. We hypothesize that this skill would facilitate generalization and support direct transfer from simulations to real world. To reduce sample complexity, we propose to learn a residual admittance policy (RAP). RAP is learned to correct the movements generated by a baseline policy in the framework of dynamic movement primitives. Given the reference trajectories generated by the baseline policy, the action space of RAP is limited to the admittance parameters. Using deep reinforcement learning, a deep neural network is trained to map task specifications to proper admittance parameters. We demonstrate that RAP handles uncertainties in board location, generalizes well over space, size and shape, and facilitates quick transfer learning. Most impressively, we demonstrate that the policy learned in simulations achieves similar robustness to uncertainties, generalization and performance when deployed on an industrial robot (UR5e) without further training. See accompanying video for demonstrations.",https://ieeexplore.ieee.org/document/9636547/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASE51524.2021.9678698,Learning Domain-Specific Edit Operations from Model Repositories with Frequent Subgraph Mining,IEEE,Conferences,"Model transformations play a fundamental role in model-driven software development. They can be used to solve or support central tasks, such as creating models, handling model co-evolution, and model merging. In the past, various (semi-)automatic approaches have been proposed to derive model transformations from meta-models or from examples. These approaches require time-consuming handcrafting or the recording of concrete examples, or they are unable to derive complex transformations. We propose a novel unsupervised approach, called Ockham, which is able to learn edit operations from model histories in model repositories. Ockham is based on the idea that meaningful domain-specific edit operations are the ones that compress the model differences. It employs frequent subgraph mining to discover frequent structures in model difference graphs. We evaluate our approach in two controlled experiments and one real-world case study of a large-scale industrial model-driven architecture project in the railway domain. We found that our approach is able to discover frequent edit operations that have actually been applied before. Furthermore, Ockham is able to extract edit operations that are meaningful to practitioners in an industrial setting.",https://ieeexplore.ieee.org/document/9678698/,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),15-19 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MSR52588.2021.00019,Learning Off-By-One Mistakes: An Empirical Study,IEEE,Conferences,"Mistakes in binary conditions are a source of error in many software systems. They happen when developers use, e.g., `<;' or `>' instead of `<;=' or `>='. These boundary mistakes are hard to find and impose manual, labor-intensive work for software developers. While previous research has been proposing solutions to identify errors in boundary conditions, the problem remains open. In this paper, we explore the effectiveness of deep learning models in learning and predicting mistakes in boundary conditions. We train different models on approximately 1.6M examples with faults in different boundary conditions. We achieve a precision of 85% and a recall of 84% on a balanced dataset, but lower numbers in an imbalanced dataset. We also perform tests on 41 real-world boundary condition bugs found from GitHub, where the model shows only a modest performance. Finally, we test the model on a large-scale Java code base from Adyen, our industrial partner. The model reported 36 buggy methods, but none of them were confirmed by developers.",https://ieeexplore.ieee.org/document/9463090/,2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),17-19 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.2019.8869172,Learning based Probabilistic Model for Migration of Industrial Control Systems,IEEE,Conferences,"The updating and upgrading of control systems is a cumbersome, expensive and time consuming task. From a software perspective, control system migration is a collective task of migrating the control logic, Human Machine Interface (HMI) and auxiliary software applications. Migrating control logic is the most challenging task owing to constraints on hard real-time behavior and execution order. Control logic typically contains engineering artifacts that specify the functionality of industrial devices taking into account various parameters. Therefore, to migrate from one Distributed Control System (DCS) system to another or to upgrade the existing DCS, one needs to map the source control entity and their parameters to the appropriate control entities in the target DCS.In this paper, we propose a machine learning based suggestion management system that identifies control entities and parameters for a source DCS and suggests the use of similar control entities and corresponding parameters for the target DCS. This in effect saves effort required in mapping of control parameters and reduces the dependence on subject matter experts. Our system uses a probabilistic approach to find these similarity mappings based on meta-data stored in an Ontology. We further describe a case study implemented for mapping heritage and legacy systems to a modern control system to verify and validate our approach.",https://ieeexplore.ieee.org/document/8869172/,2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),10-13 Sept. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9811592,Learning to Detect Slip with Barometric Tactile Sensors and a Temporal Convolutional Neural Network,IEEE,Conferences,"The ability to perceive object slip via tactile feedback enables humans to accomplish complex manipulation tasks including maintaining a stable grasp. Despite the utility of tactile information for many applications, tactile sensors have yet to be widely deployed in industrial robotics settings; part of the challenge lies in identifying slip and other events from the tactile data stream. In this paper, we present a learning-based method to detect slip using barometric tactile sensors. These sensors have many desirable properties including high durability and reliability, and are built from inexpensive, off-the-shelf components. We train a temporal convolution neural network to detect slip, achieving high detection accuracies while displaying robustness to the speed and direction of the slip motion. Further, we test our detector on two manipulation tasks involving a variety of common objects and demonstrate successful generalization to real-world scenarios not seen during training. We argue that barometric tactile sensing technology, combined with data-driven learning, is suitable for many manipulation tasks such as slip compensation.",https://ieeexplore.ieee.org/document/9811592/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASE51524.2021.9678533,Learning-based Assistant for Data Migration of Enterprise Information Systems,IEEE,Conferences,"Data migration from source to target information system is a critical step for modernizing information systems. Central to data migration is data transform that transforms the source system data into target system. In this paper we present a tool that assists the experts in creating the data transformation specification by (a) suggesting candidate field matches between the source and target data models using machine learning and knowledge representation, and (b) rules for the data transformation using program synthesis. It takes the expert’s feedback for the identified matches and synthesized rules and proposes new matches and transformation rules. We have executed our tool on real-life industrial data. Our schema matching recall at 5 is 0.76, while for the rule generator recall at 2 is 0.81.",https://ieeexplore.ieee.org/document/9678533/,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),15-19 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN45523.2021.9557472,Learning-based Co-Design of Distributed Edge Sensing and Transmission for Industrial Cyber-Physical Systems,IEEE,Conferences,"Industrial cyber-physical systems (ICPS) refer to an emerging generation of intelligent systems, where distributed data acquisition is of great importance and is influenced by data transmission. In the improvement of the overall performance of sensing accuracy and energy efficiency, sensing and transmission are tightly coupled. Due to the unknown transmission channel states in the harsh industrial field environment, intelligently performing sensor scheduling for distributed sensing is challenging. In this paper, edge computing technology is utilized to enhance the level of intelligence at the edge side and deploy advanced scheduling algorithms. We propose a learning-based distributed edge sensing-transmission co-design (LEST) algorithm under the coordination of the sensors and the edge computing unit (ECU). Deep reinforcement learning is applied to perform real-time sensor scheduling under unknown channel states. The conditions for the existence of feasible scheduling policies are analyzed. The proposed algorithm is applied to estimate the slab temperature in the hot rolling process, which is a typical ICPS. The simulation results demonstrate that the overall performance of LEST is better than other suboptimal algorithms.",https://ieeexplore.ieee.org/document/9557472/,2021 IEEE 19th International Conference on Industrial Informatics (INDIN),21-23 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.2018.8502485,Linear Classification of Badly Conditioned Data,IEEE,Conferences,"We present a method for the fast and robust linear classification of badly conditioned data. In our considerations, badly conditioned data are such data which are numerically difficult to handle. Due to, e.g. a large number of features or a large number of objects representing classes as well as noise, outliers or incompleteness, the common software computation of the discriminating linear combination of features between classes fails or is extremely time consuming. The theoretical foundations of our approach are based on the single feature ranking, which allows fast calculation of the approximative initial classification boundary. For the increasing of classification accuracy of this boundary, the refinement is performed in the lower dimensional space. Our approach is tested on several datasets from UCI Reposi-tiory. Experimental results indicate high classification accuracy of the approach. For the modern real industrial applications such a method is especially suitable in the Cyber-Physical-System environments and provides a part of the workflow for the automated classifier design.",https://ieeexplore.ieee.org/document/8502485/,2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA),4-7 Sept. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2012.137,MAST: From a Toy to Real-Life Manufacturing Control,IEEE,Conferences,"Paper reports on the evolution of agent-based simulation and control system called MAST. The system was designed originally for agent-based simulation of product routing but over the years matured into a generic purpose manufacturing simulation and control tool featuring real-time connectivity to legacy PLCs, ontology-based dynamic scheduling, advanced diagnostics, etc. Paper describes MAST architecture, behavior of agents and capabilities for dynamic reconfiguration. In addition, two examples of application of MAST to real problems from manufacturing domain are given. The latest trend of exploitation of semantic technologies in industrial agents is discussed.",https://ieeexplore.ieee.org/document/6299316/,"2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",8-10 Aug. 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR52153.2021.00038,Machine Learning Concepts for Dual-Arm Robots within Virtual Reality,IEEE,Conferences,"The collaboration between humans and artificial intelligence (AI) driven robots lay the foundations for new approaches in industrial production. However, intensive research is required to develop machine learning behavior that is not only able to execute shared tasks but also acts following the expectations of the human partner. Rigid setups and restrictive safety measures deny the acquisition of adequate training samples to build general-purpose machine learning solutions for evaluation within experimental studies. Based on established research that trains AI systems within simulated environments, we present a machine learning implementation that enables the training of a dual-arm robot within a virtual reality (VR) application. Building upon preceding research, an activity diagram for a shared task for the machine learning model to learn, was conceptualized. A first approach, using vector distances, led to flawed results, whereas a revised solution based on collision boxes resulted in a stable outcome. While the implementation of the machine learning model is fixed on the activity diagram of the shared task, the presented approach is expandable as a universal platform for evaluating Human-Robot Collaboration (HRC) scenarios in VR. Future iterations of this VR sandbox application can be used to explore optimal workplace arrangements and procedures with autonomous industrial robots in a wide range of possible scenarios.",https://ieeexplore.ieee.org/document/9644376/,2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),15-17 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TELSIKS52058.2021.9606379,Machine Learning for Air Quality Classification in IoT-based Network with Low-cost Sensors,IEEE,Conferences,"Air pollution is a rising problem, with its effects being especially severe in urban and industrial areas. A constant and local monitoring of air quality, and the suitable presentation of the results to population, demands deployment of large-scale IoT-based monitoring networks in which low-cost, low-quality sensors would be predominantly used. However, the inherent measurement errors could incur large AQI (Air Quality Index) calculation error. Also, appropriate presentation of air pollution demands that measurements of air pollutants&#x2019; concentrations are classified into Air Quality Classes, thus making the classification task for AQI of large interest. In this paper we analyzed a wide variety of Machine Learning (ML) and Deep Learning (DL) models in order to solve classification task for AQI, but under the assumption of low-cost sensor deployment in the real-world application. The results of comprehensive analysis suggest that DL models designed, optimized and tested in this paper present a viable and the most suitable solution under these demands.",https://ieeexplore.ieee.org/document/9606379/,"2021 15th International Conference on Advanced Technologies, Systems and Services in Telecommunications (TELSIKS)",20-22 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICC40277.2020.9148684,Machine Learning for Predictive Diagnostics at the Edge: an IIoT Practical Example,IEEE,Conferences,"Edge Computing is becoming more and more essential for the Industrial Internet of Things (IIoT) for data acquisition from shop floors. The shifting from central (cloud) to distributed (edge nodes) approaches will enhance the capabilities of handling real-time big data from IoT. Furthermore, these paradigms allow moving storage and network resources at the edge of the network closer to IoT devices, thus ensuring low latency, high bandwidth, and location-based awareness. This research aims at developing a reference architecture for data collecting, smart processing, and manufacturing control system in an IIoT environment. In particular, our architecture supports data analytics and Artificial Intelligence (AI) techniques, in particular decentralized and distributed hybrid twins, at the edge of the network. In addition, we claim the possibility to have distributed Machine Learning (ML) by enabling edge devices to learn local ML models and to store them at the edge. Furthermore, edges have the possibility of improving the global model (stored at the cloud) by sending the reinforced local models (stored in different shop floors) towards the cloud. In this paper, we describe our architectural proposal and show a predictive diagnostics case study deployed in an edge-enabled IIoT infrastructure. Reported experimental results show the potential advantages of using the proposed approach for dynamic model reinforcement by using real-time data from IoT instead of using an offline approach at the cloud infrastructure.",https://ieeexplore.ieee.org/document/9148684/,ICC 2020 - 2020 IEEE International Conference on Communications (ICC),7-11 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WFCS53837.2022.9779183,Machine Learning for Zero- Touch Management in Heterogeneous Industrial Networks - A Review,IEEE,Conferences,"Over the past decades industrial communication networks have evolved into highly diverse and heterogeneous environments, with a variety of different technologies being deployed to address the diverse requirements of manufacturing-and automation-specific use cases. These include stringent latency limits, high availability and reliability, as well as deterministic communication behavior. To assure the necessary allocation of re-sources and provisioning of required Quality-of-Service in highly diverse communication systems, a holistic network management approach is needed that can serve all cornerstones of modern industrial networks. More recently, this lead to the development of new adaptive and agile management approaches that imple-ment autonomous and self-organizing manufacturing networks, whereby Machine Learning (ML) methods started to become an integral part for overcoming the limiting factors of practically deploying such systems. Due to the growing complexity of today&#x0027;s networking environments, defining network management policies based on expert knowledge becomes increasingly difficult. ML has evolved as a promising technique to extract knowledge from collected data to enable cognitive network management approaches. This paper reviews past advances in ML applications for zero touch management of heterogeneous industrial communication networks. It illustrates how a network&#x0027;s management life-cycle that is based on digital twin technology can harnesses the potentials of ML to bring the concepts of organic computing and zero-touch cognitive manufacturing within industrial networks closer to reality. Lastly, recent papers that discuss the use of ML approaches for self-x features in Zero-Touch Management (ZTM) network environments are surveyed and relevant open issues are discussed.",https://ieeexplore.ieee.org/document/9779183/,2022 IEEE 18th International Conference on Factory Communication Systems (WFCS),27-29 April 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECIT54077.2021.9641210,Machine Learning-Based Sensor Drift Fault Classification using Discrete Cosine Transform,IEEE,Conferences,"In fourth industrial revolution sensors are playing a crucial role as they provide a real time health and operating conditions of a physical system through continuous real time data. With this real time data in hand, robust and intelligent condition monitoring systems are being implemented through machine learning and deep learning techniques. However, the reliability of the sensor data is critical for good condition monitoring system. Many sensors operate in harsh environment; thus, the sensor signals can be affected by various faults. In this paper we propose a hybrid approach of sensor drift fault classification which involves discrete cosine transform (DCT) for extracting features from time domain sensor signals and in the later stage machine learning algorithms are used to build classification models. Experimental results showed that our proposed DCT-based feature selection method when combined with common machine learning models can exhibit excellent classification accuracy which is above 97% for most common machine learning models.",https://ieeexplore.ieee.org/document/9641210/,"2021 International Conference on Electronics, Communications and Information Technology (ICECIT)",14-16 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/21CW48944.2021.9532537,Machine diagnosis using acoustic analysis: a review,IEEE,Conferences,"Diagnosis or fault identification in real industrial machines using audio or sound signals is a challenging task. Active research has been pursued to determine acoustic features, classification & clustering algorithms that could estimate the state of an industrial machine. Acoustic features & classifiers from different domains have been successfully implemented for fault identification in industrial machines. This paper is a comparative study of propositions, experiments, applications and systems developed by various researchers. Effort has been made to generate a collection of test benches developed, results observed and conclusion arrived. These insights suggest deep learning and anomaly detection techniques as a promising technology for preventive maintenance in real industrial machines.",https://ieeexplore.ieee.org/document/9532537/,2021 IEEE Conference on Norbert Wiener in the 21st Century (21CW),22-25 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MMAR.2016.7575171,Machine learning based power quality event classification using wavelet — Entropy and basic statistical features,IEEE,Conferences,"Today's industrial environment is smarter than ever before. Most production lines include electrical devices which are able to communicate each other and controlled from a single station with automation systems. Most of those elements have an internet connection link known as industrial internet. Development of smart technology with industrial internet comes with a need of monitoring. Monitoring technologies are emergent systems that focus on fault detection, grid self — healings and online tracking of power quality issues. Present study deals with one of the essential part of an electricity grid monitoring system called power quality event classification in a manner of machine learning topic. Power quality events to be processed are generated synthetically by means of a comprehensive software tool. Classification of real-like dataset is executed using extreme learning machine which is an extremely fast learning algorithm applied to single layer neural networks. Basic statistical criteria and wavelet — entropy methods are handled to achieve distinctive features of dataset. As a performance evaluation instrument, conventional artificial neural network structure is run too. Detailed results are discussed to prove the satisfactory performance of proposed pattern recognition model.",https://ieeexplore.ieee.org/document/7575171/,2016 21st International Conference on Methods and Models in Automation and Robotics (MMAR),29 Aug.-1 Sept. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TEST.2016.7805855,Machine learning-based defense against process-aware attacks on Industrial Control Systems,IEEE,Conferences,"The modernization of Industrial Control Systems (ICS), primarily targeting increased efficiency and controllability through integration of Information Technologies (IT), introduced the unwanted side effect of extending the ICS cyber-security threat landscape. ICS are facing new security challenges and are exposed to the same vulnerabilities that plague IT, as demonstrated by the increasing number of incidents targeting ICS. Due to the criticality and unique nature of these systems, it is important to devise novel defense mechanisms that incorporate knowledge of the underlying physical model, and can detect attacks in early phases. To this end, we study a benchmark chemical process, and enumerate the various categories of attack vectors and their practical applicability on hardware controllers in a Hardware-In-The-Loop testbed. Leveraging the observed implications of the categorized attacks on the process, as well as the profile of typical disturbances, we follow a data-driven approach to detect anomalies that are early indicators of malicious activity.",https://ieeexplore.ieee.org/document/7805855/,2016 IEEE International Test Conference (ITC),15-17 Nov. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MALTESQUE.2018.8368453,Machine learning-based run-time anomaly detection in software systems: An industrial evaluation,IEEE,Conferences,"Anomalies are an inevitable occurrence while operating enterprise software systems. Traditionally, anomalies are detected by threshold-based alarms for critical metrics, or health probing requests. However, fully automated detection in complex systems is challenging, since it is very difficult to distinguish truly anomalous behavior from normal operation. To this end, the traditional approaches may not be sufficient. Thus, we propose machine learning classifiers to predict the system's health status. We evaluated our approach in an industrial case study, on a large, real-world dataset of 7.5 &#x2022; 10<sup>6</sup> data points for 231 features. Our results show that recurrent neural networks with long short-term memory (LSTM) are more effective in detecting anomalies and health issues, as compared to other classifiers. We achieved an area under precision-recall curve of 0.44. At the default threshold, we can automatically detect 70% of the anomalies. Despite the low precision of 31 %, the rate in which false positives occur is only 4 %.",https://ieeexplore.ieee.org/document/8368453/,2018 IEEE Workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE),20-20 March 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIMSA.2006.250751,Management of Complex Dynamic Systems based on Model-Predictive Multi-objective Optimization,IEEE,Conferences,"Over the past two decades, model predictive control and decision-making strategies have established themselves as powerful methods for optimally managing the behavior of complex dynamic industrial systems and processes. This paper presents a novel model-based multi-objective optimization and decision-making approach to model-predictive decision-making. The approach integrates predictive modeling based on neural networks, optimization based on multi-objective evolutionary algorithms, and decision-making based on Pareto frontier techniques. The predictive models are adaptive, and continually update themselves to reflect with high fidelity the gradually changing underlying system dynamics. The integrated approach, embedded in a real-time plant optimization and control software environment has been deployed to dynamically optimize emissions and efficiency while simultaneously meeting load demands and other operational constraints in a complex real-world power plant. While this approach is described in the context of power plants, the method is adaptable to a wide variety of industrial process control and management applications",https://ieeexplore.ieee.org/document/4016827/,2006 IEEE International Conference on Computational Intelligence for Measurement Systems and Applications,12-14 July 2006,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMNN.1994.593731,Massively parallel VLSI-implementation of a dedicated neural network for anomaly detection in automated visual quality control,IEEE,Conferences,"In this work we will present the VLSI-implementation of a dedicated neural network architecture which we have developed in prior work for anomaly detection in automated visual industrial quality control. The network, denoted as NOVAS performs a filtering of inspection images and highlights defects or anomalies in an isomorphic image representation, allowing the detection and localisation of faults on objects. Training of NOVAS is achieved by simply presenting a set of tolerable objects to the network in a single sweep. NOVAS works with single and with multichannel image representations. The processing principle of NOVAS is closely related to nearest neighbor and hypersphere classifier approaches. We have designed an ASIC for the efficient implementation of the nearest neighbor search. Based on that ASIC we will present an architecture of a modular massively parallel computer suited to meet the real-time constraints of manufacturing processes. Further we will report on the status of a prototype system which is close to completion.",https://ieeexplore.ieee.org/document/593731/,Proceedings of the Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems,26-28 Sept. 1994,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IDAP.2019.8875881,Measuring IEEE 802.15.4 Protocol Performance over Embedded Control Systems,IEEE,Conferences,"Using the IEEE 802.15.4 could be notice in various areas such as hospitals, agricultural, industrial, as well as smart homes. All the above-mentioned areas require devices that have a mechanism that consume low energies and low costs for communication. Thus, the purpose of fulfilling the needed requirements, IEEE 802.15.4 could be seen as effective and commercial solution. Various factors exist for studying the efficiency related to the wireless networks. Emulation could be define as a toll of high importance for designing and evaluating the implementation and the efficiency related to protocols, while the majority of real applications are considered to be of high difficulty with regard to the real applications. Concerning the presented paper, the main aim has been showing performance results, which are relates to IEEE 802.15.4 through use of octave simulations. The main task of the presented paper is analyzing the protocol's performance in various conditions as well as having the ability of checking the performance with regard to reliability in real-time. One of the aims of this study is analyzing the performance parameters, that are energy consumption rate, packet received ration as well as the Received Signal Strength Indicator (RSSI). All of the mentioned parameters studied in indoor and outdoor conditions the purpose of verifying the performance of the wireless protocol. The presented paper focus on 2 nodes in for the peer-to-peer communications, where one of the nodes acts as base-station or coordinator, while the other node is considered as router. The router include Xbee with sensors and Arduino, while the coordinator consist of Xbee receiver and adapter.",https://ieeexplore.ieee.org/document/8875881/,2019 International Artificial Intelligence and Data Processing Symposium (IDAP),21-22 Sept. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA45728.2021.9613330,Mel Spectrogram Analysis for Punching Machine Operating State Classification with CNNs,IEEE,Conferences,"Data driven analysis and optimization of production processes has become a pivotal instrument to use enterprise resources more efficiently and to improve product quality. However, availability and quality requirements still limit the prevalence of big data and learning techniques in industrial applications. Therefore, retrofitting sensors to brownfield systems has been suggested as a solution to acquire relevant real-time process data. In this paper, a low-cost retrofit approach to analyze the operating state of manually operated punching machines based on sound analysis is presented. The machine operating states provide additional information about the metal forming process, required for the enterprise resource planning (ERP) system to optimally schedule orders in prefabrication and plan available resources. As an analysis tool, a transfer learning approach with a convolutional neural network was used to assess data accuracy and prediction results. The input data consists of Mel Spectrogram images acquired by sound sensors retrofitted to the punching machines. The experiments show that the adapted EfficentNet-B0 achieves an accuracy, sensitivity, and precision of approximately 98 % on unseen data in real environment thus demonstrating the applicability of the implemented system.",https://ieeexplore.ieee.org/document/9613330/,2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA ),7-10 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SCC.2010.87,Message-Based Service Brokering and Dynamic Composition in the SAI Middleware,IEEE,Conferences,"Service-Oriented Computing (SOC) is a wide and complex research area. Despite the huge effort in both industrial and academics initiatives, several challenges need to be addressed in order to effectively realize the SOC vision. One of the most relevant issues is the need of effective, flexible, reliable, low cost solutions for dynamic service brokering and composition. This paper presents results of an ongoing work on the design and development of a service- and message-oriented middleware for atomic and composite service brokering, named SAI middleware. The SAI middleware offers a set of features for service brokering and dynamic composition, while also guaranteeing loose coupling between service providers and consumers and relaxing the prerequisites for service providers to publish their capabilities in an interoperability domain. SAI dynamic composition is based on an Artificial Intelligence planning approach and on the adoption of an ontology-based functional profile encoding information for enabling automatic information extraction and combination in the service composition chain. Our main contribution consists in addressing these issues in a holistic way, as required to effectively support the SOA vision in real application scenarios, while not optimizing single aspects yet.",https://ieeexplore.ieee.org/document/5557205/,2010 IEEE International Conference on Services Computing,5-10 July 2010,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCC51557.2021.9454641,Methods of Artificial Intelligence for Simulation of Gasification of Biomass and Communal Waste,IEEE,Conferences,"Artificial intelligence (AI) methods can simulate accurately outcomes of gasification processes based on real data sets. Quality and composition of gas obtained from solid fuels depend on the temperature of the gasifying agent (air or steam) and the composition of used solid waste (biomass, industrial waste or combustibles from municipal solid waste, such as plastics, textile, wood, paper, tires, etc.). To simulate the gasification processes, a symbolic regression software AI Feynman is tested. Finally, the results of symbolic regression are compared with measured data. The results indicate that symbolic regression of AI Feynman is useful for modelling of biomass gasification technologies from measured data.",https://ieeexplore.ieee.org/document/9454641/,2021 22nd International Carpathian Control Conference (ICCC),31 May-1 June 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASE.2017.8115693,Mining constraints for event-based monitoring in systems of systems,IEEE,Conferences,"The full behavior of software-intensive systems of systems (SoS) emerges during operation only. Runtime monitoring approaches have thus been proposed to detect deviations from the expected behavior. They commonly rely on temporal logic or domain-specific languages to formally define requirements, which are then checked by analyzing the stream of monitored events and event data. Some approaches also allow developers to generate constraints from declarative specifications of the expected behavior. However, independent of the approach, deep domain knowledge is required to specify the desired behavior. This knowledge is often not accessible in SoS environments with multiple development teams independently working on different, heterogeneous systems. In this New Ideas Paper we thus describe an approach that automatically mines constraints for runtime monitoring from event logs recorded in SoS. Our approach builds on ideas from specification mining, process mining, and machine learning to mine different types of constraints on event occurrence, event timing, and event data. The approach further presents the mined constraints to users in an existing constraint language and it ranks the constraints using different criteria. We demonstrate the feasibility of our approach by applying it to event logs from a real-world industrial SoS.",https://ieeexplore.ieee.org/document/8115693/,2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE),30 Oct.-3 Nov. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00017,Mirrorlabs - creating accessible Digital Twins of robotic production environment with Mixed Reality,IEEE,Conferences,How to visualize recorded production data in Virtual Reality? How to use state of the art Augmented Reality displays that can show robot data? This paper introduces an opensource ICT framework approach for combining Unity-based Mixed Reality applications with robotic production equipment using ROS Industrial. This publication gives details on the implementation and demonstrates the use as a data analysis tool in the context of scientific exchange within the area of Mixed Reality enabled human-robot co-production.,https://ieeexplore.ieee.org/document/9319071/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMEW46912.2020.9106033,Mobile Centernet for Embedded Deep Learning Object Detection,IEEE,Conferences,"Object detection is a fundamental task in computer vision with wide application prospect. And recent years, many novel methods are proposed to tackle this task. However, most algorithms suffer from high computation cost and long inference time, which makes them impossible to be deployed on embedded devices in real industrial application scenarios. In this paper, we propose the Mobile CenterNet to solve this problem. Our method is based on CenterNet but with some key improvements. To enhance detection performance, we adopt HRNet as a powerful backbone and introduce a categorybalanced focal loss to deal with category imbalance problem. Moreover, to compress the model size as well as reduce inference time, knowledge distillation is employed to transfer knowledge from cumbersome model to a compact one. We conduct experiments on a large traffic detection dataset BDD100K and validate the effectiveness of all the modifications. Finally, our method achieves the 1st place in the Embedded Deep Learning Object Detection Model Compression Competition held in ICME 2020.",https://ieeexplore.ieee.org/document/9106033/,2020 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),6-10 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WIFS53200.2021.9648398,Mobile authentication of copy detection patterns: how critical is to know fakes?,IEEE,Conferences,"Protection of physical objects against counterfeiting is an important task for the modern economies. In recent years, the high-quality counterfeits appear to be closer to originals thanks to the rapid advancement of digital technologies. To combat these counterfeits, an anti-counterfeiting technology based on hand-crafted randomness implemented in a form of copy detection patterns (CDP) is proposed enabling a link between the physical and digital worlds and being used in various brand protection applications. The modern mobile phone technologies make the verification process of CDP easier and available to the end customers. Besides a big interest and attractiveness, the CDP authentication based on the mobile phone imaging remains insufficiently studied. In this respect, in this paper we aim at investigating the CDP authentication under the real-life conditions with the codes printed on an industrial printer and enrolled via a modern mobile phone under the regular light conditions. The authentication aspects of the obtained CDP are investigated with respect to the four types of copy fakes. The impact of fakes' type used for training of authentication classifier is studied in two scenarios: (i) supervised binary classification under various assumptions about the fakes and (ii) one-class classification under unknown fakes. The obtained results show that the modern machine-learning approaches and the technical capacity of modern mobile phones allow to make the CDP authentication under unknown fakes feasible with respect to the considered types of fakes and code design.",https://ieeexplore.ieee.org/document/9648398/,2021 IEEE International Workshop on Information Forensics and Security (WIFS),7-10 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCA.2009.5281106,Model predictive control for nonlinear affine systems based on the simplified dual neural network,IEEE,Conferences,"Model predictive control (MPC), also known as receding horizon control (RHC), is an advanced control strategy for optimizing the performance of control systems. For nonlinear systems, standard MPC schemes based on linearization would result in poor performance. In this paper, we propose an MPC scheme for nonlinear affine systems based on a recurrent neural network (RNN) called the simplified dual network. The proposed RNN-based approach is efficient and suitable for real-time MPC implementation in industrial applications. Simulation results are provided to demonstrate the effectiveness and efficiency of the proposed MPC scheme.",https://ieeexplore.ieee.org/document/5281106/,"2009 IEEE Control Applications, (CCA) & Intelligent Control, (ISIC)",8-10 July 2009,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEAA.2015.75,Model-Based Risk Assessment in Multi-disciplinary Systems Engineering,IEEE,Conferences,"In industrial production systems engineering projects, the work of software managers depends on engineering artifacts coming from multiple disciplines. In particular, it is important to software managers to assess the project risk from the status and evolution of various heterogenous distributed engineering artifacts. Thus, software risk management is most often an error prone and cumbersome task in such projects. To tackle this challenge, we introduce a model-based foundation for risk assessment in multi-disciplinary systems engineering projects. In particular, we build on the recent modeling support for the Automation ML (AML) standard which enables representing data coming from different engineering disciplines as models and employ a linking language to reason on a set of distributed engineering artifacts and their relationships. Based on this pillars, we propose in this paper a dedicated metric suite and measurement support for AML as an important ingredient for efficient risk assessment of heterogenous and distributed engineering data. We evaluate the feasibility of the proposed approach by providing tool support on top of the Eclipse Modeling Framework (EMF) and demonstrate its application with a showcase based on a real-world case study.",https://ieeexplore.ieee.org/document/7302486/,2015 41st Euromicro Conference on Software Engineering and Advanced Applications,26-28 Aug. 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MODELS-C.2019.00101,Modeling Adaptive Learning Agents for Domain Knowledge Transfer,IEEE,Conferences,"The implementation of intelligent agents in industrial applications is often prevented by the high cost of adopting such a system to a particular problem domain. This paper states the thesis that when learning agents are applied to work environments that require domain-specific experience, the agent benefits if it can be further adapted by a supervising domain expert. Closely interacting with the agent, a domain expert should be able to understand its decisions and update the underlying knowledge base as needed. The result would be an agent with individualized knowledge that comes in part from the domain experts. The model of such an adaptive learning agent must take into account the problem domain, the design of the learning agent and the perception of the domain user. Therefore, already in the modeling phase, more attention must be paid to make the learning element of the agent adaptable by an operator. Domain modeling and meta-modeling methods could help to make inner processes of the agent more accessible. In addition, the knowledge gained should be made reusable for future agents in similar environments. To begin with, the existing methods for modeling agent systems and the underlying concepts will be evaluated, based on the requirements for different industrial scenarios. The methods are then compiled into a framework that allows for the description and modeling of such systems in terms of adaptability to a problem domain. Where necessary, new methods or tools will be introduced to close the gap between inconsistent modeling artifacts. The framework shall then be used to build learning agents for real-life scenarios and observe their application in a case study. The results will be used to assess the quality of the adapted knowledge base and compare it to a manual knowledge modeling process.",https://ieeexplore.ieee.org/document/8904822/,2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C),15-20 Sept. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NRSC.2002.1022671,Modeling and automatic control of nuclear reactors,IEEE,Conferences,"This paper presents a developed real time simulator for MPR (multi purpose reactor), on which the Egyptian 2/sup nd/ research reactor is based. A VisSim S/W environment with real-time add-on is employed to achieve this put-pose. VisSim S/W supports a variety of standard and industrial grade I/O cards and uses the block diagram programming technique, which keeps use of specialized code to minimum. All necessary reactivity feedback is taken into consideration and modeled by sufficiently accurate equations. A control rod withdrawal algorithm, which represents the actual one of the MPR, is stated and modeled. Application of traditional control algorithms are implemented and discussed. Advanced control algorithms such as fuzzy, neural network, and genetic are investigated to substitute the moving control rod selection logic and core parameter prediction. The simulator can be distributed with VisSim viewer or through generated C code from VisSim block diagram. The results of the proposed simulator in the power state agree well with the published experimental and analytical results.",https://ieeexplore.ieee.org/document/1022671/,Proceedings of the Nineteenth National Radio Science Conference,21-21 March 2002,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IranianCEE.2017.7985123,Modeling and identification of an industrial gas turbine using classical and non-classical approaches,IEEE,Conferences,"In this paper, data-driven modeling and identification of an industrial, simple cycle, heavy duty Gas Turbine is taken into consideration. The GGOV1 model that was introduced by Western Electricity Coordinating Council (WECC), is suggested here for describing dynamics and behaviour of Gas Turbines. It is shown that GGOV1 model that is expressed as the classical approach, can fulfill our needs in academic studies and engineering purposes. Non-classical identification of Gas turbine is also done via Artificial Neural Networks. Comparison between the two methods and real data shows reliability and acceptable precision of both models.",https://ieeexplore.ieee.org/document/7985123/,2017 Iranian Conference on Electrical Engineering (ICEE),2-4 May 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NCA.2018.8548330,Monitoring IoT Networks for Botnet Activity,IEEE,Conferences,"The Internet of Things (IoT) has rapidly transitioned from a novelty to a common, and often critical, part of residential, business, and industrial environments. Security vulnerabilities and exploits in the IoT realm have been well documented. In many cases, improving the security of an IoT device by hardening its software is not a realistic option, especially in the cost-sensitive consumer market or in legacy-bound industrial settings. As part of a multifaceted defense against botnet activity on the IoT, this paper explores a method based on monitoring the network activity of IoT devices. A notable benefit of this approach is that it does not require any special access to the devices and adapts well to the addition of new devices. The method is evaluated on a publicly available dataset drawn from a real IoT network.",https://ieeexplore.ieee.org/document/8548330/,2018 IEEE 17th International Symposium on Network Computing and Applications (NCA),1-3 Nov. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN52387.2021.9534428,Multi-Modal Multi-Instance Multi-Label Learning with Graph Convolutional Network,IEEE,Conferences,"When applying machine learning to tackle realworld problems, it is common to see that objects come with multiple labels rather than a single label. In addition, complex objects can be composed of multiple modalities, e.g. a post on social media may contain both texts and images. Previous approaches typically treat every modality as a whole, while it is not the case in real world, as every post may contain multiple images and texts with quite diverse semantic meanings. Therefore, Multi-modal Multi-instance Multi-label (M3) learning was proposed. Previous attempt at M3 learning argues that exploiting label correlations is crucial. In this paper, we find that we can handle M3 problems using graph convolutional network. Specifically, a graph is built over all labels and each label is initially represented by its word embedding. The main goal for GCN is to map those label embed dings into inter-correlated label classifiers. Moreover, multi-instance aggregation is based on attention mechanism, making it more interpretable because it naturally learns to discover which pattern triggers the labels. Empirical studies are conducted on both benchmark datasets and industrial datasets, validating the effectiveness of our method, and it is demonstrated in ablation studies that the components in our methods are essential.",https://ieeexplore.ieee.org/document/9534428/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00215,Multi-agent deep reinforcement learning for traffic signal control with Nash Equilibrium,IEEE,Conferences,"Traffic signal control is an essential and chal-lenging real-world problem, which aims to alleviate traffic congestion by coordinating vehicles&#x0027; movements at road in-tersections. Deep reinforcement learning (DRL) combines deep neural networks (DNNs) with a framework of reinforcement learning, which is a promising method for adaptive traffic signal control in complex urban traffic networks. Now, multi-agent deep reinforcement learning (MARL) has the potential to deal with traffic signal control at a large scale. However, current traffic signal control systems still rely heavily on simplified rule- based methods in practice. In this paper, we propose: (1) a MARL algorithm based on Nash Equilibrium and DRL, namely Nash Asynchronous Advantage Actor-Critic (Nash-A3C); (2) an urban simulation environment (SENV) to be essentially close to the real-world scenarios. We apply our method in SENV, obtaining better performance than benchmark traffic signal control methods by 22.1&#x0025;, which proves that Nash-A3C to be more suitable for large industrial level deployment.",https://ieeexplore.ieee.org/document/9781235/,"2021 IEEE 23rd Int Conf on High Performance Computing & Communications; 7th Int Conf on Data Science & Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)",20-22 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSN.2017.34,Multi-level Anomaly Detection in Industrial Control Systems via Package Signatures and LSTM Networks,IEEE,Conferences,"We outline an anomaly detection method for industrial control systems (ICS) that combines the analysis of network package contents that are transacted between ICS nodes and their time-series structure. Specifically, we take advantage of the predictable and regular nature of communication patterns that exist between so-called field devices in ICS networks. By observing a system for a period of time without the presence of anomalies we develop a base-line signature database for general packages. A Bloom filter is used to store the signature database which is then used for package content level anomaly detection. Furthermore, we approach time-series anomaly detection by proposing a stacked Long Short Term Memory (LSTM) network-based softmax classifier which learns to predict the most likely package signatures that are likely to occur given previously seen package traffic. Finally, by the inspection of a real dataset created from a gas pipeline SCADA system, we show that an anomaly detection scheme combining both approaches can achieve higher performance compared to various current state-of-the-art techniques.",https://ieeexplore.ieee.org/document/8023128/,2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN),26-29 June 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCIS53392.2021.9754643,Multi-scale Defective Samples Synthesis for Surface Defect Detection,IEEE,Conferences,"Surface defect detection has received both academic and industrial attention in recent years. In real-world applications, it is usually difficult to collect defective samples since manual labeling is time-consuming and defective samples rarely appear. In this paper, we propose a novel method for multi-scale defective sample synthesis and detection. First, a Pairs Generative Adversarial Network (PairsGAN) is proposed for generating defects and their labels. To improve the generated quality of the defective area, we design a defect discriminator in PairsGAN to focuses on distinguishing the defective area. Then, a Multi-Scale Defect Fusion (MSDF) module is presented to diversify the generated defects with various scales and styles, which fuses them into normal samples in different locations, so as to obtain naturally defective samples and corresponding labels. Finally, generated samples are used as the inputs of the semantic segmentation network for defect detection. Experimental results demonstrate that our method achieves more stable and better segmentation results comparing to recent methods.",https://ieeexplore.ieee.org/document/9754643/,2021 IEEE 7th International Conference on Cloud Computing and Intelligent Systems (CCIS),7-8 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDMW.2019.00065,Mímir: Building and Deploying an ML Framework for Industrial IoT,IEEE,Conferences,"In this paper we describe Mímir, a production grade cloud and edge spanning ML framework for Industrial IoT applications. We first describe our infrastructure for optimized capture, streaming and multi-resolution storage of manufacturing data and its context. We then describe our workflow for scalable ML model training, validation, and deployment that leverages a manufacturing taxonomy and parameterized ML pipelines to determine the best metrics, hyper-parameters and models to use for a given task. We also discuss our design decisions on model deployment for real-time and batch data in the cloud and at the edge. Finally, we describe the use of the framework in building and deploying an application for Predictive Quality monitoring during a Plastics Extrusion manufacturing process.",https://ieeexplore.ieee.org/document/8955638/,2019 International Conference on Data Mining Workshops (ICDMW),8-11 Nov. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2015.7281723,NN-ANARX model based control of liquid level using visual feedback,IEEE,Conferences,"In this paper, the problem of liquid level control based on visual feedback is investigated in application to a real life model of an industrial plant. Visual detection of liquid level is implemented on the Raspberry Pi computer. Computational intelligence based controller uses input-output feedback linearization. Parameters of the controller are provided by the neural network ANARX structure. As communication between Raspberry Pi and control system is provided over WLAN, additional prediction module is used to overcome networking problems. Control of the SISO and MIMO systems is provided.",https://ieeexplore.ieee.org/document/7281723/,2015 IEEE 13th International Conference on Industrial Informatics (INDIN),22-24 July 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDE48307.2020.00039,Neighbor Profile: Bagging Nearest Neighbors for Unsupervised Time Series Mining,IEEE,Conferences,"Unsupervised time series mining has been attracting great interest from both academic and industrial communities. As the two most basic data mining tasks, the discoveries of frequent/rare subsequences have been extensively studied in the literature. Specifically, frequent/rare subsequences are defined as the ones with the smallest/largest 1-nearest neighbor distance, which are also known as motif/discord. However, discord fails to identify rare subsequences when it occurs more than once in the time series, which is widely known as the twin freak problem. This problem is just the ""tip of the iceberg"" due to the 1-nearest neighbor distance based definitions. In this work, we for the first time provide a clear theoretical analysis of motif/discord as the 1-nearest neighbor based nonparametric density estimation of subsequence. Particularly, we focus on matrix profile, a recently proposed mining framework, which unifies the discovery of motif and discord under the same computing model. Thereafter, we point out the inherent three issues: low-quality density estimation, gravity defiant behavior, and lack of reusable model, which deteriorate the performance of matrix profile in both efficiency and subsequence quality.To overcome these issues, we propose Neighbor Profile to robustly model the subsequence density by bagging nearest neighbors for the discovery of frequent/rare subsequences. Specifically, we leverage multiple subsamples and average the density estimations from subsamples using adjusted nearest neighbor distances, which not only enhances the estimation robustness but also realizes a reusable model for efficient learning. We check the sanity of neighbor profile on synthetic data and further evaluate it on real-world datasets. The experimental results demonstrate that neighbor profile can correctly model the subsequences of different densities and shows superior performance significantly over matrix profile on the real-world arrhythmia dataset. Also, it is shown that neighbor profile is efficient for massive datasets.",https://ieeexplore.ieee.org/document/9101736/,2020 IEEE 36th International Conference on Data Engineering (ICDE),20-24 April 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PEDES.2006.344292,Neural Approach for Automatic Identification of Induction Motor Load Torque in Real-Time Industrial Applications,IEEE,Conferences,"Induction motors are widely used in several industrial sectors. However, the dimensioning of induction motors is often inaccurate because, in most cases, the load behavior in the shaft is completely unknown. The proposal of this paper is to use artificial neural networks as a tool for dimensioning induction motors rather than conventional methods, which use classical identification techniques and mechanical load modeling. Since the proposed approach uses current, voltage and speed values as the only input parameters, one of its potentialities is related to the facility of hardware implementation for industrial environments and field applications. Simulation results are also presented to validate the proposed approach.",https://ieeexplore.ieee.org/document/4147999/,"2006 International Conference on Power Electronic, Drives and Energy Systems",12-15 Dec. 2006,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HIS.2011.6122180,Neural network prognostics model for industrial equipment maintenance,IEEE,Conferences,"This paper presents a new prognostics model based on neural network technique for supporting industrial maintenance decision. In this study, the probabilities of failure based on the real condition equipment are initially calculated by using logistic regression method. The failure probabilities are subsequently utilized as input for prognostics model to predict the future value of failure condition and then used to estimate remaining useful lifetime of equipment. By having a time series of predicted failure probability, the failure distribution can be generated and used in the maintenance cost model to decide the optimal time to do maintenance. The proposed prognostic model is implemented in the industrial equipment known as autoclave burner. The result from the model reveals that it can give prior warnings and indication to the maintenance department to take an appropriate decision instead of dealing with the failures while the autoclave burner is still operating. This significant contribution provides new insights into the maintenance strategy which enables the use of existing condition data from industrial equipment and prognostics approach.",https://ieeexplore.ieee.org/document/6122180/,2011 11th International Conference on Hybrid Intelligent Systems (HIS),5-8 Dec. 2011,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2010.5641990,Neural network workflow scheduling for large scale Utility Management Systems,IEEE,Conferences,"Grid computing is the future computing paradigm for enterprise applications. It can be used for executing large scale workflow applications. This paper focuses on the workflow scheduling mechanism. Although there is much work on static scheduling approaches for workflow applications in parallel environments, little work has been done on a Grid environment for industrial systems. Utility Management Systems (UMS) are executing very large numbers of workflows with very high resource requirements. Unlike the grid approach for standard scientific workflows, UMS workflows have a different set of requirements and thereby optimization of resource usage has to be made in a different way. This paper proposes a novel scheduling architecture which dynamically executes a scheduling algorithm using near real-time feedback from the execution monitor. An Artificial Neural Network was used for workflow scheduling and performance tests show that significant improvement of overall execution time can be achieved by this soft-computing method.",https://ieeexplore.ieee.org/document/5641990/,"2010 IEEE International Conference on Systems, Man and Cybernetics",10-13 Oct. 2010,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/2830772.2830789,Neuromorphic accelerators: A comparison between neuroscience and machine-learning approaches,IEEE,Conferences,"A vast array of devices, ranging from industrial robots to self-driven cars or smartphones, require increasingly sophisticated processing of real-world input data (image, voice, radio, ...). Interestingly, hardware neural network accelerators are emerging again as attractive candidate architectures for such tasks. The neural network algorithms considered come from two, largely separate, domains: machine-learning and neuroscience. These neural networks have very different characteristics, so it is unclear which approach should be favored for hardware implementation. Yet, few studies compare them from a hardware perspective. We implement both types of networks down to the layout, and we compare the relative merit of each approach in terms of energy, speed, area cost, accuracy and functionality. Within the limit of our study (current SNN and machine learning NN algorithms, current best effort at hardware implementation efforts, and workloads used in this study), our analysis helps dispel the notion that hardware neural network accelerators inspired from neuroscience, such as SNN+STDP, are currently a competitive alternative to hardware neural networks accelerators inspired from machine-learning, such as MLP+BP: not only in terms of accuracy, but also in terms of hardware cost for realistic implementations, which is less expected. However, we also outline that SNN+STDP carry potential for reduced hardware cost compared to machine-learning networks at very large scales, if accuracy issues can be controlled (or for applications where they are less important). We also identify the key sources of inaccuracy of SNN+STDP which are less related to the loss of information due to spike coding than to the nature of the STDP learning algorithm. Finally, we outline that for the category of applications which require permanent online learning and moderate accuracy, SNN+STDP hardware accelerators could be a very cost-efficient solution.",https://ieeexplore.ieee.org/document/7856622/,2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),5-9 Dec. 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ComPE49325.2020.9200139,Object Detection and Tracking Turret based on Cascade Classifiers and Single Shot Detectors,IEEE,Conferences,"The involvement of embedded systems and computer vision is increasing day by day in various segments of consumer market like industrial automation, traffic monitoring, medical imaging, modern appliance market, augmented reality systems, etc. These technologies are bound to make new developments in the domain of commercial and home security surveillance. Our project aims to make contributions to the domain of video surveillance by making use of embedded computer vision systems. Our implementation, built around the Raspberry Pi 4 SBC aims to utilize computer vision techniques like motion detection, face recognition, object detection, etc to segment the region of interest from the captured video footage. This technique is superior as compared to traditional surveillance systems as it requires minimum human interaction and intervention at the control room of such security systems. The proposed system is capable of sensing suspicious events like detection of an unknown face in the captured video or motion detection/object detection in a closed section of a building. Moreover, with the help of the turret mechanism built using servo motors, the camera integrated in the system is capable of having 360° rotation and can track a detected face or object of interest within its range. Apart from automated tracking, the system can also be manually controlled by the operator.",https://ieeexplore.ieee.org/document/9200139/,2020 International Conference on Computational Performance Evaluation (ComPE),2-4 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2009.4913200,Object orientation recognition based on SIFT and SVM by using stereo camera,IEEE,Conferences,"The goal of this research is to recognize an object and its orientation in space by using stereo camera. The principle of object orientation recognition in this paper was based on the scale invariant feature transform (SIFT) and support vector machine (SVM). SIFT has been successfully implemented on object recognition but it had a problem recognizing the object orientation. For many autonomous robotics applications, such as using a vision-guided industrial robot to grab a product, not only correct object recognition will be needed in this process but also object orientation recognition is required. In this paper we used SVM to recognize object orientation. SVM has been known as a promising method for classification accuracy and its generalization ability. The stereo camera system adopted in this research provided more useful information compared to single camera one. The object orientation recognition technique was implemented on an industrial robot in a real application. The proposed camera system and recognition algorithms were used to recognize a specific object and its orientation and then guide the industrial robot to perform some alignment operations on the object.",https://ieeexplore.ieee.org/document/4913200/,2008 IEEE International Conference on Robotics and Biomimetics,22-25 Feb. 2009,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2006.347441,Obstacle avoidance algorithm based on biological patterns for anthropomorphic robot manipulator,IEEE,Conferences,"This study addresses the problem of collision-free controlling of 3-DOF (degree of freedom) anthropomorphic manipulators with given a priori unrestricted trajectory. The robot constraints resulting from the physical robot's actuators are also taken into account during the robot movement. Obstacle avoidance algorithm is based on penalty function, which is minimized when collision is predicted. Mathematical construction of penalty function and minimization process allows modeling of variety behaviors of robot elusion moves. Implementation of artificial neural network (ANN) inside the control process gives the additional flexibility needed to remember most important robot behaviors based on biological pattern of human arm moves. Thanks to the fast collisions' detection, the presented algorithm appears to be applicable to the industrial real-time implementations. Numerical simulations of the anthropomorphic manipulator operating in three dimensional space with obstacles is also presented",https://ieeexplore.ieee.org/document/4152937/,IECON 2006 - 32nd Annual Conference on IEEE Industrial Electronics,6-10 Nov. 2006,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA46521.2020.9212164,Occupant Feedback and Context Awareness: On the Application of Building Information Modeling and Semantic Technologies for Improved Complaint Management in Commercial Buildings,IEEE,Conferences,"Common methods for submitting hardware- or comfort-related complaints in an office or industrial environment, such as online forms or via a telephone hotline, can lead to misinterpretations of the issue and/or be perceived as being cumbersome by the submitter. This can act as a barrier for the submission of feedback and thus cause the facility management to remain unaware of unsatisfactory comfort conditions or faults, which can result in further issues and costs. In order to reduce the submission effort, a novel software-based solution is proposed, which automatically determines the most probable complaints, suggests them to the user and, when possible, automatically solves them. This is achieved by employing detailed context information stemming from a Building Information Model, the Building Automation System and past complaints submitted by the occupants. This information is integrated and represented with semantic technologies, which allow to formally organize the information and describe the relationships between the data. The solution was implemented as an app and was demonstrated in a real office environment. The reduced effort of submitting feedback via the app led to a strong increase in the submission of comfort-related complaints, showing the effectiveness of the proposed solution.",https://ieeexplore.ieee.org/document/9212164/,2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),8-11 Sept. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2017.8258105,On event-driven knowledge graph completion in digital factories,IEEE,Conferences,"Smart factories are equipped with machines that can sense their manufacturing environments, interact with each other, and control production processes. Smooth operation of such factories requires that the machines and engineering personnel that conduct their monitoring and diagnostics share a detailed common industrial knowledge about the factory, e.g., in the form of knowledge graphs. Creation and maintenance of such knowledge is expensive and requires automation. In this work we show how machine learning that is specifically tailored towards industrial applications can help in knowledge graph completion. In particular, we show how knowledge completion can benefit from event logs that are common in smart factories. We evaluate this on the knowledge graph from a real world-inspired smart factory with encouraging results.",https://ieeexplore.ieee.org/document/8258105/,2017 IEEE International Conference on Big Data (Big Data),11-14 Dec. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC51774.2021.00225,On the Use of Causal Models to Build Better Datasets,IEEE,Conferences,"In recent years, Machine Learning and Deep Learning communities have devoted many efforts to studying ever better models and more efficient training strategies. Nonetheless, the fundamental role played by dataset bias in the final behaviour of the trained models calls for strong and principled methods to collect, structure and curate datasets prior to training. In this paper we provide an overview on the use of causal models to achieve a deeper understanding of the underlying structure beneath datasets and mitigate biases, supported by several real-life use cases from the medical and industrial domains.",https://ieeexplore.ieee.org/document/9529554/,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",12-16 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.1995.525123,On the design and implementation of a mobile robotic system,IEEE,Conferences,"Describes the effort being carried out in the analysis, design and implementation of the control architecture for a mobile platform for autonomous transportation, surveillance and inspection in structured and semi-structured industrial environments. The control architecture is based in a hierarchical structure organized linguistically permitting the real-time parallel execution of tasks. This architecture is composed of three levels, organization, coordination and functional, structured according to the increasing precision with decreasing intelligence principle.",https://ieeexplore.ieee.org/document/525123/,Proceedings of Tenth International Symposium on Intelligent Control,27-29 Aug. 1995,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA.2019.8816298,Online Learning of the Inverse Dynamics with Parallel Drifting Gaussian Processes: Implementation of an Approach for Feedforward Control of a Parallel Kinematic Industrial Robot,IEEE,Conferences,"The present paper deals with an online approach to learn the inverse dynamics of any robot. This is realized by the use of Gaussian Processes drifting parallel along the system data. An extension by a database enables the efficient use of data points from the past. The central component of this work is the implementation of such a method in a controller in order to achieve the actual goal: the feedforward control of an industrial robot by means of machine learning. This is done by splitting the procedure into two threads running parallel so that the prediction is decoupled from the computing-intensive training of the models. Experiments show that the method reduces the tracking errors more clearly than an elaborately identified rigid body model including friction. For a defined trajectory, the squared areas of the tracking errors of all axes are reduced by more than 54% compared to motion without pre-control. In addition, a highly dynamic pick-and-place experiment is used to investigate the possible changes in system dynamics. Compared to an offline trained model, the approximation error of the proposed online approach is smaller for the remaining time of the experiment after an initial phase. Furthermore, this error is smaller throughout the experiment for online learning with parallel drifting Gaussian Processes than when using a single one.",https://ieeexplore.ieee.org/document/8816298/,2019 IEEE International Conference on Mechatronics and Automation (ICMA),4-7 Aug. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9196769,Online LiDAR-SLAM for Legged Robots with Robust Registration and Deep-Learned Loop Closure,IEEE,Conferences,"In this paper, we present a 3D factor-graph LiDAR-SLAM system which incorporates a state-of-the-art deeply learned feature-based loop closure detector to enable a legged robot to localize and map in industrial environments. Point clouds are accumulated using an inertial-kinematic state estimator before being aligned using ICP registration. To close loops we use a loop proposal mechanism which matches individual segments between clouds. We trained a descriptor offline to match these segments. The efficiency of our method comes from carefully designing the network architecture to minimize the number of parameters such that this deep learning method can be deployed in real-time using only the CPU of a legged robot, a major contribution of this work. The set of odometry and loop closure factors are updated using pose graph optimization. Finally we present an efficient risk alignment prediction method which verifies the reliability of the registrations. Experimental results at an industrial facility demonstrated the robustness and flexibility of our system, including autonomous following paths derived from the SLAM map.",https://ieeexplore.ieee.org/document/9196769/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMA.2017.8231735,Online system modeling of chemical process plant using U-model,IEEE,Conferences,"One of the major challenges in industrial process control is to deal with nonlinearities in the plants. There have been a significant amount of research efforts towards design and development of appropriate, reliable and promising control techniques to deployed on real-time industrial applications. Some of the widely used and acknowledged control methods lack in terms of tuning unknown system parameters. The reason is their unadaptive or fixed nature. This paves the field well for adaptive controllers. Their biggest advantage is their automatic updation of unknown system paramters, that saves quite some resources and manpower and ensures an overall stable control strategy. In this regard, system modeling happens to be the prime and pertinent task so that it can set the basis for a stable control law synthesis. This reserach work proposes a polynomial adaptive model recently introduced called U-Model to be used for online system identification of Chemical Process Plant. U-Model is a simple, stable and reliable which has previously yielded encouraging results when applied to various application in different scenario. The aforementioned plant shall be used for investigation on its Flow process. The modeling results shall be compared and validated by other commonly known and utilized modeling structures.",https://ieeexplore.ieee.org/document/8231735/,2017 IEEE 3rd International Symposium in Robotics and Manufacturing Automation (ROMA),19-21 Sept. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCI-CC.2018.8482025,Ontology Faults Diagnosis Model for the Hazardous Chemical Storage Device,IEEE,Conferences,"Due to high temperature, high pressure, high corrosion and many other factors, the hazardous chemical device is facing more severe security challenges than other industries. Now, the monitoring methods have been very mature, which play a basic monitoring role, not a predictive fault diagnosis. In this paper, the hazardous chemical device's status data will been collected from the existing industrial monitoring network, the real-time data will be preprocessed and then stored in a database, and the data will be imported to the real-time data into the ontology model, the data will be performed by big data processing and automatic reasoning. So that real-time status of hazardous chemical device and the warning of security risks predict are easily got at any time. The model is proposed to solving the problem of knowledge representation and reasoning of the hazardous chemical device based on ontology. The model is analyzed and implemented in Protégé software.",https://ieeexplore.ieee.org/document/8482025/,2018 IEEE 17th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),16-18 July 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IHMSC.2009.38,Ontology-Based Fast Devices Configuration in 3D Configuration Software,IEEE,Conferences,"Using 3D user interface in the configuration software, the user interface can become realistic and friendly. And it can reflect the devices running status in the industrial field so realistically. Moreover, it can avoid the window effect effectively due to the screen size influence. The window information content has the enormous enhancement. But it brings some other problems such as configuration difficultly. This paper presents a ontology-based fast configuration method by attaching semantic information to the device model. The computer can understand the user intention and assist him do the configuration work. Using this method, the devices configuration becomes so simple and convenient. The users that are not 3D professionals can also carry on the devices configuration relaxed.",https://ieeexplore.ieee.org/document/5335932/,2009 International Conference on Intelligent Human-Machine Systems and Cybernetics,26-27 Aug. 2009,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAMAD.2019.8858503,Operational Data Based Intrusion Detection System for Smart Grid,IEEE,Conferences,"With the rapid progression of Information and Communication Technology (ICT) and especially of Internet of Things (IoT), the conventional electrical grid is transformed into a new intelligent paradigm, known as Smart Grid (SG). SG provides significant benefits both for utility companies and energy consumers such as the two-way communication (both electricity and information), distributed generation, remote monitoring, self-healing and pervasive control. However, at the same time, this dependence introduces new security challenges, since SG inherits the vulnerabilities of multiple heterogeneous, co-existing legacy and smart technologies, such as IoT and Industrial Control Systems (ICS). An effective countermeasure against the various cyberthreats in SG is the Intrusion Detection System (IDS), informing the operator timely about the possible cyberattacks and anomalies. In this paper, we provide an anomaly-based IDS especially designed for SG utilising operational data from a real power plant. In particular, many machine learning and deep learning models were deployed, introducing novel parameters and feature representations in a comparative study. The evaluation analysis demonstrated the efficacy of the proposed IDS and the improvement due to the suggested complex data representation.",https://ieeexplore.ieee.org/document/8858503/,2019 IEEE 24th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD),11-13 Sept. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCE49384.2020.9179427,Opportunistic Resource Allocation for Narrowband Internet of Things: A Literature Review,IEEE,Conferences,"Nowadays, the growing adoption of the Internet of Things (IoT) is reshaping the telecommunication landscape and has penetrated every aspect of our lives with influential applications on smart health, home automation, smart logistics, smart industries and smart cities. These advanced technologies bring about numerous benefits and has begun to play a major role in daily lives, particularly data mining applied in precision agriculture to discover knowledge. Also in agro-industrial production chain, the combination of wireless and distributed specific sensor devices with the simulation of climatic conditions in order to track the evolution of grapes for wineries is outstanding. Mobile IoT such as narrow-band-IoT (NB-IoT) and the long-term evolution (LTE) for machines (LTE-M) are significant innovations of this accelerating development of IoT technologies. In the era of ubiquitous communication where everything is connected to the internet, NB-IoT systems are expected to offer better quality-of-services (QoS) to end users than the traditional IoT paradigm. However, offering better QoS satisfaction to end users will become a great challenge due to the bottleneck caused by the dual problem of increasing IoT use cases and the shortage of wireless spectrum resources. Whilst discussing the recent innovative solutions of NB-IoT resource allocation, significant challenges and open issues related to the real-time implementation of NBIoT are identified and discussed. Therefore, this paper gives a general overview of the resource allocation solutions in this NB-IoT innovation and further suggests and motivates for the intelligentization of future resource allocation solutions (i.e., the use of artificial intelligence (AI) strategies).",https://ieeexplore.ieee.org/document/9179427/,"2020 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",12-13 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAL.2009.5262851,Optimal control of continuous annealing process using PSO,IEEE,Conferences,"The continuous annealing furnace is one of the main equipments in the continuous annealing production line for iron and steel enterprise, and has a direct impact on the quality of cold-rolled strip steel, production and cost. The temperature control of continuous annealing furnace is a complex industrial process which is difficult to control. It is difficult to obtain a good control result by using the traditional control method, so a temperature control method with good performance is of great significance. Particle swarm optimization (PSO) has evolved recently as an important branch of stochastic techniques to explore the search space for optimization (Kennedy & Eberhart, 1995). The motivation for the development of this method is based on simulation of simplified social behavior such as bird flocking or fish schooling. Nowadays, PSO has been developed to be real competitor with other well-established techniques for population-based evolutionary computation. PSO has many advantages over other evolutionary computation techniques (for example, genetic algorithms (GAs)), such as simpler implementation, faster convergence rate and fewer parameters to adjust. The proposed scheme is applied to the optimal of the continuous annealing process. Simulation shows the proposed approach is effective.",https://ieeexplore.ieee.org/document/5262851/,2009 IEEE International Conference on Automation and Logistics,5-7 Aug. 2009,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ChiCC.2017.8027747,Optimal operational control for industrial processes based on Q-learning method,IEEE,Conferences,"It is difficult to accurately model productive processes and describe relationship between operational indices and controlled variables for complex modem industrial processes. How to design the optimal setpoints by using only data generated by operational processes, without requiring the knowledge of model parameters of operational processes, poses a challenge on designing optimal setpoints. This paper presents a state-observer based Q-learning algorithm to learn the optimal setpoints by utilizing only data, such that the real operational indices can track the desired values in an approximately optimal manner. A simulation experiment in flotation process is implemented to show the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/8027747/,2017 36th Chinese Control Conference (CCC),26-28 July 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/cp.2013.0004,Optimised multivariable nonlinear predictive control for coupled tank applications,IET,Conferences,"This paper presents the design of a novel nonlinear model predictive control (NMPC) strategy using a stochastic genetic algorithm (GA) to control highly nonlinear, uncertain and complex multivariable process with significant cross coupling effects between the process input and output variables. Raw multi-input and multi-output (MIMO) data from an experimental setup were collected and analysed. Both a GA and a backpropagation gradient descent based approach known as Levenberg-Marquardt Algorithm (LMA) are employed to train artificial neural network (ANN) nonlinear model. Real time practical experimental implementation on a MIMO coupled tank system is performed and the results show the effectiveness of the strategy. The approach can easily be adapted to other industrial processes.",https://ieeexplore.ieee.org/document/6613717/,IET Conference on Control and Automation 2013: Uniting Problems and Solutions,4-5 June 2013,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICOIN50884.2021.9334026,Optimization of RSSI based indoor localization and tracking to monitor workers in a hazardous working zone using Machine Learning techniques,IEEE,Conferences,"This paper proposes a method for RSSI based indoor localization and tracking in cluttered environments using Deep Neural Networks. We implemented a real-time system to localize people using wearable active RF tags and RF receivers fixed in an industrial environment with high RF noise. The proposed solution is advantageous in analysing RSSI data in cluttered-indoor environments with the presence of human body attenuation, signal distortion, and environmental noise. Simulations and experiments on a hardware testbed demonstrated that receiver arrangement, number of receivers and amount of line of sight signals captured by receivers are important parameters for improving localization and tracking accuracy. The effect of RF signal attenuation through the person who carries the tag was combined with two neural network models trained with RSSI data pertaining to two walking directions. This method was successful in predicting the walking direction of the person.",https://ieeexplore.ieee.org/document/9334026/,2021 International Conference on Information Networking (ICOIN),13-16 Jan. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICFTIC54370.2021.9647400,Optimized Yolov3 Deployment on Jetson TX2 With Pruning and Quantization,IEEE,Conferences,"Pruning and Quantization are commonly techniques deployed on deep convolutional networks to accelerate the model and reducing size, taking the resolution as a sacrifice. In embedded systems where computation power is limited due to power and cost constraints, pruning and quantization is mandatory for such networks especially where real-time processing is required, such as image classification in live video streams. In this paper, a pruned and quantized YOLOv3 model is deployed on Nvidia's industrial standard model Jetson TX2, which demonstrated an increase of 5 FPS in image classification via an 640×480 USB camera, while allocating only 16.7% storage on disk.",https://ieeexplore.ieee.org/document/9647400/,2021 IEEE 3rd International Conference on Frontiers Technology of Information and Computer (ICFTIC),12-14 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCVE45908.2019.8965211,Optimizing coverage of simulated driving scenarios for the autonomous vehicle,IEEE,Conferences,"Self-driving cars and advanced driver-assistance systems are perceived as a game-changer in the future of road transportation. However, their validation is mandatory before industrialization; testing every component should be assessed intensively in order to mitigate potential failures and avoid unwanted problems on the road. In order to cover all possible scenarios, virtual simulations are used to complement real-test driving and aid in the validation process. This paper focuses on the validation of the command law during realistic virtual simulations. Its aim is to detect the maximum amount of failures while exploring the input search space of the scenarios. A key industrial restriction, however, is to launch simulations as little as possible in order to minimize computing power needed. Thus, a reduced model based on a random forest model helps in decreasing the number of simulations launched. It accompanies the algorithm in detecting the maximum amount of faulty scenarios everywhere in the search space. The methodology is tested on a tracking vehicle use case, which produces highly effective results.",https://ieeexplore.ieee.org/document/8965211/,2019 IEEE International Conference on Connected Vehicles and Expo (ICCVE),4-8 Nov. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MEMCOD.2015.7340480,Passive testing of production systems based on model inference,IEEE,Conferences,"This paper tackles the problem of testing production systems, i.e. systems that run in industrial environments, and that are distributed over several devices and sensors. Usually, such systems lack of models, or are expressed with models that are not up to date. Without any model, the testing process is often done by hand, and tends to be an heavy and tedious task. This paper contributes to this issue by proposing a framework called Autofunk, which combines different fields such as model inference, expert systems, and machine learning. This framework, designed with the collaboration of our industrial partner Michelin, infers formal models that can be used as specifications to perform offline passive testing. Given a large set of production messages, it infers exact models that only capture the functional behaviours of a system under analysis. Thereafter, inferred models are used as input by a passive tester, which checks whether a system under test conforms to these models. Since inferred models do not express all the possible behaviours that should happen, we define conformance with two implementation relations. We evaluate our framework on real production systems and show that it can be used in practice.",https://ieeexplore.ieee.org/document/7340480/,2015 ACM/IEEE International Conference on Formal Methods and Models for Codesign (MEMOCODE),21-23 Sept. 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCC51575.2020.9345104,Payload-based Anomaly Detection for Industrial Internet Using Encoder Assisted GAN,IEEE,Conferences,"Payload-based anomaly detection has been proved effective in discovering Internet misbehavior and potential intrusions, but highly relies on the unstructured feature engineering to generalize the distribution of normal payloads. This kind of generalization may not adapt well to the emerging industrial Internet, where the normal behaviors are more diverse and usually embedded in the raw payloads' local structures. In this paper, we tackle this generalization problem and propose a very different solution to payload-based anomaly detection without the need of feature engineering. Our basic idea is to learn the raw structures of normal payloads directly by a generative adversarial network (GAN), in which we have a generator (i.e., a reversed convolutional decoder) to sample raw payloads from a latent space as well as a discriminator (i.e., a convolutional classifier) to guide the generator produce raw payloads approximating the normal structures. We also deploy an assisted convolutional encoder to map the true payloads back to the latent space and combine with the GAN's decoder (i.e., generator) to reconstruct the payload structures. We consider anomalies appear in condition the re-constructed payloads are largely deviated from the true ones, since our encoder-decoder architecture is trained able to rebuild only the normal payload structures. We have evaluated our solution using extensive experiments on real-world industrial Internet datasets, and confirmed its effectiveness in detecting industrial Internet anomalies in the raw payloads.",https://ieeexplore.ieee.org/document/9345104/,2020 IEEE 6th International Conference on Computer and Communications (ICCC),11-14 Dec. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCI49374.2020.9145976,PeTIT: Perceiving the Technological Innovation Trends via the Heuristic Model of Community Detection,IEEE,Conferences,"Patent analysis is widely used in many kinds of research, such as competitive intelligence analysis, technology trends perceiving, industrial distribution planning, macro-economy regulations, and so on. Based on the patent data, this paper proposed a novel algorithm, which named PeTIT, to perceive the technological innovation trends by applying the heuristic community detection model. The PeTIT algorithm included four steps: patent ontology extraction, technological innovation tree construction, technological innovation community detection, and technological innovation trends perceiving. We implemented the PeTIT algorithm on the real dataset of invention patents in the field of artificial intelligence in China, which ranged from Jun 1st, 2000 to May 1st, 2019. The results showed that the innovation situation of artificial intelligent was mainly concentrated in 10 fields. Moreover, the application field of intelligent driving has become one of the fastest-growing industries. Finally, the experimental results demonstrated that the cubic exponential smoothing model had a higher performance by perceiving the technological innovation trends.",https://ieeexplore.ieee.org/document/9145976/,2020 2nd International Conference on Computer Communication and the Internet (ICCCI),26-29 June 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIEA.2006.257304,Performance Studies of Fuzzy Logic Based PI-like Controller Designed for Speed Control of Switched Reluctance Motor,IEEE,Conferences,"Switched reluctance motor (SRM) has gained significant interest in the field of industrial drive. The controller used to drive the machine is conventional PI controller. But the machine characteristics are very much nonlinear. This poses a problem for conventional controller design as regards to maintaining steady performance. There is also a need to adapt to the variable operating conditions. Fuzzy logic based heuristics is prospective since the exact analytical modelling of the system is difficult. PC implementation of the controller offers great flexibility in both design and maintenance phase. This work implements a PI like fuzzy logic controller (FLC) for SRM, which is found to work successfully in real time conditions. The work compares the performance of the FLC with respect to the conventional PI controller",https://ieeexplore.ieee.org/document/4025905/,2006 1ST IEEE Conference on Industrial Electronics and Applications,24-26 May 2006,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00221,Perturbation of Image-based Malware Detection with Smali level morphing techniques,IEEE,Conferences,"Considering the weaknesses of signature-based approaches adopted by current antimalware, from both academic and industrial side there is a boost in the development of techniques exploiting artificial intelligence, where one of the most promising are based on the representation of application under analysis as image. In order to understand whether these approaches can be effectively adopted in the real-world, starting from a detector based on deep learning, in this paper we evaluate the resilience of these approaches when morphed samples are considered. We present DexWave, a tool aimed to automatically inject perturbations techniques targeting the smali code representation of Android applications. The experimental analysis demonstrate that image-based malware classifier are vulnerable to simple perturbations attack.",https://ieeexplore.ieee.org/document/9644887/,"2021 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)",30 Sept.-3 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNSC.2014.6906671,Pixelwise object class segmentation based on synthetic data using an optimized training strategy,IEEE,Conferences,"In this paper we present an approach for low-level body part segmentation based on RGB-D data. The RGB-D sensor is thereby placed at the ceiling and observes a shared workspace for human-robot collaboration in the industrial domain. The pixelwise information about certain body parts of the human worker is used by a cognitive system for the optimization of interaction and collaboration processes. In this context, for rational decision making and planning, the pixelwise predictions must be reliable despite the high variability of the appearance of the human worker. In our approach we treat the problem as a pixelwise classification task, where we train a random decision forest classifier on the information contained in depth frames produced by a synthetic representation of the human body and the ceiling sensor, in a virtual environment. As shown in similar approaches, the samples used for training need to cover a broad spectrum of the geometrical characteristics of the human, and possible transformations of the body in the scene. In order to reduce the number of training samples and the complexity of the classifier training, we therefore apply an elaborated and coupled strategy for randomized training data sampling and feature extraction. This allows us to reduce the training set size and training time, by decreasing the dimensionality of the sampling parameter space. In order to keep the creation of synthetic training samples and real-world ground truth data simple, we use a highly reduced virtual representation of the human body, in combination with KINECT skeleton tracking data from a calibrated multi-sensor setup. The optimized training and simplified sample creation allows us to deploy standard hardware for the realization of the presented approach, while yielding a reliable segmentation in real-time, and high performance scores in the evaluation.",https://ieeexplore.ieee.org/document/6906671/,2014 First International Conference on Networks & Soft Computing (ICNSC2014),19-20 Aug. 2014,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1991.174701,Planning based sensing and task executing in an autonomous machine,IEEE,Conferences,"Implementing a control system for an autonomous machine is a challenging task. Several techniques have to be applied, such as task planning, hierarchical and/or distributed control, and advanced sensing techniques. In addition, to be useful these various techniques have to be integrated into a system that has to operate more or less in real-time. The authors present a control scheme based on hierarchically organized planning-executing-monitoring-cycles which is used to solve some of the problems related to real-time control of an autonomous machine. The implementation is also presented in which the control system is applied in a pilot system based on an industrial robot.<>",https://ieeexplore.ieee.org/document/174701/,Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91,3-5 Nov. 1991,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN45523.2021.9557519,Platform Generation for Edge AI Devices with Custom Hardware Accelerators,IEEE,Conferences,"In recent years artificial neural networks (NNs) have been at the center of research on data processing. However, their high computational demand often prohibits deployment on resource-constrained Industrial IoT Systems. Custom hardware accelerators can enable real-time NN processing on small-scale edge devices but are generally hard to develop and integrate. In this paper we present a hardware generation approach to rapidly create, test, and deploy entire SoC platforms with application-specific NN hardware accelerators. The feasibility of the approach is demonstrated by the generation of a condition monitoring system for high-speed valves.",https://ieeexplore.ieee.org/document/9557519/,2021 IEEE 19th International Conference on Industrial Informatics (INDIN),21-23 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/DATE54114.2022.9774772,Practical identity recognition using WiFi&#x0027;s Channel State Information,IEEE,Conferences,"Identity recognition is increasingly used to control access to sensitive data, restricted areas in industrial, healthcare, and defense settings, as well as in consumer electronics. To this end, existing approaches are typically based on collecting and analyzing biometric data and imply severe privacy con-cerns. Particularly when cameras are involved, users might even reject or dismiss an identity recognition system. Furthermore, iris or fingerprint scanners, cameras, microphones, etc., imply installation and maintenance costs and require the user&#x0027;s active participation in the recognition procedure. This paper proposes a non-intrusive identity recognition system based on analyzing WiFi&#x0027;s Channel State Information (CSI). We show that CSI data attenuated by a person&#x0027;s body and typical movements allows for a reliable identification - even in a sitting posture. We further propose a lightweight deep learning algorithm trained using CSI data, which we implemented and evaluated on an embedded platform (i.e., a Raspberry Pi 4B). Our results obtained using real-world experiments suggest a high accuracy in recognizing people&#x0027;s identity, with a specificity of 98&#x0025; and a sensitivity of 99&#x0025;, while requiring a low training effort and negligible cost.",https://ieeexplore.ieee.org/document/9774772/,"2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)",14-23 March 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/cp:19970735,Pre-processing of acoustic signals by neural networks for fault detection and diagnosis of rolling mill,IET,Conferences,"Incipient faults and changes in the structure of any industrial process may be detected and known by their effects: vibration and/or acoustic signals. We consider some methods for pre-processing the acoustic signal, generated by a rolling mill process, for fault detection and structural classification. The pre-processing methods are based on artificial neural networks. The methods refer to: signal decomposition algorithms, a distances measure for spectral amplitude classification and neural network structures for spectrum compression. For the signal decomposition problem an adaptive neural network algorithm is proposed in which the number of inputs is adapted to the imposed error. When the training error for two successive steps is very little, then the number of inputs in network is increased. If the spectral components are zero for sufficient time, then the number of inputs is decreased. The Hausdorff distance is proposed for spectrum classification as the distance measure for the frequency domain in a pattern recognition context. It shown that the Hausdorff distance has a monotone relationship with the signal-to-noise-ratio. Finally, the possibility of decreasing the number of spectrum components as patterns is presented, by compression with neural networks. Spectral representations of the acoustic source show that signatures collected at rolling mill sensor locations can be successfully used to identify process and structural changes in the rolling mill monitoring system. The results obtained by simulation is encouraging for real-time implementation.",https://ieeexplore.ieee.org/document/607526/,Fifth International Conference on Artificial Neural Networks (Conf. Publ. No. 440),7-9 July 1997,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTEICT49044.2020.9315660,Prediction of Air Quality in Industrial Area,IEEE,Conferences,"Air quality monitoring and prediction in many industrial and urban areas, it has become one of the most important activities. Owing to different types of pollution, air quality is heavily affected. With increasing air pollution, efficient air quality monitoring models is to be implemented; these models gather data on the concentration of air pollutants. In a proposed approach, to solve three problems- prediction, interpolation and feature analysis, previously these problems were solved using three different models but now in the proposed system can solve these three problems in one model i.e Air Pollutant Prediction. This approach relates to unlabeled spatiotemporal data to enhance interpolation efficiency and air quality prediction. Experiments to test the proposed solution based on the real-time data sources collected by the Karnataka State Pollution Control Board (KSPCB), India. The goal of this research paper is to explore various strategies based on machine learning techniques for monitoring and predicating the air quality.",https://ieeexplore.ieee.org/document/9315660/,"2020 International Conference on Recent Trends on Electronics, Information, Communication & Technology (RTEICT)",12-13 Nov. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN41052.2019.8972094,Probabilistic Modelling combined with a CNN for boundary detection of carbon fiber fabrics,IEEE,Conferences,For many industrial machine vision applications it is difficult to acquire good training data to deploy deep learning techniques. In this paper we propose a method based on probabilistic modelling and rendering to generate artificial images of carbon fiber fabrics. We deploy a convolutional neural network (CNN) to learn detection of fabric contours from artificially generated images. Our network largely follows the recently proposed U-Net architecture. We provide results for a set of real images taken under controlled lighting conditions. The method can easily be adapted to similar problems in quality control for composite parts.,https://ieeexplore.ieee.org/document/8972094/,2019 IEEE 17th International Conference on Industrial Informatics (INDIN),22-25 July 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2010.5549413,Process optimization of service-oriented automation devices based on Petri nets,IEEE,Conferences,"This paper introduces a novel method for the specification and selection of criteria-weighted operation modes for the orchestration of services in industrial automation using Petri nets. The objective is to provide to the internal decision support system of a service-oriented automation device or of another applicable computational system the capability to select the best path in a Petri net orchestration model considering different criteria to evaluate the quality of services, such as the time, energy efficiency and reliability. The transition-invariants obtained from the Petri net represent the set of possible modi operandi and these are then weighted with decision criteria. The result will be afterwards evaluated in order to select the optimal modus operandi to be executed by the device. Based on the experiments, this method permits the dynamic optimization of processes in real-time, considering available parameters from devices and other resources.",https://ieeexplore.ieee.org/document/5549413/,2010 8th IEEE International Conference on Industrial Informatics,13-16 July 2010,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECCTD.2011.6043628,Production test of an RF receiver chain based on ATM combining RF BIST and machine learning algorithm,IEEE,Conferences,"Testing an RF device in Production is expensive and technically difficult. At Wafer Test level, the RF probing technologies hardly fulfil the industrial test requirements in terms of accuracy, reliability and cost. At Package test level testing the RF parameters requires expensive RF equipments (RF automated test equipments (ATE)) and for complex RF transceivers, which address multi-modes (RF multi-paths and/or requiring different impedance matchings), it usually leads to prohibitive test time. In order to reduce the test costs for RF devices, different methods are proposed and evaluated in NXP and at competitions. These methods mainly target test time reduction (e.g. by testing parts in parallel) or propose ways of limiting the needs of expensive RF tester in Production (e.g. by using Alternate Test Methods, Design For Test, or Built In Test). In the proposed presentation we will focus on ATM and RF BI(s)T, providing some results on DC-RF correlation and an example of a real case BIT implementation into a fully integrated single-chip receiver operating in the sub-GHz ISM bands 315 MHz to 920 MHz.",https://ieeexplore.ieee.org/document/6043628/,2011 20th European Conference on Circuit Theory and Design (ECCTD),29-31 Aug. 2011,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SoCPaR.2011.6089156,QoS-oriented Service Management in clouds for large scale industrial activity recognition,IEEE,Conferences,"Motivated by the need of industrial enterprises for supervision services for quality, security and safety guarantee, we have developed an Activity Recognition Framework based on computer vision and machine learning tools, attaining good recognition rates. However, the deployment of multiple cameras to exploit redundancies, the large training set requirements of our time series classification models, as well as general resource limitations together with the emphasis on real-time performance, pose significant challenges and lead us to consider a decentralized approach. We thus adapt our application to a new and innovative real-time enabled framework for service-based infrastructures, which has developed QoS-oriented Service Management mechanisms in order to allow cloud environments to facilitate real-time and interactivity. Deploying the Activity Recognition Framework in a cloud infrastructure can therefore enable it for large scale industrial environments.",https://ieeexplore.ieee.org/document/6089156/,2011 International Conference of Soft Computing and Pattern Recognition (SoCPaR),14-16 Oct. 2011,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COGINF.2010.5599677,Quadratic neural unit is a good compromise between linear models and neural networks for industrial applications,IEEE,Conferences,"The paper discusses the quadratic neural unit (QNU) and highlights its attractiveness for industrial applications such as for plant modeling, control, and time series prediction. Linear systems are still often preferred in industrial control applications for their solvable and single solution nature and for the clarity to the most application engineers. Artificial neural networks are powerful cognitive nonlinear tools, but their nonlinear strength is naturally repaid with the local minima problem, overfitting, and high demands for application-correct neural architecture and optimization technique that often require skilled users. The QNU is the important midpoint between linear systems and highly nonlinear neural networks because the QNU is relatively very strong in nonlinear approximation; however, its optimization and performance have fast and convex-like nature, and its mathematical structure and the derivation of the learning rules is very comprehensible and efficient for implementation.",https://ieeexplore.ieee.org/document/5599677/,9th IEEE International Conference on Cognitive Informatics (ICCI'10),7-9 July 2010,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPSAsia48933.2020.9208545,Quantile Huber Function Guided TCN for Short-Term Consumer-Side Probabilistic Load Forecasting,IEEE,Conferences,"The consumer-side load probabilistic forecasting is meaningful and useful in the smart grid area and becoming more and more possible with the explosion of electricity consumption data collected by smart meters. Compared to aggregated load, forecasting on the consumer-side load prefers to be stochastic and uncertain, which places higher demands on the forecast performance of models. In this paper, a novel probabilistic forecasting method, quantile Huber guided Temporal convolutional network (TCN), is proposed to quantify the variability and uncertainty of consumer-side load forecasting. Specifically, TCN is a new deep neural network implemented for capturing the dependencies information within the load sequences. The quantile Huber function formulated weighted average multiple losses (WAML) function is used to generate probabilistic prediction intervals (PIs). Conditional distribution of deterministic forecasting residual is also considered to calibrate the probabilistic forecasting. The performance of the proposed forecasting method is validated by numerical experiments using real load datasets of residential and industrial consumers. The comparative results show the superiority of the proposed method over traditional methods.",https://ieeexplore.ieee.org/document/9208545/,2020 IEEE/IAS Industrial and Commercial Power System Asia (I&CPS Asia),13-15 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONFLUENCE.2019.8776960,Quasi-Automated Firmware in E-Automobiles: Structural Integration,IEEE,Conferences,"Intelligent transit mechanism has become the need of the hour. The topical panacea seems to centre on the development of labyrinthine Automated Vehicles. With the inevitable boom in areas like Machine Learning, Industrial Automation coupled with highly convergent heuristics and the advent of highly efficient low latency communication devices, the idea of Automated Vehicles inches closer to practical realization every passing day. Design for any vehicle can be described as a function of various interdependent parameters, which are generally defined by the level of convolution and the level of automation defined for a system.This study explores the controller system design of Quasi-Automated Electric Vehicles. Categorically, this paper is an attempt at examining novel and innovative ways of designing a schematic for control of speed and navigation subsystems along with an exhaustive feedback capability designed to give users a real time virtual emulation of the vehicle. A wide ranging discussion on possible topologies for effective implementation of feedback have also been depicted in this paper. A column-type power steering system has been investigated as a control system for navigation of direction while an inverter based speed control mechanism has also been proposed. Furthermore, the discussed control algorithms have been rigorously tested and consequently proved capable of providing 1st level of autonomy, as defined by Society of Automotive Engineers standards while also reflecting a potential schematic for integration of intelligent learning firmware in the near future.",https://ieeexplore.ieee.org/document/8776960/,"2019 9th International Conference on Cloud Computing, Data Science & Engineering (Confluence)",10-11 Jan. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE.2015.7281681,RFID indoor localization based on support vector regression and k-means,IEEE,Conferences,"Systems need to know the physical locations of objects and people to optimize user experience and solve logistical and security issues. Also, there is a growing demand for applications that need to locate individual assets for industrial automation. This work proposes an indoor positioning system (IPS) able to estimate the item-level location of stationary objects using off-the-shelf equipment. By using RFID technology, a machine learning model based on support vector regression (SVR) is proposed. A multi-frequency technique is developed in order to overcome off-the-shelf equipment constraints. A k-means approach is also applied to improve accuracy. We have implemented our system and evaluated it using real experiments. The localization error is between 17 and 31 cm in 2.25m2 area coverage.",https://ieeexplore.ieee.org/document/7281681/,2015 IEEE 24th International Symposium on Industrial Electronics (ISIE),3-5 June 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIIC48513.2020.9065249,RNN-based Prediction for Network Intrusion Detection,IEEE,Conferences,"We investigate a prediction model using RNN for network intrusion detection in industrial IoT environments. For intrusion detection, we use anomaly detection methods that estimate the next packet, measure and score the distance measurement in real packets to distinguish whether it is a normal packet or an abnormal packet. When the packet was learned in the LSTM model, two-gram and sliding window of N-gram showed the best performance in terms of errors and the performance of the LSTM model was the highest compared with other data mining regression techniques. Finally, cosine similarity was used as a scoring function, and anomaly detection was performed by setting a boundary for cosine similarity that consider as normal packet.",https://ieeexplore.ieee.org/document/9065249/,2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),19-21 Feb. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISDA.2014.7066281,ROS-based remote controlled robotic arm workcell,IEEE,Conferences,"This paper describes robotic workplace that is being developed at our faculty. It consists of industrial arm Mitsubishi Melfa RV-6SL, gripper Schunk and six Axis cameras. The purpose of this workplace is to serve students for testing algorithms from artificial intelligence and computer vision. It also gives them the opportunity to work with real industrial manipulator and allow them to test things like kinematics and dynamics of the arm. The arm is inaccessible for students so it can be operated only remotely. The software needed for remote controlling and programming is described in this paper.",https://ieeexplore.ieee.org/document/7066281/,2014 14th International Conference on Intelligent Systems Design and Applications,28-30 Nov. 2014,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9562075,Reaching Pruning Locations in a Vine Using a Deep Reinforcement Learning Policy,IEEE,Conferences,"We outline a neural network-based pipeline for perception, control and planning of a 7 DoF robot for tasks that involve reaching into a dormant grapevine canopy. The proposed system consists of a 6 DoF industrial robot arm and a linear slider that can actuate on an entire grape vine. Our approach uses Convolutional Neural Networks to detect buds in dormant grape vines and a Reinforcement Learning based control strategy to reach desired cut-point locations for pruning tasks. Within this framework, three methodologies are developed and compared to reach the desired locations: the learned policy-based approach (RL), a hybrid method that uses the learned policy and an inverse kinematics solver (RL+IK), and lastly a classical approach commonly used in robotics. We first tested and validated the suitability of the proposed learning methodology in a simulated environment that resembled laboratory conditions. A reaching accuracy of up to 61.90% and 85.71% for the RL and RL+IK approaches respectively was obtained for a vine that the agent observed while learning. When testing in a new vine, the accuracy was up to 66.66% and 76.19% for RL and RL+IK, respectively. The same methods were then deployed on a real system in an end to end procedure: autonomously scan the vine using a vision system, create its model and finally use the learned policy to reach cutting points. The reaching accuracy obtained in these tests was 73.08%.",https://ieeexplore.ieee.org/document/9562075/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IDAP.2017.8090180,Real time fabric defect detection system on Matlab and C++/Opencv platforms,IEEE,Conferences,"In industrial fabric productions, real time systems are needed to detect the fabric defects. This paper presents a real time defect detection approach which compares the time performances of Matlab and C++ programming languages. In the proposed method, important texture features of the fabric images are extracted using CoHOG method. Artificial neural network is used to classify the fabric defects. The developed method has been applied to detect the knitting fabric defects on a circular knitting machine. An overall defect detection success rate of 93% is achieved for the Matlab and C++ applications. To give an idea to the researches in defect detection area, real time operation speeds of Matlab and C++ codes have been examined. Especially, the number of images that can be processed in one second has been determined. While the Matlab based coding can process 3 images in 1 second, C++/Opencv based coding can process 55 images in 1 second. Previous works have rarely included the practical comparative evaluations of software environments. Therefore, we believe that the results of our industrial experiments will be a valuable resource for future works in this area.",https://ieeexplore.ieee.org/document/8090180/,2017 International Artificial Intelligence and Data Processing Symposium (IDAP),16-17 Sept. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCE.2008.4580693,Real time implementation of NARMA L2 feedback linearization and smoothed NARMA L2 controls of a single link manipulator,IEEE,Conferences,"Robotics is a field of modern technology which requires knowledge in vast areas such as electrical engineering, mechanical engineering, computer science as well as finance. Nonlinearities and parametric uncertainties are unavoidable problems faced in controlling robots in industrial plants. Tracking control of a single link manipulator driven by a permanent magnet brushed DC motor is a nonlinear dynamics due to effects of gravitational force, mass of the payload, posture of the manipulator and viscous friction coefficient. Furthermore uncertainties arise because of changes of the rotor resistance with temperature and random variations of friction while operating. Due to this fact classical PID controller can not be used effectively since it is developed based on linear system theory. Neural network control schemes for manipulator control problem have been proposed by researchers; in which their competency is validated through simulation studies. On the other hand, actual real time applications are rarely established. Instead of simulation studies, this paper is aimed to implement neural network controller in real time for controlling a DC motor driven single link manipulator. The work presented in this paper is concentrating on neural NARMA L2 control and its improvement called to as Smoothed NARMA L2 control. As proposed by K. S Narendra and Mukhopadhyay, Narma L2 control is one of the popular neural network architectures for prediction and control. The real time experimentation showed that the Smoothed NARMA L2 is effective for controlling the single link manipulator for both point-to-point and continuous path motion control.",https://ieeexplore.ieee.org/document/4580693/,2008 International Conference on Computer and Communication Engineering,13-15 May 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDPC51184.2020.9388185,Real-Time Inference of Neural Networks on FPGAs for Motor Control Applications,IEEE,Conferences,"Machine learning algorithms are increasingly used in industrial applications for a multitude of use-cases. However, using them in control tasks is a challenge due to real-time requirements and limited resources. In this paper, an implementation scheme for real-time inference of multilayer perceptron (MLP) neural networks on FPGAs is proposed. Design constraints for using MLPs in reinforcement learning agents for motor control applications are derived and accounted for in the implementation. Two MLP architectures are evaluated on an FPGA, and the timing and resource-usage data are reported. The real-time capability of the implementation for motor control applications is investigated for standard control frequencies. It is shown by experimental validation that real-time interference with an area-efficient implementation for motor control applications is achievable. Therefore, the proposed implementation scheme can be applied to deep reinforcement learning controllers with hard real-time requirements.",https://ieeexplore.ieee.org/document/9388185/,2020 10th International Electric Drives Production Conference (EDPC),8-9 Dec. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2CACIS.2019.8825093,Real-Time Robotic Grasping and Localization Using Deep Learning-Based Object Detection Technique,IEEE,Conferences,"This work aims to increase the impact of computer vision on robotic positioning and grasping in industrial assembly lines. Real-time object detection and localization problem is addressed for robotic grasp-and-place operation using Selective Compliant Assembly Robot Arm (SCARA). The movement of SCARA robot is guided by deep learning-based object detection for grasp task and edge detection-based position measurement for place task. Deep Convolutional Neural Network (CNN) model, called KSSnet, is developed for object detection based on CNN Alexnet using transfer learning approach. SCARA training dataset with 4000 images of two object categories associated with 20 different positions is created and labeled to train KSSnet model. The position of the detected object is included in prediction result at the output classification layer. This method achieved the state-of-the-art results at 100% precision of object detection, 100% accuracy for robotic positioning and 100% successful real-time robotic grasping within 0.38 seconds as detection time. A combination of Zerocross and Canny edge detectors is implemented on a circular object to simplify the place task. For accurate position measurement, the distortion of camera lens is removed using camera calibration technique where the measured position represents the desired location to place the grasped object. The result showed that the robot successfully moved to the measured position with positioning Root Mean Square Error (0.361, 0.184) mm and 100% for successful place detection.",https://ieeexplore.ieee.org/document/8825093/,2019 IEEE International Conference on Automatic Control and Intelligent Systems (I2CACIS),29-29 June 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNSC52481.2021.9702175,"Real-Time Sensor Fault Detection, Isolation and Accommodation for Industrial Digital Twins",IEEE,Conferences,"The development of Digital Twins (DTs) has bloomed significantly in last years and related use cases are now pervading several application domains. DTs are built upon Internet of Things (IoT) and Industrial IoT platforms and critically rely on the availability of reliable sensor data. To this aim, in this article, we propose a sensor fault detection, isolation and accommodation (SFDIA) architecture based on machine-learning methodologies. Specifically, our architecture exploits the available spatio-temporal correlation in the sensory data in order to detect, isolate and accommodate faulty data via a bank of estimators, a bank of predictors and one classifier, all implemented via multi-layer perceptrons (MLPs). Faulty data are detected and isolated using the classifier, while isolated sensors are accommodated using the estimators. Performance evaluation confirms the effectiveness of the proposed SFDIA architecture to detect, isolate and accommodate faulty data injected into a (real) wireless sensor network (WSN) dataset.",https://ieeexplore.ieee.org/document/9702175/,"2021 IEEE International Conference on Networking, Sensing and Control (ICNSC)",3-5 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SKIMA47702.2019.8982486,Real-Time Video Dehazing for Industrial Image Processing,IEEE,Conferences,"In today's industries, automation, reliability, robustness and accuracy are pivotal problem to cut costs and increase productivity and quality. Visual sensor networks are vital control and monitoring tools for continues, on-line imaging and real time image processing in production and plant process. Most of the industrial videos are captured in hazy weather and usually degraded by suspended particles of atmosphere, such as smoke, fog, rain, and snow, which limits the visual quality of image. This hinders the ability of artificial intelligent driven systems to achieve automation, reliability and accuracy. Recovery of the clear visuals from the input hazy videos is challenging problem. Instead of relying on explicitly estimating the key component of atmospheric scattering model, we present end-to-end CNN model, which directly recovers the clear images from hazy images. This end-to-end architecture makes it an ideal pre-processing tool into other deep models for increasing the efficiency of various computer vision tasks in real time systems, such as Retina-Net for object detection, ResNet for object recognition. Experimental results demonstrate the effectiveness and robustness of proposed framework by outperforming the stat-of-the-art approaches in terms of time complexity and visual quality.",https://ieeexplore.ieee.org/document/8982486/,"2019 13th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)",26-28 Aug. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2006.1713119,Real-time Advanced Intelligent Control Software Package,IEEE,Conferences,"A real-time advanced intelligent control software package is introduced in detail in this paper. It can be easily used to develop expert control system, fuzzy control system and neural network control system, meanwhile, it can work together with various industrial configuration software and equipments. It will promote the application of intelligent control technology",https://ieeexplore.ieee.org/document/1713119/,2006 6th World Congress on Intelligent Control and Automation,21-23 June 2006,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC51774.2021.00070,Real-time End-to-End Federated Learning: An Automotive Case Study,IEEE,Conferences,"With the development and the increasing interests in ML/DL fields, companies are eager to apply Machine Learning/Deep Learning approaches to increase service quality and customer experience. Federated Learning was implemented as an effective model training method for distributing and accelerating time-consuming model training while protecting user data privacy. However, common Federated Learning approaches, on the other hand, use a synchronous protocol to conduct model aggregation, which is inflexible and unable to adapt to rapidly changing environments and heterogeneous hardware settings in real-world scenarios. In this paper, we present an approach to real-time end-to-end Federated Learning combined with a novel asynchronous model aggregation protocol. Our method is validated in an industrial use case in the automotive domain, focusing on steering wheel angle prediction for autonomous driving. Our findings show that asynchronous Federated Learning can significantly improve the prediction performance of local edge models while maintaining the same level of accuracy as centralized machine learning. Furthermore, by using a sliding training window, the approach can minimize communication overhead, accelerate model training speed and consume real-time streaming data, proving high efficiency when deploying ML/DL components to heterogeneous real-world embedded systems.",https://ieeexplore.ieee.org/document/9529467/,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",12-16 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMD53520.2021.9670852,Real-time Minor Defect Recognition of Pseudo-Terahertz Images via the Improved YOLO Network,IEEE,Conferences,"Terahertz (THz) imaging has been widely used in non-destructive testing (NDT) of nonpolar materials owing to its unique properties of remarkable accuracy. However, THz imaging has been suffered from serious constraints such data deficiency, low spatial resolution, blurred contour and high background noise due to the limitation of THz wavelength and the agonizingly delayed development of THz devices. Here we have proposed a degradation model to generate massive PCB images with THz characteristics (named as PCB Pseudo-THz images) to overcome the shortcoming of the deficient dataset of THz imaging. Then, the modified YOLO V4 network is proposed to precisely identify four different types of defects on the PCB board. Moreover, the concept of transfer learning is also implemented to improve the detection and classification accuracy of various types of defects. The proposed model can not only obtain accurate detection of minor defects in the PCB samples that are inaccessible by human eyes, but also achieve the real-time fault classification and location. Overall, our proposed method can be beneficial to generalize the THz NDT in the frequency domain on the minor defects of nonpolar material, which will fulfill the impending requirements of real-time defect detection for the industrial applications.",https://ieeexplore.ieee.org/document/9670852/,"2021 International Conference on Sensing, Measurement & Data Analytics in the era of Artificial Intelligence (ICSMD)",21-23 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRV50864.2020.00032,Real-time Motion Planning for Robotic Teleoperation Using Dynamic-goal Deep Reinforcement Learning,IEEE,Conferences,"We propose Dynamic-goal Deep Reinforcement Learning (DGDRL) method to address the problem of robot arm motion planning in telemanipulation applications. This method intuitively maps human hand motions to a robot arm in real-time, while avoiding collisions, joint limits and singularities. We further propose a novel hardware setup, based on the HTC VIVE VR system, that enables users to smoothly control the robot tool position and orientation with hand motions, while monitoring its movements in a 3D virtual reality environment. A VIVE controller captures 6D hand movements and gives them as reference trajectories to a deep neural policy network for controlling the robot’s joint movements. Our DGDRL method leverages the state-of-art Proximal Policy Optimization (PPO) algorithm for deep reinforcement learning to train the policy network with the robot joint values and reference trajectory observed at each iteration. Since training the network on a real robot is time-consuming and unsafe, we developed a simulation environment called RobotPath which provides kinematic modeling, collision analysis and a 3D VR graphical simulation of industrial robots. The deep neural network trained using RobotPath is then deployed on a physical robot (ABB IRB 120) to evaluate its performance. We show that the policies trained in the simulation environment can be successfully used for trajectory planning on a real robot. The the codes, data and video presenting our experiments are available at https://github.com/kavehkamali/ppoRobotPath.",https://ieeexplore.ieee.org/document/9108691/,2020 17th Conference on Computer and Robot Vision (CRV),13-15 May 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LCN.2018.8638081,Real-time Performance Evaluation of LTE for IIoT,IEEE,Conferences,"Industrial Internet of Things (IIoT) is claimed to be a global booster technology for economic development. IIoT brings bulky use-cases with a simple goal of enabling automation, autonomation or just plain digitalization of industrial processes. The abundance of interconnected IoT and CPS generate additional burden on the telecommunication networks, imposing number of challenges to satisfy the key performance requirements. In particular, the QoS metrics related to real-time data exchange for critical machine-to-machine type communication. This paper analyzes a real-world example of IIoT from a QoS perspective, such as remotely operated underground mining vehicle. As part of the performance evaluation, a software tool is developed for estimating the absolute, one-way delay in end-to-end transmissions. The measured metric is passed to a machine learning model for one-way delay prediction based on LTE RAN measurements using a commercially available cutting-edge software tool. The achieved results prove the possibility to predict the delay figures using machine learning model with a coefficient of determination up to 90%.",https://ieeexplore.ieee.org/document/8638081/,2018 IEEE 43rd Conference on Local Computer Networks (LCN),1-4 Oct. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR46387.2019.8981549,Real-time RGB-D semantic keyframe SLAM based on image segmentation learning from industrial CAD models,IEEE,Conferences,"This paper presents methods for performing realtime semantic SLAM aimed at autonomous navigation and control of a humanoid robot in a manufacturing scenario. A novel multi-keyframe approach is proposed that simultaneously minimizes a semantic cost based on class-level features in addition to common photometric and geometric costs. The approach is shown to robustly construct a 3D map with associated class labels relevant to robotic tasks. Alternatively to existing approaches, the segmentation of these semantic classes have been learnt using RGB-D sensor data aligned with an industrial CAD manufacturing model to obtain noisy pixel-wise labels. This dataset confronts the proposed approach in a complicated real-world setting and provides insight into the practical use case scenarios. The semantic segmentation network was fine tuned for the given use case and was trained in a semi-supervised manner using noisy labels. The developed software is real-time and integrated with ROS to obtain a complete semantic reconstruction for the control and navigation of the HRP4 robot. Experiments in-situ at the Airbus manufacturing site in Saint-Nazaire validate the proposed approach.",https://ieeexplore.ieee.org/document/8981549/,2019 19th International Conference on Advanced Robotics (ICAR),2-6 Dec. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNP52444.2021.9651927,Real-time Video Transmission Optimization Based on Edge Computing in IIoT,IEEE,Conferences,"In the Industrial Internet of Things (IIoT) scenario, the increase of surveillance equipment brings challenges to the transmission of real-time video. It needs more efficient approaches to finish video transmission with more stability and accuracy. Therefore, we propose a self-adaptive transmission scheme of videos for multi-capture terminals under IIoT in this paper. To fit for the constant variation of network environment, we compress the videos that wait for transmitting from multi-capture terminals by reducing the non-key frames with Graph Convolutional Network (GCN). Moreover, a self-adaptive strategy of transmission is implemented on the Mobile Edge Computing (MEC) server to adjust the transmission volume of processed videos, and a multi-objective optimization algorithm is utilized to optimize the strategy of transmission during the video transmission. The relative experiments are conducted to validate the performance of the proposed scheme.",https://ieeexplore.ieee.org/document/9651927/,2021 IEEE 29th International Conference on Network Protocols (ICNP),1-5 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN48605.2020.9207462,"Real-time anomaly intrusion detection for a clean water supply system, utilising machine learning with novel energy-based features",IEEE,Conferences,"Industrial Control Systems have become a priority domain for cybersecurity practitioners due to the number of cyber-attacks against those systems has increased over the past few years. This paper proposes a real-time anomaly intrusion detector for a model of a clean water supply system. A testbed of such system is implemented using the Festo MPA Control Process Rig. A set of attacks to the testbed is conducted during the control process operation. During the attacks, the energy of the components is monitored and recorded to build a novel dataset for training and testing a total of five traditional supervised machine learning algorithms: K-Nearest Neighbour, Support Vector Machine, Decision Tree, Naïve Bayes and Multilayer Perceptron. The trained machine learning algorithms were built and deployed online, during the control system operation, for further testing. The performance obtained from offline and online training and testing steps are compared. The captures results show that KNN and SVM outperformed the rest of the algorithms by achieving high accuracy scores and low false-positive, false-negative alerts.",https://ieeexplore.ieee.org/document/9207462/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PES.2003.1270511,Real-time power quality waveform recognition with a programmable digital signal processor,IEEE,Conferences,"Power quality (PQ) monitoring is an important issue to electric utilities and many industrial power customers. This paper presents a DSP-based hardware monitoring system based on a recently proposed PQ classification algorithm. The algorithm is implemented with a Texas Instruments (TI) TMS320VC5416 digital signal processor (DSP) with the TI THS1206 12-bit 6 MSPS analog to digital converter. A TI TMS320VC5416 DSP starter kit (DSK) is used as the host board with the THS1206 mounted on a daughter card. The implemented PQ classification algorithm is composed of two processes: feature extraction and classification. The feature extraction projects a PQ signal onto a time-frequency representation (TFR), which is designed for maximizing the separability between classes. The classifiers include a Heaviside-function linear classifier and neural networks with feedforward structures. The algorithm is optimized according to the architecture of the DSP to meet the hard realtime constraints of classifying a 5-cycle segment of the 60 Hz sinusoidal voltage/current signals in power systems. The classification output can be transmitted serially to an operator interface or control mechanism for logging and issue resolution.",https://ieeexplore.ieee.org/document/1270511/,2003 IEEE Power Engineering Society General Meeting (IEEE Cat. No.03CH37491),13-17 July 2003,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NAFIPS.2005.1548498,Reasoning about uncertainty in prognosis: a confidence prediction neural network approach,IEEE,Conferences,"Uncertainty representation and management is the Achilles heel of fault prognosis in condition based management systems. Long-term prediction of the time to failure of critical military and industrial systems entails large-grain uncertainty that must be represented effectively and managed efficiently, i.e. as more data becomes available, means must be devised to narrow or ""shrink"" the uncertainty bounds. Prediction accuracy and precision are typical performance metrics employed to access the performance of prognostic algorithms. That is, we would like the predicted time to failure to be as close as possible to the real one. Also, the bounds or limits of uncertainty must be as ""narrow"" as possible. This paper introduces a novel confidence prediction neural network construct with a confidence distribution node based on a Parzen estimate to represent uncertainty and a learning algorithm implemented as a lazy or Q-learning routine that improves online prognostics estimates. The approach is illustrated with test and simulation results obtained from a faulty helicopter planetary gear plate.",https://ieeexplore.ieee.org/document/1548498/,NAFIPS 2005 - 2005 Annual Meeting of the North American Fuzzy Information Processing Society,26-28 June 2005,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PHM2022-London52454.2022.00042,Recent Research and Applications in Variational Autoencoders for Industrial Prognosis and Health Management: A Survey,IEEE,Conferences,"Whether in the industrial, medical, or real-world domains, more and more data are being collected. The common particularity of all these application domains is that a great part of this data is mostly unlabeled. Thus, designing a learning model with a minimum of labeled data represents a major challenge in the coming years. A particular emphasis has recently been put on unsupervised learning methods based on the idea of autoencoding. The objective of these methods is twofold: to reduce the dimensionality of the input space and to reconstruct the original observation from this lower dimensional representation space. The variational form of these autoencoders, called the Variational Autoencoders (VAEs), is particularly successful in almost all application areas. This enthusiasm comes from the fact that VAEs allow to take advantage of the theoretical foundations of the Variational Bayesian methods and the learning capabilities of artificial neural networks. This review paper gives to the PHM community a synthesis of the latest publications in the PHM domain using the VAEs related to four topics: 1) Data-Driven Soft Sensors for missing values and data outliers, 2) reconstruction error for fault detection, 3) resampling approach for imbalanced data generation and minority class and 4) the variational embedding as PHM preprocessing pipelines and data transformations. After a review of the theoretical foundations and some practical tricks to succeed the implementation of the VAEs in industrial applications, the four main topics used to exploit the VAEs in the PHM domain are detailed. Finally, a global view of the research done at the research institute of Hydro-Qu&#x00E9;bec regarding the diagnosis and failure detection of hydro-generators with VAEs are presented.",https://ieeexplore.ieee.org/document/9808794/,2022 Prognostics and Health Management Conference (PHM-2022 London),27-29 May 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2012.6389018,Recent advances in the application of real-time computational intelligence to industrial electronics,IEEE,Conferences,"The field of computational intelligence [CI] has seen advances in both the theoretical knowledge base of these techniques, and in specific applications of these techniques to real-world problems. This work first attempts to summarize the current trends and definitions in the CI branches of fuzzy systems, artificial neural networks [ANNs], and hybrid neuro-fuzzy systems and their variants. These particular branches of CI are selected for their ability to be implemented in real-time problem solving, whether computation and processing is done in software or implemented in hardware. Then, some current applications of these CI technologies for use in industrial electronics are highlighted and summarized.",https://ieeexplore.ieee.org/document/6389018/,IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society,25-28 Oct. 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEMPED.2019.8864828,Recognition of Electric Machines Boundary as The Constraint of Over Current Relay Coordination in Real Industrial Application with Serial Firefly Algorithm Optimization,IEEE,Conferences,"The electric machines such as motor, transformer, and generator plays an essential role in the production process of a factory. Due to expansion, there is an intention to invest a new electric machine that expected to work correctly with the existing system. To integrate the new electric machines, the existing protection system has to be evaluated carefully without limiting the machine's capability. Based on the actual experiences, one of the frustrating issues during the commissioning stage is energizing failure due to relay mal-trip. The objective of this paper is to summarize and model the electrical machines in the protection system perspective and offers a new method to coordinate the relays with the electric machines recognition using the artificial intelligence of serial firefly algorithm. This proposed method is endorsed by a real industrial power system and will demonstrate the ability to coordinate the relays without violating the electric machine boundary.",https://ieeexplore.ieee.org/document/8864828/,"2019 IEEE 12th International Symposium on Diagnostics for Electrical Machines, Power Electronics and Drives (SDEMPED)",27-30 Aug. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSESS52187.2021.9522231,Recognition of Real-life Activities with Smartphone Sensors using Deep Learning Approaches,IEEE,Conferences,"Due to its vast applications in various industrial sectors, sensor-based human activity recognition (SHAR) has become a prevalent study issue in machine learning (ML) and deep learning (DL). With the improvement of numerous wearable sensors, many effective use cases have recently been revealed. According to recent research, real-world data contains more contextual information than data acquired in a laboratory environment. Three deep learning models were used to investigate real-life activities using smartphone sensors in this study. As two fundamental deep learning approaches, a convolutional neural network (CNN) and a long short-term memory (LSTM) network are used to achieve recognition. In addition, we introduced the Att-CNN-LSTM network as a hybrid DL model to handle the SHAR challenge using an attention mechanism. On a public dataset called real-life HAR (RL-HAR), these three deep learning models were evaluated using four assessment indicators: accuracy, precision, recall, and F1-score. According to the experimental data, the suggested Att-CNN-LSTM surpasses existing baseline deep learning models with the highest average accuracy of 95.76%.",https://ieeexplore.ieee.org/document/9522231/,2021 IEEE 12th International Conference on Software Engineering and Service Science (ICSESS),20-22 Aug. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARM52023.2021.9536145,Reducing the Dimension of the Configuration Space with Self Organizing Neural Networks,IEEE,Conferences,"For robotics, especially industrial applications, it is crucial to reactively plan safe motions through efficient algorithms. Planning is more powerful in the configuration space than the task space. However, for robots with many degrees of freedom, this is challenging and computationally expensive. Sophisticated techniques for motion planning such as the Wavefront algorithm are limited by the high dimensionality of the configuration space, especially for robots with many degrees of freedom. For a neural implementation of the Wavefront algorithm in the configuration space, neurons represent discrete configurations and synapses are used for path planning. In order to decrease the complexity, we reduce the search space by pruning superfluous neurons and synapses. We present different models of self-organizing neural networks for this reduction. The approach takes real-life human motion data as input and creates a representation with reduced dimension. We compare six different neural network models and adapt the Wavefront algorithm to the different structures of the reduced output spaces. The method is backed up by an extensive evaluation of the reduced spaces, including their suitability for path planning by the Wavefront algorithm.",https://ieeexplore.ieee.org/document/9536145/,2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM),3-5 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSME46990.2020.00074,Regression Testing of Massively Multiplayer Online Role-Playing Games,IEEE,Conferences,"Regression testing aims to check the functionality consistency during software evolution. Although general regression testing has been extensively studied, regression testing in the context of video games, especially Massively Multiplayer Online Role-Playing Games (MMORPGs), is largely untouched so far. One big challenge is that game testing requires a certain level of intelligence in generating suitable action sequences among the huge search space, to accomplish complex tasks in the MMORPG. Existing game testing mainly relies on either the manual playing or manual scripting, which are labor-intensive and time-consuming. Even worse, it is often unable to satisfy the frequent industrial game evolution. The recent process in machine learning brings new opportunities for automatic game playing and testing. In this paper, we propose a reinforcement learning-based regression testing technique that explores differential behaviors between multiple versions of an MMORPGs such that the potential regression bugs could be detected. The preliminary evaluation on real industrial MMORPGs demonstrates the promising of our technique.",https://ieeexplore.ieee.org/document/9240641/,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),28 Sept.-2 Oct. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN45582.2020.9442114,Reinforcement Learning Driven Adaptive VR Streaming with Optical Flow Based QoE,IEEE,Conferences,"With the merit of containing full panoramic content in one camera, Virtual Reality (VR) and 360° videos have arisen in the field of industrial cloud manufacturing and training. Industrial Internet of Things (IoT), where many VR terminals needed to be online at the same time, can hardly guarantee VR's bandwidth requirement. However, by making use of users' quality of experience (QoE) awareness factors, including the relative moving speed and depth difference between the viewpoint and other content, bandwidth consumption can be reduced. In this paper, we propose Optical Flow Based VR(OFB-VR), an interactive method of VR streaming that can make use of VR users' QoE awareness to ease the bandwidth pressure. The Just-Noticeable Difference through Optical Flow Estimation (JND-OFE) is explored to quantify users' awareness of quality distortion in 360° videos. Accordingly, a novel 360° videos QoE metric based on Peak Signal-to-Noise Ratio and JND-OFE (PSNR-OF) is proposed. With the help of PSNR-OF, OFB-VR proposes a versatile-size tiling scheme to lessen the tiling overhead. A Reinforcement Learning (RL) method is implemented to make use of historical data to perform Adaptive BitRate (ABR). For evaluation, we take two prior VR streaming schemes, Pano and Plato, as baselines. Vast evaluations show that our system can increase the mean PSNR-OF score by 9.5-15.8% while maintaining the same rebuffer ratio compared with Pano and Plato in a fluctuate LTE bandwidth dataset. Evaluation results show that OFB-VR is a promising prototype for actual interactive industrial VR. A prototype of OFB-VR can be found in https://github.com/buptexplorers/OFB-VR.",https://ieeexplore.ieee.org/document/9442114/,2020 IEEE 18th International Conference on Industrial Informatics (INDIN),20-23 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AI4I46381.2019.00027,Reinforcement Learning of a Robot Cell Control Logic using a Software-in-the-Loop Simulation as Environment,IEEE,Conferences,"This paper introduces a method for automatic robot programming of industrial robots using reinforcement learning on a Software-in-the-loop simulation. The focus of the the paper is on the higher levels of a hierarchical robot programming problem. While the lower levels the skills are stored as domain specific program code, the combination of the skills into a robot control program to solve a specific task is automated. The reinforcement learning learning approach allows the shopfloor workers and technicians just to define the end result of the manufacturing process through a reward function. The programming and process optimization is done within the learning procedure. The Software-in-the-loop simulation with the robot control software makes it possible to to interpret the real program code and generate the exact motion. The exact motion of the robot is needed in order to find not just an optimal but also a collision-free policy.",https://ieeexplore.ieee.org/document/9027783/,2019 Second International Conference on Artificial Intelligence for Industries (AI4I),25-27 Sept. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCS.2015.32,Requirements-Aided Automatic Test Case Generation for Industrial Cyber-physical Systems,IEEE,Conferences,"Industrial cyber-physical systems require complex distributed software to orchestrate many heterogeneous mechatronic components and control multiple physical processes. Industrial automation software is typically developed in a model-driven fashion where abstractions of physical processes called plant models are co-developed and iteratively refined along with the control code. Testing such multi-dimensional systems is extremely difficult because often models might not be accurate, do not correspond accurately with subsequent refinements, and the software must eventually be tested on the real plant, especially in safety-critical systems like nuclear plants. This paper proposes a framework wherein high-level functional requirements are used to automatically generate test cases for designs at all abstraction levels in the model-driven engineering process. Requirements are initially specified in natural language and then analyzed and specified using a formalized ontology. The requirements ontology is then refined along with controller and plant models during design and development stages such that test cases can be generated automatically at any stage. A representative industrial water process system case study illustrates the strengths of the proposed formalism. The requirements meta-model proposed by the CESAR European project is used for requirements engineering while IEC 61131-3 and model-driven concepts are used in the design and development phases. A tool resulting from the proposed framework called REBATE (Requirements Based Automatic Testing Engine) is used to generate and execute test cases for increasingly concrete controller and plant models.",https://ieeexplore.ieee.org/document/7384248/,2015 20th International Conference on Engineering of Complex Computer Systems (ICECCS),9-12 Dec. 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICISS.2010.5656975,Research and implementation Of the temperature control system of heat treatment based on .NET and RS-485 bus,IEEE,Conferences,"RS-485 is a widely used industrial field bus. The successful application of AIBUS protocol in AI series display control instrument, make the AIDCS system's cost significantly lower than traditional DCS system. The paper successfully developed a prototype system based on RS-485 bus for high precision temperature control system of heat treatment, and based on the .net and AIBUS protocol, developed it' s system management software. This system have the characteristics of good real-time, high control-precision, high degree of automation, and friendly human-machine interface.",https://ieeexplore.ieee.org/document/5656975/,2010 International Conference on Intelligent Computing and Integrated Systems,22-24 Oct. 2010,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIBA52610.2021.9687994,Research on Security Defense System of Industrial Control Network,IEEE,Conferences,"The importance of the security of industrial control network has become increasingly prominent. Aiming at the defects of main security protection system in the intelligent manufacturing industrial control network, we propose a security attack risk detection and defense, and emergency processing capability synchronization technology system suitable for the intelligent manufacturing industrial control system. Integrating system control and network security theories, a flexible and reconfigurable system-wide security architecture method is proposed. On the basis of considering the high availability and strong real-time of the system, our research centers on key technologies supporting system-wide security analysis, defense strategy deployment and synchronization, including weak supervision system reinforcement and pattern matching, etc.. Our research is helpful to solve the problem of industrial control network of &#x201C;old but full of loopholes&#x201D; caused by the long-term closed development of the production network of important parts, and alleviate the contradiction between the high availability of the production system and the relatively backward security defense measures.",https://ieeexplore.ieee.org/document/9687994/,"2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)",17-19 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAC51589.2020.9326677,Research on Ultrasonic Belt Velocimeter Based on DSC,IEEE,Conferences,"The belt conveyor has been used in the industrial production widely because of its strong conveying capacity, long distance and simple structure. A variety of protective devices are also applied to the belt conveyor. The velocimeter can detect the running speed of the belt in real time, and it can cooperate with the controller of the conveyor to protect the belt effectively when the belt is skidded. In this paper, a non-contact, high-precision integrated belt velocimeter is proposed, the basic structure and realization principle of the belt velocimeter are also introduced. The experimental data demonstrated the feasibility of the velocimeter.",https://ieeexplore.ieee.org/document/9326677/,2020 Chinese Automation Congress (CAC),6-8 Nov. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IAI53119.2021.9619437,Research on an autonomous and controllable portable universal interface test platform,IEEE,Conferences,"Industrial software testing including software development and debugging depends on the external input interface. The development, debugging and adaptation of interface software simulation consumes a lot of time. The process of software evaluation and self-test lack a portable general software testing equipment suitable for the industrial field, in order to greatly improve the testing efficiency, test integrity and adequacy. Therefore, it is urgent for the general interface generation platform to be transformed into high performance such as hardware, distributed, hardware interface adaptation, test task load and high real-time. In this paper, the overall design framework of portable general software test equipment is carried out, which includes the design and software development of the execution host, the software transformation of general control host and other research contents. At the same time, a portable general software testing equipment for complex industrial system software and multiple interfaces is developed. This platform can satisfy the diversity of complex industrial software system interfaces and the real-time requirements of special systems. It is expected to further promote the development of interface testing automation.",https://ieeexplore.ieee.org/document/9619437/,2021 3rd International Conference on Industrial Artificial Intelligence (IAI),8-11 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE51474.2020.00070,Research on scheduling algorithm for industrial Internet of Things,IEEE,Conferences,"The continuous development of network and communication technology has a great impact on the national economy, and all countries attach great importance to the development of industrial Internet of things. Among them, the scheduling problem of industrial Internet of Things exists, such as packet transmission delay. In this paper, a TSN scheduling algorithm (NACO algorithm) based on ant colony system is proposed. Simulation experiments, the results show that the algorithm is a good way to solve the scheduling problem of industrial Iot, and compared with traditional algorithm, can avoid falling into local optimal solution, and has better convergence and optimization ability, and has certain delay change, able to provide deterministic time sensitive network real-time guarantees.",https://ieeexplore.ieee.org/document/9403824/,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),30 Oct.-1 Nov. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPEC51340.2021.9421229,Research on the Application of Machine Learning Big Data Mining Algorithms in Digital Signal Processing,IEEE,Conferences,"Traditional digital signal processing technology based on DSP and FPGA is more suitable for real-time signal processing, and is limited by data scale and frequency resolution, making it unsuitable for offline data processing, analysis and mining under large-scale data Application. At present, the industrial big data analysis platform can use Spark as a calculation engine for real-time signal processing and offline signal processing acceleration, but the analysis platform lacks mathematical calculation solutions such as digital signal processing suitable for distributed parallel calculation engines. This article is based on time the parity decomposition is selected, and the fast Fourier transform is realized by MATLAB software. Based on an example of the application of the compiled FFT source program, this article analyses the frequency spectrum of discrete-time and continuous-time signals of limited length.",https://ieeexplore.ieee.org/document/9421229/,"2021 IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)",14-16 April 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE53849.2021.00131,"Robot Swarm Navigation: Methods, Analysis, and Applications",IEEE,Conferences,"Swarm navigation is one of the possible collective behaviours a swarm robotic system can possess. Research in multiple robot navigation has gained more attention given their potential real-world applications, such as search and rescue, transportation, precision farming, and environmental monitoring. In this paper, we analyze the recent advances in the field of swarm navigation, focusing mainly on design and analysis methods that contribute to collective exploration, coordinated motion, and collective transport. Moreover, various experimental and real-world applications of swarm navigation, instead of works that involve simulations, are described. Several challenges that restrict the implementation of successful laboratory works of swarm systems to industrial applications are also identified and described. To tackle these challenges, some interesting future research directions are proposed and discussed.",https://ieeexplore.ieee.org/document/9696121/,2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),24-26 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MECHATRONIKA.2014.7018286,Robot imitation of human arm via Artificial Neural Network,IEEE,Conferences,"In this study, a robot arm that can imitate human arm is designed and presented. The potentiometers are located to the joints of the human arm in order to detect movements of human gestures, and data were collected by this way. The collected data named as “movement of human arm” are classified by the help of Artificial Neural Network (ANN). The robot performs its movements according to the classified movements of the human. Real robot and real data are used in this study. Obtained results show that the learning application of imitating human action via the robot was successfully implemented. With this application, the platforms of robot arm in an industrial environment can be controlled more easily; on the other hand, robotic automation systems which have the capability of making a standard movements of a human can become more resistant to the errors.",https://ieeexplore.ieee.org/document/7018286/,Proceedings of the 16th International Conference on Mechatronics - Mechatronika 2014,3-5 Dec. 2014,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICST53961.2022.00040,Robustness assessment and improvement of a neural network for blood oxygen pressure estimation,IEEE,Conferences,"Neural networks have been widely applied for performing tasks in critical domains, such as, for example, the medical domain; their robustness is, therefore, important to be guaranteed. In this paper, we propose a robustness definition for neural networks used for regression, by tackling some of the problems of existing robustness definitions. First of all, by following recent works done for classification problems, we propose to define the robustness of networks used for regression w.r.t. alterations of their input data that can happen in reality. Since different alteration levels are not always equally probable, the robustness definition is parameterized with the probability distribution of the alterations. The error done by this type of networks is quantifiable as the difference between the estimated value and the expected value; since not all the errors are equally critical, the robustness definition is also parameterized with a &#x201C;tolerance&#x201D; function that specifies how the error is tolerated. The current work has been motivated by the collaboration with the industrial partner that has implemented a medical sensor employing a Multilayer Perceptron for the estimation of the blood oxygen pressure. After having computed the robustness for the case study, we have successfully applied three techniques to improve the network robustness: data augmentation with recombined data, data augmentation with altered data, and incremental learning. All the techniques have proved to contribute to increasing the robustness, though in different ways.",https://ieeexplore.ieee.org/document/9787878/,"2022 IEEE Conference on Software Testing, Verification and Validation (ICST)",4-14 April 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PHM-Nanjing52125.2021.9613088,Rotating Machinery Fault Diagnosis using Light Gradient Boosting Machine,IEEE,Conferences,"Rotating machinery is widely used in modern industrial technology. Timely diagnosis of faults of rotating machinery equipment is of great significance to maintain the reliability and safety of the whole system. Since the development of fault diagnosis technology, there have been many diagnosis methods that can be applied to rotating machinery, and these methods have achieved good results. However, many of these methods cannot balance the relationship between diagnostic accuracy and timeliness very well, and require high computing capabilities of the device, which is not conducive to algorithm deployment on hardware devices, and the long diagnosis time is not conducive to real-time monitoring of the rotating machinery. This paper takes the core component bearing of rotating machinery equipment as the object, and proposes a fault diagnosis method for rotating machinery based on light gradient boosting machine (LightGBM). In this paper, two kinds of bearing data sets are used for ten-fold cross-validation, which can achieve high accuracy and very short training time. The experimental results show that LightGBM has higher diagnostic accuracy and better real-time performance.",https://ieeexplore.ieee.org/document/9613088/,2021 Global Reliability and Prognostics and Health Management (PHM-Nanjing),15-17 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIAS.2012.6306173,SCARA robot control using neural networks,IEEE,Conferences,"A SCARA industrial robot model is identified based on a 4-axis structure using Lagrangian mechanics, also the dynamic model for the electromechanical actuator and motion transmission systems is identified. A conventional PD controller is implemented and compared to neural networks control system to achieve precise position control of SCARA manipulator. The performance of the modeled system is simulated using several desired tracking motion for each joint. Neural networks control method has shown a remarkable improvement of tracking capabilities for the SCARA robot over conventional PD controller. The proposed neural network controller has the potential to accurately control real-time manipulator applications.",https://ieeexplore.ieee.org/document/6306173/,2012 4th International Conference on Intelligent and Advanced Systems (ICIAS2012),12-14 June 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2MTC50364.2021.9460075,SNR-based Reinforcement Learning Rate Adaptation for Time Critical Wi-Fi Networks: Assessment through a Calibrated Simulator,IEEE,Conferences,"Nowadays, the Internet of Things is spreading in several different research fields, such as factory automation, instrumentation and measurement, and process control, where it is referred to as Industrial Internet of Things. In these scenarios, wireless communication represents a key aspect to guarantee the required pervasive connectivity required. In particular, Wi-Fi networks are revealing ever more attractive also in time- and mission-critical applications, such as distributed measurement systems. Also, the multi-rate support feature of Wi-Fi, which is implemented by rate adaptation (RA) algorithms, demonstrated its effectiveness to improve reliability and timeliness. In this paper, we propose an enhancement of RSIN, which is a RA algorithm specifically conceived for industrial real-time applications. The new algorithm starts from the assumption that an SNR measure has been demonstrated to be effective to perform RA, and bases on Reinforcement Learning techniques. In detail, we start from the design of the algorithm and its implementation on the OmNet++ simulator. Then, the simulation model is adequately calibrated exploiting the results of a measurement campaign, to reflect the channel behavior typical of industrial environments. Finally, we present the results of an extensive performance assessment that demonstrate the effectiveness of the proposed technique.",https://ieeexplore.ieee.org/document/9460075/,2021 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),17-20 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMARTCOMP52413.2021.00052,SWIMS: the Smart Wastewater Intelligent Management System,IEEE,Conferences,"Wastewater treatment is a critical process in urban and industrial settlements aiming to clean and protect the water as well as the overall environment. Wastewater management systems are conceived explicitly for purifying wastewater, providing clean water efficiently, but this is a hard task due to frequent and quite unpredictable fluctuations of inlet wastewater flows, arising from (random) rain water or (periodical, e.g. day-night) sewage sources, sometimes also leading to failures and outages. To ensure the quality of the clean water out above a threshold and keep the overall system operating, this paper proposes the smart wastewater intelligent management system (SWIMS). It monitors and controls inlet and outlet flows as well as the water quality and parts of the plant as a cyber-physical system (CPS), starting from an Environmental Internet of Things (EIoT) platform. The data generated from the treatment plant is collected in an information system hosted by a server together with an intelligent system that processes this information in a real-time fashion and provides the feedback for optimizing the plant to maintain a good quality of water over time. Such an intelligent system exploits deep learning approaches to control the behaviour of the wastewater treatment system through anomaly detection, supporting decision making on it. SWIMS has been implemented in a real case study deployed in Briatico, Italy. The data and results collected from such a case study are presented, analyzed and discussed in this paper, demonstrating the feasibility and the effectiveness of the SWIMS solution.",https://ieeexplore.ieee.org/document/9556275/,2021 IEEE International Conference on Smart Computing (SMARTCOMP),23-27 Aug. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AVSS.2018.8639339,Scene Adaptation for Semantic Segmentation using Adversarial Learning,IEEE,Conferences,"Semantic Segmentation algorithms based on the deep learning paradigm have reached outstanding performances. However, in order to achieve good results in a new domain, it is generally demanded to fine-tune a pre-trained deep architecture using new labeled data coming from the target application domain. The fine-tuning procedure is also required when the domain application settings change, e. g., when a camera is moved, or a new camera is installed. This implies the collection and pixel-wise la-beling of images to be used for training, which slows down the deployment of semantic segmentation systems in real industrial scenarios and increases the industrial costs. Taking into account the aforementioned issues, in this paper we propose an approach based on Adversarial Learning to perform scene adaptation for semantic segmentation. We frame scene adaptation as the task of predicting semantic segmentation masks for images belonging to a Target Scene Context given labeled images coming from a Source Scene Context and unlabeled images coming from the Target Scene Context. Experiments highlight that the proposed method achieves promising performances both when the two scenes contain similar content (i.e., they are related to two different points of view of the same scene) and when the observed scenes contain unrelated content (i.e., they account to completely different scenes).",https://ieeexplore.ieee.org/document/8639339/,2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),27-30 Nov. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/2372251.2372281,Seamless integration of order processing in MS Outlook using SmartOffice: An empirical evaluation,IEEE,Conferences,"MS Outlook is currently the most widespread e-mail client in corporate environments. However, e-mail management with MS Outlook is usually decoupled from enterprise processes, making it difficult to synchronize e-mails and attachments with currently running processes. In this paper, we introduce SmartOffice - an extension for MS Outlook allowing the seamless integration of e-mail management with enterprise workflows, thus increasing the effectiveness of e-mail processing as well as coupling process-relevant e-mails and documents with the respective process instances. SmartOffice was integrated with a legacy system supporting the import management process of a large German retailer. We evaluated the SmartOffice integration in an empirical study in the context of the import process, using real data, and with the employees of the retailer's import office. We conducted a semi-structured interview, where one participant answered questions after solving three typical tasks and surveyed a group after a presentation and demonstration of SmartOffice's functionality. The results show that SmartOffice has high potential for being introduced in the process with high efficiency and high user acceptance. Although the number of participants was low, the results are considered very relevant from the perspective of the domain experts, since the study took place in an industrial setting.",https://ieeexplore.ieee.org/document/6475413/,Proceedings of the 2012 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement,20-21 Sept. 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UkrMiCo52950.2021.9716595,Securing Medical Data in 5G and 6G via Multichain Blockchain Technology using Post-Quantum Signatures,IEEE,Conferences,"Slicing a network is a key innovative feature to deliver 5G (the fifth generation) services. 5G aims to enhance mobile broadband, deliver massive machine-type and low latency communications with ultra-reliability. This generation manages the huge scale of the devices of the Mobile Internet of Things (MIoT). 5G has been invented for controlling security addressed to the threats found in 2G/3G/4G networks consequently. 5G preventive measures include enhanced authentication capabilities, subscriber identity protection, and additional security techniques. New 5G network technologies introduced new threats for the industrial organizations. GSMA (The GSM Association) announced that Mobile Internet of Things (IoT) requires more security in 5G as the number of IoT devices and connections are exponentially increasing. &#x0022;The IoT needs to be securely coded, deployed and managed throughout its lifecycle&#x0022;. Common IoT architecture is subjected to the following common attacks: 1) attacks on IoT devices through the running application; 2) remote attacks over the internet; 3) physical attacks; 4) attacks through the cloud 5) attacks through Wi-Fi or mobile air interface. Moreover, IoT devices are being used to form DDoS attacks where each IoT device forms the specific data resulting a volume-based attacks. Blockchains are considered as a main solution to solve various issues in 5G such as Mobile IoT security problems and Electronic Healthcare Records sharing problems. We analyse Hashing based Post-Quantum Signatures for enhancing blockchain security.",https://ieeexplore.ieee.org/document/9716595/,2021 IEEE International Conference on Information and Telecommunication Technologies and Radio Electronics (UkrMiCo),29 Nov.-3 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCAI53970.2022.9752493,Segmentation of Lidar Point Cloud Data using SOM,IEEE,Conferences,"Sensors play an important role in detecting and perceiving the environment. Many applications of designing a system to understand the environment semantically include vision for navigation, reverse engineering, Simultaneous Localization and Mapping (SLAM), modelling, autonomous vehicles, change detection, and autonomous robots. Accuracy concern, the Lidar sensor is now used in stunning systematic investigations. Lidar sensors generate data in various file formats such as LAS and point cloud data from the sensed environment. The ability to form simple various applications with error-free environment perception from the Lidar point cloud is critical. Navigation of an autonomous vehicle without colliding with obstacles and locating the vehicle on its own may be a difficult task, which is commonly referred to as SLAM. Creating an accurate map for local features in SLAM problems can be a time-consuming process. Many studies are concerned with the creation of a local map for a structured environment. This study will look into the implementation of a self-organized map for an unknown environment. The self-organized map (SOM) learns about the environment and acquires semantic knowledge from a local expert. This is taken as an input and further segments the region, especially unstructured environments like agricultural land, nonindustrial environments. Heretofore, the developed works for clustering and segmentation only for the roadside habitat and industrial purpose. In this paper, we implemented the real-time SOM to perceive any world environment regardless of illumination and is experimented. Implementation outputs are analysed for further rectification.",https://ieeexplore.ieee.org/document/9752493/,"2022 International Conference on Advances in Computing, Communication and Applied Informatics (ACCAI)",28-29 Jan. 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OCEANS.2016.7761412,Self-tuned PID control based on backpropagation Neural Networks for underwater vehicles,IEEE,Conferences,"For a long time, PID-like controllers have been successfully used in academic and industrial tasks. This is thanks to its simplicity and suitable performance in linear or linearized plants, and under certain conditions, in nonlinear ones. A number of PID controller gains tuning approaches have been proposed in the literature in the last decades; most of them off-line techniques. However, in those cases wherein plants are subject to continuous parametric changes or external disturbances, online gains tuning is a need. This is the case of modular underwater ROVs (Remotely Operated Vehicles) where parameters (weight, buoyancy, added mass, among others) change according to the tool they are fitted with. In practice, some amount of time is dedicated to tune the PID gains of a ROV. Once the best set of gains has been achieved the ROV is ready to work. However, when the vehicle changes its tool or it is subject to ocean currents, its performance deteriorates since the fixed set of gains is no longer valid for the new conditions. Thus, an online PID gains tuning algorithm should be implemented to overcome this problem. In this paper, an auto-tuned PID-like controller based on Neural Networks (NN) is proposed. The NN plays the role of automatically estimating the suitable set of PID gains that achieves stability of the system. The NN adjusts online the controller gains that attain the smaller position tracking error. Simulation results are given considering an underactuated 6 DOF (degrees of freedom) underwater ROV. Real time experiments on an underactuated mini ROV are conducted to show the effectiveness of the proposed scheme.",https://ieeexplore.ieee.org/document/7761412/,OCEANS 2016 MTS/IEEE Monterey,19-23 Sept. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DS-RT.2007.38,Semantic Web Service Architecture for Simulation Model Reuse,IEEE,Conferences,"COTS simulation packages (CSPs) have proved popular in an industrial setting with a number of software vendors. In contrast, options for re-using existing models seem more limited. Re-use of simulation component models by collaborating organizations is restricted by the same semantic issues however that restrict the inter-organization use of web services. The current representations of web components are predominantly syntactic in nature lacking the fundamental semantic underpinning required to support discovery on the emerging semantic web. Semantic models, in the form of ontology, utilized by web service discovery and deployment architecture provide one approach to support simulation model reuse. Semantic interoperation is achieved through the use of simulation component ontology to identify required components at varying levels of granularity (including both abstract and specialized components). Selected simulation components are loaded into a CSP, modified according to the requirements of the new model and executed. The paper presents the development of ontology, connector software and web service discovery architecture in order to understand how such ontology are created, maintained and subsequently used for simulation model reuse. The ontology is extracted from health service simulation - comprising hospitals and the National Blood Service. The ontology engineering framework and discovery architecture provide a novel approach to inter- organization simulation, uncovering domain semantics and adopting a less intrusive interface between participants. Although specific to CSPs the work has wider implications for the simulation community.",https://ieeexplore.ieee.org/document/4384541/,11th IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT'07),22-26 Oct. 2007,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.2012.6489781,Semantic design and integration of simulation models in the industrial automation area,IEEE,Conferences,"Simulations are software tools approximating and predicting the behavior of real industrial plants. Unlike real plants, the utilization of simulations cannot cause damages and it saves time and costs during series of experiments. A shortcoming of current simulation models is the complicated runtime integration into legacy industrial systems and platforms, as well as ad-hoc design phase, introducing manual and error-prone work. This paper contributes to improve the efficiency of simulation model design and integration. It utilizes a semantic knowledge base, implemented by ontologies and their mappings. The integration uses the Automation Service Bus and the paper explains how to configure the runtime integration level semantically. The main contributions are the concept of semantic configuration of the service bus and the workflows of simulation design and integration.",https://ieeexplore.ieee.org/document/6489781/,Proceedings of 2012 IEEE 17th International Conference on Emerging Technologies & Factory Automation (ETFA 2012),17-21 Sept. 2012,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2019.00715,ShieldNets: Defending Against Adversarial Attacks Using Probabilistic Adversarial Robustness,IEEE,Conferences,"Defending adversarial attack is a critical step towards reliable deployment of deep learning empowered solutions for industrial applications. Probabilistic adversarial robustness (PAR), as a theoretical framework, is introduced to neutralize adversarial attacks by concentrating sample probability to adversarial-free zones. Distinct to most of the existing defense mechanisms that require modifying the architecture/training of the target classifier which is not feasible in the real-world scenario, e.g., when a model has already been deployed, PAR is designed in the first place to provide proactive protection to an existing fixed model. ShieldNet is implemented as a demonstration of PAR in this work by using PixelCNN. Experimental results show that this approach is generalizable, robust against adversarial transferability and resistant to a wide variety of attacks on the Fashion-MNIST and CIFAR10 datasets, respectively.",https://ieeexplore.ieee.org/document/8953209/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN50785.2021.9515431,Simplifying the A.I. Planning modeling for Human-Robot Collaboration,IEEE,Conferences,"For an effective deployment in manufacturing, Collaborative Robots should be capable of adapting their behavior to the state of the environment and to keep the user safe and engaged during the interaction. Artificial Intelligence (AI) enables robots to autonomously operate understanding the environment, planning their tasks and acting to achieve some given goals. However, the effective deployment of AI technologies in real industrial environments is not straightforward. There is a need for engineering tools facilitating communication and interaction between AI engineers and Domain experts. This paper proposes a novel software tool, called TENANT (Tool fostEriNg Ai plaNning in roboTics) whose aim is to facilitate the use of AI planning technologies by providing domain experts like e.g., production engineers, with a graphical software framework to synthesize AI planning models abstracting from syntactic features of the underlying planning formalism.",https://ieeexplore.ieee.org/document/9515431/,2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN),8-12 Aug. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IntelCIS.2015.7397201,Simulation Model of the Decision-Making Support for Human-Machine Systems Operators,IEEE,Conferences,Simulation Model for the Decision-Making Support of the Human-Machine Systems Operator was developed. The term of operator's professional confidence as an index of the operator's ability to maintain or for a certain time lead to a stabile state of the Human-Machine Systems by performing the motor operator activity as the final component of the implementation of the decision was proposed. The motor operator's activity speed typed classification was developed. Algorithm and tool for the identification of the motor operator's activity speed of the Human-Machine Systems were offered. The Simulation Experiment Results were presented. The ways of the Simulation Model industrial realization and implementation as part of the real Automated Decision Support System were formulated.,https://ieeexplore.ieee.org/document/7397201/,2015 IEEE Seventh International Conference on Intelligent Computing and Information Systems (ICICIS),12-14 Dec. 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAD52417.2021.9638761,Simulation-trained AI-System for Two-Stage Fault Detection and Diagnosis of Rolling Bearings in Industrial Applications,IEEE,Conferences,"Rolling bearings are an essential component in manufacturing equipment and can lead to production losses if a failure is not detected in time. Furthermore, the data obtained in many industrial applications is not labelled, so that a classification of intact and defective bearings is often not possible. This paper presents an approach for an AI-system that is trained with a bearing fault simulation and can then be deployed in industrial applications. A method for bearing fault simulation is presented which is based on the superposition of characteristic fault frequencies of a bearing. With the data from the simulation, an AI-system is implemented with a two-stage approach for fault detection and diagnosis. First, a semi-supervised k-nearest neighbor distance measure is calculated using synthetic data from the simulation. From this, a suitable value for the number of neighbors k is defined and the threshold at which a sample is considered faulty is determined. Then the AI-system is deployed in an industrial application and real world data is analysed with the given value for k and the threshold. In the second stage, the detected faulty samples are classified with regard to the type of fault using a decision tree, which is also trained with the data from the simulation. This approach is validated with two different real world bearing datasets and the results show the effectiveness of the presented approach.",https://ieeexplore.ieee.org/document/9638761/,"2021 International Conference on Control, Automation and Diagnosis (ICCAD)",3-5 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRASET52964.2022.9737786,Smart Energy Management System: Oil Immersed Power Transformer Failure Prediction and Classification Techniques Based on DGA Data,IEEE,Conferences,"The power transformer is the key element in the electrical grid. The failures of the power transformer impact critically the grid, can cause energy loss and blackouts. In energy production, transmission, distribution, and industrial applications, the oil immersed power transformer is the most used. The maintenance of this key equipment is highly important which can be done with different techniques such as thermal and vibration analysis, frequency analysis using wavelet transform and dissolved gas analysis. The application of predictive maintenance of the power transformer represents an important feature of the Smart Energy Management System in micro grids, that can reduce the percentage of failure occurrence while increasing the availability of the power transformer and prevents blackouts. This paper represents different failure classification techniques based on dissolved gas analysis data mainly logistic regression, multiclass jungle, multiclass decision tree and artificial neural network. The application can diagnosis the power transformer failures based on the parts-per-million of the different gas generated in the oil. The results of applying different types of classification algorithms shows the best technique to be part of a bigger system of monitoring and diagnostic of different installed equipment in a micro grid. The implementation of such application in real time energy management system requires different type of sensors and the interaction of offline database, the paper also shows the steps to integrate the algorithm in the Smart Energy Management System.",https://ieeexplore.ieee.org/document/9737786/,"2022 2nd International Conference on Innovative Research in Applied Science, Engineering and Technology (IRASET)",3-4 March 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/AEIT.2018.8577226,Smart Farms for a Sustainable and Optimized Model of Agriculture,IEEE,Conferences,"Nowadays, public and private companies, are in a constant race to increase profitability, chasing the costs reduction while facing the market competition. Also in the agriculture an analysis of cost-effectiveness, measuring technological innovation and profitability becomes necessary. The `smart farm' model exploits information coming from technologies like sensors, intelligent systems and the Internet of Things (IoT) paradigm to understand the influential and non-influential factors while considering environmental, productive and structural data coming from a large number of sources. The goal of this work is to design and deploy practical tasks that exploit heterogeneous real datasets with the aim to forecast and reconstruct values using and comparing innovative machine learning techniques with more standard ones. The application of these methodologies, in fields that are only apparently refractory to the technology such as the agricultural one, shows that there are ample margins for innovation and investment while supporting requests and needs coming from companies that wish to employ a sustainable and optimized agricultural industrial business.",https://ieeexplore.ieee.org/document/8577226/,2018 AEIT International Annual Conference,3-5 Oct. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMCON.2018.8615072,Smart Mirror - a Secured Application of Artificial Intelligence Recognizing Human Face and Voice,IEEE,Conferences,"Smart mirrors are a brand new addition to the IoT product family that has been obtaining a great deal of attention in recent years by each industrial makers and hobbyists. This paper describes the planning associated implementation of a voice controlled wall mirror, referred to as “Magic Mirror” with Artificial Intelligence for the home environment. It is a mirror, which can display real time content like time, date, weather and news at the same time. The Magic Mirror consists of functionalities like real time information and data updates, voice commands, face recognition. The user can control the magic mirror by voice commands.",https://ieeexplore.ieee.org/document/8615072/,"2018 IEEE 9th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)",1-3 Nov. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCPEIC.2017.8290335,Smart personalized learning system for energy management in buildings,IEEE,Conferences,"Integration of energy management systems into existing buildings brings in several challenges and financial constraints. Some of the challenges in the existing smart building solutions are that they require large-scale deployment of sensors, high rate of data collection, real-time data analysis in short span of time, and lack of knowledge about the energy usage with respect to the behavior of individuals and groups. This work proposes an affordable wearable device system as an alternative for large-scale deployment of sensors in industrial buildings. For effective energy management in the buildings, a personalized behavior analysis has been done in machine learning and neural networks algorithm and integrated with the proposed system. The complete system is implemented and tested extensively. The results show that the proposed system could provide 85% user comfort and 23% energy savings.",https://ieeexplore.ieee.org/document/8290335/,"2017 International Conference on Computation of Power, Energy Information and Commuincation (ICCPEIC)",22-23 March 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMCIA.1999.782701,Soft computing techniques for intelligent classification system: a case study,IEEE,Conferences,"Soft computing techniques are becoming popular in designing real world industrial applications. Researchers are trying to integrate different soft computing paradigms such as fuzzy logic, artificial neural network, genetic algorithms etc., to develop hybrid intelligent autonomous systems that provide more flexibility by exploiting tolerance and uncertainty of real life situations. Intelligent classification systems are the most well known attempts. In this work a neuro fuzzy feature selector has been designed which is capable of extracting information in the form of fuzzy rules from numeric as well as non-numeric (linguistic) data. Conventional MLP and a variation of it have been used as the neural models and their performance has been compared by simulation with two different data sets. It is found that the proposed variation of the conventional MLP is better in respect to training time and classification accuracy.",https://ieeexplore.ieee.org/document/782701/,SMCia/99 Proceedings of the 1999 IEEE Midnight - Sun Workshop on Soft Computing Methods in Industrial Applications (Cat. No.99EX269),18-18 June 1999,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCAA.2016.7813753,Soft computing techniques to address various issues in wireless sensor networks: A survey,IEEE,Conferences,"Wireless sensor network (WSN) is a collection of large number of self-organized types of sensors which chain together to monitor and record physical or environmental conditions (i.e. used to measure temperature, sound, pressure) and passes gathered information to the central location. WSN build bridge between real world and virtual environment, which makes it more utilizable for many applications. Mainly WSN was used for military arena but now a days it is used in various area like industrial applications, consumer applications, health care applications and many more. Despite of having many advantages there are some issues also occurred in WSNs like hotspot problem, energy hole problem, routing, coverage problem, load balancing problem and so on. These issues effect on different factors of WSN named energy consumption, stability, quality, deployment time, lifetime of network, which degrade the performance of the WSN. To solve these issues various researchers develop different mechanisms. Among all of them, in this paper, we survey different kind of soft computing paradigms. Soft computing is a technique to use of improper solutions to solve the complicated problem in robust time. There are various types of soft computing techniques developed: swarm intelligence, fuzzy logic, neural network, reinforcement learning and evolutionary algorithm, which used to solve WSN problems so that performance of the network will be increased.",https://ieeexplore.ieee.org/document/7813753/,"2016 International Conference on Computing, Communication and Automation (ICCCA)",29-30 April 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MELCON.2010.5476042,Soft sensors and artificial intelligence for nuclear fusion experiments,IEEE,Conferences,"Soft sensors are mathematical models able to estimate process variables. They can work in parallel with hardware sensors, and can be implemented at a low-cost on existing hardware. They are useful for back-up of measuring devices, reduction of measuring hardware requirements, real-time estimation for monitoring and control, sensor validation, fault detection and diagnosis, what-if analysis. In industrial applications, data-driven approaches, especially based on soft-computing techniques, are very promising. In this paper we review important issues in soft sensor design and applications, especially concerning the applications in the field of nuclear fusion.",https://ieeexplore.ieee.org/document/5476042/,Melecon 2010 - 2010 15th IEEE Mediterranean Electrotechnical Conference,26-28 April 2010,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASE.2011.6100070,Software process evaluation: A machine learning approach,IEEE,Conferences,"Software process evaluation is essential to improve software development and the quality of software products in an organization. Conventional approaches based on manual qualitative evaluations (e.g., artifacts inspection) are deficient in the sense that (i) they are time-consuming, (ii) they suffer from the authority constraints, and (iii) they are often subjective. To overcome these limitations, this paper presents a novel semi-automated approach to software process evaluation using machine learning techniques. In particular, we formulate the problem as a sequence classification task, which is solved by applying machine learning algorithms. Based on the framework, we define a new quantitative indicator to objectively evaluate the quality and performance of a software process. To validate the efficacy of our approach, we apply it to evaluate the defect management process performed in four real industrial software projects. Our empirical results show that our approach is effective and promising in providing an objective and quantitative measurement for software process evaluation.",https://ieeexplore.ieee.org/document/6100070/,2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011),6-10 Nov. 2011,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PQ.2016.7724114,Speed control for a permanent magnet synchronous motor based on fuzzy logic with reduced perturbations on the supply network,IEEE,Conferences,"Fuzzy logic controllers, FLCs, are elements of the artificial intelligence, and, today, they are widely use in command and control of many industrial processes. It involves the acquisition of better signal wave forms at the output and the reduction of all oscillations (in case of electric drives, we consider speed variation, torque variation, current variation for all the three phases ore more) in comparison with the classic control procedures. This paper presents two case studies of two FLCs applied for some exterior permanent magnet synchronous machines, PMSM. One fuzzy logic algorithm has 25 rules and the other has 49 rules. These FLCs are developed for less computational issues, which make them suitable for real time implementation. The permanent magnet synchronous machines are used today in state of the art drives due to their dynamic performances. It is known that PMSM has a high torque density, with reduced losses/torque rates, a high power factor and a quick torque and speed time response. All the references concerning the real time implementation are made on the digital signal processor controller board type DS1104.",https://ieeexplore.ieee.org/document/7724114/,2016 Electric Power Quality and Supply Reliability (PQ),29-31 Aug. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPST.2002.1053583,State self-adaptive monitor and control system for 6 kV/1600 kVA adjustable-speed drive,IEEE,Conferences,"Based on proposed state self-adaptive monitor logic and volts per hertz (V/f) control method with current and voltage limitation, a compact operation monitor and control system (OMCS) is developed for real-time monitoring and control of a 6 kV/1600 kVA adjustable-speed drive (ASD) installation. The hardware of OMCS is based on industrial PC and composed of AI and DI data acquisition cards and TMS320C32 DSP based pulse system with specially designed configuration to exempt from strong electromagnetic interference. Implemented under Chinese Window 98 operating system with VC6.0 programming language, the software of OMCS can provide friendly man-machine interface with multi-functions. Experimental results show that the proposed monitor and control system can reliably supervise the operation conditions of ASD and efficiently control acceleration and deceleration of the induction motor without violating limits of current and voltage.",https://ieeexplore.ieee.org/document/1053583/,Proceedings. International Conference on Power System Technology,13-17 Oct. 2002,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2016.7793206,Summer school on intelligent agents in automation: Hands-on educational experience on deploying industrial agents,IEEE,Conferences,"Cyber-physical systems constitutes a framework to develop intelligent, distributed, resilient, collaborative and cooperative systems, promoting the fusion of computational entities and physical devices. Agent technology plays a crucial role to develop this kind of systems by offering a decentralized, distributed, modular, robust and reconfigurable control structure. This paper describes the implementation of a summer school aiming to enhance the participants' knowledge in the field of multi-agent systems applied to industrial environments, being able to gain the necessary theoretical and practical skills to develop real industrial agent based applications. This is accomplished in an international framework where individual knowledge and experiences are shared in a complementary level.",https://ieeexplore.ieee.org/document/7793206/,IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society,23-26 Oct. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2006.246929,Synthesis of Pulsed-Coupled Neural Networks in FPGAs for Real-Time Image Segmentation,IEEE,Conferences,"This paper describes the implementation of a system based on Pulse Coupled Neural Networks (PCNNs) and Field Programmable Gate Arrays (FPGAs). The PCNN implemented is oriented to the industrial application of segmentation in sequences of images. The work went through several real physical stages of implementation and optimization to achieve the needed performance. The greatest performance achieved by the digital system was of 250M pixels per second, enough to process a sequence of images in real time. Details of these stages about the neuron implementation with different Altera's FPGAs families are presented. Furthermore, the implementation is compared with previous implemented schemes based on floating point DSP microprocessor.",https://ieeexplore.ieee.org/document/1716657/,The 2006 IEEE International Joint Conference on Neural Network Proceedings,16-21 July 2006,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONECCT50063.2020.9198399,System and Methodology to Generate Dynamic Real-time Transparent and Bias-free Feature Modules used to Augment Model Learning Process,IEEE,Conferences,"Features are the backbone of any Machine Learning(ML) system. During the tth epoch of the model learning process, learnt features are the measurable outcomes of the model for a given input function. One of the biggest areas of research has been to try to impose even the slightest amount of control on what features a model learns. Achieving this in-turn poses various advantages, including re-usability of the features. Some of the state-of-the-art reusability techniques work by training the model weights and freeze at a random point k, on the source system and test the accuracy on the target until the ideal k point gives the highest possible accuracy. However, some of the current challenges remain in trying to control what is being sent to the target model and also in sending bias-free content. Thus, if we can aid this by adding a ""bias-free control-factor"" to the features being re-used in the model we would truly be able to perform many more tasks efficiently and achieve much higher accuracy along with lower consumption of resources required for training a model, thereby enabling ML towards many more industrial applications. We propose to provide a methodology by which we augment dynamic and bias-free feature modules for a model building process by taking an example in a Natural Language Processing(NLP) context.",https://ieeexplore.ieee.org/document/9198399/,"2020 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)",2-4 July 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSTW55395.2022.00034,TCP-Net: Test Case Prioritization using End-to-End Deep Neural Networks,IEEE,Conferences,"Regression testing is facing a bottleneck due to the growing number of test cases and the wide adoption of continuous integration (CI) in software projects, which increases the frequency of running software builds, making it challenging to run all the regression test cases. Machine learning (ML) techniques can be used to save time and hardware resources without compromising quality. In this work, we introduce a novel end-to-end, self-configurable, and incremental learning deep neural network (DNN) tool for test case prioritization (TCP-Net). TCP-Net is fed with source code-related features, test case metadata, test case coverage information, and test case failure history, to learn a high dimensional correlation between source files and test cases. We experimentally show that TCP-Net can be efficiently used for test case prioritization by evaluating it on three different real-life industrial software packages.",https://ieeexplore.ieee.org/document/9787970/,"2022 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",4-13 April 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3524843.3528094,TD Classifier: Automatic Identification of Java Classes with High Technical Debt,IEEE,Conferences,"To date, the identification and quantification of Technical Debt (TD) rely heavily on a few sophisticated tools that check for violations of certain predefined rules, usually through static analysis. Different tools result in divergent TD estimates calling into question the reliability of findings derived by a single tool. To alleviate this issue, we present a tool that employs machine learning on a dataset built upon the convergence of three widely-adopted TD Assessment tools to automatically assess the class-level TD for any arbitrary Java project. The proposed tool is able to classify software classes as high-TD or not, by synthesizing source code and repository ac-tivity information retrieved by employing four popular open source analyzers. The classification results are combined with proper vi-sualization techniques, to enable the identification of classes that are more likely to be problematic. To demonstrate the proposed tool and evaluate its usefulness, a case study is conducted based on a real-world open-source software project. The proposed tool is expected to facilitate TD management activities and enable fur-ther experimentation through its use in an academic or industrial setting. Video: https://youtu.be/umgXU8u7lIA Running Instance: http://160.40.52.130:3000/tdclassifier Source Code: https://gitlab.seis.iti.gr/root/td-classifier.git",https://ieeexplore.ieee.org/document/9804507/,2022 IEEE/ACM International Conference on Technical Debt (TechDebt),22-23 May 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSR51186.2021.9527936,TRUSTY: A Solution for Threat Hunting Using Data Analysis in Critical Infrastructures,IEEE,Conferences,"The rise of the Industrial Internet of Things (IIoT) plays a crucial role in the era of hyper-connected digital economies. Despite the valuable benefits, such as increased resiliency, self-monitoring and pervasive control, IIoT raises severe cybersecurity and privacy risks, allowing cyberattackers to exploit a plethora of vulnerabilities and weaknesses that can lead to disastrous consequences. Although the Intrusion Detection and Prevention Systems (IDPS) constitute valuable solutions, they suffer from several gaps, such as zero-day attacks, unknown anomalies and false positives. Therefore, the presence of supporting mechanisms is necessary. To this end, honeypots can protect the real assets and trap the cyberattackers. In this paper, we provide a web-based platform called TRUSTY , which is capable of aggregating, storing and analysing the detection results of multiple industrial honeypots related to Modbus/Transmission Control Protocol (TCP), IEC 60870-5-104, BACnet, Message Queuing Telemetry Transport (MQTT) and EtherNet/IP. Based on this analysis, we provide a dataset related to honeypot security events. Moreover, this paper provides a Reinforcement Learning (RL) method, which decides about the number of honeypots that can be deployed in an industrial environment in a strategic way. In particular, this decision is converted into a Multi-Armed Bandit (MAB), which is solved with the Thompson Sampling (TS) method. The evaluation analysis demonstrates the efficiency of the proposed method.",https://ieeexplore.ieee.org/document/9527936/,2021 IEEE International Conference on Cyber Security and Resilience (CSR),26-28 July 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDSBA53075.2021.00106,Target Tracking Research Hotspots and Frontier Trends Based on Citespace,IEEE,Conferences,"With the continuous development of artificial intelligence technology, target tracking is a hot problem in the field of computer vision, which has a wide range of application prospects in industrial, military, transportation, medical and other fields. In this paper, Citespace software is used to conduct descriptive statistical analysis and knowledge mapping analysis of target tracking based on domestic CNKI database literature, and to explore the development status and frontier trends in the field of target tracking in China. On this basis, point out three shortcomings of current research: low accuracy of target tracking in complex environments, poor real-time target tracking, and few application directions, and give suggestions to improve algorithm robustness, real-time, accelerate engineering implementation, and focus on future research trends.",https://ieeexplore.ieee.org/document/9693806/,2021 5th Annual International Conference on Data Science and Business Analytics (ICDSBA),24-26 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS52415.2021.9466046,The Implementation of Building Intelligent Smart Energy using LSTM Neural Network,IEEE,Conferences,"Internet of Things (IoT) makes many devices getting smarter and more connected in the 4.0 industrial revolution. One of the implementations of the Internet of Things is smart energy. It allows communication between humans or between things that make a building smarter. This paper proposes the implementation of the MQTT-based smart meter. The smart meter is used to make it easier for users to monitor and manage the energy consumption of buildings in real-time. It is considered as the main component of a smart network to make efficient and manage energy consumption remotely. Taking into account the increasing demand for electricity in Indonesia, smart meters can reduce overall energy use and reduce global warming by optimizing energy utilization through the internet of things and artificial intelligence. This paper proposes the implementation of the MQTT-based smart meter. This smart meter can measure energy consumption, transmit information related to the energy used, and provide an early warning system to stakeholders through the website in real-time analytics with predictive data on the following month and what days are most used to support energy consumption efficiency planning. This study conducted LTSM and ARIMA to determine forecasting energy consumption with 59 epochs, 8 batch sizes, 64 hidden layers with the results of MSE Error, RMSE Error, Mean Accuracy 0.14,0.373, and 95.16%, respectively. This result is better than ARIMA with MSE error results of 0.812 and 0.66 and RMSE error.",https://ieeexplore.ieee.org/document/9466046/,2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS),28-30 April 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIPS.1997.669380,The design approach to DAI system eased on software engineering,IEEE,Conferences,"As more challenging applications are automated, cooperative problem solving will be an important paradigm for the next generation of intelligent industrial systems. A key problem with using it in the engineering domain is the development of a structured design method. The authors suggest a design approach for a distributed artificial intelligence (DAI) system based on software engineering, describe the detailed design process of a real DAI system through the example of a simulative transformer substation system (STSS), and present some key problems and techniques of DAI in the engineering domain, such as system modeling, task decomposition and allocation, cooperative mechanism, etc.",https://ieeexplore.ieee.org/document/669380/,1997 IEEE International Conference on Intelligent Processing Systems (Cat. No.97TH8335),28-31 Oct. 1997,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.1994.639471,The development of a fully autonomous ground vehicle (FAGV),IEEE,Conferences,"As a first step toward the creation of a fully autonomous vehicle that operates in a real world environment, we are currently developing a prototype autonomous ground vehicle (AGV) for use in factories and other industrial/business sites based on behavior-based artificial intelligence (AI) control. This flexible and fully autonomous AGV (FAGV) is expected to operate efficiently in a normal industrial environment without any external guidance. The crucial technique employed is a non-Cartesian way of organizing software agents for the creation of a highly responsive control program. The resulting software is considerably reduced in size. Through numerous experiments using mobile robots we confirmed that these new control programs excel in functionality, efficiency, flexibility and robustness. The second key technique in the planning stage is evolutionary computation, of which genetic algorithms are a principal technique. An online, real-time evolution of the control program will be incorporated in later phases of the project to make FAGVs adaptable to any given operational environment after deployment. The first prototype FAGV has an active vision and behaviour-based control system.",https://ieeexplore.ieee.org/document/639471/,Proceedings of the Intelligent Vehicles '94 Symposium,24-26 Oct. 1994,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACITE51222.2021.9404738,The learning approaches using Augmented Reality in learning environments: Meta-Analysis,IEEE,Conferences,"With the emergence of Industrial Revolution 4.0, the educational settings are changing quickly. Augmented Reality (AR) is one of the upcoming technologies. AR enhances the real world by overlaying/augmenting the virtual/digital information over it. It provides the user with the ability to interact with the created virtual world in real space. The aim of this study is to classify the learning approaches implemented through AR technology. The technique used for the analysis is derived from systematic search of online literature databases like Taylor Francis, Web of Science, Springer, ScienceDirect and Scopus. The keywords used for the search include learning approaches, AR, AR in education, AR in learning and teaching and integration approaches. The findings of this research work highlights 4 categories of educational learning approaches that highlight AR. The approaches are experimental learning, game-based, interactive and collaborative learning. The research findings can be referred by other researchers and educators to identify the potential of AR in education and the learning approaches currently used with AR for their further research on how these approaches can be effectively and efficiently implemented in educational settings.",https://ieeexplore.ieee.org/document/9404738/,2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),4-5 March 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WSC.2017.8248046,The role of learning on industrial simulation design and analysis,IEEE,Conferences,"The capability of modeling real-world system operations has turned simulation into an indispensable problem-solving methodology for business system design and analysis. Today, simulation supports decisions ranging from sourcing to operations to finance, starting at the strategic level and proceeding towards tactical and operational levels of decision-making. In such a dynamic setting, the practice of simulation goes beyond being a static problem-solving exercise and requires integration with learning. This article discusses the role of learning in simulation design and analysis motivated by the needs of industrial problems and describes how selected tools of statistical learning can be utilized for this purpose.",https://ieeexplore.ieee.org/document/8248046/,2017 Winter Simulation Conference (WSC),3-6 Dec. 2017,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2010.5553990,The system for testing electrical performance of power generating sets based on technique of AC sampling,IEEE,Conferences,"A multilevel distributed system for testing electrical performance of power generating sets based on CAN bus is presented. Sampling and control for electrical parameters are realized utilizing Digital Signal Processor (DSP) and sixteen-bit Microcontroller Unit (MCU) that M16C/6N of renesas, an intelligent load based on the MCU of AT89S52 is designed, of which value can change automatically according to the need of testing, remote real-time supervision and integrated analysis for the electrical parameters are realized by Industrial Personal Computer (IPC) and configuration control software. A synchronous AC sampling algorithm based on Discrete Fourier Transform (DFT) is presented, and sampling precision is improved by tracing the frequency of signal under test using the method of “three points”. It indicates that the system is characterized by flexible configuration, high precision and high reliability in practical application.",https://ieeexplore.ieee.org/document/5553990/,2010 8th World Congress on Intelligent Control and Automation,7-9 July 2010,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICCAS52745.2021.9649841,The use of Neural Network for Nonlinear Predictive Control design for and Overhead Crane,IEEE,Conferences,"The importance of nonlinear model predictive control (NMPC) implementations for industrial processes rises with the increasing of computational power in all hardware units used for regulation and control in practice. However, it assumes a sufficiently accurate model. In case of more complex systems, there might be problem to perform analytical identification. Instead of this, numerical approaches may be deployed with benefit. This paper deals with the design of NMPC for a nonlinear model of an overhead crane using a neural network and compares the solution with the one achieved with the use analytical model of the system. All steps of NMPC design and verification of functionality are performed in Matlab. The paper finally suggests possibility to extend the presented approach for hosting the NMPC algorithm on some real-time embedded target.",https://ieeexplore.ieee.org/document/9649841/,"2021 21st International Conference on Control, Automation and Systems (ICCAS)",12-15 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE.2008.4677173,Three layer model for digital coal mine based on industrial ethernet,IEEE,Conferences,"In order to construct digital coal mine, a three layer model for digital coal mine model is proposed in this paper. Two basic platforms, the uniform transmission network platform and the uniform data warehouse platform, are discussed. A real 1000 M industrial Ethernet transmission platform based on Siemens PROFINET is established for Yangchangwan coal mine. The network platform is an information superhighway to integrate all existing and new automation subsystems and to provide standard interfaces for future subsystems. It established a uniform hardware and software platform in all aspects from network, data structure and management decision-making. Therefore the automation system island and information island problems in traditional mine automation systems are avoided effectively. It builds a stable foundation of digital coal mine for Yangchangwan coal mine.",https://ieeexplore.ieee.org/document/4677173/,2008 IEEE International Symposium on Industrial Electronics,30 June-2 July 2008,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCD46524.2019.00019,TiLA: Twin-in-the-Loop Architecture for Cyber-Physical Production Systems,IEEE,Conferences,"Digital twin is a virtual replica of a real-world object that lives simultaneously with its physical counterpart. Since its first introduction in 2003 by Grieves, digital twin has gained momentum in a wide range of applications such as industrial manufacturing, automotive and artificial intelligence. However, many digital-twin-related approaches, found in industries as well as literature, mainly focus on modelling individual physical things with high-fidelity methods with limited scalability. In this paper, we introduce a digital-twin architecture called TiLA (Twin-in-the-Loop Architecture). TiLA employs heterogeneous models and online data to create a digital twin, which follows a Globally Asynchronous Locally Synchronous (GALS) model of computation. It facilitates the creation of a scalable digital twin with different levels of modelling abstraction as well as giving GALS formalism for execution strategy. Furthermore, TiLA provides facilities to develop applications around the twin as well as an interface to synchronise the twin with the physical system through an industrial communication protocol. A digital twin for a manufacturing line has been developed as a case study using TiLA. It demonstrates the use of digital twin models together with online data for monitoring and analysing failures in the physical system.",https://ieeexplore.ieee.org/document/8988620/,2019 IEEE 37th International Conference on Computer Design (ICCD),17-20 Nov. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE53849.2021.00073,Time series simulation method of meteorological elements based on ARIMA model,IEEE,Conferences,"With the development of science and technology and economy, environmental monitoring technology and means are constantly enriched and improved, and the information of meteorological big data is gradually diversified, which brings convenience to the study of meteorological trend that thus provides corresponding guidance for industrial and agricultural production. In recent years, the construction of environmental monitoring stations has been accelerated, and they are popular in the study of meteorological trends in various countries because of their real-time characteristics and wide range of data. In this paper, the characteristics of key meteorological elements at different time scales are statistically analyzed firstly based on the measured meteorological data of regional meteorological stations that meteorological elements have significant tendency and seasonality. Secondly, the obtained meteorological big data is pre-processed to eliminate bad data, and then the probability distribution of meteorological elements and their change rules is extracted. Finally, a time series method based on ARIMA model is proposed to simulate the rule of weather change, so as to obtain the time series of temperature and rainfall in the next ten years, which can provide theoretical and data support for various industries, and help meteorological and security departments to make work plans in time and correctly.",https://ieeexplore.ieee.org/document/9696084/,2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),24-26 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTAS48715.2020.000-1,Timing of Autonomous Driving Software: Problem Analysis and Prospects for Future Solutions,IEEE,Conferences,"The software used to implement advanced functionalities in critical domains (e.g. autonomous operation) impairs software timing. This is not only due to the complexity of the underlying high-performance hardware deployed to provide the required levels of computing performance, but also due to the complexity, non-deterministic nature, and huge input space of the artificial intelligence (AI) algorithms used. In this paper, we focus on Apollo, an industrial-quality Autonomous Driving (AD) software framework: we statistically characterize its observed execution time variability and reason on the sources behind it. We discuss the main challenges and limitations in finding a satisfactory software timing analysis solution for Apollo and also show the main traits for the acceptability of statistical timing analysis techniques as a feasible path. While providing a consolidated solution for the software timing analysis of Apollo is a huge effort far beyond the scope of a single research paper, our work aims to set the basis for future and more elaborated techniques for the timing analysis of AD software.",https://ieeexplore.ieee.org/document/9113112/,2020 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),21-24 April 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EAIS51927.2022.9787703,Tiny-MLOps: a framework for orchestrating ML applications at the far edge of IoT systems,IEEE,Conferences,"Empowering the Internet of Things devices with Artificial Intelligence capabilities can transform all vertical applications domains within the next few years. Current approaches favor hosting Machine Learning (ML) models on Linux-based single-board computers. Nevertheless, these devices&#x2019; cost and energy requirements limit the possible application scenarios. Conversely, today&#x2019;s available 32-bit microcontrollers have much lower costs and only need a few milliwatts to operate, making them an energy-efficient and cost-effective alternative. However, the latter devices, usually referred to as far edge devices, have stringent resource constraints and host non-Linux-based embedded real-time operating systems. Therefore, orchestrating such devices executing portions of ML applications represents a major challenge with current tools and frameworks. This paper formally introduces the Tiny-MLOps framework as the specialization of standard ML orchestration practices, including far edge devices in the loop. To this aim, we will tailor each phase of the classical ML orchestration loop to the reduced resources available onboard typical IoT devices. We will rely on the proposed framework to deliver adaptation and evolving capabilities to resource-constrained IoT sensors mounted on an industrial rotary machine to detect anomalies. As a feasibility study, We will show how to programmatically re-deploy ML-based anomaly detection models to far edge devices. Our preliminary experiments measuring the system performance in terms of deployment, loading, and inference latency of the ML models will corroborate the usefulness of our proposal.",https://ieeexplore.ieee.org/document/9787703/,2022 IEEE International Conference on Evolving and Adaptive Intelligent Systems (EAIS),25-26 May 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2016.7793038,Tool compensation in walk-through programming for admittance-controlled robots,IEEE,Conferences,"This paper describes a walk-through programming technique, based on admittance control and tool dynamics compensation, to ease and simplify the process of trajectory learning in common industrial setups. In the walk-through programming, the human operator grabs the tool attached at the robot end-effector and “walks” the robot through the desired positions. During the teaching phase, the robot records the positions and then it will be able to interpolate them to reproduce the trajectory back. In the proposed control architecture, the admittance control allows to provide a compliant behavior during the interaction between the human operator and the robot end-effector, while the algorithm of compensation of the tool dynamics allows to directly use the real tool in the teaching phase. In this way, the setup used for the teaching can directly be the one used for performing the reproduction task. Experiments have been performed to validate the proposed control architecture and a pick and place example has been implemented to show a possible application in the industrial field.",https://ieeexplore.ieee.org/document/7793038/,IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society,23-26 Oct. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/SOFTCOM.2019.8903866,Toward a Smart Real Time Monitoring System for Drinking Water Based on Machine Learning,IEEE,Conferences,"Drinking-water distribution systems facilitate to carry portable water from water resources such as reservoirs, river, and water tanks to industrial, commercial and residential consumers through complex pipe networks. This system may be affected by acts of pollution that may be intentional or accidental. Hence, it's necessary to prevent any intrusion into water distribution systems and to detect pollution as soon as possible. Therefore, water monitoring is required to maintain a good water quality for human and animal life. In this paper we intend to control the quality of the drinking-water using wireless sensor networks. First, we start with a detailed architecture of our smart system. This architecture uses a new generation of wireless sensors to detect the chemical, physical and microbiological water parameters. After, the water quality limits according to the Tunisian standard will exposed. Then we develop a new detection model of water anomalies. Our model is based on machine learning to detect anomalies and malicious acts in real time. In our solution a data aggregation method is created to minimize the amount of data and reduce the processing time.",https://ieeexplore.ieee.org/document/8903866/,"2019 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",19-21 Sept. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CoASE.2014.6899348,Toward safe close-proximity human-robot interaction with standard industrial robots,IEEE,Conferences,"Allowing humans and robots to interact in close proximity to each other has great potential for increasing the effectiveness of human-robot teams across a large variety of domains. However, as we move toward enabling humans and robots to interact at ever-decreasing distances of separation, effective safety technologies must also be developed. While new, inherently human-safe robot designs have been established, millions of industrial robots are already deployed worldwide, which makes it attractive to develop technologies that can turn these standard industrial robots into human-safe platforms. In this work, we present a real-time safety system capable of allowing safe human-robot interaction at very low distances of separation, without the need for robot hardware modification or replacement. By leveraging known robot joint angle values and accurate measurements of human positioning in the workspace, we can achieve precise robot speed adjustment by utilizing real-time measurements of separation distance. This, in turn, allows for collision prevention in a manner comfortable for the human user.We demonstrate our system achieves latencies below 9.64 ms with 95% probability, 11.10 ms with 99% probability, and 14.08 ms with 99.99% probability, resulting in robust real-time performance.",https://ieeexplore.ieee.org/document/6899348/,2014 IEEE International Conference on Automation Science and Engineering (CASE),18-22 Aug. 2014,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE43902.2021.00027,Towards Automating Code Review Activities,IEEE,Conferences,"Code reviews are popular in both industrial and open source projects. The benefits of code reviews are widely recognized and include better code quality and lower likelihood of introducing bugs. However, since code review is a manual activity it comes at the cost of spending developers' time on reviewing their teammates' code. Our goal is to make the first step towards partially automating the code review process, thus, possibly reducing the manual costs associated with it. We focus on both the contributor and the reviewer sides of the process, by training two different Deep Learning architectures. The first one learns code changes performed by developers during real code review activities, thus providing the contributor with a revised version of her code implementing code transformations usually recommended during code review before the code is even submitted for review. The second one automatically provides the reviewer commenting on a submitted code with the revised code implementing her comments expressed in natural language. The empirical evaluation of the two models shows that, on the contributor side, the trained model succeeds in replicating the code transformations applied during code reviews in up to 16% of cases. On the reviewer side, the model can correctly implement a comment provided in natural language in up to 31% of cases. While these results are encouraging, more research is needed to make these models usable by developers.",https://ieeexplore.ieee.org/document/9402025/,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),22-30 May 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIKE.2018.00037,"Towards Low-Cost, Real-Time, Distributed Signal and Data Processing for Artificial Intelligence Applications at Edges of Large Industrial and Internet Networks",IEEE,Conferences,"Digital Signal Processors (DSP) are vital system components in industrial Artificial Intelligence (AI) applications. In this paper, FIR filters that could be used for industrial AI applications are designed from the Spline Biorthogonal 1.5 (SB1.5) mother wavelet using a real-time, low-cost, generic industrial IoT (IIoT) hardware: the C28x DSP and low-level, Embedded C, system software. Our contribution in this paper is the first reported application of the C28x for SB1.5 wavelet construction. The significance of this approach is to be able to repurpose low-cost, readily available hardware for distributed AI applications. Our approach is different from the state of the art, in which specialized hardware are always manufactured for implementing AI applications at large network edges. Our approach supports low-cost and fast single-stage FIR implementation suitable for use in real-time, distributed AI application at network edges, since in our case, successive recursion of FIR filters leading to a full implementation of Pyramid Algorithm is not implemented. The designed FIR filter is evaluated and found capable of both low-pass and high pass filtering operations. Results of this paper indicate that the C28x real-time DSP, which exists in many IoT devices, could have improved scalability by being deployed for other important AI and IoT network edge analytic applications, different from its present uses.",https://ieeexplore.ieee.org/document/8527469/,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),26-28 Sept. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SP.2019.00001,Towards Practical Differentially Private Convex Optimization,IEEE,Conferences,"Building useful predictive models often involves learning from sensitive data. Training models with differential privacy can guarantee the privacy of such sensitive data. For convex optimization tasks, several differentially private algorithms are known, but none has yet been deployed in practice. In this work, we make two major contributions towards practical differentially private convex optimization. First, we present Approximate Minima Perturbation, a novel algorithm that can leverage any off-the-shelf optimizer. We show that it can be employed without any hyperparameter tuning, thus making it an attractive technique for practical deployment. Second, we perform an extensive empirical evaluation of the state-of-the-art algorithms for differentially private convex optimization, on a range of publicly available benchmark datasets, and real-world datasets obtained through an industrial collaboration. We release open-source implementations of all the differentially private convex optimization algorithms considered, and benchmarks on as many as nine public datasets, four of which are high-dimensional.",https://ieeexplore.ieee.org/document/8835258/,2019 IEEE Symposium on Security and Privacy (SP),19-23 May 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON48115.2021.9589594,Towards Sustainable Models of Computation for Artificial Intelligence in Cyber-Physical Systems,IEEE,Conferences,"This paper confronts with a reflection about a deep problem in computational models for cyber-physical systems (CPS). The problem arises in the contact between digital computing and the physical realm, and affects heavily the design, modeling, and implementation of CPS. Problems are exacerbated by the introduction of artificial intelligence and autonomy in industrial applications that have to meet sustainability of solutions, both in technical and societal sense. After a brief review, a new perspective and position on the future of sustainable CPS is addressed, and a pragmatic research path is presented. The RMAS (Relational-model Multi-Agent System) architecture is proposed as a test framework for the deep integration of real-world semantics into the advancements brought about by the digital transformation wave.",https://ieeexplore.ieee.org/document/9589594/,IECON 2021 – 47th Annual Conference of the IEEE Industrial Electronics Society,13-16 Oct. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IS.2006.348456,Towards an Agent-Based Framework for Online After-Sale Services,IEEE,Conferences,"The multi-agent paradigm for building intelligent systems has gradually been accepted by researchers and practitioners in the research field of artificial intelligence. There are also attempts of adapting agents and agent-based systems for creating industrial applications and providing e-services. In this paper, we present an attempt to use agents for constructing an online after-sale services system. The system is decomposed into four major cooperative agents, and in which each agent concentrates on particular aspects in the system and expresses intelligence by using various techniques. The proposed agent-based framework for the system is presented at both the micro-level and the macro-level according to the Gaia methodology. UML notations are also used to represent some software design models. As the result of this, agents are implemented into a framework for which exploits Case-Based Reasoning (CBR) technique to fulfil real life on-line services' diagnoses and tasks.",https://ieeexplore.ieee.org/document/4155463/,2006 3rd International IEEE Conference Intelligent Systems,4-6 Sept. 2006,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/SMAGRIMET.2018.8369850,Towards statistical machine learning for edge analytics in large scale networks: Real-time Gaussian function generation with generic DSP,IEEE,Conferences,"The smart grid (SG) is a large-scale network and it is an integral part of the Internet of Things (IoT). For a more effective big data analytic in large-scale IoT networks, reliable solutions are being designed such that many real-time decisions will be taken at the edge of the network close to where data is being generated. Gaussian functions are extensively applied in the field of statistical machine learning, pattern recognition, adaptive algorithms for function approximation, etc. It is envisaged that soon, some of these machine learning solutions and other gaussian function based applications that have low computation and low-memory footprint will be deployed for edge analytics in large-scale IoT networks. Hence, it will be of immense benefit if an adaptive, low-cost, method of designing gaussian functions becomes available. In this paper, gaussian distribution functions are designed using C28x real-time digital signal processor (DSP) that is embedded in the TMS320C2000 modem designed for powerline communication (PLC) at the low voltage distribution end of the smart grid, where numerous devices that generate massive amount of data exist. Open-source embedded C programming language is used to program the C28x for real-time gaussian function generation. The designed gaussian waveforms are stored in lookup tables (LUTs) in the C28x embedded DSP, and could be deployed for a variety of applications at the edge of the SG and IoT network. The novelty of the design is that the gaussian functions are designed with a generic, low-cost, fixed-point DSP, different from state of the art in which gaussian functions are designed using expensive arbitrary waveform generators and other specialized circuits. C28x DSP is selected for this design since it is already existing as an embedded DSP in many smart grid applications and in other numerous industrial systems that are part of the large scale IoT network, hence it is envisaged that integration of any gaussian function based solution using this DSP in the smart grid and other IoT systems may not be too challenging.",https://ieeexplore.ieee.org/document/8369850/,2018 First International Colloquium on Smart Grid Metrology (SmaGriMet),24-27 April 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COASE.2018.8560470,Training CNNs from Synthetic Data for Part Handling in Industrial Environments,IEEE,Conferences,"As Convolutional Neural Network based models become reliable and efficient, two questions arise in relation to their applications for industrial purposes. The usefulness of these models in industrial environments and their implementation in these settings. This paper describes the autonomous generation of Region based CNN models trained on images from rendered CAD models and examines their applicability and performance for part handling application. The development of the automated synthetic data generation is detailed and two CNN models are trained with the aim to detect a car component and differentiate it against another similar looking part. The performance of these models is tested on real images and it was found that the proposed approach can be easily adopted for detecting a range of parts in arbitrary backgrounds. Moreover, the use of syntheic images for training CNNs automates the process of generating a detector.",https://ieeexplore.ieee.org/document/8560470/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.1992.230637,Trajectory tracking using neural networks,IEEE,Conferences,"Presents a method for tracking prespecified trajectories in industrial drive systems with a multilayered feedforward neural network. The method utilizes the backpropagation technique to learn feedback-error ranges, in which appropriate control actions can be generated from a lookup table. It can follow arbitrarily prescribed trajectories even when they are not present in the training phase. This approach is simple and practical for real-time implementation. Examples are included to demonstrate the effectiveness. The analogy between this scheme and a fuzzy logic control strategy is also investigated.<>",https://ieeexplore.ieee.org/document/230637/,1992 IEEE International Symposium on Circuits and Systems (ISCAS),10-13 May 1992,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITES53477.2021.9637088,TransTracking for UAV: An Autonomous Real-time Target Tracking System for UAV via Transformer Tracking,IEEE,Conferences,"Most of the existing Siamese-type trackers usually adopt pre-defined anchor boxes or anchor-free schemes to accurately estimate the bounding box of targets. Unfortunately, they suffer from complicated hand-designed components and tedious post-processings. It is not easy to adjust parameters for unique scenes in real applications. So, we propose a new scheme by formulating visual tracking as a direct set prediction problem to alleviate this issue. The main component is a transformer attached to the Siamese-type feature extraction networks. Thus, our new framework can be summarized as Siamese Network with Transformers (SiamTFR). With a fixed small set of learned object queries, we force the final set of predictions via bipartite matching, significantly reducing hyper-parameters associated with the candidate boxes. Due to the unique predictions of this framework, we significantly ease the heavy burden of hyper-parameters search of post-processings in visual tracking. Extensive experiments on visual tracking benchmarks, including GOT-10K, demonstrate that SiamTFR achieves competitive performance and runs at 50 FPS. Specifically, SiamTFR outperforms leading anchor-based tracker SiamRPN++ in the GOT-10K benchmark, confirming its effectiveness and efficiency. Furthermore, SiamTFR is deployed on the embedded device in which the algorithm can be run at 30FPS or 54FPS with TensorRT meeting the real-time requirements. In addition, we design the complete tracking system demo that can work in the real road to narrow the gap between the academic models and industrial deployments.",https://ieeexplore.ieee.org/document/9637088/,2021 International Conference on Intelligent Technology and Embedded Systems (ICITES),31 Oct.-2 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAI55435.2022.9773637,Transferable Learning Architecture for Scalable Visual Quality Inspection,IEEE,Conferences,"In recent years, convolutional neural networks (CNNs) have become a de facto standard in computer vision for object detection and recognition. At present, CNNs have been used in many application areas including the automation of industrial manufacturing processes. But using CNN in a real-time environment to track defects on products has many shortcomings like long training time, large data requirements, slow inference time, dynamic environment, and hardware dependency. This paper evaluates the state-of-the-art CNN architectures for object detection to address the mentioned challenges and provide the best possible solution. A set of pre-trained models has been trained on just 781 annotated images by applying transfer learning. Experimental results showed that Faster RCNN with VGG-16 backbone outperforms the other models in case of accuracy and mAP. But RetinaNet with an FPN backbone has the fastest inference time on multi-scaled defects. Paper also presents the deployment pipeline for inference on mobile devices to use in a real-time environment without any special hardware. In addition, an improved dataset of submersible pump impellers, based on the existing Kaggle dataset is introduced.",https://ieeexplore.ieee.org/document/9773637/,2022 2nd International Conference on Artificial Intelligence (ICAI),30-31 March 2022,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SERVICES51467.2021.00055,Transformation: Case Studies and Lessons Learned : Keynote 2,IEEE,Conferences,"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Industrial AI, Big Data Analytics, Machine Learning, and Cyber Physical Systems are changing the way we design product, manufacturing, and service systems. It is clear that as more sensors and smart analytics software are integrated in the networked industrial products and manufacturing systems, predictive technologies can further learn and autonomously optimize service productivity and performance. This presentation will address the trends of Industrial AI for smart service realization. First, Industrial AI systematic approach will be introduced. Case studies on advanced predictive analytics technologies for different maintenance and service operations will be demonstrated. In addition, issues on data quality for high performance and real-time data analytics in future digital service will be discussed.",https://ieeexplore.ieee.org/document/9604414/,2021 IEEE World Congress on Services (SERVICES),5-10 Sept. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TOCS53301.2021.9688914,Two-dimensional animation interactive demonstration teaching system based on multimedia technology---artificial intelligence and information technology,IEEE,Conferences,"With the development and innovation of computer technology and the heavy research of media, A multimedia technology that can interact with computers in real-time information has appeared. It is the technology of industrial life and education. On the other hand, this promotes the extent of computer applications and combines multimedia technologies in education. This is new. The development of the world is a way of finding new factors and increasing learning. An animation technology as a product plays an incomplete role in the production of the multimedia cursors. As we all know, the multimedia materials developed by Flash software are known for their exquisite two-dimensional animations. However, it is more difficult to implement interactive two-dimensional molecular model demonstrations in Flash. Combining the current animation technology and database technology, this article has conducted an in-depth study on it, and designed a set of interactive multimedia teaching system based on two-dimensional animation technology. This article first introduces the basic principles of animation technology and the implementation of interactive information platforms. On this basis, the system&#x0027;s hierarchical structure and development process are analyzed, and the main work is divided into two key parts. On the one hand, it uses animation technology to design and t make vivid, feasible and interactive teaching courseware, on the other hand, to build an interactive information management teaching platform combined with database principles.",https://ieeexplore.ieee.org/document/9688914/,"2021 IEEE Conference on Telecommunications, Optics and Computer Science (TOCS)",10-11 Dec. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIIC48513.2020.9065203,UAV-assisted Real-time Data Processing using Deep Q-Network for Industrial Internet of Things,IEEE,Conferences,"Industrial internet of things (IIoT) enables edge computing technology to provide communication between the machines that produce a large amount of data and locate at the edge network. A task scheduling is implemented in the edge node. Furthermore, the real-time data can achieve with the lowest latency that allowed by the edge node near the edge network. However, a mobile machine such as an autonomous guided vehicle can interfere in this situation. Because the vehicle also needs service by the edge node. Over that, quality of service (QoS) performance can decrease. Therefore, this paper deploys an unmanned aerial vehicle (UAV) as an edge node to provide service to the edge network through optimizing the trajectory of UAV, where the edge network request task using a Deep Q-Network (DQN) Learning. The result shows that using machine learning, notably the DQN algorithm, can increase the number of the machine that can be provided service. Subsequently, the real-time data can achieve either the interrupt occurs at the edge node.",https://ieeexplore.ieee.org/document/9065203/,2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),19-21 Feb. 2020,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP39728.2021.9414882,Unsupervised Clustering of Time Series Signals Using Neuromorphic Energy-Efficient Temporal Neural Networks,IEEE,Conferences,"Unsupervised time series clustering is a challenging problem with diverse industrial applications such as anomaly detection, bio-wearables, etc. These applications typically involve small, low-power devices on the edge that collect and process real-time sensory signals. State-of-the-art time-series clustering methods perform some form of loss minimization that is extremely computationally intensive from the perspective of edge devices. In this work, we propose a neuromorphic approach to unsupervised time series clustering based on Temporal Neural Networks that is capable of ultra low-power, continuous online learning. We demonstrate its clustering performance on a subset of UCR Time Series Archive datasets. Our results show that the proposed approach either outperforms or performs similarly to most of the existing algorithms while being far more amenable for efficient hardware implementation. Our hardware assessment analysis shows that in 7 nm CMOS the proposed architecture, on average, consumes only about 0.005 mm2 die area and 22 μW power and can process each signal with about 5 ns latency.",https://ieeexplore.ieee.org/document/9414882/,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",6-11 June 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISNCC52172.2021.9615827,Unsupervised Time-Series based Anomaly Detection in ICS/SCADA Networks,IEEE,Conferences,"Traditionally, Industrial Control Systems (ICS) have been operated as air-gapped networks, without a necessity to connect directly to the Internet. With the introduction of the Internet of Things (IoT) paradigm, along with the cloud computing shift in traditional IT environments, ICS systems went through an adaptation period in the recent years, as the Industrial Internet of Things (IIoT) became popular. ICS systems, also called Cyber-Physical-Systems (CPS), operate on physical devices (i.e., actuators, sensors) at the lowest layer. An anomaly that effect this layer, could potentially result in physical damage. Due to the new attack surfaces that came about with IIoT movement, precise, accurate, and prompt intrusion/anomaly detection is becoming even more crucial in ICS. This paper proposes a novel method for real-time intrusion/anomaly detection based on a cyber-physical system network traffic. To evaluate the proposed anomaly detection method&#x2019;s efficiency, we run our implementation against a network trace taken from a Secure Water Treatment Testbed (SWAT) of iTrust Laboratory at Singapore.",https://ieeexplore.ieee.org/document/9615827/,"2021 International Symposium on Networks, Computers and Communications (ISNCC)",31 Oct.-2 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/QUATIC.2016.011,Use of Ontologies in Embedded Systems: A Systematic Mapping,IEEE,Conferences,"Many domains of Embedded Systems (e.g., automotive, avionics, consumer electronics, industrial automation, medical...) are rapidly evolving toward solutions that integrate hardware and software, or incorporate complete systems in a single chip. The specificities of the concepts have a complexity on data analysis and during the identification of relationship between them. The usage of an ontology for embedded systems seems to be necessary. Indeed, ontology is an important engineering artifact used in several domains, that provides uniformity to the concepts in terms of syntax and semantics, facilitates the communication in various fields and improves the understanding of requirements. This work conducted a systematic mapping, to collect evidence about the need of ontologies in embedded systems development. We focus on the identification of ontology that represents languages used for the development of embedded systems, considering the various domains of these, as well as the benefits of using an ontology for this purpose. After applying selection criteria in the mapping driving phases, 19 papers were selected and analyzed. This mapping provides evidences of real benefits in using ontologies for embedded systems.",https://ieeexplore.ieee.org/document/7814508/,2016 10th International Conference on the Quality of Information and Communications Technology (QUATIC),6-9 Sept. 2016,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCI.2015.116,User-Centric Workflow Ergonomics in Industrial Environments: Concept and Architecture of an Assistance System,IEEE,Conferences,"Changes in demographic developments come along with an ageing workforce and a higher retirement age. Particularly in the industrial working environment, poor workplace ergonomics can limit the workers' quality of health and thus their ability to work until they reach their statutory retirement age. This paper presents the conceptual design of an assistance system that captures information from workers' bearings based on on-body sensors. A real-time analysis of the captured sensor data enables giving feedback to workers at the assembly site as soon as they get into an ergonomic unhealthy position during workflow execution. Workflow managers benefit from a holistic evaluation of the captured ergonomic data. By this means, critical workflow activities that are characterized by a high degree of malpositions can be identified. Based on this information, workflow managers have the possibility to optimize workflows in an ergonomic-friendly way. Furthermore, the presented approach enables a differentiated evaluation of established methods of ergonomic feedback like OWAS or EAWS.",https://ieeexplore.ieee.org/document/7424190/,2015 International Conference on Computational Science and Computational Intelligence (CSCI),7-9 Dec. 2015,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONMEDIA46929.2019.8981830,Using Naïve Bayes Classifier for Application Feedback Classification and Management in Bahasa Indonesia,IEEE,Conferences,"The world keeps moving, software products too. An application's objectives, structures, requirements, and assumptions that have been elicited and analyzed previously may need to be reassessed and updated. In order to fully understand these requirements evolutions, what changes are necessary, and why those changes are needed, one essential source of requirements is user feedback. However, handling and analyzing so many user feedbacks can be time-consuming. Using natural language processing tools for Bahasa Indonesia and Naive Bayes classifier, this research aims to develop a tool to process natural language and classify user feedbacks. The developed tool is expected to make feedback classification less time-consuming so that developers can project their energy to more productive and creative works. The machine learning models are built using the feedback dataset taken from an up-and-running university e-learning system and show promising results. The highest confusion matrix scores are 92.5% for accuracy, 85.6% precision, 85.1% recall, and lastly, 85.4% for the F-measure score. The resulting web application for feedback management is then evaluated to the users, and even though it still needs to be further polished and improved for real industrial use, it is perceived to be useful and easy to use.",https://ieeexplore.ieee.org/document/8981830/,2019 5th International Conference on New Media Studies (CONMEDIA),9-11 Oct. 2019,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/cp:19940284,Using expert systems for on-line data qualification and state variable estimation for an industrial fermentation process,IET,Conferences,"An industrial fed batch fermentation process is a nonlinear time-varying process. Important internal state variables such as biomass, substrate and product concentrations cannot be measured online and are usually determined by infrequent and time consuming off-line laboratory analysis. The online measurements are usually noisy and sometimes this leads to misinterpretation of the real situation inside the fermenter. These problems can lead to poor control of the batch and low productivity subsequently. To overcome these problems a real time expert system has been proposed which is based on the Poplog Flex real time expert system shell. The system is used to monitor the state variables of the process, diagnose any fault that might occur in the process, estimate the important unmeasurable state variables and to design a controller to control the state around a desired level. A neural network has been adopted for the online estimation of the unmeasurable state variables. Pattern recognition ideas have been used to improve the modelling ability of the neural network. Predictive control techniques have been used to control the state around a desired level. The model and the controller for the process have been designed and implemented within the Poplog Flex environment.<>",https://ieeexplore.ieee.org/document/327321/,1994 International Conference on Control - Control '94.,21-24 March 1994,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR52153.2021.00029,ViRe-in2-GP-Methodology for Virtual Reality in Major Industrial and Infrastructural Projects,IEEE,Conferences,"Even though virtual reality (VR) technologies promise to boost efficiency because their three-dimensionality, virtuality and interactivity and enable simple and objectified access to object and process information representations, the design and implementation of VR systems that support major industrial and infrastructure project planning is largely case-based, permitting virtually no systematic improvement. The ViRe-in2 -methodology presented combines a structured, iterative process for VR planning of major industrial and infrastructure projects with the integrability of established, domain-specific methods and inherent cycles of self-optimization. The complexity of the requirements in this challenging domain is already being met systematically with effectively employed VR approaches.",https://ieeexplore.ieee.org/document/9644387/,2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),15-17 Nov. 2021,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
