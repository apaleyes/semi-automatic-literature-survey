id,datePublished,description,journals,publisher,title,doi,downloadUrl,database,query_name,query_value
161050422,2017-01-03T00:00:00,"Several neural network controllers for robotic manipulators have been developed during the last decades due to their capability to learn the dynamic properties and the improvements in the global stability of the system. In this paper, an adaptive neural controller has been designed with self learning to resolve the problems caused by using a classical controller. A comparison between the improved unsupervised adaptive neural network controller and the P controller for the NXT SCARA robot system is done, and the result shows the improvement of the self learning controller to track the determined trajectory of robotic automated controllers with uncertainties. Implementation and practical results were designed to guarantee online real-time","[{'title': 'Recent Innovations in Mechatronics', 'identifiers': ['issn:2064-9622', '2064-9622']}]",'University of Debrecen/ Debreceni Egyetem',Implementation of Adaptive Neural Networks Controller for NXT SCARA Robot System,10.17667/riim.2017.1/3.,https://core.ac.uk/download/161050422.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
304935586,2018-06-30T22:22:06,"Advanced autonomous robotics space missions rely heavily on the flawless interaction of complex hardware, multiple sensors, and a mission-critical software system.  This software system consists of an operating system, device drivers, controllers, and executives; recently highly complex AI-based autonomy software have also been introduced. Prior to launch, this software has to undergo rigorous verification and validation (V&V).  Nevertheless, dormant software bugs, failing sensors, unexpected hardware-software interactions, and unanticipated environmental conditions—likely on a space exploration mission—can cause major software faults that can endanger the entire mission.

Our Integrated Software Health Management (ISWHM) system continuously monitors the hardware sensors and the software in real-time. The ISWHM uses Bayesian networks, compiled to arithmetic circuits, to model software and hardware interactions. Advanced reasoning algorithms using arithmetic circuits not only enable the ISWHM to handle large, hierarchical models that are necessary in the realm of complex autonomous systems, but also enable efficient execution on small embedded processors. The latter capability is of extreme importance for small (mobile) autonomous units with limited computational power and low telemetry bandwidth.  In this paper, we discuss the requirements of ISWHM.  As our initial demonstration platform, we use a primitive Lego rover. A Lego 
Mindstorms microcontroller is used to implement a highly simplified autonomous rover driving system, running on the OSEK real-time operating system. We demonstrate that our ISWHM, running on this small embedded microcontroller, can perform fault detection as well as on-board reasoning for advanced diagnosis and root-cause detection in real time",,,Software and System Health Management for Autonomous Robotics Missions,10.1184/r1/6710654.v1,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
323332613,2014-11-19T00:00:00,"Robotics has become a common subject in many engineering degrees and postgraduate programs. Although at undergraduate levels the students are introduced to basic theoretical concepts and tools, at postgraduate courses more complex topics have to be covered. One of those advanced subjects is Cognitive Robotics, which covers aspects like automatic symbolic reasoning, decision-making, task planning or machine learning. In particular, Reinforcement Learning (RL) is a machine learning and
decision-making methodology that does not require a model of the environment where the robot operates, overcoming this limitation by making observations. In order to get the greatest educational benefit, RL theory should be complemented with some hands-on RL task that uses a real robot, so students get a complete vision of the learning problem, as well as of the issues that arise when dealing with a physical robotic platform. There are several RL techniques that can be studied in such a subject; we have chosen Q-learning, since is a simple, effective and well-known RL algorithm.

In this paper we present a minimalist implementation of the Q-learning method for a Lego Mindstorms NXT mobile robot, focused on simplicity and applicability, and flexible enough to be adapted to several tasks. Starting from a simple wandering problem, we first design an off-line model of the learning process in which the Q-learning parameters are studied. After that, we implement the algorithm on the robot, gradually enlarging the number of states-actions of the problem. The final result of this work is a teaching framework for developing practical activities regarding Q-learning in our Robotics subjects, which will improve our teaching.Universidad de Málaga. Campus de Excelencia Internacional Andalucía Tech",,,LEGO© Mindstorms NXT and Q-Learning: a teaching approach for robotics in engineering,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
33705626,2014,"Humans have the subconscious ability to create simple abstractions from observations of their physical environment. The ability to consider the colour of an object in terms of ""red"" or ""blue"", rather than spatial distributions of reflected light wavelengths, is vital in processing and communicating information about important features within our local environment. The real-time identification of such features in image processing necessitates the software implementation of such a process; segmenting an image into regions of salient colour, and in doing so reducing the information stored and processed from 3-dimensional pixel values to a simple colour class label. This paper details a method by which colour segmentation may be performed offline and stored in a static look-up table, allowing for constant time dimensionality reduction in an arbitrary environment of coloured features. The machine learning framework requires no human supervision, and its performance is evaluated in terms of feature classification performance within a RoboCup robot soccer environment. The developed system is demonstrated to yield an 8% improvement over slower traditional methods of manual colour mapping",,Springer Verlag,Unsupervised recognition of salient colour for real-time image processing,10.1007/978-3-662-44468-9_33,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
522188581,2018-09-01T00:00:00,"© 2018 IEEE.The capability of following a person is crucial in service-oriented robots for human assistance and cooperation. Though a vast variety of following systems exist, they lack robustness against dynamic changes of the environment and relocating to continue following a lost target. Here we present a robust human following system that has the extendability to commercial service robot platforms having a RGB-D camera. The proposed framework integrates deep learning methods for perception and variational Bayesian techniques for trajectory prediction. Deep learning modules enable robots to accompany a person by detecting the target, learning the target and following while avoiding collision within the dynamic home environment. The variational Bayesian techniques robustly predict the trajectory of the target by empowering the following ability of the robot when target is lost. We experimentally demonstrate the capability of the deep Bayesian trajectory prediction method on real-time usage, following abilities, collision avoidance and trajectory prediction of the system. The proposed system was deployed at the RoboCup@Home 2017 Social Standard Platform League and successfully demonstrated its robust functions and smooth person following capability resulting in winning the 1st place.N","[{'title': None, 'identifiers': ['issn:1050-4729', '1050-4729']}]",Institute of Electrical and Electronics Engineers Inc.,Robust Human Following by Deep Bayesian Trajectory Prediction for Home Service Robots,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
352152004,2019-03-20T00:00:00,"Robotic Process Automation (RPA) is a new wave of the future technologies. Robotic Process Automation is one of the most advanced technologies in the area of computers science, electronic and communications, mechanical engineering, and information technology. It is a combination of both hardware and software, networking and automation for doing things very simple. In this light, the research manuscript investigated the secondary data - which is available on google, academic and research databases. The investigation went for totally 6 months, i.e., 1-1-2018 to 30-6-2018. A very few empirical articles, white papers, blogs and were found RPA and came across to compose this research manuscript. This study is exploratory in nature because of the contemporary phenomenon. The keywords used in searching from the database were Robotic Process Automation, RPA, Robots, Artificial Intelligence, Blue Prism. The study finally discovered that Robots and Robotic Process Automation technologies are becoming compulsory as a part to do business operations in organizations across the globe. Robotic Process Automation can bring immediate value to the core business processes including employee payroll, employee status changes, new hire recruitment, and onboarding, accounts receivable and payable, invoice processing, inventory management, report creation, software installations, data migration, and vendor onboarding etc. to name a few applications. Besides, the Robotic Process Automation has abundant applications including healthcare and pharmaceuticals, financial services, outsourcing, retail, telecom, energy and utilities, real estate and FMCG and many more sectors. To put in the right place of RPA in business operations, their many allied technologies are working at the background level, artificial intelligence, machine learning, deep learning, data analytics, HR analytics, virtual reality (second life), home automation, blockchain technologies, 4D printing etc. Moreover, it covers the content of different start-ups companies and existing companies - their RPA applications used across the world. This manuscript will be a good guideline for the academicians, researchers, students, and practitioners to get an overall idea",,"Universidade de São Paulo. Faculdade de Economia, Administração e Contabilidade",The future digital work force: Robotic Process Automation (Rpa),,https://core.ac.uk/download/352152004.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
323090892,2015-07-10T00:00:00,"[EN] Unmanned Underwater Vehicles (UUVs) explore different habitats with a view to protecting and managing them. They are developed to overcome scientific challenges and the engineering problems caused by the unstructured and hazardous underwater environment in which they operate. Their development bears the same difficulties as the rest of service robots (hardware heterogeneity, sensor uncertainty, software complexity, etc.) as well as other particular from the domain, like the underwater environment, energy constraints, and autonomy. This article describes the AEGIR UUV, used as a test bed for implementation of control strategies and oceanographic mission in the Mar Menor area in Spain, which is one of the largest coastal lagoons in Europe. It also describes the development of a tool chain that follows a model-driven approach, which has been used in the design of the vehicle control software as well as a component-based framework that provides the runtime support of the application and enables its flexible deployment in nodes, processes and threads and pre-verification of concurrent behavior.[ES] Los vehículos submarinos no tripulados (Unmanned Underwater Vehicles, UUVs) se diseñan para misiones de monitorización, inspección e intervención. En estudios oceanográficos y de monitorización ambiental son cada vez más demandados por las innumerables ventajas que presentan con respecto a las tecnologías tradicionales. Estos vehículos son desarrollados para superar los retos científicos y los problemas de ingeniería que aparecen en el entorno no estructurado y hostil del fondo marino en el que operan. Su desarrollo no solo conlleva las mismas dificultades que el resto de los robots de servicio (heterogeneidad en el hardware, incertidumbre de los sistemas de medida, complejidad del software, etc.), sino que además se les unen las propias del dominio de aplicación, la robótica submarina: condiciones de iluminación, incertidumbre en cuanto a posición y velocidad, restricciones energéticas, etc. Este artículo describe el UUV AEGIR, un vehículo utilizado como banco de pruebas para la implementación de estrategias de control y misiones oceanográficas. También describe el desarrollo de una cadena de herramientas que sigue un enfoque dirigido por modelos, utilizada en el diseño del software de control del vehículo, así como un framework basado en componentes que proporciona el soporte de ejecución de la aplicación y permite su despliegue flexible en nodos, procesos e hilos y pre-verificación del comportamiento concurrente. Su diseño ha permitido desarrollar, comprobar y añadir los componentes que proporcionan el comportamiento necesario para que el UUV AEGIR pudiera completar con éxito distintos tipos de misiones oceanográficas.Este trabajo ha sido parcialmente financiado por el proyecto
financiado por la CICYT del Gobierno Español DIVISAMOS
(ref. DPI2009-14744-C03-02) y ViSelTR (ref. TIN2012-39279),
así como por el proyecto financiado por la Fundación Séneca de
la Región de Murcia MISSION-SICUVA (ref. 15374/PI/10) y el
proyecto “Coastal Monitoring System for the Mar Menor Coastal
Lagoon (PEPLAN 463.02-08 CLUSTER de la Región de Murcia.
Francisco Sánchez Ledesma agradece la financiación recibida por
parte del programa de becas FPU del MEC (beca AP2009-5083).
Por último, los autores quieren agradecer también a la Armada
Española la cesión del vehículo UUV y su posterior ayuda en su
reconstrucción.Ortiz, F.; Guerrero, A.; Sánchez Ledesma, F.; García Córdova, F.; Alonso, D.; Gilabert, J. (2015). Diseño del software de control de un UUV para monitorización oceanográfica usando un modelo de componentes y framework con despliegue flexible. Revista Iberoamericana de Automática e Informática industrial. 12(3):325-337. https://doi.org/10.1016/j.riai.2015.06.003OJS325337123Alonso, D., Pastor, J., Sánchez, P., Álvarez, B., Vicente-Chicote, C., 2012. Generación Automática de Software para Sistemas de Tiempo Real: Un Enfoque basado en Componentes, Modelos y Frameworks. Revista Iberoamericana de Automática e Informática Industrial RIAI. Vol, 9, Num. 2, págs 170-181, doi: 10.1016/j.riai.2012.02.010.Alonso, D., Vicente-Chicote, C., Ortiz, F., Pastor, J., Álvarez, B., 2010. V3CMM: a 3-View Component Meta-Model for Model-Driven Robotic Software Development. Journal of Software Engineering for Robotics, Vol.1, no 1, pp. 3-17.Antonelli, G., Chiaverini, S., Sarkar,N., West, M., 2001. Adaptive control of an autonomous underwater vehicle: experimental results on ODIN, IEEE Trans Control Syst. Technol., vol. 9, Issue: 5, Sep. 2001, pp. 756-765.Auke Jan Ijspeert, 2008. Central pattern generators for locomotion control in animals and robots: A review. Neural Networks, Volume 21 Issue 4, pp. 642-653.Ben-Ari, M., 2008. Principles of the Spin Model Checker. Springer-Verlag.Bengtsson, J., Yi, W., 2004. Timed Automata: Semantics, Algorithms and Tools. In: Lectures on concurrency and Petri nets, Springer-Verlag, vol. 3098, pp. 87-124.Behrmann, G., Larsen, K., Moller, O., David, A., Pettersson, P., Wang, Y., 2001. UPPAAL - present and future. Proc. of the 40th IEEE Conf. on Decision and Control.Bruyninckx, H., 2008. Robotics Software: The Future Should Be Open. In: IEEE Robotics & Automation Magazine, Vol. 15, No. 1, pp. 9-11.Carreras, M., Yuh, J., Batlle, J., Ridao, P., 2005. A behavior-based scheme Using Reinforcement Learning for Autonomous Underwater Vehicles. In: IEEE Journal of Oceanic Engineering, Vol. 30, No. 2.Chang, C., and Gaudiano, P., 1998. Application of biological learning theories to mobile robot avoidance and approach behaviors. J. Complex Systems, vol. 1, pp. 79-114.Eickstedt, D.P., Sideleau, S.R., 2009. The backseat control architecture for autonomous robotic vehicles: A case study with the Iver2 AUV. In: OCEANS 2009, MTS/IEEE Biloxi - Marine Technology for Our Future: Global and Local Challenges. 1-8.Fossen, T., 1994. Guidance and control of ocean vehicles. John Wiley and Sons Ltd.Fujii, T., Arai, Y., Asama, H., and Endo, I., 1998. Multilayered reinforcement learning for complicated collision avoidance problems. In: Proceedings IEEE International Conference on Robotics and Automation, vol. 3, pp. 2186-2191, Leuven, Belgium.García-Córdova, F., 2007. A cortical network for control of voluntary movements in a robot finger. In: Neurocomputing, vol. 71, 2007, pp. 374-391.Gonzalez, J. et al, 2012. AUV Based Multi-vehicle Colla*boration: Salinity Studies in Mar Menor Coastal Lagoon. In IFAC Workshop on Navigation, Guidance and Control of Underwater Vehicles (NGCUV).Guerrero-González, A., García-Córdova, F., Ruz-Vila, F., 2010. A Solar Powered Autonomous Mobile Vehicle for Monitoring and Surveillance Missions of Long Duration. In: International Review of Electrical Engineering, Part A, vol. 5, n. 4, pp. 1580-1587.Guerrero-González, A., García-Córdova, F., Gilabert, J., 2011. A Biologically inspired neural network for navigation with obstacle avoidance in autonomous underwater and surface vehicles. In: OCEANS 2011 IEEE. Doi. 10.1109/Oceans-Spain.2011.6003432.Medina, J., González-Harbour, M., and Drake, J., 2001. MAST real-time view: A graphic UML tool for modeling object-oriented real-time systems. Proc. of the 22nd IEEE Real-Time Systems Symposium (RTSS), December, pp. 245-256. IEEE.Pérez-Ruzafa, A., Marcos, C., Gilabert, J., 2005. The Ecology of the Mar Menor coastal lagoon: a fast-changing ecosystem under human pressure. In: Coastal lagoons. Ecosystem processes and modelling for sustainable use and development. CRC press. pp.: 392-422.Ridao, P., Yuh, J., Sugihara, K., Batlle, J., 2000. On AUV control architecture. In: Proc. Int. Conf. Robots and Systems.Ritter, H., Martinez, T., Schulten, K., 1989. Topology-conserving maps for learning visuo-motor coordination, Neural Networks, vol. 2, 1989, pp. 159-168.Schlegel, C., 2006. Communication patterns as key towards component-based robotics. In: International Journal on Advanced Robotics Systems 3 (1), 49-54.Schlegel, C., Steck, C., Lotz, A., 2011. Model-driven software development in robotics: Communication patterns as key for a robotics component model, En: Introduction to Modern Robotics. iConcept Press (ed.on-line).Stutters, L., Liu, H., Tiltman, C., Brown, D., 2008. Navigation technologies for autonomous underwater vehicles. In: IEEE Transactions on Systems, Man and Cybernetics, Part C: Applications and Reviews, Vol. 38, 581-589.OMG, 2009. UML profile for MARTE: Modeling and analysis of real-time embedded systems, formal/2009-11-02.RoSta: Robot Standards and Reference Architectures, Coordination Action (CA) funded under the European Union's Sixth Framework Programme (FP6), http://www.robot-standards.org/. Last accessed 05/2013.Singhoff, F., Plantec, A., Dissaux, P., Legrand, J., 2009. Investigating the usability of real-time scheduling theory with the cheddar project. Journal of Real Time Systems, 43, 259-295",,'Elsevier BV',Design of the control software of a UUV for oceanographic monitoring using a component model and framework with flexible deployment.,10.1016/j.riai.2015.06.003,http://hdl.handle.net/10251/143643,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
195551950,2016-12-05T00:00:00,"International audienceOur demonstration presents an open-source hardware and software platform which allows non-roboticistsresearchers to conduct machine learning experiments to benchmark algorithms for autonomous explorationand active learning. In particular, in addition to showing the general properties of the platform such asits modularity and usability, we will demonstrate the online functioning of a particular algorithm whichallows efficient learning of multiple forward and inverse models and can leverage information from humanguidance. A first aspect of our demonstration is to illustrate the ease of use of the 3D printed low-costPoppy humanoid robotic platform, that allows non-roboticists to quickly set up and program roboticexperiments. A second aspect is to show how the Explauto library allows systematic comparison andevaluation of active learning and exploration algorithms in sensorimotor spaces, through a Python API toselect already implemented exploration algorithms. The third idea is to showcase Active Model Babbling,an efficient exploration algorithm dynamically choosing which task/goal space to explore and particulargoals to reach, and integrating social guidance from humans in real time to drive exploration towardsparticular objects or actions.[Forestier and Oudeyer, 2016] Forestier, S. and Oudeyer, P.-Y. (2016). Modular active curiosity-driven discovery oftool use. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, Korea.[Lapeyre et al., 2014] Lapeyre, M., Rouanet, P., Grizou, J., Nguyen, S., Depraetre, F., Le Falher, A., and Oudeyer,P.-Y. (2014). Poppy Project: Open-Source Fabrication of 3D Printed Humanoid Robot for Science, Educationand Art. In Digital Intelligence 2014, page 6, Nantes, France.[Moulin-Frier et al., 2014] Moulin-Frier, C., Rouanet, P., Oudeyer, P.-Y., and others (2014). Explauto: an open-source Python library to study autonomous exploration in developmental robotics. In ICDL-Epirob-InternationalConference on Development and Learning, Epirob",,HAL CCSD,"Autonomous exploration, active learning and human guidance with open-source Poppy humanoid robot platform and Explauto library",,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
237173517,2019,"In this paper we investigate the impact of a social robot in the context of serious games in which the robot plays the role of a game opponent by challenging and, at the same time, teaching the child to correctly recycle waste materials. To this aim we performed a study in which we investigated the dimensions that are used to evaluate serious games integrated with those that are typical of the interaction with a social robot. To endow the robot with the capability to play as a game opponent in a real-world context, we implemented an image recognition module based on a Convolutional Neural Network so that the robot could detect and classify the waste material as a child would do, by seeing it. After a preliminary evaluation of the approach, we started a formal experiment in which we measured the effectiveness of game design, the robot evaluation and the evaluation of cognitive and affective elements that can form the pro-environmental attitude and then the tendency to recycling. A primary school classroom was involved in the study and, results obtained so far, are encouraging and drew promising possibilities for robotics education in changing recycling attitude for children since Pepper is positively evaluated as trustful and believable and this allowed to be concentrated on the ‘memorization’ task during the game",,IEEE Computer Society,Investigating the Social Robots’ Role in Improving Children Attitudes toward Recycling. The case of PeppeRecycle,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
302828762,2017-11-08T00:00:00,"Intelligent Autonomous Robots deployed in human environments must have understanding of the wide range of possible semantic identities associated with the spaces they inhabit – kitchens, living rooms, bathrooms, offices, garages, etc. We believe robots should learn this information through their own exploration and situated perception in order to uncover and exploit structure in their environments – structure that may not be apparent to human engineers, or that may emerge over time during a deployment. In this work, we combine semantic web-mining and situated robot perception to develop a system capable of assigning semantic categories to regions of space. This is accomplished by looking at web-mined relationships between room categories and objects identified by a Convolutional Neural Network trained on 1000 categories. Evaluated on real-world data, we show that our system exhibits several conceptual and technical advantages over similar systems, and uncovers semantic structure in the environment overlooked by ground-truth annotators","[{'title': None, 'identifiers': ['0302-9743', 'issn:0302-9743']}]",'Springer Science and Business Media LLC',Making sense of indoor spaces using semantic web mining and situated robot perception,10.1007/978-3-319-70407-4_39,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
84962589,2014-01-01T00:00:00,"International audienceCompact Bionic Handling Assistant (CBHA) is a continuum manipulator, with pneumatic-based actuation and compliant gripper. This bionic arm is attached to a mobile robot named Robotino. Inspired by the elephant's trunk, it can reproduce biological behaviors of trunks, tentacles, or snakes. Unlike rigid link robot manipulators, the development of high performance control algorithm of continuum robot manipulators remains a challenge, particularly due to their complex mechanical design, hyper-redundancy and presence of uncertainties. Numerous studies have been investigated for modeling of such complex systems. Such continuum robots, like the CBHA present a set of nonlinearities and uncertainties, making difficult to build an accurate analytical model, which can be used for control strategies development. Hence, learning approach becomes a suitable tool in such scenarios in order to capture un-modeled nonlinear behaviors of the continuous robots. In this paper, we present a qualitative modeling approach, based on neuronal model of the inverse kinematic of CBHA. A penalty term constraint is added to the inverse objective function into Distal Supervised Learning (DSL) scheme to select one particular inverse model from the redundancy manifold. The inverse kinematic neuronal model is validated by conducting a real-time implementation on a CBHA trunk",,HAL CCSD,Qualitative approach for inverse kinematic modeling of a Compact Bionic Handling Assistant trunk,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
220589792,2019-01-01T00:00:00,"Abstract The Robotic Process Automation (RPA) is a new wave of the future technologies. Robotic Process Automation is one of the most advanced technologies in the area of computers science, electronic and communications, mechanical engineering and information technology. It is a combination of both hardware and software, networking and automation for doing things very simple. In this light, the research manuscript investigated the secondary data - which is available in google, academic and research databases. The investigation went for totally 6 months, i.e., 1-1-2018 to 30-6-2018. A very few empirical articles, white papers, blogs and were found RPA and came across to compose this research manuscript. This study is exploratory in nature because of the contemporary phenomenon. The keywords used in searching of the database were Robotic Process Automation, RPA, Robots, Artificial Intelligence, Blue Prism. The study finally discovered that Robots and Robotic Process Automation technologies are becoming compulsory as a part to do business operations in the organizations across the globe. Robotic Process Automation can bring immediate value to the core business processes including employee payroll, employee status changes, new hire recruitment and on boarding, accounts receivable and payable, invoice processing, inventory management, report creation, software installations, data migration, and vendor on boarding etc. to name a few applications. Besides, the Robotic Process Automation has abundant applications including healthcare and pharmaceuticals, financial services, outsourcing, retail, telecom, energy and utilities, real estate and FMCG and many more sectors. To put in the right place of RPA in business operations, their many allied technologies are working at the background level, artificial intelligence, machine learning, deep learning, data analytics, HR analytics, virtual reality (second life), home automation, blockchain technologies, 4D printing etc. Moreover, it covers the content of different start-ups companies and existing companies - their RPA applications used across the world. This manuscript will be a good guideline for the academicians, researchers, students, and practitioners to get an overall idea",,,The Future Digital Work Force: Robotic Process Automation (RPA),,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
398616703,2017-11-08T00:00:00,"International audienceBackground:  In maize, silks are hundreds of filaments that simultaneously emerge from the ear for collecting pollen over a period of 1–7 days, which largely determines grain number especially under water deficit. Silk growth is a major trait for drought tolerance in maize, but its phenotyping is difficult at throughputs needed for genetic analyses.Results:  We have developed a reproducible pipeline that follows ear and silk growths every day for hundreds of plants, based on an ear detection algorithm that drives a robotized camera for obtaining detailed images of ears and silks. We first select, among 12 whole‑plant side views, those best suited for detecting ear position. Images are seg‑mented, the stem pixels are labelled and the ear position is identified based on changes in width along the stem. A mobile camera is then automatically positioned in real time at 30 cm from the ear, for a detailed picture in which silks are identified based on texture and colour. This allows analysis of the time course of ear and silk growths of thousands of plants. The pipeline was tested on a panel of 60 maize hybrids in the PHENOARCH phenotyping platform. Over 360 plants, ear position was correctly estimated in 86% of cases, before it could be visually assessed. Silk growth rate, estimated on all plants, decreased with time consistent with literature. The pipeline allowed clear identification of the effects of genotypes and water deficit on the rate and duration of silk growth.Conclusions:  The pipeline presented here, which combines computer vision, machine learning and robotics, provides a powerful tool for large‑scale genetic analyses of the control of reproductive growth to changes in environ‑mental conditions in a non‑invasive and automatized way. It is available as Open Source software in the OpenAlea platform",,'Springer Science and Business Media LLC',A robot-assisted imaging pipeline for tracking the growths of maize ear and silks in a high-throughput phenotyping platform,10.1186/s13007-017-0246-7,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
144867413,2017-10-16,"Fast and accurate object detection is a desire of many vision-guided robotics based systems. Agriculture is an area where detection accuracy is often sacrificed for speed, especially in the pursuit of real time results. Pastoral landscapes are especially challenging with varying levels of complexity, as competing objects are rarely textually smooth or visibly different from surroundings. This study presents a machine learning algorithm designed for object detection called the Multiple Expert Colour Extreme Learning Machine (MEC-ELM). The MEC-ELM is a multiple expert implementation of a Colour Feature Extreme Learning Machine (CF-ELM). The CF-ELM is itself a modification of the Extreme Learning Machine (ELM) with a partially connected hidden layer and a fully connected output layer, taking 3 inputs. The inputs can be utilised by multiple colour systems, including, RGB, Y'UV and HSV. Colour inputs were chosen, as colour is not sensitive to adjustments in scale, size and location and provides information not available in the standard grey-scale ELM. In the MEC-ELM algorithm, feature extraction and classification techniques were implemented simultaneously making a fully functional object detection algorithm. The algorithm was tested on weed detection and cattle detection from a video feed, delivering 0.89 (cattle) to 0.98 (weeds) accuracy in tuning and a precision of 0.61 to 0.95 in testing, with classification times between 0.5s to 1s per frame. The algorithm has been designed with complex and unpredictable terrain in mind, making it an ideal application for agricultural or pastoral landscapes",,,Fast object detection in pastoral landscapes using a multiple expert colour feature extreme learning machine,10.5281/zenodo.897216,https://core.ac.uk/download/pdf/144867413.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
153555004,2018,"Humans have an innate tendency to anthropomorphize surrounding entities and have always been fascinated by the creation of machines endowed with human-inspired capabilities and traits. In the last few decades, this has become a reality with enormous advances in hardware performance, computer graphics, robotics technology, and artificial intelligence. New interdisciplinary research fields have brought forth cognitive robotics aimed at building a new generation of control systems and providing robots with social, empathetic and affective capabilities. This paper presents the design, implementation, and test of a human-inspired cognitive architecture for social robots. State-of-the-art design approaches and methods are thoroughly analyzed and discussed, cases where the developed system has been successfully used are reported. The tests demonstrated the system's ability to endow a social humanoid robot with human social behaviors and with in-silico robotic emotions",,'MDPI AG',Designing the mind of a social robot,10.3390/app8020302,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
395008361,2018-01-01T00:00:00,"Human-robot collaboration could be advanced by facilitating the intuitive, gaze-based control of robots, and enabling robots to recognize human actions, infer human intent, and plan actions that support human goals. Traditionally, gaze tracking approaches to action recognition have relied upon computer vision-based analyses of two-dimensional egocentric camera videos. The objective of this study was to identify useful features that can be extracted from three-dimensional (3D) gaze behavior and used as inputs to machine learning algorithms for human action recognition. We investigated human gaze behavior and gaze-object interactions in 3D during the performance of a bimanual, instrumental activity of daily living: the preparation of a powdered drink. A marker-based motion capture system and binocular eye tracker were used to reconstruct 3D gaze vectors and their intersection with 3D point clouds of objects being manipulated. Statistical analyses of gaze fixation duration and saccade size suggested that some actions (pouring and stirring) may require more visual attention than other actions (reach, pick up, set down, and move). 3D gaze saliency maps, generated with high spatial resolution for six subtasks, appeared to encode action-relevant information. The ""gaze object sequence"" was used to capture information about the identity of objects in concert with the temporal sequence in which the objects were visually regarded. Dynamic time warping barycentric averaging was used to create a population-based set of characteristic gaze object sequences that accounted for intra- and inter-subject variability. The gaze object sequence was used to demonstrate the feasibility of a simple action recognition algorithm that utilized a dynamic time warping Euclidean distance metric. Averaged over the six subtasks, the action recognition algorithm yielded an accuracy of 96.4%, precision of 89.5%, and recall of 89.2%. This level of performance suggests that the gaze object sequence is a promising feature for action recognition whose impact could be enhanced through the use of sophisticated machine learning classifiers and algorithmic improvements for real-time implementation. Robots capable of robust, real-time recognition of human actions during manipulation tasks could be used to improve quality of life in the home and quality of work in industrial environments",,"eScholarship, University of California",Exploiting Three-Dimensional Gaze Tracking for Action Recognition During Bimanual Manipulation to Enhance Human-Robot Collaboration.,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
491008618,2021-11-01T00:00:00,"Human visual inspection of drains is laborious, time-consuming, and prone to accidents. This work presents an AI-enabled robot-assisted remote drain inspection and mapping framework using our in-house developed reconfigurable robot Raptor. The four-layer IoRT serves as a bridge between the users and the robots, through which seamless information sharing takes place. The Faster RCNN ResNet50, Faster RCNN ResNet101, and Faster RCNN Inception-ResNet-v2 deep learning frameworks were trained using a transfer learning scheme with six typical concrete defect classes and deployed in an IoRT framework remote defect detection task. The efficiency of the trained CNN algorithm and drain inspection robot Raptor was evaluated through various real-time drain inspection field trials using the SLAM technique. The experimental results indicate that robot’s maneuverability was stable, and its mapping and localization were also accurate in different drain types. Finally, for effective drain maintenance, the SLAM-based defect map was generated by fusing defect detection results in the lidar-SLAM map","[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",'MDPI AG',Drain Structural Defect Detection and Mapping Using AI-Enabled Reconfigurable Robot Raptor and IoRT Framework,10.3390/s21217287,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
132005917,2017-11-08T00:00:00,"International audienceBackground:  In maize, silks are hundreds of filaments that simultaneously emerge from the ear for collecting pollen over a period of 1–7 days, which largely determines grain number especially under water deficit. Silk growth is a major trait for drought tolerance in maize, but its phenotyping is difficult at throughputs needed for genetic analyses.Results:  We have developed a reproducible pipeline that follows ear and silk growths every day for hundreds of plants, based on an ear detection algorithm that drives a robotized camera for obtaining detailed images of ears and silks. We first select, among 12 whole‑plant side views, those best suited for detecting ear position. Images are seg‑mented, the stem pixels are labelled and the ear position is identified based on changes in width along the stem. A mobile camera is then automatically positioned in real time at 30 cm from the ear, for a detailed picture in which silks are identified based on texture and colour. This allows analysis of the time course of ear and silk growths of thousands of plants. The pipeline was tested on a panel of 60 maize hybrids in the PHENOARCH phenotyping platform. Over 360 plants, ear position was correctly estimated in 86% of cases, before it could be visually assessed. Silk growth rate, estimated on all plants, decreased with time consistent with literature. The pipeline allowed clear identification of the effects of genotypes and water deficit on the rate and duration of silk growth.Conclusions:  The pipeline presented here, which combines computer vision, machine learning and robotics, provides a powerful tool for large‑scale genetic analyses of the control of reproductive growth to changes in environ‑mental conditions in a non‑invasive and automatized way. It is available as Open Source software in the OpenAlea platform",,'Springer Science and Business Media LLC',A robot-assisted imaging pipeline for tracking the growths of maize ear and silks in a high-throughput phenotyping platform,10.1186/s13007-017-0246-7,https://core.ac.uk/download/132005917.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
275635798,2017-01-01T00:00:00,"[EN] Wellness state is affected by the habitability state of the domestic environment. Monitoring it can help to discover the causes of a low wellness levels aiding people in the improvement of their quality of life. In this paper, we propose a system to monitor the wellness state of people utilizing Likert¿s scale to determine the state of the user through an emoticon-based human¿computer interaction. The system is intended for domestic environments and measures the habitability conditions of the dwelling (such as temperature, humidity, luminosity and noise) employing sensors. An algorithm is designed in order to establish how to measure those conditions and to calculate the statistics that allows tracking their progress. The obtained information is presented to the user to compare his/her wellness state with the habitability conditions. Measures in a real domestic environment were performed in order to determine the configuration of our system. The energy efficiency of the algorithm provides an improvement between 99.36 and 99.62% in the energy consumption depending on the selected parameters.This work has been partially supported by the “Ministerio de Ciencia e Innovación”, through the “Plan Nacional de I+D+i 2008–2011” and by the “Ministerio de Educación, Cultura y Deporte”, through the grand “Ayudas para contratos predoctorales de Formación del Profesorado Universitario FPU14/02953”.García-García, L.; Parra-Boronat, L.; Romero Martínez, JO.; Lloret, J. (2017). System for monitoring the wellness state of people in domestic environments employing emoticon-based HCI. The Journal of Supercomputing. 1-25. https://doi.org/10.1007/s11227-017-2214-4S125Sendra S, Parra L, Lloret J, Tomás J (2017) Smart system for children’s chronic illness monitoring. Inf Fusion 40:76–86Lloret J, Parra L, Taha M, Tomás J (2017) An architecture and protocol for smart continuous eHealth monitoring using 5G. Comput Netw. https://doi.org/10.1016/j.comnet.2017.05.018 (in press)Hettler B (1976) The six dimensions of wellness. National Wellness Institute. http://c.ymcdn.com/sites/www.nationalwellness.org/resource/resmgr/docs/sixdimensionsfactsheet.pdf . Accessed 12 Dec 2017Dunn HL (1959) What high-level wellness means. Can J Public Health 50(11):447–457Herbes DJ, Mulder CH (2016) Housing and subjective well-being of older adults in Europe. J Hous Built Environ. https://doi.org/10.1007/s10901-016-9526-1OECD (2015) How’s life? measuring well-being. http://www.oecd-ilibrary.org/economics/how-s-life_23089679;jsessionid=55pjippucpjrq.x-oecd-live-02 . Accessed 12 Dec 2017Donaldson GC, Seemungal T, Jeffries DJ, Wedzicha JA (1999) Effect of temperature on lung function and symptoms in chronic obstructive pulmonary disease. Eur Respir J ERS 13(4):844–849Schwartz J, Samet J, Patz J (2004) Hospital admissions for heart disease: the effects of temperature and humidity. Epidemiology 15(6):755–761National Institute of Statistics of Spain (2005) Defunciones según causa de muerte en 2003. http://www.ine.es/prensa/np393.pdf . Accessed 12 Dec 2017Grimes A, Denne T, Howden-Dhapman P, Arnold R, Telfar-Barnard L, Preval N, Young C (2012) Cost benefit analysis of the warm up New Zealand: heat smart programme. University of Wellington, Wellington. http://sustainablecities.org.nz/wp-content/uploads/NZIF_CBA_report2.pdf . Accessed 12 Dec 2017Martínez-Pérez B, de la Torre-Díez I, Candelas-Plasencia S, López-Coronado M (2013) Developement and evaluation of tools for measuring the quality of experience (QoE) in mHealth applications. J Med Syst 37(5):9976Walther JB, D’addario KP (2001) The impacts of emotions on message interpretation in computer-mediated communication. Soc Sci Comput Rev 19(3):324–347Ghayvat H, Liu J, Mukhopadhay SC, Gui X (2015) Wellness sensor networks: a proposal and implementation for smart home for assisted living. IEEE Sens J 15(12):7341–7348Forkan ARM, Hu W (2016) A context-aware, predictive and protective approach for wellness monitoring of cardiac patients. In: Computing in Cardiology Conference, Vancouver, Canada, pp 369–372Booc CER, San Diego CMD, Tee ML, Caro JDL (2016) A mobile application for campus-based psychosocial wellness program. In: 7th International Conference on Information, Systems and Applications, Chalkidiki, Greece, pp 1–4Khan WA, Idris M, Ali T, Ali R, Hussain S, Hussain M, Amin MB, Khattak AM, Weiwei Y, Afzal M, Lee S, Kang BH, (2015) Correlating health and wellness analytics for personalized decision making. Boston, USA, pp 256–261Lim C, Kim ZM, Choi H (2017) Context-based healthy lifestyle recommendation for enhancing user’s wellness. In: IEEE International Conference on Big Data and Smart Computing, Jeju, South Korea, pp 418–421Tulu B, Strong D, Wang L, He Q, Agu E, Pedersen P, Djamasbi S (2016) Design implications of user experience studies: the case of a diabetes wellness app. In: 49th Hawaii International Conference on System Sciences, Koloa, USA, pp 3473–3482Kaur D, Siddaraju GS (2016) Experimental study of cardiac functionality for the wellness of individual by developing an android application. In: International Conference on Computation System and Information Technology for Sustainable Solutions, Bangalore, India, pp 174–183Arshad A, Khan S, Alam AHMZ, Tasnim R, Boby RI (2016) Health and wellness monitoring of elderly people using intelligent sensing technique. In: International Conference on Computer and Communications Engineering, Kuala Lumpur, Malaysia, pp 231–235Martin CJ, Platt SD, Hunt SM (1987) Housing conditions and ill health. Br Med J (Clin Res Ed) 294(6580):1125–1127Evans GW, Wells NM, Moch A (2003) Housing and mental health: a review of the evidence and a methodological and conceptual critique. J Soc Issues 59(3):475–500Shaw M (2004) Housing and public health. Annu Rev Public Health 25:397–418Thomson H, Thomas S (2015) Developing empirically supported theories of change for housing investment and health. Soc Sci Med 124:205–214Gustafson CJ, Feldman SR, Quandt SA, Isom S, Chem H, Spears CR, Arcury TA (2014) The association of skin conditions with housing conditions among North Carolina Latino migrant farm workers. Int J Dermatol 53(9):1091–1097Laquesta R, Garcia L, Garcia-Magarino I, Lloret J (2017) System to recommend the best place to life based on wellness state of the user employing the heart rate variability. IEEE Access 5:10594–10604Isiaka F, Mwitondi K, Ibrahim A (2015) Automatic prediction and detection of affect state based on invariant human computer interaction and human physiological response. In: Seventh International Conference on Computational Intelligence, Modelling and Simulation, Kuantan, Malaysia, pp 19–25Han S, Liu R, Zhu C, Soo YG, Yu H, Liu T, Duan F (2016) Development of a human computer interaction system based on multi-modal gaze tracking methods. In: IEEE International Conference on Robotics and Biomimetics, Qingdao, China, pp 1894–1899Chen B, Huang S, Tsai W (2017) Eliminating driving distractions: human–computer interaction with built-in applications. IEEE Veh Technol Mag 12(1):20–29Kamal S, Sayeed F, Rafeeq M (2016) Facial emotion recognition for human–computer interactions using hybrid feature extraction technique. In: International Conference on Data Mining and Advanced Computing, Ernakulam, India, pp 180–184Agrawal R, Gupta N (2016) Real time hand gesture recognition for human computer interaction. In: IEEE 6th International Conference on Advanced Computing, Bhimavaram, India, pp 470–475Sánchez CS, Mavrogianni A, González FJN (2017) On the minimal thermal habitability conditions in low income dwellings in Spain for a new definition of fuel poverty. Build Environ 114:344–356Ministry of Health, Social Services and Equality of Spain (2015) Plan Nacional de Actuaciones Preventivas de los Efectos del Exceso de Temperaturas Sobre la Salud. http://www.msssi.gob.es/ciudadanos/saludAmbLaboral/planAltasTemp/2015/docs/Plan_Nacional_de_Exceso_de_Temperaturas_2015.pdf . Accessed 12 Dec 2017Bornehag CG, Blomquist G, Gyntelberg F, Järvholm B, Malmberg P, Nordvall L, Nielsen A, Pershagen G, Sundell J (2001) Dampness in buildings and health. Indoor Air 11(2):72–86Garret MH, Rayment PR, Hooper MA, Abramson MJ, Hooper BM (1997) Indoor airborne fungal spores, house dampness and associations with environmental factors and respiratory health in children. Clin Exp Allergy 28:459–467Ariës MBC, Zonneveldt L (2004) Architectural aspects of healthy lighting. In: 21th Conference on Passive and Low Energy Architecture, The Netherlands, pp 1–5Boubekri M, Cheung IN, Reid KJ, Wang C, Zee PC (2014) Impact of windows and daylight exposure on overall health and sleep quality of office workers: a case-control pilot study. J Clin Sleep Med 10(6):603–611Beute F, de Kort YAW (2014) Salutogenic effects of the environments: review of health protective effects of nature and daylight. Appl Psychol Health Well Being 6(1):67–95Boyce P, Hunter C, Howlett O (2003) The benefits of daylight through windows. Rensselaer Polytechnic Institute, TroyHoogendijk WJG, Lips P, Dik MG, Deeg DJH, Beekman ATF, Penninx BWJH (2008) Depression is associated with decreased 25-hydroxyvitamin D and increased parathyroid hormone levels in older adults. Arch Gen Psychiatry 65(5):508–512Ising H, Kruppa B (2004) Health effects caused by noise: evidence in the literature from the past 25 years. Noise Health 6(22):5–13Sandra S, Lloret J, Garcia M, Toledo JF (2011) Power saving and energy optimization techniques for wireless sensor networks. J Commun 6(6):439–459Heinzelman WR, Chandrakasan A, Balakrishnan H (2000) Energy-efficient communication protocol for wireless microsensor networks. In: Proceedings of the IEEE 33rd Annual Hawaii International Conference on System Sciences, Maui, HawaiiKaps JP, Sunar B (2006) Energy comparison of AES and SHA-1 for ubiquitous computing. In: Proceedings of the EUC 2006 Workshops: NCUS, SecUbiq, USN, TRUST, ESO, and MSA, Seoul, KoreaParra L, Sendra S, Jiménez JM, Lloret J (2016) Multimedia sensors embedded in smartphones for ambient assisted living and e-health. Multimed Tools Appl 75(21):13271–1329",,'Springer Science and Business Media LLC',System for monitoring the wellness state of people in domestic environments employing emoticon-based HCI,10.1007/s11227-017-2214-4,https://riunet.upv.es/bitstream/10251/102349/2/System%20for%20monitoring%20the%20wellness%20state%20of%20people_v11.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
402899368,2021-02-05T03:05:15,"© 2013 IEEE. We present a novel implementation of a Rock-Paper-Scissors (RPS) game interaction with a social robot. The framework is tailored to be computationally lightweight, as well as entertaining and visually appealing through collaboration with designers and animators. The fundamental gesture recognition pipeline employs a Leap motion device and two separate machine learning architectures to evaluate kinematic hand data on-the-fly. The first architecture is used to recognize and segment human motion activity in order to initialize the RPS play, and the second architecture is used to classify hand gestures into rock, paper or scissors. The employed tabletop robot interacts in the RPS play through unique animated gestural movements and vocalizations designed by animators which communicate the robot's choices as well as cognitive reflection on winning, losing and draw states. Performance of both learning architectures is carefully evaluated with respect to accuracy, reliability and run time performance under different feature and classifier types. Moreover, we assess our system during an interactive RPS play between robot and human. Experimental results show that the proposed system is robust to user variations and play style in real environment conditions. As such, it offers a powerful application for the subsequent exploration of social human-machine interaction","[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",'Institute of Electrical and Electronics Engineers (IEEE)',Developing a Lightweight Rock-Paper-Scissors Framework for Human-Robot Collaborative Gaming,10.1109/ACCESS.2020.3033550,https://opus.lib.uts.edu.au/bitstream/10453/145863/2/DevelopingaLightweightRockPaperScissorsFrameworkforHumanRobotCollaborativeGaming.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
163029050,2019-07-05T00:00:00,"The deep supervised and reinforcement learning paradigms (among others) have the potential to endow interactive multimodal social robots with the ability of acquiring skills autonomously. But it is still not very clear yet how they can be best deployed in real world applications. As a step in this direction, we propose a deep learning-based approach for efficiently training a humanoid robot to play multimodal games---and use the game of `Noughts \& Crosses' with two variants as a case study. Its minimum requirements for learning to perceive and interact are based on a few hundred example images, a few  example multimodal dialogues and physical demonstrations of robot manipulation, and automatic simulations. In addition, we propose novel algorithms for robust visual game tracking and for competitive policy learning with high winning rates, which substantially outperform DQN-based baselines. While an automatic evaluation shows evidence that the proposed approach can be easily extended to new games with competitive robot behaviours, a human evaluation with 130 humans playing with the {\it Pepper} robot confirms that highly accurate visual perception is required for successful game play",,'Elsevier BV',A Data-Efficient Deep Learning Approach for Deployable Multimodal Social Robots,10.1016/j.neucom.2018.09.104,https://core.ac.uk/download/163029050.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
346528583,2020-01-01T00:00:00,"В данной статье исследована параллельная реализация метода машинного
обучения с циклическим фрактальным кодированием и использованием словаря
доменных блоков, адаптированный для применения на мобильных платформах, с
оптимизацией производительности и объема хранимых фрактальных образов
изображений. Основная идея метода заключается в применении метода фрактального
сжатия на основе систем итерированных функций для понижения размерности исходных
изображений, и использовании циклического фрактального кодирования для
представления класса изображений в целом. В параллельной реализации метода
используется технология CUDA на аппаратной мобильной платформе NVIDIA Jetson
Nano, в результате исследований метода получено, что время распознавания в среднем
составило 76 мс, что в 2.8 раза быстрее последовательной реализации. Достигнутые
показатели производительности являются приемлемыми для использования в системах
обработки изображений в реальном времени на мобильных платформах, в т.ч. для БПЛА
и наземных автономных роботов. In this article the parallel implementation of the method of machine learning with
cyclic fractal coding and the use of domain block dictionary, adapted for use on mobile
platforms, with optimization of performance and volume of stored fractal images is
investigated. The main idea of the method is to use the fractal compression method based on
systems of iterated functions to lower the dimension of the original images, and to use cyclic
fractal coding to represent the class of images as a whole. In the parallel implementation of the
method, CUDA technology is used on the NVIDIA Jetson Nano hardware mobile platform, as
a result of research of the method, it was found that the recognition time on average was 76 ms,
which is 2.8 times faster than sequential implementation. The achieved performance indicators
are acceptable for use in real-time image processing systems on mobile platforms, including
for UAVs and ground-based autonomous robots",,,High performance implementation of machine learning method based on fractal compression,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
237693585,2019,"In this paper we investigate the use of a social robot as an interface to a serious game aiming to train kids in how to recycle materials correctly. Serious games are mostly used to induce motivations and engagement in users and support knowledge transfer during playing. They are especially effective when the goal of the game concerns behavior change. In addition, social robots have been used effectively in educational settings to engage children in the learning process. Following this trend, we designed a serious game in which the social robot Pepper plays with a child to teach him to correctly recycle the materials. To endow the robot with the capability of detecting and classifying the waste material we developed an image recognition module based on a Convolutional Neural Network. Preliminary experimental results show that the implementation of a serious game about recycling into the Pepper robot improves its social behavior. The use of real objects as waste items during the game turns out to be a successful approach not only for perceived learning effectiveness but also for engagement of the children",,'Institute of Electrical and Electronics Engineers (IEEE)',Learning waste recycling by playing with a social robot,10.1109/SMC.2019.8914455,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
145170857,2017-05-28T00:00:00,"International audienceIntelligent Autonomous Robots deployed in human environments must have understanding of the wide range of possible semantic identities associated with the spaces they inhabit – kitchens, living rooms, bathrooms, offices, garages, etc. We believe robots should learn this information through their own exploration and situated perception in order to uncover and exploit structure in their environments – structure that may not be apparent to human engineers, or that may emerge over time during a deployment. In this work, we combine semantic web-mining and situated robot perception to develop a system capable of assigning semantic categories to regions of space. This is accomplished by looking at web-mined relationships between room categories and objects identified by a Convolutional Neural Network trained on 1000 categories. Evaluated on real-world data, we show that our system exhibits several conceptual and technical advantages over similar systems, and uncovers semantic structure in the environment overlooked by ground-truth annotators",,'Springer Science and Business Media LLC',Making Sense of Indoor Spaces Using Semantic Web Mining and Situated Robot Perception,10.1007/978-3-319-70407-4_39,https://core.ac.uk/download/145170857.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
149388185,2017-10-06T13:38:21Z,"<i>Abstract</i><div><br></div><div><br></div><div><div>Robocup is an international competition for multi agent research and related subject like: Artificial intelligence, Image processing, machine learning, robot path planning, control, and obstacle avoidance. In a soccer robot game, the environment is highly competitive and dynamic. In order to work in the dynamically changing environment, the decision-making system of a soccer robot system should have the features of flexibility and real-time adaptation. In this paper we will focus on the Middle Size Soccer Robot league (MSL) and new hierarchical hybrid fuzzy methods for decision making and action selection of a robot in Middle Size Soccer Robot league (MSL) are presented. First, the behaviors of an agent are introduced, implemented and classified in two layers, the Low_Level_Behaviors and the High_Level_Behaviors. In the second layer, a two phase mechanism for decision making is introduced. In phase one, some useful methods are implemented which check the robot’s situation for performing required behaviors. In the next phase, the team strategy, team formation, robot’s role and the robot’s positioning system are introduced. A fuzzy logical approach is employed to recognize the team strategy and further more to tell the player the best position to move. We believe that a Dynamic role engine is necessary for a successful team. Dynamic role engine and formation control during offensive or defensive play, help us to prevent collision avoidance among own players when attacking the ball and obstacle avoidance of the opponents. At last, we comprised our implemented algorithm in the Robocup 2007 and 2008 and results showed the efficiency of the introduced methodology. The results are satisfactory which has already been successfully implemented in ADRO RoboCup team. This project is still in progress and some new interesting methods are described in the current report.</div></div><div><br></div><div><br></div><div><b>Learn more at:</b></div><div><b>https://www.edusoft.ro/brain/index.php/brain/article/view/96</b><br></div",,,BRAIN Journal - Design of an Action Selection Mechanism for Cooperative Soccer Robots Based on Fuzzy Decision Making Algorithm,10.6084/m9.figshare.5478223.v1,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
479314553,2021-01-01T00:00:00,"One of the biggest challenges hindering a table tennis robot to play as well as a professional player is the ball&#x2019;s accurate motion control, which depends on various factors such as the incoming ball&#x2019;s position, linear, spin velocity and so forth. Unfortunately, some factors are almost impossible to be directly measured in real practice, such as the ball&#x2019;s spin velocity, which is difficult to be estimated from vision due to the little texture on the ball&#x2019;s surface. To perform accurate motion control in table tennis, this study proposes to learn a ball stroke strategy to guarantee desirable &#x201C;target landing location&#x201D; and the &#x201C;over-net height&#x201D; which are two key indicators to evaluate the quality of a stroke. To overcome the spin velocity challenge, a deep reinforcement learning (DRL) based stroke approach is developed with the spin velocity estimation capability, through which the system can predict the relative spin velocity of the ball and stroke it back accurately by iteratively learning from the robot-environment interactions. To pre-train the DRL-based strategy effectively, this paper develops a virtual table tennis playing environment, through which various simulated data can be collected. For the real table tennis robot implementation, experimental results demonstrate the superior performance of the proposed control strategy compared to that of the traditional aerodynamics-based method with an average landing error around 80mm and the landing-within-table probability higher than 70&#x0025;","[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",'Institute of Electrical and Electronics Engineers (IEEE)',Ball Motion Control in the Table Tennis Robot System Using Time-Series Deep Reinforcement Learning,10.1109/ACCESS.2021.3093340,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
479315740,2021-06-01T00:00:00,"Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) methods are a promising approach to solving complex tasks in the real world with physical robots. In this paper, we compare several reinforcement  learning (Q-Learning, SARSA) and deep reinforcement learning (Deep Q-Network, Deep Sarsa) methods for a task aimed at achieving a specific goal using robotics arm UR3. The main optimization problem of this experiment is to find the best solution for each RL/DRL scenario and minimize the Euclidean distance accuracy error and smooth the resulting path by the Bézier spline method. The simulation and real word applications are controlled by the Robot Operating System (ROS). The learning environment is implemented using the OpenAI Gym library which uses the RVIZ simulation tool and the Gazebo 3D modeling tool for dynamics and kinematics","[{'title': 'MENDEL', 'identifiers': ['1803-3814', '2571-3701', 'issn:2571-3701', 'issn:1803-3814']}]",'Brno University of Technology',Comparison of Multiple Reinforcement Learning and Deep Reinforcement Learning Methods for the Task Aimed at Achieving the Goal,10.13164/mendel.2021.1.001,https://core.ac.uk/download/479315740.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
233572274,2019-10-10T17:51:12,"open3siThis work has beenpartially funded by projects EC H2020 OPRECOMP (732631) and ALOHA (780788).Nano-size unmanned aerial vehicles (UAVs), with few centimeters of diameter and sub-10 Watts of total power budget, have so far been considered incapable of running sophisticated visual-based autonomous navigation software without external aid from base-stations, ad-hoc local positioning infrastructure, and powerful external computation servers. In this work, we present what is, to the best of our knowledge, the first 27g nano-UAV system able to run aboard an end-to-end, closed-loop visual pipeline for autonomous navigation based on a state-of-the-art deep-learning algorithm, built upon the open-source CrazyFlie 2.0 nano-quadrotor. Our visual navigation engine is enabled by the combination of an ultra-low power computing device (the GAP8 system-on-chip) with a novel methodology for the deployment of deep convolutional neural networks (CNNs). We enable onboard real-time execution of a state-of-the-art deep CNN at up to 18Hz. Field experiments demonstrate that the system's high responsiveness prevents collisions with unexpected dynamic obstacles up to a flight speed of 1.5m/s. In addition, we also demonstrate the capability of our visual navigation engine of fully autonomous indoor navigation on a 113m previously unseen path. To share our key findings with the embedded and robotics communities and foster further developments in autonomous nano-UAVs, we publicly release all our code, datasets, and trained networks.embargoed_20200220Palossi D.; Conti F.; Benini L.Palossi D.; Conti F.; Benini L",,'Institute of Electrical and Electronics Engineers (IEEE)',An Open Source and Open Hardware Deep Learning-Powered Visual Navigation Engine for Autonomous Nano-UAVs,10.1109/DCOSS.2019.00111,https://core.ac.uk/download/233572274.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
479017238,2021-09-01T00:00:00,"Socially assistive robots (SAR) hold significant potential to assist older adults and people with dementia in human engagement and clinical contexts by supporting mental health and independence at home. While SAR research has recently experienced prolific growth, long-term trust, clinical translation and patient benefit remain immature. Affective human-robot interactions are unresolved and the deployment of robots with conversational abilities is fundamental for robustness and humanrobot engagement. In this paper, we review the state of the art within the past two decades, design trends, and current applications of conversational affective SAR for ageing and dementia support. A horizon scanning of AI voice technology for healthcare, including ubiquitous smart speakers, is further introduced to address current gaps inhibiting home use. We discuss the role of user-centred approaches in the design of voice systems, including the capacity to handle communication breakdowns for effective use by target populations. We summarise the state of development in interactions using speech and natural language processing, which forms a baseline for longitudinal health monitoring and cognitive assessment. Drawing from this foundation, we identify open challenges and propose future directions to advance conversational affective social robots for: 1) user engagement, 2) deployment in real-world settings, and 3) clinical translation","[{'title': 'IEEE Transactions on Cognitive and Developmental Systems', 'identifiers': ['2379-8920', 'issn:2379-8920']}]",'Institute of Electrical and Electronics Engineers (IEEE)',Conversational affective social robots for ageing and dementia support,10.1109/tcds.2021.3115228,https://core.ac.uk/download/479017238.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
428920125,2020-03-23T00:00:00,"This paper is the first step of an attempt to equip social robots with emotion recognition capabilities comparable to those of humans. Most of the recent deep learning solutions for facial expression recognition under-perform when deployed in Human-Robot-Interaction scenarios, although they are capable of breaking records on the most varied benchmarks on facial expression recognition. The main reason for that we believe is that they are using techniques that are developed for recognition of static pictures, while in real-life scenarios, we infer emotions from intervals of expression. Utilising on the feature of CNN to form regions of interests that are similar to human gaze patterns, we use recordings from human-gaze patterns to train such a network to infer facial emotions from 3 seconds video footage of humans expressing 6 basic emotions",,'Association for Computing Machinery (ACM)',Improving emotional expression recognition of robots using regions of interest from human data,10.1145/3371382.3378359,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
82439102,2016-12-31,"AbstractThe effort to develop an electronic skin is highly motivated by many application domains namely robotics, biomedical instrumentations, and replacement prosthetic devices. Several e-skin systems have been proposed recently and have demonstrated the need of an embedded electronic system for tactile data processing either to mimic the human skin or to respond to the application demands. Processing tactile data requires efficient methods to extract meaningful information from raw sensors data.In this framework, our goal is the development of a dedicated embedded electronic system for electronic skin. The embedded electronic system has to acquire the tactile data, process and extract structured information. Machine Learning (ML) represents an effective method for data analysis in many domains: it has recently demonstrated its effectiveness in processing tactile sensors data.This paper presents an embedded electronic system based on dedicated hardware implementation for electronic skin systems. It provides a Tensorial kernel function implementation for machine learning based on Tensorial kernel approach. Results assess the time latency and the hardware complexity for real time functionality. The implementation results highlight the high amount of power consumption needed for the input touch modalities classification task. Conclusions and future perspectives are also presented",,The Author(s). Published by Elsevier Ltd.,Embedded Electronic System Based on Dedicated Hardware DSPs for Electronic Skin Implementation ,10.1016/j.protcy.2016.08.007,https://core.ac.uk/download/pdf/82439102.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
362229542,2020-11-22T00:00:00,"International audienceDeep reinforcement learning (DRL) techniques give robotics research an AI boost in many applications. In order to simultaneously accommodate the complex robotic behaviour simulation and DRL algorithm verification, a new simulation platform, namely the RobotDrlSim, is proposed. First, we design a standardized API interfacing mechanism for coordinating diverse environments on RobotDrlSim platform, where PyBullet simulator is equipped with an API to form a physical engine for robotics simulation. Second, benchmark DRL models are included in the baseline library for evaluation. Third, real-time human-robot interactions can be captured and imported to drive the RobotDrlSim tasks, which provide big data-stream for reinforcement learning. Experimentations show that cutting-edge DRL algorithms developed in python can be seamlessly deployed to the robots, and human interactions can be availed in training the robots. RobotDrlSim is valid for efficiently developing DRL algorithms for artificial intelligence models of robots, and it is especially suitable for the robot educational purposes",,HAL CCSD,RobotDrlSim: A real time robot simulation platform for reinforcement learning and human interactive demonstration learning,,https://core.ac.uk/download/362229542.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
326681899,2019-01-01T00:00:00,"This article proposes a novel visual framework for detecting tunnel crossings/junctions in underground mine areas towards the autonomous navigation of Micro Aeril Vehicles (MAVs). Usually mine environments have complex geometries, including multiple crossings with different tunnels that challenge the autonomous planning of aerial robots. Towards the envisioned scenario of autonomous or semi-autonomous deployment of MAVs with limited Line-of-Sight in subterranean environments, the proposed module acknowledges the existence of junctions by providing crucial information to the autonomy and planning layers of the aerial vehicle. The capability for a junction detection is necessary in the majority of mission scenarios, including unknown area exploration, known area inspection and robot homing missions. The proposed novel method has the ability to feed the image stream from the vehicles’ on-board forward facing camera in a Convolutional Neural Network (CNN) classification architecture, expressed in four categories: 1) left junction, 2) right junction, 3) left &amp; right junction, and 4) no junction in the local vicinity of the vehicle. The core contribution stems for the incorporation of AlexNet in a transfer learning scheme for detecting multiple branches in a subterranean environment. The validity of the proposed method has been validated through multiple data-sets collected from real underground environments, demonstrating the performance and merits of the proposed module.ISBN för värdpublikation: 978-1-7281-4878-6, 978-1-7281-4879-3</p",,'Institute of Electrical and Electronics Engineers (IEEE)',Visual Subterranean Junction Recognition for MAVs based on Convolutional Neural Networks,10.1109/IECON.2019.8926916,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
186636174,2019-01-30T00:00:00,"The ability to classify rooms in a home is one of many attributes that are desired for social robots. In this paper, we address the problem of indoor room classification via several convolutional neural network (CNN) architectures, i.e., VGG16, VGG19, &amp; Inception V3. The main objective is to recognize five indoor classes (bathroom, bedroom, dining room, kitchen, and living room) from a Places dataset. We considered 11600 images per class and subsequently fine-tuned the networks. The simulation studies suggest that cleaning the disparate data produced much better results in all the examined CNN architectures. We report that VGG16 &amp; VGG19 fine-tuned models with training on all layers produced the best validation accuracy, with 93.29% and 93.61% on clean data, respectively. We also propose and examine a combination model of CNN and a multi-binary classifier referred to as error correcting output code (ECOC) with the clean data. The highest validation accuracy of 15 binary classifiers reached up to 98.5%, where the average of all classifiers was 95.37%. CNN and CNN-ECOC, and an alternative form called CNN-ECOC Regression, were evaluated in real-time implementation on a NAO humanoid robot. The results show the superiority of the combination model of CNN and ECOC over the conventional CNN. The implications and the challenges of real-time experiments are also discussed in the paper",,,An Indoor Room Classification System for Social Robots via Integration of CNN and ECOC,,https://core.ac.uk/download/186636174.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
286600336,2018-01-01T00:00:00,": Deep learning is the most recent approach to achieve artificial intelligence. Especially neural networks are used for solving many human problems - from repetitive operations to intelligent recognizing in image, sound and text processing. They are used in medicine, car industry, game industry and robotics. Business companies also try to find the way of exploitation of the latest technology despite the fact that it is the long way to the point where machines will be capable to replace the human intelligence. Authors of this paper explore possibilities of semi-supervised learning application in accounting. One of the latest deep learning algorithm is successfully used to reconstruct the journal entry key columns. The model was trained and tested on a real-world dataset so it could become base for developing the wide pallet of accounting and audit applications - as anomaly detection module of Enterprise Resource Planning (ERP) software or as a standalone application",,,Journal entries with deep learning model,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
493120893,2019-10-08T00:00:00,"In order to create a competitive and resource-efficient transport system, the transport policy of the European Union provides for the achievement by 2030 of almost zero carbon dioxide content in the exhaust gases of vehicles used in large urban centers, and by 2050 the phasing out the use of cars, working on traditional fuels. The Republic of Belarus has a high scientific and sufficient industrial potential to participate in the process of promoting electric mobility, taking into account the use of robotics.
JSC ""Instrument-Making Plant Optron"" developed the working documentation and produced prototypes of typical representatives of the line of personal electric vehicles. However, the ongoing research focused on the creation of a preventive diagnostic system for the electric motorcycle, developed by Belarussian researches and its intelligent on-board system, focused primarily on real-time simulation processes, related specifically to the level of artificial intelligence, and on the implementation of executive level algorithms",,'Zeal Press',Robotization as One of the Prospects for Electromobility in Belarus,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
479344785,2021-06-21T00:00:00,"Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) methods&nbsp;are a promising approach to solving complex tasks in the real world with physical&nbsp;robots. In this paper, we compare several reinforcement&nbsp; learning (Q-Learning,&nbsp;SARSA) and deep reinforcement learning (Deep Q-Network, Deep Sarsa) methods&nbsp;for a task aimed at achieving a specific goal using robotics arm UR3. The main&nbsp;optimization problem of this experiment is to find the best solution for each RL/DRL scenario and minimize the Euclidean distance accuracy error and smooth&nbsp;the resulting path by the Bézier spline method. The simulation and real word&nbsp;applications are controlled by the Robot Operating System (ROS). The learning&nbsp;environment is implemented using the OpenAI Gym library which uses the RVIZ&nbsp;simulation tool and the Gazebo 3D modeling tool for dynamics and kinematics","[{'title': 'MENDEL', 'identifiers': ['1803-3814', '2571-3701', 'issn:2571-3701', 'issn:1803-3814']}]",'Brno University of Technology',Comparison of Multiple Reinforcement Learning and Deep Reinforcement Learning Methods for the Task Aimed at Achieving the Goal,10.13164/mendel.2021.1.001,https://core.ac.uk/download/479344785.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
132585674,2016-09-06T00:00:00,"Unmanned Aerial Vehicles are currently investigated as an important sub-domain of robotics, a fast growing and truly multidisciplinary research field. UAVs are increasingly deployed in real-world settings for missions in dangerous environments or in environments which are challenging to access. Combined with autonomous flying capabilities, many new possibilities, but also challenges, open up. To overcome the challenge of early identification of degradation, machine learning based on flight features is a promising direction. Existing approaches build classifiers that consider their features to be correlated. This prevents a fine-grained detection of degradation for the different hardware components. This work presents an approach where the data is considered uncorrelated and, using machine learning
techniques, allows the precise identification of UAV’s damages",,,UAV degradation identification for pilot notification using machine learning techniques,10.1109/etfa.2016.7733537,https://core.ac.uk/download/132585674.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
196618218,2015-01-01T00:00:00,"Complex real problems increasingly require intelligent systems that combine knowledge, techniques and methodologies from various sources. Intelligent systems based on artificial intelligence techniques that are associated with the behavior of people can perform the processes of learning, reasoning and solving all kinds of problems. Such systems, which automatically can perform tasks set by the user or other software, today thankfully called intelligent agents. Independent, intelligent agents on the Internet can be very successful to perform some search work on behalf of and for the needs of different users. For efficient collection, manipulation and management of data, such software can be very interesting from the standpoint of intelligent data analysis in many areas the police. Analysis of the data collected by an intelligent agent (a software robot-bot) can be successfully utilized, among many jobs in the police, and in the field of crime and in particular manifestation of cyber­crime, traffic safety, emergencies, etc. To make the collection and analysis of data from criminal activities on the Internet effective, it is necessary to examine the existing artificial intelligence techniques to be used for the conclusion of the intelligent agents. On the other hand, using of methods of artificial intelligence in finding data along with intelligent data analysis (data mining) should be used, which has found wide use in the area of business, economics, mechanics, medicine, genetics, transport etc.Kompleksni realni problemi sve češće zahtevaju inteligentne sisteme koji kombinuju znanje, tehnike i metodologije iz različitih izvora. Inteligentni sistemi bazirani na tehnikama veštačke inteligencije koje asociraju na ponašanje ljudi mogu da obavljaju procese učenja, zaključivanja i rešavanje raznovrsnih problema. Ovakvi sistemi, koji automatski mogu da izvrše zadatke zadate od strane korisnika ili drugih softvera, danas se sreću pod imenom inteligentni agenti. Samostalno, inteligentni agenti na Internetu mogu veoma uspešno da izvode neki pretraživački posao u ime i za potrebe raznih korisnika. Zbog efikasnog sakupljanja, manipulisanja i upravljanja podacima, ovakvi softveri mogu biti veoma interesantni sa stanovišta inteligentne analize podataka u mnogim oblastima policije. Analiza podataka sakupljenih od strane inteligentnog agenta (softverskog robota - bota) može se uspešno iskoristiti, između mnogih poslova u policiji, i na polju kriminala i naročito pojavnog oblika sajber kriminala, bezbednosti saobraćaja, vanrednih situacija itd. Kako bi sakupljanje i analiza podataka iz kriminalnih aktivnosti na Internetu bila efikasna, neophodno je sagledati postojeće tehnike veštačke inteligencije koje se koriste za zaključivanje u inteligentnim agentima. S druge strane, treba iskoristiti metode veštačke inteligencije u pronalaženju podataka pri inteligentnoj analizi podataka (data mining-u) koja je našla široku primenu u oblasti poslovanja preduzeća, ekonomije, mehanike, medicine, genetike, saobraćaja i sl","[{'title': 'Nauka bezbednost policija', 'identifiers': ['issn:0354-8872', '0354-8872']}]",'Centre for Evaluation in Education and Science (CEON/CEES)',Veštačka intelegencija u prikupljanju i analizi podataka u policiji,10.5937/NBP1503131K,https://core.ac.uk/download/196618218.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
511317478,2021-07-01T00:00:00,"Making the transition to long-term interaction with social-robot systems has been identified as one of the main challenges in human-robot interaction. This article identifies four design principles to address this challenge and applies them in a real-world implementation: cloud-based robot control, a modular design, one common knowledge base for all applications, and hybrid artificial intelligence for decision making and reasoning. The control architecture for this robot includes a common Knowledge-base (ontologies), Data-base, “Hybrid Artificial Brain” (dialogue manager, action selection and explainable AI), Activities Centre (Timeline, Quiz, Break and Sort, Memory, Tip of the Day, ), Embodied Conversational Agent (ECA, i.e., robot and avatar), and Dashboards (for authoring and monitoring the interaction). Further, the ECA is integrated with an expandable set of (mobile) health applications. The resulting system is a Personal Assistant for a healthy Lifestyle (PAL), which supports diabetic children with self-management and educates them on health-related issues (48 children, aged 6–14, recruited via hospitals in the Netherlands and in Italy). It is capable of autonomous interaction “in the wild” for prolonged periods of time without the need for a “Wizard-of-Oz” (up until 6 months online). PAL is an exemplary system that provides personalised, stable and diverse, long-term human-robot interaction","[{'title': 'ACM Transactions on Human-Robot Interaction', 'identifiers': ['2573-9522', 'issn:2573-9522']}]",'Association for Computing Machinery (ACM)',"A cloud-based robot system for long-term interaction: principles, implementation, lessons learned",10.1145/3481585,https://core.ac.uk/download/511317478.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
517414449,2022-03-29T16:34:31,"The voluntarist disruptive hopes in the full application of artificial intelligence (AI) in the field of the Judiciary does not enervate an inescapable –and inevitable– reality such as the reformulated and growing link between AI and the Administration of Justice. And more specifically, between the algorithm and thejudicial decision. However, throughout the text, it will be shown that, to this day, the direct replacement of human activity in the judicial decision is purely chimerical in the short and medium term. Another thing is the activities related to the judicial prediction of the private platforms that use AI, fully developed and implemented (with some criminal reluctance as in France). Along with this prescient commercial activity, only in two areas can a timid disruption in the judicial field be rigorously affirmed: the possibilities offered by simple automation in the process and the use of automatic data processing technologies in the audiovisualfield. Disadvantages related to potentially discriminatory bias; the pure search for the technological imitation of the guidelines of human behavior; the denaturation of the heuristic principle or technical incapacity, such as the still incomplete processing of human language, are too many servitudes to consider robotjudges even more than a fiction.La voluntaristas esperanzas disruptivas en la plena aplicación de la inteligencia artificial (IA) en el ámbito del Poder Judicial no enerva una realidad ineludible –e inevitable- como es la reformulada y creciente vinculación entre la IA y la Administración de Justicia. Y más concretamente, entre el algoritmo y la decisión judicial. Ahora bien, a lo largo del texto, se pondrá de manifiesto que, al día de hoy, la&nbsp;sustitución&nbsp;directa de la actividad humana en la decisión judicial es puramente quimérica a corto y medio plazo. Otra cosa son las actividades relacionadas con la predicción judicial al socaire de las plataformas privadas que emplean la IA, plenamente desarrolladas e implantadas (con algunas reticencias penales como en Francia). Junto a esta actividad comercial presciente, únicamente en dos ámbitos puede afirmarse con rigor una tímida disrupción en el ámbito judicial: las posibilidades que ofrece la automatización simple en el proceso y el empleo de tecnologías de procesamiento automático de datos en el ámbito audiovisual. Inconvenientes relacionados con el sesgo potencialmente discriminatorio; la pura búsqueda de la imitación tecnológica de las pautas de comportamiento humano; la desnaturalización del principio heurístico o incapacidad técnicas como el todavía incompleto procesamiento del lenguaje humano, son demasiadas servidumbres para no considerar aún más que una ficción a los jueces robots",,'Universidad Nacional Abierta y a Distancia',¿Sueñan los jueces con sentencias electrónicas?,10.22490/26655489.3854,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
323090695,2019-09-01T00:00:00,"[EN] An intelligent virtual environment simulates a physical world inhabited by autonomous intelligent entities. Multi-agent systems have been usually employed to design systems of this kind. One of the key aspects in the design of intelligent virtual environments is the use of appropriate ontologies which offer a richer and more expressive representation of knowledge. In this sense, this paper proposes an ontology comprising concepts for modelling intelligent virtual environments enhanced with concepts for describing agent-based organisational features. This new ontology, called MAMbO5, is used as an input of the JaCalIVE framework, which is a toolkit for the design and implementation of agent-based intelligent virtual environments.This work was supported by the project TIN2015-65515-C4-1-R of the Spanish government. This work has been supported in part by the Croatian Science Foundation under the project number 8537.Duric, BO.; Rincon, JA.; Carrascosa Casamayor, C.; Schatten, M.; Julian Inglada, VJ. (2019). MAMbO5: A new Ontology Approach for Modelling and Managing Intelligent Virtual Environments Based on Multi-Agent Systems. Journal of Ambient Intelligence and Humanized Computing. 10(9):3629-3641. https://doi.org/10.1007/s12652-018-1089-4S36293641109Ahmed Abbas H (2015) Organization of multi-agent systems: an overview. Int J Intell Inf Syst 4(3):46 (ISSN: 2328-7675)Amiribesheli M, Bouchachia H (2017) A tailored smart home for dementia care. J Ambient Intell Hum Comput 1:1–28 (ISSN: 1868-5137, 1868-5145)Amiribesheli M, Benmansour A, Bouchachia A (2015) A review of smart homes in healthcare. J Ambient Intell Hum Comput 6(4):495–517 (ISSN: 18685145) arXiv: TSMCC.2012.2189204 [10.1109]Barella A, Ricci A, Boissier O, Carrascosa C (2012) MAM5: multi-agent model for intelligent virtual environments. In: 10th European workshop on multi-agent systems (EUMAS 2012), pp 16–30Bordel B (2017) Self-configuration in humanized cyber-physical systems. J Ambient Intell Hum Comput 8(4):485–496 (ISSN: 1868-5137)Chaib A, Boussebough I, Chaoui A (2018) Adaptive service composition in an ambient environment with a multi-agent system. J Ambient Intell Hum Comput 9(2):367–380 (ISSN: 1868-5137)Chen X (2017) A multiagent-based model for pedestrian simulation in subway stations. Simul Modell Pract Theory 71:134–148 (ISSN: 1569-190X)Chen T, Chiu MC (2018) Smart technologies for assisting the life quality of persons in a mobile environment: a review. J Ambient Intell Hum Comput 9(2):319–327 (ISSN: 1868-5137)Corkill DD, Lander SE (1998) Diversity in agent organizations. Obj Mag 8(4):41–47De Wolf T (2004) Emergence and self-organisation: a statement of similarities and differences. In: Proceedings of of the 2nd international workshop on engineering self, pp 96–110Dignum V (2009) The role of organization in agent systems. English. In: Dignum V (ed) Handbook of research on multi-agent systems. Hershey, IGI Global, pp 1–16 (ISBN: 9781605662565)Fishwick PA, Miller JA (2004) Ontologies for modeling and simulation: issues and approaches. In: Simulation conference, 2004. Proceedings of the 2004 Winter, vol 1. IEEEFurfaro A (2016) Using virtual environments for the assessment of cybersecurity issues in IoT scenarios. Simul Modell Pract Theory 0:1–12Gabriele D, Ferretti S, Ghini V (2016) Multi-level simulation of Internet of Things on smart territories. Simul Modell Pract Theory 0:1–19Hadfi R, Ito T (2016) Holonic multiagent simulation of complex adaptive systems. In: Javier B(Ed) Highlights of practical applications of scalable multi-agent systems. The PAAMS collection: international workshops of PAAMS 2016, Sevilla, Spain, June 1-3, 2016. Proceedings. Springer, Cham, pp 137-147 (ISBN: 978-3-319-39387-2)Hofmann M, Palii J, Mihelcic G (2011) Epistemic and normative aspects of ontologies in modelling and simulation. J Simul 5(3):135–146Hui TKL, Sherratt RS (2017) Towards disappearing user interfaces for ubiquitous computing: human enhancement from sixth sense to super senses. J Ambient Intell Hum Comput 8(3):449–465 (ISSN: 1868-5137, 1868-5145)Kim S, Lee I (2018) IoT device security based on proxy re-encryption. J Ambient Intell Hum Comput 9(4):1267–1273 (ISSN: 1868-5137, 1868-5145)Ko E, Kim T, Kim H (2018) Management platform of threats information in IoT environment. J Ambient Intell Hum Comput 9(4):1167–1176 (ISSN: 1868-5137, 1868-5145)Liu Y, Xu C, Zhan Y, Liu Z, Guan J, Zhang H (2017) Incentive mechanism for computation offloading using edge computing: a stackelberg game approach. Comput Netw 129:399–409Liu Y, Bashar AAE, Wu B, Wu H (2018a) Delay-constrained profit maximization for data deposition in mobile opportunistic device-to-device networks. In: 2018 IEEE 19th international symposium on” a world of wireless, mobile and multimedia networks (WoWMoM), IEEE, pp 1–10Liu Y, et al (2018b) Delay-constrained utility maximization for video ads push in mobile opportunistic D2D networks. IEEE Internet Things JLuck M, Aylett R (2000) Applying artificial intelligence to virtual reality: intelligent virtual environments. Appl Artif Intell 14(1):3–32Marcon E (2017) A multi-agent system based on reactive decision rules for solving the caregiver routing problem in home health care. Simul Modell Pract Theory 74:134–151 (ISSN: 1569-190X)Mulero R (2018) Towards ambient assisted cities using linked data and data analysis. J Ambient Intell Hum Comput 9(5):1573–1591 (ISSN: 1868-5137, 1868-5145)Okreša Đ B, Schatten M (2016) Defining ontology combining concepts of massive multi-player online role playing games and organization of large-scale multi-agent systems. In: Opatija HR (ed) 39th international convention on information and communication technology, electronics and microelectronics (MIPRO). IEEE, pp 1330–1335 (ISBN: 978-953-233-086-1)Ricci A, Viroli M, Omicini A (2007) Give agents their artifacts: the A&A approach for engineering working environments in MAS. In: Proceedings of the 6th international joint conference on autonomous agents and multiagent systems, p 150Rincon JA, Carrascosa C, Garcia E (2014) Developing intelligent virtual environments using MAM5 meta-model. In: Lecture notes in computer science (including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics) 8473 LNAI, pp 379–382 (ISSN: 16113349)Rincon J (2016) Extending MAM5 meta-model and JaCalIV E framework to integrate smart devices from real environments. PLoS One 11:e0149665. https://doi.org/10.1371/journal.pone.0149665Rincon J, Garcia E, Julian V, Carrascosa C (2018) The jacalive framework for mas in IVE: a case study in evolving modular robotics. Neurocomputing 275:608–617Rodriguez S (2011) Holonic multi-agent systems. In: Di Marzo SG, Gleizes MP, Karageorgos A (eds) Natural computing series, natural computing series, vol 37. Springer, Heidelberg, pp 251–279 (ISBN: 978-3-642-17347-9)Samara A, et al. (2017) Affective state detection via facial expression analysis within a human–computer interaction context. J Ambient Intell Hum Comput (ISSN: 1868-5137, 1868-5145)Schatten M (2014) Organizational architectures for large-scale multi-agent systems’ development: an initial ontology. In: Sigeru O, et al (Ed) Advances in intelligent systems and computing, vol 290, pp 261–268Schatten M (2014) Towards a formal conceptualization of organizational design techniques for large scale multi agent systems. Procedia Technol 15:577–586 (ISSN: 22120173)Sharpanskykh A, Treur J (2012) An ambient agent architecture exploiting automated cognitive analysis. J Ambient Intell Hum Comput 3(3):219–237 (ISSN: 1868-5137, 1868-5145)Weyns D, Haesevoets R, Helleboogh A (2010) The MACODO organization model for context-driven dynamic agent organizations. ACM Trans Auton Adapt Syst 5(4):1–29 (ISSN: 15564665)Yang G, Kifer M, Zhao C (2003) Flora-2: a rule-based knowledge representation and inference infrastructure for the semantic web. In: Robert M, Zahir T, Douglas CS(Ed) On the move to meaningful internet systems 2003: CoopIS, DOA, and ODBASE: OTM confederated international conferences, CoopIS, DOA, and ODBASE 2003, Catania, Sicily, Italy, November 3-7, 2003. Proceedings. Springer, Berlin, pp 671-688 (ISBN: 978-3-540-39964-3)Zehe D, et al (2015) SEMSim cloud service: large-scale urban systems simulation in the cloud. In: 58, pp 157–17",,'Springer Science and Business Media LLC',MAMbO5: A new Ontology Approach for Modelling and Managing Intelligent Virtual Environments Based on Multi-Agent Systems,10.1007/s12652-018-1089-4,http://hdl.handle.net/10251/143142,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
275586244,2019-06-01T00:00:00,"In this work, we present a semantic situation awareness system for multirotor aerial robots, based on 2D LIDAR measurements, targeting the understanding of the environment and assuming to have a precise robot localization as an input of our algorithm. Our proposed situation awareness system calculates a semantic map of the objects of the environment as a list of circles represented by their radius, and the position and the velocity of their center in world coordinates. Our proposed algorithm includes three main parts. First, the LIDAR measurements are preprocessed and an object segmentation clusters the candidate objects present in the environment. Secondly, a Convolutional Neural Network (CNN) that has been designed and trained using an artificially generated dataset, computes the radius and the position of the center of individual circles in sensor coordinates. Finally, an indirect-EKF provides the estimate of the semantic map in world coordinates, including the velocity of the center of the circles in world coordinates.We have quantitative and qualitative evaluated the performance of our proposed situation awareness system by means of Software-In-The-Loop simulations using VRep with one and multiple static and moving cylindrical objects in the scene, obtaining results that support our proposed algorithm. In addition, we have demonstrated that our proposed algorithm is capable of handling real environments thanks to real laboratory experiments with non-cylindrical static (i.e. a barrel) and moving (i.e. a person) objects",,,Deep learning based semantic situation awareness system for multirotor aerial robots using LIDAR,,https://core.ac.uk/download/275586244.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
301262346,2018-01-01T00:00:00,"The final publication is available at link.springer.comSocial robots should be able to search and track people in order to help them. In this paper we present two different techniques for coordinated multi-robot teams for searching and tracking people. A probability map (belief) of a target person location is maintained, and to initialize and update it, two methods were implemented and tested: one based on a reinforcement learning algorithm and the other based on a particle filter. The person is tracked if visible, otherwise an exploration is done by making a balance, for each candidate location, between the belief, the distance, and whether close locations are explored by other robots of the team. The validation of the approach was accomplished throughout an extensive set of simulations using up to five agents and a large amount of dynamic obstacles; furthermore, over three hours of real-life experiments with two robots searching and tracking were recorded and analysed.Peer Reviewe","[{'title': 'Autonomous Robots', 'identifiers': ['0929-5593', 'issn:0929-5593']}]",'Springer Science and Business Media LLC',Searching and tracking people with cooperative mobile robots,10.1007/s10514-017-9681-6,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
401950305,2020-08-05T00:00:00,"The purpose of the research is to improve theoretical and practical aspects of digital strategic concept forming modern Ukrainian society in the context of technological development, opportunities and breakthrough changes. Objectives of the research: 1) to analyze information, artificial intelligence and machine learning as powerful levers to attract huge amounts of data that revolutionize our lives and production; 2) show the impact of automation on job creation in various areas of work and talent management in digital age; 3) to find out the place and role of information as a digital strategic basis of the new information revolution. Methodology - used systemic and institutional methods that allowed to bring everything into a reliable system for managing complex areas of interdependent activities, which allows you to discover and analyze the components and consistently connect them with each other. The institutional method is necessary for the formation of a holistic view as how the institutional subsystem affects information subsystem functioning. Methods - information scientific modes - general principles - creative relationship of correlation as subject with specific information reality, as well as fixing the results of the subject in gnosis to the world, nature, society, power institutions and their interaction with each other. The result of the research. As a result of the research, information, artificial intelligence and machine learning were analyzed as powerful levers for attracting a huge amount of data that revolutionizes our lives and production; an automation influence on job formation in various fields of work and talent management in digital age is studied; the place and role of information as a digital strategic basis of the new information revolution are clarified. Practical recommendations: 1) to introduce digital strategic concept of modern Ukrainian society in the context of the breakthrough changes development in the organization; 2) to form a new digital thinking taking into account the new digital culture in order to counteract the negative technologies of the new day; 3) to form computerization with the help of artificial intelligence systems and robots, which will increase productivity, remove barriers to innovation, create new opportunities for small businesses, startups, reduce barriers to market entry, implement software as a serviceАктуальність теми дослідження. Актуальність дослідження у тому, що сучасні умови діджиталізації вимагають формування моделі цифрової стратегії сучасного світу у контексті розвитку технологій, можливостей і проривних змін, що виступають як конкурентоспроможна сила, в основі якої експоненціональний розвиток, цифрова інформація, довгострокові впливи на економіку, бізнес, суспільство, людину, національне і глобальне. Мета дослідження – удосконалення теоретичних і практичних аспектів формування концепції цифрової стратегії сучасного українського суспільства у контексті розвитку технологій, можливостей і проривних змін. Завдання дослідження: 1) проаналізувати інформацію, штучний інтелект і машинне навчання як потужні важелі для залучення величезної кількості даних, що революціонізують наше життя і виробництво; 2) показати вплив автоматизації на формування робочих місць у різноманітних сферах праці та управління талантами у цифрову епоху; 3) з’ясувати місце і роль інформації як основи цифрової стратегії нової інформаційної революції. Методологія - використано системний та інституціональний методи, що дозволили все привести в надійну систему для керування складними сферами взаємозалежної діяльності, яка дозволяє розкривати й аналізувати складові компоненти і послідовно сполучати їх один з одним. Інституційний метод необхідний для формування цілісного уявлення про те, як інституціональна підсистема впливає на функціонування інформаційної підсистеми. Методи - модуси інформаціології – загальні принципи-постулати креативного відношення кореляції суб’єкта-креатора до конкретно-наявної інформаціологічної дійсності, а також фіксації результатів діяльності суб’єкта у гностизації до світу, природи, соціуму, інститутів влади та їх взаємодії один з одним. Результат дослідження. У результаті проведеного дослідження проаналізовано інформацію, штучний інтелект і машинне навчання як потужні важелі для залучення величезної кількості даних, що революціонізують наше життя і виробництво; досліджено вплив автоматизації на формування робочих місць у різноманітних сферах праці та управління талантами у цифрову епоху; з’ясовано місце і роль інформації як основи цифрової стратегії нової інформаційної революції. Практичні рекомендації: 1) упроваджувати концепцію цифрової стратегії сучасного українського суспільства у контексті розвитку проривних змін в організації; 2) формувати нове цифрове мислення з врахуванням нової цифрової культури, щоб протидіяти негативним технологіям нового дня; 3) формувати комп’ютеризацію за допомогою систем штучного інтелекту та роботів, що приведуть до підвищення продуктивності, усунення перешкод для інновацій, появі нових можливостей для малого бізнесу, стартапів, зниження бар’єрів для входження на ринку, реалізації програмного забезпечення як послуг",,'Journal of Babylon Center for Humanities Studies',"ФОРМУВАННЯ КОНЦЕПЦІЇ ЦИФРОВОЇ СТРАТЕГІЇ СУЧАСНОГО УКРАЇНСЬКОГО СУСПІЛЬСТВА У КОНТЕКСТІ РОЗВИТКУ ТЕХНОЛОГІЙ, МОЖЛИВОСТЕЙ І ПРОРИВНИХ ЗМІН",,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
390020517,2017-12-25T00:00:00,"The economic-legal aspects of the state and trends of the Internet-based technologies (IP) technology, the place of intellectual property in it are considered. It is shown that the Internet of Things creates conditions for the emergence of a synergetic effect from the combination of possibilities of artificial intelligence, cloud computing, set of sensors, mathematical algorithms for processing large data (Big Data), robotic devices of various purposes, data transmission systems (Internet), which allows to provide various services and perform various work with or without the participation of people. The role of the state in promoting the development of IP, the existing problems and ways of their solution are shown. Many governments in recent years are taking measures to analyze the state of affairs with the introduction of IP technologies, the localization of problems and threats that may or may occur in the future in order to formulate a common strategy for the development of industry for the production of IP technologies and their application in various sectors of the economy and public life. The patent landscape of the IP is analyzed, the most productive companies and inventors of IP are discovered, the dynamics of patenting in the IP environment, the value of patents, patent research problems are shown. The problems of intellectual property protection in the sphere of IP, in particular, copyright, inventions, trademarks, commercial secrets, information security are considered. The intellectual potential and untapped potential of Ukraine in the development of IP technologies are considered. It is concluded that in the widespread use of IP technologies, there is a significant potential for increasing the efficiency of any type of human activity. It concerns the real economy, industry and agriculture, health care, public administration, education, financial turnover, etc. The development of IP technologies is the most powerful stimulating factor in the innovative development of nanotechnologies, microelectronics, semiconductor technologies, microiminating of executive devices, telecommunications, radio technologies, software computing, robotics, and moreРассмотрены экономико-правовые аспекты состояния и тенденций развития технологий Интернета вещей (ИВ), места в нем интеллектуальной собственности. Показано, что ИВ создает условия для появления синергетического эффекта от сочетания возможностей искусственного интеллекта, облачных вычислений, множества сенсоров, математических алгоритмов обработки больших данных (Big Data), роботизированных устройств различного назначения, систем передачи данных (сети Интернет), что позволяет предоставлять разнообразные услуги и осуществлять различные работы с участием или без участия людей. Показана роль государства в содействии развитию ИВ, существующие проблемы и пути их решения. Правительства многих стран в последнее время принимают меры по анализу состояния дел с внедрением ИВ-технологий, локализации проблем и угроз, имеющих место или могущих возникнуть в будущем, с целью формирования общей стратегии развития промышленности производства технологий ИВ и их применение в различных секторах экономики и общественной жизни. Проанализированы патентный ландшафт ИВ, выявлены наиболее продуктивные компании и изобретатели ИВ, показана динамика патентования в среде ИВ, ценность патентов, проблемы патентного поиска. Рассмотрены проблемы охраны интеллектуальной собственности в сфере ИВ, в частности, авторских прав, изобретений, торговых марок, коммерческой тайны, информационной безопасности. Рассмотрены интеллектуальный потенциал и неиспользованные возможности Украины в развитии технологий ИВ. Делается вывод, что в широком применении технологий ИВ заложен значительный потенциал повышения эффективности любого вида человеческой деятельности. Это касается сферы реальной экономики, промышленности и сельского хозяйства, системы здравоохранения, государственного управления, образования, финансового оборота и т. п. Развитие технологий ИВ является мощным стимулирующим фактором инновационного развития нанотехнологий, микроэлектроники, полупроводниковых технологий, микроминиатюризации исполнительных устройств, телекоммуникаций, радиотехнологий, программных вычислительных средств, робототехники и многого другого.Розглянуто економіко-правові аспекти стану та тенденцій розвитку технологій Інтернет речей (ІР), місця в ньому інтелектуальної власності. Показано роль держави у сприянні розвитку ІР, існуючі проблеми та шляхи їх вирішення. Проаналізовано патентний ландшафт ІР, виявлені найбільш продуктивні компанії та винахідники ІР, показано динаміку патентування в середовищі ІР, цінність патентів, проблеми патентного пошуку. Визначено проблеми охорони інтелектуальної власності у сфері ІР, зокрема, авторських прав, винаходів, торгових марок, комерційної таємниці, інформаційної безпеки. Розглянуто інтелектуальний потенціал та невикористані можливості України у розвитку технологій ІР.Робиться висновок, що у широкому застосуванні технологій ІР закладено значний потенціал підвищення ефективності економіки",,Науково-дослідний інститут інтелектуальної власності НAПрН України,ІНТЕЛЕКТУАЛЬНА ВЛАСНІСТЬ В СИСТЕМІ ІНТЕРНЕТ РЕЧЕЙ: ЕКОНОМІКО-ПРАВОВИЙ АСПЕКТ,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
345057677,2019-01-01T00:00:00,"In this work, we present a semantic situation awareness system for multirotor aerial robots, based on 2D LIDAR measurements, targeting the understanding of the environment and assuming to have a precise robot localization as an input of our algorithm. Our proposed situation awareness system calculates a semantic map of the objects of the environment as a list of circles represented by their radius, and the position and the velocity of their center in world coordinates. Our proposed algorithm includes three main parts. First, the LIDAR measurements are preprocessed and an object segmentation clusters the candidate objects present in the environment. Secondly, a Convolutional Neural Network (CNN) that has been designed and trained using an artificially generated dataset, computes the radius and the position of the center of individual circles in sensor coordinates. Finally, an indirect-EKF provides the estimate of the semantic map in world coordinates, including the velocity of the center of the circles in world coordinates.We have quantitative and qualitative evaluated the performance of our proposed situation awareness system by means of Software-In-The-Loop simulations using VRep with one and multiple static and moving cylindrical objects in the scene, obtaining results that support our proposed algorithm. In addition, we have demonstrated that our proposed algorithm is capable of handling real environments thanks to real laboratory experiments with non-cylindrical static (i.e. a barrel) and moving (i.e. a person) objects",,'Institute of Electrical and Electronics Engineers (IEEE)',Deep learning based semantic situation awareness system for multirotor aerial robots using LIDAR,10.1109/ICUAS.2019.8797770,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
355099865,2020-01-01T00:00:00,"Abstract: Poor management practices of road transport assets posed a challenge to the sustainable development of the transport system in developing countries like Nigeria. Studies in the past focused mainly on the performance of road construction process. However, few studies have evaluated the effect of the fourth industrial revolution (4.0IR) on the road transport assets in developing countries such as Nigeria. The current study aimed at assessing the effect of the fourth industrial revolution towards improving the management practice of road transport assets. Survey instruments were administered to project and facility managers in the Nigerian road construction sector of the economy using a proportionate random sampling technique. Partial least square structural equation modelling was used for data analysis utilising the Warp 7.0 PLS-SEM software algorithm. The software calculates p-values with WarpPLS based on non-parametric algorithms, resampling or stable algorithms and thus does not require that the variables to be normally distributed. The study concluded that 4.0IR drivers have a moderate effect change on the management practice of road transport assets in Nigeria at the moment. The findings imply that management of road assets in Nigeria would moderately improve due to 4.0IR technologies resulting in transport, safety and general efficiency and effectiveness of road networks in Nigeria. The study identified 4.0IR drivers to include; robotics, mobility, virtual and augmented reality, internet of things and cloud computing, machine learning, artificial intelligence, blockchain, 3D printing drones that are built with an attached 3D printer, (the drone hangs a 3D printing nozzle that's fed plastic, concrete mix or other material from a tube connected to the top of the drone's printing path that precisely plotted by software, for a promised printing accuracy of 0.1mm),and digital engineering. This study emanated from the government reports and past studies in the area of road transport asset management practice which the study investigated the major causes of poor practices and assessed the effect of the fourth industrial revolution on the practice",,,Effect of the Fourth Industrial Revolution on Road Transport Asset Management Practice in Nigeria,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
481190027,2021-08-01T00:00:00,"People with severe disabilities require assistance to perform their routine activities; a Human–Machine Interface (HMI) will allow them to activate devices that respond according to their needs. In this work, an HMI based on electrooculography (EOG) is presented, the instrumentation is placed on portable glasses that have the task of acquiring both horizontal and vertical EOG signals. The registration of each eye movement is identified by a class and categorized using the one hot encoding technique to test precision and sensitivity of different machine learning classification algorithms capable of identifying new data from the eye registration; the algorithm allows to discriminate blinks in order not to disturb the acquisition of the eyeball position commands. The implementation of the classifier consists of the control of a three-wheeled omnidirectional robot to validate the response of the interface. This work proposes the classification of signals in real time and the customization of the interface, minimizing the user’s learning curve. Preliminary results showed that it is possible to generate trajectories to control an omnidirectional robot to implement in the future assistance system to control position through gaze orientation","[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",'MDPI AG',Human–Machine Interface: Multiclass Classification by Machine Learning on 1D EOG Signals for the Control of an Omnidirectional Robot,10.3390/s21175882,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
203489970,2015-12-31,"AbstractIn this paper a revised reinforcement learning method is presented for stability control problems with real-value inputs and outputs. The revised eXtended Classifier System for Real-input and Real-output (XCSRR) controller is designed, which is capable of working at fully real-value environment such as stability control of robots. XCSRR is a novel approach to enhance the performance of classifier systems for more practical problems than systems with merely binary behaviour. As a case study, we use XCSRR to control the stability of a biped robot, which is subjected to unknown external forces that would disturb the robot equilibrium. The external forces and the dynamics of the upper body of the biped robot are modelled in MATLAB software to train the XCSRR controller. Theoretical and experimental results of the learning behaviour and the performance of stability control on the robot demonstrate the strength and efficiency of the proposed new approach",,Published by Elsevier B.V.,An Improved eXtended Classifier System for the Real-time-input Real-time-output (XCSRR) Stability Control of a Biped Robot ,10.1016/j.procs.2015.09.198,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
163024812,2016-12-05T00:00:00,"International audienceOur demonstration presents an open-source hardware and software platform which allows non-roboticistsresearchers to conduct machine learning experiments to benchmark algorithms for autonomous explorationand active learning. In particular, in addition to showing the general properties of the platform such asits modularity and usability, we will demonstrate the online functioning of a particular algorithm whichallows efficient learning of multiple forward and inverse models and can leverage information from humanguidance. A first aspect of our demonstration is to illustrate the ease of use of the 3D printed low-costPoppy humanoid robotic platform, that allows non-roboticists to quickly set up and program roboticexperiments. A second aspect is to show how the Explauto library allows systematic comparison andevaluation of active learning and exploration algorithms in sensorimotor spaces, through a Python API toselect already implemented exploration algorithms. The third idea is to showcase Active Model Babbling,an efficient exploration algorithm dynamically choosing which task/goal space to explore and particulargoals to reach, and integrating social guidance from humans in real time to drive exploration towardsparticular objects or actions.[Forestier and Oudeyer, 2016] Forestier, S. and Oudeyer, P.-Y. (2016). Modular active curiosity-driven discovery oftool use. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, Korea.[Lapeyre et al., 2014] Lapeyre, M., Rouanet, P., Grizou, J., Nguyen, S., Depraetre, F., Le Falher, A., and Oudeyer,P.-Y. (2014). Poppy Project: Open-Source Fabrication of 3D Printed Humanoid Robot for Science, Educationand Art. In Digital Intelligence 2014, page 6, Nantes, France.[Moulin-Frier et al., 2014] Moulin-Frier, C., Rouanet, P., Oudeyer, P.-Y., and others (2014). Explauto: an open-source Python library to study autonomous exploration in developmental robotics. In ICDL-Epirob-InternationalConference on Development and Learning, Epirob",,HAL CCSD,"Autonomous exploration, active learning and human guidance with open-source Poppy humanoid robot platform and Explauto library",,https://core.ac.uk/download/163024812.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
290312192,2016,"This paper presents control framework based
on multi-agent reinforcement approach for building
intellectual steering software for multi-wheeled mobile
robots. The framework uses modified reinforcement
learning approach based on special multi-agent structure
with virtual leader. The framework application example
will be shown with real multi-wheeled mobile platform.
The experiments were performed in simulation
environment with accurate virtual model",,Minsk: Publishing Center of BSU,Multi-agent control framework for multi-wheeled mobile platforms,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
80311420,2017-01-01T00:00:00,"The use of identical robots in the RoboCup Standard Platform League (SPL) made software development the key aspect to achieve good results in competitions. In particular, the visual detection process is crucial for extracting information about the environment. In this paper, we present a novel approach for object detection and classification based on Convolutional Neural Networks (CNN). The approach is designed to be used by NAO robots and is made of two stages: image region segmentation, for reducing the search space, and Deep Learning, for validation. The proposed method can be easily extended to deal with different objects and adapted to be used in other RoboCup leagues. Quantitative experiments have been conducted on a data set of annotated images captured in real conditions from NAO robots in action. The used data set is made available for the community. © 2017, Springer International Publishing AG",,'Springer Science and Business Media LLC',A Deep Learning Approach for Object Recognition with NAO Soccer Robots,10.1007/978-3-319-68792-6_33,https://core.ac.uk/download/80311420.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
232211197,2019-02-01T00:00:00,"Genetic algorithms (GAs) are widely used in machine learning and optimization. This paper proposes a time-dependent genetic algorithm (TDGA) based on real-coded genetic algorithm (RCGA) to improve the convergence performance of functions over time such as a foot trajectory. TDGA has several distinguishing features when compared with traditional RCGA. First, individuals are arranged over time, and then the individuals are optimized in sequence. Second, search spaces of design variables are newly comprised of processes of reductions for search spaces. Third, the search space for crossover operations is expanded to avoid local minima traps that can occur in new search spaces up to the previous search space before performing any reduction of search space, and boundary mutation operation is performed to the new search spaces. Computer simulations are implemented to verify the convergence performance of the robot locomotion optimized by TDGA. Then, TDGA optimizes the desired feet trajectories of quadruped robots that climb up a slope and the impedance parameters of admittance control so that quadruped robots can trot stably over irregular terrains. Simulation results clearly represent that the convergence performance is improved by TDGA, which also shows that TDGA could be broadly used in robot locomotion research. (C) 2018 Elsevier B.V. All rights reserved","[{'title': 'Robotics and Autonomous Systems', 'identifiers': ['1872-793x', 'issn:1872-793X', 'issn:0921-8890', '0921-8890']}]",'Elsevier BV',Time-dependent genetic algorithm and its application to quadruped&rsquo;s locomotion,10.1016/j.robot.2018.10.015,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
201869339,2018-06-01T00:00:00,"Recent studies suggest that some children with autism prefer robots as tutors for improving their social interaction and communication abilities which are impaired due to their disorder. Indeed, research has focused on developing a very promising form of intervention named Robot-Assisted Therapy. This area of intervention poses many challenges, including the necessary flexibility and adaptability to real unconstrained therapeutic settings, which are different from the constrained lab settings where most of the technology is typically tested. Among the most common impairments of children with autism and intellectual disability is social attention, which includes difficulties in establishing the correct visual focus of attention. This article presents an investigation on the use of novel deep learning neural network architectures for automatically estimating if the child is focusing their visual attention on the robot during a therapy session, which is an indicator of their engagement. To study the application, the authors gathered data from a clinical experiment in an unconstrained setting, which provided low-resolution videos recorded by the robot camera during the child&ndash;robot interaction. Two deep learning approaches are implemented in several variants and compared with a standard algorithm for face detection to verify the feasibility of estimating the status of the child directly from the robot sensors without relying on bulky external settings, which can distress the child with autism. One of the proposed approaches demonstrated a very high accuracy and it can be used for off-line continuous assessment during the therapy or for autonomously adapting the intervention in future robots with better computational capabilities","[{'title': 'Robotics', 'identifiers': ['2218-6581', 'issn:2218-6581']}]",'MDPI AG',Deep Learning Systems for Estimating Visual Attention in Robot-Assisted Therapy of Children with Autism and Intellectual Disability,10.3390/robotics7020025,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
80378725,2016-09-06T00:00:00,"International audienceThis paper presents an artificial neural network-based control architecture allowing autonomous mobile robot indoor navigation by emulating the cognition process of a human brain when navigating in an unknown environment. The proposed architecture is based on a simultaneous top-down and bottom up approach, which combines the a priori knowledge of the environment gathered from a previously examined floor plan with the visual information acquired in real time. Thus, in order to take the right decision during navigation, the robot is able to process both set of information, compare them in real time and react accordingly. The architecture is composed of two modules: a) A deliberative module, corresponding to the processing chain in charge of extracting a sequence of navigation signs expected to be found in the environment, generating an optimal path plan to reach the goal,computing and memorizing the sequence of signs [1]. The path planning stage allowing the computation of the sign sequence is based on a neural implementation of the resistive grid. b) A reactive module, integrating the said sequence information in order to use it to control online navigation and learning sensory-motor associations. It follows a perception-action mechanism that constantly evolves because of the dynamic interaction between the robot and its environment. It is composed of three layers: one layer using a cognitive mechanism and the other two using a reflex mechanism. Experimental results obtained from the physical implementation of the architecture in an indoor environment show the feasibility of this approach",,'Springer Science and Business Media LLC',Artificial Neural Network-Based Control Architecture: a Simultaneous Top-down and Bottom-up Approach to Autonomous Robot Navigation,10.1007/978-3-319-44778-0,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
127503327,2017,"Background: In maize, silks are hundreds of filaments that simultaneously emerge from the ear for collecting pollen over a period of 1–7 days, which largely determines grain number especially under water deficit. Silk growth is a major trait for drought tolerance in maize, but its phenotyping is difficult at throughputs needed for genetic analyses.
Results: We have developed a reproducible pipeline that follows ear and silk growths every day for hundreds of plants, based on an ear detection algorithm that drives a robotized camera for obtaining detailed images of ears and silks. We first select, among 12 whole-plant side views, those best suited for detecting ear position. Images are segmented, the stem pixels are labelled and the ear position is identified based on changes in width along the stem. A mobile camera is then automatically positioned in real time at 30 cm from the ear, for a detailed picture in which silks are identified based on texture and colour. This allows analysis of the time course of ear and silk growths of thousands of plants. The pipeline was tested on a panel of 60 maize hybrids in the PHENOARCH phenotyping platform. Over 360 plants, ear position was correctly estimated in 86% of cases, before it could be visually assessed. Silk growth rate, estimated on all plants, decreased with time consistent with literature. The pipeline allowed clear identification of the effects of genotypes and water deficit on the rate and duration of silk growth.
Conclusions: The pipeline presented here, which combines computer vision, machine learning and robotics,
provides a powerful tool for large-scale genetic analyses of the control of reproductive growth to changes in environmental conditions in a non-invasive and automatized way. It is available as Open Source software in the OpenAlea platform",,,A robot-assisted imaging pipeline for tracking the growths of maize ear and silks in a high-throughput phenotyping platform,10.1186/s13007-017-0246-7,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
145159196,2017-01-01T00:00:00,"18 pages, 16 figures, 3 tables, 6 pseudocodes/algorithms, video at https://youtu.be/IqtyHFrb3BUInternational audienceThe high probability of hardware failures prevents many advanced robots (e.g., legged robots) from being confidently deployed in real-world situations (e.g., post-disaster rescue). Instead of attempting to diagnose the failures, robots could adapt by trial-and-error in order to be able to complete their tasks. In this situation, damage recovery can be seen as a Reinforcement Learning (RL) problem. However, the best RL algorithms for robotics require the robot and the environment to be reset to an initial state after each episode, that is, the robot is not learning autonomously. In addition, most of the RL methods for robotics do not scale well with complex robots (e.g., walking robots) and either cannot be used at all or take too long to converge to a solution (e.g., hours of learning). In this paper, we introduce a novel learning algorithm called "" Reset-free Trial-and-Error "" (RTE) that (1) breaks the complexity by pre-generating hundreds of possible behaviors with a dynamics simulator of the intact robot, and (2) allows complex robots to quickly recover from damage while completing their tasks and taking the environment into account. We evaluate our algorithm on a simulated wheeled robot, a simulated six-legged robot, and a real six-legged walking robot that are damaged in several ways (e.g., a missing leg, a shortened leg, faulty motor, etc.) and whose objective is to reach a sequence of targets in an arena. Our experiments show that the robots can recover most of their locomotion abilities in an environment with obstacles, and without any human intervention",,'Elsevier BV',Reset-free Trial-and-Error Learning for Robot Damage Recovery,10.1016/j.robot.2017.11.010,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
428438066,2019-01-01T00:00:00,"Search and Rescue (SAR) missions represent an important challenge in the robotics research field as they usually involve exceedingly variable-nature scenarios which require a high-level of autonomy and versatile decision-making capabilities. This challenge becomes even more relevant in the case of aerial robotic platforms owing to their limited payload and computational capabilities. In this paper, we present a fully-autonomous aerial robotic solution, for executing complex SAR missions in unstructured indoor environments. The proposed system is based on the combination of a complete hardware configuration and a flexible system architecture which allows the execution of high-level missions in a fully unsupervised manner (i.e. without human intervention). In order to obtain flexible and versatile behaviors from the proposed aerial robot, several learning-based capabilities have been integrated for target recognition and interaction. The target recognition capability includes a supervised learning classifier based on a computationally-efficient Convolutional Neural Network (CNN) model trained for target/background classification, while the capability to interact with the target for rescue operations introduces a novel Image-Based Visual Servoing (IBVS) algorithm which integrates a recent deep reinforcement learning method named Deep Deterministic Policy Gradients (DDPG). In order to train the aerial robot for performing IBVS tasks, a reinforcement learning framework has been developed, which integrates a deep reinforcement learning agent (e.g. DDPG) with a Gazebo-based simulator for aerial robotics. The proposed system has been validated in a wide range of simulation flights, using Gazebo and PX4 Software-In-The-Loop, and real flights in cluttered indoor environments, demonstrating the versatility of the proposed system in complex SAR missions",,,A Fully-Autonomous Aerial Robot for Search and Rescue Applications in Indoor Environments using Learning-Based Techniques,,https://core.ac.uk/download/428438066.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
477988955,2016-04-01T00:00:00,"[EN] Human-agent societies refer to applications where virtual agents and humans coexist and interact transparently into a fully integrated environment. One of the most important aspects in this kind of applications is including emotional states of the agents (humans or not) in the decision-making process. In this sense, this paper presents the applicability of the JaCalIVE framework for developing this kind of societies. Specifically, the paper presents an ambient intelligence application where humans are immersed into a system that extracts and analyzes the emotional state of a human group. A social emotional model is employed in order to try to maximize the welfare of those humans by playing the most appropriate music in every moment.Project supported by the Ministerio de Economia y Competitividad of the Spanish Government and the European Regional Development Fund of the European Union (No. TIN2015-65515-C4-1-R)Rincon, JA.; Bajo, J.; Fernandez, A.; Julian Inglada, VJ.; Carrascosa Casamayor, C. (2016). Using emotions for the development of human-agent societies. Frontiers of Information Technology & Electronic Engineering. 17(4):325-337. https://doi.org/10.1631/FITEE.1500343S325337174Ali, F., Amin, M., 2014. The influence of physical environment on emotions, customer satisfaction and behavioural intentions in Chinese resort hotel industry. J. Glob. Bus. Adv., 7(3):249–266. http://dx.doi.org/10.1504/JGBA.2014.064109Bales, R.F., 2001. Social Interaction Systems: Theory and Measurement. Transaction Publishers, USA.Barella, A., Ricci, A., Boissier, O., et al., 2012. MAM5: multi-agent model for intelligent virtual environments. Proc. 10th European Workshop on Multi-Agent Systems, p.16–30.Billhardt, H., Julian, V., Corchado, J.M., et al., 2014. An architecture proposal for human-agent societies. Proc. Int. Workshop on Highlights of Practical Applications of Heterogeneous Multi-Agent Systems, p.344–357. http://dx.doi.org/10.1007/978-3-319-07767-3_31Ducatel, K., Bogdanowicz, M., Scapolo, F., et al., 2001. Scenarios for Ambient Intelligence in 2010. Office for Official Publications of the European Communities.Fernandez, A., Ossowski, S., 2011. A multiagent approach to the dynamic enactment of semantic transportation services. IEEE Trans. Intell. Transp. Syst., 12(2):333–342. http://dx.doi.org/10.1109/TITS.2011.2106776Hale, K.S., Stanney, K.M., 2002. Handbook of Virtual Environments: Design, Implementation, and Applications. CRC Press, USA.Han, D.M., Lim, J.H., 2010. Smart home energy management system using IEEE 802.15.4 and ZigBee. IEEE Trans. Consum. Electron., 56(3):1403–1410. http://dx.doi.org/10.1109/TCE.2010.5606276Hendler, J., 2007. Where are all the intelligent agents? IEEE Intell. Syst., 22:2–3.Holzapfel, A., Stylianou, Y., 2007. A statistical approach to musical genre classification using non-negative matrix factorization. Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing, p.693–696. http://dx.doi.org/10.1109/ICASSP.2007.366330Huhns, M.N., Singh, M.P., Burstein, M., et al., 2005. Research directions for service-oriented multiagent systems. IEEE Internet Comput., 9(6):65.Intille, S.S., 2002. Designing a home of the future. IEEE Perva. Comput., 1(2):76–82.Lawrence, S., Giles, C.L., Tsoi, A.C., et al., 1997. Face recognition: a convolutional neural-network approach. IEEE Trans. Neur. Netw., 8(1):98–113. http://dx.doi.org/10.1109/72.554195Li, T., Ogihara, M., Li, Q., 2003. A comparative study on content-based music genre classification. Proc. 26th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, p.282–289. http://dx.doi.org/10.1145/860435.860487Mangina, E., Carbo, J., Molina, J.M., 2009. Agent-Based Ubiquitous Computing. Atlantis Press, France. http://dx.doi.org/10.2991/978-94-91216-31-2Mehrabian, A., 1980. Basic Dimensions for a General Psychological Theory: Implications for Personality, Social, Environmental, and Developmental Studies. Oelgeschlager, Gunn & Hain, USA.Mehrabian, A., 1997. Analysis of affiliation-related traits in terms of the PAD temperament model. J. Psychol., 131(1):101–117. http://dx.doi.org/10.1080/00223989709603508Nanty, A., Gelin, R., 2013. Fuzzy controlled PAD emotional state of a NAO robot. Proc. Conf. on Technologies and Applications of Artificial Intelligence, p.90–96. http://dx.doi.org/10.1109/TAAI.2013.30Ortony, A., 1990. The Cognitive Structure of Emotions. Cambridge University Press, USA.Ossowski, S., 2013. Agreement Technologies: 8 (Law, Governance and Technology Series). Springer, the Netherlands. http://dx.doi.org/10.1007/978-94-007-5583-3Osuna, E., Freund, R., Girosit, F., 1997. Training support vector machines: an application to face detection. Proc. IEEE Computer Society Conf. on Computer Vision and Pattern Recognition, p.130–136. http://dx.doi.org/10.1109/CVPR.1997.609310Rincon, J.A., Garcia, E., Julian, V., et al., 2014. Developing adaptive agents situated in intelligent virtual environments. Proc. 9th Int. Conf. on Hybrid Artificial Intelligence Systems, p.98–109. http://dx.doi.org/10.1007/978-3-319-07617-1_9Rincon, J.A., Julian, V., Carrascosa, C., 2015a. Applying a social emotional model in human-agent societies. Proc. Int. Workshops of Practical Applications of Agents, Multi-Agent Systems, p.377–388. http://dx.doi.org/10.1007/978-3-319-19033-4_33Rincon, J.A., Julian, V., Carrascosa, C., 2015b. Social emotional model. Proc. 13th Int. Conf. on Practical Applications of Agents, Multi-Agent Systems, p.199–210. http://dx.doi.org/10.1007/978-3-319-18944-4_17Satyanarayanan, M., 2001. Pervasive computing: vision and challenges. IEEE Pers. Commun., 8(4):10–17. http://dx.doi.org/10.1109/98.943998Satyanarayanan, M., 2002. A catalyst for mobile and ubiquitous computing. IEEE Perva. Comput., 1(1):2–5. http://dx.doi.org/10.1109/MPRV.2002.993138Talupur, M., Nath, S., Yan, H., 2001. Classification of Music Genre. Project Report for 15781.Viola, P., Jones, M.J., 2004. Robust real-time face detection. Int. J. Comput. Vis., 57(2):137–154. http://dx.doi.org/10.1023/B:VISI.0000013087.49260.fbWeiser, M., 1991. The computer for the 21st century. Sci. Am., 265(3):94–104. http://dx.doi.org/10.1038/scientificamerican0991-94Zambonelli, F., Jennings, N.R., Wooldridge, M., 2003. Developing multiagent systems: the Gaia methodology. ACM Trans. Softw. Eng. Meth., 12(3):317–370. http://dx.doi.org/10.1145/958961.95896",,'Zhejiang University Press',Using emotions for the development of human-agent societies,10.1631/FITEE.1500343,http://hdl.handle.net/10251/168936,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
78292650,2016,"This paper presents control framework based
on multi-agent reinforcement approach for building
intellectual steering software for multi-wheeled mobile
robots. The framework uses modified reinforcement
learning approach based on special multi-agent structure
with virtual leader. The framework application example
will be shown with real multi-wheeled mobile platform.
The experiments were performed in simulation
environment with accurate virtual model",,Minsk: Publishing Center of BSU,Multi-agent control framework for multi-wheeled mobile platforms,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
326826093,2020-05-25T00:00:00,"In this study, a semantic segmentation network is presented to develop an indoor navigation system for a mobile robot. Semantic segmentation can be applied by adopting different techniques, such as a convolutional neural network (CNN). However, in the present work, a residual neural network is implemented by engaging in ResNet-18 transfer learning to distinguish between the floor, which is the navigation free space, and the walls, which are the obstacles. After the learning process, the semantic segmentation floor mask is used to implement indoor navigation and motion calculations for the autonomous mobile robot. This motion calculations are based on how much the estimated path differs from the center vertical line. The highest point is used to move the motors toward that direction. In this way, the robot can move in a real scenario by avoiding different obstacles. Finally, the results are collected by analyzing the motor duty cycle and the neural network execution time to review the robot’s performance. Moreover, a different net comparison is made to determine other architectures’ reaction times and accuracy values.This research was financed by the plant of Mercedes-Benz Vitoria through the PIF program to develop an intelligent production. Moreover, The Regional Development Agency of the Basque Country (SPRI) is gratefully acknowledged for their economic support through the research project “Motor de Accionamiento para Robot Guiado Automáticamente”, KK-2019/00099, Programa ELKARTEK","[{'title': 'Mathematics', 'identifiers': ['issn:2227-7390', '2227-7390']}]",'MDPI AG',Semantic Segmentation to Develop an Indoor Navigation System for an Autonomous Mobile Robot,10.3390/math8050855,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
323329229,2020-03-11T00:00:00,"Assistive robots collaborating with people demand strong Human-Robot interaction capabilities. In this way, recognizing the person the robot has to interact with is paramount to provide a personalized service and reach a satisfactory end-user experience. 
To this end, face recognition: a non-intrusive, automatic mechanism of identification using biometric identifiers from an user's face, has gained relevance in the recent years, as the advances in machine learning and the creation of huge public datasets have considerably improved the state-of-the-art performance.
 In this work we study different open-source implementations of the typical components of state-of-the-art face recognition pipelines, including face detection, feature extraction and classification, and propose a recognition system integrating the most suitable methods for their utilization in assistant robots. 
 Concretely, for face detection we have considered MTCNN, OpenCV's DNN, and OpenPose, while for feature extraction we have analyzed InsightFace and Facenet.
 We have made public an implementation of the proposed recognition framework, ready to be used by any robot running the Robot Operating System (ROS).
 The methods in the spotlight have been compared in terms of accuracy and performance in common benchmark datasets, namely FDDB and LFW, to aid the choice of the final system implementation, which has been tested in a real robotic platform.This work is supported by the Universidad de Málaga. Campus de Excelencia Internacional Andalucía Tech, the research projects WISER ([DPI2017-84827-R]),funded by the Spanish Government, and financed by European RegionalDevelopment’s funds (FEDER), and MoveCare ([ICT-26-2016b-GA-732158]), funded by the European H2020 program, and by a postdoc contract from the I-PPIT-UMA program financed by the University of Málaga",,,A face recognition system for assistive robots,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
147682594,2017,"In this work, we propose a 3D scene reconstruction algorithm based on a fully convolutional 3D denoising autoencoder neural network. The network is capable of reconstructing a full scene from a single depth image by creating a 3D representation of it and automatically filling holes and inserting hidden elements. We exploit the fact that our neural network is capable of generalizing object shapes by inferring similarities in geometry. Our fully convolutional architecture enables the network to be unconstrained by a fixed 3D shape, and so it is capable of successfully reconstructing arbitrary scene sizes. Our algorithm was evaluated on a real word dataset of tabletop scenes acquired using a Kinect and processed using KinectFusion software in order to obtain ground truth for network training and evaluation. Extensive measurements show that our deep neural network architecture outperforms the previous state of the art both in terms of precision and recall for the scene reconstruction task. The network has been broadly profiled in terms of memory footprint, number of floating point operations, inference time and power consumption in CPU, GPU and embedded devices. Its small memory footprint and its low computation requirements enable low power, memory constrained, real time always-on embedded applications such as autonomous vehicles, warehouse robots, interactive gaming controllers and drones",,'Institute of Electrical and Electronics Engineers (IEEE)',Fully convolutional denoising autoencoder for 3D scene reconstruction from a single depth image,10.1109/ICSAI.2017.8248355,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
491332743,2021-01-01T00:00:00,"Ultrasound imaging can be employed to improve automation levels in minimally invasive surgery and eventually enable full automation by providing real-time situational awareness to medical robots via intra-operative, real-time, volumetric mapping of the surgical site. This talk describes the development of a prototype automated image guidance system, combining high-refresh-rate, volumetric ultrasound (US) imaging (or 4D US) and advanced deep learning strategies for an autonomous robotic platform, currently being investigated for knee surgery. The feasibility of using 4D US for guidance in knee minimally invasive surgery was first proved through cadaver and volunteer studies. The workflow to enable automatic interpretation of 4D US imaging for robotic guidance was then developed and implemented. The essential steps include automatic image quality assessment, tissue segmentation and tracking (in combination with the assessment of uncertainties) and surgical tool tracking. The results obtained show US imaging's potential for quantitative autonomous tasks and for the creation of autonomous, intelligent robotic surgical systems that will hopefully soon make surgery more sustainable and improve people's quality of life.Learning Objectives:1. US imaging combined with deep learning strategies can provide a solution for real-time volumetric guidance for autonomous systems;2. The algorithms developed and implemented can be extended to US images of other body districts;3. The workflow elucidated provides solutions to the issues currently limiting the application of US imaging to quantitative autonomous tasks;<br/",,,Platform for Ultrasound-Guided Autonomous Minimally Invasive Surgery: Application to the Knee,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
323317411,2019-12-12T00:00:00,"[EN] Over the last few years, several researchers have been developing protocols and applications in order to autonomously land unmanned aerial vehicles (UAVs). However, most of the proposed protocols rely on expensive equipment or do not satisfy the high precision needs of some UAV applications such as package retrieval and delivery or the compact landing of UAV swarms. Therefore, in this work, a solution for high precision landing based on the use of ArUco markers is presented. In the proposed solution, a UAV equipped with a low-cost camera is able to detect ArUco markers sized 56×56 cm from an altitude of up to 30 m. Once the marker is detected, the UAV changes its flight behavior in order to land on the exact position where the marker is located. The proposal was evaluated and validated using both the ArduSim simulation platform and real UAV flights. The results show an average offset of only 11 cm from the target position, which vastly improves the landing accuracy compared to the traditional GPS-based landing, which typically deviates from the intended target by 1 to 3 m.This work was funded by the  Ministerio de Ciencia, Innovación y Universidades, Programa Estatal de Investigación, Desarrollo e Innovación Orientada a los Retos de la Sociedad, Proyectos I+D+I 2018 , Spain, under Grant RTI2018-096384-B-I00.Wubben, J.; Fabra Collado, FJ.; Tavares De Araujo Cesariny Calafate, CM.; Krzeszowski, T.; Márquez Barja, JM.; Cano, J.; Manzoni, P. (2019). Accurate Landing of Unmanned Aerial Vehicles Using Ground Pattern Recognition. Electronics. 8(12):1-16. https://doi.org/10.3390/electronics8121532S116812Pan, X., Ma, D., Jin, L., & Jiang, Z. (2008). Vision-Based Approach Angle and Height Estimation for UAV Landing. 2008 Congress on Image and Signal Processing. doi:10.1109/cisp.2008.78Tang, D., Li, F., Shen, N., & Guo, S. (2011). UAV attitude and position estimation for vision-based landing. Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology. doi:10.1109/emeit.2011.6023131Gautam, A., Sujit, P. B., & Saripalli, S. (2014). A survey of autonomous landing techniques for UAVs. 2014 International Conference on Unmanned Aircraft Systems (ICUAS). doi:10.1109/icuas.2014.6842377Holybro Pixhawk 4 · PX4 v1.9.0 User Guidehttps://docs.px4.io/v1.9.0/en/flight_controller/pixhawk4.htmlGarrido-Jurado, S., Muñoz-Salinas, R., Madrid-Cuevas, F. J., & Medina-Carnicer, R. (2016). Generation of fiducial marker dictionaries using Mixed Integer Linear Programming. Pattern Recognition, 51, 481-491. doi:10.1016/j.patcog.2015.09.023Romero-Ramirez, F. J., Muñoz-Salinas, R., & Medina-Carnicer, R. (2018). Speeded up detection of squared fiducial markers. Image and Vision Computing, 76, 38-47. doi:10.1016/j.imavis.2018.05.004ArUco: Augmented reality library based on OpenCVhttps://sourceforge.net/projects/aruco/Jin, S., Zhang, J., Shen, L., & Li, T. (2016). On-board vision autonomous landing techniques for quadrotor: A survey. 2016 35th Chinese Control Conference (CCC). doi:10.1109/chicc.2016.7554984Chen, X., Phang, S. K., Shan, M., & Chen, B. M. (2016). System integration of a vision-guided UAV for autonomous landing on moving platform. 2016 12th IEEE International Conference on Control and Automation (ICCA). doi:10.1109/icca.2016.7505370Nowak, E., Gupta, K., & Najjaran, H. (2017). Development of a Plug-and-Play Infrared Landing System for Multirotor Unmanned Aerial Vehicles. 2017 14th Conference on Computer and Robot Vision (CRV). doi:10.1109/crv.2017.23Shaker, M., Smith, M. N. R., Yue, S., & Duckett, T. (2010). Vision-Based Landing of a Simulated Unmanned Aerial Vehicle with Fast Reinforcement Learning. 2010 International Conference on Emerging Security Technologies. doi:10.1109/est.2010.14Araar, O., Aouf, N., & Vitanov, I. (2016). Vision Based Autonomous Landing of Multirotor UAV on Moving Platform. Journal of Intelligent & Robotic Systems, 85(2), 369-384. doi:10.1007/s10846-016-0399-zPatruno, C., Nitti, M., Petitti, A., Stella, E., & D’Orazio, T. (2018). A Vision-Based Approach for Unmanned Aerial Vehicle Landing. Journal of Intelligent & Robotic Systems, 95(2), 645-664. doi:10.1007/s10846-018-0933-2Baca, T., Stepan, P., Spurny, V., Hert, D., Penicka, R., Saska, M., … Kumar, V. (2019). Autonomous landing on a moving vehicle with an unmanned aerial vehicle. Journal of Field Robotics, 36(5), 874-891. doi:10.1002/rob.21858De Souza, J. P. C., Marcato, A. L. M., de Aguiar, E. P., Jucá, M. A., & Teixeira, A. M. (2019). Autonomous Landing of UAV Based on Artificial Neural Network Supervised by Fuzzy Logic. Journal of Control, Automation and Electrical Systems, 30(4), 522-531. doi:10.1007/s40313-019-00465-ySITL Simulator (Software in the Loop)http://ardupilot.org/dev/docs/sitl-simulator-software-in-the-loop.htmlFabra, F., Calafate, C. T., Cano, J.-C., & Manzoni, P. (2017). On the impact of inter-UAV communications interference in the 2.4 GHz band. 2017 13th International Wireless Communications and Mobile Computing Conference (IWCMC). doi:10.1109/iwcmc.2017.7986413MAVLink Micro Air Vehicle Communication Protocolhttp://qgroundcontrol.org/mavlink/startFabra, F., Calafate, C. T., Cano, J. C., & Manzoni, P. (2018). ArduSim: Accurate and real-time multicopter simulation. Simulation Modelling Practice and Theory, 87, 170-190. doi:10.1016/j.simpat.2018.06.009Careem, M. A. A., Gomez, J., Saha, D., & Dutta, A. (2019). HiPER-V: A High Precision Radio Frequency Vehicle for Aerial Measurements. 2019 16th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON). doi:10.1109/sahcn.2019.882490",,'MDPI AG',Accurate Landing of Unmanned Aerial Vehicles Using Ground Pattern Recognition,10.3390/electronics8121532,https://riunet.upv.es/bitstream/10251/144317/1/Wubben%3bFABRA%3bTavares%20-%20Accurate%20Landing%20of%20Unmanned%20Aerial%20Vehicles%20Using%20Ground%20Pattern%20Recognit....pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
154792820,2017,"Connecting biologically inspired neural simulations to physical or simulated embodiments can be useful both in robotics, for the development of a new kind of bio-inspired controllers, and in neuroscience, to test detailed brain models in complete action-perception loops. The aim of this work is to develop a fully spike-based, biologically inspired mechanism for the translation of proprioceptive feedback. The translation is achieved by implementing a computational model of neural activity of type Ia and type II afferent fibers of muscle spindles, the primary source of proprioceptive information, which, in mammals is regulated through fusimotor activation and provides necessary adjustments during voluntary muscle contractions. As such, both static and dynamic Î³-motoneurons activities are taken into account in the proposed model. Information from the actual proprioceptive sensors (i.e., motor encoders) is then used to simulate the spindle contraction and relaxation, and therefore drive the neural activity. To assess the feasibility of this approach, the model is implemented on the NEST spiking neural network simulator and on the SpiNNaker neuromorphic hardware platform and tested on simulated and physical robotic platforms. The results demonstrate that the model can be used in both simulated and real-time robotic applications to translate encoder values into a biologically plausible neural activity. Thus, this model provides a completely spike-based building block, suitable for neuromorphic platforms, that will enable the development of sensory-motor closed loops which could include neural simulations of areas of the central nervous system or of low-level reflexes",,'Frontiers Media SA',Proprioceptive feedback through a neuromorphic muscle spindle model,10.3389/fnins.2017.00341,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
162983474,2018-10-29T00:00:00,"International audienceDespite the recent successes of deep reinforcement learning, teaching complex motor skills to a physical robot remains a hard problem. While learning directly on a real system is usually impractical, doing so in simulation has proven to be fast and safe. Nevertheless, because of the ""reality gap,"" policies trained in simulation often perform poorly when deployed on a real system. In this work, we introduce a method for training a recurrent neural network on the differences between simulated and real robot trajectories and then using this model to augment the simulator. This Neural-Augmented Simulation (NAS) can be used to learn control policies that transfer significantly better to real environments than policies learned on existing simulators. We demonstrate the potential of our approach through a set of experiments on the Mujoco simulator with added backlash and the Poppy Ergo Jr robot. NAS allows us to learn policies that are competitive with ones that would have been learned directly on the real robot",,HAL CCSD,Sim-to-Real Transfer with Neural-Augmented Robot Simulation,,https://core.ac.uk/download/162983474.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
345057372,2019-11-01T00:00:00,"Deep- and reinforcement-learning techniques have increasingly required large sets of real data to achieve stable convergence and generalization, in the context of image-recognition, object-detection or motion-control strategies. On this subject, the research community lacks robust approaches to overcome unavailable real-world extensive data by means of realistic synthetic-information and domain-adaptation techniques. In this work, synthetic-learning strategies have been used for the vision-based autonomous following of a noncooperative multirotor. The complete maneuver was learned with synthetic images and high-dimensional low-level continuous robot states, with deep- and reinforcement-learning techniques for object detection and motion control, respectively. A novel motion-control strategy for object following is introduced where the camera gimbal movement is coupled with the multirotor motion during the multirotor following. Results confirm that our present framework can be used to deploy a vision-based task in real flight using synthetic data. It was extensively validated in both simulated and real-flight scenarios, providing proper results (following a multirotor up to 1.3 m/s in simulation and 0.3 m/s in real flights)",,'MDPI AG',Vision-Based Multirotor Following Using Synthetic Learning Techniques,10.3390/s19214794,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
395677188,2020-07-17T00:00:00,"International audienceThe use of Reinforcement Learning (RL) is still restricted to simulation or to enhance human-operated systems through recommendations. Real-world environments (e.g. industrial robots or power grids) are generally designed with safety constraints in mind implemented in the shape of valid actions masks or contingency controllers. For example, the range of motion and the angles of the motors of a robot can be limited to physical boundaries. Violating constraints thus results in rejected actions or entering in a safe mode driven by an external controller, making RL agents incapable of learning from their mistakes. In this paper, we propose a simple modification of a state-of-the-art deep RL algorithm (DQN), enabling learning from forbidden actions. To do so, the standard Q-learning update is enhanced with an extra safety loss inspired by structured classification. We empirically show that it reduces the number of hit constraints during the learning phase and accelerates convergence to near-optimal policies compared to using standard DQN. Experiments are done on a Visual Grid World Environment and Text-World domain",,HAL CCSD,"""I'm sorry Dave, I'm afraid I can't do that"" Deep Q-Learning From Forbidden Actions",,https://core.ac.uk/download/395677188.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
390127702,2021-04-01T00:00:00,"International audienceSimulating frictional contacts remains a challenging research topic in robotics. Recently, differentiable physics emerged and has proven to be a key element in modelbased Reinforcement Learning (RL) and optimal control fields. However, most of the current formulations deploy coarse approximations of the underlying physical principles. Indeed, the classic simulators lose precision by casting the Nonlinear Complementarity Problem (NCP) of frictional contact into a Linear Complementarity Problem (LCP) to simplify computations. Moreover, such methods deploy non-smooth operations and cannot be automatically differentiated. In this paper, we propose (i) an extension of the staggered projections algorithm for more accurate solutions of the problem of contacts with friction. Based on this formulation, we introduce (ii) a differentiable simulator and an efficient way to compute the analytical derivatives of the involved optimization problems. Finally, (iii) we validate the proposed framework with a set of experiments to present a possible application of our differentiable simulator. In particular, using our approach we demonstrate accurate estimation of friction coefficients and object masses both in synthetic and real experiments",,'Institute of Electrical and Electronics Engineers (IEEE)',Differentiable simulation for physical system identification,10.1109/LRA.2021.3062323,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
226754253,2019,"Fully-autonomous miniaturized robots (e.g., drones), with artificial intelligence (AI) based visual navigation capabilities, are extremely challenging drivers of Internet-of-Things edge intelligence capabilities. Visual navigation based on AI approaches, such as deep neural networks (DNNs) are becoming pervasive for standard-size drones, but are considered out of reach for nano-drones with a size of a few . In this work, we present the first (to the best of our knowledge) demonstration of a navigation engine for autonomous nano-drones capable of closed-loop end-to-end DNN-based visual navigation. To achieve this goal we developed a complete methodology for parallel execution of complex DNNs directly on board resource-constrained milliwatt-scale nodes. Our system is based on GAP8, a novel parallel ultra-low-power computing platform, and a 27 commercial, open-source CrazyFlie 2.0 nano-quadrotor. As part of our general methodology, we discuss the software mapping techniques that enable the state-of-the-art deep convolutional neural network presented in dronet to be fully executed aboard within a strict real-time constraint with no compromise in terms of flight results, while all processing is done with only on average. Our navigation engine is flexible and can be used to span a wide performance range: at its peak performance corner, it achieves while still consuming on average just of the power envelope of the deployed nano-aircraft. To share our key findings with the embedded and robotics communities and foster further developments in autonomous nano-UAVs, we publicly release all our code, datasets, and trained networks",,,A 64mW DNN-based Visual Navigation Engine for Autonomous Nano-Drones,10.1109/JIOT.2019.2917066,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
398618413,2017-11-22T00:00:00,"International audienceFor a robot to be deployed in unconstrained real world environments, it needs to be autonomous. In this preliminary work, we focus on the capacity of an autonomous robot to discover and recognize objects in its visual field. Current existing solutions mainly employ complex deep neural architectures that need to be pre-trained using large datasets in order to be effective. We propose a new model for autonomous and unsupervised object learning in videos that does not require supervised pre-training and uses relatively simple visual filtering. The main idea relies on the saliency-based detection and learning of objects considered similar (thanks to a spatio-temporal continuity). For this purpose the learning of objects is based on a Siamese Neural Network (SNN). We demonstrate the capacity of the SNN to learn a good feature representation despite the deliberately simple and noisy process used to extract candidate objects",,HAL CCSD,Autonomous object recognition in videos using Siamese Neural Networks,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
299941855,2018-01-01T00:00:00,"It is crucial for robots to autonomously steer in complex environments safely without colliding with any obstacles. Compared to conventional methods, deep reinforcement learning-based methods are able to learn from past experiences automatically and enhance the generalization capability to cope with unseen circumstances. Therefore, we propose an end-to-end deep reinforcement learning algorithm in this paper to improve the performance of autonomous steering in complex environments. By embedding a branching noisy dueling architecture, the proposed model is capable of deriving steering commands directly from raw depth images with high efficiency. Specifically, our learning-based approach extracts the feature representation from depth inputs through convolutional neural networks and maps it to both linear and angular velocity commands simultaneously through different streams of the network. Moreover, the training framework is also meticulously designed to improve the learning efficiency and effectiveness. It is worth noting that the developed system is readily transferable from virtual training scenarios to real-world deployment without any fine-tuning by utilizing depth images. The proposed method is evaluated and compared with a series of baseline methods in various virtual environments. Experimental results demonstrate the superiority of the proposed model in terms of average reward, learning efficiency, success rate as well as computational time. Moreover, a variety of real-world experiments are also conducted which reveal the high adaptability of our model to both static and dynamic obstacle-cluttered environments.Published versio","[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",'MDPI AG',Learn to steer through deep reinforcement learning,10.3390/s18113650,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
144878273,2017-11-02,"This paper deals with the design and the
implementation of an automatic task planner for a robot, irrespective
of whether it is a stationary robot or a mobile robot. The aim of the
task planner nothing but, they are planning systems which are used to
plan a particular task and do the robotic manipulation. This planning
system is embedded into the system software in the computer, which
is interfaced to the computer. When the instructions are given using
the computer, this is transformed into real time application using the
robot. All the AI based algorithms are written and saved in the
control software, which acts as the intelligent task planning system",,,Design of an Artificial Intelligence Based Automatic Task Planner or a Robotic System,10.5281/zenodo.1132998,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
478192965,2021-01-01T00:00:00,"Navigation on challenging terrain topographies requires the understanding of robots’ locomotion capabilities to produce optimal solutions. We present an integrated framework for real-time autonomous navigation of mobile robots based on elevation maps. The framework performs rapid global path planning and optimization that is aware of the locomotion capabilities of the robot. A GPU-aided, sampling-based path planner combined with a gradient-based path optimizer provides optimal paths by using a neural network-based locomotion cost predictor which is trained in simulation. We show that our approach is capable of planning and optimizing paths three orders of magnitude faster than RRT* on GPU-enabled hardware, enabling real-time deployment on mobile platforms. We successfully evaluate the framework on the ANYmal C quadrupedal robot in both simulations and real-world environments for path planning tasks on multiple complex terrains",,'Institute of Electrical and Electronics Engineers (IEEE)',Real-time Optimal Navigation Planning Using Learned Motion Costs,10.1109/ICRA48506.2021.9561861,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
233572457,2020-01-01T00:00:00,"In this article, we propose an augmented reality semiautomatic labeling (ARS), a semiautomatic method which leverages on moving a 2-D camera by means of a robot, proving precise camera tracking, and an augmented reality pen (ARP) to define initial object bounding box, to create large labeled data sets with minimal human intervention. By removing the burden of generating annotated data from humans, we make the deep learning technique applied to computer vision, which typically requires very large data sets, truly automated and reliable. With the ARS pipeline, we created two novel data sets effortlessly, one on electromechanical components (industrial scenario) and other on fruits (daily-living scenario) and trained two state-of-the-art object detectors robustly, based on convolutional neural networks, such as you only look once (YOLO) and single shot detector (SSD). With respect to conventional manual annotation of 1000 frames that takes us slightly more than 10 h, the proposed approach based on ARS allows to annotate 9 sequences of about 35,000 frames in less than 1 h, with a gain factor of about 450. Moreover, both the precision and recall of object detection is increased by about 15% with respect to manual labeling. All our software is available as a robot operating system (ROS) package in a public repository alongside with the novel annotated data sets",,'Institute of Electrical and Electronics Engineers (IEEE)',Semiautomatic Labeling for Deep Learning in Robotics,10.1109/TASE.2019.2938316,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
479465616,2021-01-01T00:00:00,"Robotics, Artificial Intelligence (AI), and the Internet of Things (IoT) support various processes in many scenarios of modern life such as e-health and psychological treatments. This article presents the design, development, implementation, and assessment of a Robotic Assistant (RA), named &#x201C;Atent&#x0040;&#x201D;, as a support tool in the homework activities of children with Attention Deficit Hyperactivity Disorder (ADHD). Interacting with the children the RA helps them correct their bad habits and misbehavior caused by the disorder. Its features and functionalities were designed by therapists, implementing AI algorithms to process information and make decisions in real-time to help children to be focused on their homework. This RA interacts with smart objects deployed at home, which are associated with the activity under observation (desk and chair). This solution allows therapists to receive more accurate information about the homework sessions inside the home. At the same time, remote interaction with the child is made possible (through the RA) to provide new instructions and support him/her along with the sessions. This RA is a significant evolution of an earlier version. All the improvements brought to the project by the modifications in technical and qualitative features are explained. Furthermore, the experiment and its results are presented to illustrate the clinical potential. This project shows that the RA can not only make observations with a high degree of precision like an expert (teacher/therapist) but also positively influences the homework performance of children with and without ADHD","[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",'Institute of Electrical and Electronics Engineers (IEEE)',Assessment of a Robotic Assistant for Supporting Homework Activities of Children With ADHD,10.1109/ACCESS.2021.3093233,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
227186025,,"In this paper we present a new methodology for robot learning that combines ideas from statistical generalization and reinforcement learning. First we apply statistical generalization to compute an approximation for the optimal control policy as defined by training movements that solve the given task in a number of specific situations. This way we obtain a manifold of movements, which dimensionality is usually much smaller than the dimensionality of a full space of movement primitives. Next we refine the policy by means of reinforcement learning on the approximating manifold, which results in a learning problem constrained to the low dimensional manifold. We show that in some situations, learning on the low dimensional manifold can be implemented as an error learning algorithm. We apply golden section search to refine the control policy. Furthermore, we propose a reinforcement learning algorithm with an extended parameter set, which combines learning in constrained domain with learning in full space of parametric movement primitives, which makes it possible to explore actions outside of the initial approximating manifold. The proposed approach was tested for learning of pouring action both in simulation and on a real robotSistemų analizės katedraVytauto Didžiojo universiteta",,'Institute of Electrical and Electronics Engineers (IEEE)',Applying statistical generalization to determine search direction for reinforcement learning of movement primitives,10.1109/HUMANOIDS.2012.6651500,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
395676178,2021-01-01T00:00:00,"International audienceSimulating frictional contacts remains a challenging research topic in robotics. Recently, differentiable physics emerged and has proven to be a key element in modelbased Reinforcement Learning (RL) and optimal control fields. However, most of the current formulations deploy coarse approximations of the underlying physical principles. Indeed, the classic simulators lose precision by casting the Nonlinear Complementarity Problem (NCP) of frictional contact into a Linear Complementarity Problem (LCP) to simplify computations. Moreover, such methods deploy non-smooth operations and cannot be automatically differentiated. In this paper, we propose (i) an extension of the staggered projections algorithm for more accurate solutions of the problem of contacts with friction. Based on this formulation, we introduce (ii) a differentiable simulator and an efficient way to compute the analytical derivatives of the involved optimization problems. Finally, (iii) we validate the proposed framework with a set of experiments to present a possible application of our differentiable simulator. In particular, using our approach we demonstrate accurate estimation of friction coefficients and object masses both in synthetic and real experiments",,'Institute of Electrical and Electronics Engineers (IEEE)',Differentiable simulation for physical system identification,10.1109/LRA.2021.3062323,https://core.ac.uk/download/395676178.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
301281319,2017-01-01T00:00:00,"Spatial perception, in which objects’ motion and positional relationship are recognized, is necessary for applications such as a walking robot and an autonomous car. One of the demanding features of spatial perception in real world applications is robustness. Neural network-based approaches, in which perception results are obtained by voting among a large number of neuronal activities, seem to be promising. We focused on a neural network model for motion stereo vision proposed by Kawakami et al. In this model, local motion in each small region of the visual field, which comprises optical flow, is detected by hierarchical neural network. Implementation of this model into a VLSI is required for real-time operation with low power consumption. In this study, we reduced the computational complexity of this model and showed cell responses of the reduced model by numerical simulation.Peer Reviewe",,'Springer Science and Business Media LLC',Complexity reduction of neural network model for local motion detection in motion stereo vision,10.1007/978-3-319-70136-3,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
131956132,2017-11-08T00:00:00,"International audienceBackground:  In maize, silks are hundreds of filaments that simultaneously emerge from the ear for collecting pollen over a period of 1–7 days, which largely determines grain number especially under water deficit. Silk growth is a major trait for drought tolerance in maize, but its phenotyping is difficult at throughputs needed for genetic analyses.Results:  We have developed a reproducible pipeline that follows ear and silk growths every day for hundreds of plants, based on an ear detection algorithm that drives a robotized camera for obtaining detailed images of ears and silks. We first select, among 12 whole‑plant side views, those best suited for detecting ear position. Images are seg‑mented, the stem pixels are labelled and the ear position is identified based on changes in width along the stem. A mobile camera is then automatically positioned in real time at 30 cm from the ear, for a detailed picture in which silks are identified based on texture and colour. This allows analysis of the time course of ear and silk growths of thousands of plants. The pipeline was tested on a panel of 60 maize hybrids in the PHENOARCH phenotyping platform. Over 360 plants, ear position was correctly estimated in 86% of cases, before it could be visually assessed. Silk growth rate, estimated on all plants, decreased with time consistent with literature. The pipeline allowed clear identification of the effects of genotypes and water deficit on the rate and duration of silk growth.Conclusions:  The pipeline presented here, which combines computer vision, machine learning and robotics, provides a powerful tool for large‑scale genetic analyses of the control of reproductive growth to changes in environ‑mental conditions in a non‑invasive and automatized way. It is available as Open Source software in the OpenAlea platform",,'Springer Science and Business Media LLC',A robot-assisted imaging pipeline for tracking the growths of maize ear and silks in a high-throughput phenotyping platform,10.1186/s13007-017-0246-7,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
335413867,2019-11-01T00:00:00,"Deep- and reinforcement-learning techniques have increasingly required large sets of real data to achieve stable convergence and generalization, in the context of image-recognition, object-detection or motion-control strategies. On this subject, the research community lacks robust approaches to overcome unavailable real-world extensive data by means of realistic synthetic-information and domain-adaptation techniques. In this work, synthetic-learning strategies have been used for the vision-based autonomous following of a noncooperative multirotor. The complete maneuver was learned with synthetic images and high-dimensional low-level continuous robot states, with deep- and reinforcement-learning techniques for object detection and motion control, respectively. A novel motion-control strategy for object following is introduced where the camera gimbal movement is coupled with the multirotor motion during the multirotor following. Results confirm that our present framework can be used to deploy a vision-based task in real flight using synthetic data. It was extensively validated in both simulated and real-flight scenarios, providing proper results (following a multirotor up to 1.3 m/s in simulation and 0.3 m/s in real flights)",,'MDPI AG',Vision-Based Multirotor Following Using Synthetic Learning Techniques,10.3390/s19214794,https://oa.upm.es/64118/,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
442510888,2015-01-01T00:00:00,"Complex real problems increasingly require intelligent systems that combine knowledge, techniques and methodologies from various sources. Intelligent systems based on artificial intelligence techniques that are associated with the behavior of people can perform the processes of learning, reasoning and solving all kinds of problems. Such systems, which automatically can perform tasks set by the user or other software, today thankfully called intelligent agents. Independent, intelligent agents on the Internet can be very successful to perform some search work on behalf of and for the needs of different users. For efficient collection, manipulation and management of data, such software can be very interesting from the standpoint of intelligent data analysis in many areas the police. Analysis of the data collected by an intelligent agent (a software robot-bot) can be successfully utilized, among many jobs in the police, and in the field of crime and in particular manifestation of cyber­crime, traffic safety, emergencies, etc. To make the collection and analysis of data from criminal activities on the Internet effective, it is necessary to examine the existing artificial intelligence techniques to be used for the conclusion of the intelligent agents. On the other hand, using of methods of artificial intelligence in finding data along with intelligent data analysis (data mining) should be used, which has found wide use in the area of business, economics, mechanics, medicine, genetics, transport etc","[{'title': 'Nauka bezbednost policija', 'identifiers': ['issn:0354-8872', 'issn:2620-0406', '2620-0406', '0354-8872']}]","University of Criminal Investigation and Police Studies, Belgrade",Artificial intelligence in process of collecting and analyzing data within police works,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
265120533,2019-12-13,"International audienceThe use of Reinforcement Learning (RL) is still restricted to simulation or to enhance human-operated systems through recommendations. Real-world environments (e.g. industrial robots or power grids) are generally designed with safety constraints in mind implemented in the shape of valid actions masks or contingency controllers. For example, the range of motion and the angles of the motors of a robot can be limited to physical boundaries. Violating constraints thus results in rejected actions or entering in a safe mode driven by an external controller, making RL agents incapable of learning from their mistakes. In this paper, we propose a simple modification of a state-of-the-art deep RL algorithm (DQN), enabling learning from forbidden actions. To do so, the standard Q-learning update is enhanced with an extra safety loss inspired by structured classification. We empirically show that it reduces the number of hit constraints during the learning phase and accelerates convergence to near-optimal policies compared to using standard DQN. Experiments are done on a Visual Grid World Environment and Text-World domain",,HAL CCSD,"""I'm sorry Dave, I'm afraid I can't do that"" Deep Q-Learning From Forbidden Actions",,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
297620750,,"Telepresence Mobile Robots have prominent attributes in many fields as it provides virtual presence for human robot interaction. The deployment of this robot in healthcare sector has improved patient care and health. The vision system in a telepresence robot allows two way audiovisual communication between people at different location. In spite of such advancement, the manual way of controlling a robot to recognise and track people during an emergency is not favourable for a long duration. To circumvent this problem, biometric method using human face is proposed in this research which is implemented on Medical Telediagnosis Robot. This paper details the design of the face recognition and tracking system with four automated modules which are motion detection, face detection, face recognition and face tracking. The modules are developed with different algorithm and tested individually to ensure the stability of the system. Artificial Intelligence technique was applied at the face recognition stage while a two degree of freedom mechanism for actuator control was used at face tracking stage. A sequential mode operation is proposed to reduce the execution time in a real-time environment. To achieve this, only one module is operated at each time. A Graphical User Interface was developed to ease the users at the local and robot environment. The system is designed in LabVIEW platform. The biometric system proposed with hybrid algorithm at each module adapts for face images detected at different distances, poses and lighting condition. This system was tested in real-time and has an execution time of 55ms and 98% accuracy. The stand alone system designed for Medical Telediagnosis Robot can be will be very fruitful for various biometric system using facial technology",,"Journal of Telecommunication, Electronic and Computer Engineering, Universiti Teknikal Malaysia Melaka",Towards real-time visual biometric authentication using human face for healthcare Telepresence Mobile Robots,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
304944408,2016-01-01T00:00:00,"Cameras provide a rich source of information while being passive, cheap and lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work we present the first implementation of receding horizon control, which is widely used in ground vehicles, with monocular vision as the only sensing mode for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a number of contributions: novel coupling of perception and control via relevant and diverse, multiple interpretations of the scene around the robot, leveraging recent advances in machine learning to showcase anytime budgeted cost-sensitive feature selection, and fast non-linear regression for monocular depth prediction. We empirically demonstrate the efficacy of our novel pipeline via real world experiments of more than 2 kms through dense trees with a quadrotor built from off-the-shelf parts. Moreover our pipeline is designed to combine information from other modalities like stereo and lidar as well if available</p",,,Vision and Learning for Deliberative Monocular Cluttered Flight,10.1184/r1/6561554.v1,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
234653434,2018-03-02T00:00:00,"In our days a new type of journalism is been developed : the algorithmic journalism, also known as robot journalism or automated journalism. This kind of journalism is based on an artificial intelligence software (IA) and an advanced natural language generation (Advanced NLG), which automatically generate articles in near real time and in human readable ways. Algorithmic journalism was used in 2016 at the Olympic Games of Rio and at the present time is also used by international news providers. Keywords: algorithmic journalism, robot journalism, automated journalism, sport media, Greek media",,"'International Institute for Science, Technology and Education'",Potential Applications of Algorithmic (Robot) Journalism for the Greek Sport Media,10.7176/NMMC.vol6727-30,https://core.ac.uk/download/234653434.pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
187750338,2018-01-01T00:00:00,"The cerebellum has a central role in fine motor control and in various neural processes, as in associative paradigms. In this work, a bioinspired adaptive model, developed by means of a spiking neural network made of thousands of artificial neurons, has been leveraged to control a humanoid NAO robot in real-time. The learning properties of the system have been challenged in a classic cerebellum-driven paradigm, the Pavlovian timing association between two provided stimuli, here implemented as a laser-avoidance task. The neurophysiological principles used to develop the model, succeeded in driving an adaptive motor control protocol with acquisition and extinction phases. The spiking neural network model showed learning behaviors similar to the ones experimentally measured with human subjects in the same conditioning task. The model processed in real-time external inputs, encoded as spikes, and the generated spiking activity of its output neurons was decoded, in order to trigger the proper response with a correct timing. Three long-term plasticity rules have been embedded for different connections and with different time-scales. The plasticities shaped the firing activity of the output layer neurons of the network. In the Pavlovian protocol, the neurorobot successfully learned the correct timing association, generating appropriate responses. Therefore, the spiking cerebellar model was able to reproduce in the robotic platform how biological systems acquire and extinguish associative responses, dealing with noise and uncertainties of a real-world environment",,'Institute of Electrical and Electronics Engineers (IEEE)',Bioinspired Adaptive Spiking Neural Network to Control NAO Robot in a Pavlovian Conditioning Task,10.1109/BIOROB.2018.8487202,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
203171141,2016-12-31,"AbstractThe effort to develop an electronic skin is highly motivated by many application domains namely robotics, biomedical instrumentations, and replacement prosthetic devices. Several e-skin systems have been proposed recently and have demonstrated the need of an embedded electronic system for tactile data processing either to mimic the human skin or to respond to the application demands. Processing tactile data requires efficient methods to extract meaningful information from raw sensors data.In this framework, our goal is the development of a dedicated embedded electronic system for electronic skin. The embedded electronic system has to acquire the tactile data, process and extract structured information. Machine Learning (ML) represents an effective method for data analysis in many domains: it has recently demonstrated its effectiveness in processing tactile sensors data.This paper presents an embedded electronic system based on dedicated hardware implementation for electronic skin systems. It provides a Tensorial kernel function implementation for machine learning based on Tensorial kernel approach. Results assess the time latency and the hardware complexity for real time functionality. The implementation results highlight the high amount of power consumption needed for the input touch modalities classification task. Conclusions and future perspectives are also presented",,The Author(s). Published by Elsevier Ltd.,Embedded Electronic System Based on Dedicated Hardware DSPs for Electronic Skin Implementation ,10.1016/j.protcy.2016.08.007,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
343489552,2020-09-11T08:33:27,"Several neural network controllers for robotic manipulators have been developed during the last decades due to their capability to learn the dynamic properties and the improvements in the global stability of the system. In this paper, an adaptive neural controller has been designed with self learning to resolve the problems caused by using a classical controller. A comparison between the improved unsupervised adaptive neural network controller and the P controller for the NXT SCARA robot system is done, and the result shows the improvement of the self learning controller to track the determined trajectory of robotic automated controllers with uncertainties. Implementation and practical results were designed to guarantee online real-time",,'University of Debrecen/ Debreceni Egyetem',Implementation of Adaptive Neural Networks  Controller for NXT SCARA Robot System,10.17667/riim.2017.1/3.,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
493123570,2021-12-31T00:00:00,"[EN] With current data acquisition techniques, exhaustive documentation of patrimonial goods is generated, which allows obtaining a 3D geometric model, on which data from research from research is generated. As in other fields of architecture, the latest graphic techniques and manipulation of the image, allow a working methodology other than the current one in the patrimonial field. The information systems have been evolving towards technologies developed with open source software, the use of standards, ontologies and the structuring of information and of the 3D model itself under a semantic hierarchy. Interoperability between databases is favored and the maintenance of applications is ensured. The purpose is to support decision-making related to the model and simultaneously, serve multiple purposes such as cataloging, protection, restoration, conservation, maintenance or dissemination, among others. The research carried out for the realization of the survey of the City Council and Lonja of Alcañiz (Teruel), has the aim of expanding the historical knowledge of the buildings and delimiting its possible constructive phases, in order to obtain the geometric reality of the buildings. In this way, provide greater instruments to carry out the drafting work of the set restoration project. Quintilla Castán, M.; Agustín Hernández, L. (2021). 3D survey and virtual reconstruction of heritage. The case study of the City Council and Lonja of Alcañiz. VITRUVIO - International Journal of Architectural Technology and Sustainability. 6(2):12-25. https://doi.org/10.4995/vitruvio-ijats.2021.16567OJS122562Afsari, K., Eastman, C., Shelden, D. 2016. Cloud-Based BIM Data Transmission: Current Status and Challenges. 33rd International Symposium on Automation and Robotics in Construction (ISARC 2016). https://doi.org/10.22260/ISARC2016/0129Apollonio, F.I., Benedetti, B., Gaiani, M. 2011. Construction, Management and Visualization of 3D Models of Large Archeological and Architectural Sites for E-Heritage GIS Systems. XXIIIrd International CIPA Symposium, Prague, Czech Republic. https://doi.org/10.6092/unibo/amsacta/3141Armisén Fernández, A., García Fernández-Jardón, B., Mateos Redondo, F. J., Valdeón Menéndez, L., Rojo Álvarez, A. 2016. Plataforma virtual para el diseño, planificación, control, intervención y mantenimiento en el ámbito de la conservación del patrimonio histórico ""PETROBIM"". Congreso Euro-Americano REHABEND 2016, Universidad de Cantabria.Armisén Fernández, A., Agustín, L.,..., Soto, A. 2018. BIM aplicado al patrimonio cultural. España: Building SMART Spanish Chapter: Documento 14. https://www.buildingsmart.es/recursos/gu%C3%ADas-ubim/Banfi, F. 2019. The integration of a scan-To-HBIM process in bim application: The development of an add-in to guide users in autodesk revit. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XLII-2/W11, 141-148. https://doi.org/10.5194/isprs-archives-XLII-2-W11-141-2019De Luca, L., Busayarat, C., Stefani, C., Véron, P., Florenzano, M. 2011. A semantic-based platform for the digital analysis of architectural heritage. Computers & Graphics. 35(2), 227-241. https://doi.org/10.1016/j.cag.2010.11.009Dore, C., Murphy, M., Mccarthy, M., Casidy, C., Dirix, E. 2015. Structural Simulations and Conservation Analysis -Historic Building Information Model (HBIM). ISPRS-The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XL-5/W4, 351-357. https://doi.org/10.5194/isprsarchives-XL-5-W4-351-2015Fassi, F., Achille, C., Mandelli, A., Rechichi, F., Parri, S. 2015. A new idea of BIM system for visualization, web sharing and using huge complex 3d models for facility management, ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XL-5/W4, 359-366. https://doi.org/10.5194/isprsarchives-XL-5-W4-359-2015Fassi, F., Fregonese, L., Adami, A., Rechichi, F. 2017. BIM system for the conservation and preservation of the mosaics of San Marco in Venice. ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XLII-2/W5, 229-236. doi.org/ 94/isprs-archives-XLII-2-W5-229-2017Hernández Martínez, A. 2016. La restauración monumental en Aragón en la década de los 70 del siglo XX: las intervenciones de Chueca Goitia en las casas consistoriales de Tarazona, Alcañiz y Uncastillo. El Greco en su IV Centenario: patrimonio hispánico y diálogo intercultural. Textos de ponencias y comunicaciones. Cuenca, Ediciones de la Universidad de Castilla-La Mancha, Colección ""Estudios"" 151: 338.Iadanza, E., Maietti, F., Ziri, A.E., Di Giulio, R., Medici, M., Ferrari, F., Bonsma, P., Turillazzi, B. 2019. Semantic web technologies meet BIM for accessing and understanding cultural heritage. ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XLII-2/W9, 381-388. https://doi.org/10.5194/isprs-archives-XLII-2-W9-381-2019Malinverni, E.S., Pierdicca, R., Paolanti, M., Martini, M., Morbidoni, C., Matrone, F., Lingua, A. 2019. Deep learning for semantic segmentation of 3D point cloud. ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XLII-2/W15, 735-742. https://doi.org/10.5194/isprs-archives-XLII-2-W15-735-2019Marston, S., Li, Z., Bandyopadhyay, S., Zhang, J., Ghalsasi, A. 2011. Cloud computing-The business perspective. Decision Support Systems, 51(1), 176-189. https://doi.org/10.1016/j.dss.2010.12.006Murphy, M., Mcgovern, E., Pavia, S. 2013. Historic Building Information Modelling - Adding intelligence to laser and image based surveys of European claasical architecture. ISPRS Journal of Photogrammetry and Remote Sensing, 76, 89-102. https://doi.org/10.1016/j.isprsjprs.2012.11.006Nieto Julián, J. E., Moyano Campos, J. J. 2014. El Estudio Paramental en el Modelo de Información del Edificio Histórico o ""Proyecto HBIM"". Virtual Archaeology Review, 5(11), 73-85. https://doi.org/10.4995/var.2014.4183Nieto Julián, J.E., Moyano Campos, J.J., Rico Delgado, F., Antón García, D. 2016. Management of built heritage via HBIM Project: A case of study of flooring and tiling, Virtual Archaeology Review, 7(14), 1-12. https://doi.org/ 10.4995/var.2016.4349Oreni, D., Brumana, R., Della Torre, S., Banfi, F., & Previtali, M., 2014. Survey turned into HBIM: the restoration and the work involved concerning the Basilica di Collemaggio after the earthquake (L'Aquila). ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 2(5), 267-273. https://doi.org/10.5194/isprsannals-II-5-267-2014Quattrini, R., Pierdicca, R., Morbidoni, C., Malinverni, E.S. 2017. Conservation-oriented HBIM. The Bimexplorer web tool. ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XLII-5/W1, 275-281. https://doi.org/10.5194/isprs-archives-XLII-5-W1-275-2017Rechichi, F., Mandelli, A., Achille, C., Fassi, F. 2016. Sharing high-resolution models and information on web: the web module of bim3dsg system. ISPRS-International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XLI-B5, 703-710. https://doi.org/10.5194/isprs-archives-XLI-B5-703-2016Schuster, J. M. D. 1997. Information as a tool of preservation action. Preserving the built heritage: tools for implementation. Hanover and London: 32. University Press of New England.Thomson Llisterri, T. 2015. Conjunto Lonja-Ayuntamiento de Alcañiz. Fuentes y estado de la cuestión. Jornadas de Arte sobre la Lonja de Alcañiz. Taller de investigación multidisciplinar. Jornadas de estudio y difusión del patrimonio. Contextualización histórica. Alcañiz, Centro de Estudios de Arte del Renacimiento, 1 al 3 de julio de 2015",,'Universitat Politecnica de Valencia',3D survey and virtual reconstruction of heritage. The case study of the City Council and Lonja of Alcañiz,10.4995/vitruvio-ijats.2021.16567,https://riunet.upv.es/bitstream/10251/179281/1/QuintillaAgustin%20-%203D%20survey%20and%20virtual%20reconstruction%20of%20heritage%20The%20case%20study%20of%20the%20City%20Co....pdf,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
516350676,2021-01-01T00:00:00,"International audienceCollaborative robots are currently deployed in professional environments, in collaboration with professional human operators, helping to strike the right balance between mechanization and manual intervention in manufacturing processes required by Industry 4.0. In this paper, the contribution of gesture recognition and pose estimation to the smooth introduction of cobots into an industrial assembly line is described, with a view to performing actions in parallel with the human operators and enabling interaction between them. The proposed active vision system uses two RGB-D cameras that record different points of view of gestures and poses of the operator, to build an external perception layer for the robot that facilitates spatiotemporal adaptation, in accordance with the human's behavior. The use-case of this work is concerned with LCD TV assembly of an appliance manufacturer, comprising of two parts. The first part of the above-mentioned operation is assigned to a robot, strengthening the assembly line. The second part is assigned to a human operator. Gesture recognition, pose estimation, physical interaction, and sonic notification, create a multimodal human-robot interaction system. Five experiments are performed, to test if gesture recognition and pose estimation can reduce the cycle time and range of motion of the operator, respectively. Physical interaction is achieved using the force sensor of the cobot. Pose estimation through a skeleton-tracking algorithm provides the cobot with human pose information and makes it spatially adjustable. Sonic notification is added for the case of unexpected incidents. A real-time gesture recognition module is implemented through a Deep Learning architecture consisting of Convolutional layers, trained in an egocentric view and reducing the cycle time of the routine by almost 20%. This constitutes an added value in this work, as it affords the potential of recognizing gestures independently of the anthropometric characteristics and the background. Common metrics derived from the literature are used for the evaluation of the proposed system. The percentage of spatial adaptation of the cobot is proposed as a new KPI for a collaborative system and the opinion of the human operator is measured through a questionnaire that concerns the various affective states of the operator during the collaboration",,'Frontiers Media SA',Egocentric Gesture Recognition Using 3D Convolutional Neural Networks for the Spatiotemporal Adaptation of Collaborative Robots,10.3389/fnbot.2021.703545,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
390020542,2018-02-27T00:00:00,"The economic-legal aspects of the state and trends of the Internet-based technologies (IP) technology, the place of intellectual property in it are considered. It is shown that the Internet of Things creates conditions for the emergence of a synergetic effect from the combination of possibilities of artificial intelligence, cloud computing, set of sensors, mathematical algorithms for processing large data (Big Data), robotic devices of various purposes, data transmission systems (Internet), which allows to provide various services and perform various work with or without the participation of people. The role of the state in promoting the development of IP, the existing problems and ways of their solution are shown. Many governments in recent years are taking measures to analyze the state of affairs with the introduction of IP technologies, the localization of problems and threats that may or may occur in the future in order to formulate a common strategy for the development of industry for the production of IP technologies and their application in various sectors of the economy and public life. The patent landscape of the IP is analyzed, the most productive companies and inventors of IP are discovered, the dynamics of patenting in the IP environment, the value of patents, patent research problems are shown. The problems of intellectual property protection in the sphere of IP, in particular, copyright, inventions, trademarks, commercial secrets, information security are considered. The intellectual potential and untapped potential of Ukraine in the development of IP technologies are considered. It is concluded that in the widespread use of IP technologies, there is a significant potential for increasing the efficiency of any type of human activity. It concerns the real economy, industry and agriculture, health care, public administration, education, financial turnover, etc. The development of IP technologies is the most powerful stimulating factor in the innovative development of nanotechnologies, microelectronics, semiconductor technologies, microiminating of executive devices, telecommunications, radio technologies, software computing, robotics, and more.Рассмотрены экономико-правовые аспекты состояния и тенденций развития технологий Интернета вещей (ИВ), места в нем интеллектуальной собственности. Показано, что Интернет вещей создает условия для появления синергетического эффекта от сочетания возможностей искусственного интеллекта, облачных вычислений, множества сенсоров, математических алгоритмов обработки больших данных (Big Data), роботизированных устройств различного назначения, систем передачи данных (сети Интернет), что позволяет предоставлять разнообразные услуги и осуществлять различные работы с участием или без участия людей. Показана роль государства в содействии развитию ИВ, существующие проблемы и пути их решения. Правительства многих стран в последнее время принимают меры по анализу состояния дел с внедрением ИВ-технологий, локализации проблем и угроз, имеющих место или могущих возникнуть в будущем, с целью формирования общей стратегии развития промышленности производства технологий ИВ и их применения в различных секторах экономики и общественной жизни. Проанализированы патентный ландшафт ИВ, выявлены наиболее продуктивные компании и изобретатели ИВ, показана динамика патентования в среде ИВ, ценность патентов, проблемы патентного поиска. Рассмотрены проблемы охраны интеллектуальной собственности в сфере ИВ, в частности, авторских прав, изобретений, торговых марок, коммерческой тайны, информационной безопасности. Рассмотрены интеллектуальный потенциал и неиспользованные возможности Украины в развитии технологий ИВ. Делается вывод, что в широком применении технологий ИВ заложен значительный потенциал повышения эффективности любого вида человеческой деятельности. Это касается сферы реальной экономики, промышленности и сельского хозяйства, системы здравоохранения, государственного управления, образования, финансового оборота и т. п. Развитие технологий ИВ является мощным стимулирующим фактором инновационного развития нанотехнологий, микроэлектроники, полупроводниковых технологий, микроминиатюризации исполнительных устройств, телекоммуникаций, радиотехнологий, программных вычислительных средств, робототехники и многого другого.Розглянуто економіко-правові аспекти стану та тенденцій розвитку технологій Інтернету речей (ІР), місця в ньому інтелектуальної власності. Показано роль дер- жави у сприянні розвитку ІР, проблеми та шляхи їх вирішення. Проаналізовано патентний ландшафт ІР, виявлені найбільш продуктивні компанії та винахідники ІР, показано динаміку патентування в середовищі ІР, цінність патентів, проблеми патентного пошуку. Визначено проблеми охорони інтелектуальної власності у сфері ІР, зокрема, авторських прав, винаходів, торгових марок, комерційної таємниці, інформаційної безпеки. Розглянуто інтелектуальний потенціал і невикористані мож- ливості України в розвитку технологій ІР. Обґрунтовано висновок, що в широкому застосуванні технологій ІР закладено значний потенціал підвищення ефективності економіки",,Науково-дослідний інститут інтелектуальної власності НAПрН України,ІНТЕЛЕКТУАЛЬНА ВЛАСНІСТЬ В СИСТЕМІ ІНТЕРНЕТУ РЕЧЕЙ: ЕКОНОМІКО-ПРАВОВИЙ АСПЕКТ,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
345057400,2019-08-15T00:00:00,"Search and Rescue (SAR) missions represent an important challenge in the robotics research field as they usually involve exceedingly variable-nature scenarios which require a high-level of autonomy and versatile decision-making capabilities. This challenge becomes even more relevant in the case of aerial robotic platforms owing to their limited payload and computational capabilities. In this paper, we present a fully-autonomous aerial robotic solution, for executing complex SAR missions in unstructured indoor environments. The proposed system is based on the combination of a complete hardware configuration and a flexible system architecture which allows the execution of high-level missions in a fully unsupervised manner (i.e. without human intervention). In order to obtain flexible and versatile behaviors from the proposed aerial robot, several learning-based capabilities have been integrated for target recognition and interaction. The target recognition capability includes a supervised learning classifier based on a computationally-efficient Convolutional Neural Network (CNN) model trained for target/background classification, while the capability to interact with the target for rescue operations introduces a novel Image-Based Visual Servoing (IBVS) algorithm which integrates a recent deep reinforcement learning method named Deep Deterministic Policy Gradients (DDPG). In order to train the aerial robot for performing IBVS tasks, a reinforcement learning framework has been developed, which integrates a deep reinforcement learning agent (e.g. DDPG) with a Gazebo-based simulator for aerial robotics. The proposed system has been validated in a wide range of simulation flights, using Gazebo and PX4 Software-In-The-Loop, and real flights in cluttered indoor environments, demonstrating the versatility of the proposed system in complex SAR mission",,'Springer Science and Business Media LLC',A Fully-Autonomous Aerial Robot for Search and Rescue Applications in Indoor Environments using Learning-Based Techniques,10.1007/s10846-018-0898-1,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
37835704,2007,"Deploying AI, and specifically P&S, technology into the real world entails many stimulating problems for researchers and system designers. Various smart solutions have been produced both for specialized problems and, in fewer cases, for more general purpose domains. An important aspect which is fundamental for any successful application, and more specifically for those which address a broad audience, concerns the users\u27 perception and acceptance of technology. This issue is often either neglected or underestimated. We have been working for two years on the issue of importing experimental techniques from HCI and experimental psychology into smart system development. This approach has both pros - interesting features from the user perspective are discovered and can be used to bias design and research activities - and cons - experimenting with humans adds additional difficulty to the project, and applying a correct methodology is very time consuming. This paper describes a fielded experimental investigation of a fully implemented AI system named ROBO CARE . The system uses constraint-based scheduling technology to actively monitor a pattern of activities executed by an assisted person and uses detected temporal constraint violations to trigger meaningful and contextualized proactive interaction. Dialogue with the users is managed by a robotic mediator who acts as the main communication channel between the users and the intelligent domestic environment. The paper presents an evaluation of elderly people\u27s perception of the intelligent system, focusing on aspects related to the robot\u27s aspect, interaction modalities, content and timing of interaction, providing suggestions and hints for system designers",Caring About the User\u27s View: The Joys and Sorrows of Experiments with People,https://core.ac.uk/download/pdf/37835704.pdf,,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
323332613,2014-11-19T00:00:00,"Robotics has become a common subject in many engineering degrees and postgraduate programs. Although at undergraduate levels the students are introduced to basic theoretical concepts and tools, at postgraduate courses more complex topics have to be covered. One of those advanced subjects is Cognitive Robotics, which covers aspects like automatic symbolic reasoning, decision-making, task planning or machine learning. In particular, Reinforcement Learning (RL) is a machine learning and
decision-making methodology that does not require a model of the environment where the robot operates, overcoming this limitation by making observations. In order to get the greatest educational benefit, RL theory should be complemented with some hands-on RL task that uses a real robot, so students get a complete vision of the learning problem, as well as of the issues that arise when dealing with a physical robotic platform. There are several RL techniques that can be studied in such a subject; we have chosen Q-learning, since is a simple, effective and well-known RL algorithm.

In this paper we present a minimalist implementation of the Q-learning method for a Lego Mindstorms NXT mobile robot, focused on simplicity and applicability, and flexible enough to be adapted to several tasks. Starting from a simple wandering problem, we first design an off-line model of the learning process in which the Q-learning parameters are studied. After that, we implement the algorithm on the robot, gradually enlarging the number of states-actions of the problem. The final result of this work is a teaching framework for developing practical activities regarding Q-learning in our Robotics subjects, which will improve our teaching.Universidad de Málaga. Campus de Excelencia Internacional Andalucía Tech",LEGO© Mindstorms NXT and Q-Learning: a teaching approach for robotics in engineering,,,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
33705626,2014,"Humans have the subconscious ability to create simple abstractions from observations of their physical environment. The ability to consider the colour of an object in terms of ""red"" or ""blue"", rather than spatial distributions of reflected light wavelengths, is vital in processing and communicating information about important features within our local environment. The real-time identification of such features in image processing necessitates the software implementation of such a process; segmenting an image into regions of salient colour, and in doing so reducing the information stored and processed from 3-dimensional pixel values to a simple colour class label. This paper details a method by which colour segmentation may be performed offline and stored in a static look-up table, allowing for constant time dimensionality reduction in an arbitrary environment of coloured features. The machine learning framework requires no human supervision, and its performance is evaluated in terms of feature classification performance within a RoboCup robot soccer environment. The developed system is demonstrated to yield an 8% improvement over slower traditional methods of manual colour mapping",Unsupervised recognition of salient colour for real-time image processing,,Springer Verlag,10.1007/978-3-662-44468-9_33,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
54376977,2007-01-01T00:00:00,"In real-world robotic applications, many factors, both at low-level (e.g., vision and motion control parameters) and at high-level (e.g., the behaviors) determine the quality of the robot performance. Thus, for many tasks, robots require fine tuning of the parameters, in the implementation of behaviors and basic control actions, as well as in strategic decisional processes. In recent years, machine learning techniques have been used to find optimal parameter sets for different behaviors. However, a drawback of learning techniques is time consumption: in practical applications, methods designed for physical robots must be effective with small amounts of data. In this paper, we present a method for concurrent learning of best strategy and optimal parameters, by extending the policy gradient reinforcement learning algorithm. The results of our experimental work in a simulated environment and on a real robot show a very high convergence rate. ©2007 IEEE",An extended policy gradient algorithm for robot task learning,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/IROS.2007.4399219,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
235648671,2007-01-01T00:00:00,"This paper describes an integrated system based on open-domain and domain-specific knowledge for the purpose of providing query-based intelligent web interaction. It is understood that general purpose conversational agents are not able to answer questions on specific domain subject. On the other hand, domain specific systems lack the flexibility to handle common sense questions. To overcome the above limitations, this paper proposed an integrated system comprises of an artificial intelligent conversation software robot or chatterbot, called Artificial Intelligence Natural-language Identity (hereafter, AINI), and an Automated Knowledge Extraction Agent (AKEA) for the acquisition of real world knowledge from the Internet. The objective of AKEA is to retrieve real world knowledge or information from trustworthy websites. AINI is the mechanism used to manage the knowledge and to provide appropriate answer to the user. In this paper, we compare the performance of the proposed system against two popular search engines, two question answering systems and two other conversational systems",Query based intelligent web interaction with real world knowledge,https://core.ac.uk/download/235648671.pdf,"Ohmsha, Ltd.",,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
323090892,2015-07-10T00:00:00,"[EN] Unmanned Underwater Vehicles (UUVs) explore different habitats with a view to protecting and managing them. They are developed to overcome scientific challenges and the engineering problems caused by the unstructured and hazardous underwater environment in which they operate. Their development bears the same difficulties as the rest of service robots (hardware heterogeneity, sensor uncertainty, software complexity, etc.) as well as other particular from the domain, like the underwater environment, energy constraints, and autonomy. This article describes the AEGIR UUV, used as a test bed for implementation of control strategies and oceanographic mission in the Mar Menor area in Spain, which is one of the largest coastal lagoons in Europe. It also describes the development of a tool chain that follows a model-driven approach, which has been used in the design of the vehicle control software as well as a component-based framework that provides the runtime support of the application and enables its flexible deployment in nodes, processes and threads and pre-verification of concurrent behavior.[ES] Los vehículos submarinos no tripulados (Unmanned Underwater Vehicles, UUVs) se diseñan para misiones de monitorización, inspección e intervención. En estudios oceanográficos y de monitorización ambiental son cada vez más demandados por las innumerables ventajas que presentan con respecto a las tecnologías tradicionales. Estos vehículos son desarrollados para superar los retos científicos y los problemas de ingeniería que aparecen en el entorno no estructurado y hostil del fondo marino en el que operan. Su desarrollo no solo conlleva las mismas dificultades que el resto de los robots de servicio (heterogeneidad en el hardware, incertidumbre de los sistemas de medida, complejidad del software, etc.), sino que además se les unen las propias del dominio de aplicación, la robótica submarina: condiciones de iluminación, incertidumbre en cuanto a posición y velocidad, restricciones energéticas, etc. Este artículo describe el UUV AEGIR, un vehículo utilizado como banco de pruebas para la implementación de estrategias de control y misiones oceanográficas. También describe el desarrollo de una cadena de herramientas que sigue un enfoque dirigido por modelos, utilizada en el diseño del software de control del vehículo, así como un framework basado en componentes que proporciona el soporte de ejecución de la aplicación y permite su despliegue flexible en nodos, procesos e hilos y pre-verificación del comportamiento concurrente. Su diseño ha permitido desarrollar, comprobar y añadir los componentes que proporcionan el comportamiento necesario para que el UUV AEGIR pudiera completar con éxito distintos tipos de misiones oceanográficas.Este trabajo ha sido parcialmente financiado por el proyecto
financiado por la CICYT del Gobierno Español DIVISAMOS
(ref. DPI2009-14744-C03-02) y ViSelTR (ref. TIN2012-39279),
así como por el proyecto financiado por la Fundación Séneca de
la Región de Murcia MISSION-SICUVA (ref. 15374/PI/10) y el
proyecto “Coastal Monitoring System for the Mar Menor Coastal
Lagoon (PEPLAN 463.02-08 CLUSTER de la Región de Murcia.
Francisco Sánchez Ledesma agradece la financiación recibida por
parte del programa de becas FPU del MEC (beca AP2009-5083).
Por último, los autores quieren agradecer también a la Armada
Española la cesión del vehículo UUV y su posterior ayuda en su
reconstrucción.Ortiz, F.; Guerrero, A.; Sánchez Ledesma, F.; García Córdova, F.; Alonso, D.; Gilabert, J. (2015). Diseño del software de control de un UUV para monitorización oceanográfica usando un modelo de componentes y framework con despliegue flexible. Revista Iberoamericana de Automática e Informática industrial. 12(3):325-337. https://doi.org/10.1016/j.riai.2015.06.003OJS325337123Alonso, D., Pastor, J., Sánchez, P., Álvarez, B., Vicente-Chicote, C., 2012. Generación Automática de Software para Sistemas de Tiempo Real: Un Enfoque basado en Componentes, Modelos y Frameworks. Revista Iberoamericana de Automática e Informática Industrial RIAI. Vol, 9, Num. 2, págs 170-181, doi: 10.1016/j.riai.2012.02.010.Alonso, D., Vicente-Chicote, C., Ortiz, F., Pastor, J., Álvarez, B., 2010. V3CMM: a 3-View Component Meta-Model for Model-Driven Robotic Software Development. Journal of Software Engineering for Robotics, Vol.1, no 1, pp. 3-17.Antonelli, G., Chiaverini, S., Sarkar,N., West, M., 2001. Adaptive control of an autonomous underwater vehicle: experimental results on ODIN, IEEE Trans Control Syst. Technol., vol. 9, Issue: 5, Sep. 2001, pp. 756-765.Auke Jan Ijspeert, 2008. Central pattern generators for locomotion control in animals and robots: A review. Neural Networks, Volume 21 Issue 4, pp. 642-653.Ben-Ari, M., 2008. Principles of the Spin Model Checker. Springer-Verlag.Bengtsson, J., Yi, W., 2004. Timed Automata: Semantics, Algorithms and Tools. In: Lectures on concurrency and Petri nets, Springer-Verlag, vol. 3098, pp. 87-124.Behrmann, G., Larsen, K., Moller, O., David, A., Pettersson, P., Wang, Y., 2001. UPPAAL - present and future. Proc. of the 40th IEEE Conf. on Decision and Control.Bruyninckx, H., 2008. Robotics Software: The Future Should Be Open. In: IEEE Robotics & Automation Magazine, Vol. 15, No. 1, pp. 9-11.Carreras, M., Yuh, J., Batlle, J., Ridao, P., 2005. A behavior-based scheme Using Reinforcement Learning for Autonomous Underwater Vehicles. In: IEEE Journal of Oceanic Engineering, Vol. 30, No. 2.Chang, C., and Gaudiano, P., 1998. Application of biological learning theories to mobile robot avoidance and approach behaviors. J. Complex Systems, vol. 1, pp. 79-114.Eickstedt, D.P., Sideleau, S.R., 2009. The backseat control architecture for autonomous robotic vehicles: A case study with the Iver2 AUV. In: OCEANS 2009, MTS/IEEE Biloxi - Marine Technology for Our Future: Global and Local Challenges. 1-8.Fossen, T., 1994. Guidance and control of ocean vehicles. John Wiley and Sons Ltd.Fujii, T., Arai, Y., Asama, H., and Endo, I., 1998. Multilayered reinforcement learning for complicated collision avoidance problems. In: Proceedings IEEE International Conference on Robotics and Automation, vol. 3, pp. 2186-2191, Leuven, Belgium.García-Córdova, F., 2007. A cortical network for control of voluntary movements in a robot finger. In: Neurocomputing, vol. 71, 2007, pp. 374-391.Gonzalez, J. et al, 2012. AUV Based Multi-vehicle Colla*boration: Salinity Studies in Mar Menor Coastal Lagoon. In IFAC Workshop on Navigation, Guidance and Control of Underwater Vehicles (NGCUV).Guerrero-González, A., García-Córdova, F., Ruz-Vila, F., 2010. A Solar Powered Autonomous Mobile Vehicle for Monitoring and Surveillance Missions of Long Duration. In: International Review of Electrical Engineering, Part A, vol. 5, n. 4, pp. 1580-1587.Guerrero-González, A., García-Córdova, F., Gilabert, J., 2011. A Biologically inspired neural network for navigation with obstacle avoidance in autonomous underwater and surface vehicles. In: OCEANS 2011 IEEE. Doi. 10.1109/Oceans-Spain.2011.6003432.Medina, J., González-Harbour, M., and Drake, J., 2001. MAST real-time view: A graphic UML tool for modeling object-oriented real-time systems. Proc. of the 22nd IEEE Real-Time Systems Symposium (RTSS), December, pp. 245-256. IEEE.Pérez-Ruzafa, A., Marcos, C., Gilabert, J., 2005. The Ecology of the Mar Menor coastal lagoon: a fast-changing ecosystem under human pressure. In: Coastal lagoons. Ecosystem processes and modelling for sustainable use and development. CRC press. pp.: 392-422.Ridao, P., Yuh, J., Sugihara, K., Batlle, J., 2000. On AUV control architecture. In: Proc. Int. Conf. Robots and Systems.Ritter, H., Martinez, T., Schulten, K., 1989. Topology-conserving maps for learning visuo-motor coordination, Neural Networks, vol. 2, 1989, pp. 159-168.Schlegel, C., 2006. Communication patterns as key towards component-based robotics. In: International Journal on Advanced Robotics Systems 3 (1), 49-54.Schlegel, C., Steck, C., Lotz, A., 2011. Model-driven software development in robotics: Communication patterns as key for a robotics component model, En: Introduction to Modern Robotics. iConcept Press (ed.on-line).Stutters, L., Liu, H., Tiltman, C., Brown, D., 2008. Navigation technologies for autonomous underwater vehicles. In: IEEE Transactions on Systems, Man and Cybernetics, Part C: Applications and Reviews, Vol. 38, 581-589.OMG, 2009. UML profile for MARTE: Modeling and analysis of real-time embedded systems, formal/2009-11-02.RoSta: Robot Standards and Reference Architectures, Coordination Action (CA) funded under the European Union's Sixth Framework Programme (FP6), http://www.robot-standards.org/. Last accessed 05/2013.Singhoff, F., Plantec, A., Dissaux, P., Legrand, J., 2009. Investigating the usability of real-time scheduling theory with the cheddar project. Journal of Real Time Systems, 43, 259-295",Design of the control software of a UUV for oceanographic monitoring using a component model and framework with flexible deployment.,http://hdl.handle.net/10251/143643,'Elsevier BV',10.1016/j.riai.2015.06.003,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
323317300,2011-04-08T00:00:00,"[EN] This paper proposes a methodology for the recognition of surgical maneuvers in laparoscopic surgical interventions. The aim is to create an interface between the surgeon and a surgical robotic assistant for two arms of minimally invasive surgery procedures. The proposed interface receives information about the positioning of surgical tools of the surgeon using 3D sensors and the recognition system facilitates the current maneuver is completed. Therefore, the recognition system maneuvers that supports this interface requires a library of models of maneuvers to work. The models chosen to represent the surgical maneuvers are Hidden Markov Models. To validate the proposed methodology, we have developed a series of in-vitro experiments.[ES] Este trabajo propone una metodología para el reconocimiento de maniobras quirúrgicas en intervenciones de cirugía laparoscópica. El objetivo es la creación de un interfaz entre el cirujano y un asistente robótico quirúrgico de dos brazos para procesos de cirugía mínimamente invasiva. El interfaz propuesto recibe la información sobre el posicionado de las herramientas quirúrgicas del cirujano mediante sensores 3D y el sistema de reconocimiento facilita la maniobra actual que se ha realizado. Por lo tanto, el sistema de reconocimiento de maniobras sobre el que se apoya este interfaz necesita una librería de modelos de maniobras para trabajar. Los modelos elegidos para representar las maniobras quirúrgicas son los Modelos Ocultos de Markov. Para validar la metodología propuesta, se han desarrollado una serie de experimentos in-vitro.Estebañez, B.; Del Saz-Orozco, P.; García-Morales, I.; Muñoz, VF. (2011). Interfaz Multimodal para un Asistente Robótico Quirúrgico: Uso de Reconocimiento de Maniobras Quirúrgicas. Revista Iberoamericana de Automática e Informática industrial. 8(2):24-34. https://doi.org/10.1016/S1697-7912(11)70023-1OJS243482Baum, L. E., Petrie, T., Soules, G., & Weiss, N. (1970). A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains. The Annals of Mathematical Statistics, 41(1), 164-171. doi:10.1214/aoms/1177697196Bauzano E y otros (2009). Three-Layer Control for Active Wrists in Robotized Laparoscopic Surgery. IEEE/RSJ International Conference on Intelligent Robots and Systems, Missouri, EEUU.Brell, M., & Hein, A. (2007). Positioning Tasks in Multimodal Computer-Navigated Surgery. IEEE Multimedia, 14(4), 42-51. doi:10.1109/mmul.2007.81Butner, S. E., & Ghodoussi, M. (2003). Transforming a surgical robot for human telesurgery. IEEE Transactions on Robotics and Automation, 19(5), 818-824. doi:10.1109/tra.2003.817214Fernández J.J., “Robots para movimiento de la cámara en cirugía laparoscópica”, ETSII, Universidad de Málaga, 2002.Finlay, P. A., & Ornstein, M. H. (1995). Controlling the movement of a surgical laparoscope. IEEE Engineering in Medicine and Biology Magazine, 14(3), 289-291. doi:10.1109/51.391775Gan Q., et al., “Comparison of two measurement fusion methods for Kalman filter based multisensory data fusion”, IEEE Transactions on Aerospace and Electronic Systems, 37(1), pp. 273–280.Kalman, R. E. (1960). A New Approach to Linear Filtering and Prediction Problems. Journal of Basic Engineering, 82(1), 35-45. doi:10.1115/1.3662552Megali, G., Sinigaglia, S., Tonet, O., & Dario, P. (2006). Modelling and Evaluation of Surgical Performance Using Hidden Markov Models. IEEE Transactions on Biomedical Engineering, 53(10), 1911-1919. doi:10.1109/tbme.2006.881784Miklós, I., & Meyer, I. M. (2005). BMC Bioinformatics, 6(1), 231. doi:10.1186/1471-2105-6-231Murphy K., “Hidden Markov Model (HMM)”, Toolbox for Matlab [Online]. Disponible: http://www.ai.mit.edu/∼murphyk/Software/HMM/hmm.html.Nishikawa, A., Hosoi, T., Koara, K., Negoro, D., Hikita, A., Asano, S., … Monden, M. (2003). FAce MOUSe: a novel human-machine interface for controlling the position of a laparoscope. IEEE Transactions on Robotics and Automation, 19(5), 825-841. doi:10.1109/tra.2003.817093Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2), 257-286. doi:10.1109/5.18626Rabiner, L., & Juang, B. (1986). An introduction to hidden Markov models. IEEE ASSP Magazine, 3(1), 4-16. doi:10.1109/massp.1986.1165342Rosen J., et al., “Objective Laparoscopic Skills Assessments of Surgical Residents Using Hidden Markov Models Based on Haptic Information and Tool/Tissue Interactions”, Studies in Health Technology and Informatics - Medicine Meets Virtual Reality, Newport Beach, CA, Enero 2001.Rosen, J., Hannaford, B., Richards, C. G., & Sinanan, M. N. (2001). Markov modeling of minimally invasive surgery based on tool/tissue interaction and force/torque signatures for evaluating surgical skills. IEEE Transactions on Biomedical Engineering, 48(5), 579-591. doi:10.1109/10.918597Rosen, J., Brown, J. D., Chang, L., Sinanan, M. N., & Hannaford, B. (2006). Generalized Approach for Modeling Minimally Invasive Surgery as a Stochastic Process Using a Discrete Markov Model. IEEE Transactions on Biomedical Engineering, 53(3), 399-413. doi:10.1109/tbme.2005.869771Sackier, J. M., Wooters, C., Jacobs, L., Halverson, A., Uecker, D., & Wang, Y. (1997). Voice activation of a surgical robotic assistant. The American Journal of Surgery, 174(4), 406-409. doi:10.1016/s0002-9610(97)00128-1Satava, R. M. (1993). Virtual reality surgical simulator. Surgical Endoscopy, 7(3), 203-205. doi:10.1007/bf00594110Tsekos, N. V. (2009). MRI-guided robotics at the U of houston: EvolvingMethodologies for interventions and surgeries. 2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society. doi:10.1109/iembs.2009.5333681Viterbi, A. J. (2006). A personal history of the Viterbi algorithm. IEEE Signal Processing Magazine, 23(4), 120-142. doi:10.1109/msp.2006.1657823Wang et al., Wang Y., et al., “Automated endoscope system for optimal positioning”, Patente de invención número US5815640, Estados Unidos.Zhang, S. H., Wang, D. X., Zhang, Y. R., Wang, Y. H., Wang, Y. G., & Ma, X. P. (s. f.). The human machine interface implementation for the robot assisted endoscopic surgery system. Proceedings. 11th IEEE International Workshop on Robot and Human Interactive Communication. doi:10.1109/roman.2002.104566",Multimodal Interface for a Surgical Robotic Assistant: Surgical Maneuvers Recognition Approach,http://hdl.handle.net/10251/144524,'Elsevier BV',10.1016/S1697-7912(11)70023-1,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
195551950,2016-12-05T00:00:00,"International audienceOur demonstration presents an open-source hardware and software platform which allows non-roboticistsresearchers to conduct machine learning experiments to benchmark algorithms for autonomous explorationand active learning. In particular, in addition to showing the general properties of the platform such asits modularity and usability, we will demonstrate the online functioning of a particular algorithm whichallows efficient learning of multiple forward and inverse models and can leverage information from humanguidance. A first aspect of our demonstration is to illustrate the ease of use of the 3D printed low-costPoppy humanoid robotic platform, that allows non-roboticists to quickly set up and program roboticexperiments. A second aspect is to show how the Explauto library allows systematic comparison andevaluation of active learning and exploration algorithms in sensorimotor spaces, through a Python API toselect already implemented exploration algorithms. The third idea is to showcase Active Model Babbling,an efficient exploration algorithm dynamically choosing which task/goal space to explore and particulargoals to reach, and integrating social guidance from humans in real time to drive exploration towardsparticular objects or actions.[Forestier and Oudeyer, 2016] Forestier, S. and Oudeyer, P.-Y. (2016). Modular active curiosity-driven discovery oftool use. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, Korea.[Lapeyre et al., 2014] Lapeyre, M., Rouanet, P., Grizou, J., Nguyen, S., Depraetre, F., Le Falher, A., and Oudeyer,P.-Y. (2014). Poppy Project: Open-Source Fabrication of 3D Printed Humanoid Robot for Science, Educationand Art. In Digital Intelligence 2014, page 6, Nantes, France.[Moulin-Frier et al., 2014] Moulin-Frier, C., Rouanet, P., Oudeyer, P.-Y., and others (2014). Explauto: an open-source Python library to study autonomous exploration in developmental robotics. In ICDL-Epirob-InternationalConference on Development and Learning, Epirob","Autonomous exploration, active learning and human guidance with open-source Poppy humanoid robot platform and Explauto library",,HAL CCSD,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
160335033,2012-09-04T00:00:00Z,"Advanced autonomous robotics space missions rely heavily on the flawless interaction of complex hardware, multiple sensors, and a mission-critical software system.  This software system consists of an operating system, device drivers, controllers, and executives; recently highly complex AI-based autonomy software have also been introduced. Prior to launch, this software has to undergo rigorous verification and validation (V&V).  Nevertheless, dormant software bugs, failing sensors, unexpected hardware-software interactions, and unanticipated environmental conditions—likely on a space exploration mission—can cause major software faults that can endanger the entire mission.

Our Integrated Software Health Management (ISWHM) system continuously monitors the hardware sensors and the software in real-time. The ISWHM uses Bayesian networks, compiled to arithmetic circuits, to model software and hardware interactions. Advanced reasoning algorithms using arithmetic circuits not only enable the ISWHM to handle large, hierarchical models that are necessary in the realm of complex autonomous systems, but also enable efficient execution on small embedded processors. The latter capability is of extreme importance for small (mobile) autonomous units with limited computational power and low telemetry bandwidth.  In this paper, we discuss the requirements of ISWHM.  As our initial demonstration platform, we use a primitive Lego rover. A Lego 
Mindstorms microcontroller is used to implement a highly simplified autonomous rover driving system, running on the OSEK real-time operating system. We demonstrate that our ISWHM, running on this small embedded microcontroller, can perform fault detection as well as on-board reasoning for advanced diagnosis and root-cause detection in real time",Software and System Health Management for Autonomous Robotics Missions,,,10.1184/r1/6710654.v1,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
84962589,2014-01-01T00:00:00,"International audienceCompact Bionic Handling Assistant (CBHA) is a continuum manipulator, with pneumatic-based actuation and compliant gripper. This bionic arm is attached to a mobile robot named Robotino. Inspired by the elephant's trunk, it can reproduce biological behaviors of trunks, tentacles, or snakes. Unlike rigid link robot manipulators, the development of high performance control algorithm of continuum robot manipulators remains a challenge, particularly due to their complex mechanical design, hyper-redundancy and presence of uncertainties. Numerous studies have been investigated for modeling of such complex systems. Such continuum robots, like the CBHA present a set of nonlinearities and uncertainties, making difficult to build an accurate analytical model, which can be used for control strategies development. Hence, learning approach becomes a suitable tool in such scenarios in order to capture un-modeled nonlinear behaviors of the continuous robots. In this paper, we present a qualitative modeling approach, based on neuronal model of the inverse kinematic of CBHA. A penalty term constraint is added to the inverse objective function into Distal Supervised Learning (DSL) scheme to select one particular inverse model from the redundancy manifold. The inverse kinematic neuronal model is validated by conducting a real-time implementation on a CBHA trunk",Qualitative approach for inverse kinematic modeling of a Compact Bionic Handling Assistant trunk,,HAL CCSD,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
11478250,2003,"Mechatronic design is the integrated design of a mechanical system and its embedded control system. In order to make proper choices early in the design stage, tools are required that support modelling and simulation of physical systems––together with the controllers––with parameters that are directly related to the real-world system. Such software tools are becoming available now. Components in various physical domains (e.g. mechanical or electrical) can easily be selected from a library and combined into a ‘process’ that can be controlled by block-diagram-based (digital) controllers. A few examples will be discussed that show the use of such a tool in various stages of the design. The examples include a typical mechatronic system with a flexible transmission, a mobile robot, and an industrial linear motor with a neural-network-based learning feed-forward controller that compensates for cogging",Mechatronic design,,Pergamon,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
160343943,2014-11-24T00:00:00Z,"<p>Cameras provide a rich source of information while being passive, cheap and lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work we present the first implementation of receding horizon control, which is widely used in ground vehicles, with monocular vision as the only sensing mode for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a number of contributions: novel coupling of perception and control via relevant and diverse, multiple interpretations of the scene around the robot, leveraging recent advances in machine learning to showcase anytime budgeted cost-sensitive feature selection, and fast non-linear regression for monocular depth prediction. We empirically demonstrate the efficacy of our novel pipeline via real world experiments of more than 2 kms through dense trees with a quadrotor built from off-the-shelf parts. Moreover our pipeline is designed to combine information from other modalities like stereo and lidar as well if available</p",Vision and Learning for Deliberative Monocular Cluttered Flight,,,10.1184/r1/6561554.v1,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
37835673,2009,"In this paper, we use artificial evolution to design homogeneous neural network controller for groups of robots required to align. Aligning refers to the process by which the robots managed to head towards a common arbitrary and autonomously chosen direction starting from initial randomly chosen orientations. The cooperative interactions among robots require local communications that are physically implemented using infrared signalling. We study the performance of the evolved controllers, both in simulation and in reality for different group sizes. In addition, we analyze the most successful communication strategy developed using artificial evolution",Evolution of Neuro-Controllers for Robots\u27 Alignment using Local Communication,https://core.ac.uk/download/pdf/37835673.pdf,InTech,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
82699846,2013-12-31,"AbstractThe use of mobile robots turns out to be interesting in activities where the action of human specialist is difficult or dangerous. Mobile robots are often used for the exploration in areas of difficult access, such as rescue operations and space missions, to avoid human experts exposition to risky situations. Mobile robots are also used in agriculture for planting tasks as well as for keeping the application of pesticides within minimal amounts to mitigate environmental pollution. In this paper we present the development of a system to control the navigation of an autonomous mobile robot through tracks in plantations. Track images are used to control robot direction by pre-processing them to extract image features. Such features are then submitted to a support vector machine and an artificial neural network in order to find out the most appropriate route. A comparison of the two approaches was performed to ascertain the one presenting the best outcome. The overall goal of the project to which this work is connected is to develop a real time robot control system to be embedded into a hardware platform. In this paper we report the software implementation of a support vector machine and of an artificial neural network, which so far presented respectively around 93% and 90% accuracy in predicting the appropriate route",Comparing Support Vector Machines and Artificial Neural Networks in the Recognition of Steering Angle for Driving of Mobile Robots Through Paths in Plantations ,https://core.ac.uk/download/pdf/82699846.pdf,The Authors. Published by Elsevier B.V.,10.1016/j.procs.2013.05.187,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
300380014,2008-01-01T00:00:00,"Nonlinearities and parametric uncertainties are unavoidable problems faced in controlling robot manipulator. A single link manipulator driven by a permanent magnet brushed dc motor is a nonlinear dynamics due to effects of gravitational force, mass of the payload, posture of the manipulator and viscous friction coefficient. Furthermore, uncertainties arise because of changes of the rotor resistance with temperature and random variation of friction while operating. Due to this fact, classical PID controller can not be used effectively since it is developed based on linear system theory. In order to overcome this problem, in this research, a neural network control scheme, NARMA-L2 Control is adopted and implemented in real time for controlling a DC motor driven single link manipulator with unknown dynamics. However, the real time experimentation showed that the proposed system results in chattering of the control signal. Hence, the system also chatters within the desired trajectory. As a solution, real time Smoothed NARMA-L2 Control scheme is implemented. Physical results showed that the improved control scheme has not only reduced the chattering but has successfully controlled the single link manipulator for both point-to-point and continuous path motion control",Real time implementation of NARMA-L2 control of a Single Link Manipulator,,'Science Publications',10.3844/ajassp.2008.1642.1649,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
228881183,2016-04-30T00:00:00,"Artificial Neural Networks (ANN) can be used to solve specific problems such as prediction, classification, processing data, and robotics. Based on the exposure, this study tried to develop a system by applying ANN models Fully Recurrent Neural Network (FRNN). Fully Recurrent Neural Network structures have been presence of feedback that can make faster iteration thus making the update parameters and convergence speed become faster. The learning method used is Backpropagation Through Time. The system is implemented using the C# program. Input vectors used consisted of 7 variables.The results showedt the developed system will rapidly converge and able to achieve the most optimal error value (minimum error) when using one hidden layer with 17 units of the number of neurons. The best accuracy can be obtained using the LR of 0.001, target of 0.1and momentum 0.95, with 30 the real data test, the system accuracy reaches 83.33%",KLASIFIKASI DATA MENGGUNAKAN JARINGAN SYARAF TIRUAN MODEL FRNN (FULLY RECURRENT NEURAL NETWORK),https://core.ac.uk/download/228881183.pdf,"Fakultas Sains dan Teknik, Universitas Nusa Cendana",,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
151089656,2016-01-01T00:00:00,"Telepresence Mobile Robots have prominent attributes in many fields as it provides virtual presence for human robot interaction. The deployment of this robot in healthcare sector has improved patient care and health. The vision system in a telepresence robot allows two way audiovisual communication between people at different location. In spite of such advancement, the manual way of controlling a robot to recognise and track people during an emergency is not favourable for a long duration. To circumvent this problem, biometric method using human face is proposed in this research which is implemented on Medical Telediagnosis Robot. This paper details the design of the face recognition and tracking system with four automated modules which are motion detection, face detection, face recognition and face tracking. The modules are developed with different algorithm and tested individually to ensure the stability of the system. Artificial Intelligence technique was applied at the face recognition stage while a two degree of freedom mechanism for actuator control was used at face tracking stage. A sequential mode operation is proposed to reduce the execution time in a real-time environment. To achieve this, only one module is operated at each time. A Graphical User Interface was developed to ease the users at the local and robot environment. The system is designed in LabVIEW platform. The biometric system proposed with hybrid algorithm at each module adapts for face images detected at different distances, poses and lighting condition. This system was tested in real-time and has an execution time of 55ms and 98% accuracy. The stand alone system designed for Medical Telediagnosis Robot can be will be very fruitful for various biometric system using facial technology",Towards real-time visual biometric authentication using human face for healthcare telepresence mobile robots,https://core.ac.uk/download/151089656.pdf,,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
301030587,2002-01-01T00:00:00,"Q-Learning is a Reinforcement Learning method for solving sequential decision problems, where the utility of actions depends on a sequence of decisions and there exists uncertainty about the dynamics of the environment the agent is situated on. This general framework has allowed that Q-Learning and other Reinforcement Learning methods to be applied to a broad spectrum of complex real world problems such as robotics, industrial manufacturing, games and others. Despite its interesting properties, Q-learning is a very slow method that requires a long period of training for learning an acceptable policy. In order to solve or at least reduce this problem, we propose a parallel implementation model of Q-learning using a tabular representation and via a communication scheme based on cache. This model is applied to a particular problem and the results obtained with different processor configurations are reported. A brief discussion about the properties and current limitations of our approach is finally presented.Facultad de Informátic",A parallel implementation of Q-learning based on communication with cache,https://core.ac.uk/download/301030587.pdf,,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
203255060,2013-12-31,"AbstractThe use of mobile robots turns out to be interesting in activities where the action of human specialist is difficult or dangerous. Mobile robots are often used for the exploration in areas of difficult access, such as rescue operations and space missions, to avoid human experts exposition to risky situations. Mobile robots are also used in agriculture for planting tasks as well as for keeping the application of pesticides within minimal amounts to mitigate environmental pollution. In this paper we present the development of a system to control the navigation of an autonomous mobile robot through tracks in plantations. Track images are used to control robot direction by pre-processing them to extract image features. Such features are then submitted to a support vector machine and an artificial neural network in order to find out the most appropriate route. A comparison of the two approaches was performed to ascertain the one presenting the best outcome. The overall goal of the project to which this work is connected is to develop a real time robot control system to be embedded into a hardware platform. In this paper we report the software implementation of a support vector machine and of an artificial neural network, which so far presented respectively around 93% and 90% accuracy in predicting the appropriate route",Comparing Support Vector Machines and Artificial Neural Networks in the Recognition of Steering Angle for Driving of Mobile Robots Through Paths in Plantations ,,The Authors. Published by Elsevier B.V.,10.1016/j.procs.2013.05.187,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
214472517,2014,"This work aims at developing a motor path
generator for applications in mobile robotics based on a
chaotic neural network. The computational paradigm inspired
by the neural structure of microcircuits located in the human
prefrontal cortex is adapted to work in real-time and used
to generate the joints trajectories of a lightweight quadruped
robot. The recurrent neural network was implemented in
Matlab and a software framework was developed to test the
performances of the system with the robot dynamic model.
Preliminary results demonstrate the capability of the neural
controller to learn period signals in a short period of time
allowing adaptation during the robot operatio",A Chaotic Neural Network as Motor Path Generator for Mobile Robotics,https://core.ac.uk/download/pdf/214472517.pdf,IEEE International Conference on Robotics and Biomimetics,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
148440389,2007-01-01T00:00:00,"One primary goal in rescue robotics is to deploy a team of robots for coordinated victim search after a disaster. This requires robots to perform subtasks, such as victim detection, in real-time. Human detection by computationally cheap techniques, such as color thresholding, turn out to produce a large number of false-positives. Markov Random Fields (MRFs) can be utilized to combine the local evidence of multiple weak classifiers in order to improve the detection rate. However, inference in MRFs is computational expensive. In this paper we present a novel approach for the genetic optimizing of the building process of MRF models. The genetic algorithm determines offline relevant neighborhood relations with respect to the data, which are then utilized for generating efficient MRF models from video streams during runtime. Experimental results clearly show that compared to a Support Vector Machine (SVM) based classifier, the optimized MRF models significantly reduce the false-positive rate. Furthermore, the optimized models turned out to be up to five times faster then the non-optimized ones at nearly the same detection rate.Artificial Intelligence & Integrated Computer System",Genetic MRF Model Optimization for Real-Time Victim Detection in Search and Rescue,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/IROS.2007.4399006,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
29405426,2011-11-01T00:00:00,"Autonomy is a prime issue on robotics field and it is closely related to decision making. Last researches on decision making for social robots are focused on biologically inspired mechanisms for taking decisions. Following this approach, we propose a motivational system for decision making, using internal (drives) and external stimuli for learning to choose the right action. Actions are selected from a finite set of skills in order to keep robot's needs within an acceptable range. The robot uses reinforcement learning in order to calculate the suitability of every action in each state. The state of the robot is determined by the dominant motivation and its relation to the objects presents in its environment. The used reinforcement learning method exploits a new algorithm called Object Q-Learning. The proposed reduction of the state space and the new algorithm considering the collateral effects (relationship between different objects) results in a suitable algorithm to be applied to robots living in real environments. In this paper, a first implementation of the decision making system and the learning process is implemented on a social robot showing an improvement in robot's performance. The quality of its performance will be determined by observing the evolution of the robot's wellbeing.The funds provided by the Spanish Government through the project called “Peer
to Peer Robot-Human Interaction” (R2H), of MEC (Ministry of Science and Education), the project “A new approach to social robotics” (AROS), of MICINN (Ministry of Science and Innovation), and the RoboCity2030-II-CM project (S2009/DPI-1559), funded by Programas de Actividades I+D en la Comunidad de Madrid and cofunded by Structural Funds of the EU",Learning the selection of actions for an autonomous social robot by reinforcement learning based on motivations,https://core.ac.uk/download/29405426.pdf,'Springer Science and Business Media LLC',10.1007/s12369-011-0113-z,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
33249923,2014-09-01T00:00:00,"Technological and conceptual advances in fields such as artificial intelligence, robotics, and material science have enabled robotic building to be in the last decade prototypically implemented. In this context, robotic building implies both physically built robotic environments and robotically supported building processes, whereas reconfigurable, robotic environments incorporating sensor-actuator mechanisms that enable buildings to interact with their users and surroundings in real-time require design to production, assembly, and operation chains that may be (in part or as whole) implemented by robotic means. This paper presents and discusses research and experimental developments in robotic building implemented more recently at Hyperbody.Architectural Engineering +TechnologyArchitecture and The Built Environmen",Robotic buildings(s),,'Baltzer Science Publishers',10.7564/14-NGBJ8,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
54341271,2005-01-01T00:00:00,"In order to get real time image processing for mobile robot vision, we propose to use a discrete time Cellular Neural Network implementation by a convolutional structure on Altera FPGA using VHDL language. We obtain at least 9 times faster processing than other emulations for the same problem",Real time vision by FPGA implemented CNNs,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ECCTD.2005.1522965,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
297575572,,"Developing software for Autonomous Mobile Robot (AMR) is difficult and requires knowledge in embedded systems, real-time software issues, control theories and artificial intelligence aspects. To tackle the difficulty in developing software for AMR, many researchers have proposed the approach of reusable software component for mobile robot systems. Software pattern provides a way to reuse knowledge of expert across domain at all level of software development. In this paper component-based analysis patterns applicable to AMR software at high-level software development is proposed. Some important AMR component-based analysis patterns on AMR embedded software requirements are presented. How the analysis patterns can help in documenting two existing AMR software through pattern-based reverse engineering process is also illustrated",Application of component-based analysis patterns for pattern-based reverse engineering of mobile robot software,,"Malaysian Journal of Computer Science , University of Malaya",,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
198988799,2014-01-01T00:00:00,"International audienceCompact Bionic Handling Assistant (CBHA) is a continuum manipulator, with pneumatic-based actuation and compliant gripper. This bionic arm is attached to a mobile robot named Robotino. Inspired by the elephant's trunk, it can reproduce biological behaviors of trunks, tentacles, or snakes. Unlike rigid link robot manipulators, the development of high performance control algorithm of continuum robot manipulators remains a challenge, particularly due to their complex mechanical design, hyper-redundancy and presence of uncertainties. Numerous studies have been investigated for modeling of such complex systems. Such continuum robots, like the CBHA present a set of nonlinearities and uncertainties, making difficult to build an accurate analytical model, which can be used for control strategies development. Hence, learning approach becomes a suitable tool in such scenarios in order to capture un-modeled nonlinear behaviors of the continuous robots. In this paper, we present a qualitative modeling approach, based on neuronal model of the inverse kinematic of CBHA. A penalty term constraint is added to the inverse objective function into Distal Supervised Learning (DSL) scheme to select one particular inverse model from the redundancy manifold. The inverse kinematic neuronal model is validated by conducting a real-time implementation on a CBHA trunk",Qualitative approach for inverse kinematic modeling of a Compact Bionic Handling Assistant trunk,,HAL CCSD,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
132514837,,"The collaboration between humans and robots is one of the most disruptive and challenging research areas. Even considering advances in design and artificial intelligence, humans and robots could soon ally to perform together a number of different tasks. Robots could also became new playmates. In fact, an emerging trend is associated with the so-called phygital gaming, which builds upon the idea of merging the physical world with a virtual one in order to let physical and virtual entities, such as players, robots, animated characters and other game objects interact seamlessly as if they were all part of the same reality. This paper specifically focuses on mixed reality gaming environments that can be created by using floor projection, and tackles the issue of enabling accurate and robust tracking of off-the-shelf robots endowed with limited sensing capabilities. The proposed solution is implemented by fusing visual tracking data gathered via a fixed camera in a smart environment with odometry data obtained from robot's on-board sensors. The solution has been tested within a phygital gaming platform in a real usage scenario, by experimenting with a robotic game that exhibits many challenging situations which would be hard to manage using conventional tracking techniques",Robust robot tracking for next-generation collaborative robotics-based gaming environments,,IEEE,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
19208033,2014-01-01T00:00:00,"We introduce AutoMoDe: a novel approach to the automatic design of control software for robot swarms. The core idea in AutoMoDe recalls the approach commonly adopted in machine learning for dealing with the bias-variance tradedoff: to obtain suitably general solutions with low variance, an appropriate design bias is injected. AutoMoDe produces robot control software by selecting, instantiating, and combining preexisting parametric modules-the injected bias. The resulting control software is a probabilistic finite state machine in which the topology, the transition rules and the values of the parameters are obtained automatically via an optimization process that maximizes a task-specific objective function. As a proof of concept, we define AutoMoDe-Vanilla, which is a specialization of AutoMoDe for the e-puck robot. We use AutoMoDe-Vanilla to design the robot control software for two different tasks: aggregation and foraging. The results show that the control software produced by AutoMoDe-Vanilla (i) yields good results, (ii) appears to be robust to the so called reality gap, and (iii) is naturally human-readable. © 2014 Springer Science+Business Media New York.SCOPUS: ar.jinfo:eu-repo/semantics/publishe",AutoMoDe: A novel approach to the automatic design of control software for robot swarms,,'Springer Science and Business Media LLC',10.1007/s11721-014-0092-4,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
36204612,2007-01-01T00:00:00,"One primary goal in rescue robotics is to deploy a team of robots for coordinated victim search after a disaster. This requires robots to perform subtasks, such as victim detection, in real-time. Human detection by computationally cheap techniques, such as color thresholding, turn out to produce a large number of false-positives. Markov Random Fields (MRFs) can be utilized to combine the local evidence of multiple weak classifiers in order to improve the detection rate. However, inference in MRFs is computational expensive. In this paper we present a novel approach for the genetic optimizing of the building process of MRF models. The genetic algorithm determines offline relevant neighborhood relations with respect to the data, which are then utilized for generating efficient MRF models from video streams during runtime. Experimental results clearly show that compared to a Support Vector Machine (SVM) based classifier, the optimized MRF models significantly reduce the false-positive rate. Furthermore, the optimized models turned out to be up to five times faster then the non-optimized ones at nearly the same detection rate.Artificial Intelligence & Integrated Computer System",Genetic MRF Model Optimization for Real-Time Victim Detection in Search and Rescue,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/IROS.2007.4399006,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
148452645,2011-01-01T00:00:00,"In critical domains such as urban search and rescue (USAR), and bomb disposal, the deployment of teleoperated robots is essential to reduce the risk of first responder personnel. Teleoperation is a difficult task, particularly when controlling robots from an isolated safety zone. In general, the operator has to solve simultaneously the problems of mission planning, target identification, robot navigation, and robot control. We introduce a system to support teleoperated navigation with real-time mapping consisting of a two-step scan matching method that re-considers data associations during the search. The algorithm processes data from laser range finder and gyroscope only, thereby it is independent from the robot platform. Furthermore, we introduce a user-guided procedure for improving the global consistency of maps generated by the scan matcher. Globally consistent maps are computed by a graph-based maximum likelihood method that is biased by localizing crucial parts of the scan matcher trajectory on a prior given geo-tiff image. The approach has been implemented as an embedded system and extensively tested on robot platforms designed for teleoperation in critical situations, such as bomb disposal. Furthermore, the system was evaluated in a test maze by first responders during the Disaster City event in Texas 2008.Artificial Intelligence & Integrated Computer System",Mapping for the Support of First Responders in Critical Domains,,'Springer Science and Business Media LLC',10.1007/s10846-010-9520-x,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
82439102,2016-12-31,"AbstractThe effort to develop an electronic skin is highly motivated by many application domains namely robotics, biomedical instrumentations, and replacement prosthetic devices. Several e-skin systems have been proposed recently and have demonstrated the need of an embedded electronic system for tactile data processing either to mimic the human skin or to respond to the application demands. Processing tactile data requires efficient methods to extract meaningful information from raw sensors data.In this framework, our goal is the development of a dedicated embedded electronic system for electronic skin. The embedded electronic system has to acquire the tactile data, process and extract structured information. Machine Learning (ML) represents an effective method for data analysis in many domains: it has recently demonstrated its effectiveness in processing tactile sensors data.This paper presents an embedded electronic system based on dedicated hardware implementation for electronic skin systems. It provides a Tensorial kernel function implementation for machine learning based on Tensorial kernel approach. Results assess the time latency and the hardware complexity for real time functionality. The implementation results highlight the high amount of power consumption needed for the input touch modalities classification task. Conclusions and future perspectives are also presented",Embedded Electronic System Based on Dedicated Hardware DSPs for Electronic Skin Implementation ,https://core.ac.uk/download/pdf/82439102.pdf,The Author(s). Published by Elsevier Ltd.,10.1016/j.protcy.2016.08.007,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
36204603,2009-01-01T00:00:00,"Teleoperation is a difficult task, particularly when controlling robots from an isolated operator station. In general, the operator has to solve nearly blindly the problems of mission planning, target identification, robot navigation, and robot control at the same time. The goal of the proposed system is to support teleoperated navigation with real-time mapping. We present a novel scan matching technique that re-considers data associations during the search, enabling robust pose estimation even under varying roll and pitch angle of the robot enabling mapping on rough terrain. The approach has been implemented as an embedded system and extensively tested on robot platforms designed for teleoperation in critical situations, such as bomb disposal. Furthermore, the system has been evaluated in a test maze by first responders during the Disaster City event in Texas 2008. Finally, experiments conducted within different environments show that the system yields comparably accurate maps in real-time when compared to higher sophisticated offline methods, such as Rao-Blackwellized SLAM.(Best Paper Award Finalist)Artificial Intelligence & Integrated Computer System",Operator-Assistive Mapping in Harsh Environments,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/SSRR.2009.5424159,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
11994409,2009-03-01T00:00:00,"In this paper, we use artificial evolution to design homogeneous neural network controller for groups of robots required to align. Aligning refers to the process by which the robots managed to head towards a common arbitrary and autonomously chosen direction starting from initial randomly chosen orientations. The cooperative interactions among robots require local communications that are physically implemented using infrared signalling. We study the performance of the evolved controllers, both in simulation and in reality for different group sizes. In addition, we analyze the most successful communication strategy developed using artificial evolution",Evolution of Neuro-Controllers for Robots Alignment using Local Communication,http://oa.upm.es/5383/,E.T.S.I. Telecomunicación (UPM),,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
132585674,2016-09-06T00:00:00,"Unmanned Aerial Vehicles are currently investigated as an important sub-domain of robotics, a fast growing and truly multidisciplinary research field. UAVs are increasingly deployed in real-world settings for missions in dangerous environments or in environments which are challenging to access. Combined with autonomous flying capabilities, many new possibilities, but also challenges, open up. To overcome the challenge of early identification of degradation, machine learning based on flight features is a promising direction. Existing approaches build classifiers that consider their features to be correlated. This prevents a fine-grained detection of degradation for the different hardware components. This work presents an approach where the data is considered uncorrelated and, using machine learning
techniques, allows the precise identification of UAV’s damages",UAV degradation identification for pilot notification using machine learning techniques,https://core.ac.uk/download/132585674.pdf,,10.1109/etfa.2016.7733537,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
196618218,2015-01-01T00:00:00,"Complex real problems increasingly require intelligent systems that combine knowledge, techniques and methodologies from various sources. Intelligent systems based on artificial intelligence techniques that are associated with the behavior of people can perform the processes of learning, reasoning and solving all kinds of problems. Such systems, which automatically can perform tasks set by the user or other software, today thankfully called intelligent agents. Independent, intelligent agents on the Internet can be very successful to perform some search work on behalf of and for the needs of different users. For efficient collection, manipulation and management of data, such software can be very interesting from the standpoint of intelligent data analysis in many areas the police. Analysis of the data collected by an intelligent agent (a software robot-bot) can be successfully utilized, among many jobs in the police, and in the field of crime and in particular manifestation of cyber­crime, traffic safety, emergencies, etc. To make the collection and analysis of data from criminal activities on the Internet effective, it is necessary to examine the existing artificial intelligence techniques to be used for the conclusion of the intelligent agents. On the other hand, using of methods of artificial intelligence in finding data along with intelligent data analysis (data mining) should be used, which has found wide use in the area of business, economics, mechanics, medicine, genetics, transport etc.Kompleksni realni problemi sve češće zahtevaju inteligentne sisteme koji kombinuju znanje, tehnike i metodologije iz različitih izvora. Inteligentni sistemi bazirani na tehnikama veštačke inteligencije koje asociraju na ponašanje ljudi mogu da obavljaju procese učenja, zaključivanja i rešavanje raznovrsnih problema. Ovakvi sistemi, koji automatski mogu da izvrše zadatke zadate od strane korisnika ili drugih softvera, danas se sreću pod imenom inteligentni agenti. Samostalno, inteligentni agenti na Internetu mogu veoma uspešno da izvode neki pretraživački posao u ime i za potrebe raznih korisnika. Zbog efikasnog sakupljanja, manipulisanja i upravljanja podacima, ovakvi softveri mogu biti veoma interesantni sa stanovišta inteligentne analize podataka u mnogim oblastima policije. Analiza podataka sakupljenih od strane inteligentnog agenta (softverskog robota - bota) može se uspešno iskoristiti, između mnogih poslova u policiji, i na polju kriminala i naročito pojavnog oblika sajber kriminala, bezbednosti saobraćaja, vanrednih situacija itd. Kako bi sakupljanje i analiza podataka iz kriminalnih aktivnosti na Internetu bila efikasna, neophodno je sagledati postojeće tehnike veštačke inteligencije koje se koriste za zaključivanje u inteligentnim agentima. S druge strane, treba iskoristiti metode veštačke inteligencije u pronalaženju podataka pri inteligentnoj analizi podataka (data mining-u) koja je našla široku primenu u oblasti poslovanja preduzeća, ekonomije, mehanike, medicine, genetike, saobraćaja i sl",Veštačka intelegencija u prikupljanju i analizi podataka u policiji,https://core.ac.uk/download/196618218.pdf,'Centre for Evaluation in Education and Science (CEON/CEES)',10.5937/NBP1503131K,"[{'title': 'Nauka bezbednost policija', 'identifiers': ['issn:0354-8872', '0354-8872']}]",core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
154907505,2011-01-01T00:00:00,"One of the main and recent problem in developing countries like Malaysia is lack of surgeon or specialists,

especially in rural areas. Insufficient specialized surgeons in such regions particularly in the niche of orthopedic, causes more fatalities and loss of limbs due to time and distance constrain in attending the patients. A mobile robotic system known as OTOROB (Orthopedic Robot) is designed and developed to aid surgeons to virtually present at such areas for attending patients in order to make life saving decisions. The developed mobile robotic platform is integrated with a flexible robotic arm vision system to be controlled remotely by the remote surgeon to obtain visual inspection on the patients. Fuzzy logic control is implemented in the control system as Artificial intelligence (AI) to provide safety features for the robotic arm articulation. The safety system of the robotic arm consists of Danger Monitoring System (DMS) and Obstacle Avoidance System (OAS). The experiments conducted on DMS shows that the DMS capable of conveying danger level surrounding the robotic arm to the user through GUI with warning indication and obstacle position. While, OAS developed, responded to the mobile and static obstacle around the robotic arm. The robotic arm is capable of avoiding approaching obstacle autonomously via fuzzy control. The smooth control of robotic arm coupled with safety routines improved the overall articulation of the robotic arm. The safety oriented flexible robotic arm system of OTOROB able to deliver reliable and convenient for both remote doctor and patient in real time emergency circumstances",Safety system for non-interventional flexible robotic arm of  Orthopaedic Robot (OTOROB) using fuzzy logic,https://core.ac.uk/download/154907505.pdf,'Scientific and Academic Publishing',10.5923/j.ajis.20110101.04,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
352137461,2009-07-07T00:00:00,"The Association for the Advancement of Artificial Intelligence, in cooperation with Stanford University's Department of Computer Science, was pleased to present the 2009 Spring Symposium Series, held Monday through Wednesday, March 23–25, 2009 at Stanford University. The titles of the nine symposia were Agents that Learn from Human Teachers, Benchmarking of Qualitative Spatial and Temporal Reasoning Systems, Experimental Design for Real-World Systems, Human Behavior Modeling, Intelligent Event Processing, Intelligent Narrative Technologies II, Learning by Reading and Learning to Read, Social Semantic Web: Where Web 2.0 Meets Web 3.0, and Technosocial Predictive Analytics. The goal of the Agents that Learn from Human Teachers was to investigate how we can enable software and robotics agents to learn from real-time interaction with an everyday human partner. The aim of the Benchmarking of Qualitative Spatial and Temporal Reasoning Systems symposium was to initiate the development of a problem repository in the field of qualitative spatial and temporal reasoning and identify a graded set of challenges for future midterm and long-term research. The Experimental Design symposium discussed the challenges of evaluating AI systems. The Human Behavior Modeling symposium explored reasoning methods for understanding various aspects of human behavior, especially in the context of designing intelligent systems that interact with humans. The Intelligent Event Processing symposium discussed the need for more AI-based approaches in event processing and defined a kind of research agenda for the field, coined as intelligent complex event processing (iCEP). The Intelligent Narrative Technologies II AAAI symposium discussed innovations, progress, and novel techniques in the research domain. The Learning by Reading and Learning to Read symposium explored two aspects of making natural language texts semantically accessible to, and processable by, machines. The Social Semantic Web symposium focused on the real-world grand challenges in this area. Finally, the Technosocial Predictive Analytics symposium explored new methods for anticipatory analytical thinking that provide decision advantage through the integration of human and physical models",Reports of the AAAI 2009 Spring Symposia,,'Association for the Advancement of Artificial Intelligence (AAAI)',10.1609/aimag.v30i3.2253,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
290044901,2008,"One of the areas that needs further improvement
within E-Learning environments via Internet (A big effort is
required in this area if progress is to be made) is allowing students
to access and practice real experiments in a real laboratory,
instead of using simulations [1]. Real laboratories allow students
to acquire methods, skills and experience related to real
equipment, in a manner that is very close to the way they are
being used in industry. The purpose of the project is the study,
development and implementation of an E-Learning environment
to allow undergraduate students to practice subjects related to
Robotics and Artificial Intelligence. The system, which is now at a
preliminary stage, will allow the remote experimentation with real
robotic devices (i.e. robots, cameras, etc.). It will enable the
student to learn in a collaborative manner (remote participation
with other students) where it will be possible to combine the onsite
activities (performed “in-situ” within the real lab during the
normal practical sessions), with the “on-line” one (performed
remotely from home via the Internet). Moreover, the remote
experiments within the E-Laboratory to control the real robots
can be performed by both, students and even scientist. This
project is under development and it is carried out jointly by two
Universities (UPC and UJI). In this article we present the system
architecture and the way students and researchers have been able
to perform a Remote Programming of Multirobot Systems via web",Remote Programming of Multirobot Systems within the UPC-UJI Telelaboratories: System Architecture and Agent-Based Multirobot Control,,Westing Publishing Co.,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
226329121,2012-01-01,"An inverse optimal neural controller for discrete-time unknown nonlinear systems, in the presence of external disturbances and parameter uncertainties, is presented. It is based on a discrete-time recurrent high-order neural network trained with an extended Kalman filter-based algorithm. The applicability of the proposed approach is first tested via simulations for an electrically driven nonholonomic mobile robot, and finally, the proposed methodology is implemented on real time. Copyright © 2012 John Wiley & Sons, Ltd",Discrete-time neural control for electrically driven nonholonomic mobile robots,,'Wiley',10.1002/acs.2289,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
54278558,2010-01-01T00:00:00,"In real-world robotic applications, many factors, both at low level (e.g., vision, motion control and behaviors) and at high level (e.g., plans and strategies) determine the quality of the robot performance. Consequently, fine tuning of the parameters, in the implementation of the basic functionalities, as well as in the strategic decisions, is a key issue in robot software development. In recent years, machine learning techniques have been successfully used to find optimal parameters for typical robotic functionalities. However, one major drawback of learning techniques is time consumption: in practical applications, methods designed for physical robots must be effective with small amounts of data. In this paper, we present a method for concurrent learning of best strategy and optimal parameters using policy gradient reinforcement learning algorithm. The results of our experimental work in a simulated environment and on a real robot show a very high convergence rate. (C) 2010 Elsevier B.V. All rights reserved",Policy gradient learning for quadruped soccer robots,,'Elsevier BV',10.1016/j.robot.2010.03.008,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
55177515,1994-01-01T00:00:00,"Learning plays a vital role in the development of situated agents. In this paper, we explore the use of reinforcement learning to ""shape"" a robot to perform a predefined target behavior. We connect both simulated and real robots to Alecsys, a parallel implementation of a learning classifier system with an extended genetic algorithm. After classifying different kinds of Animat-like behaviors, we explore the effects on learning of different types of agent's architecture (monolithic, flat and hierarchical) and of training strategies. In particular, hierarchical architecture requires the agent to learn how to coordinate basic learned responses. We show that the best results are achieved when both the agent's architecture and the training strategy match the structure of the behavior pattern to be learned. We report the results of a number of experiments carried out both in simulated and in real environments, and show that the results of simulations carry smoothly to real robots. While most of our experiments deal with simple reactive behavior, in one of them we demonstrate the use of a simple and general memory mechanism. As a whole, our experimental activity demonstrates that classifier systems with genetic algorithms can be practically employed to develop autonomous agent",Robot shaping: Developing autonomous agents through learning,,"Elsevier BV:PO Box 211, 1000 AE Amsterdam Netherlands:011 31 20 4853757, 011 31 20 4853642, 011 31 20 4853641, EMAIL: nlinfo-f@elsevier.nl, INTERNET: http://www.elsevier.nl, Fax: 011 31 20 4853598",10.1016/0004-3702(94)90047-7,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
196689363,2011-11-01T00:00:00,"Autonomy is a prime issue on robotics field and it is closely related to decision making. Last researches on decision making for social robots are focused on biologically inspired mechanisms for taking decisions. Following this approach, we propose a motivational system for decision making, using internal (drives) and external stimuli for learning to choose the right action. Actions are selected from a finite set of skills in order to keep robot's needs within an acceptable range. The robot uses reinforcement learning in order to calculate the suitability of every action in each state. The state of the robot is determined by the dominant motivation and its relation to the objects presents in its environment. The used reinforcement learning method exploits a new algorithm called Object Q-Learning. The proposed reduction of the state space and the new algorithm considering the collateral effects (relationship between different objects) results in a suitable algorithm to be applied to robots living in real environments. In this paper, a first implementation of the decision making system and the learning process is implemented on a social robot showing an improvement in robot's performance. The quality of its performance will be determined by observing the evolution of the robot's wellbeing.The funds provided by the Spanish Government through the project called “Peer
to Peer Robot-Human Interaction” (R2H), of MEC (Ministry of Science and Education), the project “A new approach to social robotics” (AROS), of MICINN (Ministry of Science and Innovation), and the RoboCity2030-II-CM project (S2009/DPI-1559), funded by Programas de Actividades I+D en la Comunidad de Madrid and cofunded by Structural Funds of the EU",Learning the selection of actions for an autonomous social robot by reinforcement learning based on motivations,,'Springer Science and Business Media LLC',10.1007/s12369-011-0113-z,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
54355579,2004-01-01T00:00:00,"The activities of search and rescue of victims in large-scale disasters are not only highly relevant social Problems, but pose several challenges from a scientific standpoint. In this context, the RoboCup-Rescue project focused on the problems of bringing aids immediately after a large disaster, and aims at creating system based on AI and Robotics technologies, where heterogeneous agents (software, robots, human beings) interact in a cooperative manner. In this paper we present the achievements of a research project, based on the RoboCup Rescue simulator, carried out in Italy in collaboration with the Italian Fire Department. The overall project goal is to devise tools to allow monitoring and supporting decisions which are needed in a real-time rescue operation in a large scale disaster, and to provide a methodology for evaluation of multi-agent system which considers not only the efficiency of a system, but also its robustness when conditions in the environment change, as well as other features, such as the ability to acquire a precise and coherent representation of the disaster scenario",RoboCup Rescue simulation: Methodologies tools and evaluation for practical applications,,'Springer Science and Business Media LLC',10.1007/978-3-540-25940-4_62,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
148655700,2009-03-01T00:00:00,"In this paper, we use artificial evolution to design homogeneous neural network controller for groups of robots required to align. Aligning refers to the process by which the robots managed to head towards a common arbitrary and autonomously chosen direction starting from initial randomly chosen orientations. The cooperative interactions among robots require local communications that are physically implemented using infrared signalling. We study the performance of the evolved controllers, both in simulation and in reality for different group sizes. In addition, we analyze the most successful communication strategy developed using artificial evolution",Evolution of Neuro-Controllers for Robots Alignment using Local Communication,https://core.ac.uk/download/148655700.pdf,E.T.S.I. Telecomunicación (UPM),,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
45870151,2010-05,"Online model learning in real-time is required by many applications such as in robot tracking control. It poses a difficult problem, as fast and incremental online regression with large data sets is the essential component which cannot be achieved by straightforward usage of off-the-shelf machine learning methods (such as Gaussian process regression or support vector regression). In this paper, we propose a framework for online, incremental sparsification with a fixed budget designed for large scale real-time model learning. The proposed approach combines a sparsification method based on an independence measure with a large scale database. In combination with an incremental learning approach such as sequential support vector regression, we obtain a regression method which is applicable in real-time online learning. It exhibits competitive learning accuracy when compared with standard regression techniques. Implementation on a real robot emphasizes the applicability of the proposed approach in real-time online model learning for real world systems",Incremental Sparsification for Real-time Online Model Learning,,,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
203489970,2015-12-31,"AbstractIn this paper a revised reinforcement learning method is presented for stability control problems with real-value inputs and outputs. The revised eXtended Classifier System for Real-input and Real-output (XCSRR) controller is designed, which is capable of working at fully real-value environment such as stability control of robots. XCSRR is a novel approach to enhance the performance of classifier systems for more practical problems than systems with merely binary behaviour. As a case study, we use XCSRR to control the stability of a biped robot, which is subjected to unknown external forces that would disturb the robot equilibrium. The external forces and the dynamics of the upper body of the biped robot are modelled in MATLAB software to train the XCSRR controller. Theoretical and experimental results of the learning behaviour and the performance of stability control on the robot demonstrate the strength and efficiency of the proposed new approach",An Improved eXtended Classifier System for the Real-time-input Real-time-output (XCSRR) Stability Control of a Biped Robot ,,Published by Elsevier B.V.,10.1016/j.procs.2015.09.198,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
163024812,2016-12-05T00:00:00,"International audienceOur demonstration presents an open-source hardware and software platform which allows non-roboticistsresearchers to conduct machine learning experiments to benchmark algorithms for autonomous explorationand active learning. In particular, in addition to showing the general properties of the platform such asits modularity and usability, we will demonstrate the online functioning of a particular algorithm whichallows efficient learning of multiple forward and inverse models and can leverage information from humanguidance. A first aspect of our demonstration is to illustrate the ease of use of the 3D printed low-costPoppy humanoid robotic platform, that allows non-roboticists to quickly set up and program roboticexperiments. A second aspect is to show how the Explauto library allows systematic comparison andevaluation of active learning and exploration algorithms in sensorimotor spaces, through a Python API toselect already implemented exploration algorithms. The third idea is to showcase Active Model Babbling,an efficient exploration algorithm dynamically choosing which task/goal space to explore and particulargoals to reach, and integrating social guidance from humans in real time to drive exploration towardsparticular objects or actions.[Forestier and Oudeyer, 2016] Forestier, S. and Oudeyer, P.-Y. (2016). Modular active curiosity-driven discovery oftool use. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, Korea.[Lapeyre et al., 2014] Lapeyre, M., Rouanet, P., Grizou, J., Nguyen, S., Depraetre, F., Le Falher, A., and Oudeyer,P.-Y. (2014). Poppy Project: Open-Source Fabrication of 3D Printed Humanoid Robot for Science, Educationand Art. In Digital Intelligence 2014, page 6, Nantes, France.[Moulin-Frier et al., 2014] Moulin-Frier, C., Rouanet, P., Oudeyer, P.-Y., and others (2014). Explauto: an open-source Python library to study autonomous exploration in developmental robotics. In ICDL-Epirob-InternationalConference on Development and Learning, Epirob","Autonomous exploration, active learning and human guidance with open-source Poppy humanoid robot platform and Explauto library",https://core.ac.uk/download/163024812.pdf,HAL CCSD,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
295539554,2011-12-01T00:00:00,"This paper presents a novel pattern recognition algorithm that use weightless neural network (WNNs) technique.This technique plays a role of situation classifier to judge the situation around the mobile robot environment and makes control decision in mobile robot navigation. The WNNs technique is choosen due to significant advantages over conventional neural network, such as they can be easily implemented in hardware using standard RAM, faster in training phase and work with small resources. Using a simple classification algorithm, the similar data will be grouped with each other and it will be possible to attach similar data classes to specific local areas in the mobile robot environment. This strategy is demonstrated in simple mobile robot powered by low cost microcontrollers with 512 bytes of RAM and low cost sensors. Experimental result shows, when number of neuron increases the average environmental recognition ratehas risen from 87.6% to 98.5%.The WNNs technique allows the mobile robot to recognize many and different environmental patterns and avoid obstacles in real time. Moreover, by using proposed WNNstechnique mobile robot has successfully reached the goal in dynamic environment compare to fuzzy logic technique and logic function, capable of dealing with uncertainty in sensor reading, achieving good performance in performing control actions with 0.56% error rate in mobile robot speed",A New Classification Technique in Mobile Robot Navigation,https://core.ac.uk/download/295539554.pdf,'Universitas Ahmad Dahlan',10.12928/telkomnika.v9i3.736,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
79326814,2014-09-01T00:00:00,"Technological and conceptual advances in fields such as artificial intelligence, robotics, and material science have enabled robotic building to be in the last decade prototypically implemented. In this context, robotic building implies both physically built robotic environments and robotically supported building processes, whereas reconfigurable, robotic environments incorporating sensor-actuator mechanisms that enable buildings to interact with their users and surroundings in real-time require design to production, assembly, and operation chains that may be (in part or as whole) implemented by robotic means. This paper presents and discusses research and experimental developments in robotic building implemented more recently at Hyperbody",Robotic buildings(s),,'Baltzer Science Publishers',,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
302886582,2017-07-24T00:00:00,"Inverse Reinforcement Learning (IRL) for path
planning enables robots to learn cost functions for difficult tasks
from demonstration, instead of hard-coding them. However,
IRL methods face practical limitations that stem from the need
to repeat costly planning procedures. In this paper, we propose
Rapidly Exploring Learning Trees (RLT∗
), which learns the cost
functions of Optimal Rapidly Exploring Random Trees (RRT∗
)
from demonstration, thereby making inverse learning methods
applicable to more complex tasks. Our approach extends
Maximum Margin Planning to work with RRT∗
cost functions.
Furthermore, we propose a caching scheme that greatly reduces
the computational cost of this approach. Experimental results
on simulated and real-robot data from a social navigation
scenario show that RLT∗
achieves better performance at lower
computational cost than existing methods. We also successfully
deploy control policies learned with RLT∗
on a real telepresence
robot",Rapidly exploring learning trees,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ICRA.2017.7989184,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
290312192,2016,"This paper presents control framework based
on multi-agent reinforcement approach for building
intellectual steering software for multi-wheeled mobile
robots. The framework uses modified reinforcement
learning approach based on special multi-agent structure
with virtual leader. The framework application example
will be shown with real multi-wheeled mobile platform.
The experiments were performed in simulation
environment with accurate virtual model",Multi-agent control framework for multi-wheeled mobile platforms,,Minsk: Publishing Center of BSU,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
80311420,2017-01-01T00:00:00,"The use of identical robots in the RoboCup Standard Platform League (SPL) made software development the key aspect to achieve good results in competitions. In particular, the visual detection process is crucial for extracting information about the environment. In this paper, we present a novel approach for object detection and classification based on Convolutional Neural Networks (CNN). The approach is designed to be used by NAO robots and is made of two stages: image region segmentation, for reducing the search space, and Deep Learning, for validation. The proposed method can be easily extended to deal with different objects and adapted to be used in other RoboCup leagues. Quantitative experiments have been conducted on a data set of annotated images captured in real conditions from NAO robots in action. The used data set is made available for the community. © 2017, Springer International Publishing AG",A Deep Learning Approach for Object Recognition with NAO Soccer Robots,https://core.ac.uk/download/80311420.pdf,'Springer Science and Business Media LLC',10.1007/978-3-319-68792-6_33,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
71504493,2007-01-01T00:00:00,"ABSTRACT 

Using the distributed artificial intelligence and the distribuited robotics as a frame of reference, particularly the RoboCup World Championship, in this work and architecture for multiple mobile, autonomous, rational and coordinated agents is proposed. This architecture is composed of a mechanism of reasoning or automatic decision making, a functional and control structure, an artificial vision system, a navigation system, and a robotic architecture for a team of small minibots with capacity of acting at the Small Size League. In the design, development and construction of the components are coupled methods and techniques of Emergent Computation with models based on the traditional knowledge representation of Artificial Intelligence, producing algorithms that combine properties as adaptibility, robustness, uniformity with representation, inference and universality. The Agents are conformed by mechanisms that integrate sensors out of the body of the robot with effectors embedded in the robot and processes behaviors which are exhibited by the minibots while they operate in real time. The execution and performance of the architecture proposed had been experimented and evaluated through the implementation of a Multiagent System thar allows the operation of one or more robots on a testbed conformed by a football field made to comply with RoboCup rules. Key 
Words-
Multiagent 
System, 
Distributed 
Robotics, 
RoboCup  
RESUMEN

Tomando como marco de referencia a la Inteligencia Artificial Distribuida y la Rob\uf3tica Distribuida, particularmente a la Copa Mundial de F\ufatbol de Robots RoboCup, en este trabajo se propone una arquitectura para m\ufaltiples agentes m\uf3viles, aut\uf3nomos, racionales, coordinados, la cual comprende un modelo de organizaci\uf3n para los agentes, una estructura funcional y de control para cada uno de los agentes, un mecanismo de razonamiento o toma de decisiones autom\ue1ticas, un sistema de visi\uf3n artificial, un sistema de navegaci\uf3n y una arquitectura para un equipo de peque\uf1os minibots con capacidad de actuaci\uf3n en la Liga de Peque\uf1os Robots. En el dise\uf1o, desarrollo y construcci\uf3n de estos componentes se acoplan m\ue9todos y t\ue9cnicas clasificadas dentro de Computaci\uf3n Emergente con modelos basados en la representaci\uf3n tradicional de la Inteligencia Artificial, dando lugar a algoritmos que combinan las propiedades de adaptabilidad, robustez y uniformidad con la representaci\uf3n, inferencia y la universalidad. Los Agentes est\ue1n conformados por mecanismos que integran sensores ubicados fuera del cuerpo del robot con efectores instalados en el robot y procesos que se ejecutan en forma conjunta sobre diferentes dispositivos de c\uf3mputo, dot\ue1ndolos de comportamientos competitivos y cooperativos los cuales son exhibidos por los minibots al operar en tiempo real. La ejecuci\uf3n y rendimiento de la arquitectura propuesta ha sido experimentada y evaluada mediante la implementaci\uf3n de un Sistema Multiagentes que permite la operaci\uf3n de uno o m\ue1s robots en un ambiente de trabajo constituido por un campo de f\ufatbol, construido de acuerdo a las normativas de RoboCup.Palabras 
Claves del Autor:
Sistemas 
Multiagentes, 
Rob\uf3tica 
Distribuida, 
Robocup. <br","ARQUITECTURA MULTIAGENTES PARA UN EQUIPO DE PEQUE
1OS ROBOTS",https://core.ac.uk/download/71504493.pdf,,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
41213247,2011-01-01T00:00:00,"In critical domains such as urban search and rescue (USAR), and bomb disposal, the deployment of teleoperated robots is essential to reduce the risk of first responder personnel. Teleoperation is a difficult task, particularly when controlling robots from an isolated safety zone. In general, the operator has to solve simultaneously the problems of mission planning, target identification, robot navigation, and robot control. We introduce a system to support teleoperated navigation with real-time mapping consisting of a two-step scan matching method that re-considers data associations during the search. The algorithm processes data from laser range finder and gyroscope only, thereby it is independent from the robot platform. Furthermore, we introduce a user-guided procedure for improving the global consistency of maps generated by the scan matcher. Globally consistent maps are computed by a graph-based maximum likelihood method that is biased by localizing crucial parts of the scan matcher trajectory on a prior given geo-tiff image. The approach has been implemented as an embedded system and extensively tested on robot platforms designed for teleoperation in critical situations, such as bomb disposal. Furthermore, the system was evaluated in a test maze by first responders during the Disaster City event in Texas 2008.Artificial Intelligence & Integrated Computer System",Mapping for the Support of First Responders in Critical Domains,,'Springer Science and Business Media LLC',10.1007/s10846-010-9520-x,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
323141410,2006-04-01T00:00:00,"An autonomous mobil robot has been implemented on a Digital Signal Processor (for real time operation) using neural networks as the main part of the program that runs on the processor. The neural network was based on a single layer perceptron (SLP) with two neurons, four inputs coming from four sonar sensors, and two outputs to control the direction of two CD motors. The goal of the mobil robot is to avoid obstacles while it runs randomly in a given room. Two training sets were tested to provide two di®erent reactive behaviors, the first one with forward direction, left turn and right turn, and the second which includes reverse direction. Both
behaviors were compared",Implementation of a neuron-based autonomous Mobil Robot on a DSP,https://core.ac.uk/download/323141410.pdf,'Universidad Autonoma de Zacatecas - Francisco Garcia Salinas',,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
80378725,2016-09-06T00:00:00,"International audienceThis paper presents an artificial neural network-based control architecture allowing autonomous mobile robot indoor navigation by emulating the cognition process of a human brain when navigating in an unknown environment. The proposed architecture is based on a simultaneous top-down and bottom up approach, which combines the a priori knowledge of the environment gathered from a previously examined floor plan with the visual information acquired in real time. Thus, in order to take the right decision during navigation, the robot is able to process both set of information, compare them in real time and react accordingly. The architecture is composed of two modules: a) A deliberative module, corresponding to the processing chain in charge of extracting a sequence of navigation signs expected to be found in the environment, generating an optimal path plan to reach the goal,computing and memorizing the sequence of signs [1]. The path planning stage allowing the computation of the sign sequence is based on a neural implementation of the resistive grid. b) A reactive module, integrating the said sequence information in order to use it to control online navigation and learning sensory-motor associations. It follows a perception-action mechanism that constantly evolves because of the dynamic interaction between the robot and its environment. It is composed of three layers: one layer using a cognitive mechanism and the other two using a reflex mechanism. Experimental results obtained from the physical implementation of the architecture in an indoor environment show the feasibility of this approach",Artificial Neural Network-Based Control Architecture: a Simultaneous Top-down and Bottom-up Approach to Autonomous Robot Navigation,,'Springer Science and Business Media LLC',10.1007/978-3-319-44778-0,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
127503327,2017,"Background: In maize, silks are hundreds of filaments that simultaneously emerge from the ear for collecting pollen over a period of 1–7 days, which largely determines grain number especially under water deficit. Silk growth is a major trait for drought tolerance in maize, but its phenotyping is difficult at throughputs needed for genetic analyses.
Results: We have developed a reproducible pipeline that follows ear and silk growths every day for hundreds of plants, based on an ear detection algorithm that drives a robotized camera for obtaining detailed images of ears and silks. We first select, among 12 whole-plant side views, those best suited for detecting ear position. Images are segmented, the stem pixels are labelled and the ear position is identified based on changes in width along the stem. A mobile camera is then automatically positioned in real time at 30 cm from the ear, for a detailed picture in which silks are identified based on texture and colour. This allows analysis of the time course of ear and silk growths of thousands of plants. The pipeline was tested on a panel of 60 maize hybrids in the PHENOARCH phenotyping platform. Over 360 plants, ear position was correctly estimated in 86% of cases, before it could be visually assessed. Silk growth rate, estimated on all plants, decreased with time consistent with literature. The pipeline allowed clear identification of the effects of genotypes and water deficit on the rate and duration of silk growth.
Conclusions: The pipeline presented here, which combines computer vision, machine learning and robotics,
provides a powerful tool for large-scale genetic analyses of the control of reproductive growth to changes in environmental conditions in a non-invasive and automatized way. It is available as Open Source software in the OpenAlea platform",A robot-assisted imaging pipeline for tracking the growths of maize ear and silks in a high-throughput phenotyping platform,,,10.1186/s13007-017-0246-7,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
145159196,2017-01-01T00:00:00,"18 pages, 16 figures, 3 tables, 6 pseudocodes/algorithms, video at https://youtu.be/IqtyHFrb3BUInternational audienceThe high probability of hardware failures prevents many advanced robots (e.g., legged robots) from being confidently deployed in real-world situations (e.g., post-disaster rescue). Instead of attempting to diagnose the failures, robots could adapt by trial-and-error in order to be able to complete their tasks. In this situation, damage recovery can be seen as a Reinforcement Learning (RL) problem. However, the best RL algorithms for robotics require the robot and the environment to be reset to an initial state after each episode, that is, the robot is not learning autonomously. In addition, most of the RL methods for robotics do not scale well with complex robots (e.g., walking robots) and either cannot be used at all or take too long to converge to a solution (e.g., hours of learning). In this paper, we introduce a novel learning algorithm called "" Reset-free Trial-and-Error "" (RTE) that (1) breaks the complexity by pre-generating hundreds of possible behaviors with a dynamics simulator of the intact robot, and (2) allows complex robots to quickly recover from damage while completing their tasks and taking the environment into account. We evaluate our algorithm on a simulated wheeled robot, a simulated six-legged robot, and a real six-legged walking robot that are damaged in several ways (e.g., a missing leg, a shortened leg, faulty motor, etc.) and whose objective is to reach a sequence of targets in an arena. Our experiments show that the robots can recover most of their locomotion abilities in an environment with obstacles, and without any human intervention",Reset-free Trial-and-Error Learning for Robot Damage Recovery,,'Elsevier BV',10.1016/j.robot.2017.11.010,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
477988955,2016-04-01T00:00:00,"[EN] Human-agent societies refer to applications where virtual agents and humans coexist and interact transparently into a fully integrated environment. One of the most important aspects in this kind of applications is including emotional states of the agents (humans or not) in the decision-making process. In this sense, this paper presents the applicability of the JaCalIVE framework for developing this kind of societies. Specifically, the paper presents an ambient intelligence application where humans are immersed into a system that extracts and analyzes the emotional state of a human group. A social emotional model is employed in order to try to maximize the welfare of those humans by playing the most appropriate music in every moment.Project supported by the Ministerio de Economia y Competitividad of the Spanish Government and the European Regional Development Fund of the European Union (No. TIN2015-65515-C4-1-R)Rincon, JA.; Bajo, J.; Fernandez, A.; Julian Inglada, VJ.; Carrascosa Casamayor, C. (2016). Using emotions for the development of human-agent societies. Frontiers of Information Technology & Electronic Engineering. 17(4):325-337. https://doi.org/10.1631/FITEE.1500343S325337174Ali, F., Amin, M., 2014. The influence of physical environment on emotions, customer satisfaction and behavioural intentions in Chinese resort hotel industry. J. Glob. Bus. Adv., 7(3):249–266. http://dx.doi.org/10.1504/JGBA.2014.064109Bales, R.F., 2001. Social Interaction Systems: Theory and Measurement. Transaction Publishers, USA.Barella, A., Ricci, A., Boissier, O., et al., 2012. MAM5: multi-agent model for intelligent virtual environments. Proc. 10th European Workshop on Multi-Agent Systems, p.16–30.Billhardt, H., Julian, V., Corchado, J.M., et al., 2014. An architecture proposal for human-agent societies. Proc. Int. Workshop on Highlights of Practical Applications of Heterogeneous Multi-Agent Systems, p.344–357. http://dx.doi.org/10.1007/978-3-319-07767-3_31Ducatel, K., Bogdanowicz, M., Scapolo, F., et al., 2001. Scenarios for Ambient Intelligence in 2010. Office for Official Publications of the European Communities.Fernandez, A., Ossowski, S., 2011. A multiagent approach to the dynamic enactment of semantic transportation services. IEEE Trans. Intell. Transp. Syst., 12(2):333–342. http://dx.doi.org/10.1109/TITS.2011.2106776Hale, K.S., Stanney, K.M., 2002. Handbook of Virtual Environments: Design, Implementation, and Applications. CRC Press, USA.Han, D.M., Lim, J.H., 2010. Smart home energy management system using IEEE 802.15.4 and ZigBee. IEEE Trans. Consum. Electron., 56(3):1403–1410. http://dx.doi.org/10.1109/TCE.2010.5606276Hendler, J., 2007. Where are all the intelligent agents? IEEE Intell. Syst., 22:2–3.Holzapfel, A., Stylianou, Y., 2007. A statistical approach to musical genre classification using non-negative matrix factorization. Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing, p.693–696. http://dx.doi.org/10.1109/ICASSP.2007.366330Huhns, M.N., Singh, M.P., Burstein, M., et al., 2005. Research directions for service-oriented multiagent systems. IEEE Internet Comput., 9(6):65.Intille, S.S., 2002. Designing a home of the future. IEEE Perva. Comput., 1(2):76–82.Lawrence, S., Giles, C.L., Tsoi, A.C., et al., 1997. Face recognition: a convolutional neural-network approach. IEEE Trans. Neur. Netw., 8(1):98–113. http://dx.doi.org/10.1109/72.554195Li, T., Ogihara, M., Li, Q., 2003. A comparative study on content-based music genre classification. Proc. 26th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, p.282–289. http://dx.doi.org/10.1145/860435.860487Mangina, E., Carbo, J., Molina, J.M., 2009. Agent-Based Ubiquitous Computing. Atlantis Press, France. http://dx.doi.org/10.2991/978-94-91216-31-2Mehrabian, A., 1980. Basic Dimensions for a General Psychological Theory: Implications for Personality, Social, Environmental, and Developmental Studies. Oelgeschlager, Gunn & Hain, USA.Mehrabian, A., 1997. Analysis of affiliation-related traits in terms of the PAD temperament model. J. Psychol., 131(1):101–117. http://dx.doi.org/10.1080/00223989709603508Nanty, A., Gelin, R., 2013. Fuzzy controlled PAD emotional state of a NAO robot. Proc. Conf. on Technologies and Applications of Artificial Intelligence, p.90–96. http://dx.doi.org/10.1109/TAAI.2013.30Ortony, A., 1990. The Cognitive Structure of Emotions. Cambridge University Press, USA.Ossowski, S., 2013. Agreement Technologies: 8 (Law, Governance and Technology Series). Springer, the Netherlands. http://dx.doi.org/10.1007/978-94-007-5583-3Osuna, E., Freund, R., Girosit, F., 1997. Training support vector machines: an application to face detection. Proc. IEEE Computer Society Conf. on Computer Vision and Pattern Recognition, p.130–136. http://dx.doi.org/10.1109/CVPR.1997.609310Rincon, J.A., Garcia, E., Julian, V., et al., 2014. Developing adaptive agents situated in intelligent virtual environments. Proc. 9th Int. Conf. on Hybrid Artificial Intelligence Systems, p.98–109. http://dx.doi.org/10.1007/978-3-319-07617-1_9Rincon, J.A., Julian, V., Carrascosa, C., 2015a. Applying a social emotional model in human-agent societies. Proc. Int. Workshops of Practical Applications of Agents, Multi-Agent Systems, p.377–388. http://dx.doi.org/10.1007/978-3-319-19033-4_33Rincon, J.A., Julian, V., Carrascosa, C., 2015b. Social emotional model. Proc. 13th Int. Conf. on Practical Applications of Agents, Multi-Agent Systems, p.199–210. http://dx.doi.org/10.1007/978-3-319-18944-4_17Satyanarayanan, M., 2001. Pervasive computing: vision and challenges. IEEE Pers. Commun., 8(4):10–17. http://dx.doi.org/10.1109/98.943998Satyanarayanan, M., 2002. A catalyst for mobile and ubiquitous computing. IEEE Perva. Comput., 1(1):2–5. http://dx.doi.org/10.1109/MPRV.2002.993138Talupur, M., Nath, S., Yan, H., 2001. Classification of Music Genre. Project Report for 15781.Viola, P., Jones, M.J., 2004. Robust real-time face detection. Int. J. Comput. Vis., 57(2):137–154. http://dx.doi.org/10.1023/B:VISI.0000013087.49260.fbWeiser, M., 1991. The computer for the 21st century. Sci. Am., 265(3):94–104. http://dx.doi.org/10.1038/scientificamerican0991-94Zambonelli, F., Jennings, N.R., Wooldridge, M., 2003. Developing multiagent systems: the Gaia methodology. ACM Trans. Softw. Eng. Meth., 12(3):317–370. http://dx.doi.org/10.1145/958961.95896",Using emotions for the development of human-agent societies,http://hdl.handle.net/10251/168936,'Zhejiang University Press',10.1631/FITEE.1500343,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
78292650,2016,"This paper presents control framework based
on multi-agent reinforcement approach for building
intellectual steering software for multi-wheeled mobile
robots. The framework uses modified reinforcement
learning approach based on special multi-agent structure
with virtual leader. The framework application example
will be shown with real multi-wheeled mobile platform.
The experiments were performed in simulation
environment with accurate virtual model",Multi-agent control framework for multi-wheeled mobile platforms,,Minsk: Publishing Center of BSU,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
210948176,2010-05-01T00:00:00,"Online model learning in real-time is required by many applications such as in robot tracking control. It poses a difficult problem, as fast and incremental online regression with large data sets is the essential component which cannot be achieved by straightforward usage of off-the-shelf machine learning methods (such as Gaussian process regression or support vector regression). In this paper, we propose a framework for online, incremental sparsification with a fixed budget designed for large scale real-time model learning. The proposed approach combines a sparsification method based on an independence measure with a large scale database. In combination with an incremental learning approach such as sequential support vector regression, we obtain a regression method which is applicable in real-time online learning. It exhibits competitive learning accuracy when compared with standard regression techniques. Implementation on a real robot emphasizes the applicability of the proposed approach in real-time online model learning for real world systems",Incremental Sparsification for Real-time Online Model Learning,,,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
147682594,2017,"In this work, we propose a 3D scene reconstruction algorithm based on a fully convolutional 3D denoising autoencoder neural network. The network is capable of reconstructing a full scene from a single depth image by creating a 3D representation of it and automatically filling holes and inserting hidden elements. We exploit the fact that our neural network is capable of generalizing object shapes by inferring similarities in geometry. Our fully convolutional architecture enables the network to be unconstrained by a fixed 3D shape, and so it is capable of successfully reconstructing arbitrary scene sizes. Our algorithm was evaluated on a real word dataset of tabletop scenes acquired using a Kinect and processed using KinectFusion software in order to obtain ground truth for network training and evaluation. Extensive measurements show that our deep neural network architecture outperforms the previous state of the art both in terms of precision and recall for the scene reconstruction task. The network has been broadly profiled in terms of memory footprint, number of floating point operations, inference time and power consumption in CPU, GPU and embedded devices. Its small memory footprint and its low computation requirements enable low power, memory constrained, real time always-on embedded applications such as autonomous vehicles, warehouse robots, interactive gaming controllers and drones",Fully convolutional denoising autoencoder for 3D scene reconstruction from a single depth image,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ICSAI.2017.8248355,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
36204601,2011-01-01T00:00:00,"In critical domains such as urban search and rescue (USAR), and bomb disposal, the deployment of teleoperated robots is essential to reduce the risk of first responder personnel. Teleoperation is a difficult task, particularly when controlling robots from an isolated safety zone. In general, the operator has to solve simultaneously the problems of mission planning, target identification, robot navigation, and robot control. We introduce a system to support teleoperated navigation with real-time mapping consisting of a two-step scan matching method that re-considers data associations during the search. The algorithm processes data from laser range finder and gyroscope only, thereby it is independent from the robot platform. Furthermore, we introduce a user-guided procedure for improving the global consistency of maps generated by the scan matcher. Globally consistent maps are computed by a graph-based maximum likelihood method that is biased by localizing crucial parts of the scan matcher trajectory on a prior given geo-tiff image. The approach has been implemented as an embedded system and extensively tested on robot platforms designed for teleoperation in critical situations, such as bomb disposal. Furthermore, the system was evaluated in a test maze by first responders during the Disaster City event in Texas 2008.Artificial Intelligence & Integrated Computer System",Mapping for the Support of First Responders in Critical Domains,,'Springer Science and Business Media LLC',10.1007/s10846-010-9520-x,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
154792820,2017,"Connecting biologically inspired neural simulations to physical or simulated embodiments can be useful both in robotics, for the development of a new kind of bio-inspired controllers, and in neuroscience, to test detailed brain models in complete action-perception loops. The aim of this work is to develop a fully spike-based, biologically inspired mechanism for the translation of proprioceptive feedback. The translation is achieved by implementing a computational model of neural activity of type Ia and type II afferent fibers of muscle spindles, the primary source of proprioceptive information, which, in mammals is regulated through fusimotor activation and provides necessary adjustments during voluntary muscle contractions. As such, both static and dynamic Î³-motoneurons activities are taken into account in the proposed model. Information from the actual proprioceptive sensors (i.e., motor encoders) is then used to simulate the spindle contraction and relaxation, and therefore drive the neural activity. To assess the feasibility of this approach, the model is implemented on the NEST spiking neural network simulator and on the SpiNNaker neuromorphic hardware platform and tested on simulated and physical robotic platforms. The results demonstrate that the model can be used in both simulated and real-time robotic applications to translate encoder values into a biologically plausible neural activity. Thus, this model provides a completely spike-based building block, suitable for neuromorphic platforms, that will enable the development of sensory-motor closed loops which could include neural simulations of areas of the central nervous system or of low-level reflexes",Proprioceptive feedback through a neuromorphic muscle spindle model,,'Frontiers Media SA',10.3389/fnins.2017.00341,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
148440413,2009-01-01T00:00:00,"Teleoperation is a difficult task, particularly when controlling robots from an isolated operator station. In general, the operator has to solve nearly blindly the problems of mission planning, target identification, robot navigation, and robot control at the same time. The goal of the proposed system is to support teleoperated navigation with real-time mapping. We present a novel scan matching technique that re-considers data associations during the search, enabling robust pose estimation even under varying roll and pitch angle of the robot enabling mapping on rough terrain. The approach has been implemented as an embedded system and extensively tested on robot platforms designed for teleoperation in critical situations, such as bomb disposal. Furthermore, the system has been evaluated in a test maze by first responders during the Disaster City event in Texas 2008. Finally, experiments conducted within different environments show that the system yields comparably accurate maps in real-time when compared to higher sophisticated offline methods, such as Rao-Blackwellized SLAM.(Best Paper Award Finalist)Artificial Intelligence & Integrated Computer System",Operator-Assistive Mapping in Harsh Environments,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/SSRR.2009.5424159,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
234919232,9999,"The collaboration between humans and robots is one of the most disruptive and challenging research areas. Even considering advances in design and artificial intelligence, humans and robots could soon ally to perform together a number of different tasks. Robots could also became new playmates. In fact, an emerging trend is associated with the so-called phygital gaming, which builds upon the idea of merging the physical world with a virtual one in order to let physical and virtual entities, such as players, robots, animated characters and other game objects interact seamlessly as if they were all part of the same reality. This paper specifically focuses on mixed reality gaming environments that can be created by using floor projection, and tackles the issue of enabling accurate and robust tracking of off-the-shelf robots endowed with limited sensing capabilities. The proposed solution is implemented by fusing visual tracking data gathered via a fixed camera in a smart environment with odometry data obtained from robot's on-board sensors. The solution has been tested within a phygital gaming platform in a real usage scenario, by experimenting with a robotic game that exhibits many challenging situations which would be hard to manage using conventional tracking techniques",Robust robot tracking for next-generation collaborative robotics-based gaming environments,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TETC.2017.2769705,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
131994678,2015-12-16T00:00:00,"International audienceThis paper describes the on-going development of a novel interface approach to understanding complex systems. We present a description of an interface, referred to as a Homunculus, which allows an experimenter to explore complex systems through immersive virtual reality technology. We describe an initial application under development where the Encephalon, a biologically motivated neural architecture, is used to control a robotics system. Encephalon modules are represented in the Homunculus as 3¬D icons. Information flow between modules of the neural network is represented as graphical animations. Virtual tools will be available to view, manipulate, model, diagnose, analyze, and navigate through the software and multi¬dimensional data. We discuss many important research questions revealed by this work",A VIRTUAL ENVIRONMENT  INTERFACE TO COMPLEX  AUTONOMOUS PERCEPTUAL  SYSTEMS,,IPI Press,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
145170263,2017-01-01T00:00:00,"18 pages, 16 figures, 3 tables, 6 pseudocodes/algorithms, video at https://youtu.be/IqtyHFrb3BUInternational audienceThe high probability of hardware failures prevents many advanced robots (e.g., legged robots) from being confidently deployed in real-world situations (e.g., post-disaster rescue). Instead of attempting to diagnose the failures, robots could adapt by trial-and-error in order to be able to complete their tasks. In this situation, damage recovery can be seen as a Reinforcement Learning (RL) problem. However, the best RL algorithms for robotics require the robot and the environment to be reset to an initial state after each episode, that is, the robot is not learning autonomously. In addition, most of the RL methods for robotics do not scale well with complex robots (e.g., walking robots) and either cannot be used at all or take too long to converge to a solution (e.g., hours of learning). In this paper, we introduce a novel learning algorithm called "" Reset-free Trial-and-Error "" (RTE) that (1) breaks the complexity by pre-generating hundreds of possible behaviors with a dynamics simulator of the intact robot, and (2) allows complex robots to quickly recover from damage while completing their tasks and taking the environment into account. We evaluate our algorithm on a simulated wheeled robot, a simulated six-legged robot, and a real six-legged walking robot that are damaged in several ways (e.g., a missing leg, a shortened leg, faulty motor, etc.) and whose objective is to reach a sequence of targets in an arena. Our experiments show that the robots can recover most of their locomotion abilities in an environment with obstacles, and without any human intervention",Reset-free Trial-and-Error Learning for Robot Damage Recovery,https://core.ac.uk/download/145170263.pdf,'Elsevier BV',10.1016/j.robot.2017.11.010,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
154910901,2017-01-01T00:00:00,"Enabling touch-sensing capability would help appliances understand interaction behaviors with their surroundings. Many recent studies are focusing on the development of electronic skin because of its necessity in various application domains, namely autonomous artificial intelligence (e.g., robots), biomedical instrumentation, and replacement prosthetic devices. An essential task of the electronic skin system is to locally process the tactile data and send structured information either to mimic human skin or to respond to the application demands. The electronic skin must be fabricated together with an embedded electronic system which has the role of acquiring the tactile data, processing, and extracting structured information. On the other hand, processing tactile data requires efficient methods to extract meaningful information from raw sensor data. Machine learning represents an effective method for data analysis in many domains: it has recently demonstrated its effectiveness in processing tactile sensor data. In this framework, this paper presents the implementation of digital signal processing based on FPGAs for tactile data processing. It provides the implementation of a tensorial kernel function for a machine learning approach. Implementation results are assessed by highlighting the FPGA resource utilization and power consumption. Results demonstrate the feasibility of the proposed implementation when real-time classification of input touch modalities are targeted",Real-time digital signal processing based on FPGAs for electronic skin implementation,https://core.ac.uk/download/154910901.pdf,'MDPI AG',10.3390/s17030558,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
236055346,2006-04-01T00:00:00,"In this work it presents the improvement and extension of the virtual reality software created by the Robotics, Artificial Intelligence and Automatization Outpost Laboratory in the school of electrical engineering of the Pontificia Universidad Católica de Valparaíso",Improvement and extension of Virtual Reality for flexible systems of manufacture,https://core.ac.uk/download/236055346.pdf,Agora University Press,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
226495312,2014,"The cerebellum is involved in a large number of different neural processes, especially in associative learning and in fine motor control. To develop a comprehensive theory of sensorimotor learning and control, it is crucial to determine the neural basis of coding and plasticity embedded into the cerebellar neural circuit and how they are translated into behavioral outcomes in learning paradigms. Learning has to be inferred from the interaction of an embodied system with its real environment, and the same cerebellar principles derived from cell physiology have to be able to drive a variety of tasks of different nature, calling for complex timing and movement patterns. We have coupled a realistic cerebellar spiking neural network (SNN) with a real robot and challenged it in multiple diverse sensorimotor tasks. Encoding and decoding strategies based on neuronal firing rates were applied. Adaptive motor control protocols with acquisition and extinction phases have been designed and tested, including an associative Pavlovian task (Eye blinking classical conditioning), a vestibulo-ocular task and a perturbed arm reaching task operating in closed-loop. The SNN processed in real-time mossy fiber inputs as arbitrary contextual signals, irrespective of whether they conveyed a tone, a vestibular stimulus or the position of a limb. A bidirectional long-term plasticity rule implemented at parallel fibers-Purkinje cell synapses modulated the output activity in the deep cerebellar nuclei. In all tasks, the neurorobot learned to adjust timing and gain of the motor responses by tuning its output discharge. It succeeded in reproducing how human biological systems acquire, extinguish and express knowledge of a noisy and changing world. By varying stimuli and perturbations patterns, real-time control robustness and generalizability were validated. The implicit spiking dynamics of the cerebellar model fulfill timing, prediction and learning function",Adaptive Robotic Control Driven by a Versatile Spiking Cerebellar Network,,,10.1371/journal.pone.0112265,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
398618413,2017-11-22T00:00:00,"International audienceFor a robot to be deployed in unconstrained real world environments, it needs to be autonomous. In this preliminary work, we focus on the capacity of an autonomous robot to discover and recognize objects in its visual field. Current existing solutions mainly employ complex deep neural architectures that need to be pre-trained using large datasets in order to be effective. We propose a new model for autonomous and unsupervised object learning in videos that does not require supervised pre-training and uses relatively simple visual filtering. The main idea relies on the saliency-based detection and learning of objects considered similar (thanks to a spatio-temporal continuity). For this purpose the learning of objects is based on a Siamese Neural Network (SNN). We demonstrate the capacity of the SNN to learn a good feature representation despite the deliberately simple and noisy process used to extract candidate objects",Autonomous object recognition in videos using Siamese Neural Networks,,HAL CCSD,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
144878273,2017-11-02,"This paper deals with the design and the
implementation of an automatic task planner for a robot, irrespective
of whether it is a stationary robot or a mobile robot. The aim of the
task planner nothing but, they are planning systems which are used to
plan a particular task and do the robotic manipulation. This planning
system is embedded into the system software in the computer, which
is interfaced to the computer. When the instructions are given using
the computer, this is transformed into real time application using the
robot. All the AI based algorithms are written and saved in the
control software, which acts as the intelligent task planning system",Design of an Artificial Intelligence Based Automatic Task Planner or a Robotic System,,,10.5281/zenodo.1132998,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
301281319,2017-01-01T00:00:00,"Spatial perception, in which objects’ motion and positional relationship are recognized, is necessary for applications such as a walking robot and an autonomous car. One of the demanding features of spatial perception in real world applications is robustness. Neural network-based approaches, in which perception results are obtained by voting among a large number of neuronal activities, seem to be promising. We focused on a neural network model for motion stereo vision proposed by Kawakami et al. In this model, local motion in each small region of the visual field, which comprises optical flow, is detected by hierarchical neural network. Implementation of this model into a VLSI is required for real-time operation with low power consumption. In this study, we reduced the computational complexity of this model and showed cell responses of the reduced model by numerical simulation.Peer Reviewe",Complexity reduction of neural network model for local motion detection in motion stereo vision,,'Springer Science and Business Media LLC',10.1007/978-3-319-70136-3,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
131956132,2017-11-08T00:00:00,"International audienceBackground:  In maize, silks are hundreds of filaments that simultaneously emerge from the ear for collecting pollen over a period of 1–7 days, which largely determines grain number especially under water deficit. Silk growth is a major trait for drought tolerance in maize, but its phenotyping is difficult at throughputs needed for genetic analyses.Results:  We have developed a reproducible pipeline that follows ear and silk growths every day for hundreds of plants, based on an ear detection algorithm that drives a robotized camera for obtaining detailed images of ears and silks. We first select, among 12 whole‑plant side views, those best suited for detecting ear position. Images are seg‑mented, the stem pixels are labelled and the ear position is identified based on changes in width along the stem. A mobile camera is then automatically positioned in real time at 30 cm from the ear, for a detailed picture in which silks are identified based on texture and colour. This allows analysis of the time course of ear and silk growths of thousands of plants. The pipeline was tested on a panel of 60 maize hybrids in the PHENOARCH phenotyping platform. Over 360 plants, ear position was correctly estimated in 86% of cases, before it could be visually assessed. Silk growth rate, estimated on all plants, decreased with time consistent with literature. The pipeline allowed clear identification of the effects of genotypes and water deficit on the rate and duration of silk growth.Conclusions:  The pipeline presented here, which combines computer vision, machine learning and robotics, provides a powerful tool for large‑scale genetic analyses of the control of reproductive growth to changes in environ‑mental conditions in a non‑invasive and automatized way. It is available as Open Source software in the OpenAlea platform",A robot-assisted imaging pipeline for tracking the growths of maize ear and silks in a high-throughput phenotyping platform,,'Springer Science and Business Media LLC',10.1186/s13007-017-0246-7,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
41534236,2005-01-01T08:00:00,"A real-time analogue recurrent neural network (RNN) can extract and learn the unknown dynamics (and features) of a typical control system such as a robot manipulator. The task at hand is a tracking problem in the presence of disturbances. With reference to the tasks assigned to an industrial robot, one important issue is to determine the motion of the joints and the effector of the robot. In order to model robot dynamics we use a neural network that can be implemented in hardware. The synaptic weights are modelled as variable gain cells that can be implemented with a few MOS transistors. The network output signals portray the periodicity and other characteristics of the input signal in unsupervised mode. For the specific purpose of demonstrating the trajectory learning capabilities, a periodic signal with varying characteristics is used. The developed architecture, however, allows for more general learning tasks typical in applications of identification and control. The periodicity of the input signal ensures convergence of the output to a limit cycle. Online versions of the synaptic update can be formulated using simple CMOS circuits. Because the architecture depends on the network generating a stable limit cycle, and consequently a periodic solution which is robust over an interval of parameter uncertainties, we currently place the restriction of a periodic format for the input signals. The simulated network contains interconnected recurrent neurons with continuous-time dynamics. The system emulates random-direction descent of the error as a multidimensional extension to the stochastic approximation. To achieve unsupervised learning in recurrent dynamical systems we propose a synapse circuit which has a very simple structure and is suitable for implementation in VLSI",An analogue recurrent neural networks for trajectory learning and other industrial applications,https://core.ac.uk/download/41534236.pdf,"Edith Cowan University, Research Online, Perth, Western Australia",,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
291397273,,"The final publication is available at link.springer.comSocial robots should be able to search and track people in order to help them. In this paper we present two different techniques for coordinated multi-robot teams for searching and tracking people. A probability map (belief) of a target person location is maintained, and to initialize and update it, two methods were implemented and tested: one based on a reinforcement learning algorithm and the other based on a particle filter. The person is tracked if visible, otherwise an exploration is done by making a balance, for each candidate location, between the belief, the distance, and whether close locations are explored by other robots of the team. The validation of the approach was accomplished throughout an extensive set of simulations using up to five agents and a large amount of dynamic obstacles; furthermore, over three hours of real-life experiments with two robots searching and tracking were recorded and analysed.Peer Reviewe",Searching and tracking people with cooperative mobile robots,,,,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
442510888,2015-01-01T00:00:00,"Complex real problems increasingly require intelligent systems that combine knowledge, techniques and methodologies from various sources. Intelligent systems based on artificial intelligence techniques that are associated with the behavior of people can perform the processes of learning, reasoning and solving all kinds of problems. Such systems, which automatically can perform tasks set by the user or other software, today thankfully called intelligent agents. Independent, intelligent agents on the Internet can be very successful to perform some search work on behalf of and for the needs of different users. For efficient collection, manipulation and management of data, such software can be very interesting from the standpoint of intelligent data analysis in many areas the police. Analysis of the data collected by an intelligent agent (a software robot-bot) can be successfully utilized, among many jobs in the police, and in the field of crime and in particular manifestation of cyber­crime, traffic safety, emergencies, etc. To make the collection and analysis of data from criminal activities on the Internet effective, it is necessary to examine the existing artificial intelligence techniques to be used for the conclusion of the intelligent agents. On the other hand, using of methods of artificial intelligence in finding data along with intelligent data analysis (data mining) should be used, which has found wide use in the area of business, economics, mechanics, medicine, genetics, transport etc",Artificial intelligence in process of collecting and analyzing data within police works,,"University of Criminal Investigation and Police Studies, Belgrade",,"[{'title': 'Nauka bezbednost policija', 'identifiers': ['issn:0354-8872', 'issn:2620-0406', '2620-0406', '0354-8872']}]",core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
153558361,2008-01-01T00:00:00,"This paper presents experimental results acquired from the implementation of an adaptive control scheme for nonholonomic mobile robots, which was recently proposed by the same authors and tested only by simulations. The control system comprises a trajectory tracking kinematic controller, which generates the reference wheel velocities, and a cascade dynamic controller, which estimates the robot's uncertain nonlinear dynamic functions in real-time via a multilayer perceptron neural network. In this manner precise velocity tracking is attained, even in the presence of unknown and/or time-varying dynamics. The experimental mobile robot, designed and built for the purpose of this research, is also presented in this paper.peer-reviewe",Multilayer perceptron adaptive dynamic control of mobile robots : experimental validation,https://core.ac.uk/download/153558361.pdf,'Springer Science and Business Media LLC',10.1007/978-3-540-78317-6_17,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
154803302,2017-01-01T00:00:00,"Many tasks involve the fine manipulation of objects despite limited visual feedback. In such scenarios, tactile and proprioceptive feedback can be leveraged for task completion. We present an approach for real-time haptic perception and decision-making for a haptics-driven, functional contour-following task: the closure of a ziplock bag. This task is challenging for robots because the bag is deformable, transparent, and visually occluded by artificial fingertip sensors that are also compliant. A deep neural net classifier was trained to estimate the state of a zipper within a robot&#x0027;s pinch grasp. A Contextual Multi-Armed Bandit (C-MAB) reinforcement learning algorithm was implemented to maximize cumulative rewards by balancing exploration versus exploitation of the state-action space. The C-MAB learner outperformed a benchmark Q-learner by more efficiently exploring the state-action space while learning a hard-to-code task. The learned C-MAB policy was tested with novel ziplock bag scenarios and contours (wire, rope). Importantly, this work contributes to the development of reinforcement learning approaches that account for limited resources such as hardware life and researcher time. As robots are used to perform complex, physically interactive tasks in unstructured or unmodeled environments, it becomes important to develop methods that enable efficient and effective learning with physical testbeds. IEE",Functional Contour-following via Haptic Perception and Reinforcement Learning,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TOH.2017.2753233,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
297620750,,"Telepresence Mobile Robots have prominent attributes in many fields as it provides virtual presence for human robot interaction. The deployment of this robot in healthcare sector has improved patient care and health. The vision system in a telepresence robot allows two way audiovisual communication between people at different location. In spite of such advancement, the manual way of controlling a robot to recognise and track people during an emergency is not favourable for a long duration. To circumvent this problem, biometric method using human face is proposed in this research which is implemented on Medical Telediagnosis Robot. This paper details the design of the face recognition and tracking system with four automated modules which are motion detection, face detection, face recognition and face tracking. The modules are developed with different algorithm and tested individually to ensure the stability of the system. Artificial Intelligence technique was applied at the face recognition stage while a two degree of freedom mechanism for actuator control was used at face tracking stage. A sequential mode operation is proposed to reduce the execution time in a real-time environment. To achieve this, only one module is operated at each time. A Graphical User Interface was developed to ease the users at the local and robot environment. The system is designed in LabVIEW platform. The biometric system proposed with hybrid algorithm at each module adapts for face images detected at different distances, poses and lighting condition. This system was tested in real-time and has an execution time of 55ms and 98% accuracy. The stand alone system designed for Medical Telediagnosis Robot can be will be very fruitful for various biometric system using facial technology",Towards real-time visual biometric authentication using human face for healthcare Telepresence Mobile Robots,,"Journal of Telecommunication, Electronic and Computer Engineering, Universiti Teknikal Malaysia Melaka",,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
297623281,,"Artificial intelligence is a branch of technology that is related to the development of
intelligent machine and software. A system that is able to perceive its environment and take actions
according to the situation maximizes its chances of success to perform the action. There are many
researches and studies that have been done to create this non-possible dream, which is to create a
machine that is able to replicate real life. This project aims to create an interactive robot by creating a
robot that is able to perceive the command from people and act according to the command by using the
foundation of artificial intelligence and image processing, and kept as a physical pet",Development of an interactive robotic pet dog,,"Journal of Advanced Research in Applied Mechanics, Penerbit Akademia Baru",,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
304944408,2016-01-01T00:00:00,"Cameras provide a rich source of information while being passive, cheap and lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work we present the first implementation of receding horizon control, which is widely used in ground vehicles, with monocular vision as the only sensing mode for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a number of contributions: novel coupling of perception and control via relevant and diverse, multiple interpretations of the scene around the robot, leveraging recent advances in machine learning to showcase anytime budgeted cost-sensitive feature selection, and fast non-linear regression for monocular depth prediction. We empirically demonstrate the efficacy of our novel pipeline via real world experiments of more than 2 kms through dense trees with a quadrotor built from off-the-shelf parts. Moreover our pipeline is designed to combine information from other modalities like stereo and lidar as well if available</p",Vision and Learning for Deliberative Monocular Cluttered Flight,,,10.1184/r1/6561554.v1,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
58795057,2011-11-01T00:00:00,"Autonomy is a prime issue on robotics field and it is closely related to decision making. Last researches on decision making for social robots are focused on biologically inspired mechanisms for taking decisions. Following this approach, we propose a motivational system for decision making, using internal (drives) and external stimuli for learning to choose the right action. Actions are selected from a finite set of skills in order to keep robot's needs within an acceptable range. The robot uses reinforcement learning in order to calculate the suitability of every action in each state. The state of the robot is determined by the dominant motivation and its relation to the objects presents in its environment. The used reinforcement learning method exploits a new algorithm called Object Q-Learning. The proposed reduction of the state space and the new algorithm considering the collateral effects (relationship between different objects) results in a suitable algorithm to be applied to robots living in real environments. In this paper, a first implementation of the decision making system and the learning process is implemented on a social robot showing an improvement in robot's performance. The quality of its performance will be determined by observing the evolution of the robot's wellbeing.The funds provided by the Spanish Government through the project called “Peer
to Peer Robot-Human Interaction” (R2H), of MEC (Ministry of Science and Education), the project “A new approach to social robotics” (AROS), of MICINN (Ministry of Science and Innovation), and the RoboCity2030-II-CM project (S2009/DPI-1559), funded by Programas de Actividades I+D en la Comunidad de Madrid and cofunded by Structural Funds of the EU",Learning the selection of actions for an autonomous social robot by reinforcement learning based on motivations,,'Springer Science and Business Media LLC',10.1007/s12369-011-0113-z,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
226194381,2012-01-01,"An inverse optimal neural controller for discrete-time unknown nonlinear systems, in the presence of external disturbances and parameter uncertainties, is presented. It is based on a discrete-time recurrent high-order neural network trained with an extended Kalman filter-based algorithm. The applicability of the proposed approach is first tested via simulations for an electrically driven nonholonomic mobile robot, and finally, the proposed methodology is implemented on real time. Copyright � 2012 John Wiley & Sons, Ltd",Discrete-time neural control for electrically driven nonholonomic mobile robots,,'Wiley',10.1002/acs.2289,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
203171141,2016-12-31,"AbstractThe effort to develop an electronic skin is highly motivated by many application domains namely robotics, biomedical instrumentations, and replacement prosthetic devices. Several e-skin systems have been proposed recently and have demonstrated the need of an embedded electronic system for tactile data processing either to mimic the human skin or to respond to the application demands. Processing tactile data requires efficient methods to extract meaningful information from raw sensors data.In this framework, our goal is the development of a dedicated embedded electronic system for electronic skin. The embedded electronic system has to acquire the tactile data, process and extract structured information. Machine Learning (ML) represents an effective method for data analysis in many domains: it has recently demonstrated its effectiveness in processing tactile sensors data.This paper presents an embedded electronic system based on dedicated hardware implementation for electronic skin systems. It provides a Tensorial kernel function implementation for machine learning based on Tensorial kernel approach. Results assess the time latency and the hardware complexity for real time functionality. The implementation results highlight the high amount of power consumption needed for the input touch modalities classification task. Conclusions and future perspectives are also presented",Embedded Electronic System Based on Dedicated Hardware DSPs for Electronic Skin Implementation ,,The Author(s). Published by Elsevier Ltd.,10.1016/j.protcy.2016.08.007,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
92169625,2003-01-01T00:00:00,"Mechatronic design is the integrated design of a mechanical system and its embedded control system. In order to make proper choices early in the design stage, tools are required that support modelling and simulation of physical systems––together with the controllers––with parameters that are directly related to the real-world system. Such software tools are becoming available now. Components in various physical domains (e.g. mechanical or electrical) can easily be selected from a library and combined into a ‘process’ that can be controlled by block-diagram-based (digital) controllers. A few examples will be discussed that show the use of such a tool in various stages of the design. The examples include a typical mechatronic system with a flexible transmission, a mobile robot, and an industrial linear motor with a neural-network-based learning feed-forward controller that compensates for cogging",Mechatronic Design,,'Elsevier BV',10.1016/s0957-4158(03)00042-4,,core,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
