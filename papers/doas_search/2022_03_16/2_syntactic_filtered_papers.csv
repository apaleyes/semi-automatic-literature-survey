doi,type,publication,publisher,publication_date,database,title,url,abstract,domain,id
cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,filtered,semantic_scholar,,2018-01-01 00:00:00,semantic_scholar,comparing human-robot proxemics between virtual reality and the real world,https://www.semanticscholar.org/paper/cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,"Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of Human-Robot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. Comparing Human-Robot Proxemics between Virtual Reality and the Real World Rui Li KTH Royal Institute of Technology Stockholm, Sweden Rui3@kth.se ABSTRACT Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other.Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. INTRODUCTION Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI) [1][2][3][4]. VR has been used to test teleoperation and collect demonstration data to train machine learning algorithms, which showcased the effectiveness of learning visuomotor skills using data collected by consumer-grade devices [1]. VR teleoperation systems were proposed to crowdsource robotic demonstrations at scale [2]. A VR simulation framework was also proposed to replace the physical robot, as VR can enable high level abstraction in embodiment and multimodal interaction [3]. VR has also been used as a rapid prototyping tool to design in-vehicle interactions and interfaces for self-driving cars, which showed the evocation to genuine responses from test participants [4]. Compared to other HRI experiment methods, VR as an emerging interactive media provides unique advantages. VR HRI has the potential of having higher immersion and fidelity than picture based HRI, video-based HRI and simulated HRI. In situations where the perception of the robot is challenging, compared to on-screen viewing, VR display showed significant improvement on collaborative tasks [5]. When comparing VR HRI to the physical, realworld interaction (Live HRI), there is a trade-off between the two. VR experiences still cannot replace physical experiences due to system limitation, and limited interaction modalities etc. [6]. For example, system limitations such as limited field of view and low display resolution could reduce immersion and presence of the VR experience, resulting in different behaviors from Live experiments. Limited interaction modalities, such as the absence of touch, means that the participant could not feel the robot or even go through the robot, which could potentially break the entire interaction. Figure 1: Photograph of the Live experiment setting However, with the help of the distribution of consumer-grade VR devices and online crowdsourcing platforms, VR HRI has the potential to gain massive data for training robotic behavior and studying HRI related issues. Data collection through VR can also reduce noise and improve the data quality [1], which help to ease data processing and algorithm training. Furthermore, VR HRI experiments can test concepts and interactions without physical robots, making it more resource efficient and less expensive than Live HRI. Less hardware also means that the experiment will be less cumbersome to set up, easier to be reproduced and to ensure experiment quality. In this study, HRI Proxemics (the preferred personal space between a human and a robot) was compared to give a better justification and more basic understanding of the relationship between Live and VR. Proxemics preferences rely on lower level intuition [7], therefore, reflect the differences in the perceptions between Live and VR better. Compared to other HRI subject such as conversational (audio) or gaze behavior (visual), which are more modality dependent, proxemics can give a comprehensive understanding of the human responses. In addition, variations of modalities in VR can greatly influence human perception. For example, a higher visual familiarity of the physical environment in VR can decrease the effect of distance distortion [8]. Auditory inputs play another important role in VR, the addition of spatial sound can increase the sense of presence in VR and provide sound localization [9]. Thus, this work also compares VR settings with variance in modalities to evaluate the impacts of visual familiarity and spatial sound on VR HRI experiments. A 2 x 3 mixed design experiment was conducted to evaluate the differences between Live and VR HRI, as well as the influence of visual familiarity and spatial sound in VR. For the Live HRI, the pepper robot from Softbank Robotics was used (Figure 1). In the VR HRI, a 3D model of the same robot was used. To measure visual familiarity, the VR scene was created in Blender based on a 3D scan of the physical lab. The spatial sound was created by enabling the movement of the physical robot, due to the difficulties of engineering spatial sound. The interaction was implemented in Unity. As an objective measurement for proxemics preference, the minimum comfort distance (MCD) was measured. In addition, for the psychological perception of the experience, the feeling of presence was measured with the SUS questionnaire. For the perception of the robot, two relevant factors, competence and discomfort was measured with the ROSAS questionnaire.",oceanology,1
5f3c473a98d99962b62cf044f510f5db64ca2f47,filtered,semantic_scholar,IEEE Software,2002-01-01 00:00:00,semantic_scholar,is there an extreme world? [book review],https://www.semanticscholar.org/paper/5f3c473a98d99962b62cf044f510f5db64ca2f47,"Extreme Programming’s core concept is that the client’s requirements for a software system change throughout the system’s development. The iterative method that XP uses requires considerable client involvement and a deep level of commitment to complete discrete sections of the development while meeting all the documentation, testing, and quality requirements before deployment. This nontraditional development method sounds like a silver bullet—too good to be true. So, is anyone using it? Extreme Programming Installed sets out to show that this method has indeed seen realworld deployment. To understand the real-world implementation of these theories, we need practical examples to expand the ideas. This XP book provides them. Backed by experience on XP projects, Ron Jefferies, Ann Anderson, and Chet Hendrickson give insight into the practical issues that development teams face. For example, what do you do about programmers working overtime? How do you use XP on projects to replace legacy systems? One important theme in the book is that of constantly testing the code to ensure that it meets requirements. The authors give practical help on what to test, how to test, and what tools to use in this development environment. Another theme is that the programming pair collectively owns the code. This means the pair is responsible for making it all work and ensuring that it all conforms to standards. If something is missing or needs rework to get the code to release, the pair adds it and tests the revised code. The importance of small tasks and fast turnaround of booked-out code becomes apparent as the book’s examples continue. Fortunately, the authors give guidance on code management tools and the code release process. If developers take away only one thought from this book, it should be this: make estimates for your work based on reality, not best intentions or wishes and hopes. The programming pair works on design, tests, and code on the basis of their own estimates. This gives all programmers and designers the potential to work according to their best speed and abilities rather than expectations imposed on them. The need for reality extends to reporting progress and test results. As the authors put it, “the Extreme way: tell the truth straight out.”",oceanology,2
59e10d1d4cd454635914cfd0ac5160a318fd0473,filtered,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,ub09 session 9,https://www.semanticscholar.org/paper/59e10d1d4cd454635914cfd0ac5160a318fd0473,"In the domain of Wireless Sensor Networks (WSN), providing an effective security solution to protect the motes and their communications is challenging. Due to the hard constraints on performance, storage and energy consumption, normal network-security related techniques cannot be applied. Focusing on the ""Intrusion Detection"" problem, we propose a realworld application of our WSN Intrusion Detection System (WIDS). WIDS exploits the Weak Process Models to classify potential security issues in the WSN and to notify the operators when an attack tentative is detected. In this demonstration, we show how our IDS works, how it detects some basic attacks and how the IDS can evolve to fullfil the needs of secure WSN deployments. Download Paper (PDF) UB09.2 RESCUE: EDA TOOLSET FOR INTERDEPENDENT ASPECTS OF RELIABILITY, SECURITY AND QUALITY IN NANOELECTRONIC SYSTEMS DESIGN Authors: Cemil Cem Gürsoy1, Guilherme Cardoso Medeiros2, Junchao Chen3, Nevin George4, Josie Esteban Rodriguez Condia5, Thomas Lange6, Aleksa Damljanovic5, Raphael Segabinazzi Ferreira4, Aneesh Balakrishnan6, Xinhui Anna Lai1, Shayesteh Masoumian7, Dmytro Petryk3, Troya Cagil Koylu2, Felipe Augusto da Silva8, Ahmet Cagri Bagbaba8 and Maksim Jenihhin1 1Tallinn University of Technology, EE; 2Delft University of Technology, NL; 3IHP, DE; 4BTU Cottbus-Senftenberg, DE; 5Politecnico di Torino, IT; 6IROC Technologies, FR; 7Intrinsic ID B.V., NL; 8Cadence Design Systems GmbH, DE Abstract The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF)The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF) UB09.3 ASAM: AUTOMATIC SYNTHESIS OF ALGORITHMS ON MULTI CHIP/FPGA WITH COMMUNICATION CONSTRAINTS Authors: Amir Masoud Gharehbaghi, Tomohiro Maruoka, Yukio Miyasaka, Akihiro Goda, Amir Masoud Gharehbaghi and Masahiro Fujita, The University of Tokyo, JP Abstract Mapping of large systems/computations on multiple chips/multiple cores needs sophisticated compilation methods. In this demonstration, we present our compiler tools for multi-chip and multi-core systems that considers communication architecture and the related constraints for optimal mapping. Specifically, we demonstrate compilation methods for multi-chip connected with ring topology, and multi-core connected with mesh topology, assuming fine-grained reconfigurable cores, as well as generalization techniques for large problems size as convolutional neural networks. We will demonstrate our mappings methods starting from data-flow graphs (DFGs) and equations, specifically with applications to convolutional neural networks (CNNs) for convolution layers as well as fully connected layers. Download Paper (PDF) UB09.4 HEPSYCODE-MC: ELECTRONIC SYSTEM-LEVEL METHODOLOGY FOR HW/SW CO-DESIGN OF MIXED-CRITICALITY EMBEDDED SYSTEMS Authors: Luigi Pomante1, Vittoriano Muttillo1, Marco Santic1 and Emilio Incerto2 1Università degli Studi dell'Aquila DEWS, IT; 2IMT Lucca, IT Abstract Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF)Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF) UB09.5 CS: CRAZYSQUARE Authors: Federica Caruso1, Federica Caruso1, Tania Di Mascio1, Alessandro D'Errico1, Marco Pennese2, Luigi Pomante1, Claudia Rinaldi1 and Marco Santic1 1University of L'Aquila, IT; 2Ministry of Education, IT Abstract CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF)CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF) UB09.6 LABSMILING: A SAAS FRAMEWORK, COMPOSED OF A NUMBER OF REMOTELY ACCESSIBLE TESTBEDS AND RELATED SW TOOLS, FOR ANALYSIS, DESIGN AND MANAGEMENT OF LOW DATA-RATE WIRELESS PERSONAL AREA NETWORKS BASED ON IEEE 802.15.4 Authors: Carlo Centofanti, Luigi Pomante, Marco Santic and Walter Tiberti, University of L'Aquila, IT Abstract Low data-rate wireless personal area networks (LR-WPANs) are constantly increasing their presence in the fields of IoT, wearable, home automation, health monitoring. The development, deployment and testing of SW based on IEEE 802.15.4 standard (and derivations, e.g. 15.4e), require the exploitation of a testbed as the network grows in complexity and heterogeneity. This demo shows LabSmiling: a SaaS framework which connects testbeds deployed in a real-world-environment and the related SW tools that make available a meaningful (but still scalable) number of physical devices (sensor nodes) to developers. It provides a comforta",oceanology,3
10.1109/icde.2019.00128,filtered,2019 IEEE 35th International Conference on Data Engineering (ICDE),IEEE,2019-04-11 00:00:00,ieeexplore,robust high dimensional stream classification with novel class detection,https://ieeexplore.ieee.org/document/8731449/,"A primary challenge in label prediction over a data stream is the emergence of instances belonging to unknown or novel class over time. Traditionally, studies addressing this problem aim to detect such instances using cluster-based mechanisms. They typically assume that instances from the same class are closer to each other than those belonging to different classes in observed feature space. Unfortunately, this may not hold true in higher-dimensional feature space such as images. In recent years, Convolutional neural network (CNN) have emerged as a leading system to be employed in many real-world application. Yet, based on the assumption of closed world dataset with a fixed number of categories, CNN lacks robustness for novel class detection, so it is unclear on how such models can be used to deal with novel class instances along a high-dimensional image stream. In this paper, we focus on addressing this challenge by proposing an effective learning framework called CNN-based Prototype Ensemble (CPE) for novel class detection and correction. Our framework includes a prototype ensemble loss (PE) to improve the intra-class compactness and expand inter-class separateness in the output feature representation, thereby enabling the robustness of novel class detection. Moreover, we provide an incremental learning strategy which maintains a constant amount of exemplars to update the network, making it more practical for real-world application. We empirically demonstrate the effectiveness of our framework by comparing its performance over multiple realworld image benchmark data streams with existing state-of-theart data stream detection techniques. The implementation of CPE is on: https://github.com/Vitvicky/Convolutional-Net-PrototypeEnsemble",space,4
d557d4b079b89fdd4d149697b38c9160a8234c47,filtered,semantic_scholar,,2017-01-01 00:00:00,semantic_scholar,"rapid development, real-world deployment, and evaluation of projected augmented reality applications",https://www.semanticscholar.org/paper/d557d4b079b89fdd4d149697b38c9160a8234c47,"Current interactive projected augmented reality systems are not designed to support rapid development and deployment of applications beyond the confines of research labs. I developed a series of self-contained interactive projector-sensor systems (collectively LuminAR devices) and a web-based software development framework. The design goal of this research work was to advance the state of the art of projected AR interfaces and to explore how they can manifest in day-to-day objects. This novel, tightly integrated approach allows developers who are not versed in computer graphics, vision algorithms, and augmented reality techniques to implement projected AR applications rapidly. In this work, I review several real-world uses of the system for retail presentation, desktop interaction and collaboration applications, manufacturing, and education. The work is evaluated through extensive use of the hardware and software by developers as well as two user studies that specifically explored applications for manufacturing and education. The evaluation methodology focused both on basic interaction and system usability as well as the implications of using augmented interfaces in the specific application domains of education and manufacturing. I also discuss the results of the first large-scale user studies of projected augmented reality rapid application development. Finally, I provide a set of design principles for projected augmented reality applications, and recommendations concerning how to deploy such applications in the real world. This dissertation work was partially supported by research grants from Intel, Steelcase and Pearson. Dissertation Supervisor Pattie Maes Professor of Media Arts and Sciences Program in Media Arts and Sciences Rapid Development, Real-World Deployment, and Evaluation of Projected Augmented Reality Applications",oceanology,5
5e171bf6603a03fbfdf3434abcd29496a2327100,filtered,semantic_scholar,Multimodal Technol. Interact.,2021-01-01 00:00:00,semantic_scholar,a learning analytics conceptual framework for augmented reality-supported educational case studies,https://www.semanticscholar.org/paper/5e171bf6603a03fbfdf3434abcd29496a2327100,"The deployment of augmented reality (AR) has attracted educators’ interest and introduced new opportunities in education. Additionally, the advancement of artificial intelligence has enabled educational researchers to apply innovative methods and techniques for the monitoring and evaluation of the teaching and learning process. The so-called learning analytics (LA) discipline emerged with the promise to revolutionize traditional instructional practices by introducing systematic and multidimensional ways to improve the effectiveness of the instructional process. However, the implementation of LA methods is usually associated with web-based platforms, which offer direct access to learners’ data with minimal effort or adjustments. On the other hand, the complex nature of immersive technologies and the diverse instructional approaches which are utilized in different scientific domains have limited the opportunities for research and development in this direction. Within these research contexts, we present a conceptual framework that describes the elements of an LA process tailored to the information that can be gathered from the use of educational applications, and further provide an indicative case study for AR-supported educational interventions. The current work contributes by elucidating and concretizing the design elements of AR-supported applications and provides researchers and designers with guidelines on how to apply instructional strategies in (augmented) real-world projects.",oceanology,6
83c855157830ae26e3420a8bf44877660451c8e4,filtered,semantic_scholar,,2021-01-01 00:00:00,semantic_scholar,on the suitability of current augmented reality head-mounted devices,https://www.semanticscholar.org/paper/83c855157830ae26e3420a8bf44877660451c8e4,"Simulation is a recognized and much-appreciated tool in healthcare and education. Advances in simulation have led to the burgeoning of various technologies. In recent years, one such technological advancement has been Augmented Reality (AR). Augmented Reality simulations have been implemented in healthcare on various fronts with the help of a plethora of devices including cellphones, tablets, and wearable AR headsets. AR headsets offer the most immersive experience of the AR simulation as they are head-mounted and offer a stereoscopic view of the superimposed 3D models through the attached goggles overlaid on real-world surfaces. To this effect, it is important to understand the performance capabilities of the AR headsets based on workload. In this paper, our objective is to compare the performances of two prominent AR headsets of today, the Microsoft Hololens and the Magic Leap One. We use surgical AR software that allows the surgeons to show internal structures, such as the rib cage, to assist in the surgery as a reference application to obtain performance numbers for those AR devices. Based on our research, there are no performance measurements and recommendations available for these types of devices in general yet. Introduction In an attempt to measure the feasibility and effectiveness of using AR in surgery and nursing education, we developed an application titled ARiSE (Augmented Reality in Surgery and Education) [39]. We incorporated two facets of this application, one to be used during surgery in the Operating Room (OR), and another to assist in the education of nursing students. Surgeons would use this application in the OR during rib-plating surgery and be able to visualize an accurate model of the patient’s rib cage derived from computerized tomography (CT) scans outside their body. The nursing education application would be used by nursing students during the training of fundamental cardiopulmonary physical assessment skills. The students will be able to visualize stock models of various human organs overlaid on manikins along with visual guides to correct auscultation assessment. The aforementioned applications were developed and deployed into the first generation Microsoft Hololens and Magic Leap One AR headsets for practical use. While the application was deployed successfully and demonstrated accurate usability in both devices, our objective in this paper was to measure and compare the performances between the two AR headsets to derive recommendations for when to use which of these devices. The contributions of this paper are as follows: 1. Direct comparison of head-mounted augmented reality devices from the major brands, namely Microsoft and Magic Leap. 2. Expert evaluation based on real-world application tested with the help of domain specialists. 3. Recommendations for head-mounted Augmented Reality devices. Related Work In this section, we discuss some of the other works that relate to our project and use augmented reality techniques and devices. The related work is split into separated subsections with AR being the common theme applied to different medical areas. Augmented Reality in Mobile Devices for Medical Learning Required clinical content cannot always be imparted in live settings due to various restrictions. Educators have instead started using simulation to enhance clinical education. AR simulations have been used to assist the teaching of emergency situations, procedural training, and anatomy [35]. One such AR simulation is described by Von Jan et al. [1] in their paper. The researchers present an application that may be implemented on cellphones and tablet devices that present life-like scenarios which are overlaid on real-world objects. The trainee would visualize these scenarios through their mobile or tablet devices. This application is called mARble, and the researchers report their findings that indicate that AR enhances learning in medical education settings, specifically for subjects that are visually oriented. Augmented Reality Used in Education In their paper, Steve Chi-Yin Yuen et al. [13] have implemented AR in education and training and evaluated its efficiency. The researchers discuss the applications of AR in various fields including architecture, advertising, entertainment, medicine, gaming, books, travel, and the military. With respect to medical education, their results show AR enhancing surgical procedures and aiding clinical procedures by enhancing efficiency, reducing cost, and improving safety. The researchers also state that AR ahs the potential to invent new clinical and surgical procedures. AR has been integrated with existing medical equipment by Fischer et al. in their research [17]. There has also been research that claims AR to have the potential to make surgery minimally invasive [13] and also to enhance the learning experience in educational settings [29]. Chien et al. have used AR to assist in teaching students the anatomy of a 3D skull [16]. Researchers have also demonstrated that AR may enhance the teaching of human anatomy [19]. AR in Nursing Education Wuller et. al. have reviewed existing AR research to assist nursing education [30]. Foronda et. al. have described 3 types of AR applications used to supplement nursing education [6]. Researchers use the Microsoft Hololens to overlay muscles and bones of the human anatomy on manikins. Rahn et. al., overlay 3D models of human organs in real-time on students using iPads[31]. AR was also used with the help of iPads by Abersold et. al. in their study to assist in the training of the placement of the nasogastric tube(NGT) [32]. Ferguson et. al. have claimed game-based AR applications as having the potential to enhance nursing education [33]. This is also supported by Garrett et. al. who demonstrate improved nursing and clinical skills acquisition in students who participated in AR training scenarios [34]. Simulating Surgeries Scott Delp et al. have reviewed the shortcomings of educating medical personnel in providing appropriate emergency care [2]. In their research Samset et al. [14] developed AR tools for minimal invasive therapies (MIT). Scenarios presented by them include liver surgery, liver tumors, and cardiac surgery. With the help of AR the researchers superimpose real-world objects with 3D models obtained from CT scans. Results demonstrated improved surgical procedures and hence the potential of AR to improve healthcare in terms of utility, quality, and cost-effectiveness. Other research has also been conducted with regards to using AR during surgery. Kawamata et al. describe an AR application in their research that assists in the surgery of pituitary tumors [18]. Their results demonstrated this type of AR navigation allowing surgeons to perform accurate and safe endoscopic operations on these tumors. Memory Retention While Using AR Using Steady State Topography (SST) brain imaging to examine the brain activity of people who participated in AR and non-AR tasks, Heather Andrew et. al. [12] found that the visual attention is almost double when performing AR tasks when compared to non-AR tasks. The author also found that what is stored in memory is 70% higher for AR experiences [12]. Other studies show that the long-term memory of the learner can be enhanced by using multiple media interactions in the learning process [11]. Adedukon-Shittu et. al. have also demonstrated the effectiveness of AR technology with regards to enhancing memory retention and performance [23]. Other studies have also demonstrated the enhanced knowledge acquisition and retention of adequate memory when using AR as a supplemental tool in the education process [24]. Feasibility of Using AR to Train Resuscitation Steve Balian et al. [8] introduced a method of testing the feasibility of using augmented reality to educate healthcare providers about administration of Cardio Pulmonary Resuscitation (CPR). Using the Microsoft Hololens to provide users with audio and visual feedback, the blood flow in the human body was superimposed in real time onto a manikin. The study deployed 51 volunteers for this study. The volunteering health care providers were asked to perform CPR using only the Hololens for two minutes. The chest compression parameters were then recorded for this test. The participants generally responded positively to the system. The approach was perceived to be realistic and the AR was considered a helpful tool for training in medical education. Among the volunteers, 94% stated that they would be willing to use this application for CPR training in the future. The further support the notion of AR’s usefulness in education, Balien et al. successfully demonstrated another augmented reality tool that proved to be valuable for existing education approaches in medical training[16]. Menon et al. [37] developed an augmented reality application to improve the training of nursing students that showed a measurable improvement in student outcomes. Time and again augmented reality has proven to be advantageous when integrated into education in terms of novelty, memory retention, and knowledge gained [14] [12] [23] [24] . AR Triage Training for Multi-Casualty Scenarios The order in which patients are treated can have a detrimental effect on the survival rate of a group of patients. Hence, triage, i.e. selecting the most critical patients based on their chance of survival is crucial. John Hendricks et al. [4] devised a virtual reality simulation that assists medical personnel in their training and military field medics in making appropriate decisions in triage training environments. Their model deploys a scene in which users encounter a virtual patient with multiple injury scenarios. The virtual patients can vary with respect to their injuries as well physiological conditions and these conditions can evolve with timebased on their injuries. The injuries are visually supported by animations, such as bleeding and seizures. Augmented rea",oceanology,7
3552bef6e5478dfec4a704927f0939e0b938f910,filtered,semantic_scholar,39th Annual IEEE Conference on Local Computer Networks Workshops,2014-01-01 00:00:00,semantic_scholar,senseh: from simulation to deployment of energy harvesting wireless sensor networks,https://www.semanticscholar.org/paper/3552bef6e5478dfec4a704927f0939e0b938f910,"Energy autonomy and system lifetime are critical concerns in wireless sensor networks (WSNs), for which energy harvesting (EH) is emerging as a promising solution. Nevertheless, the tools supporting the design of EH-WSNs are limited to a few simulators that require developers to re-implement the application with programming languages different from WSN ones. Further, simulators notoriously provide only a rough approximation of the reality of low-power wireless communication. In this paper we present SENSEH, a software framework that allows developers to move back and forth between the power and speed of a simulated approach and the reality and accuracy of in-field experiments. SENSEH relies on COOJA for emulating the actual, deployment-ready code, and provides two modes of operation that allow the reuse of exactly the same code in real-world WSN deployments. We describe the toolchain and software architecture of SENSEH, and demonstrate its practical use and benefits in the context of a case study where we investigate how the lifetime of a WSN used for adaptive lighting in road tunnels can be extended using harvesters based on photovoltaic panels.",oceanology,8
b8317ef9bcf24c688c67e5062cb388e53b2dd150,filtered,semantic_scholar,,2020-01-01 00:00:00,semantic_scholar,decoding motor skills of ai and human policies: a study on humanoid and human balance control,https://www.semanticscholar.org/paper/b8317ef9bcf24c688c67e5062cb388e53b2dd150,"In this study, we propose a new paradigm of using a machine learning approach to facilitate a quicker, more efficient and effective control development, as a different approach of utilising the power of machine learning in addition to other options that intent to use learning directly in real-world applications. We first develop a DRL-based control framework to learn rich motor skills of push recovery for humanoid robots that exhibit human-like push recovery behaviour. Next, we propose to take advantage of DRL to quickly discover solutions for very difficult problems, and then extract the principles of those policies as guidelines for developing engineered controllers. Furthermore, a comparison between humanoid and human balancing is conducted to show the characteristics of the learned humanoid behaviour. This comparison will show that DRL algorithms can learn a good policy with short development and training time that may require humans years to learn. We analyse input-output data collected from humanoid and human policies and postulate a Minimum-Jerk ModelPredictive Control (MJMPC) Framework that quantitatively reflects both AI and human push recovery policies. I. SCIENTIFIC MOTIVATION From the advancement in computers, computer-aided design for mechanical and electronic engineering, architecture and many other engineering fields emerged. Foreseeing a similar development curve and technology wave, we forecast a new emerging discipline in the near future that uses learning-aided approaches for catalysing control development, alongside other similar applications such as in medicine discovery. In this study, we propose a new paradigm of using a machine learning approach to facilitate a quicker, more efficient and effective control development, as a different approach of leveraging the power of machine learning in addition to other options that intent to use learning directly in real-world applications. Machine Learning and Deep Reinforcement Learning (DRL) in particular have reached an advanced stage to produce powerful policies with better autonomous performances than many state-of-the-art control and planning approaches in robot locomotion [1], robotic manipulation [2], and even the control of complex morphological machines [3]. Notably, DRL’s ability to solve complex problems with a relatively short development time is especially attractive, which is empowered by training policies that maximise the cumulative reward through the exploration of the action and state space, rather than using prior knowledge of the models about the robot, the world, and their interactions. To leverage the capabilities of DRL, we first develop a DRL-based control framework to learn rich motor skills of push recovery for humanoid robots. The complexity in whole-body balancing arises in challenges such as multi(a) Ankle Strategy (b) Hip Strategy (c) Toe Strategy (d) Step Strategy Fig. 1: Human-like Push Recovery strategies emerging from Deep Reinforcement Learning. The discovered behaviours serve as a guideline for the design of certifiable and safe controllers that replicate advantageous strategies from AI policies. contact coordination based on multi-sensory inputs, state transitions between fullyand under-actuated situations, switching policies, and generalising to external disturbances on any body parts, while accounting for all edge cases that a designer has difficulty to consider beforehand. In such a setting, manually designing the individual control strategies and finding a reliable switching mechanism requires both substantial development time, mathematical rigour, and code implementation. On the other hand, through a well-designed DRL framework and task-specific training procedures, a robust policy can be learned automatically by interacting with the environment, requiring only computational power. In particular, as shown in Fig. 1, our learned policy exhibits human-like push recovery behaviour with four typical push recovery strategies emerging naturally: ankle, hip, toe, and IEEE Robotics and Automation Magazine (RAM) paper, presented at IROS 2020. It should be cited as a RAM paper. stepping strategy. Though the learned control policy could possibly be deployed on the real robotic system, the lack of explainability and analytical reasoning of the Neural Network makes it unsuitable for safety-critical applications in real world. Furthermore, due to the demand of large data and sampleinefficient nature of DRL algorithms, complex policies are typically trained in simulation, which cannot guarantee the same performance while transferred directly to the real system [1], and the challenge of reality gap raises concerning about both the safety and performance. To benefit from both the safety and interpretability for the control policy and the versatility and adaptability from learning, we propose to take advantage of DRL to quickly discover versatile, deployable policies and solutions for very difficult problems, and then study, analyse and extract the principles of those policies as guidelines for developing engineered controllers in a reliable manner. By doing so, we utilise the AI-solutions for rapid control development (Fig. 3) to design safe and certifiable controllers which can be verified and deployed on real-world robots (Fig. 2). While classical control development is based on gradually building knowledge that increases the performance incrementally, using a template policy will provide disruptive, innovative solutions that will escalate performance (green line, Fig. 3). DRL is able to achieve good performance by a number of iterations in the DRL learning framework. However, the achieved performance is still comparatively low to what tuning in control can do. Combining both approaches to “kick-start” the iteration process helps to design good controllers. After knowing the system and the controller, it is straightforward to improve upon due to the fact that we are then able to understand why the performance is lower than the optimum, whereas in the case of DRL, there is little influence from human engineers to improve the performance but reshaping the reward and/or altering the learning framework, and relying on the exploration being sufficiently large to achieve high performance. In this paper, we are motivated to study a viable approach to infer underlying principles of an AI policy by studying its perception-action relation, i.e., to some extent, reverseengineer an equivalent controller in terms of functionality based on a black-box policy. This methodology is not only applicable to AI policies, but also to any black-box policies, such as a human policy. Without knowing exactly how push recovery policies are realised by Artificial Neural Network (ANN) or biological human Neural Network, we can still analyse the behaviour at the functionality level by studying their input-output relationship. Based on evidence of optimality in human manipulation tasks [4], we hypothesise that policies for push recovery in humans and humanoid are both optimal control process that follows certain optimal criteria that can be quantified. Following this hypothesis, we analyse and utilise inputoutput data collected from both humanoid and human policies, and propose a Minimum-Jerk Model-Predictive Control (MJMPC) Framework that is able to quantitatively reflect both the AI and human push recovery policies. The engineered controller has high similarity (Coefficient of Determination more than 90%) with the collected data, and also exhibits the same human-like push recovery strategies, which emerge from the proposed MJMPC without the need of manual switching between the strategies. Furthermore, a comparison between humanoid and human balancing is conducted to show the characteristics of the learned humanoid behaviour. This comparison will show that DRL algorithms are very powerful to learn a policy (e.g., balancing) within a short development and training time that may require humans years to learn. In contrast, in order to design an engineered controller from scratch with similar performance, months or even years are needed for developmental iterations, mainly because of the high-redundancy and a diversity of control actions, which are yet challenging to resolve the physical optimality on a high Degree of Freedom (DoF) robot. In this regard, the learning approach is very attractive because of the significant reduction of manual effort, and the learning architecture requires only the design of input-output and rewards. This article shed some light on a new paradigm: the recent high-profile successes in DRL suggest new high-quality value in learning methods that the discovered policies can be used as a basis for speeding up the development of robotic controllers (Fig. 2). As an outcome in this push recovery study, we obtain a certifiable, analysable optimal controller that does not require any state machine or switching mechanism, while exhibiting human-like push recovery strategies, such as ankle, hip, toe, and stepping strategy all in a coherent optimisation process. II. GENERATING COMPLEX MOTIONS FOR HUMANOID ROBOTS THROUGH DEEP REINFORCEMENT LEARNING To use DRL-policies as a basis for analysis, these policies must reach a certain performance threshold that ideally surpasses traditional control approaches both in the types of motions it can generate and the amount of disturbances that it can withstand. DRL has been shown to be capable of learning locomotion and fall recovery policies that surpasses traditional control approaches for quadruped robots in terms of power efficiency, and versatility of motion [1]. In this section, we present a hierarchical learning framework for achieving versatile behaviours during push recovery for humanoid robots as proposed in [5]. The learned policy exhibits a wide range of balancing strategies that are comparable to human push recovery. In particular, the learned policy is able to withstand external distu",oceanology,9
57165b0eb61847bec87cdd6df7a4eb37bd92fc59,filtered,semantic_scholar,Surgical innovation,2020-01-01 00:00:00,semantic_scholar,commercially available head-mounted displays are unsuitable for augmented reality surgical guidance: a call for focused research for surgical applications,https://www.semanticscholar.org/paper/57165b0eb61847bec87cdd6df7a4eb37bd92fc59,"Recent advances in portable computational units, optics, and photonics devices have enabled the scientific community to open many new fronts in biomedical research, with the development of innovative augmented reality (AR) applications exploiting the potentialities offered by head-mounted display (HMD) technology. Such technology has reached the maturity to be translated into commercial products, and published works on HMDs provide glimpses of how AR will disrupt the surgical field, allowing for an ergonomic, intuitive, and 3-dimensional fruition of preoperative and intraoperative information. Nowadays several commercial HMDs, such as Microsoft HoloLens, Meta or Magic Leap, integrate tracking and registration technology, and the deployment of software development kits has reduced technical complexity of custom application development, allowing for a wide range of users to easily create AR applications and attracting researchers to explore their potentialities for the implementation of surgical navigators. The above-mentioned HMDs are designed following an optical see-through (OST) approach, which augments the natural view through the projection of virtual reality information on semitransparent displays in front of the user’s eyes. The OST approach fits well in the surgical domain as it offers an instantaneous full-resolution view of the real world, allowing the natural synchronization of visual and proprioceptive information, and a complete situation awareness. Ongoing research is aimed at the goal of providing a device “conceived as a transparent interface between the user and the environment, a personal and mobile window that fully integrates real and virtual information.”1 Commercial companies are rapidly improving HMD ergonomic aspects, for example, HoloLens 2 features an improved field of view (52° diagonal), which includes eye tracking, and offers more comfortable wearability. However, maximizing surgical accuracy remains a challenge for manufacturers and researchers. Together with ergonomics, the achievement of precision objectives must be addressed to develop a visor suitable for guiding surgical operations, not to mention compliance with medical device regulations. An increasing number of research studies propose the use of commercial HMDs to guide surgical interventions.2 To the best of our knowledge, these works are principally focused on the need to strengthen virtual/real patient registration (eg, use of an external localization system),3 improve virtual content stability,4 and solve calibration issues, and they underestimate the contribution of perceptual issues to the user accuracy. One of the largest obstacles to obtain a perceptually correct augmentation is the inability to render proper focus cues in HMDs; indeed, the majority of systems offers the AR content at a fixed focal distance, failing to stimulate natural eye accommodation and retinal blur effects.5 Our recent work2 suggests to avoid the use of existing HMD-OST, which are not specifically designed for performing tasks in peripersonal space (<1 m), to guide manual tasks requiring a high level of precision, since perceptual issues, particularly “focal rivalry” (ie, inability to see simultaneously in focus the virtual and real content), can affect user performance.5 Most commercial systems (HoloLens, Lumus, Meta, Ora2) indeed have a fixed focal plane at 2 m or more (often infinite). Thus, during manual tasks, virtual content is 903197 SRIXXX10.1177/1553350620903197Surgical InnovationCarbone et al editorial2020",oceanology,10
911f53c6615a08de2d589be5af88baa6f8c7b2b4,filtered,semantic_scholar,CSEDU,2013-01-01 00:00:00,semantic_scholar,understanding pervasive games for purposes of learning,https://www.semanticscholar.org/paper/911f53c6615a08de2d589be5af88baa6f8c7b2b4,"Among the manifold of approaches to technology enhanced learning, game based learning is very attractive. In game based learning, the technological systems employed for the purpose of learning are digital games. Stand-alone serious games are rare. Games deployed for learning need to be embedded into suitable contexts. A particular approach promising from certain didactic perspectives and driven by a variety of characteristics of learning contents and training requirements is embedding those games into the surrounding physical world. Games embedded into the physical world are called pervasive games. The ways of embedding are paramount. There have been numerous attempts to design and to implement pervasive games, in general, and to deploy pervasive games for learning purposes, in particular. The majority of those pervasive games failed quite badly. Storyboarding the interaction between the real world and the virtual world of a pervasive game reveals the essential strengths and weaknesses of the game concept and allows for diagnosing didactic flaws of game play. Beyond its diagnostic power, the approach supports the design of more affective and effective pervasive games. Storyboarding is a methodology of anticipating human experience and, thus, a methodology of didactic design. 1 THE AUTHORS’ POSITION All of us–readers and authors of this manuscript–are aware of the fact that so-called digital natives 1 have other expectations when facing digital media than their parents and teachers. Playful learning, whenever possible, and using digital games for learning without any fear belongs to the widespread expectations teachers and trainers have to fulfill. In response, game based learning and serious games are terms naming some prosperous field of technology enhanced learning. When the learning contents is out there in the surrounding world, it seems plausible to bring the games out there as well–pervasive games concepts evolve. In harsh contrast to the promises, most pervasive games failed badly. There will surely be no superficial and short explanation for a large number of finally disappointing game developments. But understanding the past and 1The term digital natives as polemically opposed to denigratingly calleddigital immigrantsis, exactly in this sense, ascribed to Marc Prensky (Prensky 2001), although the idea as a whole dates back to (Barlow, 1996) writing: “You are terrified of your own children, since they are natives in a world where you will always be immigrants.” shaping the future surely needs some pondering, some exchange of opinions, and several innovative ideas. The authors aim at some small contribution to this process by advocating their position, ◦ that there are decisive characteristics of pervasive games which may be well explicated by suitable approaches of storyboarding applied to pervasive games. Using storyboarding a posteriori, it turns out to work as a diagnostic tool. Doing it a priori, storyboarding becomes a tool for design and development fostering to draw conclusions from lessons learned in earlier projects that failed. Based on the authors’ key position above, one is lead to some more viewpoints worth to be considered. ◦ Pervasive games may be classified according to their pervasiveness which is of didactic relevance. ◦ The crucial embedding of learning contents into game play may be characterized quite well by means of storyboarding terminology. ◦ The storyboarding technology, by its very nature, allows for an explication of the context conditions in which learning is likely to take place. The basic terminology will be introduced briefly to be applied to a larger number of pervasive games. 696 P. Jantke K. and Spundflasch S.. Understanding Pervasive Games for Purposes of Learning. DOI: 10.5220/0004413006960701 In Proceedings of the 5th International Conference on Computer Supported Education (CSEDU-2013), pages 696-701 ISBN: 978-989-8565-53-2 Copyright c 2013 SCITEPRESS (Science and Technology Publications, Lda.) 2 INTRODUCTORY CASE STUDY Before going into the details of discussion, the authors are aiming at an intuitive introduction. Instead of presenting notations in a formal way, a certain digital game is used to exemplify what the present paper is about, which concepts are in use, and how typical problems are formulated and attacked. The game selected for an introduction by example is TREASURE(Chalmers et al., 2005) which is one of the earliest pervasive games. The purpose of the game TREASUREis to learnabout wireless communication. The ideas underlying this game are easy in structure. Figure 1: Interface to the T REASUREgame as it appears in some PDA; picture taken from (Chalmers et al., 2005) with the permission of the authors as it appears in (Jantke, 2006) . Some real urban environment such as a park, e.g., is virtually equipped with virtual treasures 2. Teams of players are running around in pursuit of treasures. Team members in the real world are localized by means of GPS technology relating them to the virtual treasures and to each other. In certain areas, there are WLAN connections allowing players to contact their virtual treasure boxes on the server for upload. 2For the borderline between reality and virtuality, in general, and for its relevance to e-learning, in particular , interested readers are directed to (Jantke and Lengyel, 2012) . (Chalmers et al., 2005) describe variations of the game mechanics. The core idea, however, is lucid. The storyboard in fig. 2 is summarizing the essentials. Figure 2: Storyboard of T REASURE’s game mechanics. Every node is an episode or a scene describing some action. Smaller inscriptions describe actions of the computer system as opposed to actions of human players. Solid lines indicate the passing of a human player from one action to another such as, for illustration, from just walking to picking up some treasure. Dashed green lines indicate that the player’s action causes some actions of the computer system. In turn, dotted blue lines indicate the impact of earlier game actions on the player’s current actions. For instance, virtual treasures can only be discovered and picked up where the computer system has placed them virtually. Arrows indicating update operations of the players’ positions have been dropped. Game playing means moving around, collecting virtual treasures, trying to pickpocket each other, and aiming at uploads of the own virtual treasure to the safe virtual treasure box. The bookkeeping of treasure locations and treasure boxes defines the termination of game play. The simplicity of the storyboard above reflects the simple structure of the underlying game concept. Furthermore, it exhibits that there are no actions of interest performed by the game system except bookkeeping and, thus, determining preconditions of player actions. The game system is not perceived as an actor, but more seen as a supervising game master. Understanding Pervasive Games for Purposes of Learning",oceanology,11
944ad40d87fce6566211eeca78fe0b08eee1e34b,filtered,semantic_scholar,Frontiers in Robotics and AI,2020-01-01 00:00:00,semantic_scholar,trustable environmental monitoring by means of sensors networks on swarming autonomous marine vessels and distributed ledger technology,https://www.semanticscholar.org/paper/944ad40d87fce6566211eeca78fe0b08eee1e34b,"The article describes a highly trustable environmental monitoring system employing a small scalable swarm of small-sized marine vessels equipped with compact sensors and intended for the monitoring of water resources and infrastructures. The technological foundation of the process which guarantees that any third party can not alter the samples taken by the robot swarm is based on the Robonomics platform. This platform provides encrypted decentralized technologies based on distributed ledger tools, and market mechanisms for organizing the work of heterogeneous multi-vendor cyber-physical systems when automated economical transactions are needed. A small swarm of robots follows the autonomous ship, which is in charge of maintaining the secure transactions. The swarm implements a version of Reynolds' Boids model based on the Belief Space Planning approach. The main contributions of our work consist of: (1) the deployment of a secure sample certification and logging platform based on the blockchain with a small-sized swarm of autonomous vessels performing maneuvers to measure chemical parameters of water in automatic mode; (2) the coordination of a leader-follower framework for the small platoon of robots by means of a Reynolds' Boids model based on a Belief Space Planning approach. In addition, the article describes the process of measuring the chemical parameters of water by using sensors located on the vessels. Both technology testing on experimental vessel and environmental measurements are detailed. The results have been obtained through real world experiments of an autonomous vessel, which was integrated as the “leader” into a mixed reality simulation of a swarm of simulated smaller vessels.The design of the experimental vessel physically deployed in the Volga river to demonstrate the practical viability of the proposed methods is shortly described.",oceanology,12
1b2ba411791c8c173050085c44f49deec8d2c953,filtered,semantic_scholar,Computer Supported Cooperative Work (CSCW),2015-01-01 00:00:00,semantic_scholar,design and implementation of teleadvisor: a projection-based augmented reality system for remote collaboration,https://www.semanticscholar.org/paper/1b2ba411791c8c173050085c44f49deec8d2c953,"TeleAdvisor is a versatile projection-based augmented reality system designed for remote collaboration. It allows a remote expert to naturally guide a local user in need of assistance in carrying out physical tasks around real-world objects. The system consists of a small projector and two cameras mounted on top of a tele-operated robotic arm at the worker’s side, and an interface to view the camera stream, control the point-of-view and gesture using projected annotations at the remote expert’s side. TeleAdvisor provides a hands-free, mobile, low-cost solution that supports gesturing by the remote expert while minimizing the cognitive overhead of the local worker. We describe the challenges, design considerations and implementation details of the two phases of the TeleAdvisor prototype, as well as its evaluation and deployment at an industrial manufacturing center. We summarize our understandings from our experiences during the project and discuss the general implications for design of augmented reality remote collaboration systems.",oceanology,13
b2f006d98b2b66f0ee1e5c90065e30ec8fd917b6,filtered,semantic_scholar,"Day 2 Wed, September 04, 2019",2019-01-01 00:00:00,semantic_scholar,"ar and vr applications improve engineering collaboration, personnel optimization, and equipment accuracy for separation solutions",https://www.semanticscholar.org/paper/b2f006d98b2b66f0ee1e5c90065e30ec8fd917b6,"
 With the most recent industry downturn still fresh in many minds, the oil and gas E&P sector is approaching this recovery with a commitment to long-term cost discipline. As a result, augmented reality (AR) and virtual reality (VR) technologies are being adopted by operators and service companies alike as a means of cost savings while driving operational efficiency.
 AR technologies employ enhanced visualization hardware, techniques, and methodologies to create new environments wherein digital and physical objects and their data coexist and interact with one another, enhancing the user experience of the real world (Kunkel and Soechti 2017). VR refers to the full immersion of the user intoand interaction with a completely digital environment. Together, these technologies form the core of immersive experience and a new paradigm in industrial interaction.
 Until recently, these technologies were primarily applied as enhanced entertainment products, most notably within the gaming industry. However, during the past several years, and thanks to the introduction of hands-free, head-mounted display (HMD) technologies, such as Microsoft® HoloLens™ and now HoloLens 2, AR and VR are migrating into the enterprise sector.
 While the oil field has not been as quick to integrate AR and VR as other sectors, such as medicine, defense, and aeronautics, operators and service providers alike have increased adoption overthe past 12 months. Motivated by a mandate to keep operating costs low and improve efficiencies in terms of field processes, operators have begun implementing AR/VR applications as collaborative problem-solving, planning, and design tools.
 For example, some operators are initiating ARconcepts to promote internal use development and prototyping for both oilfield applications and remote refinery inspections. Additionally, service companies are embracing the use of smart glasses and wearable technologies to help improve remote work and collaboration to help increase in-field safety and reduce downtime.
 As part of its strategy to help drive the oil and gas industry's digital transformation, one major service provider is developing AR/VR applications to create digital representations of physical oilfield assets on the Microsoft® HoloLens device. One area of focus is the planning, design, and deployment of solids control, fluid separation, and handling technologies for offshore drilling applications.",oceanology,14
506e55521ddd1442ae6eae58534aa971946acc3f,filtered,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,distributed heterogeneous tracking for augmented reality,https://www.semanticscholar.org/paper/506e55521ddd1442ae6eae58534aa971946acc3f,"Augmented reality (AR) is a technique in which a user’s view of the real world is enhanced or augmented with additional information generated from a computer model (Azuma et al., 2001). The enhancement may consist of virtual artifacts to be fitted into the environment or a display of non-geometric information about existing real objects. Mobile AR (MAR) systems implement this interaction paradigm in an environment in which the user moves, possibly over wide areas (Feiner, MacIntyre, Hoellerer, & Webster, 1997). This is in contrast to non-mobile AR systems that are utilized in limited spaces such as a computer-aided surgery or by a technician’s aid in a repair shop. There are a number of challenges to implementing successful AR systems. These include a proper calibration of the optical properties of cameras and display systems (Tuceryan et al., 1995; Tuceryan, Genc, & Navab, 2002), and an accurate registration of threedimensional objects with their physical counterparts and environments (Breen, Whitaker, Rose, & Tuceryan, 1996; Whitaker, Crampton, Breen, Tuceryan, & Rose, 1995). In particular, as the observer (or an object of interest) moves over time, the 3D graphics need to be properly updated so that the realism of the resulting scene and/or alignment of necessary objects and graphics are maintained. Furthermore, this has to be done in real time and with high accuracy. The technology that allows this real-time update of the graphics as users and objects move is a tracking system that measures the position and orientation of the tracked objects (Koller et al., 1997). The ability to track objects, therefore, is one of the big challenges in MAR systems. This article describes a software framework for realizing such a distributed tracking environment by discovering independently deployed, possibly heterogeneous trackers and fusing the data from them while roaming over a wide area. In addition to the MAR domain, this kind of a tracking capability would also be useful in other domains such as robotics and locationaware applications. The novelty of this research lies in the amalgamation of the theoretical principles from the domains of AR/VR, data fusion, and the distributed software systems to create a sensor-based, wide-area tracking environment. BACKGROUND",oceanology,15
3519511ac74b509ef65457c72b81cec0fdc1a256,filtered,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,d distributed heterogeneous tracking for augmented reality,https://www.semanticscholar.org/paper/3519511ac74b509ef65457c72b81cec0fdc1a256,"Augmented reality (AR) is a technique in which a user’s view of the real world is enhanced or augmented with additional information generated from a computer model (Azuma et al., 2001). The enhancement may consist of virtual artifacts to be fitted into the environment or a display of non-geometric information about existing real objects. Mobile AR (MAR) systems implement this interaction paradigm in an environment in which the user moves, possibly over wide areas (Feiner, MacIntyre, Hoellerer, & Webster, 1997). This is in contrast to non-mobile AR systems that are utilized in limited spaces such as a computer-aided surgery or by a technician’s aid in a repair shop. There are a number of challenges to implementing successful AR systems. These include a proper calibration of the optical properties of cameras and display systems (Tuceryan et al., 1995; Tuceryan, Genc, & Navab, 2002), and an accurate registration of threedimensional objects with their physical counterparts and environments (Breen, Whitaker, Rose, & Tuceryan, 1996; Whitaker, Crampton, Breen, Tuceryan, & Rose, 1995). In particular, as the observer (or an object of interest) moves over time, the 3D graphics need to be properly updated so that the realism of the resulting scene and/or alignment of necessary objects and graphics are maintained. Furthermore, this has to be done in real time and with high accuracy. The technology that allows this real-time update of the graphics as users and objects move is a tracking system that measures the position and orientation of the tracked objects (Koller et al., 1997). The ability to track objects, therefore, is one of the big challenges in MAR systems. This article describes a software framework for realizing such a distributed tracking environment by discovering independently deployed, possibly heterogeneous trackers and fusing the data from them while roaming over a wide area. In addition to the MAR domain, this kind of a tracking capability would also be useful in other domains such as robotics and locationaware applications. The novelty of this research lies in the amalgamation of the theoretical principles from the domains of AR/VR, data fusion, and the distributed software systems to create a sensor-based, wide-area tracking environment. BACKGROUND",oceanology,16
1ab0a05b291c08c69bfadf4118dca3707f820eaf,filtered,semantic_scholar,MACOM,2012-01-01 00:00:00,semantic_scholar,reality considerations when designing a tdma-fdma based link-layer for real-time wsn,https://www.semanticscholar.org/paper/1ab0a05b291c08c69bfadf4118dca3707f820eaf,"In this article we elaborate on reality considerations when designing and implementing application tailored TDMA-FDMA medium access protocol with guaranteed end-to-end delay. We highlight importance of considering underlaying hardware and software components when designing communication protocols for resource constrained platforms. We also show that by combining medium access protocol, bootstrapping, and time synchronization mechanisms within the link-layer, we can limit on average clock drift in the network to 0.5 μs, as well as achieve 81% energy efficiency while keeping collision probability at its minimum of 1%. Finally, we conclude with challenges and lessons learned in real-world deployment of TDMA/FDMA based link-layer with guaranteed end-to-end delay in WSN.",oceanology,17
26960f55ddb24b2338a5eae012c63cd13abb7bdd,filtered,semantic_scholar,2017 IEEE International Symposium on Parallel and Distributed Processing with Applications and 2017 IEEE International Conference on Ubiquitous Computing and Communications (ISPA/IUCC),2017-01-01 00:00:00,semantic_scholar,performance evaluation for wifi dcf networks from theory to testbed,https://www.semanticscholar.org/paper/26960f55ddb24b2338a5eae012c63cd13abb7bdd,"Distributed Coordination Function (DCF) is a basic MAC protocol used in the world-wide WiFi networks and plays a key role in determining the network performance, especially in situations with a large number of users and high-density Access Point (AP) deployed. To achieve a better understanding of the real-world performance of 802.11 DCF networks, we have constructed an emulation platform and a prototype testbed for performance evaluation. The design and implementation of these two platforms are discussed in this paper. The key DCF parameters, i.e., the initial contention window size ($CW_{min}$) and the maximum contention window size ($CW_{max}$), are tuneable so that we are able to study the impact of these DCF parameters on the network performance. These experiment results are compared against with a recently proposed unified analytical framework to examine the model assumptions and system performance bottlenecks. Our results demonstrate that by adapting the values of $CW_{min}$ based on WiFi traffic load, the maximal network throughput can be achieved, and the optimal value of $CW_{min}$ varies when the network size changes. As a reality check, the emerging software defined WiFi network architecture can be optimized for performance enhancement guided by this unified performance model.",oceanology,18
9a7936acb8420f589e457c9dc2a2035f4ecc2809,filtered,semantic_scholar,2016 15th International Conference on Ubiquitous Computing and Communications and 2016 International Symposium on Cyberspace and Security (IUCC-CSS),2016-01-01 00:00:00,semantic_scholar,mixed reality cubicles and cave automatic virtual environment,https://www.semanticscholar.org/paper/9a7936acb8420f589e457c9dc2a2035f4ecc2809,"In Cave Automatic Virtual Environments (CAVEs), a computer generated environment is projected all around a user to fully immerse or eliminate all reference to the real world. Typically, Virtual Reality (VR) CAVEs also track and respond to the user's physical orientation, movements and gestures. Mixed reality environments instead focus on combining real world objects with computer generated ones. In this paper, we focus on the application of Augmented Reality (AR) as a mixed reality technology via (or to) mobile devices such as head-mounted devices, smart-phones and tablets. We present the development of mixed reality applications for mobile (smart-phone and tablet) devices leading up to the implementation of an mixed reality (AR) cubicle for immersive Three Dimensional (3D) visualizations. We also present the results of a study on the familiarity with both VR and AR technologies among students from two institutions of tertiary education. The paper concludes with a discussion of planned deployment and upgrade of mixed reality cubicles using mobile VR equipment.",oceanology,19
e7dd4afd555002393eb68db7f77ee2e1c17e9866,filtered,semantic_scholar,IHI '12,2012-01-01 00:00:00,semantic_scholar,an evolving multi-agent scenario generation framework for simulations in preventive medicine education,https://www.semanticscholar.org/paper/e7dd4afd555002393eb68db7f77ee2e1c17e9866,"We describe the design, implementation and evaluation of a novel multi-agent scenario generation framework for interactive virtual reality simulations towards preventive medicine education. Our scenario generation framework is based on recordings of human movements from a distributed sensor networks deployed in a real-world physical setting. The components of our framework include the generation of unique virtual agent behaviors from the sensor data, and algorithms for the generation of low level or gross movement behaviors such as path determination, directional traffic flows, collision avoidance and overtaking. The framework also includes the generation of high level fine actions for multi-agents such as techniques for interactive activities in pedagogical scenarios based on environment and temporal triggers. We applied our multi-agent scenario generation framework in an interactive simulation for hand hygiene education, and conduct an initial usability study to assess the educational benefits of the simulation to nursing students and evaluated the performance characteristics of our framework. Results of our quantitative and qualitative evaluations suggest that our framework was robust in creating engaging, compelling, and realistic interactive training scenarios with multiple virtual agents in simulated hospital situations.",oceanology,20
4dd4afbb17999bdf9e218001e3a6ae2252c10f8f,filtered,semantic_scholar,Defense + Security,2017-01-01 00:00:00,semantic_scholar,visualizing uas-collected imagery using augmented reality,https://www.semanticscholar.org/paper/4dd4afbb17999bdf9e218001e3a6ae2252c10f8f,"One of the areas where augmented reality will have an impact is in the visualization of 3-D data. 3-D data has traditionally been viewed on a 2-D screen, which has limited its utility. Augmented reality head-mounted displays, such as the Microsoft HoloLens, make it possible to view 3-D data overlaid on the real world. This allows a user to view and interact with the data in ways similar to how they would interact with a physical 3-D object, such as moving, rotating, or walking around it. A type of 3-D data that is particularly useful for military applications is geo-specific 3-D terrain data, and the visualization of this data is critical for training, mission planning, intelligence, and improved situational awareness. Advances in Unmanned Aerial Systems (UAS), photogrammetry software, and rendering hardware have drastically reduced the technological and financial obstacles in collecting aerial imagery and in generating 3-D terrain maps from that imagery. Because of this, there is an increased need to develop new tools for the exploitation of 3-D data. We will demonstrate how the HoloLens can be used as a tool for visualizing 3-D terrain data. We will describe: 1) how UAScollected imagery is used to create 3-D terrain maps, 2) how those maps are deployed to the HoloLens, 3) how a user can view and manipulate the maps, and 4) how multiple users can view the same virtual 3-D object at the same time.",oceanology,21
83c19e91c197218df688172968455ff9d4efc7fe,filtered,semantic_scholar,,2017-01-01 00:00:00,semantic_scholar,enhancing liveness testing for transferring data packets through using automatic test packet generation,https://www.semanticscholar.org/paper/83c19e91c197218df688172968455ff9d4efc7fe,"-Networks are getting bigger and more complex, yet administrators rely on incomplete tools such as and to debug problems. We propose an automated and systematic approach for testing and rectify networks called “Automatic evaluates Package Generation” (ATPG). ATPG reads router configurations and generates a device-independent model. The model is used to generate a minimum set of test packets to minimally exerting every link in the network or maximally exerting every rule in the network. Test packets are sent periodically, and detected failures trigger a separate mechanism to localize the revoke. ATPG can detect both functional and renderings problems. ATPG complements but goes beyond earlier work in static checking for which cannot detect liveness or performance faults or fault localization which only localize revoke given liveness results. We describe our prototype ATPG implementation and results on two real-world data sets: Stanford University’s backbone network and Internet. We find that a small number of test packets suffice to test all rules in these networks. A sending 4000 test packet 10 times per second consumes less than 1% of link capacity. ATPG code and the datasets are publicly available. Keyword: ATPG, liveness, Networks. ________________________________________________________________________________________________________ I.INTRODUCTION Networking is the word fundamentally cogitates to computers and their property. It is very often used in the world of computers and their use in different connections. The term networking express the link between two or more computers and their tendency, with the vital purpose of sharing the data stored in the computers, with each other. The networks between the computing tendencies are very public these days due to the launch of assorted hardware and computer software which aid in making the activity much more convenient to build and use. Fig: 1.1Structure of Networking between the different computers The discuss about Figure: 1.1 Structure of Networking between the different computer. Its main process of share the Internet to different things and devices. General Network Techniques When computers communicate on a network, they send out information packets without knowing if anyone is listening. Computers in a network all have an attached to the network and that is called to be attached to a network bus. What one computer sends out will reach the other computer on the local area network. © 2017 IJEDR | Volume 5, Issue 1 | ISSN: 2321-9939 IJEDR1701073 International Journal of Engineering Development and Research (www.ijedr.org) 476 Fig: 1.2 the clear idea about the networking functions The discus about figure: 1.2 clear ideas of network function and different computers to be able to distinguish between each other, every computer have unique ID called MAC-address Media Access Control Address. This address is not only unique on your network but unique for all tendencies that can be aquiline up to a network. The MAC-address is tied to the hardware and has nothing to do with IP-addresses. Since all computers on the network graduate inversion that is sent out from all other computers the MACaddresses is primarily used by the computers to filter out incoming network traffic that is addressed to the scratcher computer. When a computer expostulation with another computer on the network, it sends out both the other computers MAC-address and the MACaddress of its own. In that way the receiving computer will not only realize that this parcel is for me but also, who sent this data packet so a return response can be sent to the sender. Ethernet network as delineate here, all computers hear all network aggregation since they are attached to the same bus. This network structure is called multi-drop. One problem with this network structure is that when you have, let say ten computers on a network and they expostulation attendance and due to that they sends out there data packets randomly, collisions occur when two or more computers sends data at the same time. When that happens data gets imperfect and has to be resent. On a network that is heavy loaded even the resent packets collide with other packets and have to be resent again. In reality this soon takes affect an information problem. If respective computers communicate with each other at high speed they may not be able to utilize more than 25% of the total network information measure since the rest of the bandwidth is used for regressive antecedently corrupted packets. The way to minimize this problem is to use network switches. II.RELATED NETWORK Detecting the occurrence and location of performance anomalies is critical to ensuring the effective operation of network infrastructures. In this paper we present a framework for detecting and apposition performance anomalies based on using an active investigate measurement infrastructure deployed on the periphery of a network. Our framework has three components: an algorithm for detection performance oddball on a path, an algorithm for environs which paths to probe at a given time in order to detect performance anomalies where a path is defined as the set of links between two sampling nodes, and an algorithm for designation the links that are causing an identified anomaly on a path The path selection algorithm is designed to enable a interchange between insure that all links in a network are of times monitored to detect performance anomalies, while minimizing probing overhead.[1] This paper, we develop failure-resilient techniques for monitoring link detain and imbecility in a Service Provider or endeavor IP network. Our two-phased approach attempts to minimize both the monitoring infrastructure costs as well as the additional aggregation due to probe messages. In the first phase, we compute the particular point of a minimal set of monitoring stations such © 2017 IJEDR | Volume 5, Issue 1 | ISSN: 2321-9939 IJEDR1701073 International Journal of Engineering Development and Research (www.ijedr.org) 477 that all network links are covered, even in the bigness of several link reverting. Afterwards, in the second phase, we compute a minimal set of probe messages that are transmitted by the stations to measure link delays and isolate network faults these approximation ratios are provably very close to the best possible bounds for any algorithm. [2]We present a new symbolic execution tool, KLEE, capable of automatically induce tests that wangle high coverage on a diverse set of complex and environmentallyintense programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment position on millions of UNIX systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage on average over 90% per tool. [3] The emergence of Open Flow-capable switches enables exciting new network functionality, at the risk of programming errors that make communication less reliable. The centralized programming model, where a single accountant program manages the network, seems to reduce the likelihood of bugs. However, the system is inherently scattered and asynchronous, with events happening at different switches and end hosts, and inevitable delays affecting communication with the controller. In this paper, we present economic, regular techniques for testing unqualified controller programs. Our NICE tool applies model checking to explore the state space of the entire system the controller, the switches, and the hosts. [4] Network performance tomography, characteristics of the network interior, such as link loss and packet latency, is inferred from unrelated end-to-end measurements. Most work to date is based on employ packet level correlations. However, these methods are often limited in scope-multicast is not widely deployed-or require deployment of additional hardware or software system. Some recent work has been successful in reaching a less detailed goal: identifying the lossiest network links using only unrelated end-toend sampling. In this paper, we abstract the properties of network performance that allow this to be done and exploit them with a quick and simple deduction algorithm that, with high likely, separate the worst performing links. [5] III. SYSTEM ANALAYS Automatic Test Packet Generation (ATPG) framework that self-loading generates a minimal set of collection to test the liveness of the underlying topology and the congruence between data plane state and redundancy description. The tool can also automatically generate packets to test performance assertions such as packet latency. It can also be specialized to generate a minimal set of packets that merely test every link for network liveness.  A survey of network operators revealing common failures and root causes.  A test packet generation algorithm.  A revoke localization algorithm to isolate faulty devices and rules.  ATPG use cases for functional and performance testing.  Evaluation of a prototype ATPG system using rule sets collected from the Stanford and Internet2 backbones.",oceanology,22
2b4cfc0b672aafc526bda6f7e35cf204ece0d34c,filtered,semantic_scholar,,,semantic_scholar,ac 2009-1203: a novel interdisciplinary sensor networks laboratory,https://www.semanticscholar.org/paper/2b4cfc0b672aafc526bda6f7e35cf204ece0d34c,"Today, networks of legacy and newer sophisticated sensors and actuators that combine reconfigurable gigascale semiconductor technology with emerging micro-mechanical systems (MEMS) and nanotechnology subsystems (i.e. bio-systems/chemical/fluidics/photonics/ etc) are being designed and deployed in almost every area of technology that impacts human endeavor and commerce. These smart sensors/actuators are being networked together through: either standards based or industry specific, proprietary, wired networks or newly emerging wireless networking technologies. Presently, at the twoand four-year college level, technicians and technologists in a wide variety of impacted disciplines are not receiving an adequate education about: fundamental sensor theory, basic sensor operation, sensor system deployment planning, appropriate data-transport and networking connectivity schemes, applications software, and impending system maintenance support needs of these increasingly more sophisticated and complex, smart sensor/actuator based systems. This paper will report on the development, organization, and use of a novel interdisciplinary sensor networks laboratory. The heart of the laboratory is a dedicated data-network (SensorNet) that emulates a wide area network or WAN. The SensorNet WAN nodes and other network access points allow for the interconnection of numerous types of industry specific and standard “area networks” typically utilized for the gathering of sensor data and directing other sensor functions, as well as, the associated PC’s and servers used to direct the sensor systems and warehouse the gathered data. This laboratory environment lends itself to real world case-study and problem-based type student-centered learning experiences that can be themselves integrated into established fields of technology that do not normally include this type of activity as part of the field’s traditional educational experience at the undergraduate level. I. Overview Although it is not uncommon for several different technology fields to converge together, it is somewhat unexpected to observe such an amalgamation rapidly triggering other technologic innovations that have widespread potential to change our relationship with the environment and our daily endeavors. However, this is just what is happening today. Only a short time ago, the Internet, the result of a convergence of several technologies, spawned the development of what is commonly known as the “Information Economy.” Today another innovative and important convergence of technologies has recently gained critical mass and recognition by business and industry, government, academia, and professional societies. It is the deployment of intricate systems involving complex sensors with embedded (ambient) intelligence and advanced actuators coupled with modern data-transport and networking technologies and applicationenabling software with data fusion capabilities. This rapidly evolving convergence of technologies, which allows us to implement sensor systems that gather in situ (remote), realtime, statistically relevant information and interpret it in new and novel ways, has already started to transform automation and process control systems. The technology of networked sensor systems has the very genuine potential to significantly impact almost every aspect of human endeavor by increasing system efficiency, reducing energy consumption, permitting the real-time monitoring of the “health” of the nation’s infrastructure and environment, and improving public health and safety. Applications are limitless! P ge 1.77.3 On a global level, the NSF has been calling this “grand convergence,” cyberinfrastructure. One may find many references to this concept, forecasts of potential future applications, reports on inprogress test projects such as HPWREN, NIMS, and ROADnet, and potential research funding opportunities on the NSF’s Web site [1] . However, most of this current, enthusiastic attention and promotion of cyberinfrastructure by the NSF is aimed at senior, graduate-level research institutions. Not surprisingly, most of the NSF’s recent Requests for Proposals (RFPs) in this area have been targeted at basic research about wireless sensor networks and systems and applications of these systems to infrastructure and environmental monitoring and other technology areas. While many applications of networked sensor systems are yet to be even thought of, the reality is that they are being deployed today and will continue to proliferate for many years to come until they eventually become as commonplace as a typical public utility like electricity. This paper describes aspects of an NSF funded CCLI project (DUE 0736888), titled, “The Sensor Networks Education Project” (SNEP) that seeks to develop materials and a model teaching laboratory that will be useful for other faculty and organizations at the twoand even the four-year college level to emulate. This project looks at this evolving convergence on a more practical level and speaks to the lack of engineering technology faculty expertise and teaching materials needed to infuse the newly recognized, exponentially growing knowledge base of networked sensor technology into the curricula and hence into the skill-sets of today’s twoand four-year technical college graduates – the technicians and technologists of tomorrow. This is the community of workers that will most likely deal with the design, deployment, updating, and maintenance of these systems. Today, networks of legacy and newer sophisticated sensors that combine reconfigurable gigascale semiconductor technology with emerging micro-electromechanical systems (MEMS) and nanotechnology subsystems [2] (i.e. bio-systems/chemical/molecular/photonic) are being designed and deployed in almost every area of technology that impacts human endeavor and commerce (i.e. Aerospace, Agriculture, Automotive, Biomedical, Building Automation, Energy Exploration and Production, Environmental Monitoring, Healthcare, Homeland Security, Industrial Automation, Infrastructure Monitoring, Information Technology, Manufacturing, Military, Pharmaceutical, Telecomm, Transportation, Weather Forecasting, etc). These sensors are being networked together through: either standards based or industry proprietary wired networks or emerging wireless networking technologies. Presently, at the twoand four-year college level, technologists and technicians in a wide variety of impacted disciplines are not receiving an adequate education about: fundamental sensor theory, basic sensor operation, sensor system deployment planning, appropriate data-transport and networking connectivity schemes, applications software, and impending system maintenance support needs of these increasingly more sophisticated sensor based systems. Recently, there has been a great deal of public dialogue about the out-sourcing of American manufacturing jobs and the effect of this reality on the nation’s future. Dealing with an ever increasing base of physical sensor networks in all areas of endeavor will not be something that can be done through a call to a help desk located in a foreign country. The apparent curriculum shortcoming regarding these topics within today’s associate and bachelors degree technology oriented programs is primarily due to the extremely rapid evolution and convergence of several key areas of electronics, computer, and MEMS technology (i.e. embedded processing, smart P ge 1.77.4 sensors/actuators, wired and wireless networking, etc), the lack of appropriate up-to-date educational materials, and a lack of appropriate faculty expertise in this rapidly expanding and remarkably cross-disciplinary field. II. Project Overview Over its two-year life-span, this CCLI Phase I project has as its primary goals the creation and testing of interdisciplinary student-centered learning materials primarily designed for a “field laboratory” type environment, the dissemination of these materials, and the development of faculty expertise in the multi-disciplinary field of networked sensors and modern active-learner teaching techniques. To accomplish these goals the project will: (1) develop and deploy a model, innovative, replicate-able, multi-interdisciplinary, case-study and problem-based oriented, networked, distributed sensor laboratory, (2) develop basic and advanced instructional materials and standard and “hybrid” laboratory activities related to the sensor laboratory that can be utilized for introductory courses in sensor technology or more advanced courses in networked sensor systems for use by both twoand four-year technology programs, (3) develop several prototype multifaceted educational modules that integrate traditional science and math based theory, practical real-world laboratory exercises, and science based, high-resolution, interactive simulation software, applicable to several of the major technology areas employing networked sensor technology (i.e. building automation and infrastructure monitoring and industrial automation), and (4) provide on-going local, regional, and national dissemination of these developed materials and laboratory experiences through hands-on faculty workshops and webbased distribution technologies including the National Science Digital Library (NSDL). In addition, for the duration of the project, continuous on-going professional development in the principles and applications of student-centered and active learner techniques will be provided to the recruited college faculty that will take part in the project. Research has shown that long term professional development programs are more effective than short-term workshops. For this project to be successful, the participating faculty must learn how to effectively integrate content and pedagogy in a way that actively engages students in individual and collaborative problem solving, analysis, synthesis, critical thinking, reasoning, and skillfully applying knowledge in real-w",oceanology,23
0c177e2385aad71584840a21b23ebd4b58c00132,filtered,semantic_scholar,,2012-01-01 00:00:00,semantic_scholar,service virtualization: reality is overrated,https://www.semanticscholar.org/paper/0c177e2385aad71584840a21b23ebd4b58c00132,"Software drives innovation and success in todays business world. Yet critical software projects consistently come in late, defective, and way over budget. So whats the problem? Get ready for a shock, because the answer to the problem is to avoid reality altogether. A new IT practice and technology called Service Virtualization (SV) is industrializing the process of simulating everything in our software development and test environments. Yes, fake systems are even better than the real thing for most of the design and development lifecycle, and SV is already making a huge impact at some of the worlds biggest companies. Service Virtualization:Reality Is Overratedis the first book to present this powerful new method for simulating the behavior, data, and responsiveness of specific components in complex applications. By faking out dependency constraints, SV delivers dramatic improvements in speed, cost, performance, and agility to the development of enterprise application software. Writing for executive and technical readers alike, SV inventor John Michelsen and Jason English capture lessons learned from the first five years of applying this game-changing practice in real customer environments. Other industriesfrom aviation to medicinealready understand the power of simulation to solve real-world constraints and deliver new products to market better, faster, and cheaper. Now its time to apply the same thinking to our software. For more information, see servicevirtualization.com. What youll learnYou will learn why, when, where, and how to deploy service virtualization (SV) solutions to mitigate or eliminate the constraints of an unavailable or unready service system by simulating its dependent components in order to deliver better enterprise software faster and at lower cost. In particular, you will learn step-by-step why, when, where, and how to deploy the following SV solutions: shift-left infrastructure availability performance readiness test scenario management Who this book is for This book is not only for IT practitioners on engineering, testing, and environments teams engaged in the development and delivery of enterprise software, but also for executives of companies in all sectors who need to understand and implement emergent opportunities to improve the time to market and overall competitiveness of any outward-facing business strategy that has a software application component. Table of ContentsForeword by Burt Klein Chapter 1. Introduction Service Virtualization Briefly Defined Key Practices Enabled by SV Shift-Left Infrastructure Availability Performance Readiness Test Scenario Management Navigating This Book Chapter 2. The Business Imperative: Innovate or Die Consumers Have No Mercy Business Demands Agile Software Delivery Increased Change and Complexity for IT Simulation Is Not Just for Other Industries Chapter 3. How We Got Here From Monolithic to Composite Apps Todays Complex Service Environments From Waterfall to Agile Development Chapter 4. Constraints: The Enemy of Agility Unavailable Systems and Environments Conflicting Delivery Schedules Data Management and Volatility Third Party Costs and Control Chapter 5. What is Service Virtualization? The Opposite of Server Virtualization Creation of a Virtual Service Maintaining Virtual Services What Kinds of Things You Can virtualize Virtual Service Environments (VSEs) Chapter 6. Where to Start with SV? Pick a Hairy Problem Identify Stakeholders Set Real Value Goals for Releases Avoid Inappropriate Technologies Chapter 7. Capabilities of Service Virtualization Technology Live-Like Development Environment Automation Eliminates Manual Stubbing and Maintenance Enables Parallel Dev and Test No more Availability Problem Platform-Neutrality Chapter 8. Best Practice #1: Shift-Left Reducing Wait Time Early Component and System Testing Define SV from Capture Define Incomplete SV from Requirements Expected Results Customer Example Chapter 9. Best Practice #2: Infrastructure Availability Finding Over-Utilized Resources Virtualizing Mainframes Avoiding Big IT Outlays Expected Results Customer Example Chapter 10. Best Practice #3: Performance Readiness Virtualizing Performance Environments Informing Performance from Production Expected Results Customer Example Chapter 11. Best Practice #4: Test Scenario Management Managing Big Data Shielding Teams from Volatility Massively Parallel Regression Testing Expected Results Customer Example Chapter 12. Rolling out Service Virtualization Who Pays for Service Virtualization? Overcoming Organizational Challenges Who Manages a VSE? Should I Have More Than One? Key Skills and Roles in a Virtual IT World Chapter 13. Service Virtualization in the DevTest Cloud Constraints of Cloud Dev and Test Achieving Elastic Cloud Environments Chapter 14. Assessing the Value Key Metrics for Success Areas for Improvement Chapter 15. Conclusion The Industrialized Software Supply Chain Innovate and Thrive Whats Next for SV? Glossary About the Authors",oceanology,24
eebc85dcbbf0b5904316256d57cf7e9c3488d024,filtered,semantic_scholar,,2012-01-01 00:00:00,semantic_scholar,e-science approaches in molecular science,https://www.semanticscholar.org/paper/eebc85dcbbf0b5904316256d57cf7e9c3488d024,"Computer simulations of the properties of processes and materials are becoming increasingly necessary in several technological and environmental studies. This implies a growing demand of computing resources that severely exploits computational environments in terms of sustainability and reliability of the infrastructure.
 The developments in computing hardware and software, in particular the deployment of world-wide reliable Grid Computing infrastructures, the adoption of innovative computing approaches like the General Purpose Graphic Processing Unit (GPGPU) Computing and the High Performance Network environments, stimulate the exploitation of new approaches and methodologies in Computational Sciences. Furthermore the advances made in the World Wide Web, allow the implementation of Web sites from which the simulation of elementary chemical processes at molecular level is performed combining various techniques and computational approaches which are executed on High Throughput Computing (HTC) and/or High Performance Computing (HPC) platforms.
 The ubiquity of information and computing resources has impacted on the researchers? productivity, in a similar way the same technologies impacted everyone?s daily life. The E-science technologies facilitate the exchange of information among researchers, enhance the collaborative work and increase the quality of dissemination of results. Several European initiatives are devoted to facilitate researchers? work and to establish networks among researchers of the various disciplines, enabling some European research groups to reach leading positions in their disciplines. We have got the support of the EU COST (COllaboration in Science and Technology) Initiative in two Actions devoted to the facilitation of adoption of Grid and Distributed Computing technologies in Molecular and Matter Sciences. In particular I participated to the COST D23 Action: Metachem - Metalaboratories for Complex Computational Applications in Chemistry (2000-2005), and I coordinated the Working Group: Simbex: a metalaboratory for the a priori simulation of crossed molecular Beam Experiments. Furthermore I participated to the COST D37 Action: Grid Computing in Chemistry: GRIDCHEM (2006-2009), and I coordinated the Working Group: ELAMS: E-science and Learning Approaches in Molecular Science.
 The outcomes of both COST Actions contributed significantly to establish an active group of Computational Chemistry and Molecular and Matter Science laboratories which adopted the Grid Computing as an innovative computing paradigm for performing massive computational campaigns.
 Since 2004 the researchers of such laboratories joined the Virtual Organization (VO) CompChem established on the EGEE Grid Infrastructure, the largest distributed computing environment ever established worldwide, and coordinated by CERN (Conseil Europeen pour la Recherche Nucleaire). I served as VO Manager since the VO was established, under the coordination of Prof. Antonio Lagana, Department of Chemistry, University of Perugia.
 The present thesis covered a long period of research work focused on implementing some e-science instruments to the computational chemistry community, in particular the community of users belonging to the COMPCHEM Virtual Organization active in the EGEE/EGI European Grid Initiative.
 The Thesis describes some tools and approaches the author adopted to provide innovative tools to the Computational Chemistry community based on two main pillars:
1. approaches for running large computational campaigns on Grid Infrastructures 2. adopting virtual reality techniques for making more intuitive the interaction with nanoscale computing approaches and simplifying the definition of the initial conditions of the molecular simulations
 The research work originated 10 research papers, several of them produced as a joint work with European laboratories interested in the implementation of e-science tools for a smart, curious and demanding community like the Computational Chemistry one.
 The author has been able to provide a useful view of the molecular world through the use of virtual reality techniques, combined with the most advanced Web technologies, in particular using the ISO standard X3D for the 3D visualization and interaction with a virtual world. These innovative tools enabled the researchers to set up the environment for carrying out complex molecular simulations (as in the case of the Dl-Poly software package) in a intuitive and visual way. Once defined the species interacting in the considered molecular system, represented in a virtual world, the system produce the input file for the simulation and the Dl-Poly program may be launched, possibly on a Grid infrastructure to take benefit of the powerful available computational resources.
 In chapters 1, 2 and 3 the various steps toward the implementation of an a-priori molecular simulator on the EGEE/EGI Grid, for the COMPCHEM VO users, are reported.
 Molecular Virtual Reality applications are really useful as e-learning support tools for Chemistry students. To this purpose we implemented a Learning Management System based on a semantic web approach, described in chapter 7, and an assessment system, described in chapter 8, which have been used several times to assess the competences of students participating to the Erasmus Mundus Master of Science in Theoretical Chemistry and Computational Modeling. The system enables the coordinators of the Master to monitor the progresses made by the students an a daily basis.
 The author has shown how it is possible to use virtual reality approaches to describe a chemical experiment at both human and molecular level using a virtual reality approach. To this end a multi-scale virtual reality approach has been adopted to deal with the description of the physical environment,HVR. The main features of the virtual reality representation of the experiments and the potentiality of associating VRML and X3D with Java engine calculator are outlined in chapter 4.
 In chapter 5 an X3D Molecular Virtual Reality environment in which the researcher is able to interact with it by using immersive devices and dynamic gestures is described. By using the gestures the researcher is able to modify the composition of the molecular system by adding or subtracting functions and the molecular properties of the new species are evaluated in real time by invoking a Web Service implementing the simulation environment. This has required the assemblage of an innovative approach coupling the management of immersive devices with Web Services and molecular dynamics packages.
 In chapter 6 the author has presented an X3D Molecular Virtual Reality environment which makes usage of the most recent and powerful HTML and Web technologies. The approach implemented takes into account the modern approaches followed in implementing Social Networking environments and showed how useful these approaches are also in implementing scientific environments. We think such type of work is important also in consideration of how our lifestyle is changing, thanks to the ubiquity of the information, the availability of an increasing availability of storage and computing power. The social networking showed us how deep may be the impact of the computing and networking facility in the daily life and similarly the computational science, and the computational chemistry in particular, has to reshape the classical approaches and methodologies in order to gain advantage of the modern computing platforms and the powerfulness of the networking, distributed and mobile environments.",oceanology,25
d70a02826a51efc7d133f688a820e73e6a1a1b44,filtered,semantic_scholar,SSW,2010-01-01 00:00:00,semantic_scholar,semantic high level querying in sensor networks,https://www.semanticscholar.org/paper/d70a02826a51efc7d133f688a820e73e6a1a1b44,"The quick development and deployment of sensor technology within the general frame of the Internet of Things poses relevant opportunity and challenges. The sensor is not a pure data source, but an entity (Semantic Sensor Web) with associated metadata and it is a building block of a “worldwide distributed” real time database, to be processed through real-time queries. Important challenges are to achieve interoperability in connectivity and processing capabilities (queries) and to apply “intelligence” and processing capabilities as close as possible to the source of data. This paper presents the extension of a general architecture for data integration in which we add capabilities for processing of complex queries and discuss how they can be adapted to, and used by, an application in the Semantic Sensor Web, presenting a pilot study in environment and health domains. 1 Background and Motivation The rapid development and deployment of sensor technology involves many different types of sensors, both remote and in situ, with such diverse capabilities as range, modality, and manoeuvrability. It is possible today to utilize networks with multiple sensors to detect and identify objects of interest up close or from a great distance. Connected Objects – or the Internet of Things – is expected to be a significant new market and encompass a large variety of technologies and services in different domains. Transport, environmental management, health, agriculture, domestic appliances, building automation, energy efficiency will benefit of real-time reality mining, personal decision support capabilities provided by the growing information shadow (i.e. data traces) of people, goods and objects supplied by the huge data available from the emerging sensor Web [1]. Vertical applications can be developed to connect to and communicate with objects tailored for specific sub domains, service enablement to face fragmented connectivity, device standards, application information protocols etc. and device management. Building extending connectivity, connectivity tailored for object communication – with regards to business model, service level, billing etc, are possible exploitation areas of the Internet Connected Objects. Important challenges are to achieve interoperability in connectivity and processing capabilities (queries, etc.), to distribute “intelligence” and processing capabilities as close as possible to the source of data (the Giordani I., Toscani D., Archetti F. and Cislaghi M.. Semantic High Level Querying in Sensor Networks. DOI: 10.5220/0003116600720084 In Proceedings of the International Workshop on Semantic Sensor Web (SSW-2010), pages 72-84 ISBN: 978-989-8425-33-1 Copyright c 2010 SCITEPRESS (Science and Technology Publications, Lda.) sensor or mobile device), in order to avoid massive data flows and bottlenecks on the connectivity side. The sensor is not a pure data source, but an entity (Semantic Sensor Web) with associated domain metadata, capable of autonomous processing and it is a building block of a “worldwide distributed” real time database, to be processed through realtime queries. The vision of the Semantic Sensor Web promises to unify the real and the virtual world by integrating sensor technologies and Semantic Web technologies. Sensors and their data will be formally described and annotated in order to facilitate the common integration, discovery and querying of information. Since this semantic information ultimately needs to be communicated by the sensors themselves, one may wonder whether existing techniques for processing, querying and modeling sensor data are still applicable under this increased load of transmitted data. In the following of this paper we introduce the state of the art in data querying over network of data providers. In Sect. 2 we present the software architecture of a data integration system in which we added complex query processing features. Sect. 3 introduces the case study in which we deployed our system: the study of short term effect of air pollution on health. Sect. 4 presents the detailed implementation of the querying features together with results on real data sets. Finally, Sect 5 presents the conclusions and future work. 1.1 State of the Art This paper stems from the work presented in [12], in which is presented a software system aimed at forecasting the demand of patient admissions on health care structures due to environmental pollution. The target users of this decision sup-port tool are health care managers and public administrators, which need help in resource allocation and policies implementation. The key feature of that system was the algorithmic kernel, to perform time series analysis through Autoregressive Hidden Markov Models (AHMM) [7]. The scenario in which the system has been deployed is the research project LENVIS1, which is aimed to create a network of services for data and information sharing based on heterogeneous and distributed data sources and modeling. One of the innovations brought by LENVIS is the “service oriented business intelligence”, i.e. an approach to Business Intelligence in which the information presented to the user comes from data processing that is performed online, i.e. data are extracted under request of the applications, and on the basis of data availability, i.e. data are exchanged through web services, which does not guarantee response time neither availability. Such a complex environment, in which data sources are distributed over the internet, is common to several problems and has been faced by different approaches. One of them is that of [13], in which “monitoring queries” continuously collect data about spatially-related physical phenomena. An algorithm, called Adaptive Pocket Driven Trajectories, is used to select data collection paths based on the spatial layout of sen1 LENVIS Localised environmental and health information services for all. FP7-ICT-2007-2. Project number 223925. www.lenvis.eu 73",oceanology,26
fa12864c51585278e00cb926f185a2f69b70675b,filtered,semantic_scholar,,2002-01-01 00:00:00,semantic_scholar,equip: a software platform for distributed interactive systems,https://www.semanticscholar.org/paper/fa12864c51585278e00cb926f185a2f69b70675b,"EQUIP is a new software platform designed and engineered to support the development and deployment of distributed interactive systems, such as mixed reality user interfaces that combine distributed input and output devices to create a coordinated experience. EQUIP emphasises: cross-language development (currently C++ and Java), modularisation, extensibility, interactive performance, and heterogeneity of devices (from handheld devices to large servers and visualisation machines) and networks (including both wired and wireless technologies). A key element of EQUIP is its shared data service, which combines ideas from tuplespaces, general event systems and collaborative virtual environments. This data service provides a uniquely balanced treatment of state and event-based communication. It also supports distributed computation – through remote class loading – as well as passive data distribution. EQUIP has already been used in several projects within the EQUATOR Interdisciplinary Research Collaboration (IRC) in the UK, and is freely available in source form (currently known to work on Windows, IRIX and MacOS-X platforms). INTRODUCTION The development of novel interactive devices and the deployment of mobile communication infrastructures have fuelled a growing focus on ubiquitous interactive systems that support people within real world environments. These systems place digital information in physical spaces [28] focusing on the delivery of information to users through a heterogeneous collection of devices ranging from handheld and wearable computers to large embedded displays. The majority of these systems have exploited a sense of location as a contextual cue to drive the interaction. An equally significant trend has been the growth in the number and diversity of collaborative virtual environments to manage cooperative interaction [2, 12, 26]. Just as ubiquitous computing environments exploit real world location, these systems exploit a sense of location within a virtual world as a contextual cue for interaction. However, despite significant similarities, these two research approaches have often tended to be seen in opposition to each other, with ubiquitous computing embedding computers with the world of users, and virtual environments embedding users within a computer generated world [16]. As part of our ongoing research we are exploring the advantages to be gained through the convergence of these approaches, allowing a collaborative virtual environment to be overlaid on top of a shared physical space. A number of key advantages motivate our desire to combine the physical and virtual to support interactive systems: The ability to exploit the coextensive virtual world as a ‘behind the scenes’ resource for coordinating and managing devices and interaction in the physical space. The opportunity to develop applications that span the physical and digital realms, for example that require collaboration between field operatives and controlroom personnel. The chance to support new kinds of interactive experience, combining elements from virtual worlds (e.g. rich media content, high interactivity) with varied modes of access over extended geographical areas and periods of time (e.g. across a city, over a period of days or weeks). Our ultimate goal is to develop a rich interactive experience that combines physical and digital space, with digital interaction becoming increasingly interwoven with everyday interaction in the physical world. This paper presents the EQUIP platform [9], developed to support the merging of physical and virtual environments as part of the EQUATOR Interdisciplinary Research Collaboration (IRC) in the UK [8]. EQUIP is freely available (including source) for other practitioners to make use of [9]. The rest of this paper gives an overview of EQUIP, and its key elements before",oceanology,27
4e9f1e3cec101b425cdb304a98888e4e4db65baa,filtered,semantic_scholar,ISEC,2016-01-01 00:00:00,semantic_scholar,cognitive and contextual enterprise mobile computing: invited keynote talk,https://www.semanticscholar.org/paper/4e9f1e3cec101b425cdb304a98888e4e4db65baa,"The second wave of change presented by the age of mobility, wearables, and IoT focuses on how organizations and enterprises, from a wide variety of commercial areas and industries, will use and leverage the new technologies available. Businesses and industries that don't change with the times will simply cease to exist. Applications need to be powered by cognitive and contextual technologies to support real-time proactive decisions. These decisions will be based on the mobile context of a specific user or group of users, incorporating location, time of day, current user task, and more. Driven by the huge amounts of data produced by mobile and wearables devices, and influenced by privacy concerns, the next wave in computing will need to exploit data and computing at the edge of the network. Future mobile apps will have to be cognitive to 'understand' user intentions based on all the available interactions and unstructured data. Mobile applications are becoming increasingly ubiquitous, going beyond what end users can easily comprehend. Essentially, for both business-to-client (B2C) and business-to-business (B2B) apps, only about 30% of the development efforts appear in the interface of the mobile app. For example, areas such as the collaborative nature of the software or the shortened development cycle and time-to-market are not apparent to end users. The other 70% of the effort invested is dedicated to integrating the applications with back-office systems and developing those aspects of the application that operate behind the scenes. An important, yet often complex, part of the solution and mobile app takes place far from the public eye-in the back-office environment. It is there that various aspects of customer relationship management must be addressed: tracking usage data, pushing out messaging as needed, distributing apps to employees within the enterprise, and handling the wide variety of operational and management tasks-often involving the collection and monitoring of data from sensors and wearable devices. All this must be carried out while addressing security concerns that range from verifying user identities, to data protection, to blocking attempted breaches of the organization, and activation of malicious code. Of course, these tasks must be augmented by a systematic approach and vigilant maintenance of user privacy. The first wave of the mobile revolution focused on development platforms, run-time platforms, deployment, activation, and management tools for multi-platform environments, including comprehensive mobile device management (MDM). To realize the full potential of this revolution, we must capitalize on information about the context within which mobile devices are used. With both employees and customers, this context could be a simple piece of information such as the user location or time of use, the hour of the day, or the day of the week. The context could also be represented by more complex data, such as the amount of time used, type of activity performed, or user preferences. Further insight could include the relationship history with the user and the user's behavior as part of that relationship, as well as a long list of variables to be considered in various scenarios. Today, with the new wave of wearables, the definition of context is being further extended to include environmental factors such as temperature, weather, or pollution, as well as personal factors such as heart rate, movement, or even clothing worn. In both B2E and B2C situations, a context-dependent approach, based on the appropriate context for each specific user, offers a superior tool for working with both employees and clients alike. This mode of operation does not start and end with the individual user. Rather, it takes into account the people surrounding the user, the events taking place nearby, appliances or equipment activated, the user's daily schedule, as well as other, more general information, such as the environment and weather. Developing enterprise-wide, context-dependent, mobile solutions is still a complex challenge. A system of real added-value services must be developed, as well as a comprehensive architecture. These four-tier architectures comprise end-user devices like wearables and smartphones, connected to systems of engagement (SoEs), and systems of record (SoRs). All this is needed to enable data analytics and collection in the context where it is created. The data collected will allow further interaction with employees or customers, analytics, and follow-up actions based on the results of that analysis. We also need to ensure end-to-end (E2E) security across these four tiers, and to keep the data and application contexts in sync. These are just some of the challenges being addressed by IBM Research. As an example, these technologies could be deployed in the retail space, especially in brick-and-mortar stores. Identifying a customer entering a store, detecting her location among the aisles, and cross-referencing that data with the customer's transaction history, could lead to special offers tailor-made for that specific customer or suggestions relevant to her purchasing process. This technology enables real-world implementation of metrics, analytics, and other tools familiar to us from the online realm. We can now measure visits to physical stores in the same way we measure web page hits: analyze time spent in the store, the areas visited by the customer, and the results of those visits. In this way, we can also identify shoppers wandering around the store and understand when they are having trouble finding the product they want to purchase. We can also gain insight into the standard traffic patterns of shoppers and how they navigate a store's floors and departments. We might even consider redesigning the store layout to take advantage of this insight to enhance sales. In healthcare, the context can refer to insight extracted from data received from sensors on the patient, from either his mobile device or wearable technology, and information about the patient's environment and location at that moment in time. This data can help determine if any assistance is required. For example, if a patient is discharged from the hospital for continued at-home care, doctors can continue to remotely monitor his condition via a system of sensors and analytic tools that interpret the sensor readings. This approach can also be applied to the area of safety. Scientists at IBM Research are developing a platform that collects and analyzes data from wearable technology to protect the safety of employees working in construction, heavy industry, manufacturing, or out in the field. This solution can serve as a real-time warning system by analyzing information gathered from wearable sensors embedded in personal protective equipment, such as smart safety helmets and protective vests, and in the workers' individual smartphones. These sensors can continuously monitor a worker's pulse rate, movements, body temperature, and hydration level, as well as environmental factors such as noise level, and other parameters. The system can provide immediate alerts to the worker about any dangers in the work environment to prevent possible injury. It can also be used to prevent accidents before they happen or detect accidents once they occur. For example, with sophisticated algorithms, we can detect if a worker falls based on a sudden difference in elevations detected by an accelerometer, and then send an alert to notify her peers and supervisor or call for help. Monitoring can also help ensure safety in areas where continuous exposure to heat or dangerous materials must be limited based on regulated time periods. Mobile technologies can also help manage events with massive numbers of participants, such as professional soccer games, music festivals, and even large-scale public demonstrations, by sending alerts concerning long and growing lines or specific high-traffic areas. These technologies can be used to detect accidents typical of large-scale gatherings, send warnings about overcrowding, and alert the event organizers. In the same way, they can alleviate parking problems or guide public transportation operators- all via analysis and predictive analytics. IBM Research - Haifa is currently involved in multiple activities as part of IBM's MobileFirst initiative. Haifa researchers have a special expertise in time- and location-based intelligent applications, including visual maps that display activity contexts and predictive analytics systems for mobile data and users. In another area, IBM researchers in Haifa are developing new cognitive services driven from the unique data available on mobile and wearable devices. Looking to the future, the IBM Research team is further advancing the integration of wearable technology, augmented reality systems, and biometric tools for mobile user identity validation. Managing contextual data and analyzing the interaction between the different kinds of data presents fascinating challenges for the development of next-generation programming. For example, we need to rethink when and where data processing and computations should occur: Is it best to leave them at the user-device level, or perhaps they should be moved to the back-office systems, servers, and/or the cloud infrastructures with which the user device is connected? New-age applications are becoming more and more distributed. They operate on a wide range of devices, such as wearable technologies, use a variety of sensors, and depend on cloud-based systems. As a result, a new distributed programming paradigm is emerging to meet the needs of these use-cases and real-time scenarios. This paradigm needs to deal with massive amounts of devices, sensors, and data in business systems, and must be able to shift computation from the cloud to the edge, based on context in close to real-time. By processing data at the edge of the network, close to where the i",oceanology,28
9c97ed6973ac9c1555e29124642c546464bc85ac,filtered,semantic_scholar,2006 IEEE/WIC/ACM International Conference on Intelligent Agent Technology,2006-01-01 00:00:00,semantic_scholar,engaging in a conversation with synthetic agents along the virtuality continuum,https://www.semanticscholar.org/paper/9c97ed6973ac9c1555e29124642c546464bc85ac,"During the last decade research groups as well as a number of commercial software developers have started to deploy embodied conversational characters in the user interface especially in those application areas where a close emulation of multimodal human-human communication is needed. Incarnations of such characters differ widely in type and amount of embodiment - starting from simplistic cartoon-style 2D representations of faces, fully embodied virtual humans in 3D virtual worlds to physically embodied androids co-habiting the user's real world. Despite of their variety, most of these characters have one thing in common: In order to enter the user's physical world, they need to be physical themselves. My talk focuses on challenges that arise when embedding synthetic conversational agents in the user's physical world. Following [4], we may classify the contact between synthetic and human agents according to a ""virtuality continuum"" (see Fig. 1). At one extreme, we find android agents that are completely integrated in the user's physical world and even allow for physical contact with the user. Mel, a robotic penguin developed by Sidner and colleagues [5] (see image 1 in Fig. 1), is one of the most sophisticated physical agents that engages in face-to-face communication with a human user. At the other extreme, there are purely virtual environments that are populated by human and synthetic agents. A prominent example is the pedagogical agent Steve [3] (see Image 4 in Fig. 1). Steve is aware of the user's presence in the virtual space, monitors her actions and responds to them, but has no access to the external world. That is it is only able to perceive user actions that are performed in the virtual space. In between, we find a new generation of characters that inhabit a world in which virtual and digital objects are smoothly integrated. In these applications, projections of virtual characters overlay the user's physical environment or projections of real persons are inserted into a virtual world. For instance, Cavazza and colleagues [2] propose a magic mirror paradigm which puts the user both in the role of an actor and a spectator by inserting the user'svideo image in a virtual world that is populated by synthetic agents (see Image 3 in Fig. 1). In the Virtual Augsburg project (see [1]), a synthetic character called Ritchie jointly explores with the user a table-top application that combines virtual buildings of the city center of Augsburg with a real city map being laid out on a real table. Most work so far has concentrated on the design and implementation of conversational agents at the two extremes of the Virtuality Continuum. In my talk, I will report on a new generation of synthetic characters that are no longer bound to a flat screen, but able to enter a physical world and to engage in a conversation with a human user. Users and characters do not inhabit separated spaces, but share an informational and physical reality that is augmented by digital objects. As a consequence, communication has to take into account both the physical and the digital context. New forms of deixis are enabled by the manipulation of objects and movements of characters in the physical space. Further challenges arise from the realization of so-called traversable interfaces that allow human and synthetic agents to cross the border from the digital to the real world and vice versa.",oceanology,29
44ce98492713648fef9446f779de56029f432763,filtered,semantic_scholar,,2006-01-01 00:00:00,semantic_scholar,effectiveness of collaborative learning in online teaching,https://www.semanticscholar.org/paper/44ce98492713648fef9446f779de56029f432763,"This paper describes how e-learning is becoming popular and used as an alternative means of solving problems in education. E-learning is usually used in distance learning and may be used to replace conventional classroom teaching. Many educational institutions use Internet for collaborative learning in a distributed educational process. It has been known that traditional communications media can be replaced by electronic communication for the whole educational process and in particular, to assess the role of collaborative learning in a distributed education environment. It has been found that a distributed educational process naturally supports collaborative learning environments in which students and tutors interact and provide essential support for students studying at a distance. The tutor’s feedback to students help the learning process and there is indication that tutors are happy to work in the new environment. It is therefore suggested that “blended” online teaching – a combination of the use of the Internet as a medium of instruction and tutors to do face-to-face teaching via a collaborative learning approach – may be implemented to achieve enhanced distance-student performance. INTRODUCTION There is a number of definitions for e-learning. For example, Soekartawi et al. (2002) defined elearning as: ‘... a generic term for all technologically supported learning using an array of teaching and learning tools as phone bridging, audio and videotapes, teleconferencing, satellite transmissions, and the more recognized web-based training or computer aided instruction also commonly referred to as online courses...’. Today, e-learning has become an extremely popular alternative means of delivering educational services worldwide mainly because it is seen as a means of resolving significant educational problems that cannot be solved by conventional means. E-learning is particularly used in open and distance learning (ODL) as it can provide flexible education for those who cannot attend regular schooling particularly because they are unable to leave their work to attend regular conventional classes. Additionally, in an archipelagic country like Indonesia, where the bulk of the population is spread over thousands of islands, educational services are made more accessible through ODL. Most ODL programmes in Southeast Asia, particularly in Indonesia, deploy any one or a combination of the following media: print, radio, television, audiocassettes, videocassettes and computers. Generally, the course materials of ODL programmes in the region are largely print-based. It is likely that this will continue for a long time. However, by increasing the use of computers and information and communication technology (ICT), the role of the printed-based course materials will be replaced gradually by e-learning. MOJIT Effectiveness Of Collaborative Learning In Online Teaching* 69 The development of ICT is moving rapidly because of its pertinent role in providing education to the masses. In fact, by effectively deploying ICT, the educational institutions concerned can cope with, and cater to, the expanding student population. In general, it has been observed that the use of ICT in education in Indonesia has resulted in relatively successful outcomes. There have been numerous problems, however, but there is one significant issue that may be cited here because of its implications to future endeavours in education in Indonesia and in the Southeast Asian region as well. This issue has direct influence over the learning process. It is called transactional distance. The physical separation of teacher and student is no longer a problem given the developments in ICT. However, with ICT use, transactional distance can easily result in a misunderstanding and miscomprehension of the concepts to be learned. In fact, it can even lead to the mal-education of people, following a lack of appropriate communication between learner and teacher. If there is no communication, however, this transactional distance between learner and teacher becomes wider. THE NEW LEARNING PARADIGM In 1995, the International Council on Distance Education (ICDE) conducted what it called an anecdotal, worldwide survey to determine the nature, reality and pace of the shift in the learning paradigm (ICDE, 1996). The survey noted the following clear signs: o A shift from objective knowledge to constructed knowledge. o A shift from an industrial-based to a knowledge-based society. o A shift in education missions from providing instruction to providing learning. o A shift in technologically-mediated procedures of communication and learning. o A shift from “current college and university models to as yet undetermined structures.” Over the last five years, the fifth observation of ICDE has become very clear. The “undetermined structure” at the time of the ICDE survey has come to be known as the virtual learning structure. Experts made similar observations from the ICTsector at about the same time period (Soekartawi et al., 2002). The importance of the virtual campus in the Indonesian context has been reported by Soekartawi (2002, 2002a, 2003) and Haryono & Alatas (2002). According to Garmer & Firestone (1996), due to the development of ICT, the paradigm for learning is shifting away from the traditional notion that ‘‘knowledge” is transferred from teacher to student within the confines of the classroom where the focus of the teaching-learning process before was the teacher; now, it is shifting to the learner. A new understanding of learning places the learner at the centre of the learning process, with the teacher serving an important supporting role in facilitating the process (Garmer & Firestone, 1996). In this new paradigm, successful learning is measured by the individual’s ability to apply appropriate tools and information to solve problems in real life. There is a challenge in this new concept of learning. It is necessary to unlearn old habits and notions of how learning should be structured and to develop new habits of instruction that motivate learners to take greater control over their own education (Garmer & Firestone, 1996). This is what generally known by the “first concern” which should be to understand the learner’s motivations and goals in order that we can expand opportunities for learning. This new paradigm has critical implications for the use of ICT (e-learning) in education, particularly for ODL. For one, it requires that the learner take on more responsibility and autonomy in his learning. This is something that the learner may not be comfortable with or prepared to assume completely at this time. For another, teachers will have to give up control over the learning process and take on a new role similar to that of educational coaches who spend more time on the sidelines MOJIT Effectiveness Of Collaborative Learning In Online Teaching* 70 watching and maybe making plans for a more effective learning environment. The new paradigm has also highlighted an important issue. There is a need for all learners to upgrade their skills, particularly skills to learn on their own. This largely requires an ability to seek, understand and use information, which, in turn, requires the ability to use technology. In today’s world, for example, one must be computer literate to gain access to information and new knowledge. COLLABORATIVE LEARNING One of the critical factors in online learning is the quality issue. Many efforts can be used to meet this issue, among them obtaining feedback evaluation from the student. Further, a crucial aspect of successful distance education is the quality of feedback on student assignments. The electronic assignment handling system pioneered in these trials has now been adopted by the Open University at large and will serve a big number of students. Collaborative learning, therefore, can be used to improve the issue of quality in distance learning. Collaborative learning is an essential ingredient in the recipe to create an ""effective learning environment"" as it provides learners with the opportunity to discuss, argue, negotiate and reflect upon existing beliefs and knowledge. The learner is ""...involved in constructing knowledge through a process of discussion and interaction with learning peers and experts...."" Harasim (in Thomas, 1999, p.51). To facilitate collaboration so that personal knowledge can be constructed, there needs to be a purpose for the collaboration and the purpose needs to be meaningful to the learner. Thus, it is important that an appropriate context is set for the collaborative activity, for example, assigning a ""real world"" task for learners or a problem to which all learners can relate. In addition to setting the context, there needs to be a vehicle through which collaboration can take place. In traditional faceto-face educational settings, collaboration mostly occurs through conversation, that is, individuals interacting with one another via the use of language. Therefore, in terms of creating an effective learning environment, four attributes surface as being paramount: • Providing opportunities to foster personal construction of knowledge • Setting an appropriate context for learning to create the opportunities. • Facilitating collaboration amongst learners. • Using conversation to facilitate collaboration. Petre (1998) argued that while superficially it might appear that distance learning/education is the domain of the distance educator, the aims of educational institutions are similar in ideology; according to Thomas (1999), only the implementations differ. However, what distance educators brings to this arena is the experience of how to interact with students and tutors who are geographically and temporally remote from a campus as well as from one another. The ultimate objective is to diminish and even remove the barriers that remoteness erects. In general, the learning environment in the Open University can be described",oceanology,30
e61e75ec370207ed0dca2e81307e8901d09c0493,filtered,semantic_scholar,,2013-01-01 00:00:00,semantic_scholar,enabling customer experience and front-office transformation through business process engineering,https://www.semanticscholar.org/paper/e61e75ec370207ed0dca2e81307e8901d09c0493,"In the past, the scope of business processes has been circumscribed to the industrialization of enterprise operations. Indeed, Business Process Management (BPM) has focused on relatively mature operations, with the goal of improving performance through automation. However, in today’s world of customer-centricity and individualized services, the richest source of economic value-creation comes from enterprise-customer contacts beyond transactions. Consequently, process has recently moved out of its traditional court and is becoming prevalent in less traditional competences such as marketing operations, customer-relationship management, campaign creation and monitoring, brand management, sales and advisory services, multi-channel management, service innovation and management life-cycle, among others. These competences host customerenterprise co-creation activities characterized by innovation, human creativity, and new technologies. Above all, these work-practices call for continuous differentiation, instead of “pouring concrete” on emerging business processes. While BPM will continue to make important contributions to the factory of enterprises, Business Process Engineering (BPE) is chartered to provide a holistic approach to new opportunities related to the life-cycle of enterprise customers and the transformation of so-called Front-Office Operations. More broadly, Business Process Engineering fosters a new space for the multidisciplinary study of process, integrating individuals, information and technology, and it does so with the goal of engineering (i.e., designing and running) innovative enterprise operations to serve customers and improve their experiences. Furthermore, given past challenges in the Back-Office, it is imperative that managers focus on processes in the Front-Office where the software industry has jumped into with solutions that bury key processes within applications, thus making differentiation and agility very difficult. BPE stresses the critical importance of the integration of Information and Behavior and it is this goal that links it with Business Informatics: the information process in organizations and society. Since behavior and information are complementary and inseparable domains of concern, current approaches to decision making based on data-only evidence should be reexamined holistically: it may be catastrophic to explicate or predict the behavior of organizations or individuals meaningfully by insisting on the ongoing divorce across the two domains. In particular, Business Informatics and Business Process Engineering offer an opportunity to address potential benefits of “big data” and “business analytics” beyond the IT domain. Having IEEE lead these directions means an opportunity for stimulating new research and practice on the most fundamental problems that enterprises and customers face today in dealing with each other. Keywords— business process engineering; customer experience; business process management; business informatics; enterprise engineering I. PROCESS IS OUT OF THE INDUSTRIALIZATION BOX Business process has been at the center of the stage in both research and industry for several decades. Under the brand of Business Process Management (BPM), business process has attracted a great deal of attention from many practitioners and scholars. BPM has been defined as the analysis, design, implementation, optimization and monitoring of business processes [70], [219], [79], [229], [230]. In [266], Van der Aalst defined some targets of BPM: ‘ ... supports business processes using methods, techniques, and software to design, enact, control and analyze operational processes involving humans, organizations, applications, documents and other sources of information” . While the above definitions are quite comprehensive and broad, in reality most BPM research and industry activity has grwon upon the motivation of reducing operating costs through automation, optimization and outsourcing. There are a several schools of thought and practice (such as lean, lean sixsigma, and others [172], [6], [4], [5]) and a myriad of related literature in the last 40 years that serve to illustrate the focus on cost contention. Around the middle of the past decade, T. Davenport stated in a celebrated Harvard Business Review paper [54] that processes were being “analyzed, standardized, and quality checked”, and that this phenomenon was happening for all sort of activities, stated in Davenport’s own terms: “from making a mouse trap to hiring a CEO”. The actual situation is that industry investment and consequential research have stayed much more on “trapping the mouse” than in differentiating customer services through innovative and more intelligent processes, let alone hiring CEOs. This may be explained partly from Davenport’s own statements: “Process standards could revolutionize how businesses work. They could dramatically increase the level and breadth of outsourcing and reduce the number of processes that organizations decide to perform for themselves” (bold face is added here for emphasis). With the advent of different technologies such as mobile, cloud, social media, and related capabilities that have empowered consumers, the classical approach and scope of business process have begun to change quickly. Organizations are adopting new operating models [100] that will drastically affect the way processes are conceived and deployed. As stated by many authors in the last four decades, business process work is supposed to cover all competences in an organization, irrespective of the specific skills from human beings participating in such operations. However, in an unpublished inspection of about 1,300 papers conducted by 1 Van der Aalst excludes strategy processes from BPM, a remarkable point that will be revisited in more depth later in this paper. the author and some of his collaborators 2 , most process examples shown in the literature deal with rather simple forms of coordination of work, mostly exhibiting a flow structure and addressing administrative tasks (like those captured in early works on office information systems). Furthermore, the examples provided usually deal with rather idealized operations, probably offered as simple examples with the only purpose of illustrating theoretical or foundational research results. Thus, radically simplified versions of “managing an order”, “approving a form”, “processing a claim”, “paying a provider”, “delivering an order” etc. are among the most popular examples of processes found in the literature. The lack of public documentation of substantial collections of real-world processes is remarkable. The authors in [106] both confirmed the dominant focus on simple business processes and also suggested potential practical consequences of related research: “... there is a growing and very active research community looking at process modeling and analysis, reference models, workflow flexibility, process mining and process-centric service-oriented architecture (SOA). However, it is clear that existing approaches have problems dealing with the enormous challenges real-life BPM projects are facing [ ... ] Conventional BPM research seems to focus on situations with just a few isolated processes ...”. Of course, the list of available real-world processes would be a lot richer if one included the set defined by enterprise packaged applications [219]. However, this comprehensive collection is proprietary because it constitutes a key piece of intellectual capital coming from software vendors or integrators in the industry. The traditional focus on process has also raised much controversy. At the S-BPM ONE Conference in 2010, a keynote speaker [176] remarked: “Let me be as undiplomatic as I possibly can be without being offensive [...] The academic community is as much to blame [...] as the vendors of BPM systems, who continue to reduce the task of managing business processes to a purely technological and automation-oriented level”. While other authors in the same conference debated “who is to blame” very animatedly [78], [234] it is important to highlight that the statement from Olbrich (in bold face above for emphasis) reinforces that BPM has mostly followed the obsession of automation and optimization by means of Information Technology. A detailed inspection of the extant literature confirms that business process work has been devoted to a rather small fraction of the actual variety and complexity found in enterprise behavior. This behavior enacts many valuegenerating capabilities that organizations cultivate based on skills provided by their own workforces and through rich interactions with other enterprise stakeholders, particularly customers. The following points offer a simplified summary: 2 At the time of this publication in IEEE, the mentioned work still remains unpublished. The co-authors are L. Flores and V. Becker both from IBM. (1) Business process research in Computer Science has been traditionally focused on certain classes of enterprise operations, mostly involving simple coordination mechanisms across tasks. This type of coordination and the overall behavior represented in underlying models reflect very much an “assembly line” where work is linearly synchronized to deliver a desired artifact or outcome. Simplicity of the choreography is ensured by removing any form of overhead in communication when moving from one stage to the next. Unlike other more complex business processes, many software applications do have this simplified structure. In fact, a trend since the early 2000’s is to separate the specific application logic from the coordination / choreography needed across modules, and both of them from the actual data contained in a data-base management system. Different foundations and a plethora of languages have been created to capture this semantics of coordination such as Business Process Modeling Notation (BPMN), Business Process Execution Languag",oceanology,31
635c01ca9f63bac692d2b9e4e6f4bdaf9aef4ce7,filtered,semantic_scholar,,2003-01-01 00:00:00,semantic_scholar,tactical insertion mission planning and rehearsal using virtual reality simulation,https://www.semanticscholar.org/paper/635c01ca9f63bac692d2b9e4e6f4bdaf9aef4ce7,"Systems Technology, Inc. (STI) has developed a versatile new system for parachute mission planning and rehearsal, combining the validated technology of STI's PC-based PARASIM parachute simulation system with real-time interactive networking, powerful scene generation graphics tools, and terrain-correlated wind fields. This Tactical Insertion Mission Planning and Rehearsal Simulator (TIMPARS) was developed under the SBIR program, funded by the US Special Operations Command (SOCOM). The TIMPARS system rests on four cornerstones: the PARASIM  simulation software, the real-time interactive network, the scene generation toolkit, and the terrain-correlated wind generation module. These elements combine to produce a system with which users can utilize geospecific terrain data and imagery to recreate a real-world site as a simulation scene, input actual or forecasted wind speeds and directions at altitude above the chosen location to generate a terrain-correlated wind field specific to the simulation scene, and then plan and rehearse a mission in a real-time simulation environment with multiple live participants interacting in the same virtual space. BACKGROUND STI's original parachute simulator was developed for use by smokejumpers, US Forestry Service airborne firefighters. Designed to teach round and ramair canopy control, this early version employed rudimentary graphics with a fixed monitor; users stood before the display monitor and pulled simple toggle lines to maneuver in the simulation. Despite the austere configuration, this version provided the minimum cues required to teach parachute flight safely at low cost. In 1996, STI launched a major development effort to incorporate new photo- realistic graphics and head-mounted display/virtual reality technology into the simulator. Subsequent development efforts produced malfunctions procedures software, riser controls, harness switches, and additional simulator improvements. The implementation of these enhanced simulators by the US Marine Corps (USMC) and the Military Freefall School in Yuma, AZ, resulted in a drastic drop in the rate of training injuries. In particular, the USMC First Force Reconnaissance Company experienced a 75% reduction in main canopy cutaways after implementing the enhanced simulator in the MC-5 static line deployed ram-air parachute system (SLDRAPS) transition course at Camp Pendleton, CA. TIMPARS PROJECT",oceanology,32
ec0421dd64fdb0b41759c8d9c2875fa6ade6a526,filtered,semantic_scholar,IEICE Trans. Fundam. Electron. Commun. Comput. Sci.,2002-01-01 00:00:00,semantic_scholar,localization and dynamic tracking using wireless-networked sensors and multi-agent technology: first steps,https://www.semanticscholar.org/paper/ec0421dd64fdb0b41759c8d9c2875fa6ade6a526,"SUMMARY We describe in this paper our experience of develop-ing a large-scale, highly distributed multi-agent system using wireless-networked sensors. We provide solutions to the problems of localization(position estimation) and dynamic, real-time mobile object tracking, whichwe call PET problems for short, using wireless sensor networks. We pro-pose system architectures and a set of distributed algorithms for organiz-ing and scheduling cooperative computation in distributed environments,as well as distributed algorithms for localization and real-time object track-ing. Based on these distributed algorithms, we develop and implement ahardware system and software simulator for the PET problems. Finally, wepresent some experimental results on distance measurement accuracy usingradio signal strengths of the wireless sensors and discuss future work.key words: localization and mobile object tracking, distributed algorithms,multi-agent systems, wireless sensor network, and MEMS. 1. IntroductionIn recent years, the technology of micro-, electro-mechanical systems (MEMS) has made rapid advances.Various smart devices, such as sensors and actuators withsome information processing and communication capabili-ties embedded within, have been developed and deployed inmany real-world applications [9]–[11]. The application do-mains of such smart devices are numerous, including newgeneration pervasive computing systems, avionics and plantautomation, building and environmental monitoring, multi-hop routing discovery [6], augmented reality and virtue re-ality systems, active badge [13], and multiple robotics sys-tems. To use smart devices in these applications, it requiresto connect a large number of sensors and actuators, up tothousands, tens of thousands or even millions of units, andto integrate and embed sensing, communication, signal anddata processing, and control functionson individualdevices.The key to large-scale networked, embedded MEMS is themechanism with which individual units are programmed towork as a coherent piece toward achieving common goals.Due to the distributed and real-time nature of most applica-tions and the size of such a large system, information pro-cessing and decision making very often need to be carriedout on the units where actions take place.",oceanology,33
3121857e7fb87aed8c95f32eb7840a940736b60d,filtered,semantic_scholar,,2006-01-01 00:00:00,semantic_scholar,projeto de um ambiente 3d de visualização e reprodução de eventos capturados e interpretados a partir de ambientes físicos cientes de contexto para aplicações de preparação para emergência.,https://www.semanticscholar.org/paper/3121857e7fb87aed8c95f32eb7840a940736b60d,"Systems for emergency preparedness support, especially those for accurate monitoring of physical environments subjected to emergency situations, are valuable resources for companies and civil defense public institutions, since these systems can help avoiding and/or reducing lives and patrimony losses. Most of the existing monitoring systems described in the literature have limitations, such as: no posterior visualization of emergency situations that have occurred; limited to specific types of applications; inaccurate identification of risk situations; etc. In this work, a system was proposed and evaluated that aims to overcome these limitations through the integrated use of wireless actor and sensor networks, context aware computing and virtual reality. The work consisted on the creation, implementation and evaluation of a recording and playing 3D media in which physical environments subjected to emergency situations are deployed sensors with processing and communication resources. These sensors capture and interpret contexts, which are mapped, through a visual language, on a 3D virtual environment that mimics the physical environment. The use of virtual reality for visualization and access in real-time or afterwards of situations that are occurring in the physical environment, through a 3D Virtual Environment, can overcome the limitations of hypermedia interfaces or continuous media, like video, when the experiences of the real world are very complex. This work describes the project of a recording and playing system, which allows users to play live experiences gathered from the real world for analysis, evaluation, monitoring and training. The novelty of the system resides in two aspects: it uses an optimized recording technique that saves processing time and storage space; it records scene updating commands independent from 3D Players, allowing the visualization of the collaborative virtual environment (CVE) through any existing 3D web players. In collaboration with the Arts and Communication Department (DAC) of UFSCar, a visual language to prompt identification of emergency situations was created as well as an interface to complex systems. Examples of use include the monitoring of industrial plants, flight rehearsals, petrol exploration platforms, etc. This work is part of a collaborative Project between the Networked Virtual Reality Lab (LRVNet) of the Computer Science Department at UFSCar and PARADISE Lab of SITE at University of Ottawa. 2 All complexity of real-time systems don’t will be considered in this dissertation, real-time will be used indicating that the events must be shown faster as possible.",oceanology,34
ee2be26c5e2bfab3885adc0e6c70e4583435bd3e,filtered,semantic_scholar,,2003-01-01 00:00:00,semantic_scholar,how to harness the grid with ogsa - tutorial proposal,https://www.semanticscholar.org/paper/ee2be26c5e2bfab3885adc0e6c70e4583435bd3e,"Summary and Conclusion As a conclusion to this tutorial, we will share lessons learnt from our experience developing with OGSA and highlight the limitations as well as the benefits of deploying OGSA middleware. In particular, we will examine the behaviour of the crystal polymorph prediction system operating in a Grid environment and consider how effectively the implementation exploits available resources. We also discuss practical and political considerations that arise from “real world” Grid environments where technical arguments are often compromised by the needs and preferences of different users, organizations and domain administrators. Conduct of tutorial Delivery The tutorial will consist mostly of a talk support by a Powerpoint presentation. We also intend to illustrate some concepts with live software demonstrations: the first outlining the process of creating a simple OGSA service using the Globus Toolkit 3.0 (GT3) from simple interface description through to stub generation and wrapping an implementation using the delegation model; the second demonstrating the crystal polymorph application running in a simulated Grid environment. This will enable participants to experience Grid middleware from a developer’s perspective and lend some reality to the concepts and mechanisms the tutorial covers.",oceanology,35
0fb400e80dcda882fc293327c5c12e59fd18af93,filtered,semantic_scholar,IWUC,2006-01-01 00:00:00,semantic_scholar,on-demand loading of pervasive-oriented applications using mass-market camera phones,https://www.semanticscholar.org/paper/0fb400e80dcda882fc293327c5c12e59fd18af93,"Camera phones are the first realistic platform for the development of pervasive computing applications: they are personal, ubiquitous, and the builtin camera can be used as a context-sensing equipment. Unfortunately, currently available systems for pervasive computing, emerged from both academic and industrial research, can be adopted only on a small fraction of the devices already deployed or in production in the next future. In this paper we present an extensible programming infrastructure that turns mass-market phones into a platform for pervasive computing. 1 Mobile phone: a platform for pervasive computing Pervasive computing tries to make M. Weiser’s vision [1] a reality by saturating the environment with computing and communication devices: the most of the infrastructure is often invisible and supports user’s activities with an interaction model that is strongly human-centric. Today, almost fifteen years later, despite significant progresses in both hardware and software technologies, this vision is still not completely realizable or economically convenient. Supporting the interaction between users and the environment can be greatly simplified if we relax the interaction model and include a personal device as the access medium. Mobile phones are the most obvious candidates: they are in constant reach of their users, have wireless connectivity capabilities, and are provided with increasing computing power [2]. Even better results can be achieved with those phones that are equipped with a camera. Instead of manually getting information or editing configurations, users can point physical objects to express their will of using them: taking a picture of the objects would suffice to setup the link with the offered services. Relaying on an image acquisition device does not impose a strict limit to the share of possible users, since an always growing number of commercially available mobile phone is equipped with an integrated camera: according to recent studies [3], over 175 million camera phones were shipped in 2004 and, by the end of the decade, the global population of camera phones is expected to surpass 1 billion. ? This work is partially supported by the Italian Ministry for Education and Scientific Research (MIUR) in the framework of the FIRB-VICOM project. However, the acquisition of context-related information through images is not a trivial task, especially with resource-constrained devices. To ease the recognition process, objects can be labeled with visual tags readable by machines. Once decoded, visual tags either directly provide information about the resource they are attached to or, if the amount of information is too large, they act as resource identifiers that can be used to gather information from the network. In this paper, we describe the design and the implementation of POLPO 1 (Polpo is On-demand Loading of Pervasive-Oriented applications), a software system that turns mass-market phones into a platform for the development of pervasive applications. With POLPO, a phone with a built-in camera and compatible with the Java 2 Micro Edition (J2ME) platform is able to get context information by decoding visual tags attached to real-world objects. POLPO supports dynamic loading and installation of custom applications used to interact with the desired resources. 2 Background and contribution In this section we summarize the most relevant solutions based on visual tags and the contribution of our system in this field. Cybercode [4] is a visual tagging system based on a two-dimensional barcode technology. The system has been used to develop several augmented reality applications where the physical world is linked to the digital space trough the use of visual tags. Cybercode is one of the first systems where visual tags can be recognized by low-cost CCD or CMOS cameras, without the need for separate and dedicated readers. Each Cybercode symbol is able to encode 24 or 48 bits of information. The system has been tested with notebook PCs and PDAs. In [5] the author presents a system that turns camera-phones into mobile sensors for two-dimensional visual tags. By recognizing a visual tag, the device can determine the coded value, as well as additional parameters, such as the viewing angle of the camera. The system includes a movement detection scheme which enables to use the mobile phone as a mouse (this is achieved by associating a coordinate scheme to visual tags). The communication capability of the mobile phone is used to retrieve information related to the selected tag and to interact with the corresponding resource. Tag recognition and motion detection algorithms were implemented in C++ for Symbian OS. The Mobile Service Toolkit (MST) [6] is a client-server framework for developing site-specific services that interact with users’ smart phones. Services are advertised by means of machine-readable visual tags, which encode the Bluetooth device address of the machine that hosts the service (Internet protocols addressing could be supported as well). Visual tags also include 15 bits of application-specific data. Once the connection has been established, MST servers can request personal information to the client to provide personalized services. Site-specific services can push user interfaces, expressed with a markup language similar to WML (Wireless Markup Language), to smart phones. MST also provide thin-client functionality: servers can push arbitrary graphics 1 The Italian name for the octopus vulgaris, a cephalopod of the order octopoda, probably the most intelligent of the invertebrates. to the phone’s display which in turn forwards all keypress events to the server. The client-side is written in C++ and requires Symbian OS. A similar approach is described in [7], where the authors propose an architecture for a platform that supports ubiquitous services. Real-world objects are linked to services on the network through visual tags based on geometric invariants that do not depend on the viewing direction [8]. But differently from other solutions, image processing does not take place on the user’s device: pictures are sent to a server where they are elaborated and converted into IDs. Instead of using two-dimensional barcodes, an alternative way of performing object recognition is the one based on radio frequency identification (RFID): small tags, attached to or incorporated into objects, that respond to queries from a reader. However this solution, that can be useful in many pervasive computing scenarios, is not particularly suitable when the interaction is mediated by mobile phones, that lack the capability of reading RFIDs. In our opinion, currently available solutions present two major drawbacks: i) they are limited to specific hw/sw platforms (i.e. Symbian OS), excluding most of the models of mobile phones already shipped and in production in the near future; ii) the software needed to interact with the environment is statically installed onto the mobile phone and cannot be dynamically expanded, e.g. to interact with new classes of resources. We designed and developed a system for pervasive computing based on visual tags that overcomes these constraints as follows. Compatibility with J2ME The system runs on devices compatible with the J2ME platform. This environment is quite limited in terms of both memory and execution speed, but also extremely popular (nearly all mobile phones produced). This required the implementation of a pure Java decoder of visual tags for the J2ME environment. Downloadable applications Our system is based on the idea that the interaction with a given class of resources, e.g. printers, public displays, etc., takes place through a custom application. New custom applications can be downloaded from the network and installed onto the user’s device as needed. This brings two advantages: i) the classes of resources that can be used do not have to be known a priori; ii) the user’s device, that is resource constrained, includes only the software needed to interact with the services actually used. The J2ME platform comprises two configurations, few profiles, and several optional packages. The J2ME configurations identify different classes of devices: the Connected Device Configuration (CDC) is a framework that supports the execution of Java application on embedded devices such as network equipment, set-top boxes, and personal digital assistants; the Connected Limited Device Configuration (CLDC) defines the Java runtime for resource constrained devices as mobile phones and pagers. Our systems runs on top of the version 1.1 of the CLDC, that provides support for floating point arithmetics (unavailable in version 1.0). The adopted profile is the Mobile Information Device Profile (MIDP) that, together with CLDC, provides a complete Java application environment for mobile phones. 3 System architecture POLPO requires that physical resources are labeled with visual tags, and that a program providing access to POLPO functionalities is installed onto the user’s device. This program has the following primary functions: – Decoding of visual tags. The image captured with the built-in camera is processed to extract the data contained into the visual tag. – Management of custom applications. The program downloads and installs the custom application required to interact with a resource. Usually, resources of the same kind share the same custom application (i.e., a single application is used to interact with all printers, another is used with public displays, etc). – Management of user’s personal data. In many cases, applications need information about the user to provide customized services. For this reason, the software installed on mobile phones includes a module that manages user’s personal data and stores them into the persistent memory. Managed data comprise user’s name, telephone number, email address, homepage, etc. Each resource is identified and described by a Data Matrix visual tag. Da",oceanology,36
cdd8028ef1569a9c7191e806839ed2bb261efdb9,filtered,semantic_scholar,,2002-01-01 00:00:00,semantic_scholar,title: an integrated design environment to evaluate power/performance tradeoffs for sensor network applications,https://www.semanticscholar.org/paper/cdd8028ef1569a9c7191e806839ed2bb261efdb9,"Networks of inexpensive, low-power sensing nodes that can monitor the environment, perform limited processing on the samples, and detect events of interest in a collaborative fashion are fast becoming a reality. Examples of such monitoring and detection include target tracking based on acoustic signatures and line-of-bearing estimation, climate control, intrusion detection, etc. The advances in low-power radio technology are making wireless communication within sensor networks an attractive option. However, it is typically difficult or impossible to replenish energy resources available to a portable sensor node, once it is deployed. Maximizing the life of sensor nodes is an overriding priority, and different energy optimization techniques are being developed to addresses computation/communication tradeoffs. A large number of research efforts are focusing on different aspects of the general problem of designing efficient sensor network-based systems where the metrics to measure efficiency vary from system to system. With technological advancements such as silicon-based radios expected to become a reality in a few years, designers of sensor network-based systems will be faced with an extremely large set of design decisions. Each choice will affect the overall system performance in ways that might not always be cleanly modeled. In addition to the research challenges in design and optimization, the practical aspects of designing real-world sensor networks will become equally important. For example, the ability of the design framework to allow rapid specification and evaluation of a particular network configuration is crucial for a more exhaustive exploration of the design space. A design environment for future sensor networks should provide tools and formal methodologies that will allow designers to model, analyze, optimize, and simulate such systems. In the context of our work, design and optimization of a sensor network application involves determining the task allocation to different sensor nodes and the inter-node communication mechanism. Design of the sensor node hardware itself is also an area of active research. However, we assume that a set of node architectures is already available to our enduser, and the design problem is restricted to using the available hardware (with flexibilities, if any, such as dynamic voltage scaling) to efficiently implement the target application. We take a simple, seven-node wireless sensor network for acoustic detection [5] (Automatic Target Recognition) as the case study and demonstrate (i) a modeling and simulation methodology for a class of sensor networks, and (ii) a software framework that implements our methodology. Our formal application model is illustrated in Fig. 1. We use a data flow graph representation to model the computing tasks and their data dependencies. The end-to-end application consists of two types of such data flow graphs: the first type denotes the processing that has to be performed for each sample before it is ready to be ‘fused’ with results from other sensors, and the second type represents the computing involved in data fusion. Specifically , in our case study, a Fast Fourier Transform (FFT) operation is the only task that is performed on each block of sampled data. The outputs of FFTs from all seven nodes are provided as an input to the collaborative computing part, which consists of delay and sum beamforming (BF), and lineof-bearing (LOB) estimation. The result of collaborative computation in such a cluster model of sensor networks has to be transmitted to some observer. This is accomplished by designating one of the nodes as the cluster-head, which could be equipped with more powerful communication facilities than other sensor nodes. All communication within our cluster is one-hop, and processing of a particular data sample (FFT/BF/LOB) occurs either on its home node, or the cluster-head, or partly on both. Simulating a completely specified instance of the above class of sensor networks involves many challenges. None of the existing network simulators to our knowledge models the internal architecture of the processing nodes in the network. This is because the focus of most network simulators is on protocol development and empirical analysis. Except in areas such as high-speed router design, the node internals have little or no impact on decisions related to protocol design. Also, processor simulators do not model the environment outside the chip boundary. Therefore, to obtain detailed and accurate performance estimates for the entire system, we propose a technique to automatically generate network scenarios based on results from low-level node simulations. The network simulator is configured using the generated scenarios, and the individual simulation results are merged and presented to the end-user as a whole. Such a ‘horizontal’ simulation is accomplished through the use of a central data repository for model information, which means that the simulators never have to directly interact with each other. The simulators we integrate provide estimates about energy consumption, thereby assisting in a power/performance analysis of a specific system configuration. Our design framework facilitates multi-granular simulation, i.e., simulating the same system configuration by using simulation models at different levels of abstraction. Typically, coarse-grained models provide rapid estimates, but need to make approximations about system behavior that might not be very accurate. For such a scenario, we demonstrate a form of analytical model refinement (see Fig. 2), i.e. the data from low-level simulations can be automatically processed to ‘distill’ parameter values used by high-level simulators. Naturally, the exact processing has to be specified by someone with knowledge of the analytical model semantics. Our design environment provides the following capabilities to the user: • To graphically describe the target application, node architecture, network configuration, and task-to-node mapping. • To change (reconfigure) the system model to explore alternate designs. Some of the parameters that can be manipulated by the designer include receive/transmit power of the radio, voltage/frequency setting of the processor, cluster geometry, propagation models, etc. • To automatically simulate a design using a coarse system model. • To automatically configure and execute low-level simulators for the node (Wattch [3]) and the network (ns-2 [4]) and obtain system-wide energy and latency estimates. • To automatically update high-level model parameters using low-level simulation statistics. • To graphically visualize simulation results and facilitate (manual) identification of power/performance bottlenecks in the design. This work is an illustration of the general approach of the MILAN [1] project. A modeling and simulation framework based on the first version of the 7-node ATR system model was implemented in [2]. The primary focus of that work was a prototype demonstration of simulator integration and model refinement. Therefore, the system model itself lacked generality. Also, we use a relatively more detailed version of the high-level estimator implemented for [2]. This work represents a significant step towards the ultimate goal of a design environment for automatic optimization and synthesis of sensor network applications.",oceanology,37
336d84f6e7a7a398c70e924b4677c1fee7f3b81d,filtered,semantic_scholar,,2001-01-01 00:00:00,semantic_scholar,the advantages of micro simulation in traffic modelling with reference to the n4 platinum toll road,https://www.semanticscholar.org/paper/336d84f6e7a7a398c70e924b4677c1fee7f3b81d,"Micro simulation has been used to a limited extend in the past in South Africa, despite major advantages of this tool above static modelling and it’s popularity oversees. The main advantages are dynamic modelling and visual interpretation of the traffic conditions. This tool is ideal to test geometric designs, traffic controls and a variety of traffic management measures. These include incident and congestion management, road works, ramp metering, VMS, etc. It is an extremely suitable tool to use when low cost solutions must be found because of severely limited infrastructure resources. Times that micro simulation was not able to calculate and show reliable traffic situations is over, various traffic simulation models have developed and have reached high quality standards. Micro simulation is about to gain a real market share all around the world; South Africa is following. Modelling toll plazas at interchanges on the N4 Platinum Toll Road is used to illustrate the advantages of micro simulation. Geometric design options, measures effecting toll throughput and traffic control options were evaluated in this example as well as the estimation of the expected life span of various options within a congested network. The package used in this study is AIMSUN2, an advanced micro simulation package widely used internationally that can interact with TRANSYT, SCOOT, EMME/2 and SATURN. AIMSUN2 has been applied to traffic impact analysis, traffic control measures, HOV-lanes, tolling and geometric design within the last three years in South Africa. It has also been used successfully to convey results of investigations to nontechnical people. 1. MICRO SIMULATION 1.1 Background on the development of micro simulation The microscopic traffic simulation models are based on the reproduction of the traffic flows simulating the behavior of the individual vehicles, this not only enables them to capture the full dynamics of time dependent traffic phenomena, but also to deal with behavioral models accounting for drivers’ reactions. The underlying hypothesis is that the dynamics of a stream of traffic is the result of a series of drivers’ attempts to regulate their speed and acceleration accordingly with information received. The driver’s actions resulting from the interpretation of the information received will consist on the control of the acceleration (braking and accelerating), the control of heading (steering) and the decision of overtaking the precedent vehicle either to increase the speed or to position themselves in the right lane to perform a maneuver (i.e. a turning). The origins of microscopic traffic simulation can be traced back to the early stages of digital computers. Although the basic principles were set up many years ago, with the seminal work of, among others, Robert Hermann and the General Motors Group in the early fifties, the computing requirements made them impractical until hardware and software developments made them affordable even on today’s laptop computers. Most of the currently existing microscopic traffic simulators are based on the family of carfollowing, lane changing and gap acceptance models to model the vehicle’s behavior. Carfollowing models are a form of stimulus-response model, where the response is the reaction of the driver (follower) to the motion of the vehicle immediately preceding him (the leader) in the traffic stream. The response of the follower is to accelerate or decelerate in proportion to the magnitude of the stimulus at time t after a reaction time T. The generic form of the conceptual model is: response (t+T) = sensitivity * stimulus (t) Among the most used models are Helly’s model (1), implemented in SITRA-B+, (2), Herman’s model (3), or its improved version by Wicks (4), implemented in MITSIM, (5), the psycho-physical model of Wiedemann, (6), used in VISSIM (7), or the ad hoc version of Gipps (8), used in AIMSUN2 (9, 10). Other microscopic simulators such as INTEGRATION (11) and PARAMICS employ heuristic or other modeling not publicly available in analytic form. A common drawback of most of these models is that the model parameters are global i.e. constant for the entire network whereas it is well know that driver’s behavior is affected by traffic conditions. Therefore a more realistic car-following modeling for microscopic simulation should account for local behavior. This implies that some of the model parameters must be local depending on local geometric and traffic conditions. 1.2 What micro simulation is and the advantages thereof compared to static models The deployment of Intelligent Traffic Systems (ITS) requires support of complementary studies clearly showing the feasibility of the systems and what benefits should be expected from their operation. The large investments required have to be justified in a robust way. That means feasibility studies that validate the proposed systems, assess their expected impacts and provide the basis for sound cost benefit analyses. Microscopic traffic simulation has proven to be a useful tool to achieve these objectives. This is not only due to its ability to capture the full dynamics of time dependent traffic phenomena, but also being capable of dealing with behavioral models accounting for drivers’ reactions when exposed to ITS systems. The advent of ITS has created new objectives and requirements for micro-simulation models. Quoting from Deliverable D3 of the European Commission Project SMARTEST [12]: “The objective of micro-simulation models is essentially, from the model designers point of view, to quantify the benefits of Intelligent Transportation Systems (ITS), primarily Advanced Traveler Information Systems (ATIS) and Advanced Traffic Management Systems (ATMS). Micro-simulation is used for evaluation prior to or in parallel with on-street operation. This covers many objectives such as the study of dynamic traffic control, incident management schemes, real-time route guidance strategies, adaptive intersection signal controls, ramp and mainline metering, etc. Furthermore some models try to assess the impact and sensitivity of alternative design parameters”. The analysis of traffic systems and namely ITS systems, is beyond the capabilities of traditional static transport planning models. Microscopic simulation is then the suitable analysis tool to achieve the required objectives. An example from a real case study where microscopic simulation was used to complement static modeling will help us to understand better how both levels may help the decisionmaker. The city of San Sebastian, in the North of Spain, completed recently a new urban freeway connecting two separated neighborhoods. Figure 1 shows the typical result of the planning study with an close up of the Amara neighborhood. The road network and the demand were modeled using the EMME/2 package. The figure displays the expected impacts of the new infrastructure highlighting in green the average flow reduction due to the redistribution of flows enabled by the new paths on the network, and in red the increase of flows attracted by these paths. A significant discharge in the level of congestion in the main road network was the foreseen impact of the new infrastructure, but the access to the new freeway in the East-West direction shows some undesirable side effects in the neighborhood (Amara) inside the rectangle. The solutions to these problems demands a close up to the subnetwork and take decisions at the level of traffic control and traffic management schemes, not excluding even a partial reshaping of part of the street network. This type of decision requires a more detailed modeling, able of reproducing in a very accurate way the traffic conditions, accounting for the interactions between the vehicular flows and the infrastructure, and obviously including the influence of the traffic lights, objective that can only be achieved by a microscopic traffic simulation model. Figure 1: Expected impact of the new infrastructure in San Sebastian with an close up of the Amara neighborhood Figure 2 displays the corresponding model built with the AIMSUN2 traffic simulation software, the EMME/2 sub model has been built automatically from the AIMSUN2 model by means of an interface between both systems. Figure 2: AIMSUN2 micro simulation model of the Amara neighborhood The type of information that micro simulation can provide for a further analysis is beyond the capabilities of traditional static models. The average flows from sections to sections turning movements) for the allowed movements at selected intersections in the model, speeds and delays for every simulated time interval can be obtained. The dynamic analysis for a time period is completed with values for other traffic variables or indicators of the quality of service as number of stops, time delayed at stops, average queue lengths, etc. Figure 3 provide a further insight on the capacity of analysis provided by dynamic simulation software. The graphic in this figure describes the evolution over time of average flows. The same type of graph can be produced for average queue lengths on a subset of selected sections in the model. Figure 3: The evolution of average flows over time 1.3 The ease of model building and data input (AIMSUN2) The recent evolution of the microscopic simulators has taken advantages of the state-of-theart in the development of object-oriented simulators, and graphical user interfaces, as well as the new trends in software design and the available tools that support it adapted to traffic modeling requirements. A proper achievement of the basic requirements of a microscopic simulator implies building models as close to reality as possible. The closer the model is to reality the more data demanding it become. This has been traditionally the main barrier Section Volumes (Veh/h) 0 200 400 600 80",oceanology,38
9566723d1b3075cd14cabc8519a24a7eaa00cd5a,filtered,semantic_scholar,,2008-01-01 00:00:00,semantic_scholar,"using simulation tools for embedded software development class # 410 , embedded systems conference , silicon valley 2008",https://www.semanticscholar.org/paper/9566723d1b3075cd14cabc8519a24a7eaa00cd5a,"ion vs. Detail A key insight in building simulations is that you must always make a trade-off between simulator detail and the scope of the simulated system. Looking at some extreme cases, you cannot use the same level of abstraction when simulating the evolution of the universe on a grand scale as when simulating protein folding. You can always trade execution time for increased detail or scope, but assuming you want a result in a reasonable time scale, compromises are necessary. A corollary to the abstraction rule is that simulation is a workload that can always use maximum computer performance (unless it is limited by the speed of interaction from the world or users). A faster computer or less detailed model lets you scale up the size of the system simulated or reduce simulation run times. In general, if the processor in your computer is not loaded to 100%, you are not making optimal use of simulation. The high demands for computer power used to be a limiting factor for the use of simulation, requiring large, expensive, and rare supercomputers to be used. Today, however, even the cheapest PC has sufficient computation power to perform relevant simulations in reasonable time. Thus, the availability of computer equipment is not a problem anymore, and simulation should be a tool considered for deployment to every engineer in a development project. Simulating the Environment Simulation of the physical environment is often done for its own sake, without regard for the eventual use of the simulation model by embedded software developers. It is standard practice in mechanical and electrical engineering to design with computer aided tools and simulation. For example, control engineers developing control algorithms for physical systems such as engines or processing plants often build models of the controlled system in tools such as MatLab/Simulink and Labview. These models are then combined with a model of the controller under development, and control properties like stability and performance evaluated. From a software perspective, this is simulating the specification of the embedded software along with the controlled environment. For a space probe, the environment simulation could comprise a model of the planets, the sun, and the probe itself. This model can be used to evaluate proposed trajectories, since it is possible to work through missions of years in length in a very short time. In conjunction with embedded computer simulations, such a simulator would provide data on the attitude and distance to the sun, the amount of power being generated from solar panels, and the positions of stars seen by the navigation sensors. When the mechanical component of an embedded system is potentially dangerous or impractical to work with, you absolutely want to simulate the effects of the software before committing to physical hardware. For example, control software for heavy machinery or military vehicles are best tested in simulation. Also, the number of physical prototypes available is fairly limited in such circumstances, and not something every developer will have at their desk. Such models can be created using modeling tools, or written in C or C++ (which is quite popular in practice). In many cases, environment simulations can be simple data sequences captured from a real sensor or simply guessed by a developer. It should be noted that a simulated environment can be used for two different purposes. One is to provide “typical” data to the computer system simulation, trying to mimic the behavior of the final physical system under normal operating conditions. The other is to provide “extreme” data, corresponding to boundary cases in the system behavior, and “faulty” data corresponding to broken sensors or similar cases outside normal operating conditions. The ability to inject extreme and faulty cases is a key benefit from simulation. Simulating the Human User Interface The human interface portion of an embedded device is often also simulated during its development. For testing user interface ideas, rapid prototyping and simulation is very worthwhile and can be done in many different ways. One creative example is how the creator of the original Palm Pilot used a wooden block to simulate the effect of carrying the device. Instead of building complete implementations of the interface of a TV, mobile phone, or plant control computer, mockups are built in specialized user interface (UI) tools, in Visual Studio GUI builder on a PC, or even PowerPoint or Flash. Sometimes such simulations have complex behaviors implemented in various scripts or even simple prototype software stacks. Only when the UI design is stable do you commit to implementing it in real code for your real device, since this typically implies a greater programming effort. In later phases of development, when the hardware user interface and most of the software user interface is done, a computer simulation of a device needs to provide input and output facilities to make it possible to test software for the device without hardware. This kind of simulation runs the gamut from simple text consoles showing the output from a serial port to graphical simulations of user interface panels where the user can click on switches, turn knobs, and watch feedback on graphical dials and screens. A typical example is Nokia’s Series 60 development kit, which provides a virtual mobile phone with a keypad and small display. Another example is how virtual PC tools like VmWare and Parallels map the display, keyboard, and mouse of a PC to a target system. In consumer electronics, PC peripherals are often used to provide live test data approximating that of a real system. For example, a webcam is a good test data generator for a simulated mobile phone containing a camera. Even if the optics and sensors are different, it still provides something better than static predetermined images. Same for sound capture and playback – you want to hear the sound the machine is making, not just watch the waveform on a display. Simulating the Network Most embedded computers today are connected to one or more networks. These networks can be internal to a system; for example, in a rack-based system, VME, PCI, PCI Express, RapidIO, Ethernet, IC, serial lines, and ATM can be used to connect the boards. In cars, CAN, LIN, FlexRay, and MOST buses connect body electronics, telematics, and control systems. Aircraft control systems communicate over special bus systems like MIL-STD-1553, ARINC 429, and AFDX. Between the external interfaces of systems, Ethernet running internet standards like UDP and TCP is common. Mobile phones connect to headsets and PCs over Bluetooth, USB, and IR, and to cellular networks using UMTS, CDMA2000, GSM, and other standards. Telephone systems have traffic flowing using many different protocols and physical standards like SS7, SONET, SDH, and ATM. Smartcards connect to card readers using contacts or contact-less interfaces. Sensor nodes communicate over standard wireless networks or lower-power, lower-speed interfaces like Zigbee. Thus, existing in an internal or external network is a reality for most embedded systems. Due to the large scale of a typical network, the network part is almost universally simulated to some extent. You simply cannot test a phone switch or router inside its real deployment network, so you have to provide some kind of simulation for the external world. You don’t want to test mobile phone viruses in the live network for very practical reasons. Often, many other nodes on the network are being developed at the same time. Or you might just want to combine point simulations of several networked systems into a single simulated network. Network simulation can be applied at many levels of the networking stack. The picture below shows the most common levels at which network simulation is being performed. The two levels highlighted in green are the ones that are most useful for embedded software work on a concrete target model. Physical signalling Bit stream Packet transmission Network protocol Application protocol High-level application actions Analog signals, bit errors, radio modeling Clocked zeros and ones, CAN with contention, Ethernet with CSMA model Ethernet packets with MAC address, CAN packets, serial characters, VME data read/write TCP/IP etc. FTP, DHCP, SS7, CANopen Load software, configure node, restart Hardware/software boundary r r / ft r r The most detailed modeling level is the physical signal level. Here, the analog properties of the transmission medium and how signals pass through it is modeled. This makes it possible to simulate radio propagation, echoes, and signal degradation, or the electronic interference caused by signals on a CAN bus. It is quite rarely used in the setting of developing embedded systems software, since it complex and provides more details than strictly needed. Bit stream simulation looks at the ones and zeroes transmitted on a bus or other medium. It is possible to detect events like transmission collisions on Ethernet and the resulting back-off, priorities being clocked onto a CAN bus, and signal garbling due to simultaneous transmissions in radio networks. An open example of such a simulator is the VMNet simulator for sensor networks. Considering the abstraction levels for computer system simulation discussed below, this is at an abstraction level similar to cycle-accurate simulation. Another example is the simulation of the precise clock-by-clock communication between units inside a system-on-a-chip. Packet transmission passes entire packets around, where the definition of a packet depends on the network type. In Ethernet, packets can be up to 65kB large, while serial lines usually transmit single bytes in each “packet”. It is the network simulation equivalent of transaction-level modeling, as discussed below for computer boards. The network simulation has no knowledge of the meaning of the packets. It just passes opaqu",oceanology,39
79dd4e21811c1399a4525d82e16c8fbf23db3d51,filtered,semantic_scholar,Encyclopedia of Database Systems,1993-01-01 00:00:00,semantic_scholar,human-computer interaction,https://www.semanticscholar.org/paper/79dd4e21811c1399a4525d82e16c8fbf23db3d51,"Contents Foreword Preface to the third edition Preface to the second edition Preface to the first edition Introduction Part 1 Foundations Chapter 1 The human 1.1 Introduction 1.2 Input-output channels Design Focus: Getting noticed Design Focus: Where's the middle? 1.3 Human memory Design Focus: Cashing in Design Focus: 7 +- 2 revisited 1.4 Thinking: reasoning and problem solving Design Focus: Human error and false memories 1.5 Emotion 1.6 Individual differences 1.7 Psychology and the design of interactive systems 1.8 Summary Exercises Recommended reading Chapter 2 The computer 2.1 Introduction Design Focus: Numeric keypads 2.2 Text entry devices 2.3 Positioning, pointing and drawing 2.4 Display devices Design Focus: Hermes: a situated display 2.5 Devices for virtual reality and 3D interaction 2.6 Physical controls, sensors and special devices Design Focus: Feeling the road Design Focus: Smart-Its - making sensors easy 2.7 Paper: printing and scanning Design Focus: Readability of text 2.8 Memory 2.9 Processing and networks Design Focus: The myth of the infinitely fast machine 2.10 Summary Exercises Recommended reading Chapter 3 The interaction 3.1 Introduction 3.2 Models of interaction Design Focus: Video recorder 3.3 Frameworks and HCI 3.4 Ergonomics Design Focus: Industrial interfaces 3.5 Interaction styles Design Focus: Navigation in 3D and 2D 3.6 Elements of the WIMP interface Design Focus: Learning toolbars 3.7 Interactivity 3.8 The context of the interaction Design Focus: Half the picture? 3.9 Experience, engagement and fun 3.10 Summary Exercises Recommended reading Chapter 4 Paradigms 4.1 Introduction 4.2 Paradigms for interaction 4.3 Summary Exercises Recommended reading Part 2 Design process Chapter 5 Interaction design basics 5.1 Introduction 5.2 What is design? 5.3 The process of design 5.4 User focus Design Focus: Cultural probes 5.5 Scenarios 5.6 Navigation design Design Focus: Beware the big button trap Design Focus: Modes 5.7 Screen design and layout Design Focus: Alignment and layout matter Design Focus: Checking screen colors 5.8 Iteration and prototyping 5.9 Summary Exercises Recommended reading Chapter 6 HCI in the software process 6.1 Introduction 6.2 The software life cycle 6.3 Usability engineering 6.4 Iterative design and prototyping Design Focus: Prototyping in practice 6.5 Design rationale 6.6 Summary Exercises Recommended reading Chapter 7 Design rules 7.1 Introduction 7.2 Principles to support usability 7.3 Standards 7.4 Guidelines 7.5 Golden rules and heuristics 7.6 HCI patterns 7.7 Summary Exercises Recommended reading Chapter 8 Implementation support 8.1 Introduction 8.2 Elements of windowing systems 8.3 Programming the application Design Focus: Going with the grain 8.4 Using toolkits Design Focus: Java and AWT 8.5 User interface management systems 8.6 Summary Exercises Recommended reading Chapter 9 Evaluation techniques 9.1 What is evaluation? 9.2 Goals of evaluation 9.3 Evaluation through expert analysis 9.4 Evaluation through user participation 9.5 Choosing an evaluation method 9.6 Summary Exercises Recommended reading Chapter 10 Universal design 10.1 Introduction 10.2 Universal design principles 10.3 Multi-modal interaction Design Focus: Designing websites for screen readers Design Focus: Choosing the right kind of speech Design Focus: Apple Newton 10.4 Designing for diversity Design Focus: Mathematics for the blind 10.5 Summary Exercises Recommended reading Chapter 11 User support 11.1 Introduction 11.2 Requirements of user support 11.3 Approaches to user support 11.4 Adaptive help systems Design Focus: It's good to talk - help from real people 11.5 Designing user support systems 11.6 Summary Exercises Recommended reading Part 3 Models and theories Chapter 12 Cognitive models 12.1 Introduction 12.2 Goal and task hierarchies Design Focus: GOMS saves money 12.3 Linguistic models 12.4 The challenge of display-based systems 12.5 Physical and device models 12.6 Cognitive architectures 12.7 Summary Exercises Recommended reading Chapter 13 Socio-organizational issues and stakeholder requirements 13.1 Introduction 13.2 Organizational issues Design Focus: Implementing workflow in Lotus Notes 13.3 Capturing requirements Design Focus: Tomorrow's hospital - using participatory design 13.4 Summary Exercises Recommended reading Chapter 14 Communication and collaboration models 14.1 Introduction 14.2 Face-to-face communication Design Focus: Looking real - Avatar Conference 14.3 Conversation 14.4 Text-based communication 14.5 Group working 14.6 Summary Exercises Recommended reading Chapter 15 Task analysis 15.1 Introduction 15.2 Differences between task analysis and other techniques 15.3 Task decomposition 15.4 Knowledge-based analysis 15.5 Entity-relationship-based techniques 15.6 Sources of information and data collection 15.7 Uses of task analysis 15.8 Summary Exercises Recommended reading Chapter 16 Dialog notations and design 16.1 What is dialog? 16.2 Dialog design notations 16.3 Diagrammatic notations Design Focus: Using STNs in prototyping Design Focus: Digital watch - documentation and analysis 16.4 Textual dialog notations 16.5 Dialog semantics 16.6 Dialog analysis and design 16.7 Summary Exercises Recommended reading Chapter 17 Models of the system 17.1 Introduction 17.2 Standard formalisms 17.3 Interaction models 17.4 Continuous behavior 17.5 Summary Exercises Recommended reading Chapter 18 Modeling rich interaction 18.1 Introduction 18.2 Status-event analysis 18.3 Rich contexts 18.4 Low intention and sensor-based interaction Design Focus: Designing a car courtesy light 18.5 Summary Exercises Recommended reading Part 4 Outside the box Chapter 19 Groupware 19.1 Introduction 19.2 Groupware systems 19.3 Computer-mediated communication Design Focus: SMS in action 19.4 Meeting and decision support systems 19.5 Shared applications and artifacts 19.6 Frameworks for groupware Design Focus: TOWER - workspace awareness Exercises Recommended reading Chapter 20 Ubiquitous computing and augmented realities 20.1 Introduction 20.2 Ubiquitous computing applications research Design Focus: Ambient Wood - augmenting the physical Design Focus: Classroom 2000/eClass - deploying and evaluating ubicomp 20.3 Virtual and augmented reality Design Focus: Shared experience Design Focus: Applications of augmented reality 20.4 Information and data visualization Design Focus: Getting the size right 20.5 Summary Exercises Recommended reading Chapter 21 Hypertext, multimedia and the world wide web 21.1 Introduction 21.2 Understanding hypertext 21.3 Finding things 21.4 Web technology and issues 21.5 Static web content 21.6 Dynamic web content 21.7 Summary Exercises Recommended reading References Index",oceanology,40
d239d3668f8653d5790d7029a344c18b1bfae493,filtered,semantic_scholar,,2018-01-01 00:00:00,semantic_scholar,"geoworldsim: a time-asynchronous, distributed and intelligent environment based geosimulation platform",https://www.semanticscholar.org/paper/d239d3668f8653d5790d7029a344c18b1bfae493,"Imagine that the accessibility of the population to public infrastructures needs to be evaluated. A possible solution would be to calculate every distance and analyse which percentage of the population is in less than 300 meters. However, this solution does not take into account issues such as: the age distribution, functional diversity or people’s preferences when transiting the city. It is in these cases when it is necessary to go a step further and integrate Geographic Information Systems with other perspectives such as Multi-Agent Systems which represent the particular characteristics of each individual and their decision making processes. This integration is known as Geosimulation and builds more accurate simulations to model the physical reality together with social, demo graphic and economic components. 
Geosimulations aim at modelling systems at the scale of individuals and entity-level units of the built environment and provides a way to simulate big amounts of agents interacting in a virtual geographic environment and endowed with spatial cognitive capabilities (perception, navigation, reasoning). This dissertation presents a new Geosimulation platform design and implementation that allows analysing and simulating different urban infrastructures. The platform manages to put into practice the latest theories in Multi-Agent Systems along with the new techniques in cloud computing and asynchronism. The proposed design is evaluated for three case studies; ubiquitous IoT, sustainable transport policies and resilience of the power grid. The methodologies presented, provide progress to their respective research areas by improving state-of-the-art techniques or designing new mechanisms. 
Furthermore, by connecting these Geosimulations to the real world by sensors and actuators, the concept of mixed reality arises; simulations where changes in the real world are transferred to the virtual world through sensors and agents can influence the real world through actuators. Mixed realities allow developing distributed control systems, which not only take into account the physical reality and social preferences but also the state, where to deploy intelligent agents that provide services to citizens.",oceanology,41
f77d59740dfeb10e9b650ec8b1baba91fca70279,filtered,semantic_scholar,,2011-01-01 00:00:00,semantic_scholar,navibeam: indoor assistance and navigation for shopping malls through projector phones,https://www.semanticscholar.org/paper/f77d59740dfeb10e9b650ec8b1baba91fca70279,"We present our concept of an indoor assistance and navigation system for pedestrians that leverages projector phones. Digitally enhanced guides have many advantages over traditional paper-based indoor guides, most of all that they can be aware of their current context and display dynamic information. That is why particularly shopping malls recently started deploying indoor assistance applications for mobile phones, which also include support for navigation. Moreover, as we show in the paper, projected interfaces offer additional distinct advantages over static guides and even traditional or augmented reality mobile applications. We describe five concepts for a shopping mall indoor assistance system based on projector phones, comprising support for shop selection, precise way finding, “virtual fitting” of clothes, and context-aware and ambient advertisements. We then apply the concepts to a typical shopping scenario, where users wear the phone at their belt and constantly project the interface in front of them. Expected benefits of our system are that people find their way quicker, easier, and less distracted from their usual shopping experience. Finally, we discuss the technical feasibility of our envisioned implementation and research questions we are particularly interested in. INTRODUCTION Navigation and location-based services for pedestrians recently gained a lot of attention and are becoming rapidly adopted. Very popular among these are applications for location-based places recommendations and turn-by-turn navigation. While these applications mostly focus on outdoor navigation, less attention has been paid to the opportunities for providing indoor assistance with mobile devices. Especially in large complex buildings, e.g. shopping centers, in most cases static signs, You-Are-Here maps, or paper flyer maps are still the only available navigation assistance for visitors. Preliminary observations and interviews we conducted in nearby shopping centers show that, at least in Germany, available navigational aids are still as insufficient as Levine reported them to be almost 30 years ago [8]. Despite a lot of research projects that explored indoor assistance over the last decade, it was not before the beginning of 2011 that we saw the first mobile shopping applications reaching consumer markets, such as the Sam’s Club mobile application [1], which provides indoor navigation to specific items and/or shops in some selected American shopping malls. Similarly, some shopping centers in Asia introduced mobile AR applications for shopping assistance [2]. In our research group we study future application areas of projector phones, i.e. mobile phones with integrated projectors (see [11] for a detailed survey). We found projector phones to provide some distinguished advantages for indoor navigation assistance, e.g., that interaction can be handsfree and the projection can serve as ambient display, thereby not contradicting the traditional shopping experience. Further that the surrounding world can be directly augmented, freeing the user from mapping between mobile display and real world and that the output space is much larger than on mobile displays. And finally, that bystander can see and attach to the projected output. RELATED WORK We present relevant work dealing with mobile shopping, recommender systems, indoor positioning and navigation. One of the first works on digital mobile shopping assistants has been done by Asthana et al. [3]. They presented main usage scenarios, e.g., telling people where to find certain products or informing them on discounts and special offers. Similar can be recognized in aforementioned mobile shopping applications and as well in recently filed patents, e.g. from Apple Inc. (US 2010/0198626 A1, US 2010/0191578 A1), which include navigation, service interactions (e.g., parking tickets), and support for social networking. Yang et al. [15] developed a location-aware recommender system. It learns a customer’s interests from previously visited product websites and continuously presents a list of products around the customer’s current location, that are likely to interest him. The software also takes into account the distance to shops and is able to learn customer’s preference between highly interesting products and far distances. Butz et al. [4] present a hybrid indoor navigation system that is able to adapt route instructions to different output devices (screen resolution, device resources) and based on the precision of available location information. Results from [7,14] indicate that intelligent fusion of infrastructure techniques, e.g. GPS, GSM triangulation, and sensors like accelerometers, magnetometers, and gyroscopes, enables precise indoor location tracking with current commercially available smartphones in unaltered environments. Kray et al. [6] explore the design space of routing instructions for pedestrians on mobile devices. Narzt et al. present a specific mobile application for augmented reality (AR) [10]. Alternative systems for pedestrian navigation are the Rotating Compass by Rukzio et al. [12], showing personalised navigation information on public displays and the CabBoots system by Frey [5], which guides users by means of tactile output in the shoes. Aforementioned techniques, with the exception of the last two, rely on holding a handset device. However, we feel strongly inclined that holding a device in hand for a longer time does not fit well the traditional shopping experience. Negative side effects, e.g., arm fatigue, regular switching of the field of view, do not allow using the shopping assistance application as constant companion. CONCEPT In our envisioned prototype, people wear their projector phone on their belt, projecting a display right in front of their feet (Figure 1). In the following we present five concepts for mobile shopping assistance that are enabled by projector phones. Later we apply these concepts to a typical shopping scenario. Shadow Interaction Since our concept expects people to wear the projector fixed to the belt most of the time, direct interaction with the mobile device would not be sufficient as the only interaction technique. Audio is not an alternative because of the noisy environment in a shopping mall. This leaves users with the option to directly interact with the projection, either by feet or hands (or gaze in the future). In preliminary studies we discovered that foot-based interaction is not well suited due to the fact that foot movement inherently involves movement of the body at the same time, which makes interaction cumbersome. We found interacting with the shadow of a finger in front of the projection a much more promising solution. Figure 2 shows the stroke of the resulting shadow on the projection. With the tip of the shadow, all points on the projection can be reached. Items should be highlighted once the shadow reaches their bounding box to give adequate feedback to the user. Although the tracking of the finger’s height to enable traditional press/release behaviors would be possible, this would require the user to maintain a complex mental model of different finger height levels. Instead, our concept builds on the fact, that the tip of the index finger can be moved well without changing the shape of the rest of the hand or even the middle knuckle of the index finger. Thus, to select an item, a user moves the index finger to point on the desired item and then bends the index finger and unbends it again. Alternatively, one could use the finger’s dwell time as in Microsoft’s Kinect. To the best of our knowledge, shadow interaction with projections has not been reported before. Radar of Recommendations Another concept that is particularly useful with projected navigation is a radar of recommendations. Building on [15], we want to constantly show and update a personal radar of products the user might be interested in (Figure 2). Based on the information available from accounts with online stores (e.g., Amazon) and items recently explored with our system (see fourth concept), users see offers of nearby stores in front of them and can select these items to start a navigation in the middle of the circle. The size of an item conveys the expected interest of the user, the distance from the middle depicts the walking distance (not air path) from the user’s current location. The size of the radius of interest (distance to shops and items) can be adjusted by slightly changing the angle of the projector, similarly to looking further ahead. Different from [15], the projection provides a much larger output space and the radar serves as ambient display in the user’s periphery. Projected (augmented) Navigation When the user selected an item or shop he is interested in, the assistance system starts a projected navigation. In outdoor navigation, turn-by-turn navigation is still the most prominent, though we have seen augmented reality been used in research [10]. Especially for indoor navigation, where there are more tight and subsequent turns or small decision spaces (take the left stairs up, not the right stairs down) our experience is that turn-by-turn navigation does not work well. Therefore, we want the user not to follow Figure 1: The projector is worn at different locations on the belt (left and middle) and can optionally be taken into hand to change the angle of the projection (right). Figure 2: The user interacts with his radar of recommendations through the shadow of his finger. The orange outline shows the shadow, the left red circle the fingertip that, for clicking, can be changed independently of the finger’s middle knuckle (right circle) or the rest of the hand’s shape. this type of directions, but instead simply follow a blue line projected in front of her (see Figure 3). Since the projector phone is spatially-aware, both in terms of location and orientation, the blue line is projected as a static augmentation of the real world, i",oceanology,42
ab60ca1c14bfd348b10074258db61003349cf2de,filtered,semantic_scholar,,2006-01-01 00:00:00,semantic_scholar,an empirically derived taxonomy of information systems integration,https://www.semanticscholar.org/paper/ab60ca1c14bfd348b10074258db61003349cf2de,"Information systems integration (ISI) represents the degree of cooperation in information system practices between business functions within a firm and between a firm and its trading partners. Although the establishment of information systems integration objective has been reported as one of the key concerns of top management because ISI enhances the firms’ competitiveness and growth, the classification of the information system practices and its managerial implications are still vaguely developed. The two objectives of this paper are: (1) to develop a taxonomy of information systems integration (ISI) called ISI-Matrix, and (2) to report managerial implications for matching each information system class with business process applications. By using a systematic research investigation approach, two ISI structures are identified: Internal ISI (IISI) and External ISI (EISI) from the responses of 220 firms. The ability to identify and understand the implications of the ISI-Matrix is of critical importance to both academic and management practitioners. INTRODUCTION The rapid changes in perspective toward globalization of markets and manufacturing has forced management to re-configure the traditional views of business functions and replace them with business processes. The process view of organizations embraces cross-functional teams which penetrate networks of inter-organizations and intra-organizations. Within the process, a project team performs many tasks across functional barriers (with a firm and between a firm and its trading partners) to meet corporate goals in a more seamless way. This increased emphasis on improving business processes has triggered the need for placing information systems (IS) in a strategic role of corporate strategy as opposed to a supportive role in the traditional view (Raghunathan & Raghunathan, 1990; Chan et al., 1997; Goodhue et al., 1992). A review of the empirical literature reveals that one issue, the linkage of IS practices with organizational objectives, has been among the top problems reported by information systems (IS) managers and business executives (Reich and Benbasat, 1996; Computerworld, 1994; Lederer and Mendelow, 1986; Earl, 1989). Information Systems Integration (ISI) is the degree of cooperation between business functions within the firm and between a firm and its trading partners on an internally consistent set of strategic, operational, and infrastructural information systems practices using information systems (IS). In a broader sense, ISI often represents as a pressing concern of misalignment of information system practices between two business processes (King and Teo, 1997; Segars and Grover, 1998). In this context, information system practices, which are utilized to accomplish process tasks at each end, lack degree of congruence when certain processes/tasks involve cross-functional boundaries at the other end. Consequently, the first objective of this paper is to identify a set of IS practices that is shared by process team members. Therefore, ISI represents the degree of cooperation in information system practices between business functions within a firm and between a firm and its trading partners. ISI has been reported to facilitate the possibilities of increased productivity, customer responsiveness, and the synchronization of diverse organizational settings. It has been documented that the introduction and utilization of ISI enhance firms’ competitiveness and growth, product quality, productivity, machine utilization, space management, and logistics efficiency and flexibility (Gross, 1984; Kaltwasser, 1990; Noori and Mavaddat, 1998). A higher degree of ISI creates information visibility and captures the moments of information which enable T. Jitpaiboon, T.S. Ragu-Nathan & M. Vonderembse 2005 Volume 15 Number 2 18 collaborative members of the supply chain to manage their business processes and share information better (Lummus and Vokkurka, 1999; Gunasekaran and Ngai, 2004; Bourdreau and Couillard, 1999; Williams et al., 1997; Gangopadhyay and Huang, 2004). Although ISI has been reported to positively impact firm performance, the classification of the information system practices and its managerial implications are still vaguely developed. The classifications of ISI in the current literature are extremely broad and fragmented. There is no consensus on what constitutes an ISI taxonomy. Our goal is to develop a comprehensive ISI taxonomy to aid organizations whose ability to harness the power of IS practices is critical to their success. Development of valid and reliable instruments to be used in large-scale surveys is an important first step toward this goal. The resulting taxonomy should help organizations to match information system class with their current business process applications which should enhance a firm’s internal and external integration. In a narrow sense, focusing on the survey approach, this study arguably classified ISI into two main categories namely Internal Information Systems Integration (IISI) and External Information Systems Integration (EISI). In each category, ISI construct is also clustered into three levels namely Strategic Integration, Operational Integration, and Infrastructural Integration. Infrastructural integration is also subdivided into two sub-constructs: Data Integration and Network Integration. Table 1 shows the components of ISI classifications. By deploying this classification scheme, the second objective of this paper is to propose a classification matrix (ISI-Matrix) which will be used to provide managerial implications for both researchers and practitioners. The next section of this paper defines and discusses the ISI taxonomy. The following sections describe the research design and discuss the candidate measured used to evaluate the degree at different ISI levels. The subsequent section presents the results, and the final section discusses the implications of our findings for researchers and practitioners. Table 1: Information System Classifications. Internal Information System Integration (IISI) External Information System Integration (EISI) • Strategic Integration – Internal • Operation Integration – Internal • Infrastructural Integration – Internal o Data Integration – Internal o Network Integration – Internal • Strategic Integration – External • Operation Integration – External • Infrastructural Integration – External o Data Integration – External o Network Integration – External INFORMATION SYSTEMS INTEGRATION TAXONOMY A description of previous taxonomies The rapid changes in the role of information systems (IS) are presenting firms with significant challenges and dramatic opportunities. Revolutionary advances in hardware and software capabilities coupled with reduced prices have shifted numerous applications from infeasible to feasible, changed the structure of organizations, and forced management to rethink the classification of IS. The terms taxonomy and framework are sometimes used interchangeably in the literature (Doke and Barrier, 1994). However, a clear distinction between the two is identified: taxonomy is generally used to describe a classification scheme for “things” such as IS. Although framework is sometimes used as a synonym for taxonomy, it is more often used to describe models that organize and group “concepts and relationships” (Doke and Barrier, 1994). The taxonomy is derived from the characteristics of the measured subjects, so the categories are both exhaustive and mutually exclusive (Fiedler, Grover, and Teng, 1996). This method is especially useful when one is examining unexplored phenomena because both methods must be empirically examined to evaluate the representativeness and generalizability of the classifications to the population they are mean to describe. Unlike the predetermined, idealized categories of a typological methodology that lend themselves to prescriptive hypothesizing, taxonomy’s classifications emerge from analysis so that the classification is derived (Doty and Glick, 1994; Hair, Anderson, Tatham, and Black, 1998). Developing a taxonomy An Empirically Derived Taxonomy Journal of International Technology and Information Management 19 can be viewed as a multistep process including the classification scheme, measurement development, multivariate analysis of the classification criteria to produce the item groupings, and the evaluation of the classification groupings (Fiedler, Grover, and Teng, 1996). The classification systems should “mirror the real world...describe organizational reality in a way that is recognizable to and consistent with the vision of practitioners and researchers alike as a viable reproduction of the diverse world in which we live in” (Rich, 1992). Since the focus here is the classification of ISI, a taxonomy will be used to classify these models Dimensions of a Taxonomy Integration is “to make into a whole” (Oxford English dictionary). The study of ISI classifications started as early as 1985 by Mudie and Schafer. They analyzed ISI in process terms, as they believed ISI should not only facilitate the process of development and use of data, applications, and other processing technology, but also should provide the flexibility to meet the future business demands in workstations, processing types, and applications. Wyse and Higgins (1993) defined ISI as the extent to which data and applications through different communication networks can be shared and accessed for organizational use. They defined ISI into two components: data integration and technical integration. Data integration refers to the relevancy of the information that is collected, processed, and disseminated throughout the firm. Technical integration concerns the physical or formal linkage of information systems and subsystems that are used by the firm. Webber and Pliskin (1996) defined ISI in the merger or acquisition context as the extent of the integration of IS and data processing functi",oceanology,43
225b6e4de88e9be8caa5c36224d135e1ed6f00ee,filtered,semantic_scholar,,2009-01-01 00:00:00,semantic_scholar,extended abstract : a framework for virtual surgery,https://www.semanticscholar.org/paper/225b6e4de88e9be8caa5c36224d135e1ed6f00ee,"Surgical simulation for medical education and preoperative planning has attracted more and more attention in recent years and a number of such virtual environments have been developed and validated. However, almost all of them are focusing on simulating the surgical scene and haptic interaction to provide users with freedom to perform surgery in the virtual environment. We propose that critical surgical procedures and motion path could be guided by an information intensive process model, especially those being trained in such a virtual surgical environment. In this paper, we outline the virtual surgery framework and the design of the software environment for suturing procedure. The preliminary system that incorporates the above functionality with realistic surgical scene and haptic interaction is still on the development stage. Potentially, the information based modeling process of the surgical motion could also help automate the process of the procedural operation of surgical robot. THE CONCEPTUAL FRAMEWORK The general view of our framework architecture is shown in Figure 1, where the whole system is composed of the hardware part and the software part. In the hardware side(refer to Figure 2), besides such tradition input and output device like keyboard, mouse, haptic device and display, we leave room for sensors to record surgical motion as system input or drive micro assembly work cell to validate and measure our virtual environment in precisely simulating the microsurgery process quantitatively. The software architecture is generally composed of the following three modules. Fig. 1. Framework Architecture A. The IDEF-0 Parser In our approach, the emphasis is on the creation of an Information Intensive Process Model using the IDEF-0 modeling methodology. This model will be used to identify critical and non critical categories of information encompassing the core steps in a neurosurgical (diagnosis) and surgery process; these will possibly include the key or driving assumptions, information inputs, skill constraints, the intermediate ’attribute’ outcomes between various steps or stages of the process in reference as well as the crucial performers (which can range from the medical personnel involved in the diagnosis and surgery D/S itself to the medical assisting devices which play a key role in the outcome of various steps in this D/S process). This information model will not only capture the functional relationships among related tasks at various levels of abstraction but will also enable the representation of temporal precedence constraints among sub-tasks. They will provide a valuable insight into the process of micro surgery; our background in engineering and IT and our prior work in such modeling activities will be helpful in (a) Haptic: Phantom Premium (b) Microassembly Work Cell Fig. 2. Hardware Setup developing such a model. A lower level decomposition of this model is shown in Figure 3. Fig. 3. IDEF-0 Model (A2 Level Decomposition) We seek to develop a robust understanding of these surgical processes using information modeling strategies that have proven to be successful in understanding functional relationships within complex processes that range from performing micro devices assembly to designing virtual reality based simulation environments for satellite assembly (among others). B. Surgical Mode Selection After the framework reads and parse the customizable IDEFL-0 model file, the user will be having the option to further customize the system mode, which is composed of training mode, planning mode, and evaluation mode. The default mode is the virtual environment without any constraints. Those three modes in Figure 1 are described as: • Training and Evaluation Mode: students are able to practice surgery following those constraints defined in the information intensive process model (IIPM) by instructors or experienced surgeons. Meanwhile, the system could evaluate students’ performance based on the comparison of their operation with those constraints defined in the model. • Planning Mode: surgeons could first freely explore different potential surgical strategies in the virtual environment, and then decide a couple of final procedural surgical plans and define those procedural constraints in the IIPM for future validation. • Validation Mode: this is where users could validate those surgical operations in the virtual environment by driving the physical validation system. In our system, since we are focusing on microsurgery, a microassembly work cell (in Figure 2(b)) is used as validation purpose. C. The Virtual Reality Engine This is where the graphics visualization, collision detection and haptic rendering happens, and the FEM Analysis module in VR Engine is used to simulate the soft tissue deformation. Meanwhile, the surgical path generator module is used to define haptical constraints and visualize critical path. The red line in Figure 6 in section IV illustrates this idea. CASE STUDY: VIRTUAL SUTURING In this section, we give a overview of the model and development process of our virtual suturing system based on the above framework. During the early modeling stage, we got our first hand suturing information by learning the microsurgery lab manual and direct clinical microsurgery experience one of our author of this paper has over 20 years clinical microsurgery experience, and the real sutured scene in Figure 4 was taken by a micro lens camera after he was done with the suture. Fig. 4. Real Suturing Scene A. Information Intensive Process Model and Constraints After collecting all those real world information, we are able to model the whole process. The IDEF0 diagram in Figure 3 gives a general model of the real surgical Suturing process. For simplicity of this paper, we will not give the further decomposition here. Meanwhile, two critical steps in performing suturing is illustrated in Figure 5, where Figure 5(a) shows the critical path when the needle starts to insert into the vessel the needle has to be as perpendicular as possible to the target surface, and Figure 5(b) shows the force constraints of how to guide the needle go through the vessel wall. B. Preliminary Results From Figure 5, we can notice that the surgical path during needle insertion and going through vessel wall are critical steps of a successful suture, so to speak, they are important evaluation criteria in the training and evaluation mode, potential procedural constraints in the planning mode, as well as (a) Insertion Angle (b) Shear Force Fig. 5. Suturing (Courtesy: Internet) Fig. 6. Linear Constraints precision motion control in the validation mode. Because the framework is still in the early development stage, in order to illustrate the idea how we incorporate constraints for those critical steps, we developed a much simplified scene to show how the linear constraint guide a surgical cut procedure in Figure 6, where the red line represents the linear constraints -when the scalpel is about to cut the blood vessel (represented by a hollow cylinder), the user will be able to feel a force that guide their movement along the line. RELATED WORK Due to the huge volume of recent research papers on virtual reality system for surgical training and preoperative planning, [8],[6] and [12] have given a relatively detailed review of surgical simulation applications and technology. We limit our survey on the state of art of virtual reality system for training and complex microsurgery preoperative planing, and those key modeling and visualization techniques that enabled such virtual reality system. Surgical Training and Planning System:Though technology has been evolved a lot for the past decades, it is still a challenging task to efficiently simulate the complex surgical operation environment. To avoid the situation of starting an over ambitious project that includes everything but eventually get nothing done, most of the virtual surgery system focus on creating a realistic surgical scenario for some specific purpose, e.g. suturing training [8] [7][19] [1][13] or preoperative planning [21][18] [9][16][17]. Soft Tissue Modeling: Simulating the soft tissue deformation under surgical operation is not trivial, even a simple procedure involves great effort, eg. [3][11] [4][10] were specifically devoted to simulate the cutting procedure. [14] gives a detailed survey of the real-time deformable models used for surgery simulation. Among those models mentioned in [14], MassSpring model and FEM model are the two dominant models currently used in the research community. For its simplicity and low computational cost, the heuristic Mass-Spring model has attracted lots of research and was extensively optimized and deployed in the early years, such as in SPRING system developed by Kevin et all [15]. However, due to its lacks of realism, recent research work are shifting to center around the later continuum mechanics based FEM model as in [1] [5][2] for its accuracy and realistic mechanic behavior, though it is computational costly and needs lots of optimization to achieve real time performance.",oceanology,44
59304bc73b6d24f18d23404e0d408c462b66c4d0,filtered,semantic_scholar,Softwaretechnik-Trends,2008-01-01 00:00:00,semantic_scholar,simulationsbasierte analyse und entwicklung von peer-to-peer-systemen,https://www.semanticscholar.org/paper/59304bc73b6d24f18d23404e0d408c462b66c4d0,"Peer-to-Peer (P2P) systems are distributed systems composed of up to millions of functionally equivalent entities (peers), which form P2P overlay networks on top of physical networks to communicate. The functionality of a peer is implemented by a P2P application which de nes the behavior of the whole P2P system. The equivalence of peers is realized by providing client functionality as well as server functionality. Implementing a P2P system with speci ed behavior is a di cult task because the behavior depends on many factors, such as the used P2P search methods and the underlying physical network. Some factors cannot be taken into account completely because of their complexity or unknown or not understood parts. For instance, the prospective user behavior may only be estimated based on observed data. When engineering complex, dynamic software systems such as P2P systems, simulation is often used to analyze the properties of these systems based on simulation models in an early development phase. With simulations in natural sciences, the separation of reality and (simulation) model is clear: the reality exists in nature, while the model exists as software within some computer system. When simulating software systems, this separation is not so obvious: the simulated model is itself a software system. With P2P systems, for instance, a simpli ed P2P system is modeled and simulated for predicting properties of real P2P systems. The new software engineering contribution of this work is the Peer Software Engineering (PeerSE) method, which allows a controlled transition from simulation models to real-world software systems. The method starts with a comparative analysis of simulation models for P2P systems and proceeds iteratively toward the experimental implementation in a laboratory setting and nally a real-world P2P system deployed in a target environment. The method includes a simulation model for P2P systems and a tool supporting the execution of simulation and laboratory experiments. Simulation is an essential part of the PeerSE method used to identify and to compare models ful lling given requirements. When an appropriate model has been found, model components can be reused and further re ned to implement a laboratory P2P system. To allow for a controlled transition of model components to laboratory components, the results of simulation and laboratory experiments are directly compared using the same metrics. The applicability of the PeerSE method has been successfully evaluated by analyzing and realizing a P2P system for distributed software development.",oceanology,45
b5feeee7c1e5e6acf7b6ee0314415102b09068e0,filtered,semantic_scholar,,2003-01-01 00:00:00,semantic_scholar,a class library for manufacturing systems,https://www.semanticscholar.org/paper/b5feeee7c1e5e6acf7b6ee0314415102b09068e0,"This work presents a class library for manufacturing systems that aims at facilitating the construction of simulation models, allowing reuse and speeding up the modeling process. This library implements a modeling approach that differs from the majority of similar works in this area. It is based on the application of well known manufacturing concepts, like production routings and activities. It allows the creation of new simulations faster than other methodologies, since complex translations from the reality to simulated applications are not necessary. The development of this library was validated by modeling the production line of tractor parts. The production line case study, modeled using both Automod and the proposed class library, allowed a quantitative comparison for the validation of this work. INTRODUCTION The fast dissemination and widely acceptance of simulation methods contributed to the emergence of specialised environments for programming and simulation of manufacturing systems. Simulation is virtually the only methodology at present which is capable to provide accurate performance estimates for manufacturing systems design (Govindaraj et al. 1990). These environments offer tools that allow the efficient analysis from simple to complex systems. Their main limitation is the difficulty to develop new simulation models. Their model building activity usually requires specific information that is related to the internals of the adopted simulation tool. They are usually restricted to highly specialised users, since the systems usually adopt proprietary languages and particular simulation methodologies. On the other hand, the object-oriented paradigm has taken the attention of the scientific community. Properties like encapsulation, inheritance, and reuse have contributed to make the object-oriented technology very popular for manufacturing systems simulation and deployment. The oneto-one mapping between objects in the manufacturing systems being modelled and their abstractions in the simulation model offer conditions for a better modelling. The object-oriented paradigm have been explored to allow the construction of high-fidelity simulation software for supporting the modelling and control of integrated and complex manufacturing systems. The object-oriented paradigm and distributed objects have emerged as some of the most promising technologies since they allow a natural structure for the problem domain of industrial automation systems. This occurs because the industrial components can be easily mapped into the oriented-objects model diagrams using classes and objects. Objects encapsulate the functions of many components, modularizing the description, encouraging reuse, and allowing implementations as well in software as in hardware. There are many commercial simulation environments for manufacturing systems. They offer many features that permit to build powerful simulations in order to develop advanced solutions for new projects through experimenting many alternative configurations. One of these simulation tools is Automod (Auto Simulations, 2002). Automod (Auto Simulations, 1999) is a process-oriented simulator. The simulation method used in Automod is based on moving parts, representing the work in process, on resources, and on transportation entities. To create a simulation using Automod, the designer must control the traffic of parts through the system using a process specification, which indicates the correct pathway that the part must follow. This approach gives to the simulator robust and reliable features, allowing complex model constructions. The major problem of these tools is the difficulty to build new simulation models. The modeling activity in these environments needs a detailed previous study about the tools, generally demanding highly specialised users. Moreover, these environments do not use important modeling concepts like reuse and extensions. Analysing the current state of the art, this fact is unacceptable. An integration between advantages of the commercial simulators and of the new modeling paradigms becomes mandatory. This fact may increase the quality of produced models, since developers only need to worry about the manufacturing problems, without having to worry about particularities of simulation environments. The main goal of this work is the design of a class library for manufacturing systems that allows the development of simulation models to evaluate these systems. Basic components of these systems should be available, and it should be possible to extend them through high-level modeling features found in object-oriented systems, like aggregation and abstraction, aiming at an easier modeling of complex systems, by encouraging reuse and extensions. STATE OF THE ART IN MANUFACTURING SYSTEMS SIMULATION Bertotto (2001) presents a complete study of the state of the art in manufacturing systems simulation. Modeling, control, and simulation methodologies have attracted the attention of researchers and non-governmental consortia to find new ways of developing manufacturing systems. Narayanan (1998) describes relevant efforts in the specification of object-oriented hierarchies applied to manufacturing systems simulation. Among other efforts, it is worth mentioning the application of Distributed Artificial Intelligence (DAI) and multi-agent systems, the ideas proposed by a consortium called Holonic Manufacturing Systems (HMS), and CORBAManufacturing. Park (1997) introduces an object-oriented modeling schema for the design of automated manufacturing systems (MAS) called JR-net. To propose this schema, Park relies on the fact that the modern flexible manufacturing systems are modular and hierarchical structures, built from standard resources. This proposal is formally based on processes and resource definitions, relationships among objects, procedures to build the model, and on the model structure. Its purpose is to shown that the JR-net modeling framework could be used in the same way as a graphical modeling tool for commercial simulators, such as Automod. A CLASS LIBRARY FOR MANUFACTURING SYSTEMS SIMULATION The main feature of this library is its modeling approach, which is different from other efforts in this area. This approach is based on activities and production routings. The production control is established by production routings that own activities and activate the respective resources. This approach makes the design easier, since the current theory of manufacturing systems is based on this kind of concepts. New simulations may be developed faster than in other methodologies, since complex translations between the reality and the simulation models are unnecessary. The class diagram of the proposed library is represented in Figure 1. This library contains a set of classes that supports both the physical and logical modeling of a manufacturing system. The physical modeling is based on classes that represent the manufacturing system entities, like machines, robots, and tasks. The classes that represent the physical resources can be easily extended to represent other equipments that were not yet created. The logical modeling is based on classes, methods, and attributes also similar to real world logic concepts. Class Library Specification This library was created using UML, a standard language for object modeling containing various diagrams. The library is composed by 30 classes, linked by relationships, aggregations, and specializations. The root of the hierarchy is the Production_Line class. This class aggregates two other classes: Resource and Master_Plan. The Resource class is the root of all classes that execute some transformation on manufacturing parts. The Master_Plan class is the root of all classes that logically control the manufacturing system. In this class, the production plan based on client orders is generated. To create the master plan, the Master_Plan class uses services provided by other classes, such as Order, Scheduling_Algorithm, Part, Resource and Daily_Capacity. The user interacts with the model using the Order class, inserting orders to be produced. The Scheduling_Algorithm class implements the production scheduling algorithms that will guide the master plan creation. When the master plan is generated, the production orders should be created to control the production. Each instance of Production_Order will order the production of one part. If, for example, there is an order of 30 parts, there will be 30 instances of Production_Order. This approach makes the production flow easy. Moreover, it is necessary to update the capacity information of the involved resources. Before the creation of the production orders, Production_Order estimates the daily capacity of the resources, using the Daily_Capacity class related to the Resource class. A variation of a production routing was used in the library to control the production in the various resources. The production routing is represented by an aggregation of the Activity class within the Part class. The aggregated Activity classes describe a logical sequence represented by a relationship that shows the next and the previous activity. An activity can only be executed if all the previous activities were executed. This aggregated structure builds a production graph that may control various activities at different levels of the production hierarchy. When an activity is completed, it activates the next instance of the Activity class. If there are no next activities, an answer is returned to the calling instance. This action indicates that the production of the mentioned part is concluded. Each instance of the Activity class must own a default resource to control. Each activity has a relationship to the Resource class that will execute it. To each resource a wait queue, represented by the Queue class, was added. This class holds the scheduled parts if the respective resource is busy. The Position class stores the ph",oceanology,46
10.1109/aimsec.2011.6011012,filtered,"2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",IEEE,2011-08-10 00:00:00,ieeexplore,on constructing the practical teaching system of taxation major in newly-built academies,https://ieeexplore.ieee.org/document/6011012/,"Enterprising spirit and practice competence are of great importance in talents training of higher education in 21st century. The prime objective of newly-built academies is to train qualified specialists with enterprising spirit and practice competence, which must be guaranteed by a series of practical teaching steps. However, the reality is that a set of scientific practice teaching system has not come into being, and that teaching staff, base construction, and related auxiliary regulations establishment are in need of improvement. This article explores the construction and implementation of practical teaching system of taxation major in newly-built finance and economics academies, which expects to provide reference for similar universities and colleges‥",finance,47
10.1109/icas.2009.2,filtered,2009 Fifth International Conference on Autonomic and Autonomous Systems,IEEE,2009-04-25 00:00:00,ieeexplore,[title page iii],https://ieeexplore.ieee.org/document/4976566/,The following topics are dealt with: real-time chain-structured synchronous dataflow; memory requirement formal determination; linear singular descriptor differential system; execution-optimized paths; greedy strategy; load trend evaluation; self-managed P2P streaming; context-aware ambient assisted living application; self-adaptive distributed model; autonomic systems; wireless sensor networks; topology control; learning based method; self-recovery method; mobile data sharing; heterogeneous QoS resource manager; component-based self-healing; NGN mobility; interactive user activity; .NET Windows service agent technology; agent based Web browser; resource-definition policies; autonomic computing; autonomic system administration; automatic database performance tuning; knowledge management; adaptive reinforcement learning; VoIP services; autonomic RSS: distributed virtual reality simulations; virtual machines resources allocation; multi-lier distributed systems; network I/O extensibility; virtual keyboards; self-configuring smart homes; legged underwater vehicles; particle filters; reusable semantic components; multi-agent systems; fixed-wing unmanned aerial vehicles; fuzzy inference system; robot swarms; mobile robots; optimization architecture; autonomous unmanned helicopter landing system design; heterogeneous multi-database environments; autonomic software license management system; Web server crashes prediction; laser range finder; video quality; wireless networks; ITU-T G.1030; open IMS core; context-aware data mining methodology; supply chain finance cooperative systems; autonomous pervasive environments; distributed generic stress tool; dynamic adaptive systems; multisensory media effects and user preference.,finance,48
10.1109/cisp.2012.6469706,filtered,2012 5th International Congress on Image and Signal Processing,IEEE,2012-10-18 00:00:00,ieeexplore,a fast and highly accurate carrier acquisition for deep space applications,https://ieeexplore.ieee.org/document/6469706/,"On-board carrier acquisition in deep space telecommunications depends on the residual carrier, to overcome the detection difficulty of low-level received signal. Nowadays, it becomes reality for the receiver itself acquiring Doppler frequency offset. Frequency acquisition based on fast Fourier transform (FFT) has been applied extensively. In this paper, we introduce an algorithm of refined estimation after FFT frequency coarse estimation, which is theoretically analyzed to be maximum likelihood (ML) estimation. Its accuracy can achieve 10-4 when the signal-to-noise ratio (SNR) is 5dB. By the aid of data, the estimator performs well at low SNR ranges. In the design of carrier acquisition, the accurate estimation of frequency offset aids the phase-locked loop (PLL) into locked state. If the acquisition bandwidth of PLL is fixed, accurate estimation value helps to decrease the FFT size. Due to simple implementation and good performance, the algorithm can be applied in burst communications.",space,49
10.1109/robot.2000.844768,filtered,Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065),IEEE,2000-04-28 00:00:00,ieeexplore,application of automatic action planning for several work cells to the german ets-vii space robotics experiments,https://ieeexplore.ieee.org/document/844768/,"Experiences in space robotics show, that the user normally has to cope with a huge amount of data. So, only robot and mission specialists are able to control the robot arm directly in teleoperation mode. By means of an intelligent robot control in cooperation with virtual reality methods, it is possible for non-robot specialists to generate tasks for a robot or an automation component intuitively. Furthermore, the intelligent robot control improves the safety of the entire system. The on-ground robot control and command station for the robot arm ERA onboard the satellite ETS-VII builds on a new resource-based action planning approach to manage robot manipulators and other automation components. In the case of ERA, the action planning system also takes care of the ""real"" robot onboard the satellite and the ""virtual"" robot in the simulation system. By means of the simulation system, the user can plan tasks ahead as well as analyze and visualize different strategies. The paper describes the mechanism of resource-based action planning, its application to different work cells, the practical experiences gained from the implementation for the on-ground robot control and command station for the robot arm ERA developed in the GETEX project as well as the services it provides to support VR-based man machine interfaces.",space,50
10.1109/snpd.2012.99,filtered,"2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",IEEE,2012-08-10 00:00:00,ieeexplore,evaluation of realism of dynamic sound space using a virtual auditory display,https://ieeexplore.ieee.org/document/6299338/,"We can perceive a sound position from binaural signals using mainly head-related transfer functions (HRTFs). Using the theorem presented herein, we can display a sound image to a specific position in virtual auditory space by HRTFs. However, HRTF is defined commonly in a free-field, and a virtual sound image is perceived as a dry source without reflection, reverberation, or ambient noise. Therefore, the virtual sound space might be unnatural. The authors developed a software-based virtual auditory display (VAD) that outputs audio signals for a set of headphones with a three-dimensional position sensor. The VAD software can display a dynamic virtual auditory space that is responsive to a listener's head movement. Subjective evaluations were conducted to clarify the relation between the perceived reality of virtual sound space and ambient sound. Evaluation results of the reality of the virtual sound space displayed by the VAD software are introduced.",space,51
10.1049/cp.2012.0916,filtered,International Conference on Automatic Control and Artificial Intelligence (ACAI 2012),IET,2012-03-05 00:00:00,ieeexplore,a dvge service system for risk assessment of dam-break in barrier lake,https://ieeexplore.ieee.org/document/6492523/,"This paper employs theories and technologies of the distributed virtual reality and geographic information system (GIS) to construct a DVGE (Distributed Virtual Geographic Environment) system. The proposed DVGE System provides geographically distributed users with a shared virtual space and a collaborative platform in order to implement risk assessment work. Using five-layer service system architecture efficiently integrates and shares geographically distributed resources as well as modeling procedures. Meanwhile some key technologies including distributed virtual scene modelling and implementing mechanism of collaborative workflow are discussed. Finally, a DVGE prototype system is implemented to support risk assessment and impact analysis of dam-break in Barrier Lake. The experimental results show that the scheme developed in this paper is efficient and feasible.",space,52
10.1109/bibm52615.2021.9669629,filtered,2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),IEEE,2021-12-12 00:00:00,ieeexplore,a decoding algorithm for non-invasive ssvep-based drone flight control,https://ieeexplore.ieee.org/document/9669629/,"Many advanced researches on natural user interfaces methods based on user-centered design have been using speech, gestures and vision to interact with environment and/or control internet of things (IoT) devices. Brain computer interfaces (BCIs) technology could make this interaction/control more natural, faster, and reliable, and effective. In this paper, we propose a decoding algorithm for controlling a drone in a three-dimensional (3D) space using steady state visually evoked potential (SSVEP)-based BCI modality. SSVEP-based BCI has the great potential for use in virtual reality environment, which enables the user to control the drone using his/her brain activity in an first-person-view mode. Therefore, the user will be in a full control over the flight using BCI system by commanding the drone to take off, land, go forward, stop, and turn right/left. This system yields a super convenient way for normal people with no prior experience to interact with the drone and control a flight mission in a little to no time, over traditional manual control which takes longer time to learn and perfect. in the decoding phase, a various convolutional neural networks (CNN) models were built to accommodate different control criteria such as the generality of the model. This proposed EEG-decode-pipeline has been implemented on an open-source data-set which consists of 8-channel EEG data from 10 subjects performing 12 target SSVEP-based BCI task. A high multi-class BCI classification results were achieved with an accuracy ranging around 80-90% for performing a successful online simulation of the drone control.",space,53
10.1109/iccad51958.2021.9643557,filtered,2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD),IEEE,2021-11-04 00:00:00,ieeexplore,a general hardware and software co-design framework for energy-efficient edge ai,https://ieeexplore.ieee.org/document/9643557/,"A huge number of edge applications including self-driving cars, mobile health, robotics, and augmented reality / virtual reality are enabled by deep neural networks (DNNs). Currently, much of this computation for these applications happens in the cloud, but there are several good reasons to perform the processing on local edge platforms such as smartphones: improved accessibility to different parts of the world, low latency, and data privacy. In this paper, we present a general hardware and software co-design framework for energy-efficient edge AI for both simple classification and structured output prediction tasks (e.g., 3D shapes from images). This framework relies on two key ideas. First, we design a space of DNNs of increasing complexity (coarse to fine) and perform input-specific adaptive inference by selecting a DNN of appropriate complexity depending on the hardness of input examples. Second, we execute the selected DNN on the target edge platform using a resource management policy to save energy. We also provide instantiations of our co-design framework for three qualitatively different problem settings: convolutional neural networks for image classification, graph convolutional networks for predicting 3D shapes from images, and generative adversarial networks on photo-realistic unconditional image generation. Our experiments on real-world benchmarks and mobile platforms show the effectiveness of our co-design framework in achieving significant gain in energy with little to no loss in accuracy of predictions.",space,54
10.1109/iros51168.2021.9636757,filtered,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),IEEE,2021-10-01 00:00:00,ieeexplore,a hierarchical framework for quadruped locomotion based on reinforcement learning,https://ieeexplore.ieee.org/document/9636757/,"Quadruped locomotion is a challenging task for learning-based algorithms. It requires tedious manual tuning and is difficult to deploy in reality due to the reality gap. In this paper, we propose a quadruped robot learning system for agile locomotion which does not require any pre-training and works well in various real-world terrains. We introduce a hierarchical learning framework that uses reinforcement learning as the high-level policy to adjust the low-level trajectory generator for better adaptability to the terrain. We compact the observation and action space of the reinforcement learning to deploy it on a host computer in reality. Besides, we design a trajectory generator guided by robot posture, which can generate adaptive foot trajectory to interact with the environment. Experimental results show that our system can be easily deployed in reality while only trained in simulation, and also has the advantages of fast convergence and good terrain adaptability. The supplementary video demonstration is available at https://vsislab.github.io/hfql/.",space,55
10.1109/svr.2016.26,filtered,2016 XVIII Symposium on Virtual and Augmented Reality (SVR),IEEE,2016-06-24 00:00:00,ieeexplore,a new user-friendly sketch-based modeling method using convolution surfaces,https://ieeexplore.ieee.org/document/7517260/,"3d modeling systems are essential tools for creating3D content for virtual reality systems. They are powerfuland sophisticated computer applications with a steep learningcurve. When considering simple shape prototyping it is moreappropriate to use a software with a simpler and intuitiveuser interface. This paper proposes a new sketch-based modelingapproach relying on convolution surfaces that enablesthe user to specify 3d shapes of arbitrary topology andresolution from 2d hand-drawn sketches that are embeddedand manipulated in 3d space. To make this approach feasiblewe propose two innovations: the first one is an interactionmechanism to create, manipulate and assemble independent2d sketches embedded in 3d space relying only on simplerotations, translations and a ray casting step; the second oneis a new approach to fit the level set surfaces generated bythe convolution of skeleton primitives to the silhouette curvesspecified by the user. The proposed fitting combines a methodthat automatically computes weights for the convolutionoperation with a Stolte's blending function that controls theinfluence of each implicit functional components to the finalshape. We show via experiments and preliminary user teststhat our method permits the user to easily express his 3dconcepts by sketching and assembling 2d simple drawings intridimensional space.",space,56
10.1109/aeeca52519.2021.9574150,filtered,2021 IEEE International Conference on Advances in Electrical Engineering and Computer Applications (AEECA),IEEE,2021-08-28 00:00:00,ieeexplore,application of virtual panorama technology based on multi-source information fusion in high quality development of the yellow river,https://ieeexplore.ieee.org/document/9574150/,"Virtual panorama technology based on multi-source information fusion is a virtual reality technology based on image technology to generate realistic graphics. Panorama production software is used to generate panoramic images with a strong sense of scene and perspective effect that can be fully displayed at 720 degrees. Various kinds of voice, video, image and other hot spots are added in the virtual panorama image, and the step and front-end customized display are freely added, and multiple types of artificial intelligence graphic algorithms are applied to realize the virtual panorama platform of multi-source information fusion. The technology has been widely used in some businesses of the Yellow River, with strong practicability, large application and development space, and obvious advantages in investment benefit ratio.",space,57
10.1109/mlsp52302.2021.9596414,filtered,2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MLSP),IEEE,2021-10-28 00:00:00,ieeexplore,caesynth: real-time timbre interpolation and pitch control with conditional autoencoders,https://ieeexplore.ieee.org/document/9596414/,"In this paper, we present a novel audio synthesizer, CAESynth, based on a conditional autoencoder. CAESynth synthesizes timbre in real-time by interpolating the reference sounds in their shared latent feature space, while controlling a pitch independently. We show that training a conditional autoen-coder based on accuracy in timbre classification together with adversarial regularization of pitch content allows timbre distribution in latent space to be more effective and stable for timbre interpolation and pitch conditioning. The proposed method is applicable not only to creation of musical cues but also to exploration of audio affordance in mixed reality based on novel timbre mixtures with environmental sounds. We demonstrate by experiments that CAESynth achieves smooth and high-fidelity audio synthesis in real-time through timbre interpolation and independent yet accurate pitch control for musical cues as well as for audio affordance with environmental sound. A Python implementation along with some generated samples are shared online.",space,58
10.1109/fuzzy.1995.410037,filtered,Proceedings of 1995 IEEE International Conference on Fuzzy Systems.,IEEE,1995-03-24 00:00:00,ieeexplore,"computational intelligence and multimedia in education: nasa/rms arm, control in diabetes, intelligent workstation, and management decision aide",https://ieeexplore.ieee.org/document/410037/,"The use of computational intelligence and multimedia in the formal classroom and in continuing education is a powerful, many faceted tool. We summarize the system development principles important in educating engineers and scientists in the practical application of these tools. Intelligent systems, human interfaces, neural networks, genetic algorithms, evolutionary systems and virtual reality form a toolbox of opportunities. Examples are discussed in the light of turning fuzzy computing concepts into applications. Simulation skills needed for success in industry are emphasized. A prime example of such an educational tool is the system developed at the NASA Goddard Space Flight Center for full scale simulation in actual hardware of the Remote Manipulator System (RMS) arm used on the Shuttle. Other examples include the new Teledoc software expert system at the Diabetes Research Institute (DRI). The computer ""talks"" to callers to elicit information about glucose levels, stress, exercise, diet and various symptoms before adjusting insulin dosages, alerting patients to potential problems add their solutions-including calling the doctor. An intelligent workstation designed to filter information for busy executives, and a decision aide for managers are also discussed.&lt;<ETX>&gt;</ETX>",space,59
10.1109/icccnt51525.2021.9579976,filtered,2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT),IEEE,2021-07-08 00:00:00,ieeexplore,"data processing in iot, sensor to cloud: survey",https://ieeexplore.ieee.org/document/9579976/,"IoT is connecting Things over the Internet and the realization of the environment through smart things to create a responsive space. Many surveys predicted the growth of IoT devices is going to be around 50 billion and an average of 7 devices per person. IoT has shown promising future with its applications like smart city, connected factories, buildings, roadways, smart health and many more. To make the promise a reality IoT has to overcome many hurdles like scalability, connectivity, architectural, big data, analysis, security, and privacy. In this literature survey, an attempt has been made to identify current challenges faced by IoT implementation and possible solutions, future opportunities, and research openings. Further, the processing of sensed data at IoT device, edge/fog layer, and the cloud is discussed in detail. Keywords- IoT, IoT architecture, Machine learning, Deep",space,60
10.1109/icsens.2009.5398535,filtered,"SENSORS, 2009 IEEE",IEEE,2009-10-28 00:00:00,ieeexplore,diagnostic models for sensor measurements in rocket engine tests,https://ieeexplore.ieee.org/document/5398535/,"This paper presents our ongoing work in the area of using virtual reality (VR) environments for the Integrated Systems Health Management (ISHM) of rocket engine test stands. Specifically, this paper focuses on the development of an intelligent valve model that integrates into the control center at NASA Stennis Space Center. The intelligent valve model integrates diagnostic algorithms and 3D visualizations in order to diagnose and predict failures of a large linear actuator valve (LLAV). The diagnostic algorithm uses auto-associative neural networks to predict expected values of sensor data based on the current readings. The predicted values are compared with the actual values and drift is detected in order to predict failures before they occur. The data is then visualized in a VR environment using proven methods of graphical, measurement, and health visualization. The data is also integrated into the control software using an ActiveX plug-in.",space,61
10.1109/aivr.2018.00046,filtered,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),IEEE,2018-12-12 00:00:00,ieeexplore,foodchangelens: cnn-based food transformation on hololens,https://ieeexplore.ieee.org/document/8613665/,"In this demonstration, we implemented food category transformation in mixed reality using both image generation and HoloLens. Our system overlays transformed food images to food objects in the AR space, so that it is possible to convert in consideration of real shape. This system has the potential to make meals more enjoyable. In this work, we use the Conditional CycleGAN trained with a large-scale food image data collected from the Twitter Stream for food category transformation which can transform among ten kinds of foods mutually keeping the shape of a given food. We show the virtual meal experience which is food category transformation among ten kinds of typical Japanese foods: ramen noodle, curry rice, fried rice, beef rice bowl, chilled noodle, spaghetti with meat source, white rice, eel bowl, and fried noodle. Note that additional results including demo videos can be see at https://negi111111.github.io/FoodChangeLensProjectHP/.",space,62
10.1109/icaml54311.2021.00056,filtered,2021 3rd International Conference on Applied Machine Learning (ICAML),IEEE,2021-07-25 00:00:00,ieeexplore,gymnasium simulation design and implementation based on 3d virtual building,https://ieeexplore.ieee.org/document/9712032/,"Based on the three-dimensional virtual building-simulation design and implementation method of the gymnasium, when the building space is subjected to virtual reality and simulation design, based on the constructed building space coordinate system and scale, the position and parameters of the building are used to construct a mathematical model of the building space component., The mathematical model of each component is integrated to build a mathematical model of the overall building space. OpenGL virtual reality technology is used to expand the target building based on the mathematical model of the target building space. The target building is given materials and texture characteristics to obtain the ideal 3D virtual view of the building space. The 3D virtual view of the building space is 3D rendered and displayed. The vivid 3D virtual renderings of the building space are animated using animation design technology. The experimental results show that the proposed method has good point-line rendering and overall rendering effect, can obtain more realistic 3D virtual building based gym simulation design results, and has high interactivity and practicability.",space,63
10.23919/ituk50268.2020.9303205,filtered,2020 ITU Kaleidoscope: Industry-Driven Digital Transformation (ITU K),IEEE,2020-12-11 00:00:00,ieeexplore,immersive technologies for development: an analysis of agriculture,https://ieeexplore.ieee.org/document/9303205/,"Agricultural development is key to any economic development. Immersive technology plays a catalytic role and offers smart and sustainable choices to farmers who want to improve on agricultural productivity, and to agricultural training institutes that seek to use modern technology to advance pedagogy and reduce fatalities and operating costs in the learning space, among others. Currently, there are limited descriptive literature reviews in the area of immersive technology in agriculture, hereafter referred to as AVR-Agric. This paper presents a systematic literature review (SLR), which offers a structured, methodical, and rigorous approach to the understanding of the trend of research in AVR-Agric, and the least and most researched issues. This study explores and examines the current trends in the immersive technology-based agriculture areas, and provides a credible intellectual guide for future research direction. The SLR was limited to existing applications and peer-reviewed conference and journal articles published from 2006 to 2020. The results showed that virtual reality was implemented in 41% of the papers reviewed, augmented reality was found in 53%, while only 6% considered mixed-reality applications. The study also showed that developments that incorporate IoT, blockchain, and machine-learning technologies are still at their stage of exploration and advancement.",space,64
10.1109/i3da48870.2021.9610915,filtered,2021 Immersive and 3D Audio: from Architecture to Automotive (I3DA),IEEE,2021-09-10 00:00:00,ieeexplore,machine learning-based room classification for selecting binaural room impulse responses in augmented reality applications,https://ieeexplore.ieee.org/document/9610915/,"A key attribute of augmented reality (AR) applications is the matching reverberation of virtual sounds to the room acoustics of the real environment. However, especially in real-time scenarios where the properties of rapidly changing surroundings are unknown, creating a persistently coherent sound field synthesis within a real space is a challenging problem. While AR devices and their sensors can usually provide depth information within the field of view of the user, retrieving a complete geometric model requires significant time and user activity. Prior acoustic measurements or scans of the deployment area also severely limit many use cases, especially in the consumer sector. In this paper, we propose an automatic system that provides a fast selection of room categories and their corresponding binaural reverberation using only monoscopic images as input information. The proposed system combines existing approaches of machine learning (ML) based room classification and parametric synthesis of binaural room impulse responses (BRIRs) to provide room reverberation for arbitrary indoor environments. As a proof of concept, we present a demonstrator developed in Cycling’74s Max linked to a python-based ML model. For the ML model, we use the convolutional neural network (CNN) GoogLeNet architecture trained on a subset of the Places365 data set. This subset contains 20 custom indoor room categories which are composed of the original categories that share similar acoustic properties. The demonstrator captures images and automatically selects binaural reverberation based on the predictions of the ML classifier. Monophonic stimuli are reverberated and presented using dynamic headphone-based binauralization.",space,65
10.1109/wcica.2000.862545,filtered,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),IEEE,2000-07-02 00:00:00,ieeexplore,multipurpose virtual-reality-based motion simulator,https://ieeexplore.ieee.org/document/862545/,"Public security has become an important issue everywhere. Especially, the safe manipulation and control of various machines and vehicles has gained special attention such that the authorities keep emphasizing the strict training and censoring of human operators. Currently, such training and censoring process usually relies on the actual machines, equipment, or vehicles in the real sites. This not only has high demands in space, time and cost, but also causes another public security problem. In this connection, the world-wide trend is to tackle the above dilemma by using virtual reality (VR). However, the current researches or products on VR are more matured in the software display part of VR. How to combine 3D VR display with motion platform to achieve the aforementioned training and censoring purposes is an important research issue. This paper focuses on this research issue, and the goal is to develop a multipurpose virtual-reality-based motion simulation system to meet the requirements of public security in training and censoring of human operators.",space,66
10.1109/indin.2006.275808,filtered,2006 4th IEEE International Conference on Industrial Informatics,IEEE,2006-08-18 00:00:00,ieeexplore,"ontology for cognitics, closed-loop agility constraint, and case study - a mobile robot with industrial-grade components",https://ieeexplore.ieee.org/document/4053562/,"The paper refers to intelligent industrial automation. The objective is to present key elements and methods for best practice, as well as some results obtained. The first part presents an ontology for automated cognition (cognitics), where, based on information and time, the main cognitive concepts, including those of complexity, knowledge, expertise, learning, intelligence abstraction, and concretization are rigorously defined, along with corresponding metrics and specific units. Among important conclusions at this point are the fact that reality is much too complex to be approached better than through much simplified models, in very restricted contexts. Another conclusion is the necessity to be focused on goal. Extensions are made here for group behavior. The second part briefly presents a basic law governing the choice of overall control architecture: achievable performance level of control system in terms of agility, relative to process dynamics, dictates the type of approaches which is suitable, in a spectrum which ranges from simple threshold-based switching, to classical closed-loop calculus (PID, state space multivariable systems, etc.), up to ""impossible"" cases where additional controllers must be considered, leading to cascaded, hierarchical control structures. For complex cases such as latter ones, new tools and methodologies must be designed, as is typical in O<sup>3</sup>NEIDA initiative, at least for software components. Finally, a large part of the paper presents a case study, a mobile robot, i.e. an embedded autonomous system with distributed, networked control, featuring industry-grade components, designed with the main goal of robust functionality. The case illustrates several of the concepts introduced earlier in the paper.",space,67
10.1109/dsr.2011.6026835,filtered,2011 Defense Science Research Conference and Expo (DSR),IEEE,2011-08-05 00:00:00,ieeexplore,portality - the portal between virtuality and reality,https://ieeexplore.ieee.org/document/6026835/,"This paper proposes an innovative concept of community, called Mirror Reality, which is formed by the seamless interaction between users of physical space and virtual world. The Mirror Reality is relying on a portal to achieve the data translation and communication between virtual and reality worlds. This study employs the Ontology technology to analyze unified representation of objects and events in both virtual world and physical space. Furthermore, the research issues of creating a Mirror Reality are also identified. The portal that is implemented from the analysis is called Portality to emphasize its role between virtuality and reality. In other words, Portality takes care all of the data conversion and processing to permit the interoperability between two totally different worlds. Finally, the prototype of Mirror Reality for the university campus with its Portality is illustrated at the end.",space,68
10.1109/robio49542.2019.8961870,filtered,2019 IEEE International Conference on Robotics and Biomimetics (ROBIO),IEEE,2019-12-08 00:00:00,ieeexplore,probabilistic inferences on quadruped robots: an experimental comparison,https://ieeexplore.ieee.org/document/8961870/,"Due to the reality gap, computer software cannot fully model the physical robot in its environment, with noise, ground friction, and energy consumption. Consequently, a limited number of researchers work on applying machine learning in real-world robots. In this paper, we use two intelligent black-box optimization algorithms, Bayesian Optimization (BO) and Covariance Matrix Adaptation Evolution Strategy (CMA-ES), to solve a quadruped robot gait's parametric search problem in 10 dimensions, and compare these two methods to find which one is more suitable for legged robots' controller parameters tuning. Our results show that both methods can find an optimal solution in 130 iterations. BO converges faster than CMA-ES within its constrained range, while CMA-ES finds the optimum in the continuous space. Compared with the specific controller parameters of two methods, we also find that for quadruped robot's oscillators, the angular amplitude is the most important parameter. Thus, it is very beneficial for the quick parametric search of legged robots' controllers and avoids time-consuming manual tuning.",space,69
10.1109/icassp40776.2020.9053215,filtered,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",IEEE,2020-05-08 00:00:00,ieeexplore,real-time binaural speech separation with preserved spatial cues,https://ieeexplore.ieee.org/document/9053215/,"Deep learning speech separation algorithms have achieved great success in improving the quality and intelligibility of separated speech from mixed audio. Most previous methods focused on generating a single-channel output for each of the target speakers, hence discarding the spatial cues needed for the localization of sound sources in space. However, preserving the spatial information is important in many applications that aim to accurately render the acoustic scene such as in hearing aids and augmented reality (AR). Here, we propose a speech separation algorithm that preserves the interaural cues of separated sound sources and can be implemented with low latency and high fidelity, therefore enabling a real-time modification of the acoustic scene. Based on the time-domain audio separation network (TasNet), a single-channel time-domain speech separation system that can be implemented in real-time, we propose a multi-input-multi-output (MIMO) end-to-end extension of TasNet that takes binaural mixed audio as input and simultaneously separates target speakers in both channels. Experimental results show that the proposed end-to-end MIMO system is able to significantly improve the separation performance and keep the perceived location of the modified sources intact in various acoustic scenes.",space,70
10.1109/icuas.2016.7502588,filtered,2016 International Conference on Unmanned Aircraft Systems (ICUAS),IEEE,2016-06-10 00:00:00,ieeexplore,real-time unmanned aerial vehicle 3d environment exploration in a mixed reality environment,https://ieeexplore.ieee.org/document/7502588/,"This paper presents a novel human robot interaction system that can be used for real-time 3D environment exploration with an unmanned aerial vehicle (UAV). The method creates a mixed reality environment, in which a user can interactively control a UAV and visualize the exploration data in real-time. The method uses a combination of affordable sensors, and transforms the control and viewing space from the UAV to the controller's perspective. Different hardware and software configurations are studied so that the system can be adjusted to meet different needs and environments. A prototype system is presented and test results are discussed.",space,71
10.1109/mmrp.2019.00013,filtered,2019 International Workshop on Multilayer Music Representation and Processing (MMRP),IEEE,2019-01-24 00:00:00,ieeexplore,three-dimensional mapping of high-level music features for music browsing,https://ieeexplore.ieee.org/document/8665368/,"The increased availability of musical content comes with the need of novel paradigms for recommendation, browsing and retrieval from large music libraries. Most music players and streaming services propose a paradigm based on content listing of meta-data information, which provides little insight on the music content. In services with huge catalogs of songs, a more informative paradigm is needed. In this work we propose a framework for music browsing based on the navigation into a three-dimensional (3-D) space, where musical items are placed as a 3-D mapping of their high-level semantic descriptors. We conducted a survey to guide the design of the framework and the implementation choices. We rely on state-of-the-art techniques from Music Information Retrieval to automatically extract the high-level descriptors from a low-level representation of the musical signal. The framework is validated by means of a subjective evaluation from 33 users, who give positive feedbacks and highlight promising future developments especially in virtual reality field.",space,72
10.23919/mipro.2019.8756928,filtered,"2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",IEEE,2019-05-24 00:00:00,ieeexplore,utilizing apple’s arkit 2.0 for augmented reality application development,https://ieeexplore.ieee.org/document/8756928/,"When it comes to practical augmented reality applications, mobile platform tools are the most deserving. Thanks to the nature of mobile devices and their everyday usage, the ideal basis for this kind of content has inadvertently formed itself. Consequently, within the iOS development environment, Apple's Xcode program enables application development using the ARKit library which delivers a host of benefits. Amongst the plethora of advantages, this paper focuses on utilizing features such as the ability to measure distances between two points in space, horizontal and vertical plane detection, the ability to detect three-dimensional objects and utilize them as triggers, and the consolidated implementation of ARKit and MapKit libraries in conjunction with the Google Places API intended for displaying superimposed computer-generated content on iOS 11 and later iterations of Apple's mobile operating system.",space,73
10.1109/tvcg.2019.2946769,filtered,IEEE Transactions on Visualization and Computer Graphics,IEEE,2021-03-01 00:00:00,ieeexplore,heter-sim: heterogeneous multi-agent systems simulation by interactive data-driven optimization,https://ieeexplore.ieee.org/document/8865441/,"Interactive multi-agent simulation algorithms are used to compute the trajectories and behaviors of different entities in virtual reality scenarios. However, current methods involve considerable parameter tweaking to generate plausible behaviors. We introduce a novel approach (Heter-Sim) that combines physics-based simulation methods with data-driven techniques using an optimization-based formulation. Our approach is general and can simulate heterogeneous agents corresponding to human crowds, traffic, vehicles, or combinations of different agents with varying dynamics. We estimate motion states from real-world datasets that include information about position, velocity, and control direction. Our optimization algorithm considers several constraints, including velocity continuity, collision avoidance, attraction, direction control. Other constraints are implemented by introducing a novel energy function to control the motions of heterogeneous agents. To accelerate the computations, we reduce the search space for both collision avoidance and optimal solution computation. Heter-Sim can simulate tens or hundreds of agents at interactive rates and we compare its accuracy with real-world datasets and prior algorithms. We also perform user studies that evaluate the plausible behaviors generated by our algorithm and a user study that evaluates the plausibility of our algorithm via VR.",space,74
10.1109/lra.2020.3013848,filtered,IEEE Robotics and Automation Letters,IEEE,2020-10-01 00:00:00,ieeexplore,sim2real predictivity: does evaluation in simulation predict real-world performance?,https://ieeexplore.ieee.org/document/9158349/,"Does progress in simulation translate to progress on robots? If one method outperforms another in simulation, how likely is that trend to hold in reality on a robot? We examine this question for embodied PointGoal navigation - developing engineering tools and a research paradigm for evaluating a simulator by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy), a library for seamless execution of identical code on simulated agents and robots - transferring simulation-trained agents to a LoCoBot platform with a one-line code change. Second, we investigate the sim2real predictivity of Habitat-Sim M. Savva et al., for PointGoal navigation. We 3D-scan a physical lab space to create a virtualized replica, and run parallel tests of 9 different models in reality and simulation. We present a new metric called Sim-vs-Real Correlation Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as used for the CVPR19 challenge is low (0.18 for the success metric), suggesting that performance differences in this simulator-based challenge do not persist after physical deployment. This gap is largely due to AI agents learning to exploit simulator imperfections - abusing collision dynamics to `slide' along walls, leading to shortcuts through otherwise non-navigable space. Naturally, such exploits do not work in the real world. Our experiments show that it is possible to tune simulation parameters to improve sim2real predictivity (e.g. improving SRCC<sub>Succ</sub> from 0.18 to 0.844) - increasing confidence that in-simulation comparisons will translate to deployed systems in reality.",space,75
http://arxiv.org/abs/2112.05393v1,filtered,arxiv,arxiv,2021-12-10 00:00:00,arxiv,a self-supervised mixed-curvature graph neural network,http://arxiv.org/abs/2112.05393v1,"Graph representation learning received increasing attentions in recent years.
Most of existing methods ignore the complexity of the graph structures and
restrict graphs in a single constant-curvature representation space, which is
only suitable to particular kinds of graph structure indeed. Additionally,
these methods follow the supervised or semi-supervised learning paradigm, and
thereby notably limit their deployment on the unlabeled graphs in real
applications. To address these aforementioned limitations, we take the first
attempt to study the self-supervised graph representation learning in the
mixed-curvature spaces. In this paper, we present a novel Self-supervised
Mixed-curvature Graph Neural Network (SelfMGNN). Instead of working on one
single constant-curvature space, we construct a mixed-curvature space via the
Cartesian product of multiple Riemannian component spaces and design
hierarchical attention mechanisms for learning and fusing the representations
across these component spaces. To enable the self-supervisd learning, we
propose a novel dual contrastive approach. The mixed-curvature Riemannian space
actually provides multiple Riemannian views for the contrastive learning. We
introduce a Riemannian projector to reveal these views, and utilize a
well-designed Riemannian discriminator for the single-view and cross-view
contrastive learning within and across the Riemannian views. Finally, extensive
experiments show that SelfMGNN captures the complicated graph structures in
reality and outperforms state-of-the-art baselines.",space,76
http://arxiv.org/abs/1912.06321v2,filtered,arxiv,arxiv,2019-12-13 00:00:00,arxiv,"sim2real predictivity: does evaluation in simulation predict real-world
  performance?",http://arxiv.org/abs/1912.06321v2,"Does progress in simulation translate to progress on robots? If one method
outperforms another in simulation, how likely is that trend to hold in reality
on a robot? We examine this question for embodied PointGoal navigation,
developing engineering tools and a research paradigm for evaluating a simulator
by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy),
a library for seamless execution of identical code on simulated agents and
robots, transferring simulation-trained agents to a LoCoBot platform with a
one-line code change. Second, we investigate the sim2real predictivity of
Habitat-Sim for PointGoal navigation. We 3D-scan a physical lab space to create
a virtualized replica, and run parallel tests of 9 different models in reality
and simulation. We present a new metric called Sim-vs-Real Correlation
Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as
used for the CVPR19 challenge is low (0.18 for the success metric), suggesting
that performance differences in this simulator-based challenge do not persist
after physical deployment. This gap is largely due to AI agents learning to
exploit simulator imperfections, abusing collision dynamics to 'slide' along
walls, leading to shortcuts through otherwise non-navigable space. Naturally,
such exploits do not work in the real world. Our experiments show that it is
possible to tune simulation parameters to improve sim2real predictivity (e.g.
improving $SRCC_{Succ}$ from 0.18 to 0.844), increasing confidence that
in-simulation comparisons will translate to deployed systems in reality.",space,77
http://arxiv.org/abs/1909.06769v1,filtered,arxiv,arxiv,2019-09-15 00:00:00,arxiv,vild: variational imitation learning with diverse-quality demonstrations,http://arxiv.org/abs/1909.06769v1,"The goal of imitation learning (IL) is to learn a good policy from
high-quality demonstrations. However, the quality of demonstrations in reality
can be diverse, since it is easier and cheaper to collect demonstrations from a
mix of experts and amateurs. IL in such situations can be challenging,
especially when the level of demonstrators' expertise is unknown. We propose a
new IL method called \underline{v}ariational \underline{i}mitation
\underline{l}earning with \underline{d}iverse-quality demonstrations (VILD),
where we explicitly model the level of demonstrators' expertise with a
probabilistic graphical model and estimate it along with a reward function. We
show that a naive approach to estimation is not suitable to large state and
action spaces, and fix its issues by using a variational approach which can be
easily implemented using existing reinforcement learning methods. Experiments
on continuous-control benchmarks demonstrate that VILD outperforms
state-of-the-art methods. Our work enables scalable and data-efficient IL under
more realistic settings than before.",space,78
http://arxiv.org/abs/1804.10332v2,filtered,arxiv,arxiv,2018-04-27 00:00:00,arxiv,sim-to-real: learning agile locomotion for quadruped robots,http://arxiv.org/abs/1804.10332v2,"Designing agile locomotion for quadruped robots often requires extensive
expertise and tedious manual tuning. In this paper, we present a system to
automate this process by leveraging deep reinforcement learning techniques. Our
system can learn quadruped locomotion from scratch using simple reward signals.
In addition, users can provide an open loop reference to guide the learning
process when more control over the learned gait is needed. The control policies
are learned in a physics simulator and then deployed on real robots. In
robotics, policies trained in simulation often do not transfer to the real
world. We narrow this reality gap by improving the physics simulator and
learning robust policies. We improve the simulation using system
identification, developing an accurate actuator model and simulating latency.
We learn robust controllers by randomizing the physical environments, adding
perturbations and designing a compact observation space. We evaluate our system
on two agile locomotion gaits: trotting and galloping. After learning in
simulation, a quadruped robot can successfully perform both gaits in the real
world.",space,79
http://arxiv.org/abs/1801.05643v1,filtered,arxiv,arxiv,2018-01-17 00:00:00,arxiv,"the case for automatic database administration using deep reinforcement
  learning",http://arxiv.org/abs/1801.05643v1,"Like any large software system, a full-fledged DBMS offers an overwhelming
amount of configuration knobs. These range from static initialisation
parameters like buffer sizes, degree of concurrency, or level of replication to
complex runtime decisions like creating a secondary index on a particular
column or reorganising the physical layout of the store. To simplify the
configuration, industry grade DBMSs are usually shipped with various advisory
tools, that provide recommendations for given workloads and machines. However,
reality shows that the actual configuration, tuning, and maintenance is usually
still done by a human administrator, relying on intuition and experience.
Recent work on deep reinforcement learning has shown very promising results in
solving problems, that require such a sense of intuition. For instance, it has
been applied very successfully in learning how to play complicated games with
enormous search spaces. Motivated by these achievements, in this work we
explore how deep reinforcement learning can be used to administer a DBMS.
First, we will describe how deep reinforcement learning can be used to
automatically tune an arbitrary software system like a DBMS by defining a
problem environment. Second, we showcase our concept of NoDBA at the concrete
example of index selection and evaluate how well it recommends indexes for
given workloads.",space,80
10.1016/j.jbusres.2022.02.039,filtered,Journal of Business Research,scopus,2022-05-01,sciencedirect,multi-target cnn-lstm regressor for predicting urban distribution of short-term food delivery demand,https://api.elsevier.com/content/abstract/scopus_id/85124904640,"The food delivery market has increased rapidly in the last few years, becoming a well-established reality in the business world and a common feature of urban life. Food delivery platforms provide the end-to-end services that connect restaurants with consumers, including the delivery service to those people ordering food through an online portal. A key component of these platforms is logistics, specifically the logistics of drivers. Ideally, the number of drivers operating in an urban area should be just the right number to serve the demand in that area. Since the demand is extremely dynamic in space and time, the spatial–temporal distribution of drivers remains a challenging problem, partially solved by means of variable incentives in different city areas at different times. In this context, a precise demand prediction would avoid a local lack of drivers in some areas, and an inefficient concentration of drivers in some other areas. For this reason, we propose a deep neural network-based methodology to forecast short-term food delivery demand distribution over urban areas. The study, carried out on a real-world dataset from a food delivery company, focuses on hourly demands and frequent prediction updates. The sequential modeling approach, designed to catch rapid changes and sudden variations beyond the general demand trend, is based on a multi-target CNN-LSTM regressor trained on location-specific time series. The methodology uses a single model for all service areas simultaneously, and a single one-step volume inference for every area at each time update. The results disclose a better performance over baselines (historical estimates for the same time-area) and more traditional statistical approaches (moving averages and univariate time-series forecasting), demonstrating a promising implementation potential within an online delivery platform framework.",space,81
10.1016/j.vrih.2022.01.004,filtered,Virtual Reality and Intelligent Hardware,scopus,2022-02-01,sciencedirect,virtual-reality-based digital twin of office spaces with social distance measurement feature,https://api.elsevier.com/content/abstract/scopus_id/85124517698,"Background
                  Social distancing is an effective way to reduce the spread of the SARS-CoV-2 virus. Many students and researchers have already attempted to use computer vision technology to automatically detect human beings in the field of view of a camera and help enforce social distancing. However, because of the present lockdown measures in several countries, the validation of computer vision systems using large-scale datasets is a challenge.
               
                  Methods
                  In this paper, a new method is proposed for generating customized datasets and validating deep-learning-based computer vision models using virtual reality (VR) technology. Using VR, we modeled a digital twin (DT) of an existing office space and used it to create a dataset of individuals in different postures, dresses, and locations. To test the proposed solution, we implemented a convolutional neural network (CNN) model for detecting people in a limited-sized dataset of real humans and a simulated dataset of humanoid figures.
               
                  Results
                  We detected the number of persons in both the real and synthetic datasets with more than 90% accuracy, and the actual and measured distances were significantly correlated (r=0.99). Finally, we used intermittent-layer- and heatmap-based data visualization techniques to explain the failure modes of a CNN.
               
                  Conclusions
                  A new application of DTs is proposed to enhance workplace safety by measuring the social distance between individuals. The use of our proposed pipeline along with a DT of the shared space for visualizing both environmental and human behavior aspects preserves the privacy of individuals and improves the latency of such monitoring systems because only the extracted information is streamed.",space,82
10.1016/j.evopsy.2021.03.006,filtered,Evolution Psychiatrique,scopus,2021-05-01,sciencedirect,"from digital identity to connected personality, from augmented diagnostician to virtual caregiver: what are the challenges for the psychology and the psychiatry of the future?",https://api.elsevier.com/content/abstract/scopus_id/85104125089,"Objectifs
                  Qui sommes-nous devenus, citoyens, patients, praticiens ? En quoi les moyens de communications et l’informatisation de notre société modifient-ils, intègrent-ils nos identités ? L’intelligence artificielle comprendrait-elle bientôt plus justement l’être humain dont elle s’émanciperait ?
               
                  Matériel et méthodes
                  Cheminons à partir de la lexicologie pour tenter de saisir, via le point de vue de la philosophie, l’identité contemporaine vers la notion d’« identité numérique » dont les incidents psychologiques normaux ou pathologiques entraînent ce que nous définissons « la personnalité numérique ». Puis, posant les bases d’une psychologie de l’identité contemporaine, nous envisageons comment « la psychologie » et « la psychiatrie » actuelles considèrent « la personnalité » du patient et, en retour, comment elles se définissent du point du vue du « praticien en ligne » ou du « chercheur connecté ».
               
                  Résultats
                  En échange de son utilisation « gratuite », l’action de l’internaute sur le Web 2.0 produit du contenu et alimente des bases de données, déclaratives ou non. En perte d’intimité au fur et à mesure que « ses » données ne lui appartiennent plus, l’identité du citoyen se décompose en fonctions des supports digitaux : site de rencontre amical, plateforme de liens amoureux, blog concernant un loisir ou un voyage, etc. Par le même mouvement, l’identité numérique se compose en autre-soi possédant une part d’intelligence artificielle pourvoyeuse de capacité d’existence propre. Plutôt que deux entités parallèlement différentiables, réelle ou augmentée, naît une identité hybride « réalistiquo-virtuelle ». Quelles conséquences normales ou pathologiques chez l’être humain ? Les tendances sociétales post-modernes issues du digital ou y trouvant expression peuvent entraîner, chez un individu donné, une exacerbation des traits de personnalité préalablement existants, voire des symptômes. Parallèlement, il arrive que les moyens de communication moderne deviennent une aide pour expérimenter le monde, majorer l’estime de soi, rêver favorablement ses phantasmes, se confier plus facilement à des « inconnu(e)s », etc. Mais dans tous les cas, chez le sujet souffrant, ou ne souffrant pas, préalablement à sa surexposition, de maladie neuropsychiatrique ou de trouble psychopathologique, il s’avère aujourd’hui scientifiquement documenté que la confrontation numérique accrue induit des atteintes neuropsychiques massives (affaiblissement de la mémoire de travail, des capacités d’attention et de concentration, des aptitudes à construire des opérations cognitives élaborées, etc.). Sur le plan psychopathologique, plutôt que la terminologie de « trouble de l’identité » ou une notion de « co-identités », le terme d’« identité trouble » nous paraît le mieux rendre compte de cette mutation du « moi » où la frontière entre réalité et virtualités s’amenuise : la dissociation prévaut. L’homme post-moderne et ses objets connectés ne font plus qu’un, mais cet « uniforme » apparaît constitué d’un patchwork de confettis identificatoires plus ou moins accolés, sans réelle harmonisation d’ensemble. La personnalité commune se marque d’hyperexpressivité et d’hyperémotivité, au détriment de la possibilité de contrôle des affects et du développement des capacités d’introspection. Contre le risque du vide, tend à se développer une contra-phobie par l’ordiphone, par l’objet lui-même, par la possibilité de contacter en permanence ses proches si nécessaire, et en retour rester toujours « disponible », ce qui alimente une forme d’égocentrisme addictogène. Résulte de ses évolutions, globalement dans la société, un affaiblissement des capacités langagières, et ainsi de réflexion, y compris pour l’espace clinique et scientifique.
               
                  Discussion
                  Pour les domaines de la psychologie et de la psychiatrie, s’associent actuellement deux évolutions : une velléité d’« objectivité-scientificité » et une numérisation de la relation patient–soignant. Du côté de la « science », la médecine objective « factuelle » s’intéresse de plus en plus à la pathologie aux dépens du sujet en souffrance, confondant signe et symptôme, glissant jusqu’à un niveau moléculaire, très en-deçà du patient, vers une psychiatrie ou une psychologie « post-clinique ». Qu’on veuille la promouvoir ou l’anéantir, du côté du clinicien ou du chercheur, la « subjectivité » est devenue un signifiant à la mode pour le domaine de la santé psychique. Ce retour actuel du « subjectif » prospère sur une sorte de peur de la subjectivité depuis la fin de la seconde guerre mondiale qui avait entraîné la nosographie américaine vers les « objectifs » des DSM (Manuel Diagnostique et Statistique des Troubles Psychiques publié par l’American Psychiatric Association depuis 1952). Mais plutôt qu’une connaissance validable, et/ou invariable concernant tel ou tel trouble psychique, le changement, la relativité des entités nosographiques d’une version à l’autre du manuel traduit, en miroir, la subjectivité d’une époque, ce que nous appelons « subjectivité sociétale ». Autant qu’elle témoigne de notre temps, la révolution bio-numérique s’imposera probablement dans une future édition de la nosographie : la validité diagnostique devrait se majorer par la définition précise de marqueurs biologiques et/ou neuroradiologiques, si ceux-ci participent à construire une théorie étiopathogénique des phénomènes psychiques observés. Cette orientation reste toutefois balbutiante : outre l’infime nombre de biomarqueurs identifiés, et surtout utilisables en pratique quotidienne, leurs liens de causalité ou de conséquentialité avec les symptômes ou le processus morbide restent le plus souvent incertains autant qu’ils sont fort divers et interreliés. Le chercheur en neurosciences vise à mesurer et analyser une multitude de données, intégrant en particulier les mimiques et les émotions authentifiables par caméra thermique, les mouvements des segments des corps et dynamiques des regards enregistrables par des capteurs, la standardisation des voix et des discours pour analyse par logiciel informatique de la prosodie, des signifiants employés, de la syntaxe… le tout s’intégrant dans un phénotypage digital de la souffrance. Pourra-t-on bientôt parler, en remplacement du psychologue ou du psychiatre, de « diagnosticien augmenté » ?
               
                  Conclusion
                  Apparaît-il actuellement hasardeux de faire confiance à un thérapeute entièrement virtuel… expérience déjà lancée il y a plus de 50 ans ! L’être humain est un « être de sens », or, selon le modèle de la clinique traumatique, le surgissement du tout-numérique peut entraîner un « effondrement du sens » générateur d’une tendance à la dissociation de la personnalité. Accordant le rétablissement des liens entre émotions, affects, comportements et cognitions, le langage parlé atténue puis fait disparaître la dissociation. Guidée par le praticien, cette parole thérapeutique est parfois qualifiée de « maïeutique », du nom de la science de l’accouchement : elle construit synchroniquement à son essence la pensée, et une prise de conscience de celle-ci, plutôt qu’elle n’en rendrait compte secondairement. Il s’agit d’une réinterprétation causale d’un sens compris ou plutôt « attribué » singulièrement par le sujet, après-coup, le passé revisité dans l’instant noue une synthèse, le hasard est transformé en destin. Le sujet qui parle réélabore son histoire vers une reconstruction sémantique, une densification de ses réseaux de signification. Reconquérant son être par la création d’un discours, de méandres véridiques comme fictionnels, la narration, voire la poétisation, offre l’illusion ponctuelle d’une meilleure cohérence, toujours relative, illusoire La parole thérapeutique et le discours sur celle-ci restent en devenir, inachevés, incertains autant que vivants, caractérisant une « post-psychothérapie », c’est-à-dire une psychothérapie et non pas une technique rééducative qui se trouverait figée dans des objectifs connus à l’avance. Les notions de faits et de réalité sont ici secondaires, non pas au sens de l’objectif, ni même du subjectif, mais du second degré, puis d’autres degrés successifs ou imbriqués portant l’effort intellectuel. Vers l’apaisement, si nous voulions amener la réflexion à son paroxysme, nous pourrions avancer qu’il suffirait de donner « n’importe quel sens », d’en choisir un quel qu’il soit, du côté du patient ou du praticien, sans qu’il ne soit nécessairement le même, témoignage d’une construction intersubjective formellement invalide.
               
                  Objectives
                  Who have we become, as citizens, patients, practitioners? How do the means of communication and the computerization of our society, its digitization, modify and integrate our identities? Can we assume that artificial intelligence will soon have a more accurate understanding of the human being from whom it will have emancipated itself?
               
                  Materials and methods
                  We move from lexicology to try to grasp, from the point of view of philosophy, a contemporary identity that is moving towards the notion of a “digital identity” whose normal or pathological psychological incidents lead to what we define as “the digital personality.” Then, laying the foundations for a contemporary psychology of identity, we consider how current “psychology” and “psychiatry” view the patient's “personality” and, in turn, how they define themselves from the point of view of “the patient,” or, inversely, from the point of view of the “online practitioner” or “connected researcher.”
               
                  Results
                  In exchange for its “free” use, the Internet user's action on Web 2.0 produces content and feeds databases, whether this is declared or not. Users’ privacy is lost, as “their” data no longer belongs to them; and citizens’ identity is broken down into digital media functions: a site for meeting friends, a dating platform, a blog about hobbies or travel, etc. At the same time, digital identity is made up of an other-self, including a part of artificial intelligence that provides capacity for its own existence. Rather than two parallel, differentiable entities, real or augmented, a “realistic-virtual” hybrid identity is born. What are the normal or pathological consequences for humans? Postmodern societal trends emerging from or finding expression in the digital can lead to an exacerbation of previously existing personality traits, or even symptoms, in a given individual. At the same time, it happens that the modern means of communication become an aid to experience the world, to increase self-esteem, to dream favorably about one's fantasies, to confide more easily in “strangers,” etc. But in all cases, in the subject suffering, or not suffering, prior to his overexposure, from a neuropsychiatric disease or a psychopathological disorder, it now turns out to be scientifically documented that the increased numerical confrontation induces massive neuropsychic damage (weakening working memory, attention and concentration skills, skills in constructing sophisticated cognitive operations, etc.). On the psychopathological level, rather than the terminology of “identity disorder” or a notion of “co-identities,” the term “identity elusive"" seems to us to best account for this mutation of the “me” where the border between reality and virtualities is shrinking: dissociation prevails. The postmodern human and its connected objects become one, but this “uniformity” appears to be made up of a patchwork of identifying confetti more or less joined together, without a real overall harmonization. The common personality is marked by hyperexpressiveness and hyperemotivity, to the detriment of the possibility of controlling affects and the development of introspective capacities. Against the risk of a vacuum, a contra-phobia tends to develop through the smartphone, by the object itself, by the possibility of constantly contacting relatives if necessary, and in return always remaining “available,” which fuels a form of addicting self-centeredness. The result of these developments, for society in general, is a weakening of language skills, and thus of reflection, including in the clinical and scientific space.
               
                  Discussion
                  For the areas of psychology and psychiatry, two developments are currently associated: a desire for “objectivity-scientificity” and a digitization of the patient–caregiver relationship. On the side of “science,” objective “factual” medicine is increasingly interested in pathology at the expense of the suffering subject, confusing sign and symptom, sliding down to a molecular level, far below the patient, towards psychiatry or postclinical psychology. Whether we want to promote it or destroy it, on the side of the clinician or the researcher, “subjectivity” has become a fashionable signifier in the field of mental health. This current return of the “subjective” thrives on a kind of fear of subjectivity present since the end of World War II, which had led American nosography towards the “objectives” of the DSM (Diagnostic and Statistical Manual of Mental Disorders, published by the American Psychiatric Association since 1952). But rather than a verifiable and/or invariable knowledge concerning a particular psychic disorder, the changes and the relativity of nosographic entities from one version of the manual to another provides us with a mirror image of the subjectivity of an era, which we propose to call “societal subjectivity.” As much as it is a product of our time, the bio-digital revolution will probably impose itself in a future edition of nosography: the diagnostic validity should be increased by the precise definition of biological and/or neuroradiological markers, if these participate in building an etiopathogenic theory of observed psychic phenomena. This orientation remains in its infancy, however: in addition to the tiny number of identified biomarkers, and above all, those that are usable in daily practice, their causal or consequential links with symptoms or with the morbid process remain most often uncertain, inasmuch as they are diverse and interrelated. The neuroscience researcher aims to measure and analyze a multitude of data, integrating, in particular, mimicry and emotions authenticated by thermal camera; movements of body segments and gaze dynamics recorded by sensors; the standardization of voices and speeches for computer software analysis of prosody, used signifiers, syntax… all of which is integrated into a digital phenotyping of suffering. Will we soon be able to speak, replacing the psychologist or the psychiatrist, of an “augmented diagnostician?”.
               
                  Conclusion
                  Does it currently appear risky to trust an entirely virtual therapist… an experiment already launched more than 50 years ago! The human being is a “being of meaning,” yet, according to the model of trauma, the emergence of the all-digital can lead to a “collapse of meaning,” generating a tendency to personality dissociation. Granting the reestablishment of the links between emotions, affects, behaviors, and cognitions, spoken language attenuates dissociation, then makes it disappear. Guided by the practitioner, this therapeutic word is sometimes qualified as “maieutics,” from the name of the science of childbirth: it builds thought synchronously to its essence, and an awareness of it, rather than nondisclosure, would account for it secondarily. It is a causal reinterpretation of a meaning understood or rather “attributed” singularly by the subject, after the fact: the past revisited in the present moment creates a synthesis, and chance is transformed into fate. The speaking subject re-elaborates her/his story towards a semantic reconstruction, a densification of her/his networks of signification. Reclaiming one's being by the creation of a discourse, of veridical as well as fictional meanders, narration, even poetization, offers the punctual illusion of a better coherence, always relative, illusory… Therapeutic speech and discourse about such speech–these are still being made, unfinished, uncertain, and alive. These are the characteristics of what we could a “post-psychotherapy,” that is, a psychotherapy and not a re-educational technique whose objectives would be fixed and known in advance. The notions of facts and reality are secondary here, not in the sense of the objective, nor even of the subjective, but of the second degree, then of other successive or overlapping degrees that require intellectual effort. Moving towards appeasement, if we wanted to bring the reflection to its paroxysm, we could advance that it would be enough to give “any meaning,” whatever it may be. This would apply both to the patient and to the practitioner, without each party's meaning necessarily being the same: a testimony to a formally invalid intersubjective construction.",space,83
10.1016/j.micpro.2020.103343,filtered,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,ecological evolution path of smart education platform based on deep learning and image detection,https://api.elsevier.com/content/abstract/scopus_id/85095585894,"Smart environments are becoming a reality in our society, and the number of smart devices integrated into these spaces is overgrowing. End users are being provided a simplified way to handle complex smart features, as the combination of smart elements opens up a wide range of new opportunities to facilitate. This article explores the significant challenges to be overcome in designing an intelligent educational environment for the main characteristics and the personalized support of ecology. To integrate intelligent learning environments into learning ecology and educational environments, innovative applications, and new teaching methods should be implemented to coordinate formal and informal learning. However, despite the increased use of smart learning environments in higher education, at the same time, there is an excellent network that does not define a set of demand models for the development and evaluation of smart learning environment education that considers teaching, evaluation, and design. Deep learning is one of the modern methods that can be used to automate the process of effective intellectual education based on image detection. The deep learning process is based on image discovery. It provides an overview of ecological evaluation based smart education level analysis used image detection. The system that has been proposed here is an intelligent education system that has been customized to provide the resources of the evolution of the ecosystem to the learner to suit their perceptions and education center to start the platform.",space,84
10.1016/j.procs.2021.09.233,filtered,Procedia Computer Science,scopus,2021-01-01,sciencedirect,a note on the applications of artificial intelligence in the hospitality industry: preliminary results of a survey,https://api.elsevier.com/content/abstract/scopus_id/85116885410,"Intelligent technologies are widely implemented in different areas of modern society but specific approaches should be applied in services. Basic relationships refer to supporting customers and people responsible for services offering for these customers. The aim of the paper is to analyze and evaluate the state-of-the art of artificial intelligence (AI) applications in the hospitality industry. Our findings show that the major deployments concern in-person customer services, chatbots and messaging tools, business intelligence tools powered by machine learning, and virtual reality & augmented reality. Moreover, we performed a survey (n = 178), asking respondents about their perceptions and attitudes toward AI, including its implementation within a hotel space. The paper attempts to discuss how the hotel industry can be motivated by potential customers to apply selected AI solutions. In our opinion, these results provide useful insights for understanding the phenomenon under investigation. Nevertheless, since the results are not conclusive, more research is still needed on this topic. Future studies may concern both qualitative and quantitative methods, devoted to developing models that: a) quantify the potential benefits and risks of AI implementations, b) determine and evaluate the factors affecting the AI adoption by the customers, and c) measure the user (guest) experience of the hotel services, fueled by AI-based technologies.",space,85
10.1016/j.gaitpost.2018.11.029,filtered,Gait and Posture,scopus,2019-02-01,sciencedirect,"three-dimensional cameras and skeleton pose tracking for physical function assessment: a review of uses, validity, current developments and kinect alternatives",https://api.elsevier.com/content/abstract/scopus_id/85057183966,"Background
                  Three-dimensional camera systems that integrate depth assessment with traditional two-dimensional images, such as the Microsoft Kinect, Intel Realsense, StereoLabs Zed and Orbecc, hold great promise as physical function assessment tools. When combined with point cloud and skeleton pose tracking software they can be used to assess many different aspects of physical function and anatomy. These assessments have received great interest over the past decade, and will likely receive further study as the integration of depth sensing and augmented reality smartphone cameras occurs more in everyday life.
               
                  Research Question
                  The aim of this review is to discuss how these devices work, what options are available, the best methods for performing assessments and how they can be used in the future.
               
                  Methods
                  Firstly, a review of the Microsoft Kinect devices and associated artificial intelligence, automated skeleton tracking algorithms is provided. This includes a narrative critique of the validity and clinical utility of these devices for assessing different aspects of physical function including spatiotemporal, kinematic and inverse dynamics data derived from gait and balance trials, and anatomical assessments performed using the depth sensor information. Methods for improving the accuracy of data are examined, including multiple-camera systems and sensor fusion with inertial monitoring units, model fitting, and marker tracking. Secondly, alternative hardware, including other structured light and time of flight methods, stereoscopic cameras and augmented reality leveraging smartphone and tablet cameras to perform measurements in three-dimensional space are summarised. Software options related to depth sensing cameras are then discussed, focussing on recent advances such as OpenPose and web-based methods such as PoseNet.
               
                  Results and Significance
                  The clinical and non-laboratory utility of these devices holds great promise for physical function assessment, and recent developments could strengthen their ability to provide important and impactful health-related data.",space,86
10.1016/j.procs.2019.09.069,filtered,Procedia Computer Science,scopus,2019-01-01,sciencedirect,an innovative technology: augmented reality based information systems,https://api.elsevier.com/content/abstract/scopus_id/85076255225,"In our generation the information systems evolve with new technologies: augmented reality (AR), IoT, artificial intelligence, blockchain etc. Anymore they perform information exchange by sensors. It is estimated that the systems will be in a state of extreme interaction and reach 50 billion devices connected in Internet in 2020. We know that everything around us will be in interaction and they will do everything without any need of human interference. For example, when our dishwasher is full, it will start to wash automatically, or when the run out of the gasoline, our car will drive to the nearest station, or even when a burglar is entered to our house, it will automatically be detected and be announced to the police office. In business life, the processes will be automatical in maximum level and this technology will increase productivity and efficiency. Next to mobile technology, it is thought that these new generation information systems (IS) will take the biggest place in our lives. AR also will be integrated to these systems to augment the information in real world. Humanity will augment its habitat in an innovative way thanks to these AR based IS. This paper surveys the current state-of-the-art AR systems related with aerospace & defense, industry, education, medical and gaming sectors. The connection of AR based IS and innovation is explained with a technological insight. In addition to international use cases HAVELSAN’s use cases are also given that are performed from the aspect of applied open innovation strategy. This strategy is addressed specific to the implemented activities of AR based IS.",space,87
10.1016/j.neuron.2014.08.042,filtered,Neuron,scopus,2014-10-22,sciencedirect,engagement of neural circuits underlying 2d spatial navigation in a rodent virtual reality system,https://api.elsevier.com/content/abstract/scopus_id/84908234689,"Virtual reality (VR) enables precise control of an animal’s environment and otherwise impossible experimental manipulations. Neural activity in rodents has been studied on virtual 1D tracks. However, 2D navigation imposes additional requirements, such as the processing of head direction and environment boundaries, and it is unknown whether the neural circuits underlying 2D representations can be sufficiently engaged in VR. We implemented a VR setup for rats, including software and large-scale electrophysiology, that supports 2D navigation by allowing rotation and walking in any direction. The entorhinal-hippocampal circuit, including place, head direction, and grid cells, showed 2D activity patterns similar to those in the real world. Furthermore, border cells were observed, and hippocampal remapping was driven by environment shape, suggesting functional processing of virtual boundaries. These results illustrate that 2D spatial representations can be engaged by visual and rotational vestibular stimuli alone and suggest a novel VR tool for studying rat navigation.",space,88
10.1016/j.neucom.2013.03.060,filtered,Neurocomputing,scopus,2014-05-20,sciencedirect,autonomous uav based search operations using constrained sampling evolutionary algorithms,https://api.elsevier.com/content/abstract/scopus_id/84896707806,"This paper introduces and studies the application of Constrained Sampling Evolutionary Algorithms in the framework of an UAV based search and rescue scenario. These algorithms have been developed as a way to harness the power of Evolutionary Algorithms (EA) when operating in complex, noisy, multimodal optimization problems and transfer the advantages of their approach to real time real world problems that can be transformed into search and optimization challenges. These types of problems are denoted as Constrained Sampling problems and are characterized by the fact that the physical limitations of reality do not allow for an instantaneous determination of the fitness of the points present in the population that must be evolved. A general approach to address these problems is presented and a particular implementation using Differential Evolution as an example of CS-EA is created and evaluated using teams of UAVs in search and rescue missions. The results are compared to those of a Swarm Intelligence based strategy in the same type of problem as this approach has been widely used within the UAV path planning field in different variants by many authors.",space,89
10.26083/tuprints-00019940,filtered,core,,2021-01-01 00:00:00,core,randomizing physics simulations for robot learning,,"The ability to mentally evaluate variations of the future may well be the key to intelligence. Combined with the ability to reason, it makes humans excellent at handling new and complex situations. If we want robots to solve varying tasks autonomously, we need to endow them with such kind of ‘mental rehearsal’. Physics simulations allow predicting how the environment will change depending on a sequence of actions. For example, robots can simulate multiple control policies in different simulations instances, collect the results, and subsequently reason about which policy to execute in the real world. As such physics simulations are highly customizable, they enable generating vast amounts of diverse data at a relatively low cost. Therefore, they make it possible to apply deep learning methods for physical systems despite the exorbitant demand for data. Since state-of-the-art deep learning methods come with few guarantees, it is essential to test them in many simulated scenarios before deployment on the real system.
Over the last decade, the speed and modeling power of general-purpose physics engines increased significantly. State-of-the-art simulators feature rigid body, soft body, and fluid dynamics, as well as massive GPU-based parallelization. Despite the impressive progress, simulations will always remain an idealized model of the real world, thus are inevitably flawed. Typical error sources are unmodeled physical phenomena, or suboptimal parameter values of the underlying generative model. These discrepancies between the real and the simulated world are summarized by the term ‘reality gap’. This gap can manifest in various ways when learning from simulations. In the best case, it is only a performance drop, e.g., a lower success rate, or a reduced tracking accuracy. More likely, the learned policy is not transferable to the robot because of unknown friction effects, which lead to underestimating the friction in simulation. Thus, the commanded actions are in this case not strong enough to get the robot moving. Another reason for failure are small parameter estimation errors, which can quickly lead to unstable system dynamics. This case is particularly dangerous for humans and robots. For these reasons, bridging the reality gap is the essential step to endow robots with the ability to learn from simulated experience.
In this thesis, we will tackle the challenge of learning robot control policies from simulations such that the results can be (directly) transferred to the real world. We focus on scenarios where the source domain is a randomized simulator and the target domain is either a different simulation instance (sim-to-sim) or the physical robot (sim-to-real).
We strive to answer the following research questions:
1. How can we quantitatively estimate the transferability of a control policy from one domain to another?
2. Does randomizing the simulator during learning make the resulting policy more robust against modeling imperfections?
3. How do we adapt the randomized simulator based on real-world evaluations?
4. Can we infer the source domain parameter distribution from data and subsequently use it for learning?
5. What are the necessary assumptions and technical requirements to learn robot
control policies from randomized simulations?
Despite the recent popularity of sim-to-real methods, the first question has been unanswered up to this point in time. As a consequence, state-of-the-art algorithms can not make a quantitative statement about the transferability of the resulting control policies. Moreover, they stop training according to some heuristic like a fixed number of iterations, which can lead to a waste of computation time. In Chapter 3, we derive the simulation optimization bias as a measure of the reality gap, and show that policies learned from a source domain are optimistically biased in terms of their performance in the target domain, even if they originate from the same distribution. To mitigate this problem, we propose a policy search algorithm that estimates simulation optimization bias and continues training until an estimated upper confidence bound on this bias is below a given threshold. Thus, the resulting policy satisfies a probabilistic guarantee on the performance loss when transferring the policy to a different environment from the same source domain distribution. Moreover, our sim-to-real evaluations answer the second question with a clear “yes”.
Straightforwardly learning from randomized source domains shows the tendency to be slower and have lower performance at the nominal model than methods that close the sim-to-real loop by adapting the domain parameter distribution. Therefore, we tackle the third question in Chapter 4 by introducing a policy search algorithm which incorporates Bayesian optimization to adapt the domain parameter distribution based on real-world data. The sample-efficiency of Bayesian optimization allows updating the distribution’s parameters, including the uncertainty, while only requiring few evaluations on the physical device. Most notably, the data yielded from these evaluations can be very scarce, e.g., a scalar return value per trial. This way, the connection between distribution over simulator parameters and the target domain performance is captured by a probabilistic model. At the same time, we can eliminate the common assumption of knowing the distribution’s mean and variance a priori.
So far, existing domain randomization approaches assume that each domain parameter is independent and obeys a known probability distribution type, typically chosen to be a normal or uniform distribution. These and other assumptions impose unnecessary restrictions on the posterior distribution over simulators, and prevent us from utilizing the full power of domain randomization. In order to overcome this limitation, we propose to combine reinforcement learning with state-of-the-art likelihood-free inference methods, powered by flexible neural density estimators, to learn the posterior over domain parameters. The proposed method only requires a parametric generative model, e.g., a physics simulator, coarse prior ranges, and a small set of real-world trajectories. Together with a policy optimization algorithm, this approach iteratively updates the posterior over simulators and learns how to solve a given task. Most importantly, the generative model does not need to be differentiable, and the neural posterior can capture dependencies between domain parameters. By drastically reducing the quality and quantity of assumptions while still successfully learning transferable control policies, this procedure answers the fourth and the fifth question in Chapter 5. The methods presented in this thesis will greatly benefit from the continuous increase in computational power, allowing the randomization schemes to perform more exhaustive searches through the domain parameter space. In consequence, the required computation time as well as the variance will be reduced, alleviating the two biggest drawbacks of the domain randomization approaches. Meanwhile, financially strong actors like the video gaming industry are heavily pushing the development of physics simulators. Thus, current niche applications like simulations of muscles or interactions between fluid and solid particles are going to be consumer standard in the near future. The facilitated access to high-fidelity simulators will open the door to a whole new range of tasks which can be solved with methods presented in this thesis. One example could be to train control policies for active robotic prostheses in simulation such that to support human motion. In a subsequent step, these controllers could be customized based on user-specific data. The foreseeable establishment of (differentiable) probabilistic simulation engines will
provide access to the simulator’s likelihood function, hence boost the applicability of Bayesian inference. As a consequence, the popularity of research on highly data-efficient simulation-based inference methods will increase, leading to new algorithms that can perform complex inference in real time. These approaches have the potential to become the next mega trend in robotics research after the era deep learning",space,90
10.14746/ps.2021.1.25,filtered,core,'Adam Mickiewicz University Poznan',2021-12-29 00:00:00,core,"tematyka komunikacji cyfrowej w kontekście ewolucji technologicznej współczesnego społeczeństwa: zagrożenia, wyzwania, ryzyka",,"The purpose of this article is to identify the risks, threats, and challenges associated with possible social changes in the processes of digitalization of society and transformations of traditional communication practices, which is associated with the emergence of new digital subjects of mass public communication that form the pseudo structure of digital interaction of people. The primary tasks of the work were to identify the potential of artificial intelligence technologies and neural networks in the field of social and political communications, as well as to analyze the features of “smart” communications in terms of their subjectness. As a methodological optics, the work used the method of discourse analysis of scientific research devoted to the implementation and application of artificial intelligence technologies and self-learning neural networks in the processes of social and political digitalization, as well as the method of critical analysis of current communication practices in the socio-political sphere. At the same time, when analyzing the current digitalization practices, the case study method was used. The authors substantiate the thesis that introducing technological solutions based on artificial intelligence algorithms and self-learning neural networks into contemporary processes of socio-political communication creates the potential for a wide range of challenges, threats, and risks, the key of which is the problem of identifying the actual subjects of digital communication acts. The article also discusses the problem of increasing the manipulative potential of “smart” communications, for which the authors used the concepts of cyber simulacrum and information capsule developed by them. The paper shows that artificial intelligence and self-learning neural network algorithms, being increasingly widely introduced into the current practice of contemporary digital communications, form a high potential for information and communication impact on the mass consciousness from technological solutions that no longer require control by operators – humans. As a result, conditions arise to form a hybrid socio-technical reality – a communication reality of a new type with mixed subjectness. The paper also concludes that in the current practices of social interactions in the digital space, a person faces a new phenomenon – interfaceization, within which self-communication stimulates the universalization and standardization of digital behavior, creating, disseminating, strengthening, and imposing special digital rituals. In the article, the authors suggest that digital rituals blur the line between the activity of digital avatars based on artificial intelligence and the activity of actual people, resulting in the potential for a person to lose his own subjectness in the digital communications space.Celem niniejszego artykułu jest identyfikacja ryzyk, zagrożeń i wyzwań związanych z możliwymi zmianami społecznymi w procesach cyfryzacji społeczeństwa oraz przekształceniami tradycyjnych praktyk komunikacyjnych, co wiąże się z pojawieniem się nowych cyfrowych podmiotów masowej komunikacji publicznej tworzących pseudostrukturę cyfrowej interakcji pomiędzy ludźmi. Podstawowymi zadaniami pracy była identyfikacja potencjału technologii sztucznej inteligencji i sieci neuronowych w obszarze komunikacji społecznej i politycznej, a także analiza cech komunikacji „inteligentnej” pod kątem jej podmiotowości. Jako optykę metodologiczną w pracy wykorzystano metodę analizy dyskursu badań naukowych poświęconych wdrożeniu i zastosowaniu technologii sztucznej inteligencji oraz samouczących się sieci neuronowych w procesach cyfryzacji społecznej i politycznej, a także metodę krytycznej analizy aktualnych praktyk komunikacyjnych w sferze społeczno-politycznej. Jednocześnie przy analizie aktualnych praktyk digitalizacyjnych zastosowano metodę studium przypadku. Autorzy uzasadniają tezę, że wprowadzenie do współczesnych procesów komunikacji społeczno-politycznej rozwiązań technologicznych opartych na algorytmach sztucznej inteligencji i samouczących się sieciach neuronowych stwarza potencjał dla szerokiego wachlarza wyzwań, zagrożeń i ryzyka, których kluczem jest problem identyfikacji rzeczywistych podmiotów aktów komunikacji cyfrowej. W artykule omówiono również problem zwiększenia potencjału manipulacyjnego „inteligentnej” komunikacji, do czego autorzy wykorzystali opracowane przez siebie koncepcje cyber simulacrum i kapsuły informacyjnej. Artykuł pokazuje, że sztuczna inteligencja i samouczące się algorytmy sieci neuronowych, coraz szerzej wprowadzane do obecnej praktyki współczesnej komunikacji cyfrowej, stwarzają duży potencjał oddziaływania informacyjno-komunikacyjnego na świadomość masową z rozwiązań technologicznych, które nie wymagają już kontroli przez operatorów – ludzi. W efekcie powstają warunki do uformowania hybrydowej rzeczywistości społeczno-technicznej – rzeczywistości komunikacyjnej nowego typu o mieszanej podmiotowości. W artykule stwierdzono również, że w obecnych praktykach interakcji społecznych w przestrzeni cyfrowej człowiek staje przed nowym zjawiskiem – interfaceization, w ramach którego autokomunikacja stymuluje uniwersalizację i standaryzację zachowań cyfrowych, tworzenie, rozpowszechnianie, wzmacnianie i narzucanie szczególnego cyfrowego rytuału. W artykule autorzy sugerują, że cyfrowe rytuały zacierają granicę między aktywnością cyfrowych awatarów opartych na sztucznej inteligencji a aktywnością rzeczywistych ludzi, co skutkuje możliwością utraty przez człowieka własnej podmiotowości w cyfrowej przestrzeni komunikacyjnej",space,91
10.3390/computers10080099,filtered,core,'MDPI AG',2021-08-01 00:00:00,core,an integrated mobile augmented reality digital twin monitoring system,,"The increasing digitalization and advancement in information communication technologies has greatly changed how humans interact with digital information. Nowadays, it is not sufficient to only display relevant data in production activities, as the enormous amount of data generated from smart devices can overwhelm operators without being fully utilized. Operators often require extensive knowledge of the machines in use to make informed decisions during processes such as maintenance and production. To enable novice operators to access such knowledge, it is important to reinvent the way of interacting with digitally enhanced smart devices. In this research, a mobile augmented reality remote monitoring system is proposed to help operators with low knowledge and experience level comprehend digital twin data of a device and interact with the device. It analyses both historic logs as well as real-time data through a cloud server and enriches 2D data with 3D models and animations in the 3D physical space. A cloud-based machine learning algorithm is applied to transform learned knowledge into live presentations on a mobile device for users to interact with. A scaled-down case study is conducted using a tower crane model to demonstrate the potential benefits as well as implications when the system is deployed in industrial environments. This user study verifies that the proposed solution yields consistent measurable improvements for novice users in human-device interaction that is statistically significant",space,92
10.15353/jcvis.v6i1.3557,filtered,core,'University of Waterloo',2021-01-15 00:00:00,core,real-time quantitative visual inspection using extended reality,https://core.ac.uk/download/386113006.pdf,"In this study, we propose a technique for quantitative visual inspection that can quantify structural damage using extended reality (XR). The XR headset can display and overlay graphical information on the physical space and process the data from the built-in camera and depth sensor. Also, the device permits accessing and analyzing image and video stream in real-time and utilizing 3D meshes of the environment and camera pose information. By leveraging these features for the XR headset, we build a workflow and graphic interface to capture the images, segment damage regions, and evaluate the physical size of damage. A deep learning-based interactive segmentation algorithm called f-BRS was deployed to precisely segment damage regions through the XR headset. A ray-casting algorithm is implemented to obtain 3D locations corresponding to the pixel locations of the damage region on the image. The size of the damage region is computed from the 3D locations of its boundary. The performance of the proposed method is demonstrated through a field experiment at an in-service bridge where spalling damage is present at its abutment. The experiment shows that the proposed method provides sub-centimeter accuracy for the size estimation",space,93
10.3390/s21093061,filtered,core,'MDPI AG',2021-05-01 00:00:00,core,a navigation and augmented reality system for visually impaired people,https://core.ac.uk/download/479996476.pdf,"In recent years, we have assisted with an impressive advance in augmented reality systems and computer vision algorithms, based on image processing and artificial intelligence. Thanks to these technologies, mainstream smartphones are able to estimate their own motion in 3D space with high accuracy. In this paper, we exploit such technologies to support the autonomous mobility of people with visual disabilities, identifying pre-defined virtual paths and providing context information, reducing the distance between the digital and real worlds. In particular, we present ARIANNA+, an extension of ARIANNA, a system explicitly designed for visually impaired people for indoor and outdoor localization and navigation. While ARIANNA is based on the assumption that landmarks, such as QR codes, and physical paths (composed of colored tapes, painted lines, or tactile pavings) are deployed in the environment and recognized by the camera of a common smartphone, ARIANNA+ eliminates the need for any physical support thanks to the ARKit library, which we exploit to build a completely virtual path. Moreover, ARIANNA+ adds the possibility for the users to have enhanced interactions with the surrounding environment, through convolutional neural networks (CNNs) trained to recognize objects or buildings and enabling the possibility of accessing contents associated with them. By using a common smartphone as a mediation instrument with the environment, ARIANNA+ leverages augmented reality and machine learning for enhancing physical accessibility. The proposed system allows visually impaired people to easily navigate in indoor and outdoor scenarios simply by loading a previously recorded virtual path and providing automatic guidance along the route, through haptic, speech, and sound feedback",space,94
10.32983/2222-4459-2021-6-50-58,filtered,core,Formation of Industry X.0 on the Basis of Innovative-Digital Entrepreneurship and Virtual Mobility,2021-06-01 00:00:00,core,https://core.ac.uk/download/479727169.pdf,'Research Centre of Industrial Problems of Development of NAS of Ukraine',"The article attempts to present a number of key technologies that determine the new quality of life of people. The following content is specified and disclosed: autonomous artificial intelligence in a smartphone, professional robot assistants, available satellite intelligence, podcasts, digital urban planning tools. In the article, the authors hypothesize that Industry X.0 is by far the highest stage of digitalization and represents a concept of innovative and digital production, the components of which are «smart assets», «smart services», «smart business», and «smart government». Structural elements of the authors’ concept of X.0 Industry are indicated, its visual cut in virtual reality conditions is provided and the functioning of this Industry exclusively within the framework of the 7th technological mode is characterized. The authors have developed and presented the protocol of formation of the X.0 Industry through the prism of innovations, technologies in both the industry sector and business management. 4 stages of implementation of this protocol are defined, namely: determination of the innovative landscape of «technological breakthrough» in a particular industry within the formation of industry X.0; assessment of threats; determining the course of further development and the action plan (four main approaches to which organizations can apply: protection, adoption of innovations, initiation of subversive innovations, retreat); implementation of structural changes at the DNA level of the organization. The authors on the basis of a number of factors bring forward the argument that today’s realities of the digital space require the development of a new logic of running a platform business in terms of its digitization. It is concluded that in practice it is necessary to form a broad coalition of educators, government officials, analysts, high-tech specialists, economists, industrialists, scientists who will join the formation of the X.0 Industry on the basis of digitalization and innovatizing. The authors concluded that Industry X.0 is a new approach to the organization of production in the context of virtual reality, which is based on highly intelligent integrated new products and digital ecosystems that form an innovative digital value chain, add new competencies and implement deep cultural changes in the direction of the formation of a new virtual reality",space,95
https://core.ac.uk/download/322366596.pdf,filtered,core,'Lviv State University of Life Safety',2018-12-31 00:00:00,core,hard-soft-технологія інформаційного супроводу  процесу моделювання теплотворення/теплоспоживання  в двигуні внутрішнього згоряння,10.32447/20784643.18.2018.01,"Deterministic and, in a certain sense, ""linear"" interpretation of the world often leads to the recognition of the fact that the more accurate model we need, the more complex it must be (as in case of a formalized reproduction of the real system, or the implementation of the desired system properties in the process of formal synthesis of something new). Instead, following the principle of synergy leads to the conviction that there is always a certain model of optimal complexity e.g. in the synthesis of the new system, and in the analysis of real system peculiarities. However, the model of reality could be a part of this reality that is included to the carefully structured formal description.  Since we cannot penetrate into the working space of the serial engine while testing, we should use a test engine of a special construction when the working space corresponds to the laws of similarity and this engine will serve as a model of the working space of the serial engine.
&nbsp;
&nbsp;
The study illustrates the effectiveness of hard-soft technology while investigating the peculiarities of heat generation and heat consumption in the internal combustion engine, which will combine mathematic and algorithmic means of modelling as well as the means of real simulation. The necessity of hard-soft technology introduction arises from the excessive complexity of thermal phenomena occurring in the internal combustion engine (ICE), and the inability to fully subordinate these phenomena to existing analytical models.
The combination of original and analytical properties, reality and virtual reality while modelling the processes in internal combustion engines allows us to substantially improve the quality of information in the process of design and engine construction. Taking this into consideration, there are some natural grounds to apply principles of heuristic self-organization, self-learning, means of the neural networks, etc. in the design implementation.
The study demonstrates the example of modelling the real working space of ICE with the forced start that serves as a supplement to the mathematical algorithmic two-zone model of heat generation / heat consumption / heat extraction.
The basic information that can be obtained by means of hard-soft technology in the framework of, for example, the two-zone model of the work process in the gasoline engine, is the variability with the change in the angle of rotation of the crankshaft of the engine: absolute pressure (indicative diagram); absolute temperature; heat transmitted inside the cylinder between zones; coefficient of excess air; coefficient of heat transfer; intensity of heat extraction in the process of combustion of fuel; intensity of heat transfer through the walls of the cylindeДетерміністичне і в певному сенсі «лінійне» трактування світу часто веде до визнання того, що чим точнішою потрібна його модель, тим складнішою вона має бути (як у разі формалізованого відтворення реальної системи, так і у разі втілення бажаних системних властивостей у процесі формалізованого синтезу чогось нового). Натомість дотримання принципу синергетичності веде до переконання, що завжди існує якась модель оптимальної складності — і тоді, коли йдеться про синтез нової системи, і тоді, коли провадиться аналіз властивостей реальної системи. Але ж моделлю реальності може слугувати також і якась частина цієї реальності, долучена до ретельно структурованого формального опису. Оскільки дослідними засобами проникнути в робочий простір серійного двигуна нема змоги, то доводиться використовувати дослідний двигун особливої конструкції, робочий простір якого відповідає законам подібності і слугуватиме моделлю-аналогом робочого простору серійного двигуна.
Мета роботи — обґрунтувати ефективність hard-soft-технології дослідження особливостей теплотворення і теплоспоживання в двигуні внутрішнього згоряння, яка б системно поєднувала в собі засоби математичного й алгоритмічного моделювання та засоби натурного симулювання. Необхідність впровадження hard-soft-технології випливає з надмірної складності теплових явищ, що перебігають у двигуні внутрішнього згоряння, та неможливості уповні підпорядкувати ці явища існуючим аналітичним модельним уявленням.
Поєднання натурності та аналітичності, реальності та віртуальності в моделюванні процесів у двигунах внутрішнього згоряння дозволяє принципово підвищити якість інформаційного забезпечення процесу проектування й конструювання двигунів. При цьому виникають природні підстави для втілення у моделювання принципів евристичної самоорганізації, самонавчання, засобів штибу нейронних мереж тощо.
Наводиться приклад формування реального робочого простору двигуна внутрішнього згоряння з примусовим запаленням, покликаного доповнити математично-алгоритмічну двозонну модель теплотворення/теплоспоживання/тепловідведення.
Основною інформацією, яку можна добувати засобами hard-soft-технології в рамках, приміром, двозонної моделі робочого процесу в бензиновому двигуні, є змінюваність зі зміною кута повороту колінчастого вала двигуна: абсолютного тиску (індикаторна діаграма); абсолютної температури; теплоти, що пересилається всередині циліндра між зонами; коефіцієнта надлишку повітря; коефіцієнта тепловіддачі; інтенсивності тепловиділення у процесі згоряння палива; інтенсивність тепловідведення через стінки циліндра",space,96
https://core.ac.uk/download/162433564.pdf,filtered,core,,2018-01-01 00:00:00,core,"eduardo souto de moura. un ""dialogo antico"" tra materia, tecnica e progetto",10.13128/Techne-23985,"L’impegno e la precisione che Eduardo Souto de Moura dedica a ogni occasione di dialogo con i luoghi del suo operare, conferma la sua adesione alla concretezza e la sua aspirazione alla verità. L’architettura priva di quelle fondamenta che la ancorano alla realtà e alla specificità dei luoghi, solo pensata e discussa in termini di potenzialità e astrazione, cieca e sorda alle suggestioni dell’ambiente, non esiste. Non è Architettura. Non è, almeno, l’Architettura di Souto de Moura, che è capace, invece, di trasformare le occasioni di dialogo con il contesto in accese discussioni, tra pensiero e realtà, tra desiderio e opportunità, tra segno progettuale e atto costruttivo; dispute, tra l’immagine primitiva che si forma nella mente – un dettaglio, un materiale, uno spazio – e la sua effettiva realizzabilità; confronti, che si annullano con la presa di coscienza delle immense opportunità offerte al progetto dal contatto con il reale; e il progetto, come in un film, esegue un montaggio e una verifica delle immagini, suggestioni e sequenze che l’occhio percepisce e che la grande poesia di Herberto Helder sottende ed esalta nel suo poema “Memoria Montagem”, uno dei grandi riferimenti del maestro portoghese.

Questo metodo, che mette al vaglio le ipotesi e le cala nei luoghi, è applicato indistintamente alle varie scale e ai diversi ambiti di lavoro, alle questioni di composizione volumetrica, al sistema distributivo, a quello tecnologico e impiantistico, alle scelte figurative e materiche. Esso si risolve in una conversazione continua, eterna, un dialogo antico, tra l’architetto – ogni architetto, di ogni tempo e di ogni luogo – e gli spazi in cui egli opera, un dialogo che non ha principio né fine, che si ripete sempre uguale da millenni, che pone e ripone le stesse domande e, alla fine, riceve le stesse risposte.The commitment and precision Eduardo Souto de Moura delivers in every dialogue opportunity with the places of his work confirms his bonds to concreteness and his ambition to truth. Architecture not founded on bases linked to reality and to place specificity; architecture as a mere result of debate and thinking on abstraction, does not exist. It is no Architecture. It isn’t Souto de Moura’s Architecture, which instead is able to transform the opportunities of establishing a dialogue with the context into fervid discussions between thinking and reality, between desire and possibility, between the marks of design and the act of bulding; into discussions between the primitive image forming in the architect’s min – a detais, a material, a space – and its actual feasibility; discussions that are meant to resolve themselves into the awareness of the limitless possibilities given to the project bt the contact with reality. The project, as in a movie, performs then an editing and a test of the many images, suggestions and sequences the eye senses; the same as in Herberto Helder’s poem “Memoria Montagem”, one of the most relevant references of the portuguese master architect.

This method of evaluating the hypothesis – impressions, opinions or even a sort of architectural schemes in progress – and lowering them into the sites, is implemented indifferently at several scales and several programs; at the issues of volumetric composition, at the level of both technology and plants; at the figurative and material options. It is resumed in a continuous and eternal conversation, in an ancient dialogue between the architect – every architect, of any time and place – and the spaces the architect works into; a dialogue having no beginning nor ending, that keeps happening in the centuries always in the same way, asking again and again the same questions, getting, at the end, always the same anwers",space,97
https://core.ac.uk/download/323533353.pdf,filtered,core,'Kyiv Politechnic Institute',2018-01-01 00:00:00,core,practical aspects of innovative digital tecnologies application in marketing,10.20535/2307-5651.15.2018.139575,"У статті досліджено особливості розвитку цифрового маркетингу в сучасних умовах. Розглянуто специфіку запровадження інноваційних цифрових технологій у маркетингові діяльності. Визначено специфіку розвитку віртуальної реальності та особливості її використання у системі цифрового маркетингу. У науковому дослідженні представлено приклади передового використання технологій віртуальної реальності компаніями світу при реалізації їх маркетингових стратегій. Висвітлено специфіку застосування технології віртуальної реальності за умови витрат значних фінансових ресурсів. Також наведено приклад використання дешевих аналогів цифрових технологій за умови використання смартфонів. Висвітлено специфіку залучення клієнтів до дешевих цифрової технологій та особливості збільшення інтересу цільової аудиторії. Отримані результати дають можливість визначити основні тенденції розвитку цифрового маркетингу з застосуванням технологій віртуальної реальності. У дослідженні значну увагу приділено питанням реалізації технологій використанню LED панелей. Обґрунтовано доцільність використання зазначеної технології для indoor маркетингу – виду діяльності всередині приміщень (торговельних закладів, кафе, офісів компаній тощо) та outdoor маркетингу – виду діяльності у зовнішньому просторі (на улицях, парках та ін.). Наведено приклади застосування LED панелей передовими компаніями світу. Висвітлено специфіку застосування даної технології у сфері харчування. Встановлено, що LED панелі відіграють важливу роль у цифровій маркетинговій стратегії кафе та ресторанів. Визначено, що цифрові панелі сприяють зростанню кількості клієнтів. Доведено, що запровадження систем штучного інтелекту у подальшому сприятиме збільшенню інтерактивності та персоніфікації контенту у відповідності з умовами, які будуть проявлятись у певний момент часу в конкретному місці. Встановлено, що зазначений підхід дозволить національним компаніями підвищити рівень їх конкурентоспроможності на національному та міжнародному ринках. Доведено необхідність запровадження державної програми сприяння розвитку цифрового маркетингу в Україні.In the article the features of development of digital marketing in modern conditions are investigated. The specifics of introduction of innovative technologies in marketing strategies of companies are considered. The specificity of the development of virtual reality and the peculiarities of its using in the digital marketing system are determined. In the scientific research, examples of the advanced use of technologies of virtual reality by companies of the world in the implementation of their marketing strategies are presented. The specificity of the application of the virtual reality technology is highlighted with the cost of significant financial resources. The example is the using of cheap analogues of digital technology such as using smartphones. The specifics of attraction the clients to cheap digital technologies and interest increase features of the target audience are highlighted. The obtained results give an opportunity to define the basic tendencies of digital marketing development with application of virtual reality technologies. The study focuses on the implementation of technology for the use of LED panels. The expediency of using this technology for indoor marketing is grounded - the type of activity in the middle of the premises (shopping centers, cafes, offices of companies, etc.) and outdoor marketing - the type of activity in the outer space (in streets, parks, etc.). One example of the using of LED panels is presented by leading companies of the world. The application specifics of this technology in the field of food are highlighted. It has been established that LED panels play the important role in the digital marketing strategy of cafes and restaurants. It is determined that digital panels help to increase the number of clients. It is proved that the introduction of artificial intelligence systems will further enhance the interactivity and personalization of content in accordance with conditions that will manifest at a certain point in time in a special place. It has been established that this approach will allow national companies to increase their competitiveness in the national and international markets. The necessity of implementation of the state program of digital marketing promotion in Ukraine has been proved",space,98
https://riunet.upv.es/bitstream/10251/124242/4/11867-48104-5-pb.pdf,filtered,core,'Universitat Politecnica de Valencia',2019-07-25 00:00:00,core,"a 4d information system for the exploration of multitemporal images and maps using photogrammetry, web technologies and vr/ar",10.4995/var.2019.11867,"[EN] This contribution shows the comparison, investigation, and implementation of different access strategies on multimodal data. The first part of the research is structured as a theoretical part opposing and explaining the terms of conventional access, virtual archival access, and virtual museums while additionally referencing related work. Especially, issues that still persist in repositories like the ambiguity or missing of metadata is pointed out. The second part explains the practical implementation of a workflow from a large image repository to various four-dimensional applications. Mainly, the filtering of images and in the following, the orientation of images is explained. Selection of the relevant images is partly done manually but also with the use of deep convolutional neural networks for image classification. In the following, photogrammetric methods are used for finding the relative orientation between image pairs in a projective frame. For this purpose, an adapted Structure from Motion (SfM) workflow is presented, in which the step of feature detection and matching is replaced by the Radiant-Invariant Feature Transform (RIFT) and Matching On Demand with View Synthesis (MODS). Both methods have been evaluated on a benchmark dataset and performed superior than other approaches. Subsequently, the oriented images are placed interactively and in the future automatically in a 4D browser application showing images, maps, and building models Further usage scenarios are presented in several Virtual Reality (VR) and Augmented Reality (AR) applications. The new representation of the archival data enables spatial and temporal browsing of repositories allowing the research of innovative perspectives and the uncovering of historical details.Highlights:Strategies for a completely automated workflow from image repositories to four-dimensional (4D) access approaches.The orientation of historical images using adapted and evaluated feature matching methods.4D access methods for historical images and 3D models using web technologies and Virtual Reality (VR)/Augmented Reality (AR).[ES] Esta contribución muestra la comparación, investigación e implementación de diferentes estrategias de acceso a datos multimodales. La primera parte de la investigación se estructura en una parte teórica en la que se oponen y explican los términos de acceso convencional, acceso a los archivos virtuales, y museos virtuales, a la vez que se hace referencia a trabajos  relacionados.  En  especial,  se  señalan  los  problemas  que  aún  persisten  en  los  repositorios,  como  la ambigüedad o la falta de metadatos. La segunda parte explica la implementación práctica de un flujo de trabajo desde un gran repositorio de imágenes a varias aplicaciones en cuatro dimensiones (4D). Principalmente, se explica el filtrado de imágenes y, a continuación, la orientación de las mismas. La selección de las imágenes relevantes se hace en parte manualmente,  pero  también  con  el  uso  de  redes  neuronales  convolucionales  profundas  para  la  clasificación  de  las imágenes.  A  continuación,  se  utilizan  métodos  fotogramétricos  para  encontrar  la  orientación  relativa  entre  pares  de imágenes en un marco proyectivo. Para ello, se presenta un flujo de trabajo adaptado a partir de Structure from Motion, (SfM),  en  el  que  el  paso  de la detección  y la correspondencia  de entidades es  sustituido  por  la Transformación  de entidades  invariante  a  la  radiancia  (Radiant-Invariant  Feature  Transform, RIFT)  y  la Correspondencia  a  demanda con vistas sintéticas (Matching on Demand with View Synthesis, MODS). Ambos métodos han sido evaluados sobre la base de  un  conjunto  de  datos  de  referencia  y funcionaron  mejor  que otros procedimientos.  Posteriormente,  las  imágenes orientadas se colocan interactivamente y en el futuro automáticamente en una aplicación de navegador 4D que muestra imágenes,  mapas  y  modelos  de edificios.  Otros  escenarios  de  uso  se  presentan  en  varias  aplicación es  de  Realidad Virtual  (RV)  y  Realidad  Aumentada  (RA).  La  nueva  representación  de  los  datos  archivados permite  la  navegación espacial y temporal de los repositorios, lo que permite la investigación en perspectivas innovadoras y el descubrimiento de detalles históricos.The  research  upon  which  this  paper  is  based  is  part  of the  junior research  group  UrbanHistory4D’s  activities which  has  received funding  from  the  German  Federal Ministry   of   Education   and Research   under   grant agreement No 01UG1630. This work   was   supported   by   the   German Federal Ministry     of     Education     and     Research     (BMBF, 01IS18026BA-F)  by funding  the  competence  center  for Big Data “ScaDS Dresden/Leipzig”.Maiwald, F.; Bruschke, J.; Lehmann, C.; Niebling, F. (2019). Un sistema de información 4D para la exploración de imágenes y mapas multitemporales utilizando fotogrametría, tecnologías web y VR/AR. Virtual Archaeology Review. 10(21):1-13. https://doi.org/10.4995/var.2019.11867SWORD1131021Ackerman, A., & Glekas, E. (2017). Digital Capture and Fabrication Tools for Interpretation of Historic Sites. ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences, IV-2/W2, 107-114. doi:10.5194/isprs-annals-IV-2-W2-107-2017Armingeon, M., Komani, P., Zanwar, T., Korkut, S., & Dornberger, R. (2019). A Case Study: Assessing Effectiveness of the Augmented Reality Application in Augusta Raurica Augmented Reality and Virtual Reality (pp. 99-111): Springer.Artstor. (2019). Artstor Digital Library. Retrieved April 30, 2019, from https://library.artstor.orgBay, H., Tuytelaars, T., & Van Gool, L. (2006). SURF: Speeded Up Robust Features. Paper presented at the European Conference on Computer Vision, Berlin, Heidelberg.Beaudoin, J. E., & Brady, J. E. (2011). Finding visual information: a study of image resources used by archaeologists, architects, art historians, and artists. Art Documentation: Journal of the Art Libraries Society of North America, 30(2), 24-36.Beltrami, C., Cavezzali, D., Chiabrando, F., Iaccarino Idelson, A., Patrucco, G., & Rinaudo, F. (2019). 3D Digital and Physical Reconstruction of a Collapsed Dome using SFM Techniques from Historical Images. Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLII-2/W11, 217-224. doi:10.5194/isprs-archives-XLII-2-W11-217-2019Bevilacqua, M. G., Caroti, G., Piemonte, A., & Ulivieri, D. (2019). Reconstruction of lost Architectural Volumes by Integration of Photogrammetry from Archive Imagery with 3-D Models of the Status Quo. Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLII-2/W9, 119-125. doi:10.5194/isprs-archives-XLII-2-W9-119-2019Bitelli, G., Dellapasqua, M., Girelli, V. A., Sbaraglia, S., & Tinia, M. A. (2017). Historical Photogrammetry and Terrestrial Laser Scanning for the 3d Virtual Reconstruction of Destroyed Structures: A Case Study in Italy. ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XLII-5/W1, 113-119. doi:10.5194/isprs-archives-XLII-5-W1-113-2017Bruschke, J., Niebling, F., Maiwald, F., Friedrichs, K., Wacker, M., & Latoschik, M. E. (2017). Towards browsing repositories of spatially oriented historic photographic images in 3D web environments. Paper presented at the Proceedings of the 22nd International Conference on 3D Web Technology.Bruschke, J., Niebling, F., & Wacker, M. (2018). Visualization of Orientations of Spatial Historical Photographs. Paper presented at the Eurographics Workshop on Graphics and Cultural Heritage.Bruschke, J., & Wacker, M. (2014). Application of a Graph Database and Graphical User Interface for the CIDOC CRM. Paper presented at the Access and Understanding-Networking in the Digital Era. Session J1. The 2014 annual conference of CIDOC, the International Committee for Documentation of ICOM.Burdea, G. C., & Coiffet, P. (2003). Virtual reality technology: John Wiley & Sons.Callieri, M., Cignoni, P., Corsini, M., & Scopigno, R. (2008). Masked photo blending: Mapping dense photographic data set on high-resolution sampled 3D models. Computers & Graphics, 32(4), 464-473.Chum, O., & Matas, J. (2005). Matching with PROSAC-progressive sample consensus. Paper presented at the Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on.Coordination and Support Action Virtual Multimodal Museum (ViMM). (2018). ViMM. Retrieved April 30, 2019, from https://www.vi-mm.eu/CultLab3D. (2019). CultLab3D. Retrieved April 30, 2019, from https://www.cultlab3d.deDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. Paper presented at the 2009 IEEE conference on computer vision and pattern recognition.Deutsches Archäologisches Institut (DAI). (2019). iDAI.objects arachne (Arachne). Retrieved April 30, 2019, from https://arachne.dainst.org/Efron, B., & Tibshirani, R. J. (1994). An introduction to the bootstrap: CRC press.Europeana. (2019). Europeana Collections. Retrieved 30.04.2019, from https://www.europeana.euEvens, T., & Hauttekeete, L. (2011). Challenges of digital preservation for cultural heritage institutions. Journal of Librarianship and Information Science, 43(3), 157-165.Fischler, M. A., & Bolles, R. C. (1981). Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6), 381-395.Fleming‐May, R. A., & Green, H. (2016). Digital innovations in poetry: Practices of creative writing faculty in online literary publishing. Journal of the Association for Information Science and Technology, 67(4), 859-873.Franken, T., Dellepiane, M., Ganovelli, F., Cignoni, P., Montani, C., & Scopigno, R. (2005). Minimizing user intervention in registering 2D images to 3D models. The visual computer, 21(8-10), 619-628.Girardi, G., von Schwerin, J., Richards-Rissetto, H., Remondino, F., & Agugiaro, G. (2013). The MayaArch3D project: A 3D WebGIS for analyzing ancient architecture and landscapes. Literary and Linguistic Computing, 28(4), 736-753. doi:10.1093/llc/fqt059Grussenmeyer, P., & Al Khalil, O. (2017). From Metric Image Archives to Point Cloud Reconstruction: Case Study of the Great Mosque of Aleppo in Syria. Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLII-2/W5, 295-301. doi:10.5194/isprs-archives-XLII-2-W5-295-2017Gutierrez, M., Vexo, F., & Thalmann, D. (2008). Stepping into virtual reality: Springer Science & Business Media.Guttentag, D. A. (2010). Virtual reality: Applications and implications for tourism. Tourism Management, 31(5), 637-651.Hartley, R., & Zisserman, A. (2003). Multiple view geometry in computer vision: Cambridge university press.Koutsoudis, A., Arnaoutoglou, F., Tsaouselis, A., Ioannakis, G., & Chamzas, C. (2015). Creating 3D Replicas of Medium-to Large-Scale Monuments for Web-Based Dissemination Within the Framework of the 3D-Icons Project. CAA2015, 971.Li, J., Hu, Q., & Ai, M. (2018). RIFT: Multi-modal Image Matching Based on Radiation-invariant Feature Transform. arXiv preprint arXiv:1804.09493.Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International journal of computer vision, 60(2), 91-110.Maietti, F., Di Giulio, R., Piaia, E., Medici, M., & Ferrari, F. (2018). Enhancing Heritage fruition through 3D semantic modelling and digital tools: the INCEPTION project. Paper presented at the IOP Conference Series: Materials Science and Engineering.Maiwald, F., Schneider, D., Henze, F., Münster, S., & Niebling, F. (2018). Feature Matching of Historical Images Based on Geometry of Quadrilaterals. ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XLII-2, 643-650. doi:10.5194/isprs-archives-XLII-2-643-2018Maiwald, F., Vietze, T., Schneider, D., Henze, F., Münster, S., & Niebling, F. (2017). Photogrammetric analysis of historical image repositories for virtual reconstruction in the field of digital humanities. The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 42, 447.Matas, J., Chum, O., Urban, M., & Pajdla, T. (2004). Robust wide-baseline stereo from maximally stable extremal regions. Image and Vision Computing, 22(10), 761-767.Melero, F. J., Revelles, J., & Bellido, M. L. (2018). Atalaya3D: making universities' cultural heritage accessible through 3D technologies.Milgram, P., Takemura, H., Utsumi, A., & Kishino, F. (1995). Augmented reality: A class of displays on the reality-virtuality continuum. Paper presented at the Telemanipulator and telepresence technologies.Mishkin, D., Matas, J., & Perdoch, M. (2015). MODS: Fast and robust method for two-view matching. Computer Vision and Image Understanding, 141, 81-93.Moulon, P., Monasse, P., & Marlet, R. (2012). Adaptive structure from motion with a contrario model estimation. Paper presented at the Asian Conference on Computer Vision.Münster, S., Kamposiori, C., Friedrichs, K., & Kröber, C. (2018). Image libraries and their scholarly use in the field of art and architectural history. International journal on digital libraries, 19(4), 367-383.Niebling, F., Bruschke, J., & Latoschik, M. E. (2018). Browsing Spatial Photography for Dissemination of Cultural Heritage Research Results using Augmented Models.Niebling, F., Maiwald, F., Barthel, K., & Latoschik, M. E. (2017). 4D Augmented City Models, Photogrammetric Creation and Dissemination Digital Research and Education in Architectural Heritage (pp. 196-212). Cham: Springer International Publishing.Oliva, L. S., Mura, A., Betella, A., Pacheco, D., Martinez, E., & Verschure, P. (2015). Recovering the history of Bergen Belsen using an interactive 3D reconstruction in a mixed reality space the role of pre-knowledge on memory recollection. Paper presented at the 2015 Digital Heritage.Pani Paudel, D., Habed, A., Demonceaux, C., & Vasseur, P. (2015). Robust and optimal sum-of-squares-based point-to-plane registration of image sets and structured scenes. Paper presented at the Proceedings of the IEEE International Conference on Computer Vision.Ross, S., & Hedstrom, M. (2005). Preservation research and sustainable digital libraries. International journal on digital libraries, 5(4), 317-324.Schindler, G., & Dellaert, F. (2012). 4D Cities: Analyzing, Visualizing, and Interacting with Historical Urban Photo Collections. Journal of Multimedia, 7(2), 124-131.Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). Grad-cam: Visual explanations from deep networks via gradient-based localization. Paper presented at the Proceedings of the IEEE International Conference on Computer Vision.Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.Slater, M., & Sanchez-Vives, M. V. (2016). Enhancing our lives with immersive virtual reality. Frontiers in Robotics and AI, 3, 74.Styliani, S., Fotis, L., Kostas, K., & Petros, P. (2009). Virtual museums, a survey and some issues for consideration. Journal of cultural Heritage, 10(4), 520-528.Tschirschwitz, F., Büyüksalih, G., Kersten, T., Kan, T., Enc, G., & Baskaraca, P. (2019). Virtualising an Ottoman Fortress - Laser Scanning and 3D Modelling for the Development of an Interactive, Immersive Virtual Reality Application. International archives of the photogrammetry, remote sensing and spatial information sciences, 42(2/W9).Web3D Consortium. (2019). Open Standards for Real-Time 3D Communication. Retrieved April 30, 2019, from http://www.web3d.org/Wu, C. (2013). Towards linear-time incremental structure from motion. Paper presented at the 3D Vision-3DV 2013, 2013 International conference on.Wu, Y., Ma, W., Gong, M., Su, L., & Jiao, L. (2015). A Novel Point-Matching Algorithm Based on Fast Sample Consensus for Image Registration. IEEE Geosci. Remote Sensing Lett., 12(1), 43-47.Yoon, J., & Chung, E. (2011). Understanding image needs in daily life by analyzing questions in a social Q&A site. Journal of the American Society for Information Science and Technology, 62(11), 2201-2213",space,99
10.17638/03058825,filtered,core,Multi-Agent Learning for Security and Sustainability,,core,https://core.ac.uk/download/237018740.pdf,,"This thesis studies the application of multi-agent learning in complex domains where safety and sustainability are crucial. We target some of the main obstacles in the deployment of multi-agent learning techniques in such domains. These obstacles consist of modelling complex environments with multi-agent interaction, designing robust learning processes and modelling adversarial agents. The main goal of using modern multi-agent learning methods is to improve the effectiveness of behaviour in such domains, and hence increase sustainability and security. This thesis investigates three complex real-world domains: space debris removal, critical domains with risky states and spatial security domains such as illegal rhino poaching. We first tackle the challenge of modelling a complex multi-agent environment. The focus is on the space debris removal problem, which poses a major threat to the sustainability of earth orbit. We develop a high-fidelity space debris simulator that allows us to simulate the future evolution of the space debris environment. Using the data from the simulator we propose a surrogate model, which enables fast evaluation of different strategies chosen by the space actors. We then analyse the dynamics of strategic decision making among multiple space actors, comparing different models of agent interaction: static vs. dynamic and centralised vs. decentralised. The outcome of our work can help future decision makers to design debris removal strategies, and consequently mitigate the threat of space debris. Next, we study how we can design a robust learning process in critical domains with risky states, where destabilisation of local components can lead to severe impact on the whole network. We propose a novel robust operator κ which can be combined with reinforcement learning methods, leading to learning safe policies, mitigating the threat of external attack, or failure in the system. Finally, we investigate the challenge of learning an effective behaviour while facing adversarial attackers in spatial security domains such as illegal rhino poaching. We assume that such attackers can be occasionally observed. Our approach consists of combining Bayesian inference with temporal difference learning, in order to build a model of the attacker behaviour. Our method can effectively use the partial observability of the attacker’s location and approximate the performance of a full observability case. This thesis therefore presents novel methods and tackles several important obstacles in deploying multi-agent learning algorithms in the real-world, which further narrows the reality gap between theoretical models and real-world applications",space,100
d43f0ad90234d1e52e429251216febdf32319939,filtered,semantic_scholar,Sensors,2021-01-01 00:00:00,semantic_scholar,a navigation and augmented reality system for visually impaired people †,https://www.semanticscholar.org/paper/d43f0ad90234d1e52e429251216febdf32319939,"In recent years, we have assisted with an impressive advance in augmented reality systems and computer vision algorithms, based on image processing and artificial intelligence. Thanks to these technologies, mainstream smartphones are able to estimate their own motion in 3D space with high accuracy. In this paper, we exploit such technologies to support the autonomous mobility of people with visual disabilities, identifying pre-defined virtual paths and providing context information, reducing the distance between the digital and real worlds. In particular, we present ARIANNA+, an extension of ARIANNA, a system explicitly designed for visually impaired people for indoor and outdoor localization and navigation. While ARIANNA is based on the assumption that landmarks, such as QR codes, and physical paths (composed of colored tapes, painted lines, or tactile pavings) are deployed in the environment and recognized by the camera of a common smartphone, ARIANNA+ eliminates the need for any physical support thanks to the ARKit library, which we exploit to build a completely virtual path. Moreover, ARIANNA+ adds the possibility for the users to have enhanced interactions with the surrounding environment, through convolutional neural networks (CNNs) trained to recognize objects or buildings and enabling the possibility of accessing contents associated with them. By using a common smartphone as a mediation instrument with the environment, ARIANNA+ leverages augmented reality and machine learning for enhancing physical accessibility. The proposed system allows visually impaired people to easily navigate in indoor and outdoor scenarios simply by loading a previously recorded virtual path and providing automatic guidance along the route, through haptic, speech, and sound feedback.",space,101
5d248aace06704234de122ac354a51f1630182b8,filtered,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,augmented worlds: a proposal for modelling and engineering pervasive mixed reality smart environments,https://www.semanticscholar.org/paper/5d248aace06704234de122ac354a51f1630182b8,"In recent years, the remarkable technological advancement has revolutionised the ICT panorama offering nowadays the opportunity to exploit several technological convergences to reduce the gulf existing between the physical and the digital matter, between the physical real world and every computational software system or application. Along with Pervasive Computing - entered in the mainstream with the concept of Internet of Things (IoT) - Mixed Reality (MR) is going to be an essential ingredient for the design and development of next future smart environments. In particular, in such environments is feasible to imagine that the computation will drive the augmentation of the physical space, and software will also be executed in a cyber-physical world, eventually populated with of (interactive) holograms. 
 
After an initial exploration of the state of the art about augmentation technologies both for humans and the environment, in this dissertation we present the vision of Augmented Worlds (AW), a conceptual and a practical proposal for modelling and engineering next future pervasive mixed reality smart environments as distribute, multi-user and cooperative complex software systems. 
On the one hand, a meta-model is formalised and opportunely discussed to offer to the literature a conceptual tool for modelling AWs. On the other hand, also a concrete infrastructure - called MiRAgE - is designed and developed to produce a platform for engineering and deploy such innovative smart environments. 
 
The work carried out in this dissertation fits into the scientific literature and research of Pervasive Computing and Mixed Reality fields. Furthermore, part of the contribution is related also to the area of Cognitive Agents and Multi-Agent Systems, due to the AWs orientation to be deeply connected to a layer involving autonomous agents able to observe and act proactively in the smart environment.",space,102
10.1109/aimv53313.2021.9670978,filtered,2021 International Conference on Artificial Intelligence and Machine Vision (AIMV),IEEE,2021-09-26 00:00:00,ieeexplore,voice controlled augmented reality for real estate,https://ieeexplore.ieee.org/document/9670978/,"As technology advances, augmented reality is becoming more prevalent in every business. The most frequent usage of AR is to project real things onto the user, which is usually done via an image target. In the real estate industry, no one can deny AR's capacity to improve the buying and selling experience. AR can help real estate developers expand their marketing methods and give clients a more memorable home experience. It has already been used in apps for house design and land hunting, and the industry's strike proves that Augmented Reality has a lot more to give. Everyone nowadays has a smartphone or tablet with which to access the internet, and the technology utilized in these devices is improving every day. As a result, using AR tools in everyday life and having a comfortable AR experience on mobile devices is becoming more convenient. The amount of time spent touring each site with consumers and not having appropriate resources to impress them is a common challenge that real estate developers confront. Augmented reality software is frequently the seal of approval that realtors receive in order to grow their business and overcome these obstacles. This paper proposes a method for projecting a home onto an image target and allowing the user to explore the interior of the house. Voice controllers incorporated into AR can control the interior.",e-commerce,103
10.1016/s0957-4174(00)00010-5,filtered,Expert Systems with Applications,scopus,2000-01-01,sciencedirect,next-generation agent-enabled comparison shopping,https://api.elsevier.com/content/abstract/scopus_id/0034187526,"Agents are the catalysts for commerce on the Web today. For example, comparison-shopping agents mediate the interactions between buyers and sellers in order to yield more efficient markets. However, today's shopping agents are price-dominated, unreflective of the nature of seller/buyer differentiation or the changing course of differentiation over time. This paper aims to tackle this dilemma and advances shopping agents into a stage where both kinds of differentiation are taken into account for enhanced understanding of the realities. We call them next-generation shopping agents. These agents can leverage the interactive power of the Web for more accurate understanding of buyer's preferences. This paper then presents an architecture of the next-generation shopping agents. This architecture is composed of a Product/Merchant Information Collector, a Buyer Behavior Extractor, a User Profile Manager and an Online Learning Personalized-Ranking Module. We have implemented a system following the core of the architecture and collected preliminary evaluation results. The results show this system is quite promising in overcoming the reality challenges of comparison shopping.",e-commerce,104
10.1371/journal.pone.0211314,filtered,core,'Public Library of Science (PLoS)',2019-01-29 00:00:00,core,automatic classification of human facial features based on their appearance,https://riunet.upv.es/bitstream/10251/160091/1/Fuentes-Hurtado%3bDiego-Mas%3bNaranjo%20-%20Automatic%20classification%20of%20human%20facial%20features%20based%20on%20th....pdf,"[EN] Classification or typology systems used to categorize different human body parts have
existed for many years. Nevertheless, there are very few taxonomies of facial features.
Ergonomics, forensic anthropology, crime prevention or new human-machine interaction
systems and online activities, like e-commerce, e-learning, games, dating or social networks, are fields in which classifications of facial features are useful, for example, to create
digital interlocutors that optimize the interactions between human and machines. However,
classifying isolated facial features is difficult for human observers. Previous works reported
low inter-observer and intra-observer agreement in the evaluation of facial features. This
work presents a computer-based procedure to automatically classify facial features based
on their global appearance. This procedure deals with the difficulties associated with classifying features using judgements from human observers, and facilitates the development of
taxonomies of facial features. Taxonomies obtained through this procedure are presented
for eyes, mouths and noses.Fuentes-Hurtado, F.; Diego-Mas, JA.; Naranjo Ornedo, V.; Alcañiz Raya, ML. (2019). Automatic classification of human facial features based on their appearance. PLoS ONE. 14(1):1-20. https://doi.org/10.1371/journal.pone.0211314S120141Damasio, A. R. (1985). Prosopagnosia. Trends in Neurosciences, 8, 132-135. doi:10.1016/0166-2236(85)90051-7Bruce, V., & Young, A. (1986). Understanding face recognition. British Journal of Psychology, 77(3), 305-327. doi:10.1111/j.2044-8295.1986.tb02199.xTodorov, A. (2011). Evaluating Faces on Social Dimensions. Social Neuroscience, 54-76. doi:10.1093/acprof:oso/9780195316872.003.0004Little, A. C., Burriss, R. P., Jones, B. C., & Roberts, S. C. (2007). Facial appearance affects voting decisions. Evolution and Human Behavior, 28(1), 18-27. doi:10.1016/j.evolhumbehav.2006.09.002Porter, J. P., & Olson, K. L. (2001). Anthropometric Facial Analysis of the African American Woman. Archives of Facial Plastic Surgery, 3(3), 191-197. doi:10.1001/archfaci.3.3.191Gündüz Arslan, S., Genç, C., Odabaş, B., & Devecioğlu Kama, J. (2007). Comparison of Facial Proportions and Anthropometric Norms Among Turkish Young Adults With Different Face Types. Aesthetic Plastic Surgery, 32(2), 234-242. doi:10.1007/s00266-007-9049-yFerring, V., & Pancherz, H. (2008). Divine proportions in the growing face. American Journal of Orthodontics and Dentofacial Orthopedics, 134(4), 472-479. doi:10.1016/j.ajodo.2007.03.027Mane, D. R., Kale, A. D., Bhai, M. B., & Hallikerimath, S. (2010). Anthropometric and anthroposcopic analysis of different shapes of faces in group of Indian population: A pilot study. Journal of Forensic and Legal Medicine, 17(8), 421-425. doi:10.1016/j.jflm.2010.09.001Ritz-Timme, S., Gabriel, P., Tutkuviene, J., Poppa, P., Obertová, Z., Gibelli, D., … Cattaneo, C. (2011). Metric and morphological assessment of facial features: A study on three European populations. Forensic Science International, 207(1-3), 239.e1-239.e8. doi:10.1016/j.forsciint.2011.01.035Ritz-Timme, S., Gabriel, P., Obertovà, Z., Boguslawski, M., Mayer, F., Drabik, A., … Cattaneo, C. (2010). A new atlas for the evaluation of facial features: advantages, limits, and applicability. International Journal of Legal Medicine, 125(2), 301-306. doi:10.1007/s00414-010-0446-4Kong, S. G., Heo, J., Abidi, B. R., Paik, J., & Abidi, M. A. (2005). Recent advances in visual and infrared face recognition—a review. Computer Vision and Image Understanding, 97(1), 103-135. doi:10.1016/j.cviu.2004.04.001Tavares, G., Mourão, A., & Magalhães, J. (2016). Crowdsourcing facial expressions for affective-interaction. Computer Vision and Image Understanding, 147, 102-113. doi:10.1016/j.cviu.2016.02.001Buckingham, G., DeBruine, L. M., Little, A. C., Welling, L. L. M., Conway, C. A., Tiddeman, B. P., & Jones, B. C. (2006). Visual adaptation to masculine and feminine faces influences generalized preferences and perceptions of trustworthiness. Evolution and Human Behavior, 27(5), 381-389. doi:10.1016/j.evolhumbehav.2006.03.001Boberg M, Piippo P, Ollila E. Designing Avatars. DIMEA ‘08 Proc 3rd Int Conf Digit Interact Media Entertain Arts. ACM; 2008; 232–239. doi: 10.1145/1413634.1413679Rojas Q., M., Masip, D., Todorov, A., & Vitria, J. (2011). Automatic Prediction of Facial Trait Judgments: Appearance vs. Structural Models. PLoS ONE, 6(8), e23323. doi:10.1371/journal.pone.0023323Laurentini, A., & Bottino, A. (2014). Computer analysis of face beauty: A survey. Computer Vision and Image Understanding, 125, 184-199. doi:10.1016/j.cviu.2014.04.006Alemany S, Gonzalez J, Nacher B, Soriano C, Arnaiz C, Heras H. Anthropometric survey of the Spanish female population aimed at the apparel industry. Proceedings of the 2010 Intl Conference on 3D Body scanning Technologies. 2010. pp. 307–315.Vinué, G., Epifanio, I., & Alemany, S. (2015). Archetypoids: A new approach to define representative archetypal data. Computational Statistics & Data Analysis, 87, 102-115. doi:10.1016/j.csda.2015.01.018Jee, S., & Yun, M. H. (2016). An anthropometric survey of Korean hand and hand shape types. International Journal of Industrial Ergonomics, 53, 10-18. doi:10.1016/j.ergon.2015.10.004Kim, N.-S., & Do, W.-H. (2014). Classification of Elderly Women’s Foot Type. Journal of the Korean Society of Clothing and Textiles, 38(3), 305-320. doi:10.5850/jksct.2014.38.3.305Sarakon P, Charoenpong T, Charoensiriwath S. Face shape classification from 3D human data by using SVM. The 7th 2014 Biomedical Engineering International Conference. IEEE; 2014. pp. 1–5. doi: 10.1109/BMEiCON.2014.7017382PRESTON, T. A., & SINGH, M. (1972). Redintegrated Somatotyping. Ergonomics, 15(6), 693-700. doi:10.1080/00140137208924469Lin, Y.-L., & Lee, K.-L. (1999). Investigation of anthropometry basis grouping technique for subject classification. Ergonomics, 42(10), 1311-1316. doi:10.1080/001401399184965Malousaris, G. G., Bergeles, N. K., Barzouka, K. G., Bayios, I. A., Nassis, G. P., & Koskolou, M. D. (2008). Somatotype, size and body composition of competitive female volleyball players. Journal of Science and Medicine in Sport, 11(3), 337-344. doi:10.1016/j.jsams.2006.11.008Carvalho, P. V. R., dos Santos, I. L., Gomes, J. O., Borges, M. R. S., & Guerlain, S. (2008). Human factors approach for evaluation and redesign of human–system interfaces of a nuclear power plant simulator. Displays, 29(3), 273-284. doi:10.1016/j.displa.2007.08.010Fabri M, Moore D. The use of emotionally expressive avatars in Collaborative Virtual Environments. AISB’05 Convention:Proceedings of the Joint Symposium on Virtual Social Agents: Social Presence Cues for Virtual Humanoids Empathic Interaction with Synthetic Characters Mind Minding Agents. 2005. pp. 88–94. doi:citeulike-article-id:790934Sukhija, P., Behal, S., & Singh, P. (2016). Face Recognition System Using Genetic Algorithm. Procedia Computer Science, 85, 410-417. doi:10.1016/j.procs.2016.05.183Trescak T, Bogdanovych A, Simoff S, Rodriguez I. Generating diverse ethnic groups with genetic algorithms. Proceedings of the 18th ACM symposium on Virtual reality software and technology—VRST ‘12. New York, New York, USA: ACM Press; 2012. p. 1. doi: 10.1145/2407336.2407338Vanezis, P., Lu, D., Cockburn, J., Gonzalez, A., McCombe, G., Trujillo, O., & Vanezis, M. (1996). Morphological Classification of Facial Features in Adult Caucasian Males Based on an Assessment of Photographs of 50 Subjects. Journal of Forensic Sciences, 41(5), 13998J. doi:10.1520/jfs13998jTamir, A. (2011). Numerical Survey of the Different Shapes of the Human Nose. Journal of Craniofacial Surgery, 22(3), 1104-1107. doi:10.1097/scs.0b013e3182108eb3Tamir, A. (2013). Numerical Survey of the Different Shapes of Human Chin. Journal of Craniofacial Surgery, 24(5), 1657-1659. doi:10.1097/scs.0b013e3182942b77Richler, J. J., Cheung, O. S., & Gauthier, I. (2011). Holistic Processing Predicts Face Recognition. Psychological Science, 22(4), 464-471. doi:10.1177/0956797611401753Taubert, J., Apthorp, D., Aagten-Murphy, D., & Alais, D. (2011). The role of holistic processing in face perception: Evidence from the face inversion effect. Vision Research, 51(11), 1273-1278. doi:10.1016/j.visres.2011.04.002Donnelly, N., & Davidoff, J. (1999). The Mental Representations of Faces and Houses: Issues Concerning Parts and Wholes. Visual Cognition, 6(3-4), 319-343. doi:10.1080/135062899395000Davidoff, J., & Donnelly, N. (1990). Object superiority: A comparison of complete and part probes. Acta Psychologica, 73(3), 225-243. doi:10.1016/0001-6918(90)90024-aTanaka, J. W., & Farah, M. J. (1993). Parts and Wholes in Face Recognition. The Quarterly Journal of Experimental Psychology Section A, 46(2), 225-245. doi:10.1080/14640749308401045Wang, R., Li, J., Fang, H., Tian, M., & Liu, J. (2012). Individual Differences in Holistic Processing Predict Face Recognition Ability. Psychological Science, 23(2), 169-177. doi:10.1177/0956797611420575Rhodes, G., Ewing, L., Hayward, W. G., Maurer, D., Mondloch, C. J., & Tanaka, J. W. (2009). Contact and other-race effects in configural and component processing of faces. British Journal of Psychology, 100(4), 717-728. doi:10.1348/000712608x396503Miller, G. A. (1994). The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological Review, 101(2), 343-352. doi:10.1037/0033-295x.101.2.343Scharff, A., Palmer, J., & Moore, C. M. (2011). Evidence of fixed capacity in visual object categorization. Psychonomic Bulletin & Review, 18(4), 713-721. doi:10.3758/s13423-011-0101-1Meyers, E., & Wolf, L. (2007). Using Biologically Inspired Features for Face Processing. International Journal of Computer Vision, 76(1), 93-104. doi:10.1007/s11263-007-0058-8Cootes, T. F., Edwards, G. J., & Taylor, C. J. (2001). Active appearance models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(6), 681-685. doi:10.1109/34.927467Ahonen, T., Hadid, A., & Pietikainen, M. (2006). Face Description with Local Binary Patterns: Application to Face Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(12), 2037-2041. doi:10.1109/tpami.2006.244Belhumeur, P. N., Hespanha, J. P., & Kriegman, D. J. (1997). Eigenfaces vs. Fisherfaces: recognition using class specific linear projection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(7), 711-720. doi:10.1109/34.598228Turk, M., & Pentland, A. (1991). Eigenfaces for Recognition. Journal of Cognitive Neuroscience, 3(1), 71-86. doi:10.1162/jocn.1991.3.1.71Klare B, Jain AK. On a taxonomy of facial features. IEEE 4th International Conference on Biometrics: Theory, Applications and Systems, BTAS 2010. IEEE; 2010. pp. 1–8. doi: 10.1109/BTAS.2010.5634533Chihaoui, M., Elkefi, A., Bellil, W., & Ben Amar, C. (2016). A Survey of 2D Face Recognition Techniques. Computers, 5(4), 21. doi:10.3390/computers5040021Ma, D. S., Correll, J., & Wittenbrink, B. (2015). The Chicago face database: A free stimulus set of faces and norming data. Behavior Research Methods, 47(4), 1122-1135. doi:10.3758/s13428-014-0532-5Asthana A, Zafeiriou S, Cheng S, Pantic M. Incremental face alignment in the wild. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 2014. pp. 1859–1866. doi: 10.1109/CVPR.2014.240Bag S, Barik S, Sen P, Sanyal G. A statistical nonparametric approach of face recognition: combination of eigenface &amp; modified k-means clustering. Proceedings Second International Conference on Information Processing. 2008. p. 198.Doukas, C., & Maglogiannis, I. (2010). A Fast Mobile Face Recognition System for Android OS Based on Eigenfaces Decomposition. Artificial Intelligence Applications and Innovations, 295-302. doi:10.1007/978-3-642-16239-8_39Huang P, Huang Y, Wang W, Wang L. Deep embedding network for clustering. Proceedings—International Conference on Pattern Recognition. 2014. pp. 1532–1537. doi: 10.1109/ICPR.2014.272Dizaji KG, Herandi A, Deng C, Cai W, Huang H. Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization. Proceedings of the IEEE International Conference on Computer Vision. 2017. doi: 10.1109/ICCV.2017.612Xie J, Girshick R, Farhadi A. Unsupervised deep embedding for clustering analysis [Internet]. Proceedings of the 33rd International Conference on International Conference on Machine Learning—Volume 48. JMLR.org; 2016. pp. 478–487. Available: https://dl.acm.org/citation.cfm?id=3045442Nousi, P., & Tefas, A. (2017). Discriminatively Trained Autoencoders for Fast and Accurate Face Recognition. Communications in Computer and Information Science, 205-215. doi:10.1007/978-3-319-65172-9_18Sirovich, L., & Kirby, M. (1987). Low-dimensional procedure for the characterization of human faces. Journal of the Optical Society of America A, 4(3), 519. doi:10.1364/josaa.4.00051",e-commerce,105
,filtered,core,Digital Commons @ University of South Florida,2021-08-01 00:00:00,core,advances in global services and retail management: volume 2,,"This is the second volume of the Advances in Global Services and Retail Management Book Series. This volume has the following parts:   Part 1: Hospitality and Tourism Part 2: Marketing, E-marketing, and Consumer Behavior Part 3: Management Part 4: Human Resources Management Part 5: Retail Management Part 6: Economics Part 7: Accounting and Finance Part 8: Sustainability and Environmental Issues Part 9: Information Technology 
ISBN: 978-1-955833-03-5  Hospitality and Tourism  Significance of VR in the spa: A spatial analysis  Irini Lai Fun Tang, Schultz Zhi Bin Xu, and Eric Chan   Social media marketing in rural hospitality and tourism destination research  Samuel Adeyinka-Ojo and Shamsul Kamariah Abdullah   All aboard! Is space tourism still a fantasy or a reality: An investigation on Turkish market  Emrah Tasarer, Vahit Oguz Kiper, Orhan Batman, and Oguz Turkay   Strategic consciousness and business performance relationship of open innovation strategies in food and beverage businesses  Muhsin Halis, Kazim Ozan Ozer, Hasan Cinnioglu, and Zafer Camlibel   The effects of COVID-19 epidemic on guided tours and alternative tour samples from Turkey  Bayram Akay   The effect of COVID-19 phobia on holiday intention  Halil Akmese and Ali Ilgaz    The effect of the usage of virtual reality in tourism education on learning motivation  Sarp Tahsin Kumlu and Emrah Ozkul   The impact of effective implementation of customer relationship management to the success of hotels in Afikpo North local government of Ebonyi State, Nigeria  Ogboagha Callister and Managwu Lilian   The influence of study travel on quality-oriented education: The case of Handan, China  Wang Jingya and Alaa Nimer Abukhalifeh   The impact of U.S. Cuba policies on Cuban tourism industry: Focus on the Obama and Trump Administration  Jukka M. Laitamaki, Antonio Diaz Medina, and Lisandra Torres Hechavarria   Determination of students’ characteristics and perspectives about social entrepreneurship: A case of Anadolu University  Muhammed Kavak, Ipek Itir Can, and Emre Ozan Aksoz   The place of Kazakhstan tourism sector in the countries of the region in terms of transportation infrastructure  Maiya Myrzabekova, Muhsin Halis, and Zafer Camlibel   What are tour guides most praised for? A sharing economy perspective  Derya Demirdelen-Alrawadieh and Ibrahim Cifci   An examination of representations for USA in tourism brochures for Chinese market  Yasong Wang   An exploratory study on cognitive internship perception of tourism students  Ozge Buyuk and Gulsah Akkus   Are you afraid to travel during COVID-19?  Gulsum Tabak, Sibel Canik, and Ebru Guneren   Destination management during the health emergency: A bibliometric analysis  Valentina Della Corte, Giovanna Del Gaudio, Giuliana Nevola, Enrico Di Taranto, and Simone Luongo   Determination of food neophobia levels of International Mersin Citrus Festival participants  Sevda Sahilli Birdir, Nurhayat Iflazoglu, and Kemal Birdir   Analysis of effectiveness of industrial exposure training undertaken by students of hospitality management in star hotels  G. Saravana Kumar   Conceptualization of ecotourism service experiences framework from the dimensions of motivation and quality of experiences: Four realms of experience approach  Jennifer Kim Lian Chan   Does Coronavirus (COVID-19) transform travel and tourism to automation (robots)?  M. Omar Parvez, Ali Ozturen, and Cihan Cobanoglu   Efficiency of internal control systems and the effect of organizational structure and culture on internal control systems in accommodation industry  Kadriye Alev Akmese and Ali Ilgaz   Ethical perceptions of housekeeping department employees: A study in Izmir Province  Tuba Turkmendag and Bayram Sahin   Factors that prevent participation of tourists in online co-creation activities  Resat Arica, Feridun Duman, and Abdulkadir Corbaci   Health sector after COVID-19: Salt thermal facilities example  Azize Serap Tuncer and Sinan Bulut   PRISMA statement and thematic analysis framework in hospitality and tourism research  Samuel Adeyinka-Ojo   Evaluation of Turkish nights as a tourism product: The case of Cappadocia  Meral Buyukkuru, Eda Ozgul Katlav, and Firdevs Yonet Eren   Customer perceptions against COVID-19 precautionary measures of the restaurants: The case of Istanbul-Turkey  Elif Kaymaz and Sevki Ulema   Analysis of e-complaints regarding hotel restaurants during COVID-19 process: The case of Antalya  Sevim Usta and Serkan Sengul    Marketing, E-marketing, and Consumer Behavior  Materialistic social consumption amidst COVID-19 pandemic: Terror management theory in the Malaysia context  Seong-Yuen Toh and Siew-Wai Yuan   A conceptual framework for the mediating role of the flow experience between destination brand experience and destination loyalty  Ipek Kazancoglu and Taskin Dirsehan   Investigating drivers influencing choice behaviour of Islamic investment products  Hanudin Amin   Local food festivals within the scope of destination branding  Hatice Akturk and Atilla Akbaba   Marketing a destination on social media: Case of three municipalities of Izmir  Huseyin Ozan Altin and Ige Pirnar   Perceived usefulness, ease of use, online trust and online purchase intention: Mediating role of attitude towards online purchase  Muhammed Yazeed, Mohammed Aliyu Dantsoho, and Adamu Ado Abubakar    Social media framework for businesses  Nawel Amrouche   Social media marketing the African door of return experience in Badagry-Nigeria  Huseyin Arasli, Maryam Abdullahi, and Tugrul Gunay   The effect of corporate social responsibility on consumer-based brand equity: A research on automobile brands  Ali Koroglu and Ibrahim Avci   The effect of superstitions on consumer luck, horoscope and evil eye-oriented purchasing behavior: A study in Turkey  Ibrahim Avci and Salih Yildiz   The evaluation of S-D orientation on service innovation and performance of airline  Inci Polat and Ozlem Atalik   Brand new leisure constraint: COVID-19  Guliz Coskun    The impact of consumers price level perception on emotions towards supermarkets  Abdulcelil Cakici and Sena Tekeli   The impact of TikTok’s plastic surgery content on adolescents’ self-perception and purchase intention  Markus Rach   Accelerated modernity: What are the social media stories undergraduate students engage with?  Pericles Asher Rospigliosi and Sebastian Raza-Mejia   Virtual influencer as celebrity endorsers  Fanny Cheung and Wing-Fai Leung   Does millennial shopping orientation using augmented reality enabled mobile applications really impact product purchase intention?  Anil Kumar   Exposure to e-cigarette marketing and product use among highly educated adults  Onur Sahin   Extending the theory of planned behavior to explain intention to use online food delivery services in the context of COVID -19 pandemic  Ahmed Chemseddine Bouarar, Smail Mouloudj, and Kamel Mouloudj   Factors affecting investors’ buying decision in real estate market in Northern Cyprus  Gurkan Arslan and Karen Howells   From home to the store: Combined effect of music and traffic on consumers shopping behaviour  Luigi Piper, Lucrezia Maria de Cosmo, Maria Irene Prete, and Gianluigi Guido   Market expansion and business growth from the perspective of resources and capabilities: The case of a micro-enterprise  Jose G. Vargas-Hernandez and Omar C. Vargas-Gonzalez   How learning style interacts with voice-assisted technology (VAT) in consumer task evaluation  Bonnie Canziani and Sara MacSween     Effect of brand credibility and innovation on customer based brand equity and overall brand equity in Turkey: An investigation of GSM operators  Suphan Nasir and Ozge Guvendik   Value chain for a B school in India  Vimal Chandra Verma and Devashish Das Gupta    Management  AI as a boost for startups companies: Evidence from Italy  Irene Di Bernardo, Marco Tregua, Greco Fabio, and Ruggiero Andrea   The role of quality management applications for corporate reputations  Ibrahim Sapaloglu and Isik Cicek   Toxicity in organizations: A sample study on the perceived toxicity in Turkish academicians  Mustafa Hakan Atasoy and Muhsin Halis    Which resources are matter to healthcare performance? A case study on Bahrain  Mahmood Asad Ali and Mohamed Sayed Abou Elseoud   Case study: HereWay Inc. European expansion: A facility location problem  Mikhail M. Sher, Michael T. Paz, and Donald R. (Bob) Smith   In search of the effective mission statement: Structural support of the firm’s culture to augment financial performance  Seong-Yuen Toh   Innovation labs to support tourism organization in transforming crisis into opportunities: Insight from a case study  Francesco Santarsiero, Daniela Carlucci, and Giovanni Schiuma   Novelty and success of healthcare service innovation: A comparison between China and the Netherlands  Yu Mu, Rujun Wang and Ying Huang   Public private partnership in selected countries: A comparative analysis   Bekir Parlak and Abdullahi Suleiman Hashi   Strategic orientation of service enterprises towards customers  Korhan Arun and Saniye Yildirim Ozmutlu   The effects of organizational culture on information sharing attitude  Mohammadi Lanbaran Nasrin and Cicek Isik   The impact of industry 4.0 strategy on the work-life balance of employees  Ali Sukru Cetinkaya   The mediating effect of psychological empowerment on inclusive leadership and innovative work behaviour: A research in hotels  Emete Toros, Ahmet Maslakci, and Lutfi Surucu   Assessment of industry 4.0 on manufacturing enterprises: Demographic perspective  Ali Sukru Cetinkaya and M. Kemal Unsacar    Human Resources Management  Affective commitment in new hires’ onboarding? The role of organizational socialization in the fashion retail industry  Pui Sze Chan, Ho Ching Ching, Pui Yi Ng, and Annie Ko   Do burnout perception levels of nurses working in the health sector differ according to demographic characteristics?  Irfan Akkoc and Korhan Arun   Examining a moderating effect of employee turnover between recruitment and selection practice and organizational performance in Maldives civil service sector  Fathmath Muna, Azam S. M. Ferdous, and Ahmad Albattat   Personnel relationships in the workplace  Ali Sukru Cetinkaya, Shafiq Habibi, and Umut Yavuz   The evolution of human resources empowerment theory: A literature review (1970–2020)  Theodoros Stavrinoudis and Moschos Psimoulis   Teamwork, satisfaction and mediating effect of affective, continuance and normative commitments on employee’s loyalty  Thalita Aparecida Costa Nicolleti, Eduardo Roque Mangini, Leonardo Aureliano-Silva, Cristiane Sales Pires, and Carolina Aparecida de Freitas Dias   Perceptions of teachers in educational institutions regarding the principles of teaching professional ethics  Gulsah Aki, Nejat Ira, and Hasan Arslan   Influence of psychological empowerment on employee competence in Nigerian universal basic education system: The mediating role of work engagement  Isah Sani, Rashidah Binti Mohammad Ibrahim, and Fazida Karim    Retail Management  Artificial intelligence in retailing  Ibrahim Kircova, Munise Hayrun Saglam, and Sirin Gizem Kose    Customer value in retailing (2000-2020): A narrative review and future research directions  Rajat Gera and Ashish Pruthi   Effect of social media marketing on online retail performance of Konga Nigeria LTD  Abubakar Ado Adamu, Muhammed Yazeed, Mohammed Aliyu Dantsoho, Jamilu Abdulkadir, and Aliyu Audu Gemu   Employment of blue-collar workers in organized retail sector: The case of Turkey  Inci Kayhan-Kuzgun   Saving grace: Digitization to stay or address crisis?   Smitha Vasudevan   Inclusion of disabled consumers in online retail landscape: Web accessibility conformance of Turkish organized food retailers’ web sites  Asiye Ayben Celik   A customer segmentation model proposal for retailers: RFM-V  Pinar Ozkan and Ipek Deveci Kocakoc    Economics  Nigeria’s economic management: Reflections through monthly interest rate movement from 1996 to 2020 and beyond  Job Nmadu, Halima Sallawu, and Yebosoko Nmadu   A qualitative study of perceptions of the residents of Sidon, Lebanon regarding the economic effect on Sidon with reference to repatriation of the Palestinian refugees  Raja El Majzoub and Karen Howells   Three keys of development: Knowledge, efficiency and innovative entrepreneurship  Irfan Kalayci, Ali Soylu, and Baris Aytekin   Tourism and women empowerment: Empirical findings from past experience and predictions for the post-COVID era  Burcu Turkcan   COVID-19 effect on FDI motivation and their impact on service sector: Case of Georgia  Vakhtang Charaia and Mariam Lashkhi   Economic cooperation between Central Caucasus, China, and EU, under COVID-19 challenges  Vakhtang Charaia and Mariam Lashkhi   Effect of real exchange rate and income on international tourist arrivals for Turkey  Erhan Aslanoglu, Oral Erdogan, and Yasin Enes Aksu   Innovative entrepreneurship in Turkey: Micro and macro perspectives  Irfan Kalayci, Baris Aytekin, and Ali Soylu   Optimal fiscal and price stability in Germany: Autoregressive distributed lags (ARDL) cointegration relationship  Ergin Akalpler and Dahiru Alhaji Birnintsabas   Struggle with COVID-19 crisis within the scope of financial national security: The example of the Republic of Turkey  Silacan Karakus   The nexus between fiscal freedom and investment freedom: The case of E7 countries  Mehmet Bolukbas   To be or not to be a female entrepreneur in the Mexicali Valley  Roberto Burgueno Romero and Jose David Ledezma Torrez    Accounting and Finance  Comparative measurement of working capital efficiency for Borsa Istanbul restaurants and hotels for the COVID-19 period and previous quarters  Fatih Gunay and Gary Cokins   Relationship between business confidence index and non-financial firms foreign exchange assets and liabilities: Evidence from ARDL bound approach  Ilkut Elif Kandil-Goker   The impact of RTGS on internal control - A comparative study between some Iraqi banks  Salowan H. Al Taee and Noor A. Radhi   The impact of working capital on cash management under IAS 7 framework: An examination of tourism listed companies in Indonesia and Turkey  Tri Damayanti and Tuba Derya Baskan     A nexus between mergers & acquisitions and financial performance of firms: A study of industrial sector of Pakistan  Fiza Quareshi, Mukhtiar Ali, and Salar Hussain   Decentralized approach to deep-learning based asset allocation  Sarthak Sengupta, Priyanshu Priyam, and Anurika Vaish    Sustainability and Environmental Issues  Blockchain technology applied to the Consortium Etna DOC to avoid counterfeiting  Matarazzo Agata, Edoardo Carmelo Spampinato, Sergio Arfo, Ugo Sinigaglia, Antonino Bajeli, and Salvino Benanti   Eco-label certification, hotel performance and customer satisfaction: Analysis of a case study and future developments  Michele Preziosi, Alessia Acampora, Roberto Merli, and Maria Claudia Lucchetti   The integration of circular economy in the tourism industry: A framework for the implementation of circular hotels  Martina Sgambati, Alessia Acampora, Olimpia Martucci, and Maria Claudia Lucchetti   Using the theory of planned behavior to explore green food purchase intentions  Katrina Anna Auza and Kamel Mouloudj   Survey on purchasing methods of food products in Tarragona and Catania  Matarazzo Agata, Vazzano Tommaso Alberto, and Squillaci Carmelo    Information Technology  Comparative analysis of tools for matching work-related skill profiles with CV data and other unstructured data  Florian Beuttiker, Stefan Roth, Tobias Steinacher, and Thomas Hanne   State-of-the-art next generation open innovation platforms  Murielle De Roche, Monika Blaser, Patrick Hollinger, and Thomas Hanne   The coverage of AIOT based functional service: Case study of Asian futuristic hotel  Gege Wang, Irini Lai Fun Tang, Eric Chan, and Wai Hung Wilco Chan   The effect of the blockchain technology on service companies and food retailers: An overview of the blockchain use cases and applications  Gokhan Kirbac and Erkut Ergenc   The regulation problem of cryptocurrencies  Lamiha Ozturk and Ece Sulungur   Understanding information technology acceptance by physicians: Testing technology acceptance model  Anuruddha Indika Jagod",finance,106
,filtered,core,,2019-02-06 00:00:00,core,"an innovative approach for risk assessment in archaeology based on machine learning. a swiss case study. quantitative approaches, spatial statistics and socioecological modelling",https://core.ac.uk/download/224831956.pdf,"In a world that is more and more complex and full of artificial, we strike to master the changes which continuously affect every single domain of our reality and especially challenge human intelligence and cognitive capacities. The study of artificial intelligence (AI) is being exhibited as a core strategy to meet growing demands of science and applications, to solve complex problems in various areas such as environmental science, finance, health-sector, etc. Machine learning (ML) is as a subfield of AI, mainly concerned with the development of techniques and algorithms that allow computers to learn from data. In an innovative way, this work intends to survey and demonstrate the effectiveness of bringing together traditional archaeological questions, such as the analysis of settlement patterns and past human behavior, with cutting edge technologies related to Machine Learning. Computations were carried out using R free software environment for statistical computing and graphics; data pre- and post-processing was performed in a GIS (Geographical Information System) environment. We provide a data-driven basis example of archaeological predictive modeling (APM) for the Canton of Zurich, Switzerland. Namely, a dataset of known archaeological sites of the Roman period was considered. The APM represents an automated decision making and probabilistic reasoning tool, relevant for archaeological risk assessment and cultural heritage management. We adopted Random Forest (RF) (Breiman, 2001), an ensemble ML algorithm based on decision trees. The model is capable of learning from data and make predictions starting from the acquired knowledge through the modelling of the hidden relationships between a set of input (i.e. geo-environmental features prone to influence site locations) and output variables (i.e. the archeological sites).As result, we obtained: 1) a ranking of geo-environmental features influencing the archeological site occurrence; 2) a map of probability expressing the likelihood of archaeological site presence, at different locations in a given landscape. These outputs become important not only to verify the reliability of the data, but also to stimulate experts in different ways: they are elicited to characterize the benefits and constraints of using such techniques and ultimately to think big about archaeological data",finance,107
,filtered,core,"Hunter Library Digital Collections, Western Carolina University, Cullowhee, NC 28723;",2018-04-11 00:00:00,core,"the reporter, june 1998",,"The Reporter is a publication produced by Western Carolina University featuring news, events, and campus community updates for faculty and staff. The publication began in August of 1970 and continues digitally today. Click on the link in the “Related Materials” field to access recent issues.The Reporter
News for the Faculty and Staff of Western Carolina University
June 24, 1998
Food Services to
Switch July 1
See you at Starbucks!
Cullowhee, North Carolina
Western's Learning Communities
Helping Freshmen Find a Niche
Jr Dining in takes on
a new dimension
July 1 at Western
with a changeover4
in food services. ARAMARK
Corp. has been selected to operate
the university's campus dining
services, including those at
Dodson and Brown cafeterias, the
food court areas at Hinds
University Center and Dodson,
concessions at the Ramsey
Regional Activity Center, and
catering services throughout
the campus.
Among the changes expected
under ARAMARKs management
are new menus and formats
providing a wide variety of student
dining options. These include addi­tional
nationally known franchises,
such as Starbucks Coffee and Little
Ceasars Pizza, as well as demon­stration
cooking areas where
customers can watch as their food
is prepared, and expansion of full
menu cafeteria-style service and
""grab-and-go"" items.
Clete Myers, operations
manager of food services at
Clemson University, will become
general manager of campus dining
services at Western. ARAMARK
hopes to retain personnel currently
employed in WClFs food service
when the management change
occurs.
Retention Services Director Susan Clarke Smith (far right) and Peer Mentor
Bryan Dodge (center), meet entering LC students.
""An institution's capacity to retain students is directly related to its ability to reach out and make
contact with students and integrate them into the social and intellectual fabric of institutional life.
It hinges on the establishment of a healthy, caring environment which enables individuals to find a
niche in the social and intellectual communities of the institution."" —VINCENT TINTO IN LEAVING COLLEGE
a ±JL good many of us came of age
when linking the notion of caring
with any institution would have been
greeted with skepticism—if not out­right
scorn. But Western, changing
with, and in many instances ahead of,
the times, will set the standard for
high tech and high touch this fall.
By embracing the idea of learning
communities as part of its freshman
year experience program, Western
makes use of a proven method for
meeting both the individual's need
to belong and the institution's need
to retain.
Randomly selected as participants
in the Learning Community (or LC
Pilot) project beginning this fall, some
170 of Western's estimated 1,200
first-year students will live together
and learn together in classes linked to
their common interests and conduct­ed
by a core group of instructors—all
in an effort to address the problems
presented by the transition from high
school to college. Meanwhile, the
institution will look closely to see if
the initiative addresses its own prob­lems
associated with an unusually
high rate of student attrition,
particularly after the first year.
Frank Prochaska, associate vice
chancellor for academic affairs and
chief architect of the LC Pilot project,
believes that establishing learning
communities at Western, even on a
limited basis initially, will deliver on
both counts. The numbers would
appear to bear him out, with
universities where LCs have become
a standard for first-year programs
reporting significantly higher GPAs
and retention rates among students
who participate.
Elizabeth Shelly coordinates the
Freshman Year Experience Program
through Student Affairs and is herself
a product of an undergraduate LC.
Shelly describes it as setting out in
college life with an ""instant group of
friends"" to go to at any time for
support. She explains that Western's
approach to the idea conforms to a
pretty standard model but incorporates
some new features made possible by
our unique environment.
The pilot group is divided into eight
communities of about twenty students
each, with several of the communities
grouped according to a declared major
or according to undergraduate college.
Three LCs are made up of freshmen
undecided upon majors. Each LC will
be housed in a suite-style layout in
Walker Hall with common areas set
aside for studying and group
activities. In addition to a resident
assistant on the floor, each commu­nity
will be assigned an upperclass
peer mentor who will live with the
group and work with the USI130
class designed for it.
A revamped USI course is key to
the LC Pilot, according to Prochaska.
The USI 130 course for the individual
communities will be co-taught by a
faculty member and a student affairs
professional, both trained in freshman
issues. Themes introduced for
discussion in this course will carry
over to the other General Education
courses linked for the purposes of each
community and taught by specially
selected instructors. For example,
Instructor Nory Prochaska's USI 130
section, and its linked English,
continued on page 2
Learning, continued
computer science, and math courses,
will examine the advantages and
disadvantages of the ""virtual
university"" idea, from the perspective
of LC students who have a particular
interest in technology and a close
comfort with using electronic media.
Susan Clarke Smith, director of
Retention Services, sees the linked
courses as an effective means of
addressing the ""intimidation factor,""
identified by most retention experts
such as Vincent Tinto as one of the
main reasons students leave college.
Smith asserts that by establishing
opportunities for interaction in and
out of the classroom, students and
faculty can begin to ""break down
barriers."" Smith says, ""Students can
see that their professors are human,
and faculty can feel less hesitation, as
part of a wider network of support, to
communicate concern on a more
personal level.""
To some extent, we need look no
farther than our own campus for an
example of a learning community
beginning to deliver on its promise.
Now entering its second year, The
Honors College combines an academic
emphasis with a residential and social
component. Brian Railsback, acting
dean of the college, calculates a 92
percent fall-spring retention rate for
honors freshmen for 1997-98. (The
university's rate for freshmen was
83 percent.) ""That figure,"" Railsback
says, ""obviously points to a good bond
and a real desire for these students to
stay together.""
Perhaps the most ambitious and
the most advantageous aspects of
initiating a learning community
program now come to a confluence
with the computer implementation
project, also spearheaded by
Prochaska. Student Affairs Vice
Chancellor Robert Caruso sees the
efforts as quite extraordinary. ""The
added component of new technologies
in the classroom and the residence
halls will revolutionize our whole
notion of community,"" Caruso says.
""With something on the order of only
forty higher education institutions
out of about 2,800 nationwide that
have a computer requirement, I think
everyone is going to be looking to
Western as a model for creating the
learning community of the twenty-first
century.""
Michael Dougherty
Named Dean of
Education, Allied
Professions
Carolina
Board of
Governors.
Associate
dean of the
College of
Education
and Allied
Professions
since 1996, Dougherty is a professor
in the Department of Human
Services and is a former head of the
department. He earned his bachelor's
degree from the University of Notre
Dame in 1968, master's degrees from
Oakland University in 1970 and
Notre Dame in 1971, and doctoral
degree from Indiana State University
in 1974. He has been a member of the
human services faculty since 1976.
The 1988 recipient of Western's
Paul A. Reid Distinguished Service
Award for Faculty, he also has been
nominated for the Chancellor's
Distinguished Teaching Award and
the Taft Botner Award for Superior
Teaching. Prior to coming to Western,
he was a teacher and counselor in
public schools in Detroit; Mattoon, 111.;
and Taylor County, Fla.
Dougherty is a member of several
professional organizations, including
the American Counseling Association
and the Association for Educational
and Psychological Consultation. His
research activities have focused on
study skills and locus of control, the
effects of counseling techniques on
incarcerates, and consultation styles.
Dougherty's appointment will be
effective July 1, upon the retirement
of Chambers, who served seventeen
years as the college's dean.
Highlighting the summer's cultural
events on campus are this weekend's
concerts by the Atlanta Ballet and the
Cassatt String Quartet.
Hailed as one of the nation's
outstanding young ensembles, the
Cassatt String Quartet will perform in
recital at 8 p.m. this Friday and will
also perform as part of the Atlanta
Ballet programs at 2 p.m. and 8 p.m.
on Saturday. All performances are in
Hoey Auditorium.
The oldest continually operating
ballet company in the United States,
the acclaimed Atlanta Ballet travels to
the mountains each year for a summer
residency. The two performances on
Saturday come in conjunction with
Western's hosting the second annual
Atlanta Ballet Centre for Dance Educa­tion
summer dance camp. Selected
participants in the June 14-July 4
camp, which attracts more than fifty of
the top ballet students in the Southeast,
will share the stage with the
professional dancers for one piece.
The ballet programs on Saturday
include ""Intermezzo,"" featuring three
couples in an intricate series of
dances set to music by Johannes
Brahms; ""Prisma,"" featuring music
by Charles Ives and Atlanta Ballet
executive/music director Robert
Chumbley performed by the Cassatt
String Quartet; and ""II Distrato,"" an
abstract ballet in five movements
demonstrating how the different
parts of the dancer's body work
separately and as a unit.
The Cassatt String Quartet's
Friday program features music for
strings by Beethoven and Ravel.
Admission for the quartet's recital
is $10 for adults, $5 for WCU
students and children. Admission for
the ballet's performances is $20 for
adults and $5 for WCU students and
children. For tickets, call 227-7397.
Architects Tapped for Construction Projects
A pair of major construction projects planned for Western moved closer to reality
recently with the board of trustees' selection of two Charlotte architecture firms
to design an expansion of the university center and a federally funded workforce
development facility.
The trustees named Lee Nichols Hepler Architecture of Charlotte to design
the expansion of the Hinds University Center and Jenkins-Peer Architects of
Charlotte to design the proposed new Western North Carolina laborforce high-technology
education and training center.
The Hinds University Center project is the second in a three-phased
expansion effort designed to enhance the quality of student life at WCU. Plans
call for the construction of approximately 31,000 additional square feet to add a
retail shopping area, a movie theater, increased meeting and office space for
student organizations, and a multicultural center. Preliminary estimates for the
expansion set the cost at about $4.5 million.
The regional high-tech workforce training center, announced last fall by U.S.
Rep. Charles Taylor, is designed to help raise the economic potential of the
region by improving the availability of high-technology education to the
mountains' workforce. The center could include such high-tech training tools as
an industrial laser lab, artificial intelligence lab, geographic information lab,
robotics training, and sound and video production facilities complete with digital
editing capabilities. The facility, pending funding from Congress and the federal
Economic Development Administration, is expected to be built adjacent to the
Belk Building.
Michael Dougherty, associate dean of
the College of Education and Allied
Professions, has been named by
WClFs board of trustees to succeed
Gurney E. Chambers as dean.
Appointment of Dougherty to the
dean's post was approved by the
board Wednesday, June 10, at its
quarterly meeting. The appointment
is subject to the approval of the
University
I of North
WCU Campus
Plays Host to
Weekend of CI
Music and
June 24,1998 • T he Reporter • p age 2
University Awards Top Honors for Teaching, Research, and Service
Western's top faculty and staff
awards for teaching, research, and
service for the 1997-98 academic
year were presented at the annual
spring General Faculty and Awards
Convocation in May.
Mary C. ""Katie"" Ray, assistant
professor of elementary and middle
grades education, won the
Chancellor's Distinguished Teaching
Award. The Paul A. Reid Distin­guished
Service Award for faculty
went to Gordon Mercer, professor
of political science and public
affairs, and the Paul A. Reid
Distinguished Service Award for
administrative staff went to
Stephen White, sports information
director. David J. Butcher,
associate professor of chemistry,
received the University Scholar
Award for distinguished scholarly
achievement.
The honors, presented by
Chancellor John Bardo, carry $1,000
cash awards and engraved plaques
for each recipient. Bardo also
presented the Academic Award of
Excellence to the Department of
English. The award provides
$10,000 for program and staff
development.
Chancellor's Distinguished
Teaching Award
Katie Ray joined WCU's faculty
in 1994 after seven years of teaching
in elementary and middle schools in
New York City. In presenting the
award, Bardo quoted from Ray's
comments to the awards selection
committee.
""For students to be great
teachers, they must be passionate
about living and learning,"" Ray said.
""Consequently, I must show
students by my own passion. The
challenge I face every day is not to
know about good teaching, but to
demonstrate it in every interaction I
have with my students.""
Paul A. Reid
Distinguished Service
Award—Faculty
Gordon Mercer has been
a member of the WCU
faculty since 1980. Among
his accomplishments are the
creation of the annual
Undergraduate Research
| Conference and the
organization of faculty
forum assemblies to foster
communication about
athletic director. He received twenty-six
publication awards from the
College Sports Information Directors of
America, and eight Football Writers
Association of America awards for
""Outstanding Press Box Service.""
Following his retirement on June 30,
integrate computers and new
technology with writing and research
in a way that reflects ""real-world""
practices. The department has become
a primary user of the electronic
classrooms, and during the 1997-98
academic year, every freshman
years of continuous service leave
from usual work commitments to
pursue concentrated scholarly work.
Recipients are chosen on a competi­tive
basis by a faculty committee.
important campus issues.
He has held leadership
positions in WCU's Faculty
Senate and the University of
North Carolina's Faculty Assembly.
Mercer also has been described as ""a
superior teacher"" by his students and
has been nominated frequently for
campus teaching awards.
Paul A. Reid Distinguished Service
Award—Staff
Steve White will retire at the end of
June as WCU's sports information
director. A 1967 graduate of WCU,
White has also served as associate
Distinguished Teacher Katie Ray, University Scholar David Butcher, and Reid Service honoree
Gordon Mercer receive their awards from Chancellor John Bardo.
White will head the Catamount Sports
Network, Inc., the radio broadcaster of
WCU's football and basketball games.
University Scholar Award
David Butcher joined WCU's faculty
in 1990. An analytical chemist, he has
received several external grants and
has published numerous articles on his
scientific research, which includes the
atomic absorption spectroscopy, atomic
fluorescence spectrometry, and the
search for potential
chemical causes for
Sports Information Director Steve White receives the Reid
Service Award for Staff from Chancellor John Bardo.
of Excellence, Bardo
praised the
Department of
English for its
innovative work to
studied composition in the electronic
environment.
Other major awards recognized at
the convocation were the Beyond the
Classroom Teaching Award and the
Scholarly Development Assignment
Program Awards.
The Beyond the Classroom
Teaching Award is given to an
academic teaching unit that excels
in enhancing students' learning
through such activities outside the
classroom as mentoring programs,
effective academic advising or
cooperative learning experience. The
1998 winner of the award, which is
funded by the UNC Board of
Governors, is the Department of
Health Sciences.
Recipients in the Scholarly
Development Assignment
Program are Richard Boyer
(English); Barbara Lovin (head,
Health Sciences); and Dan Pittillo
(Biology). The Scholarly Develop­ment
Assignment Program awards
provide full-time tenured faculty
members who have a minimum of six
the decline of Fraser
fir trees in the
Southern Appala­chian
Mountains.
Academic Award
of Exc ellence
In presenting
the Academic Award
June 24,1998 • The Reporter • page 3
Bruce Henderson Receives UNC System Teaching Award
Bruce B. Henderson, professor
of psychology, was among
sixteen recipients of the fourth
annual Awards for Excellence
in Teaching, presented by the
UNC Board of Governors.
Henderson accepted the award
at a special academic convoca­tion
held at N.C. Central
University in conjunction with
the inauguration of Molly
Corbett Broad as president of the
University of North Carolina.
Winners from
each campus
received a
bronze medallion
and a $7,500
cash prize.
Recipients were
nominated by
special commit­tees
from each of
the sixteen UNC
campuses and selected by the Board
of Governors Committee on Teach­ing
Awards.
Established by the Board of
Governors in 1994 to underscore
the importance of teaching and to
reward good teaching across the
university system, the awards are
given annually to a tenured faculty
member from each UNC campus.
Winners must have taught at their
present institutions at least seven
years, and no one may receive the
award more than once.
Henderson, on WCU's faculty
since 1978 and former head of the
psychology department, received
Western's Botner Superior
Teaching Award in 1988. He co-edited
the book Curiosity and
Exploration, focusing on how
intrinsic rewards affect behavior
in children. Henderson received
his bachelor's and master's degrees
from Bucknell University and
his doctorate from the University
of Minnesota.
WCU Colleges Present Awards
Awards for teaching, service, and scholarship were presented on a college-level
at the end of spring semester. The following is a listing of award
winners by college:
College of Arts and Sciences
• Curtis Wood (History) received
the Creighton Sossomon Professor­ship
for outstanding teacher-scholars
in American, English or European
history. Appointment to the
professorship is for a three-year term.
• Richard Bruce (Biology and
director, Highlands Biological
Station) received the H.F. and
Katherine P. Robinson Professorship.
• Robert Holquist (Music and
director of choral activities) received
the James Dooley Excellence in
Music Teaching Award, which carries
a $500 cash stipend.
• Betty Farmer (Communication
and Theatre Arts) received the Board
of Governors College of Arts and
Sciences Teaching Award, which
carries a $1,000 prize.
• Faculty members in the College of
Arts and Sciences also presented
acting dean J.C. Alexander Jr. with
a ""lifetime achievement award"" in
appreciation for service as acting
dean of the college.
The College of Business
• Roger Lirely (Accounting and
Information Systems) received the
Jay I. Kneedler Professor of
Excellence Award, which includes a
$1,000 cash prize and a plaque.
• Board of Governors Creative and
Innovative Teaching Awards went to
Julie Johnson (Business Adminis­tration,
Law and Marketing); Susan
Kask (Economics); Reagan
McLaurin (Business Administration,
Law and Marketing); and Max
Schreiber (Economics, Finance and
International Business). Each award
carries a $250 stipend.
The College of Education and
Allied Professions
• Carol Burton (director, Teaching
Fellows) received the annual Taft B.
Botner Award for Superior Teaching,
which includes a $750 cash prize and
a plaque.
• Board of Governors Awards for
Superior Teaching and $250 stipends
went to Barbara Bell (Elementary
and Middle Grades Education and
director, Reading Center); Cindy
Cavanaugh (Health and Human
Performance); Richard Haynes
(Administration, Curriculum and
Instruction and director of field
experiences and teacher education);
and Hedy White (Psychology).
The College of Applied Sciences
• The Board of Governors Innovation
in Teaching Award, which carries a
$1,000 stipend, went to Walter
Floreani (Health Sciences).
Trustees Approve Appointments and
Campus Name Changes
WCU's board of trustees approved a number of administrative appoint­ments
for the coming year at its quarterly meeting June 10.
• Terry L. Ballman, assistant professor of Hispanic studies at the
University of Northern Colorado, as associate professor and head of the
Department of Modern Foreign Languages.
• Paul F. Brandt as head of the Department of Chemistry and Physics.
• James A. Lewis as head of the Department of History.
• Carol C. Stephens as director of the Master of Science in Nursing ~
Program.
• Paul Wright as head of the Department of Biology.
• Kathleen S. Wright as head of the Department of Communication
and Theatre Arts.
• Jerry L. Kinard to continue as head of the Department of Manage­ment
through spring 1999.
• John A. Wade III to continue as head of the Department of Econom­ics,
Finance and International Business through spring 2000.
The trustees also approved several administrative and departmental
changes within the College of Business. The department ",finance,108
,filtered,core,"Видавничий дім «ІНЖЕК» (Харків, Україна)",2021-01-01 00:00:00,core,formation of industry x.0 on the basis of innovative-digital entrepreneurship and virtual mobility,https://core.ac.uk/download/483410798.pdf,"В статті зроблено спробу представити низку ключових технологій, що визначають нову якість життя людей, серед чого названо та розкрито зміст: автономного штучного інтелекту у смартфоні, професійних роботів помічників, доступний супутниковий інтелект, подкасти, цифрові інструменти міського планування. У статті авторами висунута гіпотеза про те, що Індустрія Х.0 є на сьогодні найвищою стадією цифровізації і являє собою концепцію інноваційно-цифрового виробництва, складниками якого є розумні активи, розумні сервіси, розумний бізнес та розумний уряд. Вказано структурні елементи авторської концепції Індустрії Х.0 представлено візуальний її зріз в умовах віртуальної реальності та функціонування даної Індустрії виключно в рамках 7-го технологічного укладу. 

Авторами розроблено та представлено протокол становлення Індустрії Х.0 крізь призму інновацій, технологій в управлінні галуззю та бізнесом. Визначено 4 етапи реалізації даного протоколу, а саме: визначення інноваційного ландшафту “технологічного прориву” у тій чи іншій галузі, формуючи Індустрію Х.0; здійснення оцінки загроз; визначення курсу подальшого розвитку та плану дій (чотири основних підходи до яких можуть вдаватись організації: захист, прийняття інновацій, ініціювання підривних інновацій, відступ); впровадження структурних змін на рівні ДНК організації. Аргументовано низкою чинників, що сьогоднішні реалії цифрового простору потребують відпрацювання нової логіки ведення платформного бізнесу в частинні його відцифрування. Зроблено висновок, що на практиці слід утворити широку коаліцію з освітян, урядовців, аналітиків, хайтек, економістів, промисловців, науковців, які всеціло долучаться до становлення Індустрії Х.0 на засадах цифровізації та інноватизації. В ході нашого дослідження, автори дійшли висновку, що Індустрія Х.0 являє собою новий підхід до організації виробництва в умовах віртуальної реальності в основі якого лежать високоінтелектуальні інтегровані новітні продукти та цифрові екосистеми, які формують повністю інноваційно-цифровий ланцюг створення вартості, додають нові компетенції та реалізують глибинні культурні зміни в напрямі становлення нової віртуальної реальності.В статье сделана попытка представить ряд ключевых технологий, определяющих новое качество жизни людей, среди чего названо и раскрыто содержание: автономного искусственного интеллекта в смартфоне, профессиональных роботов помощников, доступный спутниковый интеллект, подкасты, цифровые инструменты городского планирования. В статье авторами выдвинута гипотеза о том, что Индустрия Х.0 является сегодня высшей стадией цифровизации и представляет собой концепцию инновационно-цифрового производства, составляющими которого есть умные активы, умные сервисы, умный бизнес и умный правительство. Указано структурные элементы авторской концепции Индустрии Х.0 и представлен визуальный ее срез в условиях виртуальной реальности и функционирования данной Индустрии исключительно в рамках 7-го технологического уклада.

Авторами разработан и представлен протокол становления Индустрии Х.0 сквозь призму инноваций, технологий в управлении отраслью и бизнесом. Определены 4 этапа реализации данного протокола, а именно: определение инновационного ландшафта “технологического прорыва” в той или иной отрасли, формируя Индустрию Х.0; осуществление оценки угроз; определения курса дальнейшего развития и плана действий (четыре основных подхода к которым могут прибегать организации: защита, принятия инноваций, инициирование подрывных инноваций, отступление) внедрение структурных изменений на уровне ДНК организации. Аргументировано рядом факторов, что сегодняшние реалии цифрового пространства требуют отработки новой логики ведения платформенного бизнеса в частные его оцифровки. Сделан вывод, что на практике следует создать широкую коалицию с педагогов, чиновников, аналитиков, хайтек, экономистов, промышленников, ученых, которые всецело присоединятся к становлению Индустрии Х.0 на основе цифровизации и инноватизации. В ходе исследования, авторы пришли к выводу, что Индустрия Х.0 представляет собой новый подход к организации производства в условиях виртуальной реальности в основе которого лежат высокоинтеллектуальные интегрированы новейшие продукты и цифровые экосистемы, которые формируют полностью инновационно-цифровой цепь создания стоимости, добавляют новые компетенции и реализуют глубинные культурные изменения в направлении становления новой виртуальной реальности.The article attempts to present a number of key technologies that determine the new quality of life of people. The following content is specified and disclosed:

autonomous artificial intelligence in a smartphone, professional robot assistants, available satellite intelligence, podcasts, digital urban planning tools. In the

article, the authors hypothesize that Industry X.0 is by far the highest stage of digitalization and represents a concept of innovative and digital production,

the components of which are «smart assets», «smart services», «smart business», and «smart government». Structural elements of the authors’ concept of

Industry X.0 are indicated, its visual cut in virtual reality conditions is provided and the functioning of this Industry exclusively within the framework of the 7th

technological mode is characterized. The authors have developed and presented the protocol of formation of the Industry X.0 through the prism of innovations,

technologies in both the industry sector and business management. 4 stages of implementation of this protocol are defined, namely: determination of the innovative

landscape of «technological breakthrough» in a particular industry within the formation of Industry X.0; assessment of threats; determining the course

of further development and the action plan (four main approaches to which organizations can apply: protection, adoption of innovations, initiation of subversive

innovations, retreat); implementation of structural changes at the DNA level of the organization. The authors on the basis of a number of factors bring forward

the argument that today’s realities of the digital space require the development of a new logic of running a platform business in terms of its digitization. It is

concluded that in practice it is necessary to form a broad coalition of educators, government officials, analysts, high-tech specialists, economists, industrialists,

scientists who will join the formation of the Industry X.0 on the basis of digitalization and innovatizing. The authors concluded that Industry X.0 is a new approach

to the organization of production in the context of virtual reality, which is based on highly intelligent integrated new products and digital ecosystems that

form an innovative digital value chain, add new competencies and implement deep cultural changes in the direction of the formation of a new virtual reality",space,109
,filtered,core,HIVE: A Space Architecture Concept,2020-08-01 00:00:00,core,https://core.ac.uk/download/334990296.pdf,DigitalCommons@USU,"The increasing number of spacefaring nations and agendas, miniaturization of subsystems, and trend toward integrated systems are no doubt influencing the evolution of space systems. The diversification of space architectures has surged at an unprecedented rate in recent history with initial deployments of planned mega-constellations. This paper explores HIVE-a reconfigurable small satellite system primed to revolutionize the concept of modular space systems and future space architectures.
Based on a mass producible functioning unit consisting of nested rings, HIVE is a comprehensive satellite design harnessing advancement in robotics, software and machine learning, precision scale manufacturing, and novel materials with multifunctional properties. HIVE is addressing solutions for detailed design of interconnected hardware, engineering analysis for multi-payload applications, and policy to accomplish modularized, in-space deployment and reconfiguration.
The HIVE unit design lends itself to the “infinite possibilities” of space mission architectures and presents a revolutionary way to design, integrate, and operate missions from space. This paper provides and overview of the HIVE concept development and provides examples of applications for HIVE to showcase the range of possible systems and architectural advantages; such as space domain awareness, large service structure, and planetary surface infrastructure. Finally, we will discuss technology transfer and possible pathways to making a resilient, adaptable, and continually upgradable space infrastructure a reality",space,110
,filtered,core,Real-time binaural speech separation with preserved spatial cues,2020-02-16 00:00:00,core,http://arxiv.org/abs/2002.06637,,"Deep learning speech separation algorithms have achieved great success in
improving the quality and intelligibility of separated speech from mixed audio.
Most previous methods focused on generating a single-channel output for each of
the target speakers, hence discarding the spatial cues needed for the
localization of sound sources in space. However, preserving the spatial
information is important in many applications that aim to accurately render the
acoustic scene such as in hearing aids and augmented reality (AR). Here, we
propose a speech separation algorithm that preserves the interaural cues of
separated sound sources and can be implemented with low latency and high
fidelity, therefore enabling a real-time modification of the acoustic scene.
Based on the time-domain audio separation network (TasNet), a single-channel
time-domain speech separation system that can be implemented in real-time, we
propose a multi-input-multi-output (MIMO) end-to-end extension of TasNet that
takes binaural mixed audio as input and simultaneously separates target
speakers in both channels. Experimental results show that the proposed
end-to-end MIMO system is able to significantly improve the separation
performance and keep the perceived location of the modified sources intact in
various acoustic scenes.Comment: To appear in ICASSP 202",space,111
,filtered,core,A Self-supervised Mixed-curvature Graph Neural Network,2021-12-10 00:00:00,core,http://arxiv.org/abs/2112.05393,,"Graph representation learning received increasing attentions in recent years.
Most of existing methods ignore the complexity of the graph structures and
restrict graphs in a single constant-curvature representation space, which is
only suitable to particular kinds of graph structure indeed. Additionally,
these methods follow the supervised or semi-supervised learning paradigm, and
thereby notably limit their deployment on the unlabeled graphs in real
applications. To address these aforementioned limitations, we take the first
attempt to study the self-supervised graph representation learning in the
mixed-curvature spaces. In this paper, we present a novel Self-supervised
Mixed-curvature Graph Neural Network (SelfMGNN). Instead of working on one
single constant-curvature space, we construct a mixed-curvature space via the
Cartesian product of multiple Riemannian component spaces and design
hierarchical attention mechanisms for learning and fusing the representations
across these component spaces. To enable the self-supervisd learning, we
propose a novel dual contrastive approach. The mixed-curvature Riemannian space
actually provides multiple Riemannian views for the contrastive learning. We
introduce a Riemannian projector to reveal these views, and utilize a
well-designed Riemannian discriminator for the single-view and cross-view
contrastive learning within and across the Riemannian views. Finally, extensive
experiments show that SelfMGNN captures the complicated graph structures in
reality and outperforms state-of-the-art baselines.Comment: Accepted by AAAI 2022, 11 page",space,112
,filtered,core,Nonlinear Modeling and Control of Driving Interfaces and Continuum Robots for System Performance Gains,2020-12-01 00:00:00,core,https://core.ac.uk/download/386380658.pdf,Clemson University Libraries,"With the rise of (semi)autonomous vehicles and continuum robotics technology and applications, there has been an increasing interest in controller and haptic interface designs. The presence of nonlinearities in the vehicle dynamics is the main challenge in the selection of control algorithms for real-time regulation and tracking of (semi)autonomous vehicles. Moreover, control of continuum structures with infinite dimensions proves to be difficult due to their complex dynamics plus the soft and flexible nature of the manipulator body. The trajectory tracking and control of automobile and robotic systems requires control algorithms that can effectively deal with the nonlinearities of the system without the need for approximation, modeling uncertainties, and input disturbances. Control strategies based on a linearized model are often inadequate in meeting precise performance requirements. To cope with these challenges, one must consider nonlinear techniques. Nonlinear control systems provide tools and methodologies for enabling the design and realization of (semi)autonomous vehicle and continuum robots with extended specifications based on the operational mission profiles. This dissertation provides an insight into various nonlinear controllers developed for (semi)autonomous vehicles and continuum robots as a guideline for future applications in the automobile and soft robotics field. A comprehensive assessment of the approaches and control strategies, as well as insight into the future areas of research in this field, are presented.First, two vehicle haptic interfaces, including a robotic grip and a joystick, both of which are accompanied by nonlinear sliding mode control, have been developed and studied on a steer-by-wire platform integrated with a virtual reality driving environment. An operator-in-the-loop evaluation that included 30 human test subjects was used to investigate these haptic steering interfaces over a prescribed series of driving maneuvers through real time data logging and post-test questionnaires. A conventional steering wheel with a robust sliding mode controller was used for all the driving events for comparison. Test subjects operated these interfaces for a given track comprised of a double lane-change maneuver and a country road driving event. Subjective and objective results demonstrate that the driver’s experience can be enhanced up to 75.3% with a robotic steering input when compared to the traditional steering wheel during extreme maneuvers such as high-speed driving and sharp turn (e.g., hairpin turn) passing.  Second, a cellphone-inspired portable human-machine-interface (HMI) that incorporated the directional control of the vehicle as well as the brake and throttle functionality into a single holistic device will be presented. A nonlinear adaptive control technique and an optimal control approach based on driver intent were also proposed to accompany the mechatronic system for combined longitudinal and lateral vehicle guidance. Assisting the disabled drivers by excluding extensive arm and leg movements ergonomically, the device has been tested in a driving simulator platform. Human test subjects evaluated the mechatronic system with various control configurations through obstacle avoidance and city road driving test, and a conventional set of steering wheel and pedals were also utilized for comparison. Subjective and objective results from the tests demonstrate that the mobile driving interface with the proposed control scheme can enhance the driver’s performance by up to 55.8% when compared to the traditional driving system during aggressive maneuvers. The system’s superior performance during certain vehicle maneuvers and approval received from the participants demonstrated its potential as an alternative driving adaptation for disabled drivers. Third, a novel strategy is designed for trajectory control of a multi-section continuum robot in three-dimensional space to achieve accurate orientation, curvature, and section length tracking. The formulation connects the continuum manipulator dynamic behavior to a virtual discrete-jointed robot whose degrees of freedom are directly mapped to those of a continuum robot section under the hypothesis of constant curvature. Based on this connection, a computed torque control architecture is developed for the virtual robot, for which inverse kinematics and dynamic equations are constructed and exploited, with appropriate transformations developed for implementation on the continuum robot. The control algorithm is validated in a realistic simulation and implemented on a six degree-of-freedom two-section OctArm continuum manipulator. Both simulation and experimental results show that the proposed method could manage simultaneous extension/contraction, bending, and torsion actions on multi-section continuum robots with decent tracking performance (e.g. steady state arc length and curvature tracking error of 3.3mm and 130mm-1, respectively). Last, semi-autonomous vehicles equipped with assistive control systems may experience degraded lateral behaviors when aggressive driver steering commands compete with high levels of autonomy. This challenge can be mitigated with effective operator intent recognition, which can configure automated systems in context-specific situations where the driver intends to perform a steering maneuver. In this article, an ensemble learning-based driver intent recognition strategy has been developed. A nonlinear model predictive control algorithm has been designed and implemented to generate haptic feedback for lateral vehicle guidance, assisting the drivers in accomplishing their intended action. To validate the framework, operator-in-the-loop testing with 30 human subjects was conducted on a steer-by-wire platform with a virtual reality driving environment. The roadway scenarios included lane change, obstacle avoidance, intersection turns, and highway exit. The automated system with learning-based driver intent recognition was compared to both the automated system with a finite state machine-based driver intent estimator and the automated system without any driver intent prediction for all driving events. Test results demonstrate that semi-autonomous vehicle performance can be enhanced by up to 74.1% with a learning-based intent predictor. The proposed holistic framework that integrates human intelligence, machine learning algorithms, and vehicle control can help solve the driver-system conflict problem leading to safer vehicle operations",space,113
,filtered,core,Medics: Medical Decision Support System for Long-Duration Space Exploration,2020-01-01 00:00:00,core,https://core.ac.uk/download/pdf/286816719.pdf,,"The Autonomous Medical Operations (AMO) group at NASA Ames is developing a medical decision support system to enable astronauts on long-duration exploration missions to operate autonomously. The system will support clinical actions by providing medical interpretation advice and procedural recommendations during emergent care and clinical work performed by crew. The current state of development of the system, called MedICS (Medical Interpretation Classification and Segmentation) includes two separate aspects: a set of machine learning diagnostic models trained to analyze organ images and patient health records, and an interface to ultrasound diagnostic hardware and to medical repositories. Three sets of images of different organs and medical records were utilized for training machine learning models for various analyses, as follows: 1. Pneumothorax condition (collapsed lung). The trained model provides a positive or negative diagnosis of the condition. 2. Carotid artery occlusion. The trained model produces a diagnosis of 5 different occlusion levels (including normal). 3. Ocular retinal images. The model extracts optic disc pixels (image segmentation). This is a precursor step for advanced autonomous fundus clinical evaluation algorithms to be implemented in FY20. 4. Medical health records. The model produces a differential diagnosis for any particular individual, based on symptoms and other health and demographic information. A probability is calculated for each of 25 most common conditions. The same model provides the likelihood of survival. All results are provided with a confidence level. Item 1 images were provided by the US Army and were part of a data set for the clinical treatment of injured battlefield soldiers. This condition is relevant to possible space mishaps, due to pressure management issues. Item 2 images were provided by Houston Methodist Hospital, and item 3 health records were acquired from the MIT laboratory of computational physiology. The machine learning technology utilized is deep multilayer networks (Deep Learning), and new models will continue to be produced, as relevant data is made available and specific health needs of astronaut crews are identified. The interfacing aspects of the system include a GUI for running the different models, and retrieving and storing data, as well as support for integration with an augmented reality (AR) system deployed at JSC by Tietronix Software Inc. (HoloLens). The AR system provides guidance for the placement of an ultrasound transducer that captures images to be sent to the MedICS system for diagnosis. The image captured and the associated diagnosis appear in the technicians AR visual display",space,114
,filtered,core,,2019-01-01 00:00:00,core,affordances of control in a paradigm of spatial computing platforms,,"Adoption of digital platform innovations afford a changing nature of work, from mobile computing platforms (e.g. Apple) enabling 24/7 work connectivity, to labour marketplace platforms (e.g. Uber) enabling precarious work arrangements. Recently, organisations are adopting/investigating spatial computing platforms (e.g. Autodesk, Toyota, BNP Paribas), offering new affordances for organising (e.g. carrying out tasks, communicating and collaborating). Spatial computing concerns achieving spatial interplay between the real and digital world (Agulhon 2016), enabling perception of physically present content. An emerging paradigm of spatial computing is enabled by hardware and software innovations for; 1) digitally mapping, tracking, understanding and predicting analog audio and visual spatial fields, 2) creating digital audio and visual spatial fields, and the (3) mixing and fusing of those fields. Mixed, augmented and immersive reality is then experienced by volumetric graphic rendering onto a human's field of view (FOV) (Martín-Gutiérrez et al. 2017). Emerging marketplace examples can be seen in 'Microsoft Hololens 2' and 'Magic Leap One' platforms, both creating/enabling an ecosystem of novel applications for both industrial, educational and leisure life contexts. With further convergence of IoT, haptics, 5G, cloud and AI etc., spatial applications will range from contextually aware and interactive; digital information layering of objects, guidance and decision support systems (DSS) within business operations (such as for industrial machine manufacture, monitoring, and maintenance), digital modelling & prototyping in R&D, through to applications for communications and collaborations (such as for spatial tele co-presence of people, objects and environments). More broadly, these advances have potential to catalyse disruptions within business, through to the labour and consumer marketplace via: (1) Virtualisation of hardware resources (e.g. fully digitising workplace equipment such as displays and interfaces, raw inputs for prototyping and even digital rendering of spaces). (2) Protection and strengthening of institutional knowledge and performance via knowledge capture, guidance and decision support of labour tasks and activity (e.g. reducing labour (re)training (e.g. parts assembly), knowledge capture of practice). (3) Creation and distribution of new value propositions in goods and services (e.g. digital item ownership in a mixed-reality cloud, spatial applications for IoT enabled devices). (4) Displacement of geographic space as cost, talent, time, access and convenience constraints on business (e.g. available talent pool, partner/customer reach and relations). (5) Collaboration through new/enhanced affordances for workers (e.g. shared digitised work tools/environments). Therefore, a paradigm of spatial computing will challenge the IS community to research new ways of working, and consequences for worker experience, meaning, productivity and power. With emerging advances in AI, automation and spatial computing, one of the pertinent enquires concerns importance of workers (sense of) agency (Chandra et al. 2019). Control in the IT context has been conceptualised as control over work, control over self, and control over technology (Beaudry and Pinsonneault 2005), with prior IS work studying locus of control related to; work stress (Chandra et al. 2019), intrinsic and extrinsic motivations (Mujinga, M Eloff, MM Kroeze 2013), and performance (Vieira da Cunha et al. 2015) etc. With spatial computing platforms and their applications, what affordances of control and for whom should be developed? For example, the electronic representation of worker activity can be further enabled. Thus, tighter or looser coupling between worker activity and the reporting/outcome of work (Vieira da Cunha et al. 2015) becomes more of an organisational decision, with capability to monitor workers, and leverage AI for learning and optimisation. Furthermore, with development of spatial tele co-presence (STcP) (e.g. Mimesys), brings new affordances for communication with any worker(s), at any time, from anywhere. However, prior CMC research suggests people can choose different communication media specifically to manage social and emotional relationships (Madianou 2014) and their time (Mcloughlin et al. 2019). Hence, will such affordances serve greater identity fusion (Swann et al. 2012) and collaboration in organisations? Thus, we propose a socio-technical research agenda exploring 'control' related affordances for emerging spatial computing platforms, such as for STcP technology. In this regard, Control Theory can offer a useful starting frame, as it deals with control mechanisms governing workers organisational actions both formal (outcome and behaviour based) and informal (group and self-control), to further the interests of organisations (Kirsch 1996). We suggest, data and communication related affordances of control (e.g. privacy, exploitation, authenticity, availability and spaces) as starting points. Social Capital (Lin 2001), Social Influence (Kelman 1958), Social Identity (Ellemers and Haslam 2012), Identity Fusion (Swann et al. 2012) and Polymedia (Madianou and Miller 2012) being just some of the many relevant social theories to this endeavour",space,115
,filtered,core,,2020-02-07 00:00:00,core,a novel robotic framework for safe inspection and telemanipulation in hazardous and unstructured environments,,"Intelligent robotic systems are becoming essential for space applications, industries, nuclear plants and for harsh environments in general, such as the European Organization for Nuclear Research (CERN) particles accelerator complex and experiments. Robotics technology has huge potential benefits for people and its ultimate scope depends on the way this technology is  used. In order to increase safety and machine availability, robots can perform repetitive, unplanned and dangerous tasks, which humans either prefer to avoid or are unable to carry out due to hazards, size constraints, or the extreme environments in which they take place. Nowadays, mechatronic systems use mature technologies that allow their robust and safe use, even in collaboration with human workers. Over the past years, the progress of robots has been based on the development of smart sensors, artificial intelligence and modular mechanical systems. Due to the multiple challenges that hazardous and unstructured environments have for the application of autonomous industrial systems, there is still a high demand for intelligent and teleoperation systems that give the control of a robot (slave) to a human operator via haptic input devices (master), as well as using human-supervised telerobotic control techniques. Modern techniques like simulation and virtual reality systems can facilitate the preparation of ad-hoc mechatronic tools and robotic intervention including recovery scenarios and failure mode analysis.  The basic contribution of this thesis is the development of a novel robotic framework for autonomous inspections and supervised teleoperations in harsh environments. The proposed framework covers all aspects of a robotic intervention, from the specification and operator training, the choice of the robot and its material in accordance with possible radiological contamination risks, to the realization of the intervention, including procedures and recovery scenarios. In a second set of contributions, new methods for mutirobots maintenance operations are developed, including intervention preparation and best practices for remote handling and advanced tools. The third set of contributions is built on a novel multimodal user-friendly human-robot interface that allows  operator training using virtual reality systems and technicians not expert in robot operation to perform inspection/maintenance tasks. In this thesis, we exploit a robotic system able to navigate autonomously and to inspect unknown environments in a safe way. A new real-time control system has been implemented in order to guarantee a fast response to environmental changes and adaptation to different type of scenarios the robot may find in a semi-structured and hazardous environment. The proposed new robotic control system  has been integrated on different robots, tested and validated with several robotic interventions in the CERN hazardous particle accelerator complex",space,116
,filtered,core,Digital Commons @ University of South Florida,2020-11-09 00:00:00,core,efficient neural architecture search with multiobjective evolutionary optimization,,"Deep neural networks have become very successful at solving many complex tasks such as image classification, image segmentation, and speech recognition. These models are composed of multiple layers that have the capacity to learn increasingly higher-level features, without prior handcrafted specifications. However, the success of a deep neural network relies on finding the proper configuration for the task in hand. Given the vast number of hyperparameters and the massive search space, manually designing or fine-tuning deep learning architectures requires extensive knowledge, time, and computational resources.
There is a growing interest in developing methods that automatically design a neural network´s architecture, known as neural architecture search (NAS). NAS is usually modeled as a single-objective optimization problem where the aim is to find an architecture that maximizes the prediction´s accuracy. However, most deep learning applications require accurate as well as efficient architectures to reduce memory consumption and enable their use in computationally-limited environments. This has led to the need to model NAS as a multiple objective problem that optimizes both the predictive performance and efficiency of the network. Furthermore, most NAS framework have focused on either optimizing the micro-structure (structure of the basic cell), or macro-structure (optimal number of cells and their connection) of the architecture. Consequently, manual engineering is required to find the topology of the non-optimized structure.
Although NAS has demonstrated great potential in automatically designing an architecture, it remains a computationally expensive and time-consuming process because it requires training and evaluating many potential configurations. Recent work has focused on improving the search time of NAS algorithms, but most techniques have been developed and applied only for single-objective optimization problems. Given that optimizing multiple objectives has a higher complexity and requires more iterations to approximate the Pareto Front, it is critical to investigate algorithms that decrease the search time of multiobjective NAS.
One critical application of deep learning is medical image segmentation. Segmentation of medical images provides valuable information for various critical tasks such as analyzing anatomical structures, monitoring disease progression, and predicting patient outcomes. Nonetheless, achieving accurate segmentation is challenging due to the inherent variability in appearance, shape, and location of the region of interest (ROI) between patients and the differences in imagining equipment and acquisition protocols. Therefore, neural networks are usually tailored to a specific application, anatomical region, and image modality. Moreover, medical image data is often volumetric requiring expensive 3D operations that result in large and complex architectures. Hence, training and deploying them requires considerable storage and memory bandwidth that makes them less suitable for clinical applications.
To overcome these challenges, the main goal of this research is to automatically design accurate and efficient deep neural networks using multiobjective optimization algorithms for medical image segmentation. The proposed research consists of three major objectives: (1) to design a deep neural network that uses a multiobjective evolutionary based algorithm to automatically adapt to different medical image datasets while minimizing the model’s size; (2) to design a self-adaptive 2D-3D Fully Convolutional network (FCN) ensemble that incorporates volumetric information and optimizes both the performance and the size of the architecture; and (3) to design an efficient multiobjective neural architecture search framework that decreases the search time while simultaneously optimizing the micro- and macro-structure of the neural architecture.
For the first objective, a multiobjective adaptive convolutional neural network named AdaResU-Net is presented for 2D medical image segmentation. The proposed AdaResU-Net is comprised of a fixed architecture and a learning framework that adjusts the hyperparameters to a particular training dataset using a multiobjective evolutionary based algorithm (MEA algorithm). The MEA algorithm evolves the AdaResU-Net network to optimize both the segmentation accuracy and model size. In the second objective, a self-adaptive ensemble of 2D-3D FCN named AdaEn-Net is proposed for 3D medical image segmentation. The AdaEn-Net is comprised of a 2D FCN that extracts intra-slice and long-range 2D context, and a 3D FCN architecture that exploits inter-slice and volumetric information. The 2D and 3D FCN architectures are automatically fitted for a specific medical image segmentation task by simultaneously optimizing the expected segmentation error and size of the network using the MEA algorithm. Finally, for the third objective, an efficient multiobjective neural architecture search framework named EMONAS is presented for 3D medical image segmentation. EMONAS has two main components, a novel search space that includes the hyperparameters that define the micro- and macro-structure of the architecture, and a Surrogate-assisted multiobjective evolutionary based algorithm (SaMEA algorithm) that efficiently searches for the best hyperparameter values using a Random Forest surrogate and guiding selection probabilities.
The broader impact of the proposed research is as follows: (1) automating the design of deep neural networks’ architecture and hyperparameters to improve the performance and efficiency of the models; and (2) increase the accessibility of deep learning to a broader range of organizations and people by reducing the need of expert knowledge and GPU time when automatically designing deep neural networks. In the medical area, the proposed models aim to improve the automatic extraction of data from medical images to potentially enhance diagnosis, treatment planning and survival prediction of various diseases such as cardiac disease and prostate cancer. Although the proposed techniques are applied to medical image segmentation tasks, they can also be implemented in other applications where accurate and resource-efficient deep neural networks are needed such as autonomous navigation, augmented reality and internet-of-things",space,117
,filtered,core,'Association for Computing Machinery (ACM)',2018-04-04 00:00:00,core,nectar: multi-user spatial augmented reality for everyone: three live demonstrations of educative applications,10.1145/3234253.3234317,"RealityTech is supported by Region Aquitaine, Pole Aquinetic and Unitec. The Interactive Map Application for Visually Impaired People is conducted with the support of the Erasmus+ Program of the European Union Pr. no 2016-1-EL01-KA201-023731.International audienceIn this demonstration we showcase a new spatial augmented reality device (interactive projection) with three applications: education and experimentation of color models, map exploration for visually impaired people and scientific vulgarization of machine learning. The first exhibition is an interactive exploration about the nature of light. Visitors can experiment with additive subtractive color models. We engage them with questions, and they have to reply using cards to find out answers. This exhibit is suitable for children. The second exhibition is about map exploration and creation for Visually Impaired Persons (VIP). VIP generally use tactile maps with braille to learn about an unknown environment. However, these maps are not accessible to the 80% of VIP who don't read braille. Our prototype augments raised-line maps with audio output. The third exhibition is destined to be used for scientific outreach. It enables the creation of artificial neural networks (ANN) using tangible interfaces. Neurons are represented by laser-cut diamond shaped tokens, and the data to learn is printed on cards. The ANN learns to differentiate shapes, and the whole learning process is made visible and interactive. These three applications demonstrate the capabilities of our hardware and software development kit in different scenarios. At ReVo, each demonstration will have its own setup and interactive space",space,118
,filtered,core,'MDPI AG',2018-12-01 00:00:00,core,hyperparameter optimization for image recognition over an ar-sandbox based on convolutional neural networks applying a previous phase of segmentation by color–space,10.3390/sym10120743,"Immersive techniques such as augmented reality through devices such as the AR-Sandbox and deep learning through convolutional neural networks (CNN) provide an environment that is potentially applicable for motor rehabilitation and early education. However, given the orientation towards the creation of topographic models and the form of representation of the AR-Sandbox, the classification of images is complicated by the amount of noise that is generated in each capture. For this reason, this research has the purpose of establishing a model of a CNN for the classification of geometric figures by optimizing hyperparameters using Random Search, evaluating the impact of the implementation of a previous phase of color&#8315;space segmentation to a set of tests captured from the AR-Sandbox, and evaluating this type of segmentation using similarity indexes such as Jaccard and S&#248;rensen&#8315;Dice. The aim of the proposed scheme is to improve the identification and extraction of characteristics of the geometric figures. Using the proposed method, an average decrease of 39.45% to a function of loss and an increase of 14.83% on average in the percentage of correct answers is presented, concluding that the selected CNN model increased its performance by applying color&#8315;space segmentation in a phase that was prior to the prediction, given the nature of multiple pigmentation of the AR-Sandbox",space,119
,filtered,core,"Engineers Australia, Royal Aeronautical Society (Melbourne, Australia)",2019-01-01 00:00:00,core,"virtual design, optimisation and testing (vdot) framework for innovative sustainment",,"Current demand for sustainment of critical aerospace assets requires management of complex systems and a product life cycle solution. This is a major concern both in civil and defence sectors in Australia. The emergence of new technologies including additive manufacturing (AM), virtual prototyping, simulated/augmented reality (AR) and artificial intelligence (AI) provide a unique opportunity for Innovative Sustainment. This paper discusses the importance of a network/system approach to sustainment and the role that emerging technologies including AM and AI can play in overcoming current challenges. In addition, the paper discusses the strategic research opportunities that can be harnessed by all stakeholders using a collaborative framework. The proposed framework can result in reduced cost of ownership, reduced logistics footprint, and enhanced resilience and flexibility by in-depth analyses of the challenges ahead. Aerospace composite material systems are used to exemplify the implementation pathway of this framework",space,120
,filtered,core,'MDPI AG',2019-02-26 00:00:00,core,hyperparameter optimization for image recognition over an ar-sandbox based on convolutional neural networks applying a previous phase of segmentation by color-space,10.3390/sym10120743,"Immersive techniques such as augmented reality through devices such as the AR-Sandbox and deep learning through convolutional neural networks (CNN) provide an environment that is potentially applicable for motor rehabilitation and early education. However, given the orientation towards the creation of topographic models and the form of representation of the AR-Sandbox, the classification of images is complicated by the amount of noise that is generated in each capture. For this reason, this research has the purpose of establishing a model of a CNN for the classification of geometric figures by optimizing hyperparameters using Random Search, evaluating the impact of the implementation of a previous phase of color-space segmentation to a set of tests captured from the AR-Sandbox, and evaluating this type of segmentation using similarity indexes such as Jaccard and Sorensen-Dice. The aim of the proposed scheme is to improve the identification and extraction of characteristics of the geometric figures. Using the proposed method, an average decrease of 39.45% to a function of loss and an increase of 14.83% on average in the percentage of correct answers is presented, concluding that the selected CNN model increased its performance by applying color-space segmentation in a phase that was prior to the prediction, given the nature of multiple pigmentation of the AR-Sandbox",space,121
,filtered,core,The computer as an irrational cabinet.,2010-10-13 00:00:00,core,https://core.ac.uk/download/17301215.pdf,,"This thesis and its accompanying project are concerned with the use of digital technology in the representation of material culture. The thesis aims to find ways of using such technology that are appropriate to our present needs and to its potential. The computer is a technology which we understand, interact with and relate to through metaphor. I propose that many of the metaphors through which we understand it invoke the idea of an enclosed space. The use of such a trope might seem suitable when using computers for representing museum collections, or material culture in general, since it invokes the enclosed space of the museum. I examine how this idea of enclosure is manifested in computer developments such as virtual reality and artificial intelligence.   I also look at how these developments are congruent with perspectival modes of visual representation privileged in the modern era. I argue that such metaphors and forms of representation, whether manifested in visual arts, the museum, or computer applications are problematic, bound up as they are with modern western ideas of mastery and transcendence, which are presently being subjected to critiques from various quarters.

Throughout the modern era there have been forms of representation which have contested the dominant visual mode of modernity. These include the art of the Baroque in the seventeenth and eighteenth centuries and, in this century, the work of the Surrealists. In contrast to the rational, orthogonal space of modernity, both these deal with complex and fragmented representations of spaces and time. Such developments have been discussed as forms of representation appropriate to contemporary concerns about knowledge They also have a corollary in computing developments, such as multimedia and hypermedia,

Yet, I argue, those working in multimedia have in the main failed to exploit the potential of such developments to enable new ways of representing knowledge. I propose looking to both the Baroque and Surrealism to find possible models and strategies for use in multimedia in the representation of material culture. In relation to this I describe practical work done in conjunction with this thesis which uses these models as the basis of a piece of multimedia software for the representation of material culture",space,122
10.1109/oceans-bergen.2013.6608083,filtered,2013 MTS/IEEE OCEANS - Bergen,IEEE,2013-06-14 00:00:00,ieeexplore,field test experience of an underwater wireless network in the atlantic ocean,https://ieeexplore.ieee.org/document/6608083/,"UnderwaterWireless Networks (UWNs) have gained significant attention in recent years given their ability to expand underwater monitoring and detection applications. In this paper, we share our experience from a recent field experiment in the Atlantic Ocean, in which we have deployed an 11 node UWN. We discuss the system architecture, both hardware and software, and evaluate two new network protocols and one existing network protocol for real-world performance. Additionally, we provide insight on how the physical environment and surroundings impact the performance of the networked system. This work will share our practical issues in real systems and inspire new advances in the area of UWN research.",oceanology,123
10.23919/oceans44145.2021.9705808,filtered,OCEANS 2021: San Diego – Porto,IEEE,2021-09-23 00:00:00,ieeexplore,ocean current observations by infrared and visual large scale particle image velocimetry (lspiv),https://ieeexplore.ieee.org/document/9705808/,"The natural dynamics of tidal currents continuously change the appearance of the Wadden Sea. Especially coastal regions and bays with shallow waters are strongly influenced by tidal dynamics. Meanwhile, in offshore study areas with harsh environmental conditions as well as dynamic ocean currents and water levels, continuous in-situ investigations of the prevailing current conditions requires a high effort. In-situ measurement technology is thereby maintenance-intensive and partly limited in time or space by limited accessibility or restricted by the measurement method. In hence, remote sensing techniques based on video image information, such as Large Scale Particle Image Velocimetry (LSPIV) for sensing, and computer vision algorithms for long-term investigation of the prevailing dynamic near-surface ocean flow conditions, have become highly relevant. However, in some environmental situations there may be not sufficient or significant natural textures evaluable in the visual camera images, or only insufficient contrast ratio under varying ambient illumination at the water observation site, to perform image based velocimetry. Thus, we consider with this study an extended approach of visual and infrared video data analysis by an automated LSPIV measurement technique under real offshore deployment conditions. The treatise of this paper first introduces the reader to the subject area and technologies as well as the principles of the LSPIV measurement method. Followed by the depiction of the motivation for an extended approach to visual and infrared video data analysis by an automated LSPIV measurement method in real offshore applications. Subsequently, related research is discussed. Thereafter, the remote sensing setup and sensor-test-bed-system for offshore deployment on an observation platform is presented, therein we also addresses routines of necessary calibration procedures for camera sensors. We then depict details of our automated LSPIV measurement procedure. This is followed with an overview of the preceding validation procedures and LSPIV multispectral remote sensing results of long-term monitoring of horizontal flow dynamics over several days. Finally, we discuss uncertainties of the LSPIV velocity measurement method encountered in real applications and conclude with a brief outlook on further developments and applications.",oceanology,124
10.1109/icrera.2014.7016517,filtered,2014 International Conference on Renewable Energy Research and Application (ICRERA),IEEE,2014-10-22 00:00:00,ieeexplore,use of artificial neural networks for real-time prediction of heave displacement in ocean buoys,https://ieeexplore.ieee.org/document/7016517/,"Many advanced control systems for wave energy converters (WEC's) require knowledge of incoming wave profiles to be implemented. This is due to the non-causal relationship between water elevation and force exerted on a floating body. This study focuses on the use of cascade feedforward neural networks to predict short-term incoming water surface displacements based on recently observed data in real time. Prediction networks are trained with time series data reconstructed from spectral data and recorded time series data from a data buoy deployed off the West Irish Coast. Both training methods are shown to have predictive capabilities with regression coefficients between 0.8–0.9 for a small range of sea states. Both networks prediction accuracies are tested on a large range of sea states as well. For sea states dramatically different from training data prediction accuracies decrease, but less so for the network trained on observed data. The need for accurate wave predictions in the field of WEC control design is also discussed.",oceanology,125
10.1109/joe.2017.2706100,filtered,IEEE Journal of Oceanic Engineering,IEEE,2018-07-01 00:00:00,ieeexplore,comparison of two hyperparameter-free sparse signal processing methods for direction-of-arrival tracking in the hf97 ocean acoustic experiment,https://ieeexplore.ieee.org/document/7939967/,"In this paper, we review and compare the performance of two recently introduced hyperparameter-free sparse signal processing methods namely, the sparse iterative covariance-based estimation method and the sparse Bayesian learning-based relevance vector machine method, for direction-of-arrival (DOA) tracking of multiple signals using an array of sensors. The methods are presented to the readers, in a tutorial style for easy understanding. Hyperparameter-free sparsity-based methods are attractive in practice since tuning of regularization parameters (hyperparameters) is not necessary as they are automatically estimated from the data. The DOA tracking problem is formulated as a snapshot-by-snapshot estimation problem and the implementation of the methods are discussed in detail. A simulation study using a uniform-linear-array is carried out to evaluate the performance of the methods in terms of the root-mean-squared error of the DOA estimates and the probability of resolution with the goal of determining when one is to be preferred over the other. The algorithms are also applied on passive sonar data from the 1997 High-Frequency (HF97) ocean acoustic experiment to demonstrate their usability in a real underwater scenario, as well as their robustness to the modeling assumptions made. We draw new conclusions about the main features of these methods that are important to the underwater acoustic practitioners.",oceanology,126
10.1109/access.2020.3018270,filtered,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,ioceansee: a novel scheme for ocean state estimation using 3d mobile convolutional neural network,https://ieeexplore.ieee.org/document/9172055/,"Ocean state estimation is a basic problem in the field of ocean engineering. Under the trend of data-driven, the development of intelligent ship decision-making, ocean energy system design and other aspects, are inseparable from the estimation of wave parameters in the ocean area. In recent years, researchers have developed remote sensing technology to monitor ocean waves. However, sensor-based methods all have a key limitation, which is high cost and fault worry. More importantly, one major limitation exists in current research: due to lack of change information and relying on a single feature of spatial data, the final predictive results are inaccurate. Adopting a 3D Convolutional Neural Network is a possible solution to improve the detection accuracy. Unfortunately, it cannot be deployed in the ocean environment due to lack of physical network connections. To resolve these issues, we develop a light-weight version of 3D Convolutional Neural Network, namely a low-cost, high-accuracy detection scheme to foresee ocean wave parameters using a 3D Mobile Convolutional Neural Network technique called iOceanSee in the marine environment. iOceanSee employs a mobile terminal composed of low-cost measuring equipment and non-interference (except light) device-an RGB camera to collect video data in real time. It extracts both space and time features through three-dimensional depthwise separable convolutions. More specifically, iOceanSee is able to capture the encoding motion information from multiple adjacent frames of the video, according to which period and height of the waves being evaluated. Our experimental results conclude that iOceanSee obtains comparable performance to 3D Convolutional Neural Network and outperforms other models in terms of measurement accuracy in the marine environments.",oceanology,127
10.23919/oceans44145.2021.9706096,filtered,OCEANS 2021: San Diego – Porto,IEEE,2021-09-23 00:00:00,ieeexplore,a deep learning approach to dead-reckoning navigation for autonomous underwater vehicles with limited sensor payloads,https://ieeexplore.ieee.org/document/9706096/,"This paper presents a deep learning approach to aid dead-reckoning (DR) navigation using a limited sensor suite. A Recurrent Neural Network (RNN) was developed to predict the relative horizontal velocities of an Autonomous Underwater Vehicle (AUV) using data from an IMU, pressure sensor, and control inputs. The RNN network is trained using experimental data, where a doppler velocity logger (DVL) provided ground truth velocities. The predictions of the relative velocities were implemented in a dead-reckoning algorithm to approximate north and east positions. The studies in this paper were twofold I) Experimental data from a Long-Range AUV was investigated. Datasets from a series of surveys in Monterey Bay, California (U.S) were used to train and test the RNN network. II) The second study explore datasets generated by a simulated autonomous underwater glider. Environmental variables e.g ocean currents were implemented in the simulation to reflect real ocean conditions. The proposed neural network approach to DR navigation was compared to the on-board navigation system and ground truth simulated positions.",oceanology,128
10.1109/iros51168.2021.9636055,filtered,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),IEEE,2021-10-01 00:00:00,ieeexplore,a predictive control method for stabilizing a manipulator-based uav landing platform on fluctuating marine surface<sup>*</sup>,https://ieeexplore.ieee.org/document/9636055/,"In the process of landing unmanned aerial vehicles (UAVs) on an unmanned surface vehicle (USV), a manipulator can be applied to help the UAV land safely and accurately. However, it is a challenge to control the manipulator on a disturbed USV due to joint velocity constraints and bandwidth limitations. To solve this problem, a predictive control framework is proposed in this paper. We leverage a first-order delay system to describe the kinematics of each joint, and control joint velocities by the model predictive controller (MPC). To generate references for MPC, the motion of the floating base needs to be predicted. We apply the recent approach for motion prediction based on the wavelet network (WN) and modify the network to get smooth trajectories. The accuracy of the modified wavelet network (MWN) for motion prediction is tested on four-hour motion data from the real ocean environment and the smoothness of the generated trajectories is also evaluated. Simulations and experiments are implemented to verify the proposed method, the results show that the average control accuracies are improved by more than 30% and 50% in position and rotation compared with the traditional inverse kinematics (IK) controller for 1 Hz base fluctuation.",oceanology,129
10.1109/fskd.2008.6,filtered,2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery,IEEE,2008-10-20 00:00:00,ieeexplore,methods of chlorophyll concentration retrieval,https://ieeexplore.ieee.org/document/4666360/,"Oceanic chlorophyll concentration is the important indicator of phytoplankton and water quality. So chlorophyll concentration retrieval is the key factor in ocean ecological system. The paper introduces the bio-optical algorithm of chlorophyll concentration retrieval, in which the algebra and neural networks methods are introduced in model methods. The SeaWiFS data of Chinese sea are processed through the software of SeaDAS, the chlorophyll concentration is retrievable by the SeaWiFS/OC4V4 method, and the results are compared between the real collected chlorophyll concentration and the chlorophyll concentration retrieval.",oceanology,130
10.1109/igarss.2018.8519369,filtered,IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium,IEEE,2018-07-27 00:00:00,ieeexplore,smos data assimilation for numerical weather prediction,https://ieeexplore.ieee.org/document/8519369/,"This paper presents the Soil Moisture and Ocean Salinity (SMOS) mission data assimilation activities conducted at the European Centre for Medium-Range Weather Forecasts (ECMWF) to analyse soil moisture for Numerical Weather Prediction (NWP) applications. Two different approaches are presented based on SMOS brightness temperature and SMOS neural network soil moisture data assimilation, respectively. For the first approach, SMOS brightness temperature data assimilation relies on forward modelling. Long term results, spanning the SMOS period, of SMOS forward modelling, monitoring and data assimilation are presented. They emphasize the relevance of SMOS data for monitoring and to support NWP model developments. For the second approach, a SMOS soil moisture product has been produced based on a Neural Network (NN) trained on ECMWF soil moisture. So, the SMOS-ECMWF NN soil moisture product captures the SMOS signal variability in time and space, while by design its climatology is consistent with that of the ECMWF soil moisture, which makes it suitable for data assimilation purpose. This approach, initially tested for 2012 in a global scale stand alone approach, shows that SMOS NN data assimilation slightly improves the two-metre air temperature forecast in the short range at regional scale. For NWP applications this approach has been further developed with a near real time production of the SMOS-ECMWF NN soil moisture product, with the implementation of the SMOS NN data assimilation in the ECMWF Integrated Forecasting System (IFS), and with high resolution (9km) global scale testing compatible with the current ECMWF NWP system.",oceanology,131
10.23919/ecc.2019.8795916,filtered,2019 18th European Control Conference (ECC),IEEE,2019-06-28 00:00:00,ieeexplore,towards reinforcement learning-based control of an energy harvesting pendulum,https://ieeexplore.ieee.org/document/8795916/,"Harvesting energy from the environment, e.g. ocean waves, is a key capability for the long-term operation of remote electronic systems where standard energy supply is not available. Rotating pendulums can be used as energy converters when excited close to their eigenfrequency. However, to ensure robust operation of the harvester, the energy of the dynamic system has to be controlled. In this study, we deploy a light-weight reinforcement learning algorithm to drive the energy of an Acrobot pendulum towards a desired value. We analyze the algorithm in an extensive series of simulations. Moreover, we explore the real world application of our energy-based reinforcement learning algorithm using a computationally constrained hardware setup based on low-cost components, such as the Raspberry Pi platform.",oceanology,132
10.1109/icra.2019.8793644,filtered,2019 International Conference on Robotics and Automation (ICRA),IEEE,2019-05-24 00:00:00,ieeexplore,underwater communication using full-body gestures and optimal variable-length prefix codes,https://ieeexplore.ieee.org/document/8793644/,"In this paper we consider inter-robot communication in the context of joint activities. In particular, we focus on convoying and passive communication for radio-denied environments by using whole-body gestures to provide cues regarding future actions. We develop a communication protocol whereby information described by codewords is transmitted by a series of actions executed by a swimming robot. These action sequences are chosen to optimize robustness and transmission duration given the observability, natural activity of the robot and the frequency of different messages. Our approach uses a convolutional network to make core observations of the pose of the robot being tracked, which is sending messages. The observer robot then uses an adaptation of classical decoding methods to infer a message that is being transmitted. The system is trained and validated using simulated data, tested in the pool and is targeted for deployment in the open ocean. Our decoder achieves.94 precision and.66 recall on real footage of robot gesture execution recorded in a swimming pool.",oceanology,133
10.1109/access.2020.3020530,filtered,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,a nonlinear model predictive controller for remotely operated underwater vehicles with disturbance rejection,https://ieeexplore.ieee.org/document/9180253/,"Remotely Operated underwater Vehicles (ROVs) are growing in importance in the ocean environment for observation and manipulation tasks, particularly when used to maintain offshore energy and offshore renewable energy assets. Many such tasks require the dynamic positioning of ROV in challenging sea conditions with multiple disturbances resulting from the effects of waves, currents and turbulence. This work presents a novel, nonlinear, model predictive dynamic positioning controller that accounts for such complex stochastic disturbances. These external disturbances are modelled as 6-degree of freedom forces and moments within the nonlinear ROV dynamic and propulsion model. A nonlinear model predictive dynamic positioning strategy based on the nonlinear model predictive control (NMPC) is proposed for the disturbance rejection in this work. A numerical water tank model is used to test the performance of the strategy using hardware in-the-loop simulation. The results of the simulation have been compared against baseline proportional-integral-derivative (PID) and linear quadratic regulator (LQR) controllers tested under wave and current conditions in the FloWave basin. A quantitative comparison of the controllers is presented. The resulting controller is shown to maintain a small root mean squared error (RMSE) in position when subjected to multiple directional disturbance, with minimal control effort. This study contributes an important insight on future theoretical design of model predictive disturbance rejection controllers and illustrates their practical implementation on real hardware.",oceanology,134
10.1109/access.2018.2845855,filtered,IEEE Access,IEEE,2018-01-01 00:00:00,ieeexplore,color transfer pulse-coupled neural networks for underwater robotic visual systems,https://ieeexplore.ieee.org/document/8377996/,"With rapid developments in cloud computing, artificial intelligence, and robotic systems, ever more complex tasks, such as space and ocean exploration, are being implemented by intelligent robots. Here, we propose an underwater image enhancement scheme for robotic visual systems. The proposed algorithm and its implementation enhances and outputs an image captured by an underwater robot in real time. In this scheme, pulse-coupled neural network (PCNN)-based image enhancement and color transfer algorithms are combined to enhance the underwater image. To avoid color imbalance in the underwater image and enhance details while suppressing noise, color correction is first carried out on the underwater image before converting it into the hue-saturation-intensity domain and enhancing it by PCNN. The enhanced result improves the color and contrast of the source image and enhances the details and edges of darker regions. Experiments are performed on real world data to demonstrate the effectiveness of the proposed scheme.",oceanology,135
10.1109/access.2019.2953326,filtered,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,learn to navigate: cooperative path planning for unmanned surface vehicles using deep reinforcement learning,https://ieeexplore.ieee.org/document/8897592/,"Unmanned surface vehicle (USV) has witnessed a rapid growth in the recent decade and has been applied in various practical applications in both military and civilian domains. USVs can either be deployed as a single unit or multiple vehicles in a fleet to conduct ocean missions. Central to the control of USV and USV formations, path planning is the key technology that ensures the navigation safety by generating collision free trajectories. Compared with conventional path planning algorithms, the deep reinforcement learning (RL) based planning algorithms provides a new resolution by integrating a high-level artificial intelligence. This work investigates the application of deep reinforcement learning algorithms for USV and USV formation path planning with specific focus on a reliable obstacle avoidance in constrained maritime environments. For single USV planning, with the primary aim being to calculate a shortest collision avoiding path, the designed RL path planning algorithm is able to solve other complex issues such as the compliance with vehicle motion constraints. The USV formation maintenance algorithm is capable of calculating suitable paths for the formation and retain the formation shape robustly or vary shapes where necessary, which is promising to assist with the navigation in environments with cluttered obstacles. The developed three sets of algorithms are validated and tested in computer-based simulations and practical maritime environments extracted from real harbour areas in the UK.",oceanology,136
http://arxiv.org/abs/2110.00661v1,filtered,arxiv,arxiv,2021-10-01 00:00:00,arxiv,"a deep learning approach to dead-reckoning navigation for autonomous
  underwater vehicles with limited sensor payloads",http://arxiv.org/abs/2110.00661v1,"This paper presents a deep learning approach to aid dead-reckoning (DR)
navigation using a limited sensor suite. A Recurrent Neural Network (RNN) was
developed to predict the relative horizontal velocities of an Autonomous
Underwater Vehicle (AUV) using data from an IMU, pressure sensor, and control
inputs. The RNN network is trained using experimental data, where a doppler
velocity logger (DVL) provided ground truth velocities. The predictions of the
relative velocities were implemented in a dead-reckoning algorithm to
approximate north and east positions. The studies in this paper were twofold I)
Experimental data from a Long-Range AUV was investigated. Datasets from a
series of surveys in Monterey Bay, California (U.S) were used to train and test
the RNN network. II) The second study explore datasets generated by a simulated
autonomous underwater glider. Environmental variables e.g ocean currents were
implemented in the simulation to reflect real ocean conditions. The proposed
neural network approach to DR navigation was compared to the on-board
navigation system and ground truth simulated positions.",oceanology,137
http://arxiv.org/abs/2101.02073v1,filtered,arxiv,arxiv,2021-01-06 00:00:00,arxiv,shallow-uwnet : compressed model for underwater image enhancement,http://arxiv.org/abs/2101.02073v1,"Over the past few decades, underwater image enhancement has attracted
increasing amount of research effort due to its significance in underwater
robotics and ocean engineering. Research has evolved from implementing
physics-based solutions to using very deep CNNs and GANs. However, these
state-of-art algorithms are computationally expensive and memory intensive.
This hinders their deployment on portable devices for underwater exploration
tasks. These models are trained on either synthetic or limited real world
datasets making them less practical in real-world scenarios. In this paper we
propose a shallow neural network architecture, \textbf{Shallow-UWnet} which
maintains performance and has fewer parameters than the state-of-art models. We
also demonstrated the generalization of our model by benchmarking its
performance on combination of synthetic and real-world datasets.",oceanology,138
http://arxiv.org/abs/2009.05167v2,filtered,arxiv,arxiv,2020-09-10 00:00:00,arxiv,accelerating real-time question answering via question generation,http://arxiv.org/abs/2009.05167v2,"Although deep neural networks have achieved tremendous success for question
answering (QA), they are still suffering from heavy computational and energy
cost for real product deployment. Further, existing QA systems are bottlenecked
by the encoding time of real-time questions with neural networks, thus
suffering from detectable latency in deployment for large-volume traffic. To
reduce the computational cost and accelerate real-time question answering
(RTQA) for practical usage, we propose to remove all the neural networks from
online QA systems, and present Ocean-Q (an Ocean of Questions), which
introduces a new question generation (QG) model to generate a large pool of QA
pairs offline, then in real time matches an input question with the candidate
QA pool to predict the answer without question encoding. Ocean-Q can be readily
deployed in existing distributed database systems or search engine for
large-scale query usage, and much greener with no additional cost for
maintaining large neural networks. Experiments on SQuAD(-open) and HotpotQA
benchmarks demonstrate that Ocean-Q is able to accelerate the fastest
state-of-the-art RTQA system by 4X times, with only a 3+% accuracy drop.",oceanology,139
http://arxiv.org/abs/1910.14540v1,filtered,arxiv,arxiv,2019-10-31 00:00:00,arxiv,"team nctu: toward ai-driving for autonomous surface vehicles -- from
  duckietown to robotx",http://arxiv.org/abs/1910.14540v1,"Robotic software and hardware systems of autonomous surface vehicles have
been developed in transportation, military, and ocean researches for decades.
Previous efforts in RobotX Challenges 2014 and 2016 facilitates the
developments for important tasks such as obstacle avoidance and docking. Team
NCTU is motivated by the AI Driving Olympics (AI-DO) developed by the
Duckietown community, and adopts the principles to RobotX challenge. With the
containerization (Docker) and uniformed AI agent (with observations and
actions), we could better 1) integrate solutions developed in different
middlewares (ROS and MOOS), 2) develop essential functionalities of from
simulation (Gazebo) to real robots (either miniaturized or full-sized WAM-V),
and 3) compare different approaches either from classic model-based or
learning-based. Finally, we setup an outdoor on-surface platform with
localization services for evaluation. Some of the preliminary results will be
presented for the Team NCTU participations of the RobotX competition in Hawaii
in 2018.",oceanology,140
10.1016/j.petrol.2021.109038,filtered,Journal of Petroleum Science and Engineering,scopus,2021-11-01,sciencedirect,optimization of wag in real geological field using rigorous soft computing techniques and nature-inspired algorithms,https://api.elsevier.com/content/abstract/scopus_id/85108013019,"To meet the ever-increasing global energy demands, it is more necessary than ever to ensure increments in the recovery factors (RF) associated with oil reservoirs. Owing to this challenge, enhanced oil recovery (EOR) techniques are increasingly gaining more significance as robust strategies for producing more oil volumes from mature reservoirs. Water alternating gas (WAG) injection is an EOR method intended at improving the microscopic and macroscopic displacement efficiencies. To handle and implement successfully this technique, it is of vital importance to optimize its operating parameters. This study targeted at implementing robust proxy paradigms for investigating the suitable design parameters of a WAG project applied to real field data from “Gullfaks” in the North Sea. The proxy models aimed at reducing significantly the rum-time related to the commercial simulators without scarifying the accuracy. To this end, machine learning (ML) approaches, including multi-layer perceptron (MLP) and radial basis function neural network (RBFNN) were implemented for estimating the needed parameters for the formulated optimization problem. To improve the reliability of these ML methods, they were evolved using optimization algorithms, namely Levenberg–Marquardt (LM) for MLP, and ant colony optimization (ACO) and grey wolf optimization (GWO) for RBFNN. The performance analysis of the proxy models revealed that MLP-LMA has better prediction ability than the other two proxy paradigms. In this context, the highest average absolute relative deviation noticed per runs by MLP-LMA was lower than 3.60%. Besides, the best-implemented proxy was coupled with ACO and GWO for resolving the studied WAG optimization problem. The findings revealed that the suggested proxies are cheap, accurate, and practical in emulating the performance of numerical reservoir model. In addition, the results demonstrated the effectiveness of ACO and GWO in optimizing the parameters of WAG process for the real field data used in this study.",oceanology,141
10.1016/j.gsf.2020.04.016,filtered,Geoscience Frontiers,scopus,2020-09-01,sciencedirect,a novel type of neural networks for feature engineering of geological data: case studies of coal and gas hydrate-bearing sediments,https://api.elsevier.com/content/abstract/scopus_id/85087418242,"The nature of the measured data varies among different disciplines of geosciences. In rock engineering, features of data play a leading role in determining the feasible methods of its proper manipulation. The present study focuses on resolving one of the major deficiencies of conventional neural networks (NNs) in dealing with rock engineering data. Herein, since the samples are obtained from hundreds of meters below the surface with the utmost difficulty, the number of samples is always limited. Meanwhile, the experimental analysis of these samples may result in many repetitive values and 0s. However, conventional neural networks are incapable of making robust models in the presence of such data. On the other hand, these networks strongly depend on the initial weights and bias values for making reliable predictions. With this in mind, the current research introduces a novel kind of neural network processing framework for the geological that does not suffer from the limitations of the conventional NNs. The introduced single-data-based feature engineering network extracts all the information wrapped in every single data point without being affected by the other points. This method, being completely different from the conventional NNs, re-arranges all the basic elements of the neuron model into a new structure. Therefore, its mathematical calculations were performed from the very beginning. Moreover, the corresponding programming codes were developed in MATLAB and Python since they could not be found in any common programming software at the time being. This new kind of network was first evaluated through computer-based simulations of rock cracks in the 3DEC environment. After the model’s reliability was confirmed, it was adopted in two case studies for estimating respectively tensile strength and shear strength of real rock samples. These samples were coal core samples from the Southern Qinshui Basin of China, and gas hydrate-bearing sediment (GHBS) samples from the Nankai Trough of Japan. The coal samples used in the experiments underwent nuclear magnetic resonance (NMR) measurements, and Scanning Electron Microscopy (SEM) imaging to investigate their original micro and macro fractures. Once done with these experiments, measurement of the rock mechanical properties, including tensile strength, was performed using a rock mechanical test system. However, the shear strength of GHBS samples was acquired through triaxial and direct shear tests. According to the obtained result, the new network structure outperformed the conventional neural networks in both cases of simulation-based and case study estimations of the tensile and shear strength. Even though the proposed approach of the current study originally aimed at resolving the issue of having a limited dataset, its unique properties would also be applied to larger datasets from other subsurface measurements.",oceanology,142
10.1016/j.energy.2018.12.084,filtered,Energy,scopus,2019-03-01,sciencedirect,recurrent wavelet-based elman neural network with modified gravitational search algorithm control for integrated offshore wind and wave power generation systems,https://api.elsevier.com/content/abstract/scopus_id/85059621878,A new approach to rotational speed control structures based on an optimized intelligent recurrent wavelet-based Elman neural network (RWENN) controller used for the integration of offshore wind and wave energy conversion systems driven by a doubly fed induction generator. The nodes connecting the weights of the RWENN are trained online using a backpropagation method. A modified gravitational search algorithm (MGSA) is developed to adjust the learning rates and improve learning capability. The proposed control scheme has improved the real power regulation and dynamic performance of a combined wind and ocean wave energy scheme over a wide range of operating conditions. The performance of this control scheme is assessed by comparing it to a traditional proportional-integral based control scheme in a series of case studies representative of maximum power generation. Simulations are carried out using PSCAD/EMTDC software to verify the robustness of the power electronics converters and the efficiency of the proposed controller under steady state and transient conditions.,oceanology,143
10.1016/j.chroma.2018.06.016,filtered,Journal of Chromatography A,scopus,2018-08-24,sciencedirect,development and optimization of a solid-phase microextraction gas chromatography–tandem mass spectrometry methodology to analyse ultraviolet filters in beach sand,https://api.elsevier.com/content/abstract/scopus_id/85048474681,"A methodology based on solid-phase microextraction (SPME) followed by gas chromatography–tandem mass spectrometry (GC–MS/MS) has been developed for the simultaneous analysis of eleven multiclass ultraviolet (UV) filters in beach sand. To the best of our knowledge, this is the first time that this extraction technique is applied to the analysis of UV filters in sand samples, and in other kind of environmental solid samples. Main extraction parameters such as the fibre coating, the amount of sample, the addition of salt, the volume of water added to the sand, and the temperature were optimized. An experimental design approach was implemented in order to find out the most favourable conditions. The final conditions consisted of adding 1 mL of water to 1 g of sample followed by the headspace SPME for 20 min at 100 °C, using PDMS/DVB as fibre coating. The SPME-GC–MS/MS method was validated in terms of linearity, accuracy, limits of detection and quantification, and precision. Recovery studies were also performed at three concentration levels in real Atlantic and Mediterranean sand samples. The recoveries were generally above 85% and relative standard deviations below 11%. The limits of detection were in the pg g−1 level. The validated methodology was successfully applied to the analysis of real sand samples collected from Atlantic Ocean beaches in the Northwest coast of Spain and Portugal, Canary Islands (Spain), and from Mediterranean Sea beaches in Mallorca Island (Spain). The most frequently found UV filters were ethylhexyl salicylate (EHS), homosalate (HMS), 4-methylbenzylidene camphor (4MBC), 2-ethylhexyl methoxycinnamate (2EHMC) and octocrylene (OCR), with concentrations up to 670 ng g−1.",oceanology,144
10.1016/j.dsr2.2016.06.020,filtered,Deep-Sea Research Part II: Topical Studies in Oceanography,scopus,2016-11-01,sciencedirect,forecast of drifter trajectories using a rapid environmental assessment based on ctd observations,https://api.elsevier.com/content/abstract/scopus_id/84994805505,"A high resolution submesoscale resolving ocean model was implemented in a limited area north of Island of Elba where a maritime exercise, named Serious Game 1 (SG1), took place on May 2014 in the framework of the project MEDESS-4MS (Mediterranean Decision Support System for Marine Safety). During the exercise, CTD data have been collected responding to the necessity of a Rapid Environmental Assessment, i.e. to a rapid evaluation of the marine conditions able to provide sensible information for initialisation of modelling tools, in the scenario of possible maritime accidents. The aim of this paper is to evaluate the impact of such mesoscale-resolving CTD observations on short-term forecasts of the surface currents, within the framework of possible oil-spill related emergencies. For this reason, modelling outputs were compared with Lagrangian observations at sea: the high resolution modelled currents, together with the ones of the coarser sub-regional model WMED, are used to force the MEDSLIK-II oil-spill model to simulate drifter trajectories. Both ocean models have been assessed by comparing the prognostic scalar and vector fields as an independent CTD data set and with real drifter trajectories acquired during SG1. The diagnosed and prognosed circulation reveals that the area was characterised by water masses of Atlantic origin influenced by small mesoscale cyclonic and anti-cyclonic eddies, which govern the spatial and temporal evolution of the drifter trajectories and of the water masses distribution. The assimilation of CTD data into the initial conditions of the high resolution model highly improves the accuracy of the short-term forecast in terms of location and structure of the thermocline and positively influence the ability of the model in reproducing the observed paths of the surface drifters.",oceanology,145
10.1016/j.chroma.2016.06.078,filtered,Journal of Chromatography A,scopus,2016-08-19,sciencedirect,a polythiophene–silver nanocomposite for headspace needle trap extraction,https://api.elsevier.com/content/abstract/scopus_id/84979519284,"A nanocomposite consisting of polythiophene–silver was prepared and implemented as a desired sorbent for headspace needle trap extraction. Colloidal silver nanoparticles (Ag NPs) with narrow size distribution and high stability were synthesized in water–in–oil microemulsion. This simple procedure was adapted to prepare highly monodispersed Ag NPs, starting from an initial synthesis in sodium bis(2-ethylhexyl) sulfosuccinate (AOT) reverse micelles. Polythiophene (PT) was synthesized by chemical oxidative polymerization in the presence of anhydrous ferric chloride while its polymeric structure was confirmed by Fourier transform infrared spectrometry (FTIR). Eventually, the prepared PT was dispersed in an AOT/n-decane solution containing Ag NPs for 1h in which the NPs were adsorbed on the polymer surface. The dynamic light scattering (DLS) analysis of NPs solution revealed that the monodisperse Ag NPs have been synthesized successfully with the size distribution below 10nm. Other instrumentations such as scanning electron microscopy (SEM), energy dispersive spectroscopy (EDS) and atomic absorption spectrometry (AAS) confirmed the fabrication of the PT–Ag nanocomposite. The applicability of the synthesized sorbent was examined by needle trap extraction of some polycyclic aromatic hydrocarbons (PAHs) in aqueous samples in conjunction with gas chromatography–mass spectrometry detection (GC–MS). Important parameters influencing the extraction process were optimized. The linearity for all analytes was in the concentration range of 0.01–10ngmL−1. The limits of detections were in the range of 0.002–0.01ngmL−1, using time–scheduled selected ion monitoring (SIM) mode while the RSD% values (n
                     
                     =
                     
                     3) were all below 12%. The developed method was successfully applied to real water samples obtained from different rivers and Persian Gulf, while the relative recovery percentages were in the range of 85–103%.",oceanology,146
10.1016/j.procs.2014.05.104,filtered,Procedia Computer Science,scopus,2014-01-01,sciencedirect,integration of artificial neural networks into operational ocean wave prediction models for fast and accurate emulation of exact nonlinear interactions,https://api.elsevier.com/content/abstract/scopus_id/84902794493,"In this paper, an implementation study was undertaken to employ Artificial Neural Networks (ANN) in third-generation ocean wave models for direct mapping of wind-wave spectra into exact nonlinear interactions. While the investigation expands on previously reported feasibility studies of Neural Network Interaction Approximations (NNIA), it focuses on a new robust neural network that is implemented in Wavewatch III (WW3) model. Several idealistic and real test scenarios were carried out. The obtained results confirm the feasibility of NNIA in terms of speeding-up model calculations and is fully capable of providing operationally acceptable model integrations. The ANN is able to emulate the exact nonlinear interaction for single-and multi-modal wave spectra with a much higher accuracy then Discrete Interaction Approximation (DIA). NNIA performs at least twice as fast as DIA and at least two hundred times faster than exact method (Web-Resio-Tracy, WRT) for a well trained dataset. The accuracy of NNIA is network configuration dependent. For most optimal network configurations, the NNIA results and scatter statistics show good agreement with exact results by means of growth curves and integral parameters. Practical possibilities for further improvements in achieving fast and highly accurate emulations using ANN for emulating time consuming exact nonlinear interactions are also suggested and discussed.",oceanology,147
10.1016/j.ocecoaman.2013.06.005,filtered,Ocean and Coastal Management,scopus,2013-01-01,sciencedirect,developing a user-friendly decision support system for the wetlands corridor of the gulf of california,https://api.elsevier.com/content/abstract/scopus_id/84880379267,"In this paper we describe the development of the software TreeDSS, a user-friendly decision support system, developed as a central component of the project “The wetlands corridor of the Gulf of California (WCGC)”. It aims to provide a support tool to partners of the project for structuring and solving frequent multicriteria evaluation problems, such as suitability and vulnerability assessment, resource allocation and zoning. TreeDSS basically allows users to build decision trees by coding expert knowledge using a friendly graphical interface and couples automatically with the geographic information software ArcMap 9.3 to present results both, as tables and maps, making it possible for evaluators reviewing criteria, ranges and decisions reached in real time. To illustrate how the software can be used in a real management situation, we present, as case study the development of a suitability assessment model to identify potential feeding grounds for migratory birds along the WCGC.",oceanology,148
10.1016/j.ocemod.2010.07.006,filtered,Ocean Modelling,scopus,2011-01-01,sciencedirect,real time wave forecasting using wind time history and numerical model,https://api.elsevier.com/content/abstract/scopus_id/78649956822,"Operational activities in the ocean like planning for structural repairs or fishing expeditions require real time prediction of waves over typical time duration of say a few hours. Such predictions can be made by using a numerical model or a time series model employing continuously recorded waves. This paper presents another option to do so and it is based on a different time series approach in which the input is in the form of preceding wind speed and wind direction observations. This would be useful for those stations where the costly wave buoys are not deployed and instead only meteorological buoys measuring wind are moored. The technique employs alternative artificial intelligence approaches of an artificial neural network (ANN), genetic programming (GP) and model tree (MT) to carry out the time series modeling of wind to obtain waves. Wind observations at four offshore sites along the east coast of India were used. For calibration purpose the wave data was generated using a numerical model. The predicted waves obtained using the proposed time series models when compared with the numerically generated waves showed good resemblance in terms of the selected error criteria. Large differences across the chosen techniques of ANN, GP, MT were not noticed. Wave hindcasting at the same time step and the predictions over shorter lead times were better than the predictions over longer lead times. The proposed method is a cost effective and convenient option when a site-specific information is desired.",oceanology,149
10.1016/j.neunet.2009.06.006,filtered,Neural Networks,scopus,2010-04-01,sciencedirect,evaluation of atmospheric poaceae pollen concentration using a neural network applied to a coastal atlantic climate region,https://api.elsevier.com/content/abstract/scopus_id/76849100176,"In the South of Europe an important percentage of population suffers pollen allergies, being the Poaceae pollen the major source. One of aerobiology’s objectives is to develop statistical models enabling the short- and long-term prediction of atmospheric pollen concentrations to take preventative measures to protect allergic patients from the severity of the atmospheric pollen season. The implementation of a computational model based on supervised MLP neural network was applied for the prediction of the atmospheric Poaceae pollen concentration. There is a good correlation between the values predicted by the ANN for the training cases in comparison with the real pollen concentrations. A high coefficient of linear regression 
                        
                           (
                           
                              
                                 R
                              
                              
                                 2
                              
                           
                           )
                        
                      of 0.9696 was obtained. The accuracy of the neural network developed was tested with data from 2006 and 2007, which was not taken into account to establish the aforementioned models. Neural networks provided us a good tool to forecasting allergenic airborne pollen concentration helping the automation of the prediction system in the aerobiological information diffusion to the population suffering from allergic problems.",oceanology,150
10.1016/j.oceaneng.2008.05.003,filtered,Ocean Engineering,scopus,2008-08-01,sciencedirect,soft computing approach for real-time estimation of missing wave heights,https://api.elsevier.com/content/abstract/scopus_id/46749151701,"This paper presents soft computing approach for estimation of missing wave heights at a particular location on a real-time basis using wave heights at other locations. Six such buoy networks are developed in Eastern Gulf of Mexico using soft computing techniques of Artificial Neural Networks (ANN) and Genetic Programming (GP). Wave heights at five stations are used to estimate wave height at the sixth station. Though ANN is now an established tool in time series analysis, use of GP in the field of time series forecasting/analysis particularly in the area of Ocean Engineering is relatively new and needs to be explored further. Both ANN and GP approach perform well in terms of accuracy of estimation as evident from values of various statistical parameters employed. The GP models work better in case of extreme events. Results of both approaches are also compared with the performance of large-scale continuous wave modeling/forecasting system WAVEWATCH III. The models are also applied on real time basis for 3 months in the year 2007. A software is developed using evolved GP codes (C++) as back end with Visual Basic as the Front End tool for real-time application of wave estimation model.",oceanology,151
10.1002/gdj3.114,filtered,core,'Wiley',2021-11-01 00:00:00,core,a real‐world dataset and data simulation algorithm for automated fish species identification,,"Abstract Developing high‐performing machine learning algorithms requires large amounts of annotated data. Manual annotation of data is labour‐intensive, and the cost and effort needed are an important obstacle to the development and deployment of automated analysis. In a previous work, we have shown that deep learning classifiers can successfully be trained on synthetic images and annotations. Here, we provide a curated set of fish image data and backgrounds, the necessary software tools to generate synthetic images and annotations, and annotated real datasets to test classifier performance. The dataset is constructed from images collected using the Deep Vision system during two surveys from 2017 and 2018 that targeted economically important pelagic species in the Northeast Atlantic Ocean. We annotated a total of 1,879 images, randomly selected across trawl stations from both surveys, comprising 482 images of blue whiting, 456 images of Atlantic herring, 341 images of Atlantic mackerel, 335 images of mesopelagic fishes and 265 images containing a mixture of the four categories",oceanology,152
10.1029/2019ms001965,filtered,core,'American Geophysical Union (AGU)',2021-01-01 00:00:00,core,a deep learning approach to spatiotemporal ssh interpolation and estimation of deep currents in geostrophic ocean turbulence,,"Satellite altimeters provide global observations of sea surface height (SSH) and present a unique dataset for advancing our theoretical understanding of upper ocean dynamics and monitoring its variability. Considering that mesoscale SSH patterns can evolve on timescales comparable to or shorter than satellite return periods, it is challenging to accurately reconstruct the continuous SSH evolution as currently available altimetry observations are still spatially and temporally sparse. Here we explore the possibility of SSH interpolation via Deep Learning by using synthetic observations from an idealized quasigeostrophic (QG) model of baroclinic ocean turbulence. We demonstrate that Convolutional Neural Networks with Residual Learning are superior in SSH reconstruction to linear and recently developed dynamical interpolation techniques. Also, the deep neural networks can provide a skillful state estimate of unobserved deep ocean currents at mesoscales. These conspicuous results suggest that SSH patterns of eddies might contain substantial information about the underlying deep ocean currents that are necessary for SSH prediction. Our training data is focused on highly idealized physics and diversification of processes needs to be considered to more accurately represent the real ocean. In addition, methodological improvements such as transfer learning and implementation of dynamically‐aware loss functions might be necessary to consider before its ultimate use with real satellite observations. Nonetheless, by providing a proof of concept based on synthetic data, our results point to deep learning as a viable alternative to existing interpolation and, more generally, state estimation methods for satellite observations of eddying currents.
Plain Language Summary
Satellite observations of sea surface height (SSH) are widely used to derive surface ocean currents on a global scale. However, due to gaps in SSH observations, it remains challenging to retrieve the dynamics of rapidly evolving upper‐ocean currents. To overcome this limitation, we propose a Deep Learning framework that is based on pattern recognition extracted from SSH observations. Using synthetic data generated from a simplified model of ocean turbulence, we demonstrate that deep learning can accurately estimate both surface and sub‐surface ocean currents, significantly outperforming the most commonly used techniques. By providing a proof of concept, our study highlights the strong potential of deep learning for estimating ocean currents from satellite observations",oceanology,153
10.3390/s17112606,filtered,core,'MDPI AG',2017-11-13 00:00:00,core,abs-fishcount: an agent-based simulator of underwater sensors for measuring the amount of fish,https://riunet.upv.es/bitstream/handle/10251/148910/Garc%c3%ada-Magari%c3%b1o%3bLacuesta%3bLloret%20-%20ABS-FishCount%3a%20An%20Agent-Based%20Simulator%20of%20Underwater%20Sensors%20f....pdf?sequence=1&isAllowed=y,"[EN] Underwater sensors provide one of the possibilities to explore oceans, seas, rivers, fish farms and dams, which all together cover most of our planet's area. Simulators can be helpful to test and discover some possible strategies before implementing these in real underwater sensors. This speeds up the development of research theories so that these can be implemented later. In this context, the current work presents an agent-based simulator for defining and testing strategies for measuring the amount of fish by means of underwater sensors. The current approach is illustrated with the definition and assessment of two strategies for measuring fish. One of these two corresponds to a simple control mechanism, while the other is an experimental strategy and includes an implicit coordination mechanism. The experimental strategy showed a statistically significant improvement over the control one in the reduction of errors with a large Cohen's d effect size of 2.55.This work acknowledges the research project  Desarrollo Colaborativo de Soluciones AAL  with reference TIN2014-57028-R funded by the Spanish Ministry of Economy and Competitiveness. This work has been supported by the program  Estancias de movilidad en el extranjero José Castillejo para jóvenes doctores  funded by the Spanish Ministry of Education, Culture and Sport with reference CAS17/00005. We also acknowledge support from  Universidad de Zaragoza ,  Fundación Bancaria Ibercaja  and  Fundación CAI  in the  Programa Ibercaja-CAI de Estancias de Investigación  with reference IT24/16. We acknowledge the research project  Construcción de un framework para agilizar el desarrollo de aplicaciones móviles en el ámbito de la salud  funded by University of Zaragoza and Foundation Ibercaja with grant reference JIUZ-2017-TEC-03. It has also been supported by  Organismo Autónomo Programas Educativos Europeos  with reference 2013-1-CZ1-GRU06-14277. We also aknowledge support from project  Sensores vestibles y tecnología móvil como apoyo en la formación y práctica de mindfulness: prototipo previo aplicado a bienestar  funded by University of Zaragoza with grant number UZ2017-TEC-02. Furthermore, we acknowledge the  Fondo Social Europeo  and the  Departamento de Tecnología y Universidad del Gobierno de Aragón  for their joint support with grant number Ref-T81.García-Magariño, I.; Lacuesta Gilabert, R.; Lloret, J. (2017). ABS-FishCount: An Agent-Based Simulator of Underwater Sensors for Measuring the Amount of Fish. Sensors. 17(11):1-19. https://doi.org/10.3390/s17112606S1191711Lloret, J. (2013). Underwater Sensor Nodes and Networks. Sensors, 13(9), 11782-11796. doi:10.3390/s130911782Akyildiz, I. F., Pompili, D., & Melodia, T. (2005). Underwater acoustic sensor networks: research challenges. Ad Hoc Networks, 3(3), 257-279. doi:10.1016/j.adhoc.2005.01.004Santos, R., Orozco, J., Micheletto, M., Ochoa, S., Meseguer, R., Millan, P., & Molina, C. (2017). Real-Time Communication Support for Underwater Acoustic Sensor Networks. Sensors, 17(7), 1629. doi:10.3390/s17071629Das, A. P., & Thampi, S. M. (2017). Simulation Tools for Underwater Sensor Networks: A Survey. Network Protocols and Algorithms, 8(4), 41. doi:10.5296/npa.v8i4.10471Kawahara, R., Nobuhara, S., & Matsuyama, T. (2016). Dynamic 3D capture of swimming fish by underwater active stereo. Methods in Oceanography, 17, 118-137. doi:10.1016/j.mio.2016.08.002Schaner, T., Fox, M. G., & Taraborelli, A. C. (2009). An inexpensive system for underwater video surveys of demersal fishes. Journal of Great Lakes Research, 35(2), 317-319. doi:10.1016/j.jglr.2008.12.003Shinoda, R., Wu, H., Murata, M., Ohnuki, H., Yoshiura, Y., & Endo, H. (2017). Development of an optical communication type biosensor for real-time monitoring of fish stress. Sensors and Actuators B: Chemical, 247, 765-773. doi:10.1016/j.snb.2017.03.034Chen, Z., Zhang, Z., Dai, F., Bu, Y., & Wang, H. (2017). Monocular Vision-Based Underwater Object Detection. Sensors, 17(8), 1784. doi:10.3390/s17081784Saberioon, M. M., & Cisar, P. (2016). Automated multiple fish tracking in three-Dimension using a Structured Light Sensor. Computers and Electronics in Agriculture, 121, 215-221. doi:10.1016/j.compag.2015.12.014Pais, M. P., & Cabral, H. N. (2017). Fish behaviour effects on the accuracy and precision of underwater visual census surveys. A virtual ecologist approach using an individual-based model. Ecological Modelling, 346, 58-69. doi:10.1016/j.ecolmodel.2016.12.011Burget, P., & Pachner, D. (2005). FISH FARM AUTOMATION. IFAC Proceedings Volumes, 38(1), 137-142. doi:10.3182/20050703-6-cz-1902.02113Simon, Y., Levavi-Sivan, B., Cahaner, A., Hulata, G., Antler, A., Rozenfeld, L., & Halachmi, I. (2017). A behavioural sensor for fish stress. Aquacultural Engineering, 77, 107-111. doi:10.1016/j.aquaeng.2017.04.001Petreman, I. C., Jones, N. E., & Milne, S. W. (2014). Observer bias and subsampling efficiencies for estimating the number of migrating fish in rivers using Dual-frequency IDentification SONar (DIDSON). Fisheries Research, 155, 160-167. doi:10.1016/j.fishres.2014.03.001Garcia, M., Sendra, S., Lloret, G., & Lloret, J. (2011). Monitoring and control sensor system for fish feeding in marine fish farms. IET Communications, 5(12), 1682-1690. doi:10.1049/iet-com.2010.0654Lloret, J., Garcia, M., Sendra, S., & Lloret, G. (2014). An underwater wireless group-based sensor network for marine fish farms sustainability monitoring. Telecommunication Systems, 60(1), 67-84. doi:10.1007/s11235-014-9922-3Bharamagoudra, M. R., Manvi, S. S., & Gonen, B. (2017). Event driven energy depth and channel aware routing for underwater acoustic sensor networks: Agent oriented clustering based approach. Computers & Electrical Engineering, 58, 1-19. doi:10.1016/j.compeleceng.2017.01.004Gallehdari, Z., Meskin, N., & Khorasani, K. (2017). Distributed reconfigurable control strategies for switching topology networked multi-agent systems. ISA Transactions, 71, 51-67. doi:10.1016/j.isatra.2017.06.008Jurdak, R., Elfes, A., Kusy, B., Tews, A., Hu, W., Hernandez, E., … Sikka, P. (2015). Autonomous surveillance for biosecurity. Trends in Biotechnology, 33(4), 201-207. doi:10.1016/j.tibtech.2015.01.003García-Magariño, I., & Plaza, I. (2015). FTS-SOCI: An agent-based framework for simulating teaching strategies with evolutions of sociograms. Simulation Modelling Practice and Theory, 57, 161-178. doi:10.1016/j.simpat.2015.07.003Cooke, S. J., Brownscombe, J. W., Raby, G. D., Broell, F., Hinch, S. G., Clark, T. D., & Semmens, J. M. (2016). Remote bioenergetics measurements in wild fish: Opportunities and challenges. Comparative Biochemistry and Physiology Part A: Molecular & Integrative Physiology, 202, 23-37. doi:10.1016/j.cbpa.2016.03.022García, M. R., Cabo, M. L., Herrera, J. R., Ramilo-Fernández, G., Alonso, A. A., & Balsa-Canto, E. (2017). Smart sensor to predict retail fresh fish quality under ice storage. Journal of Food Engineering, 197, 87-97. doi:10.1016/j.jfoodeng.2016.11.006Tušer, M., Frouzová, J., Balk, H., Muška, M., Mrkvička, T., & Kubečka, J. (2014). Evaluation of potential bias in observing fish with a DIDSON acoustic camera. Fisheries Research, 155, 114-121. doi:10.1016/j.fishres.2014.02.031Rakowitz, G., Tušer, M., Říha, M., Jůza, T., Balk, H., & Kubečka, J. (2012). Use of high-frequency imaging sonar (DIDSON) to observe fish behaviour towards a surface trawl. Fisheries Research, 123-124, 37-48. doi:10.1016/j.fishres.2011.11.018Cenek, M., & Franklin, M. (2017). An adaptable agent-based model for guiding multi-species Pacific salmon fisheries management within a SES framework. Ecological Modelling, 360, 132-149. doi:10.1016/j.ecolmodel.2017.06.024Gao, L., & Hailu, A. (2011). Evaluating the effects of area closure for recreational fishing in a coral reef ecosystem: The benefits of an integrated economic and biophysical modeling. Ecological Economics, 70(10), 1735-1745. doi:10.1016/j.ecolecon.2011.04.014Helbing, D., & Balietti, S. (2011). From social simulation to integrative system design. The European Physical Journal Special Topics, 195(1), 69-100. doi:10.1140/epjst/e2011-01402-7Reynolds, C. W. (1987). Flocks, herds and schools: A distributed behavioral model. ACM SIGGRAPH Computer Graphics, 21(4), 25-34. doi:10.1145/37402.37406Beltran, R. S., Testa, J. W., & Burns, J. M. (2017). An agent-based bioenergetics model for predicting impacts of environmental change on a top marine predator, the Weddell seal. Ecological Modelling, 351, 36-50. doi:10.1016/j.ecolmodel.2017.02.002Berman, M., Nicolson, C., Kofinas, G., Tetlichi, J., & Martin, S. (2004). Adaptation and Sustainability in a Small Arctic Community : Results of an Agent-based Simulation Model. ARCTIC, 57(4). doi:10.14430/arctic517Kadir, H. A., & Arshad, M. R. (2015). Cooperative Multi Agent System for Ocean Observation System based on Consensus Algorithm. Procedia Computer Science, 76, 203-208. doi:10.1016/j.procs.2015.12.343Trygonis, V., Georgakarakos, S., Dagorn, L., & Brehmer, P. (2016). Spatiotemporal distribution of fish schools around drifting fish aggregating devices. Fisheries Research, 177, 39-49. doi:10.1016/j.fishres.2016.01.013De Kerckhove, D. T., Milne, S., & Shuter, B. J. (2015). Measuring fish school swimming speeds with two acoustic beams and determining the angle of the school detection. Fisheries Research, 172, 432-439. doi:10.1016/j.fishres.2015.08.001Source Code of the Agent-Based Simulator of Underwater Sensors for Measuring the Amount of Fishes Called ABS-FishCounthttp://dx.doi.org/10.17632/yzmt73x8j8.1Cossentino, M., Gaud, N., Hilaire, V., Galland, S., & Koukam, A. (2009). ASPECS: an agent-oriented software process for engineering complex systems. Autonomous Agents and Multi-Agent Systems, 20(2), 260-304. doi:10.1007/s10458-009-9099-4García-Magariño, I., Palacios-Navarro, G., & Lacuesta, R. (2017). TABSAOND: A technique for developing agent-based simulation apps and online tools with nondeterministic decisions. Simulation Modelling Practice and Theory, 77, 84-107. doi:10.1016/j.simpat.2017.05.006García-Magariño, I., Gómez-Rodríguez, A., González-Moreno, J. C., & Palacios-Navarro, G. (2015). PEABS: A Process for developing Efficient Agent-Based Simulators. Engineering Applications of Artificial Intelligence, 46, 104-112. doi:10.1016/j.engappai.2015.09.003Rosenthal, J. A. (1996). Qualitative Descriptors of Strength of Association and Effect Size. Journal of Social Service Research, 21(4), 37-59. doi:10.1300/j079v21n04_0",oceanology,154
10.5194/npg-21-521-2014,filtered,core,'Copernicus GmbH',2014-04-23 00:00:00,core,full-field and anomaly initialization using a low-order climate model: a comparison and proposals for advanced formulations,https://core.ac.uk/download/305109079.pdf,"Initialization techniques for seasonal-to-decadal climate predictions fall into two main categories; namely full-field initialization (FFI) and anomaly initialization (AI). In the FFI case the initial model state is replaced by the best possible available estimate of the real state. By doing so the initial error is efficiently reduced but, due to the unavoidable presence of model deficiencies, once the model is let free to run a prediction, its trajectory drifts away from the observations no matter how small the initial error is. This problem is partly overcome with AI where the aim is to forecast future anomalies by assimilating observed anomalies on an estimate of the model climate.



The large variety of experimental setups, models and observational networks adopted worldwide make it difficult to draw firm conclusions on the respective advantages and drawbacks of FFI and AI, or to identify distinctive lines for improvement. The lack of a unified mathematical framework adds an additional difficulty toward the design of adequate initialization strategies that fit the desired forecast horizon, observational network and model at hand.



Here we compare FFI and AI using a low-order climate model of nine ordinary differential equations and use the notation and concepts of data assimilation theory to highlight their error scaling properties. This analysis suggests better performances using FFI when a good observational network is available and reveals the direct relation of its skill with the observational accuracy. The skill of AI appears, however, mostly related to the model quality and clear increases of skill can only be expected in coincidence with model upgrades.



We have compared FFI and AI in experiments in which either the full system or the atmosphere and ocean were independently initialized. In the former case FFI shows better and longer-lasting improvements, with skillful predictions until month 30. In the initialization of single compartments, the best performance is obtained when the stabler component of the model (the ocean) is initialized, but with FFI it is possible to have some predictive skill even when the most unstable compartment (the extratropical atmosphere) is observed.



Two advanced formulations, least-square initialization (LSI) and exploring parameter uncertainty (EPU), are introduced. Using LSI the initialization makes use of model statistics to propagate information from observation locations to the entire model domain. Numerical results show that LSI improves the performance of FFI in all the situations when only a portion of the system's state is observed. EPU is an online drift correction method in which the drift caused by the parametric error is estimated using a short-time evolution law and is then removed during the forecast run. Its implementation in conjunction with FFI allows us to improve the prediction skill within the first forecast year.



Finally, the application of these results in the context of realistic climate models is discussed",oceanology,155
7db240ccd5e4bbb8a520d24318e8523eda544e9a,filtered,semantic_scholar,ELIV 2021,2021-01-01 00:00:00,semantic_scholar,ansys real time physics based radar simulation – an enabler for machine learning in the context of autonomous driving,https://www.semanticscholar.org/paper/7db240ccd5e4bbb8a520d24318e8523eda544e9a,"Throughout the evolution of Advanced Driver Assistance Systems (ADAS), the correct perception of the environment has always been a decisive success factor. Capturing and defining scenarios/edge cases, various and heterogenous datasets, multiple sensors/sensorfusion architectures, and perception algorithms are just a few of the many challenges we are facing when implementing such systems. To cope with such levels of complexity, modular approaches are required. Such approaches target flexibility and standardized interfaces between data provided by various sensor modules/models and driving functions. In the Artificial Intelligence (AI) domain, and more precisely when dealing with supervised training of Neural Networks (NN), obtaining valid and accurately labeled datasets is essential. By enabling Machine Learning (ML) in electromagnetic applications, Ansys physics-based Real Time Radar (RTR) introduces a new paradigm for sensor development and integration that leverages GPU hardware and new algorithms to accelerate simulation by orders of magnitude without compromising accuracy. In this paper, a comprehensive workflow for the generation of virtual datasets using the Open Simulation Interface (OSI) will be presented. This workflow will illustrate how the scenario variation process coupled with RTR facilitates the creation of heterogenous/labeled datasets that are ready for training object detection NN. Finally, this presentation will also show the preliminary results obtained when implementing this process. Introduction Machine Learning (ML) is gradually taking over “conventional” algorithms that were previously designed to help make Autonomous Vehicles (AVs) a reality. Several auto giants like BMW, VDI-Berichte Nr. 2384, 2021 83 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. Volkswagen, and Volvo are at least partially relying on ML algorithms to solve various parts of the sense-plan-act paradigm. Others, like Tesla [1] are solely relying on ML and in some cases Artificial Intelligence (AI) to provide an end-to-end solution. AVs usually rely on several sensors of different types to perceive their surrounding environment. As such sensors continuously scan the environment and generate raw data, the perception stack processes this data and generates a meaningful virtual map of the surrounding environment. In the area of Computer Vision (CV), usually relying on optical systems, ML algorithms are already successfully deployed in commercial systems [2] – [4]. In addition to optical systems, and due to their superior performances in bad weather conditions and dark environments, radars have also made their way into the AV’s sensor stacks. Though not frequently encountered, ML has also been used to replace some of the traditional radar signal processing algorithms. For example, in [5] the author demonstrates how a fully convolutional network can be used for object detection and 3D estimation using a Frequency-Modulated ContinuousWave (FMCW) radar. Contrary to [5], which is using real data for the training process, the author in [6] illustrates the power of physics-based simulation to also demonstrate the feasibility of using ML approaches to solve radar-based perception problems. As the training process of ML algorithms highly relies on labeled training data, Ansys’ Real Time Radar (RTR) automates generation of labeled data sets by shifting data generation and labeling from the real world to the virtual world. In addition, having an API which is compatible with the Open Simulation Interface (OSI) [7] ensures that a standardized interface is being deployed to describe the virtual environment in which generated scenarios are executed. Finally, this paper illustrates how, with the help of Ansys optiSlang [8], a tool chain is developed to orchestrate the scenario variation process and the simulation workflow to automatically generate labeled datasets for training a Neural Network (NN) based object detection algorithm. Ansys Real Time Radar RTR is an all-GPU implementation of the shooting and bouncing rays (SBR) method optimized for the automotive radar application to simulate a scenario in real time/faster than real time. The simulation, which is based on an arbitrary 3D scene/actor geometry, electrical material VDI-Berichte Nr. 2384, 2021 84 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. properties, including transmissive and reflective dielectrics, 3D polarized antenna patterns, directly calculates the scattered electro-magnetic fields as observed by the radar. As an output, RTR can generate raw I and I+Q A/D data for multi-channel radars in dynamically changing driving scenarios. Objects (e.g., vehicles, pedestrians, road, infrastructure, etc.) can be assigned arbitrary positions, orientations, linear and angular velocities in a scene graph hierarchy through a light-weight API to characterize complex traffic scenarios with negligible simulation overhead. To measure Doppler velocity, automotive radars transmit, receive, and process hundreds of chirps over each Coherent Processing Interval (CPI). Fast Fourier Transformations (FFT) and several post processing algorithms are then applied to hundreds of samples from each chirp/CPI to obtain range-Doppler (RD) images, which will be used for Neural Network (NN) training. These images, as represented in Fig. 1, give a visualization of all scattered fields in terms of relative velocity (Doppler) and distance from the radar (range). Fig. 1: Example of Range-Doppler image. As previously mentioned, RTR includes a lightweight C++ and Python API, enabling it to be integrated into nearly any driving simulator available on the market. In Fig. 2 the API’s main interfaces are depicted. VDI-Berichte Nr. 2384, 2021 85 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. Fig. 2: RTR's API representing its main inputs and outputs. Starting from left to right, the API give the user access to the following: 1. “Radar Config” enables the user to configure radar waveforms, radar modes, and antenna patterns. Radar waveforms are defined by parameters such as center frequency, bandwidth, number of frequency samples, CPI duration, number of chirps, number of transmit and receive antennas, and relative antenna positions. 2. “Object and Materials” helps the user build the 3D environment to be simulated and assign dielectric material properties. For example, as presented in Fig. 3, a vehicle can be imported as a set of subcomponents. Users can assign appropriate material properties for each component. VDI-Berichte Nr. 2384, 2021 86 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. Fig. 3: Vehicle subcomponents featuring adequate assignment of material properties. 3. “Object Velocities” represents scene dynamics where position and velocity updates are provided at each simulation time step. Such data is usually obtained from any driving simulator or from a set of pre-recorded/measured GPS/IMU data. At each time step, RTR executes a physics-based radar simulation of the scene and returns either the RD data per channel or the raw I/Q channel data (post A/D conversion). Open Simulation Interface To ensure modularity and interchangeability, RTR’s inputs have been adapted to support OSI. Focusing on environment perception and automated driving functions, OSI is an interface specification for models and components of distributed simulation. It defines a generic interface that ensures modularity, interoperability, and integration of simulation framework’s individual components. In addition, OSI was developed to address and align with the emerging standard for communication interfaces of real sensors, ISO 23150 [9]. This will eventually ensure a better correlation between communication interfaces used in both virtual and real worlds. Corresponding to OSI’s message description, the OSI:SensorView message represented in Fig. 4 contains the ground truth data that can be generated by any 3rd party driving simulator. The message includes information about the states of dynamic and static actors. VDI-Berichte Nr. 2384, 2021 87 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. Fig. 4: Driving Simulator and RTR connection via OSI. Scenario Variation Neural Networks (NN) are designed to behave as low bias and high variance machines that can perform extremely well on training data. To generalize such machines to new environments, heterogeneous datasets are essential for the training process. Using Ansys optiSlang, scenario variations were generated based on a predefined set of parameters. As represented in Table 1, a set of three parameters were chosen. Table 1: Scenario Variation Parameters Parameter Description Model Name Describes the 3D geometry of the vehicle. Initial Speed A range between 0 and 80 km/h Driver Behavior Aggressive, Normal, Cautious 1. “Model Name” defines whether a traffic participant is a car, bus, or a motorcycle. It also describes what vehicle model to use, ensuring a large variety of traffic participants within each generated scenario. 2. “Initial Speed” may randomly vary between 0 and 80 km/h. Considering that the NN is being trained on RD images, this parameter will ensure that RD scattered fields are randomly distributed in the RD image space along the Doppler velocity axis. 3. “Driver Behavior” takes in three different values: Aggressive, Normal and Cautious. Each behaviour ",oceanology,156
1de1fb5a0530c5c144c8a78fb653c8c7bc58c108,filtered,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,sensor-based human activity recognition: overcoming issues in a real world setting,https://www.semanticscholar.org/paper/1de1fb5a0530c5c144c8a78fb653c8c7bc58c108,"The rapid growing of the population age in industrialized societies calls for advanced tools to continuous monitor the activities of people. The goals of those tools are usually to support active and healthy ageing, and to early detect possible health issues to enable a long and independent life. Recent advancements in sensor miniaturization and wireless communications have paved the way to unobtrusive activity recognition systems. Hence, many pervasive health care systems have been proposed which monitor activities through unobtrusive sensors and by machine learning or artificial intelligence methods. Unfortunately, while those systems are effective in controlled environments, their actual effectiveness out of the lab is still limited due to different shortcomings of existing approaches. 
 
In this work, we explore such systems and aim to overcome existing limitations and shortcomings. Focusing on physical movements and crucial activities, our goal is to develop robust activity recognition methods based on external and wearable sensors that generate high quality results in a real world setting. Under laboratory conditions, existing research already showed that wearable sensors are suitable to recognize physical activities while external sensors are promising for activities that are more complex. Consequently, we investigate problems that emerge when coming out of the lab. This includes the position handling of wearable devices, the need of large expensive labeled datasets, the requirement to recognize activities in almost real-time, the necessity to adapt deployed systems online to changes in behavior of the user, the variability of executing an activity, and to use data and models across people. As a result, we present feasible solutions for these problems and provide useful insights for implementing corresponding techniques. Further, we introduce approaches and novel methods for both external and wearable sensors where we also clarify limitations and capabilities of the respective sensor types. Thus, we investigate both types separately to clarify their contribution and application use in respect of recognizing different types of activities in a real world scenario. 
 
Overall, our comprehensive experiments and discussions show on the one hand the feasibility of physical activity recognition but also recognizing complex activities in a real world scenario. Comparing our techniques and results with existing works and state-of-the-art techniques also provides evidence concerning the reliability and quality of the proposed techniques. On the other hand, we also identify promising research directions and highlight that combining external and wearable sensors seem to be the next step to go beyond activity recognition. In other words, our results and discussions also show that combining external and wearable sensors would compensate weaknesses of the individual sensors in respect of certain activity types and scenarios. Therefore, by addressing the outlined problems, we pave the way for a hybrid approach. Along with our presented solutions, we conclude our work with a high-level multi-tier activity recognition architecture showing that aspects like physical activity, (emotional) condition, used objects, and environmental features are critical for reliable recognizing complex activities.",oceanology,157
6e04016b9502b3bf8ccc10f5ae775c5e12531ec8,filtered,semantic_scholar,2008 5th International Conference on Networked Sensing Systems,2008-01-01 00:00:00,semantic_scholar,pluggable real world interfaces physically enabled code deployment for networked sensors,https://www.semanticscholar.org/paper/6e04016b9502b3bf8ccc10f5ae775c5e12531ec8,"In this paper we present a novel abstraction and deployment process using real world interfaces, which reflect the realities of pervasive software development. Pluggable real world interfaces support ldquoplugpsilanpsilaplayrdquo deployment for sensor-augmented hardware and provide an object-oriented encapsulation of high-level contextual interfaces. The architecture adds an additional object based abstraction layer between the sensor subsystem (delivering e.g. cues) and the application (delivering the situation context). Component abstraction layers are implemented as code that comes with physical components, e.g. a chair, and provides the functionality for detecting context bundled with the sensory hardware. The approach will lead to pluggable real world interfaces: The functionality of an appliance will be composed from the functionality of its components - just like a meeting room will be composed from many chairs. This paper will present concept, architecture and a first implementation based on a Java run-time system for very tiny, very low-power embedded sensor nodes.",oceanology,158
b42fd5d2959d327a2eaa28784b744f74f6b4e6b7,filtered,semantic_scholar,,2010-01-01 00:00:00,semantic_scholar,configuration management best practices: practical methods that work in the real world,https://www.semanticscholar.org/paper/b42fd5d2959d327a2eaa28784b744f74f6b4e6b7,"Successfully Implement High-Value Configuration Management Processes in Any Development Environment As IT systems have grown increasingly complex and mission-critical, effective configuration management (CM) has become critical to an organizations success. Using CM best practices, IT professionals can systematically manage change, avoiding unexpected problems introduced by changes to hardware, software, or networks. Now, todays best CM practices have been gathered in one indispensable resource showing you how to implement them throughout any agile or traditional development organization. Configuration Management Best Practices is practical, easy to understand and apply, and fully reflects the day-to-day realities faced by practitioners. Bob Aiello and Leslie Sachs thoroughly address all six pillars of CM: source code management, build engineering, environment configuration, change control, release engineering, and deployment. They demonstrate how to implement CM in ways that support software and systems development, meet compliance rules such as SOX and SAS-70, anticipate emerging standards such as IEEE/ISO 12207, and integrate with modern frameworks such as ITIL, COBIT, and CMMI. Coverage includes Using CM to meet business objectives, contractual requirements, and compliance rules Enhancing quality and productivity through lean processes and just-in-time process improvement Getting off to a good start in organizations without effective CM Implementing a Core CM Best Practices Framework that supports the entire development lifecycle Mastering the people side of CM: rightsizing processes, overcoming resistance, and understanding workplace psychology Architecting applications to take full advantage of CM best practices Establishing effective IT controls and compliance Managing tradeoffs and costs and avoiding expensive pitfalls Configuration Management Best Practices is the essential resource for everyone concerned with CM: from CTOs and CIOs to development, QA, and project managers and software engineers to analysts, testers, and compliance professionals. Praise for Configuration Management Best Practices Understanding change is critical to any attempt to manage change. Bob Aiello and Leslie Sachss Configuration Management Best Practices presents fundamental definitions and explanations to help practitioners understand change and its potential impact. Mary Lou A. Hines Fritts, CIO and Vice Provost Academic Programs, University of Missouri-Kansas City Few books on software configuration management emphasize the role of people and organizational context in defining and executing an effective SCM process. Bob Aiello and Leslie Sachss book will give you the information you need not only to manage change effectively but also to manage the transition to a better SCM process. Steve Berczuk, Agile Software Developer, and author of Software Configuration Management Patterns: Effective Teamwork, Practical Integration Bob Aiello and Leslie Sachs succeed handsomely in producing an important book, at a practical and balanced level of detail, for this topic that often goes without saying (and hence gets many projects into deep trouble). Their passion for the topic shows as they cover a wonderful range of topicseven culture, personality, and dealing with resistance to changein an accessible form that can be applied to any project. The software industry has needed a book like this for a long time! Jim Brosseau, Clarrus Consulting Group, and author of Software Teamwork: Taking Ownership for Success A must read for anyone developing or managing software or hardware projects. Bob Aiello and Leslie Sachs are able to bridge the language gap between the myriad of communities involved with successful Configuration Management implementations. They describe practical, real world practices that can be implemented by developers, managers, standard makers, and even Classical CM Folk. Bob Ventimiglia, Bobev Consulting A fresh and smart review of todays key concepts of SCM, build management, and related key practices on day-to-day software engineering. From the voice of an expert, Bob Aiello and Leslie Sachs offer an invaluable resource to success in SCM. Pablo Santos Luaces, CEO of Codice Software Bob Aiello and Leslie Sachs have a gift for stimulating the types of conversation and thought that necessarily precede needed organizational change. What they have to say is always interesting and often important. Marianne Bays, Business Consultant, Manager and Educator",oceanology,159
1592204ecb311b4e70b9d79f9722a7878e04b886,filtered,semantic_scholar,JMIR public health and surveillance,2020-01-01 00:00:00,semantic_scholar,agile requirements engineering and software planning for a digital health platform to engage the effects of isolation caused by social distancing: case study,https://www.semanticscholar.org/paper/1592204ecb311b4e70b9d79f9722a7878e04b886,"Background Social distancing and shielding measures have been put in place to reduce social interaction and slow the transmission of the coronavirus disease (COVID-19). For older people, self-isolation presents particular challenges for mental health and social relationships. As time progresses, continued social distancing could have a compounding impact on these concerns. Objective This project aims to provide a tool for older people and their families and peers to improve their well-being and health during and after regulated social distancing. First, we will evaluate the tool’s feasibility, acceptability, and usability to encourage positive nutrition, enhance physical activity, and enable virtual interaction while social distancing. Second, we will be implementing the app to provide an online community to assist families and peer groups in maintaining contact with older people using goal setting. Anonymized data from the app will be aggregated with other real-world data sources to develop a machine learning algorithm to improve the identification of patients with COVID-19 and track for real time use by health systems. Methods Development of this project is occurring at the time of publication, and therefore, a case study design was selected to provide a systematic means of capturing software engineering in progress. The app development framework for software design was based on agile methods. The evaluation of the app’s feasibility, acceptability and usability shall be conducted using Public Health England's guidance on evaluating digital health products, Bandura’s model of health promotion, the Reach Effectiveness Adoption Implementation Maintenance (RE-AIM) framework and the Nonadoption, Abandonment and Challenges to the Scale-up, Spread and Suitability (NASSS) framework. Results Making use of a pre-existing software framework for health behavior change, a proof of concept was developed, and a multistage app development and deployment for the solution was created. Grant submissions to fund the project and study execution have been sought at the time of publication, and prediscovery iteration of the solution has begun. Ethical approval for a feasibility study design is being sought. Conclusions This case study lays the foundations for future app development to combat mental and societal issues arising from social distancing measures. The app will be tested and evaluated in future studies to allow continuous improvement of the app. This novel contribution will provide an evidence-based exemplar for future app development in the space of social isolation and loneliness.",oceanology,160
c3d3d7b56fcc775c96777b1c6123b46a452b4aee,filtered,semantic_scholar,,2010-01-01 00:00:00,semantic_scholar,data-centric programming best practices: using dds to integrate real-world systems,https://www.semanticscholar.org/paper/c3d3d7b56fcc775c96777b1c6123b46a452b4aee,".................................................................................................................................... 3 Real-World Systems Programming ........................................................................................... 4 Defining a Data Model .............................................................................................................. 4 DDS Maintains the State of the World as Defined by the Data Model ..................................... 6 About DDS ................................................................................................................................ 8 Best Practices in DDS Programming ........................................................................................ 9 G1. Start by defining a data model, then map the data model to DDS domains, data types and Topics. .............................................................................................................. 9 G2. Fully define your DDS Types, do not rely on opaque bytes or other custom encapsulations ................................................................................................................ 11 G3. Isolate subsystems into DDS Domains. Use mediation, such as RTI Routing Service, to bridge Domains ............................................................................................. 12 G4. Use keyed Topics. For each data type, indicate to DDS the fields that uniquely identify the data-object .................................................................................................... 13 G5. Large teams should create a targeted application platform with system-wide QoS profiles and limited access to the DDS APIs. .................................................................. 16 G6. Configure QoS using the XML Profiles .................................................................... 17 Conclusions ............................................................................................................................. 18 References .............................................................................................................................. 18 Best-Practices Data-Centric Programming: Using DDS to Integrate Real-World Systems November 2010 3 © 2010 Real-Time Innovations Abstract Systems are often implemented by teams using a variety of technologies, programming languages, and operating systems. Integrating and evolving these systems becomes complex. Traditional approaches rely on low-level messaging technologies, delegating much of the message interpretation and information management services to application logic. This complicates system integration because different applications could use inconsistent interpretations and implementations of information-management services, such as detecting component presence, state management, reliability and availability of the information, handling of component failures, etc. Integrating modern systems requires a new, modular network-centric approach that avoids these historic problems by relying on standard APIs and protocols that provide stronger information-management services. For example, many of these systems are heterogeneous, mixing a variety of computer hardware, operating systems, and programming languages. Developers often use Java, .NET, or web-scripting to develop consoles and other GUI-oriented applications, and C or C++ for specialized hardware, device drivers, and performanceor time-critical applications. The end system might mix computers running Windows, Linux, and other operating systems, such as Mac OS X, Android, or real-time operating systems like VxWorks and INTEGRITY. The use of standard APIs and interoperable protocols allows all these systems to be easily integrated and deployed. Today, these systems are typically developed using a service-oriented approach and integrated using standards-based middleware APIs such as DDS, JMS, and CORBA, and protocols such as DDS-RTPS, Web-Services/SOAP, REST/HTTP, AMQP, and CORBA/IIOP. This whitepaper focuses on “real-world” systems, that is, systems that interact with the external physical world and must live within the constraints imposed by real-world physics. Good examples include air-traffic control systems, real-time stock trading, command and control (C2) systems, unmanned vehicles, robotic and vetronics, and Supervisory Control and Data Acquisition (SCADA) systems. More and more these “real-world” systems are integrated using a Data-Centric PublishSubscribe approach, specifically the programming model defined by the Object Management Group (OMG) Data Distribution Service (DDS) specification. This whitepaper describes the basic characteristics of real-world systems programming, reasons why DDS is the best standard middleware technology to use to integrate these systems, and a set of “best practices” guidelines that should be applied when using DDS to implement these systems. Best-Practices Data-Centric Programming: Using DDS to Integrate Real-World Systems November 2010 4 © 2010 Real-Time Innovations Real-World Systems Programming Real-World systems refer to a class of software systems that operate continuously and interact directly with real-world objects, such as aircraft, trains, stock transactions, weapons, robotic and manufacturing equipment, etc. Unlike systems involving only humans and computers, real-world systems have to live within the constraints imposed by the physics of the external world. Notably, time cannot be slowed, paused, or reversed. The implication is that these systems must be able to handle the information at the pace it arrives at, as well as be robust to changes in the operating environment. In addition to these environmental considerations, the nature of typical real-world applications also places demands on their availability and need to continue operating even in the presence of partial failures. In order to interact with the real world, software must include a reasonable, if simplified, model of the external world. This model typically includes aspects of the “state of the world” relevant to system operations. Here the word “state” is used in the normal sense in software modeling and programming. State summarizes the past inputs to the system from its initial state and contains all the information necessary for a system or program to know how it should react to future events or inputs. Imagine that a new component or application starts and joins a system. The “state of the system” contains the information that this new component needs to acquire before it is ready to start performing its function. A typical component would normally only need access to a subset of that state, the portion that directly affects its operation. For example, in an air-traffic management problem, the relevant aspects of the state of the world might include the current location and trajectory of every aircraft, the flight plans of all flights within a 24-hour window, specific details on each aircraft (type, airline, crew), etc. Once a software component or subsystem is running, it interacts with other components by exposing part of its state, notifying other components when its state changes, and invoking operations on (or sending messages to) other components. Each component reacts to these information exchanges by updating its internal model of the world and using that to perform its necessary actions. Defining a Data Model A data model is simply an organized description of the state of the system. Thus, it includes data types, processes for transferring and updating those types, and methods for accessing the data. It does not typically include functions that can alter the data or (importantly) the application-level logic that affects the data. Governance organizations and system integrators often start their design by designing the system data-model. There are good reasons for this approach: • A data model provides governance across disparate teams and organizations, allowing components developed at different points in time by different organizations to be integrated. This makes it an ideal starting point for a central design or governance authority. Best-Practices Data-Centric Programming: Using DDS to Integrate Real-World Systems November 2010 5 © 2010 Real-Time Innovations • A data model represents the better understood, more invariant aspects of the system. Typically the data model is grounded in the “physics of the system.” That is, it describes the kinds of objects and sensors it manages (like aircraft locations, flight plans, and vehicle positions). The data model is not strongly tied to applicationspecific use cases (e.g., the possible fields in a flight plan are a consequence of the nature of aircraft flight); this makes the data model a good starting point, since the full set of use cases might not be well known in advance or might be the responsibility of a different team. • A data model increases decoupling between systems and components. The data model is grounded in the essential information present in the system and it does not depend so much on the use cases that access the information. For example, an airtraffic control model might include a definition of a “flight plan,” but not whether it is automatically generated using an optimization algorithm, checked for collisions, or altered in mid-flight. Using the data model as the basis for the integration avoids over-constraining the design, leaving it open to allow future evolution and use cases. Contrast this with a design based on defining service invocation APIs which are intimately tied to the details of each service and are likely to change as new use cases are incorporated Example Data Model Imagine designing a simple “chat” application. The underlying Data-Model could be defined to contain four kinds of objects summarized in the table below: Object Kind Key Fields Other Fields Description Person EmailAddress Name, Loc",oceanology,161
bf89fad572188e397c9bea2fae07401069da38de,filtered,semantic_scholar,IEEE Access,2021-01-01 00:00:00,semantic_scholar,"lightweight cryptography algorithms for resource-constrained iot devices: a review, comparison and research opportunities",https://www.semanticscholar.org/paper/bf89fad572188e397c9bea2fae07401069da38de,"IoT is becoming more common and popular due to its wide range of applications in various domains. They collect data from the real environment and transfer it over the networks. There are many challenges while deploying IoT in a real-world, varying from tiny sensors to servers. Security is considered as the number one challenge in IoT deployments, as most of the IoT devices are physically accessible in the real world and many of them are limited in resources (such as energy, memory, processing power and even physical space). In this paper, we are focusing on these resource-constrained IoT devices (such as RFID tags, sensors, smart cards, etc.) as securing them in such circumstances is a challenging task. The communication from such devices can be secured by a mean of lightweight cryptography, a lighter version of cryptography. More than fifty lightweight cryptography (plain encryption) algorithms are available in the market with a focus on a specific application(s), and another 57 algorithms have been submitted by the researchers to the NIST competition recently. To provide a holistic view of the area, in this paper, we have compared the existing algorithms in terms of implementation cost, hardware and software performances and attack resistance properties. Also, we have discussed the demand and a direction for new research in the area of lightweight cryptography to optimize balance amongst cost, performance and security.",oceanology,162
036a7c42a459eb751bba2f8badec3b62d6328c10,filtered,semantic_scholar,,2010-01-01 00:00:00,semantic_scholar,editorial wireless sensor networks: design for real-life deployment and deployment experiences wireless sensor networks: design for real-life deployment and deployment experiences,https://www.semanticscholar.org/paper/036a7c42a459eb751bba2f8badec3b62d6328c10,"Wireless sensor networks (WSNs) are among the most promising technologies of the new millennium. The opportunities afforded by being able to program networks of small, lightweight, low-power, computation- and bandwidth-limited nodes have attracted a large community of researchers and developers. However, the unique set of capabilities offered by the technology produces an exciting but complex design space, which is often difficult to negotiate in an application context. Deploying sensing physical environments produces its own set of challenges, and can push systems into failure modes, thus revealing problems that can be difficult to discover or reproduce in simulation or the laboratory. Sustained efforts in the area of wireless networked sensing over the last 15 years have resulted in a large number of theoretical developments, substantial practical achievements, and a wealth of lessons for the future. It is clear that in order to bridge the gap between (on the one hand) visions of very large scale, autonomous, randomly deployed networks and (on the other) the actual performance of fielded systems, we need to view deployment as an essential component in the process of developing sensor networks: a process that includes hardware and software solutions that serve specific applications and end-user needs. Incorporating deployment into the design process reveals a new and different set of requirements and considerations, whose solutions require innovative thinking, multidisciplinary teams and strong involvement from end-user communities. This special feature uncovers and documents some of the hurdles encountered and solutions offered by experimental scientists when deploying and evaluating wireless sensor networks in situ, in a variety of well specified application scenarios. The papers specifically address issues of generic importance for WSN system designers: (i) data quality, (ii) communications availability and quality, (iii) alternative, low-energy sensing modalities and (iv) system solutions with high end-user added value and cost benefits. The common thread is deployment and deployment evaluation. In particular, satisfaction of application requirements, involvement of the end-user in the design and deployment process, satisfactory system performance and user acceptance are concerns addressed in many of the contributions. The contributions form a valuable set, which help to identify the priorities for research in this burgeoning area: Robust, reliable and efficient data collection in embedded wireless multi-hop networks are essential elements in creating a true deploy-and-forget user experience. Maintaining full connectivity within a WSN, in a real world environment populated by other WSNs, WiFi networks or Bluetooth devices that constitute sources of interference is a key element in any application, but more so for those that are safety-critical, such as disaster response. Awareness of the effects of wireless channel, physical position and line-of-sight on received signal strength in real-world, outdoor environments will shape the design of many outdoor applications. Thus, the quantification of such effects is valuable knowledge for designers. Sensors' failure detection, scalability and commercialization are common challenges in many long-term monitoring applications; transferable solutions are evidenced here in the context of pollutant detection and water quality. Innovative, alternative thinking is often needed to achieve the desired long-lived networks when power-hungry sensors are foreseen components; in some instances, the very problems of wireless technology, such as RF irregularity, can be transformed into advantages. The importance of an iterative design and evaluation methodology—from analysis to simulation to real-life deployment—should be well understood by all WSN developers. The value of this is highlighted in the context of a challenging WPAN video-surveillance application based on a novel Nomadic Access Mechanism. Cost benefits to be drawn from devising a WSN based solution to classic application areas such as surveillance are often a prime motivator for WSN designers; an example is offered here based on the use of intelligent agents for intrusion monitoring. Last but not least, the practicality and usability of the WSN solutions found for novel applications is key to their adoption. This is particularly true when the end-users of the developed technology are medical patients. The importance of feedback, elegant hardware encapsulation and extraction of meaning from data is presented in the context of novel orthopedic rehabilitation aids. Overall, this feature offers wide coverage of most issues encountered in the process of design, implementation and evaluation of deployable WSN systems. We trust that designers and developers of WSN systems will find much work of value, ranging from lessons learned, through solutions to known hurdles, to novel developments that enhance applications. Finally, we would like to thank all authors for their valuable contributions!",oceanology,163
fa1387e89d40c6b68969fa26315da581db6aeb23,filtered,semantic_scholar,,2020-01-01 00:00:00,semantic_scholar,agile requirements engineering and software planning for a digital health platform to engage the effects of isolation caused by social distancing: case study (preprint),https://www.semanticscholar.org/paper/fa1387e89d40c6b68969fa26315da581db6aeb23,"
 BACKGROUND
 Social distancing and shielding measures have been put in place to reduce social interaction and slow the transmission of the coronavirus disease (COVID-19). For older people, self-isolation presents particular challenges for mental health and social relationships. As time progresses, continued social distancing could have a compounding impact on these concerns.
 
 
 OBJECTIVE
 This project aims to provide a tool for older people and their families and peers to improve their well-being and health during and after regulated social distancing. First, we will evaluate the tool’s feasibility, acceptability, and usability to encourage positive nutrition, enhance physical activity, and enable virtual interaction while social distancing. Second, we will be implementing the app to provide an online community to assist families and peer groups in maintaining contact with older people using goal setting. Anonymized data from the app will be aggregated with other real-world data sources to develop a machine learning algorithm to improve the identification of patients with COVID-19 and track for real time use by health systems.
 
 
 METHODS
 Development of this project is occurring at the time of publication, and therefore, a case study design was selected to provide a systematic means of capturing software engineering in progress. The app development framework for software design was based on agile methods. The evaluation of the app’s feasibility, acceptability and usability shall be conducted using Public Health England's guidance on evaluating digital health products, Bandura’s model of health promotion, the Reach Effectiveness Adoption Implementation Maintenance (RE-AIM) framework and the Nonadoption, Abandonment and Challenges to the Scale-up, Spread and Suitability (NASSS) framework.
 
 
 RESULTS
 Making use of a pre-existing software framework for health behavior change, a proof of concept was developed, and a multistage app development and deployment for the solution was created. Grant submissions to fund the project and study execution have been sought at the time of publication, and prediscovery iteration of the solution has begun. Ethical approval for a feasibility study design is being sought.
 
 
 CONCLUSIONS
 This case study lays the foundations for future app development to combat mental and societal issues arising from social distancing measures. The app will be tested and evaluated in future studies to allow continuous improvement of the app. This novel contribution will provide an evidence-based exemplar for future app development in the space of social isolation and loneliness.
",oceanology,164
ec589937c2bbdb93fcef6e7f99e82d4ca99f0306,filtered,semantic_scholar,ArXiv,2019-01-01 00:00:00,semantic_scholar,flight controller synthesis via deep reinforcement learning,https://www.semanticscholar.org/paper/ec589937c2bbdb93fcef6e7f99e82d4ca99f0306,"Traditional control methods are inadequate in many deployment settings involving control of Cyber-Physical Systems (CPS). In such settings, CPS controllers must operate and respond to unpredictable interactions, conditions, or failure modes. Dealing with such unpredictability requires the use of executive and cognitive control functions that allow for planning and reasoning. Motivated by the sport of drone racing, this dissertation addresses these concerns for state-of-the-art flight control by investigating the use of deep neural networks to bring essential elements of higher-level cognition for constructing low level flight controllers. 
This thesis reports on the development and release of an open source, full solution stack for building neuro-flight controllers. This stack consists of the methodology for constructing a multicopter digital twin for synthesize the flight controller unique to a specific aircraft, a tuning framework for implementing training environments (GymFC), and a firmware for the world's first neural network supported flight controller (Neuroflight). GymFC's novel approach fuses together the digital twinning paradigm for flight control training to provide seamless transfer to hardware. Additionally, this thesis examines alternative reward system functions as well as changes to the software environment to bridge the gap between the simulation and real world deployment environments. 
Work summarized in this thesis demonstrates that reinforcement learning is able to be leveraged for training neural network controllers capable, not only of maintaining stable flight, but also precision aerobatic maneuvers in real world settings. As such, this work provides a foundation for developing the next generation of flight control systems.",oceanology,165
eb0e7c3176f89fa33aa50d9d3ec1fd83ab3cf96a,filtered,semantic_scholar,Medical Technologies Journal,2019-01-01 00:00:00,semantic_scholar,"health 4.0: applications, management, technologies and review",https://www.semanticscholar.org/paper/eb0e7c3176f89fa33aa50d9d3ec1fd83ab3cf96a,"The Industry 4.0 Standard (I4S) employs technologies for automation and data exchange through cloud computing, Big Data (BD), Internet of Things (IoT), forms of wireless Internet, 5G technologies, cryptography, the use of semantic database (DB) design, Augmented Reality (AR) and Content-Based Image Retrieval (CBIR). Its healthcare extension is the so-called Health 4.0. 
This study informs about Health 4.0 and its potential to extend, virtualize and enable new healthcare-related processes (e.g., home care, finitude medicine, and personalized/remotely triggered pharmaceutical treatments) and transform them into services. 
In the future, these services will be able to virtualize multiple levels of care, connect devices and move to Personalized Medicine (PM). The Health 4.0 Cyber-Physical System (HCPS) contains several types of computers, communications, storage, interfaces, biosensors, and bioactuators. The HCPS paradigm permits observing processes from the real world, as well as monitoring patients before, during and after surgical procedures using biosensors. Besides, HCPSs contain bioactuators that accomplish the intended interventions along with other novel strategies to deploy PM. A biosensor detects some critical outer and inner patient conditions and sends these signals to a Decision-Making Unit (DMU). Mobile devices and wearables are present examples of gadgets containing biosensors. Once the DMU receives signals, they can be compared to the patient’s medical history and, depending on the protocols, a set of measures to handle a given situation will follow. The part responsible for the implementation of the automated mitigation actions are the bioactuators, which can vary from a buzzer to the remote-controlled release of some elements in a capsule inside the patient’s body. 
            Decentralizing health services is a challenge for the creation of health-related applications. Together, CBIR systems can enable access to information from multimedia and multimodality images, which can aid in patient diagnosis and medical decision-making. 
Currently, the National Health Service addresses the application of communication tools to patients and medical teams to intensify the transfer of treatments from the hospital to the home, without disruption in outpatient services. 
HCPS technologies share tools with remote servers, allowing data embedding and BD analysis and permit easy integration of healthcare professionals expertise with intelligent devices.  However, it is undeniable the need for improvements, multidisciplinary discussions, strong laws/protocols, inventories about the impact of novel techniques on patients/caregivers as well as rigorous tests of accuracy until reaching the level of automating any medical care technological initiative.",oceanology,166
3ee6c2e1aebf16c7b62774700b99a4320ebfefea,filtered,semantic_scholar,IEEE Internet of Things Journal,2019-01-01 00:00:00,semantic_scholar,mc-sdn: supporting mixed-criticality real-time communication using software-defined networking,https://www.semanticscholar.org/paper/3ee6c2e1aebf16c7b62774700b99a4320ebfefea,"Despite recent advances, there still remain many problems to design reliable cyber-physical systems. One of the typical problems is to achieve a seemingly conflicting goal, which is to support timely delivery of real-time flows while improving resource efficiency. Recently, the concept of mixed-criticality (MC) has been widely accepted as useful in addressing the goal for real-time resource management. However, it has not been yet studied well for real-time communication. In this paper, we present the first approach to support MC flow scheduling on switched Ethernet networks leveraging an emerging network architecture, software-defined networking (SDN). Though SDN provides flexible and programmatic ways to control packet forwarding and scheduling, it yet raises several challenges to enable real-time MC flow scheduling on SDN, including: 1) how to handle (i.e., drop or re-prioritize) out-of-mode packets in the middle of the network when the criticality mode changes and 2) how the mode change affects end-to-end transmission delays. Addressing such challenges, we develop MC-SDN that supports real-time MC flow scheduling by extending SDN-enabled switches and OpenFlow protocols. It manages and schedules MC packets in different ways depending on the system criticality mode. To this end, we carefully design the mode change protocol that provides analytic mode change delay bound, and then resolve implementation issues for system architecture. For evaluation, we implement a prototype of MC-SDN on top of Open vSwitch, and integrate it into a real world network testbed as well as a 1/10 autonomous vehicle. Our extensive evaluations with the network testbed and vehicle deployment show that MC-SDN supports MC flow scheduling with minimal delays on forwarding rule updates and it brings a significant improvement in safety in a real-world application scenario.",oceanology,167
9aeb35460b423cbf274e0ff1f052a602d9efbb91,filtered,semantic_scholar,,2002-01-01 00:00:00,semantic_scholar,a graphical user interface for the real world,https://www.semanticscholar.org/paper/9aeb35460b423cbf274e0ff1f052a602d9efbb91,"A number of typical Ubiquitous Computing (Ubicomp) interface problems are presented. From these, the requirements needed for a Ubicomp meta-user interface are extracted and a user interface architecture that is based on traditional GUI architectures is introduced. MOTIVATION A recent survey of more than 100 Ubiquitous Computing Applications (Rehman 2001) revealed that a lot of these applications violate basic HCI guidelines such as Norman’s design principles (Norman 1990). More specifically, developers often leave the user with too little control, don’t provide appropriate feedback about what the system is doing and fail to show appropriate affordances and constraints to the user. A more detailed analysis is given in (Rehman et al. 2002). For our purposes I shall try to highlight the fundamental problem here. My hypothesis is that these problems are not singular design flaws, but have their root in the very idea of Ubiquitous/Disappearing/Invisible Computing. First, there is a need to define “invisibility”. Some system developers have taken it to mean that the user does not see any system-related part physically. As pleasing it may seem to some developers, this is not what Weiser or Norman meant when they conceived the idea, as is evident from the examples they usually cite. Rather, they meant that the computer stays out of the user’s mind, not necessarily their sight. That said, let us look at whether even this is achievable. For this we shall look at the archetype of a good tool: the pencil. You have control over it, the user knows how to use it, and it provides constant feedback. More importantly, it is invisible in Weiser’s sense (Weiser 1994). I believe that the reason why we have not achieved this quality in our applications is, that there is an asymmetry in our capability to capture input in comparison to providing output. With the advent of sophisticated sensors any real world object, even the human body (as seen in location-aware applications) can be a highly accurate input device. Whereas the above-mentioned pencil can also deliver feedback in the same way as its input (mechanically), we cannot, say, mechanically stop a museum visitor with a contextaware electronic guide from going out of the coverage of the location system. One solution is not to worry about the output at all. Clearly, this leads to ill-designed applications. The other solution is to display this information on a display. This adds another level of indirection a conventional tool does not suffer from and, depending on where the display is placed, may imply an increase in cognitive load. Before we talk about our approach to this problem, we shall look at another problem we became aware of when analysing the applications. Norman (1993) talks about the idea that an interface in its function as a “surface representation” should convey an image of the underlying system. The problem we have in Ubicomp the system is a collection of invisibly connected heterogenous nodes that do not seem to have a “surface” a user can relate to, at all. Our aim is to not only give the user such a smooth “surface”, but also to massively increase the bandwith of the reverse channel, between system and user. In order to do this with as little cognitive load as possible, we advocate the use of Augmented Reality (AR). SYSTEM IDEA In its widest sense any system that connects the real and virtual world can be labelled ""Augmented Reality"" (AR). As such, even tangible interfaces are examples of AR. The narrower definition involves a system that uses a head-mounted display (HMD) and a tracker (Feiner et al. 1993). The tracker continuously measures the position and orientation of the head to some real object and displays a 3D graphics on a see-through HMD that makes the virtual object appear to be placed at a fixed location in the physical world. We are using a head-mounted camera and have deployed markers (Kato & Billinghurst 1999 ) in order to track objects of interest. The marker is a reference point that can be used to overlay graphics on a real world object. A file cabinet can, e.g., have its contents overlaid on it, as long as the marker has a fixed relationship to the file cabinet. The system will infer the position of the file cabinet relative to the user’ s eyes and display a corresponding virtual object on his HMD. In order to design our smooth surface, we want to leverage some of the design principles used in the GUI domain. Before GUIs arrived, computer users were typically confronted with a multitude of applications each with their own user interface. Transferring data between them was difficult and users had to interrupt their tasks in order to adhere to application boundaries. Working across applications was impossible. In a way the situation was similar to the one we now have in Ubicomp. The arrival of a meta-interface was decisive in giving the user the “ unified experience” we wish to provide in our present domain. Figure 1: A meta-interface for ubiquitous computing A VDU (VISUAL DISPLAY UNIT) The first step in building our Graphical User Interface that can send information and receive information from any everyday object, service, appliance or application, was to implement a display that covers the entire space. Of course, this is meant in a virtual sense, using Augmented Reality. This involved constructing a spatial model that abstracts from tracking sensors and sources. We are working with a spatial model that consists of a network of points. The arcs are Cartesian coordinate transformations, some dynamically changing, some fixed. We are using this type of model in order to cope with 6-DOF(degrees-of-freedom) tracking information. In order to give an object output capability, say a loudspeaker, we can attach an electromagnetic sensor or a marker to it and add it as a point of interest to our model, that will keep track of its position and orientation at all times. Figure 2: Every object has an output USER INTERFACE ARCHITECTURE We shall now describe the entire user interface architecture we are in the process of implementing. Figure 3 shows the current design plan. We are assuming that the user has a mobile unit with a head-mounted display and that his head position and orientation are trackable. The environment contains a number of “ active” appliances: devices such as a printer, services such as a web search, Applications such as Microsoft Word and Everyday Objects such as a mug that knows its temperature. All of these devices have interfaces to a tuple space in order to receive commands and send messages to the user. We thought that a tuple space is best suited for a symmetric 2-way communication. The two paths of information flow are shown by the arrows: a forward path from left to right and a feedback path from right to left. The world model and data model can be seen as the Visual Display Unit described above. The world model contains the position(and orientation) of each active object. The data model is a description of interactive information, each piece of which I N T E R F A C E Network Services Applications Devices",oceanology,168
cd51bbfd51cde0c089d9dfb7d30bfc124d9b7c55,filtered,semantic_scholar,,2020-01-01 00:00:00,semantic_scholar,"summary for cife seed proposals for academic year 2020-21 proposal number: 2020-04 proposal title: hybrid physical-digital spaces: transforming the design, operation, and experience of built environments to promote health and wellbeing",https://www.semanticscholar.org/paper/cd51bbfd51cde0c089d9dfb7d30bfc124d9b7c55,"up to 150 words) Increasing evidence suggests built office features (e.g., lighting, materials, and ventilation) have substantial impacts on occupant wellbeing. A key next direction is field studies at industry partner sites to examine real-world workplaces. We propose to develop innovative Internet of Things (IoT) techniques that integrate data from building instrumentation, personal device sensors, and self-report interfaces and then deploy this platform in-the-wild to capture rich, longitudinal, ecologically-valid data about the status of office workers and the spaces they occupy. Insights will advance scientific knowledge of how buildings impact wellbeing as well as produce practical implications for building designers and operators. A timely component will explore how covid-19 has temporally or fundamentally changed occupant behaviors and operational decisions (e.g., physical distancing desks and ventilation settings that reduce pathogen spread). Overall, our proposed research has the potential to transform the industry’s thinking on how built environments can be designed, operated, and experienced. Hybrid Physical-Digital Spaces: Transforming the Design, Operation, and Experience of Built Environments to Promote Health and Wellbeing Problem and Significance Considering that people in the U.S. spend 87% of their time in indoor spaces , we assert that 1 buildings are powerful yet underleveraged loci for promoting human wellbeing. Imagine an intelligent office that could adapt soundscape systems to manage noise in open floor plans, optimize space reservation or utilization to foster collaborations and save energy, or provide digital information displays that promote employee connectedness and physical activity. Towards actualizing our vision of such hybrid physical-digital spaces, our proposal strives to develop, apply, and evaluate novel scientific and engineering approaches that will transform the industry’s thinking around how built environments can be designed, operated, and experienced. Increasingly, hypotheses suggest that built features of indoor environments (e.g., lighting, materials, and ventilation) have substantial impacts on occupants (e.g., employee recruitment and retention, absenteeism, cognition, creativity, productivity, social interactions, physical activity and health, and psychological wellbeing). In turn, these individual outcomes also drive pivotal organizational outcomes such as product innovation, workforce diversity, employee turnover, market share, and profitability. Examples illustrate how building interventions can have huge impacts : enhancing employee exposure to daylight can save businesses ~$2,000/yr per capita 2 , better air quality can raise cognitive scores of workers by 101% 3 , and increasing indoor access to biophilic elements could recoup $23 billion considering 10% of workplace absenteeism (a $226 billion dollar problem) is attributable to architecture that inadequately connects to nature 4 . However, few of these hypotheses have been tested at scale, over time, and in real world conditions . Instead, most prior efforts are small sample, short-term correlational studies based on potentially biased and sparse self-reported data. A more rigorous, scientific, and human-centered approach to study and engineer buildings that promote wellbeing can have major implications at individual, organizational, and societal levels (see Figure 1), offering both foundational theoretical knowledge as well as practical strategies for building designers and operators. Figure 1. Relations among building features and human outcomes at various levels. Further, “smart buildings” today typically focus on basic sensing and control for energy savings, thermal comfort, and security. Connecting to CIFE’s Vision for the Future of Building Users, we argue buildings of the future can go beyond such bottom line outcomes to be more interactive and human-centered: aware of and responsive to occupants’ cognitive, mental, and physical feelings and needs, while respecting privacy and promoting positive indoor experiences . 1 Klepeis, et al., 2001; 2 Heschong & Mahone, 2003; 3 Allen et al., 2016; 4 Elzeyadi, 2011. <Landay-Billington> < Hybrid Physical-Digital Spaces> 1 Theoretical and Practical Points of Departure It is imperative to increase understanding of exactly what built attributes have what impacts and on whom, in a scalable, longitudinal, and inclusive manner. Thus through technology-driven assessment and hybrid physical-digital interventions, we aim to (a) fundamentally advance the science on how built environments impact human wellbeing and, in turn, (b) generate guidelines that can revolutionize the way spaces are designed, operated, and experienced . Our current scope focuses on office spaces and workers; though an overarching goal is for our developed approaches and insights to establish a foundation that enables future research with additional populations and environments (e.g., physicians and patients in clinical settings, students and teachers in classrooms, and traditionally marginalized shift and temporary workers). In particular, our reusable platform will help others study this wider range of buildings and occupants; and combining these approaches with emerging endeavors such as biophilic design and precision interventions provides a novel opportunity to not only more deeply investigate but also address long-running public health challenges and systemic inequities facing society. In these ways, we hope to positively impact a broad cross-section of stakeholders at individual, organizational, and institutional levels. Moreover, this project will support interdisciplinary fertilization across engineering, computing, psychology, law, and medicine . Research Methods and Work Plan Our research agenda is to support the design and operation of built facilities that augment human capabilities and wellbeing — and have a fundamental positive change on the way indoor spaces are experienced by the people that occupy them. By introducing intelligent systems capable of gathering and interpreting building and occupant data as well as delivering adaptive interventions in response, novel roles will also emerge for managing buildings and the activities that take place inside them. To achieve these goals, our research will comprise three main activities: 1. Developing an extensible and secure data collection and machine learning platform . A key aim of this research is scientifically examining how built spaces impact human wellbeing. To pursue this investigation and develop methods that enable buildings to be more aware of occupants’ states and needs, we have been developing pattern detection software that integrates data from (a) personal devices (smartphones, smartwatches, fitness trackers), (b) building instrumentation or portable environmental sensors (light levels, air quality), and (c) experience sampling interfaces that prompt occupants for subjective information through quick, validated self-report techniques. Figure 2 illustrates examples of these assessment components. This work involves addressing a number of technical challenges, such as selecting sampling rates and window sizes to maximize efficiency, developing methods for analyzing asynchronous and sparse sensor data, and developing privacy-sensitive feature engineering strategies for detecting and predicting wellbeing outcomes of interest. We also plan to package our platform as a reusable toolkit that can be applied by other researchers and building operators. This work is ongoing and a basic version will be ready by summer. Once development is complete, CIFE support would allow us to move onto the next critical phase: moving out of the lab and into the field. <Landay-Billington> < Hybrid Physical-Digital Spaces> 2 Figure 2. Platform to integrate data from personal devices, building sensors, and subjective self-report. 2. Deploying the platform through a mixed-method study with industry partners . The next step in our research is to deploy this platform at field sites in partnership with View, Inc. (specifically, at TIAA offices in Manhattan, this summer/fall) to capture rich, longitudinal, ecologically-valid data about behavioral, psychological, and physiological states of occupants and their everyday work environments. Our plan is to recruit a sample of approximately 150 employees for a period of 18 weeks, which will involve a baseline phase followed by systematic variation of built features (Views/No Views, Plants/No Plants, and Diversity/No Diversity in artwork) and measurement of indicators hypothesized to promote both personal wellbeing and organizational performance, based on the literature and our formative online and lab studies, described below. In combination with the engineering-focused activities to implement and install the platform, deployment will occur in tandem with ethnographic work (e.g., observations, interviews, and surveys) to manually validate reliability of the system’s automated inferences as well as gain a more qualitative portrait of occupant experiences in various spaces. Privacy-centric engagements will additionally investigate stakeholders’ attitudes regarding the capture of various types of information to derive implications about informed consent and personal data management. Along similar lines, it will be critical to responsibly manage captured data, especially potentially sensitive and exploitable data about wellness or performance. Therefore all studies will be conducted with oversight and approval from the Stanford Institutional Review Board (IRB). In addition to obtaining participants’ informed consent, we will also design sensor and data collection mechanisms to use an opt-in model, including partial participation. Our data management systems can also allow individuals to view and delete their personal data, including if purging is desired in the event of study withdrawal. Our research team has exp",oceanology,169
06a05c5ecdfb9ab6867722752c16b1e8c21361dd,filtered,semantic_scholar,,2020-01-01 00:00:00,semantic_scholar,on the practical security of white-box cryptography. (de la théorie à la pratique de la cryptographie en boite blanche),https://www.semanticscholar.org/paper/06a05c5ecdfb9ab6867722752c16b1e8c21361dd,"Cryptography studies how to secure communications and information. The security of a cryptosystem depends on the secrecy of the underlying key. White-box cryptography explores methods to hide a cryptographic key into some software deployed in the real world. 
 
Classical cryptography only assumes that the adversary accesses the target cryptographic primitive in a black-box manner in which she can only observe or manipulate the input and output of the primitive, but cannot know or tamper with its internal details. The gray-box model further allows an adversary to exploit key- dependent sensitive information leaked from the execution of physical implementations. All sorts of side-channel attacks exploit some physical information leakage, such as the power consumption of the device. The white-box model considers the worst-case scenario in which the adversary has complete control over the software and its execution environment. The goal of white-box cryptography is to securely implement a cryptographic primitive against such a powerful adversary. Although the scientific community has proposed some candidate solutions to build white-box cryptography, all have proven ineffective. Consequently, this problem has remained open for almost two decades since the concept was introduced. 
 
The continuous growth in market demand and the emerging potential applications have driven the industry to deploy secretly-designed proprietary solutions. Al- though this paradigm of achieving security through obscurity contradicts the widely accepted Kerckhoffs' principle in cryptography, this is currently the only option for white-box cryptography. Security experts have reported how gray-box attacks could be used to extract keys from several publicly available white-box implementations. In a gray-box attack, the adversary adapts side-channel analysis techniques to the white-box context, i.e., to target computation traces made of noise-free run-time information instead of the noisy physical leakage. Gray-box attacks are generic since they do not require any a priori knowledge of the implementation and hence avoid costly reverse engineering. Some non-publicly scrutinized industrial white-box schemes in the market are believed to be under the threat of gray-box attacks. 
 
This thesis focuses on the analysis and improvement of gray-box attacks and the associated countermeasures for white-box cryptography. We first provide an in- depth analysis of why gray-box attacks are capable of breaking the classical white-box design which is based on table encodings. Next, we introduce a new gray-box attack named linear decoding analysis and show that linearly encoding sensitive information is insufficient to protect the cryptographic software. Afterward, we describe how to combine state-of-the-art countermeasures to resist gray-box attacks and comprehensively elaborate on the (in)effectiveness of these combined countermeasures in terms of computation complexity. Finally, we introduce a new attack technique that exploits the data-dependency of the targeted implementation to substantially lower the complexity of the existing gray-box attacks on white-box cryptography. In addition to the theoretical analyses and new attack techniques introduced in this thesis, we report some attack experiments against practical white-box implementations. In particular, we could break the winning implementations of two consecutive editions of the well-known WhibOx white-box cryptography competition.",oceanology,170
b8f7fa8d93c5ee4c3df988ba7c7499b1db51706e,filtered,semantic_scholar,2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),2020-01-01 00:00:00,semantic_scholar,towards autonomous smart sensing systems,https://www.semanticscholar.org/paper/b8f7fa8d93c5ee4c3df988ba7c7499b1db51706e,"Since the 1990's, researchers in both academia and industry have been exploring ways to exploit the potential for Wireless Sensor Networks (WSNs) to revolutionize our understanding of - and interaction with - the world around us. WSNs have therefore been a major focus of research over the past 20 years. While WSNs offer a persuasive solution for accurate real-time sensing of the physical world, they are yet to be as ubiquitous as originally predicted when the technology was first envisaged. Technical difficulties exist which have inhibited the anticipated uptake in WSN technologies. The most challenging of these have been identified as system reliability, battery lifetime, maintenance requirements, node size and ease of use. Over the past decade, the Wireless Sensor Networks (WSN) group at the Tyndall National Institute, has been at the forefront of driving the vision of ubiquitously deployed, extended lifetime, low power consumption embedded systems providing information rich data streams wirelessly in (close to) real-time. In this time, the WSN group has developed multiple novel, first of kind, wireless multi-sensor systems and deployed these in the world around us, overcoming the technical challenges associated with ensuring robust and reliable long-term data sets from our environment. This work is focused on investigating and addressing these challenges through the development of the new technologies and system integration methodologies required to facilitate and implement WSNs and validate these in real deployments. Specifically, discussed are the development and deployment of novel WSN systems in the built environment, environmental monitoring and fitness and health monitoring systems.The key research challenges identified and discussed are:a)The development of resource-constrained, extremely low power consumption systems incorporating energy-efficient hardware and software algorithms.b)The development of highly reliable extremely long duration deployments which through the use of appropriate energy harvesting solutions facilitate (near) zero maintenance sensor networks.c)The development of low power consumption miniaturized wearable microsysteThe development of technologies to address these challenges in terms of cost, size, power consumption and reliability which need to be tested and validated in real world deployments of wireless sensing systems is discussed. It is clear that when looking at the scale up of deployments of novel WSNs, that to be successful, such systems need to ""be invisible, last forever, cost nothing and work out of the box"". This paper describes these relevant technologies and associated project demonstrators",oceanology,171
fd94932da9cc80e9cd9f6204d2cae2624d582e91,filtered,semantic_scholar,2016 IEEE International Conference on Autonomic Computing (ICAC),2016-01-01 00:00:00,semantic_scholar,hyperlink: virtual machine introspection and memory forensic analysis without kernel source code,https://www.semanticscholar.org/paper/fd94932da9cc80e9cd9f6204d2cae2624d582e91,"Virtual Machine Introspection (VMI) is an approach to inspecting and analyzing the software running inside a virtual machine from the hypervisor. Similarly, memory forensics analyzes the memory snapshots or dumps to understand the runtime state of a physical or virtual machine. The existing VMI and memory forensic tools rely on up-to-date kernel information of the target operating system (OS) to work properly, which often requires the availability of the kernel source code. This requirement prevents these tools from being widely deployed in real cloud environments. In this paper, we present a VMI tool called HyperLink that partially retrieves running process information from a guest virtual machine without its source code. While current introspection and memory forensic solutions support only one or a limited number of kernel versions of the target OS, HyperLink is a one-for-many introspection and forensic tool, i.e., it supports most, if not all, popular OSes regardless of their versions. We implement both online and offline versions of HyperLink. We validate the efficacy of HyperLink under different versions of Linux, Windows, FreeBSD, and Mac OS X. For all the OSes we tested, HyperLink can successfully retrieve the process information in one minute or several seconds. Through online and offline analyses, we demonstrate that HyperLink can help users detect real-world kernel rootkits and play an important role in intrusion detection. Due to its version-agnostic property, HyperLink could become the first introspection and forensic tool that works well in autonomic cloud computing environments.",oceanology,172
ea213eb5101ceaf749ca538a4135b879a464c00d,filtered,semantic_scholar,,2020-01-01 00:00:00,semantic_scholar,configurable framework for managing data produced by multiple plcs,https://www.semanticscholar.org/paper/ea213eb5101ceaf749ca538a4135b879a464c00d,"The work was carried on in collaboration with the CO.S.E. Centre of Thales Alenia Space in Italy at the Turin site. Thales Alenia Space in Italy has over 40 years of experience in building systems and equipment for space exploration, telecommunications, navigation, Earth observation and science. In the world of software development, which engages this work, telemetry can offer automatic data collection from the real world. The telemetry, therefore, allows to collect important data that becomes valuable and usable when analyzed. The main goal of this work is to develop a Configurable Framework, which manage the communication among different devices and data produced by multiple PLCs. The PLCs are connected to different equipment, contained in a module, which manage the resources needed to survive: the use and recycling of water, the storage and distribution of electrical power, thermal control and air recycling, data management and processing, remote communication and control, medical treatment and many more. The module can be deployed in all kinds of environments (desert camps, oil and gas exploration camps, military outposts and polar bases) and is the ideal solution for survival in remote or hostile areas. The realized architecture, therefore, allows to monitor the status of the module once it has been deployed in a remote location. After the first phase, the requirements were collected in a document to keep track of the data needed to create the appropriate code, which was subsequently used to simulate the various components. Afterwards, it was defined what are the functionalities that a generic Configurable Framework shall implement, then all possible architectures that can satisfy the company needs were explored and finally a working prototype was developed. Furthermore, it is also an opportunity to work with software technologies outside the domain of existing business skills and discover which advantages they can bring to the company. To provide flexibility, the GUI was built as a Web Application, taking advantage from the Bootstrap library. In fact the architecture satisfy the requested modularity and it is compatible with different operating systems and several screen resolutions. The interfaces are suitable for many electronic devices, allowing the use of keyboard, mouse and touch-screens. To speed up the development it was used a Web Framework; different solutions were analyzed, but in the end it was chosen the Django Framework, since it works without having to install additional dependencies. The application was developed using SQLite as database solution, it runs from a folder and can be moved between different machines without worrying about databases and Web Servers configurations. The GUI consist of two Web Pages, the first one allows the users to perform the different requests. The second one shows which setting are available and it allows to interact with the defined models by creating, deleting or updating items. The sampled data is collected into a non-relational Database, using the JSON format, to keep track of it, and it is also displayed in real time in order to visualize updates. After simulating the behavior of the used boards, the next step was to test and validate code using physical devices. The primary purpose of testing is to detect software failures so that defects may be discovered and corrected. At the end of the testing phase a user manual was also produced.",oceanology,173
577a8528c2dd27d9c36d9cb1e63c6667c9c3370d,filtered,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,challenges and opportunities in the future applications of iot technology,https://www.semanticscholar.org/paper/577a8528c2dd27d9c36d9cb1e63c6667c9c3370d,"The advent of internet of things (IoT) has influenced and revolutionized the information systems and computing technologies. A computing concept where physical objects used in daily life, will identify themselves by getting connected to the internet is called IoT. Physical objects embedded with electronic, radio-frequency identification, software, sensors, actuators and smart objects converge with the internet to accumulate and share data in IoT. IoT is expected to bring in extreme changes and solutions to most of the daily problems in the real world. Thus, IoT provides connectivity for everyone and everything at any time. The IoT embeds some intelligence in Internet connected objects to communicate, exchange information, take decisions, invoke actions and provide amazing services. It has an imperative economic and societal impact for the future construction of information, network, and communication technology. In the upcoming years, the IoT is expected to bridge various technologies to enable new applications by connecting physical objects together to support the intelligent decision making. As the most cost-effective and performant source of positioning and timing information in outdoor environments, the global navigation satellite systems(GNSS) has become an essential element of major contemporary technology developments notably including the IoT, Big Data, Smart Cities and Multimodal Logistics. By 2020, there will be more than 20 billion interconnected IoT devices, and its market size may reach $1.5 trillion. Projections for the impact of IoT on the Internet and economy are impressive, with some anticipating as many as 100 billion connected IoT devices and a global economic impact of more than $11 trillion by 2025. Regulators can play a role in encouraging the development and adoption of the IoT, by preventing abuse of market dominance, protecting users and protecting Internet networks while promoting efficient markets and the public interest. Regulators can consider and identify some measures to foster development of the IoT. Encourage development of LTE‐A and 5G wireless networks, and keep need for IoT‐specific spectrum under review. Universal IPv6 adoption by governments in their own services and procurements, and other incentives for private sector adoption. Increasing interoperability through competition law and give users a right to easy access to personal data. Support global standardization and deployment of remotely provisioned SIMs for greater machine to machine competition. Particular attention will be needed from regulators to IoT privacy and security issues, which are key to encouraging public trust in and adoption of the technology. This paper focuses specifically on the essential technologies that enable the implementation of IoT and the general layered architecture of IoT, the market of IoT and GNSS technologies and their impact of the world economy, application domain of IoT and finally the Policy and regulatory implications and best practices.",oceanology,174
8002fff47f40e6126bf3f9f7fabea1ac9e1cbb4e,filtered,semantic_scholar,Transportation Research Record: Journal of the Transportation Research Board,2018-01-01 00:00:00,semantic_scholar,hardware-in-the-loop testing of connected and automated vehicle applications: a use case for queue-aware signalized intersection approach and departure,https://www.semanticscholar.org/paper/8002fff47f40e6126bf3f9f7fabea1ac9e1cbb4e,"Most existing studies on connected and automated vehicle (CAV) applications apply simulation to evaluate system effectiveness. Model accuracy, limited data for calibration, and simulation assumptions limit the validity of evaluation results. One alternative approach is to use emerging hardware-in-the-loop (HIL) testing methods. HIL test environments enable physical test vehicles to interact with virtual vehicles from traffic simulation models, providing an evaluation environment that can replicate deployment conditions at early stages of CAV technology implementation without incurring excessive costs related to large field tests. In this study, a HIL testing system for vehicle-to-infrastructure (V2I) CAV applications is developed. The involved software and hardware includes a physical CAV controlled in real time, a traffic signal controller, communication devices, and a traffic simulator (VISSIM). Such HIL systems increase validity by considering the physical vehicle’s trajectories—which are constrained by real-world factors such as GPS accuracy, communication delay, and vehicle dynamics—in a simulated traffic environment. The developed HIL system is applied to test a representative early deployment CAV application: queue-aware signalized intersection approach and departure (Q-SIAD). The Q-SIAD algorithm generates recommended speed profiles based on the vehicle’s status, signal phase and timing (SPaT), downstream queue length, and system constraints and parameters (e.g., maximum acceleration and deceleration). The algorithm also considers the status of other vehicles in designing the speed profiles. The experiment successfully demonstrated this functionality with one test CAV driving through one intersection controlled by a fixed-timing traffic signal under various simulated traffic conditions.",oceanology,175
ef8e465f80f41ec5cb3dbdb527c8509e66abaf5c,filtered,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,a framework to secure applications with isa heterogeneity,https://www.semanticscholar.org/paper/ef8e465f80f41ec5cb3dbdb527c8509e66abaf5c,"Software security attacks are evolving from exploiting common code vulnerabilities to exploiting micro architecture side-channels. Traditional software diversity or code randomization techniques diversify the code memory layout and make it difficult for potential attackers to pinpoint the precise location of the target vulnerability. However, those approaches may not be sufficient enough for the new micro architecture attacks (e.g., Spectre). While some architecture researchers have proposed using diverse ISA configurations to defeat code injection or code reuse attacks, most of these works remain in the simulation stage due to legal, licensing, and verification costs involved in bringing a heterogeneous chip design into physical hardware [39]. In this paper, we report our on-going work of HeterSec, a framework to secure applications utilizing real world heterogeneous ISA machines. HeterSec runs on top of the commodity x86_64 and ARM64 machines. It gives the process the ability to dynamically select its underlying ISA environment. Therefore, the protected process would hide the vulnerable targets with the diversified instruction set, or would detect the abnormal behavior by comparing the execution results step-by-step from multiple ISA-diversified instances. To demonstrate the effectiveness of such software framework, we implemented HeterSec on Linux and showed its deployability by running it on a x86_64 and ARM64machine pair, connected using InfiniBand. We then conduct two case studies with HeterSec. In the first case, we timely randomize the process execution path across the ISA, which achieves similar security guarantees as the existing architecture based solutions. In the second case, we implement a multi-ISA based multi-version execution (MVX) system, providing a stronger security guarantee than current homogeneousISA MVX designs.",oceanology,176
2ad3366962d249b7b63c4986ebb0cb22ea212a75,filtered,semantic_scholar,,2005-01-01 00:00:00,semantic_scholar,"service-oriented architecture compass: business value, planning, and enterprise roadmap",https://www.semanticscholar.org/paper/2ad3366962d249b7b63c4986ebb0cb22ea212a75,"Praise for Service-Oriented Architecture Compass""A comprehensive roadmap to Service-Oriented Architecture (SOA). SOA is, in reality, a business architecture to be used by those enterprises intending to prosper in the 21st century. Decision makers who desire that their business become flexible can jumpstart that process by adopting the best practices and rules of thumb described in SOA Compass.""i¾Bob Laird, MCI IT Chief Architect""The book Service-Oriented Architecture Compass shows very clearly by means of real projects how agile business processes can be implemented using Service-Oriented Architectures. The entire development cycle from planning through implementation is presented very close to practice and the critical success factors are presented very convincingly.""i¾Professor Dr. Thomas Obermeier, Vice Dean of FHDW Bergisch Gladbach, Germany""This book is a major improvement in the field. It gives a clear view and all the key points on how to really face a SOA deployment in today's organizations.""i¾Mario Moreno, IT Architect Leader, Generali France""Service-Oriented Architecture enables organizations to be agile and flexible enough to adopt new business strategies and produce new services to overcome the challenges created by business dynamism today. CIOs have to consider SOA as a foundation of their Enterprise Applications Architecture primarily because it demonstrates that IT aligns to business processes and also because it positions IT as a service enabler and maximizes previous investments on business applications.To understand and profit from SOA, this book provides CIOs with the necessary concepts and knowledge needed to understand and adapt it into their IT organizations.""i¾Sabri Hamed Al-Azazi, CIO of Dubai Holding, Sabri""I am extremely impressed by the depth and scale of this book! The title is perfecti¾when you know where you want to go, you need a compass to guide you there! After good IT strategy leads you to SOA, this book is the perfect vehicle that will drive you from dream to reality. We in DSK Bank will use it as our SOA bible in the ongoing project.""i¾Miro Vichev, CIO, DSK Bank, Bulgaria, member of OTP Group""Service-Oriented Architecture offers a pathway to networking of intra- and inter-corporate business systems. The standards have the potential to create far more flexible and resilient business information systems than have been possible in the past. This book is a must-read for those who care about the future of business IT.""i¾Elizabeth Hackenson, CIO, MCI""Service-Oriented Architecture is key to help customers become on demand businessesi¾a business that can quickly respond to competitive threats and be first to take advantage of marketplace opportunities. SOA Compass is a must-read for those individuals looking to bridge the gap between IT and business in order to help their enterprises become more flexible and responsive.""i¾Michael Liebow, Vice President, Web Services and Service-Oriented Architecture, IBM Business Consulting Services""This book is a welcome addition to SOA literature. It articulates the business case and provides practical proven real-world advice, guidance, tips, and techniques for organizations to make the evolution from simple point-to-point web services to true SOA by addressing such topics as planning, organization, analysis and design, security, and systems management.""i¾Denis O'Sullivan, Fireman's Fund Enterprise ArchitectMaximize the business value and flexibility of your SOA deploymentIn this book, IBM Enterprise Integration Team experts present a start-to-finish guide to planning, implementing, and managing Service-Oriented Architecture. Drawing on their extensive experience helping enterprise customers migrate to SOA, the authors share hard-earned lessons and best practices for architects, project managers, and software development leaders alike.Well-written and practical, Service-Oriented Architecture Compass offers the perfect blend of principles and ""how-to"" guidance for transitioning your infrastructure to SOA. The authors clearly explain what SOA is, the opportunities it offers, and how it differs from earlier approaches. Using detailed examples from IBM consulting engagements, they show how to deploy SOA solutions that tightly integrate with your processes and operations, delivering maximum flexibility and value. With detailed coverage of topics ranging from policy-based management to workflow implementation, no other SOA book offers comparable value to workingIT professionals.Coverage includes SOA from both a business and technical standpointi¾and how to make the business case Planning your SOA project: best practices and pitfalls to avoid SOA analysis and design for superior flexibility and value Securing and managing your SOA environment Using SOA to simplify enterprise application integration Implementing business processes and workflow in SOA environments Case studies in SOA deployment After you've deployed: delivering better collaboration, greater scalability, and more sophisticated applicationsThe IBM Press developerWorks® Series is a unique undertaking in which print books and the Web are mutually supportive. The publications in this series are complemented by resources on the developerWorks Web site on ibm.com. Icons throughout the book alert the reader to these valuable resources.",oceanology,177
333ccba50d00f3e8c4766e0c5e179ff4de31036f,filtered,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,knowledge exchange within the particle accelerator community via cloud computing*,https://www.semanticscholar.org/paper/333ccba50d00f3e8c4766e0c5e179ff4de31036f,"The development, testing and use of particle accelerator modeling codes is a core competency of accelerator research laboratories around the world, and likewise for synchrotron radiation and X-ray optics codes at lightsource facilities. Such codes require time and training to learn a command-line workflow involving multiple input and configuration files, execution on a high-performance server or cluster, post-processing with specialized software and finally visualization. Such workflows are error prone and difficult to reproduce. Cloud computing and UI design are core competencies of RadiaSoft LLC, where the Sirepo framework is being developed to make state of the art codes available in the browser of any desktop, laptop or tablet. We present our initial successes as real world examples of knowledge exchange between industry and the research community. This work is leading to broader knowledge exchange throughout the community by facilitating education of students and enabling instantaneous sharing of simulation details between colleagues. Sirepo design objectives include: seamless integration with legacy codes, low barrier to entry for new users, configuration transfer to command-line mode, catalog of provenance to aid reproducibility, and simplified collaboration through multimodal sharing. The combination of intuitive browserbased GUIs and Sirepo's server-side application container technology enables simplified computational archiving and reproducibility. If embraced by the community, this could become an important asset for the design, commissioning and future upgrade of particle accelerator and Xray beamline facilities. SIREPO – A SOFTWARE FRAMEWORK Sirepo is an open source framework [1] for bringing any scientific, engineering or educational software to the cloud, with a GUI that works in any modern browser on any computing device with sufficient screen size, including tablets. The Sirepo client is built on HTML5 technologies, including the JavaScript libraries Bootstrap [2] and Angular [3]. The D3 library [4, 5] is used for interactive 2D graphics, while VTK [6] is used for 3D. The Sirepo server is built on Flask [7], a lightweight framework for web development with Python. The scientific codes supported by Sirepo, and all of their dependencies, are containerized via Docker [8], which is an open platform for distributed applications. RadiaSoft has developed open source software [9] and expertise for building, deploying and reliably executing scientific codes in Docker containers [10, 11]. RadiaSoft docker images are publicly available [12] as part of the Sirepo ecosystem. SIREPO – A SCIENTIFIC GATEWAY RadiaSoft maintains a free scientific gateway for the particle accelerator community [13], which provides a broad selection of supported codes. The most mature implementations are for SRW (Synchrotron Radiation Workshop) [14-16] and elegant [17, 18]. SRW is an open source [19] physical optics code with powerful features for calculating synchrotron radiation and X-Ray optics, including successful detailed benchmarking against state-of-the-art X-Ray beamlines [20]. Sirepo/SRW has a growing number of users at synchrotron and free electron laser (FEL) user facilities around the world, including: NSLS-II, LCLS, APS and ALS in the USA, ELETTRA in Italy, European XFEL in Germany, ESRF and SOLEIL in France, PSI in Switzerland, Diamond in the UK and LNLS in Brazil. Just as elegant is one of the most widely used codes in the world for particle accelerator simulation and design, Sirepo/elegant has many more users than the other supported accelerator codes, including regular classroom use at the US Particle Accelerator School (USPAS) [21, 22]. Other well-supported codes provide important capabilities: Synergia [23, 24] offers 2D and 3D space charge models and special features for simulating nonlinear integrable optics in the IOTA ring [25, 26], while Zgoubi [27] provides spin tracking and JSPEC [28, 29] models intrabeam scattering and electron cooling. There are two Sirepo implementations of the massivelyparallel open source particle-in-cell (PIC) code Warp [3033]. Warp PBA enables use of Warp’s quasi-3D electromagnetic PIC modeling of beamand laser-driven plasmabased accelerators. Warp VND uses Warp’s electrostatic PIC capabilities to simulate a wide range of problems, with near-term emphasis on thermionic converters and other types of vacuum nanoelectronic devices.",oceanology,178
7654c04648178149dad73ae3b1a93d404e631774,filtered,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,openstack in action,https://www.semanticscholar.org/paper/7654c04648178149dad73ae3b1a93d404e631774,"In the cloud computing model, a cluster of physical computers hosts an environment that provides shared services (public and private) and offers the flexibility to easily add, remove, and expand virtual servers and applications. OpenStack is an open source framework that can be installed on individual physical servers to a cloud platform and enables the building of custom infrastructure (IaaS), platform (PaaS), and software (SaaS) services without the high cost and vendor lock-in associated with proprietary cloud platforms. OpenStack in Action offers real world use cases and step-by-step instructions to develop cloud platforms from inception to deployment. It explains the design of both the physical hardware cluster and the infrastructure services needed to create a custom cloud platform. It shows how to select and set up virtual and physical servers, implement software-defined networking, and the myriad other technical details required to design, deploy, and operate an OpenStack cloud in an enterprise. It also discusses the cloud operation techniques needed to establish security practices, access control, efficient scalability, and day-to-day DevOps practices. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications.",oceanology,179
9456732ff41a7e8b125f229d00de12ffb59eaa67,filtered,semantic_scholar,,2020-01-01 00:00:00,semantic_scholar,analyzing and securing embedded systems,https://www.semanticscholar.org/paper/9456732ff41a7e8b125f229d00de12ffb59eaa67,"Author(s): Spensky, Chad | Advisor(s): Vigna, Giovanni; Kruegel, Christopher | Abstract: Embedded systems (i.e., single-purpose computers with tightly-coupled software and hardware) are now pervasive throughout in our increasingly digitized world. Due to the rapid growth of the embedded systems industry and the commercial pressure to implement new features, most of these systems are built using insecure hardware and have numerous latent software vulnerabilities. Unfortunately, the diversity of physical hardware and software implementations on these various systems along with their tight coupling between software and hardware have rendered most of our existing automated security analysis techniques ineffective. Attackers currently have the upper hand, as they need only discover a single vulnerability, whereas defenders must manually identify, and fix, all of the existing vulnerabilities. To make matters worse, many of these vulnerable embedded systems can interact with the physical world and, if compromised, could cause serious damage (e.g., a public utility) or even death (e.g., a medical device). To rectify this calamitous situation that we have created, we must be able to 1) identify and fix problems with the existing systems that are already deployed and 2) create future systems that are fundamentally secure, by design.Embedded systems are more difficult to analyze than traditional computers because the hardware platforms that they run on are far more diverse, have strict hardware dependencies, are equipped peripherals that differ wildly between systems, and their execution typically depends on external phenomena that materialize as hardware interrupts. The depth of the analysis can be improved by developing novel hardware-based introspection techniques, which would provide analysts with the ability to observe the internal state of the real embedded system in real-world scenarios. The scale of the analysis can also be improved by decoupling the firmware from the hardware through emulation techniques, which would permit analysts to parallelize their analyses across numerous emulated systems, without the need for hardware, and also experiment with the embedded system in a zero-risk virtual environment. I developed a novel hardware-based introspection technique for embedded systems that provides real-time, high-level insights into modifications made to both volatile and non-volatile memory using a Field-Programmable Gate Array (FPGA) implementation and novel semantic-gap reconstruction techniques. I also developed an approach to support the decoupling of firmware from its hardware that can use either hardware- or software-based instrumentation of the system to record the hardware interactions on the real system and then convert these recordings into generalized, composable ω-automata that can be used in place of the hardware for emulation.Embedded systems are also difficult to protect against hardware-based attacks, especially glitching. Ideally, firmware could be protected against these attacks using software-only techniques that could be deployed to the billions of existing systems to protect them from physical attacks, without physically replacing them. I developed an approach that permits embedded system developers to automatically inject various software-based glitching defenses into their code at compile-time, producing glitch-resistant firmware without the need for any code annotations or modifications to the embedded system’s hardware.",oceanology,180
b3adef935b0bcffff47f8f45900ce2c3b6bbaeed,filtered,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,enhancing interaction with augmented reality through mid-air haptic feedback: architecture design and user feedback,https://www.semanticscholar.org/paper/b3adef935b0bcffff47f8f45900ce2c3b6bbaeed,"Nowadays, Augmented-Reality (AR) head-mounted displays (HMD) deliver a more immersive visualization of virtual contents, but the available means of interaction, mainly based on gesture and/or voice, are yet limited and obviously lack realism and expressivity when compared to traditional physical means. In this sense, the integration of haptics within AR may help to deliver an enriched experience, while facilitating the performance of specific actions, such as repositioning or resizing tasks, that are still dependent on the user’s skills. In this direction, this paper gathers the description of a flexible architecture designed to deploy haptically enabled AR applications both for mobile and wearable visualization devices. The haptic feedback may be generated through a variety of devices (e.g., wearable, graspable, or mid-air ones), and the architecture facilitates handling the specificity of each. For this reason, within the paper, it is discussed how to generate a haptic representation of a 3D digital object depending on the application and the target device. Additionally, the paper includes an analysis of practical, relevant issues that arise when setting up a system to work with specific devices like HMD (e.g., HoloLens) and mid-air haptic devices (e.g., Ultrahaptics), such as the alignment between the real world and the virtual one. The architecture applicability is demonstrated through the implementation of two applications: (a) Form Inspector and (b) Simon Game, built for HoloLens and iOS mobile phones for visualization and for UHK for mid-air haptics delivery. These applications have been used to explore with nine users the efficiency, meaningfulness, and usefulness of mid-air haptics for form perception, object resizing, and push interaction tasks. Results show that, although mobile interaction is preferred when this option is available, haptics turn out to be more meaningful in identifying shapes when compared to what users initially expect and in contributing to the execution of resizing tasks. Moreover, this preliminary user study reveals some design issues when working with haptic AR. For example, users may be expecting a tailored interface metaphor, not necessarily inspired in natural interaction. This has been the case of our proposal of virtual pressable buttons, built mimicking real buttons by using haptics, but differently interpreted by the study participants.",oceanology,181
a3d76c4d3718a53465153917bba9b24205d83096,filtered,semantic_scholar,,2018-01-01 00:00:00,semantic_scholar,health 4.0 as an application of industry 4.0 in healthcare services and management,https://www.semanticscholar.org/paper/a3d76c4d3718a53465153917bba9b24205d83096,"The Industry 4.0 Standard (I4S) employs technologies for automation and data exchange through cloud computing, Big Data (BD), Internet of Things (IoT), forms of wireless Internet, 5G technologies, cryptography, the use of semantic database (DB) design, Augmented Reality (AR) and Content-Based Image Retrieval (CBIR). Its healthcare extension is the so-called Health 4.0. 
This study informs about Health 4.0 and its potential to extend, virtualize and enable new healthcare-related processes (e.g., home care, finitude medicine, and personalized/remotely triggered pharmaceutical treatments) and transform them into services. 
In the future, these services will be able to virtualize multiple levels of care, connect devices and move to Personalized Medicine (PM). The Health 4.0 Cyber-Physical System (HCPS) contains several types of computers, communications, storage, interfaces, biosensors, and bioactuators. The HCPS paradigm permits observing processes from the real world, as well as monitoring patients before, during and after surgical procedures using biosensors. Besides, HCPSs contain bioactuators that accomplish the intended interventions along with other novel strategies to deploy PM. A biosensor detects some critical outer and inner patient conditions and sends these signals to a Decision-Making Unit (DMU). Mobile devices and wearables are present examples of gadgets containing biosensors. Once the DMU receives signals, they can be compared to the patient’s medical history and, depending on the protocols, a set of measures to handle a given situation will follow. The part responsible for the implementation of the automated mitigation actions are the bioactuators, which can vary from a buzzer to the remote-controlled release of some elements in a capsule inside the patient’s body. 
            Decentralizing health services is a challenge for the creation of health-related applications. Together, CBIR systems can enable access to information from multimedia and multimodality images, which can aid in patient diagnosis and medical decision-making. 
Currently, the National Health Service addresses the application of communication tools to patients and medical teams to intensify the transfer of treatments from the hospital to the home, without disruption in outpatient services. 
HCPS technologies share tools with remote servers, allowing data embedding and BD analysis and permit easy integration of healthcare professionals expertise with intelligent devices.  However, it is undeniable the need for improvements, multidisciplinary discussions, strong laws/protocols, inventories about the impact of novel techniques on patients/caregivers as well as rigorous tests of accuracy until reaching the level of automating any medical care technological initiative.",oceanology,182
7f27c90447026bbee088a3b58a62ed57b1bd7102,filtered,semantic_scholar,2019 IEEE International Systems Conference (SysCon),2019-01-01 00:00:00,semantic_scholar,automated optimization of software parameters in a long term evolution radio base station,https://www.semanticscholar.org/paper/7f27c90447026bbee088a3b58a62ed57b1bd7102,"Radio network optimization is concerned with the configuration of radio base station parameters in order to achieve the desired level of service quality in addition to many other differentiating technical factors. Mobile network operators have different physical locations, levels of traffic profiles, number of connected devices, and the desired quality of service. All of these conditions make the problem of optimizing the parameters of a radio base station specific to the operator’s business goals. The high number of calibration parameters and the complex interaction between them make the system behave as a black-box model for any practical purpose. The computation of relevant operator metrics is often stochastic, and it can take several minutes to compute the effect of changing a single, making it impractical to optimize systems with approaches that require a large number of iterations. Operators want to optimize their already deployed system in online scenarios while minimizing the exposure of the system to a negative set of parameters during the optimization procedure. {This paper presents a novel approach to the optimization of a Long Term Evolution (LTE) radio base station in a large search space with an expensive stochastic objective and a limited regret bounds scenario. We show the feasibility of this approach by implementing it in an industrial testing bed radio base station connected to real User Equipment (UE) in collaboration with Ericsson. Two optimization processes in this experimental setup are executed to show the feasibility of the approach in real-world scenarios.",oceanology,183
5da0b64bded1123cfd8064b60dc78ca2a4a57075,filtered,semantic_scholar,2016 23rd International Conference on Telecommunications (ICT),2016-01-01 00:00:00,semantic_scholar,openwino: an open hardware and software framework for fast-prototyping in the iot,https://www.semanticscholar.org/paper/5da0b64bded1123cfd8064b60dc78ca2a4a57075,"The Internet of Things promises an always-connected future where the objects surrounding us will communicate in order to make our lives easier, more secure, etc. This evolution is a research opportunity as new solutions must be found to problems ranging from network interconnection to data mining. In the networking community, innovative solutions are being developed for the Device Layer of the Internet of Things, which includes the IoT wireless protocols. In order to study their performance, researchers turn more often to real world platforms, commonly designated by the term “testbeds”, on which they may implement and test the protocols and algorithms. This is even more important in the Industrial IoT field, where environments are perturbed by industrial systems like automated production systems. In this paper, after a brief presentation of the context of testbeds, we introduce WiNo and OpenWiNo, an open hardware and software framework for fast-prototyping in the field of the Internet of Things. Compared to existing platforms, the solution WiNo+OpenWiNo offers a wide array of Physical layers and easy integration of various sensors as it is developed as part of the Arduino ecosystem. It also allows research teams to easily and quickly deploy their own testbed into real environments.",oceanology,184
25ea28e7137dc82f157f13d5c26208918ecf38c4,filtered,semantic_scholar,MILCOM 2012 - 2012 IEEE Military Communications Conference,2012-01-01 00:00:00,semantic_scholar,a testbed for collaborative development of underwater communications and networking,https://www.semanticscholar.org/paper/25ea28e7137dc82f157f13d5c26208918ecf38c4,"The possibilities opened with the increased use of autonomous underwater vehicles and their potential interactions with existing or prospective submerged sensor networks create an end-user technological pull on the communication capabilities for the underwater domain. Simulation models, while fundamental in the scientific and technological development process cannot offer the feature richness of the physical environment and may potentially mask software and hardware behaviours exposed by the real world. This paper presents a testbed implemented by the NATO STO Centre for Maritime Research and Experimentation (CMRE), deployed to foster cooperative development of underwater communications and networking by providing an “hardware-in-the-world” capability to scientists and engineers. The data collection infrastructure provides a comprehensive data set of environmental measurements relevant to underwater acoustic propagation, arbitrary waveform generation within two frequency bands (useful for channel probing and testing of modulation schemes), full band raw acoustic data recording and access to two sets of fundamentally different commercially available acoustic modems. This structured data collection allows for a comprehensive analysis of the environment variables, their impact on the acoustic channel evolution and how this affects end-to-end connectivity of acoustic modems which can be used to steer the design choices for networking protocols.",oceanology,185
431649836fc44c444b0640b0e9efd2e7eb591f1c,filtered,semantic_scholar,,2018-01-01 00:00:00,semantic_scholar,automatic classification of natural signals for environmental monitoring.,https://www.semanticscholar.org/paper/431649836fc44c444b0640b0e9efd2e7eb591f1c,"This manuscript summarizes a three years work addressing the use of machine learning for the automatic analysis of natural signals. The main goal of this PhD is to produce efficient and operative frameworks for the analysis of environmental signals, in order to gather knowledge and better understand the considered environment. Particularly, we focus on the automatic tasks of detection and classification of natural events.This thesis proposes two tools based on supervised machine learning (Support Vector Machine, Random Forest) for (i) the automatic classification of events and (ii) the automatic detection and classification of events. The success of the proposed approaches lies in the feature space used to represent the signals. This relies on a detailed description of the raw acquisitions in various domains: temporal, spectral and cepstral. A comparison with features extracted using convolutional neural networks (deep learning) is also made, and favours the physical features to the use of deep learning methods to represent transient signals.The proposed tools are tested and validated on real world acquisitions from different environments: (i) underwater and (ii) volcanic areas. The first application considered in this thesis is devoted to the monitoring of coastal underwater areas using acoustic signals: continuous recordings are analysed to automatically detect and classify fish sounds. A day to day pattern in the fish behaviour is revealed. The second application targets volcanoes monitoring: the proposed system classifies seismic events into categories, which can be associated to different phases of the internal activity of volcanoes. The study is conducted on six years of volcano-seismic data recorded on Ubinas volcano (Peru). In particular, the outcomes of the proposed automatic classification system helped in the discovery of misclassifications in the manual annotation of the recordings. In addition, the proposed automatic classification framework of volcano-seismic signals has been deployed and tested in Indonesia for the monitoring of Mount Merapi. The software implementation of the framework developed in this thesis has been collected in the Automatic Analysis Architecture (AAA) package and is freely available.",oceanology,186
8646ca3ed644b8ae61f9432f28d5e025cb1f18ea,filtered,semantic_scholar,IECON 2015 - 41st Annual Conference of the IEEE Industrial Electronics Society,2015-01-01 00:00:00,semantic_scholar,green city: a low-cost testbed for distributed control algorithms in smart grid,https://www.semanticscholar.org/paper/8646ca3ed644b8ae61f9432f28d5e025cb1f18ea,"As a type of Cyber-Physical Systems (CPSs), Smart Grid has been adding more communication and control capabilities to improve power efficiency and availability. Especially, more and more distributed control algorithms have been developed for Smart Grids because of their flexibility and robustness. In order to deploy them in real electric power systems, distributed control algorithms must be tested, not only in theoretical simulations, but also in testbeds subject to real world constraints that can provide feedback to make the algorithm robust. Implementations of these algorithms in a Smart Grid environment are facing many cyber-physical challenges such as possible communication failures or imperfections, noisy signals, etc. These challenges can lead to increasing economical expenditure or cause failure of the power system. There exist different approaches for testing distributed control algorithms, from using state-of-the-art facilities to software or hardware-in-the-loop simulations. To better emulate real-world electric grid operation scenarios with low capital investment, in this paper the Green City (GC) testbed is proposed as a suitable platform for both control theory researchers in Smart Grid, and for engineering education, allowing students to learn through hands-on experiences. GC has been conceived as a multi-agent networked CPS with the following main features: 1- Smart Grid environment emulation with low-cost physical elements; 2- Fast prototyping capability of distributed control algorithms for Smart Grid.",oceanology,187
497ed558a130464edf5ae4b974e35cb6b374a54d,filtered,semantic_scholar,,2017-01-01 00:00:00,semantic_scholar,an active learning environment to improve first-year mechanical engineering retention rates and software skills,https://www.semanticscholar.org/paper/497ed558a130464edf5ae4b974e35cb6b374a54d,"This work proposes a foundational change from traditional lecture to an active learning environment in the Colorado State University First-Year Introduction to Mechanical Engineering course of 145 students. The goal of this approach is to improve computational capabilities in Mechanical Engineering and long-term retention rates with a single broad emphasis. Major and minor changes were implemented in the course, from specific day to day in-class activities to the addition of laboratory sessions to replace traditional classroom lecture. These laboratories of no more than fifteen students were delivered by Learning Assistants, which were upper-level undergraduate peer educators. To evaluate proficiency, a MATLAB post-test was delivered to students who were instructed through lecture only (“Lecture”) and those who were instructed with the above changes (“Active”). A survey was also provided upon completion of the course to the Active group for student reflection on their perceived software capability and the usefulness of approaches. Post-test results suggest that the Active group was more proficient in MATLAB than the Lecture group. Survey results suggest that the Active group recognize they had not achieved expert use of the software but that they were likely to use it throughout their careers and that all approaches were useful, in particular the use of Learning Assistants. Future longterm retention statistics will shed light on the possible effectiveness of this approach, which are currently unavailable. Introduction Colorado State University has a total student enrollment in excess of 33,000. As a land grant university, the historic mission of the institution is to provide students with an education in practical fields such as agriculture and engineering. The College of Engineering has a growing student cohort, with an increase from ~450 first-year students Fall 2010 to ~600 students Fall 2015 [1]. However, persistence and graduation rates have remained fairly steady over the last fifteen years. The current six year persistence rate within the college is only ~45% and the six year graduation rate within the college is similar at ~43%. Many students do not remain within the college for even a full year, as the second fall persistence rate is only 70-75% [1]. These data show a significant portion of enrolled first-year engineering students do not remain within the program long enough to be exposed to foundational engineering content, which starts in the sophomore year with engineering specific courses. A current goal of the college is to improve these retention statistics. Additionally, many students do not develop the necessary software skills required to use computational tools such as MATLAB, which are integral to success in the curriculum. Students who do not develop these skills during introductory coursework must “catch up” in later courses, where the technical content is more challenging. We hypothesize this can lead to unpreparedness for challenging content or careers as an engineer and can negatively impact academic standing, leading to decreased retention. Thus, the goals of this work were to 1) improve retention rates for first-year engineering students, specifically mechanical engineering, and 2) improve computational and software skills of first-year students, specifically MATLAB and Microsoft Excel. MATLAB is a common computational package which can be used for a broad range of engineering problems throughout a curriculum [2]. However, learning Excel and MATLAB through lecture is challenging, as these tools are best understood through utilization, not observation [3]. MATLAB and other computational tools are often taught in classrooms with computational equipment, however this is can be a challenge with a large classroom [4]. Some have utilized computer based tutorials which students can complete on their own time [5], while others implemented a large scale deployment of personal computers equipped with MATLAB and other software [6]. Additionally, the use of peer-educators can be an effective approach to facilitating MATLAB development [7]. Thus, we have chosen to employ an approach which utilizes an active environment to learn MATLAB and other introductory content through the use of laboratory sessions and peer-educators, in this case the Learning Assistant model [8]. Similar to previous approaches, we have utilized classroom lectures, hands on in-class activities, and laboratory sessions [9]. The Introduction to Mechanical Engineering Course (MECH 103) was developed to provide students with an overview of the mechanical engineering discipline and as an introduction to the computational packages MATALB [10] and Microsoft Excel. The course consists of between 140 and 250 first-year students and was previously delivered using traditional lecture. While this approach was most efficient for a single instructor due to the enrollment size, this resulted in a static learning environment for a course which should excite students about mechanical engineering and provide foundational technical skills. The overall approach to this work was to thus create an active environment for students within the course, which had an enrollment of 145 students for the Fall 2016 semester. The rationale to this approach was that by providing students with hands-on experiences working with mechanical engineering problems and computational software, the understanding of course content will improve [11,12] whereby improving retention [13]. While some immediate test and survey data were acquired and are shown in this work, it is important to note that the true impact on retention is not currently recognizable and will require future analysis. In-Class Sessions Class sessions were varied throughout the semester and the week, as they typically included lectured course material, guest lectures or panels, and activities. The course met Monday, Wednesday, and Friday from 9-9:50 AM in a large lecture hall with individual stadium seating. Friday lecture was often cancelled and this time was spent in weekly laboratory sessions instead, which are outlined in the next section. Monday class time was assigned to covering course content through lecture, teamwork activities, and in class problems. The content of the course included general introductory material such as teamwork, communication, and design, commonly used units and unit conversions, mathematical models and systems, and an introduction to Microsoft Excel and MATLAB. Active engagement in the class included a teamwork design problem, requiring students to break into groups of three. Due to the theater seating layout of the classroom, groups of four or more made successful teamwork and communication difficult. Each group of students were provided one piece of 8.5” x 11” blank printer paper, one paperclip, and two pieces of scotch tape. The design problem was simple: build the tallest free standing structure possible using only the given materials. This was an inexpensive and simple approach to teamwork design activity. In place of a lecture or even a discussion on how to use design techniques for a simple problem such as this, students were able to actively engage in this process despite the difficulties of class size and layout. While students typically have an excellent understanding of units such as a pound (lb), their physical understanding of units such as a Joule or Watt are less developed within the context of everyday life. To provide students with a meaningful representation of energy (Joule) and power (Watt), they were provided a common object – in this case a softball – and asked to calculate how high they would have to raise the object to exert one Joule of energy – in this case roughly a foot and a half. While simple and inexpensive, this activity provided students with useful knowledge they can apply without a calculator and helps them relate coursework to the real world. For example, if they can place a Joule into real-world context, they could then answer the question “Can I launch a rocket into space using a thousand Joules?”. Wednesday lecture sessions were commonly used for guest lecturers and panels. These class sessions included the College of Engineering Dean, faculty members and graduate students in mechanical engineering, industry panelists, entrepreneurs and small business owners, and an interactive teamwork theatre troupe. The goal of these sessions was to provide students with a broad overview of different disciplines within mechanical engineering and what skills are necessary to succeed in various professional roles. While emphasizing an active learning environment is inherently difficult with each and every guest, student engagement was addressed by delivering variability in all of the presentations and strongly encouraging students to ask questions. For example, the theater troupe was an interactive experience where students were able to act as a team member within a group that mocked to show a diverse team struggling with communication. This session involved humor, discussion, and lively responses from students in place of a traditional static lecture. Laboratory Sessions In place of Friday lecture, students were asked to attend laboratory sessions for one hour [14,3]. A total of eleven sessions were provided throughout the week to accommodate all schedules. Sessions included one instructor, 13-16 students, and were held in laboratories with individual workstations with Microsoft Excel and MATLAB software. Laboratory instructors included a Graduate Teaching Fellow and Undergraduate Learning Assistants (LAs). Laboratory sessions involved a short (<5 minutes) lecture briefly reviewing content from class before students began working on assigned problems. These problems implemented course content such as the use of Excel or MATLAB to analyze and display data through real-world applications. An example of utilizing MATLAB to simulate rolling a die is p",oceanology,188
55b4107c8d37629d0378671324f56f9e801a6d4e,filtered,semantic_scholar,KDD,2015-01-01 00:00:00,semantic_scholar,efficient long-term degradation profiling in time series for complex physical systems,https://www.semanticscholar.org/paper/55b4107c8d37629d0378671324f56f9e801a6d4e,"The long term operation of physical systems inevitably leads to their wearing out, and may cause degradations in performance or the unexpected failure of the entire system. To reduce the possibility of such unanticipated failures, the system must be monitored for tell-tale symptoms of degradation that are suggestive of imminent failure. In this work, we introduce a novel time series analysis technique that allows the decomposition of the time series into trend and fluctuation components, providing the monitoring software with actionable information about the changes of the system's behavior over time. We analyze the underlying problem and formulate it to a Quadratic Programming (QP) problem that can be solved with existing QP-solvers. However, when the profiling resolution is high, as generally required by real-world applications, such a decomposition becomes intractable to general QP-solvers. To speed up the problem solving, we further transform the problem and present a novel QP formulation, Non-negative QP, for the problem and demonstrate a tractable solution that bypasses the use of slow general QP-solvers. We demonstrate our ideas on both synthetic and real datasets, showing that our method allows us to accurately extract the degradation phenomenon of time series. We further demonstrate the generality of our ideas by applying them beyond classic machine prognostics to problems in identifying the influence of news events on currency exchange rates and stock prices. We fully implement our profiling system and deploy it into several physical systems, such as chemical plants and nuclear power plants, and it greatly helps detect the degradation phenomenon, and diagnose the corresponding components.",oceanology,189
1214515daae90180cf912789250888c6233f4d07,filtered,semantic_scholar,Int. J. Distributed Sens. Networks,2017-01-01 00:00:00,semantic_scholar,three-dimensional wireless ad hoc and sensor networks 2016,https://www.semanticscholar.org/paper/1214515daae90180cf912789250888c6233f4d07,"Distinct difference between twoand three-dimensional spaces has led to new research challenges to provide self-organizing communications for wireless ad hoc and sensor networks. Therefore, conventional approaches to extend or modify the existing schemes in twodimensional space cannot meet the specific requirements for three-dimensional networks. Instead, a new design and its implementation are usually demanded to accelerate deployment in real world. Based on research motivation, our previous Special Issue in 2014, ‘‘Three-dimensional wireless ad hoc and sensor networks,’’ seems successful in points of presenting the existing research efforts and attracting the interests from the community. In accordance with achievement, we intend to organize the second Special Issue for the same research area. While previous Special Issue was supposed to address the fundamental design issue, more practical approaches which contribute to advances in three-dimensional wireless ad hoc and sensor networks are our major objective. While considering our objective, editors believe that this Special Issue provides collection of articles on networking technique in three-dimensional ad hoc and sensor networks. We have selected 7 valuable papers out of 12 submissions in several aspects such as relevance to Special Issue and novelty of solution. The topic of these papers is roughly categorized into following areas: error detecting, localization, routing protocol, application, and simulation tool. In the first paper titled ‘‘A hybrid decoding of Reed– Muller codes,’’ Shuang Li et al. proposed hybrid decoding algorithm for Reed–Muller (RM) codes to decrease the number of floating-point multiplications significantly. The proposed algorithm reduced computational complexity for decoding of RM codes by terminating recursion procedure in earlier stage. A simplified maximum-likelihood (ML) decision based on fast Hadamard transform (FHT) is another source of low complexity. Simulation results were given to prove the improved performance of error correction as compared to conventional algorithms. Next two papers are related to localization problem. One is for Bluetooth and the other for wireless sensor networks. In the second paper titled ‘‘Three-dimensional positioning system using Bluetooth low-energy beacons,’’ Hyunwook Park et al. introduced threedimensional positioning scheme for Bluetooth. The proposed scheme used Bluetooth low-energy (BLE) beacons to estimate the distance and calculated threedimensional coordinates based on three-dimensional triangulation. A proposed scheme measures threedimensional location of moving nodes by employing four fixed position beacon nodes to form random sphere to collect position of each node. Simulation results reveal that the proposed method can reduce distance error rate rather than the existing twodimensional triangulation method. In the third paper titled ‘‘A distance-based maximum likelihood estimation method for sensor localization in wireless sensor networks,’’ Jing Xu et al. studied node localization in wireless sensor networks since conventional maximumlikelihood estimation (MLE) scheme based on received signal strength indicator (RSSI) failed to reflect the physical characteristics properly. To address this issue, in this paper, distance-based MLE (DB-MLE) to consider measurement errors was formulated as a complicated nonlinear optimization problem. Furthermore, two solutions based on first-order optimal condition and two-dimensional search method were presented. Simulation results showed that DB-MLE provided higher localization accuracy than the other methods. In the fourth paper titled ‘‘Autonomous drone for delay-tolerant networks in indoor applications,’’ Rados1aw O. Schoeneich et al. introduced interesting idea and application of autonomous drones as mobile message ferries in delay-tolerant networks. In order to prove applicability, universal software architecture of drone based on Android devices and its detail prototype were presented. This implementation was tested with the autonomous movement and observed to pass all relevant tests. In the fifth paper titled ‘‘Cooperative downloading in mobile ad hoc networks: a cost-energy perspective,’’ He Li et al. presented another interesting application, cooperative downloading. To improve file downloading, contents are distributed by the help of mobile",oceanology,190
e1184cc1e4725f7736d9944a33ada01a626cedc3,filtered,semantic_scholar,,2006-01-01 00:00:00,semantic_scholar,learning in robotics,https://www.semanticscholar.org/paper/e1184cc1e4725f7736d9944a33ada01a626cedc3,"For a robot, the ability to get from one place to another is one of the most basic skills. However, locomotion on legged robots is a challenging multidimensional control problem. This paper presents a machine learning approach to legged locomotion, with all training done on the physical robots. The main contributions are a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithms for learning quadrupedal locomotion. The resulting learned walk is considerably faster than all previously reported hand-coded walks for the same robot platform. Introduction The ability to deploy a fully autonomous robot in an unstructured, dynamic environment (the proverbial real world) over an extended period of time remains an open challenge in the field of robotics. Considerable progress is being made towards many components of this task including physical agility, power management, and on-board sensor technology. One such component that has drawn considerable interest recently is the ability for a robot to autonomously learn to improve its own performance (Ng et al. 2004; Bagnell & Schneider 2001; Zhang & Vadakkepat 2003). Despite this interest, considerable work remains due to the di fficulties associated with machine learning in the real world . Compared to other machine learning scenarios such as classification or action learning in simulation, learning o n physical robots presents several formidable challenges, i ncluding the following. Sparse Training Data: It is often prohibitively difficult to generate large amounts of data due to the maintenance required on robots, such as battery changes, hardware repairs, and, usually, constant human supervision. Thus, learning methods designed for physical robots must be effective with small amounts of data. Dynamical Complexity: The dynamics of many robotic control tasks are too complex for faithful simulation to be possible. Furthermore, robots are inherently situated in an unstructured environment with unpredictable sensor and actuator noise, namely the real world. Thus, even when off-line simulation is possible, it can never be fully reflective of the target environment. In this paper, we overcome these challenges for one concrete complex robot task, namely legged locomotion. Using a commercially available quadruped robot, we fully automate the training process (other than battery changes) and Copyright c © 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. employ machine learning algorithms that are sufficiently data efficient to enable productive learning on physical robots in a matter of hours. The resulting learned walk is considerably faster than all previously reported hand-cod ed walks for the same robot platform. This paper contributes both a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithms for learning quadrupedal locomotion. The remainder of the paper is organized as follows. First, we introduce the parameterized walk which our learning process seeks to optimize. We then specify our four learning approaches, and follow with detailed empirical results. We close with a discussion of their implications and possible avenues for future work. A Parameterized Walk The Sony Aibo ERS-210A is a commercially available robot that is equipped with a color CMOS camera and an optional ethernet card that can be used for wireless communication. The Aibo is a quadruped robot, and has three degrees of freedom in each of its four legs (Sony 2004). At the lowest level, the Aibo’s gait is determined by a series of joint positions for the three joints in each of its leg s. An early attempt to develop a gait by Hornby et al. (1999) involved using a genetic algorithm to learn a set of lowlevel parameters that described joint velocities and body p osition.1 More recent attempts to develop gaits for the Aibo have involved adopting a higher-level representation that deals with the trajectories of the Aibo’s four feet through three-dimensional space. An inverse kinematics calculati on is then used to convert these trajectories into joint angles . Among higher-level approaches, most of the differences between gaits that have been developed for the Aibo stem from the shape of the loci through which the feet pass and the exact parameterizations of those loci. For example, a team from the University of New South Wales achieved the fastest known hand-tuned gait using the high-level approach described above with trapezoidal loci. They subsequently generated an even faster walk via learning (Kim & Uther 2003). A team from Germany created a flexible gait implementation that allows them to use a variety of different shapes of loci (Rofer et al. 2003), and the team from the University of Newcastle was able to generate highvelocity gaits using a genetic algorithm and loci of arbitra ry shape (Quinlan, Chalup, & Middleton 2003). Our team (UT Austin Villa, Stone t al. 2004) first approached the gait optimization problem by hand-tuning Developed on an earlier version of the Aibo. a gait described by half-elliptical loci. This gait performed comparably to those of other teams participating in RoboCup 2003. The work reported in this paper uses the hand-tuned UT Austin Villa walk as a starting point for learning. Figure 1 compares the reported speeds of the gaits mentioned above, both hand-tuned and learned, including that of our starting point, the UT Austin Villa walk. The latter walk is described fully in a team technical report (Stone et al. 2004). The remainder of this section describes those details of the UT Austin Villa walk that are important to understand for the purposes of this paper. Hand-tuned gaits Learned gaits CMU Austin Villa UNSW Hornby UNSW NUBots (2002) (2003) (2003) (1999) (2003) (2003) 200 245 254 170 270 296 Figure 1: Maximum forward velocities of the best gaits (in mm/s) for different teams, both learned and hand-tuned. The half-elliptical locus used by our team is shown in Figure 2. By instructing each foot to move through a locus of this shape, with each pair of diagonally opposite legs in phase with each other and perfectly out of phase with the other two (a gait known as a trot), we enable the Aibo to walk. Four parameters define this elliptical locus: 1. The length of the ellipse; 2. The height of the ellipse; 3. The position of the ellipse on the x axis; and 4. The position of the ellipse on the y axis. Since the Aibo is roughly symz",oceanology,191
47486d010d940cc014bd1df89974b53bd73e91a2,filtered,semantic_scholar,2017 IEEE 14th International Conference on e-Business Engineering (ICEBE),2017-01-01 00:00:00,semantic_scholar,graph analysis of fog computing systems for industry 4.0,https://www.semanticscholar.org/paper/47486d010d940cc014bd1df89974b53bd73e91a2,"Increased adoption of Fog Computing concepts into Cyber Physical Systems (CPS) is a driving force for implementing Industry 4.0. The modern industrial environment focuses on providing a flexible factory floor that suits the needs of modern manufacturing through the reduction of downtimes, reconfiguration times, adoption of new technologies and the increase of its production capabilities and rates. Fog Computing through CPS aims to provide a flexible orchestration and management platform that can meet the needs of this emerging industry model. Proposals on Fog Computing platform and Software Defined Networks (SDN) for Industry allow for resource virtualization and access throughout the system enabling large composite application systems to be deployed on multiple nodes. The increase of reliability, redundancy and runtime parameters as well as the reduction of costs in such systems are of key interest to Industry and researchers as well. The development of optimization algorithms and methods is made difficult by the complexity of such systems and the lack of real-world data on fog systems resulting in algorithms that are not being designed for real world scenarios. We propose a set of use-case scenarios based on our Industrial partner that we analyze to determine the graph based parameters of the system that allows us to scale and generate a more realistic testing scenario for future optimization attempts as well as determine the nature of such systems in comparison to other networks types. To show the differences between these scenarios and our real-world use-case we have selected a set of key graph characteristics based on which we analyze and compare the resulting graphs from the systems.",oceanology,192
d753886538b5a9970a79212f5a995d55a8ee4c33,filtered,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,a adaft : a framework for adaptive fault tolerance for cyber-physical systems,https://www.semanticscholar.org/paper/d753886538b5a9970a79212f5a995d55a8ee4c33,"Cyber-physical systems frequently have to use massive redundancy to meet application requirements for high reliability. While such redundancy is required, it can be activated adaptively, based on the current state of the controlled plant. Most of the time the plant is in a state that allows for a lower level of faulttolerance. Avoiding the continuous deployment of massive fault tolerance will greatly reduce the workload of the CPS, and lower the operating temperature of the cyber sub-system, thus increasing its reliability. In this paper, we extend our prior research by demonstrating a software simulation framework (AdaFT) that can automatically generate the sub-spaces within which our adaptive fault tolerance can be applied. We also show the theoretical benefits of AdaFT, and its actual implementation in several real world CPSs. CCS Concepts: rComputer systems organization→ Embedded systems; Redundancy; Robotics;",oceanology,193
6e7e52c8f59ec975cb9b850cef1ecf8470b9c28a,filtered,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,physical hardware-in-the-loop modelling and simulation,https://www.semanticscholar.org/paper/6e7e52c8f59ec975cb9b850cef1ecf8470b9c28a,"It is too risky to install a newly-designed device, component, or controller, directly into a real power system without rigorous testing. To help to de-risk the system integration, and to assist in the design process, computer simulation is an accepted and widely-adopted tool. However, in a simulation-only environment, many real-world issues such as noise, randomness of event timings, and hardware design issues are not well explored. In addition, there are limits on the size and fidelity of system which can be simulated, due to the required computational intensity, and because control systems for devices often contain software which is proprietary and cannot be modelled accurately. Physical Hardware in the Loop Simulation provides an interim stage between purely computer-based simulation, and real device deployment. Part of the power system (or “Smart Grid”) is simulated, but specific components are implemented in actual hardware. The hardware may consist of instrumentation, relays or controllers, carrying no primary current. Such testing is termed “Secondary Hardware-in-the-Loop”, as the signals exchanged between the simulation and hardware consist only of measurements and control values. A more advanced environment is created where primary power flow is exchanged with the hardware. This is termed “Primary Hardware-in-the-Loop” or “Power Hardware-in-the-Loop” testing. In addition to measurement and control signals being exchanged with the simulation, an interface is required at which primary power is exchanged between the simulation and the hardware, at the voltage and current levels suitable for the hardware under test. Creation of such environments is complex, but allows steady-state, dynamic, and worst-case scenarios to be re-created in a controlled environment. Therefore hardware-in-the-loop testing offers a cheaper, safer, faster and more comprehensive de-risking process than trying the hardware for the first time on a real network. The complexity and interconnected nature of the Smart Grid means that such Hardware in the Loop based testing is becoming even more critical to understanding the behaviour of systems and schemes, and consequently the safe and secure introduction of new technologies.",oceanology,194
415584ce811fe9ca946336bff433171088d21da7,filtered,semantic_scholar,,2017-01-01 00:00:00,semantic_scholar,measurement-driven algorithm and system design for wireless and datacenter networks,https://www.semanticscholar.org/paper/415584ce811fe9ca946336bff433171088d21da7,"Measurement-Driven Algorithm and System Design for Wireless and Datacenter Networks Varun Gupta The growing number of mobile devices and data-intensive applications pose unique challenges for wireless access networks as well as datacenter networks that enable modern cloudbased services. With the enormous increase in volume and complexity of traffic from applications such as video streaming and cloud computing, the interconnection networks have become a major performance bottleneck. In this thesis, we study algorithms and architectures spanning several layers of the networking protocol stack that enable and accelerate novel applications and that are easily deployable and scalable. The design of these algorithms and architectures is motivated by measurements and observations in real world or experimental testbeds. In the first part of this thesis, we address the challenge of wireless content delivery in crowded areas. We present the AMuSe system, whose objective is to enable scalable and adaptive WiFi multicast. AMuSe is based on accurate receiver feedback and incurs a small control overhead. This feedback information can be used by the multicast sender to optimize multicast service quality, e.g., by dynamically adjusting transmission bitrate. Specifically, we develop an algorithm for dynamic selection of a subset of the multicast receivers as feedback nodes which periodically send information about the channel quality to the multicast sender. Further, we describe the Multicast Dynamic Rate Adaptation (MuDRA) algorithm that utilizes AMuSe’s feedback to optimally tune the physical layer multicast rate. MuDRA balances fast adaptation to channel conditions and stability, which is essential for multimedia applications. We implemented the AMuSe system on the ORBIT testbed and evaluated its performance in large groups with approximately 200 WiFi nodes. Our extensive experiments demonstrate that AMuSe can provide accurate feedback in a dense multicast environment. It outperforms several alternatives even in the case of external interference and changing network conditions. Further, our experimental evaluation of MuDRA on the ORBIT testbed shows that MuDRA outperforms other schemes and supports high throughput multicast flows to hundreds of nodes while meeting quality requirements. As an example application, MuDRA can support multiple high quality video streams, where 90% of the nodes report excellent or very good video quality. Next, we specifically focus on ensuring high Quality of Experience (QoE) for video streaming over WiFi multicast. We formulate the problem of joint adaptation of multicast transmission rate and video rate for ensuring high video QoE as a utility maximization problem and propose an online control algorithm called DYVR which is based on Lyapunov optimization techniques. We evaluated the performance of DYVR through analysis, simulations, and experiments using a testbed composed of Android devices and off the shelf APs. Our evaluation shows that DYVR can ensure high video rates while guaranteeing a low but acceptable number of segment losses, buffer underflows, and video rate switches. We leverage the lessons learnt from AMuSe for WiFi to address the performance issues with LTE evolved Multimedia Broadcast/Multicast Service (eMBMS). We present the Dynamic Monitoring (DyMo) system which provides low-overhead and real-time feedback about eMBMS performance. DyMo employs eMBMS for broadcasting instructions which indicate the reporting rates as a function of the observed Quality of Service (QoS) for each UE. This simple feedback mechanism collects very limited QoS reports which can be used for network optimization. We evaluated the performance of DyMo analytically and via simulations. DyMo infers the optimal eMBMS settings with extremely low overhead, while meeting strict QoS requirements under different UE mobility patterns and presence of network component failures. In the second part of the thesis, we study datacenter networks which are key enablers of the end-user applications such as video streaming and storage. Datacenter applications such as distributed file systems, one-to-many virtual machine migrations, and large-scale data processing involve bulk multicast flows. We propose a hardware and software system for enabling physical layer optical multicast in datacenter networks using passive optical splitters. We built a prototype and developed a simulation environment to evaluate the performance of the system for bulk multicasting. Our evaluation shows that the optical multicast architecture can achieve higher throughput and lower latency than IP multicast and peer-to-peer multicast schemes with lower switching energy consumption. Finally, we study the problem of congestion control in datacenter networks. Quantized Congestion Control (QCN), a switch-supported standard, utilizes direct multi-bit feedback from the network for hardware rate limiting. Although QCN has been shown to be fastreacting and effective, being a Layer-2 technology limits its adoption in IP-routed Layer 3 datacenters. We address several design challenges to overcome QCN feedback’s Layer2 limitation and use it to design window-based congestion control (QCN-CC) and load balancing (QCN-LB) schemes. Our extensive simulations, based on real world workloads, demonstrate the advantages of explicit, multi-bit congestion feedback, especially in a typical environment where intra-datacenter traffic with short Round Trip Times (RTT: tens of μs) run in conjunction with web-facing traffic with long RTTs (tens of milliseconds).",oceanology,195
5f0a7e99c821847a58157a1c635371f5b5824010,filtered,semantic_scholar,ICEC,2012-01-01 00:00:00,semantic_scholar,faars: a platform for location-aware trans-reality games,https://www.semanticscholar.org/paper/5f0a7e99c821847a58157a1c635371f5b5824010,"Users today can easily and intuitively record their real-world experiences through mobile devices, and commodity virtual worlds enable users from around the world to socialize in the context of realistic environments where they simulate real-world activities. This synergy of technological advances makes the design and implementation of trans-reality games, blending the boundaries of the real and virtual worlds, a compelling software-engineering problem. In this paper, we describe fAARS, a platform for developing and deploying trans-reality games that cut across the real and parallel virtual worlds, offering users a range of game-play modalities. We place fAARS in the context of recent related work, and we demonstrate its capabilities by discussing two different games developed on it, one with three different variants.",oceanology,196
46d8cfaaa23ab333cf43849b0ce9bb58ee87bcd7,filtered,semantic_scholar,,2017-01-01 00:00:00,semantic_scholar,slow wireless communication testbed based on software-defined radio,https://www.semanticscholar.org/paper/46d8cfaaa23ab333cf43849b0ce9bb58ee87bcd7,"The Internet of Things (IoT) extends the virtual cyber world into the real physical world by networking everyday smart physical objects, representing an upgraded version of Internet. The wireless sensor networks (WSN) are playing diverse sensing functions and feeding information from the physical world for IoT. Nowadays, most of the WSNs are deployed to detect slow-changing physical quantities and data from sensor nodes in these WSNs vary very slowly over a long time interval. Accordingly, a low data transmission rate is sufficient for the sensor nodes. Moreover, the low data transmission rate also enables very narrowband (VNB) radio communication with a bandwidth of several kHz to be applied in wireless sensor nodes. 
It is noteworthy that most of wireless sensor nodes are transmitting data in the unlicensed 2.4 GHz frequency band where the dominant interference is characterized by its wideband nature. Therefore, if the very narrowband (VNB) radio is adopted in a wireless sensor node, only a small portion of co-channel wideband interference will overlap with the VNB signal transmitted by the wireless sensor node. Because only a little of the wideband interference is superimposed onto the VNB signal, the VNB signal captured by a receiver has a relatively high signal-to-noise ratio (SNR), making it possible to reduce the power of the transmitter or release the noise of the receiver. 
Power consumption is a key factor that determines the lifetime of a sensor node because most of sensor nodes are powered by batteries. Once a battery is depleted, the lifetime of a sensor node will expire. The radio transceiver on a wireless sensor node consumes a lot of power when it is working but the VNB signal enables the radio transceiver to decrease its transmission power while guaranteeing arbitrarily low bit error ratio (BER). Thus, low power consumption is made possible at the VNB radio transceiver. 
In this research project, the VNB radio transceiver applied in a wireless sensor node is called “slow wireless radio”. The slow wireless radio aims at the wireless sensor nodes that are transmitting data at very-low average bit-rate of 100 bits/s in the 2.4 GHz frequency band full of wideband interference while achieving low power consumption. 
The goal of this project is to build up a point-to-point slow wireless radio communication testbed based on software-defined radio (SDR), where successful VNB wireless communication will be implemented and related communication performances such as signal-to-noise ratio (SNR) and bit error ratio (BER) will be measured in real-time. The testbed will serve as a design reference to investigate the feasibility of the slow-wireless radio communication when it is used in wireless sensor nodes and facilitate the development of a slow-wireless sensor node prototype.",oceanology,197
88aa14a159f0fad0a2b07445c3f091558ffbda62,filtered,semantic_scholar,FHPC '14,2014-01-01 00:00:00,semantic_scholar,ziria: wireless programming for hardware dummies,https://www.semanticscholar.org/paper/88aa14a159f0fad0a2b07445c3f091558ffbda62,"Software-defined radio (SDR) brings the flexibility of software to the domain of wireless protocol design, promising both an ideal platform for research and innovation and the rapid deployment of new protocols on existing hardware. Most existing SDR platforms require careful hand-tuning of low-level code to be useful in the real world. In this talk I will describe Ziria, an SDR platform that is both easily programmable and performant. Ziria introduces a programming model that builds on ideas from functional programming and that is tailored to wireless physical layer tasks. The model captures the inherent and important distinction between data and control paths in this domain. I will describe the programming model, give an overview of the execution model, compiler optimizations, and current work. We have used Ziria to produce an implementation of 802.11a/g and a partial implementation of LTE.",oceanology,198
16af2f3a6d1f07c46bd851aa2899731136fab73e,filtered,semantic_scholar,MobiCom,2014-01-01 00:00:00,semantic_scholar,poster: ziria: language for rapid prototyping of wireless phy,https://www.semanticscholar.org/paper/16af2f3a6d1f07c46bd851aa2899731136fab73e,"Software-defined radio (SDR) brings the flexibility of software to the domain of wireless protocol design, promising an ideal platform both for research and innovation and rapid deployment of new protocols on existing hardware. However, existing SDR programming platforms require either careful hand-tuning of low-level code, negating many of the advantages of software, or are too slow to be useful in the real world. We present Ziria, the first software-defined radio programming platform that is both easily programmable and performant. Ziria introduces a novel programming model tailored to wireless physical layer tasks and captures the inherent and important distinction between data and control paths in this domain. Ziria provides the capability of implementing a real-time WiFi PHY running at 20 MHz.",oceanology,199
949b01c64ba61c94ba0982ffd6abc50658d53874,filtered,semantic_scholar,SRIF@SIGCOMM,2014-01-01 00:00:00,semantic_scholar,"demo: 802.11 a/g phy implementation in ziria, domain-specific language for wireless programming",https://www.semanticscholar.org/paper/949b01c64ba61c94ba0982ffd6abc50658d53874,"Software-defined radio (SDR) brings the flexibility of software to the domain of wireless protocol design, promising an ideal platform both for research and innovation and the rapid deployment of new protocols on existing hardware. However, existing SDR programming platforms require either careful hand-tuning of low-level code, negating many of the advantages of software, or are too slow to be useful in the real world. In this demo we present Ziria, the first software-defined radio programming platform that is both easily programmable and performant. Ziria introduces a novel programming model tailored to wireless physical layer tasks and captures the inherent and important distinction between data and control paths in this domain. We show the capabilities of Ziria by demonstrating a real-time implementation of WiFi PHY running at 20 MHz.",oceanology,200
824afcd7abb4e89ddbc7d547d434cdfe47616c8b,filtered,semantic_scholar,,2013-01-01 00:00:00,semantic_scholar,modeling and simulation of radio signals attenuation using informed virtual geographic environments (ivge),https://www.semanticscholar.org/paper/824afcd7abb4e89ddbc7d547d434cdfe47616c8b,"A radio communication system is a complex dynamic phenomenon where transmitter and receiver antennas are constantly constrained by the physical environment in which they are deployed. In the real world, radio transmissions are subject to propagation effects which deeply affect the received signals because of geographic and environmental characteristics (foliage and vegetation, buildings, mountains and hills, etc.). Multi-Agent Geo-Simulation aims to simulate such phenomena involving a large number of autonomous situated actors (implemented as software agents) evolving and interacting within a representation of the physical environment. Using a geo-computation approach, we propose to use an Informed Virtual Geographic Environment (IVGE) along with MAGS paradigm. In addition, we propose a multi-agent prototype to analyze the attenuation effect due to the radio signal’s traversal between antennas (simulated as software agents) through terrain shape, vegetation area, and buildings using a 3D line-of-sight computation technique. Keywords-Line of Sight; Excess attenuation; Vegetation and Foliage; Radio Propagation; Informed Virtual Geographic Environments",oceanology,201
74fab23e3fd31db77d22074ede9de7e8c8a40c38,filtered,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,improving vehicular networking reliability and efficiency in the context of platooning applications,https://www.semanticscholar.org/paper/74fab23e3fd31db77d22074ede9de7e8c8a40c38,"Vehicular networking is a technology that enables vehicles communication system. A joint effort from the automobile industry, transportation industry, and government offices is driving the adoption of this technology to build intelligent transportation systems that consist of smart vehicles. This study attempts to improve the reliability and efficiency of vehicular networking. The study assumes the context of platooning applications, but the contributions of this study can be applied to other vehicular applications as well. There are two contributions in this study. First, a wireless emulator is designed and implemented to emulate IEEE 802.11 networks in real-time using the Ethernet infrastructure. The emulator replaces the MAC layer and physical layer of IEEE 802.11 networking stack with a real-time CSMA/CA model, thus reduces the cost of experiments. It provides upper layers the same interfaces as on a real device. As a result, the testing targets in the emulation are real-world software components as opposed to simulation scripts in a discrete event simulator. These software components can be routing protocols, transport protocols, or applications, and are the same code that can be deployed in the real world. Second, an Interframe Compression Transmission Layer is designed and implemented, to provide efficient transmission of periodical messages in vehicular environments. The transmission layer compresses the difference between frames instead of frames themselves, and reduces bandwidth consumption significantly. To improve the behaviors of the transmission layer under different scenarios and configurations studied, an adaptive version of the algorithm is designed, which achieves more than 50% in reduction of bandwidth consumption using real-world platooning data trace. With lower bandwidth consumption, delivery ratio is vastly improved in congested networking environments.",oceanology,202
6220b68cd721512098b9b14851afe5e660c0e565,filtered,semantic_scholar,,2015-01-01 00:00:00,semantic_scholar,crypto-day campeon a8,https://www.semanticscholar.org/paper/6220b68cd721512098b9b14851afe5e660c0e565,"Using the properties of a wireless channel is an alternative approach for securing the channel besides pre-shared keys or asymmetric cryptography. Numerous experiments have recently demonstrated that channel-based key establishment (CBKE) is a promising alternative to well-known symmetric/asymmetric approaches. Their run-times for establishing a symmetric key suggest that such methods are highly suitable for real-world applications that operate in a dynamic mobile environment with peer-to-peer association. CBKE is a new paradigm for generating shared secret keys. The approach is based on the estimation of the wireless transmission channel by both the sender and receiver, where the shared secret key is derived from channel parameters. The commonness of the randomness of the secret key relies on the principle of channel reciprocity. Specifically, this means that the channel from Alice to Bob is the same than the channel from Bob to Alice. This symmetry of practical channels is usually sufficiently high, as well as its entropy of spatial, temporal, and spectral characteristics. Security is given if an attacker’s distance to the two communicating nodes is high enough, so that the observed channel parameters to each node are uncorrelated and independent from each other. Typically, in real environments this is given if the distance is greater than about half of the carrier wavelength. For instance, for the frequency used in 2.4 GHz WiFi, this translates to a distance of 6.25 cm. So far, high usability and dynamic key management are very difficult to achieve for wireless devices, which operate under strict resource constraints. CBKE has the potential to significantly reduce the cost of securing small embedded devices, and hence make mass production and deployment more viable. Until now, no research has addressed the requirements for performance evaluation of real-world implementations of CBKE systems. We present a wireless CBKE security system built with standard components, e.g., quantization scheme and error correction codes, presented in recent publications. We introduce necessary implementation properties and requirements of CBKE systems. In order to validate the performance of the key generation algorithms, we define a set of metrics. Finally, we describe an end-to-end implementation on an ARM-Cortex M3 microcontroller to demonstrate the practical feasibility of channel-based key estimation using current embedded hardware. Comparative analysis of pseudorandom generators Aleksei Burlakov, Johannes vom Dorp, Joachim von zur Gathen, Sarah Hillmann, Michael Link, Daniel Loebenberger, Jan Lühr, Simon Schneider & Sven Zemanek {burlakov,dorp,luehr,schneid,zemanek}@cs.uni-bonn.de {sarah.hillmann,michael.link}@uni-bonn.de {gathen,daniel}@bit.uni-bonn.de Bonn-Aachen International Center for Information Technology Dahlmannstr. 2, Bonn We compare random generators (RGs) under controlled conditions regarding their efficiency and statistical properties. For this purpose, we distinguish between physical RGs and software RGs, which can be further subdivided into cryptographically secure and insecure RGs. Physical RGs covered by our study are the hardware generator PRG310-4 and /dev/random as implemented in the Linux kernel. Since /dev/random is fed by system events, we analyze both an idle lab environment and a server hosting several virtual machines. As examples for cryptographically secure RGs our analysis compares the RSA generator and the Blum-Blum-Shub generator, both for 3000-bit moduli. Additionally, we compare them to the Nisan-Wigderson construction with suitably selected parameters. We include two cryptographically insecure RGs, namely a linear congruential generator (LCG) and the Littlewood generator. In order to obtain repeatable and comparable results, our implementations of the software RGs were all run on the same machine and produced 512 kB of output each, using AES post-processed output of the generator PRG310-4 as source for random seed bits. We compare the results in terms of byte entropy and throughput excluding initialization. For further statistical analysis — not shown in the table — we apply the NIST test suite on the outputs. The most important finding is that in our scenarios, number-theoretic generators compete very well against hardware-based ones. byte entropy runtime throughput throughput [bit] [μs] [kB/s] normalized PRG310-4, no post-processing 7.99963 16308400 31.39486 4.34492 AES post-processing 7.99963 36524300 14.01806 1.94004 /dev/random, in the field 7.99979 9.169× 10 5.584× 10−3 7.728× 10−4 in the lab 7.99948 2.671× 10 1.917× 10−4 2.653× 10−5 Littlewood 6.47244 15206550 33.66970 4.61011 Linear congruential generator 7.99969 2644039 193.64313 26.51392 Blum-Blum-Shub 7.99962 17708350 28.91291 3.95880 RSA, e = 2 + 1, 1400 bit/round 7.99966 267604 1913.27484 261.96857 e = 3, 1 bit/round 7.99963 70103838 7.30345 1 Nisan-Wigderson 7.99961 2731227 187.46153 25.66753 Table 1: Overview of the results for generating 512 kB of output.",oceanology,203
72656d9df791499f6bb246f599e462228ff528ad,filtered,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,a channel model and coding for vehicle to vehicle communication based on a developed v-scme,https://www.semanticscholar.org/paper/72656d9df791499f6bb246f599e462228ff528ad,"Over the recent years, VANET communication has attracted a lot of attention due to its potential in facilitating the implementation of 'Intelligent Transport System'. Vehicular applications need to be completely tested before deploying them in the real world. In this context, VANET simulations would be preferred in order to evaluate and validate the proposed model, these simulations are considered inexpensive compared to the real world (hardware) tests. The development of a more realistic simulation environment for VANET is critical in ensuring high performance. Any environment required for simulating VANET, needs to be more realistic and include a precise representation of vehicle movements, as well as passing signals among different vehicles. In order to achieve efficient results that reflect the reality, a high computational power during the simulation is needed which consumes a lot of time. The existing simulation tools could not simulate the exact physical conditions of the real world, so results can be viewed as unsatisfactory when compared with real world experiments. This thesis describes two approaches to improve such vehicle to vehicle communication. The first one is based on the development of an already existing approach, the Spatial Channel Model Extended (SCME) for cellular communication which is a verified, validated and well-established communication channel model. The new developed model, is called Vehicular - Spatial Channel Model Extended (V-SCME) and can be utilised for Vehicle to Vehicle communication. V-SCME is a statistical channel model which was specifically developed and configured to satisfy the requirements of the highly dynamic network topology such as vehicle to vehicle communication. V-SCME provides a precise channel coefficients library for vehicle to vehicle communication for use by the research community, so as to reduce the overall simulation time. The second approach is to apply V-BLAST (MIMO) coding which can be implemented with vehicle to vehicle communication and improve its performance over the V-SCME. The V- SCME channel model with V-BLAST coding system was used to improve vehicle to vehicle physical layer performance, which is a novel contribution. Based on analysis and simulations, it was found that the developed channel model V-SCME is a good solution to satisfy the requirements of vehicle to vehicle communication, where it has considered a lot of parameters in order to obtain more realistic results compared with the real world tests. In addition, V-BLAST (MIMO) coding with the V-SCME has shown an improvement in the bit error rate. The obtained results were intensively compared with other types of MIMO coding.",oceanology,204
08ae139d6890717bea0e6243549d66caf24fa78e,filtered,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,teaching embedded systems in a mooc format,https://www.semanticscholar.org/paper/08ae139d6890717bea0e6243549d66caf24fa78e,"We have designed and implemented a Massive Open Online Class (MOOC) with a substantial lab component within the edX platform. We deployed this MOOC three times with a total enrollment of over 100,000 students. If MOOCs are truly going to transform engineering education, then they must be able to deliver classes with laboratory components. Our offering goes a long way in unraveling the perceived complexities in delivering a laboratory experience to thousands of students from around the globe. We believe the techniques developed in this class will significantly transform the MOOC environment. Effective education requires students to learn by doing. In the traditional academic setting this active learning is achieved through a lab component. Translating this to the online environment is a non-trivial task that required several important factors to come together. First, we have significant support from industrial partners ARM Inc. [1] and Texas Instruments [2]. Second, the massive growth of embedded microcontrollers has made the availability of lost-cost development platforms feasible. Third, we have assembled a team with the passion, patience, and experience of delivering quality lab experiences to large classes. Fourth, online tools now exist that allow students to interact and support each other. We used edX for the delivery of videos, interactive animations, text, and quizzes [3]. We used Piazza [4] for discussion boards and Zyante [5] for a programming reference. We partnered with element-14 [6], Digi-Key [7], and Mouser [8] to make the lab kit available and low-cost. Even though there was a $40-$70 cost to purchase the lab kit, the course completion numbers were slightly better than a typical MOOC. 7.3% of the students completed enough of the class to receive a certificate. Students completing end of the course surveys report a 95% overall satisfaction. Demographics show a world-wide reach with India, US, and Egypt being the countries with the most students. In this paper we will present best practices, successes and limitations of teaching a substantial lab across the globe. Background An embedded system combines mechanical, electrical, and chemical components along with a computer, hidden inside, to serve a single dedicated purpose [9-11]. There are over 50 billion processors based on the ARM architecture delivered into products, and most of these computers are single-chip microcontrollers that are the brains of an embedded system. Embedded systems are a ubiquitous component of our everyday lives. We interact with hundreds of tiny computers every day that are embedded into our houses, our cars, our bridges, our toys, and our work. As our world has become more complex, so have the capabilities of the microcontrollers embedded into our devices. Therefore the world needs a trained workforce to develop and manage products based on embedded microcontrollers. Review Other online classes have delivered laboratory experiences. Hesselink at Stanford University developed iLabs as a means to deliver science experiments to online learning. Their lab-in-a-box involves simulations and animations [12]. O’Malley et al. from the University of Manchester developed a Chemistry MOOC with a lab component using virtual labs and simulations [13-14]. University of Washington presented a hardware/software MOOC on Coursera [15]. This course is primarily a programming class without graded physical labs. Ferri et al. from Georgia Institute of Technology created a MOOC for linear circuits [16]. This class had activities to perform with NI’s myDAC, but graded lab circuits were not part of the online experience. Connor, and Huettel at Duke created a Virtual Community of Practice for electric circuits [17]. Cherner et al. created a virtual multifunctional X-Ray diffractometer for teaching science and engineering [18]. Saterbak et al. at Rice University developed online materials to teach freshman design, with the goal to free-up class time for more interactive learning experiences [19]. Harris from University of California at Irvine has a six-course sequence on Introduction to the Internet of Things and Embedded Systems where students build actual embedded devices [20]. Grading for this course uses peer assessment. Lee et al. at Berkeley developed an introduction to embedded systems MOOC with laboratory exercises. The lab itself was a robotic controller in a virtual laboratory environment. Completion of the labs themselves does have an automatic grading component based on the student’s written software [21-22]. All this work emphasizes the need for hands on learning. Pedagogy The overall educational objective of this class is to allow students to discover how computers interact with the environment. The class provides hands-on experiences of how an embedded system could be used to solve problems. The focus of this introductory course is understanding and analysis rather than design, where students learn new techniques by doing them. We feel we have solved the dilemma in learning a laboratory-based topic like embedded systems, where there is a tremendous volume of details that first must be learned before hardware and software systems can be designed. The approach taken in this course is to learn by doing in a bottom-up fashion. One of the advantages of a bottom-up approach to learning is that the student begins by mastering simple concepts. Once the student truly understands simple concepts, he or she can embark on the creative process of design, which involves putting the pieces together to create a more complex system. True creativity involves solving complex problems using effective combinations of simple components. Embedded systems afford an effective platform to teach new engineers how to program for three reasons. First, there is no operating system. Thus, in a bottom-up fashion the student can see, write, and understand all software running on a system that actually does something. Second, embedded systems involve real input/output that is easy for the student to touch, hear, and see. Many engineering students struggle with abstraction. We believe many students learn effectively by using their sense of touch, hearing and sight to first understand and internalize difficult concepts, and then they will be able to develop and appreciate abstractions. Third, embedded systems are employed in many everyday products, motivating students to see firsthand, how engineering processes can be applied in the real world. This course is intended for beginning college students with some knowledge of electricity as would have been taught in an introductory college physics class. Secondly, it is expected students will have some basic knowledge of programming and logic design. No specific language will be assumed as prior knowledge but this class could be taken as their second programming class. We hoped experienced engineers could also use this course to train or retrain in the field of embedded systems. Learning objectives of the course Although the students are engaged with a fun and rewarding lab experience, our educational pedagogy is centered on fundamental learning objectives. After the successful conclusion of this class, students should be able to understand the basic components of a computer, write C language programs that perform input/output interfacing, implement simple data structures, manipulate numbers in multiple formats, and understand how software uses global memory to store permanent information and the stack to store temporary information. Our goal is for students to learn these concepts: 0) How the computer stores and manipulates data; 1) Embedded systems using modular design and abstraction; 2) Design tools like requirements documents, data flow graphs, and call graphs; 3) C programming: considering both function and style; 4) Debugging and verification using a simulator and the real microcontroller; 5) Debugging tools like voltmeters, oscilloscopes, and logic analyzers; 6) How to input/output using switches, LEDs, DACs, ADCs, and serial ports; 7) Implementation of an I/O driver, multithreaded programming, and interrupts; 8) Analog to digital conversion (ADC), periodic sampling, and the Nyquist Theorem; 9) Stepper motors, brushed DC motors, and simple digital controllers; 10) Digital to analog conversion (DAC), used to make simple sounds; 11) Simple distributed systems that connect two microcontrollers; 12) Internet of things, connecting the embedded system to the internet; 13) System-level design that combine multiple components together. Laboratory Kit Active learning requires a platform for the student to learn by doing. Figure 1 shows the components of the basic lab kit. There are two difficulties with a physical lab kit deployed in a world-wide open classroom environment. The first problem is availability of components. We partnered with companies and distributors six months in advance of the course launch to guarantee availability. The companies wanted us to specify the number of students who would buy the kit. In this regard, we were very lucky. Six months prior to our first launch, we estimated 2000 people would register for the class and 1000 would buy the kit. In turns out Texas Instruments produced 10,000 microcontroller boards just in case. Much to our surprise 40,000 people registered and we estimate 11,000 purchased the kit during this first delivery of the course. The second solution to the problem of availability was to have three world-wide distributors (element-14, Mouser, and Digi-Key). Working with these distributors, we created one-click landing pages for students to buy the kit. Furthermore, for each component in the kit (other than the microcontroller board), we had three or more possible parts. The third solution was to design the course with flexible deadlines and pathways. Each lab had a simulation and a real-board requirement. Students who were waiting for the parts to be shipped could proceed with ",oceanology,205
87aed004b3334c17bcd87b55cff6291594e1347c,filtered,semantic_scholar,Technology and health care : official journal of the European Society for Engineering and Medicine,2009-01-01 00:00:00,semantic_scholar,development of a smart home simulator for use as a heuristic tool for management of sensor distribution.,https://www.semanticscholar.org/paper/87aed004b3334c17bcd87b55cff6291594e1347c,"Smart Homes offer potential solutions for various forms of independent living for the elderly. The assistive and protective environment afforded by smart homes offer a safe, relatively inexpensive, dependable and viable alternative to vulnerable inhabitants. Nevertheless, the success of a smart home rests upon the quality of information its decision support system receives and this in turn places great importance on the issue of correct sensor deployment. In this article we present a software tool that has been developed to address the elusive issue of sensor distribution within smart homes. Details of the tool will be presented and it will be shown how it can be used to emulate any real world environment whereby virtual sensor distributions can be rapidly implemented and assessed without the requirement for physical deployment for evaluation. As such, this approach offers the potential of tailoring sensor distributions to the specific needs of a patient in a non-evasive manner. The heuristics based tool presented here has been developed as the first part of a three stage project.",oceanology,206
8cc243723a5f33a1602efd4b743c4ae605c8e0f9,filtered,semantic_scholar,,2012-01-01 00:00:00,semantic_scholar,open archive toulouse archive ouverte (oatao),https://www.semanticscholar.org/paper/8cc243723a5f33a1602efd4b743c4ae605c8e0f9,"The Internet of Things promises an alwaysconnected future where the objects surrounding us will communicate in order to make our lives easier, more secure, etc. This evolution is a research opportunity as new solutions must be found to problems ranging from network interconnection to data mining. In the networking community, innovative solutions are being developed for the Device Layer of the Internet of Things, which includes the IoT wireless protocols. In order to study their performance, researchers turn more often to real world platforms, commonly designated by the term “testbeds”, on which they may implement and test the protocols and algorithms. This is even more important in the Industrial IoT field, where environments are perturbed by industrial systems like automated production systems. In this paper, after a brief presentation of the context of testbeds, we introduce WiNo and OpenWiNo, an open hardware and software framework for fast-prototyping in the field of the Internet of Things. Compared to existing platforms, the solution WiNo+OpenWiNo offers a wide array of Physical layers and easy integration of various sensors as it is developed as part of the Arduino ecosystem. It also allows research teams to easily and quickly deploy their own testbed into real environments. Keywords— Internet of Things; Wireless Sensor Networks; Fast prototyping; Testbed; Open Hardware; Arduino",oceanology,207
e8b695ef59db23ec1f74520c2369b4ad1aaab2b1,filtered,semantic_scholar,,2008-01-01 00:00:00,semantic_scholar,practical issues of implementing a hybrid multi-nic wireless mesh-network,https://www.semanticscholar.org/paper/e8b695ef59db23ec1f74520c2369b4ad1aaab2b1,"Testbeds are a powerful tool to study wireless mesh and sensor networks as close as possible to real world application scenarios. In contrast to simulation or analytical approaches these installations face various kinds of environment parameters. Challenges related to the shared physical medium, operating system, and used hardware components do arise. In this technical report about the work-in-progress Distributed Embedded Systems testbed of 100 routers deployed at the Freie Universitat Berlin we focus on the software architecture and give an introduction to the network protocol stack of the Linux kernel. Furthermore, we discuss our rst experiences with a pilot network setup, the encountered problems and the achieved solutions. This writing continues our rst publication and builds upon the discussed overall testbed architecture, our experiment methodology, and aspired research objectives.",oceanology,208
598a92dbe934f935de53dc24fdd0a1ba6a212daf,filtered,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,wireless network virtualization : a techno-economic analysis and a service differentiation strategy,https://www.semanticscholar.org/paper/598a92dbe934f935de53dc24fdd0a1ba6a212daf,"Virtualization of wireless networks can significantly lower the capital expenditures (CAPEX) and operational expenditures (OPEX) by enabling resource sharing among multiple parties. Virtual network operators (VNOs), i.e. mobile virtual network operators (MVNOs) and over the top service providers (SPs) are becoming prominent players in wireless network markets with their differentiated service provisioning. This changing business model is beneficial for both the network operators and the VNOs, as network operators can increase their revenues by leasing resources to the VNOs who in turn, can implement their own network without having to deploy expensive physical networks. 
 
In future 5G networks, VNOs will play even more important role by providing various differentiated services using different wireless technologies. This requires provisioning of technologyagnostic physical infrastructure on which VNOs will be able to build their customized, isolated network slices tailored for optimal performance of the intended services. Research on Wireless network virtualization is a fairly recent trend and there is lack of an end-to-end solution for wireless network virtualization architectures in the open literature. In this respect, in this thesis, three architectural frameworks for wireless network virtualization have been proposed that differ in their degree of segregation between the signal processing and the radio accessunits. The frameworks also differ in terms of their associated CAPEX & OPEX as well as in terms of their achievable quality of service (QoS). For this reason, selection of a particular virtualization model for a particular service deployment is a multi-dimensional problem. Hence, a multi-criteria utility model has been developed that accounts for network cost & QoS trade-offs in order to enable the design and optimization of wireless access virtualization architectures that best comply with the investment and service-level requirements of network operators (and/or service providers). 
 
The second phase of the thesis focuses on the architectural requirements for provisioning heterogeneous virtual networks on a common physical substrate. It has been argued that software defined network (SDN) and cloud computing technologies are the key enablers for deploying such a network model. The existing proposals in the open literature focus on wireless network solutions for a particular radio access technology (RAT), e.g., WiFi, cellular, wireless sensor network (WSN), etc. or a particular part of a network (e.g., cellular core vs access networks). But an integral solution for programmable, elastic, virtualized heterogeneous networks is not available in the open literature. Hence, a blueprint for the deployment of an end-to-end programmable & flexible heterogeneous virtual wireless network (HVWN) infrastructure using SDN & cloud computing has been laid out in this chapter. Open problems and challenges in realizing a programmable, elastic HVWN have also been identified. 
 
Next, in the third phase of the thesis, the case of provisioning differentiated services in a cloud-based software-defined virtual wireless network environment has been studied. We have focused on a particular part of the generalized architecture proposed in the second part of the thesis, i.e., the case of programmable virtualized wireless networks that consists of cellular and fixed WiFi networks. More specifically we have studied how differentiated services can be provided in such a programmable virtualized platform. We have proposed to use the spare bits of OpenFlow packet structure to implement virtual network entities. Use of northbound APIs has been emphasized for composing complex wireless network applications. Emulation results show that the SDN-based virtualized wireless networks are able to meet the critical performance requirements of carrier networks. 
 
In the final part of the thesis, we focused on full duplex (FD) deployment of multi-cell networks. Current cellular networks are suffering from spectrum ossification problem. In a virtualized environment where multiple VNOs will compete for access to shared radio resources, the spectrum scarcity problem will be more severe. In such context, FD systems can provide an efficient solution by doubling the spectrum efficiency. In our research, we have identified the critical challenges for real world deployment of multi-tier FD multi-cell networks. We have analyzed FD performance trade-offs for a dense urban multi-tier cellular network. We have used the Madrid grid model proposed by METIS project that consists of macro and pico cells. We also have investigated the impact of co-located BS interference in FD performance for a single-tier homogeneous network deployment. We have proposed intelligent proportional fair joint user selection and power control algorithms to harness the gain of FD deployment. We have developed algorithms for both cloud radio access network (C-RAN) and traditional distributed RAN (D-RAN) network models. Extensive system-level simulation results show that using the devised algorithms the FD systems are able to achieve significant performance gain .",oceanology,209
d0d2d442baa8320b3a87af64b1a5a6e98497ae3c,filtered,semantic_scholar,,2011-01-01 00:00:00,semantic_scholar,analysis of radio communication attenuation using geoprocessing techniques,https://www.semanticscholar.org/paper/d0d2d442baa8320b3a87af64b1a5a6e98497ae3c,"Multi-Agent Geo-Simulation (MAGS) aims to simulate phenomena involving a large number of autonomous situated actors (implemented as software agents) evolving and interacting within a Virtual representation of the Geographic Environment (VGE). A radio communication system is a typical complex dynamic phenomena where transmitter and receiver antennas are constantly constrained by the physical environment in which it they are deployed. In the real world, radio transmissions are subject to propagation effects which deeply affect the received signals because of geographic and environmental characteristics (foliage and vegetation, buildings, mountains and hills, etc.). Using geoprocessing techniques, we propose an automated approach to build semantically-informed and geometrically-accurate virtual geographic environments which uses Geographic Information System (GIS) data and builds an informed graph-based structure called Informed Virtual Geographic Environment (IVGE). In addition, we propose a multi-agent prototype to analyze the attenuation effect due to the radio signal’s traversal between antennas (simulated as software agents) through terrain shape, vegetation area, and buildings using a 3D line-of-sight computation technique. Keywords-Informed Virtual Geographic Environment (IVGE); Radio Signal Propagation; Line-Of-Sight; Multi-Agent GeoSimulation (MAGS).",oceanology,210
3f826f1f349ee707639c39d231259498b14215c5,filtered,semantic_scholar,,2011-01-01 00:00:00,semantic_scholar,ac 2011-2689: smart grid development in electrical dis- tribution network,https://www.semanticscholar.org/paper/3f826f1f349ee707639c39d231259498b14215c5,"This paper will focus on smart grid project design and implementation. The project was developed by students and demonstrates new ideas and teamwork. This project was successfully completed and has been developed, implemented and assessed. Topics covered are: how to build a smart gird by utilizing computer application software tools, design, simulation, and diagnoses of electrical distribution systems. All the real world components in electrical distribution network such as residential, commercial and industrial building are modeled in this project. Background The purpose of this project is to design and implement a small scale electric power network by a team of seven students, supervised by a faculty member. The students’ background is in electrical engineering with emphasis in electric power system. The students conducted a study in the field of Smart Grid technologies for history and background information. This work led to designing and implementing a small model of a smart gird power distribution network. The power grid represents the real world aspirations of both government and private industry geared toward building a more reliable, responsive, and overall efficient network of residential, industrial and commercial buildings. Since the concept of a smart grid is very vague, students chose to implement a time tested and proven aspect of such technology known as smart meters. The smart meter is a wireless device connected to every house, industrial and commercial buildings to provide essential feedback in real time to the power companies. This feedback could be in the form of a fault occurring at that said location, or illegal energy usage. This feedback in real time is very useful to the power companies, given the fact that most rely on feedback via a phone call from the consumer before they know whether or not there is a fault in the system. Another purpose of implementing the grid was to simulate metering technology at the residential, industrial and commercial level. These meters would send data to a computer which is a simulated control room in order to read where certain faults occur in the system. In turn one could control which areas of the grid would be supplying the power. This represents a simulation of the power company’s ability to read and send vital information throughout the grid, thus improving the responsiveness and reliability of the network. Figure 1 illustrates the completed model after it was built and during testing. The lifecycle of this project was implemented in three different phases and started in September of 2009 and it was completed in May of 2010. Planning and analysis was completed in phase I, design and implementation in phase II, and documentation and students’ assessment in phase III. Figure 1. A model of smart grid in electrical distribution system Phase I: Planning and Analysis Initially, each team member worked on individual research on the concepts of smart grid its purpose. Later on, a decision was made as to what the team wanted to demonstrate with the project. The decision was made to show specifically how smart meters would work and help in fault detection as well as saving money by removing the need for meter readers to read the power meters every month. A project leader was elected by the team members to coordinate the team work. Meetings were then set up by the project leader, to brainstorm on how the actual implementation was going to be planned. Microsoft Project software was very instrumental to organize the work of the team. Tasks were assigned with specific due dates to keep the project on schedule and under budget. Requirements Since this project was spread over three quarters, students had many deadlines and task that had to be met in order to have a successful project. There were three phases to this project, research, background study and planning during the 1 quarter, design and building during the 2 quarter and the final stage of testing and troubleshooting during the 3 quarter along with final oral presentation, simulation documentation and assessment of the project. The students made documents and recorded each steps of the project down to each task and timeline by using Microsoft Project software. The project advisor coordinated the project steps and students were required to present a weekly progress report. This step insured that the project was moving smooth and on the track. The group was divided into two teams, one in software teams which consisted of two members and a hardware team which consisted of other five group members. The software team was in charge of all the coding and GUI implementation so the actual grid can communicate back and forth with the computer. The hardware team was in charge of the physical grid which consisted of the circuit that was built using logic chips such as MUXs, and Flip-Flops, wiring, creating a map on the grid with houses, roads, school, power stations, sub-stations, transmission lines, and distributions lines. The commercial site consists of shopping area, factories, stadium, school and so forth. In final stages of the project, testing, debugging and troubleshooting was performed in order to assure that hardware components and related software can communicate back and forth in a proper sequence. Much of the requirements had the made along the way since this was very new to all students. Phase II: Design and Implementation The design started immediately after the clear definition of the project requirement and purpose. To lower the cost and improve the safety, the design would be a DC (Direct Current) representation of an AC (Alternate Current) system. The system was designed by drawing out the model of a city and the specific buildings to exist in that city. The design was based on what took place in the planning stage which defined how the city and buildings will receive their power and the power. Figure 2. The process of building a smart grid The next challenge in the design process was solving the problem of switches and smart meters. Figure 2 shows the design of the smart meters and placement of LEDs (Light Emitting Diode). The LEDs will represent whether a particular house, building or transformer has power on or off. If for any reason an LED was not lit, then that particular item does not have power. The faults were determined by voltages because even if the building wasn’t drawing power, then there still would be a voltage on the line. This voltage was then sent to a 64 to 1 multiplexor which was then sent to the microcontroller to determine faults. To turn the power of buildings “on” and “off” a common NPN transistor (2n2222) and the base current was provided by a flip flop integrated circuit. Flip flops were used due to I/O’s limitations of the PIC. Figure 3 was duplicated for every transformer, with the only difference being the number of buildings being fed from the transformer which is the first LED after the 12V source. Figure 3. Circuit diagram for buildings Implementation A Smart Grid system includes a power meter which enables the communication systems to update the utility about its condition and the electronics to control the meter. The old electromechanical meters that were used are becoming obsolete since they cannot support the features that the utilities desire to have such as monitoring and controlling power supplied to its customers. Utilities wishes to monitor power consumption so that they can accurately predict how much power will be used during peak and down times. This information is helpful in producing sufficient energy and better efficiency in power waste. It can also help to pinpoint locations of power outages leading to a quicker recovery time. Challenges in implementation of the system are based on a couple of issues. First is the cost. It could cost upwards of $1000 for each smart meter, depending on features to be installed for each house or business. The costs can add up quickly, and the utilities don't see any immediate savings or incentive to deploy the smart grid in a very near future. The system designed in this project is using smart meters with a simulated wireless connection to the central servers at the utilities. The meters would send a signal to the central computer to update its status, power consumption, and other things. It can be designed in a way that it will have a battery backup for when the power is interrupted, or have the central computer assume it is off when it doesn't send a signal at the regular time intervals. Obviously the latter option would be the most cost effective and would use less power to run. But having power to the smart meter could also be beneficial because diagnostics could be run to determine if the power went out or if the meter is having its own internal hardware problems. A wireless signal was simulated for this project, but in real world application one can use either wireless, normal phone lines, or communications over power line. Most utilities already have communications systems set up through their power lines and using this method would be most cost effective. Having wireless, on the other hand, frees up usage of the power lines reducing their stress and prolonging the cables life. Companies are developing and testing their own systems using one of those options. In any case, it is based on hardware availability and cost effectiveness. Figure 4 illustrates communication with the smart grid. Figure 4. Communication with the smart grid Phase III: Documentation and Students’ Assessment In phase III of the project, the students provided a detail documentation of the project which includes cost analysis and different phases of the design. An electronic copy of this documentation and demo presentation was produced in a DVD. The following assessment and lessons learned was observed during the life cycle of the project: 1) When the main board that was used in the final project was constructed, the problems of wiring of all o",oceanology,211
3230c340ca4eeb94c7648fd516ed26c70b12a249,filtered,semantic_scholar,,2011-01-01 00:00:00,semantic_scholar,loading architecture for a sensor web browser on digital earth_final_0,https://www.semanticscholar.org/paper/3230c340ca4eeb94c7648fd516ed26c70b12a249,"The world-wide sensor web observes real world phenomena at a particular moment in time with a large number of geo-referenced sensors. Sensor web needs a sensor web browser for accessing distributed and heterogeneous sensor networks in a coherent frontend. The Digital Earth provides a geo-referenced three-dimensional environment for intuitively browsing and displaying sensor observations. However, the major challenge is to load the vast amount of sensor observations from servers to a sensor web browser while minimizing the delay that a user experiences. This research uses two techniques to address the challenge. First, the browser caches transmitted data onto the local hard drive to reduce redundant internet bandwidth consumption. Second, this work designs a loading architecture to decouple sensor data loading, rendering, and browsing. The proposed scheme is implemented in the GeoCENS sensor web browser. To the best of our knowledge, with the proposed loading architecture, GeoCENS is the first Digital Earth-based sensor web browser. Background and Relevance The Digital Earth was envisioned in Gore’s 1998 speech1. The Digital Earth is a threedimensional visualization model of the physical Earth, which contains high resolution imagery and digital elevation models. Users are able to intuitively interact with Digital Earth by navigation features such as flying to places or floating above the surface. In recent years more and more publicly-available Digital Earths are revealed, such as ESRI’s ArcGIS Explorer2, NASA’s World Wind3, Microsoft’s Bing Map4, and Google Earth5. One of the visions that Gore described is that people can access vast amount of scientific information through the Digital Earth to help them understand real world. Therefore, at the same time that Digital Earths were being developed, the deployments of the world-wide sensor web were also put into practice for observing heterogeneous, environmental phenomena. The sensor web concept originated at the NASA/Jet Propulsion Laboratory in 1997 (Delin et al. 2005; Liang et al. 2005) for acquiring environmental information by integrating massive spatially distributed consumermarket sensors. The world-wide sensor web has been applied in a range of applications, including: large-scale monitoring of the environment (Hart and Martinez 2006), civil structures (Xu et al. 2004), roadways (Hsieh 2004), and animal habitats (Mainwaring et al. 2002). Ranging from video camera networks that monitor real-time traffic, matchbox-sized wireless sensor networks embedded in the environment for monitoring habitats, sensor webs generate tremendous volumes of valuable observations, enabling scientists to observe previously unobservable phenomena. Similar to how the World 1 http://www.isde5.org/al_gore_speech.htm 2 http://www.esri.com/software/arcgis/explorer/index.html 3 http://worldwind.arc.nasa.gov/java/ 4 http://www.bing.com/maps/ 5 http://www.google.com/earth/index.html Wide Web needs an Internet browser for viewing web pages, the sensor web needs a coherent frontend for accessing the distributed and heterogeneous sensor networks. This kind of coherent frontend is called sensor web browser. In order to achieve Gore’s Digital Earth and sensor web visions, an online Digital Earthbased sensor web browser for users to browse, search, manage, and exchange sensor data is needed. However, transmitting the vast amount of sensor readings from servers to a sensor web browser with minimum delay is very challenging. Therefore, the goal of this paper is to present a sensor web data loading architecture for addressing the following two issues: (1) transmitting vast amount of sensor data and (2) minimizing the delay time that user might experience. To address the first issue, efficient data loading management utilizing a local cache is applied. To address the second issue, a loading architecture that decouples loading and browsing, a speculation mechanism, and a dynamic priority queue (Xhafa & Tonguz 2001) are applied. With the proposed scheme, efficient sensor web data loading and good user experience are attained. The sensor data used in this work are historical and can be represented as points on the Digital Earth. Methodology Issue 1: Transmitting Vast Amount of Sensor Data To mitigate this issue, we can adopt strategies from Web browsers and how they are able to minimize redundant transmissions. Web browsers cache data, which means they store web page requests and associated responses as key-value pairs on the local disk in order to reduce unnecessary and redundant transmissions, and to improve end-to-end latency. Similarly, this work implements a caching strategy to reduce the unnecessary transmission of sensor data in the sensor web. However, sensor web browsers’ requests and sensor observations are different from web browsers’ requests and web pages. We cannot simply manage sensor web requests and responses as key-value pairs in web caches. A sensor web request can be interpreted as asking for sensor observations of a certain phenomenon in a set of spatio-temporal cubes. The spatio-temporal cubes can be distributed irregularly in space and time. An effective sensor web caching strategy requires efficient management of these spatio-temporal cube requests. Therefore, we develop a new spatio-temporal indexing structure, LOST-Tree (LOading SpatioTemporal indexing tree) to manage sensor data loading with a local cache. A LOST-Tree manages sensor data loading of one phenomenon. A LOST-Tree consists of two techniques. First, instead of indexing massive amounts of raw sensor data, a LOST-Tree will index requests (i.e., spatio-temporal cubes). Because no actual data stored in LOST-Tree, it is small and able to fit into memory for efficient processing. Second, a LOST-Tree applies any two regular and aggregatable structures onto the spatial and temporal domains for transforming irregular spatio-temporal cubes into regular cubes. For instance, this work implements with a Quadtree (Finkel & Bentley 1974) and the Gregorian calendar. These two structures are integrated by using temporal structure (i.e., the Gregorian calendar in this work) as main structure and embedding spatial structure (i.e., Quadtree in this work) in nodes of temporal structure. In this way, a record in LOST-Tree represents a spatio-temporal cube, which allows simple look-up searches (rather than range query) for determining unloaded cubes. Records are inserted into LOST-Tree only if the corresponding cubes have been loaded. Since both structures are aggregatable, the more cubes are loaded, the fewer records remain. Therefore, a LOST-Tree is scalable, light weight, and able to efficiently identify unloaded potions in sensor web browser. By incorporating LOST-Trees into a sensor web browser, previously loaded sensor data can be retrieved from the local cache. The end-to-end latency can be reduced and the Internet bandwidth can be efficiently utilized on transmitting the data that has not been loaded. Issue 2: Minimizing the delay time that users experience In order to minimize the delay users might experience when loading sensor web data, we need to decouple data loading, rendering, and browsing. We implemented the following performance improvement strategies in the GeoCENS sensor web browser (Liang et al. 2010). (1) Checker: In order to reduce the requesting frequency, instead of executing loading processes whenever the Digital Earth is moving, loading processes start when the earth has stopped moving for more than a defined period of time (e.g., 500 milliseconds). This follows the assumption that a user’s area of interest corresponds to where the user stops moving the Digital Earth. (2) Wrapper: After a user requests a spatio-temporal cube, a thread is trigged to determine the portions that are not available in the local cache (by using a LOST-Tree). The wrapper also filters out the requests that have been issued, and composes new loading tasks. (3) Loader: After the loading tasks are created, these tasks are stored into a queue waiting for being sent to the servers. A thread-pool with pre-defined number of threads polls these loading tasks from the queue and issues requests based on the loading tasks to the servers. After the data are transmitted to the browser, a thread is trigged to parse the data and store them in local cache. (4) Poller: The checker, wrapper, and loader are for the data loading, and the poller is for rendering. A timer runs periodically to check if user moves the Digital Earth. If not, a thread will then be trigged to retrieve data from the local cache and render the data on Digital Earth. Besides applying the decoupling architecture, two additional mechanisms are implemented for improving a user’s experience. They are speculation and dynamic priority queue. Firstly, speculation means the system issues requests before a user issues the requests. For instance, we expect users will browse the nearby regions around their initial area of interest. Therefore, when a user requests a spatio-temporal cube, the spatial component (e.g., bounding box) of the cube is then expanded. Secondly, users are allowed to navigate freely within the Digital Earth environment. A user may decide to move to other places before the previous loading tasks are digested. As a result, it is important to prioritize the loading tasks dynamically. For example, if we manage the loading tasks with a first-in-first-out (FIFO) strategy, new requests will not be executed until the old requests are finished. In this case, users may feel slower system performance because the system is background loading data they aren’t expecting. Therefore, instead of following a FIFO queue, we apply a dynamic priority queue (Xhafa & Tonguz 2001) to prioritize the loading tasks. Whenever a user moves the Digital Earth, the priorities of loading tasks in the queue will be re-assigned with the distance between the current area of interest and the tasks’ loadi",oceanology,212
a2ca591957d1081bbf4b1a04c565b8f365c014d8,filtered,semantic_scholar,,2017-01-01 00:00:00,semantic_scholar,literature survey: a design approach to smart system based on internet of thing (iot) for intelligent transportation,https://www.semanticscholar.org/paper/a2ca591957d1081bbf4b1a04c565b8f365c014d8,"Recent years, the transportation efficiency and related issues have become one of the main focuses of the global world. Along this line, intelligent transportation systems (ITS) based on Internet of Things (IoT) provided a promising chance to resolve the challenges caused by the increasing transportation problems, such as traffic prediction, road status evaluation, traffic accident detection, etc. In this, The Internet of Things is based on the Internet, network wireless sensing and detection technologies to realize the intelligent recognition on the tagged traffic object, tracking, monitoring, managing and processed automatically. IoT based intelligent transportation systems are designed to support the Smart City vision, which aims at employing the advanced and powerful communication technologies for the administration of the city and the citizens. Keywords—IoT, transportation. I. LITERATURE SURVEY K.Ashokkumar, Baron Sam, R.Arshadprabhu, Britto [1] proposes the advances in cloud computing and web of things (IoT) have provided a promising chance to resolve the challenges caused by the increasing transportation problems. They tend to gift a unique multilayered conveyance knowledge cloud platform by exploitation cloud computing and IoT technologies to resolve the challenges caused by the increasing transportation issues. They present a novel multilayered vehicular data cloud platform by using cloud computing and IoT technologies. Two innovative vehicular data cloud services, an intelligent parking cloud service and a vehicular data mining cloud service in the IoT environment are also presented reviews. Amir-Mohammad Rahmani, Nanda Kumar Thanigaivelan, Tuan Nguyen Gia, Jose Granados, Behailu Negash, Pasi Liljeberg, and Hannu Tenhunen [2] proposes the strategic position of gateways to offer several higherlevel services such as local storage, real-time local data processing, embedded data mining, etc., proposing thus a Smart e-Health Gateway. By taking responsibility for handling some burdens of the sensor network and a remote healthcare center, a Smart e-Health Gateway can cope with many challenges in ubiquitous healthcare systems such as energy efficiency, scalability, and reliability issues. Michele Nitti, Luigi Atzori, and Irena Pletikosa Cvijikj [3] addressed the issue by analyzing possible strategies for the benefit of overall network navigability.They first propose five heuristics, which are based on local network properties and that are expected to have an impact on the overall network structure. Thet then perform extensive experiments, which are intended to analyze the performance in terms of giant components, average degree of connections, local clustering, and average path length. Jianli Pan, Raj Jain, Subharthi Paul, Tam Vu, Abusayeed Saifullah, Mo Sha [4] proposes an IoT framework with smart location-based automated and networked energy control, which uses smartphone platform and cloud-computing technologies to enable multiscale energy proportionality including building-, user-, and organizational-level energy proportionality. They further build a proof-of-concept IoT network and control system prototype and carried out real-world experiments, which demonstrate the effectiveness of the proposed solution. They envision that the broad application of the proposed solution has not only led to significant economic benefits in term of energy saving, improving home/office network intelligence, but also bought in a huge social implication in terms of global sustainability Catarinucci, L. , de Donno, D. , Mainetti, L. , Palano, L. [5] proposes a novel, IoT-aware, smart architecture for automatic monitoring and tracking of patients, personnel, and biomedical devices within hospitals and nursing institutes. Staying true to the IoT vision, they propose a smart hospital system (SHS), which relies on different, yet complementary, technologies, specifically RFID, WSN, and smart mobile, interoperating with each other through a Constrained Application Protocol (CoAP)/IPv6 over lowpower wireless personal area network (6LoWPAN)/representational state transfer (REST) network International Conference on Science and Engineering for Sustainable Development (ICSESD-2017) (www.jit.org.in) International Journal of Advanced Engineering, Management and Science (IJAEMS) Special Issue-1 https://dx.doi.org/10.24001/icsesd2017.49 ISSN : 2454-1311 www.ijaems.com Page | 195 infrastructure. The SHS is able to collect, in real time, both environmental conditions and patients' physiological parameters via an ultra-low-power hybrid sensing network (HSN) composed of 6LoWPAN nodes integrating UHF RFID functionalities. Sensed data are delivered to a control center where an advanced monitoring application (MA) makes them easily accessible by both local and remote users via a REST web service. Al-Fuqaha, A., Kalamazoo, MI, Guizani, M. , Mohammadi, M., Aledhari, M. [6] provides a more thorough summary of the most relevant protocols and application issues to enable researchers and application developers to get up to speed quickly on how the different protocols fit together to deliver desired functionalities without having to go through RFCs and the standards specifications. They also provides an overview of some of the key IoT challenges presented in the recent literature and provide a summary of related research work. Moreover, they explore the relation between the IoT and other emerging technologies including big data analytics and cloud and fog computing. They also presents the need for better horizontal integration among IoT services. Stecca, M., Moiso, C., Fornasa, M., Baglietto, P. [7] presents app execution platform (AEP), a platform that supports the design, deployment, execution, and management of IoT applications in the domain of smart home, smart car, and smart city. AEP was designed to coherently fulfill a set of requirements covered only partially or in a fragmented way by other IoT application platforms. AEP focuses on SO virtualization and on composite application (CA) orchestration and supports dynamic object availability. Yi-Bing Lin, Yun-Wei Lin, Chang-Yen Chih, Tzu-Yi Li [8] proposes an IoT device which is characterized by its “features” (e.g., temperature, vibration, and display) that are manipulated by the network applications. If a network application handles the individual device features independently, then we can write a software module for each device feature, and the network application can be simply constructed by including these brick-like device feature modules. Based on the concept of device feature, brick-like software modules can provide simple and efficient mechanism to develop IoT device applications and interactions. Ganz, F. , Puschmann, D. , Barnaghi, P. , Carrez, F. [9] provides a survey of the requirements and solutions and describes challenges in the area of information abstraction and presents an efficient workflow to extract meaningful information from raw sensor data based on the current stateof-the-art in this area and also identifies research directions at the edge of information abstraction for sensor data. To ease the understanding of the abstraction workflow process, they introduce a software toolkit that implements the introduced techniques and motivates to apply them on various data sets. Aijaz, A. , Aghvami, A.H.[10] provides the state of the art in cognitive M2M communications from a protocol stack perspective, covers the emerging standardization efforts and the latest developments on protocols for cognitive M2M networks which includes a centralized cognitive medium access control (MAC) protocol, a distributed cognitive MAC protocol, and a specially designed routing protocol for cognitive M2M networks. These protocols explicitly account for the peculiarities of cognitive radio environments. Performance evaluation demonstrates that the proposed protocols not only ensure protection to the primary users (PUs) but also fulfil the utility requirements of the secondary M2M networks. Tsirmpas, C., Anastasiou, A., Bountris, P., Koutsouris, D. [11] proposes a new methodology based on self organizing maps (SOMs) and fuzzy C-means (FCM) algorithms for profile generation as regards the activities of the user and their correlation with the available sensors. Moreover, we utilize the provided context to assign the generated profiles to more contextually complex activities. This methodology is being evaluated into an AAL structure equipped with several sensors. More precisely, they assess the proposed method in a data set generated by accelerometers and its performance over a number of everyday activities Mainetti, L., Lecce, Mighali, V. ; Patrono, L. [12] proposes a software architecture to easily mash-up constrained application protocol (CoAP) resources. It is able to discover the available devices and to virtualize them outside the physical network. These virtualizations are then exposed to the upper layers by a REpresentational State Transfer (REST) interface, so that the physical devices interact only with their own virtualization. Furthermore, the system provides simplified tools allowing the development of mash-up applications to different-skilled users. Finally, the architecture allows not only to monitor but also to control the devices, thus establishing a bidirectional communication channel. Hasan Omar Al-Sakran [13] presents a novel intelligent traffic administration system, based on Internet of Things, which is featured by low cost, high scalability, high compatibility, easy to upgrade, to replace traditional traffic management system and the proposed system can improve road traffic tremendously. The Internet of Things is based on the Internet, network wireless sensing and detection technologies to realize the intelligent recognition on the tagged traffic object, tracking, monitoring, managing and processed automatically. The paper proposes an architecture that integrates internet of things with",oceanology,213
41d472aaa26abdac72c008ffa206170e88d446e9,filtered,semantic_scholar,,2009-01-01 00:00:00,semantic_scholar,replicating augmented reality objects for multi-user interaction,https://www.semanticscholar.org/paper/41d472aaa26abdac72c008ffa206170e88d446e9,"Augmented Reality (AR) is the combination of virtual objects and the physical world surrounding us. These virtual objects are used to enrich the real world. Because of technical improvements of mobile hardware, there are quite a number of AR applications deployed in the last decade. 
 
To illustrate the potential of the AR technique and to look for a new concept of human-computer interaction, we started the ’Augmented Reality for 3D Multi-user Interaction’ (ARMI) project. The goal of the ARMI project is to build an AR application where, two or more users, can work concurrently on the same virtual maquette. This maquette is visible through a Head Mounted Display (HMD) to display visual objects on top of the real world. The virtual maquette can be used for representing specific traffic situations with roads and cars but also for modelling other 3D scenarios. The following basic actions are supported: creating, selecting, moving, rotating, and deleting virtual objects. 
 
The virtual maquette, build by the ARMI project, depends on four distinct areas and we identify the following research areas: hand tracking, hand-pose estimation, 3D interfacing, and AR-object replication. This thesis looks into the possibilities of object replication for the AR application. First, a number of AR applications are considered and it becomes clear that most AR applications with multiple users are depending on a client-server approach and no object replication is used. Second, a number of replication systems are described in which more than one server is involved to keep the data consistent and a number of fundamental replication algorithms are discussed. Based on the related work the decision is made to use a speculative variant of an asynchronous majority consensus algorithm for the AR-object replication. 
 
Furthermore, in this thesis the development, implementation and evaluation of the AR-object replication is described. From the evaluation it becomes clear that it is difficult to satisfy all the required replication parameters. We notice a number of replication limits. For example a scaling problem, which means that the number of clients is limited, and specific user behaviour in terms of performed operations per second. Based on the evaluation we conclude that the relaxation of a number of replication parameters is necessary to keep the system responsive enough for an AR application.",oceanology,214
a2524b60d8c51af32ee36f9d3ccb0761c8b593f6,filtered,semantic_scholar,,2003-01-01 00:00:00,semantic_scholar,indexing and retrieving semantic web resources: the rdfstore model,https://www.semanticscholar.org/paper/a2524b60d8c51af32ee36f9d3ccb0761c8b593f6,"The Semantic Web is a logical evolution of the existing Web. It is based on a common conceptual data model of great generality that allows both humans and machines to work with interrelated, but disjoint, information as if it was a single global database. The design and implementation of a general, scalable, federated and flexible data storage and indexing model, which corresponds to the data model of the Semantic Web, is fundamental for the success and deployment of such a system. The generality of the RDF data model presents unique challenges to efficient storage, indexing and querying engines. This paper presents our experience and work related to RDFStore which implements a new flexible indexing and query model. The model is tailored to RDF data and is designed around the Semantic Web from the ground up. The paper describes the underlying indexing algorithm, together with comparisons to other existing RDF storage and query strategies. Towards a lightweight database architecture The generality of the RDF data model presents unique challenges to efficient storage, indexing and querying software. Even if the Entity-Relational (ER) data model [1] is the dominant technology for database management systems today, it has limitations in modeling RDF constructs. RDF being unbounded, the resulting data structures are irregular, expressed using different data granularity, deeply nested or even cyclic. As a consequence, it is not possible to easily fix the ""structural view"" of a piece of information (object), which is instead one of the fundaments of traditional RDBMS systems trying to be much narrower and precise as possible and where an update not conforming to a single static schema is rejected. Database systems also optimize data storage and retrieval by knowing ahead of time how records are structured and interrelated and tend to use very inefficient nested SQL SELECT statements to process nested and cyclic structures. All this is too restrictive for RDF data. Like most semi-structured formalisms [2][3] RDF is self-describing. This means that the schema information is embedded with the data, and no a priori structure can be assumed, giving a lot of flexibility to manage any data and deal with changes in the data's structure seamlessly at the application level. The only basic structure available is the RDF graph itself, which allows describing RDF vocabularies as groups of related resources and the relationships between these resources [4]. All new data can be ""safely"" accepted, eventually at the cost of tailoring the queries to the data. On the other side, RDF data management systems must be much more generic and polymorphic like most of dynamically-bound object-oriented systems [5]; changes to the schema are expected to be as frequent as changes to the data itself and could happen while the data is being processed or ingested. A drawback of RDF heterogeneity is that the schema is relatively large compared to the data itself [6]; this in contrast to traditional RDBMS where the data schema is generally several orders of magnitude smaller than the data. This also implies that RDF queries over the schema information are as important as queries on the data. Another problem is that most RDF data (e.g. metadata embedded into an HTML page or RSS1.0 news feed) might exist independently of the vocabulary schemas used to mark-up the data, further complicating data structure ""validation"" (RDF Schema validation). This de-coupling aspect also makes the data ""de-normalization"" more difficult [7][8][9]. ""De-normalization"" is needed in RDBMS to overcome query performance penalties caused by the very general ""normalized"" schemas. De-normalization must be done taking into account to how the database will be used and how data is initially structured. In RDF this is not generally possible, unless all the RDF Schema definitions of the classes and properties used are known a-priori and available to the software application. Even if that might be the case, it is not a general rule and it would be too restrictive and make RDF applications extremely fragile. In the simplest and most general case, RDF software must associate the semantics to a given property exclusively using the unique string representation of its URIs. This will not stop of course more advanced and intelligent software to go a step further and retrieve, if available, the schema of the associated namespace declarations for validation, optimization or inference purposes. It is interesting to point out that a large part of queries foreseen for Web applications are information discovery and retrieval queries (e.g. Google) that can ""ignore"" the data schema taxonomy. Simple browsing through the RDF data itself or searching for some sub-string into literals, or using common patterns is generally enough for a large family of RDF applications. On the other hand, we strongly believe that RDBMS has proven to be a very effective and efficient technology to manage large quantities of well-structured data. This will continue to be true for the foreseeable future. We thus see RDF and similar less rigid, or semi-structured data technologies as complementary to traditional RDBMS systems. We expect to see RDF increasingly appear in the middle layer where lightweight systems that focus on interoperability, flexibility and a certain degree of decoupling of rigid formats are desired. We believe that a fundamentally different storage and query architecture is required to support the efficiently and the flexibility of RDF and its query languages. At a minimum such storage system needs to be: Lightweight Native implementation of the graph Fundamentally independent from data structure Allow for very wide ranges in value sizes; where the size distribution is not known in advance, most certainly is not Gaussian and will fluctuate wildly. Be efficient it should not be necessary to retrieve very large volumes of data in order to reconstruct part of the graph. Allow built support for arbitrary complex regular-path-expressions on the graph to match RDF queries like RDQL [50] statement triple-patterns. Have some free-text support Context/provenance/scope or flavoring of triples Furthermore given that RDF and the Semantic Web are relatively new, and will require significant integration and experimentation it is important that its technology matches that of the Internet: Easy to interface to C, Perl and Java at the very least. Ruby, Python, Visual Basic and .NET are a pre. Easy to distribute (part of) the solution across physical machines or locations in order match scaling and operational habits of existing key Internet infrastructure. Very resistant to ""missing links"" and other noise. Contexts and provenance A RDF statement represents a fact that is asserted as true in a certain context space time, situation, scope, etc. The circumstances where the statement has been stated represent its ""contextual"" information [10][11]. For example, it may be useful to track the origin of triples added to the graph, e.g. the URI of the source where triples are defined, e.g. in an RDF/XML file, when and by whom they where added and the expiration date (if any) for the triples. Such context and provenance information can be thought of as an additional and orthogonal dimension to the other components of a triple. The concept is called in the literature ""statement reification"". Context and provenance are currently not included in the RDF standardisation process [48][49], but will hopefully adressed in a next release of the specification. From the application developer point of view there is a clear need for such primitive constructs to layer different levels of semantics on top of RDF which can not be represented in the RDF triples space. Applications normally need to build meta-levels of abstraction over triples to reduce complexity and provide an incremental and scaleable access to information. For example, if a Web robot is processing and syndicating news coming from various on-line newspapers, there will be overlap. An application may decide to filter the news based not only on a timeline or some other property, but perhaps select sources providing only certain information with unique characteristics. This requires the flagging of triples as belonging to different contexts and then describing in the RDF itself the relationships between the contexts. At query time such information can then be used by the application to define a search scope to filter the results. Another common example of the usage of provenance and contextual information is about digital signing RDF triples to provide a basic level of trust over the Semantic. In that case triples could be flagged for example with a PGP key to uniquely identify the source and its properties. There have been several attempts [12][13][14][15] trying to formalize and use contexts and provenance information in RDF but a common agreement has not been reached yet. However, context and provenance information come out as soon as a real application is built using RDF. Some first examples are presented below. Our approach to model contexts and provenance has been simpler and motivated by real-world RDF applications we have developed [16a][16b][16c]. We found that an additional dimension to the RDF triple can be useful or even essential. Given that the usage of full-blown RDF reification is not feasible due to its verbosity and inefficiency we developed a different modeling technique that flags or mark a given statement as belonging to a specific context. First example considers subjective assertions. The Last Minute News (LMN) [16b] and The News Blender (NB) [16c] demos allow an user rating and qualifying the source newspapers. The user can ""say"" that a newspaper is ""liberal"" or ""conservative"". Of course, two users, X and Y, will show two different opinions. Without considering the context, this will result in two triples: Newspaper A -> Quality -> ""liberal"" Newspaper A -> Qu",oceanology,215
a895c3e93c9097eaf4269e3ec246249d89a54436,filtered,semantic_scholar,2006 ACS/IEEE International Conference on Pervasive Services,2006-01-01 00:00:00,semantic_scholar,designing pervasive services for physical hypermedia,https://www.semanticscholar.org/paper/a895c3e93c9097eaf4269e3ec246249d89a54436,In this paper we describe the design and implementation of a software substrate for building pervasive services in the context of physical hypermedia applications. We first introduce the main ideas behind physical hypermedia; next we argue that physical navigation requires some software support to improve accessibility to real world objects. We next describe an architectural framework that supports specification and deployment of pervasive services. Some simple examples of use are presented. We conclude by comparing our work with others' and describing further work we are pursuing,oceanology,216
c55e4acdf98b1931bf1e00280639348d51a6283a,filtered,semantic_scholar,,2002-01-01 00:00:00,semantic_scholar,a hierarchical collective agents network for real-time sensor fusion and decision support,https://www.semanticscholar.org/paper/c55e4acdf98b1931bf1e00280639348d51a6283a,"This research addresses a problem of how to make effective use of real-time information acquired from multiple sensor and heterogeneous data resources, and reasoning on the gathered information for situation assessment and impact assessment (SA/IA), thus to provide reliable decision support for time-critical operations. A hierarchical collective agents network (HCAN) is employed as a solution to this problem. The agents network supports multi-sensor registration, real-time sensor/platform cueing, level-2 and level-3 information fusion, and has an arm toward the level-4 fusion objectives. An agent component assembly and decision-support-system-development environment, the 21 Century systems’ AEDGE software package, is used for the design and implementation of a HCAN-DSS system. The ability to integrate and correlate a vast amounts of disparate information from multiple sensor and heterogeneous data resources with varying degrees of uncertainty in real-time is an impediment issue for missioncritical decision support systems (DSS). For example, in crucial military operations command officers need real-time information and intelligences from various sensors/data resources in a theater of reconnaissance and surveillance to build a whole picture of the battle-space. It is critical for the commanders to know and to understand the relationships among the information collected. Questions are asked: what are the physical and functional constituencies among the objects in a given geographic sector? Are there sequential or temporal dependencies of the objects and what will trigger them? What are the possible consequences of the action and re-actions? Decision making based on these situation assessment and impact assessment (SA/IA) are particularly important for identifying and prioritizing “gaps” between the operation planning and the real-time interactions. To support making effective SA/IA, a data fusion and decision support system is required to use a set of coherent patterns derived from the available data sets and infer the implications (e.g., causal relations) toward the real world situations. The attribute coherence that is critical to the formation of the meaningful knowledge patterns is often obscure in the data sets obtained from heterogeneous resources. The data collections are often incomplete, imprecise, and inconsistent due to various natural constraints and human faults. Decision makers naturally desire to access large quantities of information expressed in diverse forms. However, as new sensor technology and various information sources have combined to create quantity and diversity, it has become increasingly difficult to provide decision makers with the right information at the right time and in the right quantity and format. Real-time computerized decision support systems are constructed by integrating a number of diverse components from a variety of software modules. Software developers have come to a numerous ways of querying the local and centralized data resources to access and distill the large and diverse information for the purpose of providing effective decision support. Meanwhile DSS are becoming more and more complex in terms of knowing which data resources to connect, how to keep track of the data dynamics, and assess the reliability of information from each resources. These tying links make the use of intelligent agent architecture necessary and desirable for allowing real-time responsibility and adaptive control of the DSS. Many popular agent systems of today deploy agents in a uniform level of operation. The agents respond to the same calls and cooperate at the same time toward the same goal of operation. The architecture endues some difficulties in agent communications and task control. When applied in complex real-time DSS with intensive human and system interactions, the cooperative nature makes the system less robust because the disability of one agent would affect the successive operations of the entire agent assembly. The collective nature of the agents in a HCAN paradigm overcomes some of these difficulties, for example, relieving the burden of data-exchanges between fellow agents by limiting agent communication to vertical layers of the assembly only. The hierarchical architecture simplifies the functional design of the agent interactions and enhances the security and efficiency of the process. The HCAN architecture also strikes a balance between the centralized control and distributed computation by allowing distributive agent operations within layers of the hierarchy and enforcing centralized control between the layers of the hierarchy, thus creating a federated agents integration structure. Basically, the HCAN has the functionalities of. 1). A flexible software architecture for accommodating system augmentation and evolutions; 2). A powerful representation schema for accommodating heterogeneous forms of information; 3). A diverse interface for various input resources, output formats, and human interactions; From: AAAI Technical Report WS-02-15. Compilation copyright © 2002, AAAI (www.aaai.org). All rights reserved. 4). An ability of reasoning on incomplete and inconsistent information, and extracting useful knowledge from the data of heterogeneous resources; 5). An ability of incorporating real-time dynamics of information resources into system at time of operation, and promptly adjusting the reasoning mechanisms; 6). An ability of summarizing and refining knowledge extracted, and distinguishing mission and time critical knowledge from insignificant and redundant ones; 7). A capability of supplying meaningful and accurate explanations, both qualitatively and quantitatively, of the automated system actions; and 8). A capability of providing adequate control and scrutiny of the system operations w.r.t. environment constrains. There are many sources of uncertainty at different levels of the decision support. For example, even if a situationassessor is aware of the presence of certain objects in the operation space, such as the type of contact, intention, reaction rational, etc., the exact dynamics of the object is still uncertain to the decision maker. While the knowledge about the object dynamics is critical in constructing an optimal strategy of action, various statistical methods and knowledge discovery techniques are applied in the reasoning module. The level of uncertainty forces the reasoning agents to operate with different decision strategies. The 21 Century Systems, Inc. has developed the AEDGE as an open DII-COE and CORBA compliant agentbased environment that enables the development of components-based agent systems. The system is implemented in JavaTM, with Java Database ConnectivityTM for DB access, Java Swing, AWT, and Java3D for visual interfaces, Java Media Framework and Java Speech API for audio/speech interface. AEDGE defines Agents, Entities, Avatars and their interactions with each other and with external sources of information. This standardized architecture allows additional components, such as servicespecific DSS tools, to be efficiently built upon the core functionality. Common interfaces and data structures can be exported to interested parties who wish to extend the architecture with new components, agents, servers, or clients. When the core AEDGE components are bundled with customer-specific components, a clean separation of those components, through APIs, is provided. The AEDGE is based on an extensible multi-component DSS architecture (EMDA, also referred to as the AEDGETM Architecture). The architectures describe the data objects, interfaces, communication mechanisms, component interactions, and integration mechanisms for the AEDGE and its extensions. In the AEDGE architecture, components communicate among each other via the Service Provider/Service Requester Protocol (SPSR). Service providers are components that implement an algorithm or need to share their data (data sources). Service requesters are the components that need a function performed for them by some other component or need to import data from another component. Both service requesters and service providers implement remote interfaces, which enables such components to communicate over a TCP/IP network. The remote interface implementation is currently based on Java RMI (remote method invocation), though the Architecture is not dependent on this implementation. AEDGE provides multiple levels of customization. The subject-matter users are able to build scenarios and scripts or to automatically generate them using the AEDGE-based Scenario Editor. Rules and triggers for agent behaviors can be created and modified by the advanced user. AEDGE also provides APIs for custom extensions of agents, data bridges, and the entity framework. The practical user will enjoy AEDGE’s versatile data connectivity and its near-real-time execution and monitoring of DSS functions. As a built-in bonus, AEDGE provides connections to a number of simulators and data formats, including HLA, DIS, DTED, DBDB2, XML, as well as support for multiple modes of distribution (CORBA, RMI, TCP/IP). As an example of the HCAN design using AEDGE for data fusion and DSS applications, the Advanced Battlestation with Decision Support System (ABS/DSS) which was developed as an operational agent-based C2 team decision support platform for command and control centers aboard aircraft carriers.. The ABS/DSS is based on AEDGE’s implementation of HCAN, and provides consolidated situational awareness through real-time, interactive, agent based decision support coupled with a linked 2D/3D battlespace visualization. Additionally, the ABS/DSS supports shipboard distributed training, train-asyou-fight, with a built-in scenario construction and emulation of friendly and hostile entities. Whether the watchstander is in live-feed mode or in training mode the operation of the agent-based decision support system and the 2D/3D visualization is identical. ",oceanology,217
88e866eaab83d42f392182697c3075ece34c9c62,filtered,semantic_scholar,,2014-01-01 00:00:00,semantic_scholar,the six pillars of simulation architecture,https://www.semanticscholar.org/paper/88e866eaab83d42f392182697c3075ece34c9c62,"This paper addresses simulation architecture for real-time Man-In-The-Loop (MITL) and Hardware-In-TheLoop (HITL) simulation laboratories, as used by the Lockheed Martin Aeronautics Company to support the full life cycle for aircraft and air system products. The paper discusses concepts and considerations used to establish the architecture for simulation and systems integration laboratories. The subject is presented via discussion of six pillars of simulation architecture: Composition, Functionality, Structure, Behavior, Mechanization, and Doctrine. The foundation for these pillars, the System Design Process, is also discussed. For each of these major elements, architectural goals and products are reviewed, and some real examples from major programs are provided. Biography Barry Evans is a Senior Fellow and Chief Engineer for Simulation and Systems Integration Laboratories at the Lockheed Martin Aeronautics Company. Mr. Evans provides technical leadership for simulation and integration labs across all programs and sites within the company. Prior to this role, Mr. Evans served two years as the Acting Director for F-35 Laboratories, during which time he successfully led a major reorganization and technical re-plan that resulted in substantial cost reduction and resolution of technical challenges. Prior to this assignment, he led a major program re-plan and served as air vehicle integration manager for the C-5 RERP Program. Mr. Evans was the lead architect for a common suite of laboratory software, hardware, standards, processes, and paradigms, deployed across program domains, including F-35, F-22, C-5, C-130, C-27, P-3, S-3, CRAD, and IRAD programs. He served as system architect and/or project manager for numerous simulation and systems integration projects. His design and development experience includes: operating systems; simulation executives; I/O; air vehicle and mission systems models; tactical combat simulation; OFP re-hosts; visual/audio/motion/feel cueing systems; and hardware-in-the-loop stimulation. Mr. Evans holds a BSEET from Southern Polytechnic State University, and he completed course work in Flight Dynamics from Kansas State University. Introduction The subject of simulation architecture is one that is both broad and deep, and the term “simulation architecture” can summon different views and meanings, depending on the focus of the individual. Additionally, there are different types of simulation and a variety of applications, which can influence what is architecturally important in the simulation. So, in order to discuss simulation architecture, it is necessary to describe the application context and provide a definition of what is intended by the term “architecture”. The Lockheed Martin Aeronautics Company utilizes real-time Man-In-The-Loop (MITL) and Hardware-InThe-Loop (HITL) simulation laboratories to support all phases of air system development and sustainment, including: concept exploration, theater-level analysis, system trade studies, system requirements development, design evaluation, system developmental testing, integration testing, system verification and validation (V&V), and training/familiarization. The simulations employed range from relatively small, low fidelity implementations that may focus on one element of one aircraft, to very large scale, full theater simulations that represent thousands of real-world entities (aircraft, ground vehicles, elements of the environment, weapons, etc.) with very high fidelity. Some of the simulations are oriented about human interaction with simulated systems (such as the airframe and flight controls, pilot-vehicle interface, mission systems, weapon systems, etc.), while other simulations are focused about stimulating air system hardware for integration and verification testing. The discussion below is in the context of this wide range of simulation applications. For the discussion herein, simulation architecture is defined as the design, structure, organization, and behavior of a simulation. The architecture addresses hardware, software, intellectual attributes, human interaction, and/or anything critical to the simulation design. The term “simulation architecture” may apply at different levels within a simulation, including: component, subsystem, system, and system-of-systems. The following discussion is focused on the system-level design of a simulation, addressing the information necessary to define the simulation blueprint, inclusive of component identification and interfaces, but exclusive of component-level internal design. Simulation architecture may be viewed in terms of six pillars (or elements): Composition, Functionality, Structure, Behavior, Mechanization, and Doctrine. Refer to Figure 1. In this view, the six pillars provide direct support of the simulation architecture, and the system design process is portrayed as the underlying foundation for the pillars. A discussion of the goals and products for each of these elements allows the highlighting of various key concepts and considerations that go into the development of a simulation architecture. The six pillars are not meant to represent phases or steps of architectural development, but rather a convenient way to break down and discuss the complex subject of simulation architecture. Also, it should be noted that just as the structural load of a building section may be supported by more than one physical pillar, a product or artifact of simulation architecture may be supported by, (or be the result of), the goals and activities from more than one of the architectural pillars discussed herein. Additionally, it is worth noting that most of elements of system design discussed herein could be applicable to any software-intensive system design. But, the descriptions are cast in the context of simulation, and they include some simulation-specific architectural considerations. Figure 1: The Six Pillars of Simulation Architecture",oceanology,218
4a99aa2ca1dd85ac7b82d79bf8cec1a09c9d8488,filtered,semantic_scholar,,2012-01-01 00:00:00,semantic_scholar,energy efficient protocol design in wireless sensor networks – contributions to make the ubiquitous platform greener,https://www.semanticscholar.org/paper/4a99aa2ca1dd85ac7b82d79bf8cec1a09c9d8488,"s all hardware resources as components. For example, calling the getData() command on a sensor component will cause it to later signal a dataReady() event when the hardware interrupt fires. While many components are entirely software based, the combination of split-phase operations and tasks makes this distinction transparent to the programmer. In both cases an event signals that the encryption operation is complete. ADC, ClockC, UART, SlavePin and SpiByteFifo are example hardware abstraction components. TinyOS commands and events are very short, due to limited code space and a finite state machine style of decomposition. The rich event processing model means an event or command call path can traverse several components. The TinyOS component model allows us to easily change the target platform from mote hardware to simulation by only replacing a small number of low-level components. The event-driven execution model can be exploited for efficient eventdriven simulation, and the whole program compilation process can be re-targeted for the simulator‟s storage model and native instruction set. The static component memory model of TinyOS simplifies state management for these large collections. Setting the right level of simulation abstraction can accurately capture the behavior and interactions of TinyOS applications. Figure 1.3: TinyOS Structure (Consist of scheduler and graph of components) 2.3 TOSSIM: A Simulator for TinyOS Sensor Networks The Necessity of Network Simulation: The emergence of wireless sensor networks brought many open issues to network designers. Traditionally, the three main techniques for analyzing the performance of wired and wireless networks are analytical methods, computer simulation, and physical measurement. However, because of many constraints imposed on sensor networks, such as energy limitation, decentralized collaboration and fault tolerance, algorithms for sensor networks tend to be quite complex and usually defy analytical methods that have been proved to be fairly effective for traditional networks. Furthermore, few sensor networks have come into existence, for there are still many unsolved research problems, so measurement is virtually impossible. It appears that simulation is the only feasible approach to the quantitative analysis of sensor networks. The event-driven nature of sensor networks means that testing an individual mote is insufficient. Programs must be tested at scale and in complex and rich conditions to capture a wide range of interactions. Deploying hundreds of motes is a daunting task, the focus of work shifts from research to maintenance, which is time-consuming due to the failure rate of individual motes. A simulator can deal with these difficulties, by providing controlled, reproducible environments, by enabling access to tools such as debuggers, and by postponing deployment until code is well tested and algorithms are understood. TOSSIM: TOSSIM is a discrete event simulator for TinyOS sensor networks. Instead of compiling a TinyOS application for a mote, users can compile it into the TOSSIM framework, which runs on a PC. This allows users to debug, test, and analyze algorithms in a controlled and repeatable environment. As TOSSIM runs on a PC, users can examine their TinyOS code using debuggers and other development tools. TOSSIM‟s primary goal is to provide a high fidelity simulation of TinyOS applications. For this reason, it focuses on simulating TinyOS and its execution, rather than simulating the real world. While TOSSIM can be used to understand the causes of behavior observed in the real world, it does not capture all of them, and should not be used for absolute evaluations. Related Publication: Swarup Kumar Mitra, Ayon Chakraborty, Subhajit Mandal and M.K.Naskar, Simulation of Wireless Sensor Networks using TinyOS A Case Study, In the Proceedings of the National Conference on Modern Trends in Electrical Engineering, pages EC 23 EC 26, Hooghly, West Bengal, July 2009. 3 Data Gathering Schemes in WSNs Data gathering is by far one of the most important aspects of research considering energy efficiency in the routing protocols for wireless sensor networks. Wireless sensor networks have emerged as a ubiquitous platform recently, and issues regarding the efficiency of energy usage by these devices play a very important role. These devices are equipped with negligible or less amount of battery power to sustain for a long time. Not only that, in most of the scenarios, where these networks are deployed it is infeasible or impossible sometimes to replace the battery power of the sensor nodes. One of the most fundamental aspects for energy consumption in sensor nodes is communication, other than sensing and computation costs. Optimization of communication costs is thus essential, which is a direct consequence of betterment of routing techniques in this type of wireless networks. A major portion of my contribution in this project deals with designing data gathering schemes for wireless sensor networks and optimization of routing techniques, described below. The first in this queue was the HDS or “Hybrid Data Gathering Scheme”. Published in the International Conference of Distributed Computing and Internet Technology (ICDCIT‟10), this work is a novel approach in minimizing not only the communication / energy overhead but also guarantees a minimal energy-latency product. It also distributes the energy consumption by the nodes by rotating the leader node, so as to increase the uniformity of energy content in the nodes. The uniform distribution of energy content in the nodes also helps to lessen the chances of a black hole or a sinkhole problem. The HDS protocol is based on the hybrid combination of two algorithms, SHORT and LBERRA. The LBERRA scheme is used to subdivide the sensor field into predefined clusters, and SHORT is applied to form a binary tree spanning the nodes. There are two types of leader nodes: one for each cluster, forming the root of the tree and the other one is the „sink‟ communicating the gathered data to the Base Station. In each of the data gathering rounds the leader node is changed The second work was related to optimization of routing chain through heuristic techniques. Firstly, I applied Particle Swarm Optimization to create the most energy efficient paths for communication in the sensor field. Then, I investigated the use of Genetic Algorithms (hybridized with simulated annealing) in solving the same problem. In these works, I not only devised the algorithm for the minimum-energy path formation, but also coded it in nesC discussed in the earlier section. The implementation and simulation in nesC guarantees the hardware feasibility of the algorithm in sensor nodes. Packet loss rates were also studied with varying network topology and signal strengths in communication between particular sensor nodes. In all the cases, a standard background noise was considered. This work followed a series of publications including three international conferences and two international journals. An extension of this work was to create energy efficient data gathering trees. Most algorithms developed in literature used greedy algorithms to construct routing trees which in most of the cases did not result in near-optimal energy usage. “ROOT” or “ROuting through Optimized Trees” was an answer to this need. Related Publications: International Conference: 1. Ayon Chakraborty, Kaushik Chakraborty, Swarup Mitra and Mrinal Naskar, An Optimized Lifetime Enhancement Scheme for Data Gathering in Wireless Sensor Networks, in the proceedings of The Fifth IEEE Conference on Wireless Communication and Sensor Networks, WCSN'09 Allahabad, India, (December, 2009). 2. Ayon Chakraborty, Swarup Mitra and Mrinal Naskar, An Efficient Hybrid Data Gathering Scheme in Wireless Sensor Networks, in the proceedings of The Sixth International Conference on Distributed Computing and Internet Technology, ICDCIT'10, Bhubaneswar, India. (February, 2010). 3. Ayon Chakraborty, Swarup K. Mitra and M.K. Naskar, Energy Efficient Routing in Wireless Sensor Networks: A Genetic Approach, in the Proceedings of the International Conference on Computer Communications and Devices (ICCCD 2010), IIT Kharagpur (December, 2010) 4. Kaushik Chakraborty, Ayon Chakraborty, Swarup Mitra and Mrinal Naskar, ROOT: Energy Efficient Routing through Optimized Tree in Sensor Networks, in the proceedings of The International Conference on Computer Communications and Devices – ICCCD'10, Kharagpur, India. (December, 2010).",oceanology,219
09b7e69ffd621e9025cbc1a313f41a8b63f98738,filtered,semantic_scholar,,2010-01-01 00:00:00,semantic_scholar,arms: a decentralised naming model for object-based distributed computing systems,https://www.semanticscholar.org/paper/09b7e69ffd621e9025cbc1a313f41a8b63f98738,"Entities communicate with one another in distributed computing systems via symbolic names. Implementing such communication requires a naming scheme that dynamically maps these symbolic names to physical nodes and processes. Traditionally, a centralised name server is deployed to perform such translations. However, a collaborative and dynamic environment requires a decentralised naming system due to reasons of efficiency and reliability. 
 
ARMS (Adaptive, Randomised and Migration-enabled Scheme) is a novel decentralised naming scheme for distributed object-oriented computing systems. A notable feature of ARMS is that it provides direct naming supports for the patterns of object communication and object migration processes to achieve greater performance and scalability in executing object-oriented software within a distributed environment. These supports are driven by three key components: 1) an adaptive locating protocol that exploits the patterns of object communication and explores the best routing path in the face of the changing network conditions, 2) a randomised overlay that is a scalable and flexible substrate for routing name queries, and 3) a hybrid relocation scheme that provides a transparent and efficient means of referencing migrated objects. 
 
The performance of ARMS has been examined using a number of real world Java-based benchmarking programs. Based on results in this study, ARMS has found to be superior to its structural counterpart – the Chord model because of the adaptive routing protocol and the resilient overlay. Furthermore, ARMS has shown to be superior in a number of other performance metrics.",oceanology,220
d0f2b863b1af919d08620efe960f708aabe63199,filtered,semantic_scholar,,2005-01-01 00:00:00,semantic_scholar,secure design and implementation of distributed and interoperable information systems based on overlap knowledge pattern,https://www.semanticscholar.org/paper/d0f2b863b1af919d08620efe960f708aabe63199,"New architectural and technical forms of information systems add a more significant level of complexity due to the decentralization of the constraints, treatment and data. These architectures increase the deployment and the runtime possibilities because of the number of existing sites. Indeed, the simple separation of the various functional levels as it is done in a classical architecture (Data, Treatment, Presentation) is not enough and the choice of the site of deployment or runtime becomes significant for the optimization of the production of the system. In these architectures, these decisions of distribution are generally made during the implementation phase. The conceptual structures offered to designers to allow them to express their needs for distribution (concepts of packages, business component,..) do not match with the rules used by developers for building their distributed components [Snene04B]. In fact, software components represent a single and autonomous concept of real world. They encapsulate all the data concerning this concept including name, goal, behavior and all other information with regard to them. In fact, a software component is a set of objects that can be physically deployed on two or several sites. It is usually made up of one or of several distributed components that offer together the various aspects of distribution necessary to the software component. The distributed components represent the physical modules used for application assembling. They encapsulate given data and treatments and provide their services through well-defined interfaces.",oceanology,221
3d80085c2bc6e289c5620bbf06b0878f4b3be001,filtered,semantic_scholar,,2010-01-01 00:00:00,semantic_scholar,reliable middleware framework for rfid system,https://www.semanticscholar.org/paper/3d80085c2bc6e289c5620bbf06b0878f4b3be001,"The reliability of RFID systems depends on a number of factors including: RF interference, deployment environment, configuration of the readers, and placement of readers and tags. While RFID technology is improving rapidly, a reliable deployment of this technology is still a significant challenge impeding wide-spread adoption. This research investigates system software solutions for achieving a highly reliable deployment that mitigates inherent unreliability in RFID technology. 
We have considered two different problem domains for large scale RFID deployment. One is item tracking and the other is guidance-monitoring. 
The basic contribution of our work is providing novel middleware solution that is able to serve the application taking into account the inherent unreliability of RFID technology. Our path abstraction that uses the physical flow of data as an ally to generate a logical system level flow enhances the performance in many ways. The contributions of this dissertation are summarized below: 
Defining novel system architecture for item tracking applications: We have defined a system architecture referred to as Reliable Framework for RFID (RF2ID) that takes into account the unreliability of RFID devices and provides a scalable, reliable system architecture for item tracking applications. It uses a distributed system abstraction named Virtual Reader (VR) that handles RFID data in different geographic locations. Virtual Path (VPath) is the abstraction that creates channels among the VRs and facilitates a data flow oriented data management in the system. 
Implementation of RF2ID: We have implemented RF2ID that is able to incorporate physical RFID devices as well as emulated devices for scalability study taking into account various real world challenges of large scale RFID deployment. 
Load Shedding Based Resource Management: RF2ID requires a mechanism to handle unexpected system load in the presence of asynchronous arrival of data items. Space based load shedding and time based load shedding techniques are used in RF2ID. The basic idea is to exploit the VR and Vpath abstraction to intelligently share the load among the VRs in the presence of high system load, and yet provide some guaranteed Quality of Service (QoS). 
Architecture for GuardianAngel: We define an architecture for an indoor pervasive environment which provides novel system abstraction and communication framework. The layered architecture has distributed computational elements known as the virtual station (VS) that are in charge of serving different regions of the environment. The Mobile Objects (MO) are the physical and logical entities that use sensing device and traverse the environment. The environment itself is tagged with RFID. The MO uses its sensing device to make guidance decisions locally. The VS keeps status information of MOs and keeps coarse grained information of the MO over time and space providing a virtual location for each MO. 
Implementation of GuardianAngel: We have implemented the GuardianAngle system as defined by the architecture. We have used a testbed that uses real RFID readers and tags in the pervasive environment in a limited laboratory setup. We have also developed a distributed system setup using emulated tags for a scalability study of the proposed architecture. We have also implemented a prototype application, to test its feasibility in the real world. 
Evaluation of the system: We have conducted extensive evaluation using the real RFID testbed as well as scalability study using emulated readers and tags. The evaluation using the real RFID tags and readers gives us the credibility of the system under various environmental considerations. The large scale experimentations provide us with scalability and feasibility study to strengthen our limited resource study using real RFID testbed. (Abstract shortened by UMI.)",oceanology,222
9cd7293da245d385efb2058faa39dd11678dd572,filtered,semantic_scholar,,2010-01-01 00:00:00,semantic_scholar,sistemi riconfigurabili a basso consumo per applicazioni di monitoraggio distribuito,https://www.semanticscholar.org/paper/9cd7293da245d385efb2058faa39dd11678dd572,"The term Ambient Intelligence (AmI) refers to a vision on the future of the information society where smart, electronic environment are sensitive and responsive 
to the presence of people and their activities (Context awareness). In an ambient intelligence world, devices work in concert to support people in carrying out their everyday life activities, tasks and rituals in an easy, natural way using information and intelligence that is hidden in the network connecting these devices. This promotes the creation of pervasive environments improving the quality of life of the occupants and enhancing the human experience. 
AmI stems from the convergence of three key technologies: ubiquitous computing, ubiquitous communication and natural interfaces. 
Ambient intelligent systems are heterogeneous and require an excellent cooperation between several hardware/software technologies and disciplines, including signal processing, networking and protocols, embedded systems, information 
management, and distributed algorithms. 
Since a large amount of fixed and mobile sensors embedded is deployed into the environment, the Wireless Sensor Networks is one of the most relevant enabling technologies for AmI. WSN are complex systems made up of a number of sensor nodes which can be deployed in a target area to sense physical phenomena and communicate with other nodes and base stations. These simple devices typically embed a low power computational unit (microcontrollers, FPGAs etc.), a wireless communication unit, one or more sensors and a some form of energy supply (either batteries or energy scavenger modules). 
WNS promises of revolutionizing the interactions between the real physical worlds and human beings. Low-cost, low-computational power, low energy consumption and small size are characteristics that must be taken into consideration when designing and dealing with WSNs. 
To fully exploit the potential of distributed sensing approaches, a set of challengesmust be addressed. Sensor nodes are inherently resource-constrained systems with very low power consumption and small size requirements which 
enables than to reduce the interference on the physical phenomena sensed and to allow easy and low-cost deployment. They have limited processing speed,storage capacity and communication bandwidth that must be efficiently used 
to increase the degree of local ”understanding” of the observed phenomena. 
A particular case of sensor nodes are video sensors. This topic holds strong interest for a wide range of contexts such as military, security, robotics and most recently consumer applications. Vision sensors are extremely effective for medium to long-range sensing because vision provides rich information to human operators. However, image sensors generate a huge amount of data, whichmust be heavily processed before it is transmitted due to the scarce 
bandwidth capability of radio interfaces. In particular, in video-surveillance, it has been shown that source-side compression is mandatory due to limited bandwidth and delay constraints. Moreover, there is an ample opportunity 
for performing higher-level processing functions, such as object recognition that has the potential to drastically reduce the required bandwidth (e.g. by transmitting compressed images only when something ‘interesting‘ is detected). The energy cost of image processing must however be carefully minimized. 
Imaging could play and plays an important role in sensing devices for ambient intelligence. Computer vision can for instance be used for recognising persons and objects and recognising behaviour such as illness and rioting. 
Having a wireless camera as a camera mote opens the way for distributed scene analysis. More eyes see more than one and a camera system that can observe a scene from multiple directions would be able to overcome occlusion problems and could describe objects in their true 3D appearance. In real-time, these approaches are a recently opened field of research. 
In this thesis we pay attention to the realities of hardware/software technologies and the design needed to realize systems for distributed monitoring, attempting to propose solutions on open issues and filling the gap between 
AmI scenarios and hardware reality. The physical implementation of an individual wireless node is constrained by three important metrics which are outlined below. 
Despite that the design of the sensor network and its sensor nodes is strictly application dependent, a number of constraints should almost always be considered. Among them: 
• Small form factor to reduce nodes intrusiveness. 
• Low power consumption to reduce battery size and to extend nodes lifetime. 
• Low cost for a widespread diffusion. 
These limitations typically result in the adoption of low power, low cost devices such as low powermicrocontrollers with few kilobytes of RAMand tenth of kilobytes of program memory with whomonly simple data processing algorithms can be implemented. However the overall computational power of the WNS can be very large since the network presents a high degree of parallelism that can be exploited through the adoption of ad-hoc techniques. Furthermore through the fusion of information from the dense mesh of sensors even complex phenomena can be monitored. 
In this dissertation we present our results in building several AmI applications suitable for a WSN implementation. The work can be divided into two main areas:Low Power Video Sensor Node and Video Processing Alghoritm and Multimodal 
Surveillance . 
Low Power Video Sensor Nodes and Video Processing Alghoritms 
In comparison to scalar sensors, such as temperature, pressure, humidity, velocity, and acceleration sensors, vision sensors generate much higher bandwidth data due to the two-dimensional nature of their pixel array. 
We have tackled all the constraints listed above and have proposed solutions to overcome the current WSNlimits for Video sensor node. We have designed and developed wireless video sensor nodes focusing on the small size and the flexibility of reuse in different applications. The video 
nodes target a different design point: the portability (on-board power supply, wireless communication), a scanty power budget (500mW),while still providing a prominent level of intelligence, namely sophisticated classification algorithmand high level of reconfigurability. We developed 
two different video sensor node: The device architecture of the first one is based on a low-cost low-power FPGA+microcontroller system-on-chip. 
The second one is based on ARM9 processor. Both systems designed within the above mentioned power envelope could operate in a continuous fashion with Li-Polymer battery pack and solar panel. Novel low power low cost video sensor nodes which, in contrast to sensors that just watch the world, are capable of comprehending the perceived information in order to interpret it locally, are presented. Featuring such intelligence, these nodes would be able to cope with such tasks as recognition of unattended bags in airports, persons carrying potentially dangerous objects, etc.,which normally require a human operator. Vision algorithms for object detection, acquisition like human detection with Support Vector Machine (SVM) classification and abandoned/removed object detection are implemented, described and illustrated on real world data. 
Multimodal surveillance: In several setup the use of wired video cameras may not be possible. For 
this reason building an energy efficient wireless vision network for monitoring and surveillance is one of the major efforts in the sensor network community. Energy efficiency for wireless smart camera networks is one of the major efforts in distributed monitoring and surveillance community. 
For this reason, building an energy efficient wireless vision network for monitoring and surveillance is one of the major efforts in the sensor network community. The Pyroelectric Infra-Red (PIR) sensors have been used to extend the lifetime of a solar-powered video sensor node by providing an energy level dependent trigger to the video camera and the wireless module. Such approach has shown to be able to extend node lifetime and possibly result in continuous operation of the node.Being low-cost, passive (thus low-power) and presenting a limited form factor, 
PIR sensors are well suited for WSN applications. Moreover techniques to have aggressive power management policies are essential for achieving long-termoperating on standalone distributed cameras needed to improve the power consumption. We have used an adaptive controller like Model Predictive Control (MPC) to help the system to improve the performances outperforming naive power management policies.",oceanology,223
e5334acd240594bf4a308887386ba7c4522278f4,filtered,semantic_scholar,,2011-01-01 00:00:00,semantic_scholar,interim report for mobile computing video transmission using usrp submitted,https://www.semanticscholar.org/paper/e5334acd240594bf4a308887386ba7c4522278f4,"The project envisions a real-time video transmissio n between two points using GNU Radio and Universal Software Radio Peripheral (USRP). In our project, a video signal which could be a realtime signal from a camera or simply a video file is modulated and processed by GNU radio and transmitted using a USRP. There is a USRP receiver node which receives the signal and GNU radio demodulates and re-produces the transmitted video s ignal. This project brings in several challenges like bringing the camera interface to the USRP envi ronment, packets getting lost or corrupted in air, maintaining a constant bit rate as required by the USRP. Introduction USRP is a hardware platform that allows general pur pose computers to function as high bandwidth software radios [1]. Application layer communicates with the physical layer through some intermediate layers. For a stationary host this act ivity seems to be a good option where the communication protocol is systematic and defined ac cording to the environment where it is located. But for a mobile node, the environment conditions c hange with time and hence, the transmit power, bandwidth and quality of the channel has to be cont inuously monitored and passed on to the application layer in order to select the suitable a lgorithm. In turn, the physical layer has to change as per the suggestion from application layer time to t ime. GNU Radio and USRP bring the application developer close to the hardware as near to the ante nna itself and provides user with the flexibility t o change the communication parameters on the fly [2]. USRP aids engineers for rapid prototyping and development of powerful and flexible software radio systems. The applications of USRP come in manifold. It is used widely in prototyping and rese arch applications but it has been deployed in many real-world commercial and defence systems as well [ 3]. In our project, we explore an application of USRP which transmits real-time video from one point to another using USRP. Real-time video transmission finds application in Digital Video Bro adcasting (DVB). Background There has been a tremendous growth in the field of multimedia and mobile communications. The convergence of these two has resulted in mobile mul ti edia communications which has attracted the attention of the research community around the worl d [1]. A lot of researches have been done in this area to find out new methodologies to improvise or innovate new ways to implement the technology with better bandwidth and energy efficiency as thes e two resources are limited. USRP is an emerging technology which provides a platform to excavate th mobile computing environment in different scenarios. GNU radio and USRP provides a powerful radio commun ication platform. GNU Radio is an opensource Software-Defined Radio (SDR) platform which has libraries for various modulation schemes, error-correcting codes and scheduling [3]. GNU Radi o runs as an application which interacts with the USRP hardware. The complex processing like modulati on, and signal processing which are conventionally implemented in hardware, can now be implemented in the software and is easily accessible to the user developing the application. Applications can be created using the GNU Radio blocks or the Python script language which is behin d the blocks. The performance-critical signal processing path is implemented in C++. SWIG interfa ce, which is an interface compiler, is used to link the C++ with the python. The hardware has the antenna, RF (Radio Frequency) front-end, ADC/DAC, USB interface and a user-programmable Fiel d Programmable Gate Array (FPGA) to perform down-conversion. There are newer versions o f USRPs which has Ethernet interfaces and powerful FPGAs for improved speed and processing. Basic block diagram The file source or data can be a webcam output or a video file. Two USRPs are used, one for transmitting and the other for receiving. The sourc e file is Gaussian Minimum Shift Keying (GMSK) modulated and transmitted using transmitter USRP. T he modulation is implemented on GNU Radio. The transmitted signal is received by the receiving USRP and further demodulated by GNU Radio and played back. Figure 1 shows the block diagram Figure 1 Block diagram Experiment set up and plan The project activity is planned in different steps. 1. In the first step we use a stored video file as sou rce. We try to playback this video sent from the transmitter and check whether we are able to re produce the same video at the receiver. This shall be done in simulator mode without the us of USRP and just by the loop back of transmitter and receiver. This set up implemented i s shown in Figure 2. Figure 2 Video transmission in simulator mode 2. The USRPs shall be introduced into picture and loop back removed. The receiver and transmitter side is implemented on each USRP and ag ain a video file shall be transmitted and received. 3. Real-time video capture can be established using a camera which acts as the new video source. GStreamer framework could be used to proces s the video signal and connect to the GNU radio. The block diagram of this idea is shown in Figure 3. Figure 3 Block diagram with real time video [4] The different options available to stream real-time video are: 1. GStream framework which is an open-source software that processes and encodes the video signal from the camera. We have to research other p layers like FFmpeg also available in the market. 2. UDP sockets listening to the camera port at one end and another UDP socket sending to the camera port at the other end. These two options need to be explored a little bit and finalised. After doing some research on the modulation schemes us d in video broadcasting and multimedia applications, we found out that GMSK and Orthogonal Frequency Division Multiplexing (OFDM) are widely used in video communication. Further researches are required on the format of vi deo signals or packets and their encoding schemes. As USRP works on constant bit rate data stream the video signals need to be in the correct format for communication. A good idea on the video encoding wi ll help in debugging in case any problem arises during the development and testing. Expected results and comments As per the experimental set up explained above, the first part of the project is successfully complete d. We were able to transmit and receive a video file w ith a loop back from transmitter to receiver using GMSK modulation in simulator mode. For the second a third part, we need to maintain a constant bit rate data stream for USRP. An H.264 encoder can be used to perform this and we expect that the transmission and receiving of the video file should be smooth and successful. We need to observe the following parameters during the experiment. 1. Delay in reception of the video. 2. Packet loss incurred during transmission. Once the main part of the project is successfully c ompleted, which is proper communication between the two peripherals, we can further analyse and com pare other modulation schemes suitable for video transmission. Also the maximum distance within whic h USRP’s can communicate can be explored. We anticipate that distance between the two USRPs p lay an important role. There could be distortion in the video as the distance increases due to the l oss of packets or erroneous packets. Introducing a suitable error correction scheme can correct the da ta fr mes received. This implementation can be done depending on the time we have at hand. References 1. http://gnuradio.org/redmine/attachments/129/USRP_Do cumentation.pdf 2. http://www.wu.ece.ufl.edu/projects/wirelessVideo/pr ject/H264_USRP/index.htm 3. http://www.ettus.com/downloads/ettus_broch_trifold_ v7b.pdf 4. http://wiki.oz9aec.net/index.php/Simple_DVB_with_Gs treamer_and_GNU_Radio",oceanology,224
a72d2df90c87909159ee5f59811f722b7f6ad4ad,filtered,semantic_scholar,,2003-01-01 00:00:00,semantic_scholar,"usenix association proceedings of bsdcon ’ 03 san mateo , ca , usa september",https://www.semanticscholar.org/paper/a72d2df90c87909159ee5f59811f722b7f6ad4ad,"The ever increasing mobility of computers has made protection of data on digital storage media an important requirement in a number of applications and situations. GBDE is a strong cryptographic facility for denying unauthorised access to data stored on a ‘‘cold’’ disk for decades and longer. GBDE operates on the disk(-partition) level allowing any type of file system or database to be protected. A significant focus has been put on the practical aspects in order to make it possible to deploy GBDE in the real world. 1 1. Losing data left and right In the last couple of years, gentlemen of the press have repeatedly been able to expose how laptop computers containing highly sensitive or very valuable information have been lost to carelessness, theft and in some cases espionage. [THEREG] The scope of the problem is very hard to gauge, since it is not a subject which the involved persons and, in particular, institutions are at all keen on having exposed. However, a few data points have been uncovered, revealing that the U.S. Federal Bureau of Investigation loses, on average, one laptop every three days. [DOJ0227] When a computer is lost, stolen or misplaced, it is very often the case that the computer hardware represents a value which is insignificant compared to the value of the disk contents. More often than not, the only reason the press heard about it was that the material on the disk was ‘‘hot’’ enough to make the loss of control rattle people at government level. While it is easy to blame these incidents on ‘‘user error’’, as is generally done, doing so makes it a very hard problem to fix. Human nature being what it is, seems to remain just that. In the absence of technical counter measures, administrative measures have been applied, generally with abysmal results. In one case, a bureaucracy has handled the problem according to what could easily be 1 This software was developed for the FreeBSD Project by Poul-Henning Kamp and NAI Labs, the Security Research Division of Network Associates, Inc. under DARPA/SPAWAR contract N66001-01-C-8035 (‘‘CBOSS’’), as part of the DARPA CHATS research program. mistaken for the plot from a classic Buster Keaton movie: First a laptop was forgotten and lost in a taxi-cab. New policy: always drive your own car if you bring your laptop. Then a car was stolen, including the laptop in the trunk. New policy: always bring your laptop with you. The next laptop was stolen from a pub while the owner was bowing to the pressures of nature. New policy: employees are not to carry their own laptops outside the office at any time. Laptops will be transported from and to the employees home address by the agency security force and will be chained and locked to a ring in the wall installed by the company janitors. All requests must be filed 3 days in advance on form ##-#. [PRIV] 2. Protecting disk contents Protecting the contents of a computer’s disk can in practice be done in two ways: by physically securing the disk or by encrypting its contents. Physical protection is increasingly impossible to implement. It used to be that disk drives could only be moved by forklift, but these days a gigabyte disk is the size, but not quite yet the thickness, of a postage stamp. While computers can be tied down with wires and bars can be put in front of windows, such measures are generally not acceptable, or at least not judged economically justified in any but the most sensitive operations. That leaves encryption of the disk contents as the only practical and viable mode of protection, and both the practicality and the viability has been somewhat in doubt. Until recently, nearly all aspects of cryptography were a highly political issue, this has eased a lot in the last couple of years and there now ‘‘only’’ remain a number of rather fundamental questions in the area of law enforcement and human rights, which are still unsettled. With the political issues mostly out of the way, the next roadblock is practical: While use of cryptography can never be entirely transparent, the overhead and workload it brings must be reasonable. 2.1. Application level encryption Encryption at the application level has been available for a number of years, primarily in the form of the PGP [PGP] program. This is about as intrusive and demanding as things can get: the user is explicitly responsible for doing both encryption and decryption and must enter the pass-phrase for every operation. 2 Apart from the inconvenience of this extra workload, many org anisations would trust their users neither to get this right nor even to want to get it right. From an institutional point of view it is important that cryptographic data protection can be made mandatory. 2.2. Filesystem level encryption Encryption at the file system level is a tried and acknowledged method of providing protection, but it suffers from a number of drawbacks, mainly because no mainstream file systems offer encryption. Encrypting file systems are speciality items, which means increased cost and system administration problems of all sorts. And since practically all operating systems use their own file system format, cross platform fully functional file systems are very rare. This means that a typical organisation will have to operate with a handful of different methods of encryption, which translates to system administration overhead, user confusion and extra effort to pass security and ISO9000 audits. A secondary, but increasingly important issue is that data which are stored in databases on raw disk, operating system paging areas and other such data are not protected by a cryptographic file system. To protect these would mean adding yet another set of encryption methods, which leads to a situation which is very hard to handle practically and administratively. Finally, file systems have a complex programming interface to the operating system, which traditionally 2 Interestingly, this is so impractical in real world use that various applications with PGP support resort to caching the pass-phrase at the application level, thereby weakening the protection a fair bit. has been subject to both version skew and compatibility problems. 2.3. Disk level encryption Encryption at the disk level can protect all data, no matter how they are stored, file system, database or otherwise. To a user, encryption at the disk level would require authentication before the computer can be used, everything functioning transparently thereafter, with all disk content automatically protected. Given that the programming interface for a disk device is very simple and practically identical between operating systems, there are no technical reasons why the same implementation could not be used across several operating systems. All in all this is a close to ideal solution from an operational point of view. There are significant implementation issues however. In difference from the higher levels, encryption at the disk level has no way of knowing a priori which sectors contain data and which sectors do not; neither is knowledge available about access patterns or relationships between individual sectors. Where application level or file system based encryption schemes can key each file individually, a disk based encryption must key each and every sector individually, ev en if it is not currently used to hold data. It has been argued that the encryption ideally should happen in the disk-drive, and while there are steps in this direction, they do unfortunately seem to have been made for the wrong reasons by the wrong people [CPRM], and have consequently not gained acceptance. Provided the owner of the computer remains in control of the encryption, I see no reason why encryption in the disk drives should not gain acceptance in the future. 3. Why this is not quite simple Several implementations have been produced which implement a disk encryption feature by running the user provided passphrase through a good quality one-way hash function and used the output as a key to encrypt all the sectors using a standard block cipher in CBC mode. A per sector IV for the encryption is typically derived from the passphrase and sector address using a one-way hash function. Tw o typical examples are [CGD] and [LOOPAES]. Unfortunately this approach suffers from a number of significant drawbacks, both in terms of cryptographic strength and deployability. For data to stay protected for decades or even lifetimes, sufficient margin must exist not only for technological advances in brute force technology, but also for theoretical advances in cryptoanalytical attacks on the algorithms used. Protecting a modern disk, typically having a few hundred millions of sectors, with the same single 128 or 256 bits of key material offers an incredibly large amount of data for statistical, differential or probabilistic attacks in the future. Worse, because the sectors contain file system or database data and meta data which are optimised for speed, the plaintext sector data typically have both a high degree of structure and a high predictability, offering ample opportunities for statistical and known plaintext attacks. This author would certainly not trust data so protected to be kept secret for more than maybe fiv e or ten years against a determined attacker. But far more damning to this method is that there can only be one single passphrase for the disk. This effectively rules out the ability for an organisation to implement any kind of per-user or multilevel key management scheme: the only possible scheme is ‘‘one key per disk’’. Add to this that to change the passphrase the entire disk would have to be decrypted and re-encrypted, and we have a model which may work in theory, and can be made to work in practice for a determined individual, but which would fast become an operational liability for any org anisation. 4. Designing GBDE The initial design phase of GBDE focused on determining a set of features which would make it both possible and",oceanology,225
cb311ca15659cba872bbd8a2152c814fbb4bb2ce,filtered,semantic_scholar,,1995-01-01 00:00:00,semantic_scholar,integrating qfd with object oriented software design methodologies,https://www.semanticscholar.org/paper/cb311ca15659cba872bbd8a2152c814fbb4bb2ce,"Object oriented (OO) methodologies have emerged as a popular paradigm for software design and analysis, both in research and practice. Several variants of OO methods are in use, but they all share significant similarities in their approaches to modeling the application domain. Quality Function Deployment (QFD) is also a design analysis and domain modeling technique with many parallels to OO methods. This paper contains an overview of object oriented design concepts, and shows how familiar QFD techniques are an effective aid for the OO analyst. QFD is a much easier way to approach the initial information collection and provides easy-to-understand structuring tools that do not require extensive training in OO concepts and methods. Overview of Object Oriented Software Concepts The following section is a brief overview of some object oriented concepts. It is beyond the scope and purpose of this article to discuss details of OO methodologies in depth. Many good sources of comprehensive elucidations on object oriented software engineering are given in the bibliography. Objects, messages, and encapsulation The fundamental concept in object oriented methodologies is, appropriately, the object. An object is a representation, or model, of a real-world entity. Objects have both data, which are usually called attributes, and behaviors, which are called methods . Since OO technology was heavily influenced by the analysis and design needs of real time control software applications, it was appropriate to envision objects as software representations of physical devices, such as sensors, actuators, and displays. A stepper motor, for example, has a state attribute ( on or off) and behavior ( turn_left, turn_right, and stop_motion). OO methods have also successfully been applied in more traditional information technology domains such as banking, accounting, personnel, etc. In these data-intensive applications, objects represent business entities and the data and processing operations that are associated with them. For example, a banking system might contain objects that represent individual checking accounts. Each account object contains data attributes for the name of the account owner, address, account number, current balance, and so forth. Objects communicate via messages ent to each other, with the assistance of the underlying language and operating run time support systems. Each message received by an object should have procedural code that interprets and carries out the function requested by the message; if the object does not understand the message, an error notification routine is invoked. In the checking 1 Sponsored by the U.S. Department of Defense 1995 QFD Symposium Page 1 May 14, 1995 account example, methods such as deposit_to_account and debit_from_account are invoked by messages exchanged between the account objects and transaction objects. Because these messages are the only interface that the object presents to the ""external world,"" the implementer is free to design internal object representations of data and procedures in any manner, as long as the message interface remains consistent. By taking this approach, details of the internal representation can be modified as needed without affecting the rest of the system. For example, temperature might be stored internally by the thermometer object in units of Fahrenheit, Celsius, or Kelvin, as long as the object accepts and replies correctly to messages requesting any particular reporting unit. This is known as encapsulation , or information hiding, and is a key concept in object orientation. Classes and instances Obviously, if every object in an application had to be individually and explicitly coded, it would not be practical to build a system consisting of hundreds of separate objects. Hence, every object is associated with a specific class. Classes are templates for objects, which specify the kinds of attributes and methods each object will have. Classes do not hold any of the values for the attributes of specific objects. Objects come into existence by being created as instances of a specific class. When an object instance is built from a class, a complicated process is used to determine the attributes and methods for that object, which involves allocating space, linking the object into various system data structures, and perhaps initializing certain attributes. Users of document processing products such as Ami Pro, Framemaker, or Microsoft Word for Windows have already encountered the concept of classes and instances. These products use the notion of paragraph styles to hold templates for various text characteristics, such as fonts, indentations, line alignment, margins, and so forth. Whenever a new paragraph (object) is created (instantiated) in a document, it inherits the formatting options of the template (class), but does not yet contain any data (text). All instances of paragraphs of a certain type (class) share the same formatting characteristics, but have different data attributes. 1995 QFD Symposium Page 2 May 14, 1995 Instance of a 2-door, red, ... van Class of Vans » # doors » length » color » engine option » VIN",oceanology,226
e0ff1e95e940e29e7cdc99a84492a37e3a051786,filtered,semantic_scholar,,2001-01-01 00:00:00,semantic_scholar,riacs fy2001 annual report,https://www.semanticscholar.org/paper/e0ff1e95e940e29e7cdc99a84492a37e3a051786,"Recently, there has been shift from consideration of optimal decisions in games to a consideration of optimal decision-making programs for dynamic, inaccessible, complex environments such as the real world. Perfect rationality is impossible in these environments, because of prohibiting deliberation complexity. Anytime algorithms attempt to trade off result quality for the time or memory needed to generate results. Bounded rational agents are ones that always take the actions that are expected to optimize their performance measure, given the percept sequence they have seen so far and limited resources they have. Process algebras, with basic programming operators, has been used to study the behaviors of interactive multi-agent systems and leading to more expressive models than Turing Machines, e.g., Interaction Machines. By extending process algebra operators with von Neumann/Morgenstern’s costs/utilities, anytime algorithms can be viewed as a basis for a general theory of computation. As the result we shift a computational paradigm from the design of agents achieving one-time goals, to the agents who persistently attempt to optimize their happiness. We call this approach $-calculus (pronounced “cost-calculus”), which is a higher-order polyadic process algebra with a utility (cost) allowing to capture bounded optimization and metareasoning in distributed interactive AI systems. $-calculus extends performance measures beyond time to include answer quality and uncertainty, using k Omega-optimization to deal with spatial and temporal constraints in a flexible way. This is a very general model, just as neural networks or genetic algorithms, leading to a new programming paradigm (cost languages) and a new class of computer architectures (cost-driven computers). The NSERC supported project on $-calculus aims at investigation, design and implementation of a wide class of adaptive real-time distributed complex systems exhibiting meta-computation and optimization. It has also been applied to the Office of Naval Research SAMON robotics testbed to derive GBML (Generic Behavior Message-passing Language) for behavior planning, control and communication of heterogeneous Autonomous Underwater Vehicles (AUVs). Some preliminary ideas have also been utilized in the 5th Generation ESPRIT SPAN project on integration of objectoriented, logic, procedural and functional styles of programming in parallel architectures. It appears that $-calculus can be useful for the NASA Information Power Grid (IPG) Project. The IPG testbed provides access to a widely distributed network of high performance computers. $calculus resource-bounded optimization allows for flexible allocation of resources and scalability needed to tackle hard computation problems, thus $-calculus could provide a unifying metasystem framework for the Information Power Grid. Biosketch: Dr. Eberbach is a Professor at School of Computer Science, Acadia University and an Adjunct Professor at Faculty of Graduate Studies, Dalhousie University, Canada. Previously he was Senior Scientist at Applied Research Lab, The Pennsylvania State University, Visiting Professor at The University of Memphis, USA, Research Scientist at University College London, U.K., Assistant Professor in Poland, and he also has industrial experience. Professor Eberbach’s current work is in the areas of process algebras, resource bounded optimization, autonomous agents and mobile RIACS FY2001 Annual Report October 2000 through September 2001 -135robotics. General topics of interest are new computing paradigms, languages and architectures, distributed computing, concurrency and interaction, evolutionary computing and neural nets. More information about projects, publications, courses taught can be found at http://cs.acadiau.ca/~eberbach October 27, 2000: Feng Zhao, Ph.D.,Principal Scientist, Xerox PARC “Smart Sensors, Collaborative Sensemaking” Imagine a world in which we live where smart roads would be able to tell us when they need repair and which is the best direction to get to the Giants game, smart factories would stock up just enough inventory, ... The rapid advances in micro-electro-mechanical systems (MEMS) and lower-power wireless networking have enabled a new generation of tiny, cheap, networked sensors that can be “sprayed” on roads, across machines, and on walls. However, these massively distributed sensor networks must overcome a set of technological hurdles before they become widely deployable. Keeping up with the constant onslaught of sensory data from say 100,000 sensors is akin to drinking from a fire hose. The Xerox PARC Smart Matter Diagnostics and Collaborative Sensing Project studies the fundamental problems of distilling high-level, humaninterpretable knowledge from distributed heterogeneous sensor signals in a rapid and scalable manner. We are developing powerful algorithms and software systems to enable a wide range of applications, from sensor-rich health monitoring of electro-mechanical equipment to human-aware environments that leverage sensors to support synergistic interactions with the physical world. Biosketch: Feng Zhao is a Principal Scientist in the Systems and Practices Laboratory at Xerox PARC. Dr. Zhao leads the Smart Matter Diagnostics Project that investigates how sensors and networking technology can change the way we build and interact with physical devices and environments. His research interest includes distributed sensor data analysis, diagnostics, qualitative reasoning, and control of dynamical systems. Dr. Zhao received his PhD in Electrical Engineering and Computer Science from MIT in 1992, where he developed one of the first algorithms for fast N-body computation and phase-space nonlinear control synthesis. From 1992 to 1999, he was Assistant and Associate Professor of Computer and Information Science at Ohio State University. His INSIGHT Group developed the SAL software tool for rapid prototyping of spatio-temporal data analysis applications; the tool is currently used by a number of other research groups. Currently, he is also Consulting Associate Professor of Computer Science at Stanford. Dr. Zhao was National Science Foundation and Office of Naval Research Young Investigator, and an Alfred P. Sloan Research Fellow in Computer Science. He has authored or co-authored about 50 peer-reviewed technical papers in the areas of smart matter, artificial intelligence, nonlinear control, and programming tools. October 12, 2000: Irem Tumer, Intelligent Health and Safety Group NASA/Ames “Influence of Variations on Systems’ Performance And Safety” High-risk aerospace components have to meet very stringent quality, performance, and safety requirements. Any source of variation is of concern, as it may result in scrap or rework (translating into production delays), poor performance (translating into customer dissatisfaction), and potentially unsafe flying conditions (translating into catastrophic failures). As part of the Intelligent RIACS FY2001 Annual Report October 2000 through September 2001 -136Health and Safety group, we have been designing controlled experiments to understand various sources of variations in helicopter transmissions, collecting vibration data, and analyzing the data for indicators of the variations. We are looking for normal and abnormal sources of variation that affect performance and indicators of these variations to provide warning about potential failures during flight. The experiments include: • Flight tests using an AH-1 and an OH-58 helicopter, to determine the variations introduced due to regular maneuvering and the covariance with environmental conditions, engine torque, etc.; • OH-58 transmission test-rig tests to determine the effect of variations due to different levels of torque, mast bending, and mast lifting forces, as well as pinion reinstallation effects; • Machinery Fault Simulator tests to test the effect of prefabricated defects and inherent design and manufacturing variations on gears, bearings, etc. In this talk, I will present an overview of our group’s research goals, discuss the experiments and go over some of the results from the data analyses conducted so far. I will then discuss the current work and future directions in developing formalized methods for design and manufacturing engineers, using the variation information from empirical and analytical studies. RIACS FY2001 Annual Report October 2000 through September 2001 -137III.B RIACS-Supported Workshops As part of its mission of fostering ties with the academic community in IT, RIACS provides financial, administrative, and technical support for selected workshops involving RIACS scientists. The following workshops were supported during this reporting year: Workshop on Verification and Validation of Software The RIACS Workshop on the Verification and Validation of Autonomous and Adaptive Systems took place at Asilomar Conference Center, Pacific Grove, CA, 5-7 Dec 2000. Discussions included: V&V of Intelligent Systems: How to verify and validate systems featuring some form of AI-based technique, such as model-based, rule-based or knowledge-based systems. V&V of Adaptive Systems: How to verify and validate systems featuring adaptive behavior, either in the form of parametric adaptation (e.g. neural nets, reinforcement learning) or control adaptation (e.g. genetic programming). V&V of Complex Systems: How to verify and validate systems with different interacting parts, either within a given location (e.g. layered control architectures) and among several locations (homogenous or heterogeneous multi-agent systems). Workshop on Model-based Validation of Intelligence Lina Khatib (Kestrel) and Charles Pecheur co-organized a symposium on “Model-based Validation of Intelligence” as part of the AAAI Spring Symposium Series in March 2001. We provided the technical content (announcement, reviews and selection of articles, final program) while AAAI provided the logistics (rooms, registra",oceanology,227
3a984a19f86877947561b4613b8b40b2efa01d26,filtered,semantic_scholar,,2009-01-01 00:00:00,semantic_scholar,understanding storage system problems and diagnosing them through log analysis,https://www.semanticscholar.org/paper/3a984a19f86877947561b4613b8b40b2efa01d26,"Nowadays, over 90% new information produced are stored on hard disk drives. The explosion of data is making storage system a strategic investment priority in the enterprise world. The revenue created by storage system industry steadily increases from $14.2 Billion in 2004 to over $18.4 Billion in 2007. As a key component of enterprise systems, reliable storage systems are critical. However, despite the efforts put into building robust storage systems, as the size and complexity of storage systems have grown to an unprecedented level, storage system problems are common. Unfortunately, many aspects of storage system problems are still not well understood, and most of previous studies only focus on one component - disk drives. 
To better understand storage system problems, we analyzed the failure characteristics of the core part of storage system - the storage subsystem, which contains disks and all components providing connectivity and usage of disk to the entire storage system. More specifically, we analyzed the storage system logs collected from about 39,000 storage systems commercially deployed at various customer sites. The data set covers a period of 44 months and includes about 1,800,000 disks hosted in about 155,000 storage shelf enclosures. Our study reveals many interesting findings, providing useful guideline for designing reliable storage systems. Some of the major findings include: (1) In addition to disk failures that contribute to 20–55% of storage subsystem failures, other components such as physical interconnects and protocol stacks also account for significant percentages of storage subsystem failures. (2) Each individual storage subsystem failure type and storage subsystem failure as a whole exhibit strong self-correlations. In addition, these failures exhibit bursty patterns. (3) Storage subsystems configured with dual-path interconnects experience 30–40% lower failure rates than those with a single interconnect. (4) Spanning disks of a RAID group across multiple shelves provides a more resilient solution for storage subsystems than within a single shelf. 
As we found out that storage subsystem problems are far beyond disk failures, we extend the scope of study to various storage system problems, and study the characteristics of storage system problem troubleshooting from various dimensions. Using a large set (636,108) of real world customer problem cases reported from 100,000 commercially deployed storage systems in the last two years, the analysis show that while some problems are either benign, or resolved automatically, many others can take hours or days of manual diagnosis to fix. For modern storage systems, hardware failures and misconfigurations dominate customer cases, but software failures take longer time to resolve. Interestingly, a relatively significant percentage of cases are because customers lack sufficient knowledge about the system. We also evaluate the potential of using storage system logs to resolve these problems. Our analysis shows that a failure message alone is a poor indicator of root cause, and that combining failure messages with multiple log events can improve problem root cause prediction by a factor of three. 
One key finding is that storage system logs contain useful information for narrowing down the root cause, while they are challenging to analyze manually because they are noisy and the useful log events are often separated by hundreds of irrelevant log events. Motivated by this finding, we designed and implemented an automatic tool, called Log Analyzer, to improve problem troubleshooting process. By applying statistical analysis techniques, the Log Analyzer can automatically infer the dependency relationship between log events, and identify the key log events that capture the essential system states related to storage system problems. By combining classic unsupervised classification techniques - hierarchical clustering with the event ranking techniques, the Log Analyzer can also identify recurrent storage system problems based on similar log patterns, so that previous diagnosis efforts can be systematically retrieved and leveraged. We train the Log Analyze with 18,878 week-long storage system logs and evaluate it with 164 real-world problem cases. The evaluation indicates that the Log Analyzer can effectively reduce the log event number to 3.4%. For most of the 16 real-world problem cases manually annotated with 1–3 key log events, the Log Analyzer accurately ranked the key log events within top 3 without a priori knowledge on how important the events are. For the other 148 problem cases with diagnosis and with root cause information, the Log Analyzer effectively grouped problem cases with the same root cause together with 63–93% accuracy, significantly outperforming other three alternative solutions which only achieve 30–46% accuracy.",oceanology,228
df7fb406e31a6057e2ea8f4997c3b71895e44093,filtered,semantic_scholar,,2007-01-01 00:00:00,semantic_scholar,design and implementation of a microprocessor-based sequencer for a small-scale groundnut oil production plant,https://www.semanticscholar.org/paper/df7fb406e31a6057e2ea8f4997c3b71895e44093,"A microprocessor-based Sequencer for a small-scale groundnut oil production plant was designed and a test model was implemented. The microprocessor-based Sequencer is meant to replace traditional, electromechanical sequencers, which are based on relays, contactors, limit switches and other similar devices. The INTEL 8085A microprocessor, combined with interface chips like the AD7575 ADC, the MAX378 multiplexer was used to implement the sequencer. Software was programmed into 2716 EPROM. Actuators and signal conditioning circuits were also designed and implemented. The implemented system was tested and the performance was found to be satisfactory. Introduction Background to the problem : According to Nwachuku[1] microprocessors are the state of the art in electronics digital systems’ design and the Nigerian Engineer, like his counterpart the world over, has no choice but to become interested in them. He contended further that, because of the nature of the microprocessor and its versatility, it becomes possible for the Nigerian engineer to device products to meet local needs using imported chips. This according to him, is the direction of technological development ..In the advanced industrialized and newly industrialized countries, the last couple of decades have seen the extensive application of microprocessors and computers to automate production processes. Specifically, the microprocessor acts as a micro-controller with a fixed program. Here, the microprocessor application system in most cases, involves the determination of values of physical parameters like temperature, pressure, and so on. In Nigeria, not much seems to have been achieved in this area. Some big manufacturing companies in both the public and private sectors of the Nigerian economy have had to change from along monitoring and control of plant operations to microprocessor/computer-based systems. Most of them are based on the programmable logic controllers (PLCs). But all these analog to digital (microprocessor /computer-based) implementations are mostly carried out by foreign firms and personnel, bringing along with them, their designed hardware and software. Even at that, not much is known to have been achieved in the area of deploying this latest-in-technology to improve the production processes of small-scale industries. This project was conceived as a result of a realization of this obvious short-coming. The nature of the problem: All over the world, countries have come to recognize the leading role which small-scale industries play in their economic development. They furnish over forty-percent (40%) of a nation’s output of goods and they also provide a substantial amount of total employment in an economy. A realization of this obvious fact has made the Federal Government of Nigeria to lay emphasis on the need to support Small and Medium Scale Enterprises in order that they act as catalyst for Nigeria’s industrial and economic growth. For example, the Federal Government of Nigeria has floated a Bank of Industries and has set up a Small and Medium Enterprises Development Agency of Nigeria (SMEDAN) with the objective of improving the performance of Small and Medium Enterprises (SMEs) towards achieving rapid industrialization of Nigeria and for the reversal of its over dependence on imports. Advanced Materials Research Online: 2007-06-15 ISSN: 1662-8985, Vols. 18-19, pp 107-110 doi:10.4028/www.scientific.net/AMR.18-19.107 © 2007 Trans Tech Publications Ltd, Switzerland All rights reserved. No part of contents of this paper may be reproduced or transmitted in any form or by any means without the written permission of Trans Tech Publications Ltd, www.scientific.net. (Semanticscholar.org-19/03/20,17:18:59) However, it must be realized that, the success of these initiatives and of Small and Medium Scale Industries (SMIs) themselves, would depend on indigenous locally developed production processes and technologies. Failures of this class of industries in the past have largely been attributed to their over dependence on imported production processes, technologies, and by extension, on imported plants, equipments and machines. The Raw Materials Research and Development Council (RMRDC) of Nigeria have identified the dearth of process equipment and machinery as the bane of Nigeria’s under-utilization of her agricultural and mineral raw materials. This now places a huge burden on the Nigerian engineer to now start to design, implement, and practicalize new production processes, machines, equipment and plants that can be deployed by the old and emerging SMIs. The Federal Government of Nigeria seems to have set the ball rolling by the establishment of a National Office for Technology Acquisition and Promotion (NOTAP). The literature is replete with works on interfacing microprocessors with real life situations which is exemplified by the following: Hosier[2] reviewed briefly, the issues that has to be addressed when interfacing a microprocessor to perform real world monitoring and control operations. Anazia [3] gave an overview of the status of microprocessor applications in industrial process control as it relates to the Guinness Plant in Benin-City, Nigeria. A PC-Based Data Acquisition and Supervisory Control system for a Small-Scale Industry was designed and implemented by [4]. Mansfield[5] described the use of transducers for industrial measurement purposes. Program and Data Stores design and Input/Output interfacing were well treated by Short[6]. Gregory[7] explained in good details signal conditioning circuits design. General description of the microprocessor based sequencer The groundnut-oil production plant operates as a series of logically controlled sequence of states; for example, agitator ON, crusher OFF and so on; the duration of each state being determined by sensor signals, for example, temperature or pressure; these sequence of states were transferred into software. The software part (application program), in consonance with the process-sequential flowchart (SFC) that describes the operation of the plant was written in assembler. A major part of the plant consists of electrical and mechanical components like motors which are controlled by electro-mechanical relays, these in turn are operated by signals from the microprocessor; therefore, OUTPUT INTERFACE circuits were designed. Other signals are fed to the microprocessor from temperature, pressure, and other sensors; INPUT INTERFACE circuits were therefore, also designed. The microprocessor system must have memory for storing the application program and for implementing intermediate computations and such other operations. INPUT transducers’ outputs are memory-mapped. The Y2 output of the 74ALS138 decoder that was used addresses a memory address range of 1000Hex to 17FFHex. Therefore, the six (6) input transducers reside at addresses 1000H, 1001H, 1002H, 1003H, 1004H and 1005H respectively. The analog-to-digital converter converts input voltages of between 0volts and 5volts to binary outputs of between 00000000 and 11111111, that is, between 00Hex to FFHex. The output ports (74LS373s) were assigned to a separate address space different from that occupied by main memory; hence, they were isolated or standard I/O. Therefore, the designed and implemented system is as shown in block diagrammatic form, in Fig. 1. Description of the Designed and Implemented System In Fig. 1, block 1 represents the transducers. Block 2 represents the signal conditioning circuits that condition the transducers’ signals to match the multiplexer’s-MAX 378(block 3) inputs. The combination of the multiplexer and the sample and hold-AD781 JN(block 4) selects and holds an input for the analog-to-digital converter-AD7575 JN (block 5) to convert for the input ports (block 6). Block 7 (INTEL 8085A) is the microprocessor subsystem. Block 8 is an address/data bus decoder (74ALS373), while block 9 is an address decoder (74ALS138) for the memory subsystem(blocks 10-4118 and 11-2716). Block 12 represents the output ports (74ALS373) and 108 Advances in Materials and Systems Technologies",oceanology,229
7dbe61b811bfc91e247dd4c949543468c394a1fd,filtered,semantic_scholar,,1999-01-01 00:00:00,semantic_scholar,integration of corba and web technologies in the vega dis,https://www.semanticscholar.org/paper/7dbe61b811bfc91e247dd4c949543468c394a1fd,"Distributed client/server architectures nowadays appear as a must for the wide information systems of the future virtual enterprise. At the same time, the continued growth of the Internet/WEB and its associated standard developments leads to new ways of world-wide information communication, distribution and access to information. This paper introduces to the various developments undertaken in the VEGA 1 project for a tighter integration of STEP, CORBA and WEB technologies within a DIS . Thus, the VEGA platform will allow both the support of distributed heterogeneous and interoperable client/server information systems (through CORBA) and the support of WEB based access to information and services through an Internet based navigation, building upon CGI and Java technologies. 1. Context and problematical issues The Large Scale Engineering (LSE) and Manufacturing universe is nowadays facing an increasing competitive environment where flexibility and adaptability to change are the bound paths to success, leading companies to renew their way of working. This is due to economical and technological drivers. Indeed, industrial enterprises are now devoted to the specification, design and construction of still larger and more complex manufacturing products: it has become no more possible to a single even-wide enterprise to take in charge the realisation of the whole products, both for financial reasons and because the required skills are not all within the enterprise. Thus, companies and SMEs ( Small and Medium Enterprises ) are now on the way of constituting virtual enterprises (VEs) for each new project. In a VE, contractors, partners, suppliers and customers form a temporary aggregation of non co-located actors dealing with the same product, but focusing on their core domain of competence for the shared profitability of industrial projects. At the same time, current progress in IT , providing more reliable and relevant mechanisms and software tools, the development of sophisticated new frameworks in client/server applications, and the continued growth of the Internet enable improved business processes and provide organisations with new business opportunities. Companies are now widely deploying their applications in Internet and Intranet 4 environments, assembling advanced IT based architectures encompassing among others networking, distributed information systems and concurrent engineering. In such a context, the VEGA project is developing a mandatory infrastructure for the support of the LSE business processes, particularly on the base of main standards for information modelling and exchange, with STEP (ISO 10303) or the IFC developed by the IAI , distribution and interoperability mechanisms with the OMG 6 CORBA and IIOP de facto standards, and WEB technology based on HTTP /CGI and Java. VEGA intends to cover the general needs of companies in VEs and Intranets, with a focus on the Building & Construction sector. Its main objective is to provide an information management architecture to guaranty interoperability among various software components running on different platforms, targeting STEP distributed technologies as an approach to bridge the gap between multiple and delocalised software systems in the construction industry. Within VEGA, the development of the DIS is a first approach towards an end-user oriented service for access to information through various forms. In the AEC field, large projects require the involvement of many body entities (client, architect, design engineers, various technical engineers, etc.) sitting at various locations, with different views and needs on the project, and managing different forms of documents like textual documents, structured documents, drawings, and so on. Dealing with all these kinds of information representations on the client side lead to consider, as far as possible, standardised front-end services. Access to information can be related to EDI 9 too. Until now, EDI has always been considered as a technology typically based on EDIFACT syntax and rules. But EDI can be considered as a kind of generic term, including all aspects of technical and commercial information exchange without mandatory requirements with respect to specific communication technology. Indeed, EDI can be considered from two different points of view: • the first one is a rather conceptual one: the EDI is a structured electronic exchange of data of any type between computer applications of parties involved in a transaction ; • the second is an operational one, where we consider means to realise the task as announced in the first point of view. With respect to these operational means, the Internet is nowadays more and more acknowledged as the common medium to support communication facilities within the development of client/server applications that deserve the larger audience. The WEB technology builds upon HTTP to provide the users with high-level graphical applications independent of the underlying client platform. Furthermore, the Java language now simplifies the development of WEB applications through the power and flexibility of a real object-oriented language. The Intranet perspective enfavours the use of WEB technologies on LANs 10 on a company scale, whether it be real or virtual. Therefore, a different approach has been applied in the VEGA DIS, considering EDI in the broad sense and consequently managing EDI messages through distributed networked infrastructures, emerging WEB standards (mainly HTML and VRML formats), and WEB technologies like CGI and Java. 2. Integration of new technologies in the VEGA project 2.1 Overview of the VEGA project As previously mentioned, the VEGA 11 project objective is to develop an IT platform enabling companies in a VE to work together. VEGA leans on available technology, and extend their capabilities as needed for engineering collaboration in a flexible distributed environment. To address the problem of information sharing, VEGA deals with 3 different technologies: • product-data modelling for the specification of meaningful project information; • middleware technology for the distribution of project information; • workflow management for the control of the flow of information and work in the VE. The VEGA platform relies on high level open standards for the three technologies listed above, including STEP and particularly EXPRESS for the neutral specification of product model data, CORBA for communication between distributed applications and distribution of objects over networks, workflow technology as defined by the WfMC 12 for design of process control, and information standards like SGML 13 or various WEB de facto standards for the support of valueadded distributed information services (exchange of administrative messages, document handling, presentation of information). Thus, VEGA is currently elaborating the fundamental grounds for distributed architectures, defining a service layered on top of CORBA (the COAST – COrba Access to STEP models [2]), for remote data access and manipulation of information models defined by explicit meta-models (or schemata) satisfying the STEP EXPRESS semantics. 2.2 STEP and the IAI If different software applications need to communicate and inter-operate, they first need to share the same information, without misunderstanding or loss of semantics. This implies a common way to represent and exchange the semantics. Such issues have led to dramatic research efforts to achieve effective product data exchanges, standardisation of methodologies, languages and technologies, especially in the context of PDT , among them: • STEP ([3], [4]), developed in ISO TC184/SC4 16 for the product data representation and exchange. It allows the expression in a uniform and complete way of all the information required for a product life-cycle (especially through the EXPRESS language [5]), and supplies means for exchanging data physical files [6] and sharing product databases [7] with models and applications independent mechanisms. STEP is today deeply used for real world product information modelling, communication and interpretation. • IFC [8], another major effort currently undertaken by a non-profit alliance (IAI) of the building industry including architects and engineers, building clients, software vendors, and so on. The main IAI objective is to specify the IFC as a universal model to be a basis for collaborative work in the building industry and consequently to improve communication, productivity, delivery time, cost, and quality throughout the design, construction, operation and maintenance life cycle of buildings. STEP and IAI share the same goals, i.e. application interoperability; data exchange and actor cooperation, but differ in their respective processes. The IAI promote a bottom-up approach, with an iterative and incremental development for fast implementation. STEP is a long-term project, with a top-down approach and is concerned with broad standardisation. The IAI, having a formal liaison status with STEP has partially adopted the STEP technology, mainly through an EXPRESS representation of the IFC. In the future, an integration of the IFC within STEP is planned. As an initiative driven by leading companies, the IAI is pushing the IFC as a de facto standard in the Building industry in a very near future. A key point of the VEGA infrastructure is to use PDT and especially EXPRESS to describe and store meaningful product information. Implementing the VEGA vision requires a tight coupling between STEP models and all the various components of the VEGA platform (including distribution layer, workflow, data storage). Eventually, the current IFC 1.5 release will be used in the final VEGA demonstration . 2.3 The CORBA standard CORBA ([9], [10], [11]) is an OMG specification for application interoperability and portability in distributed architectures, allowing objects described in any language to be shared across heterogeneous operating system",oceanology,230
469427dae41a91263631a3105a859b210b5d6912,filtered,semantic_scholar,VEE,2021-01-01 00:00:00,semantic_scholar,btmmu: an efficient and versatile cross-isa memory virtualization,https://www.semanticscholar.org/paper/469427dae41a91263631a3105a859b210b5d6912,"Full system dynamic binary translation (DBT) has many important applications, but it is typically much slower than the native host. One major overhead in full system DBT comes from cross-ISA memory virtualization, where multi-level memory address translation is needed to map guest virtual address into host physical address. Like the SoftMMU used in the popular open-source emulator QEMU, software-based memory virtualization solutions are not efficient. Meanwhile, mature techniques for same-ISA virtualization such as shadow page table or second level address translation are not directly applicable due to cross-ISA difficulties. Some previous studies achieved significant speedup by utilizing existing hardware (TLB or virtualization hardware) of the host. However, since the hardware is not designed with cross-ISA in mind, those solutions had some limitations that were hard to overcome. Most of them only supported guests with smaller virtual address space than the host. Some supported only guests with the same page size. And some did not support privileged memory accesses. This paper proposes a new solution named BTMMU (Binary Translation Memory Management Unit). BTMMU composes of a low-cost hardware extension of host MMU, a kernel module and a patched QEMU version. BTMMU is able to solve most known limitations of previous hardware-assisted solutions and thus versatile enough for real deployments. Meanwhile, BTMMU achieves high efficiency by directly accessing guest address space, implementing shadow page table in kernel module, utilizing dedicated entrance for guest-related MMU exceptions and various software optimizations. Evaluations on SPEC CINT2006 benchmark suite and some real-world applications show that BTMMU achieves 1.40x and 1.36x speedup on IA32-to-MIPS64 and X86_64-to-MIPS64 configurations respectively when comparing with the base QEMU version. The result is compared to a representative previous work and shows its advantage.",oceanology,231
1890380a9e14e0a82dc105c9b8ce251107af9ddf,filtered,semantic_scholar,,2020-01-01 00:00:00,semantic_scholar,2nd cfp: 7th ieee international conference on self-adaptive and self-organizing systems (saso2013),https://www.semanticscholar.org/paper/1890380a9e14e0a82dc105c9b8ce251107af9ddf,"submission: May 3, 2013 Paper submission: May 10, 2013 Notification: June 21, 2013 Camera ready copy due: July 19, 2013 Early registration: August 21, 2013 Conference: September 9-13, 2013 ------------------------------Topics of Interest ------------------------------The topics of interest to SASO include, but are not limited to: Self-* systems theory: theoretical frameworks and models; biologicallyand socially-inspired paradigms; inter-operation of self-* mechanisms; Self-* systems engineering: hardware, software and middleware development frameworks and methods, platforms and toolkits; self-* materials; Self-* system properties: robustness, resilience and stability; emergence; computational awareness and self-awareness; reflection; Self-* cyber-physical and socio-technical systems: human factors and visualization; self-* social computers; crowdsourcing and collective awareness; Applications and experiences of self-* systems: cyber security, transportation, computational sustainability, big data and creative commons, power systems. --------------------------------------Submission Instructions --------------------------------------All submissions should be 10 pages and formatted according to the IEEE Computer Society Press proceedings style guide and submitted electronically in PDF format. Please register as authors and submit your papers using the SASO 2013 conference management system. The proceedings will be published by IEEE Computer Society Press, and made available as a part of the IEEE digital library. Note that a separate call for poster submissions has also been issued. ---------------------------Review Criteria ---------------------------Papers should present novel ideas in the cross-disciplinary research context described in this call, clearly motivated by problems from current practice or applied research. We expect both theoretical and empirical contributions to be clearly stated, substantiated by formal analysis, animation or simulation, experimental evaluations, comparative studies, and so on. Appropriate reference must be made to related work. Because SASO is a cross-disciplinary conference, papers must be intelligible and relevant to researchers who are not members of the same specialized sub-field. Authors are also encouraged to submit papers describing applications. Application papers are expected to provide an indication of the real world relevance of the problem that is solved, including a description of the deployment domain, and some form of evaluation of performance, usability, or comparison to alternative approaches. Experience papers are also welcome but they must clearly state the insight into any aspect of design, implementation or management of self-* systems which is of benefit to practitioners and the SASO community. ---------------------------Program Chairs ---------------------------Tom Holvoet KU Leuven, Belgium Jeremy Pitt Imperial College London, England Ichiro Satoh National Institute of Informatics, Tokyo, Japan ? CFP RSP-2013 at ESWeek CfP: Workshop on Model-Driven and Agile Engineering for the Web (MDWE) @ ICWE 2013 ? Calls for Papers CPS Domains Architectures Secure Control Systems Multi-models Communication Embedded Software Model Integration Platforms Systems Engineering Modeling Science of Security Transportation CPS Technologies Announcement",oceanology,232
18e1739f511ab6ca4eba3c836a3ba34a5deb1972,filtered,semantic_scholar,,2020-01-01 00:00:00,semantic_scholar,a reproducible solution for implementing online laboratory systems through inexpensive and open-source technology,https://www.semanticscholar.org/paper/18e1739f511ab6ca4eba3c836a3ba34a5deb1972,"Laboratory experiences are a crucial part of the undergraduate engineering curriculum. With coursework, college programs, and professional interactions increasingly being performed online the natural evolution of a ‘digital-first’ culture suggests that traditionally hands-on educational activities should find themselves represented online as well. Transitioning laboratory-based exercises online is difficult, time consuming, and sometimes costly. In addition, the efficacy of an online laboratory experience as a worthwhile educational tool has not been explored with depth. This study focuses on the details and benefits of incorporating laboratory experiences with online infrastructure with the perspective of optimizing development time and cost. The purpose is to use FOSS (free and open source software) in addition to other open source solutions to develop modular, scalable, and easily deployable remote laboratory infrastructure capable of interacting with traditional equipment over network connections. Introduction It is commonly accepted that one of the best ways to learn technical skills is through hands-on experiences. Be it through apprenticeships, internships, laboratories, or bootcamps, an interactive experience provides concrete, engaging, and fulfilling learning opportunities. By spending time personally carrying out a task, the brain forms neural connections which make it easier to remember and duplicate the task. The understanding of cognition and epistemology has grown throughout the entirety of the history of the human race. Masters pass down skills by having pupils perform those skills according to their instruction. However, with the rise of the digital age, the question becomes, can the dissemination of all concrete knowledge be conducted via computers just as well as through physical interactions. And if so, then how? The Impact of Remote Laboratory Systems on Education The digital world has become an integral part of the lives of faculty and their students and is now irrevocably intertwined with daily routines. As such, society grows ever more comfortable interacting with the world through a digital medium and seeks to find new avenues to do so and new virtual environments to explore. Therefore, it naturally follows that transitioning the whole of education towards a system which is more frequently used by digital natives may be in the best interest of future generations. The purpose of this study is to create a case for implementing remote, on-line laboratory experiences that can successfully fill the same intellectual need as their physical counterparts. The benefit of achieving this goal is similar to that of all on-line instruction, to reach more students and to make education accessible. The chief drawback is that creating the network infrastructure necessary to implement on-line experiences as a substitute for physical laboratory work is difficult and costly. This study also seeks to find and build open-source solutions to this problem using inexpensive hardware, open-source software, and simple network configurations that may add to the list of best practices built by previous and current researchers. Impact on Students Remote laboratory systems offer unique benefits to how students retain information. By providing students with a more open platform to access knowledge, rather than traditional physical interactions, it is possible to see positive effects on engagement and learning. Nabil Lehlou et al. (2009) conducted a study in which students in two different fields (Industrial Statistics and Manufacturing Systems) performed lab exercises and recorded how the students felt they understood the material before and after the lab. The results provided a clear indicator that the students felt the remote lab system provided a beneficial educational experience as six out of eight in Industrial Statistics and eight out of eight students in Manufacturing Systems reported an increase in confidence in the subject material. In addition, five out of eight students in Industrial Statistics and eight out of eight students in Manufacturing Systems reported a drastic improvement in their confidence for their respective fields. A separate study performed by H. Vargas et al. (2010) found similar positive results. They provided 120 students across seven universities with remote laboratory experiences. The research indicated that the full lab experience included performing an actual lab over the internet, which required students to reserve a time to use the lab resources. The response by the students indicated that they enjoyed the system as well as found it useful in understanding the respective course content. According to the results, 69% of the students felt satisfied with the system and 19% felt strongly satisfied. Additionally, 51% of students felt that the remote lab was better than traditional methods, 25% felt it was equal to traditional methods, and 15% felt it was much better than traditional methods. These results strongly assert that remote laboratory experiences not only have a place in the future of education but can have a large impact on its quality. Key Features Needed To better understand what makes remote lab systems effective and their impact on students potent, it is critical to understand what key features are common among these systems. In a study performed by P. Bisták et al. (2011) at the Slovak University of Technology in Bratislava, Slovakia, it was outlined that a remote lab system server could provide the client (user) with text messages, numerical data, graphs, animations, and video clips. The system could interface with sensors and cameras in order to collect useful information and statistics for the client. The setup involved a front-end GUI being served to a client, which in turn communicated over TCP/IP to a remote server. Information could be transmitted in either direction between the server and the client with data and commands running back and forth. The server would have access to the local hardware of the lab system and be able to send any commands received from the client to the hardware. Additionally, it would be able to collect output data from the lab hardware and send it back to the client. Another remote lab study was performed by T.J. Mateo Sanguino et al. (2012) at the University of Huelva in Palos de la Frontera, Spain. In this study a similar setup was implemented with a client providing user access to a remote server, which was in turn connected to a lab system. The user would have the ability to control computer devices on a rack through this setup and perform multiple remote labs. An interesting point to make which differentiates this study from the above is that it does not send photos back to the user. The labs performed did not require cameras or video, instead relying on numerical data to provide the pertinent observation. This is an important point to make as it shows that every lab system is different and there might not be a “generic” or “one-size-fits-all” approach. If this is the case, then a truly reproducible lab system must provide means by which different hardware or software peripherals may be added or removed depending on the needed application. However, at a minimum it appears that a remote laboratory needs a client-server system and some basic means by which to send text or commands between the client and server. Making Labs More Personal As humans are social and emotional creatures, it could be argued that experiences which leverage those traits would aid in the retention of information. It could also help explain why recent concepts such as social media have become fast staples in cultures around the world. They simply exploit the natural desires of people. Similarly, despite being called “remote”, it might be possible to use remote lab systems to improve learning through social, emotional and personal growth. A study performed by C. Terkowsky et al. (2013) at TU Dortmund University in Dortmund, Germany focused on the personalization of the remote laboratory experience. They referenced a theory on education and learning called “Kolb’s Experiential Learning Cycle” wherein multiple stages of learning are introduced. These stages are Concrete Experience, Reflective Observation, Abstract Conceptualization and Active Experimentation. According to the theory, they create the “learning experience”. Armed with this information, the study introduces the concept of an E-Portfolio. This E-Portfolio provides users of remote labs with the ability to record the work they performed and document their findings. The concept of this portfolio does not stop at being a simple digital notebook, however. The study asserts that this portfolio can be used by professors to check on students’ work or be opened to the public in order to add a social dynamic. The study calls the social aspect a “community” and says that it can facilitate learning. To reinforce the main point, by adding a social aspect, be it with classmates or with the world, users will have a greater feeling of connection with their work and might retain more information. Another study performed by Z. Nedic (2013) at the University of South Australia shines light on the collaborative aspect of remote labs. The study saw international students organize themselves autonomously to complete group lab assignments and recorded their planning and communication. The results showed that students, despite being from different countries, exhibited politeness when trying to create social groups and complete the remote labs. The study gives hope to the notion of creating a more connected educational system where students from around the world participate in the same curriculum. This in turn also facilitates international cooperation and communication in the real world, as a large amount of professional communication is done remotely. One study performed by Qing Ding et al. (2017) as joint research between C",oceanology,233
fb70b32eaae4987e7230d03ec82858a8163d66c3,filtered,semantic_scholar,,2020-01-01 00:00:00,semantic_scholar,industrial control system security ( icss ) workshop,https://www.semanticscholar.org/paper/fb70b32eaae4987e7230d03ec82858a8163d66c3,"One of the hardest decisions an asset owner must make when faced with known vulnerabilities or exploits is whether to take down their plant in order to apply patches and upgrade end of life process control components. There are risks if you do (productions loss, opportunity costs, failed upgrades) and (cyber)risks if you do not. In this presentation we will discuss several options that could be considered when presented with known cyber-risks. Note: On the surface this may feel like a standard defense in depth strategy, and in some respects it is. But these strategies are meant to address specific attack techniques, known vulnerabilities and exploits, so it is better to think of these techniques as reactive rather than the defense in depth, proactive approach. Runtime Application Self Protection (RASP) is an emerging collection of approaches to address the fundamental issue with cyber-exploits, that is the ability for malicious processes to access memory where they should not be able. If you control memory access, you control an entire class of exploits (memorybased attacks) Patching – Our most traditional approach to defend against exploits in the wild Signatures – Antivirus is the most common signature-based solution. YARA rules are a way of identifying malware (or other files) by creating rules that look for certain characteristics. There are several signaturebased solutions that can be shared to slow or stop the exploitation of certain vulnerability types. Mitigations – frequently OEMs advise their customers to take specific actions in order to close off known attack vectors. Closing ports on a firewall, disabling unused services, implementing access controls, network segmentation, implementing secure protocols, etc. are all common recommendations to react to specific vulnerabilities. Security Tools – OEMS such as Schneider Electric take time to partner with security tool vendors who often bring their own unique approach to addressing active exploits Network Anomaly detection – similar to signature checking, the ability to identify an exploit on the wire before it reaches the device. Good examples are, “magic” packets that can cause crashes, buffer overflows, RCE, etc. AI/ML – an emerging technology, maligned somewhat today, but do not underestimate how this can and will be used in the future Upgrades – whether this is a component by component upgrade or a rip and replace, one way to eliminate legacy cybersecurity issues is to upgrade to the current generation Biography: Andrew Kling is an Industry Automation Product Security Officer at Schneider Electric. Andy has over three decades of software development experience, having worked in multiple industries. He has worked in the R&D organization of Schneider Electric since 2001. Currently, Andy leads the Industry Automation business unit in cybersecurity. At Schneider Electric, Andy has managed many process control engineering teams. As a result of this experience, Andy has ushered the Schneider Electric Development organization to comply with ISA 62443 standards at the process, product, and system levels, achieving several world firsts along the way. Andy has a Master’s Degree in Software Engineering – Artificial Intelligence from Northeastern University and a Bachelor’s of Science in Information Sciences from the University of Massachusetts, Lowell. In addition, Andy is a participating Senior member of ISA, primarily contributing to the ISA 62443 cybersecurity standards and the ISA Global Cybersecurity Alliance. 9:15 K7: A Protected Protocol for Industrial Control Systems that Fits Large Organizations, Eli Biham, Sara Bitan, and Alon Dankner, Israel Technion ABSTRACT: One of the main obstacles of securing industrial control systems is the lack of an appropriate security model that is both implementable by vendors and addresses the inherent security and usability issues needed by organizations. Current solutions such as device passwords and IPSec lack scalable key management infrastructure and [inc granularity access control mechanisms. In this paper we propose a novel security model for industrial control systems that supports organizational level authorizations and authentication requirements, while hiding the low—level details (e.g., keys and passwords) from the One of the main obstacles of securing industrial control systems is the lack of an appropriate security model that is both implementable by vendors and addresses the inherent security and usability issues needed by organizations. Current solutions such as device passwords and IPSec lack scalable key management infrastructure and [inc granularity access control mechanisms. In this paper we propose a novel security model for industrial control systems that supports organizational level authorizations and authentication requirements, while hiding the low—level details (e.g., keys and passwords) from the users. It also allows to easily add and remove l’I.Cs, engineering stations, HMI devices and users, and assign permissions to them. A major advantage is its support for hybrid ICS systems, and the simple ability to upgrade the security of legacy devices to functionality of new secure protocol. Without loss of generality, we base our protocol, K7, on the Siemens S7 protocol, and enhance it with new cryptographic features to support the extra functionality. We use a ticket-based system (e.g., Kerberos with LDAP server) to support the exchange of permissions and keys, and incorporate it into our protocol. To prove our solution, we implemented K7 as a protocol converter add-on to standard Siemens clients and PLCs that transform them into augmented devices that use K7. We hope that Siemens and other vendors will add direct support for K7 on their ICS systems. 9:40 What and Where to Monitor for Intrusion Detection in Industrial Control Networks, Alvaro Cardenas, University California Santa Cruz ABSTRACT: In this presentation we will look at two related problems for intrusion detection in control systems: where to monitor the system to detect anomalies, and what to monitor, in order to obtain an accurate picture of the real world. We first discuss what we can and cannot detect depending on the location of our network monitor, and identify locations that maximize our visibility to attacks. We also propose the addition of hidden sensor measurements to a system to improve its security. Hidden sensor measurements are by our definition measurements that were not considered in the original design of the system, and are not used for any operational reason. We only add them to improve the security of the system and using them in anomaly detection and mitigation. In this presentation we will look at two related problems for intrusion detection in control systems: where to monitor the system to detect anomalies, and what to monitor, in order to obtain an accurate picture of the real world. We first discuss what we can and cannot detect depending on the location of our network monitor, and identify locations that maximize our visibility to attacks. We also propose the addition of hidden sensor measurements to a system to improve its security. Hidden sensor measurements are by our definition measurements that were not considered in the original design of the system, and are not used for any operational reason. We only add them to improve the security of the system and using them in anomaly detection and mitigation. Biography: Alvaro A. Cardenas is an Associate Professor of Computer Science and Engineering at the University of California, Santa Cruz. Before joining UCSC, he was the Eugene McDermott Associate Professor of Computer Science at the University of Texas at Dallas. Earlier in his career, he was a postdoctoral scholar at the University of California, Berkeley, and a research staff member at Fujitsu Laboratories. He holds M.S. and Ph.D. degrees from the University of Maryland, College Park, and a B.S. from Universidad de Los Andes in Colombia. His research interests focus on cyber-physical systems and IoT security and privacy. He is the recipient of the NSF CAREER award, the 2018 faculty excellence in research award from the Erik Johnson School of Engineering and Computer Science, and the Eugene McDermott Fellow Endowed Chair at the University of Texas at Dallas. He has also received best paper awards from the IEEE Smart Grid Communications Conference and the U.S. Army Research Conference. One of his papers was also a finalist to the CSAW competition in Israel. 10:05 Break (30 minutes) 10:35 Securing Critical Infrastructure: Challenges and Opportunities, Jianying Zhou, Singapore University of Technology and Design ABSTRACT: Critical infrastructure becomes a strategic target in the midst of a cyber-war. Governments are investing significantly in response to the risks and challenges while researchers and vendors are aggressively developing and marketing new technologies aimed at protecting critical infrastructure. In this talk, I will briefly describe the framework and features of a cyber-physical system (CPS) which serves as the core to provide critical services in different industrial domains. Then I will discuss the challenges we face and the approaches we can take to defend against cyber attacks. After that I will present a few novel technologies developed in iTrust for preventing and detecting attacks to CPS. I will further introduce the fully operational CPS testbeds in iTrust, and show how the testbeds are used to validate the security technologies so that the owners and operators of critical infrastructure can be confident that the technologies to be deployed will actually protect their systems in the event of a cyber-war. Critical infrastructure becomes a strategic target in the midst of a cyber-war. Governments are investing significantly in response to the risks and challenges while researchers and vendors are aggressively developing and marketing new technologies aimed at protecting critical infrastru",oceanology,234
e8ad5f723fe13574614be08c5a8aeed5602e7b97,filtered,semantic_scholar,,2020-01-01 00:00:00,semantic_scholar,vulnerability discovery in embedded systems,https://www.semanticscholar.org/paper/e8ad5f723fe13574614be08c5a8aeed5602e7b97,"Objective Embedded systems are omnipresent in our everyday life. For example, they are the core of various Common-Off-The-Shelf (COTS) devices such as printers, mobile phones, home appliances, and computer components and peripherals. They are also present in many devices that are less consumer oriented such as video surveillance systems, medical implants, automotive elements, military systems, SCADA and PLC devices, and basically anything we usually call “electronics”. Moreover, the emerging phenomenon of the Internet-of-Things (IoT) will make them even more widespread and interconnected. The security and reliability of this broad range of devices is paramount to ensure both the proper functioning of our society and our physical safety. Unfortunately, embedded devices did not reach yet the same level of security we obtained for software of typical personal computers. For example, because of the very heterogeneous set of platforms and architectures, embedded systems still lack a solid set of vulnerability discovery and analysis techniques. The goal of this thesis is to bridge this gap by improving the state of the art of vulnerability discovery in software binaries. In particular, the student will focus on the development of novel static and dynamic analysis techniques that can be applied to the study of real-world, complex firmware images. The proposed approach will need to cope with a number of challenges, ranging from scalability issues, heterogeneity of targets, need for low false positive rates, and the intrinsic difficulty of running dynamic analysis on real embedded devices. To ensure the deployability of the developed techniques, real examples and test cases for the Ph.D. research will be provided by a close industrial support and collaboration with Siemens. Background and PreviousWork The work performed in this thesis builds upon two lines of research which are ongoing in our group at Eurecom. The first is related to the use and application of advanced dynamic analysis techniques on the firmware of embedded devices. Zaddach et al. designed and implemented and open source system named Avatar [1], whose goal is to execute a firmware inside an instrumented emulator. Emulating firmwares of embedded devices requires accurate models of all hardware components used by the system under analysis. Unfortunately, the lack of documentation and the large variety of hardware on the market make this approach infeasible in practice. Avatar fills this gap and overcomes the limitation of pure firmware emulation by acting as an orchestration engine between the physical device and an external emulator [7]. By injecting a 1 special software proxy in the embedded device, Avatar can execute the firmware instructions inside the emulator while channeling the I/O operations to the physical hardware. Since it is infeasible to perfectly emulate an entire embedded system and it is currently impossible to perform advanced dynamic analysis by running code on the device itself, Avatar takes a hybrid approach. It leverages the real hardware to handle I/O operations, but extracts the firmware code from the embedded device and emulates it on an external machine. A similar architecture, but supported by a FPGA bridge to increase the throughput, was used by Koscher et al. [5] in their Surrogates system.",oceanology,235
c613158391c247d42c45c79a83120a817183611b,filtered,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,proposition de recherche doctorale vulnerability discovery in embedded systems,https://www.semanticscholar.org/paper/c613158391c247d42c45c79a83120a817183611b,"Objective Embedded systems are omnipresent in our everyday life. For example, they are the core of various Common-Off-The-Shelf (COTS) devices such as printers, mobile phones, home appliances, and computer components and peripherals. They are also present in many devices that are less consumer oriented such as video surveillance systems, medical implants, automotive elements, military systems, SCADA and PLC devices, and basically anything we usually call “electronics”. Moreover, the emerging phenomenon of the Internet-of-Things (IoT) will make them even more widespread and interconnected. The security and reliability of this broad range of devices is paramount to ensure both the proper functioning of our society and our physical safety. Unfortunately, embedded devices did not reach yet the same level of security we obtained for software of typical personal computers. For example, because of the very heterogeneous set of platforms and architectures, embedded systems still lack a solid set of vulnerability discovery and analysis techniques. The goal of this thesis is to bridge this gap by improving the state of the art of vulnerability discovery in software binaries. In particular, the student will focus on the development of novel static and dynamic analysis techniques that can be applied to the study of real-world, complex firmware images. The proposed approach will need to cope with a number of challenges, ranging from scalability issues, heterogeneity of targets, need for low false positive rates, and the intrinsic difficulty of running dynamic analysis on real embedded devices. To ensure the deployability of the developed techniques, real examples and test cases for the Ph.D. research will be provided by a close industrial support and collaboration with Siemens. Background and PreviousWork The work performed in this thesis builds upon two lines of research which are ongoing in our group at Eurecom. The first is related to the use and application of advanced dynamic analysis techniques on the firmware of embedded devices. Zaddach et al. designed and implemented and open source system named Avatar [1], whose goal is to execute a firmware inside an instrumented emulator. Emulating firmwares of embedded devices requires accurate models of all hardware components used by the system under analysis. Unfortunately, the lack of documentation and the large variety of hardware on the market make this approach infeasible in practice. Avatar fills this gap and overcomes the limitation of pure firmware emulation by acting as an orchestration engine between the physical device and an external emulator [7]. By injecting a 1 special software proxy in the embedded device, Avatar can execute the firmware instructions inside the emulator while channeling the I/O operations to the physical hardware. Since it is infeasible to perfectly emulate an entire embedded system and it is currently impossible to perform advanced dynamic analysis by running code on the device itself, Avatar takes a hybrid approach. It leverages the real hardware to handle I/O operations, but extracts the firmware code from the embedded device and emulates it on an external machine. A similar architecture, but supported by a FPGA bridge to increase the throughput, was used by Koscher et al. [5] in their Surrogates system.",oceanology,236
5516eea2b00bba86118be6f90d7c678b4c022226,filtered,semantic_scholar,,2020-01-01 00:00:00,semantic_scholar,developments in the united kingdom road transport from a smart cities perspective,https://www.semanticscholar.org/paper/5516eea2b00bba86118be6f90d7c678b4c022226,"Purpose: Smart city is a city which functions in a sustainable and intelligent way, by integrating all of its infrastructures and services in a cohesive way using intelligent devices for monitoring and control, to ensure efficiency and better quality of life for its citizens. As other countries globally, UK is keen for economic development and investment in smart city missions to create interest in monetary environment and inward investment. This paper explores the driving forces of smart road transport transformation and implementation in the UK. Design/methodology/approach: The study involved interviews with sixteen professionals from the UK road transport sector. A semi-structured interview technique was used to collect experts’ perception, which was then examined using content analysis. Findings: The results of the study revealed that the technological advancement is a key driver. The main challenges faced for the implementation of smart city elements in the UK road network are: lack of investment; maintenance; state of readiness and the awareness of the smart road transport concept. The study concludes that an understanding of the concept of smart cities from a road transport perspective is very important to create awareness of the benefits and the way it works. A wider collaboration between every sector is crucial to create a successful smart city. Originality/value: The study contributes to the field of digitalisation of road transport sector. This paper reveals the key driving forces of smart road transport transformation, the current status of smart road transport implementation in UK and challenges of the smart road transport development in the UK. Engineering, Construction and Architectural Management http://mc.manuscriptcentral.com/ecaam 2 Introduction Enormous global urbanisation and growth has caused migration of people in urban areas and spatial development of urban infrastructure. According to Obaidat and Nicopolitidis (2016), 85-90% of the world population evolution is a result of a 21 st century cities. Therefore, smart cities are being created from scratch or being developed gradually by improving the prevailing cities infrastructure and primary resources. A study of McKinsey Global Institute’s by Department for Business innovation and Skills (2013) shows that more than 50% of the global GDP (Gross Domestic Product) is generated in the 190 major cities in the developed countries compare to 22 largest cities in the developing countries. However, the increase of growth in the developed countries is healthy but this phenomena also set a high expectation on public services and the quality of the urban infrastructure and environment. Due to the urbanisation, more problems such as overpopulation, pollutions, scarcity of natural resources, public and private services are being created (Yigitcanlar et. al. 2020, Dameri, 2014). Smart cities are an emerging strategy to mitigate the problems generated by the rapid urban population growth and rapid urbanization (Prakash, 2019). A ‘smart city’ is defined as an urban area that is highly innovative in terms of overall facilities, ecological real estate, communication and market feasibility (Chirisa et al, 2016). Also, smart cities is defined as “a place where the traditional networks and services are made more efficient with the use of digital and telecommunication technologies, for the benefits of its inhabitants and businesses” (Kumar et al., 2018). Whereas BSI (2014) noted that smart cities as an effective integration of physical, digital and human systems in a built environment to deliver sustainable, prosperous and inclusive future for its citizens. From the above three definitions, it could be inferred that smart cities are cities that make use of physical, digital and human systems in order to enable sustainability and efficiency for the citizens within that city. The economic growth of any country is supported by its good infrastructure. The United Kingdom (UK) has strategic roads, railway and airports; however, it is one of the top 10 most congested country in the world (Korosec, 2018). According to ONS (2017), the population in the UK in 2016 was 65.6 million which was the largest ever. It is also projected that the population in the UK would grow, reaching over 74 million by 2039. Due to the population rise amalgamated with increase of cars on the road, which has increased by 4.6% higher than the previous peak in 2016 (Department of Transport, 2017), the present UK transport system faces many challenges. The UK road transportation system is gradually taking steps to overcome the problems. As road transport is a significant source of both safety and environmental concerns. This research aims to explore the driving forces of smart road transport transformation, and implementation in the UK along with the challenges faced. In doing so the next section delves into the relevant literature review followed by methodology and findings. The paper finally concludes with recommendations. 2. Literature review An extensive review of literature on drivers, current status of smart cities and the challenges are discussed in this section. Three main drivers include: technology, environment, and socioeconomy (See Table 1). The technological advancement is clearly seen as strong impact on the cities in the recent years. It can be seen clearly that, the communication technologies (ICT) develops city management, enhances technical and social networks, and urban affordability. Within the technology as a driver, the review of literature revealed that technologies such as Big Data, Internet of Things and Artificial Intelligence are widely Engineering, Construction and Architectural Management http://mc.manuscriptcentral.com/ecaam 3 implemented within the smart cities context. However, there is a lack of studies focusing on the UK smart road transport sector. Table 1: Literature on classification of drivers of smart cities development Drivers Description Source Technology Big data Big volume of digital data contents through online services such as Facebook, Twitter, You Tube, Instagram and SnapChat Olshannikova et al , 2017 Data is transmitted to various smart applications through sensor devices or other cloud computing integrated devices Hashem et al , 2016 Big data development highlights information and communication tools in the cyber physical farm management cycle and it identifies the interconnection related to socio-economic challenges Wolfert et al , 2017 Internet of Things IoT is widely in use for many multimedia application, smart transportation system and smart city design and deployment issues KeertiKumar et al , 2016 The implementation of IoT in transportation system means underlying technology and creating smart environments to increase their efficiency in tackling the transportation and environmental issues Kyriazis et al , 2013 Artificial Intelligence Artificial Intelligence (AI) is a technology that is influencing every pace of life which enable people to reconsider how to integrate information, analyse data, and real time data usage for the purpose of improve decision making West and Allen , 2018 Artificial neutral networks, expert system and hybrid intelligent system are computer based modelling tools that have recently aroused and found extensive recognition in many discipline for modelling complex real-world problem. Bahrammirzaee , 2010 China is a leading competitor and primarily focusing in the use of AI in military vehicles. While, Russia actively looking for opportunity in AI development and focused on robots in military Hoadley and Lucas , 2018 Environment Songdo, the Korean model of smart city was subjected to become one of the sustainable smart cities around the world Yigitcanlar , et al, 2018 Songdo, consist 40% of parks and green spaces and waste processing centre placed by the underground system and to recycle Benedikt , 2016 ; Shwayri , 2013 European Union aimed to reduce greenhouse gas emission in urban design through the implementation of innovation technologies Ahvenniemi et al , 2017 Engineering, Construction and Architectural Management http://mc.manuscriptcentral.com/ecaam 4 Cities have been setting high expectation on reaching the target of creating a clean future as shared by Covenant of Mayors’ vision for 2050 to accelerate the decarbonisation Covenant of Mayors for Climate & Energy , 2018 Socioeconomy By 2050, six hundred cities will generate almost 65% of world economic growth by contribute to a higher global GDP Dobbs et al, 2012 Smart cities indicated as the next evolution of ‘new community management where urban problems converted into opportunities for business investment and profit earning Anand and Marcco , 2018 Global smart cities market size to grow reaching USD 2.57 trillion by 2050 Grand View Research , 2018 Governments are obligated to protect citizen and their control over the active connection of local public groups, the police force, and the citizen such as the senior and disabled Neirotti et al ,2014 Level of observing has been focused by an increasing safety and security that desires to manage risks Kitchin , 2014 Safety and security features implemented with help of big data and data controls centres that joined and bind data stream collectively for example the installation of CCTV and street monitored camera are to monitor activity remotely Firmino and Duarte, 2014 Big data is a trend of utilising software tools to store data and share data collected with the use of technology. Nowadays, big data has been a tool that facilitating individual, businesses and government to discover new solutions to certain problems. Data is a crucial aspect as when an asset is built, asset management goes on and the more data collected in, the asset is being constructed, the more the asset can be maintained in an efficient manner (Loshin, 2018). Furthermore, KeertiKumar et al (2016) mention that IoT is widely used for many multim",oceanology,237
3a7a35c3e64c8e4a546ad3656ffb78cc92f42a7a,filtered,semantic_scholar,,2018-01-01 00:00:00,semantic_scholar,a review on intrusion detection and its analysis,https://www.semanticscholar.org/paper/3a7a35c3e64c8e4a546ad3656ffb78cc92f42a7a,"Now a day wireless detection network could be a unit that is loosely used in environmental management, investigation tasks, observing military applications, health connected applications, pursuit and dominant etc. A wireless intrusion detection additionally aids among the detection of a range of attacks. so as to identify gaps and attacks in wireless network intrusion detection analysis, this paper survey the literature of this area. This paper is to classify existing up to date wireless intrusion detection system (IDS); techniques based on target wireless network, detection technique, assortment method, trust model and analysis technique. This paper summarize pros and cons of a similar or differing types of considerations and concerns for wireless intrusion detection with respect to specific attributes of target wireless networks together with wireless local area networks (WLANs), wireless personal area networks (WPANs), wireless sensor networks (WSNs), ad hoc networks, mobile telecommunication, wireless mesh networks (WMNs) and cyber physical systems (CPSs). This paper is to outline the fundamentals of intrusion detection in wireless network, describing the kinds of attacks and state the motivation for intrusion detection in wireless network. A REVIEW ON INTRUSION DETECTION AND ITS ANALYSIS Pradeep Kumar, N. Sreema Iswarya, S. Sharmila Jeyarani 318 [1] INTRODUCTION Intrusion detection is a significant exploring topic with many prospective applications. Along with intrusion prevention, response and tolerance, intrusion detection is one tool that can defend against the real-world cyber attacks threatening critical systems. Vulnerabilities in most computer systems. And, it can be exploited by either non authorized or authorized users. Having said that, several tools are being designed and implemented for a variety of exploitations in diverse range of security attacks. Among these tools is the intrusion detection systems (IDS) which allow us to monitor a range of computer systems: an information system, a network or a cloud computing. These IDS detect intrusions and defined as attempts to break the security objectives such as confidentiality, integrity and availability and non-repudiation. The objective of this paper is to compare the different type of intrusion detection systems and describe their mode of use. In addition, we will include the different approaches currently proposed by others on IDS system, network and cloud computing based vulnerabilities in most computer systems. And, it can be exploited by either non authorized or authorized users. [2] LITERATURE REVIEW Wireless networks are not immune to the risks of destruction and decommissioning. Some of these risks are identical to those in Ad-Hoc networks, and others are specific to the sensors. Several articles [1][2][3][4][5] have presented security attacks and issues in WSNs. Intrusion detection system (IDS) defined as the second line of defense after cryptography, allows the detection and prevention of internal and external attacks. In [6], it is presented a Rule-based IDS called also Signature-based. Most of the techniques in these schemes follow three main phases: data acquisition phase, rule application phase and intrusion detection phase. In [7], it is proposed two approaches to improve the security of clusters for sensor networks using IDS. The first approach uses a model-based on authentication, and the second scheme is called Energy-Saving. IN [8] a hybrid intrusion detection system (HIDS) model has been anticipated for wireless sensor networks. The paper does not promote a solution. Rather, it is a comparative study of existing model of intrusion detection in wireless sensor networks. The paper aim is to provide a better understanding of the current research issues in this field. The paper [9] focus on detecting intrusion or anomalous behavior of nodes in WLAN’s Using a modular technique. We explore the security vulnerabilities of 802.11, numerous intrusion detection techniques, and different network traffic metrics also called as features. Based on the study of metrics, propose a modular based intrusion detection approach. The intrusion detection is a mechanism for a WSN to detect the existence of improper, inaccurate, or anomalous moving attackers. In the paper [10], consider the issue according to heterogeneous WSN models. Furthermore, consider two sensing detection models: single-sensing detection and multiplesensing detection. [3] SURVEY ON SECURITY ATTACKS AND INTRUSION TYPES 3.1 INTRUDER TYPE: Computer security specialists normally distinguish between internal and external network attacks. This is because intruder profiles, methods of attack and intruder objectives can vary significantly between internal and external attacks. International Journal of Computer Engineering and Applications, Volume XII, Issue I, Jan. 18, www.ijcea.com ISSN 2321-3469 Pradeep Kumar, N. Sreema Iswarya, S. Sharmila Jeyarani 319 3.1.1 External intruder  External intruder attacks can be made against the internal network, using the target’s own computers.  This is often done with the active or passive collusion of the members of the target’s own staff.  However, if the ultimate initiator of the attacks is someone holding no legitimate privileges on the network, then it is considered an external attack.  Attacks where the intruder has no privileges on the target network, and either gains access from outside the network perimeter or by evading or undermining the target’s physical and/or network security measures to achieve some degree of access to the target’s internal network. 3.1.2 Internal intruder  Attacks where the intruder has legitimate privileges on the target network.  Access is obtained using existing privileges, privileges the intruder has extended without permission, or privileges stolen from other users.  To gain access to data and resources to which the intruder is not authorized.  Internal attacks are typically far more common than external ones. 3.2 INTRUSION TYPE Several kinds of IDS technologies exist because of the variance of network configurations. Mainly, there are 3 necessary distinct families of IDS: the categories of IDPS technologies are differentiated primarily by the types of events that they monitor and therefore the ways that during which they're deployed. 3.2.1 Attempted break-in  A Firewall has the task to examine data traffic across borders between networks, and to reject those packets, which do not have a permission for transmission.  Beside attempts to access directly a computer in the protected network, there are also attacks against the Firewall itself, or attempts to outwit a Firewall with falsified data packets. Such break-in attempts are recognized, repelled and logged by the Intrusion Detection system (IDS).  Thereby it can be selected between logging within the device, email notification, SNMP traps or SYSLOG alarms. IDS checks the data traffic for certain properties and detects in this way also new attacks proceeding with conspicuous patterns. 3.2.2 Masquerade  A masquerade attack is an attack that uses a fake identity, such as a network identity, to gain unauthorized access to personal computer information through legitimate access identification.  If an authorization process is not fully protected, it can become extremely vulnerable to a masquerade attack. 3.2.3 Leakage  DoS Attack designed to cause an interruption or suspension of services of a specific host/server by flooding it with large quantities of useless traffic or external communication requests.  When the DoS attack succeeds the server is not able to answer even to legitimate requests any more this can be observed in numbers of ways: slow response of the server, slow network performance, unavailability of software or web page, inability to access data, website or other A REVIEW ON INTRUSION DETECTION AND ITS ANALYSIS Pradeep Kumar, N. Sreema Iswarya, S. Sharmila Jeyarani 320 resources.  Distributed Denial of Service Attack (DDoS) occurs where multiple compromised or infected systems (botnet) flood a particular host with traffic simultaneously. 3.2.4 Phishing attack  This type of attack use social engineering techniques to steal confidential information the most common purpose of such attack targets victim's banking account details and credentials.  Phishing attacks tend to use schemes involving spoofed emails send to users that lead them to malware infected websites designed to appear as real on-line banking websites.  Emails received by users in most cases will look authentic sent from sources known to the user (very often with appropriate company logo and localised information) those emails will contain a direct request to verify some account information, credentials or credit card numbers by following the provided link and confirming the information online.  The request will be accompanied by a threat that the account may become disabled or suspended if the mentioned details are not being verified by the user. 3.3 DETECTION METHODOLOGIES The Attempt The d Information Leak rule deals with signatures from potentdetection method defines the characteristics of analyzer. It is categorized on the basis of information being used by IDS. 3.3.1 Anomaly based detection  The anomaly based detection is based on defining the network behavior.  The network behavior is in accordance with the predefined behavior, then it is accepted or else it triggers the event in the anomaly detection.  The accepted network behavior is prepared or learned by the specifications of the network administrators. 3.3.2 Misuse based detection  Misused based detection involves searching network traffic for a series of malicious bytes or packet sequences.  The main advantage of this technique is that signatures are very easy to develop and understand if we know what network behavior we are trying to identify.  For instance, we might use a signature that looks for part",oceanology,238
3b71fb8879e0d5f480b1379f58b902e904b743fb,filtered,semantic_scholar,,2018-01-01 00:00:00,semantic_scholar,towards network softwarization: a modular approach for network control delegation. (une approche modulaire avec délégation de contrôle pour les réseaux programmables),https://www.semanticscholar.org/paper/3b71fb8879e0d5f480b1379f58b902e904b743fb,"Network operators are facing great challenges in terms of cost and complexity in order to incorporate new communication technologies (e.g., 4G, 5G, fiber) and to keep up with increasing demands of new network services to address emerging use cases. Softwarizing the network operations using SoftwareDefined Networking (SDN) and Network Function Virtualization (NFV) paradigms can simplify control and management of networks and provide network services in a cost effective way. SDN decouples control and data traffic processing in the network and centralizes the control traffic processing to simplify the network management, but may face scalability issues due to the same reasons. NFV decouples hardware and software of network appliances for cost effective operations of network services, but faces performance degradation issues due to data traffic processing in software. In order to address scalability and performance issues in SDN/NFV, we propose in the first part of the thesis, a modular network control and management architecture, in which the SDN controller delegates part of its responsibilities to specific network functions instantiated in network devices at strategic locations in the infrastructure. We have chosen to focus on a modern application using an IP multicast service for live video streaming applications (e.g., Facebook Live or Periscope) that illustrates well the SDN scalability problems. Our solution exploits benefits of the NFV paradigm to address the scalability issue of centralized SDN control plane by offloading processing of multicast service specific control traffic to Multicast Network Functions (MNFs) implemented in software and executed in NFV environment at the edge of the network. Our approach provides smart, flexible and scalable group management and leverages centralized control of SDN for Lazy Load Balance Multicast (L2BM) traffic engineering policy in software defined ISP networks. Evaluation of this approach is tricky, as real world SDN testbeds are costly and not easily available for the research community. So, we designed a tool that leverages the huge amount of resources available in the grid, to easily emulate such scenarios. Our tool, called DiG, takes into account the physical resources (memory, CPU, link capacity) constraints to provide a realistic evaluation environment with controlled conditions. Our NFV-based approach requires multiple application specific functions (e.g., MNFs) to control and manage the network devices and process the related data traffic in an independent way. Ideally, these specific functions should be implemented directly on hardware programmable routers. In this case, new routers must be able to execute multiple independently developed programs. Packet-level programming language P4, one of the promising SDN-enabling technologies, allows applications to program their data traffic processing on P4 compatible network devices. In the second part of the thesis, we propose a novel approach to deploy and execute multiple independently developed and compiled applications programs on the same network device. This solution, called P4Bricks, allows multiple applications to control and manage their data traffic, independently. P4Bricks merges programmable blocks (parsers/deparsers and packet processing pipelines) of P4 programs according to processing semantics (parallel or sequential) provided at the time of deployment.",oceanology,239
19f66130a77d96ab599e51cf67ae00fc2064fe6a,filtered,semantic_scholar,,2009-01-01 00:00:00,semantic_scholar,a survey on cloud computing,https://www.semanticscholar.org/paper/19f66130a77d96ab599e51cf67ae00fc2064fe6a,"Cloud computing provides customers the illusion of infinite computing resources which are available from anywhere, anytime, on demand. Computing at such an immense scale requires a framework that can support extremely large datasets housed on clusters of commodity hardware. Two examples of such frameworks are Google’s MapReduce and Microsoft’s Dryad. First we discuss implementation details of these frameworks and drawbacks where future work is required. Next we discuss the challenges of computing at such a large scale. In particular, we focus on the security issues which arise in the cloud: the confidentiality of data, the retrievability and availability of data, and issues surrounding the correctness and confidentiality of computation executing on third party hardware. Today, the most popular applications are Internet services with millions of users. Websites like Google, Yahoo! and Facebook receive millions of clicks daily. This generates terabytes of invaluable data which can be used to improve online advertising strategies and user satisfaction. Real time capturing, storage, and analysis of this data are common needs of all high-end online applications. To address these problems, a number of cloud computing technologies have emerged in last few years. Cloud computing is a style of computing where dynamically scalable and virtualized resources are provided as a service over the Internet. The cloud refers to the datacenter hardware and software that supports a clients needs, often in the form of datastores and remotely hosted applications. These infrastructures enable companies to cut costs by eliminating the need for physical hardware, allowing companies to outsource data and computations on demand. Developers with innovative ideas for Internet services no longer need large capital outlays in hardware to deploy their services; this paradigm shift is transforming the IT industry. The operation of large scale, commodity computer datacenters was the key enabler of cloud computing, as these datacenters take advantage of economies of scale, allowing for decreases in the cost of electricity, bandwidth, operations, and hardware [Armbrust et al. 2009]. It is well known that writing efficient parallel and distributed applications is complex. Google proposed the MapReduce [Dean and Ghemawat 2004] programming framework in 2004 to address this complexity. It allows programmers to specify a map function that processes a key/value pair to generate an intermediate key/value pairs, and a reduce function that merges all the intermediate key/value pairs to produce the required output. Many real world tasks, especially in the domain of search can be expressed in this model. Hadoop 1 is the most popular open source implementation of MapReduce. It has been widely adopted both in academic and industrial users, including at organiza",oceanology,240
04dc3ef332c7f83e66f50653945e621d4bf403b1,filtered,semantic_scholar,ISC Workshops,2017-01-01 00:00:00,semantic_scholar,simulation of hierarchical storage systems for tco and qos,https://www.semanticscholar.org/paper/04dc3ef332c7f83e66f50653945e621d4bf403b1,"Due to the variety of storage technologies deep storage hierarchies turn out to be the most feasible choice to meet performance and cost requirements when handling vast amounts of data. Long-term archives employed by scientific users are mainly reliant on tape storage, as it remains the most cost-efficient option. Archival systems are often loosely integrated into the HPC storage infrastructure. In expectation of exascale systems and in situ analysis also burst buffers will require integration with the archive. Exploring new strategies and developing open software for tape systems is a hurdle due to the lack of affordable storage silos and availability outside of large organizations and due to increased wariness requirements when dealing with ultra-durable data. Lessening these problems by providing virtual storage silos should enable community-driven innovation and enable site operators to add features where they see fit while being able to verify strategies before deploying on production systems. Different models for the individual components in tape systems are developed. The models are then implemented in a prototype simulation using discrete event simulation. The work shows that the simulations can be used to approximate the behavior of tape systems deployed in the real world and to conduct experiments without requiring a physical tape system.",oceanology,241
877faa6486db570bdbe7aa24d5b40cac6017843d,filtered,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,software and system engineering for cyber-physical systems: technical challenges and collaboration opportunities,https://www.semanticscholar.org/paper/877faa6486db570bdbe7aa24d5b40cac6017843d,"of presentations Holger Pfeifer (FORTISS) – The European Smart Anything Everywhere initiative and funding opportunities by CPSE-Labs experiments The European ‘Smart Anything Everywhere’ (SAE) initiative supports the innovation on smart digital systems thanks to networks of competence centres. The ecosystems built under these initiatives are based on collaboration between researchers, large industries and SMEs which will help to transfer knowledge and resources available to a much wider group of companies. SMEs and middle size companies can experiment with new technologies, try them out in their processes and work together with the suppliers of the technology to adapt it to their specific needs. CPSELabs is one SAE innovation action which provides an open forum for sharing platforms, architectures and SW tools for the engineering of dependable and trustworthy CPS. It provides funding for focussed experiments (36 partners) and fast-track (12-18 months) with innovation objective. Next call for experiment will be published in Spring 2016 http://www.cpse-labs.eu/calls.php Holger Pfeifer (FORTISS) CPSE-Labs experiments of Germany South centre: Model-driven engineering for industrial automation systems The importance of software in industrial automation is continuously increasing. New approaches to the development and maintenance are needed to cope with the growing complexity of control software for future automation systems. 4DIAC Framework for Distributed Industrial Automation & Control is an open source solution for the programming of programmable logic controllers (PLCs) according to the standard IEC 61499. With this standard it provides higher level modelling means and better support for networked control devices. The main components of 4DIAC are the Eclipse-based integrated development environment 4DIAC-IDE and the real-time capable run-time environment FORTE. Martin Grimheden (KTH) – CPSE-Labs experiments of Sweden centre: Overcoming thresholds for data integration in CPS engineering environments The talk will describe the Kth model-based approach to data integration based on the OSLC interoperability standard. Patrick Leserf (ESTACA) Trade-off analysis with SysML and Papyrus : a drone application Obtaining the set of trade-off architectures from a SysML model is an important objective for the system designer. To achieve this goal, we propose a methodology combining SysML with the variability concept and multiobjectives optimization techniques. An initial SysML model is completed with variability information to show up the different alternatives for component redundancy and selection from a library. The constraints and objective functions are also added to the initial SysML model, with an optimization context. Then a representation of a constraint satisfaction problem (CSP) is generated with an algorithm from the optimization context and solved with an existing solver. The presentation will illustrate our methodology by designing an Embedded Cognitive Safety System (ECSS). From a component repository and redundancy alternatives, the best design alternatives are generated in order to minimize the total cost and maximize the estimated system reliability. Benoît Combemale (IRISA) Using models for a broader engagement in smart systems. Various disciplines use models for different purposes. While engineering models, including software engineering models, are often developed to guide the construction of a non-existent system, scientific models, in contrast, are created to better understand a natural phenomenon (i.e., an already existing system). An engineering model may incorporate scientific models to build a smart system. This talk proposes a vision promoting an approach that synergistically combines engineering and scientific models to enable broader engagement of end users in smart systems, informed decision-making based on more-accessible scientific models and data, and automated feedback to the engineering models to support dynamic adaptation of smart systems. To support this vision, we identify a number of challenges to be addressed with particular emphasis on the socio-technical benefits of modeling. Claire Ingram (Newcastle University) CPSE-Labs experiments of UK centre: Pragmatic techniques for modelbased Engineering of Cyber-Physical Systems Newcastle University's Cyber-Physical Systems Lab conducts research into pragmatic techniques for model-based engineering of Cyber-Physical Systems (CPSs). In this talk I will introduce some platforms supported by Newcastle's CPS Lab, including an approach for co-modelling which allows separate design teams working with discrete-event and continuous-time formalisms to develop CPS designs collaboratively. I will also introduce an experiment which has been funded previously under the CPSE Labs initiative. Michael Siegel (OFFIS) CPSE-Labs experiments of Germany North centre: The Maritime Architecture Framework (MAF) and eMaritime Integrated Reference Platform (eMIR) The Maritime Architecture Framework (MAF) is a stakeholder-oriented CPS architecture framework for existing and future maritime CPS and services. MAF provides the conceptual basis, methods, tools and technologies to define, document, and align existing or future CPS architectures and architectural reference models for e-navigation and e-maritime applications. The MAF is a key enabler in the maritime domain for system harmonization, interoperability and standardization. The MAF is also a reference for the definition and design of testbeds for enavigation and e-maritime systems and services. It helps to define the context, to check completeness and provides a semantic basis to discuss the outcome and results. Additionally it offers a semantic basis for integration of testbeds e.g. for larger demonstrators. To support the development of maritime CPS – i.e. the integration of heterogeneous systems of the maritime transportation space , the Design Centre North provides the eMaritime Integrated Reference Platform (eMIR) for rapid prototyping in simulation environments and testing in real world environments. This talk gives an overview about the background, context and concepts of the MAF and why testbed environments (e.g. eMIR) for the development, integration testing and demonstration for CPS must take into account and support the design aspects of such an architecture framework. Andre Pierre Mattei (SCA-ITA) SysML Design of an observation satellite for agriculture surveillance in Brazil François Fouquet (SnT, Interdisciplinary Centre for Security, Reliability and Trust)Models for managing IoT data Internet of Things applications analyze our past habits through sensor measurements to anticipate future trends. To yield accurate predictions, intelligent systems not only rely on single numerical values, but also on structured models aggregated from different sensors. Computation theory, based on the discretization of observable data into timed events, can easily lead to millions of values. Time series and similar database structures can efficiently index the mere data, but quickly reach computation and storage limits when it comes to structuring and processing IoT data. During this talk, I will present various results presented at Models’15 and SmartGridCom’15 that tackles this complexity by exploiting IoT data characteristics. Notably, I will present a concept of continuous models that can handle high-volatile IoT data by defining a meta type for continuous attributes. In addition to traditional discrete object-oriented modeling APIs, we enable models to represent very large sequences of sensor values by inferring mathematical models that can efficiently replace raw values. We show on various IoT datasets that sequences of polynomials can significantly improve storage and reasoning efficiency. I will present an application of this IoT model extension for suspicious value detection in the SmartGrid domain. We proposed a method to learn and store a profile of “typical” values and their probability in IoT context models. We show that using such profiles together with a contextual checker we can improve suspicious value detection, both in terms of efficiency and effectiveness. Juan Garbajosa (UPM) Experiments of Spain centre: Open CPS platform for building and deploying smart city services Bran Selic (Simula) Modeling uncertainty in cyber-physical systems For the past year, we have been working on a core model of Uncertainty and its application to requirements specification and system testing in the context of the European Commission's H2020 UTEST project. (More information on this project, which involves a number of industrial and research partners from Europe, can be found at: http://certus-sfi.no/u-test-a-horizon-2020-funding-recipient/). Fabien Peureux Fabien Peureux (Femto-st/EGM/Smartesting S&S) Model-Based Testing for Internet of Things and Cyber-Physical Systems Testing Cyber-Physical Systems (CPS) is challenging due to the various uncertainties in their behaviour. The purpose of this talk is to present our ongoing work on a model-based testing framework for automatic generation of executable test cases for CPS in the presence of various uncertainties. Basically, uncertainties can be described as a lack of certainty about the current state or about the future outcome of the system. Within CPS, it can be due to the stimulus and data sent from the user environment to the physical units (application level), to the interactions between the physical units and the network services (infrastructure level), or a combination of the both (integration level). To test such issues, the proposed model-based testing approach is implemented on the EMF framework, and based on the test generation tool Smartesting CertifyIt1. It relies on a UML behavioral model of the system under test, from which abstract test cases are automatically generated by applying dedicated coverage strategies focusing on uncertainty testing. Afterwards, ",oceanology,242
1e4380f2aec7922062899b4becff1c1db53679c7,filtered,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,modeling and simulation of tape libraries for hierarchical storage systems,https://www.semanticscholar.org/paper/1e4380f2aec7922062899b4becff1c1db53679c7,"The variety of storage technologies results in deep storage hierarchies to be the only feasible choice to meet performance and cost requirements when handling vast amounts of data. Long-term storage systems employed by scientific users are mainly reliant on tape storage, as it remained the most costefficient option. Archival systems are often loosely integrated into the HPC storage infrastructure. With the rise of exascale systems and in situ analysis also burst buffers will require integration with the archive. Exploring new strategies and developing open software for tape systems is a hurdle due to the lack of affordable storage silos and availability outside of large organizations and due to increased wariness requirements when dealing with ultradurable data. Lessening these problems by providing virtual storage silos should enable community-driven innovation, and enable site operators to add features where they see fit while being able to verify strategies before deploying on production systems. Different models for the individual components in tape systems are developed. The models are then implemented in a prototype simulation using discrete event simulation. It is shown that the simulation can be used to approximate the behavior of tape systems deployed in the real world and to conduct experiments without requiring a physical tape system.",oceanology,243
30f346032407b43e811107a46bf07a38f30527d7,filtered,semantic_scholar,,2005-01-01 00:00:00,semantic_scholar,the build master: microsoft's software configuration management best practices,https://www.semanticscholar.org/paper/30f346032407b43e811107a46bf07a38f30527d7,"""Wow, what can I say? Chapter 4, 'The Build Lab and Personnel,' by itself is enough justification to purchase the book! Vince is obviously a 'Dirty Finger Nails' build meister and there is a lot we can all learn from how he got them dirty! There are so many gems of wisdom throughout this book it's hard to know where to start describing them! It starts where SCM should start, at the end, and works its way forward. This book is a perfect complement to the 'Follow the Files' approach to SCM that I espouse. I will recommend that every software lead and software configuration management person I work with be required to read this book!""-Bob Ventimiglia, autonomic logistics software configuration manager, Lockheed Martin Aeronautics""The Build Master contains some truly new information; most of the chapters discuss points that many people in the industry don't have a full understanding of and need to know. It's written in a way that is easy to read and will help a reader fill holes in their vision regarding software build management. I especially liked Vince's use of Microsoft stories to make his points throughout the book. I will purchase the book and make certain chapters mandatory reading for my build manager consultants.""-Steve Konieczka, SCM consultant""Vince does a great job of providing the details of an actual working build process. It can be very useful for those who must tackle this task within their own organization. Also the 'Microsoft Notes' found throughout the book provide a very keen insight into the workings of Microsoft. This alone is worth purchasing this book.""-Mario E. Moreira, author of Software Configuration Management Implementation Roadmap and columnist at CM Crossroads""Software configuration management professionals will find this book presents practical ideas for managing code throughout the software development and deployment lifecycles. Drawing on lessons learned, the author provides real-world examples and solutions to help you avoid the traps and pitfalls common in today's environments that require advanced and elegant software controls.""-Sean W. Sides, senior technical configuration manager, Great-West Healthcare Information Systems""If you think compiling your application is a build process, then this book is for you. Vince gives us a real look at the build process. With his extensive experience in the area at Microsoft, a reader will get a look in at the Microsoft machine and also how a mature build process should work. This is a must read for anyone doing serious software development.""-Jon Box, Microsoft regional director, ProTech Systems Group""Did you ever wonder how Microsoft manages to ship increasingly complex software? In The Build Master, specialist Vince Maraia provides an insider's look.""-Bernard Vander Beken, software developer, jawn.net""This book offers an interesting look into how Microsoft manages internal development of large projects and provides excellent insight into the kinds of build/SCM things you can do for your large-scale projects.""-Lance Johnston, vice president of Software Development, SCM Labs, Inc.""The Build Master provides an interesting insight into how large software systems are built at Microsoft covering the set up of their build labs and the current and future tools used. The sections on security, globalization, and versioning were quite helpful as these areas tend to be overlooked.""-Chris Brown, ThoughtWorks, consultant""The Build Master is a great read. Managing builds is crucial to the profitable delivery of high-quality software. Until now, the build process has been one of the least-understood stages of the entire development lifecycle. This book helps you implement a smoother, faster, more effective build process and use it to deliver better software.""-Robert J. Shimonski, Networking and Security Expert, http://www.rsnetworks.netThe first best-practice, start-to-finish guide for the software build processManaging builds is crucial to the profitable delivery of high-quality software; however, the build process has been one of the least-understood stages of the entire development lifecycle. Now, one of Microsoft's leading software build experts introduces step-by-step best practices for maximizing the reliability, effectiveness, timeliness, quality, and security of every build you create.Drawing on his extensive experience working with Microsoft's enterprise and development customers, Vincent Maraia covers all facets of the build process-introducing techniques that will work on any platform, on projects of any size. Maraia places software builds in context, showing how they integrate with configuration management, setup, and even customer support. Coverage includes How Microsoft manages builds: process flows, check-in windows, reporting status, and more Understanding developer and project builds, pre- and post-build steps, clean builds, incremental builds, continuous integration builds, and more Choosing the right build tools for your projects Configuring source trees and establishing your build environment-introducing Virtual Build Labs (VBLs) Planning builds for multiple-site development projects or teams Determining what should (and shouldn't) be kept under source control Managing versioning, including build, file, and .NET assembly versions Using automation as effectively as possible Securing builds: a four layer approach-physical, tracking sources, binary/release bits assurance, and beyondBuilds powerfully impact every software professional: developers, architects, managers, project leaders, configuration specialists, testers, and release managers. Whatever your role, this book will help you implement a smoother, faster, more effective build process-and use it to deliver better software.© Copyright Pearson Education. All rights reserved.",oceanology,244
60ad8f69b0d150763afe9fde95ff61c475ba6acf,filtered,semantic_scholar,,2012-01-01 00:00:00,semantic_scholar,a study of 802 . 11 bitrate selection in linux,https://www.semanticscholar.org/paper/60ad8f69b0d150763afe9fde95ff61c475ba6acf,"This paper investigates rate adaptation in 802.11 wireless networks, with a focus on algorithms currently available in the Linux operating system. The algorithms are compared with a simple rate adaptation algorithm from the literature, and modifications are presented that increase the performance of th e existing routines in the studied scenarios. In order to compare simulated results with physical results, and to leverage the Linux software ecosystem, a new software simulator based on a virtual 802.11 device is presented. I. I NTRODUCTION In 802.11 wireless networks, data may be transmitted with any of a number of rates, from low-speed bitrates that are resilient under poor channel conditions, to high-speed rat es that require a high signal level to function. The process of a utomatically selecting the rate that maximizes throughput, given the current channel conditions, is known as r te adaptation[1] and has been studied extensively. The first published rate adaptation algorithm, Auto-Rate Fallback (ARF), is an extremely simple state machine that predicts the rate based on the most recent successful rate. SampleRate [2] is a popular algorithm that builds a statisti cal model of rates based on frame success rate and computed throughput. These two algorithms form the basis for others examined later in the paper. Other algorithms attempt to predict the rate based on direct measurements of the signal level at either the transmitter o r receiver [3], [4]. Unfortunately, these solutions often re quire changes to the MAC layer, or expensive low-bitrate broadcas t packets. Recently, loss differentiation has emerged as a promising improvement to frame-loss based algorithms, particularly in congested networks. As these also usually require changes t o the 802.11 specifications [5], [6], or modifications to physi cal hardware [7], uptake of these algorithms in deployed system has been slow. Consequently, it is instructive to study the algorithms currently in wide use in 802.11 LANs. Simulation of rate algorithms has typically been performed using network simulators, such as ns2, originally develope d for wired networks. While these systems work well for comparing different algorithms under controlled circumstances, by t heir nature it is difficult to compare experimental results with real-world trials. Moreover, simulations from the literat ure often fail to account for cross-layer effects that would imp act practical implementations, such as routing delays, and TCP timeouts. One observation is that a simulator may account for crosslayer effects implicitly, by directly using the networking stack of the operating system. One prior attempt to bridge the gap between research simulators and deployed systems is given in [8]. The authors present the library libmac, which allows experimenters to capture and inject frames using modified 802.11 device drivers. This system utilizes physical radio s for packet collection and transport. For simulation purposes, it would be advantageous to instead use virtual radios and mode l the medium. Thus, this paper introduces a simulator based on a virtualized 802.11 device driver, using the Linux Mac80211 [9] wireless stack. In addition to capturing cross-layer effec ts, the proposed simulator provides the ability to directly compar e experiments utilizing virtual devices with those from phys ical devices. This simulator is then used in an investigation of r ate adaptation algorithms used in the Linux operating system. Section III describes the assumptions made about the network and typical hardware devices. In section IV, the rate lgorithms are briefly described. Section V formulates the channel models used in simulation. In section VI, a new 802.11 simulator that utilizes the Linux wireless stack is presented. In section VII, the rate algorithms are compared both in the simulator and in real world experiments. Finally , in section VIII, modifications to the Minstrel algorithm are proposed. II. D IFFERENCES FROMPREVIOUS WORK In this paper, three rate adaptation algorithms are examine d: Minstrel, PID, and AARF [10]. AARF has been presented and reviewed in the literature, as has SampleRate [11], the pred ecessor of Minstrel. Yet, the author is unaware of published comparisons of Minstrel and PID, the two rate adaptation algorithms presently available in the Linux kernel 2.6.32. Simulations of rate adaptation algorithms have previously been carried out in network simulators with the same or similar channel models as those used in this work. The cross-layer accuracy of such simulations relies in some par t on the accuracy of models of other network layers. A new simulator is introduced that models only the 802.11 device and wireless medium while using the existing infrastructur e for the remaining layers. The virtual wireless device driver mac80211_hwsim existed prior to this project for testing Mac80211. In its more limited role as an API testing tool, the driver performed onl y TABLE I: 802.11a Rate Set Rate (Mbps) Modulation Coding Rate Bits per OFDM symbol 6 BPSK 1/2 48 9 BPSK 3/4 48 12 QPSK 1/2 96 18 QPSK 3/4 96 24 16-QAM 1/2 192 36 16-QAM 3/4 192 48 64-QAM 2/3 288 54 64-QAM 3/4 288 basic operations and did not attempt to simulate the wireles s medium. This kernel driver was rewritten to pass frames to user programs to ease development of the channel simulator. III. N ETWORK MODEL For this paper, the 802.11a PHY is used as the basis for experimentation. The newest standard, 802.11n, has recent ly been approved and provides 32 additional rates; however, th Linux rate adaptation API for 802.11n rates is still evolvin g at this time. The 802.11a rate set (Table I) is still in use as part of 802.11g, and provides a variety of speeds. In addition, this paper is primarily interested in applicat ons to small infrastructure networks. In ad-hoc and mesh system , both the range of the network and number of nodes is often large. As a result, rates that work over long distances may be preferred to high throughput, short range rates. Also, in large networks, hidden terminals are common, leading to the frequent use of low-bitrate RTS/CTS protection. A trend in consumer-oriented 802.11 hardware is the increasing use of so-called soft-MAC designs: devices consis ti g primarily of radios and small embedded CPUs where most of the 802.11 MAC Layer Management Entity (MLME) features are performed off-chip by the host computer. These designs a re low cost and have the advantage of being software-updateabl e. Such designs often omit explicit rate control features, rel ying on the host to provide a rate or a set of candidate rates for a frame. A typical design is the Atheros 5212, in which each frame is accompanied by a multi-rate retry (MRR) descriptor . The descriptor consists of four candidate rates, r1, r2, r3, r4, along with a set of retry counts, c1, c2, c3, c4. The device will attempt to transmit a frame c1 times at rater1, thenc2 times at r2, etc., until the retry counts are exhausted or until an ACK is received. The simulator assumes a similar design. IV. RATE ALGORITHMS ARF [12] is among the earliest developed automatic rate selection algorithms. In ARF, if packets are transmitted su ccessfully a fixed number of times, then the rate is raised. If there is a frame loss immediately after a rate change, or if there are two consecutive failures, the rate is lowered. Ada ptive Auto-Rate Fallback (AARF) [10] utilizes the basic results of ARF, but adds the notion of an exponentially increasing threshold for raising the rate. This is intended to correct the observed problem that the periodic failed transmission attempts at higher rates led to decreased overall throughpu t. Minstrel, based on [2], takes a probabilistic approach. Ten percent of sent frames include a random probe rate as the first rate in the MRR chain. Success at each rate is recorded as packets are sent. Every 100 ms, the probabilities of success and computed throughput are updated for all packets, and the se are combined with previous results using an exponentially weighted moving average. The MRR descriptor includes the two best throughputs followed by the best probability rate, then followed by the lowest available rate. The MRR retry counts are selected such that transmissions at a given rate f or all attempts should take no more than 6 ms, and the entire transmission takes less than 24 ms. PID is based on the concept of the proportional-integralderivative feedback controller [13]. The algorithm adjust s the transmission rate to achieve a maximum of 14% transmission failures. Every 125 ms, the controller recomputes the avera ge number of failed transmissions with an exponentially weigh ted moving average. If a large amount of frame loss is detected, the controller can enter a sharpening mode, in which large adjustments to the rate can be made to more quickly approach the targeted success percentage.",oceanology,245
abb0cd9f718bf472e16bee39f525aa6ee219e411,filtered,semantic_scholar,,2010-01-01 00:00:00,semantic_scholar,intelligent network design of intelligent multinode sensor networking,https://www.semanticscholar.org/paper/abb0cd9f718bf472e16bee39f525aa6ee219e411,". The paper deals with the self configured intelligent sensor networking. The individual sensors are acting on the body or an object to measure different parameters. Although the sensors are measuring parameters accurately, but they are failed to act depending on different situations. For example a robot is moving on a surface can take decision to turn left or right when an obstacle come across. But the same robots take wrong decision when the obstacle is not static. The robot can wait till the obstacle passed away from its way. But the robot still follows the traditional way, which is turning left or turn. In this case the robot is failed to take correct decision depending on the situation. If we consider other example such as traditional automatic water supply to plants or crops, the system supplies the water at regular intervals of time with accurate quantity. But the system takes same decisions in all seasons irrespective of the soil type and crop type. In our system we are proposing a Wireless Distributing sensor system design which is able to take wise decisions as a farmer. A farmer can understands how much water the soil needs and at what time it need to apply. In our work, we are developing, (1) Home Area Networking (2)software supporting above functions; (3) Wireless Sensor Networking. Introduction: My paper describes about advanced self configured Wireless Distributed Sensor networking. My project support universal sensors, network management, GUI software, house area network (HAN) [1]. The smart environment relies first and foremost on sensory data from the real world. Sensory data comes from multiple sensors of different modalities in distributed locations. The smart environment needs information about its surroundings as well as about its internal workings. Our wireless sensor networks is involved with challenging issues wireless sensor systems, self-organization, signal processing and decision-making, and finally some concepts for home automation, We have identified some facts are : 1. Most networks are application specific, extensive secondary development is necessary to adapt to different circumstances. 2. Most of them are run by developers professionals rather than end users. 4. It is difficult for end users to configure and deploy a practical sensor network. 5. Systematic compatibility for diverse sensors and communication channels is limited. 6. The aquatic sensor network technology lags behind terrestrial development in terms of use of modern technology. Often, a single sensor cannot fully capture the measured phenomenon, so researchers develop multisensor systems to obtain more accurate information, as shown in Figure 2-1. Smart Sensor Enhanced functions include “compensation of secondary parameters (e.g. temperature), failure prevention and detection, self-testing, auto-calibration”. Figure 2-1: Multi-sensor Sensing Model Figure 2-2: Smart Sensor Network A sensor network consists of multiple detection stations called sensor nodes [3].The transducer generates electrical signals based on sensed physical effects and phenomena. The microcomputer processes and stores the sensor output. The transceiver, which can be hard-wired or wireless, receives commands from a central computer and transmits data to that computer. The power for each sensor node is derived from the electric utility or from a battery. The observations made against the characteristics of DSN are: Extended wider coverage of the environment Better fault tolerance Higher quality of measurements liminate ambient interference. Shorter response delay for changing events. Flexible size of network ISSN : 0975-3397 468 N. Suresh kumar et al. / (IJCSE) International Journal on Computer Science and Engineering Vol. 02, No. 03, 2010, 468-472 Wireless House Area Network WHAN The research group started related investigations under Low Power Wireless Sensor networking. We are integrating DSN with WHAN in house area network system. Based on this DSN platform, we are implementing wireless house area network design. Research Issues and Challenges Many research issues and challenges have been exposed. Design considerations for sensor networks [7]. Sensing aspect Computation part Wireless Sensor Networking Faster algorithm for data tranceiving. Signal Conditioning [2] Smart Sensor includes basic blocks for signal conditioning (SC), digital signal processing (DSP), and A/D conversion. Signal conditioning [6] is performed using electronic circuitry analog low pass filter. Temperature compensation can also be added during the Signal Conditioning stage. A basic technique for improving the signal-to-noise ratio (SNR) is low-pass filtering, since noise generally dominates the desirable signals at high frequencies. Shown in the figure is an analog LPF that also amplifies, constructed from an operational amplifier. The transfer function of this filter is with 3 dB cutoff frequency given by rad.",oceanology,246
e349556d5302749ee79643792e60b192020a42b2,filtered,semantic_scholar,,2009-01-01 00:00:00,semantic_scholar,special issue on wireless sensor network: theory and practice,https://www.semanticscholar.org/paper/e349556d5302749ee79643792e60b192020a42b2,"Wireless sensor network (WSN) is an emergent multi-disciplinary science, and it may be considered as the foundation of pervasive computing, mobile computing and wearable computing. WSN is a very active and competitive research area due to its diverse and unlimited potential applications: air, underground and underwater. In spite of its young age, economic impact of WSN is important, for examples the industrial control segment market will be worth $5.3B by 2010 and the smart home market will be worth $2.8 billion worldwide by 2012 (Source: Stamatis Karnouskos, EU-US 08 Workshop). WSN is a set of wireless nodes. On one hand, each wireless node (WN) has similar hardware and software functionalities as a PC: CPU, memory, operating system, and communication protocol to fulfill a specific task. On the other hand, a WN has a limited power supply (embedded battery) and consumes approximately 1 million less power (~100μW instead of ~100W) than a PC. Due to resources constraints: energy consumption and form factor, the approaches applied in general purpose computer systems are not adapted to the requirements of WSN. When it comes to the design of energy efficient oriented hardware and software components of WSN, cross-layering optimization approaches are generally adopted such as application-specific unified hardware and software by taking into account the following criteria: trade-off between complexity, efficiency and resource consumption, and application context (context-aware) etc. Currently two main hardware development and design trends are carried out to implement the WN: Commercial OffThe-Shelf ‘COTS’ and System on Chip ‘SoC’. The first and second generations of WN were designed by using low power 8-bit or 16-bit microcontroller processor core, Bluetooth and non standard wireless access medium (MICA Mote). The current trend of WN design such as Tmotesky, iMote and LiveNode are based on low power 16-bit or 32-bit RISC microcontroller, and full compliance IEEE802.14.5 standard. However the ultimate goal of all the researchers in the world is the implementation of long life, low cost and invisible WN integrated and embedded into environment. Three key technologies make possible to achieve this objective: MEMS ‘MicroElectroMEchanical systems’, UWB ’Ultra-Wide Band’ and low power CMOS technology. Different WN prototypes are realized by Intel (iMOTE2), University of Michigan (MOTE: Michigan Uni Prototype) and University of Berkeley (Pico-Mote). WN hardware seems easier to solve than embedded software for diverse WSN applications. The main questions which are related to WSN basic software design is how to keep modularity, high level abstraction and reliability to enable to implement complex massively distributed WSNs to meet resource constraint requirements. Real-time operating system (RTOS) plays a key role to support high level abstraction and distributed collaborative processing. Currently four categories of WSN’s RTOS are developed: Event driven (TinyOS), Multitask (RETOS, tKernel, NutOS, MANTIS), Data-Centric (AmbientRT), and Hybrid (Contiki, LIMOS). Note that TinyOS is very popular but it not adapted to complex hard real-time application. The development challenges of the WSN RTOS are energy-efficiency (context aware, configurable, small footprint), robustness, fault tolerance, support hard real-time constraint, and support component based model (high level of abstraction to ease the integration of high level SW such as protocol, middleware, application, and simulator). Furthermore, for WSN applications, message sending is energy consuming. Thus it is important to implement embedded energy efficient wireless routing protocol to increase WSN lifetime. It is clear that general purpose MANET routing protocol such as AODV (active), OLSR (proactive) and ZBR (hybrid) etc. are not suitable for WSN due to resource constraints. For example optimal routing path is well adapted to general purpose MANET but not suitable for WSN because the repetitive use of the same path will exhaust the battery of WNs belonging to the optimal routing path (black hole). Many WSN dedicated protocols are implemented (spin, cougar, gear, leach, speed ...) but it is currently very difficult to have a clear idea concerning their performance (energy consumption, scalability, connectivity, lifetime ...) because of the lack of large scale WSN real world experimentation results and because the simulation model does not reflect the real-world ones (physical layer). Note that, there is no standard scenario and the application program (with a known number of WNs) enables to evaluate rationally the performance of wireless routing protocols. In addition routing protocol relies on the WSN topology. On one hand, an optimal WSN topology facilitates the implementation of routing and administration protocols. On the other hand, the deployment of large scale WS nodes in a large area is random and its topology is a priori unknown. Then, it is important to investigate the auto-configuration algorithms to increase the efficiency of routing and administration protocols. However the frontier between the administration protocol and the routing protocol is not as clearly defined as in a classical network (e.g. TCP/IP and SNMP) due to cross-layering approach. Moreover WSN security, reliability, and fault tolerance are still an open problem. In this special issue 5 papers are selected among 40 submitted papers for the NTMS workshop on wireless sensor network: theory and practice, held at Tangier at Morocco in 2008, the rest of the papers are selected from an open call for paper. WSN is a multi-disciplinary science. It impossible to present all its topics but this special issue addresses most of the WSN embedded software problems dealing with real-world applications (EU NeT-ADDED FP6 project, French ANR research project and industrial projects). JOURNAL OF NETWORKS, VOL. 4, NO. 6, AUGUST 2009 379",oceanology,247
368687003560b21e53865cd604aae8c00dc62c4b,filtered,semantic_scholar,2009 6th IEEE Consumer Communications and Networking Conference,2009-01-01 00:00:00,semantic_scholar,passiton: an opportunistic messaging prototype on mobile devices,https://www.semanticscholar.org/paper/368687003560b21e53865cd604aae8c00dc62c4b,"With the increasing popularity of mobile handheld devices and the growing capability of these devices, it is becoming possible that information sharing/dissemination is carried out through human networks, as a complement to the traditional computer networks. In such human networks, people come across one another, while their mobile devices exchange and store information in a spontaneous and transparent way. Such an encounter could be established through direct device-todevice connectivity when two devices come into each other’s communication range, or be enabled by, e.g., a Wi-Fi access point, when the devices both enter its coverage. A new form of dissemination, which we call opportunistic messaging, is such an application that is based on human encounters and mobilities. When human encounters are exploited for communications, the reliance on network infrastructure access is eliminated; communications can be performed even where infrastructure is absent or infrastructure access is intermittent. By leveraging human mobilities, data delivery does not require an end-to-end path from the source to a recipient; instead, people carrying mobile devices serve as relays – they cache others’ data and forward/deliver the data when appropriate. Thus, the propagation of information is tied to people’s physical proximity when they move around, and incorporates the social aspects of communications as people tend to spend more time co-locating with their social relations. Opportunistic messaging is applicable anywhere, and is especially appealing where network infrastructure access is limited or intermittent (e.g., on cruise ships, in national parks, after disasters). Another intriguing characteristic of it is its ease of deployment – no central server is needed, but only a single piece of software on users’ mobile devices. However, as human encounters and mobilities are unpredictable, when used for social applications, opportunistic messaging is most suited for disseminating user-generated information that is non-formal, less important, and thus not time-critical. In recent years, a considerable amount of efforts have been invested in the research on opportunistic networking and delay-tolerant networking (which encompasses opportunistic networking but is a broader concept). A large portion of prior work has focused on routing issues, e.g., through whom as intermediate carriers to deliver a message to the destinations [1] [2] [3] [4]. The routing issues have been further explored in various contexts, such as in vehicular networks [5] [6] [7] and in social networking applications [8] [9] [10] [11]. However, serious real-world application development, deployment and evaluation of the opportunistic networking concepts still fall behind [12], in which many challenging issues remain to be addressed (to name a few, location-awareness, user incentives and preferences, power preservation, encounter controls, etc.). In this work, we design and prototype PassItOn, a fully distributed opportunistic messaging system. Our goal is to build up a proof-of-concept platform on real mobile devices, and thus show the feasibility and potentials of utilizing human movements for dissemination applications. Meanwhile, we seek to shed lights on the design, implementation and deployment issues in building such systems, and thus stimulate new ideas and perspectives on addressing these issues. Moreover, we aim to offer a real testbed on which new mechanisms, protocols and use cases can be tested and evaluated.",oceanology,248
d0270dcfa7ca058cea59512b832be6c91408676f,filtered,semantic_scholar,Scalable Comput. Pract. Exp.,2005-01-01 00:00:00,semantic_scholar,challenges concerning symbolic computations on grids,https://www.semanticscholar.org/paper/d0270dcfa7ca058cea59512b832be6c91408676f,"Challenges concerning symbolic computations on grids Symbolic and algebraic computations are currently ones of fastest growing areas of scientific computing. For a long time, the numerical approach to computational solution of mathematical problems had an advantage of being capable of solving a substantially larger set of problems than the other approach, the symbolic one. Only recently the symbolic approach gained more recognition as a viable tool for solving large-scale problems from physics, engineering or economics, reasoning, robotics or life sciences. Developments in symbolic computing were lagging relative to numerical computing, mainly due to the inadequacy of available computational resources, most importantly computer memory, but also processor power. Continuous growth in the capabilities of computer hardware led naturally to an increasing interest in symbolic calculations and resulted, among others things, in development of sophisticated Computer Algebra Systems (CASs). CASs allow users to study computational problems on the basis of their mathematical formulations and to focus on the problems themselves instead of spending time transforming the problems into forms that are numerically solvable. While their major purpose is to manipulate formulas symbolically, many systems have substantially extended their capabilities, offering nowadays functionalities like graphics allowing a comprehensive approach to problem solving. While, typically, CAS systems are utilized in an interactive mode, in order to solve large problems they can be also used in a batch mode and programmed using languages that are close to common mathematical notation. As CASs become capable of solving large problems, they follow the course of development that has already been taken by numerical software: from sequential computers to parallel machines to distributed computing and finally to the grid. It is particularly the grid that has the highest potential as a discovery accelerator. Currently, its widespread adoption is still impeded by a number of problems, one of which is difficulty of developing and implementing grid-enabled programs. That it is also the case for grid-enabled symbolic computations. There are several classes of symbolic and algebraic algorithms that can perform better in parallel and distributing computing environments. For example for multiprecision integer arithmetic, that appears among others in factorizations, were developed already twenty years ago systolic algorithms and implementations on massive parallel processors, and more recently, on the Internet. Another class that utilize significant amount of computational resources is related to the implementations of polynomial arithmetic: knowledge based algorithms such as symbolic differentiation, factorization of polynomials, greatest common divisor, or, more complicated, Groebner base computations. For example, in the latest case, the size of the computation and the irregular data structures make the parallel or distributed implementation not only an attractive option for improving the algorithm performance, but also a challenge for the computational environment. A third class of algorithms that can benefit from multiple resources in parallel and distributed environments is concerning the exact solvers of large systems of equations. The main reason driving the development of parallel and distributed algorithms for symbolic computations is the ability to solve problems that are memory bound, i.e. that cannot fit into memory of a single computer. An argument for this statement relies on the observation that the input size of a symbolic or algebraic computation can be small, but the memory used in the intermediate stages of the computation may grow considerably. Modern CASs increase their utility not only through new symbolic capabilities, but also expending their applicability using visualization or numerical modules and becoming more than only specific computational kernels. They are real problem solving environments based on interfaces to a significant number of computational engines. In this context it appears also the need to address the ability to reduce the wall-clock time by using parallel or distributed computing environment. A simple example is the case of rendering the images for a simulation animation. Several approaches can be identified in the historical evolution of parallel and distributed CASs: developing versions for shared memory architectures, developing computer algebra hardware, adding facilities for communication and cooperation between existing CASs, or building distributed systems for distributed memory parallel machines or even across Internet. Developing completely new parallel or distributed systems, although efficient, in most cases is rather difficult. Only a few parallel or distributed algorithms within such a system are fully implemented and tested. Still there are several successful special libraries and systems falling in this category: ParSac-2 system, the parallel version of SAC-2, Paclib system, the parallel extension of Saclib, FLATS based on special hardware, STAR/MPI, the parallel version of GAP, ParForm, the parallel version of Form, Cabal, MuPAD, or the recent Givaro, for parallel computing environments, FoxBox or DSC, for distributed computing environments. An alternative approach to build parallel and distributed CASs is to add the new value, the parallelism or the distribution, to an existing system. The number of parallel and distributed versions of most popular CASs is impressive and it can be explained by the different requirements or targeted architectures. For example, for Maple there are several implementations on parallel machines, like the one for Intel Paragon or ||Maple||, and several implementations on networks of workstations, like Distributed Maple or PVMaple. For Mathematica there is a Parallel Computing Toolkit, a Distributed Mathematica and a gridMathematica (for dedicated clusters). Matlab that provides a Symbolic Math Toolbox based on a Maple kernel has more than twenty different parallel or distributed versions: DP-Toolbox, MPITB/PVMTB, MultiMatlab, Matlab Parallelization Toolkit, ParMatlab, PMI, MatlabMPI, MATmarks, Matlab*p, Conlab, Otter and others. More recent web-enabled systems were proved to be efficient in number theory for finding large prime numbers, factoring large numbers, or finding collisions on known encryption algorithms. Online systems for complicated symbolic computations were also built: e.g. OGB for Groebner basis computations. A framework for description and provision of web-based mathematical services was recently designed within the Monet project and a symbolic solver wrapper was build to provide an environment that encapsulates CASs and expose their functionalities through symbolic services (Maple and Axiom were chosen as computing engines). Another platform is MapleNet build on client-server architecture: the server manages concurrent Maple instances launched to server client requests for mathematical computations. WebMathematica is a similar system that offers access to Mathematica applications through a web browser. Grid-oriented projects that involve CASs were only recent initiated. The well-known NetSolve system was one of the earliest grid system developed. Version 2 released in 2003 introduces GridSolve for interoperability with the grid based on agent technologies. APIs are available for Mathematica, Octave and Matlab. The Genss project (Grid Enabled Numerical and Symbolic Services) follows the ideas of the Monet project and intends also to combine grid computing and mathematical web services using a common agent-based framework. Several projects are porting Matlab on grids: from small ones, like Matlab*g, to very complex ones, like Geodise. Maple2g and MathGridLink are two different approaches for grid-enabled version of Maple and Mathematica. Simple to use front-end were recently build in projects like Gemlca and Websolve to deploy legacy code applications as grid services and to allows the submission of computational requests. The vision of grid computing is that of a simple and low cost access to computing resources without artificial barriers of physical location or ownership. Unfortunately, none of the above mentioned grid-enabled CAS is responding simultaneously to some elementary requirements of a possible implementation of this vision: deploy grid symbolic services, access within CAS to available grid services, and couple different grid symbolic services. Moreover a number of major obstacles remain to be addressed. Amongst the most important are mechanisms for adapting to dynamic changes in either computations or systems. This is especially important for symbolic computations, which may be highly irregular in terms of data and general computational demands. Such demands received until now relatively little attention from the research community. In the context of a growing interest in symbolic computations, powerful computer algebra systems are required for complex applications. Freshly started projects shows that porting a CAS to a current distributed environment like a grid is not a trivial task not only from technological point of view but also from algorithmic point of view. Already existing tools are allowing experimental work to be initiated, but a long way is still to be cross until real-world problems will be solved using symbolic computations on grids. Dana Petcu, Western University of Timisoara",oceanology,249
10.1109/vtcspring.2014.7022797,filtered,2014 IEEE 79th Vehicular Technology Conference (VTC Spring),IEEE,2014-05-21 00:00:00,ieeexplore,a standardized path loss model for the gsm-railway based high-speed railway communication systems,https://ieeexplore.ieee.org/document/7022797/,"High-speed railway (HSR) has been widely introduced to meet the increasing demand for passenger rail travel. While it provides more and more conveniences to the people, the huge cost of the HSR has laid big burden on the government finance. Reducing the cost of HSR has been necessary and urgent. Optimizing the arrangement of the base stations (BS) by improving the prediction of the communication link is one of the most effective methods, which could reduce the number of the BSs to a reasonable number. However, it requires a carefully developed propagation model, which has been largely neglected before in the research on the HSR. In this paper, we propose a standardized path loss model for HSR channels based on an extensive measurement campaign in 4594 HSR cells of the ""Zhengzhou-Xian"" HSR line. The measurements are conducted using a practically deployed and operative GSM-Railway (GSM-R) system to reflect the real conditions of the HSR channels. The proposed model is validated by the measurements conducted in another operative railway - ""Beijing-Shanghai"" HSR line. The results are helpful for the HSR communications system designers to gain a better tool in the system planning, and propagation researchers to assess where the most pressing needs in the modeling of HSR channels lie.",finance,250
10.1109/telfor48224.2019.8971360,filtered,2019 27th Telecommunications Forum (TELFOR),IEEE,2019-11-27 00:00:00,ieeexplore,development of intelligent systems and application of gamification in artificial intelligent learning,https://ieeexplore.ieee.org/document/8971360/,"CToday, intelligent systems are used in many fields - medicine, agriculture, transport, telecommunications, industrial process management and control, finance, commerce, the computer game industry and many others. This paper describes a complete way to develop an intelligent software system for simulation and visualization of artificial intelligence algorithms. The system includes artificial intelligence algorithms from basic search strategies and game theory, inference algorithms and knowledge representation models, to advanced search techniques, machine and inductive learning. The application of these algorithms to real everyday problems and the application of this software system as an auxiliary tool for analysis of input data and inference in various fields are presented. The system is also applicable to education in an introductory artificial intelligence course at the university, so the last phase of the research involved the transition of the software system into a game-based tool and application of gamification.",finance,251
10.1109/itnt52450.2021.9649038,filtered,2021 International Conference on Information Technology and Nanotechnology (ITNT),IEEE,2021-09-24 00:00:00,ieeexplore,"digital twin of rice as a decision-making service for precise farming, based on environmental datasets from the fields",https://ieeexplore.ieee.org/document/9649038/,"In this paper a ready-to-use software component, which simulates real state of rice crop in the field and called “Digital Twin of rice” (DT), is studied. DT uses ontology-based knowledge base of plant cultivation to execute the rules of plant growth. The software provides real-time data collection from the fields and distributed decision making to find the optimum solution in planning process of rice growth stages. Rice DT is developed as an autonomous service and can be integrated to any existing digital agricultural platform. A pilot integration with cyber-physical system (CPS) for precise farming is described in the paper. The CPS has a number of services to provide digital transformation in plant cultivation enterprises and big farms. The system performs adaptive scheduling of resources, such as fertilizers, protection agents, vehicles, personnel and finance. Results of DT implementation shows adequate decision-making of the service compared to experiments on the pilot farms. So, DT of plant could be a next step in digital transformation of agriculture, providing improvement of ROI from precision farming, automate decision-making processes for farmers and service companies and make their business smarter, more flexible, and more cost-efficient, providing better productivity of plant cultivation and sustainability of agriculture under global climate changes.",finance,252
10.1109/kcic.2018.8628512,filtered,2018 International Electronics Symposium on Knowledge Creation and Intelligent Computing (IES-KCIC),IEEE,2018-10-30 00:00:00,ieeexplore,estimating adaptive individual interests and needs based on online local variational inference for a logistic regression mixture model,https://ieeexplore.ieee.org/document/8628512/,"In real companies engaged in economic activities through transactions involving consumer items, such as retail, distribution, finance, and information materials, supplying an opportunity to customers to choose specialized items is an important factor that can improve customer satisfaction and convenience allowing their diverse and time-dependent needs to be met. However, capturing the specialized needs of customers accurately is a difficult task because their needs depend on time, context, situation, and meaning. Recently, physical computational environments have been developing rapidly, thereby allowing easy implementation to sense a customer's action and deal with it sequentially. In this paper, we propose a personalized method to predict individual interests and demands appropriately. In particular, the system learns the customers' situation, meaning, and action from their action history, and reflects a feedback of the result to predict the next action. To realize this method, we utilize the following two methodologies: the mathematical model of meaning (MMM), which is a semantic associative search technology; and the local variational inference (LVI), which is an approximation of the Bayesian inference. A numerical experiment shows that the proposed method performed better than a typical method.",finance,253
10.1109/rteict.2016.7808177,filtered,"2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)",IEEE,2016-05-21 00:00:00,ieeexplore,high speed and area efficient single precision floating point arithmetic unit,https://ieeexplore.ieee.org/document/7808177/,"Many fields of science, engineering, finance, mathematical optimization methods, Artificial Neural Networks, signal and image processing algorithms requires the operations and manipulations of real numbers. Floating-point operations are most extensively adopted approach for exploiting real numbers. The speed of Floating-point arithmetic unit is very crucial performance parameter which impinges the operation of the system. On that account a 32 bit floating point arithmetic unit is designed for different applications which insists for eminent speed. The intent of this design is to reduce the area and combinational path delay to enhance the speed of operation which is attained by parallelism in multiplier which is used for mantissa multiplication. For Floating-point multiplier Booth recoded multiplier is used where the number of partial product are reduced which in turns boost the speed of multiplication. The proposed module is implemented on Spartan 6 FPGA. Performance of the floating point arithmetic unit is compared with latest research papers regarding delay and it is ascertained that there is 59% of optimization in critical path delay of floating point multiplier and 50 % of optimization of floating point adder. The result illustrates that proposed arithmetic unit has a great impact on convalescent the speed and area of the design.",finance,254
10.2498/iti.2012.0379,filtered,Proceedings of the ITI 2012 34th International Conference on Information Technology Interfaces,IEEE,2012-06-28 00:00:00,ieeexplore,model of the new sales planning optimization and sales force deployment erp business intelligence module for direct sales of the products and services with temporal characteristics,https://ieeexplore.ieee.org/document/6307985/,"Considering that direct sales today has significant share in the whole sales niche and that existing travelling salesman problem solutions still do not provide a comprehensive resolution for sales force deployment when selling products and services with temporal or periodic recurring, here we present a new method that combines business intelligence and easy-to-use graphical user interface to give the answers. Module is self-learning due to implemented statistics with tendency for more accurate sales predictions as time goes by. Presented model is applicable in various industries like telecommunications, finance, pharmaceuticals etc. and capable of solving large-scale real-world problems in real time.",finance,255
10.1109/pesgre52268.2022.9715908,filtered,"2022 IEEE International Conference on Power Electronics, Smart Grid, and Renewable Energy (PESGRE)",IEEE,2022-01-05 00:00:00,ieeexplore,online prediction of dga results for intelligent condition monitoring of power transformers,https://ieeexplore.ieee.org/document/9715908/,"Transformers form a major part of a power system in transmission as well as distribution of power. Considering the criticality, finance, and time involved in repair, periodic condition monitoring and maintenance of transformers are the key to ensure electrical safety as well as stable operation of the large interconnected power system. Dissolved Gas Analysis (DGA) is an established tool used to determine the incipient faults within the transformer by analyzing the concentration of different gases in the transformer oil and giving early warnings and diagnoses. Currently, transformers worldwide utilise online sensors to monitor dissolved gases and moisture content in oil. The online DGA sensor uses a small amount of oil from transformer to perform real-time DGA analysis and gives the ppm content of dissolved gases for further course of action. Considering the large quantity of assets and the huge amount of data produced, it is imperative to develop a tool to aid the operators in assimilating the available data for diagnosis and proactive decision making. The present study improvises AI techniques to predict future dissolved gas concentrations using real time DGA data collected from the transmission utility of the country. The prediction helps to forecast the trend of development of incipient faults in the transformer. The complete project scope is to develop a highly reliable diagnostic tool to emulate the decision-making ability of a human expert in transformer DGA analysis to enhance transformer life. In the present paper, models based on Auto-regressive Integrated Moving Average (ARIMA), Long Short-Term Memory (LSTM), and Vector Auto Regression (VAR) are implemented to predict DGA data of three in-service transformers. DGA data is forecasted for up to 8 monthly samples in the future, and the accuracy of results is compared with each other. The LSTM-VAR combined model is seen to provide the best results among them.",finance,256
10.1109/scc49832.2020.00053,filtered,2020 IEEE International Conference on Services Computing (SCC),IEEE,2020-11-11 00:00:00,ieeexplore,ponzi contracts detection based on improved convolutional neural network,https://ieeexplore.ieee.org/document/9284582/,"As one of the leading blockchain systems in operation, Ethereum has numerous smart contracts deployed to implement a variety of functions. Unfortunately, speculators introduce scams such as Ponzi scheme in the traditional financial sector into some of these smart contracts, causing millions of dollars of losses to investors. At present, there are a few of quantitative identification methods for new fraud modes under the background of Internet finance, and detection methods for the Ponzi scheme contracts on Ethereum are even less. In this paper, we propose an improved convolutional neural network as a detection model for Ponzi schemes in smart contracts. We use real smart contracts to evaluate the feasibility and usefulness of our mode. Results show that our improved convolutional neural network can overcome difficulties in training caused by different length of smart contracts' bytecodes. Compared with the state-of-the-art methods, the precision and recall rate of our model for Ponzi scheme detection are improved by 3.2% and 24.8% respectively.",finance,257
,filtered,"2015 7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K)",IEEE,2015-11-14 00:00:00,ieeexplore,prediction of earnings per share for industry,https://ieeexplore.ieee.org/document/7526950/,"Prediction of Earnings Per Share (EPS) is the fundamental problem in finance industry. Various Data Mining technologies have been widely used in computational finance. This research work aims to predict the future EPS with previous values through the use of data mining technologies, thus to provide decision makers a reference or evidence for their economic strategies and business activity. We created three models LR, RBF and MLP for the regression problem. Our experiments with these models were carried out on the real datasets provided by a software company. The performance assessment was based on Correlation Coefficient and Root Mean Squared Error. These algorithms were validated with the data of six different companies. Some differences between the models have been observed. In most cases, Linear Regression and Multilayer Perceptron are effectively capable of predicting the future EPS. But for the high nonlinear data, MLP gives better performance.",finance,258
10.1109/bigdata50022.2020.9378023,filtered,2020 IEEE International Conference on Big Data (Big Data),IEEE,2020-12-13 00:00:00,ieeexplore,zero-shot machine learning technique for classification of multi-user big data workloads,https://ieeexplore.ieee.org/document/9378023/,"During the last decade machine learning has revolutionized computer science applications. Supervised machine learning algorithms have become especially successful in many industries including health, legal, security, finance, travel, and others. Training supervised learning algorithms, however, is expensive because the real world contains a very large number of different classes that need to be covered by the training set. This is especially true for the highly variable, multi-user workload data produced by the strategically vital big data and cloud software stacks. It is very important, however, to be able to accurately classify these complex workloads in order to enable autonomic management and optimization. Zero-Shot Learning (ZSL) is an advanced machine learning approach that enables classification of objects without having to explicitly train on examples of those objects. In this paper we present a new ZSL technique intended to reduce the expense of assembling workload training sets for big data analytic workloads. We demonstrate that multi-user big data workloads can be treated as hybrids of simpler, single-user workload classes, and classified accurately without having to explicitly train on example instances of multi-user workloads. Our technique is able to accurately classify both unseen multi-user workloads, and seen single-user workloads using the same classifier. We demonstrate 83% classification accuracy for the unseen multi-user workloads, and 92% classification accuracy for the seen, single-user workload classes.",finance,259
10.1109/uksim.2012.123,filtered,2012 UKSim 14th International Conference on Computer Modelling and Simulation,IEEE,2012-03-30 00:00:00,ieeexplore,[cover art],https://ieeexplore.ieee.org/document/6205540/,The following topics are dealt with: neural networks; evolutionary computation; adaptive dynamic programming; re-enforcement learning; bio-informatics; bio-engineering; computational finance; economics; semantic mining; data mining; virtual reality; data visualization; intelligent systems; soft computing; hybrid computing; e-science; e-systems; robotics; cybernetics; manufacturing; engineering; operations research; discrete event systems; real time systems; image processing; speech processing; signal processing; industry; business; social issues; human factors; marine simulation; power systems; logistics;parallel systems; distributed systems; software architectures;Internet modelling; semantic Web; ontologies; mobile ad hoc wireless networks; Mobicast; sensor placement; target tracking; circuits; sensors and devices.,finance,260
10.1109/tkde.2018.2822307,filtered,IEEE Transactions on Knowledge and Data Engineering,IEEE,2018-12-01 00:00:00,ieeexplore,ensemble learning for multi-type classification in heterogeneous networks,https://ieeexplore.ieee.org/document/8329517/,"Heterogeneous networks are networks consisting of different types of objects and links. They can be found in several fields, ranging from the Internet to social sciences, biology, epidemiology, geography, finance, and many others. In the literature, several methods have been proposed for the analysis of network data, but they usually focus on homogeneous networks, where all the objects are of the same type, and links among them describe a single type of relationship. More recently, the complexity of real scenarios has impelled researchers to design methods for the analysis of heterogeneous networks, especially focused on classification and clustering tasks. However, they often make assumptions on the structure of the network that are too restrictive or do not fully exploit different forms of network correlation and autocorrelation. Moreover, when nodes which are the main subject of the classification task are linked to several nodes of the network having missing values, standard methods can lead to either building incomplete classification models or to discarding possibly relevant dependencies (correlation or autocorrelation). In this paper, we propose an ensemble learning approach for multi-type classification. We adopt the system Mr-SBC, which is originally able to analyze heterogeneous networks of arbitrary structure, within an ensemble learning approach. The ensemble allows us to improve the classification accuracy of Mr-SBC by exploiting i) the possible presence of correlation and autocorrelation phenomena and ii) the classification of instances (which contain missing values) of other node types in the network. As a beneficial side effect, we have also that the models are more stable in terms of standard deviation of the accuracy, over different samples used for training. Experiments performed on real-world datasets show that the proposed method is able to significantly outperform the standard implementation of Mr-SBC. Moreover, it gives Mr-SBC the advantage of outperforming four other well-known algorithms for the classification of data organized in a network.",finance,261
10.1109/tits.2015.2390614,filtered,IEEE Transactions on Intelligent Transportation Systems,IEEE,2015-08-01 00:00:00,ieeexplore,reducing the cost of high-speed railway communications: from the propagation channel view,https://ieeexplore.ieee.org/document/7052340/,"High-speed railways (HSRs) have been widely introduced to meet the increasing demand for passenger rail travel. While it provides more and more conveniences to people, the huge cost of the HSR has laid big burden on the government finance. Reducing the cost of HSR has been necessary and urgent. Optimizing arrangement of base stations (BS) by improving prediction of the communication link is one of the most effective methods, which could reduce the number of BSs to a reasonable number. However, it requires a carefully developed propagation model, which has been largely neglected before in the research on the HSR. In this paper, we propose a standardized path loss/shadow fading model for HSR channels based on an extensive measurement campaign in 4594 HSR cells. The measurements are conducted using a practically deployed and operative GSM-Railway (GSM-R) system to reflect the real conditions of the HSR channels. The proposed model is validated by the measurements conducted in a different operative HSR line. Finally, a heuristic method to design the BS separation distance is proposed, and it is found that using an improved propagation model can theoretically save around 2/5 cost of the BSs.",finance,262
10.1109/comst.2018.2843533,filtered,IEEE Communications Surveys & Tutorials,IEEE,2018-12-01 00:00:00,ieeexplore,the dark side(-channel) of mobile devices: a survey on network traffic analysis,https://ieeexplore.ieee.org/document/8371242/,"In recent years, mobile devices (e.g., smartphones and tablets) have met an increasing commercial success and have become a fundamental element of the everyday life for billions of people all around the world. Mobile devices are used not only for traditional communication activities (e.g., voice calls and messages) but also for more advanced tasks made possible by an enormous amount of multi-purpose applications (e.g., finance, gaming, and shopping). As a result, those devices generate a significant network traffic (a consistent part of the overall Internet traffic). For this reason, the research community has been investigating security and privacy issues that are related to the network traffic generated by mobile devices, which could be analyzed to obtain information useful for a variety of goals (ranging from fine-grained user profiling to device security and network optimization). In this paper, we review the works that contributed to the state of the art of network traffic analysis targeting mobile devices. In particular, we present a systematic classification of the works in the literature according to three criteria: 1) the goal of the analysis; 2) the point where the network traffic is captured; and 3) the targeted mobile platforms. In this survey, we consider points of capturing such as Wi-Fi access points, software simulation, and inside real mobile devices or emulators. For the surveyed works, we review and compare analysis techniques, validation methods, and achieved results. We also discuss possible countermeasures, challenges, and possible directions for future research on mobile traffic analysis and other emerging domains (e.g., Internet of Things). We believe our survey will be a reference work for researchers and practitioners in this research field.",finance,263
10.1109/mcit.2010.5444840,filtered,2010 International Conference on Multimedia Computing and Information Technology (MCIT),IEEE,2010-03-04 00:00:00,ieeexplore,associative classification techniques for predicting e-banking phishing websites,https://ieeexplore.ieee.org/document/5444840/,"This paper presents a novel approach to overcome the difficulty and complexity in detecting and predicting e-banking phishing website. We proposed an intelligent resilient and effective model that is based on using association and classification Data Mining algorithms. These algorithms were used to characterize and identify all the factors and rules in order to classify the phishing website and the relationship that correlate them with each other. We implemented six different classification algorithm and techniques to extract the phishing training data sets criteria to classify their legitimacy. We also compared their performances, accuracy, number of rules generated and speed. The rules generated from the associative classification model showed the relationship between some important characteristics like URL and Domain Identity, and Security and Encryption criteria in the final phishing detection rate. The experimental results demonstrated the feasibility of using Associative Classification techniques in real applications and its better performance as compared to other traditional classifications algorithms.",finance,264
10.1109/bigcomp.2019.8679121,filtered,2019 IEEE International Conference on Big Data and Smart Computing (BigComp),IEEE,2019-03-02 00:00:00,ieeexplore,question understanding based on sentence embedding on dialog systems for banking service,https://ieeexplore.ieee.org/document/8679121/,"This paper introduce a question understanding system to respond appropriate answers in a dialog system for banking services. The question understanding system provides an automated response service in a specific domain (e.g. banking). This can increase response rate of a customer counseling service, and improve business efficiency and expertise. The question understanding system classify domains, specific categories, and speech acts of questions. Finally, the system analyze meanings and intents of the questions, and searching correct answers even various input sentences. In this paper, we describe methods of keyword tokenizing, pattern recognition, sentence embedding, analyzing dialogue intention, and searching similar FAQs. Through these methods, we have developed the question understanding unit in a real interactive system for financial services for real insurance companies and banks, and analyze the usefulness of the system through practical system implementation examples.",finance,265
10.1109/mipro.2016.7522376,filtered,"2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",IEEE,2016-06-03 00:00:00,ieeexplore,reciprocal payers identification in banking logs using sat solvers,https://ieeexplore.ieee.org/document/7522376/,"In this paper we presented solvers for satisfiability testing (SAT) as a novel approach to finding reciprocal payers in banking logs. A term “reciprocal payers” is usually treated as general fraud by using standard techniques such as expert systems, machine learning and in recent times social network analysis. SAT as a technique for data analysis was abandoned due to the unfeasibility of SAT solvers. SAT solvers, however continued to develop in the hardware and software verification communities. We presented a proof-of-concept solution for identification of reciprocal payers (formally called a clique), which is a group of bank clients that issue payments to each other (each member to each member). We do not use real data due to client confidentiality, but the reader can see the principle. In the basic approach it is assumed that each client has only one account, and in the extended, second approach, it was allowed that a client can have more than one account.",finance,266
10.1109/access.2020.3015616,filtered,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,intelligent performance-aware adaptation of control policies for optimizing banking teller process using machine learning,https://ieeexplore.ieee.org/document/9163345/,"In the current banking systems and business processes, the permission granted to employees is controlled and managed by the configured access control methods, in which static role-based models focus on access to information and functions. The deployed configuration is not reviewed/updated systematically and is handled manually by managers. Consequently, banks and companies are looking for systems and applications to automate and optimize their business processes and data management intelligently. In this context, the notion of integrating machine learning (ML) techniques in banking business processes has emerged. In order to build an intelligent and systematic solution, we combine in this paper ML and dynamic authorization techniques to enable performance-based policy evaluation into the banking teller process, where policies adapt to the changes recognized by the ML model. The objective of this work is to focus on the banking teller process that may be generalized to other operational banking processes. In this context, we propose in this paper a new model providing Intelligent Performance-Aware Adaptation of Roles and Policy Control using a support vector machine (SVM). We demonstrate that our model is capable of assessing the deployed control policies and updating them systematically with new roles and authorization levels based on tellers' performance, work history, and system constraints. We evaluated different machine learning models on a real dataset generated from a real-life banking environment. Experimental results explore the relevance and efficiency of our proposed scheme in terms of prediction accuracy, required authorizations, transaction time, and employees' working hours.",finance,267
10.1109/icisc.2018.8398982,filtered,2018 2nd International Conference on Inventive Systems and Control (ICISC),IEEE,2018-01-20 00:00:00,ieeexplore,a rule-based classification of short message service type,https://ieeexplore.ieee.org/document/8398982/,"Short message service (SMS) is one of the most popular means of communication due to its type of usages like one-time passwords(OTPs), banking transaction alerts, and other promotional messages. SMS is the most time-sensitive channel of communication that demands service providers to send any information globally without any delay in respective time zone. For successful implementation of this service, the Telecom Regulatory of India (TRAI) has recommended certain guidelines that must be followed for the delivery of short messages. The prevalent practice in the industry is to store rules or raw string in a database and to match with incoming SMSs in synchronous mode. However, there are various shortcomings in this traditional approach that have been also discussed in this paper. Further, to address these issues, this research work proposes a novel approach for automated classification of SMSs in real time by the SMS service using a rule-based system of database template matching. Further to validate the proposed approach, SMS database of Netcore Solutions Pvt Limited, India, has been used for training the proposed algorithm. Proposed rule-based classifier keeps learning and updating the training dataset as more SMSs are accumulated and processed through the server. Other advantages, in terms of performance metrics, of the classification using the proposed rule-based database template matching over the traditional database matching approach have been reported in this work as evaluation measures. Reported rule-based SMS classification algorithm shows highest average classification accuracy of 100% which is better as compared to the similar research works already available in the literature.",finance,268
10.1109/ccwc47524.2020.9031269,filtered,2020 10th Annual Computing and Communication Workshop and Conference (CCWC),IEEE,2020-01-08 00:00:00,ieeexplore,an automated framework for real-time phishing url detection,https://ieeexplore.ieee.org/document/9031269/,"An increasing number of services, including banking and social networking, are being integrated with world wide web in recent years. The crux of this increasing dependence on the internet is the rise of different kinds of cyberattacks on unsuspecting users. One such attack is phishing, which aims at stealing user information via deceptive websites. The primary defense against phishing consists of maintaining a black list of the phishing URLs. However, a black list approach is reactive and cannot defend against new phishing websites. For this reason, a number of research have been done on using machine learning techniques to detect previously unseen phishing URLs. While they show promising results, any such implementation is yet to be seen. This is because 1) little work has been done on developing a complete end-to-end framework for phishing URL detection 2) it is prohibitively slow to detect phishing URLs using machine learning algorithms. In this work we address these two issues by formulating a robust framework for fast and automated detection of phishing URLs. We have validated our framework with a real dataset achieving 87% accuracy in a real-time setup.",finance,269
10.1109/icdmw.2016.0185,filtered,2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW),IEEE,2016-12-15 00:00:00,ieeexplore,financial data analysis with pgms using amidst,https://ieeexplore.ieee.org/document/7836816/,"The AMIDST Toolbox an open source Java 8 library for scalable learning of probabilistic graphical models (PGMs) based on both batch and streaming data. An important application domain with streaming data characteristics is the banking sector, where we may want to monitor individual customers (based on their financial situation and behavior) as well as the general economic climate. Using a real financial data set from a Spanish bank, we have previously proposed and demonstrated a novel PGM framework for performing this type of data analysis with particular focus on concept drift. The framework is implemented in the AMIDST Toolbox, which was also used to conduct the reported analyses. In this paper, we provide an overview of the toolbox and illustrate with code examples how the toolbox can be used for setting up and performing analyses of this particular type.",finance,270
10.1109/icit52682.2021.9491636,filtered,2021 International Conference on Information Technology (ICIT),IEEE,2021-07-15 00:00:00,ieeexplore,improvement of personal loans granting methods in banks using machine learning methods and approaches in palestine,https://ieeexplore.ieee.org/document/9491636/,"For banking organizations, loan approval and risk assessment which is related is a very complex and significant process which is needs a high effort for relevant employee or manager to take a decision, because of manual or traditional methods that used in banks. The banking industry still needs a more precise method of predictive modeling for several problems. In general, for financial institutions and especially for banks forecasting credit defaulters is a hard challenge. The primary role of the current systems is to accept, or sending loan application to a specific level of approval to be studied and it is very difficult to foresee the probability of the borrower for paying the due dues amount without using methods to predict. Machine learning (ML) techniques and the algorithm that belongs to are a very amazing and promising technique in predicting for a large amount of data. Our research proposed to study three machine learning algorithms [1], Decision Tree (DT), Logistic Regression (LR), and Random Forest (RF), by using real data collected from Quds Bank with a variables that cover credit restriction and regulator instructions. The algorithm has been implemented to predict the loan approval of customers and the output tested in terms of the predicted accuracy.",finance,271
10.1109/sist50301.2021.9465891,filtered,2021 IEEE International Conference on Smart Information Systems and Technologies (SIST),IEEE,2021-04-30 00:00:00,ieeexplore,information - education milieu: methodology of forming knowledge content and practice oriented training of it-disciplines,https://ieeexplore.ieee.org/document/9465891/,"The article herein considers the innovative methodology of forming the knowledge content, based on ontological model. The basic methodology concepts are project-oriented technology of CDIO training and graduate's competence model, and implementation tool is information - education milieu. Information - education milieu allows implement adaptive, in compliance with production requirements, knowledge trend planning and knowledge content forming both for specialty degree programs disciplines and individual training trajectory, using for those aims the smart-contract.For the discipline Technologies of processing the distributed applications there have been shown examples of using the technologies on modeling the knowledge, connected with designing the real time systems of parallel and distributed applications, where as a project there has been used the banking system.The methodology's software implementation has been fulfilled in the form of a web-application and currently it is going through approbation at the chair Computer and software engineering of Turan university.",finance,272
10.1109/smcia.2005.1466968,filtered,"Proceedings of the 2005 IEEE Midnight-Summer Workshop on Soft Computing in Industrial Applications, 2005. SMCia/05.",IEEE,2005-06-30 00:00:00,ieeexplore,neuro-classification of fatigued bill based on tensional acoustic signal,https://ieeexplore.ieee.org/document/1466968/,"In the practical use of automated teller machines (ATM's), dealing with much fatigued bills causes serious trouble. To avoid this problem, rapid development of automatic classification methods that can be implemented on banking machines is desired. We propose a new automatic classification method of fatigued bill based on acoustic signal feature of a banking machine. Feeding a bill to a banking machine, a typical acoustic signal is emitted in the transportation part of the machine by tensioning the slackness of the bill transportation. The proposed method focuses on the fact that the tensional acoustic signal features differ in fatigue level of the bill, and uses spectral information of the tensional acoustic signal as the feature for classification of fatigued bill. The proposed method also uses the self organizing map (SOM) type neural network as the classifier to get high classification performance. Simulation results by using real tensional acoustic signal show the effectiveness of the proposed method.",finance,273
10.1109/itng.2010.117,filtered,2010 Seventh International Conference on Information Technology: New Generations,IEEE,2010-04-14 00:00:00,ieeexplore,predicting phishing websites using classification mining techniques with experimental case studies,https://ieeexplore.ieee.org/document/5501434/,"Classification Data Mining (DM) Techniques can be a very useful tool in detecting and identifying e-banking phishing websites. In this paper, we present a novel approach to overcome the difficulty and complexity in detecting and predicting e-banking phishing website. We proposed an intelligent resilient and effective model that is based on using association and classification Data Mining algorithms. These algorithms were used to characterize and identify all the factors and rules in order to classify the phishing website and the relationship that correlate them with each other. We implemented six different classification algorithm and techniques to extract the phishing training data sets criteria to classify their legitimacy. We also compared their performances, accuracy, number of rules generated and speed. A Phishing Case study was applied to illustrate the website phishing process. The rules generated from the associative classification model showed the relationship between some important characteristics like URL and Domain Identity, and Security and Encryption criteria in the final phishing detection rate. The experimental results demonstrated the feasibility of using Associative Classification techniques in real applications and its better performance as compared to other traditional classifications algorithms.",finance,274
10.1109/comitcon.2019.8862225,filtered,"2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)",IEEE,2019-02-16 00:00:00,ieeexplore,stock market analysis using supervised machine learning,https://ieeexplore.ieee.org/document/8862225/,"Stock market or Share market is one of the most complicated and sophisticated way to do business. Small ownerships, brokerage corporations, banking sector, all depend on this very body to make revenue and divide risks; a very complicated model. However, this paper proposes to use machine learning algorithm to predict the future stock price for exchange by using open source libraries and preexisting algorithms to help make this unpredictable format of business a little more predictable. We shall see how this simple implementation will bring acceptable results. The outcome is completely based on numbers and assumes a lot of axioms that may or may not follow in the real world so as the time of prediction.",finance,275
10.1007/978-3-030-54173-6_10,filtered,"Robotics, AI, and Humanity",Springer,2021-01-01 00:00:00,springer,"humans judged by machines: the rise of artificial intelligence in finance, insurance, and real estate",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-54173-6_10,"There are opportunities but also worrisome trends as AI is applied in finance, insurance, and real estate. In these domains, persons are increasingly assessed and judged by machines. The financial technology (Fintech) landscape ranges from automation of office procedures, to new approaches for storing and transferring value, to the granting of credit. The Fintech landscape can be separated into “incrementalist Fintech” and “futurist Fintech.” Incrementalist Fintech uses data, algorithms, and software to complement professionals who perform traditional tasks of existing financial institutions. It promises financial inclusion, but this inclusion can be predatory, creepy, and subordinating. These forms of financial inclusion undermine their solvency, dignity, and political power of borrowers. Futurist Fintech’s promoters claim to be more equitable, but are likely to falter in their aspiration to substitute technology for key financial institutions. When used to circumvent or co-opt state monetary authorities, both incrementalist and futurist Fintech expose deep problems at the core of the contemporary digitization of finance.",finance,276
10.1007/s42461-018-0036-4,filtered,"Mining, Metallurgy & Exploration",Springer,2019-02-01 00:00:00,springer,grinding and flotation optimization using operational intelligence,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s42461-018-0036-4,"In recent years, metal-producing companies have increased their investment in automation and technological innovation, embracing new opportunities to enable transformational change. Transformation to a digital plant can fundamentally revolutionize how industrial complexes operate. The abundant and growing quantity of real-time data and events collected in the grinding and flotation circuits in a mineral processing plant can be used to solve operational issues and optimize plant performance. A grade recovery model is used to identify the best operating conditions in real time. The strategy for increasing the value of instrumentation in current plants is reviewed. An optimal Gaudin size distribution model provides augmented information from traditional sensors to find the optimal grind cut size to reduce metal losses in the flotation circuits. Sensors in flotation circuits enable an estimate of the recovery and determination of the optimal froth depth and aeration using an air hold up flotation model. A strategy of classifying data for on-line generation of insights to using operational intelligence tools is described. The implementation of a recovery/grind strategy with industrial examples in non-ferrous mineral processing is presented.",finance,277
10.1007/978-3-319-99978-4_29,filtered,Artificial Neural Networks in Pattern Recognition,Springer,2018-01-01 00:00:00,springer,atm protection using embedded deep learning solutions,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-319-99978-4_29,"Last decade advances in Deep Learning methods lead to sensible improvements in state of the art results in many real world applications, thanks to the exploitation of particular Artificial Neural Networks architectures. In this paper we present an investigation of the application of such kind of structures to a Video Surveillance case of study, in which the special nature and the small amount of available data increases the difficulties during the training phase. The analyzed scenario involves the protection of Automatic Teller Machines (ATM), representing a sensitive problem in the world of both banking and public security. Because of the critical issues related to this environment, even apparently small improvements in either accuracy or responsiveness of surveillance systems can produce a fundamental contribution. Even if the experimentation has been reproduced in an artificial scenario, the results show that the implemented architecture is able to classify depth data in real-time on an embedded system, detecting all the test attacks in a few seconds.",finance,278
10.1007/s00068-016-0757-3,filtered,European Journal of Trauma and Emergency Surgery,Springer,2017-12-01 00:00:00,springer,survival prediction of trauma patients: a study on us national trauma data bank,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00068-016-0757-3,"Background Exceptional circumstances like major incidents or natural disasters may cause a huge number of victims that might not be immediately and simultaneously saved. In these cases it is important to define priorities avoiding to waste time and resources for not savable victims. Trauma and Injury Severity Score (TRISS) methodology is the well-known and standard system usually used by practitioners to predict the survival probability of trauma patients. However, practitioners have noted that the accuracy of TRISS predictions is unacceptable especially for severely injured patients. Thus, alternative methods should be proposed. Methods In this work we evaluate different approaches for predicting whether a patient will survive or not according to simple and easily measurable observations. We conducted a rigorous, comparative study based on the most important prediction techniques using real clinical data of the US National Trauma Data Bank. Results Empirical results show that well-known Machine Learning classifiers can outperform the TRISS methodology. Based on our findings, we can say that the best approach we evaluated is Random Forest: it has the best accuracy, the best area under the curve, and k-statistic, as well as the second-best sensitivity and specificity. It has also a good calibration curve. Furthermore, its performance monotonically increases as the dataset size grows, meaning that it can be very effective to exploit incoming knowledge. Considering the whole dataset, it is always better than TRISS. Finally, we implemented a new tool to compute the survival of victims. This will help medical practitioners to obtain a better accuracy than the TRISS tools. Conclusion Random Forests may be a good candidate solution for improving the predictions on survival upon the standard TRISS methodology.",finance,279
10.3103/s0146411615050065,filtered,Automatic Control and Computer Sciences,Springer,2015-09-01 00:00:00,springer,simulation of digital filter banks and signal classification in wideband monitoring tasks,http://link.springer.com/openurl/fulltext?id=doi:10.3103/S0146411615050065,"The paper is devoted to handling radio monitoring tasks (wideband monitoring tasks) in real time. We suggest a new multichannel modification of the weighted overlap-add algorithm (WOLA-algorithm) for processing vector (multichannel) signals and performing multichannel signal classification. Filter bank implementation is considered using computers based on the CUDA (Compute Unified Device Architecture) technology. We show that CUDA is efficient for large signal sets due to its low temporal costs. We study signal classification in filter bank channels for different signal-tonoise ratios using binary decision trees, the iterative procedure Adaboost, and neural networks. Our experiments provided a total classification error less than 10%.",finance,280
http://arxiv.org/abs/2202.11296v1,filtered,arxiv,arxiv,2022-02-23 00:00:00,arxiv,deep reinforcement learning: opportunities and challenges,http://arxiv.org/abs/2202.11296v1,"This article is a gentle discussion about the field of reinforcement learning
for real life, about opportunities and challenges, with perspectives and
without technical details, touching a broad range of topics. The article is
based on both historical and recent research papers, surveys, tutorials, talks,
blogs, and books. Various groups of readers, like researchers, engineers,
students, managers, investors, officers, and people wanting to know more about
the field, may find the article interesting. In this article, we first give a
brief introduction to reinforcement learning (RL), and its relationship with
deep learning, machine learning and AI. Then we discuss opportunities of RL, in
particular, applications in products and services, games, recommender systems,
robotics, transportation, economics and finance, healthcare, education,
combinatorial optimization, computer systems, and science and engineering. The
we discuss challenges, in particular, 1) foundation, 2) representation, 3)
reward, 4) model, simulation, planning, and benchmarks, 5) learning to learn
a.k.a. meta-learning, 6) off-policy/offline learning, 7) software development
and deployment, 8) business perspectives, and 9) more challenges. We conclude
with a discussion, attempting to answer: ""Why has RL not been widely adopted in
practice yet?"" and ""When is RL helpful?"".",finance,281
http://arxiv.org/abs/2202.07787v1,filtered,arxiv,arxiv,2022-02-15 00:00:00,arxiv,trustworthy anomaly detection: a survey,http://arxiv.org/abs/2202.07787v1,"Anomaly detection has a wide range of real-world applications, such as bank
fraud detection and cyber intrusion detection. In the past decade, a variety of
anomaly detection models have been developed, which lead to big progress
towards accurately detecting various anomalies. Despite the successes, anomaly
detection models still face many limitations. The most significant one is
whether we can trust the detection results from the models. In recent years,
the research community has spent a great effort to design trustworthy machine
learning models, such as developing trustworthy classification models. However,
the attention to anomaly detection tasks is far from sufficient. Considering
that many anomaly detection tasks are life-changing tasks involving human
beings, labeling someone as anomalies or fraudsters should be extremely
cautious. Hence, ensuring the anomaly detection models conducted in a
trustworthy fashion is an essential requirement to deploy the models to conduct
automatic decisions in the real world. In this brief survey, we summarize the
existing efforts and discuss open problems towards trustworthy anomaly
detection from the perspectives of interpretability, fairness, robustness, and
privacy-preservation.",finance,282
http://arxiv.org/abs/2202.02751v1,filtered,arxiv,arxiv,2022-02-06 00:00:00,arxiv,pipe overflow: smashing voice authentication for fun and profit,http://arxiv.org/abs/2202.02751v1,"Recent years have seen a surge of popularity of acoustics-enabled personal
devices powered by machine learning. Yet, machine learning has proven to be
vulnerable to adversarial examples. Large number of modern systems protect
themselves against such attacks by targeting the artificiality, i.e., they
deploy mechanisms to detect the lack of human involvement in generating the
adversarial examples. However, these defenses implicitly assume that humans are
incapable of producing meaningful and targeted adversarial examples. In this
paper, we show that this base assumption is wrong. In particular, we
demonstrate that for tasks like speaker identification, a human is capable of
producing analog adversarial examples directly with little cost and
supervision: by simply speaking through a tube, an adversary reliably
impersonates other speakers in eyes of ML models for speaker identification.
Our findings extend to a range of other acoustic-biometric tasks such as
liveness, bringing into question their use in security-critical settings in
real life, such as phone banking.",finance,283
http://arxiv.org/abs/2106.12563v2,filtered,arxiv,arxiv,2021-06-23 00:00:00,arxiv,feature attributions and counterfactual explanations can be manipulated,http://arxiv.org/abs/2106.12563v2,"As machine learning models are increasingly used in critical decision-making
settings (e.g., healthcare, finance), there has been a growing emphasis on
developing methods to explain model predictions. Such \textit{explanations} are
used to understand and establish trust in models and are vital components in
machine learning pipelines. Though explanations are a critical piece in these
systems, there is little understanding about how they are vulnerable to
manipulation by adversaries. In this paper, we discuss how two broad classes of
explanations are vulnerable to manipulation. We demonstrate how adversaries can
design biased models that manipulate model agnostic feature attribution methods
(e.g., LIME \& SHAP) and counterfactual explanations that hill-climb during the
counterfactual search (e.g., Wachter's Algorithm \& DiCE) into
\textit{concealing} the model's biases. These vulnerabilities allow an
adversary to deploy a biased model, yet explanations will not reveal this bias,
thereby deceiving stakeholders into trusting the model. We evaluate the
manipulations on real world data sets, including COMPAS and Communities \&
Crime, and find explanations can be manipulated in practice.",finance,284
http://arxiv.org/abs/2106.07875v1,filtered,arxiv,arxiv,2021-06-15 00:00:00,arxiv,s-lime: stabilized-lime for model explanation,http://arxiv.org/abs/2106.07875v1,"An increasing number of machine learning models have been deployed in domains
with high stakes such as finance and healthcare. Despite their superior
performances, many models are black boxes in nature which are hard to explain.
There are growing efforts for researchers to develop methods to interpret these
black-box models. Post hoc explanations based on perturbations, such as LIME,
are widely used approaches to interpret a machine learning model after it has
been built. This class of methods has been shown to exhibit large instability,
posing serious challenges to the effectiveness of the method itself and harming
user trust. In this paper, we propose S-LIME, which utilizes a hypothesis
testing framework based on central limit theorem for determining the number of
perturbation points needed to guarantee stability of the resulting explanation.
Experiments on both simulated and real world data sets are provided to
demonstrate the effectiveness of our method.",finance,285
http://arxiv.org/abs/2010.02716v2,filtered,arxiv,arxiv,2020-10-03 00:00:00,arxiv,ai lifecycle models need to be revised. an exploratory study in fintech,http://arxiv.org/abs/2010.02716v2,"Tech-leading organizations are embracing the forthcoming artificial
intelligence revolution. Intelligent systems are replacing and cooperating with
traditional software components. Thus, the same development processes and
standards in software engineering ought to be complied in artificial
intelligence systems. This study aims to understand the processes by which
artificial intelligence-based systems are developed and how state-of-the-art
lifecycle models fit the current needs of the industry. We conducted an
exploratory case study at ING, a global bank with a strong European base. We
interviewed 17 people with different roles and from different departments
within the organization. We have found that the following stages have been
overlooked by previous lifecycle models: data collection, feasibility study,
documentation, model monitoring, and model risk assessment. Our work shows that
the real challenges of applying Machine Learning go much beyond sophisticated
learning algorithms - more focus is needed on the entire lifecycle. In
particular, regardless of the existing development tools for Machine Learning,
we observe that they are still not meeting the particularities of this field.",finance,286
http://arxiv.org/abs/1902.06343v2,filtered,arxiv,arxiv,2019-02-17 00:00:00,arxiv,"fetch: a deep-learning based classifier for fast transient
  classification",http://arxiv.org/abs/1902.06343v2,"With the upcoming commensal surveys for Fast Radio Bursts (FRBs), and their
high candidate rate, usage of machine learning algorithms for candidate
classification is a necessity. Such algorithms will also play a pivotal role in
sending real-time triggers for prompt follow-ups with other instruments. In
this paper, we have used the technique of Transfer Learning to train the
state-of-the-art deep neural networks for classification of FRB and Radio
Frequency Interference (RFI) candidates. These are convolutional neural
networks which work on radio frequency-time and dispersion measure-time images
as the inputs. We trained these networks using simulated FRBs and real RFI
candidates from telescopes at the Green Bank Observatory. We present 11 deep
learning models, each with an accuracy and recall above 99.5% on our test
dataset comprising of real RFI and pulsar candidates. As we demonstrate, these
algorithms are telescope and frequency agnostic and are able to detect all FRBs
with signal-to-noise ratios above 10 in ASKAP and Parkes data. We also provide
an open-source python package FETCH (Fast Extragalactic Transient Candidate
Hunter) for classification of candidates, using our models. Using FETCH, these
models can be deployed along with any commensal search pipeline for real-time
candidate classification.",finance,287
http://arxiv.org/abs/1903.03202v2,filtered,arxiv,arxiv,2019-02-17 00:00:00,arxiv,nowcasting recessions using the svm machine learning algorithm,http://arxiv.org/abs/1903.03202v2,"We introduce a novel application of Support Vector Machines (SVM), an
important Machine Learning algorithm, to determine the beginning and end of
recessions in real time. Nowcasting, ""forecasting"" a condition about the
present time because the full information about it is not available until
later, is key for recessions, which are only determined months after the fact.
We show that SVM has excellent predictive performance for this task, and we
provide implementation details to facilitate its use in similar problems in
economics and finance.",finance,288
http://arxiv.org/abs/1811.08933v2,filtered,arxiv,arxiv,2018-11-18 00:00:00,arxiv,analyzing machine learning workloads using a detailed gpu simulator,http://arxiv.org/abs/1811.08933v2,"Most deep neural networks deployed today are trained using GPUs via
high-level frameworks such as TensorFlow and PyTorch. This paper describes
changes we made to the GPGPU-Sim simulator to enable it to run PyTorch by
running PTX kernels included in NVIDIA's cuDNN library. We use the resulting
modified simulator, which has been made available publicly with this paper, to
study some simple deep learning workloads. With our changes to GPGPU-Sim's
functional simulation model, we find GPGPU-Sim performance model running a
cuDNN enabled implementation of LeNet for MNIST reports results within 30% of
real hardware. Using GPGPU-Sim's AerialVision performance analysis tool we
observe that cuDNN API calls contain many varying phases and appear to include
potentially inefficient microarchitecture behaviour such as DRAM partition bank
camping, at least when executed on GPGPU-Sim's current performance model.",finance,289
http://arxiv.org/abs/1802.06259v2,filtered,arxiv,arxiv,2018-02-17 00:00:00,arxiv,"exact and consistent interpretation for piecewise linear neural
  networks: a closed form solution",http://arxiv.org/abs/1802.06259v2,"Strong intelligent machines powered by deep neural networks are increasingly
deployed as black boxes to make decisions in risk-sensitive domains, such as
finance and medical. To reduce potential risk and build trust with users, it is
critical to interpret how such machines make their decisions. Existing works
interpret a pre-trained neural network by analyzing hidden neurons, mimicking
pre-trained models or approximating local predictions. However, these methods
do not provide a guarantee on the exactness and consistency of their
interpretation. In this paper, we propose an elegant closed form solution named
$OpenBox$ to compute exact and consistent interpretations for the family of
Piecewise Linear Neural Networks (PLNN). The major idea is to first transform a
PLNN into a mathematically equivalent set of linear classifiers, then interpret
each linear classifier by the features that dominate its prediction. We further
apply $OpenBox$ to demonstrate the effectiveness of non-negative and sparse
constraints on improving the interpretability of PLNNs. The extensive
experiments on both synthetic and real world data sets clearly demonstrate the
exactness and consistency of our interpretation.",finance,290
http://arxiv.org/abs/1710.07709v1,filtered,arxiv,arxiv,2017-10-20 00:00:00,arxiv,"solving the ""false positives"" problem in fraud prediction",http://arxiv.org/abs/1710.07709v1,"In this paper, we present an automated feature engineering based approach to
dramatically reduce false positives in fraud prediction. False positives plague
the fraud prediction industry. It is estimated that only 1 in 5 declared as
fraud are actually fraud and roughly 1 in every 6 customers have had a valid
transaction declined in the past year. To address this problem, we use the Deep
Feature Synthesis algorithm to automatically derive behavioral features based
on the historical data of the card associated with a transaction. We generate
237 features (>100 behavioral patterns) for each transaction, and use a random
forest to learn a classifier. We tested our machine learning model on data from
a large multinational bank and compared it to their existing solution. On an
unseen data of 1.852 million transactions, we were able to reduce the false
positives by 54% and provide a savings of 190K euros. We also assess how to
deploy this solution, and whether it necessitates streaming computation for
real time scoring. We found that our solution can maintain similar benefits
even when historical features are computed once every 7 days.",finance,291
http://arxiv.org/abs/1601.02781v2,filtered,arxiv,arxiv,2016-01-12 00:00:00,arxiv,bamcloud: a cloud based mobile biometric authentication framework,http://arxiv.org/abs/1601.02781v2,"With an exponential increase in number of users switching to mobile banking,
various countries are adopting biometric solutions as security measures. The
main reason for biometric technologies becoming more common in the everyday
lives of consumers is because of the facility to easily capture biometric data
in real time, using their mobile phones. Biometric technologies are providing
the potential security framework to make banking more convenient and secure
than it has ever been. At the same time, the exponential growth of enrollment
in the biometric system produces massive amount of high dimensionality data
that leads to degradation in the performance of the mobile banking systems.
Therefore, in order to overcome the performance issues arising due to this data
deluge, this paper aims to propose a distributed mobile biometric system based
on a high performance cluster Cloud. High availability, better time efficiency
and scalability are some of the added advantages of using the proposed system.
In this paper a Cloud based mobile biometric authentication framework
(BAMCloud) is proposed that uses dynamic signatures and performs
authentication. It includes the steps involving data capture using any handheld
mobile device, then storage, preprocessing and training the system in a
distributed manner over Cloud. For this purpose we have implemented it using
MapReduce on Hadoop platform and for training Levenberg-Marquardt
backpropagation neural network has been used. Moreover, the methodology adopted
is very novel as it achieves a speedup of 8.5x and a performance of 96.23%.
Furthermore, the cost benefit analysis of the implemented system shows that the
cost of implementation and execution of the system is lesser than the existing
ones. The experiments demonstrate that the better performance is achieved by
proposed framework as compared to the other methods used in the recent
literature.",finance,292
10.1016/j.engappai.2019.103427,filtered,Engineering Applications of Artificial Intelligence,scopus,2020-03-01,sciencedirect,stochastic parallel extreme artificial hydrocarbon networks: an implementation for fast and robust supervised machine learning in high-dimensional data,https://api.elsevier.com/content/abstract/scopus_id/85076620125,"Artificial hydrocarbon networks (AHN) – a supervised learning method inspired on organic chemical structures and mechanisms – have shown improvements in predictive power and interpretability in comparison with other well-known machine learning models. However, AHN are very time-consuming that are not able to deal with large data until now. In this paper, we introduce the stochastic parallel extreme artificial hydrocarbon networks (SPE-AHN), an algorithm for fast and robust training of supervised AHN models in high-dimensional data. This training method comprises a population-based meta-heuristic optimization with defined individual encoding and objective function related to the AHN-model, an implementation in parallel-computing, and a stochastic learning approach for consuming large data. We conducted three experiments with synthetic and real data sets to validate the training execution time and performance of the proposed algorithm. Experimental results demonstrated that the proposed SPE-AHN outperforms the original-AHN method, increasing the speed of training more than 
                        
                           10
                           ,
                           000
                           x
                        
                      times in the worst case scenario. Additionally, we present two case studies in real data sets for solar-panel deployment prediction (regression problem), and human falls and daily activities classification in healthcare monitoring systems (classification problem). These case studies showed that SPE-AHN improves the state-of-the-art machine learning models in both engineering problems. We anticipate our new training algorithm to be useful in many applications of AHN like robotics, finance, medical engineering, aerospace, and others, in which large amounts of data (e.g. big data) is essential.",finance,293
10.1016/j.future.2018.01.043,filtered,Future Generation Computer Systems,scopus,2019-07-01,sciencedirect,combining humans and machines for the future: a novel procedure to predict human interest,https://api.elsevier.com/content/abstract/scopus_id/85042366370,"This paper proposes a method to quantify interest. In common terminology, when we engage with an object, e.g. Online Games, Social Networking Websites, Mobile Apps, etc., there is a degree of interest between us and the object. But, owing to the lack of a procedure that can quantify interest, we are unable to tell by how ‘much’ of a factor are we interested in the object. In other words, can we find a number for someone’s interest? In this article, we propose a method that uses the principle of Bayesian Inference to tackle this issue. We formulate the “interest estimation problem” as a state estimation problem to deduce interest (in any object) indirectly from user activity. Activity caused by interest is computed through a subjective–objectiveweighted approach, then using indirect inference rules, we provide numerical estimates of interest. To do that, we model the dynamics of interest through the Ornstein–Uhlenbeck process. To further enhance the base performance, we draw inspiration from Stochastic Volatility models from Finance. Subsequently, drawing upon a self-adapting transfer function, we provide an avant-garde statistical procedure to model the transformation of interest into activity. The individual contributions are then combined and a solution is provided via Particle filters. Validation of the method is done in two ways. (1) Experimentation is performed on real datasets. Through numerical investigation we have found that the method shows good performance. (2) We implement the framework as a Web application and deploy it on an Enterprise Service Bus. The framework has been successfully hosted on a Cloud based Virtualized testbed consisting of several Virtual Machines constructed over XENServer as the underlying hypervisor. Through this experimental setup, we show the efficacy of the proposed algorithm in estimating interest, at much the same time, we demonstrate the viability of the method in practical cloud based deployment scenarios.",finance,294
10.1016/j.patrec.2017.08.024,filtered,Pattern Recognition Letters,scopus,2018-04-01,sciencedirect,end-to-end neural network architecture for fraud scoring in card payments,https://api.elsevier.com/content/abstract/scopus_id/85028581551,"Millions of euros are lost every year due to fraudulent card transactions. The design and implementation of efficient fraud detection methods is mandatory to minimize such losses. In this paper, we present a neural network based system for fraud detection in banking systems. We use a real world dataset, and describe an end-to-end solution from the practitioner’s perspective, by focusing on the following crucial aspects: unbalancedness, data processing and cost metric evaluation. Our analysis shows that the proposed solution achieves comparable performance values with state-of-the-art proprietary and costly solutions.",finance,295
10.1016/j.procs.2018.10.523,filtered,Procedia Computer Science,scopus,2018-01-01,sciencedirect,foreign currency exchange rate prediction using neuro-fuzzy systems,https://api.elsevier.com/content/abstract/scopus_id/85061158308,"The complex nature of the foreign exchange (FOREX) market along with the increased interest towards the currency exchange market has prompted extensive research from various academic disciplines. With the inclusion of more in-depth analysis and forecasting methods, traders will be able to make an informed decision when trading. Therefore, an approach incorporating the use of historical data along with computational intelligence for analysis and forecasting is proposed in this paper. Firstly, the Gaussian Mixture Model method is applied for data partitioning on historical observations. While the antecedent part of the neuro-fuzzy system of AnYa type is initialised by the partitioning result, the consequent part is trained using the fuzzily weighted RLS algorithm based on the same data. Numerical examples based on the real currency exchange data demonstrated that the proposed approach trained with historical data produce promising results when used to forecast the future foreign exchange rates over a long-term period. Although implemented in an offline environment, it could potentially be utilised in real-time application in the future.",finance,296
10.1016/j.procs.2018.05.047,filtered,Procedia Computer Science,scopus,2018-01-01,sciencedirect,analysis of fuzzification process in fuzzy expert system,https://api.elsevier.com/content/abstract/scopus_id/85049088318,"The fuzzy expert systems are oriented towards handling uncertain or imprecise information. The fuzzy expert system is used in the domains where the input variables do not have fixed values. The success of fuzzy system depends upon the selection of appropriate membership function. The paper presents the analysis of fuzzification process of Fuzzy expert systems implemented in the domains of health care, education, career selection, real estate and finance. The parameters used for analyzing the systems are the input factors, type of membership function used for fuzzification, de-fuzzification of fuzzy sets generated. Based on analysis of the fuzzy expert system, the paper presents recommendations for selecting appropriate membership function. At the end paper presents guidelines for fuzzification process which can be useful in creating Fuzzy Expert System.",finance,297
10.1016/j.procs.2018.05.199,filtered,Procedia Computer Science,scopus,2018-01-01,sciencedirect,predictive modelling for credit card fraud detection using data analytics,https://api.elsevier.com/content/abstract/scopus_id/85049081698,"The finance and banking is very important sector in our present day generation, where almost every human has to deal with bank either physically or online [10]. The productivity and profitability of both public and private sector has tremendously increased because of banking information system. Nowadays most of E-commerce application system transactions are done through credit card and online net banking. These systems are vulnerable with new attacks and techniques at alarming rate. Fraud detection in banking is one of the vital aspects nowadays as finance is major sector in our life. As data is increasing in terms of Peta Bytes (PB) and to improve the performance of analytical server in model building, we have interface analytical framework with Hadoop which can read data efficiently and give to analytical server for fraud prediction. In this paper we have discussed a Big data analytical framework to process large volume of data and implemented various machine learning algorithms for fraud detection and observed their performance on benchmark dataset to detect frauds on real time basis there by giving low risk and high customer satisfaction.",finance,298
10.1016/j.cose.2015.04.002,filtered,Computers and Security,scopus,2015-09-01,sciencedirect,banksealer: a decision support system for online banking fraud analysis and investigation,https://api.elsevier.com/content/abstract/scopus_id/84940722662,"The significant growth of online banking frauds, fueled by the underground economy of malware, raised the need for effective fraud analysis systems. Unfortunately, almost all of the existing approaches adopt black box models and mechanisms that do not give any justifications to analysts. Also, the development of such methods is stifled by limited Internet banking data availability for the scientific community. In this paper we describe BankSealer, a decision support system for online banking fraud analysis and investigation. During a training phase, BankSealer builds easy-to-understand models for each customer's spending habits, based on past transactions. First, it quantifies the anomaly of each transaction with respect to the customer historical profile. Second, it finds global clusters of customers with similar spending habits. Third, it uses a temporal threshold system that measures the anomaly of the current spending pattern of each customer, with respect to his or her past spending behavior. With this threefold profiling approach, it mitigates the under-training due to the lack of historical data for building well-trained profiles, and the evolution of users' spending habits over time. At runtime, BankSealer supports analysts by ranking new transactions that deviate from the learned profiles, with an output that has an easily understandable, immediate statistical meaning.
                  Our evaluation on real data, based on fraud scenarios built in collaboration with domain experts that replicate typical, real-world attacks (e.g., credential stealing, banking trojan activity, and frauds repeated over time), shows that our approach correctly ranks complex frauds. In particular, we measure the effectiveness, the computational resource requirements and the capabilities of BankSealer to mitigate the problem of users that performed a low number of transactions. Our system ranks frauds and anomalies with up to 98% detection rate and with a maximum daily computation time of 4 min. Given the good results, a leading Italian bank deployed a version of BankSealer in their environment to analyze frauds.",finance,299
10.1186/1939-4551-7-15,filtered,World Allergy Organization Journal,scopus,2014-06-25,sciencedirect,"allergenius, an expert system for the interpretation of allergen microarray results",https://api.elsevier.com/content/abstract/scopus_id/84910666431,"Background An in vitro procedure based on a microarray containing many different allergen components has recently been introduced for use in allergy diagnosis. Recombinant and highly purified allergens belonging to different allergenic sources (inhalants, food, latex and hymenoptera) are present in the array. These components can either be genuine or cross-reactive, resistant or susceptible to heat and low pH, and innocuous or potentially dangerous. A large number of complex and heterogeneous relationships among these components has emerged, such that sometimes these interactions cannot be effectively managed by the allergist. In the 1960s, specialized languages and environments were developed to support the replacement of human experts with dedicated decision-making information systems. Currently, expert systems (ES) are advanced informatics tools that are widely used in medicine, engineering, finance and trading.
                  
                     Methods We developed an ES, named Allergenius ®, to support the interpretation of allergy tests based on microarray technology (ImmunoCAP ISAC ®). The ES was implemented using Flex, a LPA Win-Prolog shell. Rules representing the knowledge base (KB) were derived from the literature and specialized databases. The input data included the patient’s ID and disease(s), the results of either a skin prick test or specific IgE assays and ISAC results. The output was a medical report.
                  
                     Results The ES was first validated using artificial and real life cases and passed all in silico validations. Then, the opinions of allergists with experience in molecular diagnostics were compared with the ES reports. The Allergenius reports included all of the allergists’ opinions and considerations, as well as any additional information.
                  
                     Conclusions Allergenius is a trustable ES dedicated to molecular tests for allergy. In the present version, it provides a powerful method to understand ISAC results and to obtain a comprehensive interpretation of the patient’s IgE profiling.",finance,300
10.1016/j.procs.2014.09.087,filtered,Procedia Computer Science,scopus,2014-01-01,sciencedirect,volatility forecasting using a hybrid gjr-garch neural network model,https://api.elsevier.com/content/abstract/scopus_id/84937968773,"Volatility forecasting in the financial markets, along with the development of financial models, is important in the areas of risk management and asset pricing, among others. Previous testing has shown that asymmetric GARCH models outperform other GARCH family models with regard to volatility prediction. Utilizing this information, three popular Neural Network models (Feed-Forward with Back Propagation, Generalized Regression, and Radial Basis Function) are implemented to help improve the performance of the GJR(1,1) method for estimating volatility over the next forty-four trading days. During training and testing, four different economic cycles have been considered between 1997-2011 to represent real and contemporary periods of market calm and crisis. In addition to stress testing for different neural network architectures to assess their performance under various turmoil and normal situations in the U.S. market, their synergy along with another econometric model is also accessed.",finance,301
10.1016/j.eswa.2013.03.010,filtered,Expert Systems with Applications,scopus,2013-05-22,sciencedirect,analysing user trust in electronic banking using data mining methods,https://api.elsevier.com/content/abstract/scopus_id/84877872204,"The potential fraud problems, international economic crisis and the crisis of trust in markets have affected financial institutions, which have tried to maintain customer trust in many different ways. To maintain these levels of trust they have been forced to make significant adjustments to economic structures, in efforts to recoup their investments and maintain the loyalty of their customers. To achieve these objectives, the implementation of electronic banking for customers has been considered a successful strategy. The use of electronic banking in Spain in the last decade has been fostered due to its many advantages, giving rise to real integration of channels in financial institutions. This paper reviews different methods and techniques to determine which variables could be the most important to financial institutions in order to predict the likely levels of trust among electronic banking users including socio-demographic, economic, financial and behavioural strategic variables that entities have in their databases. To do so, the most recent advances in machine learning and soft-computing have been used, including a new selection operator for multiobjective genetic algorithms. The results obtained by the algorithms were validated by an expert committee, ranking the quality of them. The new methodology proposed, obtained the best results in terms of optimisation as well as the highest punctuation given by the experts.",finance,302
10.1016/j.eswa.2010.02.101,filtered,Expert Systems with Applications,scopus,2010-01-01,sciencedirect,neural networks for credit risk evaluation: investigation of different neural models and learning schemes,https://api.elsevier.com/content/abstract/scopus_id/78649516309,"This paper describes a credit risk evaluation system that uses supervised neural network models based on the back propagation learning algorithm. We train and implement three neural networks to decide whether to approve or reject a credit application. Credit scoring and evaluation is one of the key analytical techniques in credit risk evaluation which has been an active research area in financial risk management. The neural networks are trained using real world credit application cases from the German credit approval datasets which has 1000 cases; each case with 24 numerical attributes; based on which an application is accepted or rejected. Nine learning schemes with different training-to-validation data ratios have been investigated, and a comparison between their implementation results has been provided. Experimental results will suggest which neural network model, and under which learning scheme, can the proposed credit risk evaluation system deliver optimum performance; where it may be used efficiently, and quickly in automatic processing of credit applications.",finance,303
10.1016/j.patrec.2005.03.026,filtered,Pattern Recognition Letters,scopus,2005-10-01,sciencedirect,"a comparison of global, recurrent and smoothed-piecewise neural models for istanbul stock exchange (ise) prediction",https://api.elsevier.com/content/abstract/scopus_id/24344476729,"This paper makes a comparison of global, feedback and smoothed-piecewise neural prediction models for financial time series (FTS) prediction problem. Each model is implemented by various neural network (NN) architectures: global model by a multilayer perceptron (MLP), feedback model by a recurrent neural network (RNN) and smoothed-piecewise model by a mixture of experts (MoE) structure. The advantages and disadvantages of each model are discussed by using real world finance data: 12 years data of Istanbul stock exchange (ISE) index (XU100) from 1990 to 2002. A conventional exponential generalized autoregressive conditional heteroskedasticity (EGARCH) volatility model is also implemented for comparison purpose. The comparison for each model is done based on well-known criterions of index return series of market: hit rate (H
                     R), positive hit rate (
                        
                           
                              
                                 H
                              
                              
                                 R
                              
                              
                                 +
                              
                           
                        
                     ), negative hit rate (
                        
                           
                              
                                 H
                              
                              
                                 R
                              
                              
                                 -
                              
                           
                        
                     ), mean squared error (MSE), mean absolute error (MAE) and correlation (ζ). Finally, it is observed that the smoothed-piecewise neural model becomes advantageous in capturing volatility in index return series when it is compared to global and feedback neural model, and also the conventional EGARCH volatility model.",finance,304
10.5075/epfl-thesis-4532,filtered,core,"Lausanne, EPFL",2009-10-15 00:00:00,core,essays on individual decision making under uncertainty,,"This research project is an experimental study of decision-making in very difficult contexts resembling those encountered in financial markets. The starting point was the empirical observation that financial assets are objects of a very complex kind. Specifically, financial assets generate rewards in an unpredictable way (stochastic processes), and they jump regularly (unstable processes). What's more, they are covert: finance practitioners lack information about their nature, and may well be unable to uncover it. After all, can humans really learn the nature of unstable stochastic processes? To address this question, I designed an investment task, the ""Boardgame,"" characterized by assets – in the form of locations displayed on a board – which are, like the financial assets in the real world, unpredictable, unstable, and (initially) unknown. Specifically, the game consists of choosing between six locations of initially unknown probability of winning or losing. Each trial, the player selects one location and immediately receives the outcome returned by the chosen location (a reward, a loss, or 0 CHF). She accumulates gains and losses throughout the game, with the goal of maximizing the cumulative earnings. One essential characteristic of the Boardgame is that outcome probabilities – at all locations – jump regularly. An ideal player accounts for jump occurrence and re-learns outcome probabilities once a jump has occurred. Such two-stage mental process – consisting of jump detection and subsequent reappraisal of the probabilities – is extremely demanding. I invited 62 subjects – all undergraduate or graduate students at the EPFL – to play the Boardgame during 30 minutes, and recorded their action in each trial (namely, which location they chose). From their choices, I attempted to infer the nature of their learning in the game. To do so, I followed an inter-disciplinary approach, at the intersection of economics, machine learning, and neurobiology. I first formalized normative learning in the game (how ideal players learn), as well as bounded rational learning (how adaptive – intelligent – agents, albeit of limited cognitive capabilities, are expected to learn). I then examined whether my subjects did a good job of learning in the Boardgame, by comparing the fits of the normative and bounded rational models. I eventually studied the neural implementation of the behavior at work. It appears that for the majority of my subjects, the normative learning model did a much better job of explaining actual behavior than the alternative, bounded rational model. In other words, subjects were apt at learning the outcome probabilities of the locations, despite the difficulty of the enterprise. This suggests that people may be better fit to cope with the instability of financial markets than previously thought. Beyond being of interest for the finance field, this result has actually a broader scope. For it demonstrates the human ability to adapt behavior to suit an uncertain changing environment. This cognitive flexibility, which some eminent neuroscientists have been taken to be the most relevant definition of human intelligence, is essential in many domains beyond the one studied here (reward learning) – in particular, the same neuroscientists have suggested that it is chief in contexts of social interaction. The finding that my subjects were very good at learning the nature of the locations is also important from an epistemological point of view. For one message drawn from this study is that people can be very sophisticated in the face of very complex problems. This message should prompt a reevaluation of the scope of the dominant paradigm in decision making – namely, the limited cognition paradigm. The limited cognition paradigm has led either to a hyperemphasis on cognitive biases as sources of human mistakes (irrationality), or to the idea that complexity in a cognitive task precludes the emergence of optimal behavior (bounded rationality). In contrast, the present work highlights human adaptation, and further suggests that complexity is not necessarily an obstacle for full rationality to emerge. One reason that my subjects could fare so well in my complex task is that the human brain is fit to cope with the Boardgame, which is reminiscent of the living conditions of the early man. For Homo has long had to deal with an uncertain unstable environment. As a result, the hominoid brain was geared to cope with instability. The finding that optimal behavior prevailed in my experiment prompted me to flesh out the neural implementation of this behavior. In the final stage of this project, I conjectured a neural system that is fit to cope with unforeseen and ever-changing conditions. I attempted to describe this neural system rigorously, in the vein of a recent computational trend in decision neuroscience. This computational approach aims to reveal the neural substrates of subjective states that the behavioral models purport to estimate – e.g., how much reward a subject expected on each trial, how much surprise a subject experienced on each trial, how much uncertainty a subject perceived in her environment on each trial. Likewise, I estimated subjective variables that are essential to implement the optimal Bayesian rule in my task. In particular, I estimated how much credit a subject assigned to the belief that a jump occurred on each trial. This estimation enabled me to make precise predictions as to the neural mechanisms involved in the Boardgame. The incoming step shall be to put these predictions to a test in an imaging study of the Boardgame. As such, the present research has laid the basis for a larger research program to be pursued. It is hoped that it shows the utility of bringing the modeling tools from economics to bear on neurobiological questions. I think that such scientific exchanges carry the possibility of insights useful to build an adequate model of individual decision-making",finance,305
10.20998/2413-3000.,filtered,core,"НТУ ""ХПІ""",2019-01-01 00:00:00,core,the model of it project management system based on machine learning,https://core.ac.uk/download/pdf/196573376.pdf,"Запропоновано модель інтеграції сучасних ІТ з управління проектами із технологіями штучного інтелекту, що враховує сучасні тенденції та розробки у галузі ІТ з комп'ютерних наук та дозволяє ефективно обробляти зростаючі потоки даних щодо параметрів та характеристик складних проектів при розробці та прийнятті рішень по управлінню складними проектами. Визначено та класифіковано основні причини, що впливають на неуспішне завершення проектів. Показано складові запропонованої моделі інтегрованої системи управління проектами та надано їх деталізовану характеристику. Визначено, що запропонована модель ґрунтується на трьох складових, які включають перелік базових методологій та стандартів з управління проектами, що можуть на основі конвергенції утворювати гібридні методології, сукупність ІТ, баз даних та знань з управління проектами для розробки, обґрунтування та управління проектами та сучасні технології штучного інтелекту, які базуються на використанні методів машинного навчання. Обґрунтовано роль, складові та оточення машинного навчання для використання в управління проектами. Щодо умов інтеграції проведено аналіз та побудована таблиця сучасних ІТ для управління проектами та проведена кластерізації їх на три групи щодо можливостей використання технологій штучного інтелекту, зокрема машинного навчання. Результати впровадження елементів запропонованої моделі при реалізації складних ІТ проектів у банківській сфері засвідчили ефективність запропонованого підходу. Збільшилася успішність поточних проектів та портфелів проектів банку, зросла кількість учасників проектної діяльності, що працюють у реальних проектах з обробкою великих масивів інформації щодо управління розробкою та впровадженням складних ІТ продуктів.A model is proposed for integrating modern IT project management with artificial intelligence technologies, taking into account current trends and developments in the field of IT computer science and allows you to effectively handle the growing data flows on the parameters and characteristics of complex projects when developing and making decisions on managing complex projects. Identified and classified the main reasons affecting the unsuccessful completion of projects. The components of the proposed model for integrating the project management system are shown and their detailed characteristics are presented. It is determined that the proposed model is based on three components, including a list of basic methodologies and standards for project management, which can form hybrid methodologies, a set of IT, database and project management knowledge for developing, substantiating and managing projects and modern artificial technologies intelligence based on the use of machine learning methods. The role, components and environment of machine learning for use in project management is substantiated. The integration conditions were used to analyze and build a table of modern IT for project management, clustering them into three groups concerning the possibilities of using artificial intelligence technologies, in particular machine learning. The results of introducing elements of the proposed model in the implementation of complex IT projects in the banking sector have shown the effectiveness of the proposed approach. The success of current projects and portfolios of projects of the bank has increased, the number of participants in project activities working in real projects with processing large amounts of information on managing the development and implementation of complex IT products has increased",finance,306
10.31590/ejosat.598036,filtered,core,'European Journal of Science and Technology',2019-01-01 00:00:00,core,detection of fake websites by classification algorithms,,"Günümüzde kimlik avı yapan sahte web sitelerinin sayısı oldukça artmıştır. Bu web sitelerinin amaçları genel anlamda kişilerin,kişisel bilgilerini ele geçirerek çıkar sağlamaktır. Sosyal medya hesaplarımızdaki kimlik ve parola bilgilerimiz, alışveriş sitelerindekikimlik ve adres bilgilerimiz bize ait kişisel bilgilerimizdir. Bu tür bilgiler istenmeyen kişilerin eline geçmesi durumunda, tahmin bileedemeyeceğimiz kötü sonuçlar doğurabilmektedir. Ayrıca online bankacılık işlemlerimiz gibi finansal işlemlerimizin önemli birkısmını internet ortamında yapıyor olmamız bu tür sitelerden korunmamız açısından önemli bir sorun teşkil etmektedir. Bu amaçlaantivürüs yazılım firmaları, tarayıcılar, arama motorları daha iyi kullanıcı hizmeti ve memnunniyet sağlamak açısından bu tür zararlısitelerden kullanıcılarını korumak için çalışmalar yapmaktadırlar. Ayrıca sahte web sayfalarının kullanıcıların önüne gelmeden tespitedilip engellenmesi günümüz yapay zeka çalışmalarınında önemli bir çalışma alanı olmaktadır. Hergün milyarlarca insanın gezindiğiinternet ortamında bu sahte sitelerden korunmasının en kolay yöntemi, sahte web sayfalarının otomatik olarak tespit edilipengellenmesi olacaktır. Makine öğrenmesi sınıflandırma algoritmaları ile bir sayfaya ait bilgilere bakarak sistem tarafından otomatikolarak sahte veya gerçek olarak tespit edilmesi yapay zeka çalışmalarının sunduğu önemli avantajların başında gelmektedir. Buçalışma ile bir web sitesi adresine ait belirlenmiş 10 özellik kullanılarak; bu adresin sahte mi, yoksa gerçek bir adres mi olduğu tespitedilmeye çalışılmaktadır. Çalışmada kullanılan veriler Machine Learning Repository (UCI)’dan alınmıştır. Verilerin analizi ÇaprazEndüstri Standart Süreç Modeli(CRISP-DM) baz alınarak gerçekleştirilmiştir. Veri setinde web sitelerinin durumunu belirleyen nitelik(Class, Kimlik Avı=-1, Şüpheli=0 ve Meşru=1) olarak etiketlenmiştir. Çalışma da RStudio kullanılarak R programlama dili ileanalizler yapılmıştır. Kullanılan sınıflandırma algoritmaları Rastgele Orman (RF), Destek Vektör Makineleri (SVM), J48, K-En YakınKomşu (KNN) ve Naive Bayes algoritmalarıdır. Yapılan değerlendirmeler sonucunda Rastgele Orman algoritması ile en yüksekdoğruluk performansı elde edilmiştir.Nowadays, phishing web sites have been increased. The purpose of these sites is to obtain benefits by acquiring personal information of people in general. Our identity and password information in our social media accounts and identity and address information on shopping sites are our personal information. If such information is received by unwanted people, it can have bad unpredictable consequences. In addition, the fact that we carry out a significant portion of our financial transactions such as our online banking transactions on the internet constitutes an important problem in terms of protection from such sites. For this purpose, antivirus software companies, browsers, search engines are working to protect users from such harmful sites in terms of providing better user service and satisfaction. In addition, the detection and prevention of fake web pages before the users is an important area of work in today's artificial intelligence studies. The easiest method of protecting these fraudulent sites in the internet environment where billions of people are browsing every day will be to detect and block fake web pages automatically. Machine learning classification algorithms are automatically identified as fake or real by the system by looking at the information of a page and this is one of the important advantages offered by artificial intelligence studies. With this study, using 10 properties determined for a website address; it is attempted to determine whether this address is a fake or a real address. The data used in this study were taken from Machine Learning Repository (UCI). Data analysis was performed based on the Cross Industry Standard Process Model (CRISP-DM). In the data set, it is labeled as the attribute that determines the status of websites (Class, Phishing = -1, Suspicious = 0 and Legitimate = 1). The study was also done by using RStudio analysis with R programming language. The classification algorithms used are Random Forest (RF), Support Vector Machines (SVM), J48, K-Nearest Neighbor (KNN) and Naive Bayes algorithms. The highest accuracy performance was obtained by Random Forest algorithm",finance,307
10.1145/3167918.3167967,filtered,core,'Association for Computing Machinery (ACM)',2018-01-29 00:00:00,core,stock market analysis using social networks,http://hdl.handle.net/10453/133006,"© 2018 ACM. Nowadays, the use of social media has reached unprecedented levels. Among all social media, with its popular micro-blogging service, Twitter enables users to share short messages in real time about events or express their own opinions. In this paper, we examine the effectiveness of various machine learning techniques on retrieved tweet corpus. A machine learning model is deployed to predict tweet sentiment, as well as gain an insight into the correlation between twitter sentiment and stock prices. Specifically, that correlation is acquired by mining tweets using Twitter's search API and process it for further analysis. To determine tweet sentiment, two types of machine learning techniques are adopted including Naïve Bayes classification and Support vector machines. By evaluating each model, we discover that support vector machine gives higher accuracy through cross validation. After predicting tweet sentiment, we mine historical stock data using Yahoo finance API, while the designed feature matrix for stock market prediction includes positive, negative, neutral and total sentiment score and stock price for each day. In order to capturing the correlation situation between tweet opinions and stock market prices, hence, evaluating the direct correlation between tweet sentiments and stock market prices, the same machine learning algorithm is implemented for conducting our empirical study",finance,308
10.1186/s12920-018-0398-y,filtered,core,'Springer Science and Business Media LLC',2018-10-11 00:00:00,core,privacy-preserving logistic regression training,,"BACKGROUND: Logistic regression is a popular technique used in machine learning to construct classification models. Since the construction of such models is based on computing with large datasets, it is an appealing idea to outsource this computation to a cloud service. The privacy-sensitive nature of the input data requires appropriate privacy preserving measures before outsourcing it. Homomorphic encryption enables one to compute on encrypted data directly, without decryption and can be used to mitigate the privacy concerns raised by using a cloud service. METHODS: In this paper, we propose an algorithm (and its implementation) to train a logistic regression model on a homomorphically encrypted dataset. The core of our algorithm consists of a new iterative method that can be seen as a simplified form of the fixed Hessian method, but with a much lower multiplicative complexity. RESULTS: We test the new method on two interesting real life applications: the first application is in medicine and constructs a model to predict the probability for a patient to have cancer, given genomic data as input; the second application is in finance and the model predicts the probability of a credit card transaction to be fraudulent. The method produces accurate results for both applications, comparable to running standard algorithms on plaintext data. CONCLUSIONS: This article introduces a new simple iterative algorithm to train a logistic regression model that is tailored to be applied on a homomorphically encrypted dataset. This algorithm can be used as a privacy-preserving technique to build a binary classification model and can be applied in a wide range of problems that can be modelled with logistic regression. Our implementation results show that our method can handle the large datasets used in logistic regression training.status: publishe",finance,309
10.1504/ijbidm.2016.081604,filtered,core,,2016-01-01 00:00:00,core,data-driven techniques for mass appraisals. applications to the residential market of the city of bari (italy),,"The need for evaluation models capable of returning ‘slender’ and

reliable mass appraisals of properties belonging to different market segments

has been made mandatory by the events that are covering the global real estate

finance, because of the emergence of non-performing loans in the banks’

balance sheets. In Italy, the non-performing loans have been estimated by the

Italian Banking Association equal to about 300 billion euro in 2014. In the

present paper, three approaches of data-driven techniques (hedonic price

model, artificial neural networks and evolutionary polynomial regression) have

been applied to a sample of residential apartments recently sold in a district of

the city of Bari (Italy), in order to test the respective performance for mass

appraisals. The models obtained by the implementation of the three procedures

have been compared in terms of statistical accuracy, empirical compliance of

the results and complexity of the functional relationships",finance,310
10.1155/2020/5057801,filtered,core,'Hindawi Limited',2020-01-01 00:00:00,core,nonlinear autoregressive neural network and extended kalman filters for prediction of financial time series,,"Time series analysis and prediction are major scientific challenges that find their applications in fields as diverse as finance, biology, economics, meteorology, and so on. Obtaining the method with the least prediction error is one of the difficult problems of financial market and investment analysts. State space modelling is an efficient and flexible method for statistical inference of a broad class of time series and other data. The neural network is an important tool for analyzing time series especially when it is nonlinear and nonstationary. Essential tools for the study of Box-Jenkins methodology, neural networks, and extended Kalman filter were put together. We examine the use of the nonlinear autoregressive neural network method as a prediction technique for financial time series and the application of the extended Kalman filter algorithm to improve the accuracy of the model. As application on a real example, we are analyzing the time series of the daily price of steel over a 790-day period for establishing the superiority of this method over other existing methods. The simulation results using MATLAB and R software show that the model is capable of producing a reasonable accuracy",finance,311
10.14529/ctcr200315,filtered,core,'FSAEIHE South Ural State University (National Research University)',2020-08-04 00:00:00,core,технология blockchain и возможности ее использования,,"With the development of digital technology, most familiar processes are changing. Without information technology, it is already impossible to imagine either medicine or the field of education. Big data and artificial intelligence come to almost every area of society, everything around is becoming “smart”. According to the forecasts of the participants of the St. Petersburg International Economic Forum, held in 2019, the global market for products using artificial intelligence and advanced information technologies can grow by almost 17 times by 2024. A national strategy for the development of technologies in the field of artificial intelligence and information technologies has been prepared in the Russian Federation, and a detailed action plan has been integrated into the national program “Digital Economy”. Digitalization today covers almost all aspects of the interaction of the citizen and business with authorities. Digitalization of business processes takes place, technologies are introduced into the activities of industrial enterprises, in the organization of public services and financial institutions.Aim. Consider the technology of blockchain, its origin, advantages and disadvantages, as well as the possibilities and prospects of using this technology.Materials and methods. As part of the materials and methods, one should point to an analysis of the theoretical foundations of the blockchain technology, the practical results of its implementation and use in various fields of activity: from public life to business. Since it is blockchain technology that occupies a special place among the promising information technologies and is increasingly used in business, as well as government, and will undoubtedly affect the familiar picture of the world, how the Internet has changed the world of information exchange in its time.Results. The article makes an attempt to assess the origin, current state, the possibilities of using blockchain technology, as well as its impact on traditional business processes and socio-economic transformation as a result of the digitalization process.Conclusion. It is believed that blockchain technology can be a real breakthrough in the field of finance, secure databases and the reliability of certain facts. For the largest foreign and domestic companies, this is not just a concept from an approximate future, but now it is part of the business, and the future life of society as a whole depends on how well it is to use and manage blockchain technology.Введение. С развитием цифровых технологий меняется большинство привычных процессов. Без информационных технологий уже невозможно представить ни медицину, ни сферу образования. Большие данные и искусственный интеллект приходят практически в каждую сферу жизни общества, все вокруг становится «умным». По прогнозам участников Петербургского международного экономического форума, проходившего в 2019 году, мировой рынок продуктов с использованием искусственного интеллекта и перспективных информационных технологий к 2024 году может вырасти почти в 17 раз. В Российской Федерации подготовлена национальная стратегия развития технологий в области искусственного интеллекта и информационных технологий, а детальный план действий интегрирован в национальную программу «Цифровая экономика». Цифровизация сегодня охватывает почти все стороны взаимодействия гражданина и бизнеса с органами власти. Происходит цифровизация бизнес-процессов, технологии внедряются в деятельность промышленных предприятий, в организацию государственных услуг и финансовых учреждений.Цель исследования. Рассмотреть технологию blockchain, ее происхождение, достоинства и недостатки, а также возможности и перспективы применения данной технологии.Материалы и методы. В составе материалов и методов следует указать на анализ теоретических основ технологии blockchain, практических результатов ее внедрения и использования в различных сферах деятельности – от общественной жизни до бизнеса, так как именно технология blockchain занимает особое место в ряду перспективных информационных технологий и находит все большее применение в сфере бизнеса, а также государственного управления и, несомненно, повлияет на привычную картину мира, как в свое время Интернет изменил мир обмена информацией.Результаты. В статье делается попытка оценить происхождение, текущее состояние, возможности использования технологии blockchain, а также ее влияние на традиционные бизнес-процессы и социально-экономическую трансформацию в результате процесса цифровизации.Заключение. Считается, что технология blockchain способна стать настоящим прорывом в области финансов, защищенных баз данных и достоверности тех или иных фактов. Для крупнейших зарубежных и отечественных компаний это не просто понятие из приближенного будущего, а уже сейчас часть бизнеса, и от того, насколько грамотно подходить к применению и управлению технологией blockchain, зависит и будущая жизнь общества в целом",finance,312
10.21533/pen.v7i4.898,filtered,core,'International University of Sarajevo',2019-12-04 00:00:00,core,dynamic filtering of malicious records using machine learning integrated databases,https://core.ac.uk/download/270220076.pdf,"Machine Learning, Deep Learning and Predictive Analytics are the key domains of research in assorted domains of implementations including engineering, finance, economics, real time imaging and many others. The researchers are working on different tools and technologies including open source and own developed frameworks so that the higher degree of accuracy can be achieved. The research reports from Market Research News US predicted that the global market size of machine learning based implementations will exceed 20 billion dollars in year 2024. Most of the government and social services are nowadays in process to be deployed with the advanced technologies of machine learning and deep learning so that the minimum error factor can be there. The key players in the industry include; Google, Facebook, IBM Watson, Baidu, Apple, Microsoft, Wipro, Amazon, Intel, Nuance and many others which are working on the advanced algorithms and implementation perspectives of machine learning",finance,313
10.32702/2307-2105-2020.10.50,filtered,core,'DKS Center',2020-01-01 00:00:00,core,теорія та практика забезпечення кіберстійкості банків,https://core.ac.uk/download/395139328.pdf,"У статті доведено важливість формування концепції забезпечення кіберстійкості банків на сучасному етапі розвитку цифрової економіки країни, зважаючи на негативний фінансовий та нефінансовий вплив кібератак на банківську систему та економіку країни в цілому. Автором на основі узагальнення досліджень з цієї тематики уточнено зміст поняття “кіберстійкість банку” та визначено його сутнісні характеристики за якісним та кількісним підходами. В статті проведено дослідження теоретичних підходів до забезпечення кіберстійкості банків та на цій основі розроблено модель механізму забезпечення кіберстійкості, адекватну сучасному стану та умовам, в яких функціонують банки України. За результатами дослідження визначено, що ефективне функціонування механізму забезпечення кіберстійкості потребує відповідного організаційного забезпечення, зокрема створення Центра кіберстійкості банку.В статье доказана важность формирования концепции обеспечения киберустойчивости банков на современном этапе развития цифровой экономики страны, несмотря на отрицательное финансовое и нефинансовое влияние кибератак на банковскую систему и экономику страны в целом. Автором на основе обобщения исследований по этой тематике уточнено содержание понятия ""киберустойчивость банка"" и определены его сущностные характеристики согласно качественному и количественному подходам. В статье проведено исследование теоретических подходов к обеспечению киберустойчивости банков и на этой основе разработана модель механизма обеспечения киберустойчивости, адекватная условиям, в которых функционируют банки Украины. В результате исследования установлено, что эффективное функционирование механизма обеспечения киберустойчивости требует соответствующего организационного обеспечения, в частности создания Центра киберустойчивости банка.The article proves the importance of forming the concept of ensuring the cyber resilience of banks at the present stage of development of the country's digital economy in the transition to the sixth technological mode and the associated use of industry 4.0 technologies, such as artificial intelligence, «cloud» and «foggy» computing, IoT / IIoT, Big Data, Blockchain, VR / AR. This leads to a significant complication of the cyber threat landscape, an increase in the number of cyberattacks with a significant increase in the negative financial and non-financial consequences that cyberattacks have on the banking system and the economy as a whole. The author, based on the generalization of research on this topic, clarified the content of the concept of ""cyber resilience of a bank"". Its essential characteristics were determined in the context of qualitative and quantitative approaches. The article studies theoretical approaches to ensuring the cyber resilience of banks, which made it possible to develop a conceptual model of the mechanism for ensuring cyber resilience, adequate to the current state and conditions in which the banks of Ukraine operate. The developed mechanism for ensuring cyber resilience allows for: 1) the formalization of the landscape of real and potential cyber threats; 2) ensures the consistency of mechanisms and tools for countering them, adapting and/or recovering from cyber incidents; 3) allows not only to adequately respond to existing cyber threats but also to identify negative factors that can lead to the emergence and implementation of new cyber threats and cyber-attacks. According to the results of the study, it was found that the effective functioning of the mechanism for ensuring the cyber resilience of the bank requires appropriate organizational support. For this, the author substantiated the need to create a Bank Cyber Resilience Center. It should include representatives from the departments responsible for banking business continuity, cybersecurity, cyber risk management and IT quality. This will allow obtaining a synergistic effect by creating a single interconnected process-based model, including metrics of the bank's cyber resilience level and KPIs, as well as tools for monitoring, controlling and resisting external and internal cyber threats, adaptation and/or recovery after them",finance,314
b0ed25630d66d052bff2a9218d141a24b19e4d68,filtered,semantic_scholar,,2021-01-01 00:00:00,semantic_scholar,the benefits of applying ai to compression,https://www.semanticscholar.org/paper/b0ed25630d66d052bff2a9218d141a24b19e4d68,"Artificial intelligence (AI) is a popular subject today. Currently used across various verticals, from medicine to autonomous vehicles and finance, it is projected to have a significant impact. Today, AI is used for video compression, not just to provide bitrate savings but also to improve the quality of experience (QoE) and savings in processing power. This paper will present three applications of AI for video compression, explaining how each helps with the delivery of video content over broadcast and OTT networks. The applications that will be examined include Dynamic Encoding Style (DES), which enables a better trade-off between video quality and bitrate; Dynamic Resolution Encoding (DRE), which enables a superior QoE and density; and Dynamic Frame Rate Encoding (DFE), which allows for improved density and QoE. After a brief presentation of the methods, the paper will then present the results of implementing these technologies in the real world. INTRODUCTION Video compression for broadcast TV services started more than 20 years ago. Over time, several key improvements, such as dual-pass, statistical multiplexing, and software migration, were made to compression technology in order to boost performance. Artificial Intelligence (AI) is driving the next frontier of video compression enhancements. AI is effective at detecting objects and at surveillance. Machines are capable of detecting cancer cells with excellent accuracy, which can be a great help for medical doctors (1, 2). AI algorithms can also be useful at processing a lot of data. Some companies use it to clean large data sets, an activity called data wrangling. More and more, AI can be used for decision-making. The autonomous vehicle collapses many of these uses. Indeed, detection is important in an autonomous car, as other vehicles, persons, objects, and signs on the road need to be clearly identified along with their motion. Together with the internals of the car, it becomes a lot of data to process. The autonomous car has to constantly make decisions about the speed, direction, signaling, and more. In other terms, AI is very effective at predictions (3). More details on the evolution from human-designed algorithm to using AI for live video compression can be found in (10). In the VOD encoding domain, Netflix has been the pioneer in developing an AI-based system to assist file encoding, known as per-title or per-chunk encoding (4). Those techniques only apply to offline encoding and cannot be used for live video. This paper presents three examples of AI applied to live video encoding to optimize broadcast and OTT content delivery. The first three sections present the three examples. For each example, the paper presents a brief presentation of the methods followed by the results, including real-life effects. In this paper, both “AI” and “machine learning” expressions are used, knowing that machine learning is, in fact, a part of AI. DYNAMIC ENCODING STYLE (DES) OR CONTENT-AWARE ENCODING (CAE) FOR BITRATE SAVINGS In this first application, the video compression algorithm itself has improved thanks to machine learning technology. The goal is to improve the video quality/bitrate trade-off, meaning reducing the bitrate while maintaining the video quality or keeping a bitrate and improving the video quality. This is done by the means of encoding styles. Encoding styles are compression algorithm configurations well-suited for particular content. Results DES has been thoroughly tested across a lot of material, and it has shown a bitrate reduction vs. deployed system from 20% up to 30% on VBR content in broadcast applications, and 35% on average up to 50% compared with CBR for streaming applications. Table 1 shows the comparison of the AI-based algorithm with the deployed solution for a customer’s use case. The AI-based algorithm is run at different lower bitrates compared with the deployed solution, from 10% to 30% lower. At 10% the AI-based algorithm is better, at 20% it is equal and at 30% it is worse. The last two columns provide a comparison of lowering the bitrate for both algorithms for verification purposes. The conclusion is that the AI-based algorithm provides a 20% gain. Prog Channel AI version Pool bitrate -10% AI version Pool bitrate -20% AI version Pool bitrate -30% Both versions Pool bitrate -10% Both versions Pool bitrate -20% 1 Documentary = AI slightly lower AI lower AI better AI better 2 Cartoon = = AI lower = = 3 General Entertainment = = AI lower = AI slightly better 4 Movie = = AI slightly lower = AI slightly better 5 Sport AI better = AI lower AI better AI better 6 High action shows AI better AI slightly better = AI better AI better Table 1 – Video quality comparison on different channels between deployed and AI-based algorithm DES and CAE have been deployed in many streaming situations, with some examples and results shown below. The first example is a large streaming service with more than 1 million subscribers and more than 50 channels. This service supports live, VOD, cloud DVR, time-shift and serverside dynamic ad insertion. Due to the COVID-19 global health crisis, the service provider observed a dramatic increase in the bandwidth use and needed a solution to relieve the pressure without changing its infrastructure. By turning on DES and CAE the service provider saw significant improvements on their network. The backbone traffic was reduced by 50%, and the CDN peak usage was reduced by 30%. Figure 1 shows the backbone traffic reduction after DES/CAE was activated. Figure 1 Backbone traffic reduction thanks to DES and CAE The second example involves a large European streaming provider. The measurements were also made during the lockdown period due to COVID-19. In this example we show the average bitrate variation between normal compression and with DES/CAE turned on. For sports content, a bitrate reduction of 30% was measured, and for studio content a bitrate reduction of 40% was observed. Studio content includes television programs, such as talk shows and games shows. Figure 2 Studio content average bitrate reduction thanks to DES/CAE DES/CAE Activated",finance,315
10.1109/sice.2002.1195611,filtered,Proceedings of the 41st SICE Annual Conference. SICE 2002.,IEEE,2002-08-07 00:00:00,ieeexplore,a reinforcement learning using adaptive state space construction strategy for real autonomous mobile robots,https://ieeexplore.ieee.org/document/1195611/,"In the recent robotics, much attention has been focused on utilizing reinforcement learning for designing robot controllers. However, there still exists difficulties, one of them is well known as state space explosion problem. As the state space for a learning system becomes continuous and high dimensional, its combinational state space exponentially explodes and the learning process is time consuming. In this paper, we propose an adaptive state space recruitment strategy for reinforcement learning, which enables the system to divide state space gradually according to task complexity and progress of learning. Some simulation results and real robot implementation show the validity of the method.",space,316
10.1109/irds.2002.1041504,filtered,IEEE/RSJ International Conference on Intelligent Robots and Systems,IEEE,2002-10-04 00:00:00,ieeexplore,a reinforcement learning with adaptive state space recruitment strategy for real autonomous mobile robots,https://ieeexplore.ieee.org/document/1041504/,"In the recent robotics, much attention has been focused on utilizing reinforcement learning for designing robot controllers. However, there still exists difficulties, one of them is well known as state space explosion problem. As the state space for learning system becomes continuous and high dimensional, the learning process results in time-consuming since its combinational states explodes exponentially. In order to adopt reinforcement learning for such complicated systems, it should be taken not only ""adaptability"" but ""computational efficiencies"" into account. In the paper, we propose an adaptive state space recruitment strategy for reinforcement learning, which enables the system to divide state space gradually according to task complexity and progress of learning. Some simulation results and real robot implementation show the validity of the method.",space,317
10.1109/cec.2005.1555005,filtered,2005 IEEE Congress on Evolutionary Computation,IEEE,2005-09-05 00:00:00,ieeexplore,a space saving digital vlsi evolutionary engine for ctrnn-eh devices,https://ieeexplore.ieee.org/document/1555005/,"Continuous time recurrent neural network - evolvable hardware (CTRNN-EH) control devices are composed of an analog continuous time recurrent neural network (CTRNN) with an onboard evolutionary algorithm (EA) engine that evolves the parameters of the neural network. These control devices have been demonstrated to be useful in a variety of real time control applications and are amenable to mixed-signal VLSI implementation for the control applications under stringent size and power constraints. Unlike the CTRNNs, which are analog in nature, the EA engine has to be implemented using digital VLSI techniques. Because these techniques do not offer the advantages of small area and power directly, the task of adhering to size and power constraints is challenging and must be accomplished at the algorithmic level. In this paper, the authors discussed the aforementioned issues in detail and also propose a space-saving digital EA engine for the CTRNN-EH device. The EA engine has been modeled in Verilog HDL. The synthesis results are presented and the functionality of the EA is demonstrated on a small test problem.",space,318
10.1109/roman.1995.531967,filtered,Proceedings 4th IEEE International Workshop on Robot and Human Communication,IEEE,1995-07-07 00:00:00,ieeexplore,a study of real time facial expression detection for virtual space teleconferencing,https://ieeexplore.ieee.org/document/531967/,"A new method for real-time detection of facial expressions from time-sequential images is proposed. The proposed method does not need the tape marks that were pasted to the face for detecting expressions in real-time in the current implementation for the virtual space teleconferencing. In the proposed method, four windows are applied to the four areas in the face image: the left and right eyes, mouth and forehead. Each window is divided into blocks that consist of 8 by 8 pixels. The discrete cosine transform (DCT) is applied to each block, and the feature vector of each window is obtained from taking the summations of the DCT energies in the horizontal, vertical and diagonal directions. By a conversion table, the feature vectors are related to real 3D movements in the face. Experiment show some promising results for accurate expression detection and for the realization of real-time hardware implementation of the proposed method.",space,319
10.1109/grc.2010.114,filtered,2010 IEEE International Conference on Granular Computing,IEEE,2010-08-16 00:00:00,ieeexplore,an algorithm and hardware design for very fast similarity search in high dimensional space,https://ieeexplore.ieee.org/document/5575953/,"Similarity search in very high dimensions is vital for many scientific research activities as well as real applications. A high performance, scalable, and optimal quality solution to the problem still remains challenging. We propose a vote count based algorithm using p-stable distribution for approximate similarity search. Approximate similarity search effectively serves purpose for many real applications. Our algorithm is efficient and scalable with both dimension and database size. We also propose a novel hardware implementation of the algorithm using simple modification to Random Access Memory (RAM). The hardware design gives real time search for millions of points at practical cost. We empirically achieve high accuracy for query results using our algorithm on 128 dimensional synthetic and real datasets.",space,320
10.1109/isncc.2018.8530988,filtered,"2018 International Symposium on Networks, Computers and Communications (ISNCC)",IEEE,2018-06-21 00:00:00,ieeexplore,building an intelligent and efficient smart space to detect human behavior in common areas,https://ieeexplore.ieee.org/document/8530988/,"Smart spaces have become an integral part of our daily routines to improve quality of life for many different groups of people. The use of embedded systems to build these smart spaces, in combination with data analytics, can provide real-time information about the environment and how it interacts with the people in it. In this paper, we demonstrate how one embedded system that acquires data based on a 2-dimensional positional-grid, movement, temperature and vibration is used to build a smart and pervasive space. Data collected from these sensors is used for real time localization in conjunction with machine learning mechanisms to analyze human activities. We evaluate five machine learning algorithms, namely Logistic Regression, Support Vector Machine, Decision Tree, Random Forest, Naive Bayes and Artificial Neural Network applied on a dataset collected in our lab. Results show high classification performance for all methods giving up-to 99.95% classification accuracy. These patterns provide useful information about occupancy patterns, movement patterns, etc., which will be later used to allocate computational resources in the smart space accordingly. Furthermore, our implementation does not use any camera or microphone deployment, hence addressing potential privacy issues.",space,321
10.1109/aero.2017.7943916,filtered,2017 IEEE Aerospace Conference,IEEE,2017-03-11 00:00:00,ieeexplore,classification of multi-failure mechanisms in space operations in using novel pls-da approach,https://ieeexplore.ieee.org/document/7943916/,"The article addresses an innovative statistical classification methodology to the real spacecraft telemetry based on statistical multivariate latent technique called projection to latent structure discriminant analysis (PLS-DA). The models are generated via using a well-known statistical multivariate software called soft independent modeling for class analogy (SIMCA-P) developed by Umetrics which is used for data exploration and classification. Models are used to detect and classify multi-failure mechanisms in the attitude determination and control subsystem (ADCS) of Egypt-Sat1 for the first time. Models taken altogether the ADCS two faults mechanisms for both angular velocity meters and reaction wheel malfunctions in one model, as well as to identify key contributors to inconsistent events autonomously which lead to characterize the spacecraft (ADCS) behavior. The analysis results lead to give a deep explanation to the system state-of-health (SOH) and characterize its operation behavior during the mission phases.",space,322
10.1109/caia.1990.89189,filtered,Sixth Conference on Artificial Intelligence for Applications,IEEE,1990-05-09 00:00:00,ieeexplore,demonstrating artificial intelligence for space systems-integration and project management issues,https://ieeexplore.ieee.org/document/89189/,"As part of its systems autonomy demonstration project (SADP), the National Aeronautics and Space Administration (NASA) has recently demonstrated the Thermal Expert System (TEXSYS). Advanced real-time expert system and human interface technology was successfully developed and integrated with conventional controllers of prototype space hardware to provide intelligent fault detection, isolation and recovery capability. Many specialized skills were required, and responsibility for the various phases of the project therefore spanned multiple NASA centers, internal departments and contractor organizations. The test environment required communication among many types of hardware and software as well as between many people. The integration, testing, and configuration management tools and methodologies which were applied to the TEXSYS project to assure its safe and successful completion are detailed. The project demonstrated that artificial intelligence technology, including model-based reasoning, is capable of the monitoring and control of a large, complex system in real time.&lt;<ETX>&gt;</ETX>",space,323
10.1109/ijcnn.2019.8852005,filtered,2019 International Joint Conference on Neural Networks (IJCNN),IEEE,2019-07-19 00:00:00,ieeexplore,design space evaluation of a memristor crossbar based multilayer perceptron for image processing,https://ieeexplore.ieee.org/document/8852005/,"This paper describes a simulated memristor-based neuromorphic system that can be used for ex-situ training of a multi-layer perceptron algorithm. The presented programming technique can be used to map the weights required of a neural algorithm directly onto the grid of resistances in a memristor crossbar. Using this weight-to-crossbar mapping approach along with the dot product calculation circuit, neural algorithms can be easily implemented using this system. To show the effectiveness of this circuit, a Multilayer Perceptron is trained to perform Sobel edge detection. Following these simulations, an analysis was presented that shows how memristor programming accuracy and network size are related to output error; the results show that network size can be increased to reduce testing error. In some cases, the memristors in the circuit may be capable of operating with at lower precision if the network size is increased. This means that less precise (or lower resolution) memristor devices may be used to implement the proposed system. Furthermore, a power, timing, and energy analysis shows that this circuit has a computation throughput that allows it to process 4K UHD video in real time at approximately 337mW.",space,324
10.1109/etfa.2015.7301549,filtered,2015 IEEE 20th Conference on Emerging Technologies & Factory Automation (ETFA),IEEE,2015-09-11 00:00:00,ieeexplore,design and implementation for multiple-robot deployment in intelligent space,https://ieeexplore.ieee.org/document/7301549/,"This paper presents the problem of robot deployment for a number of scattered tasks. We aim to minimize the duration it takes for all robots to reach their assigned task locations. In previous work, we have proposed a team composed of one carrier robot (CR) and several servant robots to accomplish the mission. Then we have suggested an algorithm that determines a path of the CR for an efficient deployment under a few constraints, which is verified by simulations. Assuming that the servant robots are unmanned aerial vehicles (UAVs), the present paper extends the discussion to a real robot experiment. We design and implement a deployment system in intelligent space. The feasibility of the study is demonstrated through an experiment.",space,325
10.1109/iccs.2008.4737145,filtered,2008 11th IEEE Singapore International Conference on Communication Systems,IEEE,2008-11-21 00:00:00,ieeexplore,design of new minimum decoding complexity quasi-orthogonal space-time block code for four transmit antennas,https://ieeexplore.ieee.org/document/4737145/,"A new Space-Time Block Code (STBC) achieving full rate and full diversity for general QAM and four transmit antennas is proposed. This code also possesses a quasi-orthogonal (QO) property like the conventional Minimum Decoding Complexity QO-STBC (MDC-QO-STBC), leading to joint ML detection of only two real symbols. The proposed code is shown to exhibit the identical error performance with the existing MDC-QO-STBC. However, the proposed code has an advantage in the transceiver implementation since this code can be modified so that the increase of PAPR occurs at only two transmit antennas, but the MDC-QO-STBC at all of transmit antennas.",space,326
10.1109/iscas.2018.8351685,filtered,2018 IEEE International Symposium on Circuits and Systems (ISCAS),IEEE,2018-05-30 00:00:00,ieeexplore,design-space exploration of pareto-optimal architectures for deep learning with dvfs,https://ieeexplore.ieee.org/document/8351685/,"Specialized computing engines are required to accelerate the execution of Deep Learning (DL) algorithms in an energy-efficient way. To adapt the processing throughput of these accelerators to the workload requirements while saving power, Dynamic Voltage and Frequency Scaling (DVFS) seems the natural solution. However, DL workloads need to frequently access the off-chip memory, which tends to make the performance of these accelerators memory-bound rather than computation-bound, hence reducing the effectiveness of DVFS. In this work we use a performance-power analytical model fitted on a parametrized implementation of a DL accelerator in a 28-nm FDSOI technology to explore a large design space and to obtain the Pareto points that maximize the effectiveness of DVFS in the sub-space of throughput and energy efficiency. In our model we consider the impact on performance and power of the off-chip memory using real data of a commercial low-power DRAM.",space,327
10.1109/3dim.2005.33,filtered,Fifth International Conference on 3-D Digital Imaging and Modeling (3DIM'05),IEEE,2005-06-16 00:00:00,ieeexplore,discrete pose space estimation to improve icp-based tracking,https://ieeexplore.ieee.org/document/1443287/,"Iterative closest point (ICP)-based tracking works well when the interframe motion is within the ICP minimum well space. For large interframe motions resulting from a limited sensor acquisition rate relative to the speed of the object motion, it suffers from slow convergence and a tendency to be stalled by local minima. A novel method is proposed to improve the performance of ICP-based tracking. The method is based upon the bounded Hough transform (BHT) which estimates the object pose in a coarse discrete pose space. Given an initial pose estimate, and assuming that the interframe motion is bounded in all 6 pose dimensions, the BHT estimates the current frame's pose. On its own, the BHT is able to track an object's pose in sparse range data both efficiently and reliably, albeit with a limited precision. Experiments on both simulated and real data show the BHT to be more efficient than a number of variants of the ICP for a similar degree of reliability. A hybrid method has also been implemented wherein at each frame the BHT is followed by a few ICP iterations. This hybrid method is more efficient than the ICP, and is more reliable than either the BHT or ICP separately.",space,328
10.1109/wcica.2000.863468,filtered,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),IEEE,2000-07-02 00:00:00,ieeexplore,estimated force emulation for space robot using neural networks,https://ieeexplore.ieee.org/document/863468/,"This paper introduces the telerobotic system estimated force emulation using neural networks. A delay-compensating 3D stereo-graphic simulator is implemented in SGI ONYX/4 RE/sup 2/. The estimated force emulation can protect the real robot in time from being damaged in collision. The neural network is used to learn the mapping between the contact force error and the accommodated position command to the controller of the space robot. Finally, the controller can feel the emulated force with a two-hand 6-DOF master arm using the force feedback interface.",space,329
10.1109/biocas.2015.7348397,filtered,2015 IEEE Biomedical Circuits and Systems Conference (BioCAS),IEEE,2015-10-24 00:00:00,ieeexplore,general-purpose lsm learning processor architecture and theoretically guided design space exploration,https://ieeexplore.ieee.org/document/7348397/,"This paper presents a general-purpose liquid state machine based neuromorphic learning processor with integrated training and recognition for real world pattern recognition problems. The proposed architecture consists of a generic preprocessor and one or multiple task processors. The pre-processor, or the reservoir, consists of a recurrent spiking neural network with fixed synaptic weights. Task processors are light weight and comprise a set of readout spiking neurons with plastic weights, which are tuned by a biologically plausible supervised learning rule. Importantly, we leverage the unique computational structure of the reservoir for highly efficient implementation of multiple tasks on the same learning processor. A novel theoretical measure of computational power, which is strongly correlated with the true learning performance, is proposed to facilitate fast design space exploration of the recurrent reservoir. We demonstrate the application of our processor architecture by mapping four recognition tasks onto a reconfigurable FPGA processor platform.",space,330
10.1109/tai.1995.479372,filtered,Proceedings of 7th IEEE International Conference on Tools with Artificial Intelligence,IEEE,1995-11-08 00:00:00,ieeexplore,genetic algorithms as a tool for restructuring feature space representations,https://ieeexplore.ieee.org/document/479372/,"This paper describes an approach being explored to improve the usefulness of machine learning techniques to classify complex, real world data. The approach involves the use of genetic algorithms as a ""front end"" to a traditional tree induction system (ID3) in order to find the best feature set to be used by the induction system. This approach has been implemented and tested on difficult texture classification problems. The results are encouraging and indicate significant advantages of the presented approach.",space,331
10.1109/allerton.2009.5394528,filtered,"2009 47th Annual Allerton Conference on Communication, Control, and Computing (Allerton)",IEEE,2009-10-02 00:00:00,ieeexplore,highly parallel decoding of space-time codes on graphics processing units,https://ieeexplore.ieee.org/document/5394528/,Graphics processing units (GPUs) with a few hundred extremely simple processors represent a paradigm shift for highly parallel computations. We use this emergent GPU architecture to provide a first demonstration of the feasibility of real time ML decoding (in software) of a high rate space-time block code that is representative of codes incorporated in 4th generation wireless standards such as WiMAX and LTE. The decoding algorithm is conditional optimization which reduces to a parallel calculation that is a natural fit to the architecture of low cost GPUs. Experimental results demonstrate that asymptotically the GPU implementation is more than 700 times faster than a standard serial implementation. These results suggest that GPU architectures have the potential to improve the cost / performance tradeoff of 4th generation wireless base stations. Additional benefits might include reducing the time required for system development and the time required for configuration and testing of wireless base stations.,space,332
10.1109/vrais.1995.512487,filtered,Proceedings Virtual Reality Annual International Symposium '95,IEEE,1995-03-15 00:00:00,ieeexplore,human figure synthesis and animation for virtual space teleconferencing,https://ieeexplore.ieee.org/document/512487/,"Human figure animation is it widely researched area with many applications. This paper addresses specific issues that deal with the synthesis, animation and environmental interaction of human figures within a virtual space teleconferencing system. A layered representation of the human figure is adopted. Skeletal posture is determined from magnetic sensors on the body, using heuristics and inverse kinematics. This paper describes the use of implicit function techniques in the synthesis and animation of a polymesh geometric skin over the skeletal structure. Implicit functions perform detection and handling of collisions with an optimal worst case time complexity that is linear in the number polymesh vertices. Body deformations resulting from auto-collisions are handled elegantly and homogeneously as part of the environment. Further, implicit functions generate precise collision contact surfaces and have the capability to model the physical characteristics of muscles in systems that employ force feedback. The real time implementation within a virtual space teleconferencing system, illustrates this new approach, coupling polymesh and implicit surface based modeling and animation techniques.",space,333
10.1109/icassp39728.2021.9414951,filtered,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",IEEE,2021-06-11 00:00:00,ieeexplore,kernel regression on graphs in random fourier features space,https://ieeexplore.ieee.org/document/9414951/,"This work proposes an efficient batch-based implementation for kernel regression on graphs (KRG) using random Fourier features (RFF) and a low-complexity online implementation. Kernel regression has proven to be an efficient learning tool in the graph signal processing framework. However, it suffers from poor scalability inherent to kernel methods. We employ RFF to overcome this issue and derive a batch-based KRG whose model size is independent of the training sample size. We then combine it with a stochastic gradient-descent approach to propose an online algorithm for KRG, namely the stochastic-gradient KRG (SGKRG). We also derive sufficient conditions for convergence in the mean sense of the online algorithms. We validate the performance of the proposed algorithms through numerical experiments using both synthesized and real data. Results show that the proposed batch-based implementation can match the performance of conventional KRG while having reduced complexity. Moreover, the online implementations effectively learn the target model and achieve competitive performance compared to the batch implementations.",space,334
10.1109/radar.2017.7944428,filtered,2017 IEEE Radar Conference (RadarConf),IEEE,2017-05-12 00:00:00,ieeexplore,localized random projections for space-time adaptive processing,https://ieeexplore.ieee.org/document/7944428/,"High-dimensional multi-sensor radar data suffers from the well known curse of dimensionality. For example, in radar space time adaptive processing (STAP), training data from neighboring range cells is limited, since the statistical properties vary significantly over range and azimuth. Therefore, precluding straightforward implementation of standard detectors, for example, the whitening minimum variance distortionless response filter. Using random projections, we can reduce the dimension of the radar problem by random sampling, i.e. by projecting the data into a random d-dimensional subspace. The Johnson-Lindenstrauss (JL) theorem provides theoretical guarantees which explicitly states that the low dimensional data after random projections is only very slightly perturbed when compared to the data from the original problem in an l<sub>2</sub> norm sense. Random projections offers significant computational savings permitting possible real time solutions, however, at the cost of reducing the clairvoyant SINR for radar STAP. To alleviate this issue of SINR loss, we use localized random projections where the random projection matrix incorporates the look angle information, thereby minimizing the noise and interference effects from other angles, and increasing the SINR. We show that the resulting detector is CFAR, and the transformation matrix satisfies all the necessary conditions for the the JL theorem to hold.",space,335
10.1109/iscas.1994.409629,filtered,Proceedings of IEEE International Symposium on Circuits and Systems - ISCAS '94,IEEE,1994-06-02 00:00:00,ieeexplore,neural networks using bit stream arithmetic: a space efficient implementation,https://ieeexplore.ieee.org/document/409629/,"In this paper an expandable digital architecture that provides an efficient implementation base for large neural networks, is presented. The architecture uses the circuit for arithmetic operations on delta encoded signals to carry out the large number of required parallel synaptic calculations. All real valued quantities are encoded on delta bit streams. The actual digital circuitry is simple and highly regular, thus allowing very efficient space usage of fine grained FPGAs.&lt;<ETX>&gt;</ETX>",space,336
10.1109/icdm.2003.1250919,filtered,Third IEEE International Conference on Data Mining,IEEE,2003-11-22 00:00:00,ieeexplore,op-cluster: clustering by tendency in high dimensional space,https://ieeexplore.ieee.org/document/1250919/,"Clustering is the process of grouping a set of objects into classes of similar objects. Because of unknownness of the hidden patterns in the data sets, the definition of similarity is very subtle. Until recently, similarity measures are typically based on distances, e.g Euclidean distance and cosine distance. We propose a flexible yet powerful clustering model, namely OP-cluster (Order Preserving Cluster). Under this new model, two objects are similar on a subset of dimensions if the values of these two objects induce the same relative order of those dimensions. Such a cluster might arise when the expression levels of (coregulated) genes can rise or fall synchronously in response to a sequence of environment stimuli. Hence, discovery of OP-Cluster is essential in revealing significant gene regulatory networks. A deterministic algorithm is designed and implemented to discover all the significant OP-Clusters. A set of extensive experiments has been done on several real biological data sets to demonstrate its effectiveness and efficiency in detecting coregulated patterns.",space,337
10.1109/uic-atc-scalcom.2014.75,filtered,2014 IEEE 11th Intl Conf on Ubiquitous Intelligence and Computing and 2014 IEEE 11th Intl Conf on Autonomic and Trusted Computing and 2014 IEEE 14th Intl Conf on Scalable Computing and Communications and Its Associated Workshops,IEEE,2014-12-12 00:00:00,ieeexplore,private smart space: cost-effective adls (activities of daily livings) recognition based on superset transformation,https://ieeexplore.ieee.org/document/7307038/,"Aging population inspired the market on advanced real time caring for the elder in home setting, accurately recognizing human activities is a challenging task. Activities of daily living are good indicators for behavior recognition. In this paper, we describe a new method to deploy a cost-effective solution which can be run on embedded device as smart router. We use the open dataset, map the raw dataset into a sparse binary matrix, unique by the time line and activity tags. Decision tree algorithm is applied to train the model, in order to achieve the goal that simple comparison work to implement the model and get a quick respond at high accuracy. We evaluate our approach by 3-fold cross validation and achieve a time-slice accuracy of 98.45%.",space,338
10.1109/ccaaw.2019.8904903,filtered,2019 IEEE Cognitive Communications for Aerospace Applications Workshop (CCAAW),IEEE,2019-06-26 00:00:00,ieeexplore,quantifying degradations of convolutional neural networks in space environments,https://ieeexplore.ieee.org/document/8904903/,"Advances in machine learning applications for image processing, natural language processing, and direct ingestion of radio frequency signals continue to accelerate. Less attention, however, has been paid to the resilience of these machine learning algorithms when implemented on real hardware and subjected to unintentional and/or malicious errors during execution, such as those occurring from space-based single event upsets (SEU). This paper presents a series of results quantifying the rate and level of performance degradation that occurs when convolutional neural nets (CNNs) are subjected to selected bit errors in single-precision number representations. This paper provides results that are conditioned upon ten different error case events to isolate the impacts showing that CNN performance can be gradually degraded or reduced to random guessing based on where errors arise. The degradations are then translated into expected operational lifetimes for each of four CNNs when deployed to space radiation environments. The discussion also provides a foundation for ongoing research that enhances the overall resilience of neural net architectures and implementations in space under both random and malicious error events, offering significant improvements over current implementations. Future work to extend these CNN resilience evaluations, conditioned upon architectural design elements and well-known error correction methods, is also introduced.",space,339
10.1109/cvpr46437.2021.00844,filtered,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),IEEE,2021-06-25 00:00:00,ieeexplore,robust neural routing through space partitions for camera relocalization in dynamic indoor environments,https://ieeexplore.ieee.org/document/9577932/,"Localizing the camera in a known indoor environment is a key building block for scene mapping, robot navigation, AR, etc. Recent advances estimate the camera pose via optimization over the 2D/3D-3D correspondences established between the coordinates in 2D/3D camera space and 3D world space. Such a mapping is estimated with either a convolution neural network or a decision tree using only the static input image sequence, which makes these approaches vulnerable to dynamic indoor environments that are quite common yet challenging in the real world. To address the aforementioned issues, in this paper, we propose a novel outlier-aware neural tree which bridges the two worlds, deep learning and decision tree approaches. It builds on three important blocks: (a) a hierarchical space partition over the indoor scene to construct the decision tree; (b) a neural routing function, implemented as a deep classification network, employed for better 3D scene understanding; and (c) an outlier rejection module used to filter out dynamic points during the hierarchical routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark developed for camera relocalization in dynamic indoor environments. It achieves robust neural routing through space partitions and outperforms the state-of-the-art approaches by around 30% on camera pose accuracy, while running comparably fast for evaluation.",space,340
10.1109/icwapr.2007.4421653,filtered,2007 International Conference on Wavelet Analysis and Pattern Recognition,IEEE,2007-11-04 00:00:00,ieeexplore,small-shaped space target recognition based on wavelet decomposition and support vector machine,https://ieeexplore.ieee.org/document/4421653/,"A kind of method for small-shaped space target recognition was proposed in this paper based on feature extraction with wavelet decomposition and formative support vector machine (FSVM) with sequential minimal optimization (SMO) algorithm. Firstly, the significance and characteristics of space target recognition were discussed and a two-stage recognition strategy was designed. And then aiming at the characteristics of small-shaped space target recognition, a new method was implemented based on feature extraction with wavelet decomposition and FSVM with SMO algorithm. Simulation results show the good performance of the algorithm proposed in this paper: the correct rate is more than 97% within 1360 simulation samples of ten classes of small shaped space targets; meanwhile the algorithm is characterized with high speed of near real time in both implementation of training and testing.",space,341
10.23919/eusipco.2019.8902815,filtered,2019 27th European Signal Processing Conference (EUSIPCO),IEEE,2019-09-06 00:00:00,ieeexplore,"state space models with dynamical and sparse variances, and inference by em message passing",https://ieeexplore.ieee.org/document/8902815/,"Sparse Bayesian learning (SBL) is a probabilistic approach to estimation problems based on representing sparsity-promoting priors by Normals with Unknown Variances. This representation blends well with linear Gaussian state space models (SSMs). However, in classical SBL the unknown variances are a priori independent, which is not suited for modeling group sparse signals, or signals whose variances have structure. To model signals with, e.g., exponentially decaying or piecewise-constant (in particular block-sparse) variances, we propose SSMs with dynamical and sparse variances (SSM-DSV). These are two-layer SSMs, where the bottom layer models physical signals, and the top layer models dynamical variances that are subject to abrupt changes. Inference and learning in these hierarchical models is performed with a message passing version of the expectation maximization (EM) algorithm, which is a special instance of the more general class of variational message passing algorithms. We validated the proposed model and estimation algorithm with two applications, using both simulated and real data. First, we implemented a block-outlier insensitive Kalman smoother by modeling the disturbance process with a SSM-DSV. Second, we used SSM-DSV to model the oculomotor system and employed EM-message passing for estimating neural controller signals from eye position data.",space,342
10.1109/irsec.2015.7455135,filtered,2015 3rd International Renewable and Sustainable Energy Conference (IRSEC),IEEE,2015-12-13 00:00:00,ieeexplore,state space neural network control (ssnnc) of upfc for compensation power,https://ieeexplore.ieee.org/document/7455135/,"In our present communication, we present the effectiveness of the controller's Unified Power Flow Controller UPFC with the choice of a control strategy. This Unified Power Flow Controller (UPFC) is used to control the power flow in the transmission systems by controlling the impedance, voltage magnitude and phase angle. This controller offers advantages in terms of static and dynamic operation of the power system. It also brings in new challenges in power electronics and power system design. To evaluate the performance and robustness of the system, we proposed a hybrid control combining the concept of identification neural networks with conventional regulators (SSNNC) and with the changes in characteristics of the transmission line in order to improve the stability of the electrical power network. With its unique capability to control simultaneously real and reactive power flows on a transmission line as well as to regulate voltage at the bus where it is connected, this device creates a tremendous quality impact on power system stability. The result which has been obtained from using Matlab and Simulink software showed a good agreement with the simulation result.",space,343
10.1109/ecai.2016.7861123,filtered,"2016 8th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",IEEE,2016-07-02 00:00:00,ieeexplore,the approach of wavelength dense multiplexing using free space optical systems,https://ieeexplore.ieee.org/document/7861123/,"This paper aims to explore the ways of implementation for WDM technology that is specific for high capacity fiber optic systems, using FSO terrestrial links. It will study the peculiarities of the CWDM-SO versus DWDM-SO transmission, stating their limits (transmission distance versus BER for constant bitrate and atmospheric attenuation). It will calculate the BER value for each CWDM/DWDM channel, idealized and real conditions (with attenuation). The simulations will be made under Optiwave environment for a system with 8 CWDM/DWDM channels, performance evaluation being made using the eye diagram, WDM and optical spectrum analyzers.",space,344
10.1109/icdcsw53096.2021.00009,filtered,2021 IEEE 41st International Conference on Distributed Computing Systems Workshops (ICDCSW),IEEE,2021-07-10 00:00:00,ieeexplore,towards understanding the adaptation space of ai-assisted data protection for video analytics at the edge,https://ieeexplore.ieee.org/document/9545916/,"Edge computing facilitates the deployment of distributed AI applications, capable of processing video data in real time. AI-assisted video analytics can provide valuable information and benefits in various domains. Face recognition, object detection, or movement tracing are prominent examples enabled by this technology. However, such mechanisms also entail threats regarding privacy and security, for example if the video contains identifiable persons. Therefore, adequate data protection is an increasing concern in video analytics. AI-assisted data protection mechanisms, such as face blurring, can help, but are often computationally expensive. Additionally, the heterogeneous hardware of end devices and the time-varying load on edge services need to be considered. Therefore, such systems need to adapt to react to changes during their operation, ensuring that conflicting requirements on data protection, performance, and accuracy are addressed in the best possible way. Sound adaptation decisions require an understanding of the adaptation options and their impact on different quality attributes. In this paper, we identify factors that can be adapted in AI-assisted data protection for video analytics using the example of a face blurring pipeline. We measure the impact of these factors using a heterogeneous edge computing hardware testbed. The results show a large and complex adaptation space, with varied impacts on data protection, performance, and accuracy.",space,345
10.1109/aim43001.2020.9158908,filtered,2020 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM),IEEE,2020-07-09 00:00:00,ieeexplore,towards accelerated robotic deployment by supervised learning of latent space observer and policy from simulated experiments with expert policies,https://ieeexplore.ieee.org/document/9158908/,"Up until today robotic tasks in highly variable environments remain very difficult to solve. We propose accelerated robotic deployment through task solving on low-level sensor data in simulation. A simulation allows for a lot of data, which is usually not available in a real world robotic setup due to cost and feasibility. Solving tasks in simulation is safe and a lot easier due to the huge amount of feedback from virtual sensory data. We present a novel sim2real architecture for converting simulated low level sensor data policies to high level real world policies. After solving a task we let the robot complete it a number of times in simulation using domain randomization, while doing so we save the simulated sensor data corresponding to the real robotic setup and actions taken. Given these sensor data and actions a task specific policy can be trained using our architecture. In this paper we work towards a proof of concept by simulating a simple low cost manipulator in pybullet to pick and place an object based on image observations.",space,346
10.1109/ieit53597.2021.00077,filtered,"2021 International Conference on Internet, Education and Information Technology (IEIT)",IEEE,2021-04-18 00:00:00,ieeexplore,uav control in smart city based on space-air-ground integrated network,https://ieeexplore.ieee.org/document/9526159/,"Unmanned Aerial Vehicle (UAV) is an important part of the wireless network system of the future smart city. As a difficult point in the large-scale application of UAV, UAV control gradually attracts people's attention. Aiming at the problems of UAV control in smart city application, a near real time online learning architecture of UAV control based on the software-defined space-air-ground integrated network (SSAG) was proposed. This architecture uses the two-layer software defined network (SDN) controller architecture of SSAG framework to separate UAV control. The upper-tier SDN controller is responsible for the scheduling of UAV configuration, while the lower-tier SDN controller is responsible for regional coordination of UAV. The upper-tier SDN controller updates the tendency of network states by acquiring network states information in time interval. By simulating the network state in the next time interval, the optimal strategy of UAV scheduling of the next time interval is obtained by using the strategy iteration algorithm. Finally, an example is given to verify that the near real-time online learning architecture can accurately predict the UAV requirement, and increase the throughput of the network system compared with the traditional approach.",space,347
10.1109/mmar49549.2021.9528467,filtered,2021 25th International Conference on Methods and Models in Automation and Robotics (MMAR),IEEE,2021-08-26 00:00:00,ieeexplore,"virtual urban space simulator — — gliwice, poland city centre example in the context of covid-19 pandemic",https://ieeexplore.ieee.org/document/9528467/,"Virtual environments (VE) are commonly used in learning or training in many different areas. We are focused on the problem when a group of artificial agents are simulated at the same time; their behavior is easily recognized as ‘artificial’. To avoid this undesired property presented work was aimed at creating a virtual world with agents equipped with psychosomatic elements that occur in the real world. It has been chosen the implementation of basic needs based on Maslow's hierarchy of needs (MHN), since they are one of necessary elements for the development of simulation of the real world. It is shown that the VE design based on artificial intelligence (AI) planning algorithms and the theory of MHN can be efficiently applied to generate semi-realistic behavior of agents population. Models of coronavirus spread have been introduced into the simulated environment. It allowed to build cases related to the number of infected people and the rate of infection depending on the level of avatar activity associated with basic needs.",space,348
10.1109/tits.2018.2882439,filtered,IEEE Transactions on Intelligent Transportation Systems,IEEE,2019-12-01 00:00:00,ieeexplore,convolutional neural networks for on-street parking space detection in urban networks,https://ieeexplore.ieee.org/document/8577026/,"The purpose of this paper is the development of data science models for the detection of empty on-street parking spaces in urban road networks based on data provided by in-vehicle cameras that are already, or soon will be, a standard vehicle equipment. A rolling spatial interval is used to identify the existence of an on-street parking space and the properties of empty spaces are used to determine the availability of the parking space. Convolutional neural networks are developed, trained, and evaluated with the use of images from a moving vehicle camera. The images are preprocessed and converted to suitable matrices, so that only the useful information for the empty on-street parking space detection problem is preserved. The optimized convolutional networks, in terms of structural and learning parameters, provided predictions for the detection of empty on-street parking spaces with approximately 90% average accuracy. The proposed model performs better than the relatively complex SVMs, which supports its appropriateness as an approach. Finally, the implementation of a framework, which integrates the developed models to produce meaningful parking information for drivers in real time, is discussed.",space,349
10.1109/titb.2004.832550,filtered,IEEE Transactions on Information Technology in Biomedicine,IEEE,2004-09-01 00:00:00,ieeexplore,dbmap: a space-conscious data visualization and knowledge discovery framework for biomedical data warehouse,https://ieeexplore.ieee.org/document/1331412/,"Advances in digital imaging modalities as well as other diagnosis and therapeutic techniques have generated a massive amount of diverse data for clinical research. The purpose of this study is to investigate and implement a new intuitive and space-conscious visualization framework, called DBMap, to facilitate efficient multidimensional data visualization and knowledge discovery against the large-scale data warehouses of integrated image and nonimage data. The DBMap framework is built upon the TreeMap concept. TreeMap is a space constrained graphical representation of large hierarchical data sets, mapped to a matrix of rectangles, whose size and color represent interested database fields. It allows the display of a large amount of numerical and categorical information in limited real estate of the computer screen with an intuitive user interface. DBMap has been implemented and integrated into a large brain research data warehouse to support neurologic and neuroradiologic research at the University of California, San Francisco Medical Center. For imaging specialists and clinical researchers, this novel DBMap framework facilitates another way to better explore and classify the hidden knowledge embedded in medical image data warehouses.",space,350
10.1109/access.2021.3064928,filtered,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,deep space network scheduling via mixed-integer linear programming,https://ieeexplore.ieee.org/document/9373338/,"NASA’s Deep Space Network (DSN) is a globally-spanning communications network responsible for supporting the interplanetary spacecraft missions of NASA and other international users. The DSN is a highly utilized asset, and the large demand for its’ services makes the assignment of DSN resources a daunting computational problem. In this paper we study the DSN scheduling problem, which is the problem of assigning the DSN’s limited resources to its users within a given time horizon. The DSN scheduling problem is oversubscribed, meaning that only a subset of the activities can be scheduled, and network operators must decide which activities to exclude from the schedule. We first formulate this challenging scheduling task as a Mixed-Integer Linear Programming (MILP) optimization problem. Next, we develop a sequential algorithm which solves the resulting MILP formulation to produce valid schedules for large-scale instances of the DSN scheduling problem. We use real world DSN data from week 44 of 2016 in order to evaluate our algorithm’s performance. We find that given a fixed run time, our algorithm outperforms a simple implementation of our MILP model, generating a feasible schedule in which 17% more activities are scheduled by the algorithm than by the simple implementation. We design a non-MILP based heuristic to further validate our results. We find that our algorithm also outperforms this heuristic, scheduling 8% more activities and 20% more tracking time than the best results achieved by the non-MILP implementation.",space,351
10.1109/18.985979,filtered,IEEE Transactions on Information Theory,IEEE,2002-03-01 00:00:00,ieeexplore,diagonal algebraic space-time block codes,https://ieeexplore.ieee.org/document/985979/,"We construct a new family of linear space-time (ST) block codes by the combination of rotated constellations and the Hadamard transform, and we prove them to achieve the full transmit diversity over a quasi-static or fast fading channels. The proposed codes transmit at a normalized rate of 1 symbol/s. When the number of transmit antennas n=1, 2, or n is a multiple of four, we spread a rotated version of the information symbol vector by the Hadamard transform and send it over n transmit antennas and n time periods; for other values of n, we construct the codes by sending the components of a rotated version of the information symbol vector over the diagonal of an n /spl times/ n ST code matrix. The codes maintain their rate, diversity, and coding gains for all real and complex constellations carved from the complex integers ring Z [i], and they outperform the codes from orthogonal design when using complex constellations for n &gt; 2. The maximum-likelihood (ML) decoding of the proposed codes can be implemented by the sphere decoder at a moderate complexity. It is shown that using the proposed codes in a multiantenna system yields good performances with high spectral efficiency and moderate decoding complexity.",space,352
10.1109/access.2019.2918480,filtered,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,distribution network reconfiguration using selective firefly algorithm and a load flow analysis criterion for reducing the search space,https://ieeexplore.ieee.org/document/8720166/,"This paper proposes an alternative to solve the distribution network reconfiguration (DNR) problem, aiming real power losses' minimization. For being a problem that has complexity for its solution, approximate techniques are adequate for solving it. Here, the proposition is a technique based on the firefly metaheuristic, named selective firefly algorithm, where the positioning of these insects is compressed in a selective range of values. The algorithm is applied to the DNR, and all its implementation and adequacy to the problem studied are presented. To define the search space, the methodology presented initially considers a set of candidate switches for opening based on the studied systems' mesh analysis. To reduce these possibilities, a refinement through a load flow analysis criterion (LFAC) is proposed. This LFAC considers the real power losses on each branch for a configuration with all switches closed, then, selecting possible switches to elimination from the set previously established. To demonstrate the behavior and the viability of the LFAC, it was initially applied on a 5 buses' and 7 branches' system. Also, to avoid getting stuck on results that may be considered not good, a disturbance resetting the population is set to occur every time a counter reaches a pre-defined number of times that the best solution does not change. Results found for simulations with 33, 70, and 84 buses are presented and comparisons with selective particle swarm optimization (SPSO) and selective bat algorithm (SBAT) are made.",space,353
10.1109/tcbb.2013.119,filtered,IEEE/ACM Transactions on Computational Biology and Bioinformatics,IEEE,2013-11-01 00:00:00,ieeexplore,eeg/erp adaptive noise canceller design with controlled search space (css) approach in cuckoo and other optimization algorithms,https://ieeexplore.ieee.org/document/6606790/,"This paper explores the migration of adaptive filtering with swarm intelligence/evolutionary techniques employed in the field of electroencephalogram/event-related potential noise cancellation or extraction. A new approach is proposed in the form of controlled search space to stabilize the randomness of swarm intelligence techniques especially for the EEG signal. Swarm-based algorithms such as Particles Swarm Optimization, Artificial Bee Colony, and Cuckoo Optimization Algorithm with their variants are implemented to design optimized adaptive noise canceler. The proposed controlled search space technique is tested on each of the swarm intelligence techniques and is found to be more accurate and powerful. Adaptive noise canceler with traditional algorithms such as least-mean-square, normalized least-mean-square, and recursive least-mean-square algorithms are also implemented to compare the results. ERP signals such as simulated visual evoked potential, real visual evoked potential, and real sensorimotor evoked potential are used, due to their physiological importance in various EEG studies. Average computational time and shape measures of evolutionary techniques are observed 8.21E-01 sec and 1.73E-01, respectively. Though, traditional algorithms take negligible time consumption, but are unable to offer good shape preservation of ERP, noticed as average computational time and shape measure difference, 1.41E-02 sec and 2.60E+00, respectively.",space,354
10.1109/access.2021.3136138,filtered,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,extending the space of software test monitoring: practical experience,https://ieeexplore.ieee.org/document/9652520/,"Software reliability depends on the performed tests. Bug detection and diagnosis are based on test outcome (oracle) analysis. Most of practical test reports do not provide sufficient information for localizing and correcting bugs. We have found the need to extend the space of test result observation in data and time perspectives. This resulted in tracing supplementary test result features in event logs. They are explored with combined text mining and log parsing techniques. Another important point is correlating test life cycle with project development history journaled in issue tracking and software version control repositories. Dealing with the outlined problems, neglected in the literature, we have introduced original analysis schemes. They focus on assessing test coverage, reasons of low diagnosability, and test result profiles. Multidimensional investigation of test features and their management is supported with the developed test infrastructure. This assures a holistic insight into the test efficiency to identify test scheme deficiencies (e.g., functional inadequacy, aging, insufficient coverage) and possible improvements (test set updates). Our studies have been verified in relevance to a real commercial project and confronted with the experience of testers engaged in other projects.",space,355
10.1109/tcomm.2005.847166,filtered,IEEE Transactions on Communications,IEEE,2005-05-01 00:00:00,ieeexplore,generalized psk in space-time coding,https://ieeexplore.ieee.org/document/1431123/,"A wireless communication system using multiple antennas promises reliable transmission under Rayleigh flat fading assumptions. Design criteria and practical schemes have been presented for both coherent and noncoherent communication channels. In this paper, we generalize one-dimensional (1-D) phase-shift keying (PSK) signals and introduce space-time constellations from generalized PSK (GPSK) signals based on the complex and real orthogonal designs. The resulting space-time constellations reallocate the energy for each transmitting antenna and feature good diversity products; consequently, their performances are better than some of the existing comparable codes. Moreover, since the maximum-likelihood (ML) decoding of our proposed codes can be decomposed to 1-D PSK signal demodulation, the ML decoding of our codes can be implemented in a very efficient way.",space,356
10.1109/tamd.2011.2106781,filtered,IEEE Transactions on Autonomous Mental Development,IEEE,2011-03-01 00:00:00,ieeexplore,implicit sensorimotor mapping of the peripersonal space by gazing and reaching,https://ieeexplore.ieee.org/document/5703113/,"Primates often perform coordinated eye and arm movements, contextually fixating and reaching towards nearby objects. This combination of looking and reaching to the same target is used by infants to establish an implicit visuomotor representation of the peripersonal space, useful for both oculomotor and arm motor control. In this work, taking inspiration from such behavior and from primate visuomotor mechanisms, a shared sensorimotor map of the environment, built on a radial basis function framework, is configured and trained by the coordinated control of eye and arm movements. Computational results confirm that the approach seems especially suitable for the problem at hand, and for its implementation on a real humanoid robot. By exploratory gazing and reaching actions, either free or goal-based, the artificial agent learns to perform direct and inverse transformations between stereo vision, oculomotor, and joint-space representations. The integrated sensorimotor map that allows to contextually represent the peripersonal space through different vision and motor parameters is never made explicit, but rather emerges thanks to the interaction of the agent with the environment.",space,357
10.1093/comjnl/bxu058,filtered,The Computer Journal,OUP,2015-06-01 00:00:00,ieeexplore,multi-agent architecture for control of heating and cooling in a residential space,https://ieeexplore.ieee.org/document/8131352/,"Energy demand in a smart grid is directly related to energy consumption, as defined by user needs and comfort experience. This article presents a multi-agent architecture for smart control of space heating and cooling processes, in an attempt to enable flexible ways of monitoring and adjusting energy supply and demand. In this proposed system, control agents are implemented in order to perform temperature set-point delegation for heating and cooling systems in a building, offering a means to observe and learn from both the environment and the occupant. Operation of the proposed algorithms is compared with traditional algorithms utilized for room heating, using a simulated model of a residential building and real data about user behaviour. The results show (i) the performance of machine learning for the occupancy forecasting problem and for the problem of calculating the time to heat or cool a room; and (ii) the performance of the control algorithms, with respect to energy consumption and occupant comfort. The proposed control agents make it possible to significantly improve an occupant comfort with a relatively small increase in energy consumption, compared with simple control strategies that always maintain predefined temperatures. The findings enable the smart grid to anticipate the energy needs of the building.",space,358
10.1109/tpami.2012.82,filtered,IEEE Transactions on Pattern Analysis and Machine Intelligence,IEEE,2013-01-01 00:00:00,ieeexplore,removing atmospheric turbulence via space-invariant deconvolution,https://ieeexplore.ieee.org/document/6178259/,"To correct geometric distortion and reduce space and time-varying blur, a new approach is proposed in this paper capable of restoring a single high-quality image from a given image sequence distorted by atmospheric turbulence. This approach reduces the space and time-varying deblurring problem to a shift invariant one. It first registers each frame to suppress geometric deformation through B-spline-based nonrigid registration. Next, a temporal regression process is carried out to produce an image from the registered frames, which can be viewed as being convolved with a space invariant near-diffraction-limited blur. Finally, a blind deconvolution algorithm is implemented to deblur the fused image, generating a final output. Experiments using real data illustrate that this approach can effectively alleviate blur and distortions, recover details of the scene, and significantly improve visual quality.",space,359
10.1109/36.317435,filtered,IEEE Transactions on Geoscience and Remote Sensing,IEEE,1993-11-01 00:00:00,ieeexplore,space instrument neural network for real-time data analysis,https://ieeexplore.ieee.org/document/317435/,"A simple software implementation of an artificial neural network (ANN) was used to analyze up to 200 autocorrelation functions (ACFs) per second within the Shuttle Potential and Return Electron Experiment (SPREE) flown on the Shuttle STS46 mission, July 31, 1992. As all ACF data are stored onboard until postmission, this facility provided ground-based experimenters with their only access to ACF data in real time for optimum instrument control. ACFs contain data either as waveforms or as radar echoes. Operating directly on the ACF, the neural network identifies the type of data, ascertains the wave frequency or radar peak separation, and provides a score or measure of significance of its decision. An effective 16:1 data reduction is achieved and the data interpretation performance is comparable to that achieved by an expert data analyst. Erroneous analysis accounts for less than 1% of data analyzed.&lt;<ETX>&gt;</ETX>",space,360
10.1109/tps.2013.2276537,filtered,IEEE Transactions on Plasma Science,IEEE,2013-09-01 00:00:00,ieeexplore,space-varying templates for real-time applications of cellular nonlinear networks to pattern recognition in nuclear fusion,https://ieeexplore.ieee.org/document/6582663/,"In this paper, a new methodology consisting of space-varying templates in cellular nonlinear networks (CNNs) for real-time visual pattern recognition in nuclear fusion devices is presented. The development of space-varying templates is a new upgrade, driven by the need to process different parts of the images in different ways. The new approach has been applied to the identification in real time of various objects present in the Joint European Torus videos of both infrared (IR) and visible cameras. IR videos are here used to detect hot spots and the regions of the walls in which dangerously high temperatures are reached, whereas visible cameras provide information about multifaceted asymmetric radiations from the edge, which are dangerous instabilities that can lead to disruptions. Their identification is particularly difficult because of their movement and their shape which is similar to other objects present in the frames. Therefore, in addition to space-varying template CNNs, quite sophisticated morphological operators have to be deployed and their outputs processed by machine learning tools, such as support vector machines. The implementation of the whole methodology was performed in a field-programmable gate array board, obtaining, in both applications, a final success rate close to 100% and a frame rate higher than 200 frames/s.",space,361
10.1109/jiot.2021.3051343,filtered,IEEE Internet of Things Journal,IEEE,2021-07-15 00:00:00,ieeexplore,the study of urban residential’s public space activeness using space-centric approach,https://ieeexplore.ieee.org/document/9321451/,"With the advancement of the Internet of Things (IoT) and communication platform, large-scale sensor deployment can be easily implemented in an urban city to collect various information. To date, there are only a handful of research studies about understanding the usage of urban public spaces. Leveraging IoT, various sensors have been deployed in an urban residential area to monitor and study public space utilization patterns. In this article, we propose a data processing system to generate space-centric insights about the utilization of an urban residential region of multiple Points of Interests (PoIs) that consists of 190 000 m<sup>2</sup> real estate. We identify the activeness of each PoI based on the spectral clustering, and then study their corresponding static features, which are composed of transportation, commercial facilities, population density, along with other characteristics. Through the heuristic features inferring, the residential density and commercial facilities are the most significant factors affecting public place utilization.",space,362
10.1109/isscc.2019.8662455,filtered,2019 IEEE International Solid- State Circuits Conference - (ISSCC),IEEE,2019-02-21 00:00:00,ieeexplore,2.5 a 40×40 four-neighbor time-based in-memory computing graph asic chip featuring wavefront expansion and 2d gradient control,https://ieeexplore.ieee.org/document/8662455/,"Single-source shortest path (SSP) problems have a rich history of algorithm development [1-3]. SSP has many applications including AI decision making, robot navigation, VLSI signal routing, autonomous vehicles and many other classes of problems that can be mapped onto graphs. Conventional algorithms rely on sequentially traversing the search space, which is inherently limited by traditional computer architecture. In graphs which become very large, this slow processing time can become a bottleneck in real world applications. We propose a time-based ASIC to address this issue. Our design leverages a dedicated hardware implementation to solve these problems in linear time complexity with superior energy efficiency. A $40\times40$ four-neighbor grid implements a wavefront (WF) expansion with a first-in lockout mechanism to enable traceback. Outside the array, a programmable resistive ladder provides bias voltages to the edge cells, which enables pulse shaping reminiscent of the A* algorithm [3].",space,363
10.1109/iih-msp.2007.3,filtered,Third International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP 2007),IEEE,2007-11-28 00:00:00,ieeexplore,2d(pc)2 a for face recognition with one training image per person,https://ieeexplore.ieee.org/document/4457509/,"In the real-world application of face recognition system, owing to the difficulties of collecting samples or storage space of systems, only one sample image per person is stored in the system, which is so-called one sample per person problem. In this paper, we propose a novel algorithm, called 2D(PC)<sup>2</sup>A, to solve this problem. The procedure of 2D(PC)<sup>2</sup>A can be divided into the three stages: 1) creating the combined image from the original image 2) performing 2DPCA on the combined images; 3) classifying a new face based on assembled matrix distance (AMD). Experiments implemented on two real datasets show that 2D(PC)<sup>2</sup>A method is an efficient and practical approach for face recognition.",space,364
10.1109/adconip.2017.7983776,filtered,2017 6th International Symposium on Advanced Control of Industrial Processes (AdCONIP),IEEE,2017-05-31 00:00:00,ieeexplore,a bayesian learning and data mining approach to reaction system identification: application to biomass conversion,https://ieeexplore.ieee.org/document/7983776/,"The growing environmental concern over the use of fossil fuels calls for alternative sources of energy with smaller environmental footprint, and biomass-derived fuels have been extensively investigated as a substitute. In biofuels production, the development of reaction networks and kinetic models is unquestionably a major challenge due to the difficulty in characterizing the reaction products. Therefore, there is a need for a better way to retrieve the information about the reaction from the available experimental data. This study uses a data mining and Bayesian learning approach to estimate the reaction network of the acid and base catalyzed hydrous pyrolysis of hemicellulose from Fourier Transform Infrared (FTIR) spectroscopy. Cluster analysis is used to model the system in terms of lumps and a Bayesian network structure-learning algorithm is then used to device a reaction network. Three Bayesian network structure-learning algorithms were implemented to estimate the reaction network. The results from each were identical, indicating that the model representing the reaction network is most probably in the optimal equivalence space. The model was compared against expert-based reaction models and the agreement is encouraging. A useful aspect of this model is its self-updating capability, i.e., the reaction model can provide a quantitative description of the effect of the change in the operation condition from spectroscopic data. Hence, the model may be used for the real time analysis of the investigated process.",space,365
10.1109/indiancc.2019.8715586,filtered,2019 Fifth Indian Control Conference (ICC),IEEE,2019-01-11 00:00:00,ieeexplore,a chance constrained programming based multi-criteria decision making under uncertainty,https://ieeexplore.ieee.org/document/8715586/,"Multi-criteria decision making under uncertainty is a common practice followed in industries and academia. Among several types of uncertainty handling techniques, Chance Constrained Programming (CCP) is considered as an efficient and tractable approach provided one has accessibility to distribution of the data for uncertain parameters. However, the assumption that the uncertain parameters must follow some well-behaved probability distribution is a myth for most of the practical applications. This paper proposes a methodology to amalgamate machine learning algorithms with CCP and thereby make it data-driven. A novel fuzzy clustering mechanism is implemented to transcript the uncertain space such that the exact regions of uncertainty are identified. Subsequently, density based boundary point detection and Delaunay triangulation based boundary construction enable intelligent Sobol based sampling in these regions for use in CCP. The Fuzzy clustering mechanism used in the proposed method transforms the existing fuzzy C-means technique such that the decision variables are significantly reduced. This enables evolutionary optimizers to obtain better approximations of the uncertain space by identifying the true clusters. A highly nonlinear real life model for continuous casting from steelmaking industries is considered as a case study for testing the efficiency of data based CCP along with a comprehensive comparison between conventional CCP approach using box uncertainty set and proposed methodology. As the resulting CCP problem is multi-objective in nature, the Pareto solutions are obtained by NSGA II.",space,366
10.1109/icisce48695.2019.00014,filtered,2019 6th International Conference on Information Science and Control Engineering (ICISCE),IEEE,2019-12-22 00:00:00,ieeexplore,a deep reinforcement learning malware detection method based on pe feature distribution,https://ieeexplore.ieee.org/document/9107644/,"Existing anti-virus software and malware detection methods, including signature-based and the machine learning-based malware detection methods, are unable to update the virus database in real time, resulting in poor resistance to malware variants. To solve this problem, this paper proposes a novel malware detection method based on deep reinforcement learning, which combines the advantages of Q-learning and neural network. Q-learning action selection strategy is adopted while solving the problem of high dimensional state space. Theoretical analysis and experimental results show that the proposed method can not only detect malware variants efficiently, but also perform well in many well-known anti-virus software, which is a new direction in the field of malware detection.",space,367
10.1109/pesgm41954.2020.9281856,filtered,2020 IEEE Power & Energy Society General Meeting (PESGM),IEEE,2020-08-06 00:00:00,ieeexplore,a fault classification method for medium voltage networks with a high penetration of photovoltaic systems using artificial neural networks,https://ieeexplore.ieee.org/document/9281856/,"With the rapid advancement of power electronic technologies and the reduction of photovoltaic cell price, the share of solar energy in the total power production has been booming recently. On the one hand, the increase in the amount of power delivered by solar energy can be beneficial in many economic and environmental aspects. On the other hand, this can cause various technical challenges to network operators. One of these issues is related to classifying faults located in distribution networks with high penetration of photovoltaic systems. Although many studies have paid significant attention to developing new algorithms applicable for a more active today distribution networks, there is still space for other improvements. Hence, after reviewing state-of-the-art researches, this paper was intended to develop a fault classification that is based on artificial neural networks. In particular, a technique so-called Multiplayer Perceptron Classifier was selected for the proposed algorithm. First, the authors generated a data set for the study by modeling and simulating a real distribution network with practical parameters provided by a local utility in the environment software PowerFactory/DigSILENT. Multiple fault scenarios were simulated. Second, a part of the generated data collection was used for network learning. Finally, the performance of the proposed methodology was demonstrated via testing on the remaining number of generated data.",space,368
10.1109/time.2014.27,filtered,2014 21st International Symposium on Temporal Representation and Reasoning,IEEE,2014-09-10 00:00:00,ieeexplore,a formal account of planning with flexible timelines,https://ieeexplore.ieee.org/document/6940372/,"Planning for real world problems with explicit temporal constraints is a challenging problem. Among several approaches, the use of flexible timelines in Planning and Scheduling (P&amp;S) has demonstrated to be successful in a number of concrete applications, such as, for instance, autonomous space systems. A flexible timeline describes an envelope of possible solutions which can be exploited by an executive system for robust on-line execution. A remarkable research effort has been dedicated to design, build and deploy software environments, like EUROPA, ASPEN, and APSI-TRF, for the synthesis of timeline-based P&amp;S applications. Several attempts have also been made to characterize the concept of timelines. Nevertheless, a formal characterization of flexible timelines and plans is still missing. This paper presents a formal account of flexible timelines aiming at providing a general semantics for related planning concepts such as domains, goals, problems, constraints and flexible plans. Some basic properties of the defined concepts are also stated and proved. A simple running example inspired by a real world planning domain is exploited to illustrate the proposed formal notions. Finally, a planning tool, called Extensible Planning and Scheduling Library (EPSL), is briefly presented, which is able to generate flexible plans that are compliant with the given semantics.",space,369
10.1109/i2ct51068.2021.9417940,filtered,2021 6th International Conference for Convergence in Technology (I2CT),IEEE,2021-04-04 00:00:00,ieeexplore,a lightweight classifier for facial expression recognition based on evolutionary svm ensembles,https://ieeexplore.ieee.org/document/9417940/,"Evaluation criteria for solutions to facial expression recognition usually bias to classification accuracy. Hence, the utilization of deep neural networks has become a straightforward and popular option in theoretical studies despite the limitations in real usage from data collection, storage space, and power consumption issues. Our work proposes a practical alternative that is consisted of a minimum model configuration and still matches the state-of-the-art performance of deep learning approaches. We establish a conventional two-stage procedure, where feature extraction of a facial subject depends on a universal filter, histogram of oriented gradients (HOG), and classification is implemented through an ensemble learning approach using basic binary classifiers, support vector machines (SVM). Our two designs considerably improve prediction accuracy. One is that we adopt post-hoc statistics, rather than a priori expectations, to interpret the outputs of weak classifiers. The other is we design a genetic algorithm to search for the optimal ensemble of weak classifiers efficiently. Our method demonstrates supreme performance in several benchmark datasets and even outperforms those based on deep learning from big data. Besides, from a practical viewpoint, our model shows the advantage and flexibility of its storage size and power consumption. Lastly, we further display how the evolutionary SVM ensembles in our model contain information about the dependency and similarity among facial expression categories.",space,370
10.1109/chase.2016.40,filtered,"2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)",IEEE,2016-06-29 00:00:00,ieeexplore,a mobile cloud computing model using the cloudlet scheme for big data applications,https://ieeexplore.ieee.org/document/7545816/,"The wide spread of smart phones and their capabilities made them an important part of many people's life over the world. However, there are many challenges facing these devices such as: low computing power and fast energy drain from their batteries. One solution is to use mobile cloud computing services to run certain tasks at the cloud and returning back the results to the mobile device saving space and processing power. In this research, we introduce efficient Mobile Cloud Computing model based on the Cloudlet sheme. In our model, the mobile device don't need to communicate with the enterprise cloud server and instead contact the Cloudlet directly using cheaper technologies such as Wi-Fi, and no need for 3G/4G. Also, we propose a master-cloudlet management scheme to organize the communication between the cloudlets themselves. Our efficient mobile cloud computing model can be applied in many environments including universities and hospitals were big amounts of data is collected, stored and processed. The real implementation results show that our model out performs classical non-cloudlet mobile cloud computing models.",space,371
10.1109/icra40945.2020.9196677,filtered,2020 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2020-08-31 00:00:00,ieeexplore,a mobile manipulation system for one-shot teaching of complex tasks in homes,https://ieeexplore.ieee.org/document/9196677/,"We describe a mobile manipulation hardware and software system capable of autonomously performing complex human-level tasks in real homes, after being taught the task with a single demonstration from a person in virtual reality. This is enabled by a highly capable mobile manipulation robot, whole-body task space hybrid position/force control, teaching of parameterized primitives linked to a robust learned dense visual embeddings representation of the scene, and a task graph of the taught behaviors. We demonstrate the robustness of the approach by presenting results for performing a variety of tasks, under different environmental conditions, in multiple real homes. Our approach achieves 85% overall success rate on three tasks that consist of an average of 45 behaviors each. The video is available at: https://youtu.be/HSyAGMGikLk.",space,372
10.1109/cvprw53098.2021.00227,filtered,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),IEEE,2021-06-25 00:00:00,ieeexplore,a monocular pose estimation case study: the hayabusa2 minerva-ii2 deployment,https://ieeexplore.ieee.org/document/9523026/,"In an environment of increasing orbital debris and remote operation, visual data acquisition methods are becoming a core competency of the next generation of spacecraft. However, deep space missions often generate limited data and noisy images, necessitating complex data analysis methods. Here, a state-of-the-art convolutional neural network (CNN) pose estimation pipeline is applied to the Hayabusa2 Minerva-II2 rover deployment; a challenging case with noisy images and a symmetric target. To enable training of this CNN, a custom dataset is created. The deployment velocity is estimated as 0.1908 m/s using a projective geometry approach and 0.1934 m/s using a CNN landmark detector approach, as compared to the official JAXA estimation of 0.1924 m/s (relative to the spacecraft). Additionally, the attitude estimation results from the real deployment images are shared and the associated tumble estimation is discussed.",space,373
10.1109/csicc52343.2021.9420614,filtered,"2021 26th International Computer Conference, Computer Society of Iran (CSICC)",IEEE,2021-03-04 00:00:00,ieeexplore,a new approach for mapping of soccer robot agents position to real filed based on multi-core fuzzy clustering,https://ieeexplore.ieee.org/document/9420614/,"Mapping the position of soccer robot agents to a real field, is one of the essential issues in the practical implementation of scientific contributions in this context. The lack of a proper assignment affects the scientific implementation of many subjects, such as routing, obstacle avoidance, and robot guidance. For this reason, the use of a clustering method is proposed in this article. Upon the entrance of a new agent, its position is mapped to the real field based on the clustering algorithm. After this mapping, the system begins to work according to the position of the agents, which is defined as the position of the centers of the clusters, as well as the rules defined in the knowledge-base. Considering the unknown and dynamic environment of the robot, some objects inherit common traits from multiple clusters. One reasonable solution for considering the cluster overlaps is to assign a set of membership degrees to each of them. Multiple membership degree assignments result from the fuzzy nature of the clusters. Due to the reduction of segmentations and the shrinkage of the search space, fuzzy clustering generally faces less computational overhead, while the identification and handling of vague, noisy, and outlier data also become much easier in them. The approach of the proposed method is based on the feasibility ideas and uses multi-core learning to identify clusters with complex data structures. The feasibility score of each data represents the percentages of the properties that data inherits from the clusters. Automatically adjusting the weights of the cores in an optimization framework, the proposed method avoids the damage caused by problems such as adopting inefficient cores, or irrelevant features.",space,374
10.1109/iwisa.2009.5072659,filtered,2009 International Workshop on Intelligent Systems and Applications,IEEE,2009-05-24 00:00:00,ieeexplore,a novel feature selection approach based on swarm intelligence,https://ieeexplore.ieee.org/document/5072659/,"The computational complexity of a texture classification algorithm is limited by the dimensionality of the feature space. A feature selection algorithm that can reduce the dimensionality of problem is often desirable, which has been studied by many authors because of its impact on the complexity of classifiers, Furthermore, feature selection in high dimension space is a NP hard problem. This paper presents a novel approach to solve feature subset selection based on improved ant colony optimization algorithm which hybrids heuristics information. The proposed approach has been implemented and tested on a real image texture classification problem. The results of proposed method are encouraging and outperform that of the presented ant colony optimization algorithm without heuristic information in this domain.",space,375
10.1109/dsaa.2016.42,filtered,2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA),IEEE,2016-10-19 00:00:00,ieeexplore,a parallel framework for grid-based bottom-up subspace clustering,https://ieeexplore.ieee.org/document/7796919/,"Clustering is a popular data mining and machine learning technique which discovers interesting patterns from unlabeled data by grouping similar objects together. Clustering high-dimensional data is a challenging task as points in high dimensional space are nearly equidistant from each other, rendering commonly used similarity measures ineffective. Subspace clustering has emerged as a possible solution to the problem of clustering high-dimensional data. In subspace clustering, we try to find clusters in different subspaces within a dataset. Many subspace clustering algorithms have been proposed in the last two decades to find clusters in multiple overlapping subspaces of high-dimensional data. Subspace clustering algorithms iteratively find the best subset of dimensions for a cluster from 2d-1 possible combinations in d-dimensional data. Subspace clustering is extremely compute intensive because of exhaustive search of subspaces, especially in the bottom-up subspace clustering algorithms. To address this issue, an efficient parallel framework for grid-based bottom-up subspace clustering algorithms is developed, considering popular algorithms belonging to this category. The framework is implemented for shared memory, distributed memory, and hybrid systems and is tested for three grid-based bottom-up subspace clustering algorithms: CLIQUE, MAFIA, and ENCLUS. All parallel implementations exhibit impressive speedup and scalability on real datasets.",space,376
10.1109/aero50100.2021.9438232,filtered,2021 IEEE Aerospace Conference (50100),IEEE,2021-03-13 00:00:00,ieeexplore,a pipeline for vision-based on-orbit proximity operations using deep learning and synthetic imagery,https://ieeexplore.ieee.org/document/9438232/,"Deep learning has become the gold standard for image processing over the past decade. Simultaneously, we have seen growing interest in orbital activities such as satellite servicing and debris removal that depend on proximity operations between spacecraft. However, two key challenges currently pose a major barrier to the use of deep learning for vision-based on-orbit proximity operations. Firstly, efficient implementation of these techniques relies on an effective system for model development that streamlines data curation, training, and evaluation. Secondly, a scarcity of labeled training data (images of a target spacecraft) hinders creation of robust deep learning models. This paper presents an open-source deep learning pipeline, developed specifically for on-orbit visual navigation applications, that addresses these challenges. The core of our work consists of two custom software tools built on top of a cloud architecture that interconnects all stages of the model development process. The first tool leverages Blender, an open-source 3D graphics toolset, to generate labeled synthetic training data with configurable model poses (positions and orientations), lighting conditions, backgrounds, and commonly observed in-space image aberrations. The second tool is a plugin-based framework for effective dataset curation and model training; it provides common functionality like metadata generation and remote storage access to all projects while giving complete independence to project-specific code. Time-consuming, graphics-intensive processes such as synthetic image generation and model training run on cloud-based computational resources which scale to any scope and budget and allow development of even the largest datasets and models from any machine. The presented system has been used in the Texas Spacecraft Laboratory with marked benefits in development speed and quality. Remote development, scalable compute, and automatic organization of data and artifacts have dramatically decreased iteration time while increasing reproducibility and system comprehension. Diverse, high-fidelity synthetic images that more closely replicate the real environment have improved model performance against real-world data. These results demonstrate that the presented pipeline offers tangible benefits to the application of deep learning for vision-based on-orbit proximity operations.",space,377
10.1109/ijcnn.1989.118711,filtered,International 1989 Joint Conference on Neural Networks,IEEE,1989-01-01 00:00:00,ieeexplore,a constraint satisfaction network for matching 3d objects,https://ieeexplore.ieee.org/document/118711/,"A new approach is presented for matching visible surfaces of 3D objects using a constraint satisfaction network. This in turn provides the necessary basis for volumetric reconstruction from multiple views. By matching, the authors mean both to establish correspondence between individual faces and to compute 3D transform that would bring one in correspondence with the other. Toward this goal, constraints at three different levels of complexities are specified to produce stable and coherent assignments. The constraint satisfaction is implemented as a Hopfield network with an appropriate energy functional and minimized using simulated annealing. The system extracts objects faces by computing their bounding contours with adaptive scale space filtering. This process identifies important surface features such as jumps or occluding boundaries and creases. The pointwise feature descriptors are then linked, and an attributed graph is derived to represent the object. The nodes in the graph represent geometric surface features, and the links in the graph represent the relationship between adjacent surfaces. The authors present results on real images.&lt;<ETX>&gt;</ETX>",space,378
10.1109/iccic.2013.6724210,filtered,2013 IEEE International Conference on Computational Intelligence and Computing Research,IEEE,2013-12-28 00:00:00,ieeexplore,a framework for text summarization in mobile web browsers,https://ieeexplore.ieee.org/document/6724210/,"In mobile devices, limited space and time for browsing contents of web pages is an important issue. Summarization solutions running on web browsers can help the user to view and access summarized content of web pages in real time. In this paper we propose a framework for summarizing text viewed through mobile web browsers. The framework consists of an application that runs as part the mobile browser and extracts raw text. The text is then preprocessed and input to the summarization algorithm. The algorithm can be implemented either on the device as part of the browser engine or on a remote cloud service. The cloud service uses appropriate techniques to summarize the text and returns the summarized content to the device, where it is then displayed. During summarization, an extraction algorithm is used that uses ontology and a weighing mechanism to determine suitability score for each sentence. The sentences with the highest suitability scores are selected for inclusion in the summary as per the compression required. We describe a number of user interface elements in the mobile web browser to trigger the summarization and display the results, and compare the performance of publicly available summarization services.",space,379
10.1109/ijcnn.2000.857872,filtered,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,IEEE,2000-07-27 00:00:00,ieeexplore,a hippocampal model of visually guided navigation as implemented by a mobile agent,https://ieeexplore.ieee.org/document/857872/,"Visually guided landmark navigation is based on space coding by hippocampal place cells (PC). A biologically realistic architecture of cooperative-competitive associative networks (implemented as a control system for mobile agents) emulates PC activity during local navigation in exploration and goal-retrieval. The system builds and stores panoramic views from landmarks and compares them with current inputs. Mismatch-induced low levels of recognition trigger a vigilance burst, which favors either the recognition of an alternative place category or the creation of a new category. The sole implementation of visual ""What"" and ""Where"" information does not restrain the generality of the model since several modalities could cooperate to give rise to more robust place field spatial categories. Providing the system with real visual inputs automatically extracted from a natural environment demonstrates that interspecies differences in PC coding result more from characteristics of the visual systems than from differences in processing. Conversely, differences in PC multiple codes within a system result from different levels of processing and/or different degrees of multimodality. Each code could be used within different navigational strategies. A control system derived from the model allows a mobile agent to learn a few places and the associated actions required to reach a goal. Generalisation property of the model provides the capacity to join the goal from any place in the learned environment.",space,380
10.1109/cnna.2010.5430245,filtered,2010 12th International Workshop on Cellular Nanoscale Networks and their Applications (CNNA 2010),IEEE,2010-02-05 00:00:00,ieeexplore,a multi-fpga distributed embedded system for the emulation of multi-layer cnns in real time video applications,https://ieeexplore.ieee.org/document/5430245/,"This paper describes the design and the implementation of an embedded system based on multiple FPGAs that can be used to process real time video streams in standalone mode for applications that require the use of large Multi-Layer CNNs (ML-CNNs). The system processes video in progressive mode and provides a standard VGA output format. The main features of the system are determined by using a distributed computing architecture, based on Independent Hardware Modules (IHM), which facilitate system expansion and adaptation to new applications. Each IHM is composed by an FPGA board that can hold one or more CNN layers. The total computing capacity of the system is determined by the number of IHM used and the amount of resources available in the FPGAs. Our architecture supports traditional cloned templates, but also the (simultaneous) use of time-variant and space-variant templates.",space,381
10.1109/ijcnn.2002.1007503,filtered,Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290),IEEE,2002-05-17 00:00:00,ieeexplore,a neural network based automatic road signs recognizer,https://ieeexplore.ieee.org/document/1007503/,"Automatic road sign recognition systems are aimed at detection and recognition of one or more road signs from real-world color images. In this research, road signs are detected and extracted from real world scenes on the basis of their color and shape features. A dynamic region growing technique is adopted to enhance color segmentation results obtained in the HSV color space. The technique is based on a dynamic threshold that reduces the effect of hue instability in real scenes due to external brightness variation. Classification is then performed on extracted candidate regions using multilayer perceptron neural networks. The obtained results show good detection and recognition rates of the entire system with real outdoor scenes, using several light conditions. Finally, the implementation of the neural layer on the Georgia Institute of Technology SIMD Pixel Processor is outlined.",space,382
10.1109/icdim.2010.5664217,filtered,2010 Fifth International Conference on Digital Information Management (ICDIM),IEEE,2010-07-08 00:00:00,ieeexplore,a new combined ksvm and kfd model for classification and recognition,https://ieeexplore.ieee.org/document/5664217/,"The Kernel Support Vector Machine (KSVM) is a powerful nonlinear classification methodology where, the Support Vectors (SVs) fully describe the decision surface by incorporating local information in the Kernel space. On the other hand, the Kernel Fisher Discriminant(KFD) is a non-linear classifier which has proven to be powerful and competitive to several state-of-the-art classifiers. This paper proposes a novel KSVM + KFD model which combines these two methods. This model can be viewed as an extension to the KSVM by incorporating 'global' characteristics of the data to estimate the decision boundary in the Kernel space. On the other hand, this new model could also be considered as an improvement to the KFD by incorporating the Support Vectors (local margin concept) into the KFD formulation. The KSVM + KFD model can be reduced to the classical KSVM model so that existing KSVM software can be used for easy implementation. An extensive comparison of the KSVM + KFD to the KFD, KSVM, Linear Discriminant Analysis (LDA), Linear Support Vector Machine (LSVM) and the combined LSVM and LDA, performed on real data sets, has shown the advantages of our proposed model. In particular, the experiments on face recognition have clearly shown the superiority of the KSVM + KFD over other methods.",space,383
10.1109/iconip.2002.1201950,filtered,"Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.",IEEE,2002-11-22 00:00:00,ieeexplore,a new kernel clustering algorithm,https://ieeexplore.ieee.org/document/1201950/,We propose a new kernel clustering algorithm. It estimates an in advance fixed number of vectors and margins in a feature space. Each pair of vector and margin defines a hyperplane in feature space and thus separates the data in two clusters. All the clusters together carry important information about the data set. The estimation in feature space is done implicitly by the use of a kernel. Therefore nonlinear clusters in the space of the data can be obtained. The clusters are estimated by optimizing a homogeneous quadratic program. We show how our algorithm can be efficiently implemented and we demonstrate the usefulness with a real world example.,space,384
10.1109/icsmc.2003.1244605,filtered,"SMC'03 Conference Proceedings. 2003 IEEE International Conference on Systems, Man and Cybernetics. Conference Theme - System Security and Assurance (Cat. No.03CH37483)",IEEE,2003-10-08 00:00:00,ieeexplore,a novel approach for person authentication and content-based tracking in videos using kernel methods and active appearance models,https://ieeexplore.ieee.org/document/1244605/,"A novel integration of methods for person authentication and tracking is proposed for real time security systems. The implementation of the idea for this real time implementation follows a three step procedure-face detection, recognition and content-based tracking. Instead of analyzing continuous videos we sample the frame based on a method derived from Shannon's information theory model. The Face-detector detects multi-viewed faces in a video using feature-based kernel methods in a reduced feature space obtained using ICA. The identified ""face regions"" are then passed on to the face recognition system which is based on Active Appearance Models (AAM). Once the subject is recognized, it can be tracked in the video using kernel based object tracking method. Several space reduction techniques have been used like ICA, PCA and skin-color segmentation.",space,385
10.1109/plasma.2016.7534202,filtered,2016 IEEE International Conference on Plasma Science (ICOPS),IEEE,2016-06-23 00:00:00,ieeexplore,a parallel electrostatic solver for xoopic code,https://ieeexplore.ieee.org/document/7534202/,"Summary form only given. We report on the implementation of a parallel electrostatic field solver in the plasma particle simulation code XOOPIC1. Poisson's equation is solved by finite differences in real space, using iterative methods to solve a large sparse system of linear equations. Domain decomposition is extended to two dimensions, including particle passing and electrostatic field solution. We utilize Trilinos solvers for our implementation. In particular AZTEC, a set of iterative solvers, preconditioners and matrix-vector multiplication routines, and ML, a multigrid preconditioning package, from Sandia National Lab are used2. These libraries are optimized for a range of highly parallel platforms and are expected to be highly efficient and scalable to more than 1000 processors. Accuracy of the solver is compared to analytic results, and performance scaling is measured.",space,386
10.1109/empdp.2001.905053,filtered,Proceedings Ninth Euromicro Workshop on Parallel and Distributed Processing,IEEE,2001-02-09 00:00:00,ieeexplore,a parallel neurochip for neural networks implementing the reactive tabu search algorithm: application case studies,https://ieeexplore.ieee.org/document/905053/,"In this work we present two different applications implemented on the neurocomputer Totem Nc3001 from Neuricam Inc. The goal of the experimentation is to test, on real problems, the performance of this powerful parallel unit consisting of 32 Digital Signal Processors (DSPs) and to evaluate its suitability to neural network applications. The first problem implemented is a typical classification algorithm in which the network recognises which points belong to different regions inside a 2D space. The second problem is more computationally heavy and consists of a network able to reproduce the eye movements, if properly stimulated. A comparison is reported between Matlab implementations or handwritten code run on workstations and the performance obtained from the Totem chip.",space,387
10.1109/empdp.1999.746650,filtered,Proceedings of the Seventh Euromicro Workshop on Parallel and Distributed Processing. PDP'99,IEEE,1999-02-05 00:00:00,ieeexplore,a parallel processor for neural networks,https://ieeexplore.ieee.org/document/746650/,"We present two different algorithms implemented through neural networks on a multiprocessor device. The parallel single-chip TI TMS32C80 Multimedia Video Processor (MVP). The goal of this experimentation is to test, on real problems, the performance of this powerful unit made up by one Master Risc Processor and by four Slave Digital Signal Processors (DSPs) and to evaluate its suitability to neural network applications. The first problem implemented is a typical classification algorithm in which the network recognises which points belong to different regions inside a 2D space. The second problem is more computationally heavy and consists of a network able to recognise 'handwritten' digits. The parallel version of the first algorithm, was also tested on a commercially available supercomputer.",space,388
10.1109/ccaaw.2017.8001611,filtered,2017 Cognitive Communications for Aerospace Applications Workshop (CCAA),IEEE,2017-06-28 00:00:00,ieeexplore,a pattern matching approach to map cognitive domain ontologies to the ibm truenorth neurosynaptic system,https://ieeexplore.ieee.org/document/8001611/,"Cognitive agents are typically utilized in autonomous systems for automated decision making. These systems interact in real time with their environments and are generally heavily power constrained. Thus there is a strong need for a real time agent running on a low power computing platform for these systems. This paper examines how some of the components of a cognitive agent can be mapped onto the IBM TrueNorth neurosynaptic system to achieve real time performance at low power. The agent examined is the Cognitively Enhanced Complex Event Processing (CECEP) architecture. Although it is geared towards autonomous decision making, CECEP also has applications in operations research, intelligence analysis, and data mining. One of the key components of CECEP is the Cognitive Domain Ontology (CDO), used for mining decisions from a large knowledge repository based on situational inputs and constraints. As CDOs are the most time and power consuming part of CECEP, we implemented them on the TrueNorth processor by mapping the solution space of CDOs into a pattern matching form.",space,389
10.1109/iros.1991.174683,filtered,Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91,IEEE,1991-11-05 00:00:00,ieeexplore,a realtime trajectory planning method for manipulators based on kinetic energy,https://ieeexplore.ieee.org/document/174683/,"Most conventional trajectory planners are based on the trapezoidal pattern of the end-effector velocity on the desired path in a task space. This method can be implemented in real time. However, the motion planned by this method usually leads to a very slow motion, because it is difficult to take into consideration a constraint on the magnitude of joint velocities or joint torques. The authors propose a new trajectory planner for three DOF manipulators based on the kinetic energy of manipulators. The supplied electric power for the manipulators mainly depends on the differentiation of the kinetic energy. Therefore, the proposed method can take into consideration the constraints on the magnitude of the supplied electric power and joint velocities, i.e. one can realize a 'high speed motion' under the constraints on the supplied electric power and joint velocities. Also, the method proposed does not require much calculation and can be implemented in real time.&lt;<ETX>&gt;</ETX>",space,390
10.1109/icpp.2000.876164,filtered,Proceedings 2000 International Conference on Parallel Processing,IEEE,2000-08-24 00:00:00,ieeexplore,a scalable parallel subspace clustering algorithm for massive data sets,https://ieeexplore.ieee.org/document/876164/,"Clustering is a data mining problem which finds dense regions in a sparse multi-dimensional data set. The attribute values and ranges of these regions characterize the clusters. Clustering algorithms need to scale with the data base size and also with the large dimensionality of the data set. Further, these algorithms need to explore the embedded clusters in a subspace of a high dimensional space. However the time complexity of the algorithm to explore clusters in subspaces is exponential in the dimensionality of the data and is thus extremely compute intensive. Thus, parallelization is the choice for discovering clusters for large data sets. In this paper we present a scalable parallel subspace clustering algorithm which has both data and task parallelism embedded in it. We also formulate the technique of adaptive grids and present a truly unsupervised clustering algorithm requiring no user inputs. Our implementation shows near linear speedups with negligible communication overheads. The use of adaptive grids results in two orders of magnitude improvement in the computation time of our serial algorithm over current methods with much better quality of clustering. Performance results on both real and synthetic data sets with very large number of dimensions on a 16 node IBM SP2 demonstrate our algorithm to be a practical and scalable clustering technique.",space,391
10.1109/fpga.1993.279462,filtered,[1993] Proceedings IEEE Workshop on FPGAs for Custom Computing Machines,IEEE,1993-04-07 00:00:00,ieeexplore,a stochastic neural architecture that exploits dynamically reconfigurable fpgas,https://ieeexplore.ieee.org/document/279462/,"The authors present an expandable digital architecture that provides an efficient real time implementation platform for large neural networks. The architecture makes heavy use of the techniques of bit serial stochastic computing to carry out the large number of required parallel synaptic calculations. In this design all real valued quantities are encoded on to stochastic bit streams in which the '1' density is proportional to the given quantity. The actual digital circuitry is simple and highly regular thus allowing very efficient space usage of fine grained FPGAs. Another feature of the design is that the large number of weights required by a neural network are generated by circuitry tailored to each of their specific values, thus saving valuable cells. Whenever one of these values is required to change, the appropriate circuitry must be dynamically reconfigured. This may always be achieved in a fixed and minimum number of cells for a given bit stream resolution.&lt;<ETX>&gt;</ETX>",space,392
10.1109/iconip.1999.843962,filtered,ICONIP'99. ANZIIS'99 & ANNES'99 & ACNN'99. 6th International Conference on Neural Information Processing. Proceedings (Cat. No.99EX378),IEEE,1999-11-20 00:00:00,ieeexplore,a visualised software library: a software self-organising map,https://ieeexplore.ieee.org/document/843962/,"This paper presents an approach to self-structuring software libraries. The authors developed a representation scheme to construct a feature space over a collection of software assets. The feature space is self-organised by an unsupervised neural network. The self-organising map (SOM). The high-dimensional feature space is then protected onto the two-dimensional SOM output layer that makes the most important distance relationships among the software assets geometrically explicit. This approach has been applied to a real case, where a visualised library containing a set of UNIX commands is constructed. The results obtained from a retrieval experiment based on the library demonstrated great potential.",space,393
10.1109/codit.2018.8394934,filtered,"2018 5th International Conference on Control, Decision and Information Technologies (CoDIT)",IEEE,2018-04-13 00:00:00,ieeexplore,adaptive dynamic programming based motion control of autonomous underwater vehicles,https://ieeexplore.ieee.org/document/8394934/,"In this paper, Adaptive Dynamic Programming (ADP) technique is utilized to achieve optimal motion control of Autonomous Underwater Vehicle (AUV) System. The paper proposes a model-free based method that takes into consideration the actuator input and obstacle position while tracing an optimal path. The concept of machine learning enables to develop a path-planner which aims to avoid collisions with static obstacles. The ADP approach is realized to approximate the solution of the cost functional for optimization purpose by which the positions of the locally situated obstacles need not be priori-known until they are within a designed approximation safety envelope. The methodology is implemented to achieve the path-planning objective using dynamic programming technique. The Least-squares policy method serves as a recursive algorithm to approximate the value function for the domain, providing an approach for the finite space discrete control system. The concept behind the design of an obstacle-free path finder is to generate an optimal action that minimizes the local cost, defined by a functional, under constrained optimization. The most advantageous value function is described by the Hamilton Jacobi Bellman (HJB) equation, that is impractical to solve using analytical methods. To overcome the complex calculations subject to HJB, a method based on Reinforcement Learning (RL), called ADP is implemented. This paper outlines the concept of machine learning to realize a real time obstacle avoidance system.",space,394
10.1109/micai.2007.26,filtered,"2007 Sixth Mexican International Conference on Artificial Intelligence, Special Session (MICAI)",IEEE,2007-11-10 00:00:00,ieeexplore,adaptive hierarchical fuzzy cmac controller with stable learning algorithm for unknown nonlinear systems,https://ieeexplore.ieee.org/document/4659319/,"In this paper, adaptive hierarchical fuzzy CMAC neural network controller (HFCMAC), for a certain class of nonlinear dynamical system is presented. The main advantages of adaptive HFCMAC control are: Better performance of the controller because adaptive HFCMAC can adjust itself to the changing enviroment and can be implemented in real time applications. The proposed method provides a simple control architecture that merges hierarchical structure, CMAC neural network and fuzzy logic. The input space dimension in CMAC is a time-consuming task especially when the number of inputs is huge this would be overload the memory and make the neuro-fuzzy system very hard to implement. This is can be simplified using a number of low-dimensional fuzzy CMAC in a hierarchical form. A new adaptation law is obtained for the method proposed, the overall adaptive scheme guarantees the global stability of the resulting closed-loop system in the sense that all signals involved are uniformly bounded. Simulation results for its applications to one example is presented to demonstrate the performance of the proposed methodology.",space,395
10.1109/ipdpsw52791.2021.00012,filtered,2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),IEEE,2021-06-21 00:00:00,ieeexplore,adaptive stochastic gradient descent for deep learning on heterogeneous cpu+gpu architectures,https://ieeexplore.ieee.org/document/9460628/,"The widely-adopted practice is to train deep learning models with specialized hardware accelerators, e.g., GPUs or TPUs, due to their superior performance on linear algebra operations. However, this strategy does not employ effectively the extensive CPU and memory resources - which are used only for preprocessing, data transfer, and scheduling - available by default on the accelerated servers. In this paper, we study training algorithms for deep learning on heterogeneous CPU+GPU architectures. Our two-fold objective - maximize convergence rate and resource utilization simultaneously - makes the problem challenging. In order to allow for a principled exploration of the design space, we first introduce a generic deep learning framework that exploits the difference in computational power and memory hierarchy between CPU and GPU through asynchronous message passing. Based on insights gained through experimentation with the framework, we design two heterogeneous asynchronous stochastic gradient descent (SGD) algorithms. The first algorithm - CPU+GPU Hogbatch - combines small batches on CPU with large batches on GPU in order to maximize the utilization of both resources. However, this generates an unbalanced model update distribution which hinders the statistical convergence. The second algorithm - Adaptive Hogbatch - assigns batches with continuously evolving size based on the relative speed of CPU and GPU. This balances the model updates ratio at the expense of a customizable decrease in utilization. We show that the implementation of these algorithms in the proposed CPU+GPU framework achieves both faster convergence and higher resource utilization than TensorFlow on several real datasets.",space,396
10.1109/icra40945.2020.9196582,filtered,2020 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2020-08-31 00:00:00,ieeexplore,adversarial skill networks: unsupervised robot skill learning from video,https://ieeexplore.ieee.org/document/9196582/,"Key challenges for the deployment of reinforcement learning (RL) agents in the real world are the discovery, representation and reuse of skills in the absence of a reward function. To this end, we propose a novel approach to learn a task-agnostic skill embedding space from unlabeled multi-view videos. Our method learns a general skill embedding independently from the task context by using an adversarial loss. We combine a metric learning loss, which utilizes temporal video coherence to learn a state representation, with an entropy-regularized adversarial skill-transfer loss. The metric learning loss learns a disentangled representation by attracting simultaneous viewpoints of the same observations and repelling visually similar frames from temporal neighbors. The adversarial skill-transfer loss enhances re-usability of learned skill embeddings over multiple task domains. We show that the learned embedding enables training of continuous control policies to solve novel tasks that require the interpolation of previously seen skills. Our extensive evaluation with both simulation and real world data demonstrates the effectiveness of our method in learning transferable skills from unlabeled interaction videos and composing them for new tasks. Code, pretrained models and dataset are available at http://robotskills.cs.uni-freiburg.de.",space,397
10.1109/igarss.2011.6048931,filtered,2011 IEEE International Geoscience and Remote Sensing Symposium,IEEE,2011-07-29 00:00:00,ieeexplore,aggregation of parallel computing and hardware/software co-design techniques for high-performance remote sensing applications,https://ieeexplore.ieee.org/document/6048931/,"Developing computationally efficient processing techniques for massive volumes of hyperspectral data is critical for space-based Earth science and planetary exploration. In particular, many remote sensing imaging applications require a response in real time in areas such as environmental modeling and assessment, target detection for military and homeland defense/security purposes, and risk prevention and response. This paper propose the aggregation of parallel computing and HW/SW co-design techniques using processor arrays (PAs) units as specialized hardware architectures for the real time enhancement of remote sensing imagery. An extended descriptive experiment design regularization (DEDR) method that incorporates projections onto convex solution sets (POCS) for spatial spectrum pattern (SSP) reconstruction is used to be efficiently implemented (i.e., HW-level) via the new proposition of the aggregation techniques. Finally, it is reported and discussed the Xilinx Virtex-5 FPGA implementation and high-performance issues related to real time enhancement of large-scale real-world RS imagery.",space,398
10.1109/cbd.2016.066,filtered,2016 International Conference on Advanced Cloud and Big Data (CBD),IEEE,2016-08-16 00:00:00,ieeexplore,allocation of resources after disaster based on big data from sns and spatial scan,https://ieeexplore.ieee.org/document/7815232/,"After a disaster such as earthquakes, debris flows, forest fires, or landslides, etc., a lot of people have to be away from their home and gather in a shelter. In addition, the refugees suffer from the shortage of necessary resources due to impaired life infrastructure, such as damaged roads and communication networks. The degree of reducing damage depends on the amount of food, water, daily necessities and communication resources required by each shelter. How to effectively and efficiently allocate resources according to grasp the exact need of a disaster situation will be an important issue. We estimate the degree of the disaster by collecting and analyzing big data from the SNS, and building a platform for the communication resources to be efficiently and effectively allocated. In order to achieve this goal, we are challenging the following issues A) Understanding situations (user requirements) after disaster occur The SNS streams large scale semantic information about real time situation in society, especially during and after disaster. It is both domain-specific and computational challenge in processing the heterogeneous large data set to extract the exact situational content with reduced semantic uncertainty. The machine learning (ML) and natural language processing (NPL) tool kits are useful in semantic analysis, but still needs domain-specific implementation and computational improvement for the situation understanding from the SNS big data. B) Understanding distribution patterns of situations/users' requirements The disaster related situation is spatiotemporally correlated, and varies dynamically in space and time. It is also domain-specific and computational challenge in estimating the spatiotemporal distribution patterns of the disaster affect based on the spatial big data from SNS. The scan statistics such as the spatial scan have provided well tested mathematical tools and software for spatial data mining. However, new methodologies are necessary since the assumptions have to be different when it meets the spatial big data in SNS. And the computational complexity in spatial big data is also a bottleneck for real-time processing. C) Solving uncertainty of big crowd data One of the major features in big crowd data, e.g., SNS data, is uncertainty behind the data. Especially in a disaster scenario, the collecting time period cannot be long enough to smooth the data automatically. How to efficiently solve uncertainty problem in the big crowd data in a disaster scenario becomes a new and big challenge for disaster management.",space,399
10.1109/rsp.2018.8631989,filtered,2018 International Symposium on Rapid System Prototyping (RSP),IEEE,2018-10-05 00:00:00,ieeexplore,ambient intelligence for the internet of things through context-awareness,https://ieeexplore.ieee.org/document/8631989/,"With the advent of the Internet of Things, new devices feature advanced capabilities that are used in homes, rooms, and offices for better comfort and to save user's time and energy. A complete smart space control system should automatically adjust settings to the user preferences based on data previously collected from such smart things. This paper describes a system that uses environmental data collected from sensors in smart devices to define contexts and user preferences to accomplish this requirement. Contextual data is fed to a context-aware decision engine, composed by a combination of machine learning and data mining techniques. The context-aware decision engine is capable of identifying important data relationships. This allows the use the environment's contextual information to make intelligent control decisions and adjust the environment's settings to user's preferences and reduce the overall power consumption, yielding better services to the user and improving human-technology interaction. We implemented and evaluated the system through a real case study. Results confirm the system's ability to automatically control a smart room with little overhead and latency, while promoting user comfort and energy savings.",space,400
10.1109/greencom-cpscom.2010.25,filtered,"2010 IEEE/ACM Int'l Conference on Green Computing and Communications & Int'l Conference on Cyber, Physical and Social Computing",IEEE,2010-12-20 00:00:00,ieeexplore,an embedded software power model based on algorithm complexity using back-propagation neural networks,https://ieeexplore.ieee.org/document/5724868/,"Nowadays as low carbon economy is greatly advocated worldwide, the electricity consumption caused by a huge number of embedded computer systems is gaining more and more attention. Different instruction set, software algorithm and high-level software architecture can significantly affect the system energy consumption. In this paper, we first analyze the relations between software power consumption and some software characteristics on algorithm level. Through measuring three algorithm complexity characteristics, i.e., time complexity, space complexity and input scale, we propose an embedded software power model based on algorithm complexity. Then, we design and train a back propagation neural network to fit the power model accurately based on a sample training function set and more than 400 software power data. Simulation results show that the error between the estimation values of this model and the real measured values is below 10 percent, and this model can effectively estimate the power consumption of software in an early stage of software design.",space,401
10.1109/hpcs.2018.00034,filtered,2018 International Conference on High Performance Computing & Simulation (HPCS),IEEE,2018-07-20 00:00:00,ieeexplore,an ensemble-based p2p framework for the detection of deviant business process instances,https://ieeexplore.ieee.org/document/8514339/,"The problem of discriminating ""deviant"" traces (i.e. traces diverging from normal/desired outcomes, such as frauds, faults) in the execution log of a business process can be faced by extracting a classification model for the traces, after mapping them onto some suitable feature space. An ensemble-learning approach was recently proposed that trains multiple base learners on different vector-space views of the given log, and a probabilistic meta-model that combines the predictions of the discovered base classifiers. However, the sequential centralised implementation of this learning approach makes it unsuitable for real applications, where large volumes of traces are produced continuously, while both deviant and normal behaviours tend to change over the time. We here propose an online deviance detection framework that leverages a novel incremental learning scheme, which extracts different base models from different chunks of a trace stream, and dynamically combines them in an ensemble model. Notably, the system is based on a p2p architecture that allows it to distribute the entire learning procedure among multiple nodes and to exploit the power of HPC resources (e.g. cloud computing environments). Preliminary tests on a real-life log confirmed the validity of the approach, in terms of both effectiveness and efficiency.",space,402
10.1109/cec.2019.8790094,filtered,2019 IEEE Congress on Evolutionary Computation (CEC),IEEE,2019-06-13 00:00:00,ieeexplore,an evolutionary machine learning approach towards less conservative robust optimization,https://ieeexplore.ieee.org/document/8790094/,"In the recent era, multi-criteria decision making under uncertainty is gaining importance due to its wide range of applicability. Among several types of uncertainty handling techniques, Robust Optimization (RO) is considered as an efficient and tractable approach provided one has accessibility to data in uncertain regions. However, solutions of RO may actually deviate from actual results in real scenarios, due to conservative sampling. This paper proposes a methodology to amalgamate unsupervised machine learning algorithms with RO which thereby makes it data-driven. A novel evolutionary fuzzy clustering mechanism is implemented to transcript the uncertain space such that the exact regions of uncertainty are identified. Subsequently, density based boundary point detection and Delaunay triangulation based boundary construction enables intelligent Sobol based sampling in these regions for use in RO. Results of two test cases with varying dimensions are presented along with a comprehensive comparison between conventional RO approach using box uncertainty set and proposed methodology. Considered case studies include highly nonlinear real life model for continuous casting from steelmaking industries, where a time expensive multi-objective optimization problem under uncertainty is formulated to resolve the conflict in productivity and energy consumption. Optimal Artificial Neural Network (ANN) surrogate assisted optimization under uncertainty for casting model is performed to obtain solutions in realistic time. The resulting RO problem being multi-objective in nature, the Pareto solutions are obtained by NSGA II.",space,403
10.23919/chicc.2018.8483465,filtered,2018 37th Chinese Control Conference (CCC),IEEE,2018-07-27 00:00:00,ieeexplore,an improved ensemble adaptive kernel pls soft sensor model and its application,https://ieeexplore.ieee.org/document/8483465/,"To avoid the disadvantage of traditional PLS model in dealing with nonlinear data, Kernel PLS (KPLS) algorithm has been proposed. By mapping nonlinear data into high-dimensional space with kernel function, the original data set can be processed using linear models in the new space. However, when facing diverse complicated nonlinear features, the simple kernel method also exhibits some limitations. To tackle this problem, an improved k-means based Ensemble Adaptive Kernel Partial Least Squares (EAKPLS) is proposed. Its whole processes are implemented as follows. In the modeling phase, the data is first divided into k sub datasets by k-means clustering algorithm. Then for each subset, different kernels and corresponding kernel parameters are chosen adaptively by introducing PSO algorithm. In the prediction phase, ensemble learning is introduced to obtain the final predictable value where Bayes' theorem is applied, where an improved weights assignment strategy is also presented. Ultimately, numerical and real industrial test cases are both given to demonstrate its feasibility and effectiveness.",space,404
10.1109/icdmw.2013.135,filtered,2013 IEEE 13th International Conference on Data Mining Workshops,IEEE,2013-12-10 00:00:00,ieeexplore,an online clustering algorithm that ignores outliers: application to hierarchical feature learning from sensory data,https://ieeexplore.ieee.org/document/6753963/,"Surveillance sensors are a major source of unstructured Big Data. Discovering and recognizing spatiotemporal objects (e.g., events) in such data is of paramount importance to the security and safety of facilities and individuals. Hierarchical feature learning is at the crux to the problems of discovery and recognition. We present a multilayered convergent neural architecture for storing repeating spatially and temporally coincident patterns in data at multiple levels of abstraction. The bottom-up weights in each layer are learned to encode a hierarchy of over complete and sparse feature dictionaries from space- and time-varying sensory data by recursive layer-by-layer spherical clustering. This density-based clustering algorithm ignores outliers by the use of a unique adaptive threshold in each neuron's transfer function. The model scales to full-sized high-dimensional input data and also to an arbitrary number of layers, thereby possessing the capability to capture features at any level of abstraction. It is fully-learnable with only two manually tunable parameters. The model was deployed to learn meaningful feature hierarchies from audio, images and videos which can then be used for recognition and reconstruction. Besides being online, operations in each layer of the model can be implemented in parallelized hardware, making it very efficient for real world Big Data applications.",space,405
10.23919/oceans44145.2021.9705666,filtered,OCEANS 2021: San Diego – Porto,IEEE,2021-09-23 00:00:00,ieeexplore,an underwater simulation server oriented to cooperative robotic interventions: the educational approach,https://ieeexplore.ieee.org/document/9705666/,"Experiments that require the use of Supervised Autonomous Underwater Vehicles for Intervention (I-AUV) are not easy to be performed, specially when deployed in the sea or in scenarios where the robot might face lack of space and communication (e.g. interior of pipes). Also, there are some applications where the robots need to cooperate in a closed manner, for example when transporting and assembling big pipes. In fact, these two scenarios are being studied in the context of the H2020-ElPeacetolero and TWINBOT (TWIN roBOTs for cooperative underwater intervention mission) [1] projects, being necessary to have a simulation tool that offer more realistic rendering and being compatible with the real robot Application Programming Interface.This paper presents a new underwater simulation server, implemented using video game and robotic techniques, which operates by enabling the researchers control the robots in the scene in a simple and efficient manner, while using HTTP commands that have demonstrated a huge facility in the project integration process. Moreover, this simplicity has allowed the application of the simulation server in the educational context. The use of this tool has resulted to be very adequate for the students, who have used it to learn computer science and artificial intelligence algorithms to solve problems like a cooperative transportation robotic task. As case study, four educational experiments are presented, performed by master’s degree students, focusing on user interfaces, image compression for underwater channels, autonomous cooperative grasping and robot arm movement in a AUV.",space,406
10.1109/imvip.2007.37,filtered,International Machine Vision and Image Processing Conference (IMVIP 2007),IEEE,2007-09-07 00:00:00,ieeexplore,an unsupervised approach for segmentation and clustering of soccer players,https://ieeexplore.ieee.org/document/4318147/,"In this work we consider the problem of soccer team discrimination. The approach we propose starts from the monocular images acquired by a still camera. The first step is the soccer player detection, performed by means of background subtraction. An algorithm based on pixels energy content has been implemented in order to detect moving objects. The use of energy information, combined with a temporal sliding window procedure, allows to be substantially independent from motion hypothesis. Colour histograms in RGB space are extracted from each player, and provided to the unsupervised classification phase. This is composed by two distinct modules: firstly, a modified version of the BSAS clustering algorithm builds the clusters for each class of objects. Then, at runtime, each player is classified by evaluating its distance, in the features space, from the classes previously detected. Algorithms have been tested on different real soccer matches of the Italian Serie A.",space,407
10.1109/robot.1992.220085,filtered,Proceedings 1992 IEEE International Conference on Robotics and Automation,IEEE,1992-05-14 00:00:00,ieeexplore,an optimal scheduling of pick place operations of a robot-vision-tracking system by using back-propagation and hamming networks,https://ieeexplore.ieee.org/document/220085/,"The authors present a neural network approach to solve the dynamic scheduling problem for pick-place operations of a robot-vision-tracking system. An optimal scheduling problem is formulated to minimize robot processing time without constraint violations. This is a real-time optimization problem which must be repeated for each group of objects. A scheme which uses neural networks to learn the mapping from object pattern space to optimal order space offline and to recall online what has been learned is presented. The idea was implemented in a real system to solve a problem in large commercial dishwashing operations. Experimental results have been shown that with four different objects, time savings of up to 21% are possible over first-come, first-served schemes currently used in industry.&lt;<ETX>&gt;</ETX>",space,408
10.1109/ent47717.2019.9030591,filtered,2019 International Conference on Engineering and Telecommunication (EnT),IEEE,2019-11-21 00:00:00,ieeexplore,analysis of approaches to the universal approximation of a continuous function using kolmogorov’s superposition,https://ieeexplore.ieee.org/document/9030591/,"Kolmogorov and Arnold proved that any real continuous bounded function of many variables can be represented as a superposition of functions of one variable and addition. In subsequent works by Neht-Nielsen, it was shown that such a specific type of superposition can be interpreted as a two-layer forward neural network. Such a superposition can also be used as a universal approximation of the function of many variables. In the work, one of the variants of numerical implementation of Kolmogorov's superposition, proposed by Sprecher and modified by Koppen, was investigated. In addition, the functions of the first layer were considered as a generator of space-filling curves (Peano curves). Numerical experiments were conducted to study the accuracy of the approximation of the function of many variables for the numerical implementation of Kolmogorov's superposition, proposed by Sprecher and modified by Koppen, for a simplified version of this superposition and for an approach using filling curves. A comparative analysis showed that the best results are obtained using space-filling curves.",space,409
10.1109/iciteed.2016.7863311,filtered,2016 8th International Conference on Information Technology and Electrical Engineering (ICITEE),IEEE,2016-10-06 00:00:00,ieeexplore,android based real-time static indonesian sign language recognition system prototype,https://ieeexplore.ieee.org/document/7863311/,"Sign language uses gestures instead of speech sounds to communicate. But in general, normal people rarely trying to learn sign language to interact with the deaf community. Recently, there are many sign language recognition system that had been developed. But most of them were implemented using desktop and laptop computer, which is impractical due to its weight and size. This paper presents a prototype of real time static Indonesian sign language recognition using the Android Smart Phone, so it can be used anywhere and anytime. YCrCb color space combined with skin color detection is used to remove the background image and form a segmented image. Detection contour with convex hull algorithms are used to localize and save an area of the hand. Convexity defect algorithms then are used to extract the hand gesture's features using a radiant line from the center of the palm to the fingertips. The classification of hand gesture that performs sign alphabets is accomplished using back propagation neural network algorithm in order to determine a suitable alphabet. The performance test of the system is done by recognizing some variation hand gesture poses for Indonesian sign language alphabet. The results show that the system can detect the position of the user's hand. Furthermore, the system can recognize the alphabet sign from user hand gesture input, reaching 91.66% success rate in testing using Android devices in real time.",space,410
10.1109/dsc50466.2020.00029,filtered,2020 IEEE Fifth International Conference on Data Science in Cyberspace (DSC),IEEE,2020-07-30 00:00:00,ieeexplore,anobeat: anomaly detection for electrocardiography beat signals,https://ieeexplore.ieee.org/document/9172828/,"Electrocardiography signals are composed of variform heartbeats which could indicate the condition of the heart and reveal the risk of heart attacks. Many existing classification-based works for abnormal beats detection are limited by the class-imbalanced data or labor-intensive manual annotation bias. A promising trend to address the issue is to identify the abnormal data that differs from the normal data by utilizing normal (oneclass) data to learn the manifold and detect the anomaly to the unseen and unlabeled data in an/aunsupervised/semi-supervised manner. In this paper, we propose Anobeat, a semi-supervised approach, to perform the abnormal beat detection by facilitating adversarial regularized autoencoders constrained with multifeature and reconstruction error. In order to obtain a robust and reasonable latent coding, we deploy two discriminators in the latent space and visual space to distinguish real and fake features and minimize the distance between two features to train the visual discriminator in alternate steps. Meanwhile, we minimize the reconstruction error and maximum distance between input and noise features to improve the decoder. The adversarial multi-feature constraints enable the generator to learn the latent representations of the target normal data and reconstruct the beats properly. Experiments showed that Anobeat achieved ROC-AUC of 0.960 and 0.894 in the MIT-BIH intrapatient and inter-patient dataset respectively, which outperforms the most competitive baseline by 1.61% and 0.62% respectively. Anobeat also performs comparative robustness and shows good interpretability in the European ST-T and MIT-BIH Arrhythmia Database.",space,411
10.1109/humanoids.2012.6651500,filtered,2012 12th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2012),IEEE,2012-12-01 00:00:00,ieeexplore,applying statistical generalization to determine search direction for reinforcement learning of movement primitives,https://ieeexplore.ieee.org/document/6651500/,"In this paper we present a new methodology for robot learning that combines ideas from statistical generalization and reinforcement learning. First we apply statistical generalization to compute an approximation for the optimal control policy as defined by training movements that solve the given task in a number of specific situations. This way we obtain a manifold of movements, which dimensionality is usually much smaller than the dimensionality of a full space of movement primitives. Next we refine the policy by means of reinforcement learning on the approximating manifold, which results in a learning problem constrained to the low dimensional manifold. We show that in some situations, learning on the low dimensional manifold can be implemented as an error learning algorithm. We apply golden section search to refine the control policy. Furthermore, we propose a reinforcement learning algorithm with an extended parameter set, which combines learning in constrained domain with learning in full space of parametric movement primitives, which makes it possible to explore actions outside of the initial approximating manifold. The proposed approach was tested for learning of pouring action both in simulation and on a real robot.",space,412
10.1109/icuems50872.2020.00074,filtered,2020 International Conference on Urban Engineering and Management Science (ICUEMS),IEEE,2020-04-26 00:00:00,ieeexplore,apriori-based spatial pattern mining algorithm for big data,https://ieeexplore.ieee.org/document/9151537/,"With the establishment of recent information system networks, maritime vessel trajectory data are becoming increasingly available. Spatial pattern mining in these data can measure the behaviour of ships in space and time and can be used in traffic monitoring or other security purpose. The difficulty of this topic is that it needs to deal with both highdimensional data and huge amount of data.In this paper, a cooccurrence pattern mining algorithm based on Mapreduce architecture is presented. The algorithm is focused on the mining on spatial co-occurrence pattern, and implemented in the parallel partitioning architecture. The experiments on real data sets show that the large-scale ship trajectory data can be processed effectively.",space,413
10.1109/icmla.2016.0118,filtered,2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA),IEEE,2016-12-20 00:00:00,ieeexplore,area-specific crime prediction models,https://ieeexplore.ieee.org/document/7838222/,"The convergence of public data and statistical modeling has created opportunities for public safety officials to prioritize the deployment of scarce resources on the basis of predicted crime patterns. Current crime prediction methods are trained using observed crime and information describing various criminogenic factors. Researchers have favored global models (e.g., of entire cities) due to a lack of observations at finer resolutions (e.g., ZIP codes). These global models and their assumptions are at odds with evidence that the relationship between crime and criminogenic factors is not homogeneous across space. In response to this gap, we present area-specific crime prediction models based on hierarchical and multi-task statistical learning. Our models mitigate sparseness by sharing information across ZIP codes, yet they retain the advantages of localized models in addressing non-homogeneous crime patterns. Out-of-sample testing on real crime data indicates predictive advantages over multiple state-of-the-art global models.",space,414
10.1109/dicta.2012.6411741,filtered,2012 International Conference on Digital Image Computing Techniques and Applications (DICTA),IEEE,2012-12-05 00:00:00,ieeexplore,automated detection of root crowns using gaussian mixture model and bayes classification,https://ieeexplore.ieee.org/document/6411741/,"In this paper a method for automatic detection of root crowns in root images, are designed, implemented and quantitatively compared. The approach is based on the theory of statistical learning. The root images are preprocessed with algorithms for intensity normalization, segmentation, edge detection and scale space corner detection. The features used in the experiments are the Zernike moments of the bi-level image patch centered around high curvature detections. Zernike moments are orthogonal and thus can be rightly assumed to be independent. The densities of the feature vectors for different classes are modelled with Gaussian mixture model (GMM), with a diagonal covariance matrix. The parameters for the feature's distribution densities for different classes are learnt by expectation maximization. Bayes rule and Neymann-Pearson criteria is used to design the classification method. We experiment with different orders of Zernike moments and different number of Gaussians in the GMM. The experiments are done on a real dataset with images of rice, corn, and grass roots. Pattern classification results are quantitatively analyzed using Receiver Operating Characteristic (ROC) curves and area under the ROC curves. We quantitatively compare the results of the proposed method with that of support vector machine (SVM) which is another very popular statistical learning method for pattern classification.",space,415
10.1109/embc.2016.7592132,filtered,2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),IEEE,2016-08-20 00:00:00,ieeexplore,automated classification of pathological gait after stroke using ubiquitous sensing technology,https://ieeexplore.ieee.org/document/7592132/,"This study uses machine learning methods to distinguish between healthy and pathological gait. Examples of multi-dimensional pathological and normal gait sequences were collected from post-stroke and healthy individuals in a real clinical setting and with two Kinect sensors. The trajectories of rotational angle and global velocity of selected body joints (hips, spine, shoulders, neck, knees and ankles) over time formed the gait sequences. The combination of k nearest neighbor (kNN) and dynamic time warping (DTW) was used for classification. Leave one subject out cross validation was implemented to evaluate the performance of the binary classifier in terms of F1-score in the original feature space, and also in a reduced dimensional feature space using PCA. The pair of k = 1 in kNN and the warping window size 25% of gait sequences in DTW achieved maximum F1-score. Using PCA, pathological gait sequences were discriminated from healthy sequences with the F1-score = 96%.",space,416
10.1109/iccchina.2014.7008377,filtered,2014 IEEE/CIC International Conference on Communications in China (ICCC),IEEE,2014-10-15 00:00:00,ieeexplore,automatic collecting of indoor localization fingerprints: an crowd-based approach,https://ieeexplore.ieee.org/document/7008377/,"For typical indoor positioning systems employing a training/positioning model based on Wi-Fi fingerprints, significant training costs extremely restrict this kind of indoor localization system to be widely deployed and implemented with real location based applications. In this paper, we present a crowd-based approach to solve this problem, which automatically collects and constructs fingerprints database for anonymous buildings through common crowd customers with their smart-phones. However, such a crowd-based approach also introduces an accuracy degradation problem as crowd customers are not professional trained and equipped. So in this approach we employ fixed and hint landmarks to do error resetting. In our practical system, common corridor crossing points will serve as fixed landmarks and cross points between different crowd paths serve as hint landmarks. Machine-learning techniques are utilized for short range approximation around fixed landmarks and fuzzy logic decision technology is applied for searching hint landmarks in crowd traces space. We test this crowd-based automatic collecting approach on a dataset of about 5.09km walking in four corridors and the rooms besides. Experimental results indicate that this automatic collecting approach successfully construct indoor fingerprint radio map with rather high accuracy.",space,417
10.1109/iv47402.2020.9304624,filtered,2020 IEEE Intelligent Vehicles Symposium (IV),IEEE,2020-11-13 00:00:00,ieeexplore,autonomous driving: framework for pedestrian intention estimation in a real world scenario,https://ieeexplore.ieee.org/document/9304624/,"Rapid advancements in driver assistance technology will lead to the integration of fully autonomous vehicles on our roads that will interact with other road users. To address the problem that driverless vehicles make interaction through eye contact impossible, we describe a framework for estimating the crossing intentions of pedestrians in order to reduce the uncertainty that the lack of eye contact between road users creates. The framework was deployed in a real vehicle and tested with three experimental cases that showed a variety of communication messages to pedestrians in a shared space scenario. Results from the performed field tests showed the feasibility of the presented approach.",space,418
10.1109/ies53407.2021.9594013,filtered,2021 International Electronics Symposium (IES),IEEE,2021-09-30 00:00:00,ieeexplore,ball position transformation with artificial intelligence based on tensorflow libraries,https://ieeexplore.ieee.org/document/9594013/,Research on wheeled soccer robots has been carried out by several researchers. This is due to the existence of national and international competitions. Previous research was to create a ball position transformation system with a modified method of neural network architecture. This research was developed by building an intelligent transformation system with the Tensorflow library. This transformation system aims to be able to directly measure the distance of objects in real terms without first changing the environmental image from an omni field to a flat plane with conventional camera calibration techniques. This process can replace manual calibration with a variety of field size changes The system can transform with mean error 0.0000026 on epoch 10000 using “conda-tensorflowneural network” libraries. It can transform the position of the ball from the omni space to the cartesian space. This system was implemented on wheeled soccer robot as keeper.,space,419
10.1109/icdl-epirob48136.2020.9278071,filtered,2020 Joint IEEE 10th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob),IEEE,2020-10-30 00:00:00,ieeexplore,bayesian optimization for developmental robotics with meta-learning by parameters bounds reduction,https://ieeexplore.ieee.org/document/9278071/,"In robotics, methods and softwares usually require optimizations of hyperparameters in order to be efficient for specific tasks, for instance industrial bin-picking from homogeneous heaps of different objects. We present a developmental framework based on long-term memory and reasoning modules (Bayesian Optimisation, visual similarity and parameters bounds reduction) allowing a robot to use meta-learning mechanism increasing the efficiency of such continuous and constrained parameters optimizations. The new optimization, viewed as a learning for the robot, can take advantage of past experiences (stored in the episodic and procedural memories) to shrink the search space by using reduced parameters bounds computed from the best optimizations realized by the robot with similar tasks of the new one (e.g. bin-picking from an homogenous heap of a similar object, based on visual similarity of objects stored in the semantic memory). As example, we have confronted the system to the constrained optimizations of 9 continuous hyperparameters for a professional software (Kamido) in industrial robotic arm bin-picking tasks, a step that is needed each time to handle correctly new object. We used a simulator to create bin-picking tasks for 8 different objects (7 in simulation and one with real setup, without and with meta-learning with experiences coming from other similar objects) achieving goods results despite a very small optimization budget, with a better performance reached when meta-learning is used (84.3 % vs 78.9 % of success overall, with a small budget of 30 iterations for each optimization) for every object tested (p-value=0.036).",space,420
10.1109/idaacs53288.2021.9660834,filtered,2021 11th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS),IEEE,2021-09-25 00:00:00,ieeexplore,biometric identification via oculomotor system based on the volterra model,https://ieeexplore.ieee.org/document/9660834/,"In recent years, there has been an increase in interest in biometrics research involving the use of brain characteristics commonly known as behavioral traits. Human eyes contain a rich source of idiosyncratic information which may be used for the recognition of an individual's identity. This article implements an innovative experiment and a new approach to processing human eye movements, ultimately aimed at biometric identification of individuals. In our experiment, the subjects observe special test visual stimuli, which are generated on the computer monitor screen. The eye movements are tracked in dynamics providing information for constructing a nonparametric nonlinear dynamic model (Volterra model) of a human's oculomotor system (OMS) in the form of multivariate transient functions. The implemented method treats eye trajectories as 2-D distributions of points on the “Coordinate-Time” plane. The efficiency of dynamic characteristics for personality identification is confirmed by examples of models built on the basis of data from real experiments. The resulting OMS models are a source of information for the selection of informative features, in the space of which the decisive rule of optimal identification of individuals is determined using machine learning methods. Promising results at the task of identification according to behavioral characteristics of an individual have been obtained - recognition accuracy is higher than 97%.",space,421
10.1109/infocom42981.2021.9488865,filtered,IEEE INFOCOM 2021 - IEEE Conference on Computer Communications,IEEE,2021-05-13 00:00:00,ieeexplore,can you fix my neural network? real-time adaptive waveform synthesis for resilient wireless signal classification,https://ieeexplore.ieee.org/document/9488865/,"Due to the sheer scale of the Internet of Things (IoT) and 5G, the wireless spectrum is becoming severely congested. For this reason, wireless devices will need to continuously adapt to current spectrum conditions by changing their communication parameters in real-time. Therefore, wireless signal classification (WSC) will become a compelling necessity to decode fast-changing signals from dynamic transmitters. Thanks to its capability of classifying complex phenomena without explicit mathematical modeling, deep learning (DL) has been demonstrated to be a key enabler of WSC. Although DL can achieve a very high accuracy under certain conditions, recent research has unveiled that the wireless channel can disrupt the features learned by the DL model during training, thus drastically reducing the classification performance in real-world live settings. Since retraining classifiers is cumbersome after deployment, existing work has leveraged the usage of carefully-tailored Finite Impulse Response (FIR) filters that, when applied at the transmitter's side, can restore the features that are lost because of the the channel actions, i.e., waveform synthesis. However, these approaches compute FIRs using offline optimization strategies, which limits their efficacy in highly-dynamic channel settings. In this paper, we improve the state of the art by proposing Chares, a Deep Reinforcement Learning (DRL)-based framework for channel-resilient adaptive waveform synthesis. Chares adapts to new and unseen channel conditions by optimally computing through DRL the FIRs in real time. Chares is a DRL agent whose architecture is based upon the Twin Delayed Deep Deterministic Policy Gradients (TD3), which requires minimal feedback from the receiver and explores a continuous action space for best performance. Chares has been extensively evaluated on two well-known datasets with an extensive number of channels. We have also evaluated the real-time latency of Chares with an implementation on field-programmable gate array (FPGA). Results show that Chares increases the accuracy up to 4.1x when no waveform synthesis is performed, by 1.9x with respect to existing work, and can compute new actions within 41 μs.",space,422
10.1109/robot.1993.292250,filtered,[1993] Proceedings IEEE International Conference on Robotics and Automation,IEEE,1993-05-06 00:00:00,ieeexplore,cellular robotics: simulation and hw implementation,https://ieeexplore.ieee.org/document/292250/,"Aspects of self-organization are presented in this paper. Computer simulations as well as a real prototypical implementation are used to illustrate the proposed approach. Results of simulations are presented to compare different strategies of self-organization enabling a system of autonomous robots to form a chain between two landmarks in a completely unknown environment. This chain implicitly represents a path between any two points of the environment without an explicit representation of free space (no single robot has a global map of the environment). The experimental part, even if restricted to a few robots, demonstrates that the set of stimuli-action processes used in the simulations are indeed feasible on real systems.&lt;<ETX>&gt;</ETX>",space,423
10.1109/enc.2005.14,filtered,Sixth Mexican International Conference on Computer Science (ENC'05),IEEE,2005-09-30 00:00:00,ieeexplore,cervical cancer detection using colposcopic images: a temporal approach,https://ieeexplore.ieee.org/document/1592214/,"In the present work, we propose a methodology analysis of the colposcopic images to help the expert to make a more robust diagnosis of precursor lesions of cervical cancer. Although some others approaches have been used to assess cervical lesion, a complete methodology to evaluate temporal changes of tissue color is still missing. The different processes involved in the analysis are described. The image registration was implemented using the phase correlation method followed by a locally applied algorithm based on the normalized cross-correlation. During the parameterization process, each time series obtained from the image sequences was represented as a parabola in a parameter space. A supervised Bayesian learning approach is proposed to classify the features in the parameter space according to the classification made by the colposcopist. Then those labels are used as a criterion to categorize the tissue and perform the image segmentation. Some preliminary results are shown using unsupervised learning with real data.",space,424
10.1109/iembs.2007.4353509,filtered,2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,IEEE,2007-08-26 00:00:00,ieeexplore,channel and feature selection in multifunction myoelectric control,https://ieeexplore.ieee.org/document/4353509/,"Real time controlling devices based on myoelectric singles (MES) is one of the challenging research problems. This paper presents a new approach to reduce the computational cost of real time systems driven by myoelectric signals (MES) (a.k.a Electromyography-EMG). The new approach evaluates the significance of feature/channel selection on MES pattern recognition. Particle Swarm Optimization (PSO), an evolutionary computational technique, is employed to search the feature/channel space for important subsets. These important subsets will be evaluated using a multilayer perceptron trained with back propagation neural network (BPNN). Practical results acquired from tests done on six subject's datasets of MES signals measured in a noninvasive manner using surface electrodes are presented. It is proved that minimum error rates can be achieved by considering the correct combination of features/channels, thus providing a feasible system for practical implementation purpose for rehabilitation of patients.",space,425
10.1109/cnna.1998.685324,filtered,1998 Fifth IEEE International Workshop on Cellular Neural Networks and their Applications. Proceedings (Cat. No.98TH8359),IEEE,1998-04-17 00:00:00,ieeexplore,classification systems based on neural networks,https://ieeexplore.ieee.org/document/685324/,"Classification is a problem that appears in many real life applications. We describe the general case of multi-class classification, where the task of the classification system is to map an input vector x to one of K&gt;2 given classes. This problem is split in many two-class classification problems, each of them describing a part of the whole problem. These are solved by neural networks, producing an intermediate output in a reference space, which is then decoded to the solution of the original problem. The methods described here are then applied to the handwritten character recognition problem to produce the results described later in the article. It is suspected that they also may be applied successfully in the context of the CNN paradigm and be implemented on a CNN-Universal Machine.",space,426
10.1109/embc.2018.8513597,filtered,2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),IEEE,2018-07-21 00:00:00,ieeexplore,clustering based kernel reinforcement learning for neural adaptation in brain-machine interfaces,https://ieeexplore.ieee.org/document/8513597/,"Reinforcement learning (RL) interprets subject's movement intention in Brain Machine Interfaces (BMIs) through trial-and-error with the advantage that it does not need the real limb movements. When the subjects try to control the external devices purely using brain signals without actual movements (brain control), they adjust the neural firing patterns to adapt to device control, which expands the state-action space for the RL decoder to explore. The challenge is to quickly explore the new knowledge in the sizeable state-action space and maintain good performance. Recently quantized attention-gated kernel reinforcement learning (QAGKRL) was proposed to quickly explore the global optimum in Reproducing Kernel Hilbert Space (RKHS). However, its network size will grow large when the new input comes, which makes it computationally inefficient. In addition, the output is generated using the whole input structure without being sensitive to the new knowledge. In this paper, we propose a new kernel based reinforcement learning algorithm that utilizes the clustering technique in the input domain. The similar neural inputs are grouped, and a new input only activates its nearest cluster, which either utilizes an existing sub-network or forms a new one. In this way, we can build the sub-feature space instead of the global mapping to calculate the output, which transfers the old knowledge effectively and also consequently reduces the computational complexity. To evaluate our algorithm, we test on the synthetic spike data, where the subject's task mode switches between manual control and brain control. Compared with QAGKRL, the simulation results show that our algorithm can achieve a faster learning curve, less computational time, and more accuracy. This indicates our algorithm to be a promising method for the online implementation of BMIs.",space,427
10.1109/iceccme52200.2021.9591113,filtered,"2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)",IEEE,2021-10-08 00:00:00,ieeexplore,cobots for fintech,https://ieeexplore.ieee.org/document/9591113/,"Embedded devices enabling payments transaction processing in Financial Services industry cannot have any margin for error. These devices need to be tested &amp; validated by replicating production like environment to the extent possible. This means literally handling payments related events like swiping a credit card, tapping a mobile phone or pressing buttons amongst many other things like in real world. Embedded Software development is time consuming as it involves multiple man-machine interactions and dependencies such as managing and handling embedded devices, operating devices (Push buttons, interpret display panels, read receipt printouts etc.) and sharing devices for collaboration within team. During the current pandemic, it was impossible for software teams to travel to office, share devices or even procure necessary devices on time for project related tasks. This caused delay to project delivery and increased Time to market. The paper describes how the team used Capgemini's flexible Robotics as a Service (RaaS) platform that helped during pandemic to automate feasible man-machine interactions using Robotic arms. The paper provides details of the work done by the team that involves internet of things (IoT), Artificial Intelligence (AI) to remotely handle and operate hardware and devices thereby completing embedded software development life cycles faster and well within budget while ensuring superior product quality and importantly ensuring team's health and safety. This is novel in Financial Services space.",space,428
10.1109/cimca.2005.1631533,filtered,"International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)",IEEE,2005-11-30 00:00:00,ieeexplore,cognitive perception in rafale-sp methodology,https://ieeexplore.ieee.org/document/1631533/,"Several methodologies based on multi-agent systems (MAS) already exist. They help designers to describe software or to create MAS which aim at solving complex problems by simulations. Due to used formalisms, a methodology may be more or less generic. In this context, we have created a mobility oriented methodology called RAFALE-SP based on multi-agent systems. It helps us to describe mobiles which move on a space. This environment can be a virtual representation of a real space like a town where unpredictable events arise. We apply our methodology to solve problems which come from different research areas. We use it to find answers to geographical problems. The presented methodology begins by a conceptual representation of each mobile type and finishes by a mobility simulator. It uses several formalisms: UML and Ploom-unity. They allow us to define mobiles, their interactions and their environment. According to their knowledge, their behaviour rules, mobiles moves on a space by following few motion types. They get an individual perception of the world. In this paper we focus on mobile perception description",space,429
10.1109/itsc.2017.8317627,filtered,2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC),IEEE,2017-10-19 00:00:00,ieeexplore,cognitive map-based model: toward a developmental framework for self-driving cars,https://ieeexplore.ieee.org/document/8317627/,"End-to-end learning and multi-sensor fusion-based methods are two major frameworks used for self-driving cars. To enable these intelligence vehicles to acquire driving skills at a level comparable to that of human drivers, long short-term memory of previous self-driving processes is necessary, but is difficult to introduce into the above-mentioned frameworks. In this paper, we propose a model for self-driving cars called the cognitive map-based neural network (CMNN). Our framework consists of three parts: a convolutional neural network that can perceive the environment in the manner that the human visual cortex does, a cognitive map to describe the locations of objects in a complex traffic scene and the relationships among them, and a recurrent neural network to process long short-term memory from the cognitive map, which is updated in real time. The proposed model is built to simultaneously handle three tasks: i) detecting free space and lane boundaries, ii) estimating vehicle pose and obstacle distance, and iii) learning to plan and control based on the behaviors of a human driver. More significantly, our approach introduces external instructions during an end-to-end driving process. To test it, we created a large-scale road vehicle dataset (RVD) containing more than 50,000 labeled road images captured by three cameras. We implemented the proposed model on an embedded system.",space,430
10.1109/iri-05.2005.1506442,filtered,"IRI -2005 IEEE International Conference on Information Reuse and Integration, Conf, 2005.",IEEE,2005-08-17 00:00:00,ieeexplore,collaborative knowledge management by integrating knowledge modeling and workflow modeling,https://ieeexplore.ieee.org/document/1506442/,"Recently both industrial and academic researches show great interest in knowledge management. However, users still find it hard to obtain a suitable knowledge management tool that fits for their needs. Although business requirements always change, current software systems still cannot be adapted to these changes quickly. In this paper, a framework is put forward to facilitate the design of an adaptive knowledge management system. In this framework, the structural knowledge modeling is combined with processes, which are used for ensuring the quality of knowledge acquisition in the framework. Two kinds of spaces, knowledge space and process space, and a knowledge state model are introduced. Finally, application systems based on this framework, which are being used in three real business enterprises for controlling life cycle of knowledge management in China, are discussed.",space,431
10.1109/icca.2016.7505247,filtered,2016 12th IEEE International Conference on Control and Automation (ICCA),IEEE,2016-06-03 00:00:00,ieeexplore,communication of spatial expressions on multi-agent systems using the qualitative ego-sphere,https://ieeexplore.ieee.org/document/7505247/,"The need for spatial representations and spatial reasoning is omnipresent in various real world applications of autonomous systems. The task of the qualitative spatial reasoning sub-field of Artificial Intelligence is to provide formalisms allowing a machine to represent and make inferences about spatial entities. In this work we make use of one such formalism for representing qualitative location, named Qualitative Ego-Sphere (QES), that discretises the world around a visual agent into sectors, as well as with respect to the relative distance of objects from the observer's point of view. QES was used in this paper as a means for communicating spatial expressions between pairs of agents. Four situations were proposed and implemented in order to address interactions between pairs of artificial agents and between an artificial agent and a human. Tests with human volunteers suggested that the human description of space in sectors agrees with the qualitative discretisation provided by QES. However, no similar agreement rates were obtained regarding the description of space related to the distance between objects. This was arguably due to the fact that qualitative distance judgements imply the existence of some relative external reference frame (not taken into account in the Qualitative Ego-Sphere formalism).",space,432
10.1109/pic.2018.8706141,filtered,2018 IEEE International Conference on Progress in Informatics and Computing (PIC),IEEE,2018-12-16 00:00:00,ieeexplore,comparison and analysis on typical network representation learning algorithms,https://ieeexplore.ieee.org/document/8706141/,"Large-scale complex networks show complex nonlinear relationships among objects, such as the social relationships in the real world, the citation relationship among papers and the interactions among proteins in biology. The analysis of complex network systems make it possible to reveal network structures, information disseminating laws, and communication patterns. Network representation learning (NRL) algorithms focus on mapping the original network structure information to a low-dimensional vector space through a series of operations under the premise of maximally retaining the network structure. In order to analyze current representative NRL algorithms effectively to provide valuable references for other researchers, we built an experimental platform to perform and test the NRL algorithms based on matrix factorization, the NRL algorithms based on shallow neural network and the NRL algorithms based on deep neural network, with datasets on Collaboration Network, Social Network and Citation Network. We implemented a series of comprehensive experiments, based on metrics include precision@k, micro-F1 and macro-F1. Our experiments include network reconstruction, vertex classification, and link prediction, and show readers principles, performances and applications of typical NRL algorithms.",space,433
10.1109/itnt52450.2021.9649145,filtered,2021 International Conference on Information Technology and Nanotechnology (ITNT),IEEE,2021-09-24 00:00:00,ieeexplore,comparison of reinforcement learning algorithms for motion control of an autonomous robot in gazebo simulator,https://ieeexplore.ieee.org/document/9649145/,"This article compares various implementations of deep Q learning as it is one of the most efficient reinforcement learning algorithms for discrete action space systems. The efficiency of the implementations for the classical Cartpole problem ported to the Gazebo environment is investigated. Then, these algorithms are compared for a self-created bipedal robot problem. Since the creation and configuration of a real robotic system is a laborious process, the initial debugging of the robot can be performed using the appropriate software that simulates the real environment. In our case, the Gazebo simulator was used. Using the simulator allows you to conduct research without having a real robotic system. In this case, it is possible to transfer the results from the simulator to the real system. The result of the study is the conclusion about the greatest efficiency of deep Q-learning with the experience reproduction mechanism. Also, the conclusion is that even for a robot with two degrees of freedom, Q-learning algorithms are not effective enough, and a comparative study with other families of reinforcement learning algorithms is needed.",space,434
10.1109/aitest.2019.00015,filtered,2019 IEEE International Conference On Artificial Intelligence Testing (AITest),IEEE,2019-04-09 00:00:00,ieeexplore,constraint-based testing of an industrial multi-robot navigation system,https://ieeexplore.ieee.org/document/8718216/,"Intelligent multi-robot systems get more and more deployed in industrial settings to solve complex and repetitive tasks. Due to safety and economic reasons they need to operate dependably. To ensure a high degree of dependability, testing the deployed system has to be done in a rigorous way. Advanced multi-robot systems show a rich set of complex behaviors. Thus, these systems are difficult to test manually. Moreover, the space of potential environments and tasks for such systems is enormous. Therefore, methods that are able to explore this space in a structured way are needed. One way to address these issues is through model-based testing. In this paper we present an approach for testing the navigation system of a fleet of industrial transport robots. We show how all potential environments and navigation behaviors as well as requirements and restrictions can be represented in a formal constraint-based model. Moreover, we present the concept of coverage criteria in order to handle the potentially infinite space of test cases. Finally, we show how test cases can be derived from this model in an efficient way. In order to show the feasibility of the proposed approach we present an empirical evaluation of a prototype implementation using a real industrial use case.",space,435
10.1109/qrs.2015.17,filtered,"2015 IEEE International Conference on Software Quality, Reliability and Security",IEEE,2015-08-05 00:00:00,ieeexplore,cross-project aging related bug prediction,https://ieeexplore.ieee.org/document/7272913/,"In a long running system, software tends to encounter performance degradation and increasing failure rate during execution, which is called software aging. The bugs contributing to the phenomenon of software aging are defined as Aging Related Bugs (ARBs). Lots of manpower and economic costs will be saved if ARBs can be found in the testing phase. However, due to the low presence probability and reproducing difficulty of ARBs, it is usually hard to predict ARBs within a project. In this paper, we study whether and how ARBs can be located through cross-project prediction. We propose a transfer learning based aging related bug prediction approach (TLAP), which takes advantage of transfer learning to reduce the distribution difference between training sets and testing sets while preserving their data variance. Furthermore, in order to mitigate the severe class imbalance, class imbalance learning is conducted on the transferred latent space. Finally, we employ machine learning methods to handle the bug prediction tasks. The effectiveness of our approach is validated and evaluated by experiments on two real software systems. It indicates that after the processing of TLAP, the performance of ARB bug prediction can be dramatically improved.",space,436
10.1109/cec45853.2021.9504779,filtered,2021 IEEE Congress on Evolutionary Computation (CEC),IEEE,2021-07-01 00:00:00,ieeexplore,cultural weight-based fish school search: a flexible optimization algorithm for engineering,https://ieeexplore.ieee.org/document/9504779/,"Many real-life engineering applications are optimization problems. To find the best configuration of variables to minimize costs and maximize efficiency are typically used engineering software as CAD, CAE and CAM. In this context, Machine Learning can be used to automate and improve this type of application. This despite, those searches not seldomly evoke unreliable areas and suggest risky solutions. Because of inaccuracies, volatility, unfeasibility, and specificities of real environments, the easy incorporation of cultural practices (i.e. normative, situational, domain and historic knowledge) and as well as the production of multiple acceptable solutions for a problem is always welcome, especially in Engineering. The present article put forward a hybridization of a multi-modal algorithm (Weight-Based Fish School Search - wFSS) with Cultural Algorithms' belief space. New cwFSS is able to guide the optimization also considering normative knowledge from experts, technical literature and problem domain readily available knowledge to prevent the incorporation of constraints into the fitness function. We also evaluated the use of temporal knowledge to guide the simulation. The proposed method was tested in a thermal power plant efficiency optimization and compared with standard wFSS and the Niching Migratory Multi-Swarm Optimizer (NMMSO), winner of CEC'2015 niching competition. As results, cwFSS has outperformed at times NMMSO about time, fitness and variability, as well as traditional wFSS about time, stability, safeness and variability of the multimodal solutions. By avoiding penalties, the appropriation of a priori search directly into the search can effectively and elegantly help better support for engineering decisions.",space,437
10.1109/adfsp.1998.685683,filtered,1998 IEEE Symposium on Advances in Digital Filtering and Signal Processing. Symposium Proceedings (Cat. No.98EX185),IEEE,1998-06-06 00:00:00,ieeexplore,d-fanns (dynamical functional artificial neural networks)-a new avenue for intelligent analog signal processing,https://ieeexplore.ieee.org/document/685683/,"Summary form only given. Intelligent signal processing may be defined as the process of mapping a signal x into a binary vector or matrix y, so that y enables the detection, classification, or interpretation of an event present in x. (In the case of an interpretation in an appropriate language, y would represent a digitally coded relational structure.) We denote by f the input-output map of such an intelligent signal processing filter. In a number of applications, it is possible to naturally implement the nonlinear filter map f by an artificial neural network (ANN). We consider the case in which x is an analog signal (waveform) belonging to L2(I), where I is an appropriate interval of the real line R1 (i.e., L2(I) is the space of square integrable functions on I), and propose the realization of f by an artificial neural network in which the synaptic weight actions of the first layer are implemented by a filter bank. We call such a network a dynamical functional artificial neural network (D-FANN) to distinguish it from a conventional functional artificial neural network (FANN), where a synaptic weight action is implemented by a scalar product (integration) in L2(I), between the incoming waveform x and a ""distributed"" functional weight. Compared with conventional FANNs, D-FANNs permit simple and meaningful causal realizations of intelligent analog signal processors. A novel element in the present paper is the introduction of a ""D-FANN gain equation"", in a way analogous to that in Kalman filtering. Applications of D-FANNs to real and simulated data are now in progress and these results are discussed.",space,438
10.23919/irs.2019.8768102,filtered,2019 20th International Radar Symposium (IRS),IEEE,2019-06-28 00:00:00,ieeexplore,data analytics and machine learning in wide area surveillance systems,https://ieeexplore.ieee.org/document/8768102/,"Modern surveillance networks are able to provide trajectories of all kind of vessels and aircrafts within worldwide or at least extended environment. Most widely used are Automatic Dependent Surveillance - Broadcast (ADS-B) and (Satellite-) Automatic Identification System (AIS) used within air and maritime surveillance. Both of them are cooperative systems. Besides these systems, sensor networks based on ground installations or mounted on airborne and space-based platforms deliver object trajectories independent of any cooperation. Examples include GMTI radar-based systems operating on UAV platforms and coastal or air traffic control sensor network installations. These surveillance systems provide mid- and long-term trajectories. The challenging part is the related situational awareness and the estimation of the intent of the tracked objects. New technologies include activity-based intelligence and the determination of patterns of life. An approach for these technologies can be found in the advanced analysis of those trajectories, which are extracted by the mentioned surveillance systems. Trajectories are partitioned into specific segments of interest using cluster algorithms. This helps to decode their pattern of life based on unsupervised machine learning. Trajectories are aggregated into different routes with dedicated representatives. Calculated probabilities indicate the frequentation of these routes. This allows predictive analytics and the identification of anomalous behaviour. Finally, these new data analytic techniques have to be integrated in existing near real time surveillance systems. This requires specific system architectures as well as a completely new software and hardware landscape. So, trajectory-based Machine Learning is embedded in local or global clouds and uses dedicated mechanisms for distributed and parallel processing.",space,439
10.23919/irs48640.2020.9253816,filtered,2020 21st International Radar Symposium (IRS),IEEE,2020-10-08 00:00:00,ieeexplore,"data analytics, machine learning and risk assessment for surveillance and situation awareness",https://ieeexplore.ieee.org/document/9253816/,"Modern surveillance networks are able to provide trajectories of all kinds for aircrafts and vessels worldwide or at least in extended areas of the airspace or earth surface. Best known are Automatic Dependent Surveillance - Broadcast (ADS-B) and (Satellite-) Automatic Identification System (AIS) used in air and maritime surveillance. Both of them are cooperative systems. Besides these sources, sensors based on ground installations or mounted on airborne and space-based platforms deliver object trajectories independently of any transponders. This is done by advanced tracking and fusion algorithms generating trajectories out of sensor measurements. Examples include GMTI radar-based systems operating on UAV platforms or imaging systems based on high altitude pseudo satellites (HAPS) and satellites. These surveillance systems enable a continuous extraction of mid- and long-term trajectories of objects. Besides the trajectory generation, the challenge will be to place them into the right context and to provide situational awareness. This includes the estimation of the intents of the tracked objects, activity-based intelligence, and the determination of patterns of life. Otherwise, even modern surveillance systems are not able to take a real advantage of the gathered data. Therefore, trajectories are further processed by data analytics and machine learning. Unsupervised machine learning offers techniques to cluster and to partition trajectories, extract highly frequented routes and points of interest, predict object movement and identify anomalous behaviour. On the other hand, transponder and broadcast systems provide additional attributes of the tracked trajectories. These labels pave the way for numerous supervised machine learning methods. The derived predictors realise the determination of object types and activities. Finally, these new data analytic techniques have to be integrated in existing near real time surveillance systems. This requires specific system architectures as well as a completely new software and hardware landscape. In summary, trajectory-based data analytics, machine learning and risk assessment are embedded on local or global clouds and use dedicated mechanisms for distributed and parallel processing.",space,440
10.1109/icra48506.2021.9561145,filtered,2021 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2021-06-05 00:00:00,ieeexplore,deep reinforcement learning framework for underwater locomotion of soft robot,https://ieeexplore.ieee.org/document/9561145/,"Soft robotics is an emerging technology with excellent application prospects. However, due to the inherent compliance of the materials used to build soft robots, it is extremely complicated to control soft robots accurately. In this paper, we introduce a data-based control framework for solving the soft robot underwater locomotion problem using deep reinforcement learning (DRL). We first built a soft robot that can swim based on the dielectric elastomer actuator (DEA). We then modeled it in a simulation for the purpose of training the neural network and tested the performance of the control framework through real experiments on the robot. The framework includes the following: a simulation method for the soft robot that can be used to collect data for training the neural network, the neural network controller of the swimming robot trained in the simulation environment, and the computer vision method to collect the observation space from the real robot using a camera. We confirmed the effectiveness of the learning method for the soft swimming robot in the simulation environment by allowing the robot to learn how to move from a random initial state to a specific direction. After obtaining the trained neural network through the simulation, we deployed it on the real robot and tested the performance of the control framework. The soft robot successfully achieved the goal of moving in a straight line in disturbed water. The experimental results suggest the potential of using deep reinforcement learning to improve the locomotion ability of mobile soft robots.",space,441
10.1109/cns48642.2020.9162219,filtered,2020 IEEE Conference on Communications and Network Security (CNS),IEEE,2020-07-01 00:00:00,ieeexplore,deepbloc: a framework for securing cps through deep reinforcement learning on stochastic games,https://ieeexplore.ieee.org/document/9162219/,"One important aspect in protecting Cyber Physical System (CPS) is ensuring that the proper control and measurement signals are propagated within the control loop. The CPS research community has been developing a large set of check blocks that can be integrated within the control loop to check signals against various types of attacks (e.g., false data injection attacks). Unfortunately, it is not possible to integrate all these “checks” within the control loop as the overhead introduced when checking signals may violate the delay constraints of the control loop. Moreover, these blocks do not completely operate in isolation of each other as dependencies exist among them in terms of their effectiveness against detecting a subset of attacks. Thus, it becomes a challenging and complex problem to assign the proper checks, especially with the presence of a rational adversary who can observe the check blocks assigned and optimizes her own attack strategies accordingly. This paper tackles the inherent state-action space explosion that arises in securing CPS through developing DeepBLOC (DB)-a framework in which Deep Reinforcement Learning algorithms are utilized to provide optimal/sub-optimal assignments of check blocks to signals. The framework models stochastic games between the adversary and the CPS defender and derives mixed strategies for assigning check blocks to ensure the integrity of the propagated signals while abiding to the real-time constraints dictated by the control loop. Through extensive simulation experiments and a real implementation on a water purification system, we show that DB achieves assignment strategies that outperform other strategies and heuristics.",space,442
10.1109/iske47853.2019.9170453,filtered,2019 IEEE 14th International Conference on Intelligent Systems and Knowledge Engineering (ISKE),IEEE,2019-11-16 00:00:00,ieeexplore,design and implementation of on-orbit valuable image extraction for the tianzhi-1 satellite,https://ieeexplore.ieee.org/document/9170453/,"Recently, software-defined satellite has become a research hotspot in the aerospace. Based on an advanced computing platform with open system architecture, researchers can upload software for specific tasks even the satellite has been launched into space. This paper we have designed an on-orbit application for China's first software-defined satellite TianZhi-1, which use Android smartphone as a system platform. Two main tasks are focused on our work, one is to reduce data redundancy and the other is to compress the size of the software. First, a light-weight and extensible framework is designed to support different image processing algorithms. Following this, we propose a three-step approach for on-orbit valuable image extraction, include image denoising, stitching, and salient object extraction. Experiments on the real satellite achieve outstanding results.",space,443
10.1109/fuzzy.1994.343884,filtered,Proceedings of 1994 IEEE 3rd International Fuzzy Systems Conference,IEEE,1994-06-29 00:00:00,ieeexplore,design and analysis of an anti-slip fuzzy controller for heavy duty off road vehicles,https://ieeexplore.ieee.org/document/343884/,This paper illustrates the design and analysis of a distributed fuzzy control system to prevent wheel slippage in heavy-duty off-road vehicles with hydrostatic power transmission using the vector space analytical approach. The controller described have been fully implemented in a commercially available city tractor and tested on a test track as well as under real working conditions.&lt;<ETX>&gt;</ETX>,space,444
10.1109/icbaie52039.2021.9389950,filtered,"2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",IEEE,2021-03-28 00:00:00,ieeexplore,design of cloud computing platform based accurate measurement for structure monitoring using fiber bragg grating sensors,https://ieeexplore.ieee.org/document/9389950/,"The efficient integration of distributed fiber Bragg grating (FBG) sensors and cloud computing platform is used to achieve accurate measurement and evaluation of physical quantities, which solves the problems of traditional fiber Bragg grating sensing technology for health structure monitoring system, such as the cost and space constraints, it is difficult to deploy enough servers to deal with data collection, transmission and storage in real time. The cloud platform using fiber Bragg grating sensors adopts the structure of erbium-doped fiber cascaded Bragg grating, reasonably configures the FBG demodulator acquisition and analysis software, deploys the health monitoring system in the cloud, constructs the cloud platform of high-efficiency health monitoring optical fiber sensor network, improves the scalability of the system, flexibly deploys applications and services, and ensures the security and reliability of various real-time monitoring data and professional data. It can meet the needs of some specific or wide application fields for the automation technology, structural mechanics, computer technology, Internet architecture, cloud deployment and interdisciplinary practical comprehensive valuable application research.",space,445
10.1109/compsac.2008.76,filtered,2008 32nd Annual IEEE International Computer Software and Applications Conference,IEEE,2008-08-01 00:00:00,ieeexplore,designing simulated context-aware telephone in pervasive spaces,https://ieeexplore.ieee.org/document/4591762/,"Personal context information is important to enable context-aware applications in pervasive spaces. As privacy, many investigators have pointed out that it must be in the controllable state by using personal privacy policy. The long term objective of this research was to extract privacy policy from personal context information database with machine learning method. A context-aware telephone system was designed to collect data, learn policy and examine the accuracy of policy. In the step of PDA simulation, the results showed that the method was possible to make suitable decision for telephone communication in consideration of privacy protection. In order to further validate the proposed privacy protection method, the pervasive space was designed and implemented to get more real data about using context-aware telephone. At the step of pervasive space simulation, we design and code the prototype of context-aware telephone system from the view of software engineering.",space,446
10.1109/emctech49634.2020.9261546,filtered,2020 International Conference on Engineering Management of Communication and Technology (EMCTECH),IEEE,2020-10-22 00:00:00,ieeexplore,designing of a classifier for the unstructured text formalization model based on word embedding,https://ieeexplore.ieee.org/document/9261546/,"The active use of artificial intelligence technologies has a direct positive impact on the development of society in various areas of human life. The article describes the developed model of processing and formalization of textual unstructured information in the form of a continuous flow of text information taken from the news feed of news agencies. A method for preprocessing text to reduce the execution time of the algorithm and save CPU resources is given. A method for representing words as a real vector is formed using various algorithms for training artificial neural networks and their properties. A model of the first stage of the text information processing system as a subsystem for classifying the subject of a news article text based on a vector representation of words, including a description of the word vectorization algorithm, an example of the type of word structure with a corresponding numeric vector, and a metric that determines the proximity of vectors to each other in space. The results of the experiment are obtained and a method for setting a decision criterion for the implemented classifier is proposed. The area of use of the proposed classifier is the sphere of information security. The results of the experiment can be indicators of the suitability of using the classifier as a definition of the subject of a news article.",space,447
10.1109/lcn.2008.4664303,filtered,2008 33rd IEEE Conference on Local Computer Networks (LCN),IEEE,2008-10-17 00:00:00,ieeexplore,detection of anomalous network packets using lightweight stateless payload inspection,https://ieeexplore.ieee.org/document/4664303/,"A real-time packet-level anomaly detection approach for high-speed network intrusion prevention is described. The approach is suitable for small and fast hardware implementation and was designed to be embedded in network appliances. Each network packet is characterized using a novel technique that efficiently maps the payload histogram onto a simple pair of features using hypercube hash functions, which were chosen for their implementation efficiency in both hardware and software. This two-dimensional feature space is quantized into a binary bitmap representing the normal and anomalous feature regions. The potential loss of accuracy due to the reduction in feature space is countered by the ability of the bitmaps to capture nearly arbitrary shaped regions in the feature space. These bitmaps are used as the classifiers for real-time detection. The proposed method is extremely efficient in both the offline machine learning and real-time detection components. Results using the 1999 DARPA Intrusion Detection Evaluation Data Set yield a 100% detection of all applicable attacks, with extremely low false positive rate. The approach is also evaluated on real traffic captures.",space,448
10.1109/camsap.2007.4497964,filtered,2007 2nd IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing,IEEE,2007-12-14 00:00:00,ieeexplore,determining the number of propagation paths from broadband mimo measurements via bootstrapped likelihoods and the false discovery rate criterion – part i: methodology,https://ieeexplore.ieee.org/document/4497964/,"In this paper, we propose a multiple hypotheses test for determining the number of propagation paths from broadband MIMO channel measurements. For this test, maximum– likelihood (ML) estimates for propagation delay, direction of arrival, direction of departure, and Doppler shifts are required for each potential number of propagation paths. The ML-estimator is implemented via a variant of the space alternating generalized expectation-maximization (SAGE) algorithm. The proposed test is based on the Benjamini– Hochberg procedure for guaranteeing a false discovery rate and employs the simple bootstrap approach for approximating the required p-values for the multiple test. In a companion paper, we apply the proposed test to real broadband MIMO antenna array measurements and discuss its performance.",space,449
10.1109/roman.2008.4600679,filtered,RO-MAN 2008 - The 17th IEEE International Symposium on Robot and Human Interactive Communication,IEEE,2008-08-03 00:00:00,ieeexplore,development of a 3d real time gesture recognition methodology for virtual environment control,https://ieeexplore.ieee.org/document/4600679/,"In this paper, we propose a real time 3D gesture recognition system that relies on the state based approach. The novelty of this work is the introduction of probabilistic neural networks (PNNs) to characterize the uncertain boundaries of each state. The 3D gestures are modeled as a sequence of states in a configuration space; the number of states and their spatial parameters are calculated by dynamic k-means clustering on the training data of the gesture without temporal information. Gesture recognition is performed using a simple Finite State Machine (FSM), where, each state transition depends only on the output of its corresponding PNN and optionally on its time restrictions (minimum and maximum time permitted in the state). If a recognizer reaches its final state, then it could be said that a gesture is recognized. The approach is illustrated with the implementation of a real time system that recognizes the semantic meaning of seven basic gestures of the Indian Dance, the description of the system and the technologies used, it will be described in detail in the paper.",space,450
10.1109/biyomut.2010.5479790,filtered,2010 15th National Biomedical Engineering Meeting,IEEE,2010-04-24 00:00:00,ieeexplore,diffusion tensor fiber tracking based on unsupervised learning,https://ieeexplore.ieee.org/document/5479790/,"Using Hebbian learning rule and its special case Self-Organizing Map (SOM) as unsupervised learning, a solution is proposed for defining the fiber paths which is a critical problem in diffusion tensor literature, and synthetic diffusion patterns are analyzed by artificial neural network (ANN) approach. Unsupervised learning in training neural networks is a method, where network classification rules are self developed and which does not require any knowledge about the desired output. Only input data is presented to the ANN in the learning process of the network, in other words the input space of the unsupervised learning ANN is the diffusion tensor eigenvector data of each imaging matrix. The network then adjusts the weightings to determine patterns having similar characteristics and classification is done in that way. The resulting classification represents the principal diffusion direction and the weighted diffusion distribution tracked by the fibers. Verification of the application on synthetic data enabled the implementation of the method on real brain images. The aim of the proposed method is to accomplish brain fiber tracking based on learning algorithms according to the modeling studies accepted in artificial neural network literature. Implementing SOM for fiber path discrimination purposes was successful and future work relies in 3D diffusion tensor tractography.",space,451
10.1109/emctech49634.2020.9261512,filtered,2020 International Conference on Engineering Management of Communication and Technology (EMCTECH),IEEE,2020-10-22 00:00:00,ieeexplore,digital socio-political communication and its transformation in the technological evolution of artificial intelligence and neural network algorithms,https://ieeexplore.ieee.org/document/9261512/,"The study aims to analyze the specifics of determining the subjects of digital social and political communication in the context of the development of artificial intelligence technologies and neural network algorithms. The work uses a case-study design. As a research methodology, the method of critical analysis of the digital communications practice in the socio-political sphere, as well as discourse analysis of modern scientific research in the field of the development of artificial intelligence and neural network algorithms, are used. It is concluded that the implementation of technological solutions based on artificial intelligence and neural network algorithms into the processes of socio-political communications creates a problem of defining the subject of communication acts in the socio-political sphere. Society may face such communication practices in which hybrid subjectness is realized. In the conditions of hybrid subjectness, both real subjects and programmed neural network algorithms acting as real subjects (but only imitating own subjectivity) interact in common communication space. The originality of the work lies in the formulation of the author's hypothesis about the emergence of the phenomenon of hybrid subjectness in the space of modern socio-political communications and its potential in the aspect of influencing the mass consciousness of citizens.",space,452
10.1109/ijcnn48605.2020.9207522,filtered,2020 International Joint Conference on Neural Networks (IJCNN),IEEE,2020-07-24 00:00:00,ieeexplore,discrete-time lyapunov based kinematic control of robot manipulator using actor-critic framework,https://ieeexplore.ieee.org/document/9207522/,"Stability and optimality are the two foremost re-quirements for robotic systems that are deployed in critical operations and are to work for long hours or under limited energy resources. To address these, in this work we present a novel Lyapunov stability based discrete-time optimal kinematic control of a robot manipulator using actor-critic (AC) framework. The robot is actuated using optimal joint-space velocity control input to track a time-varying end-effector trajectory in its task space. In comparison to the existing near-optimal kinematic control solutions for robot manipulator under AC framework, proposed controller exhibits guaranteed analytical stability. We derive a novel critic weight update law based on Lyapunov stability, thus ensuring that the weights are updated along the negative gradient of Lyapunov function. This eventually ensures closed-loop system stability and convergence to the optimal control in discrete-time. Extensive simulations are performed on a 3D model of 6-DoF Universal Robot (UR) 10 in Gazebo, followed by implementation on real UR 10 robot manipulator to show the efficacy of the proposed scheme.",space,453
10.1109/icccnt49239.2020.9225309,filtered,"2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",IEEE,2020-07-03 00:00:00,ieeexplore,distributed artificial neural network model for neutron flux mapping in nuclear reactors,https://ieeexplore.ieee.org/document/9225309/,"Neutron flux distribution inside the core of large size nuclear reactors is a function of space and time. An online Flux Mapping System (FMS) is needed to monitor the core during the reactor operation. FMS estimates the core flux distribution from the measurements of few in-core detectors using an appropriate algorithm. Here, a Distributed Artificial Neural Network (D-ANN) model is developed using parallel-forward multi-layer perceptron architecture to capture the spatial core flux variation in a nuclear reactor. The proposed D-ANN model is tested with simulated test case data of Advanced Heavy Water Reactor (AHWR) for multiple operating conditions of the reactor. The model estimates the neutron flux in all horizontal mesh locations (2-D) from the multiple networks distributed spatially across AHWR core. Estimation error using the proposed D-ANN model is found to be significantly lower than that with lumped ANN model. Validation exercises establish that this D-ANN model could effectively capture the spatial variations in the reactor core and therefore could be utilized for efficient flux mapping. The real time implementation of D-ANN based flux mapping method is also proposed.",space,454
10.1109/icmla.2013.17,filtered,2013 12th International Conference on Machine Learning and Applications,IEEE,2013-12-07 00:00:00,ieeexplore,distributed kernel matrix approximation and implementation using message passing interface,https://ieeexplore.ieee.org/document/6784587/,"We propose a distributed method to compute similarity (also known as kernel and Gram) matrices used in various kernel-based machine learning algorithms. Current methods for computing similarity matrices have quadratic time and space complexities, which make them not scalable to large-scale data sets. To reduce these quadratic complexities, the proposed method first partitions the data into smaller subsets using various families of locality sensitive hashing, including random project and spectral hashing. Then, the method computes the similarity values among points in the smaller subsets to result in approximated similarity matrices. We analytically show that the time and space complexities of the proposed method are sub quadratic. We implemented the proposed method using the Message Passing Interface (MPI) framework and ran it on a cluster. Our results with real large-scale data sets show that the proposed method does not significantly impact the accuracy of the computed similarity matrices and it achieves substantial savings in running time and memory requirements.",space,455
10.1109/icton.2019.8840514,filtered,2019 21st International Conference on Transparent Optical Networks (ICTON),IEEE,2019-07-13 00:00:00,ieeexplore,distributed and centralized options for self-learning,https://ieeexplore.ieee.org/document/8840514/,"In general, the availability of enough real data from real fog computing scenarios to produce accurate Machine Learning (ML) models is rarely ensured since new equipment, techniques, etc., are continuously being deployed in the field. Although an option is to generate data from simulation and lab experiments, such data could not cover the whole features space, which would translate into ML models inaccuracies. In this paper, we propose a self-learning approach to facilitate ML deployment in real scenarios. A dataset for ML training can be initially populated based on the results from simulation and lab experiments and once ML models are generated, ML re-training can be performed after inaccuracies are detected to improve their precision. Illustrative numerical results show the benefits from the proposed self-learning approach for two general use cases of regression and classification.",space,456
10.1109/itsc.2010.5624970,filtered,13th International IEEE Conference on Intelligent Transportation Systems,IEEE,2010-09-22 00:00:00,ieeexplore,distributed evolutionary estimation of dynamic traffic origin/destination,https://ieeexplore.ieee.org/document/5624970/,"This paper focuses on updating time varying demand matrices using real-time information. An Artificial Intelligence technique based on Distributed Evolutionary Algorithms (DEA), which is capable to exploit the use of grid computing, is developed. This EA-based demand estimation framework is implemented into a model that we call DynODE (Dynyamic O/D Estimator). DynODE provides a direct way of fusing information of varying types, with different levels of accuracy and from different sensors/sources. DynODE is integrated with an existing Dynamic Traffic Assignment platform (i.e. Dynasmart-P) and is evaluated on a medium size network for various search space sizes and for different quality of the apriori matrix. The obtained results, in terms of replicating observed vehicle counts and the closeness to the real demand, are promising and point to the robustness of the gradient-free framework and its high performance irrespective of the quality of the apriori travel information. The use of Distributed EA is also shown to provide good results within fast computing speeds.",space,457
10.23919/acc.2017.7963641,filtered,2017 American Control Conference (ACC),IEEE,2017-05-26 00:00:00,ieeexplore,distributed mean-field-type filter for vehicle tracking,https://ieeexplore.ieee.org/document/7963641/,"Particle filter is an effective tool for vehicle tracking. However, we need to maintain a large number of particles to keep a reasonable tracking accuracy for multi-target tracking in large scale state space. This paper proposes a new distributed mean-field-type filter to handle those noisy, partial-observed and high-dimensional data. The state space is decomposed and the particles are deployed locally and updated independently in the simplified subspaces. The filtering framework contains four operations: sampling, prediction, decomposition and correction. A mean-field term is included in the system dynamic so that the prediction is based on the previous state as well as its statistic distribution, which is estimated by a multi-frame learning procedure. The experiment on real data shows that our approach can achieve accurate tracking results with a small number of particles.",space,458
10.1109/asonam.2018.8508284,filtered,2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),IEEE,2018-08-31 00:00:00,ieeexplore,droideye: fortifying security of learning-based classifier against adversarial android malware attacks,https://ieeexplore.ieee.org/document/8508284/,"To combat the evolving Android malware attacks, systems using machine learning techniques have been successfully deployed for Android malware detection. In these systems, based on different feature representations, various kinds of classifiers are constructed to detect Android malware. Unfortunately, as classifiers become more widely deployed, the incentive for defeating them increases. In this paper, we first extract a set of features from the Android applications (apps) and represent them as binary feature vectors; with these inputs, we then explore the security of a generic learning-based classifier for Android malware detection in the presence of adversaries. To harden the evasion, we first present count featurization to transform the binary feature space into continuous probabilities encoding the distribution in each class (either benign or malicious). To improve the system security while not compromising the detection accuracy, we further introduce softmax function with adversarial parameter to find the best trade-off between security and accuracy for the classifier. Accordingly, we develop a system named DroidEye which integrates our proposed method for Android malware detection. Comprehensive experiments on the real sample collection from Comodo Cloud Security Center are conducted to validate the effectiveness of DroidEye against adversarial Android malware attacks. Our proposed secure-learning paradigm is also applicable for other detection tasks, such as spammer detection in social media.",space,459
10.1109/csnet47905.2019.9108976,filtered,2019 3rd Cyber Security in Networking Conference (CSNet),IEEE,2019-10-25 00:00:00,ieeexplore,dynamic security management driven by situations: an exploratory analysis of logs for the identification of security situations,https://ieeexplore.ieee.org/document/9108976/,"Situation awareness consists of ""the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future"". Being aware of the security situation is then mandatory to launch proper security reactions in response to cybersecurity attacks. Security Incident and Event Management solutions are deployed within Security Operation Centers. Some vendors propose machine learning based approaches to detect intrusions by analysing networks behaviours. But cyberattacks like Wannacry and NotPetya, which shut down hundreds of thousands of computers, demonstrated that networks monitoring and surveillance solutions remain insufficient. Detecting these complex attacks (a.k.a. Advanced Persistent Threats) requires security administrators to retain a large number of logs just in case problems are detected and involve the investigation of past security events. This approach generates massive data that have to be analysed at the right time in order to detect any accidental or caused incident. In the same time, security administrators are not yet seasoned to such a task and lack the desired skills in data science. As a consequence, a large amount of data is available and still remains unexplored which leaves number of indicators of compromise under the radar. Building on the concept of situation awareness, we developed a situation-driven framework, called dynSMAUG, for dynamic security management. This approach simplifies the security management of dynamic systems and allows the specification of security policies at a high-level of abstraction (close to security requirements). This invited paper aims at exposing real security situations elicitation, coming from networks security experts, and showing the results of exploratory analysis techniques using complex event processing techniques to identify and extract security situations from a large volume of logs. The results contributed to the extension of the dynSMAUG solution.",space,460
10.1109/secon52354.2021.9491609,filtered,"2021 18th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)",IEEE,2021-07-09 00:00:00,ieeexplore,efcam: configuration-adaptive fog-assisted wireless cameras with reinforcement learning,https://ieeexplore.ieee.org/document/9491609/,"Visual sensing has been increasingly employed in industrial processes. This paper presents the design and implementation of an industrial wireless camera system, namely, EFCam, which uses low-power wireless communications and edge-fog computing to achieve cordless and energy-efficient visual sensing. The camera performs image pre-processing (i.e., compression or feature extraction) and transmits the data to a resourceful fog node for advanced processing using deep models. EFCam admits dynamic configurations of several parameters that form a configuration space. It aims to adapt the configuration to maintain desired visual sensing performance of the deep model at the fog node with minimum energy consumption of the camera in image capture, pre-processing, and data communications, under dynamic variations of application requirement and wireless channel conditions. However, the adaptation is challenging due primarily to the complex relationships among the involved factors. To address the complexity, we apply deep reinforcement learning to learn the optimal adaptation policy. Extensive evaluation based on trace-driven simulations and experiments show that EFCam complies with the accuracy and latency requirements with lower energy consumption for a real industrial product object tracking application, compared with four baseline approaches incorporating hysteresis-based adaptation.",space,461
10.1109/dac.2005.193910,filtered,"Proceedings. 42nd Design Automation Conference, 2005.",IEEE,2005-06-17 00:00:00,ieeexplore,efficient sat solving: beyond supercubes,https://ieeexplore.ieee.org/document/1510430/,"SAT (Boolean satisfiability) has become the primary Boolean reasoning engine for many EDA applications, so the efficiency of SAT solving is of great practical importance. Recently, Goldberg et al introduced supercubing, a different approach to search-space pruning, based on a theory that unifies many existing methods. Their implementation reduced the number of decisions, but no speedup was obtained. In this paper, we generalize beyond supercubes, creating a theory we call B-cubing, and show how to implement B-cubing in a practical solver. On extensive benchmark runs, using both real problems and synthetic benchmarks, the new technique is competitive on average with the newest version of ZChaff, is much faster in some cases, and is more robust.",space,462
10.1109/iccd.2001.955078,filtered,Proceedings 2001 IEEE International Conference on Computer Design: VLSI in Computers and Processors. ICCD 2001,IEEE,2001-09-26 00:00:00,ieeexplore,efficient function approximation for embedded and asic applications,https://ieeexplore.ieee.org/document/955078/,"In embedded systems and application specific integrated circuits (ASICs) that typically do not have a floating-point processor, measured data or function-sampled data is commonly described by means of an analytic function derived using standard numerical methods. The resultant errors are not caused by rounding the coefficients but by translating a real solution to a restricted fixed-point environment. A genetic algorithm has been constructed that discovers a superior piecewise polynomial approximation with coefficients restricted to the integer target space. This paper discusses the problem being solved and presents an overview of the implemented solution.",space,463
10.1109/bdcloud.2015.51,filtered,2015 IEEE Fifth International Conference on Big Data and Cloud Computing,IEEE,2015-08-28 00:00:00,ieeexplore,efficient k-nearest neighbors search in high dimensions using mapreduce,https://ieeexplore.ieee.org/document/7310711/,"Finding the k-Nearest Neighbors (kNN) of a query object for a given dataset S is a primitive operation in many application domains. kNN search is very costly, especially many applications witness a quick increase in the amount and dimension of data to be processed. Locality sensitive hashing (LSH) has become a very popular method for this problem. However, most such methods can't obtain good performance in terms of search quality, search efficiency and space cost at the same time, such as RankReduce, which gains good search efficiency at the sacrifice of the search quality. Motivated by these, we propose a novel LSH-based inverted index scheme and design an efficient search algorithm, called H-c2kNN, which enables fast high-dimensional kNN search with excellent quality and low space cost. For efficiency and scalability concerns, we implemented our proposed approach to solve the kNN search in high dimensional space using MapReduce, which is a well-known framework for data-intensive applications and conducted extensive experiments to evaluate our proposed approach using both synthetic and real datasets. The results show that our proposed approach outperforms baseline methods in high dimensional space.",space,464
10.1109/nlpke.2003.1275966,filtered,"International Conference on Natural Language Processing and Knowledge Engineering, 2003. Proceedings. 2003",IEEE,2003-10-29 00:00:00,ieeexplore,efficient mining of textual associations,https://ieeexplore.ieee.org/document/1275966/,"We describe an efficient implementation for mining textual associations from text corpora. In order to tackle real world applications, efficient algorithms and data structures are needed to manage, in reasonable time and space, the overgrowing volume of text data. For that purpose, we introduce a global architecture based on masks, suffix arrays and multidimensional arrays to implement the SENTA extractor (Dias, 2002). In particular, SENTA has shown great flexibility and accuracy for mining textual associations such as collocations, cognates, morphemes and chunks. Our solution shows O(h(F) N log N) time complexity and O(N) space complexity where N is the size of the corpus and h(F) is a function of the context window size.",space,465
10.1109/icnc.2010.5584314,filtered,2010 Sixth International Conference on Natural Computation,IEEE,2010-08-12 00:00:00,ieeexplore,emotion generation for virtual human using cognitive map,https://ieeexplore.ieee.org/document/5584314/,"Affective computing is an indispensable aspect in harmonious human-computer interaction and artificial intelligence. Making computers have the ability of generating emotions is a challenging task of affective computing. The paper first introduces the basic affective elements, and the representation of affections in a computer. An emotion generation model using Cognitive Map is proposed. The model is used to generate emotion based on the evaluation of the overall influences of the mood, the personality, the previous emotion and the external stimulations. Then the paper describes a method to build a mapping from the emotion space to the facial expression space with a competitive network, and the implementation of facial expression animation. Finally, the paper constructs an intelligent virtual human system with facial expression, voice and vision communication. Experimental results show that the emotion generation model using Cognitive Map can produce realistic emotions similar to those of a real human.",space,466
10.1109/ecctd.2017.8093280,filtered,2017 European Conference on Circuit Theory and Design (ECCTD),IEEE,2017-09-06 00:00:00,ieeexplore,emulating cnn with template learning on fpga,https://ieeexplore.ieee.org/document/8093280/,"A 2-D Cellular Neural Network structure with space invariant neural weights is widely used in image processing applications. Recent advances VLSI technology appears to be very promising to use discrete time CNNs for real time vision applications. In this paper, a system-on-chip implementation which consists of a new CNN emulator design and a processor which performs template learning algorithm is shown. SoC design is programmed to perform a sequential CNN operations on different input and state images with different templates. Furthermore, the presented SoC design allows that templates can be updated by a learning algoritm in run time. SoC design is realised on a target FPGA. Test results on FPGA and MATLAB are presented and compared with structural similarity map.",space,467
10.1109/icip.2017.8296489,filtered,2017 IEEE International Conference on Image Processing (ICIP),IEEE,2017-09-20 00:00:00,ieeexplore,encyclopedia enhanced semantic embedding for zero-shot learning,https://ieeexplore.ieee.org/document/8296489/,"There are tremendous object categories in the real world besides those in image datasets. Zero-shot learning aims to recognize image categories which are unseen in the training set. A large number of previous zero-shot learning models use word vectors of the class labels directly as category prototypes in the semantic embedding space. But word vectors cannot obtain the global knowledge of an image category sufficiently. In this paper, we propose a new encyclopedia enhanced semantic embedding model to promote the discriminative capability of word vector prototypes with the global knowledge of each image category. The proposed model extracts the TF-IDF key words from encyclopedia articles to acquire the global knowledge of each category. The convex combination of the key words' word vectors acts as the prototypes of the object categories. The prototypes of seen and unseen classes build up the embedding space where the nearest neighbour search is implemented to recognize the unseen images. The experiments show that the proposed method achieves the state-of-the-art performance on the challenging ImageNet Fall 2011 1k2hop dataset.",space,468
10.1109/iadcc.2015.7154739,filtered,2015 IEEE International Advance Computing Conference (IACC),IEEE,2015-06-13 00:00:00,ieeexplore,enhanced smote algorithm for classification of imbalanced big-data using random forest,https://ieeexplore.ieee.org/document/7154739/,"In the era of big data, the applications generating tremendous amount of data are becoming the main focus of attention as the wide increment of data generation and storage that has taken place in the last few years. This scenario is challenging for data mining techniques which are not arrogated to the new space and time requirements. In many of the real world applications, classification of imbalanced data-sets is the point of attraction. Most of the classification methods focused on two-class imbalanced problem. So, it is necessary to solve multi-class imbalanced problem, which exist in real-world domains. In the proposed work, we introduced a methodology for classification of multi-class imbalanced data. This methodology consists of two steps: In first step we used Binarization techniques (OVA and OVO) for decomposing original dataset into subsets of binary classes. In second step, the SMOTE algorithm is applied against each subset of imbalanced binary class in order to get balanced data. Finally, to achieve classification goal Random Forest (RF) classifier is used. Specifically, oversampling technique is adapted to big data using MapReduce so that this technique is able to handle as large data-set as needed. An experimental study is carried out to evaluate the performance of proposed method. For experimental analysis, we have used different datasets from UCI repository and the proposed system is implemented on Apache Hadoop and Apache Spark platform. The results obtained shows that proposed method outperforms over other methods.",space,469
10.1109/icra40945.2020.9197510,filtered,2020 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2020-08-31 00:00:00,ieeexplore,episodic koopman learning of nonlinear robot dynamics with application to fast multirotor landing,https://ieeexplore.ieee.org/document/9197510/,"This paper presents a novel episodic method to learn a robot's nonlinear dynamics model and an increasingly optimal control sequence for a set of tasks. The method is based on the Koopman operator approach to nonlinear dynamical systems analysis, which models the flow of observables in a function space, rather than a flow in a state space. Practically, this method estimates a nonlinear diffeomorphism that lifts the dynamics to a higher dimensional space where they are linear. Efficient Model Predictive Control methods can then be applied to the lifted model. This approach allows for real time implementation in on-board hardware, with rigorous incorporation of both input and state constraints during learning. We demonstrate the method in a real-time implementation of fast multirotor landing, where the nonlinear ground effect is learned and used to improve landing speed and quality.",space,470
10.1109/cvprw50498.2020.00337,filtered,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),IEEE,2020-06-19 00:00:00,ieeexplore,evading deepfake-image detectors with white- and black-box attacks,https://ieeexplore.ieee.org/document/9150604/,"It is now possible to synthesize highly realistic images of people who do not exist. Such content has, for example, been implicated in the creation of fraudulent socialmedia profiles responsible for dis-information campaigns. Significant efforts are, therefore, being deployed to detect synthetically-generated content. One popular forensic approach trains a neural network to distinguish real from synthetic content.We show that such forensic classifiers are vulnerable to a range of attacks that reduce the classifier to near- 0% accuracy. We develop five attack case studies on a state- of-the-art classifier that achieves an area under the ROC curve (AUC) of 0.95 on almost all existing image generators, when only trained on one generator. With full access to the classifier, we can flip the lowest bit of each pixel in an image to reduce the classifier's AUC to 0.0005; perturb 1% of the image area to reduce the classifier's AUC to 0.08; or add a single noise pattern in the synthesizer's latent space to reduce the classifier's AUC to 0.17. We also develop a black-box attack that, with no access to the target classifier, reduces the AUC to 0.22. These attacks reveal significant vulnerabilities of certain image-forensic classifiers.",space,471
10.1109/dcoss.2013.49,filtered,2013 IEEE International Conference on Distributed Computing in Sensor Systems,IEEE,2013-05-23 00:00:00,ieeexplore,event prediction and modeling of variable rate sampled data using dynamic bayesian networks,https://ieeexplore.ieee.org/document/6569444/,"Event detection is an important issue in sensor networks for a variety of real-world applications. Many events in real world are often correlated on a complex spatio-temporal level whereby they are manifested via observations over time and space proximities. In order to predict events in these spatiotemporal observations, the prediction model should be capable of modeling codependencies between data observed at various locations. In this paper, we propose a Dynamic Bayesian Network (DBN) with such spatio-temporal event prediction capability in sensor networks deployed for sensing environmental data. More specifically, we develop a DBN model with mixture distribution and a novel learning algorithm, for water level data prediction for different canals, using rainfall data at multiple locations. Experiments on real data demonstrates that our model and training method can provide accurate event prediction in real time for spatio-temporal sensor networks.",space,472
10.1109/sieds49339.2020.9106581,filtered,2020 Systems and Information Engineering Design Symposium (SIEDS),IEEE,2020-04-24 00:00:00,ieeexplore,"explorer51 – indoor mapping, discovery, and navigation for an autonomous mobile robot",https://ieeexplore.ieee.org/document/9106581/,"The nexus of robotics, autonomous systems, and artificial intelligence (AI) has the potential to change the nature of human guided exploration of indoor and outdoor spaces. Such autonomous mobile robots can be incorporated into a variety of applications, ranging from logistics and maintenance, to intelligence gathering, surveillance, and reconnaissance (ISR). One such example is that of a tele-operator using the robot to generate a map of the inside of a building while discovering and tagging the objects of interest. During this process, the tele-operator can also assign an area for the robot to navigate autonomously or return to a previously marked area/object of interest. Search and rescue and ISR abilities could be immensely improved with such capabilities. The goal of this research is to prototype and demonstrate the above autonomous capabilities in a mobile ground robot called Explorer51. Objectives include: (i) enabling an operator to drive the robot non-line of sight to explore a space by incorporating a first-person view (FPV) system to stream data from the robot to the base station; (ii) implementing automatic collision avoidance to prevent the operator from running the robot into obstacles; (iii) creating and saving 2D and 3D maps of the space in real time by using a 2D laser scanner, tracking, and depth/RGB cameras; (iv) locating and tagging objects of interest as waypoints within the map; (v) autonomously navigate within the map to reach a chosen waypoint.To accomplish these goals, we are using the AION Robotics R1 Unmanned Ground Vehicle (UGV) rover as the platform for Explorer51 to demonstrate the autonomous features. The rover runs the Robot Operating System (ROS) onboard an NVIDIA Jetson TX2 board, connected to a Pixhawk controller. Sensors include a 2D scanning LiDAR, depth camera, tracking camera, and an IMU. Using existing ROS packages such as Cartographer and TEB planner, we plan to implement ROS nodes for accomplishing these tasks. We plan to extend the mapping ability of the rover using Visual Inertial Odometry (VIO) using the cameras. In addition, we will explore the implementation of additional features such as autonomous target identification, waypoint marking, collision avoidance, and iterative trajectory optimization. The project will culminate in a series of demonstrations to showcase the autonomous navigation, and tele-operation abilities of the robot. Success will be evaluated based on ease of use by the tele-operator, collision avoidance ability, autonomous waypoint navigation accuracy, and robust map creation at high driving speeds.",space,473
10.1109/sieds52267.2021.9483745,filtered,2021 Systems and Information Engineering Design Symposium (SIEDS),IEEE,2021-04-30 00:00:00,ieeexplore,extensions and application of the robust shared response model to electroencephalography data for enhancing brain-computer interface systems,https://ieeexplore.ieee.org/document/9483745/,"Brain Computer Interfaces (BCI) decode electroencephalography (EEG) data collected from the human brain to predict subsequent behavior. While this technology has promising applications, successfully implementing a model is challenging. The typical BCI control application requires many hours of training data from each individual to make predictions of intended activity specific to that individual. Moreover, there are individual differences in the organization of brain activity and low signal-to-noise ratios in noninvasive measurement techniques such as EEG. There is a fundamental bias-variance trade-off between developing a single model for all human brains vs. an individual model for each specific human brain. The Robust Shared Response Model (RSRM) attempts to resolve this tradeoff by leveraging both the homogeneity and heterogeneity of brain signals across people. RSRM extracts components that are common and shared across individual brains, while simultaneously learning unique representations between individual brains. By learning a latent shared space in conjunction with subject-specific representations, RSRM tends to result in better predictive performance on functional magnetic resonance imaging (fMRI) data relative to other common dimension reduction techniques. To our knowledge, we are the first research team attempting to expand the domain of RSRM by applying this technique to controlled experimental EEG data in a BCI setting. Using the openly available Motor Movement/ Imagery dataset, the decoding accuracy of RSRM exceeded models whose input was reduced by Principal Component Analysis (PCA), Independent Component Analysis (ICA), and subject-specific PCA. The results of our experiments suggest that RSRM can recover distributed latent brain signals and improve decoding accuracy of BCI tasks when dimension reduction is implemented as a feature engineering step. Future directions of this work include augmenting state-of-the art BCI with efficient reduced representations extracted by RSRM. This could enhance the utility of BCI technology in the real world. Furthermore, RSRM could have wide-ranging applications across other machine-learning applications that require classification of naturalistic data using reduced representations.",space,474
10.1109/itsc48978.2021.9564641,filtered,2021 IEEE International Intelligent Transportation Systems Conference (ITSC),IEEE,2021-09-22 00:00:00,ieeexplore,fast collision prediction for autonomous vehicles using a stochastic dynamics model,https://ieeexplore.ieee.org/document/9564641/,"Autonomous Vehicles (AVs) have the potential to save millions of lives by reducing traffic deaths and accidents. However, despite recent advances, AVs have not met safety standard expectations for a variety of reasons, key among them being the difficulty in certifying AV safety. The development of model-based methods is essential for achieving more explainable tools that provide better safety assurances, in contrast to popular data-dependent end-to-end learning methods. This paper introduces a model-based collision prediction method that uses discretized Gaussian processes for future vehicle position estimation. It can incorporate road layout information, statistical agent dynamics, and be coupled with any trajectory prediction module. The discretization of the space together with a single Normal random variable for each vehicle trajectory allows fast and efficient computation for real-time deployment and computational intensive applications, such as simulation and training of Deep Learning and Reinforcement Learning models. The method can be applied to various scenarios by the adjustment of the model parameters that control dynamics uncertainty. Two scenarios extracted from real data are used as case studies.",space,475
10.1109/ictai.2009.60,filtered,2009 21st IEEE International Conference on Tools with Artificial Intelligence,IEEE,2009-11-04 00:00:00,ieeexplore,flockstream: a bio-inspired algorithm for clustering evolving data streams,https://ieeexplore.ieee.org/document/5364942/,"Existing density-based data stream clustering algorithms use a two-phase scheme approach consisting of an online phase, in which raw data is processed to gather summary statistics, and an offline phase that generates the clusters by using the summary data. In this paper we propose a data stream clustering method based on a multi-agent system that uses a decentralized bottom-up self-organizing strategy to group similar data points. Data points are associated with agents and deployed onto a 2D space, to work simultaneously by applying a heuristic strategy based on a bio-inspired model, known as flocking model. Agents move onto the space for a fixed time and, when they encounter other agents into a predefined visibility range, they can decide to form a flock if they are similar. Flocks can join to form swarms of similar groups. This strategy allows to merge the two phases of density-based approaches and thus to avoid the offline cluster computation, since a swarm represents a cluster. Experimental results show the capability of the bio-inspired approach to obtain very good results on real and synthetic data sets.",space,476
10.1109/humanoids.2014.7041373,filtered,2014 IEEE-RAS International Conference on Humanoid Robots,IEEE,2014-11-20 00:00:00,ieeexplore,footstep planning on uneven terrain with mixed-integer convex optimization,https://ieeexplore.ieee.org/document/7041373/,"We present a new method for planning footstep placements for a robot walking on uneven terrain with obstacles, using a mixed-integer quadratically-constrained quadratic program (MIQCQP). Our approach is unique in that it handles obstacle avoidance, kinematic reachability, and rotation of footstep placements, which typically have required non-convex constraints, in a single mixed-integer optimization that can be efficiently solved to its global optimum. Reachability is enforced through a convex inner approximation of the reachable space for the robot's feet. Rotation of the footsteps is handled by a piecewise linear approximation of sine and cosine, designed to ensure that the approximation never overestimates the robot's reachability. Obstacle avoidance is ensured by decomposing the environment into convex regions of obstacle-free configuration space and assigning each footstep to one such safe region. We demonstrate this technique in simple 2D and 3D environments and with real environments sensed by a humanoid robot. We also discuss computational performance of the algorithm, which is currently capable of planning short sequences of a few steps in under one second or longer sequences of 10-30 footsteps in tens of seconds to minutes on common laptop computer hardware. Our implementation is available within the Drake MATLAB toolbox [1].",space,477
10.1109/iros.2016.7759701,filtered,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),IEEE,2016-10-14 00:00:00,ieeexplore,from indoor gis maps to path planning for autonomous wheelchairs,https://ieeexplore.ieee.org/document/7759701/,"This work focuses on how to compute trajectories for an autonomous wheelchair based on indoor GIS maps, in particular on IndoorGML maps, which set the standard in this context. Good wheelchair trajectories are safe and comfortable for the user and the people sharing the space with him, turn gently, are high legible, and smooth (at least G<sup>2</sup> continuos). We derive a navigation graph from a given IndoorGML map. We define and solve an optimization problem to find the desired path: given a succession of cells to traverse, the path corresponds to the best composite Bézier trajectory for the wheelchair. We discuss a related multi-objective path planning problem. Experimental results and an implementation on real robots show the planner performance.",space,478
10.1109/fuzz-ieee.2016.7737911,filtered,2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),IEEE,2016-07-29 00:00:00,ieeexplore,fuzzyfication of principle component analysis for data dimensionalty reduction,https://ieeexplore.ieee.org/document/7737911/,"Principal component analysis (PCA) extracts small uncorrelated data from original high dimensional data space and is widely used for data analysis. The methodology of classical PCA is based on orthogonal projection defined in convex vector space. Thus, a norm between two projected vectors is unavoidably smaller than the norm between any two objects before implementation of PCA. Due to this, in some cases when the PCA cannot capture the data structure, its implementation does not necessarily confirm the real similarity of data in the higher dimensional space, making the results unacceptable. In order to avoid this problem for the purposes of high dimensional data clustering, we propose new Fuzzy PCA (FPCA) algorithm. The novelty is in the extracted similarity structures of objects in high-dimensional space and dissimilarities between objects based on their cluster structure and dimension when the algorithm is implemented in conjunction with known fuzzy clustering algorithms as FCM, GK or GG algorithms. This is done by using the fuzzy membership functions and modification of the classical PCA approach by considering the similarity structures during the construction of projections in smaller dimensional space. The effectiveness of the proposed algorithm is tested on several benchmark data sets. We also evaluate the clustering efficiency by implementing validation measures.",space,479
10.1109/indin.2009.5195905,filtered,2009 7th IEEE International Conference on Industrial Informatics,IEEE,2009-06-26 00:00:00,ieeexplore,gps and sonar based area mapping and navigation by mobile robots,https://ieeexplore.ieee.org/document/5195905/,"In this paper, we have presented a GPS and sonar based area mapping and navigation scheme for a mobile robot. A mapping is achieved between the GPS space and the world coordinates of the mobile robot which enables us to generate direct motion commands for it. This mapping enables the robot to navigate among different GPS locations within the mapped area. The GPS data is extracted online to get the latitude and longitude information of a particular location. In the training phase, a 2-D axis transformation is used to relate local robot frame with the robot world coordinates and then the actual world coordinates are mapped from the GPS data using a RBFN (radial basis function network) based Neural Network. In the second phase, direct GPS data is used to get the mapping into the world coordinates of mobile robot using the trained network and the motion commands are generated accordingly. The physical placement of sonar devices, their ranging limits and beam opening angles are considered during navigation for possible collision detection and obstacle avoidance. This scheme is successfully implemented in real time with Pioneer mobile robot from ActivMedia Robotics and GPS receiver. The scheme is also tested in the simulation to justify its application in the real world.",space,480
10.1109/prdc50213.2020.00018,filtered,2020 IEEE 25th Pacific Rim International Symposium on Dependable Computing (PRDC),IEEE,2020-12-04 00:00:00,ieeexplore,generative deep learning for internet of things network traffic generation,https://ieeexplore.ieee.org/document/9320384/,"The rapid development of the Internet of Things (IoT) has prompted a recent interest into realistic IoT network traffic generation. Security practitioners need IoT network traffic data to develop and assess network-based intrusion detection systems (NIDS). Emulating realistic network traffic will avoid the costly physical deployment of thousands of smart devices. From an attacker's perspective, generating network traffic that mimics the legitimate behavior of a device can be useful to evade NIDS. As network traffic data consist of sequences of packets, the problem is similar to the generation of sequences of categorical data, like word by word text generation. Many solutions in the field of natural language processing have been proposed to adapt a Generative Adversarial Network (GAN) to generate sequences of categorical data. In this paper, we propose to combine an autoencoder with a GAN to generate sequences of packet sizes that correspond to bidirectional flows. First, the autoencoder is trained to learn a latent representation of the real sequences of packet sizes. A GAN is then trained on the latent space, to learn to generate latent vectors that can be decoded into realistic sequences. For experimental purposes, bidirectional flows produced by a Google Home Mini are used, and the autoencoder is combined with a Wassertein GAN. Comparison of different network characteristics shows that our proposed approach is able to generate sequences of packet sizes that behave closely to real bidirectional flows. We also show that the synthetic bidirectional flows are close enough to the real ones that they can fool anomaly detectors into labeling them as legitimate.",space,481
10.1109/isspit.2011.6151580,filtered,2011 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT),IEEE,2011-12-17 00:00:00,ieeexplore,genetic algorithm implementation of multi-user detection in sdma-ofdm systems,https://ieeexplore.ieee.org/document/6151580/,Number of supported users in the orthogonal frequency division multiplexing (OFDM) systems can be increased considerably using powerful multi-user detector (MUD) combined with space division multiple access (SDMA) techniques. This paper presents the results of implementing MUD in SDMA-OFDM systems based on an advanced genetic-algorithm (GA) optimization tool. The hardware implementation is performed using Field Programmable Gate array (FPGA) devices which allow the real time performance of the proposed tool. Results show that the GA scheme enhances the performance and provides BER near to that attained using maximum likelihood (ML) detector at considerably lower computation complexity. Investigation of the GA population size is presented and FPGA implementation is described based on the shared memory approach.,space,482
10.1109/cogann.1992.273945,filtered,[Proceedings] COGANN-92: International Workshop on Combinations of Genetic Algorithms and Neural Networks,IEEE,1992-06-06 00:00:00,ieeexplore,genetic sparse distributed memory,https://ieeexplore.ieee.org/document/273945/,"Kanerva's 'sparse distributed memory' (SDM) is a type of self-organizing neural network which is able to extract a statistical summary from large volumes of data as it is being processed online. Genetic algorithms have been used to optimize the 'location address space' which corresponds to the mapping from the input layer to the hidden units in the neural network implementation of the sparse distributed memory. If treated as a global optimization problem, the genetic algorithm will attempt to optimize the sparse distributed memory so as to extract a single best statistical predictor. However, the real objective is to obtain not just a single global optimum, but to extract information about as many local optima as possible, since each local optimum in this particular definition of the search space represents a different and distinct data pattern that correlates with some output in which we may be interested. The implementation details of a genetic sparse distributed memory as well as modified algorithm designed to deal better with multiple data patterns are presented.&lt;<ETX>&gt;</ETX>",space,483
10.1109/icme51207.2021.9428448,filtered,2021 IEEE International Conference on Multimedia and Expo (ICME),IEEE,2021-07-09 00:00:00,ieeexplore,graph attention neural network for image restoration,https://ieeexplore.ieee.org/document/9428448/,"Self-similarity underpins modern non-local attention mechanism, which has been verified to be an effective prior for image restoration. However, most existing non-local attention restorers are implemented based on pixels, which tend to be biased due to image degeneration. Furthermore, most non-local methods for image restoration are restricted to construct fully-connected correlations in a regular Euclidean space so that all features within the search region have to participate in the feature aggregation process, no matter how similar the key feature is to the query feature. To rectify these weaknesses, in this paper, we propose a novel graph attention network for image restoration, dubbed GATIR, which establishes the non-local attention based on feature patches and utilizes the graph convolution to perform feature aggregation selectively in a non-Euclidean space. Experimental results demonstrate that our GATIR can achieve state-of-the-art performance on synthetic image denoising, real image denoising, image demosaicing, and compression artifact reduction tasks.",space,484
10.1109/asp-dac47756.2020.9045442,filtered,2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC),IEEE,2020-01-16 00:00:00,ieeexplore,hl-pow: a learning-based power modeling framework for high-level synthesis,https://ieeexplore.ieee.org/document/9045442/,"High-level synthesis (HLS) enables designers to customize hardware designs efficiently. However, it is still challenging to foresee the correlation between power consumption and HLS-based applications at an early design stage. To overcome this problem, we introduce HL-Pow, a power modeling framework for FPGA HLS based on state-of-the-art machine learning techniques. HL-Pow incorporates an automated feature construction flow to efficiently identify and extract features that exert a major influence on power consumption, simply based upon HLS results, and a modeling flow that can build an accurate and generic power model applicable to a variety of designs with HLS. By using HL-Pow, the power evaluation process for FPGA designs can be significantly expedited because the power inference of HL-Pow is established on HLS instead of the time-consuming register-transfer level (RTL) implementation flow. Experimental results demonstrate that HL-Pow can achieve accurate power modeling that is only 4.67% (24.02 mW) away from onboard power measurement. To further facilitate power-oriented optimizations, we describe a novel design space exploration (DSE) algorithm built on top of HL-Pow to trade off between latency and power consumption. This algorithm can reach a close approximation of the real Pareto frontier while only requiring running HLS flow for 20% of design points in the entire design space.",space,485
10.1109/icdmw.2009.69,filtered,2009 IEEE International Conference on Data Mining Workshops,IEEE,2009-12-06 00:00:00,ieeexplore,hoct: a highly scalable algorithm for training linear crf on modern hardware,https://ieeexplore.ieee.org/document/5360418/,"Conditional Random Fields (CRFs) are widely used in machine learning and natural language processing fields. A number of methods have been developed for CRF training. However, even with state-of-the-art algorithms, the training of CRF is still very time and space consuming. This make it infeasible to use CRFs in large-scale data analysis tasks. This paper proposes an efficient algorithm, HOCT, for CRF training on modern computer architectures. First, software prefetching techniques are utilized to hide cache miss latency. Second, we exploit SIMD to process data in parallel. Third, when dealing with large data sets, we let HOCT instead of operating system to manage swapping operations. Our experiments on various real data sets show that HOCT yields a fourfold speedup when the data can fit in memory, and over a 30-fold speedup when the memory requirement exceeds the physical memory.",space,486
10.1109/ijcnn.2006.247206,filtered,The 2006 IEEE International Joint Conference on Neural Network Proceedings,IEEE,2006-07-21 00:00:00,ieeexplore,handwritten signature authentication using artificial neural networks,https://ieeexplore.ieee.org/document/1716797/,"The main goal of this paper is to describe our research and implementation of a handwritten signature authentication system based on artificial neural networks. In this system the authentication process occurs in the following way: firstly, the users' signatures are read using a pen tablet device and then stored; after that some adjustments in position and scale are accomplished; representative signature features are extracted; the input space dimensionality is reduced using principal component analysis; and finally, the users' signatures are classified as authentic or not, through the use of a neural network. Several experiments were accomplished using a 2440 real signatures database, and the obtained results were very satisfactory.",space,487
10.1109/ictai.2019.00146,filtered,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),IEEE,2019-11-06 00:00:00,ieeexplore,heterogeneous transfer clustering for partial co-occurrence data,https://ieeexplore.ieee.org/document/8995324/,"Heterogeneous transfer clustering can translate knowledge from some related heterogeneous source domains to the target domain without any supervision. Existing works usually use a large amount of complete co-occurrence data to learn the projection functions mapping heterogeneous data to a common latent feature subspace. However, in many real applications, it is not practical to collect abundant co-occurrence data, while the available co-occurrence data are always incomplete. Another commonly encountered problem is that the complex structure of real heterogeneous data may result in substantial degeneration in clustering performance. To address these issues, we propose a heterogeneous transfer clustering method specifically designed for partial co-occurrence data (HTCPC). It is superior to the existing methods in three facets. First, HTCPC fully uses the partial co-occurrence data in both source and target domains to learn a latent space, maximally extracting useful knowledge for clustering from limited information. Second, it incorporates multi-layer hidden representations, accurately preserving the complex hierarchical structure of data. Third, it enforces approximately orthogonal constraint in representations, effectively characterizing the latent subspace with minimal redundancy. An efficient algorithm has been derived and implemented to realize the proposed HTCPC. A series of experiments on the real datasets have illustrated the advantage of the proposed approach compared with state-of-the-art methods.",space,488
10.1109/bigdata.2013.6691727,filtered,2013 IEEE International Conference on Big Data,IEEE,2013-10-09 00:00:00,ieeexplore,hierarchical feature learning from sensorial data by spherical clustering,https://ieeexplore.ieee.org/document/6691727/,"Surveillance sensors are a major source of unstructured Big Data. Discovering and recognizing spatiotemporal objects (e.g., events) in such data is of paramount importance to the security and safety of facilities and individuals. What kind of computational model is necessary for discovering spatiotemporal objects at the level of abstraction they occur? Hierarchical invariant feature learning is the crux to the problems of discovery and recognition in Big Data. We present a multilayered convergent neural architecture for storing repeating spatially and temporally coincident patterns in data at multiple levels of abstraction. A node is the canonical computational unit consisting of neurons. Neurons are connected in and across nodes via bottom-up, top-down and lateral connections. The bottom-up weights are learned to encode a hierarchy of overcomplete and sparse feature dictionaries from space- and time-varying sensorial data by recursive layer-by-layer spherical clustering. The model scales to full-sized high-dimensional input data and also to an arbitrary number of layers thereby having the capability to capture features at any level of abstraction. The model is fully-learnable with only two manually tunable parameters. The model is generalpurpose (i.e., there is no modality-specific assumption for any spatiotemporal data), unsupervised and online. We use the learning algorithm, without any alteration, to learn meaningful feature hierarchies from images and videos which can then be used for recognition. Besides being online, operations in each layer of the model can be implemented in parallelized hardware, making it very efficient for real world Big Data applications.",space,489
10.1109/naecon46414.2019.9057909,filtered,2019 IEEE National Aerospace and Electronics Conference (NAECON),IEEE,2019-07-19 00:00:00,ieeexplore,high speed approximate cognitive domain ontologies for constrained asset allocation based on spiking neurons,https://ieeexplore.ieee.org/document/9057909/,"Cognitive agents are typically utilized in autonomous systems for automated decision making. These systems interact at real time with their environment and are generally heavily power constrained. Thus, there is a strong need for a real time agent running on a low power platform. The agent examined is the Cognitively Enhanced Complex Event Processing (CECEP) architecture. This is an autonomous decision support tool that reasons like humans and enables enhanced agent-based decision-making. It has applications in a large variety of domains including autonomous systems, operations research, intelligence analysis, and data mining. One of the most time consuming and key components of CECEP is the mining of knowledge from a repository described as a Cognitive Domain Ontology (CDO). One problem that is often tasked to CDOs is asset allocation. Given the number of possible solutions in this allocation problem, determining the optimal solution via CDO can be very time and energy consuming. A grid of isolated spiking neurons is capable of generating solutions to this problem very quickly, although some degree approximation is required to achieve the speedup. The approximate spiking approach presented in this work was able to complete nearly all allocation simulations with greater than 98% accuracy. Our results in this work show that constraining the possible solution space by creating specific rules for a scenario can alter the quality of the allocation result. We present a study compares allocation score and computation time for three different constraint implementation cases. Given the vast increase in speed, as well as the reduction computational requirements, the presented algorithm is ideal for moving asset allocation to low power embedded hardware.",space,490
10.1109/isuvr.2010.25,filtered,2010 International Symposium on Ubiquitous Virtual Reality,IEEE,2010-07-10 00:00:00,ieeexplore,high-performance real-time face-detection architecture for hci applications,https://ieeexplore.ieee.org/document/5557933/,"This paper proposes a novel hardware structure and FPGA implementation method for real-time detection of multiple human faces with robustness against illumination variations and Rotated faces. These are designed to greatly improve face detection in various environments, using the Adaboost learning algorithm and MCT techniques, Rotation Transformation, which is robust against variable illumination and rotated faces. The overall structure of proposed hardware is composed of a Color Space Converter, Noise Filter, Memory Controller Interface, Rotation Transformation, MCT (Modified Census Transform), Candidate Detector/Confidence Mapper, Position Resizer, Data Grouper, Overlay Processor and Color Overlay Processor. The experiment was conducted in various environments using a QVGA Camera, LCD Display and Virtext5 XC5VLX330 FF1760 FPGA, made by Xilinx. Implementation and verification results showed that it is possible to detect at least 32 faces in a wide variety of sizes at a maximum speed of 149 frames per second in real time.",space,491
10.1109/icaect49130.2021.9392459,filtered,"2021 International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)",IEEE,2021-02-20 00:00:00,ieeexplore,highly accurate real time human counter with minimum computation cost,https://ieeexplore.ieee.org/document/9392459/,"During the ongoing Covid-19 Pandemic when we need to operate any public facility like museum, shopping mall, restaurants, or public dealing organizations, we not only need to keep the operations going but also have to ensure precautionary measures to ascertain their safety. As per all SOPs (Standard Operating Procedures) it is advisable to restrict number of visitors inside these enclosed spaces which are most likely to be weather controlled. Automatic safety compliance thus becomes imperative in such situations. Even though absolute compliance and alert signalling will require scrutiny and cross-checking at several levels, a beginning towards automation of compliance monitoring seems mandatory in the neo-normal era. Hence In this project we have designed a low cost rapidly implementable design to monitor the number of visitors inside the self-contained hall. The system will give signal once the maximum permissible visitor population is reached at a given time. Monitoring the optimal population and the density and enforcing visitor to wear mask even within the space manually is tantamount to imposing health hazards to the person who will physically have to monitor and it may as well render the visitors vulnerable. Here we have used Artificial Intelligence based model person detection and tracking. Real time tracking with accuracy is still an important area in computer vision. There are some commercial solution available for the problem but all of them either implemented considering ideal situation or need huge cost and infrastructure. But as a part of museums community we are passing through a financial crises as due to pandemic we closed for visitors. Hence neither we can afford costly system nor a system designed with ideal condition. This motivates us to develop a new system according to our criteria. Here we have modified the available solution for implementation in real world environment using very minimum hardware infrastructure requirements to work on real time with maximum possible efficiency. This system is not only useful for COVID-19 Situation but also its use can be extended beyond the boundary of museums for visitor density monitoring system for large public establishment with minimum computation cost.",space,492
10.1109/6gsummit49458.2020.9083875,filtered,2020 2nd 6G Wireless Summit (6G SUMMIT),IEEE,2020-03-20 00:00:00,ieeexplore,histograms to quantify dataset shift for spectrum data analytics: a soc based device perspective,https://ieeexplore.ieee.org/document/9083875/,"Cloud/software-based wireless resource controllers have been recently proposed to exploit radio frequency (RF) data analytics for a network control, configuration and management. For efficient resource controller design, tracking the right metrics in real-time (analytics) and making realistic predictions (deep learning) will play an important role to increase its efficiency. This factor becomes particularly critical as radio environments are generally dynamic, and the data sets collected may exhibit shift in distribution over time and/or space. When a trained model is deployed at the controller without taking into account dataset shift, a large amount of prediction errors may take place. This paper quantifies dataset shift in real wireless physical layer data by using a statistical distance method called earth mover's distance (EMD). It utilizes an FPGA to process in real-time the inphase and quadrature (IQ) samples to obtain useful information, such as histograms of wireless channel utilization (CU). We have prototyped the data processing modules on a Xilinx System on Chip (SoC) board using Vivado, Vivado HLS, SDK and MATLAB tools. The histograms are sent as low-overhead analytics to the resource controller server where they are processed to evaluate dataset shift. The presented results provide insight into dataset shift in real wireless CU data collected over multiple weeks in the University of Oulu using the implemented modules on SoC devices. The results can be used to design approaches that can prevent failures due to datashift in deep learning models for wireless networks.",space,493
10.1109/acsos49614.2020.00036,filtered,2020 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS),IEEE,2020-08-21 00:00:00,ieeexplore,how far should i watch? quantifying the effect of various observational capabilities on long-range situational awareness in multi-robot teams,https://ieeexplore.ieee.org/document/9196255/,"In our previous work, we showed that individual robots within a multi-robot team can gain long-distance situational awareness from passive observations of a single nearby neighbor without any explicit robot-to-robot communication. However, that prior work was developed only in simulation, and performance was not measured for real robot teams in physical space with realistic hardware limitations. Toward this end, we studied the performance of these methods in real robot scenarios with methods using more sophisticated techniques in machine learning to mitigate practical implementation problems. In this study, we further extend that work by characterizing the effects of changing history length and sensor range. Rather than finding that increasing history length and sensor range always yield better estimation performance, we find that the optimal history length and sensor range varies depending on the distance between the estimating robot and the robot being estimated. For estimation problems where the estimation target is nearby, longer histories actually degrade performance, and so sensor ranges could be increased instead. Conversely, for farther targets, history length is as valuable or more valuable than sensor range. Thus, just as optimal shutter speed varies with light availability and speed of the subject, passive situational awareness in multi-robot teams is best achieved with different strategies depending on proximity to locations of interest. All studies use the teams of Thymio II physical, two-wheeled robots in laboratory environments <sup>1</sup>.<sup>1</sup>Data and models used are available at https://github.com/PavlicLab/ACSOS2020_ReTLo_Extension.git.",space,494
10.1109/robio.2011.6181717,filtered,2011 IEEE International Conference on Robotics and Biomimetics,IEEE,2011-12-11 00:00:00,ieeexplore,human-like gradual multi-agent q-learning using the concept of behavior-based robotics for autonomous exploration,https://ieeexplore.ieee.org/document/6181717/,"In the last few years, the field of mobile robotics has made lots of advancements. These advancements are due to the extensive application of mobile robots for autonomous exploration. Mobile robots are being popularly used for applications in space, underwater explorations, underground coal mines monitoring, inspection in chemical/toxic/ nuclear factories etc. But if these environments are unknown/unpredictable, conventional/ classical robotics may not serve the purpose. In such cases robot learning is the best option. Learning from the past experiences, is one such way for real time application of robots for completely unknown environments. Reinforcement learning is one of the best learning methods for robots using a constant system-environment interaction. Both single and multi-agent concepts are available for implementation of learning. The current research work describes a multi-agent based reinforcement learning using the concept of behaviour-based robotics for autonomous exploration of mobile robots. The concept has also been tested both in indoor and outdoor environments using real-time robots.",space,495
10.1109/fuzzy.1994.343840,filtered,Proceedings of 1994 IEEE 3rd International Fuzzy Systems Conference,IEEE,1994-06-29 00:00:00,ieeexplore,human-motion recognition by means of fuzzy associative inference,https://ieeexplore.ieee.org/document/343840/,"A real time human motion recognition method is proposed that uses fuzzy associative inference. If transforms space time patterns into state transition patterns, which are then recognized by means of fuzzy associative inference using associative memories. The tracking data is given as time series data, from which the characteristic states are extracted. Each human motion has a specific state transition pattern that consists of characteristic states. To recognize these motions, the specific state transition patterns of the motions are defined as fuzzy rules and these fuzzy rules are implemented in a fuzzy associative memory system. This method is independent of the person being measured and the speed of the motion. In real time experiments, this method was able to recognize three basic tennis motions (forehand stroke, backhand stroke, and smash) for six unspecified people. The recognition ratio of the fuzzy associative memory system is better than that of conventional fuzzy inference and a multilayer perceptron.&lt;<ETX>&gt;</ETX>",space,496
10.1109/tai.1998.744843,filtered,Proceedings Tenth IEEE International Conference on Tools with Artificial Intelligence (Cat. No.98CH36294),IEEE,1998-11-12 00:00:00,ieeexplore,ibhys: a new approach to learn users habits,https://ieeexplore.ieee.org/document/744843/,"Learning interface agents search regularities in the user behavior and use them to predict user's actions. We propose a new inductive concept learning approach, called IBHYS, to learn such regularities. This approach limits the hypothesis search to a small portion of the hypothesis space by letting each training example build a local approximation of the global target function. It allows to simultaneously search several hypothesis spaces and to simultaneously handle hypotheses described in different languages. This approach is particularly suited for learning interface agents because it provides an incremental algorithm with low training time and decision time, which does not require the designer of the interface agent to describe in advance and quite carefully the repetitive patterns searched. We illustrate our approach with two autonomous software agents, the Apprentice and the Assistant, devoted to assist users of interactive programming environments and implemented in Objectworks Smalltalk-80. The Apprentice learns user's work habits using an IBHYS algorithm and the Assistant, based on what has been learnt, proposes to the programmer sequences of actions the user might want to redo. We show, with experimental results on real data, that IBHYS outperforms ID3 both in computing time and predictive accuracy.",space,497
10.1109/apsec.2002.1182989,filtered,"Ninth Asia-Pacific Software Engineering Conference, 2002.",IEEE,2002-12-06 00:00:00,ieeexplore,ibistro: a learning environment for knowledge construction in distributed software engineering courses,https://ieeexplore.ieee.org/document/1182989/,"We have taught several distributed software engineering project courses with students and real clients. During these projects, students in Pittsburgh and Munich, Germany collaborated in the development of a single system. Our experiences showed that software development is communication intensive and requires the collaboration of many stakeholders. Communication is challenging in distributed contexts: participants do not all know each other and work at different times and locations; the number of participants and their organization change during the project; and participants belong to different communities. Hence, to deal with the global marketplace, it is critical to provide students with distributed collaboration skills. To improve the teaching of collaboration in software engineering, we propose iBistro, an augmented, distributed, and ubiquitous communication space. iBistro aims to overcome problems resulting from miscommunications and information loss in informal or casual meetings. iBistro enables distributed groups to collaborate and cooperate in software projects and therefore provides an environment for learning in diverse aspects such as project management, programming skills, and social skills. With the addition of techniques from artificial intelligence, such as student modeling, and intelligent support mechanisms, such as computer supported group formation, distributed tutoring becomes feasible.",space,498
10.1109/cscn.2015.7390444,filtered,2015 IEEE Conference on Standards for Communications and Networking (CSCN),IEEE,2015-10-30 00:00:00,ieeexplore,ieee 1900.6b: sensing support for spectrum databases,https://ieeexplore.ieee.org/document/7390444/,"A number of key examples of spectrum databases in wireless communications either persist or are in the process of being instantiated. Perhaps one of the most notable recent developments in this area is the spectrum databases that enable secondary usage of TV White Space (TVWS), authorized by regulators such as the FCC in the US, Ofcom in the UK, and various others internationally. Such developments have moved away from spectrum sensing for detection and secondary usage of TV band spectrum opportunities. However, it is clear that spectrum sensing might still viably assist opportunistic spectrum usage, even from a regulatory point of view, both in TVWS and in other forms of spectrum sharing. This also might be the case in wireless communications in general (e.g., in the context of self-organizing networks), particularly when spectrum sensing methods are employed to enhance or verify the operation of spectrum databases. To this end, the IEEE 1900.6 working group is undertaking an amendment standard project, IEEE 1900.6b, on spectrum sensing support for spectrum databases. This paper addresses the IEEE 1900.6 background, and reasoning for the 1900.6b amendment standard, as well as the use cases for the amendment standard and the deployment scenarios and benefits for such standardized spectrum sensing support for spectrum databases. It also provides qualitative arguments of the benefits of the approach using real information from an operational TVWS spectrum database compared with measurements at the same location. It is shown that spectrum sensing to support such a database might viably increase the amount of TV band spectrum available at that location for opportunistic usage, with 4 Watts EIRP, from around 24 MHz to around 240 MHz.",space,499
10.1109/mdm48529.2020.00023,filtered,2020 21st IEEE International Conference on Mobile Data Management (MDM),IEEE,2020-07-03 00:00:00,ieeexplore,ifloc: indoor height estimation by telco data,https://ieeexplore.ieee.org/document/9162333/,"Understanding the fine-grained distribution of telecommunication (Telco) signals in terms of a three-dimensional (3D) space is important for Telco operators to manage, operate and optimize Telco networks. It is particularly true in nowadays urban cities with a large number of high buildings. One of the key tasks is to infer the location height of mobile devices, e.g., the floor within a high building where mobile devices are located. However, precise height estimation is challenging due to complex Telco signal propagation within an indoor 3D space, sparse cell tower deployment and scarce training samples. To tackle these issues, in this paper, we propose an indoor MR height estimation framework, namely IFLoc, via a machine learning model. IFLoc first builds a training MR database via a pre-processing step to comfortably tag raw MR samples by precisely inferred height from auxiliary data such as GPS and barometer readings. Next, IFLoc trains a regression model for height estimation by a set of developed techniques including 3D space division, post-processing techniques, feature augmentation and an improved SVR (Supported Vector Regression) model. Our evaluation on eight real datasets collected within five representative high buildings in Shanghai validates that IFLoc outperforms state-of-the-art counterparts in particularly with scarce training data.",space,500
10.1109/ica-acca.2018.8609763,filtered,2018 IEEE International Conference on Automation/XXIII Congress of the Chilean Association of Automatic Control (ICA-ACCA),IEEE,2018-10-19 00:00:00,ieeexplore,"identification and process control for miso systems, with artificial neural networks and pid controller",https://ieeexplore.ieee.org/document/8609763/,"Industrial processes with multiple input and single manipulated variables are very complex systems to control in automatic models. Such is the case with processes related to gases extraction or transport phenomena. The present research is focused on the development of a control algorithm (automatic control strategy), based on artificial neural networks, to identify an industrial process by using process historical records, as well as knowledge from the operation itself. The output of the identification stage feeds a classic PID controller to perform control actions (hybrid controller). Here, an actuator or final control element is modeled, estimating its space-state dynamic equation. With the estimated model, a local control loop is conformed controlling the main process or manipulated variable. For this, the process of gases transport in a copper smelter plant was chosen, where the necessary data and scenarios for the proposed control algorithm testing was obtained. This application attempts to present a solution to problems inherent to manual control, multiple key variables coexisting in a system, mechanical stress in equipment due to manual actions, etc. The control strategy is based on a computer simulation made with real process data, showing improvement of the transient periods in the final actuators due to control signals, as well as establishing that these kinds of technologies could be implemented in both, an existing plant hardware/software or in a conventional control system.",space,501
10.1109/globecom42002.2020.9348202,filtered,GLOBECOM 2020 - 2020 IEEE Global Communications Conference,IEEE,2020-12-11 00:00:00,ieeexplore,image download and rate allocation of internet-of-things analytics at gateways in smart cities,https://ieeexplore.ieee.org/document/9348202/,"Internet-of-Things (IoT) devices are connected to the Internet through a gateway, which can host IoT analytics encapsulated in containers to convert raw sensor data into more condensed processed data. In this paper, we study two research problems to maximize the overall Quality-of-Service (QoS) level of all IoT analytics that run on both data center servers and gateways. The first problem is to select additional IoT analytics to deploy on a gateway to save upload bandwidth due to transmitting raw sensor data. The second problem is to allocate the residue upload bandwidth among all IoT analytics to maximize the overall QoS level. We propose several algorithms to solve these two research problems. We have implemented real testbeds to evaluate our proposed system and algorithms. Our experiment results reveal that the proposed algorithms: (i) capitalize the download bandwidth and storage space of the gateway for saving the upload bandwidth consumption and (ii) achieve high QoS levels without overloading the network and gateway.",space,502
10.1109/cnna.2010.5430286,filtered,2010 12th International Workshop on Cellular Nanoscale Networks and their Applications (CNNA 2010),IEEE,2010-02-05 00:00:00,ieeexplore,implementation of a drosophila-inspired orientation model on the eye-ris platform,https://ieeexplore.ieee.org/document/5430286/,"A behavioral model, recently derived from experiments on fruit-flies, was implemented, with successful comparative experiments on orientation control in real robots. This model has been firstly implemented in a standard CNN structure, using an algorithm based on classical, space-invariant templates. Subsequently, the Eye-Ris platform was utilised for the implementation of the whole strategy, at the aim to constitute a stand alone smart sensor for orientation control in bio-inspired robotic platforms. The Eye-Ris vl.2 is a visual system, made by Anafocus, that employs a fully-parallel mixed-signal array sensor-processor chip. Some experiments are reported using a commercial roving platform, the Pioneer P3-AT, showing the reliability of the proposed implementation and usefulness in higher level perceptual tasks.",space,503
10.1109/nssmic.1994.474587,filtered,Proceedings of 1994 IEEE Nuclear Science Symposium - NSS'94,IEEE,1994-11-05 00:00:00,ieeexplore,improved resolution via 3d iterative reconstruction for pet volume imaging,https://ieeexplore.ieee.org/document/474587/,"The authors have implemented iterative filtered backprojection (IFBP) and maximum likelihood by expectation maximization (ML-EM) algorithms in 3D space and applied them to phantom and real PET data. Transaxial resolution improves /spl ap/50% and axial resolution improves /spl ap/15% for IFBP at 15 iterations without a sieve compared to FBP. With a sieve, the improvements are reduced to /spl ap/6%. 3D ML-EM reconstruction shows similar resolution improvement with a much slower convergence rate compared to IFBP. The improvements in resolution from both IFBP and ML-EM are apparent in 3D FDG brain data.&lt;<ETX>&gt;</ETX>",space,504
10.1109/hpdc.2006.1652187,filtered,2006 15th IEEE International Conference on High Performance Distributed Computing,IEEE,2006-06-23 00:00:00,ieeexplore,improving resource matching through estimation of actual job requirements,https://ieeexplore.ieee.org/document/1652187/,"Heterogeneous clusters and grid infrastructures are becoming increasingly popular. In these computing infrastructures, machines have different resources (e.g., memory sizes, disk space, and installed software packages). These differences give rise to a problem of over-provisioning, that is, sub-optimal utilization of a cluster due to users requesting resource capacities greater than what their jobs actually need. Our analysis of a real workload file (LANL CM 5) revealed differences of up to two orders of magnitude between requested memory capacity and actual memory usage. The problem of over-provisioning has received very little attention so far. We discuss different approaches for applying machine learning methods to estimate the actual resource capacities used by jobs. These approaches are independent of the scheduling policies and the dynamic resource-matching schemes used. Our simulations show that these methods can yield an improvement of over 50% in utilization (throughput) of heterogeneous clusters",space,505
10.1109/hpca.2018.00018,filtered,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),IEEE,2018-02-28 00:00:00,ieeexplore,in-situ ai: towards autonomous and incremental deep learning for iot systems,https://ieeexplore.ieee.org/document/8327001/,"Recent years have seen an exploration of data volumes from a myriad of IoT devices, such as various sensors and ubiquitous cameras. The deluge of IoT data creates enormous opportunities for us to explore the physical world, especially with the help of deep learning techniques. Traditionally, the Cloud is the option for deploying deep learning based applications. However, the challenges of Cloud-centric IoT systems are increasing due to significant data movement overhead, escalating energy needs, and privacy issues. Rather than constantly moving a tremendous amount of raw data to the Cloud, it would be beneficial to leverage the emerging powerful IoT devices to perform the inference task. Nevertheless, the statically trained model could not efficiently handle the dynamic data in the real in-situ environments, which leads to low accuracy. Moreover, the big raw IoT data challenges the traditional supervised training method in the Cloud. To tackle the above challenges, we propose In-situ AI, the first Autonomous and Incremental computing framework and architecture for deep learning based IoT applications. We equip deep learning based IoT system with autonomous IoT data diagnosis (minimize data movement), and incremental and unsupervised training method (tackle the big raw IoT data generated in ever-changing in-situ environments). To provide efficient architectural support for this new computing paradigm, we first characterize the two In-situ AI tasks (i.e. inference and diagnosis tasks) on two popular IoT devices (i.e. mobile GPU and FPGA) and explore the design space and tradeoffs. Based on the characterization results, we propose two working modes for the In-situ AI tasks, including Single-running and Co-running modes. Moreover, we craft analytical models for these two modes to guide the best configuration selection. We also develop a novel two-level weight shared In-situ AI architecture to efficiently deploy In-situ tasks to IoT node. Compared with traditional IoT systems, our In-situ AI can reduce data movement by 28-71%, which further yields 1.4X-3.3X speedup on model update and contributes to 30-70% energy saving.",space,506
10.1109/stpec52385.2021.9718693,filtered,"2021 IEEE 2nd International Conference on Smart Technologies for Power, Energy and Control (STPEC)",IEEE,2021-12-22 00:00:00,ieeexplore,incipient faults detection in induction motor using mlp-nn and rbf-nn-based fault classifier,https://ieeexplore.ieee.org/document/9718693/,"Stator winding inter-turn faults (SITFs) diagnosis has enormously exploited motor current signatures, and eccentricity faults (EFs) detection has significantly investigated vibration and current signals. However, the motor-current signature analysis sometimes may need other diagnostics techniques for indicating the faulty events. The measurement of vibration signals using an accelerometer is an expensive task. The researchers appreciably implemented artificial intelligence (AI) systems for incipient fault detection in induction motors (IMs). State of the art mainly signifies fault detection techniques for IMs based on the statistical parameters as an input frame to the neural networks (NN). However, in this paper, the proposed neural networks are equipped with the twelve real input parameters to meet the desired fault identification and classification. The NN is trained by deliberately creating the SITFs, EFs, and both faults simultaneously. Multilayer perceptron (MLP) and radial basis function (RBF) based NN models are designed and verified for optimal performance in fault detection techniques. The experimental data set of a three-phase, 420 V, 4 pole squirrel cage induction motor (SCIM) is harnessed to develop the proposed NN. The proposed fault classifier has proven to have reduced mean square error (MSE) and better classification accuracy as compared to classifiers with statistical parameters used as an input feature space.",space,507
10.1109/devlrn.2014.6983001,filtered,4th International Conference on Development and Learning and on Epigenetic Robotics,IEEE,2014-10-16 00:00:00,ieeexplore,incremental training of restricted boltzmann machines using information driven saccades,https://ieeexplore.ieee.org/document/6983001/,"In the context of developmental robotics, a robot has to cope with complex sensorimotor spaces by reducing their dimensionality. In the case of sensor space reduction, classical approaches for pattern recognition use either hardcoded feature detection or supervised learning. We believe supervised learning and hard-coded feature extraction must be extended with unsupervised learning of feature representations. In this paper, we present an approach to learn representations using space-variant images and saccades. The saccades are driven by a measure of quantity of information in the visual scene, emerging from the activations of Restricted Boltzmann Machines (RBMs). The RBM, a generative model, is trained incrementally on locations where the system saccades. Our approach is implemented using real data captured by a NAO robot in indoor conditions.",space,508
10.1109/bwcca.2011.52,filtered,"2011 International Conference on Broadband and Wireless Computing, Communication and Applications",IEEE,2011-10-28 00:00:00,ieeexplore,indoor location fingerprinting based on data reduction,https://ieeexplore.ieee.org/document/6103053/,"Agent localization in indoor wireless environments is a challenging issue. Numerous techniques have been developed. Location fingerprinting, which is based on received signal strength measurements, is a frequently used approach for indoor applications. In this paper, we examine the possibility to obtain the location fingerprinting method characterized with more accurate mapping between the signal-space and the physical-space. An implemented well-known weighted k-nearest neighbor (WkNN) method is enhanced by two steps: a) pre-processing by the unsupervised learning technique during radio map building and b) post-processing of initial estimates obtained by the WkNN localization method. In this post-processing step signal-space and physical-space are transformed and mapped using two techniques of the dimension reduction: principal component analysis and multidimensional scaling. The aim of this transformation step is to de-correlate and refine initially obtained location estimates. Parameters such as number of access points and number of nearest reference nodes are examined for their impact on accuracy of the presented localization techniques. Performances are examined and verified through the experiments with real environment data.",space,509
10.1109/smc.2013.183,filtered,"2013 IEEE International Conference on Systems, Man, and Cybernetics",IEEE,2013-10-16 00:00:00,ieeexplore,indoor positioning with virtual fingerprint mapping by using linear and exponential taper functions,https://ieeexplore.ieee.org/document/6721936/,"A 2D localization system is constructed by using Wireless Sensor Nodes (WSN) to create a Virtual Fingerprint map. Linear and exponential taper functions are utilized with the received signal strength distributions between the fingerprint nodes to generate virtual fingerprint maps. Thus, a real and virtual combined fingerprint map is generated across the test area. k-NN and k-NN weighted algorithms have been implemented on virtual fingerprint maps to find the coordinates of the unknown objects. The system Localization accuracies of less than a grid space are obtained in calculations.",space,510
10.1109/mlsp.2014.6958856,filtered,2014 IEEE International Workshop on Machine Learning for Signal Processing (MLSP),IEEE,2014-09-24 00:00:00,ieeexplore,inferring clinical depression from speech and spoken utterances,https://ieeexplore.ieee.org/document/6958856/,"In this paper, we investigate the problem of detecting depression from recordings of subjects' speech using speech processing and machine learning. There has been considerable interest in this problem in recent years due to the potential for developing objective assessments from real-world behaviors, which may provide valuable supplementary clinical information or may be useful in screening. The cues for depression may be present in “what is said” (content) and “how it is said” (prosody). Given the limited amounts of text data, even in this relatively large study, it is difficult to employ standard method of learning models from n-gram features. Instead, we learn models using word representations in an alternative feature space of valence and arousal. This is akin to embedding words into a real vector space albeit with manual ratings instead of those learned with deep neural networks [1]. For extracting prosody, we employ standard feature extractors such as those implemented in openSMILE and compare them with features extracted from harmonic models that we have been developing in recent years. Our experiments show that our features from harmonic model improve the performance of detecting depression from spoken utterances than other alternatives. The context features provide additional improvements to achieve an accuracy of about 74%, sufficient to be useful in screening applications.",space,511
10.1109/iccairo.2018.00041,filtered,"2018 International Conference on Control, Artificial Intelligence, Robotics & Optimization (ICCAIRO)",IEEE,2018-05-21 00:00:00,ieeexplore,iterative algorithm for solving a split common null point problem for demicontractive operators,https://ieeexplore.ieee.org/document/8698385/,"In this paper, first we introduce an iterative algorithm which does not require prior knowledge of operator norm and prove strong convergence theorem for approximating a solution of split common null point problem of demicontractive mappings in a real Hilbert space. Widely known the computation of algorithms involving the operator norm for solving split common null point problem may be difficult and for this reason, authors have recently started constructing iterative algorithms with a way of selecting the step-sizes such that the implementation of the algorithm does not require the calculation or estimation of the operator norm. We introduce a new algorithm for solving the split common null point problem for demicontractive mappings with a way of selecting the step-sizes such that the implementation of the algorithm does not require the calculation or estimation of the operator norm and then prove strong convergence of the sequence in real Hilbert spaces. Finally, we give some numerical examples to illustrate our main result.",space,512
10.1109/iciiecs.2015.7193182,filtered,"2015 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS)",IEEE,2015-03-20 00:00:00,ieeexplore,kernel centric machine learning classifiers for anomaly detection with real bank datasets,https://ieeexplore.ieee.org/document/7193182/,"The machine learning is more effective today in anomaly detection to improve the classification accuracy. The use of powerful kernel based learning is very practical in current trends may expose accurate results in real time database applications. In this context, we need to use the new and adorned machine learning classifiers. In this paper we have given very successful and emerged kernels SVM (Support Vector Machines) which uses the marginal hyperplane uniquely determine the classes by mapping of data and KPCA (Kernel Principal Component Analysis) is an extension to PCA. Both used to classify the data and detecting anomalies by transforming input space into high dimensional feature space. The SVM kernel is use non-linear mapping function and inner product replace with kernel ingredients. KPCA extract principal components from set of corresponding eigenvectors and used as threshold with reference to kernel width. The SVM and KPCA are implemented by taking one real-time bank dataset and other from UCI machine learning repository sets. Finally performance compared with non-kernel techniques (CART, k-NN, PLSDA, PCA) applied on same datasets using training and test set combinations.",space,513
10.1109/percomworkshops51409.2021.9431114,filtered,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),IEEE,2021-03-26 00:00:00,ieeexplore,keynote: explainable-by-design deep learning,https://ieeexplore.ieee.org/document/9431114/,"MACHINE and AI justifiably attract the attention and interest not only of the wider scientific community and industry, but also society and policy makers. However, even the most powerful (in terms of accuracy) algorithms such as deep learning (DL) can give a wrong output, which may be fatal. Due to the opaque and cumbersome model structure used by DL, some authors started to talk about a dystopian “black box” society. Despite the success in this area, the way computers learn is still principally different from the way people acquire new knowledge, recognise objects and make decisions. People do not need a huge amount of annotated data. They learn by example, using similarities to previously acquired prototypes, not by using parametric analytical models. Current ML approaches are focused primarily on accuracy and overlook explainability, the semantic meaning of the internal model representation, reasoning and its link with the problem domain. They also overlook the efforts to collect and label training data and rely on assumptions about the data distribution that are often not satisfied. The ability to detect the unseen and unexpected and start learning this new class/es in real time with no or very little supervision is critically important and is something that no currently existing classifier can offer. The challenge is to fill this gap between high level of accuracy and the semantically meaningful solutions. The most efficient algorithms that have fuelled interest towards ML and AI recently are also computationally very hungry - they require specific hardware accelerators such as GPU, huge amounts of labeled data and time. They produce parametrised models with hundreds of millions of coefficients, which are also impossible to interpret or be manipulated by a human. Once trained, such models are inflexible to new knowledge. They cannot dynamically evolve their internal structure to start recognising new classes. They are good only for what they were originally trained for. They also lack robustness, formal guarantees about their behaviour and explanatory and normative transparency. This makes problematic use of such algorithms in high stake complex problems such as aviation, health, bailing from jail, etc. where the clear rationale for a particular decision is very important and the errors are very costly. All these challenges and identified gaps require a dramatic paradigm shift and a radical new approach. In this talk the speaker will present such a new approach towards the next generation of computationally lean ML and AI algorithms that can learn in real-time using normal CPUs on computers, laptops, smartphones or even be implemented on chip that will change dramatically the way these new technologies are being applied. It is explainable-by-design. It focuses on addressing the open research challenge of developing highly efficient, accurate ML algorithms and AI models that are transparent, interpretable, explainable and fair by design. Such systems are able to self-learn lifelong, and continuously improve without the need for complete retraining, can start learning from few training data samples, explore the data space, detect and learn from unseen data patterns, collaborate with humans or other such algorithms seamlessly.",space,514
10.23919/ecc.2001.7076545,filtered,2001 European Control Conference (ECC),IEEE,2001-09-07 00:00:00,ieeexplore,lateral auto-pilot design for an agile missile using dynamic fuzzy neural networks,https://ieeexplore.ieee.org/document/7076545/,"This paper presents a new approach, which exploits the recently developed Dynamic Fuzzy Neural Networks (DFNN) learning algorithm. The DFNN is based on extended Radial Basis Function (RBF) neural networks, which are functionally equivalent to Takagi-Sugeno-Kang (TSK) fuzzy systems. The algorithm comprises 4 parts: (1) Criteria of rules generation; (2) Allocation of premise parameters; (3) Determination of consequent parameters and (4) Pruning technology. The salient characteristics of the approach are: (1) A hierarchical on-line self-organizing learning paradigm is employed so that not only parameters can be adjusted, but also the determination of structure can be self-adaptive without partitioning the input space a priori; (2) Fast learning speed can be achieved so that the system can be implemented in real time. The application of the proposed approach is demonstrated in application to a demanding, highly nonlinear, missile control design task. Scheduling on instantaneous incidence (a rapidly varying quantity) is well known to lead to considerable difficulties with classical gain-scheduling methods. It is shown that the methods proposed here can, however, be used to successfully design an effective intelligent controller.",space,515
10.1109/iros51168.2021.9636547,filtered,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),IEEE,2021-10-01 00:00:00,ieeexplore,learning contact-rich assembly skills using residual admittance policy<sup>*</sup>,https://ieeexplore.ieee.org/document/9636547/,"Contact-rich assembly tasks may result in large and unpredictable forces and torques when the locations of the contacting parts are uncertain. The ability to correct the trajectory in response to haptic feedback and accomplish the task despite location uncertainties is an important skill. We hypothesize that this skill would facilitate generalization and support direct transfer from simulations to real world. To reduce sample complexity, we propose to learn a residual admittance policy (RAP). RAP is learned to correct the movements generated by a baseline policy in the framework of dynamic movement primitives. Given the reference trajectories generated by the baseline policy, the action space of RAP is limited to the admittance parameters. Using deep reinforcement learning, a deep neural network is trained to map task specifications to proper admittance parameters. We demonstrate that RAP handles uncertainties in board location, generalizes well over space, size and shape, and facilitates quick transfer learning. Most impressively, we demonstrate that the policy learned in simulations achieves similar robustness to uncertainties, generalization and performance when deployed on an industrial robot (UR5e) without further training. See accompanying video for demonstrations.",space,516
10.1109/icpr48806.2021.9413196,filtered,2020 25th International Conference on Pattern Recognition (ICPR),IEEE,2021-01-15 00:00:00,ieeexplore,learning defects in old movies from manually assisted restoration,https://ieeexplore.ieee.org/document/9413196/,"We propose to detect defects in old movies, as the first step of a larger framework of old movies restoration by inpainting techniques. The specificity of our work is to learn a film restorer's expertise from a pair of sequences, composed of a movie with defects, and the same movie which was semiautomatically restored with the help of a specialized software. In order to detect those defects with minimal human interaction and further reduce the time spent for a restoration, we feed a U-Net with consecutive defective frames as input to detect the unexpected variations of pixel intensity over space and time. Since the output of the network is a mask of defect location, we first have to create the dataset of mask frames on the basis of restored frames from the software used by the film restorer, instead of classical synthetic ground truth, which is not available. These masks are estimated by computing the absolute difference between restored frames and defectuous frames, combined with thresholding and morphological closing. Our network succeeds in automatically detecting real defects with more precision than the manual selection with an all-encompassing shape, including some the expert restorer could have missed for lack of time.",space,517
10.1109/ijcnn.1993.716991,filtered,"Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)",IEEE,1993-10-29 00:00:00,ieeexplore,learning goal-directed navigation as attractor dynamics for a sensory motor system. (an experiment by the mobile robot yamabico),https://ieeexplore.ieee.org/document/716991/,"This paper describes experimental results based on the authors' prior-proposed scheme: learning of sensory-based, goal-directed behavior. The scheme was implemented on the mobile robot ""YAMABICO"" and learning of a set of goal-directed navigations were conducted. The experiment assumed that the robot receives no global information such as position nor prior environment model. Instead, the robot was trained to learn adequate maneuvering in the adopted workspace by building a correct mapping between a spatio-temporal sequence of sensory inputs and maneuvering outputs on a neural structure. The experimental results showed that sufficient training generated rigid dynamical structure of a fixed point and limit cycling in the sensory-based state space, which realized robust navigations of homing and cyclic routing even against certain changes of environment as well as miscellaneous noises in the real world.",space,518
10.1109/cec.2017.7969562,filtered,2017 IEEE Congress on Evolutionary Computation (CEC),IEEE,2017-06-08 00:00:00,ieeexplore,learning of a tracker model from multi-radar data for performance prediction of air surveillance system,https://ieeexplore.ieee.org/document/7969562/,"A valid model of the air surveillance system performance is highly valued when making decisions related to the optimal control of the system. We formulate a model for a multi-radar tracker system by combining a radar performance model with a tracker performance model. A tracker as a complex software system is hard to model mathematically and physically. Our novel approach is to utilize machine learning to create a tracker model based on measurement data from which the input and target output for the model are calculated. The measured data comprises the time series of 3D coordinates of cooperative aircraft flights, the corresponding target detection recordings from multiple radars, and the related multi-radar track recordings. The collected data is used to calculate performance measures for the radars and the tracker at specific locations in the air space. We apply genetic programming to learning such rules from radar performance measures that explain tracker performance. The easily interpretable rules are intended to reveal the real behavior of the system providing comprehension for its control and further development. The learned rules allow predicting tracker performance level for the system control in all radar geometries, modes, and conditions at any location. In the experiments, we show the feasibility of our approach to learning a tracker model and compare our rule learner with two tree classifiers, another rule learner, a neural network, and an instance-based classifier using the real air surveillance data. The tracker model created by our rule learner outperforms the models by the other methods except for the neural network whose prediction performance is equal.",space,519
10.1109/ictai.2019.00220,filtered,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),IEEE,2019-11-06 00:00:00,ieeexplore,learning to drive via apprenticeship learning and deep reinforcement learning,https://ieeexplore.ieee.org/document/8995417/,"With the implementation of reinforcement learning (RL) algorithms, current state-of-art autonomous vehicle technology have the potential to get closer to full automation. However, most of the applications have been limited to game domains or discrete action space which are far from the real world driving. Moreover, it is very tough to tune the parameters of reward mechanism since the driving styles vary a lot among the different users. For instance, an aggressive driver may prefer driving with high acceleration whereas some conservative drivers prefer a safer driving style. Therefore, we propose an apprenticeship learning in combination with deep reinforcement learning approach that allows the agent to learn the driving and stopping behaviors with continuous actions. We use gradient inverse reinforcement learning (GIRL) algorithm to recover the unknown reward function and employ REINFORCE as well as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal policy. The performance of our method is evaluated in simulation-based scenario and the results demonstrate that the agent performs human like driving and even better in some aspects after training.",space,520
10.1109/etfa.2018.8502485,filtered,2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA),IEEE,2018-09-07 00:00:00,ieeexplore,linear classification of badly conditioned data,https://ieeexplore.ieee.org/document/8502485/,"We present a method for the fast and robust linear classification of badly conditioned data. In our considerations, badly conditioned data are such data which are numerically difficult to handle. Due to, e.g. a large number of features or a large number of objects representing classes as well as noise, outliers or incompleteness, the common software computation of the discriminating linear combination of features between classes fails or is extremely time consuming. The theoretical foundations of our approach are based on the single feature ranking, which allows fast calculation of the approximative initial classification boundary. For the increasing of classification accuracy of this boundary, the refinement is performed in the lower dimensional space. Our approach is tested on several datasets from UCI Reposi-tiory. Experimental results indicate high classification accuracy of the approach. For the modern real industrial applications such a method is especially suitable in the Cyber-Physical-System environments and provides a part of the workflow for the automated classifier design.",space,521
10.1109/ijcnn.2014.6889658,filtered,2014 International Joint Conference on Neural Networks (IJCNN),IEEE,2014-07-11 00:00:00,ieeexplore,long-term learning behavior in a recurrent neural network for sound recognition,https://ieeexplore.ieee.org/document/6889658/,"In this paper, the long-term learning properties of an artificial neural network model, designed for sound recognition and computational auditory scene analysis in general, are investigated. The model is designed to run for long periods of time (weeks to months) on low-cost hardware, used in a noise monitoring network, and builds upon previous work by the same authors. It consists of three neural layers, connected to each other by feedforward and feedback excitatory connections. It is shown that the different mechanisms that drive auditory attention emerge naturally from the way in which neural activation and intra-layer inhibitory connections are implemented in the model. Training of the artificial neural network is done following the Hebb principle, dictating that ""Cells that fire together, wire together"", with some important modifications, compared to standard Hebbian learning. As the model is designed to be on-line for extended periods of time, also learning mechanisms need to be adapted to this. The learning needs to be strongly attention- and saliency-driven, in order not to waste available memory space for sounds that are of no interest to the human listener. The model also implements plasticity, in order to deal with new or changing input over time, without catastrophically forgetting what it already learned. On top of that, it is shown that also the implementation of short-term memory plays an important role in the long-term learning properties of the model. The above properties are investigated and demonstrated by training on real urban sound recordings.",space,522
10.1109/isqed.2018.8357325,filtered,2018 19th International Symposium on Quality Electronic Design (ISQED),IEEE,2018-03-14 00:00:00,ieeexplore,low cost and power cnn/deep learning solution for automated driving,https://ieeexplore.ieee.org/document/8357325/,"Automated driving functions, like highway driving and parking assist, are increasingly getting deployed in high-end cars with the ultimate goal of realizing self-driving car using Deep learning techniques like convolution neural network (CNN). For mass-market deployment, the embedded solution is required to address the right cost and performance envelope along with security and safety. In the case of automated driving, one of the key functionality is “finding drivable free space”, which is addressed using deep learning techniques like CNN. These CNN networks pose huge computing requirements in terms of hundreds of GOPS/TOPS (Giga or Tera operations per second), which seems beyond the capability of today's embedded SoC. This paper covers various techniques consisting of fixed-point conversion, sparse multiplication, fusing of layers and network pruning, for tailoring on the embedded solution. These techniques are implemented on the device by means of optimized Deep learning library for inference. The paper concludes by demonstrating the results of a CNN network running in real time on TI's TDA2X embedded platform producing a high-quality drivable space output for automated driving.",space,523
10.1109/aike.2018.00051,filtered,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),IEEE,2018-09-28 00:00:00,ieeexplore,management of subdivided dynamic indoor environments by autonomous scanning system,https://ieeexplore.ieee.org/document/8527483/,"With the development of sensing technologies, various spatial applications have been expanding into indoor spaces. For smooth spatial services, grasping indoor space information is most essential task. However, the indoor spaces is not only becoming increasingly complex, but also frequently changed than outdoor spaces. This makes it hard to provide an accurate location based service in an indoor space. This paper propose a way of managing a dynamic indoor environment by defining a multi-layered indoor model in terms of an object mobility. It allows an indoor space to be managed more elaborate and realistic than up-to-date indoor models which only consider an indoor floor plan. We firstly define a classification of indoor objects based on their characteristic to frequently change location, and propose three-layers indoor model followed by the classified objects with its mobility. Secondly, we design and implement an autonomous scanning system to understand changes of indoor situation quickly and automatically. The system is made up of a combination of IoT devices, including a programmable robot, lidar scanner and single-board computer. Finally, we demonstrate an implementation of the system with constructing the proposed model from a real indoor environment.",space,524
10.1109/icde48307.2020.00144,filtered,2020 IEEE 36th International Conference on Data Engineering (ICDE),IEEE,2020-04-24 00:00:00,ieeexplore,maxson: reduce duplicate parsing overhead on raw data,https://ieeexplore.ieee.org/document/9101499/,"JSON is a very popular data format in many applications in Web and enterprise. Recently, many data analytical systems support the loading and querying JSON data. However, JSON parsing can be costly, which dominates the execution time of querying JSON data. Many previous studies focus on building efficient parsers to reduce this parsing cost, and little work has been done on how to reduce the occurrences of parsing. In this paper, we start with a study with a real production workload in Alibaba, which consists of over 3 million queries on JSON. Our study reveals significant temporal and spatial correlations among those queries, which result in massive redundant parsing operations among queries. Instead of repetitively parsing the JSON data, we propose to develop a cache system named Maxson for caching the JSON query results (the values evaluated from JSONPath) for reuse. Specifically, we develop effective machine learning-based predictor with combining LSTM (long shortterm memory) and CRF (conditional random field) to determine the JSONPaths to cache given the space budget. We have implemented Maxson on top of SparkSQL. We experimentally evaluate Maxson and show that 1) Maxson is able to eliminate the most of duplicate JSON parsing overhead, 2) Maxson improves end-to-end workload performance by 1.5-6.5×.",space,525
10.1109/ivcnz51579.2020.9290736,filtered,2020 35th International Conference on Image and Vision Computing New Zealand (IVCNZ),IEEE,2020-11-27 00:00:00,ieeexplore,melanoma and nevi classification using convolution neural networks,https://ieeexplore.ieee.org/document/9290736/,"Early identification of melanoma skin cancer is vital for the improvement of patients' prospects of five year disease free survival. The majority of malignant skin lesions present at a general practice level where a diagnosis is based on a clinical decision algorithm. As a false negative diagnosis is an unacceptable outcome, clinical caution tends to result in a low positive predictive value of as low at 8%. There has been a large burden of surgical excisions that retrospectively prove to have been unnecessary.This paper proposes a method to identify melanomas in dermoscopic images using a convolution neural network (CNN). The proposed method implements transfer learning based on the ResNet50 CNN, pretrained using the ImageNet dataset. Datasets from the ISIC Archive were implemented during training, validation and testing. Further tests were performed on a smaller dataset of images taken from the Dermnet NZ website and from recent clinical cases still awaiting histological results to indicate the trained network's ability to generalise to real cases. The 86% test accuracy achieved with the proposed method was comparable to the results of prior studies but required significantly less pre-processing actions to classify a lesion and was not dependant on consistent image scaling or the presence of a scale on the image. This method also improved on past research by making use of all of the information present in an image as opposed to focusing on geometric and colour-space based aspects independently.",space,526
10.1109/icra40945.2020.9196540,filtered,2020 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2020-08-31 00:00:00,ieeexplore,meta reinforcement learning for sim-to-real domain adaptation,https://ieeexplore.ieee.org/document/9196540/,"Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.",space,527
10.1109/robot.1998.681416,filtered,Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146),IEEE,1998-05-20 00:00:00,ieeexplore,mobile robot exploration and map-building with continuous localization,https://ieeexplore.ieee.org/document/681416/,"Our research addresses how to integrate exploration and localization for mobile robots. A robot exploring and mapping an unknown environment needs to know its own location, but it may need a map in order to determine that location. In order to solve this problem, we have developed ARIEL, a mobile robot system that combines frontier based exploration with continuous localization. ARIEL explores by navigating to frontiers, regions on the boundary between unexplored space and space that is known to be open. ARIEL finds these regions in the occupancy grid map that it builds as it explores the world. ARIEL localizes by matching its recent perceptions with the information stored in the occupancy grid. We have implemented ARIEL on a real mobile robot and tested ARIEL in a real-world office environment. We present quantitative results that demonstrate that ARIEL can localize accurately while exploring, and thereby build accurate maps of its environment.",space,528
10.1109/sami.2015.7061869,filtered,2015 IEEE 13th International Symposium on Applied Machine Intelligence and Informatics (SAMI),IEEE,2015-01-24 00:00:00,ieeexplore,model predictive control of a ball and plate laboratory model,https://ieeexplore.ieee.org/document/7061869/,"The papers presents an implementation of the predictive state space control algorithm, called Model Predictive Control (MPC). This control algorithm is verified on the Ball and Plate laboratory model, called B&amp;P_KYB, for the reference trajectory tracking. The control algorithm is first verified using the derived nonlinear simulation model in Matlab/Simulink. Since simulation results are acceptable, an experiment is realized on the real laboratory model. The results of the experiment are demonstrated as the time response of the ball position and the voltage.",space,529
10.1109/robot.1992.220046,filtered,Proceedings 1992 IEEE International Conference on Robotics and Automation,IEEE,1992-05-14 00:00:00,ieeexplore,model-driven pose correction,https://ieeexplore.ieee.org/document/220046/,"Pose determination for robot navigation is discussed. The problem is to maintain the system's instantaneous precept of its position and orientation in space for performing various tasks. The authors describe a system in which models were used to guide the sensory interpretation and to correct expectations. In this system, simulated images were used to analyze the real images and to correct the pose parameters. The reported techniques have been implemented and experiments with real images in a real environment have been performed.&lt;<ETX>&gt;</ETX>",space,530
10.1109/ictai.2011.89,filtered,2011 IEEE 23rd International Conference on Tools with Artificial Intelligence,IEEE,2011-11-09 00:00:00,ieeexplore,multi-agent simulation design driven by real observations and clustering techniques,https://ieeexplore.ieee.org/document/6103379/,"The multi-agent simulation consists in using a set of interacting agents to reproduce the dynamics and the evolution of the phenomena that we seek to simulate. It is considered now as an alternative to classical simulations based on analytical models. But, its implementation remains difficult, particularly in terms of behaviors extraction and agents modelling. This task is usually performed by the designer who has some expertise and available observation data on the process. In this paper, we propose a novel way to make use of the observations of real world agents to model simulated agents. The modelling is based on clustering techniques. Our approach is illustrated through an example in which the behaviors of agents are extracted as trajectories and destinations from video sequences analysis. This methodology is investigated with the aim to apply it, in particular, in a retail space simulation for the evaluation of marketing strategies. This paper presents experiments of our methodology in the context of a public area modelling.",space,531
10.1109/ijcnn.2015.7280577,filtered,2015 International Joint Conference on Neural Networks (IJCNN),IEEE,2015-07-17 00:00:00,ieeexplore,multi-kernel probability distribution regressions,https://ieeexplore.ieee.org/document/7280577/,"This paper presents a multi-layer reproducing kernel Hilbert space (RKHS) approach for probability distribution to real and probability distribution to function regressions. The approach maps the distributions into RKHS by distribution embeddings and, then, constructs a multi-layer RKHS within which the multi-kernel distribution regression can be implemented using an existing kernel regression algorithm, such as kernel recursive least squares (KRLS). The numerical simulations on synthetic data obtained via Gaussian mixtures show that the proposed approach outperforms existing probability distribution (DR) regression algorithms by achieving smaller mean squared errors (MSEs) and requiring less training samples.",space,532
10.1109/iccp.2013.6646076,filtered,2013 IEEE 9th International Conference on Intelligent Computer Communication and Processing (ICCP),IEEE,2013-09-07 00:00:00,ieeexplore,multi-objective dse algorithms' evaluations on processor optimization,https://ieeexplore.ieee.org/document/6646076/,"Very complex micro-architectures, like complex superscalar/SMT or multicore systems, have lots of configurations. Exploring this huge design space and trying to optimize multiple objectives, like performance, power consumption and hardware complexity is a real challenge. In this paper, using the multi-objective design space exploration tool FADSE, we tried to optimize the hardware parameters of the complex superscalar Grid ALU Processor. We compared how different heuristic algorithms handle the DSE optimization. Three of these algorithms are taken from the jMetal library (NSGAII, SPEA2 and SMPSO) while the other two, CNSGAII and MOHC were implemented by us. We show that in this huge design space the differences between the best found individuals by every algorithm are very small, only the time in which they got to these solutions differs. In order to accelerate the DSE process we also did a feature selection through machine learning techniques and ran all DSE algorithms again with a smaller number of input parameters.",space,533
10.1109/ijcnn.2004.1380187,filtered,2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541),IEEE,2004-07-29 00:00:00,ieeexplore,neural network approach for user activity monitoring in computer networks,https://ieeexplore.ieee.org/document/1380187/,"A system is proposed for user activity monitoring in computer networks. The system is based on the use of neural networks and is implemented using agent approach. The monitoring system allows to detect anomalies in user activity, and consists of two components-on-line and off-line. On-line monitoring is carried out in real time and is used to predict the processes started by an user on the basis of previous ones. Off-line monitoring is carried out at the end of the day and is based on the analysis of statistical parameters of user behavior (user signature). Both on-line and off-line monitoring use neural network approach to detect anomalies in user behavior. Proposed system was verified on real data obtained in Intranet of Space Research Institute of NASU-NSAU and Institute of Physics and Technologies of National Technical University of Ukraine ""Kiev Polytechnic Institute"".",space,534
10.1109/cec.1999.785521,filtered,Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406),IEEE,1999-07-09 00:00:00,ieeexplore,neural network training with constrained integer weights,https://ieeexplore.ieee.org/document/785521/,"Presents neural network training algorithms which are based on the differential evolution (DE) strategies introduced by Storn and Price (J. of Global Optimization, vol. 11, pp. 341-59, 1997). These strategies are applied to train neural networks with small integer weights. Such neural networks are better suited for hardware implementation than the real weight ones. Furthermore, we constrain the weights and biases in the range [-2/sup k/+1, 2/sup k/-1], for k=3,4,5. Thus, they can be represented by just k bits. These algorithms have been designed keeping in mind that the resulting integer weights require less bits to be stored and the digital arithmetic operations between them are more easily implemented in hardware. Obviously, if the network is trained in a constrained weight space, smaller weights are found and less memory is required. On the other hand, the network training procedure can be more effective and efficient when large weights are allowed. Thus, for a given application, a trade-off between effectiveness and memory consumption has to be considered. We present the results of evolution algorithms for this difficult task. Based on the application of the proposed class of methods on classical neural network benchmarks, our experience is that these methods are effective and reliable.",space,535
10.1109/ipin.2019.8911773,filtered,2019 International Conference on Indoor Positioning and Indoor Navigation (IPIN),IEEE,2019-10-03 00:00:00,ieeexplore,no-sweat detective: no effort anomaly detection for wi-fi-based localization,https://ieeexplore.ieee.org/document/8911773/,"At present, Wi-Fi localization is the main approach to estimating location indoors. However, age deterioration of the localization model due to dynamic environmental changes degrades its accuracy. Therefore, periodic model recalibration is inescapable. Existing methods for doing this use transfer learning and a small set of additional and supervised datasets. However, the reference points to obtain these datasets are determined either randomly or comprehensively. Such poor datasets catastrophically destabilize model recovery after recalibration because overfitting occurs. We propose a new approach that detects anomalous reference points to gain felicitous supervised datasets in order to prevent overfitting. Unsupervised datasets obtained from off-the-shelf mobile navigation applications, i.e., user logs uploaded from phones, are used. Our approach is implemented in a system we call ""No-Sweat Detective"". The results of an experiment in a controlled environment demonstrate that No-Sweat Detective can detect anomalies caused by environmental changes, and the results of a five-month experiment show that No-Sweat Detective has redundancy against a complex open-space environment in the real world. In addition, it could suppress model age deterioration by up to 10.9% compared to existing methods.",space,536
10.1109/indin45523.2021.9557354,filtered,2021 IEEE 19th International Conference on Industrial Informatics (INDIN),IEEE,2021-07-23 00:00:00,ieeexplore,novelty detection for iterative learning of mimo fuzzy systems,https://ieeexplore.ieee.org/document/9557354/,"This paper proposes a methodology for iterative learning of multi-input multi-output (MIMO) fuzzy models focusing on dynamic system identification. The first step of the proposed method is the learning of the antecedent part of the fuzzy system, which is learned iteratively, where fuzzy rules can be added or merged based on the presented novelty detection and similarity criteria defined by a recursive extension of the Gath-Geva clustering algorithm. Then, the consequent part consists in the direct implementation of a non-recursive fuzzy approach that uses global least squares, Observer Kalman Filter Identification (OKID) and the Eigensystem Realization Algorithm (ERA). The proposed method is validated using experimental data from a real quadrotor aerial robot, a nonlinear dynamic system. Using quantitative performance metrics, the proposed method is compared with Hammerstein-Wiener models (H.-W.), nonlinear autoregressive models with exogenous input (NARX), and state-space models using subspace method with time-domain data (N4SID), other MIMO system identification techniques. The proposed method achieved better results compared to other techniques, showing the importance and versatility of learning based on novelty detection for MIMO problems.",space,537
10.1109/robio.2009.4913200,filtered,2008 IEEE International Conference on Robotics and Biomimetics,IEEE,2009-02-25 00:00:00,ieeexplore,object orientation recognition based on sift and svm by using stereo camera,https://ieeexplore.ieee.org/document/4913200/,"The goal of this research is to recognize an object and its orientation in space by using stereo camera. The principle of object orientation recognition in this paper was based on the scale invariant feature transform (SIFT) and support vector machine (SVM). SIFT has been successfully implemented on object recognition but it had a problem recognizing the object orientation. For many autonomous robotics applications, such as using a vision-guided industrial robot to grab a product, not only correct object recognition will be needed in this process but also object orientation recognition is required. In this paper we used SVM to recognize object orientation. SVM has been known as a promising method for classification accuracy and its generalization ability. The stereo camera system adopted in this research provided more useful information compared to single camera one. The object orientation recognition technique was implemented on an industrial robot in a real application. The proposed camera system and recognition algorithms were used to recognize a specific object and its orientation and then guide the industrial robot to perform some alignment operations on the object.",space,538
10.1109/iadcc.2013.6514374,filtered,2013 3rd IEEE International Advance Computing Conference (IACC),IEEE,2013-02-23 00:00:00,ieeexplore,off-line signature verification using neural networks,https://ieeexplore.ieee.org/document/6514374/,"This paper proposes a signature verification system that can authenticate a signature to avoid forgery cases. In the real world environment, it is often very difficult for any verification system to handle a huge collection of data, and to detect the genuine signatures with relatively good accuracy. Consequently, some artificial intelligence technique are used that can learn from the huge data set, in its training phase and can respond accurately, in its application phase without consuming much storage memory space and computational time. In addition, it should also have the ability to continuously update its knowledge from real time experiences. One such adaptive machine learning technique called a Multi-Layered Neural Network Model (NN Model) is implemented for the purpose of this work. Initially, a huge set of data is generated by collecting the images of several genuine and forgery signatures. The quality of the images is improved by using image processing followed by further extracting certain unique standard statistical features in its feature extraction phase. This output is given as the input to the above proposed NN Model to further improve its decision making capabilities. The performance of the proposed model is evaluated by calculating the fault acceptance and rejection rates for a small set of data. Further possible developments of this model are also outlined.",space,539
10.23919/acc.1988.4789994,filtered,1988 American Control Conference,IEEE,1988-06-17 00:00:00,ieeexplore,on discrete inner-outer and spectral factorizations,https://ieeexplore.ieee.org/document/4789994/,"In this paper, reliable algorithms are developed to perform inner-outer, coprime, and spectral factorizations for discrete FDLTI systems. It is shown that the discrete algebraic Riccati equation plays an important role in obtaining state-space representations for all key factorizations. The implementation of algorithms can be carried out efficiently using real matrix operations.",space,540
10.1109/ccdc.2018.8407436,filtered,2018 Chinese Control And Decision Conference (CCDC),IEEE,2018-06-11 00:00:00,ieeexplore,one-dimensional data augmentation using a wasserstein generative adversarial network with supervised signal,https://ieeexplore.ieee.org/document/8407436/,"In recent years, Generative Adversarial Network (GAN) is widely applied in many domains; however, there is still some difficulties in training the network, which are mainly caused by mode collapse, vanishing gradient of generator and indirect assessment criteria of generated samples. In this paper, the supervised signal is introduced into Wasserstein Generative Adversarial Network (WGAN) on the application of one-dimensional data augmentation to alleviate this difficulty. In the proposed method, besides generating fake samples, a well trained generative model is implemented to reconstruct the real samples , whose input data are latent space samples obtained from autoencoder (AE). In addition, the mode collapse can be prevented by the new model through ensuring that the supervised signal grounded in all the available training data. The performance of our method is verified based on parameters of electronic equipment and stock index systematically and quantitatively, and the superiority of the algorithm is demonstrated by the experiment results both in convergence rate and the quality of samples compared with WGAN and VAEGAN.",space,541
10.1109/ismar52148.2021.00016,filtered,2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR),IEEE,2021-10-08 00:00:00,ieeexplore,"openrdw: a redirected walking library and benchmark with multi-user, learning-based functionalities and state-of-the-art algorithms",https://ieeexplore.ieee.org/document/9583831/,"Redirected walking (RDW) is a locomotion technique that guides users on virtual paths, which might vary from the paths they physically walk in the real world. Thereby, RDW enables users to explore a virtual space that is larger than the physical counterpart with near-natural walking experiences. Several approaches have been proposed and developed; each using individual platforms and evaluated on a custom dataset, making it challenging to compare between methods. However, there are seldom public toolkits and recognized benchmarks in this field. In this paper, we introduce OpenRDW, an open-source library and benchmark for developing, deploying and evaluating a variety of methods for walking path redirection. The OpenRDW library provides application program interfaces to access the attributes of scenes, to customize the RDW controllers, to simulate and visualize the navigation process, to export multiple formats of the results, and to evaluate RDW techniques. It also supports the deployment of multi-user real walking, as well as reinforcement learning-based models exported from TensorFlow or PyTorch. The OpenRDW benchmark includes multiple testing conditions, such as walking in size varied tracking spaces or shape varied tracking spaces with obstacles, multiple user walking, etc. On the other hand, procedurally generated paths and walking paths collected from user experiments are provided for a comprehensive evaluation. It also contains several classic and state-of-the-art RDW techniques, which include the above mentioned functionalities.",space,542
10.1109/ispa-bdcloud-socialcom-sustaincom52081.2021.00137,filtered,"2021 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)",IEEE,2021-10-03 00:00:00,ieeexplore,optimal fpga-oriented lightweight network architecture search under multi-objective constraints,https://ieeexplore.ieee.org/document/9644718/,"The neural architecture search (NAS) technology has developed rapidly, but most algorithms mainly target the GPUs, causing the searched models to lack consideration of the target hardware platform. This paper proposes a novel NAS algorithm specific for the unique calculation and storage character of FPGA to accelerate the deployment efficiency of neural networks and automated build the FPGA-oriented neural structure. Firstly, we design the search space based on the lightweight candidate blocks. Then, we measure the real simulation latency of each candidate block on the FPGA platform and for the first time using it to direct the NAS process. Eventually, this paper proposed a differentiable representation function of hardware latency and built a multi-objective NAS framework based on real simulation latency and FLOPS. In addition, by analyzing the relationship between the network convergence process and the model size, we achieve an optimal trade-off balance between precision and latency and build a more suitable network structure for deployment on FPGA. Compared with state-of-the-art manual and automated lightweight models, our method has relatively better accuracy and latency performance. Extensive experiments on the ImageNet dataset show that the proposed method can achieve 2x speedup compared with the similar accuracy manual lightweight network (MobileNet V2) and even get 3x speedup compared with the equivalent auto lightweight network (DARTS).",space,543
10.1109/sii.2012.6426933,filtered,2012 IEEE/SICE International Symposium on System Integration (SII),IEEE,2012-12-18 00:00:00,ieeexplore,optimization of obstacle avoidance using reinforcement learning,https://ieeexplore.ieee.org/document/6426933/,Walking through narrow space for multi-legged robot is optimized using reinforcement learning in this paper. The walking is generated by the virtual repulsive force from the estimated obstacle position and the virtual impedance field. The resulted action depends on the parameter of the virtual impedance coefficients. The reinforcement learning is employed to find an optimal motion. The temporal walking through motion consists of each parameter optimized for a situation. Optimization of integrated walking through motion is finally achieved evaluating walking in compound encountering obstacle on simulator. The resulted motion is implemented to a real multi-legged robot and results show the effectiveness of the proposed method.,space,544
10.1109/robot.1991.131722,filtered,Proceedings. 1991 IEEE International Conference on Robotics and Automation,IEEE,1991-04-11 00:00:00,ieeexplore,parallel robot motion planning,https://ieeexplore.ieee.org/document/131722/,"A fast, parallel method for computing configuration space maps is presented. The method is made possible by recognizing that one can compute a family of primitive maps which can be combined by superposition based on the distribution of real obstacles. This motion planner has been implemented for the first three degrees-of-freedom of a Puma robot in *Lisp on a Connection Machine with 8 K processors. A six degree-of-freedom version of the algorithm which performs a sequential search of the six-dimensional configuration space, building three-dimensional cross sections in parallel, has also been implemented.&lt;<ETX>&gt;</ETX>",space,545
10.1109/icufn.2016.7537070,filtered,2016 Eighth International Conference on Ubiquitous and Future Networks (ICUFN),IEEE,2016-07-08 00:00:00,ieeexplore,performance evaluation of in-memory computing on scale-up and scale-out cluster,https://ieeexplore.ieee.org/document/7537070/,"Apache Spark framework, which is the implementation of Resilient Distributed Datasets(RDD), is used instead of MapReduce on recent data processing models of Hadoop ecosystem. In this paper, we evaluated the performance and resource usage of real world workloads on scale-up and scale-out clusters using the in-memory caching feature of Spark framework. In our experiments, scale-up processed data more efficiently than scaleout in write intensive workloads such as Sort and Scan, whereas scale-out had strength in those utilizing iterative algorithms such as Join, Pagerank and KMeans. Considering the efficiency in physical factors including performance per watt and the physical space each occupies, we show that it is more advantages to use scale up cluster than scale out.",space,546
10.1109/indo-taiwanican48429.2020.9181341,filtered,"2020 Indo – Taiwan 2nd International Conference on Computing, Analytics and Networks (Indo-Taiwan ICAN)",IEEE,2020-02-15 00:00:00,ieeexplore,planogram design analytics using image processing,https://ieeexplore.ieee.org/document/9181341/,"A planogram is a tool for visual merchandising of retail stores. It displays a detailed view of the retail store with the major intent for product placement. The planogram is highly useful for examining the point of sale as it demonstrates the exact positioning of products. Two major benefits are there for building planograms while planning the store layout such as maximizing sales and space of the retail store.An important part of Planogram management is to read planogram drawings and images and convert these to representational data in CSV or database formats.In this part, Contour analysis of image using OpenCV along with combination of text detection and recognition is performed using Connectionist Text Proposal Network and Convolutional Recurrent Neural Network deep learning models respectively. The proposed planogram design analytics system is implemented using real data. The customer had tested the system as per different cases. The feedback obtained from them confirms that the system meets the requirements to their satisfaction.",space,547
10.1109/cira.2005.1554245,filtered,2005 International Symposium on Computational Intelligence in Robotics and Automation,IEEE,2005-06-30 00:00:00,ieeexplore,plenary talk june 29; the 3<sup>rd</sup>generation of robotics: ubiquitous robot,https://ieeexplore.ieee.org/document/1554245/,"This talk shows its possibility of implementation in real life through demonstrations using a Sobot, Rity: i) continuous interface between physical and virtual worlds ii) seamless transmission of Sobot between a PC and a Mobot, and iii) omnipresence of Sobot. Rity, developed at the Robot Intelligence Technology (RIT) Laboratory, KAIST, is a Sobot implemented as a 12 DOF artificial creature in the virtual 3D world created in a PC. It has virtual sensors to survive in the virtual world and physical sensors attached to the PC to interact with the real world. Based on sensor information it can express its emotion, and interact with human beings through a web camera in the real world. It can generate behaviors autonomously and has its own IP. This means that it can be accessed through a network at anywhere and anytime using any device. With this technique omnipresence of Sobot can be realized in a ubiquitous space. The eventual goal of this research is to integrate Sobot, Embot, and Mobot to build up a Ubibot so that ubiquitous services through it can be available in a ubiquitous era",space,548
10.1109/icde48307.2020.00021,filtered,2020 IEEE 36th International Conference on Data Engineering (ICDE),IEEE,2020-04-24 00:00:00,ieeexplore,poisonrec: an adaptive data poisoning framework for attacking black-box recommender systems,https://ieeexplore.ieee.org/document/9101655/,"Data-driven recommender systems that can help to predict users' preferences are deployed in many real online service platforms. Several studies show that they are vulnerable to data poisoning attacks, and attackers have the ability to mislead the system to perform as their desires. Considering the realistic scenario, where the recommender system is usually a black-box for attackers and complex algorithms may be deployed in them, how to learn effective attack strategies on such recommender systems is still an under-explored problem. In this paper, we propose an adaptive data poisoning framework, PoisonRec, which can automatically learn effective attack strategies on various recommender systems with very limited knowledge. PoisonRec leverages the reinforcement learning architecture, in which an attack agent actively injects fake data (user behaviors) into the recommender system, and then can improve its attack strategies through reward signals that are available under the strict black-box setting. Specifically, we model the attack behavior trajectory as the Markov Decision Process (MDP) in reinforcement learning. We also design a Biased Complete Binary Tree (BCBT) to reformulate the action space for better attack performance. We adopt 8 widely-used representative recommendation algorithms as our testbeds, and make extensive experiments on 4 different real-world datasets. The results show that PoisonRec has the ability to achieve good attack performance on various recommender systems with limited knowledge.",space,549
10.1109/infcomw.2019.8845276,filtered,IEEE INFOCOM 2019 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),IEEE,2019-05-02 00:00:00,ieeexplore,poster abstract: deep learning workloads scheduling with reinforcement learning on gpu clusters,https://ieeexplore.ieee.org/document/8845276/,"With the recent widespread adoption of deep learning (DL) in academia and industry, more attention are attracted by DL platform, which can support research and development (R&amp;D) of AI firms, institutes and universities. Towards an off-the-shelf distributed GPU cluster, prior work propose prediction-based schedulers to allocate resources for diverse DL workloads. However, the prediction-based schedulers have disadvantages on prediction accuracy and offline-profiling costs. In this paper, we propose a learning-based scheduler, which models the scheduling problem as a reinforcement learning problem, achieving minimum average job completion time and maximum system utilization. The scheduler contains the designs of state space, action space, reward function and update scheme. Furthermore, we will evaluate our proposed scheduler implemented as a plugin of Tensorflow on real cluster and large-scale simulation.",space,550
10.1109/isce.2013.6570213,filtered,2013 IEEE International Symposium on Consumer Electronics (ISCE),IEEE,2013-06-06 00:00:00,ieeexplore,power monitor v2: novel power saving android application,https://ieeexplore.ieee.org/document/6570213/,"This paper presents a novel scheme to derive power saving profiles based on the usage patterns of the Android devices. The entire architecture is developed as an Android app ""Power Monitor v2"" and is deployed to the smart devices. A monitoring module of the app periodically collects several data from the devices and stores them locally. A learning engine then operates on the raw data to generate multiple usage patterns over time and space, which characterizes the user contexts. The engine then processes the patterns to generate power saving profiles dynamically within the devices. The profiles contain several system settings of the smart devices and intelligently optimize power consumption. We also present a real life usage pattern and the power saving profile. The overall battery life for the device estimated to increase by 82%.",space,551
10.1109/iccds.2017.8120453,filtered,"2017 International Conference on Circuits, Devices and Systems (ICCDS)",IEEE,2017-09-08 00:00:00,ieeexplore,power optimization using markov decision process based on multi-parameter constraint modeling,https://ieeexplore.ieee.org/document/8120453/,"Power optimization based on intelligent algorithm draws more and more attention. This article presents a novel low power optimization strategy based on the high level software power management employing Markov Process for charactering the real running workload. This article formulates workload characterization and selection with stochastic process method, and solves the formula using dynamic voltage frequency scaling base on microprocessor. Based on Markov process, the multi-parameter constraints has been employed to exploit the optimization space. Comparing with existing power optimization algorithm, our proposed power optimization algorithm doesn't need any prior data and maintains a value function representing expected reward. As many hardware events can be effectively captured and modeled, this optimization technique is capable to explore an ideal tradeoff in the constraint space.",space,552
10.1109/icnc.2010.5584403,filtered,2010 Sixth International Conference on Natural Computation,IEEE,2010-08-12 00:00:00,ieeexplore,practical travel time prediction algorithms based on neural network and data fusion for urban expressway,https://ieeexplore.ieee.org/document/5584403/,"Current travel time prediction algorithms often need large amount of travel time data to identify the algorithms parameters. However, it is highly costly and time-consuming to obtain many travel time data. In this paper, we proposed an algorithm structure to calculate the travel time which utilize neural network to dynamically predict future speed and employ data fusion to integrate the speed data of different detectors in a urban expressway link. Based on the algorithm structure, two practical travel time prediction algorithms, called as Space Discretization Travel Time Calculation Algorithm (SDTCM) and Speed Integral Travel Time Calculation Method (SITCM), were developed by the discretization of the space and integral of the predicted speed. Vehicle plate recognition technology was used to collect the real travel time data in the test section on Beijing Third-Ring urban expressway to evaluate the algorithms. The obtained results show that the average prediction error of two algorithms are both less than 10% which can meet the requirement of the field applications and the algorithms are easy to be implemented as no travel time data collection are needed in advance. The two algorithms have some advantages and disadvantages over each other in accuracy and smoothness. Although SDTCM is more accurate than SITCM in general, the fluctuation of error of the SITCM is a little smoother than SDTCM.",space,553
10.1109/ijcnn.2003.1224017,filtered,"Proceedings of the International Joint Conference on Neural Networks, 2003.",IEEE,2003-07-24 00:00:00,ieeexplore,prediction of pitch and yaw head movements via recurrent neural networks,https://ieeexplore.ieee.org/document/1224017/,"In virtual-environment (VE) applications, where virtual objects are presented in a head-mounted display, virtual images must be continuously stabilized in space against the user's head motion. Latencies in head-motion compensation cause virtual objects to swim around instead of being stable in space. This results in an unnatural feel, disorientation, and simulation sickness in addition to errors in fitting/matching of virtual and real objects. Visual update delays are a critical technical obstacle for implementation of head-mounted displays in a wide variety of applications. To address this problem, we propose to use machine learning techniques to define a forward model of head movement based on angular velocity information. In particular, we utilize recurrent neural network to capture the temporal pattern of pitch and yaw motion. Our results demonstrate an ability to predict head motion up to 40 ms. ahead thus eliminating the main source of latencies. The accuracy of the system is tested for conditions akin to those encountered in virtual environments. These results demonstrate successful generalization by the learning system.",space,554
10.1109/fpa.1994.636106,filtered,Proceedings of PerAc '94. From Perception to Action,IEEE,1994-09-09 00:00:00,ieeexplore,rcs: a reference model architecture for intelligent control,https://ieeexplore.ieee.org/document/636106/,"The Real-time Control System (RCS) is a reference model architecture for intelligent real time control systems. It partitions the control problem into four basic elements: task decomposition, world modeling, sensory processing, and value judgment. It clusters these elements into computational nodes that have responsibility for specific subsystems and arranges these nodes in hierarchical layers such that each layer has characteristic functionality and timing. The RCS architecture has a systematic regularity, and recursive structure that suggests a canonical form. Systems based on the RCS architecture have been implemented more or less for a wide variety of applications that include loading and unloading of parts and tools in machine tools, controlling machining workstations, performing robotic deburring and chamfering, and controlling space station telerobots, multiple autonomous undersea vehicles, unmanned land vehicles, coal mining automation systems. postal service mail handling systems, and submarine operational automation systems. Software developers accustomed to using RCS for building control systems have found it provides a structured design approach that makes it possible to reuse a great deal of software.",space,555
10.1109/smc-it.2009.40,filtered,2009 Third IEEE International Conference on Space Mission Challenges for Information Technology,IEEE,2009-07-23 00:00:00,ieeexplore,rapid prototyping of planning &amp; scheduling tools,https://ieeexplore.ieee.org/document/5226820/,"The Advanced Planning and Scheduling Initiative, or APSI, is an ESA programme to design and implement an Artificial Intelligence (AI) software infrastructure for planning and scheduling that can generically support different types and classes of space mission operations. The goal of the APSI is twofold: (1)~creating a software framework to improve the cost-effectiveness and flexibility of mission planning support tool development; (2)~bridging the gap between AI planning and scheduling technology and the world of space mission planning. A key aspect of the success of this project is the presence of a flexible timeline representation module that allows to exploit alternatives in the modeling of mission features. This paper shows an example of such a flexibility by using a real problem in the space realm - the HERSCHEL Science Long Term Planning process.",space,556
10.1109/icit.2006.372319,filtered,2006 IEEE International Conference on Industrial Technology,IEEE,2006-12-17 00:00:00,ieeexplore,real time classifier for industrial wireless sensor network using neural networks with wavelet preprocessors,https://ieeexplore.ieee.org/document/4237641/,"Wireless sensor node is embedded of computation unit, sensing unit and a radio unit for communication. Amongst three units communication is the largest consumer of energy. Energy is the prime source for wireless sensor node to function. Hence every aspects of sensor node are designed with energy constraints. Neural Networks in particular the combination of ART1 and FuzzyART(FA) can be used very efficiently for developing Real time Classifier. Wireless sensor networks demand for the real time classification of sensor data. In this paper classification technique using ART1 and Fuzzy ART is discussed. ART1 and FA have very good architectural strategy, which makes it simple for VLSI implementation. The VLSI implementation of the proposed classifier can be a part of embedded microsensor. The paper discusses classification technique, which can reduce the energy need for communication and improves communications bandwidth. The proposed sensor clustering architecture can give distributed storage space for the sensor networks. Wavelet Transform is used as preprocessor for denoising the real word data from sensor node, this makes it much suitable for industrial environment. Many methods of wavelet transforms are available. Simplest Haar 1D transform is used for preprocessing and smoothing the sensor signals. The discrete wavelet transform implemented here helps to extract important feature in the sensor data like sudden changes at various scales.",space,557
10.1109/ic-etite47903.2020.163,filtered,2020 International Conference on Emerging Trends in Information Technology and Engineering (ic-ETITE),IEEE,2020-02-25 00:00:00,ieeexplore,real time object detection in surveillance cameras with distance estimation using parallel implementation,https://ieeexplore.ieee.org/document/9077855/,"Object detection is not only shaping how Computers see and analyze things but it is also helping in the behavior of how an object reacts to the change in its environment. The main application of these object detection sensors or software is to find the location of an object in space or to track its movement. Object detection has infinitely many use cases and in this paper, we are introducing an application that will allow safety of users struck in a disaster and who need to be evacuated. In such cases the main thing to focus and to eradicate is camera noise, saturation and image compression. Our solution is to establish a connection between the person struck in a disaster with fire safety people. This works over a convolutional network that allows us to detect vulnerable things present inside a room that needs to be rescued and can also give an insight of any explosive inside the room. Our model uses Faster-RCNN and COCO which is a pretrained dataset. This allows real time object detection and classification on our network. Using this we were able to detect an object or a person and get him to rescue by providing them a shortest way out of that place. With this we were able to get an accuracy of more than 75% in our object detection model.",space,558
10.1109/ultsym.2009.5441886,filtered,2009 IEEE International Ultrasonics Symposium,IEEE,2009-09-23 00:00:00,ieeexplore,real time adaptive parametric equalization of ultrasonic transducers,https://ieeexplore.ieee.org/document/5441886/,"Parametric equalization is often used to achieve a desired response from an audio transmitter, but is rarely applied to ultrasonic transducer systems. The ability of a broadband ultrasonic transmission and reception system to adapt its frequency and time domain response to changing acoustic conditions would be a distinct advantage in certain applications. Ultrasonic remote monitoring systems would benefit significantly from this ability, as signal levels could be minimized and consequentially the transmitter power consumption decreased. This work presents a real-time adaptive ultrasonic parametric equalizer using optimization driven Matlab code to control the coefficients of a switched capacitor filter network implemented in a Cypress PSOC (Programmable System On a Chip). In this work, adaptive parametric magnitude equalization of a through-transmission ultrasonic system using CUTs (Capacitive Ultrasonic Transducers) has been achieved in real time by tracking a desired SNR (signal to noise ratio) across the operational frequency spectrum. A Matlab general radial basis function (GRBF) artificial neural network (ANN) was developed to control the equalization filter coefficients based on the received frequency response data. The adaptive parametric equaliser adjusts the magnitude of the driving signal to maintain the desired SNR as closely as possible. The neural network was trained using PSO (Particle Swarm Optimization) back-propagation, based on a state space model of the system developed from frequency response data. The developed equalization circuitry, which is switched capacitor based and was fully implemented on the PSOC, is also described.",space,559
10.1109/icdsp.2002.1028228,filtered,2002 14th International Conference on Digital Signal Processing Proceedings. DSP 2002 (Cat. No.02TH8628),IEEE,2002-07-03 00:00:00,ieeexplore,real time nonlinear arma model structure identification,https://ieeexplore.ieee.org/document/1028228/,"This paper addresses the nonlinear autoregressive moving average (NARMA) identification problem in connection with the choice of the model structure (order) and computation of the time varying system coefficients. We introduce an intelligent method that is based on the reformulation of the problem in the standard state space form and the subsequent implementation of a bank of extended Kalman filters, each fitting a different order model. The problem is reduced then to selecting the true model, using the well known multi-model partitioning theory. Simulations illustrate that the proposed method is selecting the correct model order and identifies the time varying model parameters in real time, while it is insensitive to the noise variations.",space,560
10.1109/cac51589.2020.9327198,filtered,2020 Chinese Automation Congress (CAC),IEEE,2020-11-08 00:00:00,ieeexplore,real time production scheduling based on asynchronous advanced actor critic and composite dispatching rule,https://ieeexplore.ieee.org/document/9327198/,"In the era of smart manufacturing, the requirements of real-time, adaptability and long-term optimization of the semiconductor manufacturing system (SMS) are increased due to the further expanded, more complicated and unpredictable uncertainties. This paper addresses the real time production scheduling of SMS to maximize on productivity (PROD) and average daily movement (AvgMOV), and minimize mean cycle time (MCT). We propose an Asynchronous Advanced Actor Critic and composite dispatching rule based real time production scheduling (A3C-CR2) framework, which involves a scheduling knowledge training module and a deployment module. The action space is designed as a combination of the composite dispatching rule (CDR) based continuous scheduling actions. In terms of various performance indices over a long period, the proposed A3C-CR2 approach outperforms other dispatching rules.",space,561
10.1109/icarm52023.2021.9536118,filtered,2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM),IEEE,2021-07-05 00:00:00,ieeexplore,real-time monocular 3d people localization and tracking on embedded system,https://ieeexplore.ieee.org/document/9536118/,"Localizing people in 3D space, rather than in original 2D image plane, provides a more comprehensive understanding of the scene and brings up more potential applications. However, inferring 3D locations usually requires stereo camera or additional sensors since deriving depth information from single image is regarded as an ill-posed problem. With recent progress in deep learning methods, depth estimation neural network can provide convincing depth map by a single RGB image. This work develops a people localization and tracking method based on a monocular camera. Specifically, an efficient self-supervised monocular depth estimation method is adopted to generate pseudo depth map. Afterwards, 2D object detection results are adopted for finding accurate people location. Finally, a filter based tracking method is adopted to fuse temporal information and improve the accuracy. Aiming to provide a real time solution for people tracking on embedded system, our methods are deployed and tested on a NVIDIA Jetson Xavier NX develop kit. The proposed efficient localization and tracking method is validated by a group of field tests. The overall performance reaches 12 fps with an acceptable accuracy compared to ground truth.",space,562
10.1109/icmlc.2003.1260131,filtered,Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),IEEE,2003-11-05 00:00:00,ieeexplore,real-time face detection based on skin-color model and morphology filters,https://ieeexplore.ieee.org/document/1260131/,"This paper presents a new real-time face detection method that uses the skin color model in the YCrCb chrominance space to remove the non-skin-color pixels from the image from which we then extract candidate human face regions. We use the mathematical morphological filter to remove noisy regions and fill holes in the skin-color region. To locate the face region, we compute the similarity between the human face features and the candidate face regions in the image. We have implemented this algorithm in our smart media systems and found it effective in a real environment.",space,563
10.1109/icar.1997.620222,filtered,1997 8th International Conference on Advanced Robotics. Proceedings. ICAR'97,IEEE,1997-07-09 00:00:00,ieeexplore,real-time navigation of a mobile robot using kohonen's topology conserving neural network,https://ieeexplore.ieee.org/document/620222/,"This paper proposes a real-time sensor based navigation method using Kohonen's topology conserving network for navigation of a mobile robot in any uncertain environment. The sensory information including target location with respect to current location of the mobile robot, have been discretely conserved using a two dimensional Kohonen lattice. Reinforcement learning based on a stochastic real valued technique have been implemented to compute the action space for this Kohonen lattice. The proposed scheme learns the input and output weight space of the Kohonen lattice which is generalized to any workspace. The effectiveness of the proposed scheme has been established by simulation where the complete domain of the input-space is quantized based on experience on sensory data encountered in real-time. The input-output mapping conserved by the Kohonen lattice during simulation was used to guide a mobile robot in a real-time environment. Successful navigation of the mobile robot without further training confirms the robustness of the proposed scheme.",space,564
10.1109/icassp.2008.4517901,filtered,"2008 IEEE International Conference on Acoustics, Speech and Signal Processing",IEEE,2008-04-04 00:00:00,ieeexplore,realtime detection of salient moving object: a multi-core solution,https://ieeexplore.ieee.org/document/4517901/,"Detection of salient moving object has great potentials in activity recognition, scene understanding, etc. However techniques to characterizing the object in fine granularity have not been well developed in real applications due to the computational intensity. The emerging multi-core technology in hardware design provides an opportunity for the compute intensive algorithms to boost speed in parallel. This paper proposed a scalable approach to detecting salient moving object which is designed inherently for parallelization. To characterize the object in fine granularity, we extract color-texture homogenous regions as the basic processing unit by image segmentation. To identify salient object, we generate probabilistic template by learning the space-time context. The parallel algorithm is implemented using OpenMP. Evaluations have been carried out on sports, news, and home video data. For the CIF size image, we get processing speed of 51.1 frames per second and near linear speed up on an eight-core machine. It indicates that the algorithm parallelization is a promising solution for practical applications in the multimedia field.",space,565
10.1109/aero47225.2020.9172719,filtered,2020 IEEE Aerospace Conference,IEEE,2020-03-14 00:00:00,ieeexplore,recurrent neural network based prediction to enhance satellite telemetry compression,https://ieeexplore.ieee.org/document/9172719/,"Because of the gradually increasing number of remote measured low and/or high frequency sampled parameters in space applications, aerospace mission operators have to make hard choices on which parameters at which sampling rates should be downlinked. On-board aerospace applications are characterized by limited storage and communication budgets, while lossless data compression schemes should be sufficient enough to enhance transmission efficiency and hence the whole aerospace mission. In this paper, a proposed two-stage lossless compression method for telemetry data is presented. The proposed method consists of a decorrelation stage and an entropy coding one. The Long-Short-Term Memory (LSTM) Recurrent Neural Network (RNN) is implemented as a predictor in the decorrelation stage of the proposed method, and an illustrative method of applying LSTM network for telemetry data samples prediction is presented and figured out. In experiments, different entropy coders: Rice codes, arithmetic method and Huffman algorithm are separately implemented at the second stage. The proposed method is tested by different real telemetry data sets of FUNcube satellite in frames of data words of 8-,10-,16-bits widths. Experimental results show that the proposed method improved compression efficiency based on a single stage of entropy coder: Rice codes, arithmetic code, and Huffman algorithm by a ratio up to: 98%, 21%, and 1.6%, respectively.",space,566
10.1109/icsme46990.2020.00074,filtered,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE,2020-10-02 00:00:00,ieeexplore,regression testing of massively multiplayer online role-playing games,https://ieeexplore.ieee.org/document/9240641/,"Regression testing aims to check the functionality consistency during software evolution. Although general regression testing has been extensively studied, regression testing in the context of video games, especially Massively Multiplayer Online Role-Playing Games (MMORPGs), is largely untouched so far. One big challenge is that game testing requires a certain level of intelligence in generating suitable action sequences among the huge search space, to accomplish complex tasks in the MMORPG. Existing game testing mainly relies on either the manual playing or manual scripting, which are labor-intensive and time-consuming. Even worse, it is often unable to satisfy the frequent industrial game evolution. The recent process in machine learning brings new opportunities for automatic game playing and testing. In this paper, we propose a reinforcement learning-based regression testing technique that explores differential behaviors between multiple versions of an MMORPGs such that the potential regression bugs could be detected. The preliminary evaluation on real industrial MMORPGs demonstrates the promising of our technique.",space,567
10.1109/asama.1999.805407,filtered,"Proceedings. First and Third International Symposium on Agent Systems Applications, and Mobile Agents",IEEE,1999-10-06 00:00:00,ieeexplore,robot media communication: an interactive real-world guide agent,https://ieeexplore.ieee.org/document/805407/,"Describes a guide system and the software architecture for an autonomous, interactive robot based on a multi-agent system. A robot navigation system has been developed allowing the robot to guide people through halls in various types of exhibitions. Our approach uses an infrared location system in the hallway ceilings, making the environment part of a sensor-distributed robot system. The real-world guide agent is composed of a guide agent on a hand-held mobile computer and a robot agent on an autonomous mobile robot. The guide agent plays the role of ""robot media"" in order to integrate information in the information space of the mobile computer and the physical space of the exhibits in order to guide visitors through the physical space. This research aims to develop a cooperative adaptive system using two-way communication among spaces, media and human beings to construct transparent knowledge boundaries between the real space and the virtual space. The virtual space is generated from computer data using shared space technology and it creates a distributed intelligence in order to manage the communication and control the guide in a laboratory. We have experimented with and verified this software architecture using a prototype autonomous mobile robot equipped with a compass.",space,568
10.1109/icsmc.2008.4811760,filtered,"2008 IEEE International Conference on Systems, Man and Cybernetics",IEEE,2008-10-15 00:00:00,ieeexplore,robot navigation using kflann place field,https://ieeexplore.ieee.org/document/4811760/,"This paper presents an implementation of place cells for a robot navigation using the K-iterations fast learning artificial neural networks (KFLANN) clustering algorithm. The KFLANN possesses several desirable properties suitable for place cell robot navigation tasks. The technique proposed is able to autonomously adjust the resolution of cells according to the complexity of the environment. This is achieved through two parameters known as the tolerance and the vigilance of the network. In addition, a navigation system consisting of a topological map building and a place cell path planning strategy is presented. A physical implementation of the system was developed on an autonomous platform and actual results were obtained. The experimental results obtained indicate that the system was able to navigate successfully through the experimental space and also tolerate unexpected discrepancies arising from motor and sensor errors present in a real environment. Furthermore, despite abrupt changes in an environment due to the deliberate introduction of obstacles, the system was still able to cope without changes to the program. The experiment was also extended to include a kidnapped robot scenario and the results were favorable, indicating a positive use of allothetic cue recognition capabilities.",space,569
10.1109/indicon.2016.7839069,filtered,2016 IEEE Annual India Conference (INDICON),IEEE,2016-12-18 00:00:00,ieeexplore,selfie continuous sign language recognition using neural network,https://ieeexplore.ieee.org/document/7839069/,"This works objective is to bring sign language closer to real time implementation on mobile platforms with a video database of Indian sign language created with a mobile front camera in selfie mode. Pre-filtering, segmentation and feature extraction on video frames creates a sign language feature space. Artificial Neural Network classifier on the sign feature space are trained with feed forward nets and tested. ASUS smart phone with 5M pixel front camera captures continuous sign videos containing on average of 220 frames for 18 single handed signs at a frame rate of 30fps. Sobel edge operator's power is enhanced with morphology and adaptive thresholding giving a near perfect segmentation of hand and head portions. Word matching score (WMS) gives the performance of the proposed method with an average WMS of around 90% for ANN with an execution time of 0.5221 seconds during classification. Fully novel method of implementing sign language to put sign language recognition systems on smart phones to make it a real time usage application.",space,570
10.1109/saconet.2018.8585543,filtered,2018 International Conference on Smart Communications in Network Technologies (SaCoNeT),IEEE,2018-10-31 00:00:00,ieeexplore,semantic networks based approach for saas management in cloud computing,https://ieeexplore.ieee.org/document/8585543/,"Nowadays, Cloud Computing has emerged as a new model for hosting, managing and delivering services (IaaS, PaaS, SaaS) over the Internet. Enterprises and vendors are continuously migrating their services to the Cloud, this resulted an exponential amount of data, services and resources stored in data centers. Since there is no standard about Cloud service description or publication, therefore a key requirement that every Cloud provider needs to take into consideration is efficient management of resources and services by providing automated solutions, for a better service publication and discovery. The proposed approach aims to manage Cloud SaaS services for an efficient publication by classifying them into sets according to their domain to reduce the search space then interconnected the SaaS services of the same domain in a Semantic Network using the similarity measure (Input/Output similarity) between concepts. The proposed solution takes advantages from multidimensional index framework, WordNet Domain Ontology and semantic Web. A guided publication process and an implemented prototype are introduced validating the system using a real data set corpus.",space,571
10.1109/aero47225.2020.9172454,filtered,2020 IEEE Aerospace Conference,IEEE,2020-03-14 00:00:00,ieeexplore,semi-supervised machine learning for spacecraft anomaly detection &amp; diagnosis,https://ieeexplore.ieee.org/document/9172454/,"This paper describes Anomaly Detection via Topological-feature Map (ADTM), a data-driven approach to Integrated System Health Management (ISHM) for monitoring the health of spacecraft and space habitats. Developed for NASA Ames Research Center, ADTM leverages proven artificial intelligence techniques for rapidly detecting and diagnosing anomalies in near real-time. ADTM combines Self-Organizing Maps (SOMs) as the basis for modeling system behavior with supervised machine learning techniques for localizing detected anomalies. A SOM is a two-layer artificial neural network (ANN) that produces a low-dimensional representation of the training samples. Once trained on normal system behavior, SOMs are adept at detecting behavior previously not encountered in the training data. Upon detecting anomalous behavior, ADTM uses a supervised classification approach to determine a subset of measurands that characterize the anomaly. This allows it to localize faults and thereby provide extra insight. We demonstrate the effectiveness of our approach on telemetry data collected from a lab-stationed CubeSat (the “LabSat”) connected to software that gave us the ability to trigger several real hardware faults. We include an analysis and discussion of ADTM's performance on several of these fault cases. We conclude with a brief discussion of future work, which contains investigation of a hierarchical SOM-architecture as well as a Case-Based Reasoning module for further assisting astronauts in diagnosis and remediation activities.",space,572
10.1109/icgec.2010.33,filtered,2010 Fourth International Conference on Genetic and Evolutionary Computing,IEEE,2010-12-15 00:00:00,ieeexplore,semi-supervised kernel based progressive svm,https://ieeexplore.ieee.org/document/5715381/,"Most existing semi-supervised methods implemented either the cluster assumption or the manifold assumption. The performance will degrade if the assumption was not proper for the data. A method was proposed by combining both the cluster assumption and the manifold assumption. A semi-supervised kernel which reflected geometric information of the samples was constructed through warping the Reproducing Kernel Hilbert Space. Then the semi-supervised kernel was used in SVM which was based on cluster assumption, and a progressive learning procedure was used in the proposed method. Experiments had been took on synthetic and real data sets, and the results showed that, compared with the progressive SVM with common kernel and the standard SVM with semi supervised kernel, the proposed method using semi-supervised kernel in progressive SVM had competitive performance.",space,573
10.1109/synasc54541.2021.00037,filtered,2021 23rd International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC),IEEE,2021-12-10 00:00:00,ieeexplore,severity prediction of software vulnerabilities based on their text description,https://ieeexplore.ieee.org/document/9700266/,"Software vulnerabilities represent a real challenge nowadays, often resulting in disruption of vital systems and data loss. Due to the multitude of software applications used within a company, system administrators often end up in the situation of facing multiple vulnerabilities at the same time, having no choice but to prioritize the most critical ones. Administrators commonly use vulnerability databases and metric systems to rank vulnerabilities; however, it usually takes from days to weeks for the metrics to be published since these metrics are established by human security analysts and the number of daily discovered exploits is constantly increasing. Therefore, newly discovered vulnerabilities, especially those without an available patch, represent the largest problem. In this paper, we propose a deep learning approach to predict the severity score and other metrics of a vulnerability using only its text description, which is available on discovery. We use a Multi-Task Learning architecture with a pre-trained BERT model for computing vector-space representations of words. Our best configuration achieves a mean absolute error of 0.86 for the severity score and an accuracy of 71.55% for the severity level.",space,574
10.1109/kimas.2003.1245110,filtered,IEMC '03 Proceedings. Managing Technologically Driven Organizations: The Human Side of Innovation and Change (IEEE Cat. No.03CH37502),IEEE,2003-10-04 00:00:00,ieeexplore,sharing learning policies between multiple mobile robots,https://ieeexplore.ieee.org/document/1245110/,"Learning of a complex task usually requires a long learning period. In order to reduce the time of learning, the task is divided into several subtasks. Multiple agents can be used to serve a complex task by learning these subtasks concurrently. With a good knowledge sharing mechanism, the learning policy can be shared or exchanged among these agents and can enhance their learning efficiency. The learning policy is a mapping from system states to actions. The mechanism of sharing or exchanging learning knowledge among multiagent system is proposed. An index of expertise, which indicates the skill level of each learning agent, is presented. This index is used to select the best preferable advice among multiple advices, which can increase the probability of finding solution in the search space. The experiment in which the learning knowledge is exchanged between a mobile robot and a computer simulated agent is implemented in order to verify the validity of the proposed algorithm. The experimental results show that the learning efficiency of the advisor agent is increased and the advisee robot can use the given advice for avoiding collision with obstacle successfully in the real world implementation.",space,575
10.1109/lars-sbr-wre48964.2019.00060,filtered,"2019 Latin American Robotics Symposium (LARS), 2019 Brazilian Symposium on Robotics (SBR) and 2019 Workshop on Robotics in Education (WRE)",IEEE,2019-10-25 00:00:00,ieeexplore,sim-to-real in reinforcement learning for everyone,https://ieeexplore.ieee.org/document/9018558/,"In reinforcement learning (RL), it remains a challenge to have a robotic agent perform a task in the real world for which it was trained in simulation. In this paper, we present our work training a low-cost robotic arm in simulation to move towards a predefined target in space, represented by a red ball in an RGB image, and transferring the capability to the real arm. We exercised the entire end-to-end flow including the 3D modeling of the arm, training of a state-of-the-art RL policy in simulation with multiple actors in a distributed fashion, domain randomization in order to close the sim-to-real gap, and finally the execution of the trained model in the real robot. We also implemented a mechanism to edit the image captured from the camera before sending it to the model for inference, which allowed us to automate reward computation in the physical world. Our work highlights important challenges of training RL agents and moving them to the real world, validating important aspects shown by other works as well as detailing steps not explained by some of them (e.g. how to compute the reward in the real world). The conducted experiments show the improvements observed as the techniques were added to the final solution.",space,576
10.1109/ijcnn.2000.859462,filtered,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,IEEE,2000-07-27 00:00:00,ieeexplore,"simulating the evolution of 2d pattern recognition on the cam-brain machine, an evolvable hardware tool for building a 75 million neuron artificial brain",https://ieeexplore.ieee.org/document/859462/,"This paper presents some simulation results of the evolution of 2D visual pattern recognizers to be implemented very shortly on real hardware, namely the ""CAM-Brain Machine"" (CBM), an FPGA based piece of evolvable hardware which implements a genetic algorithm (GA) to evolve a 3D cellular automata (CA) based neural network circuit module, of approximately 1,000 neurons, in about a second, i.e. a complete run of a GA, with tens of thousands of circuit growths and performance evaluations. Up to 65,000 of these modules, each of which is evolved with a humanly specified function, can be downloaded into a large RAM space, and interconnected according to humanly specified artificial brain architectures. This RAM, containing an artificial brain with up to 75 million neurons, is then updated by the CBM at a rate of 130 billion CA cells per second. Such speeds will enable real time control of robots and hopefully the birth of a new research field that we call ""brain building"". The first such artificial brain, to be built at STARLAB in 2000 and beyond, will be used to control the behaviors of a life sized kitten robot called ""Robokitty"". This kitten robot will need 2D pattern recognizers in the visual section of its artificial brain. This paper presents simulation results on the evolvability and generalization properties of such recognizers.",space,577
10.1109/aero47225.2020.9172439,filtered,2020 IEEE Aerospace Conference,IEEE,2020-03-14 00:00:00,ieeexplore,"smart &amp; integrated management system - smart cities, epidemiological control tool using drones",https://ieeexplore.ieee.org/document/9172439/,"This paper describes the development of a real application using Drones over urban regions to help the authorities at epidemiological control through a disruptive solutions based on a customizable Smart &amp; Integrated Management System (SIGI), devices and software based on the Enterprise Resource Planning (ERP) concept. Compound by management software, Drones and specific IoT devices, both referred to as sensors, the sensors collect the data of the interest areas in real time, creating a specified database. Based on the data collected from the interest areas, SIGI software has the ability to show real-time situational analysis of these areas and allows that the administrator can optimize resources (material and human) improving the efficiency of resource allocation in these areas. In addition to the development of the management software, the development of sensors to collect the information in the field and update these information to the database of the management software, are considered. The sensors will be recognized as IoT devices for the collection of meteorological data, images and command / control Drones. Initially the system will be customized, using an Artificial Intelligence tool, to collect data and identify the outbreaks of the dengue mosquito, zika and Chikungunya, nominee by risk areas. After the definition of the potential risk areas, in a complementary way, a totally customized Drone will be used to map these areas of interest, generating aerial photographs, identifying and geotagging the potential “targets”, which will allow the agents to identify potential mosquito breeding sites. After the identification of breeding areas, the next step will be the effective combat of the vectors, using the Drones to fly over the areas of interest, where biological defenses will be “dropped” over the targets to combat mosquitoes. Due some Drone flight restrictions over the cities, the whole process will be monitored by a situation room, that will be able to control the Drone remotely, access the air space controller, reads the sensors installed in the city (field), that will measure, for example, rainfall through weather stations installed in risk areas and subsequently processed by Intelligent System Integrated Management (SIGI), which will result to the information public official reflecting the situational analysis of the areas, which will enable a better management of available resources, helping the public agent, preventively in the decision making.",space,578
10.1109/sere-c.2014.24,filtered,2014 IEEE Eighth International Conference on Software Security and Reliability-Companion,IEEE,2014-07-02 00:00:00,ieeexplore,software reliability virtual testing for reliability assessment,https://ieeexplore.ieee.org/document/6901643/,"The basic condition of software reliability assessment is failure time, which must be acquired during a test based on operational profile or on real usage. Failure data from software development or other non-software reliability testing (SRT) cannot be used for reliability evaluation because such data do not include usage information and failure time. This paper presents a software reliability virtual test (SRVT), which constructs the software input space model and the known failure input space model through which possible failure time can be determined by matching the randomly generate inputs. An experiment comparing SRT and SRVT with different thresholds is introduced to verify SRVT. Results indicate that SRVT saves a large amount of testing time while providing reliability assessment with acceptable accuracy.",space,579
10.1109/wict.2012.6409060,filtered,2012 World Congress on Information and Communication Technologies,IEEE,2012-11-02 00:00:00,ieeexplore,software effort prediction using unsupervised learning (clustering) and functional link artificial neural networks,https://ieeexplore.ieee.org/document/6409060/,"Software cost estimation continues to be an area of concern for managing of software development industry. We use unsupervised learning (e.g., clustering algorithms) combined with functional link artificial neural networks for software effort prediction. The unsupervised learning (clustering) indigenously divide the input space into the required number of partitions thus eliminating the need of ad-hoc selection of number of clusters. Functional link artificial neural networks (FLANNs), on the other hand is a powerful computational model. Chebyshev polynomial has been used in the FLANN as a choice for functional expansion to exhaustively study the performance. Three real life datasets related to software cost estimation have been considered for empirical evaluation of this proposed method. The experimental results show that our method could significantly improve prediction accuracy of conventional FLANN and has the potential to become an effective method for software cost estimation.",space,580
10.1109/grc.2010.13,filtered,2010 IEEE International Conference on Granular Computing,IEEE,2010-08-16 00:00:00,ieeexplore,solving course timetabling problem using interrelated approach,https://ieeexplore.ieee.org/document/5576024/,"University timetabling is very hectic resources allocation job against tough constraints. The problem is broadly recognized on account of its crucial significance for curriculum activities. Its intensive complexity has challenged the researchers from diverse disciplines for several decades. In the research paper, a novel interrelated approach is employed that primarily depends on Genetic Algorithm supported by Local Search algorithm. Local Search systematizes the events in each timetabling chromosome up to certain degree. Later on GA is likely to obtain more feasible solution available on the search space. The approach has been applied on real dataset and the research direction is validated by promising outcome. The bottom line is minimizing computational time for GA by initializing the set of partial solutions. In addition, exploitation of the resources usage and effective events deployment are key objectives.",space,581
10.1109/raics.2013.6745450,filtered,2013 IEEE Recent Advances in Intelligent Computational Systems (RAICS),IEEE,2013-12-21 00:00:00,ieeexplore,sparsity-based representation for categorical data,https://ieeexplore.ieee.org/document/6745450/,"Over the past few decades, many algorithms have been continuously evolving in the area of machine learning. This is an era of big data which is generated by different applications related to various fields like medicine, the World Wide Web, E-learning networking etc. So, we are still in need for more efficient algorithms which are computationally cost effective and thereby producing faster results. Sparse representation of data is one giant leap toward the search for a solution for big data analysis. The focus of our paper is on algorithms for sparsity-based representation of categorical data. For this, we adopt a concept from the image and signal processing domain called dictionary learning. We have successfully implemented its sparse coding stage which gives the sparse representation of data using Orthogonal Matching Pursuit (OMP) algorithms (both Batch and Cholesky based) and its dictionary update stage using the Singular Value Decomposition (SVD). We have also used a preprocessing stage where we represent the categorical dataset using a vector space model based on the TF-IDF weighting scheme. Our paper demonstrates how input data can be decomposed and approximated as a linear combination of minimum number of elementary columns of a dictionary which so formed will be a compact representation of data. Classification or clustering algorithms can now be easily performed based on the generated sparse coded coefficient matrix or based on the dictionary. We also give a comparison of the dictionary learning algorithm when applying different OMP algorithms. The algorithms are analysed and results are demonstrated by synthetic tests and on real data.",space,582
10.1109/iros.2014.6943162,filtered,2014 IEEE/RSJ International Conference on Intelligent Robots and Systems,IEEE,2014-09-18 00:00:00,ieeexplore,spatio-temporal motion features for laser-based moving objects detection and tracking,https://ieeexplore.ieee.org/document/6943162/,"This paper proposes a spatio-temporal motion feature detection and tracking method using range sensors working on a moving platform. The proposed spatio-temporal motion features are similar to optical flow but are extended on a moving platform with fusion of odometry and show much better classification accuracy with consideration of different uncertainties. In the proposal, the ego motion is compensated by odometry sensors and the laser scan points are accumulated and represented as space-time point clouds, from which the velocities and moving directions can be extracted. Based on these spatio-temporal features, a supervised learning technique is applied to classify the points as static or moving and Kalman filters are implemented to track the moving objects. A real experiment is performed during day and night on an autonomous vehicle platform and shows promising results in a crowded and dynamic environment.",space,583
10.1109/icnn.1994.374437,filtered,Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94),IEEE,1994-07-02 00:00:00,ieeexplore,spatiotemporal computation with a general purpose analog neural computer: real-time visual motion estimation,https://ieeexplore.ieee.org/document/374437/,"An analog neural network implementation of spatiotemporal feature extraction for real-time visual motion estimation is presented. Visual motion can be represented as an orientation in the space-time domain. Thus, motion estimation translates into orientation detection. The spatiotemporal orientation detector discussed is based on Adelson and Bergen's model with modifications to accommodate the computational limitations of hardware analog neural networks. The analog neural computer used here has the unique property of offering temporal computational capabilities through synaptic time-constants. These time-constants are crucial for implementing the spatiotemporal filters. Analysis, implementation and performance of the motion filters are discussed. The performance of the neural motion filters is found to be consistent with theoretical predictions and the real stimulus motion.&lt;<ETX>&gt;</ETX>",space,584
10.1109/robot.1996.506888,filtered,Proceedings of IEEE International Conference on Robotics and Automation,IEEE,1996-04-28 00:00:00,ieeexplore,stereo sketch: stereo vision-based target reaching behavior acquisition with occlusion detection and avoidance,https://ieeexplore.ieee.org/document/506888/,"In this paper, we proposed a method by which a stereo vision-based mobile robot learns to reach a target by detecting and avoiding occlusions. We call the internal representation that describes the learning behavior ""stereo sketch"". First, an input scene is segmented into homogeneous regions by the enhanced ISODATA algorithm with minimum description length principle in terms of image coordinates and disparity information obtained from the fast stereo matching unit based on the coarse-to-fine control method. Then, in terms of the segmented regions including the target area and their occlusion status identified during the stereo and motion disparity estimation process, we construct a state space for the reinforcement learning method to obtain a target reaching behavior. As a result the robot can avoid obstacles without explicitly describing them. We give the computer simulation results and real robot implementation to show the validity of our method.",space,585
10.1109/icse-nier.2019.00031,filtered,2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER),IEEE,2019-05-31 00:00:00,ieeexplore,structural coverage criteria for neural networks could be misleading,https://ieeexplore.ieee.org/document/8805667/,"There is a dramatically increasing interest in the quality assurance for DNN-based systems in the software engineering community. An emerging hot topic in this direction is structural coverage criteria for testing neural networks, which are inspired by coverage metrics used in conventional software testing. In this short paper, we argue that these criteria could be misleading because of the fundamental differences between neural networks and human written programs. Our preliminary exploration shows that (1) adversarial examples are pervasively distributed in the finely divided space defined by such coverage criteria, while available natural samples are very sparse, and as a consequence, (2) previously reported fault-detection ""capabilities"" conjectured from high coverage testing are more likely due to the adversary-oriented search but not the real ""high"" coverage.",space,586
10.1109/icassp.2003.1201643,filtered,"2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03).",IEEE,2003-04-10 00:00:00,ieeexplore,structural risk minimization using nearest neighbor rule,https://ieeexplore.ieee.org/document/1201643/,"We present a novel nearest neighbor rule-based implementation of the structural risk minimization principle to address a generic classification problem. We propose a fast reference set thinning algorithm on the training data set similar to a support vector machine approach. We then show that the nearest neighbor rule based on the reduced set implements the structural risk minimization principle, in a manner which does not involve selection of a convenient feature space. Simulation results on real data indicate that this method significantly reduces the computational cost of the conventional support vector machines, and achieves a nearly comparable test error performance.",space,587
10.1109/roman.1994.365922,filtered,Proceedings of 1994 3rd IEEE International Workshop on Robot and Human Communication,IEEE,1994-07-20 00:00:00,ieeexplore,surface display: a force feedback system simulating the surface of an object,https://ieeexplore.ieee.org/document/365922/,"Force feedback is an interface based on the phenomenon of contact. In the implementation of a virtual force feedback environment, the object is defined in a computer and the user is in the real world. Therefore, an interface device is required to transmit touch sensation to the user's finger when he or she touches a virtual object in virtual space. In this paper, methodology to realize force feedback is categorized from this view point, and the idea of ""surface display"" is presented as a force feedback method in a virtual environment.&lt;<ETX>&gt;</ETX>",space,588
10.1109/tencon.2010.5685846,filtered,TENCON 2010 - 2010 IEEE Region 10 Conference,IEEE,2010-11-24 00:00:00,ieeexplore,symbol-based soft relaying strategy for cooperative wireless networks,https://ieeexplore.ieee.org/document/5685846/,"We propose an improved Soft Forwarding (SF) protocol which is based on the symbol-by-symbol detection at the relay node, unlike in the conventional relaying strategy which implements a bit-by-bit signal analysis at the relay node. A relay node which is typically acts like a repeater can avoid unnecessary computation complexity and above all, is resource efficient if compared to the baseline SF. The proposed strategy is implemented based on Maximum Likelihood Detector (MLD) criterion. We simplify the MLD rule to one-dimensional real space such that all metric values are compared in a straight line making the ML performance analysis tractable. Simulations have shown that the proposed schemes outperform the conventional schemes in error rate performance.",space,589
10.1109/syscon47679.2020.9275860,filtered,2020 IEEE International Systems Conference (SysCon),IEEE,2020-09-20 00:00:00,ieeexplore,the role of attribute ranker using classification for software defect-prone data-sets model: an empirical comparative study,https://ieeexplore.ieee.org/document/9275860/,"Feature selection, is an issue firmly identified with size decrease of data-sets model. The target of feature selection is to recognize features in the data-set as significant, and dispose of some other feature as unimportant and repetitive data. Since feature selection diminishes the dimensionality of the data-sets model, it holds out the probability of increasingly successful and quick activity of data mining algorithm which can be worked quicker and all the more adequately by utilizing feature selection. In this research paper we will investigate feature extraction Principal Component Analysis (PCA) with attribute ranker search technique. In practice, Principal Component Analysis with attribute ranker search strategy isn’t just used to improve extra storage space or the computational accuracy and efficiency of the classification algorithm, however can likewise enhance the prescient presentation by diminishing the scourge of dimensionality — particularly on the off chance that we are working with software defect-prone or non-defected data-sets models. We have used 10 datasets models, these datasets models basically REPOSITORY model of NASA which contain binary class defected and non-defected datasets models. We have also used 6 classifiers for comparatively analysis between objective model and real datasets model. We illustrated the comparatively analysis between PCA Ranker Search method with No-PCA. We have also compared classifiers efficiency with each other. The most efficient and useful classifier are the Bagging and Multilayer Perceptron at all attribute ranking search method. But the comparatively analysis between the classifiers that Naïve bayes and MultiLayer Perceptron have well increased the correctly classified instances percentage in overall software fault forecast. One more comparison between the PCA Ranker Search Method and No-PCA that is attribute ranker search method is really good for increasing accuracy and efficiency for software fault forecast dataset model as compare the no-PCA method.",space,590
10.1109/fie.2001.963683,filtered,31st Annual Frontiers in Education Conference. Impact on Engineering and Science Education. Conference Proceedings (Cat. No.01CH37193),IEEE,2001-10-13 00:00:00,ieeexplore,the design of pedagogical agent for distance virtual experiment,https://ieeexplore.ieee.org/document/963683/,"Experimental learning about electric-machinery is high risk and time-consuming, students do not always enjoy the laboratory, the teacher also needs to pay full attention to all students and equipment, besides it is hard to provide enough machine sets and space for learning. So it is our motivation to design a virtual laboratory for the teaching of electric-machinery experiment by Internet under the more real environment without concern about danger or limitations. It is taught by an experienced teacher using a pedagogical agent which is the most important key factor. Taking the above features into consideration, an interactive virtual laboratory based on an expert system has been designed and implemented to improve the learning, operation and control in electric-machinery experiments. The system set up is a highly intelligent and interactive mechanism. Through the system, students could have a more complete environment for distance learning.",space,591
10.1109/icacite51222.2021.9404738,filtered,2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),IEEE,2021-03-05 00:00:00,ieeexplore,the learning approaches using augmented reality in learning environments: meta-analysis,https://ieeexplore.ieee.org/document/9404738/,"With the emergence of Industrial Revolution 4.0, the educational settings are changing quickly. Augmented Reality (AR) is one of the upcoming technologies. AR enhances the real world by overlaying/augmenting the virtual/digital information over it. It provides the user with the ability to interact with the created virtual world in real space. The aim of this study is to classify the learning approaches implemented through AR technology. The technique used for the analysis is derived from systematic search of online literature databases like Taylor Francis, Web of Science, Springer, ScienceDirect and Scopus. The keywords used for the search include learning approaches, AR, AR in education, AR in learning and teaching and integration approaches. The findings of this research work highlights 4 categories of educational learning approaches that highlight AR. The approaches are experimental learning, game-based, interactive and collaborative learning. The research findings can be referred by other researchers and educators to identify the potential of AR in education and the learning approaches currently used with AR for their further research on how these approaches can be effectively and efficiently implemented in educational settings.",space,592
10.1109/ijcnn.2015.7280318,filtered,2015 International Joint Conference on Neural Networks (IJCNN),IEEE,2015-07-17 00:00:00,ieeexplore,the on-line curvilinear component analysis (oncca) for real-time data reduction,https://ieeexplore.ieee.org/document/7280318/,"Real time pattern recognition applications often deal with high dimensional data, which require a data reduction step which is only performed offline. However, this loses the possibility of adaption to a changing environment. This is also true for other applications different from pattern recognition, like data visualization for input inspection. Only linear projections, like the principal component analysis, can work in real time by using iterative algorithms while all known nonlinear techniques cannot be implemented in such a way and actually always work on the whole database at each epoch. Among these nonlinear tools, the Curvilinear Component Analysis (CCA), which is a non-convex technique based on the preservation of the local distances into the lower dimensional space, plays an important role. This paper presents the online version of CCA. It inherits the same features of CCA, is adaptive in real time and tracks non-stationary high dimensional distributions. It is composed of neurons with two weights: one, pointing to the input space, quantizes the data distribution, and the other, pointing to the output space, represents the projection of the first weight. This on-line CCA has been conceived not only for the previously cited applications, but also as a basic tool for more complex supervised neural networks for modelling very complex high dimensional data. This algorithm is tested on 2-D and 3-D synthetic data and on an experimental database concerning the bearing faults of an electrical motor, with the goal of novelty (fault) detection.",space,593
10.1109/iccv.2013.129,filtered,2013 IEEE International Conference on Computer Vision,IEEE,2013-12-08 00:00:00,ieeexplore,towards motion aware light field video for dynamic scenes,https://ieeexplore.ieee.org/document/6751235/,"Current Light Field (LF) cameras offer fixed resolution in space, time and angle which is decided a-priori and is independent of the scene. These cameras either trade-off spatial resolution to capture single-shot LF or tradeoff temporal resolution by assuming a static scene to capture high spatial resolution LF. Thus, capturing high spatial resolution LF video for dynamic scenes remains an open and challenging problem. We present the concept, design and implementation of a LF video camera that allows capturing high resolution LF video. The spatial, angular and temporal resolution are not fixed a-priori and we exploit the scene-specific redundancy in space, time and angle. Our reconstruction is motion-aware and offers a continuum of resolution tradeoff with increasing motion in the scene. The key idea is (a) to design efficient multiplexing matrices that allow resolution tradeoffs, (b) use dictionary learning and sparse representations for robust reconstruction, and (c) perform local motion-aware adaptive reconstruction. We perform extensive analysis and characterize the performance of our motion-aware reconstruction algorithm. We show realistic simulations using a graphics simulator as well as real results using a LCoS based programmable camera. We demonstrate novel results such as high resolution digital refocusing for dynamic moving objects.",space,594
10.1109/iv.2014.39,filtered,2014 18th International Conference on Information Visualisation,IEEE,2014-07-18 00:00:00,ieeexplore,towards the identification of consumer trajectories in geo-located search data,https://ieeexplore.ieee.org/document/6902904/,"Modern geo-positioning system (GPS) enabled smart phones are generating an increasing volume of information about their users, including geo-located search, movement, and transaction data. While this kind of data is increasingly rich and offers many grand opportunities to identify patterns and predict behaviour of groups and individuals, it is not immediately obvious how to develop a framework for extracting plausible inferences from these data. In our case, we have access to a large volume of real user data from the Point smart phone application, and we have developed a generic and layered system architecture to incrementally find aggregate items of interest within that data. This includes time and space correlations, e.g., are people searching for dinner and a movie, distributions of usage patterns and platforms, e.g., geographic distribution of Android, Apple, and BlackBerry users, and clustering to identify relatively complex search and movement patterns we call ""consumer trajectories."" Our pursuit of these kinds of patterns has helped guide our development of information extraction, machine learning, and visualization methods that provide systematic tools for investigating the geo-located data, and for the development of both conceptual tools and visualization tools in aid of finding both interesting and useful patterns in that data. Included in our system architecture is the ability to consider the difference between exploratory and explanatory hypotheses on data patterns, as well as the deployment of multiple visualization methods that can provide alternatives to help expose interesting patterns. In our introduction to our framework here, we provide examples of formulating hypotheses on geo-located behaviour, and how a variety of methods including those from machine learning and visualization, can help confirm or deny the value of such hypotheses as they emerge. In this particular case, we provide an initial basis for identifying semantically motivated data artifacts we call geo-located consumer trajectories. We investigate their plausibility with a variety of time and space series clustering and visualization models.",space,595
10.1109/icivc47709.2019.8980828,filtered,"2019 IEEE 4th International Conference on Image, Vision and Computing (ICIVC)",IEEE,2019-07-07 00:00:00,ieeexplore,traffic lights detection and recognition algorithm based on multi-feature fusion,https://ieeexplore.ieee.org/document/8980828/,"Detection and recognition of traffic lights is important for intelligent assisted driving. Traditional color space based traffic lights detection algorithms could be easily affected by other objects (such as buildings, car taillights) in the surrounding environment, and the detection accuracy and real-time performance are not ideal enough. Generally, the deep learning based methods have better advantages of real-time and accuracy performance for the normal scene with obvious traffic lights targets. However, the small traffic lights targets detection rate and accuracy in night-time of these methods are still can't be satisfactory. To solve this problem, this paper proposed a novel traffic lights detection and recognition algorithm based on multi-feature fusion, which can be implemented in two steps (detection and recognition). For the first step, the SLIC (simple linear iterative clustering) super-pixel segmentation algorithm is used for purposes reducing the image data processing complexity and improving the real-time performance. The mean-shift algorithm was used to cluster the HSV (Hue, Saturation, Value) color space components respectively for enhancing the target data and reducing the interference from other targets. For the second step, the feature information extracted by CNN (Convolutional Neural Network) and HOG(Histogram of Oriented Gradient) feature are fused. The SVM (Support Vector Machine) classifier is trained on a data set of traffic lights established by our own. To verify the proposed algorithm in this paper, amount of experiments were carried out in real traffic scenes. Experimental results show that this algorithm almost has the same real-time performance with YOLO_V3 neural network and a better accuracy.",space,596
10.1109/globecom38437.2019.9014245,filtered,2019 IEEE Global Communications Conference (GLOBECOM),IEEE,2019-12-13 00:00:00,ieeexplore,traffic offloading and power allocation for green hetnets using reinforcement learning method,https://ieeexplore.ieee.org/document/9014245/,"In order to satisfy the boosting mobile traffic demand, the deployment of small cells has been regarded as a feasible solution. But the growth of network infrastructure leads to a tremendous increase of energy consumption. Using renewable energy harvested from the environment to power the small cell base station, forming green heterogeneous networks (HetNets), can help reduce the conventional energy consumption. However, the stochastic user demand and random renewable energy harvesting amount have brought new challenges for the network operation. Based on reinforcement learning, this paper proposes a decentralized and a centralized base station operation scheme. Assuming each base station operates individually, the energy efficiency maximization problem is modeled as a general-sum game. After defining the state, action and reward of each base station, the problem can be solved by multi-agent reinforcement learning. Assuming there is a centralized controller, then the whole network can be seen as a huge agent, thus, the problem can be solved using deep reinforcement learning since the state space is too complicated. Simulation results show the centralized scheme shows higher performance but need more signaling overheads. The energy efficiency is lower for the decentralized scheme but it can be realized easier in real life. Both our proposed scheme can achieve significant energy efficiency improvement compared to the greedy scheme.",space,597
10.1109/ijcnn.2000.861451,filtered,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,IEEE,2000-07-27 00:00:00,ieeexplore,training neural networks with threshold activation functions and constrained integer weights,https://ieeexplore.ieee.org/document/861451/,"Evolutionary neural network training algorithms are presented. These algorithms are applied to train neural networks with weight values confined to a narrow band of integers. We constrain the weights and biases in the range [-2/sup k+1/+1, 2/sup k-1/-1], for k=3, 4, 5, thus they can be represented by just k bits. Such neural networks are better suited for hardware implementation than the real weight ones. Mathematical operations that are easy to implement in software might often be very burdensome in the hardware and therefore more costly. Hardware-friendly algorithms are essential to ensure the functionality and cost effectiveness of the hardware implementation. To this end, in addition to the integer weights, the trained neural networks use threshold activation functions only, so hardware implementation is even easier. These algorithms have been designed keeping in mind that the resulting integer weights require less bits to be stored and the digital arithmetic operations between them are easier to be implemented in hardware. Obviously, if the network is trained in a constrained weight space, smaller weights are found and less memory is required. On the other hand, as we have found here, the network training procedure can be more effective and efficient when larger weights are allowed. Thus, for a given application a trade off between effectiveness and memory consumption has to be considered. Our intention is to present results of evolutionary algorithms on this difficult task. Based on the application of the proposed class of methods on classical neural network benchmarks, our experience is that these methods are effective and reliable.",space,598
10.1109/seams.2017.11,filtered,2017 IEEE/ACM 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),IEEE,2017-05-23 00:00:00,ieeexplore,transfer learning for improving model predictions in highly configurable software,https://ieeexplore.ieee.org/document/7968130/,"Modern software systems are built to be used in dynamic environments using configuration capabilities to adapt to changes and external uncertainties. In a self-adaptation context, we are often interested in reasoning about the performance of the systems under different configurations. Usually, we learn a black-box model based on real measurements to predict the performance of the system given a specific configuration. However, as modern systems become more complex, there are many configuration parameters that may interact and we end up learning an exponentially large configuration space. Naturally, this does not scale when relying on real measurements in the actual changing environment. We propose a different solution: Instead of taking the measurements from the real system, we learn the model using samples from other sources, such as simulators that approximate performance of the real system at low cost. We define a cost model that transform the traditional view of model learning into a multi-objective problem that not only takes into account model accuracy but also measurements effort as well. We evaluate our cost-aware transfer learning solution using real-world configurable software including (i) a robotic system, (ii) 3 different stream processing applications, and (iii) a NoSQL database system. The experimental results demonstrate that our approach can achieve (a) a high prediction accuracy, as well as (b) a high model reliability.",space,599
10.1109/wcnc.2018.8377359,filtered,2018 IEEE Wireless Communications and Networking Conference (WCNC),IEEE,2018-04-18 00:00:00,ieeexplore,transferring knowledge for tilt-dependent radio map prediction,https://ieeexplore.ieee.org/document/8377359/,"Fifth generation wireless networks (5G) will face key challenges caused by diverse patterns of traffic demands and massive deployment of heterogeneous access points. In order to handle this complexity, machine learning techniques are expected to play a major role. However, due to the large space of parameters related to network optimization, collecting data to train models for all possible network configurations can be prohibitive. In this paper, we analyze the possibility of performing a knowledge transfer, in which a machine learning model trained on a particular network configuration is used to predict a quantity of interest in a new, unknown setting. We focus on the tilt-dependent received signal strength maps as quantities of interest and we analyze two cases where the knowledge acquired for a particular antenna tilt setting is transferred to (i) a different tilt configuration of the same antenna or (ii) a different antenna with the same tilt configuration. Promising results supporting knowledge transfer are obtained through extensive experiments conducted using different machine learning models on a real dataset.",space,600
10.1109/ictai.2014.37,filtered,2014 IEEE 26th International Conference on Tools with Artificial Intelligence,IEEE,2014-11-12 00:00:00,ieeexplore,triangulation versus graph partitioning for tackling large real world qualitative spatial networks,https://ieeexplore.ieee.org/document/6984473/,"There has been interest in recent literature in tackling very large real world qualitative spatial networks, primarily because of the real datasets that have been, and are to be, offered by the Semantic Web community and scale up to millions of nodes. The proposed techniques for tackling such large networks employ the following two approaches for retaining the sparseness of their underlying graphs and reasoning with them: (i) graph triangulation and sparse matrix implementation, and (ii) graph partitioning and parallelization. Regarding the latter approach, an implementation has been offered recently, presented in [AAAI, 2014]. However, although the implementation looks promising and with space for improvement, an improper use of competing solvers in the evaluation process resulted in the wrong conclusion that it is able to provide fast consistency for very large qualitative spatial networks with respect to the state-of-the-art. In this paper, we review the two aforementioned approaches and provide new results that are different to the results presented in [AAAI, 2014] by properly re-evaluating them with the benchmark dataset of that paper. Thus, we establish a clear view on the state-of-the-art solutions for reasoning with large real world qualitative spatial networks efficiently, which is the main result of this paper.",space,601
10.1109/spawc51858.2021.9593170,filtered,2021 IEEE 22nd International Workshop on Signal Processing Advances in Wireless Communications (SPAWC),IEEE,2021-09-30 00:00:00,ieeexplore,turning channel noise into an accelerator for over-the-air principal component analysis,https://ieeexplore.ieee.org/document/9593170/,"T In recent years, the attempts on distilling mobile data into useful knowledge have led to the deployment of machine learning algorithms at the network edge. Principal component analysis (PCA) is a classic technique for extracting the linear structure of a dataset, which is useful for feature extraction and data compression. In this work, we propose the deployment of distributed PCA over a multi-access channel based on the algorithm of stochastic gradient descent to learn the dominant feature space of a distributed dataset at multiple devices. Over- the-air aggregation is adopted to reduce the multi-access latency, giving the name over-the-air PCA. The novelty of this design lies in exploiting channel noise to accelerate the descent in the region around each saddle point encountered by gradient descent, thereby increasing the convergence speed of over-the-air PCA. The idea is materialized by proposing a power-control scheme controlling the level of channel noise accordingly. The scheme is proved to achieve faster convergence than in the case without power control by experiments on real datasets.",space,602
10.1109/vrw52623.2021.00262,filtered,2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),IEEE,2021-04-01 00:00:00,ieeexplore,turning a messy room into a fully immersive vr playground,https://ieeexplore.ieee.org/document/9419119/,"In this study, to enable a VR experience with an HMD even in a space with obstacles, we developed a real-time construction system of a reality-based VR space that does not impair the atmosphere of the virtual world. In addition, we aim to construct a VR space that is easier to recognize its structure by classifying ""objects that are boundaries of space"" and ""ordinary obstacles"" using a deep learning network and superimposing virtual objects corresponding to each type of real object. We implemented a real-time construction system for a VR space that reflects the distribution of objects in a real space using two depth cameras mounted on HMD, and created guidelines and applications for using the proposed system.",space,603
10.1109/isic.2000.882942,filtered,Proceedings of the 2000 IEEE International Symposium on Intelligent Control. Held jointly with the 8th IEEE Mediterranean Conference on Control and Automation (Cat. No.00CH37147),IEEE,2000-07-19 00:00:00,ieeexplore,two suggestions for efficient implementation of cmac's,https://ieeexplore.ieee.org/document/882942/,"The CMAC (Cerebellar Model Articulation Controller) suffers from two important problems: the huge amount of memory needed for its implementation in many common situations, and the lack of a systematic way for selecting appropriate values for its parameters, particularly number of quantization intervals. This paper presents two proposals for addressing these difficulties: 1) a dynamic implementation that requires memory only for those weights needed to represent the training data set, and that performs linear interpolation when a query using other weights is requested; and 2) consists of the definition of an index of correlation from which the optimal number of quantization intervals that should be assigned to each dimension of the input space that can be found. Experiments are performed for two synthetic cases and for one set of real data. These are used to model the dynamic behaviour of a real sensor-based car. Figures are given to show the memory savings and mean squared error obtained.",space,604
10.1109/aero.2017.7943775,filtered,2017 IEEE Aerospace Conference,IEEE,2017-03-11 00:00:00,ieeexplore,uav tracking and following a ground target under motion and localisation uncertainty,https://ieeexplore.ieee.org/document/7943775/,"Unmanned Aerial Vehicles (UAVs) are increasingly being used in numerous applications, such as remote sensing, environmental monitoring, ecology and search and rescue missions. Effective use of UAVs depends on the ability of the system to navigate in the mission scenario, especially if the UAV is required to navigate autonomously. There are particular scenarios in which UAV navigation faces challenges and risks. This creates the need for robust motion planning capable of overcoming different sources of uncertainty. One example is a UAV flying to search, track and follow a mobile ground target in GPS-denied space, such as below canopy or in between buildings, while avoiding obstacles. A UAV navigating under these conditions can be affected by uncertainties in its localization and motion due to occlusion of GPS signals and the use of low cost sensors. Additionally, the presence of strong winds in the airspace can disturb the motion of the UAV. In this paper, we describe and flight test a novel formulation of a UAV mission for searching, tracking and following a mobile ground target. This mission is formulated as a Partially Observable Markov Decision Process (POMDP) and implemented in real flight using a modular framework. We modelled the UAV dynamic system, the uncertainties in motion and localization of both the UAV and the target, and the wind disturbances. The framework computes a motion plan online for executing motion commands instead of flying to way-points to accomplish the mission. The system enables the UAV to plan its motion allowing it to execute information gathering actions to reduce uncertainty by detecting landmarks in the scenario, while making predictions of the mobile target trajectory and the wind speed based on observations. Results indicate that the system overcomes uncertainties in localization of both the aircraft and the target, and avoids collisions into obstacles despite the presence of wind. This research has the potential of use particularly for remote monitoring in the fields of biodiversity and ecology.",space,605
10.1109/scc.2016.75,filtered,2016 IEEE International Conference on Services Computing (SCC),IEEE,2016-07-02 00:00:00,ieeexplore,uclao* and bhuc: two novel planning algorithms for uncertain web service composition,https://ieeexplore.ieee.org/document/7557495/,"The inherent uncertainty of Web service is the most important characteristic due to its deployment and invocation within a real and highly dynamic Internet environment. Web service composition with uncertainty (U-WSC) has become an important research issue in service computing. Although some research has been done on U-WSC via non-deterministic planning in Artificial Intelligence, they cannot handle the situation that uncertain Web services with the same functionality exist in a service repository and could not get all of possible solution plans that constitute an uncertain composition solution for a given request. To solve above research challenges, this paper models a U-WSC problem into a U-WSC planning problem. Accordingly, two novel uncertain planning algorithms using heuristic search called UCLAO* and BHUC, are presented to solve the U-WSC planning problem with state space reduction, which leads to high efficiency of finding a service composition solution. We have conducted empirical experiments based on a running example in e-commerce application as well as our large-scale simulated datasets. The experimental results demonstrate that our proposed algorithms outperform the state-of-the-art non-deterministic planning algorithms in terms of effectiveness, efficiency and scalability.",space,606
10.1109/iv47402.2020.9304826,filtered,2020 IEEE Intelligent Vehicles Symposium (IV),IEEE,2020-11-13 00:00:00,ieeexplore,uncertainty-aware energy management of extended range electric delivery vehicles with bayesian ensemble,https://ieeexplore.ieee.org/document/9304826/,"In recent years, deep reinforcement learning (DRL) algorithms have been widely studied and utilized in the area of Intelligent Transportation Systems (ITS). DRL agents are mostly trained with transition pairs and interaction trajectories generated from simulation, and they can achieve satisfying or near optimal performances under familiar input states. However, for relative rare visited or even unvisited regions in the state space, there is no guarantee that the agent could perform well. Unfortunately, novel conditions are inevitable in real-world problems and there is always a gap between the real data and simulated data. Therefore, to implement DRL algorithms in real-world transportation systems, we should not only train the agent learn a policy that maps states to actions, but also the model uncertainty associated with each action. In this study, we adapt the method of Bayesian ensemble to train a group of agents with imposed diversity for an energy management system of a delivery vehicle. The agents in the ensemble agree well on familiar states but show diverse results on unfamiliar or novel states. This uncertainty estimation facilitates the implementation of interpretable postprocessing modules which can ensure robust and safe operations under high uncertainty conditions.",space,607
10.23919/acc45564.2020.9147911,filtered,2020 American Control Conference (ACC),IEEE,2020-07-03 00:00:00,ieeexplore,unmanned aerial vehicle angular velocity control via reinforcement learning in dimension reduced search spaces,https://ieeexplore.ieee.org/document/9147911/,"Search space dimension reduction strategies are studied for reinforcement learning based angular velocity control of multirotor unmanned aerial vehicles. Reinforcement learning approximates the value function iteratively over the state-action space, which is 6-dimensional in the case of multirotor angular velocity control. An inverse-dynamics approach is applied to reduce the 6-dimensional state-action space to a 3-dimensional state-only search space while estimating the uncertain model parameters. The search space dimension is further reduced when the state variables are only allowed to vary following either a motion camouflage strategy or a hyperbolic tangent path. Simulation results show that the modified reinforcement learning algorithms can be implemented in real time for multirotor angular velocity control.",space,608
10.1109/icpr48806.2021.9412661,filtered,2020 25th International Conference on Pattern Recognition (ICPR),IEEE,2021-01-15 00:00:00,ieeexplore,unsupervised domain adaptation for object detection in cultural sites,https://ieeexplore.ieee.org/document/9412661/,"The ability to detect objects in cultural sites from the egocentric point of view of the user can enable interesting applications for both the visitors and the manager of the site. Unfortunately, current object detection algorithms have to be trained on large amounts of labeled data, the collection of which is costly and time-consuming. While synthetic data generated from the 3D model of the cultural site can be used to train object detection algorithms, a significant drop in performance is generally observed when such algorithms are deployed to work with real images. In this paper, we consider the problem of unsupervised domain adaptation for object detection in cultural sites. Specifically, we assume the availability of synthetic labeled images and real unlabeled images for training. To study the problem, we propose a dataset containing 75244 synthetic and 2190 real images with annotations for 16 different artworks. We hence investigate different domain adaptation techniques based on image-to-image translation and feature alignment. Our analysis points out that such techniques can be useful to address the domain adaptation issue, while there is still plenty of space for improvement on the proposed dataset. We release the dataset at our web page to encourage research on this challenging topic: https://iplab.dmi.unict.it/EGO-CH-OBJ-ADAPT/.",space,609
10.1109/ccnc49033.2022.9700501,filtered,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,unsupervised root-cause identification of software bugs in 5g ran,https://ieeexplore.ieee.org/document/9700501/,"Developers of complex system like 5G Radio Access Networks (RAN) need algorithms that can automatically locate the root causes of software bugs. Existing methods mainly use supervised learning to track down root causes and only a few of these provide enough information to identify the function in which a software bug occurs. Supervised learning methods work well when scenarios can be repeated, and the normal behavior is somewhat similar. In RAN, where thousands of different configurations are used, software is updated frequently, and each node has its own traffic intensity, using unsupervised learning that does not require any pre-training can be more suitable. The few existing methods that use unsupervised learning to locate the root cause of software bugs can only detect delays or software hangs, and are not able to identify the many types of bugs that occur in RAN. We propose a multi-step method that uses unsupervised learning to analyze kernel and user space traces in system logs. The methods can guide developers by suggesting top-k candidate functions that are likely to contain a software bug. Our methods, MultiSpace and CallGraph were evaluated using an advanced 5G testbed in which many different software bugs that are common in RAN, were injected. The results shows that MultiSpace and CallGraph, can detect a wider range of software bugs than previous methods and only adds an average CPU load of 1.3% on the testbed. An important aspect is also that our methods scale well with large amount of data produced by real time systems, like RAN, and can analyze the data much faster.",space,610
10.1109/coginf.2009.5250777,filtered,2009 8th IEEE International Conference on Cognitive Informatics,IEEE,2009-06-17 00:00:00,ieeexplore,user-oriented healthcare support system based on symbiotic computing,https://ieeexplore.ieee.org/document/5250777/,"We propose a multi-agent-based healthcare support system in ubiquitous computing environment. By utilizing knowledge about healthcare and various information including vital sign, physical location, and video data of a user under observation from real space, the system provides useful information regarding health condition effectively and in user-oriented manner. This paper describes a user-oriented healthcare support system based on concept of symbiotic computing, focusing on design and initial prototype implementation of the system.",space,611
10.1109/iccw.2019.8756759,filtered,2019 IEEE International Conference on Communications Workshops (ICC Workshops),IEEE,2019-05-24 00:00:00,ieeexplore,using deep q-learning to prolong the lifetime of correlated internet of things devices,https://ieeexplore.ieee.org/document/8756759/,"Battery-powered sensors deployed in the Internet of Things (IoT) require energy-efficient solutions to prolong their lifetime. When these sensors observe a physical phenomenon distributed in space and evolving in time, the collected observations are expected to be correlated. In this paper, we propose an updating mechanism leveraging Reinforcement Learning (RL) to take advantage of the exhibited correlation in the information collected. We implement the proposed updating mechanism employing deep Q-learning. Our mechanism is capable of learning the correlation in the information collected and determine the frequency with which sensors should transmit their updates, while taking into consideration a highly dynamic environment. We evaluate our solution using environmental observations, namely temperature and humidity, obtained in a real deployment. We demonstrate that our mechanism is capable of adapting the transmission frequency of sensors' updates according to the ever-changing environment. We show that our proposed mechanism is capable of significantly extending battery-powered sensors' lifetime without compromising the accuracy of the observations provided to the IoT service.",space,612
10.1109/iccv.2005.246,filtered,Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1,IEEE,2005-10-21 00:00:00,ieeexplore,vector boosting for rotation invariant multi-view face detection,https://ieeexplore.ieee.org/document/1541289/,"In this paper, we propose a novel tree-structured multiview face detector (MVFD), which adopts the coarse-to-fine strategy to divide the entire face space into smaller and smaller subspaces. For this purpose, a newly extended boosting algorithm named vector boosting is developed to train the predictors for the branching nodes of the tree that have multicomponents outputs as vectors. Our MVFD covers a large range of the face space, say, +/-45/spl deg/ rotation in plane (RIP) and +/-90/spl deg/ rotation off plane (ROP), and achieves high accuracy and amazing speed (about 40 ms per frame on a 320 /spl times/ 240 video sequence) compared with previous published works. As a result, by simply rotating the detector 90/spl deg/, 180/spl deg/ and 270/spl deg/, a rotation invariant (360/spl deg/ RIP) MVFD is implemented that achieves real time performance (11 fps on a 320 /spl times/ 240 video sequence) with high accuracy.",space,613
10.1109/sami.2016.7422985,filtered,2016 IEEE 14th International Symposium on Applied Machine Intelligence and Informatics (SAMI),IEEE,2016-01-23 00:00:00,ieeexplore,vehicle navigation by fuzzy cognitive maps using sonar and rfid technologies,https://ieeexplore.ieee.org/document/7422985/,"Emerging concept of the so-called intelligent space (IS) offers means for use of mobile autonomous devices like vehicles or robots in a very broad area without necessity for these devices to own all necessary sensors. From this reason also new navigation methods are developing, which utilize IS means, with the aim to offer maybe not so accurate but first of all cheep and reliable solutions for a wide variety of devices. Our paper deals with the examination of possibility to interconnect sparsely deployed RFID tags with sonars. As signals produced by these two technologies are often affected by uncertainty and incompleteness we use fuzzy logic for their processing as well as control of the entire navigation process. For this purpose a special type of a fuzzy cognitive map was proposed. The paper describes real navigation experiments with a simple vehicle and evaluates them by selected criteria. Based on obtained results their explanations and conclusions for potential future research are sketched.",space,614
10.1109/icnnsp.2008.4590383,filtered,2008 International Conference on Neural Networks and Signal Processing,IEEE,2008-06-11 00:00:00,ieeexplore,video object matching based on sift algorithm,https://ieeexplore.ieee.org/document/4590383/,"SIFT (Scale Invariant Feature Transform) is used to solve visual tracking problem, where the appearances of the tracked object and scene background change during tracking. The implementation of this algorithm has five major stages: scale-space extrema detection; keypoint localization; orientation assignment; keypoint descriptor; keypoint matching. From the beginning frame, object is selected as the template, its SIFT features are computed. Then in the following frames, the SIFT features are computed. Euclidean distance between the object’s SIFT features and the frames’ SIFT features can be used to compute the accurate position of the matched object. The experimental results on real video sequences demonstrate the effectiveness of this approach and show this algorithm is of higher robustness and real-time performance. It can solve the matching problem with translation, rotation and affine distortion between images. It plays an important role in video object tracking and video object retrieval.",space,615
10.1109/acc.2006.1657291,filtered,2006 American Control Conference,IEEE,2006-06-16 00:00:00,ieeexplore,virtual air-fuel ratio sensors for engine control and diagnostics,https://ieeexplore.ieee.org/document/1657291/,"Virtual air-fuel ratio sensors for an internal combustion engine using recurrent neural and wavelet networks have been developed. A nonlinear state-space modeling strategy is proposed for the architecture of the stated recurrent neural network which is trained using some variants of real time recurrent learning (RTRL) algorithm. A two-stage training approach is proposed for improving the accuracy of the RNN topology. Additionally, wavelets as activation functions have been employed to construct a single-layer network called wavenet. The wavenet is used to model the exhaust air-fuel ratio that has proved a more challenging task in a purely neural net-based architecture using sigmoid activation functions. The methodology has been implemented in a V8 spark ignition engine through rapid prototyping tools for the real time generalization and performance evaluation. Observations and comments are made on the test patterns used for the training. Some of the limitations of such a data driven approach are highlighted. Representative experimental results for the 8-cylinder engine test data are listed. The virtual sensor may be used for more precise average air-fuel ratio control and enhanced reliability engendered through the diagnostic capabilities of the sensor",space,616
10.1109/fskd.2017.8393254,filtered,"2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)",IEEE,2017-07-31 00:00:00,ieeexplore,visual control system design of wheeled inverted pendulum robot based on beaglebone black,https://ieeexplore.ieee.org/document/8393254/,"The wheeled inverted pendulum robot has broad prospects of applications in real life. It can use two coaxial wheels to achieve the body self-balancing, forward moving and turning. But the general wheeled inverted pendulum robot seldom has vision function to perceive enviromental change. In order to realize the robust visual control, a wheeled inverted-pendulum vision robot with attitude sensors, photoelectric encoders, ultrasonic sensors and so on is designed based on Beaglebone Black board. The moving object is separated in the space domain by obtaining the image sequence which is sent by a robot-mounted camera, and the modeling, identification and tracking of target sequence are implemented in the time domain. The balance PD, speed PI and steering PD controllers are designed to realize the dynamic balance, forward and steering function of the robot. To satisfy the functional requirements of the visual tracking system, an improved tracking-learning-detection algorithm based on kernelized correlation filtering is used, and a tracking anomaly based on spatial context is detected to determine the tracking state and reduce the error rate. Experimental results show that the robot reaches the requirement of design and achieves better visual control effectiveness.",space,617
10.1109/iciap.2003.1234077,filtered,"12th International Conference on Image Analysis and Processing, 2003.Proceedings.",IEEE,2003-09-19 00:00:00,ieeexplore,visual self-localisation using automatic topology construction,https://ieeexplore.ieee.org/document/1234077/,"The paper proposes a machine learning method for self-localising a mobile agent, using the images supplied by a single omni-directional camera. The images acquired by the camera may be viewed as an implicit topological representation of the environment. The environment is a priori unknown and the topological representation is derived by unsupervised neural network architecture. The architecture includes a self-organising neural network, and is constituted by a growing neural gas, which is well known for its topology preserving quality. The growth depends on the topology that is not a priori defined, and on the need of discovering it, by the neural network, during the learning. The implemented system is able to recognise correctly the input frames and to reconstruct a topological map of the environment. Each node of the neural network identifies a single zone of the environment and the connections between the nodes correspond to the real space connections in the environment.",space,618
10.1109/idea49133.2020.9170686,filtered,"2nd International Conference on Data, Engineering and Applications (IDEA)",IEEE,2020-02-29 00:00:00,ieeexplore,“error back propagation based recurrent neural networks for intrusion detection system”,https://ieeexplore.ieee.org/document/9170686/,"Internet usage is increasing daily, which adds to the system's vulnerability. Network security is always a major problem for network administrators and is constantly changing due to the additional application space and requirements of a smart and efficient network. Simple and more efficient software tools include victims on the security side of the protocol that hackers use to perform various types of attacks on the network. The purpose of this review is to establish and coordinate processes to prevent one member against new and known attacks, and to act as a separate security system or an independent network. The neural network connects the body's immune system to receive memory, errors, and synchronized learning. This paper discusses interrelated processes that are largely based on the application of artificial intelligence for intrusion detection and learning processes.Proposed approach finds the type of session i.e. either normal or intrusion where if intrusion found than class of intrusion was detected. Here artificial neural network was used for finding the patterns in the input data. In this work Back propagation is used for the ANN in recursive manner. Proposed algorithm gives an effective framework which has been a promising one for distinguishing interruption of various kind where, one can get the detail of the class of attack also. Experiment has been conduced on real data set where various set of testing data were pass for comparison on different evaluation parameters. Proposed approach detects all sort of attacks applied on the network such as DoS (Denial of service), (R2L) Remote to local, (U2R) User to remote, Probe etc. In this work a Random Forest Tree is used for the detection of intrusion in network. Proposed approach improved 6.87% accuracy, 12.06% Precision and 1.15% recall.",space,619
10.1109/fuzzy.2008.4630539,filtered,2008 IEEE International Conference on Fuzzy Systems (IEEE World Congress on Computational Intelligence),IEEE,2008-06-06 00:00:00,ieeexplore,“tell me the important stuff” - fuzzy ontologies and personal assessments for interaction with the semantic web,https://ieeexplore.ieee.org/document/4630539/,"The semantic Web attempts to make the Web an universal medium of data exchange. To achieve this it needs to both make data sources accessible to machine-based systems, such as software agents and allow humans to easily create, understand and search for these data sources. Crisp ontologies are extremely useful for improving data extraction from structured data. However many ontological approaches require an unnatural level of precision and rigidity when dealing with real user queries or data sources. Previous work has suggested that fuzzification of ontologies may increase their utility. A major area of activity in the search and ubiquitous computing space is the development of location aware services. This paper suggests that in the mobile and context-aware semantic Web environment, fuzzification needs to extend to the representation of the importance of particular. Additionally, mobile devices tend to require simple interfaces and work with low bandwidth, this implies that obtaining small numbers of relevant results is extremely important. The work previously done in representing uncertainty in geographical information systems may assist in this development. This paper suggest that the combination of the use of fuzzy ontologies and fuzzy spatial relations may be effective in increasing the usefulness of the mobile semantic Web.",space,620
10.1109/tnsre.2017.2697415,filtered,IEEE Transactions on Neural Systems and Rehabilitation Engineering,IEEE,2017-12-01 00:00:00,ieeexplore,a 128-channel fpga-based real-time spike-sorting bidirectional closed-loop neural interface system,https://ieeexplore.ieee.org/document/7908970/,"A multichannel neural interface system is an important tool for various types of neuroscientific studies. For the electrical interface with a biological system, high-precision high-speed data recording and various types of stimulation capability are required. In addition, real-time signal processing is an important feature in the implementation of a real-time closed-loop system without unwanted substantial delay for feedback stimulation. Online spike sorting, the process of assigning neural spikes to an identified group of neurons or clusters, is a necessary step to make a closed-loop path in real time, but massive memory-space requirements commonly limit hardware implementations. Here, we present a 128-channel field-programmable gate array (FPGA)-based real-time closed-loop bidirectional neural interface system. The system supports 128 channels for simultaneous signal recording and eight selectable channels for stimulation. A modular 64-channel analog front-end (AFE) provides scalability and a parameterized specification of the AFE supports the recording of various electrophysiological signal types with 1.59 ± 0.76 $\mu {V}$ root-mean-square noise. The stimulator supports both voltage-controlled and current-controlled arbitrarily shaped waveforms with the programmable amplitude and duration of pulse. An empirical algorithm for online real-time spike sorting is implemented in an FPGA. The spike-sorting is performed by template matching, and templates are created by an online real-time unsupervised learning process. A memory saving technique, called dynamic cache organizing, is proposed to reduce the memory requirement down to 6 kbit per channel and modular implementation improves the scalability for further extensions.",space,621
10.1109/tkde.2019.2961657,filtered,IEEE Transactions on Knowledge and Data Engineering,IEEE,2021-08-01 00:00:00,ieeexplore,a data-driven sequential localization framework for big telco data,https://ieeexplore.ieee.org/document/8939387/,"The proliferation of telco networks and mobile terminals brings the accumulation of tremendous amounts of measure report(MR) data at a rapid pace. The MR data is generated by mobile objects while connecting to data services and is stored in backend data centers. To geo-tag or localize such MR data is believed to have a profound effect on the analytics and optimizations of telco and traffic networks. However, MR records are of noisy and partial observations regarding to mobile objects' geo-locations and hence pose challenges to accurate telco data localization. There have been quite a few attempts. Single-point localization methods map a MR record to a location, but come out with limited accuracies due to the ignorance of spatiotemporal coherence of successive MR records. Recent efforts on sequential localization techniques alleviate this by mapping a sequence of MR records to a trajectory. However, existing solutions are often with assumptions on specific models, e.g., mobility and signal strength distributions, or priori knowledge on topology space, e.g., road networks, limiting the deployment in practice. To this end, we propose a data-driven framework to tackle the challenges in sequential telco localization. We solely use raw MR records and a public third-party GPS dataset for the learning of the correlations between mobile objects' locations and MR records, requiring no model assumptions and priori knowledge. To handle the data-intensive workloads during the learning process, we use materialized views for efficient online localization and light-weighted indexing techniques for periodical parameters tuning, in order to improve the efficiency and scalability. Results on real data show that our solution achieves 58.8 percent improvement in median localization errors compared with state-of-art sequential localization techniques that require hypothesis models and priori knowledge, making our solution superior in terms of effectiveness, efficiency, and employability.",space,622
10.1162/neco_a_00065,filtered,Neural Computation,MIT Press,2011-01-01 00:00:00,ieeexplore,a framework for simulating and estimating the state and functional topology of complex dynamic geometric networks,https://ieeexplore.ieee.org/document/6796295/,"We introduce a framework for simulating signal propagation in geometric networks (networks that can be mapped to geometric graphs in some space) and developing algorithms that estimate (i.e., map) the state and functional topology of complex dynamic geometric networks. Within the framework, we define the key features typically present in such networks and of particular relevance to biological cellular neural networks: dynamics, signaling, observation, and control. The framework is particularly well suited for estimating functional connectivity in cellular neural networks from experimentally observable data and has been implemented using graphics processing unit high-performance computing. Computationally, the framework can simulate cellular network signaling close to or faster than real time. We further propose a standard test set of networks to measure performance and compare different mapping algorithms.",space,623
10.1109/access.2020.2973411,filtered,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,a parallel multi-verse optimizer for application in multilevel image segmentation,https://ieeexplore.ieee.org/document/8995472/,"Multi-version optimizer (MVO) inspired by the multi-verse theory is a new optimization algorithm for challenging multiple parameter optimization problems in the real world. In this paper, a novel parallel multi-verse optimizer (PMVO) with the communication strategy is proposed. The parallel mechanism is implemented to randomly divide the initial solutions into several groups, and share the information of different groups after each fixed iteration. This can significantly promote the cooperation individual of MVO algorithm, and reduce the deficiencies that the original MVO is premature convergence, search stagnation and easily trap into local optimal search space. To confirm the performance of the proposed scheme, the PMVO algorithm was compared with the other well-known optimization algorithms, such as gray wolf optimizer (GWO), particle swarm optimization (PSO), multi-version optimizer (MVO), and parallel particle swarm optimization (PPSO) under CEC2013 test suite. The experimental results prove that the PMVO is superior to the other compared algorithms. In addition, PMVO is also applied to solve complex multilevel image segmentation problems based on minimum cross entropy thresholding. The application results appear that the proposed PMVO algorithm can achieve higher quality image segmentation compared to other similar algorithms.",space,624
10.1109/taslp.2014.2375572,filtered,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",IEEE,2015-01-01 00:00:00,ieeexplore,a recursive dialogue game for personalized computer-aided pronunciation training,https://ieeexplore.ieee.org/document/6971195/,"Learning languages in addition to the native language is very important for all people in the globalized world today, and computer-aided pronunciation training (CAPT) is attractive since the software can be used anywhere at any time, and repeated as many times as desired. In this paper, we introduce the immersive interaction scenario offered by spoken dialogues to CAPT by proposing a recursive dialogue game to make CAPT personalized. A number of tree-structured sub-dialogues are linked sequentially and recursively as the script for the game. The system policy at each dialogue turn is to select in real-time along the dialogue the best training sentence for each specific individual learner within the dialogue script, considering the learner's learning status and the future possible dialogue paths in the script, such that the learner can have the scores for all pronunciation units considered reaching a predefined standard in a minimum number of turns. The purpose here is that those pronunciation units poorly produced by the specific learner can be offered with more practice opportunities in the future sentences along the dialogue, which enables the learner to improve the pronunciation without having to repeat the same training sentences many times. This makes the learning process for each learner completely personalized. The dialogue policy is modeled by Markov decision process (MDP) with high-dimensional continuous state space, and trained with fitted value iteration using a huge number of simulated learners. These simulated leaners have the behavior similar to real learners, and were generated from a corpus of real learner data. The experiments demonstrated very promising results and a real cloud-based system is also successfully implemented.",space,625
10.1109/tnnls.2013.2280126,filtered,IEEE Transactions on Neural Networks and Learning Systems,IEEE,2014-03-01 00:00:00,ieeexplore,a robust and scalable neuromorphic communication system by combining synaptic time multiplexing and mimo-ofdm,https://ieeexplore.ieee.org/document/6740875/,"This paper describes a novel architecture for enabling robust and efficient neuromorphic communication. The architecture combines two concepts: 1) synaptic time multiplexing (STM) that trades space for speed of processing to create an intragroup communication approach that is firing rate independent and offers more flexibility in connectivity than cross-bar architectures and 2) a wired multiple input multiple output (MIMO) communication with orthogonal frequency division multiplexing (OFDM) techniques to enable a robust and efficient intergroup communication for neuromorphic systems. The MIMO-OFDM concept for the proposed architecture was analyzed by simulating large-scale spiking neural network architecture. Analysis shows that the neuromorphic system with MIMO-OFDM exhibits robust and efficient communication while operating in real time with a high bit rate. Through combining STM with MIMO-OFDM techniques, the resulting system offers a flexible and scalable connectivity as well as a power and area efficient solution for the implementation of very large-scale spiking neural architectures in hardware.",space,626
10.1109/tdsc.2020.2971477,filtered,IEEE Transactions on Dependable and Secure Computing,IEEE,2021-12-01 00:00:00,ieeexplore,a security analysis of captchas with large character sets,https://ieeexplore.ieee.org/document/8979440/,"Captcha, which can prevent computer programs from attacking websites, has been the most important security technology for many years. The most popularly deployed Captcha is the text-based scheme. The vast majority of the existing text Captchas are designed with English letters and Arabic numerals. Recently, text Captchas with large character sets are being increasingly popular. From the perspective of attackers, larger character set means greater solution space and better theoretical security. However, the security of Captchas with large character sets in real world has never been studied comprehensively. In this article, we introduce a simple, fast, and effective deep learning method to attack these newly emerging Captchas. Taking 11 Chinese Captchas as representatives, we ran our experimental attack on each of them. Our attack achieved high success rates, ranging from 34.7 to 86.9 percent at an average speed of 0.175 seconds on these schemes. All of the results show that the Chinese text Captcha can be easily broken, demonstrating that text Captchas with large character sets are also insecure in existing forms. As a substitute, we proposed a 3D image-based scheme combining semantic comprehension and dragging action. The preliminary experimental results show that it is more robust than current text-based schemes.",space,627
10.1109/tits.2018.2873137,filtered,IEEE Transactions on Intelligent Transportation Systems,IEEE,2019-09-01 00:00:00,ieeexplore,a unified spatio-temporal model for short-term traffic flow prediction,https://ieeexplore.ieee.org/document/8525272/,"This paper proposes a unified spatio-temporal model for short-term road traffic prediction. The contributions of this paper are as follows. First, we develop a physically intuitive approach to traffic prediction that captures the time-varying spatio-temporal correlation between traffic at different measurement points. The spatio-temporal correlation is affected by the road network topology, time-varying speed, and time-varying trip distribution. Distinctly different from previous black-box approaches to road traffic modeling and prediction, parameters of the proposed approach have physically intuitive meanings which make them readily amendable to suit changing road and traffic conditions. Second, unlike some existing techniques that capture the variation of spatio-temporal correlation by a complete re-design and calibration of the model, the proposed approach uses a unified model that incorporates the physical factors potentially affecting the variation of spatio-temporal correlation into a series of parameters. These parameters are relatively easy to control and adjust when road and traffic conditions change, thereby greatly reducing the computational complexity. Experiments using two sets of real traffic traces demonstrate that the proposed approach has superior accuracy compared with the widely used space-time autoregressive integrated moving average (STARIMA) and the back propagation neural network approaches, and is only marginally inferior to that obtained by constructing multiple STARIMA models for different times of the day, however, with a much reduced computational and implementation complexity.",space,628
10.1109/tase.2020.3032075,filtered,IEEE Transactions on Automation Science and Engineering,IEEE,2021-10-01 00:00:00,ieeexplore,a virtual mechanism approach for exploiting functional redundancy in finishing operations,https://ieeexplore.ieee.org/document/9246671/,"We propose a new approach to programming by the demonstration of finishing operations. Such operations can be carried out by industrial robots in multiple ways because an industrial robot is typically functionally redundant with respect to a finishing task. In the proposed system, a human expert demonstrates a finishing operation, and the demonstrated motion is recorded in the Cartesian space. The robot’s kinematic model is augmented with a virtual mechanism, which is defined according to the applied finishing tool. This way, the kinematic model is expanded with additional degrees of freedom that can be exploited to compute the optimal joint space motion of the robot without altering the essential aspects of the Cartesian space task execution as demonstrated by the human expert. Finishing operations, such as polishing and grinding, occur in contact with the treated workpiece. Since information about the contact point position is needed to control the robot during the operation, we have developed a novel approach for accurate estimation of contact points using the measured forces and torques. Finally, we applied iterative learning control to refine the demonstrated operations and compensate for inaccurate calibration and different dynamics of the robot and human demonstrator. The proposed method was verified on real robots and real polishing and grinding tasks. <i>Note to Practitioners</i>—This work was motivated by the need for automation of finishing operations, such as polishing and grinding, on contemporary industrial robots. Existing approaches are both too complex and too time-consuming to be applied in flexible and small-scale production, which often requires the frequent deployment of new applications. Our approach is based on programming by demonstration and enables the programming of finishing operations also for users who are not specialists in robot programming. Programming by demonstration is especially useful for teaching finishing operations because it enables the transfer of expert knowledge about finishing skills to robots without providing lengthy task descriptions or manual coding. Besides the human demonstration of the desired operation, the proposed approach also requires the availability of the kinematic model for the machine tool applied to carry out the finishing operation. We provide several practical examples of grinding and polishing tools and how to integrate them into our approach. Another feature of the proposed system is that user demonstrations of finishing operations can be transferred between different combinations of robots and machine tools.",space,629
10.1109/42.563667,filtered,IEEE Transactions on Medical Imaging,IEEE,1997-04-01 00:00:00,ieeexplore,a focus-of-attention preprocessing scheme for em-ml pet reconstruction,https://ieeexplore.ieee.org/document/563667/,"The expectation-maximization maximum-likelihood (EM-ML) algorithm belongs to a family of algorithms that compute positron emission tomography (PET) reconstructions by iteratively solving a large linear system of equations. The authors describe a preprocessing scheme for automatically focusing the attention, and thus the computational resources, on a subset of the equations and unknowns. Experimental work with a CM-5 parallel computer implementation using a simulated phantom as well as real data obtained from an ECAT 921 PET scanner indicates that quite significant savings can be obtained with respect to both time and space requirements of the EM-ML algorithm without compromising the quality of the reconstructed images.",space,630
10.1109/36.628775,filtered,IEEE Transactions on Geoscience and Remote Sensing,IEEE,1997-09-01 00:00:00,ieeexplore,a neural network approach to estimating rainfall from spaceborne microwave data,https://ieeexplore.ieee.org/document/628775/,"Various techniques use microwave (MW) brightness temperature (BT) data, obtained from remote sensing orbiting platforms, to calculate rain rates. The most commonly used techniques are based on regressions or other statistical methods. An emerging tool in rainfall estimation using satellite data is artificial neural networks (NNs), NNs are mathematical models that are capable of learning complex relationships. They consist of highly interconnected, interactive data processing units. NNs are implemented in this study to estimate rainfall, and backpropagation is used as a learning scheme. The inputs for the training phase are BTs and the outputs are rainfall rates, all generated by three-dimensional (3D) simulations based on a 3D stochastic, space-time rainfall model, and a 3D radiative transfer model. Once training is complete the NNs are presented with multi-frequency and polarized (horizontal and vertical) BT data, obtained from the Special Sensor Microwave/Imager (SSM/I) instrument onboard the F10 and F11 polar-orbiting meteorological satellites. Hence, rainrates corresponding to real BT measurements are generated. The rainfall rates are also estimated using a log-linear regression model. Comparison of the two approaches, using simulated data, shows that the NN can represent more accurately the underlying relationship between BT and rainrate than the regression model, Comparison of the rates, estimated by both methods, with radar-estimated rainrates shows that NNs outperform the regression model. This study demonstrates the great potential of NNs in estimating rainfall from remotely sensed data.",space,631
10.1109/access.2019.2950232,filtered,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,accelerating api-based program synthesis via api usage pattern mining,https://ieeexplore.ieee.org/document/8886418/,"Program Synthesis is an exciting topic in software engineering which aims to generate programs satisfying user intent automatically. Although different approaches have been proposed in program synthesis, only small or domain-specific programs can be generated in practice, the main obstacle of which lies in the intractability of program space. With the rapid growth of reusable libraries, component-based synthesis provides a promising way, such as synthesizing Java programs that are only composed of APIs. However, the efficiency of searching for proper solutions for complex tasks is still a challenge. In certain scenarios, some API methods are frequently called together. The usage of these API methods always follows some usage patterns. Incorporating the information about API usage patterns will help to accelerate the speed of program synthesis. However, state-of-the-art synthesis tools do not capture the inherent relationships between API methods. Aiming at this problem, we propose a novel approach to accelerate the speed of API-based program synthesis via API usage pattern mining. It is a general approach that can be applied to any approach of API-based synthesis. We first collect open source projects of high quality from the Internet and use an off-the-shelf API-usage-pattern-mining tool to mine API usage patterns from these code snippets. We use two strategies to incorporate the information about API usage patterns with program synthesis, and either strategy can improve the efficiency of program synthesis. We evaluate our approach on 20 real programming tasks, which shows that our approach can accelerate the speed of program synthesis by 86% compared to the baseline.",space,632
10.1109/tip.2021.3050303,filtered,IEEE Transactions on Image Processing,IEEE,2021-01-01 00:00:00,ieeexplore,adversarial attack against deep saliency models powered by non-redundant priors,https://ieeexplore.ieee.org/document/9325048/,"Saliency detection is an effective front-end process to many security-related tasks, <i>e.g.</i> automatic drive and tracking. Adversarial attack serves as an efficient surrogate to evaluate the robustness of deep saliency models before they are deployed in real world. However, most of current adversarial attacks exploit the gradients spanning the entire image space to craft adversarial examples, ignoring the fact that natural images are high-dimensional and spatially over-redundant, thus causing expensive attack cost and poor perceptibility. To circumvent these issues, this paper builds an efficient bridge between the accessible <i>partially-white-box source</i> models and the unknown <i>black-box target</i> models. The proposed method includes two steps: 1) We design a new <i>partially-white-box</i> attack, which defines the cost function in the compact hidden space to punish a fraction of feature activations corresponding to the salient regions, instead of punishing every pixel spanning the entire dense output space. This <i>partially-white-box</i> attack reduces the redundancy of the adversarial perturbation. 2) We exploit the non-redundant perturbations from some <i>source</i> models as the prior cues, and use an iterative zeroth-order optimizer to compute the <i>directional derivatives</i> along the non-redundant prior directions, in order to estimate the actual gradient of the <i>black-box target</i> model. The non-redundant priors boost the update of some “critical” pixels locating at non-zero coordinates of the prior cues, while keeping other redundant pixels locating at the zero coordinates unaffected. Our method achieves the best tradeoff between attack ability and perturbation redundancy. Finally, we conduct a comprehensive experiment to test the robustness of 18 state-of-the-art deep saliency models against 16 malicious attacks, under both of <i>white-box</i> and <i>black-box</i> settings, which contributes a new robustness benchmark to the saliency community for the first time.",space,633
10.1109/tie.2018.2847704,filtered,IEEE Transactions on Industrial Electronics,IEEE,2019-05-01 00:00:00,ieeexplore,algorithm for estimating online bearing fault upon the ability to extract meaningful information from big data of intelligent structures,https://ieeexplore.ieee.org/document/8408700/,"Bearing is an important machine detail that appears in almost all mechanical systems. Estimating its operating condition online in order to hold the initiative in exploiting the systems, therefore, is one of the most urgent requirements. In this paper, we propose an online bearing damage identifying method named ASSBDIM based on adaptive neuro-fuzzy inference system (ANFIS), singular spectrum analysis (SSA), and sparse filtering (SF). It is an online process with offline and online phases. In the offline phase, by applying SSA and SF to the measured data stream typed big data with noise, both preprocessing data and extracting valuable information are implemented to build two offline databases signed Off_DaB and Off_testDaB. The ANFIS identifies the dynamic response of the mechanical system via Off_DaB. Based on Off_testDaB, the parameters of the ASSBDIM are then optimized. In the online phase, at each time, another database called On_DaB is built in the way similar to the one used for building the input space of the two offline databases. On_DaB participates as inputs of the ANFIS to estimate its outputs, which are then compared with the corresponding encoded outputs to specify bearing real status at this time. Survey results based on different data sources showed the effectiveness of the proposed method.",space,634
10.1109/tpami.2002.1017616,filtered,IEEE Transactions on Pattern Analysis and Machine Intelligence,IEEE,2002-07-01 00:00:00,ieeexplore,an efficient k-means clustering algorithm: analysis and implementation,https://ieeexplore.ieee.org/document/1017616/,"In k-means clustering, we are given a set of n data points in d-dimensional space R/sup d/ and an integer k and the problem is to determine a set of k points in Rd, called centers, so as to minimize the mean squared distance from each data point to its nearest center. A popular heuristic for k-means clustering is Lloyd's (1982) algorithm. We present a simple and efficient implementation of Lloyd's k-means clustering algorithm, which we call the filtering algorithm. This algorithm is easy to implement, requiring a kd-tree as the only major data structure. We establish the practical efficiency of the filtering algorithm in two ways. First, we present a data-sensitive analysis of the algorithm's running time, which shows that the algorithm runs faster as the separation between clusters increases. Second, we present a number of empirical studies both on synthetically generated data and on real data sets from applications in color quantization, data compression, and image segmentation.",space,635
10.1109/access.2020.3005513,filtered,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,analysis and design of computational news angles,https://ieeexplore.ieee.org/document/9127417/,"A key skill for a journalist is the ability to assess the newsworthiness of an event or situation. To this purpose journalists often rely on news angles, conceptual criteria that are used both i) to assess whether something is newsworthy and also ii) to shape the structure of the resulting news item. As journalism becomes increasingly computer-supported, and more and more sources of potentially newsworthy data become available in real time, it makes sense to try and equip journalistic software tools with operational versions of news angles, so that, when searching this vast data space, these tools can both identify effectively the events most relevant to the target audience, and also link them to appropriate news angles. In this paper we analyse the notion of news angle and, in particular, we i) introduce a formal framework and data schema for representing news angles and related concepts and ii) carry out a preliminary analysis and characterization of a number of commonly used news angles, both in terms of our formal model and also in terms of the computational reasoning capabilities that are needed to apply them effectively to real-world scenarios. This study provides a stepping stone towards our ultimate goal of realizing a solution capable of exploiting a library of news angles to identify potentially newsworthy events in a large journalistic data space.",space,636
10.1109/access.2017.2765626,filtered,IEEE Access,IEEE,2017-01-01 00:00:00,ieeexplore,application of eos-elm with binary jaya-based feature selection to real-time transient stability assessment using pmu data,https://ieeexplore.ieee.org/document/8081764/,"Recent studies show that pattern-recognition-based transient stability assessment (PRTSA) is a promising approach for predicting the transient stability status of power systems. However, many of the current well-known PRTSA methods suffer from excessive training time and complex tuning of parameters, resulting in inefficiency for real-time implementation and lacking the online model updating ability. In this paper, a novel PRTSA approach based on an ensemble of OS-extreme learning machine (EOSELM) with binary Jaya (BinJaya)-based feature selection is proposed with the use of phasor measurement units (PMUs) data. After briefly describing the principles of OS-ELM, an EOS-ELM-based PRTSA model is built to predict the post-fault transient stability status of power systems in real time by integrating OS-ELM and an online boosting algorithm, respectively, as a weak classifier and an ensemble learning algorithm. Furthermore, a BinJaya-based feature selection approach is put forward for selecting an optimal feature subset from the entire feature space constituted by a group of system-level classification features extracted from PMU data. The application results on the IEEE 39-bus system and a real provincial system show that the proposal has superior computation speed and prediction accuracy than other state-of-the-art sequential learning algorithms. In addition, without sacrificing the classification performance, the dimension of the input space has been reduced to about one-third of its initial value.",space,637
10.1109/tnnls.2018.2854796,filtered,IEEE Transactions on Neural Networks and Learning Systems,IEEE,2019-03-01 00:00:00,ieeexplore,asymptotically optimal contextual bandit algorithm using hierarchical structures,https://ieeexplore.ieee.org/document/8424511/,"We propose an online algorithm for sequential learning in the contextual multiarmed bandit setting. Our approach is to partition the context space and, then, optimally combine all of the possible mappings between the partition regions and the set of bandit arms in a data-driven manner. We show that in our approach, the best mapping is able to approximate the best arm selection policy to any desired degree under mild Lipschitz conditions. Therefore, we design our algorithm based on the optimal adaptive combination and asymptotically achieve the performance of the best mapping as well as the best arm selection policy. This optimality is also guaranteed to hold even in adversarial environments since we do not rely on any statistical assumptions regarding the contexts or the loss of the bandit arms. Moreover, we design an efficient implementation for our algorithm using various hierarchical partitioning structures, such as lexicographical or arbitrary position splitting and binary trees (BTs) (and several other partitioning examples). For instance, in the case of BT partitioning, the computational complexity is only log-linear in the number of regions in the finest partition. In conclusion, we provide significant performance improvements by introducing upper bounds (with respect to the best arm selection policy) that are mathematically proven to vanish in the average loss per round sense at a faster rate compared to the state of the art. Our experimental work extensively covers various scenarios ranging from bandit settings to multiclass classification with real and synthetic data. In these experiments, we show that our algorithm is highly superior to the state-of-the-art techniques while maintaining the introduced mathematical guarantees and a computationally decent scalability.",space,638
10.1109/tase.2018.2876430,filtered,IEEE Transactions on Automation Science and Engineering,IEEE,2019-04-01 00:00:00,ieeexplore,automatic composition and optimization of multicomponent predictive systems with an extended auto-weka,https://ieeexplore.ieee.org/document/8550732/,"Composition and parameterization of multicomponent predictive systems (MCPSs) consisting of chains of data transformation steps are a challenging task. Auto-WEKA is a tool to automate the combined algorithm selection and hyperparameter (CASH) optimization problem. In this paper, we extend the CASH problem and Auto-WEKA to support the MCPS, including preprocessing steps for both classification and regression tasks. We define the optimization problem in which the search space consists of suitably parameterized Petri nets forming the sought MCPS solutions. In the experimental analysis, we focus on examining the impact of considerably extending the search space (from approximately 22000 to 812 billion possible combinations of methods and categorical hyperparameters). In a range of extensive experiments, three different optimization strategies are used to automatically compose MCPSs for 21 publicly available data sets. The diversity of the composed MCPSs found is an indication that fully and automatically exploiting different combinations of data cleaning and preprocessing techniques is possible and highly beneficial for different predictive models. We also present the results on seven data sets from real chemical production processes. Our findings can have a major impact on the development of high-quality predictive models as well as their maintenance and scalability aspects needed in modern applications and deployment scenarios. Note to Practitioners-The extension of Auto-WEKA to compose and optimize multicomponent predictive systems (MCPSs) developed as part of this paper is freely available on GitHub under GPL license, and we encourage practitioners to use it on a broad variety of classification and regression problems. The software can either be used as a blackbox-where search space is made of all possible WEKA filters, predictors, and metapredictors (e.g., ensembles)-or as an optimization tool on a subset of preselected machine learning methods. The application has a graphical user interface, but it can also run from command line and can be embedded in any project as a Java library. There are three main outputs once an Auto-WEKA run has finished: 1) the trained MCPS ready to make predictions on unseen data; 2) the WEKA configuration (i.e., parameterized components); and 3) the Petri net in a Petri Net Markup Language format that can be analyzed using any tool supporting this standard language. There are, however, some practical considerations affecting the quality of the results that must be taken into consideration, such as the CPU time budget or the search starting point. These are extensively discussed in this paper.",space,639
10.1109/tro.2021.3084374,filtered,IEEE Transactions on Robotics,IEEE,2022-02-01 00:00:00,ieeexplore,cat-like jumping and landing of legged robots in low gravity using deep reinforcement learning,https://ieeexplore.ieee.org/document/9453856/,"In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we train a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of 3-D (re)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for 2-D microgravity experiments. The experimental results demonstrate that repetitive controlled jumping and landing with natural agility is possible.",space,640
10.1109/tifs.2010.2051543,filtered,IEEE Transactions on Information Forensics and Security,IEEE,2010-09-01 00:00:00,ieeexplore,centered hyperspherical and hyperellipsoidal one-class support vector machines for anomaly detection in sensor networks,https://ieeexplore.ieee.org/document/5483231/,"Anomaly detection in wireless sensor networks is an important challenge for tasks such as intrusion detection and monitoring applications. This paper proposes two approaches to detecting anomalies from measurements from sensor networks. The first approach is a linear programming-based hyperellipsoidal formulation, which is called a centered hyperellipsoidal support vector machine (CESVM). While this CESVM approach has advantages in terms of its flexibility in the selection of parameters and the computational complexity, it has limited scope for distributed implementation in sensor networks. In our second approach, we propose a distributed anomaly detection algorithm for sensor networks using a one-class quarter-sphere support vector machine (QSSVM). Here a hypersphere is found that captures normal data vectors in a higher dimensional space for each sensor node. Then summary information about the hyperspheres is communicated among the nodes to arrive at a global hypersphere, which is used by the sensors to identify any anomalies in their measurements. We show that the CESVM and QSSVM formulations can both achieve high detection accuracies on a variety of real and synthetic data sets. Our evaluation of the distributed algorithm using QSSVM reveals that it detects anomalies with comparable accuracy and less communication overhead than a centralized approach.",space,641
10.1109/access.2021.3130231,filtered,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,classification of bird and drone targets based on motion characteristics and random forest model using surveillance radar data,https://ieeexplore.ieee.org/document/9626001/,"Accurate detection and tracking of birds and drones are of great significance in various low altitude airspace surveillance scenarios. Radar is currently the most proper long range surveillance technology for this problem but also challenged by various difficulties on effective distinguishing between birds and drones. This paper explores the inherent flight mechanic and behavior mode of birds and drones. A target classification method is proposed by extracting target motion characteristics from radar tracks. The random forest model is selected for target classification in the new feature space. The proposed method is verified by real bird surveillance radar systems deployed in airport region. Classification results on birds, quadcopter drones and dynamic precipitations indicate that the proposed method could provide good classification accuracy. The Gini importance descriptors in random forest model provide extra reference on motion characteristic evaluation and mining. High sample flexibility and efficiency make the classification system capable of handling complicated low altitude target surveillance and classification problems. Limitations of the existing method and potential optimization strategy are also discussed as future works.",space,642
10.1109/tsmc.2020.2967936,filtered,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",IEEE,2021-12-01 00:00:00,ieeexplore,deep q-learning with q-matrix transfer learning for novel fire evacuation environment,https://ieeexplore.ieee.org/document/8989970/,"Deep reinforcement learning (RL) is achieving significant success in various applications like control, robotics, games, resource management, and scheduling. However, the important problem of emergency evacuation, which clearly could benefit from RL, has been largely unaddressed. Indeed, emergency evacuation is a complex task that is difficult to solve with RL. An emergency situation is highly dynamic, with a lot of changing variables and complex constraints that make it challenging to solve. Also, there is no standard benchmark environment available that can be used to train RL agents for evacuation. A realistic environment can be complex to design. In this article, we propose the first fire evacuation environment to train RL agents for evacuation planning. The environment is modeled as a graph capturing the building structure. It consists of realistic features like fire spread, uncertainty, and bottlenecks. The implementation of our environment is in the OpenAI gym format, to facilitate future research. We also propose a new RL approach that entails pretraining the network weights of a DQN-based agent [DQN/Double-DQN (DDQN)/Dueling-DQN] to incorporate information on the shortest path to the exit. We achieved this by using tabular <inline-formula> <tex-math notation=""LaTeX"">$Q$ </tex-math></inline-formula>-learning to learn the shortest path on the building model’s graph. This information is transferred to the network by deliberately overfitting it on the <inline-formula> <tex-math notation=""LaTeX"">$Q$ </tex-math></inline-formula>-matrix. Then, the pretrained DQN model is trained on the fire evacuation environment to generate the optimal evacuation path under time varying conditions due to fire spread, bottlenecks, and uncertainty. We perform comparisons of the proposed approach with state-of-the-art RL algorithms like DQN, DDQN, Dueling-DQN, PPO, VPG, state-action-reward-state-action (SARSA), actor–critic method, and ACKTR. The results show that our method is able to outperform state-of-the-art models by a huge margin including the original DQN-based models. Finally, our model is tested on a large and complex real building consisting of 91 rooms, with the possibility to move to any other room, hence giving 8281 actions. In order to reduce the action space, we propose a strategy that involves one step simulation. That is, an action importance vector is added to the final output of the pretrained DQN and acts like an attention mechanism. Using this strategy, the action space is reduced by 90.1%. In this manner, the model is able to deal with large action spaces. Hence, our model achieves near optimal performance on the real world emergency environment.",space,643
10.1109/tmc.2019.2893250,filtered,IEEE Transactions on Mobile Computing,IEEE,2020-02-01 00:00:00,ieeexplore,deepwear: adaptive local offloading for on-wearable deep learning,https://ieeexplore.ieee.org/document/8618364/,"Due to their on-body and ubiquitous nature, wearables can generate a wide range of unique sensor data creating countless opportunities for deep learning tasks. We propose DeepWear, a deep learning (DL) framework for wearable devices to improve the performance and reduce the energy footprint. DeepWear strategically offloads DL tasks from a wearable device to its paired handheld device through local network connectivity such as Bluetooth. Compared to the remote-cloud-based offloading, DeepWear requires no Internet connectivity, consumes less energy, and is robust to privacy breach. DeepWear provides various novel techniques such as context-aware offloading, strategic model partition, and pipelining support to efficiently utilize the processing capacity from nearby paired handhelds. Deployed as a user-space library, DeepWear offers developer-friendly APIs that are as simple as those in traditional DL libraries such as TensorFlow. We have implemented DeepWear on the Android OS and evaluated it on COTS smartphones and smartwatches with real DL models. DeepWear brings up to 5.08× and 23.0× execution speedup, as well as 53.5 and 85.5 percent energy saving compared to wearable-only and handheld-only strategies, respectively.",space,644
10.1109/access.2020.2999351,filtered,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,detecting memory life-cycle bugs with extended define-use chain analysis,https://ieeexplore.ieee.org/document/9106323/,"OS kernels leverage various memory allocation functions to carry out memory allocation, and memory data in kernel space of OS should be cautiously handled, e.g., allocating with kmalloc() and freeing with kfree(). However, real cases do exist where memory data is incorrectly allocated/freed, not checked before dereferenced, or left unfreed when out of use. We define these cases as Memory Life-cycle (MLC) bugs, and according to what we know, this new type of software bugs has not been deeply researched yet. In this paper, we go deep into the life-cycle of kernel memory space, including allocation, dereference and free, and propose the first systematical study of MLC bugs and build an automated and scalable detection framework, MLC bug sanitizer (MLCSan). MLCSan is capable of revealing memory allocation and free functions OS kernels. Besides, the occurrences of allocating, dereferencing and freeing sites can be automatically detected by MLCSan, leading to cases where MLC bugs may appear. Moreover, experiment result of analyzing the latest mainline OS kernels with MLCSan is a strong proof that MLCSan is effective in detecting MLC bugs and can scale to different platforms, in which 41 new bugs are identified in Linux and FreeBSD. And undoubtedly, we will open source MLCSan prototype to contribute to the security research in this area.",space,645
10.1109/access.2020.2983829,filtered,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,dynamic graph regularization and label relaxation-based sparse matrix regression for two-dimensional feature selection,https://ieeexplore.ieee.org/document/9049130/,"Sparse matrix regression (SMR) is a two-dimensional supervised feature selection method that can directly select the features on matrix data. It uses several couples of left and right regression vectors for each classifier and integrates them in formulating the regression function. However, SMR does not consider the local geometry of image samples, and it assumes that the training samples should exactly fit a linear model or a strict binary label matrix by left and right regression matrices. In order to enlarge margins between different classes and preserve the intrinsic geometry structure of samples in the transformed space, we will propose dynamic graph regularization and label relaxation-based SMR (abbreviated as DGRLR-SMR) method for two-dimensional supervised feature selection. First, the label relaxation SMR is established by relaxing the strict binary label matrix into a slack variable matrix via a nonnegative label relaxation matrix by the $\varepsilon $ -dragging technique. Second, we construct a dynamic graph matrix learning model, rather than using the heat kernel function to obtain a fixed graph matrix, to capture the discriminative information and the local manifold structure of the image samples. Therefore, the proposed model not only enlarges margins between different classes, but also obtains a sparse transformation matrix and avoids the problem of over-fitting. An optimization algorithm is devised to solve this model, and it has closed-form solutions in each iteration so that it can be implemented easily in real application. Extensive experiments on several data sets demonstrate the superiority of our method.",space,646
10.1109/tse.2017.2786222,filtered,IEEE Transactions on Software Engineering,IEEE,2018-07-01 00:00:00,ieeexplore,effectively incorporating expert knowledge in automated software remodularisation,https://ieeexplore.ieee.org/document/8259332/,"Remodularising the components of a software system is challenging: sound design principles (e.g., coupling and cohesion) need to be balanced against developer intuition of which entities conceptually belong together. Despite this, automated approaches to remodularisation tend to ignore domain knowledge, leading to results that can be nonsensical to developers. Nevertheless, suppling such knowledge is a potentially burdensome task to perform manually. A lot information may need to be specified, particularly for large systems. Addressing these concerns, we propose the SUpervised reMOdularisation (SUMO) approach. SUMO is a technique that aims to leverage a small subset of domain knowledge about a system to produce a remodularisation that will be acceptable to a developer. With SUMO, developers refine a modularisation by iteratively supplying corrections. These corrections constrain the type of remodularisation eventually required, enabling SUMO to dramatically reduce the solution space. This in turn reduces the amount of feedback the developer needs to supply. We perform a comprehensive systematic evaluation using 100 real world subject systems. Our results show that SUMO guarantees convergence on a target remodularisation with a tractable amount of user interaction.",space,647
10.1109/access.2019.2905000,filtered,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,electrocardiogram reconstruction based on compressed sensing,https://ieeexplore.ieee.org/document/8667447/,"Compressed Sensing (CS) attempts to acquire and reconstruct a sparse signal from a sampling much below the Nyquist rate. In this paper, we proposed novel CS algorithms for reconstructing under-sampled and compressed electrocardiogram (ECG) signal. In the proposed CS-ECG scheme, the ECG signal was first sub-sampled randomly and mapped onto a two-dimensional (2D) space by using Cut and Align (CAB), for the purpose of promoting sparsity. A nonlinear optimization model was then used to reconstruct the 2D signal. In the compression scheme, the ECG signal was mapped into the frequency domain, and the compression was achieved by a series of multiplying and accumulating between the original ECG and a Gaussian random matrix. For the reconstruction, two matching pursuits (MP) methods and two blocks sparse Bayesian learning (BSBL) methods were implemented and evaluated by the percentage root-mean-square difference (PRD). Based on the test with real ECG data, it was found that the proposed CS scheme was capable of faithfully reconstructing ECG signals with only 30% acquisition.",space,648
10.1109/lra.2020.3012951,filtered,IEEE Robotics and Automation Letters,IEEE,2020-10-01 00:00:00,ieeexplore,end-to-end tactile feedback loop: from soft sensor skin over deep gru-autoencoders to tactile stimulation,https://ieeexplore.ieee.org/document/9152113/,"Tactile feedback is a key sensory channel that contributes to our ability to perform precise manipulations. In this regard, sensor skin provides robots with the sense of touch making them increasingly capable of dexterous object manipulation. However, in applications like teleoperation, the complex sensory input of an infinite number of different textures must be projected to the human user's skin in a meaningful manner. In addressing this issue, a deep gated recurrent unit-based autoencoder (GRU-AE) that captured the perceptual dimensions of tactile textures in latent space was deployed to implicitly understand unseen textures. The expression of unknown textures in this latent space allowed for the definition of a control law to effectively drive tactile displays and to convey tactile feedback in a psycho-physically meaningful manner. The approach was experimentally verified by evaluating the prediction performance of the GRU-AE on seen and unseen data that were gathered during active tactile exploration of objects commonly encountered in daily living. A user study on a custom-made tactile display was conducted in which real tactile perceptions in response to active tactile object exploration were compared to the emulated tactile feedback using the proposed tactile feedback loop. The results suggest that the deep GRU-AE for tactile display control offers an effective and intuitive method for efficient end-to-end tactile feedback during active tactile texture exploration.",space,649
10.1109/access.2020.3047343,filtered,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,establishing trust in online advertising with signed transactions,https://ieeexplore.ieee.org/document/9306812/,"Programmatic advertising operates one of the most sophisticated and efficient service platforms on the Internet. However, the complexity of this ecosystem is a direct cause of one of the most important problems in online advertising, the lack of transparency. This lack of transparency enables subsequent problems such as advertising fraud, which causes billions of dollars in losses. In this paper we propose Ads.chain, a technological solution to the lack-of-transparency problem in programmatic advertising. Ads.chain extends the current effort of the Internet Advertising Bureau (IAB) in providing traceability in online advertising through the Ads.txt and Ads.cert solutions, addressing the limitations of these techniques. Ads.chain is (to the best of the authors' knowledge) the first solution that provides end-to-end cryptographic traceability at the ad transaction level. It is a communication protocol that can be seamlessly embedded into ad-tags and the OpenRTB protocol, the de-facto standards for communications in online advertising, allowing an incremental adoption by the industry. We have implemented Ads.chain and made the code publicly available. We assess the performance of Ads.chain through a thorough analysis in a lab environment that emulates a real ad delivery process at real-life throughputs. The obtained results show that Ads.chain can be implemented with limited impact on the hardware resources and marginal delay increments at the publishers lower than 0.20 milliseconds per ad space on webpages and 2.6 milliseconds at the programmatic advertising platforms. These results confirm that Ads.chain's impact on the user experience and the overall operation of the programmatic ad delivery process can be considered negligible.",space,650
10.1109/jsait.2020.2991005,filtered,IEEE Journal on Selected Areas in Information Theory,IEEE,2020-05-01 00:00:00,ieeexplore,extracting robust and accurate features via a robust information bottleneck,https://ieeexplore.ieee.org/document/9088132/,"We propose a novel strategy for extracting features in supervised learning that can be used to construct a classifier which is more robust to small perturbations in the input space. Our method builds upon the idea of the information bottleneck, by introducing an additional penalty term that encourages the Fisher information of the extracted features to be small when parametrized by the inputs. We present two formulations where the relevance of the features to output labels is measured using either mutual information or MMSE. By tuning the regularization parameter, we can explicitly trade off the opposing desiderata of robustness and accuracy when constructing a classifier. We derive optimal solutions to both robust information bottleneck formulations when the inputs and outputs are jointly Gaussian, proving that the optimally robust features are also jointly Gaussian in this setting. We also propose methods for optimizing variational bounds on the robust information bottleneck objectives in general settings using stochastic gradient descent, which may be implemented efficiently in neural networks. Our experimental results for synthetic and real data sets show that the proposed feature extraction methods indeed produce classifiers with increased robustness to perturbations.",space,651
10.1109/tnn.2002.804315,filtered,IEEE Transactions on Neural Networks,IEEE,2003-01-01 00:00:00,ieeexplore,fast minimization of structural risk by nearest neighbor rule,https://ieeexplore.ieee.org/document/1176133/,"In this paper, we present a novel nearest neighbor rule-based implementation of the structural risk minimization principle to address a generic classification problem. We propose a fast reference set thinning algorithm on the training data set similar to a support vector machine (SVM) approach. We then show that the nearest neighbor rule based on the reduced set implements the structural risk minimization principle, in a manner which does not involve selection of a convenient feature space. Simulation results on real data indicate that this method significantly reduces the computational cost of the conventional SVMs, and achieves a nearly comparable test error performance.",space,652
10.1109/72.97911,filtered,IEEE Transactions on Neural Networks,IEEE,1991-05-01 00:00:00,ieeexplore,fast training algorithms for multilayer neural nets,https://ieeexplore.ieee.org/document/97911/,"An algorithm that is faster than back-propagation and for which it is not necessary to specify the number of hidden units in advance is described. The relationship with other fast pattern-recognition algorithms, such as algorithms based on k-d trees, is discussed. The algorithm has been implemented and tested on artificial problems, such as the parity problem, and on real problems arising in speech recognition. Experimental results, including training times and recognition accuracy, are given. Generally, the algorithm achieves accuracy as good as or better than nets trained using back-propagation. Accuracy is comparable to that for the nearest-neighbor algorithm, which is slower and requires more storage space.&lt;<ETX>&gt;</ETX>",space,653
10.1109/tmi.2015.2409265,filtered,IEEE Transactions on Medical Imaging,IEEE,2015-09-01 00:00:00,ieeexplore,feature-preserving noise removal,https://ieeexplore.ieee.org/document/7055927/,"Conventional image restoration algorithms use transform-domain filters, which separate the noise from the sparse signal among the transform components or apply spatial smoothing filters in real space whose design relies on prior assumptions about the noise statistics. These filters also reduce the information content of the image by suppressing spatial frequencies or by recognizing only a limited set of shapes. Here we show that denoising can be efficiently done using a nonlinear filter, which operates along patch neighborhoods and multiple copies of the original image. The use of patches enables the algorithm to account for spatial correlations in the random field whereas the multiple copies are used to recognize the noise statistics. The nonlinear filter, which is implemented by a hierarchical multistage system of multilayer perceptrons, outperforms state-of-the-art denoising algorithms such as those based on collaborative filtering and total variation. Compared to conventional denoising algorithms, our filter can restore images without blurring them, making it attractive for use in medical imaging where the preservation of anatomical details is critical.",space,654
10.1109/jstqe.2020.2975579,filtered,IEEE Journal of Selected Topics in Quantum Electronics,IEEE,2020-09-01 00:00:00,ieeexplore,femtojoule per mac neuromorphic photonics: an energy and technology roadmap,https://ieeexplore.ieee.org/document/9006831/,"Photonic artificial neural networks have garnered enormous attention due to their potential to perform multiply-accumulate (MAC) operations at much higher clock rates and consuming significantly lower power and chip real-estate compared to digital electronic alternatives. Herein, we present a comprehensive power consumption analysis of photonic neurons, taking into account global design parameters and concluding to analytical expressions for the neuron's energy- and footprint efficiencies. We identify the optimal design-space and analyze the performance plateaus and their dependence on a range of physical parameters, highlighting the existence of an optimal data-rate for maximizing the energy efficiency. Following a survey of the best-in-class integrated photonic devices, including on-chip lasers, photodetectors, modulators and weighting elements, the mathematically calculated energy and footprint efficiencies are mapped into real photonic neuron deployment scenarios. We reveal that silicon photonics can compete with the best-performing currently available digital electronic neural network engines, reaching TMAC/s/mm<sup>2</sup> footprint- and sub-pJ/MAC energy efficiencies. Simultaneously, neuromorphic plasmonics, plasmo-photonics and sub-wavelength photonics hold the credentials for 1 to 3 orders of magnitude improvements even when the laser requirements and a reasonable waveguide pitch are accounted for, promising performance at a few fJ/MAC and up to a few TMAC/s/mm<sup>2</sup>.",space,655
10.1109/83.704306,filtered,IEEE Transactions on Image Processing,IEEE,1998-08-01 00:00:00,ieeexplore,foveal automatic target recognition using a multiresolution neural network,https://ieeexplore.ieee.org/document/704306/,"This paper presents a method for detecting and classifying a target from its foveal (graded resolution) imagery using a multiresolution neural network. Target identification decisions are based on minimizing an energy function. This energy function is evaluated by comparing a candidate blob with a library of target models at several levels of resolution simultaneously available in the current foveal image. For this purpose, a concurrent (top-down-and-bottom-up) matching procedure is implemented via a novel multilayer Hopfield (1985) neural network. The associated energy function supports not only interactions between cells at the same resolution level, but also between sets of nodes at distinct resolution levels. This permits features at different resolution levels to corroborate or refute one another contributing to an efficient evaluation of potential matches. Gaze control, refoveation to more salient regions of the available image space, is implemented as a search for high resolution features which will disambiguate the candidate blob. Tests using real two-dimensional (2-D) objects and their simulated foveal imagery are provided.",space,656
10.1109/lra.2019.2955941,filtered,IEEE Robotics and Automation Letters,IEEE,2020-01-01 00:00:00,ieeexplore,generative localization with uncertainty estimation through video-ct data for bronchoscopic biopsy,https://ieeexplore.ieee.org/document/8913461/,"Robot-assisted endobronchial intervention requires accurate localization based on both intra- and pre-operative data. Most existing methods achieve this by registering 2D videos with 3D CT models according to a defined similarity metric with local features. Instead, we formulate the bronchoscopic localization as a learning-based global localisation using deep neural networks. The proposed network consists of two generative architectures and one auxiliary learning component. The cycle generative architecture bridges the domain variance between the real bronchoscopic videos and virtual views derived from pre-operative CT data so that the proposed approach can be trained through a large number of generated virtual images but deployed through real images. The auxiliary learning architecture leverages complementary relative pose regression to constrain the search space, ensuring consistent global pose predictions. Most importantly, the uncertainty of each global pose is obtained through variational inference by sampling within the learned underlying probability distribution. Detailed validation results demonstrate the localization accuracy with reasonable uncertainty achieved and its potential clinical value. A demonstration video demo can be found on the website <uri>https://youtu.be/ci9LMY49aF8</uri>.",space,657
10.1109/tmech.2021.3079935,filtered,IEEE/ASME Transactions on Mechatronics,IEEE,2021-08-01 00:00:00,ieeexplore,geometrical-based displacement measurement with pseudostereo monocular camera on bidirectional cascaded linear actuator,https://ieeexplore.ieee.org/document/9430727/,"This article details the development of a geometrical-based displacement extraction framework capable of automatically extracting critical infrastructure measurements in one sequence. The framework is a novel rail viaduct bearing inspection pipeline implemented on Bearing Inspector for Narrow-space Observation Version 2 (BINOv2). BINOv2 is a tethered custom unmanned aerial vehicle system utilized to supplant labor-intensive pipelines and enhance inspection accuracy of infrastructure conditions in confined remote locations. The algorithm accepts stereoscopic images taken from a single monocular camera on a bidirectional cascaded linear actuator system in a rack-and-pinion configuration. A point cloud model generated from the image sets then runs through a hierarchical neural network for 3-D segmentation to extract targeted regions of interest. Our training pipeline generates and forms the full model's training dataset using only a small sample of real point clouds. The point cloud generated is inadequate to form the full bearing geometry profile. Therefore, the proposed framework projects best-fit circles based on the point cloud curvature to form the full bearing geometry profile so that the required displacement measurement is available for extraction. Several experiments were conducted on a mock-up and actual operational site to validate the proposed framework's accuracy, its robustness and comparison with other state-of-the-art alternatives.",space,658
10.1109/access.2019.2953176,filtered,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,group recommendation via self-attention and collaborative metric learning model,https://ieeexplore.ieee.org/document/8896961/,"Group recommendation has attracted wide attention owing to its significance in real applications. One of the big challenges for group recommendation systems is how to integrate individual preferences of each group member and attain overall preferences for the group. Most of the traditional group recommendation solutions regard group members as equal participants and assign a same weight to each member. As a result, performance of this type of recommendation methods is not as good as expected. To improve the performance of group recommendation, a novel group recommendation model via Self-Attention and Collaborative Metric Learning (SACML) is presented in this paper. With the employment of Self-Attention mechanism, the SACML model can learn the similarity interactions between group members and services and decide a different weight for different group member. Based on these weights, group preferences for services can be generated by the aggregation of group members' preferences and the group's own preference. Similar metric space between group and services is obtained via collaborative metric learning with the group preferences and positive and negative services' features. Group recommendation is finally implemented based on the obtained metric space. Simulation has been conducted on CAMRa2011 and Meetup datasets, and experimental results show that the proposed SACML model has better performance in comparison with those baseline methods.",space,659
10.1109/access.2017.2768405,filtered,IEEE Access,IEEE,2017-01-01 00:00:00,ieeexplore,hand-mouse interface using virtual monitor concept for natural interaction,https://ieeexplore.ieee.org/document/8091117/,"The growing interest in human-computer interaction has prompted research in this area. In addition, research has been conducted on a natural user interface/natural user experience (NUI/NUX), which utilizes a user's gestures and voice. In the case of NUI/NUX, a recognition algorithm is needed for the gestures or voice. However, such recognition algorithms have weaknesses because their implementation is complex, and they require a large amount of time for training. Therefore, steps that include pre-processing, normalization, and feature extraction are needed. In this paper, we designed and implemented a hand-mouse interface that introduces a new concept called a “virtual monitor”, to extract a user's physical features through Kinect in real time. This virtual monitor allows a virtual space to be controlled by the hand mouse. It is possible to map the coordinates on the virtual monitor to the coordinates on the real monitor accurately. A hand-mouse interface based on the virtual monitor concept maintains the outstanding intuitiveness that is the strength of the previous study and enhances the accuracy of mouse functions. In order to evaluate the intuitiveness and accuracy of the interface, we conducted an experiment with 50 volunteers ranging from teenagers to those in their 50s. The results of this intuitiveness experiment showed that 84% of the subjects learned how to use the mouse within 1 min. In addition, the accuracy experiment showed the high accuracy level of the mouse functions [drag (80.9%), click (80%), double-click (76.7%)]. This is a good example of an interface for controlling a system by hand in the future.",space,660
10.1109/3477.499796,filtered,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",IEEE,1996-06-01 00:00:00,ieeexplore,hidden state and reinforcement learning with instance-based state identification,https://ieeexplore.ieee.org/document/499796/,"Real robots with real sensors are not omniscient. When a robot's next course of action depends on information that is hidden from the sensors because of problems such as occlusion, restricted range, bounded field of view and limited attention, we say the robot suffers from the hidden state problem. State identification techniques use history information to uncover hidden state. Some previous approaches to encoding history include: finite state machines, recurrent neural networks and genetic programming with indexed memory. A chief disadvantage of all these techniques is their long training time. This paper presents instance-based state identification, a new approach to reinforcement learning with state identification that learns with much fewer training steps. Noting that learning with history and learning in continuous spaces both share the property that they begin without knowing the granularity of the state space, the approach applies instance-based (or ""memory-based"") learning to history sequences-instead of recording instances in a continuous geometrical space, we record instances in action-percept-reward sequence space. The first implementation of this approach, called Nearest Sequence Memory, learns with an order of magnitude fewer steps than several previous approaches.",space,661
10.1109/tgrs.2021.3102136,filtered,IEEE Transactions on Geoscience and Remote Sensing,IEEE,2022-01-01 00:00:00,ieeexplore,hy-demosaicing: hyperspectral blind reconstruction from spectral subsampling,https://ieeexplore.ieee.org/document/9513279/,"This article proposes a smart hyperspectral sensing strategy, implemented in the spectral domain, conceived for spaceborne sensor systems, where physical space, storage resources, and communication bandwidth are extremely scarce and expensive. Smart sensing means faster and hardware-friendly imaging. Instead of acquiring all band samples in the spectral domain, we randomly select a few band samples per spatial pixel location. A periodic structure of spectral band selector array (SBSA) is designed so that we can learn a subspace basis from subsamples, which is essential to the underlying hyperspectral image (HSI) recovery algorithm. This spectral subsampling sensing strategy yields a demosaicing problem. We propose a blind hyperspectral reconstruction technique termed hyperspectral demosaicing (Hy-demosaicing) exploiting spectral low-rankness and spatial correlation of HSIs. It is blind in the sense that the signal subspace is learned from measured spectral subsamples. The subspace basis is data-adaptive and provides a more compact representation than other non-adaptive representations. This adaptiveness leads to improved image recovery as illustrated in experiments with real data.",space,662
10.1109/access.2019.2894524,filtered,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,hybrid stochastic exploration using grey wolf optimizer and coordinated multi-robot exploration algorithms,https://ieeexplore.ieee.org/document/8631022/,"Multi-robot exploration is a search of uncertainty in restricted space seeking to build a finite map by a group of robots. It has the main task to distribute the search assignments among robots in real time. In this paper, we proposed a stochastic optimization for multi-robot exploration that mimics the coordinated predatory behavior of grey wolves via simulation. Here, the robot movement is computed by the combined deterministic and metaheuristic techniques. It uses the Coordinated Multi-Robot Exploration and GreyWolf Optimizer algorithms as a new method called the hybrid stochastic exploration. Initially, the deterministic cost and utility determine the precedence of adjacent cells around a robot. Then, the stochastic optimization improves the overall solution. It implies that the robots evaluate the environment by the deterministic approach and move on using the metaheuristic algorithm. The proposed hybrid method was implemented on simple and complex maps and compared with the Coordinated Multi-Robot Exploration algorithm. The simulation results show that the stochastic optimization enhances the deterministic approach to completely explore and map out the areas.",space,663
10.1109/70.63270,filtered,IEEE Transactions on Robotics and Automation,IEEE,1990-12-01 00:00:00,ieeexplore,hybrid hierarchical scheduling and control systems in manufacturing,https://ieeexplore.ieee.org/document/63270/,"Some experiments on the integration of algorithmic techniques with knowledge-based ones are discussed. Two case studies are presented: an FMS cell and a press shop. It was found that the algorithmic procedures developed for production scheduling resulted in limiting the ability to cope with the complexity of the real manufacturing world. The scheduling problem, seen as a constraint satisfaction problem, can be approached with rule-based techniques. Nevertheless, algorithmic techniques are found to be valuable for their efficiency and ability to deal with aggregated data. This ability is fundamental for an efficient implementation of hierarchical control systems in general and in the manufacturing context in particular. This suggests that the integration of rule-based techniques with algorithmic ones can increase the efficiency of searching in the space of possible solutions. The ability to deal with aggregated data can have little value when detailed real-time operation scheduling is needed. In this case, simple dispatching rules are often used, and sophisticated operations research methods are not used. In such a dynamic situation, a purely-rule based approach may be more suitable.&lt;<ETX>&gt;</ETX>",space,664
10.1109/tip.2009.2025560,filtered,IEEE Transactions on Image Processing,IEEE,2009-10-01 00:00:00,ieeexplore,image segmentation based on grabcut framework integrating multiscale nonlinear structure tensor,https://ieeexplore.ieee.org/document/5075666/,"In this paper, we propose an interactive color natural image segmentation method. The method integrates color feature with multiscale nonlinear structure tensor texture (MSNST) feature and then uses GrabCut method to obtain the segmentations. The MSNST feature is used to describe the texture feature of an image and integrated into GrabCut framework to overcome the problem of the scale difference of textured images. In addition, we extend the Gaussian Mixture Model (GMM) to MSNST feature and GMM based on MSNST is constructed to describe the energy function so that the texture feature can be suitably integrated into GrabCut framework and fused with the color feature to achieve the more superior image segmentation performance than the original GrabCut method. For easier implementation and more efficient computation, the symmetric KL divergence is chosen to produce the estimates of the tensor statistics instead of the Riemannian structure of the space of tensor. The Conjugate norm was employed using Locality Preserving Projections (LPP) technique as the distance measure in the color space for more discriminating power. An adaptive fusing strategy is presented to effectively adjust the mixing factor so that the color and MSNST texture features are efficiently integrated to achieve more robust segmentation performance. Last, an iteration convergence criterion is proposed to reduce the time of the iteration of GrabCut algorithm dramatically with satisfied segmentation accuracy. Experiments using synthesis texture images and real natural scene images demonstrate the superior performance of our proposed method.",space,665
10.1109/tamd.2011.2128318,filtered,IEEE Transactions on Autonomous Mental Development,IEEE,2011-09-01 00:00:00,ieeexplore,improved binocular vergence control via a neural network that maximizes an internally defined reward,https://ieeexplore.ieee.org/document/5734799/,"We describe the autonomous development of binocular vergence control in an active robotic vision system through attention-gated reinforcement learning (AGREL). The control policy is implemented by a neural network, which maps the outputs from a population of disparity energy neurons to a set of vergence commands. The network learns to maximize a reward signal that is based on an internal representation of the visual input: the total activation in the population of disparity energy neurons. This system extends previous work using Q learning by increasing the complexity of the policy in two ways. First, the input state space is continuous, rather than discrete, and is based upon a larger diversity of neurons. Second, we increase the number of possible actions. We evaluate the network learning and performance on natural images and with real objects in a cluttered environment. The policies learned by the network outperform policies by Q learning in two ways: the mean squared errors are smaller and the closed loop frequency response has larger bandwidth.",space,666
10.1109/access.2019.2902564,filtered,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,improving indoor fingerprinting positioning with affinity propagation clustering and weighted centroid fingerprint,https://ieeexplore.ieee.org/document/8667640/,"Nowadays, research and development of various indoor positioning systems (IPS) have been increasing owing to flourishing social and commercial interest in location-based services (LBSs). Among LBS technologies, we used the Bluetooth low energy beacon in our system, which consumes less energy and is embedded in many current smartphones and tablets. In particular, the fingerprinting method has become a prime choice in the design of IPS owing to its good location estimation and the fact that a line-of-sight from access points is not required. We propose an improved two-step fingerprinting localization using multiple fingerprint features to enhance the localization accuracy. The proposed system uses a propagation model to convert RSS of beacons to distance and estimate the weighted centroid (WC) of nearby beacons. The estimated WCs along with signal strength and rank of the nearby beacons are stored in the server database for localization instead of RSS from all the deployed beacons. First, the proposed system makes use of diverse fingerprinting features to increase localization accuracy that also reduces both the physical size of the database and the amount of data communication with the server in the execution phase; second, affinity propagation clustering minimizes the searching space of RPs and reduces the computational cost; third, exponential averaging is introduced to smooth the noisy RSS. The experimental results obtained by real field deployment show that the proposed method significantly improves the performance of the positioning system in both the positioning accuracy and radio-map database size.",space,667
10.1109/access.2021.3109862,filtered,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,influential attributed communities search in large networks (infacom),https://ieeexplore.ieee.org/document/9528361/,"Community search is a fundamental problem in graph analysis. In many applications, network nodes have specific properties that are essential for making sense of communities. In these networks, attributes are associated with nodes to capture their properties. The community influence is a key property of the community that can be employed to sort the communities in a network based on the relevance/importance of certain attributes. Unfortunately, most of the previously introduced community search algorithms over attributed networks neglected the community influence. In this paper, we study the influential attributed community search problem. Different factors for measuring the influence are discussed. Also, different Influential Attributed Community (InfACom) algorithms based on the concept of k-clique are proposed. Two techniques are presented one for sequential implementation with three variations and one for parallel implementation. In addition, we propose efficient algorithms for maintaining the proposed algorithms on dynamic graphs. The proposed algorithms are evaluated on different real datasets. The experimental results show that the summarization technique reduces the size of the graph by approximately half. In addition, it shows that the proposed algorithms <inline-formula> <tex-math notation=""LaTeX"">$EnhancedExact$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=""LaTeX"">$Approximate$ </tex-math></inline-formula> outperform the state-of-the-art approaches Incremental Time efficient <inline-formula> <tex-math notation=""LaTeX"">$(Inc-T)$ </tex-math></inline-formula>, Incremental Space efficient <inline-formula> <tex-math notation=""LaTeX"">$(Inc-S)$ </tex-math></inline-formula>, <inline-formula> <tex-math notation=""LaTeX"">$Exact$ </tex-math></inline-formula>, and 2-Approximation <inline-formula> <tex-math notation=""LaTeX"">$(AppInc)$ </tex-math></inline-formula> in both efficiency and effectiveness. For the <inline-formula> <tex-math notation=""LaTeX"">$EnhancedExact$ </tex-math></inline-formula> algorithm, the results show that the efficiency is at least 7 times faster than the <inline-formula> <tex-math notation=""LaTeX"">$Inc-S$ </tex-math></inline-formula> algorithm, at least 4.5 times faster than the <inline-formula> <tex-math notation=""LaTeX"">$Inc-T$ </tex-math></inline-formula> algorithm, and 2 times faster than the <inline-formula> <tex-math notation=""LaTeX"">$AppInc$ </tex-math></inline-formula> algorithm. For the <inline-formula> <tex-math notation=""LaTeX"">$Approximate$ </tex-math></inline-formula> algorithm, the results show that its efficiency is at least 10 times faster than the <inline-formula> <tex-math notation=""LaTeX"">$Inc-S$ </tex-math></inline-formula> algorithm, at least 6.4 times faster than the <inline-formula> <tex-math notation=""LaTeX"">$Inc-T$ </tex-math></inline-formula> algorithm, and 3 times faster than the <inline-formula> <tex-math notation=""LaTeX"">$AppInc$ </tex-math></inline-formula> algorithm. Finally, the results show that the proposed algorithms retrieve cohesive communities with a smaller diameter than all the state-of-the-art approaches.",space,668
10.1109/lra.2020.3013937,filtered,IEEE Robotics and Automation Letters,IEEE,2020-10-01 00:00:00,ieeexplore,invariant transform experience replay: data augmentation for deep reinforcement learning,https://ieeexplore.ieee.org/document/9158366/,"Deep Reinforcement Learning (RL) is a promising approach for adaptive robot control, but its current application to robotics is currently hindered by high sample requirements. To alleviate this issue, we propose to exploit the symmetries present in robotic tasks. Intuitively, symmetries from observed trajectories define transformations that leave the space of feasible RL trajectories invariant and can be used to generate new feasible trajectories, which could be used for training. Based on this data augmentation idea, we formulate a general framework, called Invariant Transform Experience Replay that we present with two techniques: (i) Kaleidoscope Experience Replay exploits reflectional symmetries and (ii) Goal-augmented Experience Replay which takes advantage of lax goal definitions. In the Fetch tasks from OpenAI Gym, our experimental results show significant increases in learning rates and success rates. Particularly, we attain a 13, 3, and 5 times speedup in the pushing, sliding, and pick-and-place tasks respectively in the multi-goal setting. Performance gains are also observed in similar tasks with obstacles and we successfully deployed a trained policy on a real Baxter robot. Our work demonstrates that invariant transformations on RL trajectories are a promising methodology to speed up learning in deep RL. Code, video, and supplementary materials are available at [1].",space,669
10.1109/access.2019.2895626,filtered,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,joint access mode selection and spectrum allocation for fog computing based vehicular networks,https://ieeexplore.ieee.org/document/8629049/,"The explosive growth of data with the requirements of high reliability and low latency has posed huge challenges to the vehicular networks. One potential solution is to deploy the fog computing servers geographically closer to the vehicles to serve the vehicle-based applications in real time. However, due to the constraint of caching storage space as well as lack of tractable access mode selection and spectrum allocation algorithm, it is very challenging to balance the network transmission performance and fronthaul savings. In this paper, we investigate the joint optimization problem of access mode selection and spectrum allocation in fog computing-based vehicular networks. To solve the problem in an efficient way, the original high complexity optimization problem is divided into two subproblems. Wherein, the vehicle access mode selection problem is solved by Q-learning-based algorithm by considering spectrum allocation profiles and the fronthaul link cost. The randomly vehicular network topologies are modeled as CoX processes and the closed-form payoff expressions are derived by the stochastic geometry tools. On the other hand, the optimal spectrum allocation indicator value can be finally obtained via convex optimization. The analytical results for the proposed algorithm as well as the traditional baseline approaches are evaluated with different weight factors, which verify our theoretical analysis and confirm the proposed approach can achieve significant performance gains.",space,670
10.1109/tpami.2006.97,filtered,IEEE Transactions on Pattern Analysis and Machine Intelligence,IEEE,2006-05-01 00:00:00,ieeexplore,joint multiregion segmentation and parametric estimation of image motion by basis function representation and level set evolution,https://ieeexplore.ieee.org/document/1608040/,"The purpose of this study is to investigate a variational method for joint segmentation and parametric estimation of image motion by basis function representation of motion and level set evolution. The functional contains three terms. One term is of classic regularization to bias the solution toward a segmentation with smooth boundaries. A second term biases the solution toward a segmentation with boundaries which coincide with motion discontinuities, following a description of motion discontinuities by a function of the image spatio-temporal variations. The third term refers to region information and measures conformity of the parametric representation of the motion of each region of segmentation to the image spatio-temporal variations. The components of motion in each region of segmentation are represented as functions in a space generated by a set of basis functions. The coefficients of the motion components considered combinations of the basis functions are the parameters of representation. The necessary conditions for a minimum of the functional, which are derived taking into consideration the dependence of the motion parameters on segmentation, lead to an algorithm which condenses to concurrent curve evolution, implemented via level sets, and estimation of the parameters by least squares within each region of segmentation. The algorithm and its implementation are verified on synthetic and real images using a basis of cosine transforms.",space,671
10.1109/lra.2020.3010739,filtered,IEEE Robotics and Automation Letters,IEEE,2020-10-01 00:00:00,ieeexplore,learning force control for contact-rich manipulation tasks with rigid position-controlled robots,https://ieeexplore.ieee.org/document/9145608/,"Reinforcement Learning (RL) methods have been proven successful in solving manipulation tasks autonomously. However, RL is still not widely adopted on real robotic systems because working with real hardware entails additional challenges, especially when using rigid position-controlled manipulators. These challenges include the need for a robust controller to avoid undesired behavior, that risk damaging the robot and its environment, and constant supervision from a human operator. The main contributions of this work are, first, we proposed a learning-based force control framework combining RL techniques with traditional force control. Within said control scheme, we implemented two different conventional approaches to achieve force control with position-controlled robots; one is a modified parallel position/force control, and the other is an admittance control. Secondly, we empirically study both control schemes when used as the action space of the RL agent. Thirdly, we developed a fail-safe mechanism for safely training an RL agent on manipulation tasks using a real rigid robot manipulator. The proposed methods are validated both on simulation and a real robot with an UR3 e-series robotic arm.",space,672
10.1364/jocn.11.000226,filtered,Journal of Optical Communications and Networking,IEEE,2019-05-01 00:00:00,ieeexplore,learning life cycle to speed up autonomic optical transmission and networking adoption,https://ieeexplore.ieee.org/document/8717575/,"Autonomic optical transmission and networking requires machine learning (ML) models to be trained with large datasets. However, the availability of enough real data to produce accurate ML models is rarely ensured since new optical equipment and techniques are continuously being deployed in the network. One option is to generate data from simulations and lab experiments, but such data could not cover the whole features space and would translate into inaccuracies in the ML models. In this paper, we propose an ML-based algorithm life cycle to facilitate ML deployment in real operator networks. The dataset for ML training can be initially populated based on the results from simulations and lab experiments. Once ML models are generated, ML retraining can be performed after inaccuracies are detected to improve their precision. Illustrative numerical results show the benefits of the proposed learning cycle for general use cases. In addition, two specific use cases are proposed and demonstrated that implement different learning strategies: (i) a two-phase strategy performing out-of-field training using data from simulations and lab experiments with generic equipment, followed by an in-field adaptation to support heterogeneous equipment (the accuracy of this strategy is shown for a use case of failure detection and identification), and (ii) in-field retraining, where ML models are retrained after detecting model inaccuracies. Different approaches are analyzed and evaluated for a use case of autonomic transmission, where results show the significant benefits of collective learning.",space,673
10.1109/jsen.2013.2263381,filtered,IEEE Sensors Journal,IEEE,2013-08-01 00:00:00,ieeexplore,log-logistic modeling of sensory flow delays in networked telerobots,https://ieeexplore.ieee.org/document/6516944/,"This paper deals with the modeling of the delays in the transmission of sensory data coming from a networked telerobot, which would allow us to predict future times of arrival and provide guarantees on the time requirements of these systems. Considering these delay sequences as an uni-dimensional temporal signal, they easily exhibit rich stochastic behavior—abrupt changes of regime and bursts—due to the heterogeneity of the hardware and software components in the data path. There exist approaches for modeling this kind of signals without explicit knowledge of the system components: state-space reconstruction, hidden Markov models, neural networks, etc., but they are mostly focused on the stochasticity of the network only, without taking into account other elements in the sensory flow that also have an important influence in the delays. Previously, we have proposed simpler statistical methods that do not require any component knowledge either and are suitable for more lightweight implementations (e.g., in mobile phone interfaces). In this sense, we report elsewhere a log-normal three-parametrical model that fits reasonably well these delays as long as change detection is completely solved. Now we propose a more flexible solution: the log-logistic distribution, which has been found to fit delays better than the log-normal. In addition, we present two algorithms to model an entire delay signal, including abrupt nonlinearities, based on the log-logistic assumption. Our results show quite good fittings of real datasets gathered from a number of combinations of sensors, networks, and application software, provided that some mild assumptions hold.",space,674
10.1109/access.2017.2690987,filtered,IEEE Access,IEEE,2017-01-01 00:00:00,ieeexplore,nc-link: a new linkage method for efficient hierarchical clustering of large-scale data,https://ieeexplore.ieee.org/document/7891959/,"In various disciplines, hierarchical clustering (HC) has been an effective tool for data analysis due to its ability to summarize hierarchical structures of data in an intuitive and interpretable manner. A run of HC requires multiple iterations, each of which needs to compute and update the pairwise distances between all intermediate clusters. This makes the exact algorithm for HC inevitably suffer from quadratic time and space complexities. To address large-scale data, various approximate/parallel algorithms have been proposed to reduce the computational cost of HC. However, such algorithms still rely on conventional linkage methods (such as single, centroid, average, complete, or Ward's) for defining pairwise distances, mostly focusing on the approximation/parallelization of linkage computations. Given that the choice of linkage profoundly affects not only the quality but also the efficiency of HC, we propose a new linkage method named NC-link and design an exact algorithm for NC-link-based HC. To guarantee the exactness, the proposed algorithm maintains the quadratic nature in time complexity but exhibits only linear space complexity, thereby allowing us to address million-object data on a personal computer. To underpin the extensibility of our approach, we showthat the algorithmic nature of NC-link enables single instruction multiple data (SIMD) parallelization and subquadratic-time approximation of HC. To verify our proposal, we thoroughly tested it with a number of large-scale real and synthetic data sets. In terms of efficiency, NC-link allowed us to perform HC substantially more space efficiently or faster than conventional methods: compared with average and complete linkages, using NC-link incurred only 0.7%-1.75% of the memory usage, and the NC-link-based implementation delivered speedups of approximately 3.5 times over the centroid and Ward's linkages. With regard to clustering quality, the proposed method was able to retrieve hierarchical structures from input data as faithfully as in the popular average and centroid linkage methods. We anticipate that the existing approximation/parallel algorithms will be able to benefit from adopting NC-link as their linkage method for obtaining better clustering results and reduced time and space demands.",space,675
10.1109/jproc.2018.2856739,filtered,Proceedings of the IEEE,IEEE,2018-11-01 00:00:00,ieeexplore,navigating the landscape for real-time localization and mapping for robotics and virtual and augmented reality,https://ieeexplore.ieee.org/document/8436423/,"Visual understanding of 3-D environments in real time, at low power, is a huge computational challenge. Often referred to as simultaneous localization and mapping (SLAM), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, and virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are: 1) tools and methodology for systematic quantitative evaluation of SLAM algorithms; 2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives; 3) end-to-end simulation tools to enable optimization of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches; and 4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",space,676
10.1109/tnnls.2015.2451535,filtered,IEEE Transactions on Neural Networks and Learning Systems,IEEE,2016-07-01 00:00:00,ieeexplore,near-optimal controller for nonlinear continuous-time systems with unknown dynamics using policy iteration,https://ieeexplore.ieee.org/document/7174547/,"This paper presents a single-network adaptive critic-based controller for continuous-time systems with unknown dynamics in a policy iteration (PI) framework. It is assumed that the unknown dynamics can be estimated using the Takagi-Sugeno-Kang fuzzy model with arbitrary precision. The successful implementation of a PI scheme depends on the effective learning of critic network parameters. Network parameters must stabilize the system in each iteration in addition to approximating the critic and the cost. It is found that the critic updates according to the Hamilton-Jacobi-Bellman formulation sometimes lead to the instability of the closed-loop systems. In the proposed work, a novel critic network parameter update scheme is adopted, which not only approximates the critic at current iteration but also provides feasible solutions that keep the policy stable in the next step of training by combining a Lyapunov-based linear matrix inequalities approach with PI. The critic modeling technique presented here is the first of its kind to address this issue. Though multiple literature exists discussing the convergence of PI, however, to the best of our knowledge, there exists no literature, which focuses on the effect of critic network parameters on the convergence. Computational complexity in the proposed algorithm is reduced to the order of (F<sub>z</sub>)<sup>n-1</sup>, where n is the fuzzy state dimensionality and F<sub>z</sub> is the number of fuzzy zones in the states space. A genetic algorithm toolbox of MATLAB is used for searching stable parameters while minimizing the training error. The proposed algorithm also provides a way to solve for the initial stable control policy in the PI scheme. The algorithm is validated through real-time experiment on a commercial robotic manipulator. Results show that the algorithm successfully finds stable critic network parameters in real time for a highly nonlinear system.",space,677
10.1109/60.475850,filtered,IEEE Transactions on Energy Conversion,IEEE,1995-12-01 00:00:00,ieeexplore,neural-net based coordinated stabilizing control for the exciter and governor loops of low head hydropower plants,https://ieeexplore.ieee.org/document/475850/,"This paper presents a design technique of a new adaptive optimal controller of the low head hydropower plant using artificial neural networks (ANN). The adaptive controller is to operate in real time to improve the generating unit transients through the exciter input, the guide vane position and the runner blade position. The new design procedure is based on self-organization and the predictive estimation capabilities of neural-nets implemented through the cluster-wise segmented associative memory scheme. The developed neural-net based controller (NNC) whose control signals are adjusted using the on-line measurements, can offer better damping effects for generator oscillations over a wide range of operating conditions than conventional controllers. Digital simulations of hydropower plant equipped with low head Kaplan turbines are performed and the comparisons of conventional excitation-governor state-space optimal control and neural-net based control are presented. Results obtained on the nonlinear mathematical model demonstrate that the effects of the NNC closely agree with those obtained using the state-space multivariable discrete-time optimal controllers.",space,678
10.1093/mnras/stx687,filtered,Monthly Notices of the Royal Astronomical Society,OUP,2017-01-01 00:00:00,ieeexplore,on the realistic validation of photometric redshifts,https://ieeexplore.ieee.org/document/8209446/,"Two of the main problems encountered in the development and accurate validation of photometric redshift (photo-z) techniques are the lack of spectroscopic coverage in the feature space (e.g. colours and magnitudes) and the mismatch between the photometric error distributions associated with the spectroscopic and photometric samples. Although these issues are well known, there is currently no standard benchmark allowing a quantitative analysis of their impact on the final photo-z estimation. In this work, we present two galaxy catalogues, Teddy and Happy, built to enable a more demanding and realistic test of photo-z methods. Using photometry from the Sloan Digital Sky Survey and spectroscopy from a collection of sources, we constructed data sets that mimic the biases between the underlying probability distribution of the real spectroscopic and photometric sample. We demonstrate the potential of these catalogues by submitting them to the scrutiny of different photo-z methods, including machine learning (ML) and template fitting approaches. Beyond the expected bad results from most ML algorithms for cases with missing coverage in the feature space, we were able to recognize the superiority of global models in the same situation and the general failure across all types of methods when incomplete coverage is convoluted with the presence of photometric errors – a data situation which photo-z methods were not trained to deal with up to now and which must be addressed by future large-scale surveys. Our catalogues represent the first controlled environment allowing a straightforward implementation of such tests. The data are publicly available within the COINtoolbox (https://github.com/COINtoolbox/photoz_catalogues).",space,679
10.1109/tcad.2012.2188401,filtered,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,IEEE,2012-07-01 00:00:00,ieeexplore,on-chip network-enabled multicore platforms targeting maximum likelihood phylogeny reconstruction,https://ieeexplore.ieee.org/document/6218233/,"In phylogenetic inference, which aims at finding a phylogenetic tree that best explains the evolutionary relationship among a given set of species, statistical estimation approaches such as maximum likelihood (ML) and Bayesian inference provide more accurate estimates than other nonstatistical approaches. However, the improved quality comes at a higher computational cost, as these approaches, even though heuristic driven, involve optimization over multidimensional real continuous space. The number of possible search trees in ML is at least exponential, thereby making runtimes on even modest-sized datasets to clock up to several million CPU hours. Evaluation of these trees, involving node-level likelihood vector computation and branch-length optimization, can be partitioned into tasks (or kernels), providing the application with the potential to benefit from hardware acceleration. The range of hardware acceleration architectures tried so far offer limited degree of fine-grain parallelism. Network-on-chip (NoC) is an emerging paradigm that can efficiently support integration of massive number of cores on a chip. In this paper, we explore the design and performance evaluation of 2-D and 3-D NoC architectures for RAxML, which is one of the most widely used ML software suites. Specifically, we implement the computation kernels of the top three functions consuming more than 85% of the total software runtime. Simulations show that through appropriate choice of NoC architecture, and novel core design, allocation and placement strategies, our NoC-based implementation can achieve individual function-level speedups of 390x to 847x, speed up the targeted kernels in excess of 6500x, and provide end-to-end runtime reductions up to 5x over state-of-the-art multithreaded software.",space,680
10.1109/access.2019.2912215,filtered,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,online shape modification of molecular weight distribution based on the principle of active disturbance rejection controller,https://ieeexplore.ieee.org/document/8694847/,"Molecular weight distribution (MWD), an important microcosmic quality index of high polymer, is online unmeasurable, which makes its closed-loop control extremely difficult. To solve this problem, an online shape modification strategy for polymer MWD is proposed based on the active disturbance rejection controller (ADRC). The temporal-spatial property of MWD is estimated in real time by a three-layers forward network based on orthogonal polynomials basis function, and the shape modification of distribution function is transformed into the tracking control of moment statistics in the state-space description. Taking full advantage of the online measured lower-order moments, dual ADRCs are constructed with two manipulated variables (the flow rate of monomer and initiator) to achieve the high precision tracking of lower-order moments and distribution functions, simultaneously. Furthermore, the stability condition of the closed loop system is proved which can guide the parameter tuning of ADRCs. The proposed control strategy is implemented on the polymerization reaction in the laboratory scale continuous stirred tank reactor (CSTR). The feasibility and robustness are verified in the simulation.",space,681
10.1109/lgrs.2019.2916225,filtered,IEEE Geoscience and Remote Sensing Letters,IEEE,2020-01-01 00:00:00,ieeexplore,optimal canny’s parameters regressions for coastal line detection in satellite-based sar images,https://ieeexplore.ieee.org/document/8736022/,"Canny's algorithm is a very well-known and widely implemented multistage edge detector. The extraction of coastal lines in space-borne-based synthetic aperture radar (SAR) images using this algorithm is particularly complicated because of the multiplicative speckle noise present in them and can only be used if Canny's parameters (CaPP) are chosen appropriately. This letter introduces a methodology for computing functional forms for the CaPP, using functions of the image characteristics through a system that combines artificial neural networks (ANN) with statistical regression. A set of CaPP functional forms is obtained by applying this method on synthetic SAR images. Pratt's figure of merit (PFoM) is used to measure the performance of them, obtaining more than 0.75, on average, in the 14400 synthetic SAR images analyzed. Finally, this set of formulas has been tested for extracting coastal edges from real polynyas SAR images, acquired from Sentinel-1.",space,682
10.1109/titb.2006.884374,filtered,IEEE Transactions on Information Technology in Biomedicine,IEEE,2007-05-01 00:00:00,ieeexplore,optimal coding of vectorcardiographic sequences using spatial prediction,https://ieeexplore.ieee.org/document/4167889/,"This paper discusses principles, implementation details, and advantages of sequence coding algorithm applied to the compression of vectocardiograms (VCG). The main novelty of the proposed method is the automatic management of distortion distribution controlled by the local signal contents in both technical and medical aspects. As in clinical practice, the VCG loops representing P, QRS, and T waves in the three-dimensional (3-D) space are considered here as three simultaneous sequences of objects. Because of the similarity of neighboring loops, encoding the values of prediction error significantly reduces the data set volume. The residual values are de-correlated with the discrete cosine transform (DCT) and truncated at certain energy threshold. The presented method is based on the irregular temporal distribution of medical data in the signal and takes advantage of variable sampling frequency for automatically detected VCG loops. The features of the proposed algorithm are confirmed by the results of the numerical experiment carried out for a wide range of real records. The average data reduction ratio reaches a value of 8.15 while the percent root-mean-square difference (PRD) distortion ratio for the most important sections of signal does not exceed 1.1%",space,683
10.1109/access.2020.2967817,filtered,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,patent analytic citation-based vsm: challenges and applications,https://ieeexplore.ieee.org/document/8963731/,"Patent citations are significant components of patents, which play a vital role in the implementation of patent analysis. However, most of the existed models only focus on the text of patents and do not realize that citations can remedy missing information in the text. A method for citation modeling in patent analysis is proposed to generate patent citation trees in this paper. Correspondingly, a specific neural network is designed for extracting abstract features in patent citation trees. Then, on the basis of extracted features, a new citation-based vector space model (CVSM) combining citations with text of the patent database is constructed for the subsequent applications. An experiment is conducted based on real patents of USPTO. The experimental results show that the proposed CVSM has good performances in several applications, which demonstrate the effectiveness of the proposed CVSM.",space,684
10.1109/access.2018.2875677,filtered,IEEE Access,IEEE,2018-01-01 00:00:00,ieeexplore,patient2vec: a personalized interpretable deep representation of the longitudinal electronic health record,https://ieeexplore.ieee.org/document/8490816/,"The wide implementation of electronic health record (EHR) systems facilitates the collection of large-scale health data from real clinical settings. Despite the significant increase in adoption of EHR systems, these data remain largely unexplored, but present a rich data source for knowledge discovery from patient health histories in tasks, such as understanding disease correlations and predicting health outcomes. However, the heterogeneity, sparsity, noise, and bias in these data present many complex challenges. This complexity makes it difficult to translate potentially relevant information into machine learning algorithms. In this paper, we propose a computational framework, Patient2Vec, to learn an interpretable deep representation of longitudinal EHR data, which is personalized for each patient. To evaluate this approach, we apply it to the prediction of future hospitalizations using real EHR data and compare its predictive performance with baseline methods. Patient2Vec produces a vector space with meaningful structure, and it achieves an area under curve around 0.799, outperforming baseline methods. In the end, the learned feature importance can be visualized and interpreted at both the individual and population levels to bring clinical insights.",space,685
10.1109/access.2021.3071485,filtered,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,perceptual borderline for balancing multi-class spontaneous emotional data,https://ieeexplore.ieee.org/document/9398699/,"Speech is a behavioural biometric signal that can provide important information to understand the human intends as well as their emotional status. The paper is centered on the speech-based identification of the seniors's emotional status during their interaction with a virtual agent playing the role of a health professional coach. Under real conditions, we can just identify a small set of task-dependent spontaneous emotions. The number of identified samples is largely different for each emotion, which results in an imbalanced dataset problem. This research proposes the dimensional model of emotions as a perceptual representation space alternative to the generally used acoustic one. The main contribution of the paper is the definition of a perceptual borderline for the oversampling of minority emotion classes in this space. This limit, based on arousal and valence criteria, leads to two methods of balancing the data: the Perceptual Borderline oversampling and the Perceptual Borderline SMOTE (Synthetic Minority Oversampling TEchnique). Both methods are implemented and compared to state-of-the-art approaches of Random oversampling and SMOTE. The experimental evaluation was carried out on three imbalanced datasets of spontaneous emotions acquired in human-machine scenarios in three different cultures: Spain, France and Norway. The emotion recognition results obtained by neural networks classifiers show that the proposed perceptual oversampling methods led to significant improvements when compared with the state-of-the art, for all scenarios and languages.",space,686
10.1109/tim.2021.3092518,filtered,IEEE Transactions on Instrumentation and Measurement,IEEE,2021-01-01 00:00:00,ieeexplore,pipeline safety early warning by multifeature-fusion cnn and lightgbm analysis of signals from distributed optical fiber sensors,https://ieeexplore.ieee.org/document/9541184/,"Energy pipelines are the backbones of global energy systems. Monitoring their safety and automatically identifying and locating third-party damage events are crucial to energy supply. However, most traditional methods lack in-depth consideration of distributed fiber signals and have not been tested on real-world long-distance pipelines, making it difficult to deploy them in operating long-distance pipelines. In this study, we utilize a novel real-time machine-learning method based on phase-sensitive optical time domain reflectometer technology to monitor the safety of oil and gas pipelines. Specifically, we build a multifeature-fusion convolutional neural network and LightGBM fusion model based on two novel complementary spatiotemporal features. The method was applied to a large amount of data collected from real-world oil–gas transportation pipelines of the China National Petroleum Corporation. The proposed method could accurately locate and identify third-party damage events in real-time under conditions of strong noise and various types of system hardware, and could effectively handle signal drift in the time and space dimensions. Our methodology has been deployed at real long-distance energy pipeline sites and our work will contribute to energy pipeline safety and energy supply security. Furthermore, the proposed solution could be generalized to other fields, such as industrial inspection, measurement, and monitoring.",space,687
10.1109/access.2019.2948017,filtered,IEEE Access,IEEE,2019-01-01 00:00:00,ieeexplore,real time dynamic magnetic resonance imaging via dictionary learning and combined fourier transform,https://ieeexplore.ieee.org/document/8873554/,"Real time dynamic magnetic resonance imaging (dMRI) requires that the image acquisition and reconstruction are carried out simultaneously and the reconstruction speed catches up with imaging speed. In this paper, a novel compressed sensing (CS) reconstruction algorithm for real time dynamic MRI is proposed. The first frame with more k-space measurements is reconstructed precisely as the reference image. Different from previous methods who start their reconstructions from zero-filled k-space measurements, a Combined Fourier Transform (CFT) algorithm is implemented in our method, which can dynamically aggregate the k-space measurements from previous sampled frames to create a highly accurate predictive image for the current frame. We then combine the CFT algorithm with a 3D path-based dictionary leaning algorithm, which is named as DLCFT in our work for fast real time dMRI reconstruction. The proposed algorithm is compared with four state-of-the-art online and offline methods on two real and complex perfusion MR sequences and a real functional brain MR sequence. Experimental results show that the proposed algorithm outperforms these methods with faster convergence and higher reconstruction accuracy.",space,688
10.1109/tifs.2007.902037,filtered,IEEE Transactions on Information Forensics and Security,IEEE,2007-09-01 00:00:00,ieeexplore,real-time face detection and motion analysis with application in “liveness” assessment,https://ieeexplore.ieee.org/document/4291551/,"A robust face detection technique along with mouth localization, processing every frame in real time (video rate), is presented. Moreover, it is exploited for motion analysis onsite to verify ""liveness"" as well as to achieve lip reading of digits. A methodological novelty is the suggested quantized angle features (""quangles"") being designed for illumination invariance without the need for preprocessing (e.g., histogram equalization). This is achieved by using both the gradient direction and the double angle direction (the structure tensor angle), and by ignoring the magnitude of the gradient. Boosting techniques are applied in a quantized feature space. A major benefit is reduced processing time (i.e., that the training of effective cascaded classifiers is feasible in very short time, less than 1 h for data sets of order 10<sup>4</sup>). Scale invariance is implemented through the use of an image scale pyramid. We propose ""liveness"" verification barriers as applications for which a significant amount of computation is avoided when estimating motion. Novel strategies to avert advanced spoofing attempts (e.g., replayed videos which include person utterances) are demonstrated. We present favorable results on face detection for the YALE face test set and competitive results for the CMU-MIT frontal face test set as well as on ""liveness"" verification barriers.",space,689
10.1109/tcyb.2013.2275291,filtered,IEEE Transactions on Cybernetics,IEEE,2013-10-01 00:00:00,ieeexplore,real-time multiple human perception with color-depth cameras on a mobile robot,https://ieeexplore.ieee.org/document/6583249/,"The ability to perceive humans is an essential requirement for safe and efficient human-robot interaction. In real-world applications, the need for a robot to interact in real time with multiple humans in a dynamic, 3-D environment presents a significant challenge. The recent availability of commercial color-depth cameras allow for the creation of a system that makes use of the depth dimension, thus enabling a robot to observe its environment and perceive in the 3-D space. Here we present a system for 3-D multiple human perception in real time from a moving robot equipped with a color-depth camera and a consumer-grade computer. Our approach reduces computation time to achieve real-time performance through a unique combination of new ideas and established techniques. We remove the ground and ceiling planes from the 3-D point cloud input to separate candidate point clusters. We introduce the novel information concept, depth of interest, which we use to identify candidates for detection, and that avoids the computationally expensive scanning-window methods of other approaches. We utilize a cascade of detectors to distinguish humans from objects, in which we make intelligent reuse of intermediary features in successive detectors to improve computation. Because of the high computational cost of some methods, we represent our candidate tracking algorithm with a decision directed acyclic graph, which allows us to use the most computationally intense techniques only where necessary. We detail the successful implementation of our novel approach on a mobile robot and examine its performance in scenarios with real-world challenges, including occlusion, robot motion, nonupright humans, humans leaving and reentering the field of view (i.e., the reidentification challenge), human-object and human-human interaction. We conclude with the observation that the incorporation of the depth information, together with the use of modern techniques in new ways, we are able to create an accurate system for real-time 3-D perception of humans by a mobile robot.",space,690
10.1109/tnn.2006.886854,filtered,IEEE Transactions on Neural Networks,IEEE,2007-01-01 00:00:00,ieeexplore,reducing and filtering point clouds with enhanced vector quantization,https://ieeexplore.ieee.org/document/4049818/,"Modern scanners are able to deliver huge quantities of three-dimensional (3-D) data points sampled on an object's surface, in a short time. These data have to be filtered and their cardinality reduced to come up with a mesh manageable at interactive rates. We introduce here a novel procedure to accomplish these two tasks, which is based on an optimized version of soft vector quantization (VQ). The resulting technique has been termed enhanced vector quantization (EVQ) since it introduces several improvements with respect to the classical soft VQ approaches. These are based on computationally expensive iterative optimization; local computation is introduced here, by means of an adequate partitioning of the data space called hyperbox (HB), to reduce the computational time so as to be linear in the number of data points N, saving more than 80% of time in real applications. Moreover, the algorithm can be fully parallelized, thus leading to an implementation that is sublinear in N. The voxel side and the other parameters are automatically determined from data distribution on the basis of the Zador's criterion. This makes the algorithm completely automatic. Because the only parameter to be specified is the compression rate, the procedure is suitable even for nontrained users. Results obtained in reconstructing faces of both humans and puppets as well as artifacts from point clouds publicly available on the web are reported and discussed, in comparison with other methods available in the literature. EVQ has been conceived as a general procedure, suited for VQ applications with large data sets whose data space has relatively low dimensionality",space,691
10.1109/tse.2020.2979701,filtered,IEEE Transactions on Software Engineering,IEEE,2022-01-01 00:00:00,ieeexplore,reinforcement-learning-guided source code summarization using hierarchical attention,https://ieeexplore.ieee.org/document/9031440/,"Code summarization (aka comment generation) provides a high-level natural language description of the function performed by code, which can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, the state-of-the-art approaches follow an encoder-decoder framework which encodes source code into a hidden space and later decodes it into a natural language space. Such approaches suffer from the following drawbacks: (a) they are mainly input by representing code as a sequence of tokens while ignoring code hierarchy; (b) most of the encoders only input simple features (e.g., tokens) while ignoring the features that can help capture the correlations between comments and code; (c) the decoders are typically trained to predict subsequent words by maximizing the likelihood of subsequent ground truth words, while in real world, they are excepted to generate the entire word sequence from scratch. As a result, such drawbacks lead to inferior and inconsistent comment generation accuracy. To address the above limitations, this paper presents a new code summarization approach using hierarchical attention network by incorporating multiple code features, including type-augmented abstract syntax trees and program control flows. Such features, along with plain code sequences, are injected into a deep reinforcement learning (DRL) framework (e.g., actor-critic network) for comment generation. Our approach assigns weights (pays “attention”) to tokens and statements when constructing the code representation to reflect the hierarchical code structure under different contexts regarding code features (e.g., control flows and abstract syntax trees). Our reinforcement learning mechanism further strengthens the prediction results through the actor network and the critic network, where the actor network provides the confidence of predicting subsequent words based on the current state, and the critic network computes the reward values of all the possible extensions of the current state to provide global guidance for explorations. Eventually, we employ an advantage reward to train both networks and conduct a set of experiments on a real-world dataset. The experimental results demonstrate that our approach outperforms the baselines by around 22 to 45 percent in BLEU-1 and outperforms the state-of-the-art approaches by around 5 to 60 percent in terms of S-BLEU and C-BLEU.",space,692
10.1109/jsen.2020.3035057,filtered,IEEE Sensors Journal,IEEE,2015-02-15 20:21:00,ieeexplore,remote appliance load monitoring and identification in a modern residential system with smart meter data,https://ieeexplore.ieee.org/document/9245531/,"In this article, an innovative procedure for residential electrical load monitoring applicable to smart meters is proposed based on a modified decisive multi-objective optimization. In this procedure, different load features are extracted from household loads for optimization-based monitoring. Each load monitoring feature reflects an objective function, which is also simultaneously minimized using modified Artificial Bee Colony (ABC) algorithm. This technique is not heavily dependent on high amount of training data which is essential for the most machine learning techniques with some modification in choosing the initial search space. The proposed technique uses with real life raw data using low sampling rates. An Internet of Things (IoT) based approach is also implemented for remote monitoring of connected loads in the system along with monitoring. The proposed technique is user friendly, requires only load signature data for verification purpose which can be easily extracted. The paper demonstrates event-based appliance load monitoring and comparison with benchmark dataset which shows marked improvement over existing state-of-the-art-techniques.",space,693
10.1109/jiot.2018.2812210,filtered,IEEE Internet of Things Journal,IEEE,2018-10-01 00:00:00,ieeexplore,sdcor: software defined cognitive routing for internet of vehicles,https://ieeexplore.ieee.org/document/8306876/,"The Internet of Vehicles (IoV) is a subapplication of the Internet of Things in the automotive field. Large amounts of sensor data require to be transferred in real-time. Most of the routing protocols are specifically targeted to specific situations in IoV. But communication environment of IoV usually changes in the space-time dimension. Unfortunately, the traditional vehicular networks cannot select the optimal routing policy when facing the dynamic environment, due to the lack of abilities of sensing the environment and learning the best strategy. Sensing and learning constitute two key steps of the cognition procedure. Thus, in this paper, we present a software defined cognitive network for IoV (SDCIV), in which reinforcement learning and software defined network technology are considered for IoV to achieve cognitive capability. To the best of our knowledge, this paper is the first one that can give the optimal routing policy adaptively through sensing and learning from the environment of IoV. We perform experiments on a real vehicular dataset to validate the effectiveness and feasibility of the proposed algorithm. Results show that our algorithm achieves better performance than several typical protocols in IoV. We also show the feasibility and effectiveness of our proposed SDCIV.",space,694
10.1109/tsc.2017.2777478,filtered,IEEE Transactions on Services Computing,IEEE,2021-02-01 00:00:00,ieeexplore,solar: services-oriented deep learning architectures-deep learning as a service,https://ieeexplore.ieee.org/document/8119814/,"Deep learning has been an emerging field of machine learning during past decades. However, the diversity and large scale data size have posed significant challenge to construct a flexible and high performance implementations of deep learning neural networks. In order to improve the performance as well to maintain the scalability, in this paper we present SOLAR, a services-oriented deep learning architecture using various accelerators like GPU and FPGA. SOLAR provides a uniform programming model to users so that the hardware implementation and the scheduling is invisible to the programmers. At runtime, the services can be executed either on the software processors or the hardware accelerators. To leverage the trade-offs between the metrics among performance, power, energy, and efficiency, we present a multitarget design space exploration. Experimental results on the real state-of-the-art FPGA board demonstrate that the SOLAR is able to provide a ubiquitous framework for diverse applications without increasing the burden of the programmers. Moreover, the speedup of the GPU and FPGA hardware accelerator in SOLAR can achieve significant speedup comparing to the conventional Intel i5 processors with great scalability.",space,695
10.1109/access.2022.3152191,filtered,IEEE Access,IEEE,2022-01-01 00:00:00,ieeexplore,stbc identification for multi-user uplink sc-fdma asynchronous transmissions exploiting iterative soft information feedback of error correcting codes,https://ieeexplore.ieee.org/document/9715055/,"With the advancement and widespread implementation of multiple-input multiple-output (MIMO) wireless communication systems over the last decade, space-time block coding (STBC) identification has become a critical task for intelligent radios. Previous examinations of STBC identification were focused on single-user transmissions over single-carrier and multi-carrier systems in combination with uncoded broadcasts. Practical systems, on the other hand, contain many users and employ error-correcting codes. For the first time in literature, this work explores the problem of STBC identification for multi-user uplink transmissions in single-carrier frequency division multiple access (SC-FDMA) systems. We take another step closer to real systems by addressing asynchronous transmissions and by conducting multi-user channel estimation. We also exploit the outputs of the channel decoder, which is usually used in many practical systems, to improve the identification and estimation processes. The mathematical analysis demonstrates that the maximum-likelihood (ML) solution of STBC identification, channel estimation, and synchronization can be executed by an iterative approach. The space-alternating generalized expectation-maximization (SAGE) algorithm is used to separate the overlaid signals arriving at the base-station (BS). The parameters under consideration for each user are then updated using an expectation-maximization (EM) processor. Simulation results show that the proposed architecture outperforms other identification methods published in the literature while maintaining a reasonable level of processing time.",space,696
10.1109/access.2020.3020799,filtered,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,short-term industrial load forecasting based on ensemble hidden markov model,https://ieeexplore.ieee.org/document/9183956/,"Short-term load forecasting (STLF) for industrial customers has been an essential task to reduce the cost of energy transaction and promote the stable operation of smart grid throughout the development of the modern power system. Traditional STLF methods commonly focus on establishing the non-linear relationship between loads and features, but ignore the temporal relationship between them. In this paper, an STLF method based on ensemble hidden Markov model (e-HMM) is proposed to track and learn the dynamic characteristics of industrial customer’s consumption patterns in correlated multivariate time series, thereby improving the prediction accuracy. Specifically, a novel similarity measurement strategy of log-likelihood space is designed to calculate the log-likelihood value of the multivariate time series in sliding time windows, which can effectively help the hidden Markov model (HMM) to capture the dynamic temporal characteristics from multiple historical sequences in similar patterns, so that the prediction accuracy is greatly improved. In order to improve the generalization ability and stability of a single HMM, we further adopt the framework of Bagging ensemble learning algorithm to reduce the prediction errors of a single model. The experimental study is implemented on a real dataset from a company in Hunan Province, China. We test the model in different forecasting periods. The results of multiple experiments and comparison with several state-of-the-art models show that the proposed approach has higher prediction accuracy.",space,697
10.1109/tim.2021.3119138,filtered,IEEE Transactions on Instrumentation and Measurement,IEEE,2021-01-01 00:00:00,ieeexplore,statistical <italic>n</italic>-best afd-based sparse representation for ecg biometric identification,https://ieeexplore.ieee.org/document/9565931/,"Electrocardiogram (ECG) biometric recognition as a personal identification method is receiving more and more attention because it can support live verification results. Compared with other biometric-based methods, it can provide higher security performance. The difficulty of the problem lies in how to stably extract ECG signal features and achieve real-time verification. In this study, a new type of sparse representation learning framework called statistical <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula>-best adaptive Fourier decomposition (SAFD) originated by Qian is adopted in ECG biometric identification. Adaptive Fourier decomposition (AFD) is a recently developed combination of transform-based signal decomposition and sparse representation method, which can adaptively select the atoms from a redundant dictionary through orthogonal processing. The advantage of the AFD-type methods is that each atom in the dictionary has a precise mathematical formula with good analytic properties. This characteristic is significantly distinguished it from other existing sparse representations, where the atoms learned are usually matrix data and cannot be described mathematically. The proposed SAFD extends the existing <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula>-best AFD from processing single signal to multi-signals and implements the <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula>-best AFD in the stochastic Hardy space. Therefore, the small number of learned atoms by SAFD is sufficient to capture internal structure and robustness of the signal and generate a discriminative representation that reflects the time–frequency characteristics of signals. It is very suitable for non-stationary signals like ECG. The proof of convergence of the algorithm is presented. Extensive experiments are conducted on five public databases collected in different realistic conditions, and an average identification accuracy of 98.0% is achieved. In addition, less than 1 ms for one matching process makes it possible to be implemented in real time. Experimental results demonstrate that the proposed method can achieve superior performance compared to other state-of-the-art ECG biometric identification methods.",space,698
10.1109/access.2020.3008165,filtered,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,using eeg and deep learning to predict motion sickness under wearing a virtual reality device,https://ieeexplore.ieee.org/document/9137130/,"Virtual Reality (VR) research has been widely applied in many fields. VR promises to deliver the experience that is beyond the user's imagination. One of the advantages of VR is the feeling it gives of being there. VR can provide experiences impossible in the real world, such as flying, diving in deep water, exploring outer space, or living with dinosaurs. Despite the improvements in the software and hardware, the problem of motion sickness remains. We implement a deep learning model to train and predict motion sickness. A questionnaire is a well-known method to measure motion sickness. The weakness of the questionnaire is the measurement carried out after the user experiences motion sickness symptoms. By using the deep learning and EEG, the system will learn and classify motion sickness. The system learns the user's EEG pattern when they begin to feel the sickness symptoms. The system will be trained using deep learning to identify the sickness patterns in the future. By the EEG patterns, the system can predict the sickness symptoms before it occurs. Our model outperforms traditional models in loss values, accuracy, and F-measure metrics in Roller Coaster. With other datasets, our model also performs well. Our model can achieve 82.83% accuracy from the dataset. We also found that the time steps to predict motion sickness during 5 minute periods is a suitable configuration.",space,699
10.1109/tgrs.2020.3017937,filtered,IEEE Transactions on Geoscience and Remote Sensing,IEEE,2021-05-01 00:00:00,ieeexplore,x-svm: an extension of c-svm algorithm for classification of high-resolution satellite imagery,https://ieeexplore.ieee.org/document/9180087/,"The accurate land cover mapping of the Earth's surface using Earth observation data is one of the most studied, but yet the most challenging tasks of remote sensing field, particularly when it comes to urban areas. The large spectral variability of man-made structures, as well as the mixed pixel phenomenon, imposes the use of computational demanding techniques, which are not always effective for real case applications. Support vector machines (SVMs) are supervised learning models with associated learning algorithms, which are mainly used for classification and regression analysis. Specifically, a support vector classifier (SVC) constructs a hyperplane or a set of hyperplanes in a high-dimensional space, which separates the training data into different classes. These are then used to classify a whole image, or series of images. The current standard SVM algorithm for classification used by the most popular mapping software (e.g., ENVI, EnMAP) is the C-SVC. The parameterization of a C-SVC strongly affects the final classification result. Yet, there is no rule of thumb to choose the optimal parameters when classifying satellite imagery. Optimal parameterization totally depends on the training data, and to determine it for a specific case, a time-consuming trial-and-error process is inevitable. In this work, advancements for the C-SVC algorithm are proposed to enhance its performance when used to classify remote sensing data, eliminating the need for a part of manual parametrization, while ensuring increasing its performance.",space,700
10.1109/itng.2008.213,filtered,Fifth International Conference on Information Technology: New Generations (itng 2008),IEEE,2008-04-09 00:00:00,ieeexplore,developing an aerospace system software using pbl and mda,https://ieeexplore.ieee.org/document/4492700/,"This paper reports a problem-based learning - PBL approach for the development of real time embedded software for the aerospace sector, employing a commercial suite of tools that supports model-driven architecture - MDA projects. Using this approach, graduate and last-year undergraduate students in computer engineering of the Brazilian Aeronautical Institute of Technology (LTA) have developed an aerospace system prototype during an academic semester.",space,701
10.1109/iecon43393.2020.9254506,filtered,IECON 2020 The 46th Annual Conference of the IEEE Industrial Electronics Society,IEEE,2020-10-21 00:00:00,ieeexplore,adaptive online gated recurrent unit for lithium-ion battery soc estimation,https://ieeexplore.ieee.org/document/9254506/,"The Li-ion batteries are commonly used for Electric Vehicles (EVs) and aerospace applications. One of the essential parameters in Li-ion batteries is state of charge (SOC) that shows the available energy in a battery. Various methods were proposed for SOC estimation. Since the battery has a nonlinear equations, it is important to use a method that does not require the system model. In the present study, a new Adaptive Online Gated Recurrent Unit (GRU) method is proposed for the State of Charge (SOC) estimation. It is a kind of deep Recurrent Neural Network(RNN) which solved the vanishing gradient problem in RNNs with GRU units. For Optimization a robust adaptive Online gradient learning method is used. This method is able to tune online the learning rate in the process. Adaptive GRU is a nondependent method from the nonlinear batteries model and simplifies the mathematical computation. The proposed technique is implemented on the real dataset of LifePO4 Li-ion batteries for finding SOC estimation. The exprimental result indicate that the Adaptive GRU method is more accurate than simple RNN.",space,702
10.1109/naecon.1989.40244,filtered,Proceedings of the IEEE National Aerospace and Electronics Conference,IEEE,1989-05-26 00:00:00,ieeexplore,an american knowledge base in england: alternate implementations of an expert system flight status monitor,https://ieeexplore.ieee.org/document/40244/,"A joint activity between the Dryden Flight Research Facility of the NASA Ames Research Center (Ames-Dryden) and the Royal Aerospace Establishment (RAE) on knowledge-based systems has been agreed. Under the agreement, a flight status monitor knowledge base developed at Ames-Dryden has been implemented using the real time AI (artificial intelligence) toolkit MUSE, which was developed in the UK. The background to the cooperation is described and the details of the flight status monitor and a prototype MUSE implementation are presented. It is noted that the capabilities of the expert-system flight status monitor data downlinked from the flight test aircraft and to generate information on the state and health of the system for the test engineers provides increased safety during flight testing of new systems. The expert-system flight status monitor provides the systems engineers with ready access to the large amount of information required to describe a complex aircraft system.&lt;<ETX>&gt;</ETX>",space,703
10.1109/iecec.1997.659200,filtered,IECEC-97 Proceedings of the Thirty-Second Intersociety Energy Conversion Engineering Conference (Cat. No.97CH6203),IEEE,1997-08-01 00:00:00,ieeexplore,an object oriented neuro-controller design for motor drive technologies,https://ieeexplore.ieee.org/document/659200/,"This paper deals with the object-oriented model development of a neuro-controller design for permanent magnet (PM) DC motor drives. The system under study is described as a collection of interacting objects. Each object module describes the object behaviors, called methods. The characteristics of the object are included in its variables. The knowledge of the object exists within its variables, and the performance is determined by its methods. This structure maps well to the real world objects that comprise the system being modeled. A dynamic learning architecture that possesses the capabilities of simultaneous on-line identification and control is incorporated to enforce constraints on connections and control the dynamics of the motor. The control action is implemented ""on-line"", in ""real time"" in such a way that the predicted trajectory follows a specified reference model. A design example of controlling a PM DC motor on-line shows the effectiveness of the design tool. This will therefore be very useful in aerospace applications. It is expected to provide an innovative and novel software model for the rocket engine numerical simulator executive.",space,704
10.1109/robot.1986.1087518,filtered,Proceedings. 1986 IEEE International Conference on Robotics and Automation,IEEE,1986-04-10 00:00:00,ieeexplore,architecture and early experience with planning for the alv,https://ieeexplore.ieee.org/document/1087518/,"This paper describes the software architecture and the initial algorithms that have proved to be effective for a real time robot planning system. The architecture is designed to incorporate planning technology from research on artificial intelligence while at the same time supporting the high performance decision making needed to control a fast-moving autonomous vehicle. The symbolic representation of the vehicle's plan is a key element in this architecture. Our initial algorithms use an especially efficient version of dynamic programming to find the best routes. The route is then translated into a symbolic plan. Replanning happens at several levels with the cost of replanning proportionate to the scope of the changes. This software is currently running in an environment which simulates the vehicle and perception systems, but it will be transferred to the DARPA Autonomous Land Vehicle built by Martin Marietta Denver Aerospace [Lowrie 86].",space,705
10.1109/rteict.2016.7808123,filtered,"2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)",IEEE,2016-05-21 00:00:00,ieeexplore,data driven prognosis approach for safety critical systems,https://ieeexplore.ieee.org/document/7808123/,"Safety critical systems are being developed to improve the performance and cost effectiveness. The safety critical system are used in various domain such as aerospace domain, military, defense etc. In an aerospace domain there are many parameters affects the system environmental conditions, or hazards which cause many faults in the system which leads to failure. It is necessary to know before the system fails, so that necessary remedies can take to prevent the failure. The tool/software is needed to monitor the health management of safety critical systems. In this paper a prognostic technique is being used to mitigate the system failure. There are many techniques for the prognosis such as data driven technique, model based technique, and hybrid technique. This paper proposes implementation of the artificial neural network [ANN] based prognosis illustrates the use of data driven technique. The novelty of the proposed algorithm is that it uses formal techniques to develop a robust &amp; reliable prognostics algorithm. The approach developed will be demonstrated for gyro sensor a critical component in the aerospace domain. The ANN can train and classify real data from the gyro sensors, and it is implemented using high level interpreted language GNU-Octave. The cost function/error function is calculated for the trained ANN data and it is being observed that the values are converging to the minimum value. At last the system is classified as healthy, partially healthy, and unhealthy state of the system.",space,706
10.1109/aero.2018.8396547,filtered,2018 IEEE Aerospace Conference,IEEE,2018-03-10 00:00:00,ieeexplore,data-driven quality prognostics for automated riveting processes,https://ieeexplore.ieee.org/document/8396547/,"Technologies based in robotics and automatics are reshaping the aerospace industry. Aircraft manufacturers and top-tier suppliers now rely on robotics to perform most of its operational tasks. Over the years, a succession of implemented mobile robots has been developed with the mission of automating important industrial processes such as welding, material handling or assembly procedures. However, despite the progress achieved, a major limitation is that the process still requires human supervision and an extensive quality control process. An approach to address this limitation is to integrate machine learning methods within the quality control process. The idea is to develop algorithms that can direct manufacturing experts towards critical areas requiring human supervision and quality control. In this paper we present an application of machine learning to a concrete industrial problem involving the quality control of a riveting machine. The proposal consists of an intelligent predictive model that can be integrated within the existing real time sensing and pre-processing sub-systems at the equipment level. The framework makes use of several data-driven techniques for pre-processing and feature engineering, combined with the most accurate algorithms, validated through k-folds cross validation technique which also estimates prediction errors. The model is able to classify the manufacturing process of the machine as nominal or anomalous according to a real-world data set of design requirements and operational data. Several machine learning algorithms are compared such as linear regression, nearest neighbor, support vector machines, decision trees, random forests and extreme gradient boost. Results obtained from the case study suggest that the proposed model produces accurate predictions which meet industrial standards.",space,707
10.1109/tai.2000.889866,filtered,Proceedings 12th IEEE Internationals Conference on Tools with Artificial Intelligence. ICTAI 2000,IEEE,2000-11-15 00:00:00,ieeexplore,debugging knowledge-based applications with a generic toolkit,https://ieeexplore.ieee.org/document/889866/,"Knowledge refinement tools assist in the debugging and maintenance of knowledge based systems (KBSs) by attempting to identify and correct faults in the knowledge that account for incorrect problem-solving. Most refinement systems target a single shell and are able to refine only KBSs implemented in this shell. Our KRUSTWorks toolkit is unusual in that it provides refinement facilities that can be applied to a number of different shells, and is designed to be extensible to new shells. The paper outlines the components of the KRUSTWorks toolkit and how it is applied to faulty KBSs. It describes its application to two real aerospace KBSs implemented in CLIPS and POWER-MODEL to demonstrate its flexibility of application.",space,708
10.1109/aiam54119.2021.00049,filtered,2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture (AIAM),IEEE,2021-10-25 00:00:00,ieeexplore,design of online measurement system for molten pool morphology in selective laser melting (slm) process,https://ieeexplore.ieee.org/document/9724859/,"Selective Laser Melting (SLM), as the processing technology with the highest forming accuracy and the best processing quality in additive manufacturing, is widely used in aerospace fields, etc. Laser molten pool is the basic unit of SLM processing, and online monitoring of the laser molten pool morphology can reflect the SLM processing accuracy and process stability in real time. In this paper, an online measurement system for side-axis laser molten pool morphology is developed inside the SLM molding cabin. Temperature isolation and sealing protection is designed for the online measurement system under complex high-temperature environments. It uses high-speed COMS camera and image acquisition card equipped with FPGA chips to collect and transmit the molten pool image information in real time. The software design of the online measurement system includes the development of the camera SDK based on Visual Studio, the design of real-time image processing algorithms, the evaluation of the molten pool, etc., which aims to evaluate the image acquisition, transmission and storage of the molten pool. Through simulation analysis and experimental verification, it is proved that the system can allow on-line measurement of molten pool morphology.",space,709
10.1109/icves.2009.5400189,filtered,2009 IEEE International Conference on Vehicular Electronics and Safety (ICVES),IEEE,2009-11-12 00:00:00,ieeexplore,digital implementation of fuzzy logic controller for wide range speed control of brushless dc motor,https://ieeexplore.ieee.org/document/5400189/,"The brushless DC motors find wide applications such as in battery operated vehicles, wheel chairs, automotive fuel pumps, robotics, machine tools, aerospace and in many industrial applications due to their superior electrical and mechanical characteristics and its capability to operate in hazardous environment. Conventional controllers fail to yield desired performance in BLDC motor control systems due to the non-linearity arising out of variation in the system parameters and change in load. The main focus is now on the application of artificial intelligent techniques such as fuzzy logic to solve this problem. Another great challenge is to reduce the size and cost of the drive system without compromising the performance. In this paper, the design and digital implementation of fuzzy logic controller using a versatile ADUC812 microcontroller, and low-cost, compact, superior performance components are used in order to reduce the cost and size of the drive system. The experimental results are presented to prove the flexibility of the control scheme in real time.",space,710
10.1109/iciafs.2007.4544783,filtered,2007 Third International Conference on Information and Automation for Sustainability,IEEE,2007-12-06 00:00:00,ieeexplore,dynamic power management of an embedded sensor network based on actor-critic reinforcement based learning,https://ieeexplore.ieee.org/document/4544783/,"Wireless sensor networks (WSNs) have gained tremendous popularity in recent years due to the wide range of applications envisioned - ranging from aerospace and defense to industrial and commercial. Although limited by communication and energy constraints, the low cost, small sensor nodes lend themselves to be deployed in large numbers to form a network with high spatial distribution. The overall effectiveness of the sensor network depends on how well the mutually contradicting objectives of conserving the limited on-board battery power and keeping the sensors awake for stimuli, are managed. In this paper, we have proposed an actor-critic based reinforcement learning mechanism that can be practically implemented on an embedded sensor with limited memory and processing power. Specifically, the contribution of this paper is the development of the value function (or critic/reinforcement function) that is implemented on each sensor node which aids in dynamic power scheduling based on different situations. The effectiveness of the proposed method has been demonstrated with real world experiments.",space,711
10.1109/aero.2002.1036133,filtered,"Proceedings, IEEE Aerospace Conference",IEEE,2002-03-16 00:00:00,ieeexplore,integrating model-based diagnostics with simulation for real time health monitoring,https://ieeexplore.ieee.org/document/1036133/,"In systems that contain continuous-valued variables, such as aerospace systems, faults can be sensitively detected and diagnosed by combining a real time simulation with a conventional model-based diagnostic engine. We present a unified approach in which the user builds a single model that contains a sufficiently detailed description of the system to support both simulation and automatic diagnosis. This approach is implemented in a modeling environment called CNModeler/spl trade/.",space,712
10.1109/iecon.2012.6389520,filtered,IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society,IEEE,2012-10-28 00:00:00,ieeexplore,matlab/simulink software implementation and interfacing of a strap-down inertial attitude method,https://ieeexplore.ieee.org/document/6389520/,"The paper presents a software tool used to calculate the angular attitude of a vehicle starting from the readings of a strap-down gyro triad. Firstly, the problem statement is presented and the implied mathematical equations are shown. The theoretical model is based on the discretized form of a matrix differential equation solution, and focuses on its first six truncation order. In order to have an easier communication between the user and the software, especially in the off-line data processing applications, a Graphical User Interface is developed. The obtained software is validated by using an integrated INS/GPS navigation system as reference system, the inputs of our software being the outputs of the gyro sensors in the strap-down INS. The tool can be used in all strap-down inertial systems application fields like aerospace, naval and automotive navigation, robotics, as well as in medicine surgery. Also, the developed software can be used in the real time applications or in the off-line processing of the navigation inertial sensors recorded data.",space,713
10.1109/ijcnn.2013.6706957,filtered,The 2013 International Joint Conference on Neural Networks (IJCNN),IEEE,2013-08-09 00:00:00,ieeexplore,optimized neuro genetic fast estimator (ongfe) for efficient distributed intelligence instantiation within embedded systems,https://ieeexplore.ieee.org/document/6706957/,"The Optimized Neuro Genetic Fast Estimator (ONGFE) is a software tool that allows for embedding system, subsystem, and component failure detection, identification, and prognostics (FDI&amp;P) capability by using Intelligent Software Elements (ISE) based upon Artificial Neural Networks (ANN). With an Application Programming Interface (API), highly innovative algorithms are compiled for efficient distributed intelligence instantiation within embedded systems. The original design had the purpose of providing a real time kernel to deploy health monitoring functions for Condition Based Maintenance (CBM) and Real Time Monitoring (RTM) systems in a broad variety of applications (such as aerospace, structural, and widely distributed support systems). The ONGFE contains embedded fast and on-line training for designing ANNs to perform several high performance FDI&amp;P functions. A key advantage of this technology is an optimization block based upon pseudogenetic algorithms which compensate for effects due to initial weight values and local minimums without the computational burden of genetic algorithms. The ONGFE also provides a synchronization block for communication with secondary diagnostic modules. The algorithms are designed for a distributed, scalar, and modular deployment. Based on this technology, a scheme for conducting sensor data validation has been embedded in Smart Sensors.",space,714
10.1109/dasc52595.2021.9594351,filtered,2021 IEEE/AIAA 40th Digital Avionics Systems Conference (DASC),IEEE,2021-10-07 00:00:00,ieeexplore,seda: a self-explaining decision architecture implemented using deep learning for on-board command and control,https://ieeexplore.ieee.org/document/9594351/,"Machine learning (ML) is a powerful tool for solving stochastic optimization problems. The aerospace and defense sectors have a number of stochastic optimization problems that would benefit from the application of ML; however, people often have difficulties interpreting solutions arrived at via ML, which undermines trust, producing an obstacle to widespread adoption in these sectors. This paper introduces the Self-Explaining Decision Architecture (SEDA) for ML-based decision-making systems capable of generating intuitive explanations for their decisions in real time. SEDA makes use of a feature extraction subsystem and a sequence interpretation subsystem to identify patterns in data followed by a decision generation subsystem that determines appropriate actions based on those patterns. Internal state information from each of these subsystems is used to generate explanations of the system’s decisions. Using this information to create explanations provides insight as to the data elements the system focused on when making decisions as well as the reasoning that was used. As a proof-of-concept, we present a first implementation of SEDA using start-of-the-art deep learning components including a combined convolutional neural network and long short-term memory network with attention mechanisms and demonstrate its use on both standard and custom datasets.",space,715
10.1109/imtc.1994.352108,filtered,Conference Proceedings. 10th Anniversary. IMTC/94. Advanced Technologies in I & M. 1994 IEEE Instrumentation and Measurement Technolgy Conference (Cat. No.94CH3424-9),IEEE,1994-05-12 00:00:00,ieeexplore,use of artificial neural networks for optimal sensing in complex structures analysis,https://ieeexplore.ieee.org/document/352108/,"Among the advantages offered by the Artificial Neural Networks (ANNs), in the analysis and active control of structures characterized by high modal densities and complexity, it should be mentioned the possibility of optimizing the number and the position of sensors and actuators. This feature can result in a sensible reduction of cost in the analysis and control of large structures and could be of great advantage when some of the points under investigation of the structure are not physically accessible. The first step of research is the validation of the ANN approach to a satisfactory analysis of the structure. Previous papers have highlighted the encouraging results obtained from an ANN implementation for the description of a stiffened aluminum panel behaviour. In that case the ANN was trained with data obtained by a validated FEM (Finite Element Method) structural model. In the simulation phase the ANN estimated the behaviour of all the grid points with data referred only to a portion of points with interesting performances in terms of accuracy and computing time. The present paper describes a measurement system in which the ANN is trained with real data, experimentally obtained by accelerometers. The investigation was carried out considering a sinusoidal excitation, typically produced by the rotating engines of turboprob aircrafts. Real data allowed us to rest the ability of the ANN to learn the structural dynamics taking into the right account the influence of noise, not considered in the previous phase of the research. The experimental results highlight again the good ANN behaviour accuracy in the estimation of all the rest points considered for the analysis. The present research is the necessary premise for ANNs based active control applications in aeronautics and aerospace.&lt;<ETX>&gt;</ETX>",space,716
10.1109/iecon.2012.6389251,filtered,IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society,IEEE,2012-10-28 00:00:00,ieeexplore,zno crystalline nanowires array for application in gas ionization sensor,https://ieeexplore.ieee.org/document/6389251/,"The air monitoring becomes daily necessity not only in industrial environment and in aerospace applications but also in a living milieu as a consequence of the gas pollution. For detection of gaseous pollutants gas sensors are employed. In this work the successful p-type and n-type ZnO nanowires (NWs) were accomplished during electrochemical deposition. P-type ZnO NWs doped with Ag dopant were achieved with omitted post annealing procedure. Also, the novel gas ionization sensor (GIS) with integrated p-type ZnO NWs as the anode is proposed. P-type ZnO NWs-based gas sensor's characteristics compared with the gold NWs-based GIS, which was developed and reported by our group previously. It showed the comparable breakdown voltages in inert gas (Ar) atmosphere. P-type ZnO NWs-based GIS demonstrated good repeatability. The practical and low cost p-type ZnO NWs-based gas sensor presented in this article shows potential for future implementation in real world gas sensors' applications.",space,717
10.1109/taes.2019.2961824,filtered,IEEE Transactions on Aerospace and Electronic Systems,IEEE,2020-08-01 00:00:00,ieeexplore,intrusion detection system for the mil-std-1553 communication bus,https://ieeexplore.ieee.org/document/8946705/,"MIL-STD-1553 is a military standard that defines the specification of a serial communication bus that has been implemented in military and aerospace avionic platforms for over 40 years. MIL-STD-1553 was designed for a high level of fault tolerance while less attention was paid to cyber security issues. Thus, as indicated in recent studies, it is exposed to various threats. In this article, we suggest enhancing the security of MIL-STD-1553 communication buses by integrating a machine learning-based intrusion detection system (IDS); such anIDS will be capable of detecting cyber attacks in real time. The IDS consists of two modules: 1) a remote terminal (RT) authentication module that detects illegitimately connected components and data transfers and 2) a sequence-based anomaly detection module that detects anomalies in the operation of the system. The IDS showed high detection rates for both normal and abnormal behavior when evaluated in a testbed using real 1553 hardware, as well as a very fast and accurate training process using logs from a real system. The RT authentication module managed to authenticate RTs with +0.99 precision and +0.98 recall; and detect illegitimate component (or a legitimate component that impersonates other components) with +0.98 precision and +0.99 recall. The sequence-based anomaly detection module managed to perfectly detect both normal and abnormal behavior. Moreover, the sequencebased anomaly detection module managed to accurately (i.e., zero false positives) model the normal behavior of a real system in a short period of time (~22 s).",space,718
10.1109/aero.2016.7500497,filtered,2016 IEEE Aerospace Conference,IEEE,2016-03-12 00:00:00,ieeexplore,statistical learning approach for spacecraft systems health monitoring,https://ieeexplore.ieee.org/document/7500497/,"The operations support technology continues to seek more efficient and effective ways to monitor for spacecraft operations and future low earth orbit exploration missions and beyond. This search for improvement has led to a significant movement to advance mission operations monitoring tools. Anomaly detection is an important field for the anticipation of spacecraft operations, working as an enabler of diagnostic and prognostic functions. This article discusses a new real application of a well-known data driven statistical software known as soft independent modeling for class analogy (SIMCA-P) developed by Umetrics to historical telemetry for attitude determination and control system (ADCS) of actual remote sensing spacecraft. Our design work is to detect anomalies from mission control center (MCC) through analyzing the telemetry readings received by MCC and respond to ADCS off-nominal situations by sending corrective actions tele-commands within real time to avoid critical and risky situations. We have implemented our approach on the engineering model of Egyptian satellite project Egypt-Sat1 and this model was our vehicle to simulate and verify the results. In conclusion, the analysis results provide a deep insight information and physical interpretation about the ADCS performance behavior.",space,719
10.1109/cscwd.2006.253127,filtered,2006 10th International Conference on Computer Supported Cooperative Work in Design,IEEE,2006-05-05 00:00:00,ieeexplore,an obsm method for real time embedded system,https://ieeexplore.ieee.org/document/4019163/,"The traditional OBSM method in spacecraft and other real time embedded system is to re-compile the whole system after source code modification, and then reboot the target machine with the new version software. This hurts the availability of the system. Most of the new methods are based software or hardware redundancy. To reduce the cost and make the OBSM more conveniently, we classify OBSM, and patch the target system with different method according to different OBSM type, so the new solution can reduce the cost",space,720
10.1109/escience.2014.7,filtered,2014 IEEE 10th International Conference on e-Science,IEEE,2014-10-24 00:00:00,ieeexplore,automated real-time classification and decision making in massive data streams from synoptic sky surveys,https://ieeexplore.ieee.org/document/6972266/,"The nature of scientific and technological data collection is evolving rapidly: data volumes and rates grow exponentially, with increasing complexity and information content, and there has been a transition from static data sets to data streams that must be analyzed in real time. Interesting or anomalous phenomena must be quickly characterized and followed up with additional measurements via optimal deployment of limited assets. Modern astronomy presents a variety of such phenomena in the form of transient events in digital synoptic sky surveys, including cosmic explosions (supernovae, gamma ray bursts), relativistic phenomena (black hole formation, jets), potentially hazardous asteroids, etc. We have been developing a set of machine learning tools to detect, classify and plan a response to transient events for astronomy applications, using the Catalina Real-time Transient Survey (CRTS) as a scientific and methodological testbed. The ability to respond rapidly to the potentially most interesting events is a key bottleneck that limits the scientific returns from the current and anticipated synoptic sky surveys. Similar challenge arise in other contexts, from environmental monitoring using sensor networks to autonomous spacecraft systems. Given the exponential growth of data rates, and the time-critical response, we need a fully automated and robust approach. We describe the results obtained to date, and the possible future developments.",space,721
10.1109/rast.2005.1512560,filtered,"Proceedings of 2nd International Conference on Recent Advances in Space Technologies, 2005. RAST 2005.",IEEE,2005-06-11 00:00:00,ieeexplore,autonomous onboard computer systems using real time trace models,https://ieeexplore.ieee.org/document/1512560/,"In the paper some particularities of the autonomous spacecraft computer systems are discussed. The ways to reach the autonomy are described - high reliability and adaptability of the system, reached by application of AI elements formal structural models (specification, verification and real time control) and dynamic reconfiguration. A method using trace models In real time for testing of system operations is described. The method is suitable for systems specified by process algebras (CSP - Communicating Sequential Processes, Timed CSP, ASM - Abstract State Machine, and TAM - Temporal Agent Model) where process tracts are used. The computing processes and these in the hardware are observed. Special control processes (tracers) are specified. They allow to control in real time the correspondence of the real system traces and preliminary computed traces (the formal specification) and send messages to the other subsystems when no correspondence (e.g. to specification and dynamic reconfiguration subsystems). The method allows finding in real, time the software and hardware incorrectly functioning (disparity of the traces) based on rigorous mathematical specification. In the paper are described: an autonomous control system structural model, an algorithm for real time traces control, software simulations (laboratory models) of autonomous systems, the faults and reaction of the tracer-processes, all based on CPPCSP - C++ Communicating Sequential Processes library. The application of trace models in real time allows find any system incorrectly operating and this is one of the ways for,increasing the onboard computer systeme reliability and to achieve full operation autonomy.",space,722
10.1109/icsmc.1993.385064,filtered,Proceedings of IEEE Systems Man and Cybernetics Conference - SMC,IEEE,1993-10-20 00:00:00,ieeexplore,use of case-based reasoning techniques for intelligent computer-aided-design systems,https://ieeexplore.ieee.org/document/385064/,"Reuse of designs is an important research direction for the future intelligent CAD systems. The main applications of such a research are various, from mechanical systems design (spacecraft, robot, ...) to software design. This paper will present a survey of the use of case-based reasoning (CBR) techniques for intelligent CAD systems in order to reuse designs or parts of designs. First, we will briefly resume some work issued from cognitive psychology, showing the importance of analogical-reasoning for design activities and then the origins of the CBR technology in AI. Second, we will then present the main systems using case-based reasoning for design activities followed by a comparative analysis between these systems. To conclude, we will indicate the main directions in CBR for design and will propose to adopt a cognitive approach from knowledge acquisition until the development of real design support systems.&lt;<ETX>&gt;</ETX>",space,723
10.1109/jproc.2018.2849003,filtered,Proceedings of the IEEE,IEEE,2018-09-01 00:00:00,ieeexplore,smc: satisfiability modulo convex programming,https://ieeexplore.ieee.org/document/8428633/,"The design of cyber-physical systems (CPSs) requires methods and tools that can efficiently reason about the interaction between discrete models, e.g., representing the behaviors of “cyber” components, and continuous models of physical processes. Boolean methods such as satisfiability (SAT) solving are successful in tackling large combinatorial search problems for the design and verification of hardware and software components. On the other hand, problems in control, communications, signal processing, and machine learning often rely on convex programming as a powerful solution engine. However, despite their strengths, neither approach would work in isolation for CPSs. In this paper, we present a new satisfiability modulo convex programming (SMC) framework that integrates SAT solving and convex optimization to efficiently reason about Boolean and convex constraints at the same time. We exploit the properties of a class of logic formulas over Boolean and nonlinear real predicates, termed monotone satisfiability modulo convex formulas, whose satisfiability can be checked via a finite number of convex programs. Following the lazy satisfiability modulo theory (SMT) paradigm, we develop a new decision procedure for monotone SMC formulas, which coordinates SAT solving and convex programming to provide a satisfying assignment or determine that the formula is unsatisfiable. A key step in our coordination scheme is the efficient generation of succinct infeasibility proofs for inconsistent constraints that can support conflict-driven learning and accelerate the search. We demonstrate our approach on different CPS design problems, including spacecraft docking mission control, robotic motion planning, and secure state estimation. We show that SMC can handle more complex problem instances than state-of-the-art alternative techniques based on SMT solving and mixed integer convex programming.",space,724
10.1007/978-3-030-22559-9_17,filtered,Evaluation of Novel Approaches to Software Engineering,Springer,2019-01-01 00:00:00,springer,effective decision making in self-adaptive systems using cost-benefit analysis at runtime and online learning of adaptation spaces,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-22559-9_17,"Self-adaptation is an established approach to deal with uncertainties that are difficult to predict before a system is deployed. A self-adaptative system employs a feedback loop that tracks changes and adapts the system accordingly to ensure its quality goals. However, making effective adaptation decisions at runtime is challenging. In this chapter we tackle two problems of effective decision making in self-adaptive systems. First, current research typically focusses on the benefits adaptaton can bring but ignores the cost of adaptation, which may invalidate the expected benefits. To tackle this problem, we introduce CB@R (Cost-Benefit analysis @ Runtime), a novel model-based approach for runtime decision-making in self-adaptive systems that handles both the benefits and costs of adaptation as first-class citizens in decision making. Second, we look into the adaptation space of self-adaptive systems, i.e. the set of adaption options to select from. For systems with a large number of adaptation options, analyzing the entire adaptation space is often not feasible given the time and resources constraints at hand. To tackle this problem, we present a machine learning approach that integrates learning with the feedback loop to select a subset of the adaption options that are valid in the current situation. We evaluate CB@R and the learning approach for a real world deployed Internet of Things (IoT) application.",space,725
10.1007/s41060-017-0071-0,filtered,International Journal of Data Science and Analytics,Springer,2018-03-01 00:00:00,springer,"spin: cleaning, monitoring, and querying image streams generated by ground-based telescopes for space situational awareness",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s41060-017-0071-0,"With the increasing number of objects in earth orbits, space situational awareness (SSA) becomes critical to space safety. As an economical option, ground-based telescopes can be deployed around the world and continuously provide imaginary information of space objects. However, they also raise unique challenges regarding big, noisy, and streaming data processing. In this paper, we present the SPIN system to address these challenges. The core algorithms process image sequences generated by ground-based telescopes and conduct: (1) image quality classification for data cleaning, (2) stream-based key-object identification and anomaly detection, and (3) efficient query processing on large image sequence repositories. Our goal is to design or adopt algorithms that handle the domain-specific image streams most efficiently and effectively. We use a 17-inch telescope to collect a large real dataset for evaluating the core algorithms, which covers more than ten satellites in one month and contains about 16,400 images. The experimental results show that the developed algorithms are fast enough for stream-based real-time processing and also yield high-quality results for all the primary tasks.",space,726
10.1186/s13638-018-1034-4,filtered,EURASIP Journal on Wireless Communications and Networking,Springer,2018-02-12 00:00:00,springer,a general neuro-space mapping technique for microwave device modeling,https://www.biomedcentral.com/openurl?doi=10.1186/s13638-018-1034-4,"Accurate modeling of nonlinear microwave devices is critical for reliable design of microwave circuit and system. In this paper, a more general neuro-space mapping (Neuro-SM) method is proposed to fulfill the needs of the increased modeling complexity. The proposed technique retains the capability of the existing dynamic Neuro-SM in modifying the dynamic voltage relationship between the coarse model and the desired model. The proposed Neuro-SM also considers dynamic current mapping besides voltage mappings. In this way, the proposed Neuro-SM generalizes the previously published Neuro-SM methods and has the potential to produce a more accurate model of microwave devices with more dynamics and nonlinearity. A new formulation and new sensitivity analysis technique are derived to train the general Neuro-SM with dc, small-, and large-signal data. A new gradient-based training algorithm is also proposed to speed up the training. The validity and efficiency of the general Neuro-SM method are demonstrated through a real 2 × 50 μm GaAs pseudomorphic high-electron mobility transistor (pHEMT) modeling example. The proposed general Neuro-SM model can be implemented into circuit simulators conveniently.",space,727
http://arxiv.org/abs/2202.10574v1,filtered,arxiv,arxiv,2022-02-21 00:00:00,arxiv,"a multi-agent reinforcement learning framework for off-policy evaluation
  in two-sided markets",http://arxiv.org/abs/2202.10574v1,"The two-sided markets such as ride-sharing companies often involve a group of
subjects who are making sequential decisions across time and/or location. With
the rapid development of smart phones and internet of things, they have
substantially transformed the transportation landscape of human beings. In this
paper we consider large-scale fleet management in ride-sharing companies that
involve multiple units in different areas receiving sequences of products (or
treatments) over time. Major technical challenges, such as policy evaluation,
arise in those studies because (i) spatial and temporal proximities induce
interference between locations and times; and (ii) the large number of
locations results in the curse of dimensionality. To address both challenges
simultaneously, we introduce a multi-agent reinforcement learning (MARL)
framework for carrying policy evaluation in these studies. We propose novel
estimators for mean outcomes under different products that are consistent
despite the high-dimensionality of state-action space. The proposed estimator
works favorably in simulation experiments. We further illustrate our method
using a real dataset obtained from a two-sided marketplace company to evaluate
the effects of applying different subsidizing policies. A Python implementation
of the proposed method is available at
https://github.com/RunzheStat/CausalMARL.",space,728
http://arxiv.org/abs/2202.01197v3,filtered,arxiv,arxiv,2022-02-02 00:00:00,arxiv,vos: learning what you don't know by virtual outlier synthesis,http://arxiv.org/abs/2202.01197v3,"Out-of-distribution (OOD) detection has received much attention lately due to
its importance in the safe deployment of neural networks. One of the key
challenges is that models lack supervision signals from unknown data, and as a
result, can produce overconfident predictions on OOD data. Previous approaches
rely on real outlier datasets for model regularization, which can be costly and
sometimes infeasible to obtain in practice. In this paper, we present VOS, a
novel framework for OOD detection by adaptively synthesizing virtual outliers
that can meaningfully regularize the model's decision boundary during training.
Specifically, VOS samples virtual outliers from the low-likelihood region of
the class-conditional distribution estimated in the feature space. Alongside,
we introduce a novel unknown-aware training objective, which contrastively
shapes the uncertainty space between the ID data and synthesized outlier data.
VOS achieves state-of-the-art performance on both object detection and image
classification models, reducing the FPR95 by up to 7.87% compared to the
previous best method. Code is available at
https://github.com/deeplearning-wisc/vos.",space,729
http://arxiv.org/abs/2201.12705v1,filtered,arxiv,arxiv,2022-01-30 00:00:00,arxiv,"a robust framework for deep learning approaches to facial emotion
  recognition and evaluation",http://arxiv.org/abs/2201.12705v1,"Facial emotion recognition is a vast and complex problem space within the
domain of computer vision and thus requires a universally accepted baseline
method with which to evaluate proposed models. While test datasets have served
this purpose in the academic sphere real world application and testing of such
models lacks any real comparison. Therefore we propose a framework in which
models developed for FER can be compared and contrasted against one another in
a constant standardized fashion. A lightweight convolutional neural network is
trained on the AffectNet dataset a large variable dataset for facial emotion
recognition and a web application is developed and deployed with our proposed
framework as a proof of concept. The CNN is embedded into our application and
is capable of instant real time facial emotion recognition. When tested on the
AffectNet test set this model achieves high accuracy for emotion classification
of eight different emotions. Using our framework the validity of this model and
others can be properly tested by evaluating a model efficacy not only based on
its accuracy on a sample test dataset, but also on in the wild experiments.
Additionally, our application is built with the ability to save and store any
image captured or uploaded to it for emotion recognition, allowing for the
curation of more quality and diverse facial emotion recognition datasets.",space,730
http://arxiv.org/abs/2112.13389v1,filtered,arxiv,arxiv,2021-12-26 00:00:00,arxiv,"attributed graph neural networks for recommendation systems on
  large-scale and sparse graph",http://arxiv.org/abs/2112.13389v1,"Link prediction in structured-data is an important problem for many
applications, especially for recommendation systems. Existing methods focus on
how to learn the node representation based on graph-based structure.
High-dimensional sparse edge features are not fully exploited. Because
balancing precision and computation efficiency is significant for
recommendation systems in real world, multiple-level feature representation in
large-scale sparse graph still lacks effective and efficient solution. In this
paper, we propose a practical solution about graph neural networks called
Attributed Graph Convolutional Networks(AGCN) to incorporate edge attributes
when apply graph neural networks in large-scale sparse networks. We formulate
the link prediction problem as a subgraph classification problem. We firstly
propose an efficient two-level projection to decompose topological structures
to node-edge pairs and project them into the same interaction feature space.
Then we apply multi-layer GCN to combine the projected node-edge pairs to
capture the topological structures. Finally, the pooling representation of two
units is treated as the input of classifier to predict the probability. We
conduct offline experiments on two industrial datasets and one public dataset
and demonstrate that AGCN outperforms other excellent baselines. Moreover, we
also deploy AGCN method to important scenarios on Xianyu and AliExpress. In
online systems, AGCN achieves over 5% improvement on online metrics.",space,731
http://arxiv.org/abs/2112.03765v1,filtered,arxiv,arxiv,2021-12-07 00:00:00,arxiv,in-flight novelty detection with convolutional neural networks,http://arxiv.org/abs/2112.03765v1,"Gas turbine engines are complex machines that typically generate a vast
amount of data, and require careful monitoring to allow for cost-effective
preventative maintenance. In aerospace applications, returning all measured
data to ground is prohibitively expensive, often causing useful, high value,
data to be discarded. The ability to detect, prioritise, and return useful data
in real-time is therefore vital. This paper proposes that system output
measurements, described by a convolutional neural network model of normality,
are prioritised in real-time for the attention of preventative maintenance
decision makers.
  Due to the complexity of gas turbine engine time-varying behaviours, deriving
accurate physical models is difficult, and often leads to models with low
prediction accuracy and incompatibility with real-time execution. Data-driven
modelling is a desirable alternative producing high accuracy, asset specific
models without the need for derivation from first principles.
  We present a data-driven system for online detection and prioritisation of
anomalous data. Biased data assessment deriving from novel operating conditions
is avoided by uncertainty management integrated into the deep neural predictive
model. Testing is performed on real and synthetic data, showing sensitivity to
both real and synthetic faults. The system is capable of running in real-time
on low-power embedded hardware and is currently in deployment on the
Rolls-Royce Pearl 15 engine flight trials.",space,732
http://arxiv.org/abs/2112.01031v2,filtered,arxiv,arxiv,2021-12-02 00:00:00,arxiv,"improving sensitivity of the arianna detector by rejecting thermal noise
  with deep learning",http://arxiv.org/abs/2112.01031v2,"The ARIANNA experiment is an Askaryan detector designed to record radio
signals induced by neutrino interactions in the Antarctic ice. Because of the
low neutrino flux at high energies ($E > 10^{16} $), the physics output is
limited by statistics. Hence, an increase in sensitivity significantly improves
the interpretation of data and offers the ability to probe new parameter
spaces. The amplitudes of the trigger threshold are limited by the rate of
triggering on unavoidable thermal noise fluctuations. We present a real-time
thermal noise rejection algorithm that enables the trigger thresholds to be
lowered, which increases the sensitivity to neutrinos by up to a factor of two
(depending on energy) compared to the current ARIANNA capabilities. A deep
learning discriminator, based on a Convolutional Neural Network (CNN), is
implemented to identify and remove thermal events in real time. We describe a
CNN trained on MC data that runs on the current ARIANNA microcomputer and
retains 95 percent of the neutrino signal at a thermal noise rejection factor
of $10^5$, compared to a template matching procedure which reaches only $10^2$
for the same signal efficiency. Then the results are verified in a lab
measurement by feeding in generated neutrino-like signal pulses and thermal
noise directly into the ARIANNA data acquisition system. Lastly, the same CNN
is used to classify cosmic-rays events to make sure they are not rejected. The
network classified 102 out of 104 cosmic-ray events as signal.",space,733
http://arxiv.org/abs/2111.07171v1,filtered,arxiv,arxiv,2021-11-13 00:00:00,arxiv,"deep reinforcement learning with shallow controllers: an experimental
  application to pid tuning",http://arxiv.org/abs/2111.07171v1,"Deep reinforcement learning (RL) is an optimization-driven framework for
producing control strategies for general dynamical systems without explicit
reliance on process models. Good results have been reported in simulation. Here
we demonstrate the challenges in implementing a state of the art deep RL
algorithm on a real physical system. Aspects include the interplay between
software and existing hardware; experiment design and sample efficiency;
training subject to input constraints; and interpretability of the algorithm
and control law. At the core of our approach is the use of a PID controller as
the trainable RL policy. In addition to its simplicity, this approach has
several appealing features: No additional hardware needs to be added to the
control system, since a PID controller can easily be implemented through a
standard programmable logic controller; the control law can easily be
initialized in a ""safe'' region of the parameter space; and the final product
-- a well-tuned PID controller -- has a form that practitioners can reason
about and deploy with confidence.",space,734
http://arxiv.org/abs/2110.14007v1,filtered,arxiv,arxiv,2021-10-26 00:00:00,arxiv,tod: tensor-based outlier detection,http://arxiv.org/abs/2110.14007v1,"To scale outlier detection (OD) to large-scale, high-dimensional datasets, we
propose TOD, a novel system that abstracts OD algorithms into basic tensor
operations for efficient GPU acceleration. To make TOD highly efficient in both
time and space, we leverage recent advances in deep learning infrastructure in
both hardware and software. To deploy large OD applications on GPUs with
limited memory, we introduce two key techniques. First, provable quantization
accelerates OD computation and reduces the memory requirement by performing
specific OD computations in lower precision while provably guaranteeing no
accuracy loss. Second, to exploit the aggregated compute resources and memory
capacity of multiple GPUs, we introduce automatic batching, which decomposes OD
computations into small batches that can be executed on multiple GPUs in
parallel.
  TOD supports a comprehensive set of OD algorithms and utility functions.
Extensive evaluation on both real and synthetic OD datasets shows that TOD is
on average 11.9X faster than the state-of-the-art comprehensive OD system PyOD,
and takes less than an hour to detect outliers within a million samples. TOD
enables straightforward integration for additional OD algorithms and provides a
unified framework for combining classical OD algorithms with deep learning
methods. These combinations result in an infinite number of OD methods, many of
which are novel and can be easily prototyped in TOD.",space,735
http://arxiv.org/abs/2111.09410v1,filtered,arxiv,arxiv,2021-10-14 00:00:00,arxiv,"edgeml: towards network-accelerated federated learning over wireless
  edge",http://arxiv.org/abs/2111.09410v1,"Federated learning (FL) is a distributed machine learning technology for
next-generation AI systems that allows a number of workers, i.e., edge devices,
collaboratively learn a shared global model while keeping their data locally to
prevent privacy leakage. Enabling FL over wireless multi-hop networks can
democratize AI and make it accessible in a cost-effective manner. However, the
noisy bandwidth-limited multi-hop wireless connections can lead to delayed and
nomadic model updates, which significantly slows down the FL convergence speed.
To address such challenges, this paper aims to accelerate FL convergence over
wireless edge by optimizing the multi-hop federated networking performance. In
particular, the FL convergence optimization problem is formulated as a Markov
decision process (MDP). To solve such MDP, multi-agent reinforcement learning
(MA-RL) algorithms along with domain-specific action space refining schemes are
developed, which online learn the delay-minimum forwarding paths to minimize
the model exchange latency between the edge devices (i.e., workers) and the
remote server. To validate the proposed solutions, FedEdge is developed and
implemented, which is the first experimental framework in the literature for FL
over multi-hop wireless edge computing networks. FedEdge allows us to fast
prototype, deploy, and evaluate novel FL algorithms along with RL-based system
optimization methods in real wireless devices. Moreover, a physical
experimental testbed is implemented by customizing the widely adopted Linux
wireless routers and ML computing nodes.Finally, our experimentation results on
the testbed show that the proposed network-accelerated FL system can
practically and significantly improve FL convergence speed, compared to the FL
system empowered by the production-grade commercially available wireless
networking protocol, BATMAN-Adv.",space,736
http://arxiv.org/abs/2110.06196v1,filtered,arxiv,arxiv,2021-10-12 00:00:00,arxiv,grape: fast and scalable graph processing and embedding,http://arxiv.org/abs/2110.06196v1,"Graph Representation Learning methods have enabled a wide range of learning
problems to be addressed for data that can be represented in graph form.
Nevertheless, several real world problems in economy, biology, medicine and
other fields raised relevant scaling problems with existing methods and their
software implementation, due to the size of real world graphs characterized by
millions of nodes and billions of edges. We present GraPE, a software resource
for graph processing and random walk based embedding, that can scale with large
and high-degree graphs and significantly speed up-computation. GraPE comprises
specialized data structures, algorithms, and a fast parallel implementation
that displays everal orders of magnitude improvement in empirical space and
time complexity compared to state of the art software resources, with a
corresponding boost in the performance of machine learning methods for edge and
node label prediction and for the unsupervised analysis of graphs.GraPE is
designed to run on laptop and desktop computers, as well as on high performance
computing clusters",space,737
http://arxiv.org/abs/2110.04003v2,filtered,arxiv,arxiv,2021-10-08 00:00:00,arxiv,learning to centralize dual-arm assembly,http://arxiv.org/abs/2110.04003v2,"Robotic manipulators are widely used in modern manufacturing processes.
However, their deployment in unstructured environments remains an open problem.
To deal with the variety, complexity, and uncertainty of real-world
manipulation tasks, it is essential to develop a flexible framework with
reduced assumptions on the environment characteristics. In recent years,
reinforcement learning (RL) has shown great results for single-arm robotic
manipulation. However, research focusing on dual-arm manipulation is still
rare. From a classical control perspective, solving such tasks often involves
complex modeling of interactions between two manipulators and the objects
encountered in the tasks, as well as the two robots coupling at a control
level. Instead, in this work, we explore the applicability of model-free RL to
dual-arm assembly. As we aim to contribute towards an approach that is not
limited to dual-arm assembly, but dual-arm manipulation in general, we keep
modeling efforts at a minimum. Hence, to avoid modeling the interaction between
the two robots and the used assembly tools, we present a modular approach with
two decentralized single-arm controllers which are coupled using a single
centralized learned policy. We reduce modeling effort to a minimum by using
sparse rewards only. Our architecture enables successful assembly and simple
transfer from simulation to the real world. We demonstrate the effectiveness of
the framework on dual-arm peg-in-hole and analyze sample efficiency and success
rates for different action spaces. Moreover, we compare results on different
clearances and showcase disturbance recovery and robustness, when dealing with
position uncertainties. Finally we zero-shot transfer policies trained in
simulation to the real world and evaluate their performance.",space,738
http://arxiv.org/abs/2110.03979v2,filtered,arxiv,arxiv,2021-10-08 00:00:00,arxiv,"millitrace-ir: contact tracing and temperature screening via mm-wave and
  infrared sensing",http://arxiv.org/abs/2110.03979v2,"Social distancing and temperature screening have been widely employed to
counteract the COVID-19 pandemic, sparking great interest from academia,
industry and public administrations worldwide. While most solutions have dealt
with these aspects separately, their combination would greatly benefit the
continuous monitoring of public spaces and help trigger effective
countermeasures. This work presents milliTRACE-IR, a joint mmWave radar and
infrared imaging sensing system performing unobtrusive and privacy preserving
human body temperature screening and contact tracing in indoor spaces.
milliTRACE-IR combines, via a robust sensor fusion approach, mmWave radars and
infrared thermal cameras. It achieves fully automated measurement of distancing
and body temperature, by jointly tracking the subjects's faces in the thermal
camera image plane and the human motion in the radar reference system.
Moreover, milliTRACE-IR performs contact tracing: a person with high body
temperature is reliably detected by the thermal camera sensor and subsequently
traced across a large indoor area in a non-invasive way by the radars. When
entering a new room, a subject is re-identified among several other individuals
by computing gait-related features from the radar reflections through a deep
neural network and using a weighted extreme learning machine as the final
re-identification tool. Experimental results, obtained from a real
implementation of milliTRACE-IR, demonstrate decimeter-level accuracy in
distance/trajectory estimation, inter-personal distance estimation (effective
for subjects getting as close as 0.2 m), and accurate temperature monitoring
(max. errors of 0.5{\deg}C). Furthermore, milliTRACE-IR provides contact
tracing through highly accurate (95%) person re-identification, in less than 20
seconds.",space,739
http://arxiv.org/abs/2110.02718v1,filtered,arxiv,arxiv,2021-10-06 00:00:00,arxiv,generalizing neural networks by reflecting deviating data in production,http://arxiv.org/abs/2110.02718v1,"Trained with a sufficiently large training and testing dataset, Deep Neural
Networks (DNNs) are expected to generalize. However, inputs may deviate from
the training dataset distribution in real deployments. This is a fundamental
issue with using a finite dataset. Even worse, real inputs may change over time
from the expected distribution. Taken together, these issues may lead deployed
DNNs to mis-predict in production.
  In this work, we present a runtime approach that mitigates DNN
mis-predictions caused by the unexpected runtime inputs to the DNN. In contrast
to previous work that considers the structure and parameters of the DNN itself,
our approach treats the DNN as a blackbox and focuses on the inputs to the DNN.
Our approach has two steps. First, it recognizes and distinguishes ""unseen""
semantically-preserving inputs. For this we use a distribution analyzer based
on the distance metric learned by a Siamese network. Second, our approach
transforms those unexpected inputs into inputs from the training set that are
identified as having similar semantics. We call this process input reflection
and formulate it as a search problem over the embedding space on the training
set. This embedding space is learned by a Quadruplet network as an auxiliary
model for the subject model to improve the generalization.
  We implemented a tool called InputReflector based on the above two-step
approach and evaluated it with experiments on three DNN models trained on
CIFAR-10, MNIST, and FMINST image datasets. The results show that
InputReflector can effectively distinguish inputs that retain semantics of the
distribution (e.g., blurred, brightened, contrasted, and zoomed images) and
out-of-distribution inputs from normal inputs.",space,740
http://arxiv.org/abs/2110.00330v2,filtered,arxiv,arxiv,2021-10-01 00:00:00,arxiv,"discovering boundary values of feature-based machine learning
  classifiers through exploratory datamorphic testing",http://arxiv.org/abs/2110.00330v2,"Testing has been widely recognised as difficult for AI applications. This
paper proposes a set of testing strategies for testing machine learning
applications in the framework of the datamorphism testing methodology. In these
strategies, testing aims at exploring the data space of a classification or
clustering application to discover the boundaries between classes that the
machine learning application defines. This enables the tester to understand
precisely the behaviour and function of the software under test. In the paper,
three variants of exploratory strategies are presented with the algorithms
implemented in the automated datamorphic testing tool Morphy. The correctness
of these algorithms are formally proved. Their capability and cost of
discovering borders between classes are evaluated via a set of controlled
experiments with manually designed subjects and a set of case studies with real
machine learning models.",space,741
http://arxiv.org/abs/2110.00218v2,filtered,arxiv,arxiv,2021-10-01 00:00:00,arxiv,"on the importance of gradients for detecting distributional shifts in
  the wild",http://arxiv.org/abs/2110.00218v2,"Detecting out-of-distribution (OOD) data has become a critical component in
ensuring the safe deployment of machine learning models in the real world.
Existing OOD detection approaches primarily rely on the output or feature space
for deriving OOD scores, while largely overlooking information from the
gradient space. In this paper, we present GradNorm, a simple and effective
approach for detecting OOD inputs by utilizing information extracted from the
gradient space. GradNorm directly employs the vector norm of gradients,
backpropagated from the KL divergence between the softmax output and a uniform
probability distribution. Our key idea is that the magnitude of gradients is
higher for in-distribution (ID) data than that for OOD data, making it
informative for OOD detection. GradNorm demonstrates superior performance,
reducing the average FPR95 by up to 16.33% compared to the previous best
method.",space,742
http://arxiv.org/abs/2108.08631v2,filtered,arxiv,arxiv,2021-08-19 00:00:00,arxiv,"determinant-free fermionic wave function using feed-forward neural
  networks",http://arxiv.org/abs/2108.08631v2,"We propose a general framework for finding the ground state of many-body
fermionic systems by using feed-forward neural networks. The anticommutation
relation for fermions is usually implemented to a variational wave function by
the Slater determinant (or Pfaffian), which is a computational bottleneck
because of the numerical cost of $O(N^3)$ for $N$ particles. We bypass this
bottleneck by explicitly calculating the sign changes associated with particle
exchanges in real space and using fully connected neural networks for
optimizing the rest parts of the wave function. This reduces the computational
cost to $O(N^2)$ or less. We show that the accuracy of the approximation can be
improved by optimizing the ""variance"" of the energy simultaneously with the
energy itself. We also find that a reweighting method in Monte Carlo sampling
can stabilize the calculation. These improvements can be applied to other
approaches based on variational Monte Carlo methods. Moreover, we show that the
accuracy can be further improved by using the symmetry of the system, the
representative states, and an additional neural network implementing a
generalized Gutzwiller-Jastrow factor. We demonstrate the efficiency of the
method by applying it to a two-dimensional Hubbard model.",space,743
http://arxiv.org/abs/2108.01846v2,filtered,arxiv,arxiv,2021-08-04 00:00:00,arxiv,"learning barrier certificates: towards safe reinforcement learning with
  zero training-time violations",http://arxiv.org/abs/2108.01846v2,"Training-time safety violations have been a major concern when we deploy
reinforcement learning algorithms in the real world. This paper explores the
possibility of safe RL algorithms with zero training-time safety violations in
the challenging setting where we are only given a safe but trivial-reward
initial policy without any prior knowledge of the dynamics model and additional
offline data. We propose an algorithm, Co-trained Barrier Certificate for Safe
RL (CRABS), which iteratively learns barrier certificates, dynamics models, and
policies. The barrier certificates, learned via adversarial training, ensure
the policy's safety assuming calibrated learned dynamics model. We also add a
regularization term to encourage larger certified regions to enable better
exploration. Empirical simulations show that zero safety violations are already
challenging for a suite of simple environments with only 2-4 dimensional state
space, especially if high-reward policies have to visit regions near the safety
boundary. Prior methods require hundreds of violations to achieve decent
rewards on these tasks, whereas our proposed algorithms incur zero violations.",space,744
http://arxiv.org/abs/2106.14739v1,filtered,arxiv,arxiv,2021-06-28 00:00:00,arxiv,"real-time human pose estimation on a smart walker using convolutional
  neural networks",http://arxiv.org/abs/2106.14739v1,"Rehabilitation is important to improve quality of life for mobility-impaired
patients. Smart walkers are a commonly used solution that should embed
automatic and objective tools for data-driven human-in-the-loop control and
monitoring. However, present solutions focus on extracting few specific metrics
from dedicated sensors with no unified full-body approach. We investigate a
general, real-time, full-body pose estimation framework based on two RGB+D
camera streams with non-overlapping views mounted on a smart walker equipment
used in rehabilitation. Human keypoint estimation is performed using a
two-stage neural network framework. The 2D-Stage implements a detection module
that locates body keypoints in the 2D image frames. The 3D-Stage implements a
regression module that lifts and relates the detected keypoints in both cameras
to the 3D space relative to the walker. Model predictions are low-pass filtered
to improve temporal consistency. A custom acquisition method was used to obtain
a dataset, with 14 healthy subjects, used for training and evaluating the
proposed framework offline, which was then deployed on the real walker
equipment. An overall keypoint detection error of 3.73 pixels for the 2D-Stage
and 44.05mm for the 3D-Stage were reported, with an inference time of 26.6ms
when deployed on the constrained hardware of the walker. We present a novel
approach to patient monitoring and data-driven human-in-the-loop control in the
context of smart walkers. It is able to extract a complete and compact body
representation in real-time and from inexpensive sensors, serving as a common
base for downstream metrics extraction solutions, and Human-Robot interaction
applications. Despite promising results, more data should be collected on users
with impairments, to assess its performance as a rehabilitation tool in
real-world scenarios.",space,745
http://arxiv.org/abs/2106.09357v1,filtered,arxiv,arxiv,2021-06-17 00:00:00,arxiv,"cat-like jumping and landing of legged robots in low-gravity using deep
  reinforcement learning",http://arxiv.org/abs/2106.09357v1,"In this article, we show that learned policies can be applied to solve legged
locomotion control tasks with extensive flight phases, such as those
encountered in space exploration. Using an off-the-shelf deep reinforcement
learning algorithm, we trained a neural network to control a jumping quadruped
robot while solely using its limbs for attitude control. We present tasks of
increasing complexity leading to a combination of three-dimensional
(re-)orientation and landing locomotion behaviors of a quadruped robot
traversing simulated low-gravity celestial bodies. We show that our approach
easily generalizes across these tasks and successfully trains policies for each
case. Using sim-to-real transfer, we deploy trained policies in the real world
on the SpaceBok robot placed on an experimental testbed designed for
two-dimensional micro-gravity experiments. The experimental results demonstrate
that repetitive, controlled jumping and landing with natural agility is
possible.",space,746
http://arxiv.org/abs/2106.08437v1,filtered,arxiv,arxiv,2021-06-15 00:00:00,arxiv,deep reinforcement learning on a multi-asset environment for trading,http://arxiv.org/abs/2106.08437v1,"Financial trading has been widely analyzed for decades with market
participants and academics always looking for advanced methods to improve
trading performance. Deep reinforcement learning (DRL), a recently
reinvigorated method with significant success in multiple domains, still has to
show its benefit in the financial markets. We use a deep Q-network (DQN) to
design long-short trading strategies for futures contracts. The state space
consists of volatility-normalized daily returns, with buying or selling being
the reinforcement learning action and the total reward defined as the
cumulative profits from our actions. Our trading strategy is trained and tested
both on real and simulated price series and we compare the results with an
index benchmark. We analyze how training based on a combination of artificial
data and actual price series can be successfully deployed in real markets. The
trained reinforcement learning agent is applied to trading the E-mini S&P 500
continuous futures contract. Our results in this study are preliminary and need
further improvement.",space,747
http://arxiv.org/abs/2105.13429v1,filtered,arxiv,arxiv,2021-05-27 00:00:00,arxiv,"flow based features and validation metric for machine learning
  reconstruction of piv data",http://arxiv.org/abs/2105.13429v1,"Reconstruction of flow field from real sparse data by a physics-oriented
approach is a current challenge for fluid scientists in the AI community. The
problem includes feature recognition and implementation of AI algorithms that
link data to a physical feature space in order to produce reconstructed data.
The present article applies machine learning approach to study contribution of
different flow-based features with practical fluid mechanics applications for
reconstruction of the missing data of turbomachinery PIV measurements. Support
vector regression (SVR) and multi-layer perceptron (MLP) are selected as two
robust regressors capable of modelling non-linear fluid flow phenomena. The
proposed flow-based features are optimally scaled and filtered to extract the
best configuration. In addition to conventional data-based validation of the
regressors, a metric is proposed that reflects mass conservation law as an
important requirement for a physical flow reproduction. For a velocity field
including 25% of clustered missing data, the reconstruction accuracy achieved
by SVR in terms of R2-score is as high as 0.993 for the in-plane velocity
vectors in comparison with that obtained by MLP which is up to 0.981. In terms
of mass conservation metric, the SVR model by R2-score up to 0.96 is
considerably more accurate than the MLP estimator. For extremely sparse data
with a gappiness of 75%, vector and contour plots from SVR and MLP were
consistent with those of the original field.",space,748
http://arxiv.org/abs/2104.09684v2,filtered,arxiv,arxiv,2021-04-19 00:00:00,arxiv,suppressing simulation bias using multi-modal data,http://arxiv.org/abs/2104.09684v2,"Many problems in science and engineering require making predictions based on
few observations. To build a robust predictive model, these sparse data may
need to be augmented with simulated data, especially when the design space is
multi-dimensional. Simulations, however, often suffer from an inherent bias.
Estimation of this bias may be poorly constrained not only because of data
sparsity, but also because traditional predictive models fit only one type of
observed outputs, such as scalars or images, instead of all available output
data modalities, which might have been acquired and simulated at great cost. To
break this limitation and open up the path for multi-modal calibration, we
propose to combine a novel, transfer learning technique for suppressing the
bias with recent developments in deep learning, which allow building predictive
models with multi-modal outputs. First, we train an initial neural network
model on simulated data to learn important correlations between different
output modalities and between simulation inputs and outputs. Then, the model is
partially retrained, or transfer learned, to fit the experiments; a method that
has never been implemented in this type of architecture. Using fewer than 10
inertial confinement fusion experiments for training, transfer learning
systematically improves the simulation predictions while a simple output
calibration, which we design as a baseline, makes the predictions worse. We
also offer extensive cross-validation with real and carefully designed
synthetic data. The method described in this paper can be applied to a wide
range of problems that require transferring knowledge from simulations to the
domain of experiments.",space,749
http://arxiv.org/abs/2104.07282v1,filtered,arxiv,arxiv,2021-04-15 00:00:00,arxiv,"rule-based reinforcement learning for efficient robot navigation with
  space reduction",http://arxiv.org/abs/2104.07282v1,"For real-world deployments, it is critical to allow robots to navigate in
complex environments autonomously. Traditional methods usually maintain an
internal map of the environment, and then design several simple rules, in
conjunction with a localization and planning approach, to navigate through the
internal map. These approaches often involve a variety of assumptions and prior
knowledge. In contrast, recent reinforcement learning (RL) methods can provide
a model-free, self-learning mechanism as the robot interacts with an initially
unknown environment, but are expensive to deploy in real-world scenarios due to
inefficient exploration. In this paper, we focus on efficient navigation with
the RL technique and combine the advantages of these two kinds of methods into
a rule-based RL (RuRL) algorithm for reducing the sample complexity and cost of
time. First, we use the rule of wall-following to generate a closed-loop
trajectory. Second, we employ a reduction rule to shrink the trajectory, which
in turn effectively reduces the redundant exploration space. Besides, we give
the detailed theoretical guarantee that the optimal navigation path is still in
the reduced space. Third, in the reduced space, we utilize the Pledge rule to
guide the exploration strategy for accelerating the RL process at the early
stage. Experiments conducted on real robot navigation problems in hex-grid
environments demonstrate that RuRL can achieve improved navigation performance.",space,750
http://arxiv.org/abs/2104.02459v2,filtered,arxiv,arxiv,2021-04-06 00:00:00,arxiv,contrastive explanations for explaining model adaptations,http://arxiv.org/abs/2104.02459v2,"Many decision making systems deployed in the real world are not static - a
phenomenon known as model adaptation takes place over time. The need for
transparency and interpretability of AI-based decision models is widely
accepted and thus have been worked on extensively. Usually, explanation methods
assume a static system that has to be explained. Explaining non-static systems
is still an open research question, which poses the challenge how to explain
model adaptations. In this contribution, we propose and (empirically) evaluate
a framework for explaining model adaptations by contrastive explanations. We
also propose a method for automatically finding regions in data space that are
affected by a given model adaptation and thus should be explained.",space,751
http://arxiv.org/abs/2104.02306v1,filtered,arxiv,arxiv,2021-04-06 00:00:00,arxiv,binary neural network for speaker verification,http://arxiv.org/abs/2104.02306v1,"Although deep neural networks are successful for many tasks in the speech
domain, the high computational and memory costs of deep neural networks make it
difficult to directly deploy highperformance Neural Network systems on
low-resource embedded devices. There are several mechanisms to reduce the size
of the neural networks i.e. parameter pruning, parameter quantization, etc.
This paper focuses on how to apply binary neural networks to the task of
speaker verification. The proposed binarization of training parameters can
largely maintain the performance while significantly reducing storage space
requirements and computational costs. Experiment results show that, after
binarizing the Convolutional Neural Network, the ResNet34-based network
achieves an EER of around 5% on the Voxceleb1 testing dataset and even
outperforms the traditional real number network on the text-dependent dataset:
Xiaole while having a 32x memory saving.",space,752
http://arxiv.org/abs/2104.01716v2,filtered,arxiv,arxiv,2021-04-05 00:00:00,arxiv,"quaternion factorization machines: a lightweight solution to intricate
  feature interaction modelling",http://arxiv.org/abs/2104.01716v2,"As a well-established approach, factorization machine (FM) is capable of
automatically learning high-order interactions among features to make
predictions without the need for manual feature engineering. With the prominent
development of deep neural networks (DNNs), there is a recent and ongoing trend
of enhancing the expressiveness of FM-based models with DNNs. However, though
better results are obtained with DNN-based FM variants, such performance gain
is paid off by an enormous amount (usually millions) of excessive model
parameters on top of the plain FM. Consequently, the heavy parameterization
impedes the real-life practicality of those deep models, especially efficient
deployment on resource-constrained IoT and edge devices. In this paper, we move
beyond the traditional real space where most deep FM-based models are defined,
and seek solutions from quaternion representations within the hypercomplex
space. Specifically, we propose the quaternion factorization machine (QFM) and
quaternion neural factorization machine (QNFM), which are two novel lightweight
and memory-efficient quaternion-valued models for sparse predictive analytics.
By introducing a brand new take on FM-based models with the notion of
quaternion algebra, our models not only enable expressive inter-component
feature interactions, but also significantly reduce the parameter size due to
lower degrees of freedom in the hypercomplex Hamilton product compared with
real-valued matrix multiplication. Extensive experimental results on three
large-scale datasets demonstrate that QFM achieves 4.36% performance
improvement over the plain FM without introducing any extra parameters, while
QNFM outperforms all baselines with up to two magnitudes' parameter size
reduction in comparison to state-of-the-art peer methods.",space,753
http://arxiv.org/abs/2103.14251v1,filtered,arxiv,arxiv,2021-03-26 00:00:00,arxiv,"embedding power flow into machine learning for parameter and state
  estimation",http://arxiv.org/abs/2103.14251v1,"Modern state and parameter estimations in power systems consist of two
stages: the outer problem of minimizing the mismatch between network
observation and prediction over the network parameters, and the inner problem
of predicting the system state for given values of the parameters. The standard
solution of the combined problem is iterative: (a) set the parameters, e.g. to
priors on the power line characteristics, (b) map input observation to
prediction of the output, (c) compute the mismatch between predicted and
observed output, (d) make a gradient descent step in the space of parameters to
minimize the mismatch, and loop back to (a). We show how modern Machine
Learning (ML), and specifically training guided by automatic differentiation,
allows to resolve the iterative loop more efficiently. Moreover, we extend the
scheme to the case of incomplete observations, where Phasor Measurement Units
(reporting real and reactive powers, voltage and phase) are available only at
the generators (PV buses), while loads (PQ buses) report (via SCADA controls)
only active and reactive powers. Considering it from the implementation
perspective, our methodology of resolving the parameter and state estimation
problem can be viewed as embedding of the Power Flow (PF) solver into the
training loop of the Machine Learning framework (PyTorch, in this study). We
argue that this embedding can help to resolve high-level optimization problems
in power system operations and planning.",space,754
http://arxiv.org/abs/2103.12344v2,filtered,arxiv,arxiv,2021-03-23 00:00:00,arxiv,"joint distribution across representation space for out-of-distribution
  detection",http://arxiv.org/abs/2103.12344v2,"Deep neural networks (DNNs) have become a key part of many modern software
applications. After training and validating, the DNN is deployed as an
irrevocable component and applied in real-world scenarios. Although most DNNs
are built meticulously with huge volumes of training data, data in the real
world still remain unknown to the DNN model, which leads to the crucial
requirement of runtime out-of-distribution (OOD) detection. However, many
existing approaches 1) need OOD data for classifier training or parameter
tuning, or 2) simply combine the scores of each hidden layer as an ensemble of
features for OOD detection. In this paper, we present a novel outlook on
in-distribution data in a generative manner, which takes their latent features
generated from each hidden layer as a joint distribution across representation
spaces. Since only the in-distribution latent features are comprehensively
understood in representation space, the internal difference between
in-distribution and OOD data can be naturally revealed without the intervention
of any OOD data. Specifically, We construct a generative model, called Latent
Sequential Gaussian Mixture (LSGM), to depict how the in-distribution latent
features are generated in terms of the trace of DNN inference across
representation spaces. We first construct the Gaussian Mixture Model (GMM)
based on in-distribution latent features for each hidden layer, and then
connect GMMs via the transition probabilities of the inference traces.
Experimental evaluations on popular benchmark OOD datasets and models validate
the superiority of the proposed method over the state-of-the-art methods in OOD
detection.",space,755
http://arxiv.org/abs/2103.07541v1,filtered,arxiv,arxiv,2021-03-12 00:00:00,arxiv,"machine learning aided k-t sense for fast reconstruction of highly
  accelerated pcmr data",http://arxiv.org/abs/2103.07541v1,"Purpose: We implemented the Machine Learning (ML) aided k-t SENSE
reconstruction to enable high resolution quantitative real-time phase contrast
MR (PCMR). Methods: A residual U-net and our U-net M were used to generate the
high resolution x-f space estimate for k-t SENSE regularisation prior. The
networks were judged on their ability to generalise to real undersampled data.
The in-vivo validation was done on 20 real-time 18x prospectively undersmapled
GASperturbed PCMR data. The ML aided k-t SENSE reconstruction results were
compared against the free-breathing Cartesian retrospectively gated sequence
and the compressed sensing (CS) reconstruction of the same data. Results: In
general, the ML aided k-t SENSE generated flow curves that were visually
sharper than those produced using CS. In two exceptional cases, U-net M
predictions exhibited blurring which propagated to the extracted velocity
curves. However, there were no statistical differences in the measured peak
velocities and stroke volumes between the tested methods. The ML aided k-t
SENSE was estimated to be ~3.6x faster in processing than CS. Conclusion: The
ML aided k-t SENSE reconstruction enables artefact suppression on a par with CS
with no significant differences in quantitative measures. The timing results
suggest the on-line implementation could deliver a substantial increase in
clinical throughput.",space,756
http://arxiv.org/abs/2102.05334v2,filtered,arxiv,arxiv,2021-02-10 00:00:00,arxiv,"enhancing real-world adversarial patches through 3d modeling of complex
  target scenes",http://arxiv.org/abs/2102.05334v2,"Adversarial examples have proven to be a concerning threat to deep learning
models, particularly in the image domain. However, while many studies have
examined adversarial examples in the real world, most of them relied on 2D
photos of the attack scene. As a result, the attacks proposed may have limited
effectiveness when implemented in realistic environments with 3D objects or
varied conditions. There are few studies on adversarial learning that use 3D
objects, and in many cases, other researchers are unable to replicate the
real-world evaluation process. In this study, we present a framework that uses
3D modeling to craft adversarial patches for an existing real-world scene. Our
approach uses a 3D digital approximation of the scene as a simulation of the
real world. With the ability to add and manipulate any element in the digital
scene, our framework enables the attacker to improve the adversarial patch's
impact in real-world settings. We use the framework to create a patch for an
everyday scene and evaluate its performance using a novel evaluation process
that ensures that our results are reproducible in both the digital space and
the real world. Our evaluation results show that the framework can generate
adversarial patches that are robust to different settings in the real world.",space,757
http://arxiv.org/abs/2101.06582v1,filtered,arxiv,arxiv,2021-01-17 00:00:00,arxiv,"tailored learning-based scheduling for kubernetes-oriented edge-cloud
  system",http://arxiv.org/abs/2101.06582v1,"Kubernetes (k8s) has the potential to merge the distributed edge and the
cloud but lacks a scheduling framework specifically for edge-cloud systems.
Besides, the hierarchical distribution of heterogeneous resources and the
complex dependencies among requests and resources make the modeling and
scheduling of k8s-oriented edge-cloud systems particularly sophisticated. In
this paper, we introduce KaiS, a learning-based scheduling framework for such
edge-cloud systems to improve the long-term throughput rate of request
processing. First, we design a coordinated multi-agent actor-critic algorithm
to cater to decentralized request dispatch and dynamic dispatch spaces within
the edge cluster. Second, for diverse system scales and structures, we use
graph neural networks to embed system state information, and combine the
embedding results with multiple policy networks to reduce the orchestration
dimensionality by stepwise scheduling. Finally, we adopt a two-time-scale
scheduling mechanism to harmonize request dispatch and service orchestration,
and present the implementation design of deploying the above algorithms
compatible with native k8s components. Experiments using real workload traces
show that KaiS can successfully learn appropriate scheduling policies,
irrespective of request arrival patterns and system scales. Moreover, KaiS can
enhance the average system throughput rate by 14.3% while reducing scheduling
cost by 34.7% compared to baselines.",space,758
http://arxiv.org/abs/2101.03989v2,filtered,arxiv,arxiv,2021-01-11 00:00:00,arxiv,technology readiness levels for machine learning systems,http://arxiv.org/abs/2101.03989v2,"The development and deployment of machine learning (ML) systems can be
executed easily with modern tools, but the process is typically rushed and
means-to-an-end. The lack of diligence can lead to technical debt, scope creep
and misaligned objectives, model misuse and failures, and expensive
consequences. Engineering systems, on the other hand, follow well-defined
processes and testing standards to streamline development for high-quality,
reliable results. The extreme is spacecraft systems, where mission critical
measures and robustness are ingrained in the development process. Drawing on
experience in both spacecraft engineering and ML (from research through product
across domain areas), we have developed a proven systems engineering approach
for machine learning development and deployment. Our ""Machine Learning
Technology Readiness Levels"" (MLTRL) framework defines a principled process to
ensure robust, reliable, and responsible systems while being streamlined for ML
workflows, including key distinctions from traditional software engineering.
Even more, MLTRL defines a lingua franca for people across teams and
organizations to work collaboratively on artificial intelligence and machine
learning technologies. Here we describe the framework and elucidate it with
several real world use-cases of developing ML methods from basic research
through productization and deployment, in areas such as medical diagnostics,
consumer computer vision, satellite imagery, and particle physics.",space,759
http://arxiv.org/abs/2012.12142v1,filtered,arxiv,arxiv,2020-12-22 00:00:00,arxiv,high-speed robot navigation using predicted occupancy maps,http://arxiv.org/abs/2012.12142v1,"Safe and high-speed navigation is a key enabling capability for real world
deployment of robotic systems. A significant limitation of existing approaches
is the computational bottleneck associated with explicit mapping and the
limited field of view (FOV) of existing sensor technologies. In this paper, we
study algorithmic approaches that allow the robot to predict spaces extending
beyond the sensor horizon for robust planning at high speeds. We accomplish
this using a generative neural network trained from real-world data without
requiring human annotated labels. Further, we extend our existing control
algorithms to support leveraging the predicted spaces to improve collision-free
planning and navigation at high speeds. Our experiments are conducted on a
physical robot based on the MIT race car using an RGBD sensor where were able
to demonstrate improved performance at 4 m/s compared to a controller not
operating on predicted regions of the map.",space,760
http://arxiv.org/abs/2012.10610v3,filtered,arxiv,arxiv,2020-12-19 00:00:00,arxiv,"spaceml: distributed open-source research with citizen scientists for
  the advancement of space technology for nasa",http://arxiv.org/abs/2012.10610v3,"Traditionally, academic labs conduct open-ended research with the primary
focus on discoveries with long-term value, rather than direct products that can
be deployed in the real world. On the other hand, research in the industry is
driven by its expected commercial return on investment, and hence focuses on a
real world product with short-term timelines. In both cases, opportunity is
selective, often available to researchers with advanced educational
backgrounds. Research often happens behind closed doors and may be kept
confidential until either its publication or product release, exacerbating the
problem of AI reproducibility and slowing down future research by others in the
field. As many research organizations tend to exclusively focus on specific
areas, opportunities for interdisciplinary research reduce. Undertaking
long-term bold research in unexplored fields with non-commercial yet great
public value is hard due to factors including the high upfront risk, budgetary
constraints, and a lack of availability of data and experts in niche fields.
Only a few companies or well-funded research labs can afford to do such
long-term research. With research organizations focused on an exploding array
of fields and resources spread thin, opportunities for the maturation of
interdisciplinary research reduce. Apart from these exigencies, there is also a
need to engage citizen scientists through open-source contributors to play an
active part in the research dialogue. We present a short case study of SpaceML,
an extension of the Frontier Development Lab, an AI accelerator for NASA.
SpaceML distributes open-source research and invites volunteer citizen
scientists to partake in development and deployment of high social value
products at the intersection of space and AI.",space,761
http://arxiv.org/abs/2012.08015v2,filtered,arxiv,arxiv,2020-12-15 00:00:00,arxiv,active learning for deep gaussian process surrogates,http://arxiv.org/abs/2012.08015v2,"Deep Gaussian processes (DGPs) are increasingly popular as predictive models
in machine learning (ML) for their non-stationary flexibility and ability to
cope with abrupt regime changes in training data. Here we explore DGPs as
surrogates for computer simulation experiments whose response surfaces exhibit
similar characteristics. In particular, we transport a DGP's automatic warping
of the input space and full uncertainty quantification (UQ), via a novel
elliptical slice sampling (ESS) Bayesian posterior inferential scheme, through
to active learning (AL) strategies that distribute runs non-uniformly in the
input space -- something an ordinary (stationary) GP could not do. Building up
the design sequentially in this way allows smaller training sets, limiting both
expensive evaluation of the simulator code and mitigating cubic costs of DGP
inference. When training data sizes are kept small through careful acquisition,
and with parsimonious layout of latent layers, the framework can be both
effective and computationally tractable. Our methods are illustrated on
simulation data and two real computer experiments of varying input
dimensionality. We provide an open source implementation in the ""deepgp""
package on CRAN.",space,762
http://arxiv.org/abs/2012.08364v1,filtered,arxiv,arxiv,2020-12-13 00:00:00,arxiv,gap-net for snapshot compressive imaging,http://arxiv.org/abs/2012.08364v1,"Snapshot compressive imaging (SCI) systems aim to capture high-dimensional
($\ge3$D) images in a single shot using 2D detectors. SCI devices include two
main parts: a hardware encoder and a software decoder. The hardware encoder
typically consists of an (optical) imaging system designed to capture
{compressed measurements}. The software decoder on the other hand refers to a
reconstruction algorithm that retrieves the desired high-dimensional signal
from those measurements. In this paper, using deep unfolding ideas, we propose
an SCI recovery algorithm, namely GAP-net, which unfolds the generalized
alternating projection (GAP) algorithm. At each stage, GAP-net passes its
current estimate of the desired signal through a trained convolutional neural
network (CNN). The CNN operates as a denoiser that projects the estimate back
to the desired signal space. For the GAP-net that employs trained
auto-encoder-based denoisers, we prove a probabilistic global convergence
result. Finally, we investigate the performance of GAP-net in solving video SCI
and spectral SCI problems. In both cases, GAP-net demonstrates competitive
performance on both synthetic and real data. In addition to having high
accuracy and high speed, we show that GAP-net is flexible with respect to
signal modulation implying that a trained GAP-net decoder can be applied in
different systems. Our code is at https://github.com/mengziyi64/ADMM-net.",space,763
http://arxiv.org/abs/2012.04746v2,filtered,arxiv,arxiv,2020-12-08 00:00:00,arxiv,"robust neural routing through space partitions for camera relocalization
  in dynamic indoor environments",http://arxiv.org/abs/2012.04746v2,"Localizing the camera in a known indoor environment is a key building block
for scene mapping, robot navigation, AR, etc. Recent advances estimate the
camera pose via optimization over the 2D/3D-3D correspondences established
between the coordinates in 2D/3D camera space and 3D world space. Such a
mapping is estimated with either a convolution neural network or a decision
tree using only the static input image sequence, which makes these approaches
vulnerable to dynamic indoor environments that are quite common yet challenging
in the real world. To address the aforementioned issues, in this paper, we
propose a novel outlier-aware neural tree which bridges the two worlds, deep
learning and decision tree approaches. It builds on three important blocks: (a)
a hierarchical space partition over the indoor scene to construct the decision
tree; (b) a neural routing function, implemented as a deep classification
network, employed for better 3D scene understanding; and (c) an outlier
rejection module used to filter out dynamic points during the hierarchical
routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark
developed for camera relocalization in dynamic indoor environments. It achieves
robust neural routing through space partitions and outperforms the
state-of-the-art approaches by around 30% on camera pose accuracy, while
running comparably fast for evaluation.",space,764
http://arxiv.org/abs/2011.09747v2,filtered,arxiv,arxiv,2020-11-19 00:00:00,arxiv,"energy aware deep reinforcement learning scheduling for sensors
  correlated in time and space",http://arxiv.org/abs/2011.09747v2,"Millions of battery-powered sensors deployed for monitoring purposes in a
multitude of scenarios, e.g., agriculture, smart cities, industry, etc.,
require energy-efficient solutions to prolong their lifetime. When these
sensors observe a phenomenon distributed in space and evolving in time, it is
expected that collected observations will be correlated in time and space. In
this paper, we propose a Deep Reinforcement Learning (DRL) based scheduling
mechanism capable of taking advantage of correlated information. We design our
solution using the Deep Deterministic Policy Gradient (DDPG) algorithm. The
proposed mechanism is capable of determining the frequency with which sensors
should transmit their updates, to ensure accurate collection of observations,
while simultaneously considering the energy available. To evaluate our
scheduling mechanism, we use multiple datasets containing environmental
observations obtained in multiple real deployments. The real observations
enable us to model the environment with which the mechanism interacts as
realistically as possible. We show that our solution can significantly extend
the sensors' lifetime. We compare our mechanism to an idealized, all-knowing
scheduler to demonstrate that its performance is near-optimal. Additionally, we
highlight the unique feature of our design, energy-awareness, by displaying the
impact of sensors' energy levels on the frequency of updates.",space,765
http://arxiv.org/abs/2011.06501v2,filtered,arxiv,arxiv,2020-11-12 00:00:00,arxiv,varclust: clustering variables using dimensionality reduction,http://arxiv.org/abs/2011.06501v2,"VARCLUST algorithm is proposed for clustering variables under the assumption
that variables in a given cluster are linear combinations of a small number of
hidden latent variables, corrupted by the random noise. The entire clustering
task is viewed as the problem of selection of the statistical model, which is
defined by the number of clusters, the partition of variables into these
clusters and the 'cluster dimensions', i.e. the vector of dimensions of linear
subspaces spanning each of the clusters. The optimal model is selected using
the approximate Bayesian criterion based on the Laplace approximations and
using a non-informative uniform prior on the number of clusters. To solve the
problem of the search over a huge space of possible models we propose an
extension of the ClustOfVar algorithm which was dedicated to subspaces of
dimension only 1, and which is similar in structure to the $K$-centroid
algorithm. We provide a complete methodology with theoretical guarantees,
extensive numerical experimentations, complete data analyses and
implementation. Our algorithm assigns variables to appropriate clusterse based
on the consistent Bayesian Information Criterion (BIC), and estimates the
dimensionality of each cluster by the PEnalized SEmi-integrated Likelihood
Criterion (PESEL), whose consistency we prove. Additionally, we prove that each
iteration of our algorithm leads to an increase of the Laplace approximation to
the model posterior probability and provide the criterion for the estimation of
the number of clusters. Numerical comparisons with other algorithms show that
VARCLUST may outperform some popular machine learning tools for sparse subspace
clustering. We also report the results of real data analysis including TCGA
breast cancer data and meteorological data. The proposed method is implemented
in the publicly available R package varclust.",space,766
http://arxiv.org/abs/2010.14605v3,filtered,arxiv,arxiv,2020-10-27 00:00:00,arxiv,"traffic refinery: cost-aware data representation for machine learning on
  network traffic",http://arxiv.org/abs/2010.14605v3,"Network management often relies on machine learning to make predictions about
performance and security from network traffic. Often, the representation of the
traffic is as important as the choice of the model. The features that the model
relies on, and the representation of those features, ultimately determine model
accuracy, as well as where and whether the model can be deployed in practice.
Thus, the design and evaluation of these models ultimately requires
understanding not only model accuracy but also the systems costs associated
with deploying the model in an operational network. Towards this goal, this
paper develops a new framework and system that enables a joint evaluation of
both the conventional notions of machine learning performance (e.g., model
accuracy) and the systems-level costs of different representations of network
traffic. We highlight these two dimensions for two practical network management
tasks, video streaming quality inference and malware detection, to demonstrate
the importance of exploring different representations to find the appropriate
operating point. We demonstrate the benefit of exploring a range of
representations of network traffic and present Traffic Refinery, a
proof-of-concept implementation that both monitors network traffic at 10 Gbps
and transforms traffic in real time to produce a variety of feature
representations for machine learning. Traffic Refinery both highlights this
design space and makes it possible to explore different representations for
learning, balancing systems costs related to feature extraction and model
training against model accuracy.",space,767
http://arxiv.org/abs/2010.08600v2,filtered,arxiv,arxiv,2020-10-16 00:00:00,arxiv,"robot navigation in constrained pedestrian environments using
  reinforcement learning",http://arxiv.org/abs/2010.08600v2,"Navigating fluently around pedestrians is a necessary capability for mobile
robots deployed in human environments, such as buildings and homes. While
research on social navigation has focused mainly on the scalability with the
number of pedestrians in open spaces, typical indoor environments present the
additional challenge of constrained spaces such as corridors and doorways that
limit maneuverability and influence patterns of pedestrian interaction. We
present an approach based on reinforcement learning (RL) to learn policies
capable of dynamic adaptation to the presence of moving pedestrians while
navigating between desired locations in constrained environments. The policy
network receives guidance from a motion planner that provides waypoints to
follow a globally planned trajectory, whereas RL handles the local
interactions. We explore a compositional principle for multi-layout training
and find that policies trained in a small set of geometrically simple layouts
successfully generalize to more complex unseen layouts that exhibit composition
of the structural elements available during training. Going beyond walls-world
like domains, we show transfer of the learned policy to unseen 3D
reconstructions of two real environments. These results support the
applicability of the compositional principle to navigation in real-world
buildings and indicate promising usage of multi-agent simulation within
reconstructed environments for tasks that involve interaction.",space,768
http://arxiv.org/abs/2010.05878v2,filtered,arxiv,arxiv,2020-10-12 00:00:00,arxiv,pecos: prediction for enormous and correlated output spaces,http://arxiv.org/abs/2010.05878v2,"Many large-scale applications amount to finding relevant results from an
enormous output space of potential candidates. For example, finding the best
matching product from a large catalog or suggesting related search phrases on a
search engine. The size of the output space for these problems can range from
millions to billions, and can even be infinite in some applications. Moreover,
training data is often limited for the long-tail items in the output space.
Fortunately, items in the output space are often correlated thereby presenting
an opportunity to alleviate the data sparsity issue. In this paper, we propose
the Prediction for Enormous and Correlated Output Spaces (PECOS) framework, a
versatile and modular machine learning framework for solving prediction
problems for very large output spaces, and apply it to the eXtreme Multilabel
Ranking (XMR) problem: given an input instance, find and rank the most relevant
items from an enormous but fixed and finite output space. We propose a three
phase framework for PECOS: (i) in the first phase, PECOS organizes the output
space using a semantic indexing scheme, (ii) in the second phase, PECOS uses
the indexing to narrow down the output space by orders of magnitude using a
machine learned matching scheme, and (iii) in the third phase, PECOS ranks the
matched items using a final ranking scheme. The versatility and modularity of
PECOS allows for easy plug-and-play of various choices for the indexing,
matching, and ranking phases. We also develop very fast inference procedures
which allow us to perform XMR predictions in real time; for example, inference
takes less than 1 millisecond per input on the dataset with 2.8 million labels.
The PECOS software is available at https://libpecos.org.",space,769
http://arxiv.org/abs/2009.11380v2,filtered,arxiv,arxiv,2020-09-23 00:00:00,arxiv,"combining weighted total variation and deep image prior for natural and
  medical image restoration via admm",http://arxiv.org/abs/2009.11380v2,"In the last decades, unsupervised deep learning based methods have caught
researchers attention, since in many real applications, such as medical
imaging, collecting a great amount of training examples is not always feasible.
Moreover, the construction of a good training set is time consuming and hard
because the selected data have to be enough representative for the task. In
this paper, we focus on the Deep Image Prior (DIP) framework and we propose to
combine it with a space-variant Total Variation regularizer with an automatic
estimation of the local regularization parameters. Differently from other
existing approaches, we solve the arising minimization problem via the flexible
Alternating Direction Method of Multipliers (ADMM). Furthermore, we provide a
specific implementation also for the standard isotropic Total Variation. The
promising performances of the proposed approach, in terms of PSNR and SSIM
values, are addressed through several experiments on simulated as well as real
natural and medical corrupted images.",space,770
http://arxiv.org/abs/2009.00871v1,filtered,arxiv,arxiv,2020-09-02 00:00:00,arxiv,"hl-pow: a learning-based power modeling framework for high-level
  synthesis",http://arxiv.org/abs/2009.00871v1,"High-level synthesis (HLS) enables designers to customize hardware designs
efficiently. However, it is still challenging to foresee the correlation
between power consumption and HLS-based applications at an early design stage.
To overcome this problem, we introduce HL-Pow, a power modeling framework for
FPGA HLS based on state-of-the-art machine learning techniques. HL-Pow
incorporates an automated feature construction flow to efficiently identify and
extract features that exert a major influence on power consumption, simply
based upon HLS results, and a modeling flow that can build an accurate and
generic power model applicable to a variety of designs with HLS. By using
HL-Pow, the power evaluation process for FPGA designs can be significantly
expedited because the power inference of HL-Pow is established on HLS instead
of the time-consuming register-transfer level (RTL) implementation flow.
Experimental results demonstrate that HL-Pow can achieve accurate power
modeling that is only 4.67% (24.02 mW) away from onboard power measurement. To
further facilitate power-oriented optimizations, we describe a novel design
space exploration (DSE) algorithm built on top of HL-Pow to trade off between
latency and power consumption. This algorithm can reach a close approximation
of the real Pareto frontier while only requiring running HLS flow for 20% of
design points in the entire design space.",space,771
http://arxiv.org/abs/2008.13223v2,filtered,arxiv,arxiv,2020-08-30 00:00:00,arxiv,"deep reinforcement learning for contact-rich skills using compliant
  movement primitives",http://arxiv.org/abs/2008.13223v2,"In recent years, industrial robots have been installed in various industries
to handle advanced manufacturing and high precision tasks. However, further
integration of industrial robots is hampered by their limited flexibility,
adaptability and decision making skills compared to human operators. Assembly
tasks are especially challenging for robots since they are contact-rich and
sensitive to even small uncertainties. While reinforcement learning (RL) offers
a promising framework to learn contact-rich control policies from scratch, its
applicability to high-dimensional continuous state-action spaces remains rather
limited due to high brittleness and sample complexity. To address those issues,
we propose different pruning methods that facilitate convergence and
generalization. In particular, we divide the task into free and contact-rich
sub-tasks, perform the control in Cartesian rather than joint space, and
parameterize the control policy. Those pruning methods are naturally
implemented within the framework of dynamic movement primitives (DMP). To
handle contact-rich tasks, we extend the DMP framework by introducing a
coupling term that acts like the human wrist and provides active compliance
under contact with the environment. We demonstrate that the proposed method can
learn insertion skills that are invariant to space, size, shape, and closely
related scenarios, while handling large uncertainties. Finally we demonstrate
that the learned policy can be easily transferred from simulations to real
world and achieve similar performance on UR5e robot.",space,772
http://arxiv.org/abs/2008.13585v1,filtered,arxiv,arxiv,2020-08-26 00:00:00,arxiv,at your service: coffee beans recommendation from a robot assistant,http://arxiv.org/abs/2008.13585v1,"With advances in the field of machine learning, precisely algorithms for
recommendation systems, robot assistants are envisioned to become more present
in the hospitality industry. Additionally, the COVID-19 pandemic has also
highlighted the need to have more service robots in our everyday lives, to
minimise the risk of human to-human transmission. One such example would be
coffee shops, which have become intrinsic to our everyday lives. However,
serving an excellent cup of coffee is not a trivial feat as a coffee blend
typically comprises rich aromas, indulgent and unique flavours and a lingering
aftertaste. Our work addresses this by proposing a computational model which
recommends optimal coffee beans resulting from the user's preferences.
Specifically, given a set of coffee bean properties (objective features), we
apply different supervised learning techniques to predict coffee qualities
(subjective features). We then consider an unsupervised learning method to
analyse the relationship between coffee beans in the subjective feature space.
Evaluated on a real coffee beans dataset based on digitised reviews, our
results illustrate that the proposed computational model gives up to 92.7
percent recommendation accuracy for coffee beans prediction. From this, we
propose how this computational model can be deployed on a service robot to
reliably predict customers' coffee bean preferences, starting from the user
inputting their coffee preferences to the robot recommending the coffee beans
that best meet the user's likings.",space,773
http://arxiv.org/abs/2008.05381v1,filtered,arxiv,arxiv,2020-08-12 00:00:00,arxiv,"improving the performance of fine-grain image classifiers via generative
  data augmentation",http://arxiv.org/abs/2008.05381v1,"Recent advances in machine learning (ML) and computer vision tools have
enabled applications in a wide variety of arenas such as financial analytics,
medical diagnostics, and even within the Department of Defense. However, their
widespread implementation in real-world use cases poses several challenges: (1)
many applications are highly specialized, and hence operate in a \emph{sparse
data} domain; (2) ML tools are sensitive to their training sets and typically
require cumbersome, labor-intensive data collection and data labelling
processes; and (3) ML tools can be extremely ""black box,"" offering users little
to no insight into the decision-making process or how new data might affect
prediction performance. To address these challenges, we have designed and
developed Data Augmentation from Proficient Pre-Training of Robust Generative
Adversarial Networks (DAPPER GAN), an ML analytics support tool that
automatically generates novel views of training images in order to improve
downstream classifier performance. DAPPER GAN leverages high-fidelity
embeddings generated by a StyleGAN2 model (trained on the LSUN cars dataset) to
create novel imagery for previously unseen classes. We experimentally evaluate
this technique on the Stanford Cars dataset, demonstrating improved vehicle
make and model classification accuracy and reduced requirements for real data
using our GAN based data augmentation framework. The method's validity was
supported through an analysis of classifier performance on both augmented and
non-augmented datasets, achieving comparable or better accuracy with up to 30\%
less real data across visually similar classes. To support this method, we
developed a novel augmentation method that can manipulate semantically
meaningful dimensions (e.g., orientation) of the target object in the embedding
space.",space,774
http://arxiv.org/abs/2007.12327v2,filtered,arxiv,arxiv,2020-07-24 00:00:00,arxiv,"stochastic dynamic information flow tracking game using supervised
  learning for detecting advanced persistent threats",http://arxiv.org/abs/2007.12327v2,"Advanced persistent threats (APTs) are organized prolonged cyberattacks by
sophisticated attackers. Although APT activities are stealthy, they interact
with the system components and these interactions lead to information flows.
Dynamic Information Flow Tracking (DIFT) has been proposed as one of the
effective ways to detect APTs using the information flows. However, wide range
security analysis using DIFT results in a significant increase in performance
overhead and high rates of false-positives and false-negatives generated by
DIFT. In this paper, we model the strategic interaction between APT and DIFT as
a non-cooperative stochastic game. The game unfolds on a state space
constructed from an information flow graph (IFG) that is extracted from the
system log. The objective of the APT in the game is to choose transitions in
the IFG to find an optimal path in the IFG from an entry point of the attack to
an attack target. On the other hand, the objective of DIFT is to dynamically
select nodes in the IFG to perform security analysis for detecting APT. Our
game model has imperfect information as the players do not have information
about the actions of the opponent. We consider two scenarios of the game (i)
when the false-positive and false-negative rates are known to both players and
(ii) when the false-positive and false-negative rates are unknown to both
players. Case (i) translates to a game model with complete information and we
propose a value iteration-based algorithm and prove the convergence. Case (ii)
translates to a game with unknown transition probabilities. In this case, we
propose Hierarchical Supervised Learning (HSL) algorithm that integrates a
neural network, to predict the value vector of the game, with a policy
iteration algorithm to compute an approximate equilibrium. We implemented our
algorithms on real attack datasets and validated the performance of our
approach.",space,775
http://arxiv.org/abs/2007.05889v5,filtered,arxiv,arxiv,2020-07-12 00:00:00,arxiv,"deep learning techniques to make gravitational wave detections from weak
  time-series data",http://arxiv.org/abs/2007.05889v5,"Gravitational waves are ripples in the space time fabric when high energy
events such as black hole mergers or neutron star collisions take place. The
first Gravitational Wave (GW) detection (GW150914) was made by the Laser
Interferometer Gravitational-wave Observatory (LIGO) and Virgo Collaboration on
September 14, 2015. Furthermore, the proof of the existence of GWs had
countless implications from Stellar Evolution to General Relativity.
Gravitational waves detection requires multiple filters and the filtered data
has to be studied intensively to come to conclusions on whether the data is a
just a glitch or an actual gravitational wave detection. However, with the use
of Deep Learning the process is simplified heavily, as it reduces the level of
filtering greatly, and the output is more definitive, even though the model
produces a probabilistic result. Our technique, Deep Learning, utilizes a
different implementation of a one-dimensional convolutional neural network
(CNN). The model is trained by a composite of real LIGO noise, and injections
of GW waveform templates. The CNN effectively uses classification to
differentiate weak GW time series from non-gaussian noise from glitches in the
LIGO data stream. In addition, we are the first study to utilize fine-tuning as
a means to train the model with a second pass of data, while maintaining all
the learned features from the initial training iteration. This enables our
model to have a sensitivity of 100%, higher than all prior studies in this
field, when making real-time detections of GWs at an extremely low
Signal-to-noise ratios (SNR), while still being less computationally expensive.
This sensitivity, in part, is also achieved through the use of deep signal
manifolds from both the Hanford and Livingston detectors, which enable the
neural network to be responsive to false positives.",space,776
http://arxiv.org/abs/2008.03226v1,filtered,arxiv,arxiv,2020-06-28 00:00:00,arxiv,"the photoswitch dataset: a molecular machine learning benchmark for the
  advancement of synthetic chemistry",http://arxiv.org/abs/2008.03226v1,"The space of synthesizable molecules is greater than $10^{60}$, meaning only
a vanishingly small fraction of these molecules have ever been realized in the
lab. In order to prioritize which regions of this space to explore next,
synthetic chemists need access to accurate molecular property predictions.
While great advances in molecular machine learning have been made, there is a
dearth of benchmarks featuring properties that are useful for the synthetic
chemist. Focussing directly on the needs of the synthetic chemist, we introduce
the Photoswitch Dataset, a new benchmark for molecular machine learning where
improvements in model performance can be immediately observed in the throughput
of promising molecules synthesized in the lab. Photoswitches are a versatile
class of molecule for medical and renewable energy applications where a
molecule's efficacy is governed by its electronic transition wavelengths. We
demonstrate superior performance in predicting these wavelengths compared to
both time-dependent density functional theory (TD-DFT), the incumbent first
principles quantum mechanical approach, as well as a panel of human experts.
Our baseline models are currently being deployed in the lab as part of the
decision process for candidate synthesis. It is our hope that this benchmark
can drive real discoveries in photoswitch chemistry and that future benchmarks
can be introduced to pivot learning algorithm development to benefit more
expansive areas of synthetic chemistry.",space,777
http://arxiv.org/abs/2006.13379v1,filtered,arxiv,arxiv,2020-06-23 00:00:00,arxiv,deep generative model-based quality control for cardiac mri segmentation,http://arxiv.org/abs/2006.13379v1,"In recent years, convolutional neural networks have demonstrated promising
performance in a variety of medical image segmentation tasks. However, when a
trained segmentation model is deployed into the real clinical world, the model
may not perform optimally. A major challenge is the potential poor-quality
segmentations generated due to degraded image quality or domain shift issues.
There is a timely need to develop an automated quality control method that can
detect poor segmentations and feedback to clinicians. Here we propose a novel
deep generative model-based framework for quality control of cardiac MRI
segmentation. It first learns a manifold of good-quality image-segmentation
pairs using a generative model. The quality of a given test segmentation is
then assessed by evaluating the difference from its projection onto the
good-quality manifold. In particular, the projection is refined through
iterative search in the latent space. The proposed method achieves high
prediction accuracy on two publicly available cardiac MRI datasets. Moreover,
it shows better generalisation ability than traditional regression-based
methods. Our approach provides a real-time and model-agnostic quality control
for cardiac MRI segmentation, which has the potential to be integrated into
clinical image analysis workflows.",space,778
http://arxiv.org/abs/2006.13018v3,filtered,arxiv,arxiv,2020-06-21 00:00:00,arxiv,the classification for high-dimension low-sample size data,http://arxiv.org/abs/2006.13018v3,"Huge amount of applications in various fields, such as gene expression
analysis or computer vision, undergo data sets with high-dimensional
low-sample-size (HDLSS), which has putted forward great challenges for standard
statistical and modern machine learning methods. In this paper, we propose a
novel classification criterion on HDLSS, tolerance similarity, which emphasizes
the maximization of within-class variance on the premise of class separability.
According to this criterion, a novel linear binary classifier is designed,
denoted by No-separated Data Maximum Dispersion classifier (NPDMD). The
objective of NPDMD is to find a projecting direction w in which all of training
samples scatter in as large an interval as possible. NPDMD has several
characteristics compared to the state-of-the-art classification methods. First,
it works well on HDLSS. Second, it combines the sample statistical information
and local structural information (supporting vectors) into the objective
function to find the solution of projecting direction in the whole feature
spaces. Third, it solves the inverse of high dimensional matrix in low
dimensional space. Fourth, it is relatively simple to be implemented based on
Quadratic Programming. Fifth, it is robust to the model specification for
various real applications. The theoretical properties of NPDMD are deduced. We
conduct a series of evaluations on one simulated and six real-world benchmark
data sets, including face classification and mRNA classification. NPDMD
outperforms those widely used approaches in most cases, or at least obtains
comparable results.",space,779
http://arxiv.org/abs/2006.04403v1,filtered,arxiv,arxiv,2020-06-08 00:00:00,arxiv,global robustness verification networks,http://arxiv.org/abs/2006.04403v1,"The wide deployment of deep neural networks, though achieving great success
in many domains, has severe safety and reliability concerns. Existing
adversarial attack generation and automatic verification techniques cannot
formally verify whether a network is globally robust, i.e., the absence or not
of adversarial examples in the input space. To address this problem, we develop
a global robustness verification framework with three components: 1) a novel
rule-based ``back-propagation'' finding which input region is responsible for
the class assignment by logic reasoning; 2) a new network architecture Sliding
Door Network (SDN) enabling feasible rule-based ``back-propagation''; 3) a
region-based global robustness verification (RGRV) approach. Moreover, we
demonstrate the effectiveness of our approach on both synthetic and real
datasets.",space,780
http://arxiv.org/abs/2005.10224v2,filtered,arxiv,arxiv,2020-05-20 00:00:00,arxiv,the random feature model for input-output maps between banach spaces,http://arxiv.org/abs/2005.10224v2,"Well known to the machine learning community, the random feature model is a
parametric approximation to kernel interpolation or regression methods. It is
typically used to approximate functions mapping a finite-dimensional input
space to the real line. In this paper, we instead propose a methodology for use
of the random feature model as a data-driven surrogate for operators that map
an input Banach space to an output Banach space. Although the methodology is
quite general, we consider operators defined by partial differential equations
(PDEs); here, the inputs and outputs are themselves functions, with the input
parameters being functions required to specify the problem, such as initial
data or coefficients, and the outputs being solutions of the problem. Upon
discretization, the model inherits several desirable attributes from this
infinite-dimensional viewpoint, including mesh-invariant approximation error
with respect to the true PDE solution map and the capability to be trained at
one mesh resolution and then deployed at different mesh resolutions. We view
the random feature model as a non-intrusive data-driven emulator, provide a
mathematical framework for its interpretation, and demonstrate its ability to
efficiently and accurately approximate the nonlinear parameter-to-solution maps
of two prototypical PDEs arising in physical science and engineering
applications: viscous Burgers' equation and a variable coefficient elliptic
equation.",space,781
http://arxiv.org/abs/2005.08859v2,filtered,arxiv,arxiv,2020-05-18 00:00:00,arxiv,"pde constraints on smooth hierarchical functions computed by neural
  networks",http://arxiv.org/abs/2005.08859v2,"Neural networks are versatile tools for computation, having the ability to
approximate a broad range of functions. An important problem in the theory of
deep neural networks is expressivity; that is, we want to understand the
functions that are computable by a given network. We study real infinitely
differentiable (smooth) hierarchical functions implemented by feedforward
neural networks via composing simpler functions in two cases:
  1) each constituent function of the composition has fewer inputs than the
resulting function;
  2) constituent functions are in the more specific yet prevalent form of a
non-linear univariate function (e.g. tanh) applied to a linear multivariate
function.
  We establish that in each of these regimes there exist non-trivial algebraic
partial differential equations (PDEs), which are satisfied by the computed
functions. These PDEs are purely in terms of the partial derivatives and are
dependent only on the topology of the network. For compositions of polynomial
functions, the algebraic PDEs yield non-trivial equations (of degrees dependent
only on the architecture) in the ambient polynomial space that are satisfied on
the associated functional varieties. Conversely, we conjecture that such PDE
constraints, once accompanied by appropriate non-singularity conditions and
perhaps certain inequalities involving partial derivatives, guarantee that the
smooth function under consideration can be represented by the network. The
conjecture is verified in numerous examples including the case of tree
architectures which are of neuroscientific interest. Our approach is a step
toward formulating an algebraic description of functional spaces associated
with specific neural networks, and may provide new, useful tools for
constructing neural networks.",space,782
http://arxiv.org/abs/2005.07460v1,filtered,arxiv,arxiv,2020-05-15 00:00:00,arxiv,"collective risk minimization via a bayesian model for statistical
  software testing",http://arxiv.org/abs/2005.07460v1,"In the last four years, the number of distinct autonomous vehicles platforms
deployed in the streets of California increased 6-fold, while the reported
accidents increased 12-fold. This can become a trend with no signs of subsiding
as it is fueled by a constant stream of innovations in hardware sensors and
machine learning software. Meanwhile, if we expect the public and regulators to
trust the autonomous vehicle platforms, we need to find better ways to solve
the problem of adding technological complexity without increasing the risk of
accidents. We studied this problem from the perspective of reliability
engineering in which a given risk of an accident has severity and probability
of occurring. Timely information on accidents is important for engineers to
anticipate and reuse previous failures to approximate the risk of accidents in
a new city. However, this is challenging in the context of autonomous vehicles
because of the sparse nature of data on the operational scenarios (driving
trajectories in a new city). Our approach was to mitigate data sparsity by
reducing the state space through monitoring of multiple-vehicles operations. We
then minimized the risk of accidents by determining proper allocation of tests
for each equivalence class. Our contributions comprise (1) a set of strategies
to monitor the operational data of multiple autonomous vehicles, (2) a Bayesian
model that estimates changes in the risk of accidents, and (3) a feedback
control-loop that minimizes these risks by reallocating test effort. Our
results are promising in the sense that we were able to measure and control
risk for a diversity of changes in the operational scenarios. We evaluated our
models with data from two real cities with distinct traffic patterns and made
the data available for the community.",space,783
http://arxiv.org/abs/2004.08771v1,filtered,arxiv,arxiv,2020-04-19 00:00:00,arxiv,heterogeneous cpu+gpu stochastic gradient descent algorithms,http://arxiv.org/abs/2004.08771v1,"The widely-adopted practice is to train deep learning models with specialized
hardware accelerators, e.g., GPUs or TPUs, due to their superior performance on
linear algebra operations. However, this strategy does not employ effectively
the extensive CPU and memory resources -- which are used only for
preprocessing, data transfer, and scheduling -- available by default on the
accelerated servers. In this paper, we study training algorithms for deep
learning on heterogeneous CPU+GPU architectures. Our two-fold objective --
maximize convergence rate and resource utilization simultaneously -- makes the
problem challenging. In order to allow for a principled exploration of the
design space, we first introduce a generic deep learning framework that
exploits the difference in computational power and memory hierarchy between CPU
and GPU through asynchronous message passing. Based on insights gained through
experimentation with the framework, we design two heterogeneous asynchronous
stochastic gradient descent (SGD) algorithms. The first algorithm -- CPU+GPU
Hogbatch -- combines small batches on CPU with large batches on GPU in order to
maximize the utilization of both resources. However, this generates an
unbalanced model update distribution which hinders the statistical convergence.
The second algorithm -- Adaptive Hogbatch -- assigns batches with continuously
evolving size based on the relative speed of CPU and GPU. This balances the
model updates ratio at the expense of a customizable decrease in utilization.
We show that the implementation of these algorithms in the proposed CPU+GPU
framework achieves both faster convergence and higher resource utilization than
TensorFlow on several real datasets and on two computing architectures -- an
on-premises server and a cloud instance.",space,784
http://arxiv.org/abs/2003.09052v2,filtered,arxiv,arxiv,2020-03-20 00:00:00,arxiv,design and operation of the atlas transient science server,http://arxiv.org/abs/2003.09052v2,"The Asteroid Terrestrial impact Last Alert System (ATLAS) system consists of
two 0.5m Schmidt telescopes with cameras covering 29 square degrees at plate
scale of 1.86 arcsec per pixel. Working in tandem, the telescopes routinely
survey the whole sky visible from Hawaii (above $\delta > -50^{\circ}$) every
two nights, exposing four times per night, typically reaching $o < 19$
magnitude per exposure when the moon is illuminated and $c < 19.5$ per exposure
in dark skies. Construction is underway of two further units to be sited in
Chile and South Africa which will result in an all-sky daily cadence from 2021.
Initially designed for detecting potentially hazardous near earth objects, the
ATLAS data enable a range of astrophysical time domain science. To extract
transients from the data stream requires a computing system to process the
data, assimilate detections in time and space and associate them with known
astrophysical sources. Here we describe the hardware and software
infrastructure to produce a stream of clean, real, astrophysical transients in
real time. This involves machine learning and boosted decision tree algorithms
to identify extragalactic and Galactic transients. Typically we detect 10-15
supernova candidates per night which we immediately announce publicly. The
ATLAS discoveries not only enable rapid follow-up of interesting sources but
will provide complete statistical samples within the local volume of 100 Mpc. A
simple comparison of the detected supernova rate within 100 Mpc, with no
corrections for completeness, is already significantly higher (factor 1.5 to 2)
than the current accepted rates.",space,785
http://arxiv.org/abs/2003.00628v3,filtered,arxiv,arxiv,2020-03-02 00:00:00,arxiv,"learning force control for contact-rich manipulation tasks with rigid
  position-controlled robots",http://arxiv.org/abs/2003.00628v3,"Reinforcement Learning (RL) methods have been proven successful in solving
manipulation tasks autonomously. However, RL is still not widely adopted on
real robotic systems because working with real hardware entails additional
challenges, especially when using rigid position-controlled manipulators. These
challenges include the need for a robust controller to avoid undesired
behavior, that risk damaging the robot and its environment, and constant
supervision from a human operator. The main contributions of this work are,
first, we proposed a learning-based force control framework combining RL
techniques with traditional force control. Within said control scheme, we
implemented two different conventional approaches to achieve force control with
position-controlled robots; one is a modified parallel position/force control,
and the other is an admittance control. Secondly, we empirically study both
control schemes when used as the action space of the RL agent. Thirdly, we
developed a fail-safe mechanism for safely training an RL agent on manipulation
tasks using a real rigid robot manipulator. The proposed methods are validated
on simulation and a real robot, an UR3 e-series robotic arm.",space,786
http://arxiv.org/abs/2002.09063v1,filtered,arxiv,arxiv,2020-02-20 00:00:00,arxiv,"real-time optimal guidance and control for interplanetary transfers
  using deep networks",http://arxiv.org/abs/2002.09063v1,"We consider the Earth-Venus mass-optimal interplanetary transfer of a
low-thrust spacecraft and show how the optimal guidance can be represented by
deep networks in a large portion of the state space and to a high degree of
accuracy. Imitation (supervised) learning of optimal examples is used as a
network training paradigm. The resulting models are suitable for an on-board,
real-time, implementation of the optimal guidance and control system of the
spacecraft and are called G&CNETs. A new general methodology called Backward
Generation of Optimal Examples is introduced and shown to be able to
efficiently create all the optimal state action pairs necessary to train
G&CNETs without solving optimal control problems. With respect to previous
works, we are able to produce datasets containing a few orders of magnitude
more optimal trajectories and obtain network performances compatible with real
missions requirements. Several schemes able to train representations of either
the optimal policy (thrust profile) or the value function (optimal mass) are
proposed and tested. We find that both policy learning and value function
learning successfully and accurately learn the optimal thrust and that a
spacecraft employing the learned thrust is able to reach the target conditions
orbit spending only 2 permil more propellant than in the corresponding
mathematically optimal transfer. Moreover, the optimal propellant mass can be
predicted (in case of value function learning) within an error well within 1%.
All G&CNETs produced are tested during simulations of interplanetary transfers
with respect to their ability to reach the target conditions optimally starting
from nominal and off-nominal conditions.",space,787
http://arxiv.org/abs/2001.09346v2,filtered,arxiv,arxiv,2020-01-25 00:00:00,arxiv,"corgan: correlation-capturing convolutional generative adversarial
  networks for generating synthetic healthcare records",http://arxiv.org/abs/2001.09346v2,"Deep learning models have demonstrated high-quality performance in areas such
as image classification and speech processing. However, creating a deep
learning model using electronic health record (EHR) data, requires addressing
particular privacy challenges that are unique to researchers in this domain.
This matter focuses attention on generating realistic synthetic data while
ensuring privacy. In this paper, we propose a novel framework called
correlation-capturing Generative Adversarial Network (CorGAN), to generate
synthetic healthcare records. In CorGAN we utilize Convolutional Neural
Networks to capture the correlations between adjacent medical features in the
data representation space by combining Convolutional Generative Adversarial
Networks and Convolutional Autoencoders. To demonstrate the model fidelity, we
show that CorGAN generates synthetic data with performance similar to that of
real data in various Machine Learning settings such as classification and
prediction. We also give a privacy assessment and report on statistical
analysis regarding realistic characteristics of the synthetic data. The
software of this work is open-source and is available at:
https://github.com/astorfi/cor-gan.",space,788
http://arxiv.org/abs/2001.03864v1,filtered,arxiv,arxiv,2020-01-12 00:00:00,arxiv,"learning to drive via apprenticeship learning and deep reinforcement
  learning",http://arxiv.org/abs/2001.03864v1,"With the implementation of reinforcement learning (RL) algorithms, current
state-of-art autonomous vehicle technology have the potential to get closer to
full automation. However, most of the applications have been limited to game
domains or discrete action space which are far from the real world driving.
Moreover, it is very tough to tune the parameters of reward mechanism since the
driving styles vary a lot among the different users. For instance, an
aggressive driver may prefer driving with high acceleration whereas some
conservative drivers prefer a safer driving style. Therefore, we propose an
apprenticeship learning in combination with deep reinforcement learning
approach that allows the agent to learn the driving and stopping behaviors with
continuous actions. We use gradient inverse reinforcement learning (GIRL)
algorithm to recover the unknown reward function and employ REINFORCE as well
as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal
policy. The performance of our method is evaluated in simulation-based scenario
and the results demonstrate that the agent performs human like driving and even
better in some aspects after training.",space,789
http://arxiv.org/abs/2001.03855v1,filtered,arxiv,arxiv,2020-01-12 00:00:00,arxiv,"hyperparameters optimization for deep learning based emotion prediction
  for human robot interaction",http://arxiv.org/abs/2001.03855v1,"To enable humanoid robots to share our social space we need to develop
technology for easy interaction with the robots using multiple modes such as
speech, gestures and share our emotions with them. We have targeted this
research towards addressing the core issue of emotion recognition problem which
would require less computation resources and much lesser number of network
hyperparameters which will be more adaptive to be computed on low resourced
social robots for real time communication. More specifically, here we have
proposed an Inception module based Convolutional Neural Network Architecture
which has achieved improved accuracy of upto 6% improvement over the existing
network architecture for emotion classification when combinedly tested over
multiple datasets when tried over humanoid robots in real - time. Our proposed
model is reducing the trainable Hyperparameters to an extent of 94% as compared
to vanilla CNN model which clearly indicates that it can be used in real time
based application such as human robot interaction. Rigorous experiments have
been performed to validate our methodology which is sufficiently robust and
could achieve high level of accuracy. Finally, the model is implemented in a
humanoid robot, NAO in real time and robustness of the model is evaluated.",space,790
http://arxiv.org/abs/1912.08202v2,filtered,arxiv,arxiv,2019-12-17 00:00:00,arxiv,"extrinsic kernel ridge regression classifier for planar kendall shape
  space",http://arxiv.org/abs/1912.08202v2,"Kernel methods have had great success in Statistics and Machine Learning.
Despite their growing popularity, however, less effort has been drawn towards
developing kernel based classification methods on Riemannian manifolds due to
difficulty in dealing with non-Euclidean geometry. In this paper, motivated by
the extrinsic framework of manifold-valued data analysis, we propose a new
positive definite kernel on planar Kendall shape space $\Sigma_2^k$, called
extrinsic Veronese Whitney Gaussian kernel. We show that our approach can be
extended to develop Gaussian kernels on any embedded manifold. Furthermore,
kernel ridge regression classifier (KRRC) is implemented to address the shape
classification problem on $\Sigma_2^k$, and their promising performances are
illustrated through the real data analysis.",space,791
http://arxiv.org/abs/1912.03647v2,filtered,arxiv,arxiv,2019-12-08 00:00:00,arxiv,compressing 3dcnns based on tensor train decomposition,http://arxiv.org/abs/1912.03647v2,"Three dimensional convolutional neural networks (3DCNNs) have been applied in
many tasks, e.g., video and 3D point cloud recognition. However, due to the
higher dimension of convolutional kernels, the space complexity of 3DCNNs is
generally larger than that of traditional two dimensional convolutional neural
networks (2DCNNs). To miniaturize 3DCNNs for the deployment in confining
environments such as embedded devices, neural network compression is a
promising approach. In this work, we adopt the tensor train (TT) decomposition,
a straightforward and simple in situ training compression method, to shrink the
3DCNN models. Through proposing tensorizing 3D convolutional kernels in TT
format, we investigate how to select appropriate TT ranks for achieving higher
compression ratio. We have also discussed the redundancy of 3D convolutional
kernels for compression, core significance and future directions of this work,
as well as the theoretical computation complexity versus practical executing
time of convolution in TT. In the light of multiple contrast experiments based
on VIVA challenge, UCF11, and UCF101 datasets, we conclude that TT
decomposition can compress 3DCNNs by around one hundred times without
significant accuracy loss, which will enable its applications in extensive real
world scenarios.",space,792
http://arxiv.org/abs/1911.12672v1,filtered,arxiv,arxiv,2019-11-28 00:00:00,arxiv,"improved cross-validation for classifiers that make algorithmic choices
  to minimise runtime without compromising output correctness",http://arxiv.org/abs/1911.12672v1,"Our topic is the use of machine learning to improve software by making
choices which do not compromise the correctness of the output, but do affect
the time taken to produce such output. We are particularly concerned with
computer algebra systems (CASs), and in particular, our experiments are for
selecting the variable ordering to use when performing a cylindrical algebraic
decomposition of $n$-dimensional real space with respect to the signs of a set
of polynomials.
  In our prior work we explored the different ML models that could be used, and
how to identify suitable features of the input polynomials. In the present
paper we both repeat our prior experiments on problems which have more
variables (and thus exponentially more possible orderings), and examine the
metric which our ML classifiers targets. The natural metric is computational
runtime, with classifiers trained to pick the ordering which minimises this.
However, this leads to the situation were models do not distinguish between any
of the non-optimal orderings, whose runtimes may still vary dramatically. In
this paper we investigate a modification to the cross-validation algorithms of
the classifiers so that they do distinguish these cases, leading to improved
results.",space,793
http://arxiv.org/abs/1910.13676v1,filtered,arxiv,arxiv,2019-10-30 00:00:00,arxiv,multi modal semantic segmentation using synthetic data,http://arxiv.org/abs/1910.13676v1,"Semantic understanding of scenes in three-dimensional space (3D) is a
quintessential part of robotics oriented applications such as autonomous
driving as it provides geometric cues such as size, orientation and true
distance of separation to objects which are crucial for taking mission critical
decisions. As a first step, in this work we investigate the possibility of
semantically classifying different parts of a given scene in 3D by learning the
underlying geometric context in addition to the texture cues BUT in the absence
of labelled real-world datasets. To this end we generate a large number of
synthetic scenes, their pixel-wise labels and corresponding 3D representations
using CARLA software framework. We then build a deep neural network that learns
underlying category specific 3D representation and texture cues from color
information of the rendered synthetic scenes. Further on we apply the learned
model on different real world datasets to evaluate its performance. Our
preliminary investigation of results show that the neural network is able to
learn the geometric context from synthetic scenes and effectively apply this
knowledge to classify each point of a 3D representation of a scene in
real-world.",space,794
http://arxiv.org/abs/2001.09938v1,filtered,arxiv,arxiv,2019-10-22 00:00:00,arxiv,"autonomous discovery of battery electrolytes with robotic
  experimentation and machine-learning",http://arxiv.org/abs/2001.09938v1,"Innovations in batteries take years to formulate and commercialize, requiring
extensive experimentation during the design and optimization phases. We
approached the design and selection of a battery electrolyte through a
black-box optimization algorithm directly integrated into a robotic test-stand.
We report here the discovery of a novel battery electrolyte by this experiment
completely guided by the machine-learning software without human intervention.
Motivated by the recent trend toward super-concentrated aqueous electrolytes
for high-performance batteries, we utilize Dragonfly - a Bayesian
machine-learning software package - to search mixtures of commonly used lithium
and sodium salts for super-concentrated aqueous electrolytes with wide
electrochemical stability windows. Dragonfly autonomously managed the robotic
test-stand, recommending electrolyte designs to test and receiving experimental
feedback in real time. In 40 hours of continuous experimentation over a
four-dimensional design space with millions of potential candidates, Dragonfly
discovered a novel, mixed-anion aqueous sodium electrolyte with a wider
electrochemical stability window than state-of-the-art sodium electrolyte. A
human-guided design process may have missed this optimal electrolyte. This
result demonstrates the possibility of integrating robotics with
machine-learning to rapidly and autonomously discover novel battery materials.",space,795
http://arxiv.org/abs/1910.06893v1,filtered,arxiv,arxiv,2019-10-15 00:00:00,arxiv,"extracting robust and accurate features via a robust information
  bottleneck",http://arxiv.org/abs/1910.06893v1,"We propose a novel strategy for extracting features in supervised learning
that can be used to construct a classifier which is more robust to small
perturbations in the input space. Our method builds upon the idea of the
information bottleneck by introducing an additional penalty term that
encourages the Fisher information of the extracted features to be small, when
parametrized by the inputs. By tuning the regularization parameter, we can
explicitly trade off the opposing desiderata of robustness and accuracy when
constructing a classifier. We derive the optimal solution to the robust
information bottleneck when the inputs and outputs are jointly Gaussian,
proving that the optimally robust features are also jointly Gaussian in that
setting. Furthermore, we propose a method for optimizing a variational bound on
the robust information bottleneck objective in general settings using
stochastic gradient descent, which may be implemented efficiently in neural
networks. Our experimental results for synthetic and real data sets show that
the proposed feature extraction method indeed produces classifiers with
increased robustness to perturbations.",space,796
http://arxiv.org/abs/1909.10707v6,filtered,arxiv,arxiv,2019-09-24 00:00:00,arxiv,"invariant transform experience replay: data augmentation for deep
  reinforcement learning",http://arxiv.org/abs/1909.10707v6,"Deep Reinforcement Learning (RL) is a promising approach for adaptive robot
control, but its current application to robotics is currently hindered by high
sample requirements. To alleviate this issue, we propose to exploit the
symmetries present in robotic tasks. Intuitively, symmetries from observed
trajectories define transformations that leave the space of feasible RL
trajectories invariant and can be used to generate new feasible trajectories,
which could be used for training. Based on this data augmentation idea, we
formulate a general framework, called Invariant Transform Experience Replay
that we present with two techniques: (i) Kaleidoscope Experience Replay
exploits reflectional symmetries and (ii) Goal-augmented Experience Replay
which takes advantage of lax goal definitions. In the Fetch tasks from OpenAI
Gym, our experimental results show significant increases in learning rates and
success rates. Particularly, we attain a 13, 3, and 5 times speedup in the
pushing, sliding, and pick-and-place tasks respectively in the multi-goal
setting. Performance gains are also observed in similar tasks with obstacles
and we successfully deployed a trained policy on a real Baxter robot. Our work
demonstrates that invariant transformations on RL trajectories are a promising
methodology to speed up learning in deep RL.",space,797
http://arxiv.org/abs/1909.08030v2,filtered,arxiv,arxiv,2019-09-17 00:00:00,arxiv,auto-tuning of double dot devices in situ with machine learning,http://arxiv.org/abs/1909.08030v2,"The current practice of manually tuning quantum dots (QDs) for qubit
operation is a relatively time-consuming procedure that is inherently
impractical for scaling up and applications. In this work, we report on the
{\it in situ} implementation of a recently proposed autotuning protocol that
combines machine learning (ML) with an optimization routine to navigate the
parameter space. In particular, we show that a ML algorithm trained using
exclusively simulated data to quantitatively classify the state of a double-QD
device can be used to replace human heuristics in the tuning of gate voltages
in real devices. We demonstrate active feedback of a functional double-dot
device operated at millikelvin temperatures and discuss success rates as a
function of the initial conditions and the device performance. Modifications to
the training network, fitness function, and optimizer are discussed as a path
toward further improvement in the success rate when starting both near and far
detuned from the target double-dot range.",space,798
http://arxiv.org/abs/1909.02583v2,filtered,arxiv,arxiv,2019-09-05 00:00:00,arxiv,"spatiotemporally constrained action space attacks on deep reinforcement
  learning agents",http://arxiv.org/abs/1909.02583v2,"Robustness of Deep Reinforcement Learning (DRL) algorithms towards
adversarial attacks in real world applications such as those deployed in
cyber-physical systems (CPS) are of increasing concern. Numerous studies have
investigated the mechanisms of attacks on the RL agent's state space.
Nonetheless, attacks on the RL agent's action space (AS) (corresponding to
actuators in engineering systems) are equally perverse; such attacks are
relatively less studied in the ML literature. In this work, we first frame the
problem as an optimization problem of minimizing the cumulative reward of an RL
agent with decoupled constraints as the budget of attack. We propose a
white-box Myopic Action Space (MAS) attack algorithm that distributes the
attacks across the action space dimensions. Next, we reformulate the
optimization problem above with the same objective function, but with a
temporally coupled constraint on the attack budget to take into account the
approximated dynamics of the agent. This leads to the white-box Look-ahead
Action Space (LAS) attack algorithm that distributes the attacks across the
action and temporal dimensions. Our results shows that using the same amount of
resources, the LAS attack deteriorates the agent's performance significantly
more than the MAS attack. This reveals the possibility that with limited
resource, an adversary can utilize the agent's dynamics to malevolently craft
attacks that causes the agent to fail. Additionally, we leverage these attack
strategies as a possible tool to gain insights on the potential vulnerabilities
of DRL agents.",space,799
http://arxiv.org/abs/1908.00754v1,filtered,arxiv,arxiv,2019-08-02 00:00:00,arxiv,"a visual technique to analyze flow of information in a machine learning
  system",http://arxiv.org/abs/1908.00754v1,"Machine learning (ML) algorithms and machine learning based software systems
implicitly or explicitly involve complex flow of information between various
entities such as training data, feature space, validation set and results.
Understanding the statistical distribution of such information and how they
flow from one entity to another influence the operation and correctness of such
systems, especially in large-scale applications that perform classification or
prediction in real time. In this paper, we propose a visual approach to
understand and analyze flow of information during model training and serving
phases. We build the visualizations using a technique called Sankey Diagram -
conventionally used to understand data flow among sets - to address various use
cases of in a machine learning system. We demonstrate how the proposed
technique, tweaked and twisted to suit a classification problem, can play a
critical role in better understanding of the training data, the features, and
the classifier performance. We also discuss how this technique enables
diagnostic analysis of model predictions and comparative analysis of
predictions from multiple classifiers. The proposed concept is illustrated with
the example of categorization of millions of products in the e-commerce domain
- a multi-class hierarchical classification problem.",space,800
http://arxiv.org/abs/1907.09209v1,filtered,arxiv,arxiv,2019-07-22 00:00:00,arxiv,"automatic calibration of artificial neural networks for zebrafish
  collective behaviours using a quality diversity algorithm",http://arxiv.org/abs/1907.09209v1,"During the last two decades, various models have been proposed for fish
collective motion. These models are mainly developed to decipher the biological
mechanisms of social interaction between animals. They consider very simple
homogeneous unbounded environments and it is not clear that they can simulate
accurately the collective trajectories. Moreover when the models are more
accurate, the question of their scalability to either larger groups or more
elaborate environments remains open. This study deals with learning how to
simulate realistic collective motion of collective of zebrafish, using
real-world tracking data. The objective is to devise an agent-based model that
can be implemented on an artificial robotic fish that can blend into a
collective of real fish. We present a novel approach that uses Quality
Diversity algorithms, a class of algorithms that emphasise exploration over
pure optimisation. In particular, we use CVT-MAP-Elites, a variant of the
state-of-the-art MAP-Elites algorithm for high dimensional search space.
Results show that Quality Diversity algorithms not only outperform classic
evolutionary reinforcement learning methods at the macroscopic level (i.e.
group behaviour), but are also able to generate more realistic biomimetic
behaviours at the microscopic level (i.e. individual behaviour).",space,801
http://arxiv.org/abs/1905.13118v1,filtered,arxiv,arxiv,2019-05-30 00:00:00,arxiv,"standing on the shoulders of giants: ai-driven calibration of
  localisation technologies",http://arxiv.org/abs/1905.13118v1,"High accuracy localisation technologies exist but are prohibitively expensive
to deploy for large indoor spaces such as warehouses, factories, and
supermarkets to track assets and people. However, these technologies can be
used to lend their highly accurate localisation capabilities to low-cost,
commodity, and less-accurate technologies. In this paper, we bridge this link
by proposing a technology-agnostic calibration framework based on artificial
intelligence to assist such low-cost technologies through highly accurate
localisation systems. A single-layer neural network is used to calibrate less
accurate technology using more accurate one such as BLE using UWB and UWB using
a professional motion tracking system. On a real indoor testbed, we demonstrate
an increase in accuracy of approximately 70% for BLE and 50% for UWB. Not only
the proposed approach requires a very short measurement campaign, the low
complexity of the single-layer neural network also makes it ideal for
deployment on constrained devices typically for localisation purposes.",space,802
http://arxiv.org/abs/1905.09673v2,filtered,arxiv,arxiv,2019-05-23 00:00:00,arxiv,"deep q-learning with q-matrix transfer learning for novel fire
  evacuation environment",http://arxiv.org/abs/1905.09673v2,"We focus on the important problem of emergency evacuation, which clearly
could benefit from reinforcement learning that has been largely unaddressed.
Emergency evacuation is a complex task which is difficult to solve with
reinforcement learning, since an emergency situation is highly dynamic, with a
lot of changing variables and complex constraints that makes it difficult to
train on. In this paper, we propose the first fire evacuation environment to
train reinforcement learning agents for evacuation planning. The environment is
modelled as a graph capturing the building structure. It consists of realistic
features like fire spread, uncertainty and bottlenecks. We have implemented the
environment in the OpenAI gym format, to facilitate future research. We also
propose a new reinforcement learning approach that entails pretraining the
network weights of a DQN based agents to incorporate information on the
shortest path to the exit. We achieved this by using tabular Q-learning to
learn the shortest path on the building model's graph. This information is
transferred to the network by deliberately overfitting it on the Q-matrix.
Then, the pretrained DQN model is trained on the fire evacuation environment to
generate the optimal evacuation path under time varying conditions. We perform
comparisons of the proposed approach with state-of-the-art reinforcement
learning algorithms like PPO, VPG, SARSA, A2C and ACKTR. The results show that
our method is able to outperform state-of-the-art models by a huge margin
including the original DQN based models. Finally, we test our model on a large
and complex real building consisting of 91 rooms, with the possibility to move
to any other room, hence giving 8281 actions. We use an attention based
mechanism to deal with large action spaces. Our model achieves near optimal
performance on the real world emergency environment.",space,803
http://arxiv.org/abs/1904.10698v1,filtered,arxiv,arxiv,2019-04-24 00:00:00,arxiv,multi-scale deep neural networks for real image super-resolution,http://arxiv.org/abs/1904.10698v1,"Single image super-resolution (SR) is extremely difficult if the upscaling
factors of image pairs are unknown and different from each other, which is
common in real image SR. To tackle the difficulty, we develop two multi-scale
deep neural networks (MsDNN) in this work. Firstly, due to the high computation
complexity in high-resolution spaces, we process an input image mainly in two
different downscaling spaces, which could greatly lower the usage of GPU
memory. Then, to reconstruct the details of an image, we design a multi-scale
residual network (MsRN) in the downscaling spaces based on the residual blocks.
Besides, we propose a multi-scale dense network based on the dense blocks to
compare with MsRN. Finally, our empirical experiments show the robustness of
MsDNN for image SR when the upscaling factor is unknown. According to the
preliminary results of NTIRE 2019 image SR challenge, our team
(ZXHresearch@fudan) ranks 21-st among all participants. The implementation of
MsDNN is released https://github.com/shangqigao/gsq-image-SR",space,804
http://arxiv.org/abs/1904.05343v2,filtered,arxiv,arxiv,2019-04-10 00:00:00,arxiv,stegastamp: invisible hyperlinks in physical photographs,http://arxiv.org/abs/1904.05343v2,"Printed and digitally displayed photos have the ability to hide imperceptible
digital data that can be accessed through internet-connected imaging systems.
Another way to think about this is physical photographs that have unique QR
codes invisibly embedded within them. This paper presents an architecture,
algorithms, and a prototype implementation addressing this vision. Our key
technical contribution is StegaStamp, a learned steganographic algorithm to
enable robust encoding and decoding of arbitrary hyperlink bitstrings into
photos in a manner that approaches perceptual invisibility. StegaStamp
comprises a deep neural network that learns an encoding/decoding algorithm
robust to image perturbations approximating the space of distortions resulting
from real printing and photography. We demonstrates real-time decoding of
hyperlinks in photos from in-the-wild videos that contain variation in
lighting, shadows, perspective, occlusion and viewing distance. Our prototype
system robustly retrieves 56 bit hyperlinks after error correction - sufficient
to embed a unique code within every photo on the internet.",space,805
http://arxiv.org/abs/1903.04824v1,filtered,arxiv,arxiv,2019-03-12 00:00:00,arxiv,"proceedings of the fifth international conference on cloud and robotics
  (iccr2018)",http://arxiv.org/abs/1903.04824v1,"The 5th edition of the International Conference on Cloud and Robotics (ICCR
2018 - http://cloudrobotics.info) will be held on November 12-14 2018 in Paris
and Saint-Quentin, France. The conference is a co-event with GDR ALROB and the
industry exposition Robonumerique (http://www.robonumerique.fr).
  The domain of cloud robotics aims to converge robots with computation,
storage and communication resources provided by the cloud. The cloud may
complement robotic resources in several ways, including crowd-sourcing
knowledge databases, context information, computational offloading or
data-intensive information processing for artificial intelligence. Today, the
paradigms of cloud/fog/edge computing propose software architecture solutions
for robots to share computations or offload them to ambiant and networked
resources. Yet, combining distant computations with the real time constraints
of robotics is very challenging. As the challenges in this domain are
multi-disciplinary and similar in other research areas, Cloud Robotics aims at
building bridges among experts from academia and industry working in different
fields, such as robotics, cyber-physical systems, automotive, aerospace,
machine learning, artificial intelligence, software architecture, big data
analytics, Internet-of-Things, networked control and distributed cloud systems.",space,806
http://arxiv.org/abs/1903.03404v2,filtered,arxiv,arxiv,2019-03-08 00:00:00,arxiv,"accelerating generalized linear models with mlweaving: a
  one-size-fits-all system for any-precision learning (technical report)",http://arxiv.org/abs/1903.03404v2,"Learning from the data stored in a database is an important function
increasingly available in relational engines. Methods using lower precision
input data are of special interest given their overall higher efficiency but,
in databases, these methods have a hidden cost: the quantization of the real
value into a smaller number is an expensive step. To address the issue, in this
paper we present MLWeaving, a data structure and hardware acceleration
technique intended to speed up learning of generalized linear models in
databases. ML-Weaving provides a compact, in-memory representation enabling the
retrieval of data at any level of precision. MLWeaving also takes advantage of
the increasing availability of FPGA-based accelerators to provide a highly
efficient implementation of stochastic gradient descent. The solution adopted
in MLWeaving is more efficient than existing designs in terms of space (since
it can process any resolution on the same design) and resources (via the use of
bit-serial multipliers). MLWeaving also enables the runtime tuning of
precision, instead of a fixed precision level during the training. We
illustrate this using a simple, dynamic precision schedule. Experimental
results show MLWeaving achieves up to16 performance improvement over
low-precision CPU implementations of first-order methods.",space,807
http://arxiv.org/abs/1902.06824v2,filtered,arxiv,arxiv,2019-02-18 00:00:00,arxiv,"autonomous airline revenue management: a deep reinforcement learning
  approach to seat inventory control and overbooking",http://arxiv.org/abs/1902.06824v2,"Revenue management can enable airline corporations to maximize the revenue
generated from each scheduled flight departing in their transportation network
by means of finding the optimal policies for differential pricing, seat
inventory control and overbooking. As different demand segments in the market
have different Willingness-To-Pay (WTP), airlines use differential pricing,
booking restrictions, and service amenities to determine different fare classes
or products targeted at each of these demand segments. Because seats are
limited for each flight, airlines also need to allocate seats for each of these
fare classes to prevent lower fare class passengers from displacing higher fare
class ones and set overbooking limits in anticipation of cancellations and
no-shows such that revenue is maximized. Previous work addresses these problems
using optimization techniques or classical Reinforcement Learning methods. This
paper focuses on the latter problem - the seat inventory control problem -
casting it as a Markov Decision Process to be able to find the optimal policy.
Multiple fare classes, concurrent continuous arrival of passengers of different
fare classes, overbooking and random cancellations that are independent of
class have been considered in the model. We have addressed this problem using
Deep Q-Learning with the goal of maximizing the reward for each flight
departure. The implementation of this technique allows us to employ large
continuous state space but also presents the potential opportunity to test on
real time airline data. To generate data and train the agent, a basic
air-travel market simulator was developed. The performance of the agent in
different simulated market scenarios was compared against theoretically optimal
solutions and was found to be nearly close to the expected optimal revenue.",space,808
http://arxiv.org/abs/1901.08548v1,filtered,arxiv,arxiv,2019-01-20 00:00:00,arxiv,a tensorized logic programming language for large-scale data,http://arxiv.org/abs/1901.08548v1,"We introduce a new logic programming language T-PRISM based on tensor
embeddings. Our embedding scheme is a modification of the distribution
semantics in PRISM, one of the state-of-the-art probabilistic logic programming
languages, by replacing distribution functions with multidimensional arrays,
i.e., tensors. T-PRISM consists of two parts: logic programming part and
numerical computation part. The former provides flexible and interpretable
modeling at the level of first order logic, and the latter part provides
scalable computation utilizing parallelization and hardware acceleration with
GPUs. Combing these two parts provides a remarkably wide range of high-level
declarative modeling from symbolic reasoning to deep learning. To embody this
programming language, we also introduce a new semantics, termed tensorized
semantics, which combines the traditional least model semantics in logic
programming with the embeddings of tensors. In T-PRISM, we first derive a set
of equations related to tensors from a given program using logical inference,
i.e., Prolog execution in a symbolic space and then solve the derived equations
in a continuous space by TensorFlow. Using our preliminary implementation of
T-PRISM, we have successfully dealt with a wide range of modeling. We have
succeeded in dealing with real large-scale data in the declarative modeling.
This paper presents a DistMult model for knowledge graphs using the FB15k and
WN18 datasets.",space,809
http://arxiv.org/abs/1901.01632v1,filtered,arxiv,arxiv,2019-01-07 00:00:00,arxiv,atp: a datacenter approximate transmission protocol,http://arxiv.org/abs/1901.01632v1,"Many datacenter applications such as machine learning and streaming systems
do not need the complete set of data to perform their computation. Current
approximate applications in datacenters run on a reliable network layer like
TCP. To improve performance, they either let sender select a subset of data and
transmit them to the receiver or transmit all the data and let receiver drop
some of them. These approaches are network oblivious and unnecessarily transmit
more data, affecting both application runtime and network bandwidth usage. On
the other hand, running approximate application on a lossy network with UDP
cannot guarantee the accuracy of application computation. We propose to run
approximate applications on a lossy network and to allow packet loss in a
controlled manner. Specifically, we designed a new network protocol called
Approximate Transmission Protocol, or ATP, for datacenter approximate
applications. ATP opportunistically exploits available network bandwidth as
much as possible, while performing a loss-based rate control algorithm to avoid
bandwidth waste and re-transmission. It also ensures bandwidth fair sharing
across flows and improves accurate applications' performance by leaving more
switch buffer space to accurate flows. We evaluated ATP with both simulation
and real implementation using two macro-benchmarks and two real applications,
Apache Kafka and Flink. Our evaluation results show that ATP reduces
application runtime by 13.9% to 74.6% compared to a TCP-based solution that
drops packets at sender, and it improves accuracy by up to 94.0% compared to
UDP.",space,810
http://arxiv.org/abs/1811.08716v1,filtered,arxiv,arxiv,2018-11-21 00:00:00,arxiv,autonomous dual-arm manipulation of familiar objects,http://arxiv.org/abs/1811.08716v1,"Autonomous dual-arm manipulation is an essential skill to deploy robots in
unstructured scenarios. However, this is a challenging undertaking,
particularly in terms of perception and planning. Unstructured scenarios are
full of objects with different shapes and appearances that have to be grasped
in a very specific manner so they can be functionally used. In this paper we
present an integrated approach to perform dual-arm pick tasks autonomously. Our
method consists of semantic segmentation, object pose estimation, deformable
model registration, grasp planning and arm trajectory optimization. The entire
pipeline can be executed on-board and is suitable for on-line grasping
scenarios. For this, our approach makes use of accumulated knowledge expressed
as convolutional neural network models and low-dimensional latent shape spaces.
For manipulating objects, we propose a stochastic trajectory optimization that
includes a kinematic chain closure constraint. Evaluation in simulation and on
the real robot corroborates the feasibility and applicability of the proposed
methods on a task of picking up unknown watering cans and drills using both
arms.",space,811
http://arxiv.org/abs/1810.10335v1,filtered,arxiv,arxiv,2018-10-24 00:00:00,arxiv,emulating quantum computation with artificial neural networks,http://arxiv.org/abs/1810.10335v1,"We demonstrate, that artificial neural networks (ANN) can be trained to
emulate single or multiple basic quantum operations. In order to realize a
quantum state, we implement a novel ""quantumness gate"" that maps an arbitrary
matrix to the real representation of a positive hermitean normalized density
matrix. We train the CNOT gate, the Hadamard gate and a rotation in Hilbert
space as basic building blocks for processing the quantum density matrices of
two entangled qubits. During the training process the neural networks learn to
represent the complex structure, the hermiticity, the normalization and the
positivity of the output matrix. The requirement of successful training allows
us to find a critical bottleneck dimension which reflects the relevant quantum
information. Chains of individually trained neural quantum gates can be
constructed to realize any unitary transformation. For scaling to larger
quantum systems, we propose to use correlations of stochastic macroscopic
two-level observables or classical bits. This novel concept provides a path for
a classical implementation of computationally relevant quantum information
processing on classical neural networks, in particular on neuromorphic
computing machines featuring stochastic operations.",space,812
http://arxiv.org/abs/1810.04793v3,filtered,arxiv,arxiv,2018-10-10 00:00:00,arxiv,"patient2vec: a personalized interpretable deep representation of the
  longitudinal electronic health record",http://arxiv.org/abs/1810.04793v3,"The wide implementation of electronic health record (EHR) systems facilitates
the collection of large-scale health data from real clinical settings. Despite
the significant increase in adoption of EHR systems, this data remains largely
unexplored, but presents a rich data source for knowledge discovery from
patient health histories in tasks such as understanding disease correlations
and predicting health outcomes. However, the heterogeneity, sparsity, noise,
and bias in this data present many complex challenges. This complexity makes it
difficult to translate potentially relevant information into machine learning
algorithms. In this paper, we propose a computational framework, Patient2Vec,
to learn an interpretable deep representation of longitudinal EHR data which is
personalized for each patient. To evaluate this approach, we apply it to the
prediction of future hospitalizations using real EHR data and compare its
predictive performance with baseline methods. Patient2Vec produces a vector
space with meaningful structure and it achieves an AUC around 0.799
outperforming baseline methods. In the end, the learned feature importance can
be visualized and interpreted at both the individual and population levels to
bring clinical insights.",space,813
http://arxiv.org/abs/1808.03941v2,filtered,arxiv,arxiv,2018-08-12 00:00:00,arxiv,"denoising of 3-d magnetic resonance images using a residual
  encoder-decoder wasserstein generative adversarial network",http://arxiv.org/abs/1808.03941v2,"Structure-preserved denoising of 3D magnetic resonance imaging (MRI) images
is a critical step in medical image analysis. Over the past few years, many
algorithms with impressive performances have been proposed. In this paper,
inspired by the idea of deep learning, we introduce an MRI denoising method
based on the residual encoder-decoder Wasserstein generative adversarial
network (RED-WGAN). Specifically, to explore the structure similarity between
neighboring slices, a 3D configuration is utilized as the basic processing
unit. Residual autoencoders combined with deconvolution operations are
introduced into the generator network. Furthermore, to alleviate the
oversmoothing shortcoming of the traditional mean squared error (MSE) loss
function, the perceptual similarity, which is implemented by calculating the
distances in the feature space extracted by a pretrained VGG-19 network, is
incorporated with the MSE and adversarial losses to form the new loss function.
Extensive experiments are implemented to assess the performance of the proposed
method. The experimental results show that the proposed RED-WGAN achieves
performance superior to several state-of-the-art methods in both simulated and
real clinical data. In particular, our method demonstrates powerful abilities
in both noise suppression and structure preservation.",space,814
http://arxiv.org/abs/1807.05211v1,filtered,arxiv,arxiv,2018-07-11 00:00:00,arxiv,"learning deployable navigation policies at kilometer scale from a single
  traversal",http://arxiv.org/abs/1807.05211v1,"Model-free reinforcement learning has recently been shown to be effective at
learning navigation policies from complex image input. However, these
algorithms tend to require large amounts of interaction with the environment,
which can be prohibitively costly to obtain on robots in the real world. We
present an approach for efficiently learning goal-directed navigation policies
on a mobile robot, from only a single coverage traversal of recorded data. The
navigation agent learns an effective policy over a diverse action space in a
large heterogeneous environment consisting of more than 2km of travel, through
buildings and outdoor regions that collectively exhibit large variations in
visual appearance, self-similarity, and connectivity. We compare pretrained
visual encoders that enable precomputation of visual embeddings to achieve a
throughput of tens of thousands of transitions per second at training time on a
commodity desktop computer, allowing agents to learn from millions of
trajectories of experience in a matter of hours. We propose multiple forms of
computationally efficient stochastic augmentation to enable the learned policy
to generalise beyond these precomputed embeddings, and demonstrate successful
deployment of the learned policy on the real robot without fine tuning, despite
environmental appearance differences at test time. The dataset and code
required to reproduce these results and apply the technique to other datasets
and robots is made publicly available at rl-navigation.github.io/deployable.",space,815
http://arxiv.org/abs/1806.10409v1,filtered,arxiv,arxiv,2018-06-27 00:00:00,arxiv,"a neuro-inspired system for online learning and recognition of parallel
  spike trains, based on spike latency and heterosynaptic stdp",http://arxiv.org/abs/1806.10409v1,"Humans perform remarkably well in many cognitive tasks including pattern
recognition. However, the neuronal mechanisms underlying this process are not
well understood. Nevertheless, artificial neural networks, inspired in brain
circuits, have been designed and used to tackle spatio-temporal pattern
recognition tasks. In this paper we present a multineuronal spike pattern
detection structure able to autonomously implement online learning and
recognition of parallel spike sequences (i.e., sequences of pulses belonging to
different neurons/neural ensembles). The operating principle of this structure
is based on two spiking/synaptic neurocomputational characteristics: spike
latency, that enables neurons to fire spikes with a certain delay and
heterosynaptic plasticity, that allows the own regulation of synaptic weights.
From the perspective of the information representation, the structure allows
mapping a spatio-temporal stimulus into a multidimensional, temporal, feature
space. In this space, the parameter coordinate and the time at which a neuron
fires represent one specific feature. In this sense, each feature can be
considered to span a single temporal axis. We applied our proposed scheme to
experimental data obtained from a motor inhibitory cognitive task. The test
exhibits good classification performance, indicating the adequateness of our
approach. In addition to its effectiveness, its simplicity and low
computational cost suggest a large scale implementation for real time
recognition applications in several areas, such as brain computer interface,
personal biometrics authentication or early detection of diseases.",space,816
http://arxiv.org/abs/1806.07851v2,filtered,arxiv,arxiv,2018-06-20 00:00:00,arxiv,sim-to-real reinforcement learning for deformable object manipulation,http://arxiv.org/abs/1806.07851v2,"We have seen much recent progress in rigid object manipulation, but
interaction with deformable objects has notably lagged behind. Due to the large
configuration space of deformable objects, solutions using traditional
modelling approaches require significant engineering work. Perhaps then,
bypassing the need for explicit modelling and instead learning the control in
an end-to-end manner serves as a better approach? Despite the growing interest
in the use of end-to-end robot learning approaches, only a small amount of work
has focused on their applicability to deformable object manipulation. Moreover,
due to the large amount of data needed to learn these end-to-end solutions, an
emerging trend is to learn control policies in simulation and then transfer
them over to the real world. To-date, no work has explored whether it is
possible to learn and transfer deformable object policies. We believe that if
sim-to-real methods are to be employed further, then it should be possible to
learn to interact with a wide variety of objects, and not only rigid objects.
In this work, we use a combination of state-of-the-art deep reinforcement
learning algorithms to solve the problem of manipulating deformable objects
(specifically cloth). We evaluate our approach on three tasks --- folding a
towel up to a mark, folding a face towel diagonally, and draping a piece of
cloth over a hanger. Our agents are fully trained in simulation with domain
randomisation, and then successfully deployed in the real world without having
seen any real deformable objects.",space,817
http://arxiv.org/abs/1805.03045v2,filtered,arxiv,arxiv,2018-05-08 00:00:00,arxiv,"a new method for unveiling open clusters in gaia: new nearby open
  clusters confirmed by dr2",http://arxiv.org/abs/1805.03045v2,"The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in
Astronomy. It includes precise astrometric data (positions, proper motions and
parallaxes) for more than $1.3$ billion sources, mostly stars. To analyse such
a vast amount of new data, the use of data mining techniques and machine
learning algorithms are mandatory. The search for Open Clusters, groups of
stars that were born and move together, located in the disk, is a great example
for the application of these techniques. Our aim is to develop a method to
automatically explore the data space, requiring minimal manual intervention. We
explore the performance of a density based clustering algorithm, DBSCAN, to
find clusters in the data together with a supervised learning method such as an
Artificial Neural Network (ANN) to automatically distinguish between real Open
Clusters and statistical clusters. The development and implementation of this
method to a $5$-Dimensional space ($l$, $b$, $\varpi$, $\mu_{\alpha^*}$,
$\mu_\delta$) to the Tycho-Gaia Astrometric Solution (TGAS) data, and a
posterior validation using Gaia DR2 data, lead to the proposal of a set of new
nearby Open Clusters. We have developed a method to find OCs in astrometric
data, designed to be applied to the full Gaia DR2 archive.",space,818
http://arxiv.org/abs/1805.00330v1,filtered,arxiv,arxiv,2018-04-24 00:00:00,arxiv,"real-time human detection as an edge service enabled by a lightweight
  cnn",http://arxiv.org/abs/1805.00330v1,"Edge computing allows more computing tasks to take place on the decentralized
nodes at the edge of networks. Today many delay sensitive, mission-critical
applications can leverage these edge devices to reduce the time delay or even
to enable real time, online decision making thanks to their onsite presence.
Human objects detection, behavior recognition and prediction in smart
surveillance fall into that category, where a transition of a huge volume of
video streaming data can take valuable time and place heavy pressure on
communication networks. It is widely recognized that video processing and
object detection are computing intensive and too expensive to be handled by
resource limited edge devices. Inspired by the depthwise separable convolution
and Single Shot Multi-Box Detector (SSD), a lightweight Convolutional Neural
Network (LCNN) is introduced in this paper. By narrowing down the classifier's
searching space to focus on human objects in surveillance video frames, the
proposed LCNN algorithm is able to detect pedestrians with an affordable
computation workload to an edge device. A prototype has been implemented on an
edge node (Raspberry PI 3) using openCV libraries, and satisfactory performance
is achieved using real world surveillance video streams. The experimental study
has validated the design of LCNN and shown it is a promising approach to
computing intensive applications at the edge.",space,819
http://arxiv.org/abs/1804.04756v1,filtered,arxiv,arxiv,2018-04-13 00:00:00,arxiv,machine learning peeling and loss modelling of time-domain reflectometry,http://arxiv.org/abs/1804.04756v1,"A fundamental pursuit of microwave metrology is the determination of the
characteristic impedance profile of microwave systems. Among other methods,
this can be practically achieved by means of time-domain reflectometry (TDR)
that measures the reflections from a device due to an applied stimulus.
Conventional TDR allows for the measurement of systems comprising a single
impedance. However, real systems typically feature impedance variations that
obscure the determination of all impedances subsequent to the first one. This
problem has been studied previously and is generally known as scattering
inversion or, in the context of microwave metrology, time-domain ""peeling"". In
this article, we demonstrate the implementation of a space-time efficient
peeling algorithm that corrects for the effect of prior impedance mismatch in a
nonuniform lossless transmission line, regardless of the nature of the
stimulus. We generalize TDR measurement analysis by introducing two tools: A
stochastic machine learning clustering tool and an arbitrary lossy transmission
line modeling tool. The former mitigates many of the imperfections typically
plaguing TDR measurements (except for dispersion) and allows for an efficient
processing of large datasets; the latter allows for a complete transmission
line characterization including both conductor and dielectric loss.",space,820
http://arxiv.org/abs/1804.03289v1,filtered,arxiv,arxiv,2018-04-10 00:00:00,arxiv,"planning multi-fingered grasps as probabilistic inference in a learned
  deep network",http://arxiv.org/abs/1804.03289v1,"We propose a novel approach to multi-fingered grasp planning leveraging
learned deep neural network models. We train a convolutional neural network to
predict grasp success as a function of both visual information of an object and
grasp configuration. We can then formulate grasp planning as inferring the
grasp configuration which maximizes the probability of grasp success. We
efficiently perform this inference using a gradient-ascent optimization inside
the neural network using the backpropagation algorithm. Our work is the first
to directly plan high quality multifingered grasps in configuration space using
a deep neural network without the need of an external planner. We validate our
inference method performing both multifinger and two-finger grasps on real
robots. Our experimental results show that our planning method outperforms
existing planning methods for neural networks; while offering several other
benefits including being data-efficient in learning and fast enough to be
deployed in real robotic applications.",space,821
http://arxiv.org/abs/1804.02395v2,filtered,arxiv,arxiv,2018-04-06 00:00:00,arxiv,"structured evolution with compact architectures for scalable policy
  optimization",http://arxiv.org/abs/1804.02395v2,"We present a new method of blackbox optimization via gradient approximation
with the use of structured random orthogonal matrices, providing more accurate
estimators than baselines and with provable theoretical guarantees. We show
that this algorithm can be successfully applied to learn better quality compact
policies than those using standard gradient estimation techniques. The compact
policies we learn have several advantages over unstructured ones, including
faster training algorithms and faster inference. These benefits are important
when the policy is deployed on real hardware with limited resources. Further,
compact policies provide more scalable architectures for derivative-free
optimization (DFO) in high-dimensional spaces. We show that most robotics tasks
from the OpenAI Gym can be solved using neural networks with less than 300
parameters, with almost linear time complexity of the inference phase, with up
to 13x fewer parameters relative to the Evolution Strategies (ES) algorithm
introduced by Salimans et al. (2017). We do not need heuristics such as fitness
shaping to learn good quality policies, resulting in a simple and theoretically
motivated training mechanism.",space,822
http://arxiv.org/abs/1802.03938v1,filtered,arxiv,arxiv,2018-02-12 00:00:00,arxiv,"revisiting the vector space model: sparse weighted nearest-neighbor
  method for extreme multi-label classification",http://arxiv.org/abs/1802.03938v1,"Machine learning has played an important role in information retrieval (IR)
in recent times. In search engines, for example, query keywords are accepted
and documents are returned in order of relevance to the given query; this can
be cast as a multi-label ranking problem in machine learning. Generally, the
number of candidate documents is extremely large (from several thousand to
several million); thus, the classifier must handle many labels. This problem is
referred to as extreme multi-label classification (XMLC). In this paper, we
propose a novel approach to XMLC termed the Sparse Weighted Nearest-Neighbor
Method. This technique can be derived as a fast implementation of
state-of-the-art (SOTA) one-versus-rest linear classifiers for very sparse
datasets. In addition, we show that the classifier can be written as a sparse
generalization of a representer theorem with a linear kernel. Furthermore, our
method can be viewed as the vector space model used in IR. Finally, we show
that the Sparse Weighted Nearest-Neighbor Method can process data points in
real time on XMLC datasets with equivalent performance to SOTA models, with a
single thread and smaller storage footprint. In particular, our method exhibits
superior performance to the SOTA models on a dataset with 3 million labels.",space,823
http://arxiv.org/abs/1801.00864v3,filtered,arxiv,arxiv,2018-01-02 00:00:00,arxiv,"fns: an event-driven spiking neural network simulator based on the lifl
  neuron model",http://arxiv.org/abs/1801.00864v3,"Limitations in processing capabilities and memory of today's computers make
spiking neuron-based (human) whole-brain simulations inevitably characterized
by a compromise between bio-plausibility and computational cost. It translates
into brain models composed of a reduced number of neurons and a simplified
neuron's mathematical model, leading to the search for new simulation
strategies. Taking advantage of the sparse character of brain-like computation,
the event-driven technique could represent a way to carry out efficient
simulation of large-scale Spiking Neural Networks (SNN). The recent Leaky
Integrate-and-Fire with Latency (LIFL) spiking neuron model is event-driven
compatible and exhibits some realistic neuronal features, opening new avenues
for brain modelling. In this paper we introduce FNS, the first LIFL-based
spiking neural network framework, which combines spiking/synaptic neural
modelling with the event-driven approach, allowing us to define heterogeneous
neuron modules and multi-scale connectivity with delayed connections and
plastic synapses. In order to allow multi-thread implementations a novel
parallelization strategy is also introduced. This paper presents mathematical
models, software implementation and simulation routines on which FNS is based.
Finally, a brain subnetwork is modeled on the basis of real brain structural
data, and the resulting simulated activity is compared with associated brain
functional (source-space MEG) data, demonstrating a good matching between the
activity of the model and that of the experimetal data. This work aims to lay
the groundwork for future event-driven based personalised brain models.",space,824
http://arxiv.org/abs/1712.02294v4,filtered,arxiv,arxiv,2017-12-06 00:00:00,arxiv,joint 3d proposal generation and object detection from view aggregation,http://arxiv.org/abs/1712.02294v4,"We present AVOD, an Aggregate View Object Detection network for autonomous
driving scenarios. The proposed neural network architecture uses LIDAR point
clouds and RGB images to generate features that are shared by two subnetworks:
a region proposal network (RPN) and a second stage detector network. The
proposed RPN uses a novel architecture capable of performing multimodal feature
fusion on high resolution feature maps to generate reliable 3D object proposals
for multiple object classes in road scenes. Using these proposals, the second
stage detection network performs accurate oriented 3D bounding box regression
and category classification to predict the extents, orientation, and
classification of objects in 3D space. Our proposed architecture is shown to
produce state of the art results on the KITTI 3D object detection benchmark
while running in real time with a low memory footprint, making it a suitable
candidate for deployment on autonomous vehicles. Code is at:
https://github.com/kujason/avod",space,825
http://arxiv.org/abs/1702.06329v1,filtered,arxiv,arxiv,2017-02-21 00:00:00,arxiv,"towards a common implementation of reinforcement learning for multiple
  robotic tasks",http://arxiv.org/abs/1702.06329v1,"Mobile robots are increasingly being employed for performing complex tasks in
dynamic environments. Reinforcement learning (RL) methods are recognized to be
promising for specifying such tasks in a relatively simple manner. However, the
strong dependency between the learning method and the task to learn is a
well-known problem that restricts practical implementations of RL in robotics,
often requiring major modifications of parameters and adding other techniques
for each particular task. In this paper we present a practical core
implementation of RL which enables the learning process for multiple robotic
tasks with minimal per-task tuning or none. Based on value iteration methods,
this implementation includes a novel approach for action selection, called
Q-biased softmax regression (QBIASSR), which avoids poor performance of the
learning process when the robot reaches new unexplored states. Our approach
takes advantage of the structure of the state space by attending the physical
variables involved (e.g., distances to obstacles, X,Y,{\theta} pose, etc.),
thus experienced sets of states may favor the decision-making process of
unexplored or rarely-explored states. This improvement has a relevant role in
reducing the tuning of the algorithm for particular tasks. Experiments with
real and simulated robots, performed with the software framework also
introduced here, show that our implementation is effectively able to learn
different robotic tasks without tuning the learning method. Results also
suggest that the combination of true online SARSA({\lambda}) with QBIASSR can
outperform the existing RL core algorithms in low-dimensional robotic tasks.",space,826
http://arxiv.org/abs/1701.08748v3,filtered,arxiv,arxiv,2017-01-30 00:00:00,arxiv,"on the realistic validation of photometric redshifts, or why teddy will
  never be happy",http://arxiv.org/abs/1701.08748v3,"Two of the main problems encountered in the development and accurate
validation of photometric redshift (photo-z) techniques are the lack of
spectroscopic coverage in feature space (e.g. colours and magnitudes) and the
mismatch between photometric error distributions associated with the
spectroscopic and photometric samples. Although these issues are well known,
there is currently no standard benchmark allowing a quantitative analysis of
their impact on the final photo-z estimation. In this work, we present two
galaxy catalogues, Teddy and Happy, built to enable a more demanding and
realistic test of photo-z methods. Using photometry from the Sloan Digital Sky
Survey and spectroscopy from a collection of sources, we constructed datasets
which mimic the biases between the underlying probability distribution of the
real spectroscopic and photometric sample. We demonstrate the potential of
these catalogues by submitting them to the scrutiny of different photo-z
methods, including machine learning (ML) and template fitting approaches.
Beyond the expected bad results from most ML algorithms for cases with missing
coverage in feature space, we were able to recognize the superiority of global
models in the same situation and the general failure across all types of
methods when incomplete coverage is convoluted with the presence of photometric
errors - a data situation which photo-z methods were not trained to deal with
up to now and which must be addressed by future large scale surveys. Our
catalogues represent the first controlled environment allowing a
straightforward implementation of such tests. The data are publicly available
within the COINtoolbox (https://github.com/COINtoolbox/photoz_catalogues).",space,827
http://arxiv.org/abs/1610.07862v2,filtered,arxiv,arxiv,2016-10-24 00:00:00,arxiv,intelligence in artificial intelligence,http://arxiv.org/abs/1610.07862v2,"The elusive quest for intelligence in artificial intelligence prompts us to
consider that instituting human-level intelligence in systems may be (still) in
the realm of utopia. In about a quarter century, we have witnessed the winter
of AI (1990) being transformed and transported to the zenith of tabloid fodder
about AI (2015). The discussion at hand is about the elements that constitute
the canonical idea of intelligence. The delivery of intelligence as a
pay-per-use-service, popping out of an app or from a shrink-wrapped software
defined point solution, is in contrast to the bio-inspired view of intelligence
as an outcome, perhaps formed from a tapestry of events, cross-pollinated by
instances, each with its own microcosm of experiences and learning, which may
not be discrete all-or-none functions but continuous, over space and time. The
enterprise world may not require, aspire or desire such an engaged solution to
improve its services for enabling digital transformation through the deployment
of digital twins, for example. One might ask whether the ""work-flow on
steroids"" version of decision support may suffice for intelligence? Are we
harking back to the era of rule based expert systems? The image conjured by the
publicity machines offers deep solutions with human-level AI and preposterous
claims about capturing the ""brain in a box"" by 2020. Even emulating insects may
be difficult in terms of real progress. Perhaps we can try to focus on worms
(Caenorhabditis elegans) which may be better suited for what business needs to
quench its thirst for so-called intelligence in AI.",space,828
http://arxiv.org/abs/1609.08018v1,filtered,arxiv,arxiv,2016-09-26 00:00:00,arxiv,"small near-earth asteroids in the palomar transient factory survey: a
  real-time streak-detection system",http://arxiv.org/abs/1609.08018v1,"Near-Earth asteroids (NEAs) in the 1-100 meter size range are estimated to be
$\sim$1,000 times more numerous than the $\sim$15,000 currently-catalogued
NEAs, most of which are in the 0.5-10 kilometer size range. Impacts from 10-100
meter size NEAs are not statistically life-threatening but may cause
significant regional damage, while 1-10 meter size NEAs with low velocities
relative to Earth are compelling targets for space missions. We describe the
implementation and initial results of a real-time NEA-discovery system
specialized for the detection of small, high angular rate (visually-streaked)
NEAs in Palomar Transient Factory (PTF) images. PTF is a 1.2-m aperture,
7.3-deg$^2$ field-of-view optical survey designed primarily for the discovery
of extragalactic transients (e.g., supernovae) in 60-second exposures reaching
$\sim$20.5 visual magnitude. Our real-time NEA discovery pipeline uses a
machine-learned classifier to filter a large number of false-positive streak
detections, permitting a human scanner to efficiently and remotely identify
real asteroid streaks during the night. Upon recognition of a streaked NEA
detection (typically within an hour of the discovery exposure), the scanner
triggers follow-up with the same telescope and posts the observations to the
Minor Planet Center for worldwide confirmation. We describe our ten initial
confirmed discoveries, all small NEAs that passed 0.3-15 lunar distances from
Earth. Lastly, we derive useful scaling laws for comparing
streaked-NEA-detection capabilities of different surveys as a function of their
hardware and survey-pattern characteristics. This work most directly informs
estimates of the streak-detection capabilities of the Zwicky Transient Facility
(ZTF, planned to succeed PTF in 2017), which will apply PTF's current
resolution and sensitivity over a 47-deg$^2$ field-of-view.",space,829
http://arxiv.org/abs/1601.04385v1,filtered,arxiv,arxiv,2016-01-18 00:00:00,arxiv,real-time data mining of massive data streams from synoptic sky surveys,http://arxiv.org/abs/1601.04385v1,"The nature of scientific and technological data collection is evolving
rapidly: data volumes and rates grow exponentially, with increasing complexity
and information content, and there has been a transition from static data sets
to data streams that must be analyzed in real time. Interesting or anomalous
phenomena must be quickly characterized and followed up with additional
measurements via optimal deployment of limited assets. Modern astronomy
presents a variety of such phenomena in the form of transient events in digital
synoptic sky surveys, including cosmic explosions (supernovae, gamma ray
bursts), relativistic phenomena (black hole formation, jets), potentially
hazardous asteroids, etc. We have been developing a set of machine learning
tools to detect, classify and plan a response to transient events for astronomy
applications, using the Catalina Real-time Transient Survey (CRTS) as a
scientific and methodological testbed. The ability to respond rapidly to the
potentially most interesting events is a key bottleneck that limits the
scientific returns from the current and anticipated synoptic sky surveys.
Similar challenge arise in other contexts, from environmental monitoring using
sensor networks to autonomous spacecraft systems. Given the exponential growth
of data rates, and the time-critical response, we need a fully automated and
robust approach. We describe the results obtained to date, and the possible
future developments.",space,830
http://arxiv.org/abs/1510.02055v1,filtered,arxiv,arxiv,2015-10-07 00:00:00,arxiv,"diverse large-scale its dataset created from continuous learning for
  real-time vehicle detection",http://arxiv.org/abs/1510.02055v1,"In traffic engineering, vehicle detectors are trained on limited datasets
resulting in poor accuracy when deployed in real world applications. Annotating
large-scale high quality datasets is challenging. Typically, these datasets
have limited diversity; they do not reflect the real-world operating
environment. There is a need for a large-scale, cloud based positive and
negative mining (PNM) process and a large-scale learning and evaluation system
for the application of traffic event detection. The proposed positive and
negative mining process addresses the quality of crowd sourced ground truth
data through machine learning review and human feedback mechanisms. The
proposed learning and evaluation system uses a distributed cloud computing
framework to handle data-scaling issues associated with large numbers of
samples and a high-dimensional feature space. The system is trained using
AdaBoost on $1,000,000$ Haar-like features extracted from $70,000$ annotated
video frames. The trained real-time vehicle detector achieves an accuracy of at
least $95\%$ for $1/2$ and about $78\%$ for $19/20$ of the time when tested on
approximately $7,500,000$ video frames. At the end of 2015, the dataset is
expect to have over one billion annotated video frames.",space,831
http://arxiv.org/abs/1508.00317v1,filtered,arxiv,arxiv,2015-08-03 00:00:00,arxiv,"time-series modeling with undecimated fully convolutional neural
  networks",http://arxiv.org/abs/1508.00317v1,"We present a new convolutional neural network-based time-series model.
Typical convolutional neural network (CNN) architectures rely on the use of
max-pooling operators in between layers, which leads to reduced resolution at
the top layers. Instead, in this work we consider a fully convolutional network
(FCN) architecture that uses causal filtering operations, and allows for the
rate of the output signal to be the same as that of the input signal. We
furthermore propose an undecimated version of the FCN, which we refer to as the
undecimated fully convolutional neural network (UFCNN), and is motivated by the
undecimated wavelet transform. Our experimental results verify that using the
undecimated version of the FCN is necessary in order to allow for effective
time-series modeling. The UFCNN has several advantages compared to other
time-series models such as the recurrent neural network (RNN) and long
short-term memory (LSTM), since it does not suffer from either the vanishing or
exploding gradients problems, and is therefore easier to train. Convolution
operations can also be implemented more efficiently compared to the recursion
that is involved in RNN-based models. We evaluate the performance of our model
in a synthetic target tracking task using bearing only measurements generated
from a state-space model, a probabilistic modeling of polyphonic music
sequences problem, and a high frequency trading task using a time-series of
ask/bid quotes and their corresponding volumes. Our experimental results using
synthetic and real datasets verify the significant advantages of the UFCNN
compared to the RNN and LSTM baselines.",space,832
http://arxiv.org/abs/1407.3502v1,filtered,arxiv,arxiv,2014-07-13 00:00:00,arxiv,"automated real-time classification and decision making in massive data
  streams from synoptic sky surveys",http://arxiv.org/abs/1407.3502v1,"The nature of scientific and technological data collection is evolving
rapidly: data volumes and rates grow exponentially, with increasing complexity
and information content, and there has been a transition from static data sets
to data streams that must be analyzed in real time. Interesting or anomalous
phenomena must be quickly characterized and followed up with additional
measurements via optimal deployment of limited assets. Modern astronomy
presents a variety of such phenomena in the form of transient events in digital
synoptic sky surveys, including cosmic explosions (supernovae, gamma ray
bursts), relativistic phenomena (black hole formation, jets), potentially
hazardous asteroids, etc. We have been developing a set of machine learning
tools to detect, classify and plan a response to transient events for astronomy
applications, using the Catalina Real-time Transient Survey (CRTS) as a
scientific and methodological testbed. The ability to respond rapidly to the
potentially most interesting events is a key bottleneck that limits the
scientific returns from the current and anticipated synoptic sky surveys.
Similar challenge arise in other contexts, from environmental monitoring using
sensor networks to autonomous spacecraft systems. Given the exponential growth
of data rates, and the time-critical response, we need a fully automated and
robust approach. We describe the results obtained to date, and the possible
future developments.",space,833
http://arxiv.org/abs/0908.4564v1,filtered,arxiv,arxiv,2009-08-31 00:00:00,arxiv,theory and modeling of the magnetic field measurement in lisa pathfinder,http://arxiv.org/abs/0908.4564v1,"The magnetic diagnostics subsystem of the LISA Technology Package (LTP) on
board the LISA PathFinder (LPF) spacecraft includes a set of four tri-axial
fluxgate magnetometers, intended to measure with high precision the magnetic
field at their respective positions. However, their readouts do not provide a
direct measurement of the magnetic field at the positions of the test masses,
and hence an interpolation method must be designed and implemented to obtain
the values of the magnetic field at these positions. However, such
interpolation process faces serious difficulties. Indeed, the size of the
interpolation region is excessive for a linear interpolation to be reliable
while, on the other hand, the number of magnetometer channels does not provide
sufficient data to go beyond the linear approximation. We describe an
alternative method to address this issue, by means of neural network
algorithms. The key point in this approach is the ability of neural networks to
learn from suitable training data representing the behavior of the magnetic
field. Despite the relatively large distance between the test masses and the
magnetometers, and the insufficient number of data channels, we find that our
artificial neural network algorithm is able to reduce the estimation errors of
the field and gradient down to levels below 10%, a quite satisfactory result.
Learning efficiency can be best improved by making use of data obtained in
on-ground measurements prior to mission launch in all relevant satellite
locations and in real operation conditions. Reliable information on that
appears to be essential for a meaningful assessment of magnetic noise in the
LTP.",space,834
http://arxiv.org/abs/0908.3934v3,filtered,arxiv,arxiv,2009-08-27 00:00:00,arxiv,"a framework for simulating and estimating the state and functional
  topology of complex dynamic geometric networks",http://arxiv.org/abs/0908.3934v3,"We present a framework for simulating signal propagation in geometric
networks (i.e. networks that can be mapped to geometric graphs in some space)
and for developing algorithms that estimate (i.e. map) the state and functional
topology of complex dynamic geometric net- works. Within the framework we
define the key features typically present in such networks and of particular
relevance to biological cellular neural networks: Dynamics, signaling,
observation, and control. The framework is particularly well-suited for
estimating functional connectivity in cellular neural networks from
experimentally observable data, and has been implemented using graphics
processing unit (GPU) high performance computing. Computationally, the
framework can simulate cellular network signaling close to or faster than real
time. We further propose a standard test set of networks to measure performance
and compare different mapping algorithms.",space,835
http://arxiv.org/abs/math/0401157v1,filtered,arxiv,arxiv,2004-01-14 00:00:00,arxiv,generalized psk in space time coding,http://arxiv.org/abs/math/0401157v1,"A wireless communication system using multiple antennas promises reliable
transmission under Rayleigh flat fading assumptions. Design criteria and
practical schemes have been presented for both coherent and non-coherent
communication channels. In this paper we generalize one dimensional phase shift
keying (PSK) signals and introduce space time constellations from generalized
phase shift keying (GPSK) signals based on the complex and real orthogonal
designs. The resulting space time constellations reallocate the energy for each
transmitting antenna and feature good diversity products, consequently their
performances are better than some of the existing comparable codes. Moreover
since the maximum likelihood (ML) decoding of our proposed codes can be
decomposed to one dimensional PSK signal demodulation, the ML decoding of our
codes can be implemented in a very efficient way.",space,836
10.1016/j.future.2022.02.008,filtered,Future Generation Computer Systems,scopus,2022-07-01,sciencedirect,hatch: self-distributing systems for data centers,https://api.elsevier.com/content/abstract/scopus_id/85125223395,"Designing and maintaining distributed systems remains highly challenging: there is a high-dimensional design space of potential ways to distribute a system’s sub-components over a large-scale infrastructure; and the deployment environment for a system tends to change in unforeseen ways over time. For engineers, this is a complex prediction problem to gauge which distributed design may best suit a given environment. We present the concept of self-distributing systems, in which any local system built using our framework can learn, at runtime, the most appropriate distributed design given its perceived operating conditions. Our concept abstracts distribution of a system’s sub-components to a list of simple actions in a reward matrix of distributed design alternatives to be used by reinforcement learning algorithms. By doing this, we enable software to experiment, in a live production environment, with different ways in which to distribute its software modules by placing them in different hosts throughout the system’s infrastructure. We implement this concept in a framework we call Hatch, which has three major elements: (i) a transparent and generalized RPC layer that supports seamless relocation of any local component to a remote host during execution; (ii) a set of primitives, including relocation, replication and sharding, from which to create an action/reward matrix of possible distributed designs of a system; and (iii) a decentralized reinforcement learning approach to converge towards more optimal designs in real time. Using an example of a self-distributing web-serving infrastructure, Hatch is able to autonomously select the most suitable distributed design from among 
                        ≈
                     700,000 alternatives in about 5 min.",space,837
10.1016/j.jss.2022.111231,filtered,Journal of Systems and Software,scopus,2022-05-01,sciencedirect,discovering boundary values of feature-based machine learning classifiers through exploratory datamorphic testing,https://api.elsevier.com/content/abstract/scopus_id/85124473834,"Testing has been widely recognised as difficult for AI applications. This paper proposes a set of testing strategies for testing machine learning applications in the framework of the datamorphism testing methodology. In these strategies, testing aims at exploring the data space of a classification or clustering application to discover the boundaries between classes that the machine learning application defines. This enables the tester to understand precisely the behaviour and function of the software under test. In the paper, three variants of exploratory strategies are presented with the algorithms implemented in the automated datamorphic testing tool Morphy. The correctness of these algorithms are formally proved. Their capability and cost of discovering borders between classes are evaluated via a set of controlled experiments with manually designed subjects and a set of case studies with real machine learning models.",space,838
10.1016/j.conengprac.2021.105046,filtered,Control Engineering Practice,scopus,2022-04-01,sciencedirect,deep reinforcement learning with shallow controllers: an experimental application to pid tuning,https://api.elsevier.com/content/abstract/scopus_id/85122624409,"Deep reinforcement learning (RL) is an optimization-driven framework for producing control strategies for general dynamical systems without explicit reliance on process models. Good results have been reported in simulation. Here we demonstrate the challenges in implementing a state of the art deep RL algorithm on a real physical system. Aspects include the interplay between software and existing hardware; experiment design and sample efficiency; training subject to input constraints; and interpretability of the algorithm and control law. At the core of our approach is the use of a PID controller as the trainable RL policy. In addition to its simplicity, this approach has several appealing features: No additional hardware needs to be added to the control system, since a PID controller can easily be implemented through a standard programmable logic controller; the control law can easily be initialized in a “safe” region of the parameter space; and the final product—a well-tuned PID controller—has a form that practitioners can reason about and deploy with confidence.",space,839
10.1016/j.jbi.2022.103996,filtered,Journal of Biomedical Informatics,scopus,2022-03-01,sciencedirect,evaluating pointwise reliability of machine learning prediction,https://api.elsevier.com/content/abstract/scopus_id/85123373748,"Interest in Machine Learning applications to tackle clinical and biological problems is increasing. This is driven by promising results reported in many research papers, the increasing number of AI-based software products, and by the general interest in Artificial Intelligence to solve complex problems. It is therefore of importance to improve the quality of machine learning output and add safeguards to support their adoption. In addition to regulatory and logistical strategies, a crucial aspect is to detect when a Machine Learning model is not able to generalize to new unseen instances, which may originate from a population distant to that of the training population or from an under-represented subpopulation. As a result, the prediction of the machine learning model for these instances may be often wrong, given that the model is applied outside its “reliable” space of work, leading to a decreasing trust of the final users, such as clinicians. For this reason, when a model is deployed in practice, it would be important to advise users when the model’s predictions may be unreliable, especially in high-stakes applications, including those in healthcare. Yet, reliability assessment of each machine learning prediction is still poorly addressed.
                  Here, we review approaches that can support the identification of unreliable predictions, we harmonize the notation and terminology of relevant concepts, and we highlight and extend possible interrelationships and overlap among concepts. We then demonstrate, on simulated and real data for ICU in-hospital death prediction, a possible integrative framework for the identification of reliable and unreliable predictions. To do so, our proposed approach implements two complementary principles, namely the density principle and the local fit principle. The density principle verifies that the instance we want to evaluate is similar to the training set. The local fit principle verifies that the trained model performs well on training subsets that are more similar to the instance under evaluation. Our work can contribute to consolidating work in machine learning especially in medicine.",space,840
10.1016/j.engappai.2021.104514,filtered,Engineering Applications of Artificial Intelligence,scopus,2022-01-01,sciencedirect,instance-based defense against adversarial attacks in deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85118104144,"Deep Reinforcement Learning systems are now a hot topic in Machine Learning for their effectiveness in many complex tasks, but their application in safety-critical domains (e.g., robot control or self-autonomous driving) remains dangerous without mechanism to detect and prevent risk situations. In Deep RL, such risk is mostly in the form of adversarial attacks, which introduce small perturbations to sensor inputs with the aim of changing the network-based decisions and thus cause catastrophic situations. In the light of these dangers, a promising line of research is that of providing these Deep RL algorithms with suitable defenses, especially when deploying in real environments. This paper suggests that this line of research could be greatly improved by the concepts from the existing research field of Safe Reinforcement Learning, which has been postulated as a family of RL algorithms capable of providing defenses against many forms of risks. However, the connections between Safe RL and the design of defenses against adversarial attacks in Deep RL remain largely unexplored. This paper seeks to explore precisely some of these connections. In particular, this paper proposes to reuse some of the concepts from existing Safe RL algorithms to create a novel and effective instance-based defense for the deployment stage of Deep RL policies. The proposed algorithm uses a risk function based on how far a state is from the state space known by the agent, that allows identifying and preventing adversarial situations. The success of the proposed defense has been evaluated in 4 Atari games.",space,841
10.1016/j.comnet.2021.108560,filtered,Computer Networks,scopus,2021-12-24,sciencedirect,distributed scheduling method for multiple workflows with parallelism prediction and dag prioritizing for time constrained cloud applications,https://api.elsevier.com/content/abstract/scopus_id/85118887015,"Fog computing is an emerging popular paradigm that extends the availability of resources to the network's edge in order to improve the quality metrics of existing Cloud-based applications. However, scheduling workflow applications with time-constraints are complex regarding the count of resources, physical topology of clusters, and the structure of the task graph of the workflows. Adding Fog resources to the intricate problem space of Cloud-based scheduling needs even more time-consuming and complicated algorithms. In this paper, a multi-criteria Mamdani fuzzy algorithm is proposed to analyze the workflow graphs with the assistance of a Long-Short Term Memory neural network parallelism prediction module. The group-based priority assignment schema performed by the fuzzy inference system assigns a priority value to workflows to indicate the relative precedence of requests. Distributed schedulers then send the workflows to target sites according to their current workloads. The whole process is performed in a decentralized manner to prevent any bottlenecks. We have used an extensive software simulation study to compare the proposed algorithm in real workloads with two recent and notable algorithms. The simulation results confirm the proposed algorithm's superiority in fulfilling time-constraints, resource utilization, and overall application scheduling success rate.",space,842
10.1016/j.softx.2021.100854,filtered,SoftwareX,scopus,2021-12-01,sciencedirect,microvip: microscopy image simulation on the virtual imaging platform,https://api.elsevier.com/content/abstract/scopus_id/85119052033,"MicroVIP is an open source software that assembles, in a unified web-application running on distributed computing resources, simulators of the main fluorescent microscopy imaging modalities (with existing codes or newly developed). MicroVIP provides realistic simulated images including several sources of noise (microfluidic blur effect, diffraction, Poisson noise, camera read out noise). MicroVIP also includes a module which simulates single cells with fluorescent markers and a module to analyze the simulated images with textural and pointillist feature spaces. MicroVIP is shown to be of value for supervised machine learning. It allow to automatically generate large sets of training images and virtual instrumentation to optimize the optical parameters before realizing real experiments.",space,843
10.1016/j.eswa.2021.115498,filtered,Expert Systems with Applications,scopus,2021-12-01,sciencedirect,real-time human pose estimation on a smart walker using convolutional neural networks,https://api.elsevier.com/content/abstract/scopus_id/85109217957,"Rehabilitation is important to improve quality of life for mobility-impaired patients. Smart walkers are a commonly used solution that should embed automatic and objective tools for data-driven human-in-the-loop control and monitoring. However, present solutions focus on extracting few specific metrics from dedicated sensors with no unified full-body approach. We investigate a general, real-time, full-body pose estimation framework based on two RGB+D camera streams with non-overlapping views mounted on a smart walker equipment used in rehabilitation. Human keypoint estimation is performed using a two-stage neural network framework. The 2D-Stage implements a detection module that locates body keypoints in the 2D image frames. The 3D-Stage implements a regression module that lifts and relates the detected keypoints in both cameras to the 3D space relative to the walker. Model predictions are low-pass filtered to improve temporal consistency. A custom acquisition method was used to obtain a dataset, with 14 healthy subjects, used for training and evaluating the proposed framework offline, which was then deployed on the real walker equipment. An overall keypoint detection error of 3.73 pixels for the 2D-Stage and 44.05 mm for the 3D-Stage were reported, with an inference time of 26.6 ms when deployed on the constrained hardware of the walker. We present a novel approach to patient monitoring and data-driven human-in-the-loop control in the context of smart walkers. It is able to extract a complete and compact body representation in real-time and from inexpensive sensors, serving as a common base for downstream metrics extraction solutions, and Human-Robot interaction applications. Despite promising results, more data should be collected on users with impairments, to assess its performance as a rehabilitation tool in real-world scenarios.",space,844
10.1016/j.actaastro.2021.07.012,filtered,Acta Astronautica,scopus,2021-11-01,sciencedirect,"a review of space surgery - what have we achieved, current challenges, and future prospects",https://api.elsevier.com/content/abstract/scopus_id/85110745640,"Major surgical events/incidents onboard are rare but can be catastrophic to any mission. National Aeronautics and Space Administration (NASA) uses the Integrated Medical Model (IMM) to develop an integrated, quantified, evidence-based decision support tool useful for crew health and mission planners to assess risk and design medical systems. In 2017, the IMM of the NASA Human Research Program included a list of 100 medical conditions that could be anticipated during space flight. Of those conditions, 27 are expected to need surgical treatment. Consequently, there has been a continuing interest in surgical capabilities for exploration space flight. The surgical system capabilities aboard all space stations and analogue flights have been designed and implemented with an emphasis on stabilisation, medical evacuation, and ATLS capabilities. However, with future missions to the Moon and Mars, evacuation is not a possibility and astronauts will need to troubleshoot, adapt, and self-administer complex surgical care autonomously.
                  This narrative review aims to examine the published work on surgical care in space, discuss the inherent challenges, and identify scope for future studies. The review evaluates and analyses results from several landmark experiments covering important technical aspects such as basic surgical skills, laparoscopic surgery, robotic surgery, and tele surgery. Relevant studies for the review were identified from the MEDLINE, PubMed, and EMBASE databases. Eligible studies were published between 1960 and June 2021 and were identified using the terms “space surgery”, “microgravity”, “zero gravity”, “weightlessness”, “parabolic flight”, “neutral buoyancy”, and “spaceflight”. Only articles in English were selected and references cited in the selected publications were followed up and included where appropriate. Documents available in the public domain and/or archives of National Space agencies were also included. The search yielded a total of 86 hits including review articles, commentaries, studies, meeting summaries and technical reports submitted to National Space agencies. Results were then filtered for eligible papers relevant to this narrative review. Challenges on a long-duration mission will be unique, unlike anything we have faced so far in the last 60 years of space travel. Despite the progress in space surgery in the last 40 years, there are several challenges to achieving a fully functional surgical care system on any mission outside Low Earth Orbit. The microgravity environment presents unique challenges related to altered physiology as well as mechanics and techniques pertinent to surgical care. Some of the challenges include but are not limited to crew selection, role of prophylactic surgery, adaptation to zero gravity, lack of ground support, training and maintenance of surgical skills and limitation of weight and volume for hardware. Ultrasound imaging, 3D printing and AI-based surgical assistance coupled with robotic surgery have shown promise, but their real efficacy and functionality remains to be tested.",space,845
10.1016/j.ins.2021.06.008,filtered,Information Sciences,scopus,2021-10-01,sciencedirect,discriminative group-sparsity constrained broad learning system for visual recognition,https://api.elsevier.com/content/abstract/scopus_id/85113805302,"Broad Learning System (BLS) is an emerging network paradigm that has received considerable attention in the regression and classification fields. However, there are two deficiencies which seriously hinder its deployment in real applications. The first one is the internal correlations among samples are not fully considered in the modeling process. Second, the strict binary label matrix utilized in BLS provides little freedom for classification. In this paper, to address the above issues, we propose to impose group-sparsity constraints on the class-specific transformed features and label error terms, respectively. The effect is not only the more appropriate margins between data can be preserved, but also the learnt label space can be flexible for recognition. As a result, the obtained projection matrix can show more vital discriminative ability. Further, we employ the alternating direction method of multipliers to solve the resulting optimization problem. Extensive experiments and analysis on diverse benchmark databases are carried out to confirm our proposed model’s superiority in comparison with other competing classification methods.",space,846
10.1016/j.jappgeo.2021.104434,filtered,Journal of Applied Geophysics,scopus,2021-10-01,sciencedirect,a convolutional neural network approach to electrical resistivity tomography,https://api.elsevier.com/content/abstract/scopus_id/85112776120,"Electrical resistivity tomography (ERT) is an ill-posed and non-linear inverse problem commonly solved through deterministic gradient-based methods. These algorithms guarantee fast convergence toward the final solution but hinder accurate uncertainty assessments. On the contrary, numerical Markov Chain Monte Carlo algorithms provide accurate uncertainty appraisals but at the expense of a considerable computational effort. In this work, we develop a novel approach to ERT that guarantees an extremely fast inversion process and reliable uncertainty appraisals. The implemented method combines a Discrete Cosine Transform (DCT) reparameterization of data and model spaces with a Convolutional Neural Network. The CNN is employed to learn the inverse non-linear mapping between the DCT-compressed data and the DCT-compressed 2-D resistivity model. The DCT is an orthogonal transformation that here acts as an additional feature extraction technique that reduces the dimensionality of the input and output of the network. The DCT also acts as a regularization operator in the model space that significantly reduces the number of unknown parameters and the ill-conditioning of the inversion procedure, thereby preserving the spatial continuity of the resistivity values in the recovered solution. The estimation of model uncertainties is a key step of geophysical inverse problems and hence we implement a Monte Carlo simulation framework that propagates onto the estimated model the uncertainties related to both noise contamination and network approximation (the so-called modeling error). We first apply the approach to synthetic data to investigate its robustness in case of erroneous assumptions on the noise and model statistics used to generate the training set. Then, we demonstrate the applicability of the method through inverting real data measured along a river embankment. We also demonstrate that transfer learning avoids retraining the network from scratch when the statistical properties of training and target sets are different. Our tests confirm the suitability of the proposed approach, opening the possibility to estimate the subsurface resistivity values and the associated uncertainties in near real-time.",space,847
10.1016/j.jobe.2021.102799,filtered,Journal of Building Engineering,scopus,2021-10-01,sciencedirect,a step-by-step numerical method for optimization of mechanical ventilation in deep underground enclosed parking lots: a case-design study,https://api.elsevier.com/content/abstract/scopus_id/85110290956,"To reduce polluted air, mechanical ventilation (MV) is essential for enclosed parking spaces. The traditional prescriptive design method, the index-based design, cannot guarantee ventilation performance of each fan in the enclosed parking lot. To solve this, the performance-based design approach is the best alternative that specifically addresses performance-related criteria of MV system. In this study of practice-based learning in a real construction project, we proposed a unique design optimization methodology for improving the performance of MV systems using iterative, step-by-step computational fluid dynamics (CFD) simulation. Five numerical simulation levels on seven engineering steps and a techno-economic analysis were utilized. Ultimately, fan selection was based on calculating the airflow and pressure requirements of a MV system and finding a fan of the right design to hedge against the risk of system effect and surging phenomenon. Results showed that a stable fan selection with an error of 5% of the design air flow rate could be implemented by repeating numerical analysis for the performance optimization of the MV system. When the MV design optimization was applied to the reference parking lot, the number of fans could be reduced by 30%, and energy demand of the MV system by at least 16%. Consequently, the annual energy savings was projected to recover the increase in initial investment cost in about 5.8 years. The key contribution of this research is that it overcame the limitations of the traditional index-based design for selecting the optimal fans of MV systems.",space,848
10.1016/j.scs.2021.103071,filtered,Sustainable Cities and Society,scopus,2021-09-01,sciencedirect,a stochastic machine learning based approach for observability enhancement of automated smart grids,https://api.elsevier.com/content/abstract/scopus_id/85107608455,"This paper develops a machine learning aggregated integer linear programming approach for the full observability of the automated smart grids by positioning of micro-synchrophasor units, taking into account the reconfigurable structure of the distribution systems. The proposed stochastic approach presents a strategy occurring in several stages to micro-synchrophasor unit positioning based on the load level and demand in the system and based on the pre-determined sectionalizing and tie switches. Such a technique can also deploy the zero-injection limitations of the model and reduce the search space of the problem. Moreover, a novel method based on whale optimization method (WOM) is introduced to simultaneously enhance the reliability indices in order to specify the optimum topology for each phase and reduce the costs of power losses and customer interruptions. Although the problem of micro-synchrophasor placement is formulated in an integer linear programming framework, the restructuring technique is resolved on the basis of the WOM heuristic approach. Considering the uncertainty due to the metering devices or forecast errors, a stochastic framework based on point estimation is deployed to handle the uncertainty effects. The simulation and numerical results on a real system verify that the proposed method assures visibility of the distribution network pre and post reconfiguration in the time horizon of the planning. Furthermore, the results show that the system observability can be guaranteed at different load levels even though the system experiences different reconfiguration and topologies.",space,849
10.1016/j.jprocont.2021.06.004,filtered,Journal of Process Control,scopus,2021-08-01,sciencedirect,online reinforcement learning for a continuous space system with experimental validation,https://api.elsevier.com/content/abstract/scopus_id/85111075597,"Reinforcement learning (RL) for continuous state/action space systems has remained a challenge for nonlinear multivariate dynamical systems even at a simulation level. Implementing such schemes for real-time control is still of a difficulty and remains largely unanswered. In this study, several critical strategies for practical implementation of RL are developed, and a multivariable, multi-modal, hybrid three-tank (HTT) physical process is utilized to illustrate the proposed strategies. A successful real-time implementation of RL is reported. The first step is a meta-heuristic first principles model parameter optimization, where a custom pseudo random binary signal (PRBS) is used to obtain open-loop experimental data. This is followed by in silico asynchronous advantage actor–critic (A3C/A-A2C) based policy learning. In the second step, three different approaches (namely proximal learning, single trajectory learning, and multiple trajectory learning) are utilized to explore the state/action space. In the final step, online learning (A2C) using the best in silico policy on the real process using a socket connection is established. The extent of exploration (EoE, a measure of exploration) is proposed as a parameter for quantifying exploration of the state/action space. While the online sample efficiency of RL application is enhanced, a soft constraint based constrained learning is proposed and validated. With considerations of the proposed strategies, this work demonstrates the possibility of applying RL to solve practical control problems.",space,850
10.1016/j.jmsy.2021.04.005,filtered,Journal of Manufacturing Systems,scopus,2021-07-01,sciencedirect,learningadd: machine learning based acoustic defect detection in factory automation,https://api.elsevier.com/content/abstract/scopus_id/85106283308,"Defect inspection of glass bottles in the beverage industrial is of significance to prevent unexpected losses caused by the damage of bottles during manufacturing and transporting. The commonly used manual methods suffer from inefficiency, excessive space consumption, and beverage wastes after filling. To replace the manual operations in the pre-filling detection with improved efficiency and reduced costs, this paper proposes a machine learning based Acoustic Defect Detection (LearningADD) system. Moreover, to realize scalable deployment on edge and cloud computing platforms, deployment strategies especially partitioning and allocation of functionalities need to be compared and optimized under realistic constraints such as latency, complexity, and capacity of the platforms. In particular, to distinguish the defects in glass bottles efficiently, the improved Hilbert-Huang transform (HHT) is employed to extend the extracted feature sets, and then Shuffled Frog Leaping Algorithm (SFLA) based feature selection is applied to optimize the feature sets. Five deployment strategies are quantitatively compared to optimize real-time performances based on the constraints measured from a real edge and cloud environment. The LearningADD algorithms are validated by the datasets from a real-life beverage factory, and the F-measure of the system reaches 98.48 %. The proposed deployment strategies are verified by experiments on private cloud platforms, which shows that the Distributed Heavy Edge deployment outperforms other strategies, benefited from the parallel computing and edge computing, where the Defect Detection Time for one bottle is less than 2.061 s in 99 % probability.",space,851
10.1016/j.petrol.2021.108488,filtered,Journal of Petroleum Science and Engineering,scopus,2021-07-01,sciencedirect,data-driven machine learning for accurate prediction and statistical quantification of two phase flow regimes,https://api.elsevier.com/content/abstract/scopus_id/85102536455,Two different two-phase flow regimes including slug and dispersed flows are examined through the implementation of system identification methods to attain reduced-order models. The obtained models accurately capture the flow dynamics of the studied regimes. The models also provide state-space frequency by defining the transfer functions. The system identification results are compared with those of the bidirectional neural network to predict the phase fraction of the considered two-phase flows. The result of long short-term memory shows correlations of 91% between the real and predicted phase fractions.,space,852
10.1016/j.eswa.2020.114139,filtered,Expert Systems with Applications,scopus,2021-04-15,sciencedirect,a dynamic framework for tuning svm hyper parameters based on moth-flame optimization and knowledge-based-search,https://api.elsevier.com/content/abstract/scopus_id/85099517458,"In the real world, most of the collections of data are dynamic in nature, i.e. their size may grow with time. This dynamic nature of the data not only reduces the performance of the classifiers but also demands more optimized models for retaining the performance. Due to this, machine learning models developed in a static environment cannot be deployed efficiently to solve the real-world problems. Nowadays, maximum existing works consider only the static behaviour of the data for the training of machine learning models where the size of the collection of training data does not change over time. This paperwork imposes Support Vector Machine (SVM) in a dynamic environment. It has been identified that shifting of the optimum values of two hyper-parameters C (Penalty Parameter) and γ (Kernel Parameter) in the search space is one of the primary reasons for the performance degradation of SVM in dynamic environment. This paper proposes a novel framework that uses a new optimization module Knowledge-Based-Search (KBS) along with Moth –Flame Optimization (MFO) to optimize 
                        
                           C
                        
                      and 
                        
                           γ
                        
                      in a dynamic environment to train SVM efficiently. KBS uses knowledge gathered at various instances of time, which are the bi-products of MFO. MFO in our framework is the base optimization algorithm which works underneath KBS. The experiments have shown that KBS helps in controlling the exponential growth of the time complexity of the optimization process where only MFO is used to optimize 
                        
                           C
                        
                      and
                        
                           γ
                        
                     . Integration of KBS with MFO brings down the time complexity to a large extent. To validate the proposed framework we have used a simulated dynamic environment for profit/loss classification problem for organizations. The experiments have also shown that KBS's integration with MFO outperforms integration of KBS with other modern optimization techniques such as Particle Swarm Optimization (PSO), Multi-Verse Optimization (MVO), Grey-Wolf Optimization (GWO), Cuckoo Search (CS), Whale Optimization Algorithm (WOA), Genetic Algorithm (GA), Fire-Fly Algorithm (FFA) and Salp Swarm Algorithm (SSA).",space,853
10.1016/j.eswa.2020.114402,filtered,Expert Systems with Applications,scopus,2021-04-15,sciencedirect,unsupervised feature selection for attributed graphs,https://api.elsevier.com/content/abstract/scopus_id/85097572982,"Many real-world applications generate attributed graphs that contain both link structures and content information associated with nodes. Content information in real networks always contains high dimensional feature space. In recent years, unsupervised feature selection has been widely used in handling high dimensional data without label information. Most existing unsupervised feature selection methods assume that instances in datasets are independent and identically distributed. However, instances in attributed graphs are intrinsically correlated. Considering the wide applications of feature selection in attributed graphs, we propose a new unsupervised feature selection method based on regularized sparse learning. We use pseudo class labels to learn the interdependency from both link and content information, and embed the obtained information into a sparse learning based feature selection framework. In particular, a new regularization term is designed to learn link information, which capture group behavior among the connected instances utilizing latent social dimensions. To solve the proposed feature selection model, we consider both convex and nonconvex cases and design the corresponding algorithms based on the Alternating Direction Method of Multipliers (ADMM) combined with ConCave Convex Procedure (CCCP). Numerical studies are implemented on real-world datasets to validate the advantage of our new method.",space,854
10.1016/j.actaastro.2021.01.048,filtered,Acta Astronautica,scopus,2021-04-01,sciencedirect,a transfer learning approach to space debris classification using observational light curve data,https://api.elsevier.com/content/abstract/scopus_id/85100001105,"This paper presents a data driven approach to space object characterisation through the application of machine learning techniques to observational light curve data. One-dimensional convolutional neural networks are shown to be effective at classifying the shape of objects from both simulated and real light curve data. To the best of the authors’ knowledge this is the first generalised attempt to classify the shape of space objects using real observational light curve data.
                  It is also demonstrated that transfer learning is successful in improving the overall classification accuracy on real light curve datasets. The authors develop a simulated light curve dataset using a high fidelity three-dimensional ray-tracing software. The simulator takes in a textured geometric model of a Resident Space Object as well as its ephemeris and uses ray-tracing software to generate photo-realistic images of the object that are then processed to extract the light curve. Models that are pre-trained on the simulated dataset and then fine-tuned on the real datasets are shown to outperform models purely trained on the real datasets. This result indicates that transfer learning will allow organisations to effectively utilise deep learning techniques without the requirement to build up large real light curve datasets for training.",space,855
10.1016/j.quaint.2020.08.018,filtered,Quaternary International,scopus,2021-02-20,sciencedirect,characterization of geomorphological features of lunar surface using chandrayaan-1 mini-sar and lro mini-rf data,https://api.elsevier.com/content/abstract/scopus_id/85090021453,"The lunar surface comprises complex geomorphological features, which have been formed by the conjunction of processes namely impact cratering and volcanism. Geological features on the Lunar surface can be bifurcated into two main areas named Maria region and the Highland region. Taurus-Littrow valley, which was the Apollo-17 mission landing site, consisting of unique geomorphological characteristics by having a sample size of both Lunar Maria and Highland regions. The dielectric constant is a parameter that gives an approximate distribution of the constituent material of the target area. It is a complex quantity, which indicates a periodic variation of the electric field. The real part of dielectric constant indicates stored energy and the imaginary part indicates dielectric loss factor or the loss of the electric field in the medium due to continuous varying electric field. Planetary surfaces for which determining dielectric constant is an important analysis for most of the space missions, ground measurement is not feasible. This work includes the machine learning-based modeling of dielectric constant for the Apollo 17 landing site the Taurus-Littrow valley. Based on the surface roughness of the study area, two models Gaussian and Exponential have been implemented and compared for the modeled output of the dielectric constant values.The modeling approaches for dielectric characterization of the lunar surface were implemented on NASA's LRO Mini-RF SAR data and Mini-SAR hybrid-pol data of ISRO's Chandrayaan-1 mission. The coefficient of determination (r2) and the root mean square error (RMSE) of the theoretical Gaussian model was 0.995, 0.042 and the Exponential model was 0.948, 0.1349 respectively. When compared with the already calculated values of dielectric constant from Apollo 17 return samples and literature survey, the Gaussian model gives a better variation. Gaussian model was further applied to the Lunar north pole crater namely Hermite-A crater, whose distinctive geomorphological characteristics and location being lunar north pole region, makes it one of the coldest places in the Solar System and a prominent location of water ice deposits.",space,856
10.1016/j.ifacol.2021.10.504,filtered,IFAC-PapersOnLine,scopus,2021-01-01,sciencedirect,advanced state fuzzy cognitive maps applied on nearly zero energy building model,https://api.elsevier.com/content/abstract/scopus_id/85120711263,"Fuzzy Cognitive Maps method combines the advantages of Fuzzy Logic, such as their human reasoning and linguistic features, with the advantages of Neural Networks, such as their low mathematical calculation requirements, in order to model complex dynamic systems on a wide variety of applications. The system variables and their interconnections are described using a graph and a weight matrix. Application of experts’ knowledge leads towards more realistic system models. In addition, the implementation of state-space theory in combination with learning algorithms, lead to a new generation of Fuzzy Cognitive Maps, the Advanced State Fuzzy Cognitive Maps. All the above are implemented on a nearly Zero Energy Building model, using real weather data and presenting its annual energy response.",space,857
10.1016/j.ymssp.2020.107061,filtered,Mechanical Systems and Signal Processing,scopus,2021-01-01,sciencedirect,recovering compressed images for automatic crack segmentation using generative models,https://api.elsevier.com/content/abstract/scopus_id/85086994715,"In a structural health monitoring (SHM) system that uses digital cameras to monitor cracks of structural surfaces, techniques for reliable and effective data compression are essential to ensure a stable and energy-efficient crack images transmission in wireless devices, e.g., drones and robots with high definition cameras installed. Compressive sensing (CS) is a signal processing technique that allows accurate recovery of a signal from a sampling rate much smaller than the limitation of the Nyquist sampling theorem. Different from the popular approach of simultaneously training encoder and decoder using neural network models, the CS theory ensures a high probability of accurate signal reconstruction based on random measurements that is shorter than the length of the original signal under a sparsity constraint. Such method is particularly useful when measurements are expensive, such as wireless sensing of civil structures, because its hardware implementation allows down sampling of signals during the sensing process. Hence, CS methods can achieve significant energy saving for the sensing devices. However, the strong assumption of the signals being highly sparse in an invertible space is relatively hard to guarantee for many real images, such as image of cracks. In this paper, we present a new approach of CS that replaces the sparsity regularization with a generative model that is able to effectively capture a low dimension representation of targeted images. We develop a recovery framework for automatic crack segmentation of compressed crack images based on this new CS method. We demonstrate the remarkable performance of our method that takes advantage of the strong capability of generative models to capture the necessary features required in the crack segmentation task even the backgrounds of the generated images are not well reconstructed. The superior performance of our recovery framework is illustrated by comparisons to three existing CS algorithms. Furthermore, we show that our framework is potentially extensible to other common problems in automatic crack segmentation, such as defect recovery from motion blurring and occlusion.",space,858
10.1016/j.jpdc.2020.08.008,filtered,Journal of Parallel and Distributed Computing,scopus,2020-12-01,sciencedirect,towards cost-effective service migration in mobile edge: a q-learning approach,https://api.elsevier.com/content/abstract/scopus_id/85090113588,"Service migration in mobile edge computing is a promising approach to improving the quality of service (QoS) for mobile users and reducing the network operational cost for service providers as well. However, these benefits are not free, coming at costs of bulk-data transfer, and likely service disruption, which could consequently increase the overall service costs. To gain the benefits of service migration while minimizing its cost across the edge nodes, in this paper, we leverage reinforcement learning (RL) method to design a cost-effective framework, called Mig-RL, for the service migration with a reduction of total service costs as a goal in a mobile edge environment. The Mig-RL leverages the infrastructure of edge network and deploys a migration agent through Q-learning to learn the optimal policy with respect to the service migration status. We distinguish the Mig-RL from other existing works in several major aspects. First, we fully exploit the nature of this problem in a modest migration space, which allows us to constrain the number of service replicas whereby a defined state–action space could be effectively handled, as opposed to those methods that need to always approximate a huge state–action space for policy optimality. Second, we advocate a migration policy-base as a cache to save the learning process by retrieving the most effective policy whenever a similar migration pattern is encountered as time goes on. Finally, by exploiting the idea of software defined network, we also investigate the efficient implementation of Mig-RL in mobile edge network. Experimental results based on some real and synthesized access sequences show that Mig-RL, compared with the selected existing algorithms, can substantially minimize the service costs, and in the meantime, efficiently improve the QoS by adapting to the changes of mobile access patterns.",space,859
10.1016/j.inffus.2020.07.003,filtered,Information Fusion,scopus,2020-12-01,sciencedirect,"data fusion strategies for energy efficiency in buildings: overview, challenges and novel orientations",https://api.elsevier.com/content/abstract/scopus_id/85087624082,"Recently, tremendous interest has been devoted to develop data fusion strategies for energy efficiency in buildings, where various kinds of information can be processed. However, applying the appropriate data fusion strategy to design an efficient energy efficiency system is not straightforward; it requires a priori knowledge of existing fusion strategies, their applications and their properties. To this regard, seeking to provide the energy research community with a better understanding of data fusion strategies in building energy saving systems, their principles, advantages, and potential applications, this paper proposes an extensive survey of existing data fusion mechanisms deployed to reduce excessive consumption and promote sustainability. We investigate their conceptualizations, advantages, challenges and drawbacks, as well as performing a taxonomy of existing data fusion strategies and other contributing factors. Following, a comprehensive comparison of the state-of-the-art data fusion based energy efficiency frameworks is conducted using various parameters, including data fusion level, data fusion techniques, behavioral change influencer, behavioral change incentive, recorded data, platform architecture, IoT technology and application scenario. Moreover, a novel method for electrical appliance identification is proposed based on the fusion of 2D local texture descriptors, where 1D power signals are transformed into 2D space and treated as images. The empirical evaluation, conducted on three real datasets, shows promising performance, in which up to 99.68% accuracy and 99.52% F1 score have been attained. In addition, various open research challenges and future orientations to improve data fusion based energy efficiency ecosystems are explored.",space,860
10.1016/j.neunet.2020.08.012,filtered,Neural Networks,scopus,2020-12-01,sciencedirect,latent dirichlet allocation based generative adversarial networks,https://api.elsevier.com/content/abstract/scopus_id/85083895792,"Generative adversarial networks have been extensively studied in recent years and powered a wide range of applications, ranging from image generation, image-to-image translation, to text-to-image generation, and visual recognition. These methods typically model the mapping from latent space to image with single or multiple generators. However, they have obvious drawbacks: (i) ignoring the multi-modal structure of images, and (ii) lacking model interpretability. Importantly, the existing methods mostly assume one or more generators can cover all image modes even if we do not know the structure of data. Thus, mode dropping and collapse often take place along with GANs training. Despite the importance of exploring the data structure in generation, it has been almost unexplored. In this work, aiming at generating multi-modal images and interpreting model explicitly, we explore the theory on how to integrate GANs with data structure prior, and propose latent Dirichlet allocation based generative adversarial networks (LDAGAN). This framework is extended to combine with a variety of state-of-the-art single-generator GANs and achieves improved performance. Extensive experiments on synthetic and real datasets demonstrate the efficacy of LDAGAN for multi-modal image generation. An implementation of LDAGAN is available at https://github.com/Sumching/LDAGAN.",space,861
10.1016/j.comnet.2020.107436,filtered,Computer Networks,scopus,2020-11-09,sciencedirect,arena: a 64-antenna sdr-based ceiling grid testing platform for sub-6 ghz 5g-and-beyond radio spectrum research,https://api.elsevier.com/content/abstract/scopus_id/85090236272,"Arena is an open-access wireless testing platform based on a grid of antennas mounted on the ceiling of a large office-space environment. Each antenna is connected to programmable software-defined radios (SDR) enabling sub-6 GHz 5G-and-beyond spectrum research. With 12 computational servers, 24 SDRs synchronized at the symbol level, and a total of 64 antennas, Arena provides the computational power and the scale to foster new technology development in some of the most crowded spectrum bands. Arena is based on a three-tier design, where the servers and the SDRs are housed in a double rack in a dedicated room, while the antennas are hung off the ceiling of a 2240 square feet office space and cabled to the radios through 100 ft-long cables. This ensures a reconfigurable, scalable, and repeatable real-time experimental evaluation in a real wireless indoor environment.
                  In this paper, we introduce the architecture, capabilities, and system design choices of Arena, and provides details of the software and hardware implementation of various testbed components. Furthermore, we describe key capabilities by providing examples of published work that employed Arena for applications as diverse as synchronized MIMO transmission schemes, multi-hop ad hoc networking, multi-cell 5G networks, AI-powered Radio-Frequency fingerprinting, secure wireless communications, and spectrum sensing for cognitive radio.",space,862
10.1016/j.sigpro.2020.107717,filtered,Signal Processing,scopus,2020-11-01,sciencedirect,fast and efficient implementation of image filtering using a side window convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85088128229,"Convolutional neural networks (CNNs) designed for object recognition have been successfully applied to low-level tasks such as image filtering. However, these networks are usually very large which occupy large memory space and demand very high computational capacity. This makes them unsuitable for real time low-level applications on smart and portable devices with limited memory and computational capacities. In this paper, we specifically design a novel CNN, side window convolutional neural network (SW-CNN), for the fast and efficient implementation of image filtering. In SW-CNN, a new convolutional strategy, called side kernel convolution (SKC) is proposed which aligns the side or corner of the convolutional window with the pixels under processing to preserve edges during convolution. By combining SKC and the representational power of CNNs, SW-CNN can learn various image-filtering tasks very effectively. Compared to the state-of-the-art networks, the superiority of SW-CNN includes three aspects. First, the number of learnable parameters is reduced by 96%. Second, the memory consumption is reduced to 50%. Third, the running time is decreased to 50%. Results of extensive experiments demonstrate that SW-CNN not only has good performance on implementing various edge-preserving filters, but also has the adaptability and flexibility on other low-level image processing applications.",space,863
10.1016/j.physd.2020.132615,filtered,Physica D: Nonlinear Phenomena,scopus,2020-11-01,sciencedirect,a reduced order deep data assimilation model,https://api.elsevier.com/content/abstract/scopus_id/85087917230,"A new Reduced Order Deep Data Assimilation (RODDA) model combining Reduced order models (ROM), Data Assimilation (DA) and Machine Learning is proposed in this paper. The RODDA model aims to improve the accuracy of Computational Fluid Dynamics (CFD) simulations. The DA model ingests information from observed data in the simulation provided by the CFD model. The results of the DA are used to train a neural network learning a function which predicts the misfit between the results of the CFD model and the DA model. Thus, the trained function is combined with the original CFD model in order to generate forecasts with implicit DA given by neural network. Due to the time complexity of the numerical models used to implement DA and the neural network, and due to the scale of the forecasting area considered for forecasting problems in real case scenarios, the implementation of RODDA mandated the introduction of opportune reduced spaces. Here, RODDA is applied to a CFD simulation for air pollution, using the CFD software Fluidity, in South London (UK). We show that, using this framework, the data forecasted by the coupled model CFD+RODDA are closer to the observations with a gain in terms of execution time with respect to the classic prediction–correction cycle given by coupling CFD with a standard DA. Additionally, RODDA predicts future observations, if not available, since these are embedded in the data assimilated state in which the network is trained on. The RODDA framework is not exclusive to air pollution, Fluidity, or the study area in South London, and therefore the workflow could be applied to different physical models if enough temporal data are available.",space,864
10.1016/j.cpc.2020.107365,filtered,Computer Physics Communications,scopus,2020-11-01,sciencedirect,conundrum: a program for orbital-free density functional theory calculations,https://api.elsevier.com/content/abstract/scopus_id/85086471143,"We present a new code for energy minimization, structure relaxation and evaluation of bulk parameters in the framework of orbital-free density functional theory (OF-DFT). The implementation is based on solving the Euler–Lagrange equation on an equidistant real space grid on which density dependent variables and derivatives are computed. Some potential components are computed in Fourier space. The code is able to use semilocal and non-local kinetic energy functionals (KEF) as well as neural network based KEFs thus facilitating testing and development of emerging machine-learned KEFs. For semi-local and machine-learned KEFs the kinetic energy potentials are evaluated with real-space differentiation of the components, which are partial derivatives of the KE with respect to the electron density, its gradient and Laplacian.
               
                  Program summary
                  
                     Program title: CONUNDrum.
                  
                     CPC Library link to program files: 
                     http://dx.doi.org/10.17632/phnz2gg8mz.1
                  
                  
                     Licensing provision: GNU GPL v3
                  
                     Programming language: C++
                  
                     External routines: Fastest Fourier Transform in the West (FFTW) library (http://www.fftw.org/)
                  
                     Nature of problem: Calculation of the electronic and structural properties of molecules and extended systems in the framework of the orbital-free density functional theory. Evaluation of the bulk parameters of solid compounds.
                  
                     Solution method: High-order central finite-difference method and fast Fourier transform are used for calculation of different total energy components. Density optimization is performed with the steepest descent or the Polak–Ribière variant of the non-linear conjugate-gradient method with a line search procedure based on the Armijo condition. A numerical approach is used for structural optimization — the total energies with respect to small variations in lattice geometries are computed directly, with subsequent evaluation of the force components via a high-order central-finite difference method. The same numerical procedure is used for evaluation of bulk properties.
                  
                     Restrictions: Local pseudopotentials.",space,865
10.1016/j.rser.2020.109920,filtered,Renewable and Sustainable Energy Reviews,scopus,2020-09-01,sciencedirect,virtual testbed for model predictive control development in district cooling systems,https://api.elsevier.com/content/abstract/scopus_id/85086021217,"Recently, with increasing cooling demands, district cooling has assumed an important role as it is more efficient than stand-alone cooling systems. District cooling reduces the environmental impact and promotes the use of renewable sources. Earlier studies to optimise the production plants of district cooling systems were focused primarily on plants with compressor chillers and thermal energy storage devices. Although absorption chillers are crucial for integrating renewable sources into these systems, very few studies have considered them from the cooling perspective. In this regard, this paper presents the progress and results of the implementation of a virtual testbed based on a digital twin of a district cooling production plant with both compressor and absorption chillers. The aim of this study, carried out within the framework of INDIGO, a European Union-funded project, was (i) to develop a reliable model that can be used in a model predictive controller and (ii) to simulate the plant using this controller. The production plant components, which included absorption and compressor chillers, as well as cooling towers, were built using the equation-based Modelica programming language, and were calibrated using information from the manufacturer, together with real operation data. The remainder of the plant was modelled in Python. To integrate the Modelica models into the Python environment, a combination of machine learning techniques and state-space representation models was used. With these techniques, models with a high computational speed were obtained, which were suitable for real-time applications. These models were then used to build a model predictive control for the production plant to minimise the primary energy usage. The improvements in the control and the resultant energy savings achieved were compared with a baseline case working on a standard cascade control. Energy savings up to 50% were obtained in the simulation-based experiments.",space,866
10.1016/j.ast.2020.105902,filtered,Aerospace Science and Technology,scopus,2020-08-01,sciencedirect,hybrid multigene genetic programming - artificial neural networks approach for dynamic performance prediction of an aeroengine,https://api.elsevier.com/content/abstract/scopus_id/85086503677,"Dynamic aeroengine models have an important role in the design of real-time control systems. Modelling of aeroengines using dynamic performance simulations is a key step in the design process in order to reduce costs and the development period. A dynamic model can provide a numerical counterpart for the development of control systems and for the study of the engine behaviour in both steady and unsteady scenarios. The latter situation is particularly felt in the military field. The Viper 632-43 engine analysed in this work is a military turbojet, so it was necessary to develop a model that would replicate its behaviour as realistically as possible. The model was built using the Gas turbine Simulation Program (GSP) software and validated both in steady and transient conditions.
                  Once the engine model was validated, different machine learning techniques were used to estimate (data mining) and predict an engine parameter; the Exhaust Gas Temperature (EGT) has been chosen as the key parameter. A MultiGene Genetic Programming (MGGP) technique has been used to derive simple mathematical relationships between different input parameters and the EGT. These, then, can be used to calculate the EGT value of a real Viper 632-43 engine knowing a priori the input parameters and in any operating condition.
                  Finally, the EGT estimated by this algorithm has been added to the dataset used for the one-step-ahead EGT prediction by Artificial Neural Network (ANN). A time-series ANN was used for the EGT prediction, i.e. the Nonlinear AutoRegressive with eXogenous inputs (NARX) neural network. This network recognizes the input data as a real time series and is therefore able to predict the output in the next time step. It was chosen to use, as forecasting method, the one-step-ahead technique which allows to predict the EGT in the immediately next time step.",space,867
10.1016/j.pmcj.2020.101210,filtered,Pervasive and Mobile Computing,scopus,2020-07-01,sciencedirect,a deep learning approach for path prediction in a location-based iot system,https://api.elsevier.com/content/abstract/scopus_id/85086567816,"Knowing in real-time the position of objects and people, both in indoor and outdoor spaces, allows companies and organizations to improve their processes and offer new kind of services. Nowadays Location-based Services (LBS) generate a significant amount of data thank to the widespread of the Internet of Things; since they have been quickly perceived as a potential source of profit, several companies have started to design and develop a wide range of such services. One of the most challenging research tasks is undoubtedly represented by the analysis of LBS data through Machine Learning algorithms and methodologies in order to infer new knowledge and build-up even more customized services. Cultural Heritage is a domain that can benefit from such studies since it is characterized by a strong interaction between people, cultural items and spaces. Data gathered in a museum on visitor movements and behaviours can constitute the knowledge base to realize an advanced monitoring system able to offer museum stakeholders a complete and real-time snapshot of the museum locations occupancy. Furthermore, exploiting such data through Deep Learning methodologies can lead to the development of a predictive monitoring system able to suggest stakeholders the museum locations occupancy not only in real-time but also in the next future, opening new scenarios in the management of a museum. In this paper, we present and discuss a Deep Learning methodology applied to data coming from a non-invasive Bluetooth IoT monitoring system deployed inside a cultural space. Through the analysis of visitors’ paths, the main goal is to predict the occupancy of the available rooms. Experimental results on real data demonstrate the feasibility of the proposed approach; it can represent a useful instrument, in the hands of the museum management, to enhance the quality-of-service within this kind of spaces.",space,868
10.1016/j.actaastro.2020.02.036,filtered,Acta Astronautica,scopus,2020-06-01,sciencedirect,terminal adaptive guidance via reinforcement meta-learning: applications to autonomous asteroid close-proximity operations,https://api.elsevier.com/content/abstract/scopus_id/85080064409,"Current practice for asteroid close proximity maneuvers requires extremely accurate characterization of the environmental dynamics and precise spacecraft positioning prior to the maneuver. This creates a delay of several months between the spacecraft's arrival and the ability to safely complete close proximity maneuvers. In this work we develop an adaptive integrated guidance, navigation, and control system that can complete these maneuvers in environments with unknown dynamics, with initial conditions spanning a large deployment region, and without a shape model of the asteroid. The system is implemented as a policy optimized using reinforcement meta-learning. The lander is equipped with an optical seeker that locks to either a terrain feature, reflected light from a targeting laser, or an active beacon, and the policy maps observations consisting of seeker angles and LIDAR range readings directly to engine thrust commands. The policy implements a recurrent network layer that allows the deployed policy to adapt real time to both environmental forces acting on the agent and internal disturbances such as actuator failure and center of mass variation. We validate the guidance system through simulated landing maneuvers in a six degrees-of-freedom simulator. The simulator randomizes the asteroid's characteristics such as solar radiation pressure, density, spin rate, and nutation angle, requiring the guidance and control system to adapt to the environment. We also demonstrate robustness to actuator failure, sensor bias, and changes in the lander's center of mass and inertia tensor. Finally, we suggest a concept of operations for asteroid close proximity maneuvers that is compatible with the guidance system.",space,869
10.1016/j.renene.2020.01.092,filtered,Renewable Energy,scopus,2020-06-01,sciencedirect,solar irradiance forecasting models without on-site training measurements,https://api.elsevier.com/content/abstract/scopus_id/85078400450,"Much effort has been made to increase the integration of solar photovoltaic (PV) systems to reduce the environmental impacts of fossil fuels. An essential process in PV systems is the forecasting of solar irradiance to avoid safety and stability problems due to its intermittent nature. Most of the research has been focused on improving the prediction accuracy based on the assumption that enough on-site training data are available. However, in many situations, it is required for the implementation of PV systems in locations where not enough solar irradiance measurements have been collected. Our hypothesis is that measurements from other sites can be used to train accurate forecasting models, given an appropriate definition of site similarity. We propose a methodology that takes information from exogenous variables that are correlated to on-site solar irradiance and constructs a multidimensional space equipped with a metric. Each site is a point in this space, and the learned metric is used to select those sites that can provide measurements to train an accurate forecasting model on an unobserved site. We show through experiments with real data that using the learned metric provides better predictions than using the measurements collected from the whole set of available sites.",space,870
10.1016/j.ress.2020.106821,filtered,Reliability Engineering and System Safety,scopus,2020-05-01,sciencedirect,towards efficient robust optimization using data based optimal segmentation of uncertain space,https://api.elsevier.com/content/abstract/scopus_id/85078707908,"Performing multi-objective optimization under uncertainty is a common requirement in industries and academia. Robust optimization (RO) is considered as an efficient and tractable approach provided one has access to behavioral data for the uncertain parameters. However, solutions of RO may be far from the real solution and less reliable due to inability to map the uncertain space accurately, especially when the data appears discontinuous and scattered in the uncertain domain. Amalgamating machine learning algorithms with RO, this paper proposes a data-driven methodology, where a novel fuzzy clustering mechanism is implemented along-with boundary construction, to transcript the uncertain space such that the specific regions of uncertainty are identified. Subsequently, using intelligent Sobol sampling, samples are generated in the mapped uncertain regions. Results of two test cases are presented along with a comprehensive comparison study. Considered case-studies include highly nonlinear model for continuous casting process from steelmaking industries, where a multi-objective optimization problem under uncertainty is solved to balance the conflict between productivity and energy consumption. The Pareto-optimal solutions of the resulting RO problem are obtained through Non-Dominated Sorting Genetic Algorithm – II, and ~23–29% improvement is observed in the uncertain objective function. Further, the spread and diversity metrics are enhanced by ~10–95% as compared to those obtained using other standard uncertainty sets.",space,871
10.1016/j.physa.2019.123151,filtered,Physica A: Statistical Mechanics and its Applications,scopus,2020-02-15,sciencedirect,early warning system: from face recognition by surveillance cameras to social media analysis to detecting suspicious people,https://api.elsevier.com/content/abstract/scopus_id/85074532417,"Surveillance security cameras are increasingly deployed in almost every location for monitoring purposes, including watching people and their actions for security purposes. For criminology, images collected from these cameras are usually used after an incident occurs to analyze who could be the people involved. While this usage of the cameras is important for a post crime action, there exists the need for real time monitoring to act as an early warning to prevent or avoid an incident before it occurs. In this paper, we describe the development and implementation of an early warning system that recognizes people automatically in a surveillance camera environment and then use data from various sources to identify these people and build their profile and network. The current literature is still missing a complete workflow from identifying people/criminals from a video surveillance to building a criminal information extraction framework and identifying those people and their interactions with others We train a feature extraction model for face recognition using convolutional neural networks to get a good recognition rate on the Chokepoint dataset collected using surveillance cameras. The system also provides the function to record people appearance in a location, such that unknown people passing through a scene excessive number of times (above a threshold decided by a security expert) will then be further analyzed to collect information about them. We implemented a queue based system to record people entrance. We try to avoid missing relevant individuals passing through as in some cases it is not possible to add every passing person to the queue which is maintained using some cache handling techniques. We collect and analyze information about unknown people by comparing their images from the cameras to a list of social media profiles collected from Facebook and intelligent services archives. After locating the profile of a person, traditional news and other social media platforms are crawled to collect and analyze more information about the identified person. The analyzed information is then presented to the analyst where a list of keywords and verb phrases are shown. We also construct the person’s network from individuals mentioned with him/her in the text. Further analysis will allow security experts to mark this person as a suspect or safe. This work shows that building a complete early warning system is feasible to tackle and identify criminals so that authorities can take the required actions on the spot.",space,872
10.1016/j.cogsys.2019.09.015,filtered,Cognitive Systems Research,scopus,2020-01-01,sciencedirect,multi-agent neurocognitive models of semantics of spatial localization of events,https://api.elsevier.com/content/abstract/scopus_id/85072851037,"The purpose of the study is to develop a learning system for internal representation of the events localization space to realize orientation and navigation of autonomous mobile systems. The task of the research is the development of simulation models of the semantics of the event localization space based on multi-agent neurocognitive architectures. The paper proves that the multi-agent neurocognitive architecture is an effective formalism for describing the semantics of the spatial localization of events. Main theoretical foundations have been developed for the simulation of spatial relations using the so-called multi-agent facts, consisting of software agents-concepts, reflecting semantic categories corresponding to parts of speech. It is shown that locative software agents that describe the spatial location of objects and events, forming homogeneous connections, compose the so-called field locations. The latter describes a holistic view of the intellectual agent about the environment. The paper defines conceptual foundations of multi-agent modeling of the semantics of subjective reflexive mapping of the interaction between real objects, space and time.",space,873
10.1016/j.comcom.2019.09.014,filtered,Computer Communications,scopus,2019-12-15,sciencedirect,varman: multi-plane security framework for software defined networks,https://api.elsevier.com/content/abstract/scopus_id/85072873993,"In the context of future networking technologies, Software-Defined paradigm offers compelling solutions and advantages for traffic orchestration and shaping, flexible and dynamic routing, programmable control and smart application-driven resource management. But the SDN operation has to confront critical issues and technical vulnerabilities, security problems and threats in the enabling technical architecture itself. To address the critical security problems in SDN enabled data centers, we propose a collaborative “Network Security and Intrusion Detection System(NIDS)” scheme called ‘
                        VARMAN
                     : adVanced multi-plAne secuRity fraMework for softwAre defined Networks’. The SDN security scheme comprises of coarse-grained flow monitoring algorithms on the dataplane for rapid anomaly detection and prediction of network-centric DDoS/botnet attacks. In addition, this is combined with a fine-grained hybrid deep-learning based classifier pipeline on the control plane. It is observed that existing ML-based classifiers improve the accuracy of NIDS, however, at the cost of higher processing power and memory requirement, thus unrealistic for real-time solutions. To address these problems and still achieve accuracy and speed, we designed a hybrid model, combining both deep and shallow learning techniques, that are implemented in an improved SDN stack. The data plane deploys attack prediction and behavioral trigger mechanisms, efficient data filtering, feature selection, and data reduction techniques. To demonstrate the practical feasibility of our security scheme in real modern datacenters, we utilized the popular NSL-KDD dataset, most recent CICIDS2017 dataset, and refined it to a balanced dataset containing a comparable number of normal traffic and malware samples. We further augmented the training by organically generating datasets from lab-simulated and public-network hosted hackathon websites. The results show that VARMAN framework is capable of detecting attacks in real-time with accuracy more than 98% under attack intensities up to 50k packets/second. In a multi-controller interconnected SDN domain, the flow setup time improves by 70% on an average, and controller response time reduces by 40%, without incurring additional latency due to security intelligence processing overhead in SDN stack. The comparisons of VARMAN under similar attack scenarios and test environment, with related recent works that utilized ML-based NIDS, demonstrate that our scheme offers higher accuracy, less than 5% false positive rate for various attack intensities and significant training space/time reduction.",space,874
10.1016/j.eswa.2019.06.066,filtered,Expert Systems with Applications,scopus,2019-12-15,sciencedirect,double q-pid algorithm for mobile robot control,https://api.elsevier.com/content/abstract/scopus_id/85068505390,"Many expert systems have been developed for self-adaptive PID controllers of mobile robots. However, the high computational requirements of the expert systems layers, developed for the tuning of the PID controllers, still require previous expert knowledge and high efficiency in algorithmic and software execution for real-time applications. To address these problems, in this paper we propose an expert agent-based system, based on a reinforcement learning agent, for self-adapting multiple low-level PID controllers in mobile robots. For the formulation of the artificial expert agent, we develop an incremental model-free algorithm version of the double Q-Learning algorithm for fast on-line adaptation of multiple low-level PID controllers. Fast learning and high on-line adaptability of the artificial expert agent is achieved by means of a proposed incremental active-learning exploration-exploitation procedure, for a non-uniform state space exploration, along with an experience replay mechanism for multiple value functions updates in the double Q-learning algorithm. A comprehensive comparative simulation study and experiments in a real mobile robot demonstrate the high performance of the proposed algorithm for a real-time simultaneous tuning of multiple adaptive low-level PID controllers of mobile robots in real world conditions.",space,875
10.1016/j.neucom.2018.06.095,filtered,Neurocomputing,scopus,2019-08-18,sciencedirect,speeding up k-nearest neighbors classifier for large-scale multi-label learning on gpus,https://api.elsevier.com/content/abstract/scopus_id/85065140025,"Multi-label classification is one of the most dynamically growing fields of machine learning, due to its numerous real-life applications in solving problems that can be described by multiple labels at the same time. While most of works in this field focus on proposing novel and accurate classification algorithms, the issue of the computational complexity on growing dataset sizes is somehow marginalized. Owning to the ever-increasing capabilities of data capturing, we are faced with the problem of large-scale data mining that forces learners to be not only highly accurate, but also fast and scalable on high-dimensional spaces of instances, features, and labels. In this paper, we propose a highly efficient parallel approach for computing the multi-label k-Nearest Neighbor classifier on GPUs. While this method is highly effective due to its accuracy and simplicity, its computational complexity makes it prohibitive for large-scale data. We propose a four-step implementation that takes an advantage of the GPU architecture, allowing for an efficient execution of the multi-label k-Nearest Neighbors classifier without any loss of accuracy. Experiments carried out on a number of real and artificial benchmarks show that we are able to achieve speedups up to 200 times when compared to a sequential CPU execution, while efficiently scaling up to varying number of instances and features.",space,876
10.1016/j.media.2019.05.001,filtered,Medical Image Analysis,scopus,2019-07-01,sciencedirect,denoising of 3d magnetic resonance images using a residual encoder–decoder wasserstein generative adversarial network,https://api.elsevier.com/content/abstract/scopus_id/85065426790,"Structure-preserved denoising of 3D magnetic resonance imaging (MRI) images is a critical step in medical image analysis. Over the past few years, many algorithms with impressive performances have been proposed. In this paper, inspired by the idea of deep learning, we introduce an MRI denoising method based on the residual encoder–decoder Wasserstein generative adversarial network (RED-WGAN). Specifically, to explore the structure similarity between neighboring slices, a 3D configuration is utilized as the basic processing unit. Residual autoencoders combined with deconvolution operations are introduced into the generator network. Furthermore, to alleviate the oversmoothing shortcoming of the traditional mean squared error (MSE) loss function, the perceptual similarity, which is implemented by calculating the distances in the feature space extracted by a pretrained VGG-19 network, is incorporated with the MSE and adversarial losses to form the new loss function. Extensive experiments are implemented to assess the performance of the proposed method. The experimental results show that the proposed RED-WGAN achieves performance superior to several state-of-the-art methods in both simulated and real clinical data. In particular, our method demonstrates powerful abilities in both noise suppression and structure preservation.",space,877
10.1016/j.cogsys.2019.01.003,filtered,Cognitive Systems Research,scopus,2019-06-01,sciencedirect,the cortex cognitive robotics architecture: use cases,https://api.elsevier.com/content/abstract/scopus_id/85060622773,"CORTEX is a cognitive robotics architecture inspired by three key ideas: modularity, internal modelling and graph representations. CORTEX is also a computational framework designed to support early forms of intelligence in real world, human interacting robots, by selecting an a priori functional decomposition of the capabilities of the robot. This set of abilities was then translated to computational modules or agents, each one built as a network of software interconnected components. The nature of these agents can range from pure reactive modules connected to sensors and/or actuators, to pure deliberative ones, but they can only communicate with each other through a graph structure called Deep State Representation (DSR). DSR is a short-term dynamic representation of the space surrounding the robot, the objects and the humans in it, and the robot itself. All these entities are perceived and transformed into different levels of abstraction, ranging from geometric data to high-level symbolic relations such as “the person is talking and gazing at me”. The combination of symbolic and geometric information endows the architecture with the potential to simulate and anticipate the outcome of the actions executed by the robot. In this paper we present recent advances in the CORTEX architecture and several real-world human-robot interaction scenarios in which they have been tested. We describe our interpretation of the ideas inspiring the architecture and the reasons why this specific computational framework is a promising architecture for the social robots of tomorrow.",space,878
10.1016/j.datak.2019.05.003,filtered,Data and Knowledge Engineering,scopus,2019-05-01,sciencedirect,subspacedb: in-database subspace clustering for analytical query processing,https://api.elsevier.com/content/abstract/scopus_id/85066093010,"High dimensional data analysis within relational database management systems (RDBMS) is challenging because of inadequate support from SQL. Currently, subspace clustering of high dimensional data is implemented either outside DBMS using wrapper code or inside DBMS using SQL User Defined Functions/Aggregates(UDFs/UDAs). However, both these approaches have potential disadvantages from performance, resource usage, and security perspective for voluminous and frequently updated data. Hence, we propose an efficient querying system, named SubspaceDB, that implements subspace clustering directly within an RDBMS. SubspaceDB provides a novel set of query operators, each with an optimization objective, to facilitate interactive analysis for subspace clustering. The query operators focus on retrieving optimal answers to four key query types : (a) Medoid queries, (b) Neighbourhood queries, (c) Partial similarity queries, and (d) Prominence queries, that aid the formation of subspace clusters. Experimental studies on real and synthetic databases of size 
                        15
                        M
                      tuples and 104 attributes show that our proposed approach SubspaceDB can be over 10 times faster as compared to a conventional wrapper-based or SQL UDF approach. The proposed approach is also efficient in retrieving at least 50% data with performance improvement of at least 25%.",space,879
10.1016/j.dss.2019.01.003,filtered,Decision Support Systems,scopus,2019-03-01,sciencedirect,deep learning based personalized recommendation with multi-view information integration,https://api.elsevier.com/content/abstract/scopus_id/85060338648,"With the rapid proliferation of images on e-commerce platforms today, embracing and integrating versatile information sources have become increasingly important in recommender systems. Owing to the heterogeneity in information sources and consumers, it is necessary and meaningful to consider the potential synergy between visual and textual content as well as consumers' different cognitive styles. This paper proposes a multi-view model, namely, Deep Multi-view Information iNtEgration (Deep-MINE), to take multiple sources of content (i.e., product images, descriptions and review texts) into account and design an end-to-end recommendation model. In doing so, stacked auto-encoder networks are deployed to map multi-view information into a unified latent space, a cognition layer is added to depict consumers' heterogeneous cognition styles and an integration module is introduced to reflect the interaction of multi-view latent representations. Extensive experiments on real world data demonstrate that Deep-MINE achieves high accuracy in product ranking, especially in the cold-start case. In addition, Deep-MINE is able to boost overall model performance compared with models taking a single view, further verifying the proposed model's effectiveness on information integration.",space,880
10.1016/j.ress.2018.07.024,filtered,Reliability Engineering and System Safety,scopus,2019-03-01,sciencedirect,a new approach for estimating the parameters of weibull distribution via particle swarm optimization: an application to the strengths of glass fibre data,https://api.elsevier.com/content/abstract/scopus_id/85056745362,"Three-parameter Weibull is one of the most popular and most widely-used distribution in many fields of science. Therefore, many studies have been conducted concerning the statistical inferences of the parameters of Weibull distribution. In general, the maximum likelihood (ML) methodology is used in the estimation process of unknown parameters. In this study, the ML estimation of the parameters of Weibull distribution is considered using particle swarm optimization (PSO). As in other heuristic optimization methods, the performance of PSO is affected by initial conditions. The novelty of this study comes from the fact that we propose a new adaptive search space based on confidence intervals in PSO. The modified maximum likelihood (MML) estimators are utilized for constructing the confidence intervals. MML based confidence intervals allow a narrower search space for the parameters of Weibull distribution than the search space used in the literature. Therefore, the performance of PSO increases, since the search space is wisely narrowed. In order to show the performance of the proposed approach, an extensive Monte-Carlo simulation study is conducted. Simulation results show that the proposed approach works well. In addition, real world data is analyzed to show implementation of the proposed method.",space,881
10.1016/j.euromechsol.2018.10.011,filtered,"European Journal of Mechanics, A/Solids",scopus,2019-01-01,sciencedirect,a dual interpolation boundary face method for elasticity problems,https://api.elsevier.com/content/abstract/scopus_id/85055917199,"A dual interpolation boundary face method (DiBFM) is proposed to unify the conforming and nonconforming elements in boundary element method (BEM) implementation. In the DiBFM, the nodes of a conventional conforming element are sorted into two groups: the nodes on the boundary (called virtual nodes) and the internal nodes (called source nodes). Without virtual nodes, the conforming element turns to be a conventional nonconforming element of a lower order. Physical variables are interpolated using the conforming elements, the same way as conforming BEM. Boundary integral equations are collocated at source nodes, the same way as nonconforming BEM. To make the final system of linear equations solvable, additional constraint equations are required to condense the degrees of freedom for all the virtual nodes. These constraints are constructed using the moving least-squares (MLS). Besides, both boundary integration and MLS are performed in the parametric spaces of curves, namely, the geometric data, such as coordinates, out normals and Jacobians, are calculated directly from curves rather than from elements. Thus, no geometric errors are introduced no matter how coarse the discretization is. The method has been implemented successfully for solving two-dimensional elasticity problems. A number of numerical examples with real engineering background have demonstrated the accuracy and efficiency of the new method.",space,882
10.1016/j.jnca.2018.09.023,filtered,Journal of Network and Computer Applications,scopus,2018-12-15,sciencedirect,dynamic workload patterns prediction for proactive auto-scaling of web applications,https://api.elsevier.com/content/abstract/scopus_id/85054442625,"Proactive auto-scaling methods dynamically manage the resources for an application according to the current and future load predictions to preserve the desired performance at a reduced cost. However, auto-scaling web applications remain challenging mainly due to dynamic workload intensity and characteristics which are difficult to predict. Most existing methods mainly predict the request arrival rate which only partially captures the workload characteristics and the changing system dynamics that influence the resource needs. This may lead to inappropriate resource provisioning decisions. In this paper, we address these challenges by proposing a framework for prediction of dynamic workload patterns as follows. First, we use an unsupervised learning method to analyze the web application access logs to discover URI (Uniform Resource Identifier) space partitions based on the response time and the document size features. Then for each application URI, we compute its distribution across these partitions based on historical access logs to accurately capture the workload characteristics compared to just representing the workload using the request arrival rate. These URI distributions are then used to compute the Probabilistic Workload Pattern (PWP), which is a probability vector describing the overall distribution of incoming requests across URI partitions. Finally, the identified workload patterns for a specific number of last time intervals are used to predict the workload pattern of the next interval. The latter is used for future resource demand prediction and proactive auto-scaling to dynamically control the provisioning of resources. The framework is implemented and experimentally evaluated using historical access logs of three real web applications, each with increasing, decreasing, periodic, and randomly varying arrival rate behaviors. Results show that the proposed solution yields significantly more accurate predictions of workload patterns and resource demands of web applications compared to existing approaches.",space,883
10.1016/j.neuroimage.2018.08.031,filtered,NeuroImage,scopus,2018-12-01,sciencedirect,phase shift invariant imaging of coherent sources (psiicos) from meg data,https://api.elsevier.com/content/abstract/scopus_id/85054308167,"Increasing evidence suggests that neuronal communication is a defining property of functionally specialized brain networks and that it is implemented through synchronization between population activities of distinct brain areas. The detection of long-range coupling in electroencephalography (EEG) and magnetoencephalography (MEG) data using conventional metrics (such as coherence or phase-locking value) is by definition contaminated by spatial leakage. Methods such as imaginary coherence, phase-lag index or orthogonalized amplitude correlations tackle spatial leakage by ignoring zero-phase interactions. Although useful, these metrics will by construction lead to false negatives in cases where true zero-phase coupling exists in the data and will underestimate interactions with phase lags in the vicinity of zero. Yet, empirically observed neuronal synchrony in invasive recordings indicates that it is not uncommon to find zero or close-to-zero phase lag between the activity profiles of coupled neuronal assemblies.
                  Here, we introduce a novel method that allows us to mitigate the undesired spatial leakage effects and detect zero and near zero phase interactions. To this end, we propose a projection operation that operates on sensor-space cross-spectrum and suppresses the spatial leakage contribution but retains the true zero-phase interaction component. We then solve the network estimation task as a source estimation problem defined in the product space of interacting source topographies. We show how this framework provides reliable interaction detection for all phase-lag values and we thus refer to the method as Phase Shift Invariant Imaging of Coherent Sources (PSIICOS).
                  Realistic simulations demonstrate that PSIICOS has better detector characteristics than existing interaction metrics. Finally, we illustrate the performance of PSIICOS by applying it to real MEG dataset recorded during a standard mental rotation task. Taken together, using analytical derivations, data simulations and real brain data, this study presents a novel source-space MEG/EEG connectivity method that overcomes previous limitations and for the first time allows for the estimation of true zero-phase coupling via non-invasive electrophysiological recordings.",space,884
10.1016/j.eswa.2017.11.011,filtered,Expert Systems with Applications,scopus,2018-06-15,sciencedirect,towards a common implementation of reinforcement learning for multiple robotic tasks,https://api.elsevier.com/content/abstract/scopus_id/85035079318,"Mobile robots are increasingly being employed for performing complex tasks in dynamic environments. Those tasks can be either explicitly programmed by an engineer or learned by means of some automatic learning method, which improves the adaptability of the robot and reduces the effort of setting it up. In this sense, reinforcement learning (RL) methods are recognized as a promising tool for a machine to learn autonomously how to do tasks that are specified in a relatively simple manner. However, the dependency between these methods and the particular task to learn is a well-known problem that has strongly restricted practical implementations in robotics so far. Breaking this barrier would have a significant impact on these and other intelligent systems; in particular, having a core method that requires little tuning effort for being applicable to diverse tasks would boost their autonomy in learning and self-adaptation capabilities. In this paper we present such a practical core implementation of RL, which enables the learning process for multiple robotic tasks with minimal per-task tuning or none. Based on value iteration methods, we introduce a novel approach for action selection, called Q-biased softmax regression (QBIASSR), that takes advantage of the structure of the state space by attending the physical variables involved (e.g., distances to obstacles, robot pose, etc.), thus experienced sets of states accelerate the decision-making process of unexplored or rarely-explored states. Intensive experiments with both real and simulated robots, carried out with the software framework also introduced here, show that our implementation is able to learn different robotic tasks without tuning the learning method. They also suggest that the combination of true online SARSA(λ) (TOSL) with QBIASSR can outperform the existing RL core algorithms in low-dimensional robotic tasks. All of these are promising results towards the possibility of learning much more complex tasks autonomously by a robotic agent.",space,885
10.1016/j.compind.2018.03.014,filtered,Computers in Industry,scopus,2018-06-01,sciencedirect,real-time object detection in agricultural/remote environments using the multiple-expert colour feature extreme learning machine (mec-elm),https://api.elsevier.com/content/abstract/scopus_id/85044151304,"It is necessary for autonomous robotics in agriculture to provide real time feedback, but due to a diverse array of objects and lack of landscape uniformity this objective is inherently complex. The current study presents two implementations of the multiple-expert colour feature extreme learning machine (MEC-ELM). The MEC-ELM is a cascading algorithm that has been implemented along side a summed area table (SAT) for fast feature extraction and object classification, for a fully functioning object detection algorithm. The MEC-ELM is an implementation of the colour feature extreme learning machine (CF-ELM), which is an extreme learning machine (ELM) with a partially connected hidden layer; taking three colour bands as inputs. The colour implementation used with the SAT enable the MEC-ELM to find and classify objects quickly, with 84% precision and 91% recall in weed detection in the Y’UV colour space and in 0.5 s per frame. The colour implementation is however limited to low resolution images and for this reason a colour level co-occurrence matrix (CLCM) variant of the MEC-ELM is proposed. This variant uses the SAT to produce a CLCM and texture analyses, with texture values processed as an input to the MEC-ELM. This enabled the MEC-ELM to achieve 78–85% precision and 81–93% recall in cattle, weed and quad bike detection and in times between 1 and 2 s per frame. Both implementations were benchmarked on a standard i7 mobile processor. Thus the results presented in this paper demonstrated that the MEC-ELM with SAT grid and CLCM makes an ideal candidate for fast object detection in complex and/or agricultural landscapes.",space,886
10.1016/j.artint.2017.11.006,filtered,Artificial Intelligence,scopus,2018-03-01,sciencedirect,strong temporal planning with uncontrollable durations,https://api.elsevier.com/content/abstract/scopus_id/85036477809,"Planning in real world domains often involves modeling and reasoning about the duration of actions. Temporal planning allows such modeling and reasoning by looking for plans that specify start and end time points for each action. In many practical cases, however, the duration of actions may be uncertain and not under the full control of the executor. For example, a navigation task may take more or less time, depending on external conditions such as terrain or weather.
                  In this paper, we tackle the problem of strong temporal planning with uncontrollable action durations (STPUD). For actions with uncontrollable durations, the planner is only allowed to choose the start of the actions, while the end is chosen, within known bounds, by the environment. A solution plan must be robust with respect to all uncontrollable action durations, and must achieve the goal on all executions, despite the choices of the environment.
                  We propose two complementary techniques. First, we discuss a dedicated planning method, that generalizes the state-space temporal planning framework, leveraging SMT-based techniques for temporal networks under uncertainty. Second, we present a compilation-based method, that reduces any STPUD problem to an ordinary temporal planning problem. Moreover, we investigate a set of sufficient conditions to simplify domains by removing some of the uncontrollability.
                  We implemented both our approaches, and we experimentally evaluated our techniques on a large number of instances. Our results demonstrate the practical applicability of the two techniques, which show complementary behavior.",space,887
10.1016/j.neucom.2017.08.036,filtered,Neurocomputing,scopus,2018-01-31,sciencedirect,data-driven model-free slip control of anti-lock braking systems using reinforcement q-learning,https://api.elsevier.com/content/abstract/scopus_id/85029168035,"This paper proposes the design and implementation of a model-free tire slip control for a fast and highly nonlinear Anti-lock Braking System (ABS). A reinforcement Q-learning optimal control approach is inserted in a batch neural fitted scheme using two neural networks to approximate the value function and the controller, respectively. The transition samples required for learning high performance control can be collected by interacting with the process either by online exploiting the current iteration controller (or policy) under an ε-greedy exploration strategy, or by using data collected under any other controller that is capable of ensuring efficient exploration of the action-state space. Both approaches are highlighted in the paper. Fortunately, the ABS process fits this type of learning-by-interaction because it does not need an initial stabilizing controller. The validation case studies conducted on a real laboratory setup reveal that high control system performance can be achieved using the proposed approaches. Insightful comments on the observed control behavior are offered along with performance comparisons with several types of model-based and model-free controllers including relay, model-based optimal PI, an original model-free neural network state-feedback VRFT controller and a model-free neural network adaptive actor-critic one. With the ability to improve control performance starting from different supervisory controllers or to learn high performance controllers from scratch, the proposed Q-learning optimal control approach proves its performance in a wide operating range and is therefore recommended to its industrial application on ABS.",space,888
10.1016/j.procs.2018.05.113,filtered,Procedia Computer Science,scopus,2018-01-01,sciencedirect,real time high performance of sliding mode controlled induction motor drives,https://api.elsevier.com/content/abstract/scopus_id/85049099142,"Several industrial applications demand high performance speed functioning and require new control techniques so as to ensure a fast dynamic response. The present work investigates real time implementation and experimental sliding mode controlled (SMC) induction motor drives (IM). The strategy of sliding mode control is a powerful tool to ensure robustness. Nevertheless, the chattering phenomenon is a major disadvantage for non linear systems. For this purpose, two different types of analysis such as layer boundary methods are implemented in dSPACE 1104 controller board and compared between them in order to obtain the best method to reduce or eliminate chattering phenomenon. An experimental results using dSPACE 1104 based on TMS320F240 DSP are described in this work.",space,889
10.1016/j.procs.2018.05.105,filtered,Procedia Computer Science,scopus,2018-01-01,sciencedirect,geo map visualization for frequent purchaser in online shopping database using an algorithm lp-growth for mining closed frequent itemsets,https://api.elsevier.com/content/abstract/scopus_id/85049073263,"Numerous frequent itemsets are explored using frequent itemset mining algorithm which contains redundant information. Fortunately, this issue is decreased to the mining of closed frequent itemsets. However, these approaches still have some performance bottlenecks like processing time and storage space. Moreover, new algorithms of closed frequent itemsets are presented. In this paper, the proposed closed frequent itemset (LP-Closed-tree) using Linear prefix growth method is introduced which is a powerful approach for mining frequent itemsets. It builds basic tree and mines frequent itemsets. The proposed LP-Closed-tree is implemented on real and dense database like online shopping database and chess and is evaluated with other existing algorithms. While using online shopping dataset, the frequent purchaser of the dataset is visualized using google map in geographical method. After comprehensive empirical appraisal it is found that the proposed LP-Closed-tree algorithms are faster in many cases. Moreover, these algorithms are known for relatively lesser consumption of time and memory in cases of large and dense database.",space,890
10.1016/j.compmedimag.2016.11.006,filtered,Computerized Medical Imaging and Graphics,scopus,2017-09-01,sciencedirect,quantitative pet image reconstruction employing nested expectation-maximization deconvolution for motion compensation,https://api.elsevier.com/content/abstract/scopus_id/85007011515,"Bulk body motion may randomly occur during PET acquisitions introducing blurring, attenuation-emission mismatches and, in dynamic PET, discontinuities in the measured time activity curves between consecutive frames. Meanwhile, dynamic PET scans are longer, thus increasing the probability of bulk motion. In this study, we propose a streamlined 3D PET motion-compensated image reconstruction (3D-MCIR) framework, capable of robustly deconvolving intra-frame motion from a static or dynamic 3D sinogram. The presented 3D-MCIR methods need not partition the data into multiple gates, such as 4D MCIR algorithms, or access list-mode (LM) data, such as LM MCIR methods, both associated with increased computation or memory resources. The proposed algorithms can support compensation for any periodic and non-periodic motion, such as cardio-respiratory or bulk motion, the latter including rolling, twisting or drifting. Inspired from the widely adopted point-spread function (PSF) deconvolution 3D PET reconstruction techniques, here we introduce an image-based 3D generalized motion deconvolution method within the standard 3D maximum-likelihood expectation-maximization (ML-EM) reconstruction framework. In particular, we initially integrate a motion blurring kernel, accounting for every tracked motion within a frame, as an additional MLEM modeling component in the image space (integrated 3D-MCIR). Subsequently, we replaced the integrated model component with a nested iterative Richardson-Lucy (RL) image-based deconvolution method to accelerate the MLEM algorithm convergence rate (RL-3D-MCIR). The final method was evaluated with realistic simulations of whole-body dynamic PET data employing the XCAT phantom and real human bulk motion profiles, the latter estimated from volunteer dynamic MRI scans. In addition, metabolic uptake rate Ki
                      parametric images were generated with the standard Patlak method. Our results demonstrate significant improvement in contrast-to-noise ratio (CNR) and noise-bias performance in both dynamic and parametric images. The proposed nested RL-3D-MCIR method is implemented on the Software for Tomographic Image Reconstruction (STIR) open-source platform and is scheduled for public release.",space,891
10.1016/j.comcom.2016.12.015,filtered,Computer Communications,scopus,2017-06-01,sciencedirect,imola: a decentralised learning-driven protocol for multi-hop white-fi,https://api.elsevier.com/content/abstract/scopus_id/85028254628,"In this paper we tackle the digital exclusion problem in developing and remote locations by proposing Imola, an inexpensive learning-driven access mechanism for multi-hop wireless networks that operate across TV white-spaces (TVWS). Stations running Imola only rely on passively acquired neighbourhood information to achieve scheduled-like operation in a decentralised way, without explicit synchronisation. Our design overcomes pathological circumstances such as hidden and exposed terminals that arise due to carrier sensing and are exceptionally problematic in low frequency bands. We present a prototype implementation of our proposal and conduct experiments in a real test bed, which confirms the practical feasibility of deploying our solution in mesh networks that build upon the IEEE 802.11af standard. Finally, the extensive system level simulations we perform demonstrate that Imola achieves up to 4× more throughput than the channel access protocol defined by the standard and reduces frame loss rate by up to 100%.",space,892
10.1016/j.isprsjprs.2017.01.002,filtered,ISPRS Journal of Photogrammetry and Remote Sensing,scopus,2017-03-01,sciencedirect,appearance learning for 3d pose detection of a satellite at close-range,https://api.elsevier.com/content/abstract/scopus_id/85009110439,"In this paper we present a learning-based 3D detection of a highly challenging specular object exposed to a direct sunlight at very close-range. An object detection is one of the most important areas of image processing, and can also be used for initialization of local visual tracking methods. While the object detection in 3D space is generally a difficult problem, it poses more difficulties when the object is specular and exposed to the direct sunlight as in a space environment. Our solution to a such problem relies on an appearance learning of a real satellite mock-up based on a vector quantization and the vocabulary tree. Our method, implemented on a standard computer (CPU), exploits a full perspective projection model and provides near real-time 3D pose detection of a satellite for close-range approach and manipulation. The time consuming part of the training (feature description, building the vocabulary tree and indexing, depth buffering and back-projection) are performed offline, while a fast image retrieval and 3D-2D registration are performed on-line. In contrast, the state of the art image-based 3D pose detection methods are slower on CPU or assume a weak perspective camera projection model. In our case the dimension of the satellite is larger than the distance to the camera, hence the assumption of the weak perspective model does not hold. To evaluate the proposed method, the appearance of a full scale mock-up of the rear part of the TerraSAR-X satellite is trained under various illumination and camera views. The training images are captured with a camera mounted on six degrees of freedom robot, which enables to position the camera in a desired view, sampled over a sphere. The views that are not within the workspace of the robot are interpolated using image-based rendering. Moreover, we generate ground truth poses to verify the accuracy of the detection algorithm. The achieved results are robust and accurate even under noise due to specular reflection, and able to initialize a local tracking method.",space,893
10.1016/j.energy.2017.07.005,filtered,Energy,scopus,2017-01-01,sciencedirect,mathematical modeling and evolutionary generation of rule sets for energy-efficient flexible job shops,https://api.elsevier.com/content/abstract/scopus_id/85024852274,"As environmental awareness grows, sustainable scheduling is attracting increasing attention. The purposes of this paper are obtain the lower bound of energy-efficient flexible job shops with machine selection, job sequencing, and machine on-off decision making via a new mathematical model and to discover more energy-efficient rules with easy implementation in real practice via an efficient Gene Expression Programming (eGEP) algorithm. This paper first formulates a novel mixed-integer linear mathematical model to achieve effective machine selection, job sequencing, and machine off-on decision making. Then for the purpose of avoiding the empirical combination, five attributes exerting direct influence on the total energy consumption are extracted and consequently involved in the evolutionary process of eGEP. Furthermore, diversified rule mining operations with multi-gene representation and self-study are designed to enhance the search space and solutions quality. And, unsupervised learning is utilized in which global best and current worst are set to guide evolution direction since the learning progress has no prior knowledge. Experimental results show that machine off-on decisions efficiently reduce the total energy consumption; and, the discovered rules reach the lower bound calculated by GAMS/CPLEX in small problems and have significant superiority over other dispatching rules in energy saving.",space,894
10.1016/j.jnca.2016.06.010,filtered,Journal of Network and Computer Applications,scopus,2016-09-01,sciencedirect,traffic and mobility aware resource prediction using cognitive agent in mobile ad hoc networks,https://api.elsevier.com/content/abstract/scopus_id/84989815364,"Mobile Ad hoc NETwork (MANET) characteristics such as limited resources, shared channel, unpredictable mobility, improper load balancing, and variation in signal strength affect the routing of real-time multimedia data that requires Quality of Service (QoS) provisioning. Accurate prediction of the resource availability assists efficient resource allocation before the routing of such data. Most of the published work on resource prediction in MANET focuses on either bandwidth or energy without considering mobility effects. Adoption of intelligent software agent such as Cognitive Agent (CA) for the accurate resource prediction has a significant potential to solve the challenges of resource prediction in MANET. The intelligence provided in CA is similar to the logical thinking like a human for decision-making. The predominant CA architecture is the Belief-Desire-Intention (BDI) model, which performs the various tasks on behalf of the human user as an assistant.
                  In this paper, we propose a CA-based Resource Prediction mechanism considering Mobility (CA-RPM) that predicts the resources using agents through the resource prediction agency consisting of one static agent, one cognitive agent and two mobile agents. Agents predict the traffic, mobility, buffer space, energy, and bandwidth effectively that is necessary for efficient resource allocation to support real-time and multimedia communications. The mobile agents collect and distribute network traffic statistics over MANET whereas a static agent collects the local statistics. CA creates static/mobile agent during the process of resource prediction. Initially, the designed time-series Wavelet Neural Networks (WNNs) predict traffic and mobility. Buffer space, energy, and bandwidth prediction use the predicted mobility and traffic. Simulation results show that the predicted resources closely match with the real values at the cost of little overheads due to the usage of agents. Simulation analysis of predicted traffic and mobility also shows the improvement compared to recurrent WNN in terms of mean square error, covariance, memory overhead, agent overhead and computation overhead. We plan to use these predicted resources for its efficient utilization in QoS routing is our future work.",space,895
10.1016/j.ijpe.2016.06.005,filtered,International Journal of Production Economics,scopus,2016-09-01,sciencedirect,hybrid flow shop batching and scheduling with a bi-criteria objective,https://api.elsevier.com/content/abstract/scopus_id/84975859979,"This paper addresses the hybrid flow shop batching and scheduling problem where sequence-dependent family setup times are present and the objective is to simultaneously minimize the weighted sum of the total weighted completion time and total weighted tardiness. In particular, it disregards the group technology assumptions by allowing for the possibility of splitting pre-determined groups of jobs into inconsistent batches in order to improve the operational efficiency. A benchmark of small size problems is considered to show the benefits of batching on group scheduling. Since the problem is strongly NP-hard, several algorithms based upon tabu search are developed at three levels, which move back and forth between batching and scheduling phases. Two algorithms incorporate tabu search into the framework of path-relinking to exploit the information on good solutions. These tabu search/path-relinking algorithms comprise several distinguishing features including two relinking procedures to effectively construct paths and the stage-based improvement procedure to consider the move interdependency. The best tabu search algorithm as a local search algorithm is compared to a population-based algorithm, and the superiority of the former over the latter is shown using a statistical experiment. The initial solution finding mechanism is implemented to trigger the search into the solution space. The efficiency and effectiveness of the best algorithm is verified with the help of the results found by CPLEX. The results show that the best algorithm, based on tabu search/path relinking and the stage-based improvement procedure, could find solutions at least as good as CPLEX, but in drastically shorter computational time. In order to reflect the real industry requirements, dynamic machine availability times, dynamic job release times, machine eligibility and machine capability for processing jobs, desired lower bounds on batch sizes, and job skipping are considered.",space,896
10.1016/j.biosystems.2016.05.007,filtered,BioSystems,scopus,2016-07-01,sciencedirect,robotic action acquisition with cognitive biases in coarse-grained state space,https://api.elsevier.com/content/abstract/scopus_id/84973440991,"Some of the authors have previously proposed a cognitively inspired reinforcement learning architecture (LS-Q) that mimics cognitive biases in humans. LS-Q adaptively learns under uniform, coarse-grained state division and performs well without parameter tuning in a giant-swing robot task. However, these results were shown only in simulations. In this study, we test the validity of the LS-Q implemented in a robot in a real environment. In addition, we analyze the learning process to elucidate the mechanism by which the LS-Q adaptively learns under the partially observable environment. We argue that the LS-Q may be a versatile reinforcement learning architecture, which is, despite its simplicity, easily applicable and does not require well-prepared settings.",space,897
10.1016/j.cmpb.2016.04.005,filtered,Computer Methods and Programs in Biomedicine,scopus,2016-07-01,sciencedirect,a mapreduce approach to diminish imbalance parameters for big deoxyribonucleic acid dataset,https://api.elsevier.com/content/abstract/scopus_id/84964534094,"Background
                  In the age of information superhighway, big data play a significant role in information processing, extractions, retrieving and management. In computational biology, the continuous challenge is to manage the biological data. Data mining techniques are sometimes imperfect for new space and time requirements. Thus, it is critical to process massive amounts of data to retrieve knowledge. The existing software and automated tools to handle big data sets are not sufficient. As a result, an expandable mining technique that enfolds the large storage and processing capability of distributed or parallel processing platforms is essential.
               
                  Method
                  In this analysis, a contemporary distributed clustering methodology for imbalance data reduction using k-nearest neighbor (K-NN) classification approach has been introduced. The pivotal objective of this work is to illustrate real training data sets with reduced amount of elements or instances. These reduced amounts of data sets will ensure faster data classification and standard storage management with less sensitivity. However, general data reduction methods cannot manage very big data sets. To minimize these difficulties, a MapReduce-oriented framework is designed using various clusters of automated contents, comprising multiple algorithmic approaches.
               
                  Results
                  To test the proposed approach, a real DNA (deoxyribonucleic acid) dataset that consists of 90 million pairs has been used. The proposed model reduces the imbalance data sets from large-scale data sets without loss of its accuracy.
               
                  Conclusions
                  The obtained results depict that MapReduce based K-NN classifier provided accurate results for big data of DNA.",space,898
10.1016/j.neucom.2016.02.014,filtered,Neurocomputing,scopus,2016-06-12,sciencedirect,model reference fractional order control using type-2 fuzzy neural networks structure: implementation on a 2-dof helicopter,https://api.elsevier.com/content/abstract/scopus_id/84959905863,"In this paper, an adaptive learning algorithm is proposed for an interval type-2 fuzzy fractional order controller. The use of fractional order controller adds more degrees of freedom which makes it possible to obtain superior performance in comparison with ordinary differential controllers. A fractional order reference model is used to define the desired trajectory of the nonlinear dynamic system. The structure of the system is based on the feedback error learning method. The stability of the adaptation laws is proved using Lyapunov theory. In order to test the efficiency and efficacy of the proposed learning and the control algorithm, the trajectory tracking problem of a magnetic rigid spacecraft is studied. The simulation results show that the proposed control algorithm outperforms the case when ordinary differential fuzzy controller is used. Furthermore, it is shown that it is possible to define a master chaotic system as a reference model and obtain synchronization between the two chaotic systems using the proposed approach. In the simulation part the synchronization between two Duffing–Holmes system is also achieved. In order to show the implementability of the proposed method, it is used to control a real time laboratory setup 2-DOF helicopter. It is shown that the proposed fractional order controller can be implemented in a low cost embedded system and can successfully control a highly nonlinear dynamic system.",space,899
10.1016/j.neucom.2015.12.013,filtered,Neurocomputing,scopus,2016-02-29,sciencedirect,a distributed spatial-temporal weighted model on mapreduce for short-term traffic flow forecasting,https://api.elsevier.com/content/abstract/scopus_id/84955646724,"Accurate and timely traffic flow prediction is crucial to proactive traffic management and control in data-driven intelligent transportation systems (D2ITS), which has attracted great research interest in the last few years. In this paper, we propose a Spatial–Temporal Weighted K-Nearest Neighbor model, named STW-KNN, in a general MapReduce framework of distributed modeling on a Hadoop platform, to enhance the accuracy and efficiency of short-term traffic flow forecasting. More specifically, STW-KNN considers the spatial–temporal correlation and weight of traffic flow with trend adjustment features, to optimize the search mechanisms containing state vector, proximity measure, prediction function, and 
                        K
                      selection. Furthermore, STW-KNN is implemented on a widely adopted Hadoop distributed computing platform with the MapReduce parallel processing paradigm, for parallel prediction of traffic flow in real time. Finally, with extensive experiments on real-world big taxi trajectory data, STW-KNN is compared with the state-of-the-art prediction models including conventional K-Nearest Neighbor (KNN), Artificial Neural Networks (ANNs), Naïve Bayes (NB), Random Forest (RF), and C4.5. The results demonstrate that the proposed model is superior to existing models on accuracy by decreasing the mean absolute percentage error (MAPE) value more than 11.59% only in time domain and even achieves 89.71% accuracy improvement with the MAPEs of between 3.34% and 6.00% in both space and time domains, and also significantly improves the efficiency and scalability of short-term traffic flow forecasting over existing approaches.",space,900
10.1016/j.neucom.2015.02.096,filtered,Neurocomputing,scopus,2016-01-22,sciencedirect,building feature space of extreme learning machine with sparse denoising stacked-autoencoder,https://api.elsevier.com/content/abstract/scopus_id/84940061662,"The random-hidden-node extreme learning machine (ELM) is a much more generalized cluster of single-hidden-layer feed-forward neural networks (SLFNs) which has three parts: random projection, non-linear transformation, and ridge regression (RR) model. Networks with deep architectures have demonstrated state-of-the-art performance in a variety of settings, especially with computer vision tasks. Deep learning algorithms such as stacked autoencoder (SAE) and deep belief network (DBN) are built on learning several levels of representation of the input. Beyond simply learning features by stacking autoencoders (AE), there is a need for increasing its robustness to noise and reinforcing the sparsity of weights to make it easier to discover interesting and prominent features. The sparse AE and denoising AE was hence developed for this purpose. This paper proposes an approach: SSDAE-RR (stacked sparse denoising autoencoder – ridge regression) that effectively integrates the advantages in SAE, sparse AE, denoising AE, and the RR implementation in ELM algorithm. We conducted experimental study on real-world classification (binary and multiclass) and regression problems with different scales among several relevant approaches: SSDAE-RR, ELM, DBN, neural network (NN), and SAE. The performance analysis shows that the SSDAE-RR tends to achieve a better generalization ability on relatively large datasets (large sample size and high dimension) that were not pre-processed for feature abstraction. For 16 out of 18 tested datasets, the performance of SSDAE-RR is more stable than other tested approaches. We also note that the sparsity regularization and denoising mechanism seem to be mandatory for constructing interpretable feature representations. The fact that a SSDAE-RR approach often has a comparable training time to ELM makes it useful in some real applications.",space,901
10.1016/j.trb.2015.02.008,filtered,Transportation Research Part B: Methodological,scopus,2015-06-01,sciencedirect,nonlinear multivariate time-space threshold vector error correction model for short term traffic state prediction,https://api.elsevier.com/content/abstract/scopus_id/84925046498,We propose Time–Space Threshold Vector Error Correction (TS-TVEC) model for short term (hourly) traffic state prediction. The theory and method of cointegration with error correction mechanism is employed in the general design of the new statistical model TS-TVEC. An inherent connection between mathematical form of error correction model and traffic flow theory is revealed through the transformation of the well-known Fundamental Traffic Diagrams. A threshold regime switching framework is implemented to overcome any unknown structural changes in traffic time series. Spatial cross correlated information is incorporated with a piecewise linear vector error correction model. A Neural Network model is also constructed in parallel to comparatively test the effectiveness and robustness of the new statistical model. Our empirical study shows that the TS-TVEC model is an effective tool that is capable of modeling the complexity of stochastic traffic flow processes and potentially applicable to real time traffic state prediction.,space,902
10.1016/j.jbi.2014.10.009,filtered,Journal of Biomedical Informatics,scopus,2015-02-01,sciencedirect,quantifying the determinants of outbreak detection performance through simulation and machine learning,https://api.elsevier.com/content/abstract/scopus_id/84924493147,"Objective
                  To develop a probabilistic model for discovering and quantifying determinants of outbreak detection and to use the model to predict detection performance for new outbreaks.
               
                  Materials and methods
                  We used an existing software platform to simulate waterborne disease outbreaks of varying duration and magnitude. The simulated data were overlaid on real data from visits to emergency department in Montreal for gastroenteritis. We analyzed the combined data using biosurveillance algorithms, varying their parameters over a wide range. We then applied structure and parameter learning algorithms to the resulting data set to build a Bayesian network model for predicting detection performance as a function of outbreak characteristics and surveillance system parameters. We evaluated the predictions of this model through 5-fold cross-validation.
               
                  Results
                  The model predicted performance metrics of commonly used outbreak detection methods with an accuracy greater than 0.80. The model also quantified the influence of different outbreak characteristics and parameters of biosurveillance algorithms on detection performance in practically relevant surveillance scenarios. In addition to identifying characteristics expected a priori to have a strong influence on detection performance, such as the alerting threshold and the peak size of the outbreak, the model suggested an important role for other algorithm features, such as adjustment for weekly patterns.
               
                  Conclusion
                  We developed a model that accurately predicts how characteristics of disease outbreaks and detection methods will influence on detection. This model can be used to compare the performance of detection methods under different surveillance scenarios, to gain insight into which characteristics of outbreaks and biosurveillance algorithms drive detection performance, and to guide the configuration of surveillance systems.",space,903
10.1016/j.ins.2014.01.010,filtered,Information Sciences,scopus,2014-05-10,sciencedirect,traffic sign recognition using group sparse coding,https://api.elsevier.com/content/abstract/scopus_id/84894099092,"Recognizing traffic signs is a challenging problem; and it has captured the attention of the computer vision community for several decades. Essentially, traffic sign recognition is a multi-class classification problem that has become a real challenge for computer vision and machine learning techniques. Although many machine learning approaches are used for traffic sign recognition, they are primarily used for classification, not feature design. Identifying rich features using modern machine learning methods has recently attracted attention and has achieved success in many benchmarks. However these approaches have not been fully implemented in the traffic sign recognition problem. In this paper, we propose a new approach to tackle the traffic sign recognition problem. First, we introduce a new feature learning approach using group sparse coding. The primary goal is to exploit the intrinsic structure of the pre-learned visual codebook. This new coding strategy preserves locality and encourages similar descriptors to share similar sparse representation patterns. Second, we use a non-uniform quantization approach based on log-polar mapping. Using the log-polar mapping of the traffic sign image, rotated and scaled patterns are converted into shifted patterns in the new space. We extract the local descriptors from these patterns to learn the features. Finally, by evaluating the proposed approach using the German Traffic Sign Recognition Benchmark dataset, we show that the proposed coding strategy outperforms existing coding methods and the obtained results are comparable to the state-of-the-art.",space,904
10.1016/j.neunet.2014.07.001,filtered,Neural Networks,scopus,2014-01-01,sciencedirect,ordinal regression neural networks based on concentric hyperspheres,https://api.elsevier.com/content/abstract/scopus_id/84904878701,"Threshold models are one of the most common approaches for ordinal regression, based on projecting patterns to the real line and dividing this real line in consecutive intervals, one interval for each class. However, finding such one-dimensional projection can be too harsh an imposition for some datasets. This paper proposes a multidimensional latent space representation with the purpose of relaxing this projection, where the different classes are arranged based on concentric hyperspheres, each class containing the previous classes in the ordinal scale. The proposal is implemented through a neural network model, each dimension being a linear combination of a common set of basis functions. The model is compared to a nominal neural network, a neural network based on the proportional odds model and to other state-of-the-art ordinal regression methods for a total of 12 datasets. The proposed latent space shows an improvement on the two performance metrics considered, and the model based on the three-dimensional latent space obtains competitive performance when compared to the other methods.",space,905
10.1016/j.chemolab.2013.08.009,filtered,Chemometrics and Intelligent Laboratory Systems,scopus,2013-10-15,sciencedirect,genetic algorithm search space splicing particle swarm optimization as general-purpose optimizer,https://api.elsevier.com/content/abstract/scopus_id/84884383367,"A heuristic search space splicing scheme has been implemented to aid the convergence of the particle swarm optimization (PSO) algorithm to the global optimum. Genetic algorithm (GA) was used to splice the search space into smaller subspaces, thereby reducing the number of local minima. PSO algorithm was subsequently used to locate the global optima in the subspaces. A set of 11 well-known test functions had been used for the assessment of this novel GA search space splicing PSO (GA-SSS-PSO) architecture. Of the methods tested in this study, the GA-SSS-PSO approach was the only one that could optimize all functions to a desirable level. To demonstrate the algorithm's applicability, three optimization tasks of different categories commonly faced in the field of chemometrics were subjected to optimization by GA-SSS-PSO and results indicated that the novel hybrid algorithm provided robust performance for both theoretical and real life problems and may be suited as general-purpose optimizer for medium-sized optimization tasks.",space,906
10.1016/j.eswa.2012.12.080,filtered,Expert Systems with Applications,scopus,2013-08-01,sciencedirect,a decision support system for optimal deployment of sonobuoy networks based on sea current forecasts and multi-objective evolutionary optimization,https://api.elsevier.com/content/abstract/scopus_id/84875365598,"A decision support system for the optimal deployment of drifting acoustic sensor networks for cooperative track detection in underwater surveillance applications is proposed and tested on a simulated scenario. The system integrates sea water current forecasts, sensor range models and simple drifting buoy kinematic models to predict sensor positions and temporal network performance. A multi-objective genetic optimization algorithm is used for searching a set of Pareto optimal deployment solutions (i.e. the initial position of drifting sonobuoys of the network) by simultaneously optimizing two quality of service metrics: the temporal mean of the network area coverage and the tracking coverage. The solutions found after optimization, which represent different efficient tradeoffs between the two metrics, can be conveniently evaluated by the mission planner in order to choose the solution with the desired compromise between the two conflicting objectives. Sensitivity analysis through the Unscented Transform is also performed in order to test the robustness of the solutions with respect to network parameters and environmental uncertainty. Results on a simulated scenario making use of real probabilistic sea water current forecasts are provided showing the effectiveness of the proposed approach. Future work is envisioned to make the tool fully operational and ready to use in real scenarios.",space,907
10.1016/j.sigpro.2012.10.020,filtered,Signal Processing,scopus,2013-06-01,sciencedirect,3d cbir with sparse coding for image-guided neurosurgery,https://api.elsevier.com/content/abstract/scopus_id/84875248258,"This research takes an application-specific approach to investigate, extend and implement the state of the art in the fields of both visual information retrieval and machine learning, bridging the gap between theoretical models and real world applications. During an image-guided neurosurgery, path planning remains the foremost and hence the most important step to perform an operation and ensures the maximum resection of an intended target and minimum sacrifice of health tissues. In this investigation, the technique of content-based image retrieval (CBIR) coupled with machine learning algorithms are exploited in designing a computer aided path planning system (CAP) to assist junior doctors in planning surgical paths while sustaining the highest precision. Specifically, after evaluation of approaches of sparse coding and K-means in constructing a codebook, the model of sparse codes of 3D SIFT has been furthered and thereafter employed for retrieving, The novelty of this work lies in the fact that not only the existing algorithms for 2D images have been successfully extended into 3D space, leading to promising results, but also the application of CBIR that is mainly in a research realm, to a clinical sector can be achieved by the integration with machine learning techniques. Comparison with the other four popular existing methods is also conducted, which demonstrates that with the implementation of sparse coding, all methods give better retrieval results than without while constituting the codebook, implying the significant contribution of machine learning techniques.",space,908
10.1016/j.procs.2013.05.187,filtered,Procedia Computer Science,scopus,2013-01-01,sciencedirect,comparing support vector machines and artificial neural networks in the recognition of steering angle for driving of mobile robots through paths in plantations,https://api.elsevier.com/content/abstract/scopus_id/84896966222,"The use of mobile robots turns out to be interesting in activities where the action of human specialist is difficult or dangerous. Mobile robots are often used for the exploration in areas of difficult access, such as rescue operations and space missions, to avoid human experts exposition to risky situations. Mobile robots are also used in agriculture for planting tasks as well as for keeping the application of pesticides within minimal amounts to mitigate environmental pollution. In this paper we present the development of a system to control the navigation of an autonomous mobile robot through tracks in plantations. Track images are used to control robot direction by pre-processing them to extract image features. Such features are then submitted to a support vector machine and an artificial neural network in order to find out the most appropriate route. A comparison of the two approaches was performed to ascertain the one presenting the best outcome. The overall goal of the project to which this work is connected is to develop a real time robot control system to be embedded into a hardware platform. In this paper we report the software implementation of a support vector machine and of an artificial neural network, which so far presented respectively around 93% and 90% accuracy in predicting the appropriate route.",space,909
10.1016/j.actaastro.2012.06.003,filtered,Acta Astronautica,scopus,2012-11-01,sciencedirect,"robotic mission to mars: hands-on, minds-on, web-based learning",https://api.elsevier.com/content/abstract/scopus_id/84865224510,"Problem-based learning has been demonstrated as an effective methodology for developing analytical skills and critical thinking. The use of scenario-based learning incorporates problem-based learning whilst encouraging students to collaborate with their colleagues and dynamically adapt to their environment. This increased interaction stimulates a deeper understanding and the generation of new knowledge. The Victorian Space Science Education Centre (VSSEC) uses scenario-based learning in its Mission to Mars, Mission to the Orbiting Space Laboratory and Primary Expedition to the M.A.R.S. Base programs. These programs utilize methodologies such as hands-on applications, immersive-learning, integrated technologies, critical thinking and mentoring to engage students in Science, Technology, Engineering and Mathematics (STEM) and highlight potential career paths in science and engineering. The immersive nature of the programs demands specialist environments such as a simulated Mars environment, Mission Control and Space Laboratory, thus restricting these programs to a physical location and limiting student access to the programs. To move beyond these limitations, VSSEC worked with its university partners to develop a web-based mission that delivered the benefits of scenario-based learning within a school environment. The Robotic Mission to Mars allows students to remotely control a real rover, developed by the Australian Centre for Field Robotics (ACFR), on the VSSEC Mars surface. After completing a pre-mission training program and site selection activity, students take on the roles of scientists and engineers in Mission Control to complete a mission and collect data for further analysis. Mission Control is established using software developed by the ACRI Games Technology Lab at La Trobe University using the principles of serious gaming. The software allows students to control the rover, monitor its systems and collect scientific data for analysis. This program encourages students to work scientifically and explores the interaction between scientists and engineers. This paper presents the development of the program, including the involvement of university students in the development of the rover, the software, and the collation of the scientific data. It also presents the results of the trial phase of this program including the impact on student engagement and learning outcomes.",space,910
10.1016/j.ijepes.2012.02.008,filtered,International Journal of Electrical Power and Energy Systems,scopus,2012-09-01,sciencedirect,assessing the relevance of load profiling information in electrical load forecasting based on neural network models,https://api.elsevier.com/content/abstract/scopus_id/84860742606,"The article is focused on evaluating the relevance of load profiling information in electrical load forecasting, using neural networks as the forecasting methodology. Different models, with and without load profiling information, were tested and compared, and, the importance of the different inputs was investigated, using the concept of partial derivatives to understand the relevance of including this type of data in the input space. The paper presents a model for the day ahead load profile prediction for an area with many consumers. The results were analyzed with a simulated load diagram (to illustrate a distribution feeder) and also with a specific output of a 60/15kV real distribution substation that feeds a small town. The adopted methodology was successfully implemented and resulted in reducing the mean absolute percentage error between 0.5% and 16%, depending on the nature of the concurrent methodology used and the forecasted day, with a major benefit regarding the treatment of special days (holidays). The results illustrate an interesting potential for the use of the load profiling information in forecasting.",space,911
10.1016/j.optcom.2012.05.037,filtered,Optics Communications,scopus,2012-08-15,sciencedirect,semi-supervised kernel learning based optical image recognition,https://api.elsevier.com/content/abstract/scopus_id/84863471802,"This paper is to propose semi-supervised kernel learning based optical image recognition, called Semi-supervised Graph-based Global and Local Preserving Projection (SGGLPP) through integrating graph construction with the specific DR process into one unified framework. SGGLPP preserves not only the positive and negative constraints but also the local and global structure of the data in the low dimensional space. In SGGLPP, the intrinsic and cost graphs are constructed using the positive and negative constraints from side-information and k nearest neighbor criterion from unlabeled samples. Moreover, kernel trick is applied to extend SGGLPP called KSGGLPP by on the performance of nonlinear feature extraction. Experiments are implemented on UCI database and two real image databases to testify the feasibility and performance of the proposed algorithm.",space,912
10.1016/j.patrec.2012.01.015,filtered,Pattern Recognition Letters,scopus,2012-07-01,sciencedirect,a cluster-assumption based batch mode active learning technique,https://api.elsevier.com/content/abstract/scopus_id/84859302340,"In this paper, we propose an active learning technique for solving multiclass problems with support vector machine (SVM) classifiers. The technique is based on both uncertainty and diversity criteria. The uncertainty criterion is implemented by analyzing the one-dimensional output space of the SVM classifier. A simple histogram thresholding algorithm is used to find out the low density region in the SVM output space to identify the most uncertain samples. Then the diversity criterion exploits the kernel k-means clustering algorithm to select uncorrelated informative samples among the selected uncertain samples. To assess the effectiveness of the proposed method we compared it with other batch mode active learning techniques presented in the literature using one toy data set and three real data sets. Experimental results confirmed that the proposed technique provided a very good tradeoff among robustness to biased initial training samples, classification accuracy, computational complexity, and number of new labeled samples necessary to reach the convergence.",space,913
10.1016/j.patcog.2011.08.005,filtered,Pattern Recognition,scopus,2012-03-01,sciencedirect,accurate real-time neural disparity map estimation with fpga,https://api.elsevier.com/content/abstract/scopus_id/80055025069,"We propose in this paper a new method for real-time dense disparity map computing using a stereo pair of rectified images. Based on the neural network and Disparity Space Image (DSI) data structure, the disparity map computing consists of two main steps: initial disparity map estimation by combining the neuronal network and the DSI structure, and its refinement. Four improvements are introduced so that an accurate and fast result will be reached. The first one concerns the proposition of a new strategy in order to optimize the computation time of the initial disparity map. In the second one, a specific treatment is proposed in order to obtain more accurate disparity for the neighboring pixels to boundaries. The third one, it concerns the pixel similarity measure for matching score computation and it consists of using in addition to the traditional pixel intensities, the magnitude and orientation of the gradients providing more accuracy. Finally, the processing time of the method has been decreased consequently to our implementation of some critical steps on FPGAs. Experimental results on real datasets are conducted and a comparative evaluation of the obtained results relative to the state-of-art methods is presented.",space,914
10.1016/b978-0-444-59520-1.50104-4,filtered,Computer Aided Chemical Engineering,scopus,2012-01-01,sciencedirect,intelligent automation platform for bioprocess development,https://api.elsevier.com/content/abstract/scopus_id/84862870640,"High throughput technology has been increasingly adapted for drug screen and bioprocess development, due to the small amount of processing materials and reagents required and parallel experiment execution. It allows a wide design space to be explored in order to discover novel bioprocess solutions. Currently, the high throughput experiments for bioprocess development are implemented in a sequential fashion in which liquid handling system will perform the web lab experiment to prepare the samples; standalone analysis devices detect the data such as protein concentration; and specific software is used to realise the data analysis for process design or further experimentation.
                  The aim of this paper is to show how the efficiency of the high throughput bioprocess development approach can be enhanced by creating an intelligent automation platform that systematically drives liquid handling system, analysis devices and data analysis to perform a closed-loop learning. The first generation prototype has been established which consists of three parts: automated devices, design algorithms and database. In order to prove the concept of prototype, both simulation and real experiments studies have been established. In this case study, the platform is used to investigate the solubility of lysozyme at various ion strengths and pH values. Tecan liquid handling system for experimentation as well as buffer preparation and a plate reader for uv absorption measurement to determine protein concentration were used as the automated devices. The simplex search algorithm and artificial neural network modelling were utilised as design algorithm to iteratively select the experiments to execute and determine the optimal design solution. An entity-relationship database with Tecan system configuration information and experimental data was established. The results demonstrate this integrated approach can implement experiments and data analysis automatically to provide specific bioprocess design solutions in a closed loop strategy at first time. It is a promising approach that may significant increase the level of lab automation to release the engineer from the labour intensive R&D activities and provides the base for sophisticated artificial intelligent learning in the future.",space,915
10.1016/j.autcon.2011.05.018,filtered,Automation in Construction,scopus,2012-01-01,sciencedirect,simulation and analytical techniques for construction resource planning and scheduling,https://api.elsevier.com/content/abstract/scopus_id/81355138778,"To date, few construction methods or models in the literature have discussed about helping the project managers decide the near-optimum distributions of manpower, material, equipment and space according to their project objectives and project constraints. Thus, the traditional scheduling methods or models often result in a “seat-of-the-pants” style of management, rather than decision making based on an analysis of real data. This paper presents an intelligent scheduling system (ISS) that can help the project managers to find the near-optimum schedule plan according to their project objectives and project constraints. ISS uses simulation techniques to distribute resources and assign different levels of priorities to different activities in each simulation cycle to find the near-optimal solution. ISS considers and integrates most of the important construction factors (schedule, cost, space, manpower, equipment and material) simultaneously in a unified environment, which makes the resulting schedule that will be closer to optimal. Furthermore, ISS allows for what-if analyses of possible scenarios, and schedule adjustments based on unforeseen conditions (change orders, late material delivery, etc.). Finally, two sample applications and one real-world construction project are utilized to illustrate and compare the effectiveness of ISS with two widely used software packages, Primavera Project Planner and Microsoft Project.",space,916
10.1016/j.conengprac.2011.03.002,filtered,Control Engineering Practice,scopus,2011-07-01,sciencedirect,survey and application of sensor fault detection and isolation schemes,https://api.elsevier.com/content/abstract/scopus_id/79957944993,"Model-based sensor fault detection, isolation and accommodation (SFDIA) is a direction of development in particular with UAVs where sensor redundancy may not be an option due to weight, cost and space implications. SFDIA via neural networks (NNs) have been proposed over the years due to their nonlinear structures and online learning capabilities. The majority of papers tend to consider single sensor faults. While useful, this assumption can limit application to real systems where sensor faults can occur simultaneously or consecutively. In this paper we consider the latter scenario, where it is assumed that a 1s time gap is present between consecutive faults. Furthermore few applications have considered fixed-wing UAVs where full autonomy is most needed. In this paper an EMRAN RBF NN is chosen for modelling purposes due to its ability to adapt well to nonlinear environments while maintaining high computational speeds. A nonlinear UAV model is used for demonstration, where decoupled longitudinal motion is considered. System and measurement noise is also included in the UAV model as wind gust disturbances on the angle of attack and sensor noise, respectively. The UAV is assumed to operate at an initial trimmed condition of speed, 32m/s and altitude, 1000m. After 30 separate SFDIA tests implemented on a 1.6GHz Pentium processor, the NN-SFDIA scheme detected all but 2 faults and the NN processing time was 97% lower than the flight data sampling time.",space,917
10.1016/j.knosys.2011.01.007,filtered,Knowledge-Based Systems,scopus,2011-05-01,sciencedirect,a knowledge-based problem solving method in gis application,https://api.elsevier.com/content/abstract/scopus_id/79952451251,"Model design for theme analysis is one of the biggest challenges in GIS. Many real applications in GIS require functioning not only in data management and visualization, but also in analysis and decision-making. Confronted with an application of planning a new metro line in a city, a typical GIS is unable to accomplish the task in the absence of human experts or artificial intelligence technologies. Apart from being models for analyzing in different themes, some applications are also instances of problem solving in AI. Therefore, in order to strengthen its ability in automatic analysis, many theories and technologies from AI can be embedded in the GIS. In this paper, a state space is defined to formalize the metro line planning problem. By utilizing the defined state evaluation function, knowledge-based rules and strategies, a heuristic searching method is developed to optimize the solutions iteratively. Experiments are implemented to illuminate the validity of this AI-enhanced automatic analysis model of GIS.",space,918
10.1016/j.microc.2010.09.008,filtered,Microchemical Journal,scopus,2011-03-01,sciencedirect,full-range optical ph sensor array based on neural networks,https://api.elsevier.com/content/abstract/scopus_id/78649931852,"A neural network multivariate calibration is used to predict the pH of a solution in the full-range (0–14) from hue (H) values coming from imaging an optical pH sensor array based on 11 sensing elements with immobilized pH indicators. Different colorimetric acid-base indicators were tested for membrane preparation fulfilling the following conditions: 1) no leaching; 2) change in tonal coordinate by reaction and 3) covering the full pH range with overlapping between their pH responses. The sensor array was imaged after equilibration with a solution using a scanner working in transmission mode. Using software developed by us, the H coordinate of the colour space HSV was calculated from the RGB coordinates of each element.
                  The neural network was trained with the calibration data set using the Levenberg–Marquardt training method. The network structure has 11 input neurons (each one matching the hue of a single element in the sensor array), 1 output (the pH approximation value) and 1 hidden layer with 10 hidden neurons. The network provides an MSE=0.0098 in the training data, MSE=0.0183 in the validation data and MSE=0.0426 in the test data coming from a set of real water samples. The resulting correlation coefficient R obtained in the Pearson correlation test is R=0.999.",space,919
10.3182/20110828-6-it-1002.03103,filtered,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2011-01-01,sciencedirect,unibot remote laboratory: a scalable web-based set-up for education and experimental activities in robotics,https://api.elsevier.com/content/abstract/scopus_id/84866747278,"Abstract
                  The direct work on a real set-ups is an important experience for students in control theory and robotics. On the other hand, for several reasons (space, costs, complexity, etc.), it is not always possible to give students an individual access to laboratory set-ups, for their practical activities. Therefore, in recent years many tele-laboratories have been implemented by different universities, providing experimental set-ups to each student, while minimizing problems related to costs, spaces, and so on. The UniBot Remote Lab has been implemented to provide remote access via TCP connection, to assign to students different time-slots for their experiences, and to reduce the financial effort required by real set-ups. Moreover, the entire framework has been developed with high modularity both from the hardware and software point of view and, even if the basic set-up has been conceived for mobile robotics, different kind of robots or automatic machines can be easily added and be available for experimental activities.",space,920
10.1016/j.procs.2011.07.038,filtered,Procedia Computer Science,scopus,2011-01-01,sciencedirect,from context to micro-context - issues and challenges in sensorizing smart spaces for assistive living,https://api.elsevier.com/content/abstract/scopus_id/84863045088,"Most smart home based monitoring / assistive systems that attempt to recognize activities within a smart home are targeted towards living-alone elderly, and stop at providing instantaneous coarse grained information such as room-occupancy or provide specific programmed reminders for taking medication etc. In our work, we target multiple residents, while restricting the use of wearable devices / sensors. In addition we do away with video due to privacy concerns. In this paper we present the design challenges and issues in putting together a sensor network for obtaining micro-context information in multi-person smart spaces. In order to support greater levels of ambient intelligence we support fine grained spatio-temporal data and context acquisition. The architecture is being currently developed into a prototype in a modular fashion for deployment and testing in a variety of environments, and is being concurrently evaluated and tested in real conditions, prior to deployment in a facility for elderly residents with mild cognitive disorder.",space,921
10.1016/j.atmosenv.2010.07.024,filtered,Atmospheric Environment,scopus,2010-11-01,sciencedirect,prediction of hourly o<inf>3</inf> concentrations using support vector regression algorithms,https://api.elsevier.com/content/abstract/scopus_id/77956886806,"In this paper we present an application of the Support Vector Regression algorithm (SVMr) to the prediction of hourly ozone values in Madrid urban area. In order to improve the training capacity of SVMrs, we have used a recently proposed approach, based on reductions of the SVMr hyper-parameters search space. Using the modified SVMr, we study different influences which may modify the ozone prediction, such as previous ozone measurements in a given station, measurements in neighbors stations, and the influence of meteorologic variables. We use statistical tests to verify the significance of incorporating different variables into the SVMr. A comparison with the results obtained using a neural network (multi-layer perceptron) is also carried out. This study has been carried out in 5 different stations of the air pollution monitoring network of Madrid, so the conclusions raised are backed by real data. The final result of the work is a robust and powerful software for tropospheric ozone prediction in Madrid. Also, the prediction tool based on SVMr is flexible enough to incorporate any other prediction variable, such as city models, or traffic patters, which may improve the prediction obtained with the SVMr.",space,922
10.1016/j.ejor.2010.01.026,filtered,European Journal of Operational Research,scopus,2010-09-16,sciencedirect,a travel demand management strategy: the downtown space reservation system,https://api.elsevier.com/content/abstract/scopus_id/77949310027,"In this paper, a Travel Demand Management strategy known as the Downtown Space Reservation System (DSRS) is introduced. The purpose of this system is to facilitate the mitigation of traffic congestion in a cordon-based downtown area by requiring people who want to drive into this area to make reservations in advance. An integer programming formulation is provided to obtain the optimal mix of vehicles and trips that are characterized by a series of factors such as vehicle occupancy, departure time, and trip length with an objective of maximizing total system throughput and revenue. Based upon the optimal solution, an “intelligent” module is built using artificial neural networks that enables the transportation authority to make decisions in real time on whether to accept an incoming request. An example is provided that demonstrates that the solution of the “intelligent” module resembles the optimal solution with an acceptable error rate. Finally, implementation issues of the DSRS are addressed.",space,923
10.1016/j.tsf.2009.10.152,filtered,Thin Solid Films,scopus,2010-05-31,sciencedirect,application of neural classification in ellipsometry for robust thin-film characterizations,https://api.elsevier.com/content/abstract/scopus_id/77953297755,"Nowadays, ellipsometry is a widely used technique for thin-film analyses among the existing methods. It offers a rapid, accurate and non-destructive control. The main task in this technique remains in the inverse problem which goal is to extract the interesting characteristics of the sample from the ellipsometric measurement. This is a purely mathematical step and the common algorithms used to achieve it are based on the gradient method. These latter proceed iteratively and hence require a well-chosen initial solution in order to converge towards a global minimum corresponding to the real physical solution. The main limitation of these algorithms is the risk to slip into a local minimum, leading to an erroneous solution. In this paper, we propose an original method based on neural classification in order to give a first estimation about the location of the solution in the multi-parameter space. This operation generally takes place before the characterization step itself. In this work, the method is implemented experimentally to estimate the thickness range of photoresist thin films deposited on a glass substrate.",space,924
10.1016/j.landurbplan.2010.03.002,filtered,Landscape and Urban Planning,scopus,2010-05-30,sciencedirect,incorporating spatio-temporal knowledge in an intelligent agent model for natural resource management,https://api.elsevier.com/content/abstract/scopus_id/77951254750,"Space and time are intrinsic components of the decision-making process in natural resource management. Decisions to extract resources from a specific location have consequences for all future decisions as they may lead to profitable opportunities or, conversely, towards unfavorable outcomes. As such, the spatio-temporal nature of decision-making should be acknowledged and incorporated into models developed to assist the management of natural resources. The objective of this research is to develop an Intelligent Agent Model that is able to learn through repetitive simulation how to make decisions regarding natural resource extraction. Specifically, an agent is guided by heuristic algorithms to search a natural landscape and learn which locations hold the highest profits and when it is best to extract the resource in order to improve the potential of future opportunities. The model is implemented using hypothetical and real data sets to emulate the process of harvesting trees in natural forests in order to maximize profits while respecting spatial constraints that are imposed in order to conserve various aspects of the forest. Simulation results reveal the ability of the Intelligent Agent Model to utilize spatio-temporal knowledge in order learn how to devise optimal solutions in a variety of scenarios. Furthermore, the model demonstrates how the timing of decisions is linked to the spatial constraints imposed on the operation. The findings from this research can be used to inform natural resource management about the importance of the relationship between the location and timing of resource-based activities.",space,925
10.1016/j.advwatres.2009.01.001,filtered,Advances in Water Resources,scopus,2009-04-01,sciencedirect,pumping optimization of coastal aquifers based on evolutionary algorithms and surrogate modular neural network models,https://api.elsevier.com/content/abstract/scopus_id/62349136438,"Pumping optimization of coastal aquifers involves complex numerical models. In problems with many decision variables, the computational burden for reaching the optimal solution can be excessive. Artificial Neural Networks (ANN) are flexible function approximators and have been used as surrogate models of complex numerical models in groundwater optimization. However, this approach is not practical in cases where the number of decision variables is large, because the required neural network structure can be very complex and difficult to train. The present study develops an optimization method based on modular neural networks, in which several small subnetwork modules, trained using a fast adaptive procedure, cooperate to solve a complex pumping optimization problem with many decision variables. The method utilizes the fact that salinity distribution in the aquifer, depends more on pumping from nearby wells rather than from distant ones. Each subnetwork predicts salinity in only one monitoring well, and is controlled by relatively few pumping wells falling within certain control distance from the monitoring well. While the initial control area is radial, its shape is adaptively improved using a Hermite interpolation procedure. The modular neural subnetworks are trained adaptively during optimization, and it is possible to retrain only the ones not performing well. As optimization progresses, the subnetworks are adapted to maximize performance near the current search space of the optimization algorithm. The modular neural subnetwork models are combined with an efficient optimization algorithm and are applied to a real coastal aquifer in the Greek island of Santorini. The numerical code SEAWAT was selected for solving the partial differential equations of flow and density dependent transport. The decision variables correspond to pumping rates from 34 wells. The modular subnetwork implementation resulted in significant reduction in CPU time and identified an even better solution than the original numerical model.",space,926
10.1016/j.commatsci.2008.04.030,filtered,Computational Materials Science,scopus,2009-03-01,sciencedirect,hybrid intelligent approach for modeling and optimization of semiconductor devices and nanostructures,https://api.elsevier.com/content/abstract/scopus_id/59749102668,"In this work, we present a hybrid intelligent approach for parameter extraction and design optimization of semiconductor nanoscale devices and nanostructures. Based on evolutionary algorithms, numerical methods, neural network scheme and parallel computing technique, the optimization methodology is developed and successfully implemented. In the hybrid approach, an evolutionary algorithm, such as genetic algorithm or particle swarm optimization, firstly searches the entire problem space to get a set of roughly estimated solutions. The numerical method, such as Levenberg–Marquardt method, then performs a local optima search and sets the local optima as the suggested values for the genetic algorithm to perform further optimizations. Meanwhile, the neural network is applied to investigate the influence of parameters on the optimized functions which thus guides the evolutionary direction of genetic algorithm. For solving real world problems, all functional blocks are performed under a PC-based Linux cluster system with message-passing interface libraries. This hybrid intelligent approach has experimentally been implemented and validated for different applications in semiconductor nanodevices and nanostructures. For semiconductor nanodevice parameter extraction, this approach shows its capability to automatically extract a set of global parameters among sixteen 90nm complementary metal oxide semiconductor (CMOS) devices. Compared with the measured current–voltage (I–V) curves of fabricated CMOS samples, the optimized I–V results are within 3% of accuracy. The computational examinations including sensitivity, convergence property, and parallelization are discussed. For parameter extraction of organic light emitting diode (OLED), the approach also achieves good accuracy for red, green, blue OLEDs. For the third and fourth applications, optimal structure design of silicon photonic taper waveguide and photonic crystal are further advanced by integrating a simulation-based technique in the developed system. All of these experiments demonstrate interesting results and validate the optimization methodology. The concept of hybrid intelligent approach may benefit modeling and optimization in diverse science and engineering problems.",space,927
10.1016/j.eswa.2008.08.022,filtered,Expert Systems with Applications,scopus,2009-01-01,sciencedirect,text feature selection using ant colony optimization,https://api.elsevier.com/content/abstract/scopus_id/58349085948,"Feature selection and feature extraction are the most important steps in classification systems. Feature selection is commonly used to reduce dimensionality of datasets with tens or hundreds of thousands of features which would be impossible to process further. One of the problems in which feature selection is essential is text categorization. A major problem of text categorization is the high dimensionality of the feature space; therefore, feature selection is the most important step in text categorization. At present there are many methods to deal with text feature selection. To improve the performance of text categorization, we present a novel feature selection algorithm that is based on ant colony optimization. Ant colony optimization algorithm is inspired by observation on real ants in their search for the shortest paths to food sources. Proposed algorithm is easily implemented and because of use of a simple classifier in that, its computational complexity is very low. The performance of proposed algorithm is compared to the performance of genetic algorithm, information gain and CHI on the task of feature selection in Reuters-21578 dataset. Simulation results on Reuters-21578 dataset show the superiority of the proposed algorithm.",space,928
10.1016/j.actaastro.2006.12.021,filtered,Acta Astronautica,scopus,2008-01-01,sciencedirect,abort determination with non-adaptive neural networks for the mars precision landers,https://api.elsevier.com/content/abstract/scopus_id/36049019599,"The 2009 Mars Science Laboratory (MSL) will attempt the first precision landing on Mars using a modified version of the Apollo Earth entry guidance program. The guidance routine, Entry Terminal Point Controller (ETPC), commands the deployment of a supersonic parachute while converging the range to the landing target. For very dispersed cases, ETPC is unlikely to converge the range to the target and command parachute deployment within Mach number and dynamic pressure constraints. A full-lift-up abort can save 85% of these failed trajectories while abandoning the precision landing objective. In order to implement an abort, a failed trajectory needs to be recognized in real time. The application of artificial neural networks (ANNs) as an abort determination technique was evaluated. An ANN was designed, trained and tested using Monte Carlo simulations of MSL descent for a severe dust storm scenario. When incorporated into ETPC, the ANN correctly classifies 87% of descent trajectories as abort or non-abort, reducing the probability of losing MSL in a severe dust storm from 18% to 3.5%. This research shows that ANNs are capable of recognizing failed descent trajectories and can significantly increase the survivability of MSL for very dispersed cases.",space,929
10.1016/j.measurement.2006.09.004,filtered,Measurement: Journal of the International Measurement Confederation,scopus,2007-07-01,sciencedirect,neuro-fuzzy state modeling of flexible robotic arm employing dynamically varying cognitive and social component based pso,https://api.elsevier.com/content/abstract/scopus_id/34249704095,The present paper proposes the development of a neuro-fuzzy state-space model for flexible robotic arm on the basis of real sensor data acquired. The training problem of the neuro-fuzzy architecture has been configured as a highly multidimensional stochastic global optimization problem and improved variants of particle swarm optimization (PSO) techniques have been successfully implemented for it. The effects of dynamically varying the “cognitive” and the “social” components of the improved PSOs on the training performance have been studied in detail. The practical utility of such a model development procedure is aptly demonstrated by employing the best trained model to design a stable fuzzy state controller and implementing it in real life for the same flexible robotic arm.,space,930
10.1016/j.neucom.2005.04.008,filtered,Neurocomputing,scopus,2006-03-01,sciencedirect,cell assemblies for diagnostic problem-solving,https://api.elsevier.com/content/abstract/scopus_id/32544433164,"We describe a neuronal model for diagnostic problem-solving. This model which is inspired by cell assemblies gives some hints on how diagnostic problem-solving might actually be performed by the human brain. The diagnostic process is described by a deduction system that performs an abductive inference. The abductive inference itself is described by the verbal category theory. A mapping of a diagnostic problem into a diagnostic system represented by an associative memory with feedback connections is presented. The associative memory with feedback connections offers a self-contained architecture for the administration and representation of manifestations and disorders. This can be implemented efficiently on a serial computer, requiring low memory space and low computational costs. Because of these advantages, this model was chosen for the implementation of a real embedded diagnostic system for a wire bonder machine. The knowledge base of this system is composed of 350 rules, which are stored in 11 modules. These modules model the error behaviour of the microcontroller based units of the machine and are arranged in a taxonomy which corresponds to the hierarchical chains that describe the relationship between disorders and manifestations.",space,931
10.1016/j.asr.2005.12.004,filtered,Advances in Space Research,scopus,2006-01-01,sciencedirect,analysis of sileye-3/alteino data with a neural network technique: particle discrimination and energy reconstruction,https://api.elsevier.com/content/abstract/scopus_id/33744809796,"In this work, we present the data analysis of the Sileye-3/Alteino experiment with neural network technique. Sileye-3/Alteino is composed of two devices: the cosmic ray-advanced silicon telescope (an 8 plane, 32 strip silicon detector) and an electroencephalograph. It was placed on board the ISS on April the 27th 2002 to investigate on the Light Flash phenomenon and the radiation environment in space. We show the possibility of using neural networks as an useful tool for real-time data analysis. A feed-forward neural network (Multi-Layer Perceptron – MLP) has been implemented and trained (with Monte Carlo data) to perform on line particle identification for ions with Atomic Number (Z) ⩽8 and incident kinetic energy reconstruction for ions Z
                     >2. The result of the analysis of Sileye-3/Alteino real data with the neural network and the improvements over classical analysis techniques are discussed.",space,932
10.1016/j.agsy.2004.07.019,filtered,Agricultural Systems,scopus,2005-12-01,sciencedirect,merging genomic control networks and soil-plant-atmosphere-continuum models,https://api.elsevier.com/content/abstract/scopus_id/27344439349,"Advances in genomic science make it desirable to include genomic controls in soil-plant-atmosphere-continuum (SPAC) models by methods proposed in this paper. Molecular genetic concepts suggest that a differential equation similar to ones used in neural networks can be used to model single-gene elements of larger systems. Natural modifications to the equation incorporate temperature dependency. Multi-gene components based on this element function as Boolean logic gates, linear arithmetic units, delays, differentiators, integrators, oscillators, coincidence detectors, and bi-stable devices. Related genetic circuitry from real organisms is shown. Genomic integration with SPAC models entails whole-plant modeling with realistic morphology. Plants are networks of parts, iterated in time and space under genetic control, that induce and modulate conservative SPAC mass/energy flows. Network developmental rules can be stated as Lindenmayer grammars whose symbols represent plant parts programmed as software objects. A structure is presented for simulators based on these concepts. The discussion argues that prior object-oriented plant modeling approaches (i) do not reflect how plants actually develop morphologically and (ii) may represent processes in tactically unwise ways at a time when genomics is advancing knowledge of process interactions. Finally, genomics and expanding computing power redefine concepts of model “simplicity” and “complexity” to favor increased realism.",space,933
10.1016/j.neunet.2005.06.029,filtered,Neural Networks,scopus,2005-07-01,sciencedirect,on-chip visual perception of motion: a bio-inspired connectionist model on fpga,https://api.elsevier.com/content/abstract/scopus_id/27744451750,"Visual motion provides useful information to understand the dynamics of a scene to allow intelligent systems interact with their environment. Motion computation is usually restricted by real time requirements that need the design and implementation of specific hardware architectures. In this paper, the design of hardware architecture for a bio-inspired neural model for motion estimation is presented. The motion estimation is based on a strongly localized bio-inspired connectionist model with a particular adaptation of spatio-temporal Gabor-like filtering. The architecture is constituted by three main modules that perform spatial, temporal, and excitatory–inhibitory connectionist processing. The biomimetic architecture is modeled, simulated and validated in VHDL. The synthesis results on a Field Programmable Gate Array (FPGA) device show the potential achievement of real-time performance at an affordable silicon area.",space,934
10.1016/j.compind.2004.06.003,filtered,Computers in Industry,scopus,2005-02-01,sciencedirect,a solution to the unequal area facilities layout problem by genetic algorithm,https://api.elsevier.com/content/abstract/scopus_id/13444304124,"The majority of the issued facilities layout problems (FLPs) minimize the material handling cost and ignore other factors, such as area utilization, department shape and site shape size. These factors, however, might influence greatly the objective function and should give consideration. The research range of this paper is focus on the unequal areas department facilities layout problem, and implement analysis of variance (ANOVA) of statistics to find out the best site size of layout by genetic algorithm. The proposed module takes the minimum total layout cost (TLC) into account. TLC is an objective function combining material flow factor cost (MFFC), shape ratio factor (SRF) and area utilization factor (AUF). In addition, a rule-based of expert system is implemented to create space-filling curve for connecting each unequal area department to be continuously placed without disjoint (partition). In this manner, there is no gap between each unequal area department. The experimental results show that the proposed approach is more feasible in dealing with the facilities layout problems in the real world.",space,935
10.1016/j.acra.2004.09.012,filtered,Academic Radiology,scopus,2005-01-01,sciencedirect,hidden markov event sequence models: toward unsupervised functional mri brain mapping,https://api.elsevier.com/content/abstract/scopus_id/13244273665,"Rationale and objectives
                  Most methods used in functional MRI (fMRI) brain mapping require restrictive assumptions about the shape and timing of the fMRI signal in activated voxels. Consequently, fMRI data may be partially and misleadingly characterized, leading to suboptimal or invalid inference. To limit these assumptions and to capture the broad range of possible activation patterns, a novel statistical fMRI brain mapping method is proposed. It relies on hidden semi-Markov event sequence models (HSMESMs), a special class of hidden Markov models (HMMs) dedicated to the modeling and analysis of event-based random processes.
               
                  Materials and methods
                  Activation detection is formulated in terms of time coupling between (1) the observed sequence of hemodynamic response onset (HRO) events detected in the voxel’s fMRI signal and (2) the “hidden” sequence of task-induced neural activation onset (NAO) events underlying the HROs. Both event sequences are modeled within a single HSMESM. The resulting brain activation model is trained to automatically detect neural activity embedded in the input fMRI data set under analysis. The data sets considered in this article are threefold: synthetic epoch-related, real epoch-related (auditory lexical processing task), and real event-related (oddball detection task) fMRI data sets.
               
                  Results
                  
                     Synthetic data: Activation detection results demonstrate the superiority of the HSMESM mapping method with respect to a standard implementation of the statistical parametric mapping (SPM) approach. They are also very close, sometimes equivalent, to those obtained with an “ideal” implementation of SPM in which the activation patterns synthesized are reused for analysis. The HSMESM method appears clearly insensitive to timing variations of the hemodynamic response and exhibits low sensitivity to fluctuations of its shape (unsustained activation during task). Real epoch-related data: HSMESM activation detection results compete with those obtained with SPM, without requiring any prior definition of the expected activation patterns thanks to the unsupervised character of the HSMESM mapping approach. Along with activation maps, the method offers a wide range of additional fMRI analysis functionalities, including activation lag mapping, activation mode visualization, and hemodynamic response function analysis. Real event-related data: Activation detection results confirm and validate the overall strategy that consists in focusing the analysis on the transients, time-localized events that are the HROs.
               
                  Conclusion
                  All the experiments performed on synthetic and real fMRI data demonstrate the relevance of HSMESMs in fMRI brain mapping. In particular, the statistical character of these models, along with their learning and generalizing abilities are of particular interest when dealing with strong variabilities of the active fMRI signal across time, space, experiments, and subjects.",space,936
10.1016/j.envsoft.2003.10.003,filtered,Environmental Modelling and Software,scopus,2004-08-01,sciencedirect,modelling so<inf>2</inf> concentration at a point with statistical approaches,https://api.elsevier.com/content/abstract/scopus_id/3342982389,"In this paper, the results obtained by inter-comparing several statistical techniques for modelling SO2 concentration at a point such as neural networks, fuzzy logic, generalised additive techniques and other recently proposed statistical approaches are reported. The results of the inter-comparison are the fruits of collaboration between some of the partners of the APPETISE project funded under the Framework V Information Societies and Technologies (IST) programme. Two different cases for study were selected: the Siracusa industrial area, in Italy, where the pollution is dominated by industrial emissions and the Belfast urban area, in the UK, where domestic heating makes an important contribution. The different kinds of pollution (industrial/urban) and different locations of the areas considered make the results more general and interesting. In order to make the inter-comparison more objective, all the modellers considered the same datasets. Missing data in the original time series was filled by using appropriate techniques. The inter-comparison work was carried out on a rigorous basis according to the performance indices recommended by the European Topic Centre on Air and Climate Change (ETC/ACC). The targets for the implemented prediction models were defined according to the EC normative relating to limit values for sulphur dioxide. According to this normative, three different kinds of targets were considered namely daily mean values, daily maximum values and hourly mean values. The inter-compared models were tested on real cases of poor air quality. In the paper, the inter-compared techniques are ranked in terms of their capability to predict critical episodes. A ranking in terms of their predictability of the three different targets considered is also proposed. Several key issues are illustrated and discussed such as the role of input variable selection, the use of meteorological data, and the use of interpolated time series. Moreover, a novel approach referred to as the technique of balancing the training pattern set, which was successfully applied to improve the capability of ANN models to predict exceedences is introduced. The results show that there is no single modelling approach, which generates optimum results in terms of the full range of performance indices considered. In view of the implementation of a warning system for air quality control, approaches that are able to work better in the prediction of critical episodes must be preferred. Therefore, the artificial neural network prediction models can be recommended for this purpose. The best forecasts were achieved for daily averages of SO2 while daily maximum and hourly mean values are difficult to predict with acceptable accuracy.",space,937
10.1016/j.nima.2004.01.052,filtered,"Nuclear Instruments and Methods in Physics Research, Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",scopus,2004-05-21,sciencedirect,a neural network device for on-line particle identification in cosmic ray experiments,https://api.elsevier.com/content/abstract/scopus_id/2342492426,"On-line particle identification is one of the main goals of many experiments in space both for rare event studies and for optimizing measurements along the orbital trajectory. Neural networks can be a useful tool for signal processing and real time data analysis in such experiments. In this document we report on the performances of a programmable neural device which was developed in VLSI analog/digital technology. Neurons and synapses were accomplished by making use of Operational Transconductance Amplifier (OTA) structures. In this paper we report on the results of measurements performed in order to verify the agreement of the characteristic curves of each elementary cell with simulations and on the device performances obtained by implementing simple neural structures on the VLSI chip. A feed-forward neural network (Multi-Layer Perceptron, MLP) was implemented on the VLSI chip and trained to identify particles by processing the signals of two-dimensional position-sensitive Si detectors. The radiation monitoring device consisted of three double-sided silicon strip detectors. From the analysis of a set of simulated data it was found that the MLP implemented on the neural device gave results comparable with those obtained with the standard method of analysis confirming that the implemented neural network could be employed for real time particle identification.",space,938
10.1016/s0045-7906(01)00050-7,filtered,Computers and Electrical Engineering,scopus,2003-06-01,sciencedirect,two-phase neural network based estimation of degree of insecurity of power system,https://api.elsevier.com/content/abstract/scopus_id/0037409373,"Increased loading and contingencies often lead to situations where the optimal power flow solution no longer remains within the secure region. In such situations there is a need of determining control actions to be taken quickly, as otherwise the system may become unstable. Hence it is important to quantify the degree of insecurity of the power system both in planning as well as at operational stages. The distance in parameter space between an insecure operating point and the closest point on feasible (secure) hyper-surface has been used as a measure of degree of insecurity. A method based on two-phase optimization neural network has been presented to compute the degree of insecurity and the voltages and angles at all the buses of the system corresponding to the closest secure point. Inclusion of security limits on power system variables assures a solution representing a secure system. When compared with conventional non-linear optimization techniques, the proposed neural network is superior, as it can be easily implemented using digital hardware and is highly suitable for real time implementation in energy management system.
                  The proposed method has been tested on IEEE 30-bus test system and a practical 75-bus Indian system. The results achieved are compared with results from a conventional method. Insecurity arising due to increase in load and contingencies has been considered in this work.",space,939
10.1016/s0378-7753(03)00309-4,filtered,Journal of Power Sources,scopus,2003-05-15,sciencedirect,power supply quality improvement with a sofc plant by neural-network-based control,https://api.elsevier.com/content/abstract/scopus_id/0038216724,"This paper demonstrates the potential of a solid-oxide fuel cell (SOFC) to perform functions other than the supply of real power to the grid. These additional functions however require the use of an inverter. The flux-vector control is used very effectively for the control of this inverter, where the space-vector pulsewidth modulation (SVM) is implemented by neural networks (NNs). The results presented in the paper show the effect of the fuel cell on the voltage at the sensitive load point. The performance of the fuel cell was found to be excellent.",space,940
10.1016/s0196-8904(02)00138-3,filtered,Energy Conversion and Management,scopus,2003-05-01,sciencedirect,a rotor position estimator for switched reluctance motors using cmac,https://api.elsevier.com/content/abstract/scopus_id/0037400491,"This paper presents an approach to rotor position estimation in switched reluctance motors (SRMs) by using a cerebellum model articulation controller (CMAC). Previous research has shown that an artificial neural network (ANN) forms an efficient mapping structure through measurement of the flux linkages and currents for the phases. A CMAC is investigated in this paper in order to overcome the high computational power requirement problem that is encountered in a feedforward ANN based rotor position estimator. The CMAC structure does not contain neurons with activation functions, and all mathematical operations are performed without multiplication. These simplicities increase the throughput in real time implementation performed with conventional embedded controllers. However, the distributed memory structure of a CMAC requires more space. The issues involved in designing, training and implementing a CMAC are presented. In order to demonstrate the feasibility of the concept, a 20 kW, 6/4, three phase SRM is studied with training and evaluation data, which are obtained from a simulation program. A CMAC that is based on experimentally measured training and testing data for the same SRM is also used to demonstrate the promise of this approach.",space,941
10.1016/s0925-2312(02)00619-7,filtered,Neurocomputing,scopus,2003-04-01,sciencedirect,pattern recognition using multilayer neural-genetic algorithm,https://api.elsevier.com/content/abstract/scopus_id/0037381137,"The genetic algorithm implemented with neural network to determine automatically the suitable network architecture and the set of parameters from a restricted region of space. The multilayer neural-genetic algorithm was applied in image processing for pattern recognition, and to determine the object orientation. The library to cover the views of object was build from real images of (10×10) pixels. Which is the smallest image size can be used in this algorithm to recognize the type of aircraft with its direction
                  The multilayer perecptron neural network integrated with the genetic algorithm, the result showed good optimization, by reducing the number of hidden nodes required to train the neural network (the number of epoch's reduced to less than 50%). One of the important results of the implemented algorithm is the reduction in the time required to train the neural network.",space,942
10.1016/s0893-6080(03)00015-7,filtered,Neural Networks,scopus,2003-01-01,sciencedirect,improved system for object detection and star/galaxy classification via local subspace analysis,https://api.elsevier.com/content/abstract/scopus_id/0037380809,"The two traditional tasks of object detection and star/galaxy classification in astronomy can be automated by neural networks because the nature of the problems is that of pattern recognition. A typical existing system can be further improved by using one of the local Principal Component Analysis (PCA) models. Our analysis in the context of object detection and star/galaxy classification reveals that local PCA is not only superior to global PCA in feature extraction, but is also superior to gaussian mixture in clustering analysis. Unlike global PCA which performs PCA for the whole data set, local PCA applies PCA individually to each cluster of data. As a result, local PCA often outperforms global PCA for data of multi-modes. Moreover, since local PCA can effectively avoid the trouble of having to specify a large number of free elements of each covariance matrix of gaussian mixture, it can give a better description of local subspace structures of each cluster when applied on high dimensional data with small sample size. In this paper, the local PCA model proposed by Xu [IEEE Trans. Neural Networks 12 (2001) 822] under the general framework of Bayesian Ying Yang (BYY) normalization learning will be adopted. Endowed with the automatic model selection ability of BYY learning, the BYY normalization learning-based local PCA model can cope with those object detection and star/galaxy classification tasks with unknown model complexity. A detailed algorithm for implementation of the local PCA model will be proposed, and experimental results using both synthetic and real astronomical data will be demonstrated.",space,943
10.1016/s0165-0114(01)00034-3,filtered,Fuzzy Sets and Systems,scopus,2002-03-16,sciencedirect,a fast learning algorithm for parsimonious fuzzy neural systems,https://api.elsevier.com/content/abstract/scopus_id/0037117202,"In this paper, a novel learning algorithm for dynamic fuzzy neural networks based on extended radial basis function neural networks, which are functionally equivalent to Takagi–Sugeno–Kang fuzzy systems, is proposed. The algorithm comprises 4 parts: (1) criteria of rules generation; (2) allocation of premise parameters; (3) determination of consequent parameters and (4) pruning technology. The salient characteristics of the approach are: (1) a hierarchical on-line self-organizing learning paradigm is employed so that not only parameters can be adjusted, but also the determination of structure can be self-adaptive without partitioning the input space a priori; (2) fast learning speed can be achieved so that the system can be implemented in real time. Simulation studies and comprehensive comparisons with some other learning algorithms demonstrate that the proposed algorithm is superior in terms of simplicity of structure, learning efficiency and performance.",space,944
10.1016/s0950-7051(01)00151-4,filtered,Knowledge-Based Systems,scopus,2001-11-01,sciencedirect,specifying fault tolerance in mission critical intelligent systems,https://api.elsevier.com/content/abstract/scopus_id/0035505448,"Real time intelligent systems are being increasingly used in mission critical applications in domains like military, aerospace, process control industry and medicine. Despite this vast potential, the major concern about deploying mission critical intelligent systems is their dependability. Dependability encompasses such notions as reliability, safety, security, maintainability and portability. A major concern about mission critical intelligent systems is their performance in the presence of failures. Intelligent systems are characterized by often non-existent, imprecise or rapidly changing specifications. This makes the task of characterizing an intelligent system's performance in the presence of failures much more difficult. In this paper, we characterize the failures that are likely in a mission critical intelligent system. We propose an extended I/O automata model to capture these failure specifications. We further demonstrate how these specifications can be realized in a real time expert system by structuring the knowledge base. This formalism can also be used to specify the fault tolerant properties of the underlying hardware and software over which the intelligent system resides. Thus we have an unified formalism to specify fault tolerance properties in hardware, system software and the intelligent system. This will enable us to reason about the performance of the entire system inclusive of all its components in an uniform manner.",space,945
10.1016/s0921-8890(01)00113-0,filtered,Robotics and Autonomous Systems,scopus,2001-07-31,sciencedirect,acquisition of stand-up behavior by a real robot using hierarchical reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/0035979437,"In this paper, we propose a hierarchical reinforcement learning architecture that realizes practical learning speed in real hardware control tasks. In order to enable learning in a practical number of trials, we introduce a low-dimensional representation of the state of the robot for higher-level planning. The upper level learns a discrete sequence of sub-goals in a low-dimensional state space for achieving the main goal of the task. The lower-level modules learn local trajectories in the original high-dimensional state space to achieve the sub-goal specified by the upper level.
                  We applied the hierarchical architecture to a three-link, two-joint robot for the task of learning to stand up by trial and error. The upper-level learning was implemented by Q-learning, while the lower-level learning was implemented by a continuous actor–critic method. The robot successfully learned to stand up within 750 trials in simulation and then in an additional 170 trials using real hardware. The effects of the setting of the search steps in the upper level and the use of a supplementary reward for achieving sub-goals are also tested in simulation.",space,946
10.1016/s0893-6080(99)00102-1,filtered,Neural Networks,scopus,2000-01-01,sciencedirect,partially pre-calculated weights for the backpropagation learning regime and high accuracy function mapping using continuous input ram-based sigma-pi nets,https://api.elsevier.com/content/abstract/scopus_id/0033980299,"In this article we present a methodology that partially pre-calculates the weight updates of the backpropagation learning regime and obtains high accuracy function mapping. The paper shows how to implement neural units in a digital formulation which enables the weights to be quantised to 8-bits and the activations to 9-bits. A novel methodology is introduced to enable the accuracy of sigma–pi units to be increased by expanding their internal state space. We, also, introduce a novel means of implementing bit-streams in ring memories instead of utilising shift registers. The investigation utilises digital “Higher Order” sigma–pi nodes and studies continuous input RAM-based sigma–pi units. The units are trained with the backpropagation learning regime to learn functions to a high accuracy. The neural model is the sigma–pi units which can be implemented in digital microelectronic technology.
                  The ability to perform tasks that require the input of real-valued information, is one of the central requirements of any cognitive system that utilises artificial neural network methodologies. In this article we present recent research which investigates a technique that can be used for mapping accurate real-valued functions to RAM-nets. One of our goals was to achieve accuracies of better than 1% for target output functions in the range Y∈[0,1], this is equivalent to an average Mean Square Error (MSE) over all training vectors of 0.0001 or an error modulus of 0.01. We present a development of the sigma–pi node which enables the provision of high accuracy outputs. The sigma–pi neural model was initially developed by Gurney (Learning in nets of structured hypercubes. PhD Thesis, Department of Electrical Engineering, Brunel University, Middlessex, UK, 1989; available as Technical Memo CN/R/144). Gurney's neuron models, the Time Integration Node (TIN), utilises an activation that was derived from a bit-stream. In this article we present a new methodology for storing sigma–pi node's activations as single values which are averages.
                  In the course of the article we state what we define as a real number; how we represent real numbers and input of continuous values in our neural system. We show how to utilise the bounded quantised site-values (weights) of sigma–pi nodes to make training of these neurocomputing systems simple, using pre-calculated look-up tables to train the nets. In order to meet our accuracy goal, we introduce a means of increasing the bandwidth capability of sigma–pi units by expanding their internal state-space. In our implementation we utilise bit-streams when we calculate the real-valued outputs of the net. To simplify the hardware implementation of bit-streams we present a method of mapping them to RAM-based hardware using ‘ring memories’. Finally, we study the sigma–pi units’ ability to generalise once they are trained to map real-valued, high accuracy, continuous functions. We use sigma–pi units as they have been shown to have shorter training times than their analogue counterparts and can also overcome some of the drawbacks of semi-linear units (Gurney, 1992. Neural Networks, 5, 289–303).",space,947
10.1016/s0031-3203(98)00159-9,filtered,Pattern Recognition,scopus,1999-07-01,sciencedirect,curvature scale-space-driven object recognition with an indexing scheme based on artificial neural networks,https://api.elsevier.com/content/abstract/scopus_id/0033167353,"This paper addresses the problem of recognizing real flat objects from two-dimensional images. In particular, a new object recognition technique which performs under occlusion and geometric transformations is presented. The method has mainly been designed to handle complex objects and incorporates two main ideas. First, matching operates hierarchically, guided by a curvature scale space segmentation scheme, and takes advantage of important object features, that is, features which distinguish an object from other objects. This is different from many classical approaches which employ a rather large number of very local features. Second, the model database is built by using artificial neural networks (ANNs). This is also different from traditional approaches where classical indexing schemes, such as hashing, are utilized to organize and search the model database. Important object features are obtained in two steps: first, by segmenting the object boundary at multiple scales using its resampled curvature scale space (RCSS) and second, by concentrating at each scale separately, searching for groups of segments which distinguish an object from other objects. These groups of segments are then used to build a model database which stores associations between segments and models. The model database is implemented using a set of ANNs which provide the essential mechanism not only for establishing correct associations between groups of segments and models but also for enabling efficient searching and robust retrieval. The method has been tested using both artificial and real data illustrating good performance.",space,948
10.1016/s0167-9236(99)00055-x,filtered,Decision Support Systems,scopus,1999-01-01,sciencedirect,partitioning-based clustering for web document categorization,https://api.elsevier.com/content/abstract/scopus_id/0033323923,"Clustering techniques have been used by many intelligent software agents in order to retrieve, filter, and categorize documents available on the World Wide Web. Clustering is also useful in extracting salient features of related Web documents to automatically formulate queries and search for other similar documents on the Web. Traditional clustering algorithms either use a priori knowledge of document structures to define a distance or similarity among these documents, or use probabilistic techniques such as Bayesian classification. Many of these traditional algorithms, however, falter when the dimensionality of the feature space becomes high relative to the size of the document space. In this paper, we introduce two new clustering algorithms that can effectively cluster documents, even in the presence of a very high dimensional feature space. These clustering techniques, which are based on generalizations of graph partitioning, do not require pre-specified ad hoc distance functions, and are capable of automatically discovering document similarities or associations. We conduct several experiments on real Web data using various feature selection heuristics, and compare our clustering schemes to standard distance-based techniques, such as hierarchical agglomeration clustering, and Bayesian classification methods, such as AutoClass.",space,949
10.1016/s0921-8890(96)00026-7,filtered,Robotics and Autonomous Systems,scopus,1997-01-01,sciencedirect,efficient parallel processing for depth calculation using stereo,https://api.elsevier.com/content/abstract/scopus_id/0031121096,"Stereo vision generates the depth map of a scene by fusing information in images taken from two or more views. Stereo is a computationally intensive task and an efficient real time stereo application needs either a dedicated hardware or a parallel computer. This paper proposes a parallel stereo algorithm developed at CAIR using both edges and regions as features for matching. The algorithm employs an intelligent stereo matching technique that reduces the search space for correspondence thereby reducing the computational load. A load balancing scheme is also proposed for further improving the efficiency of the algorithm, with the increase in the number of nodal processors. The algorithm is implemented on a PACE parallel processor developed at the DRDO laboratory ANURAG at Hyderabad, India, and its capability is demonstrated through an application example.",space,950
10.1016/0004-3702(94)00089-j,filtered,Artificial Intelligence,scopus,1995-01-01,sciencedirect,cabins: a framework of knowledge acquisition and iterative revision for schedule improvement and reactive repair,https://api.elsevier.com/content/abstract/scopus_id/0029332288,"Practical scheduling problems generally require allocation of resources in the presence of a large, diverse and typically conflicting set of constraints and optimization criteria. The ill-structuredness of both the solution space and the desired objectives make scheduling problems difficult to formalize. This paper describes a case-based learning method for acquiring context-dependent user optimization preferences and tradeoffs and using them to incrementally improve schedule quality in predictive scheduling and reactive schedule management in response to unexpected execution events. The approach, implemented in the CABINS system, uses acquired user preferences to dynamically modify search control to guide schedule improvement. During iterative repair, cases are exploited for: (1) repair action selection, (2) evaluation of intermediate repair results and (3) recovery from revision failures. The method allows the system to dynamically switch between repair heuristic actions, each of which operates with respect to a particular local view of the problem and offers selective repair advantages. Application of a repair action tunes the search procedure to the characteristics of the local repair problem. This is achieved by dynamic modification of the search control bias. There is no a priori characterization of the amount of modification that may be required by repair actions. However, initial experimental results show that the approach is able to (a) capture and effectively utilize user scheduling preferences that were not present in the scheduling model, (b) produce schedules with high quality, without unduly sacrificing efficiency in predictive schedule generation and reactive response to unpredictable execution events along a variety of criteria that have been recognized as important in real operating environments.",space,951
10.1016/0098-1354(94)e0034-k,filtered,Computers and Chemical Engineering,scopus,1995-01-01,sciencedirect,building a chemical process design system within soar-1. design issues,https://api.elsevier.com/content/abstract/scopus_id/0029236841,"We explore the potential to include automatic learning in a design agent by implementing a simple distillation sequencing system, CPD-Soar, within Soar. Soar is an integrated software architecture with a build-in set of mechanisms for exhibiting intelligent behavior, including problem-solving, learning and interaction with the environment. Soar has a number of scientific uses: computer scientists build artificially intelligent agents with Soar as a foundation, and cognitive psychologists use Soar to model human cognition. CPD-Soar illustrates how design-related tasks can be cast within Soar's framework, hence demonstrating the functioning and potential of its problem- solving and learning mechanisms. This simple example system, which involves computations with real numbers, automatically learns things which are too specific, leading to the hypothesis that the generalization an agent infers from specific examples is strongly dependent upon the model the agent brings to the learning process.
                  We introduced the Soar architecture as a vehicle for developing design systems with capabilities seen to be important for chemical process domains but missing in most existing design systems. We reported upon CPD-Soar, a system developed within the Soar framework, and described in depth its tasks, problem-space structure, operation and performance.
                  The construction of CPD-Soar was a valuable exercise for two main reasons: one, it provided evidence that the mechanisms present in Soar can provide design systems with useful abilities, and two, the act of creating the system was useful in distinguishing between those aspects of the task domain that are well understood from those that are not. Selecting among competent evaluation functions and learning within numerically-intensive domains were two areas identified as not being well understood.
                  Our observation of CPD-Soar's learning behavior lead us to postulate an hypothesis about learning: namely, that the richer the model an agent has of its evaluation functions, the more general its learning will be.",space,952
10.1016/0893-6080(91)90062-a,filtered,Neural Networks,scopus,1991-01-01,sciencedirect,defanet-a deterministic neural network concept for function approximation,https://api.elsevier.com/content/abstract/scopus_id/0026375514,"A deterministic neural network concept for a “universal approximator” is proposed. The network has two hidden layers; only the synapses of the output layer are required to be plastic and only those depend on the function to be approximated. It is shown that a DEterministic Function Approximation Network (DEFAnet) allows to approximate an arbitrary continuous function from the finite-dimensional unit interval into the finite-dimensional real space with arbitrary accuracy; arbitrary Boolean functions may be implemented exactly in a simple subset of DEFAnets. In a supervised learning scheme, convergence to the desired function is guaranteed; back propagation of errors is not required. The concept is also open for reinforcement learning. In addition, when the topology of the network is determined according to the DEFAnet concept, it is possible to calculate all plastic synaptic weights in closed form, thus reducing the training considerably or replacing it altogether. Efficient algorithms for the calculation of synapse weights are given.",space,953
10.1016/0094-5765(89)90052-0,filtered,Acta Astronautica,scopus,1989-01-01,sciencedirect,"telescience and microgravity. impact on future facilities, ground segments and operations",https://api.elsevier.com/content/abstract/scopus_id/0024856605,"Scientific activities related to experimentation in long duration microgravity missions can only be accomplished by the implementation of the Telescience Concept.
                  Telescience is in fact the logical answer to the need of an intelligent interactive conduct of experiments, to the lack (or very little availability) of crew time on board of the Segments of the Columbus project and to the PIs demand for decentralized operations. Telescience could also be seen as the preparative phase for the ultimate, future exploitation of Microgravity by means of Expert Systems that will utilize AI and Robotics for routine operations (Data Factories, Space Productions and Commercial Enterprises).
                  The implications of Telescience on future Space Activities is reviewed with reference to the Principal Investigator Activities, Crew Members Roles and Facilities. The possibilities offered by newly designed Facilities to be operated in Telescience are pointed out with reference to the scientific objectives that would not be achieved otherwise.
                  Diagnostic facilities (mainly non invasive) that provide digital measurements to be inputted (in real time) into numerical codes for computation of field parameters are being considered. Ground Segment Structure, User Support Centers Organization and Test Bedding activities will be discussed as essential factors of the Telescience Scenario of the Multiuser, permanent platform Facilities for the Microgravity disciplines (Material, Fluid, Life and Engineering Science).",space,954
10.1016/0734-189x(83)90095-6,filtered,"Computer Vision, Graphics and Image Processing",scopus,1983-01-01,sciencedirect,an implementation of a computational theory of visual surface interpolation,https://api.elsevier.com/content/abstract/scopus_id/0020642298,"Computational theories of structure-from-motion (Ullman, The Interpretation of Visual Motion, MIT Press, 1979) and stereo vision (Marr and Poggio, Proc. R. Soc. London Ser. B 
                        204, 1979, 301–328) only specify the computation of three-dimensional surface information at particular points in the image. Yet, the visual perception is clearly of complete surfaces. To account for this, a computational theory of the interpolation of surfaces from visual information was presented in Grimson. (From Images to Surfaces: A Computational Study of the Human Early Visual System, MIT Press, 1981; 
                        A Computational Theory of Visual Surface Interpolation, MIT Artificial Intelligence Lab Memo, No. 613, 1981; and 
                        Philos. Trans. R. Soc. London Ser. B 
                        298, 1982, 395–427). The problem is constrained by the fact that the surface must agree with the information from stereo or motion correspondence, and not vary radically between these points. Using the image irradiance equation (Horn, MIT Project MAC Tech. Rep. MACTR-79, 1970; 
                        The Psychology of Computer Vision, McGraw-Hill, 1975; and 
                        Artif. Intell. 
                        8, 1977, 201–231), an explicit form of this surface consistency constraint can be derived (Grimson, MIT Artificial Intelligence Lab Memo, No. 646, 1981). To determine which of two possible surfaces is more consistent with the surface consistency constraint, one must be able to compare the two surfaces. To do this, a functional from the space of possible functions to the real numbers is required. In this way, the surface most consistent with the visual information will be that which minimizes the functional. In Grimson, a set of conditions was derived which ensures that the functional has a unique minimal surface. Based on these conditions, a number of possible functionals were proposed. In Brady and Horn (MIT Artificial Intelligence Lab Memo, No. 654, 1981), it was shown that this set of possible functionals forms a vector space, spanned by the functional of quadratic variation and the functional of the square Laplacian. Analytic arguments were given in Grimson to support the choice of the quadratic variation as the functional whose minimal surface is the “best” interpolation of the known points. In this paper, algorithms for computing the minimal surface are derived. Using this implementation of the computational theory derived in Grimson the differences between minimal surfaces computed using quadratic variation and those computed using the square Laplacian are illustrated. These examples provide additional support for the choice of the quadratic variation. The performance of the algorithm in interpolating both random dot and natural stereograms which have been processed by the Marr-Poggio stereo algorithm (Grimson, Computing Shape Using a Theory of Human Stereo Vision, Ph.D. Thesis, MIT, Cambridge, 1980 and 
                        Philos. Trans. R. Soc. London Ser. B 
                        292, 1981, 217–253) is also illustrated.",space,955
10.3390/s21217278,filtered,core,'MDPI AG',2021-11-01 00:00:00,core,human activity recognition: a dynamic inductive bias selection perspective,,"In this article, we study activity recognition in the context of sensor-rich environments. In these environments, many different constraints arise at various levels during the data generation process, such as the intrinsic characteristics of the sensing devices, their energy and computational constraints, and their collective (collaborative) dimension. These constraints have a fundamental impact on the final activity recognition models as the quality of the data, its availability, and its reliability, among other things, are not ensured during model deployment in real-world configurations. Current approaches for activity recognition rely on the activity recognition chain which defines several steps that the sensed data undergo: This is an inductive process that involves exploring a hypothesis space to find a theory able to explain the observations. For activity recognition to be effective and robust, this inductive process must consider the constraints at all levels and model them explicitly. Whether it is a bias related to sensor measurement, transmission protocol, sensor deployment topology, heterogeneity, dynamicity, or stochastic effects, it is essential to understand their substantial impact on the quality of the data and ultimately on activity recognition models. This study highlights the need to exhibit the different types of biases arising in real situations so that machine learning models, e.g., can adapt to the dynamicity of these environments, resist sensor failures, and follow the evolution of the sensors’ topology. We propose a metamodeling approach in which these biases are specified as hyperparameters that can control the structure of the activity recognition models. Via these hyperparameters, it becomes easier to optimize the inductive processes, reason about them, and incorporate additional knowledge. It also provides a principled strategy to adapt the models to the evolutions of the environment. We illustrate our approach on the SHL dataset, which features motion sensor data for a set of human activities collected in real conditions. The obtained results make a case for the proposed metamodeling approach; noticeably, the robustness gains achieved when the deployed models are confronted with the evolution of the initial sensing configurations. The trade-offs exhibited and the broader implications of the proposed approach are discussed with alternative techniques to encode and incorporate knowledge into activity recognition models",space,956
10.2514/6.2021-1119,filtered,core,'American Institute of Aeronautics and Astronautics (AIAA)',2021-01-01 00:00:00,core,intelligent adaptive control using ladp and iadp applied to f-16 aircraft with imperfect measurements,,"Linear Approximate Dynamic Programming (LADP) and Incremental Approximate Dynamic Programming (IADP) are Reinforcement Learning methods that seek to contribute to the field of Adaptive Flight Control. This paper assesses their performance and convergence, as well as the impact of sensor noise on policy convergence, online system identification, performance and control surface deflection. After summarising their theory and derivation with full state (FS) and output feedback (OPFB), they are implemented on the linearised longitudinal F16 model. In order to establish an objective performance comparison, their hyper-parameters were tuned with an evolutionary algorithm: Particle Swarm Optimisation (PSO). Results show that LADP and IADP have the same performance in the presence of FS feedback, whereas LADP outperforms IADP when only OPFB is available. Output noise causes LADP based on OPFB to diverge. In the case of IADP based on OPFB, sensor noise improves the performance due to a better exploration of the solution space. The present research aims at bridging the gap between the discussed ADP algorithms and real world systems.Virtual/online event due to COVID-19Control & Simulatio",space,957
10.29138/ijebd.v4i2.1117,filtered,core,'Narotama University',2021-03-29 00:00:00,core,covid -19: the impact on malaysian visual arts scene,https://core.ac.uk/download/401914974.pdf,"Purpose: This paper discusses the impact of COVID-19 on the Visual Arts industry in Malaysia. In general, this pandemic has affected various forms of artistic activities and the income of visual arts artists and galleries. The cancellation of art projects and exhibitions has greatly affected the artist's source of income as well as disrupted the sale of works and forms of art appreciation. The crisis has also opened up a new form to the visual arts industry by looking at alternative approaches to the continuity of the arts field by switching to virtual or online methods. This emerging crisis of COVID-19 might be the starting point for all art practitioners including artists, art critics, galleries/museums, collectors, and curators in using the online space to continue to capitalize on and expand the Visual Arts industry.
Design/methodology/approach: Review approach.
Findings: The COVID-19 pandemic has made a huge impact on the country's Visual Arts industry where a wide range of art activities cannot be implemented and opened up opportunities for online activities
Practical implications: Exhibition and sale of works through online approach has become one of the main methods that support the Visual Arts industry with the application of a combination of the latest technologies such as VR and AI that enable the representation of real experiences in the context of art appreciation.
Originality/value: This paper is original.
Paper type: This paper can be categorized as a viewpoin",space,958
10.1039/d1nr01109j,filtered,core,'Royal Society of Chemistry (RSC)',2021-01-01 00:00:00,core,enabling autonomous scanning probe microscopy imaging of single molecules with deep learning,,"Scanning probe microscopies allow investigating surfaces at the nanoscale, in real space and with unparalleled signal-to-noise ratio. However, these microscopies are not used as much as it would be expected considering their potential. The main limitations preventing a broader use are the need of experienced users, the difficulty in data analysis and the time-consuming nature of experiments that require continuous user supervision. In this work, we addressed the latter and developed an algorithm that controlled the operation of an Atomic Force Microscope (AFM) that, without the need of user intervention, allowed acquiring multiple high-resolution images of different molecules. We used DNA on mica as a model sample to test our control algorithm, which made use of two deep learning techniques that so far have not been used for real time SPM automation. One was an object detector, YOLOv3, which provided the location of molecules in the captured images. The second was a Siamese network that could identify the same molecule in different images. This allowed both performing a series of images on selected molecules while incrementing the resolution, as well as keeping track of molecules already imaged at high resolution, avoiding loops where the same molecule would be imaged an unlimited number of times. Overall, our implementation of deep learning techniques brings SPM a step closer to full autonomous operation",space,959
10.3390/fi13080202,filtered,core,'MDPI AG',2021-08-01 00:00:00,core,education 4.0: teaching the basis of motor imagery classification algorithms for brain-computer interfaces,,"Education 4.0 is looking to prepare future scientists and engineers not only by granting them with knowledge and skills but also by giving them the ability to apply them to solve real life problems through the implementation of disruptive technologies. As a consequence, there is a growing demand for educational material that introduces science and engineering students to technologies, such as Artificial Intelligence (AI) and Brain–Computer Interfaces (BCI). Thus, our contribution towards the development of this material is to create a test bench for BCI given the basis and analysis on how they can be discriminated against. This is shown using different AI methods: Fisher Linear Discriminant Analysis (LDA), Support Vector Machines (SVM), Artificial Neural Networks (ANN), Restricted Boltzmann Machines (RBM) and Self-Organizing Maps (SOM), allowing students to see how input changes alter their performance. These tests were done against a two-class Motor Image database. First, using a large frequency band and no filtering eye movement. Secondly, the band was reduced and the eye movement was filtered. The accuracy was analyzed obtaining values around 70∼80% for all methods, excluding SVM and SOM mapping. Accuracy and mapping differentiability increased for some subjects for the second scenario 70∼85%, meaning either their band with the most significant information is on that limited space or the contamination because of eye movement was better mitigated by the regression method. This can be translated to saying that these methods work better under limited spaces. The outcome of this work is useful to show future scientists and engineers how BCI experiments are conducted while teaching them the basics of some AI techniques that can be used in this and other several experiments that can be carried on the framework of Education 4.0",space,960
10.1029/2019gl086615,filtered,core,'American Geophysical Union (AGU)',2021-03-08 00:00:00,core,predicting imminence of analog megathrust earthquakes with machine learning: implications for monitoring subduction zones,,"Subduction zones are monitored using space geodesy with increasing resolution, with the aim of better capturing the deformation accompanying the seismic cycle. Here, we investigate data characteristics that maximize the performance of a machine learning binary classifier predicting slip‐event imminence. We overcome the scarcity of recorded instances from real subduction zones using data from a seismotectonic analog model monitored with a spatially dense, continuously recording onshore geodetic network. We show that a 70–85 km‐wide coastal swath recording interseismic deformation gives the most important information on slip imminence. Prediction performances are mainly influenced by the alarm duration (amount of time that we consider an event as imminent), with density of stations and record length playing a secondary role. The techniques developed in this study are most likely applicable in regions of slow earthquakes, where stick‐slip‐like failures occur at time intervals of months to years.Plain Language Summary:
Machine learning, a group of algorithms that produce predictions based on past “experience,” has been successfully used to predict various aspects of the earthquake process, including slip imminence. The accuracy of those algorithms depends on a variety of data characteristics, for example, the amount of data used for building the “experience” of the model. We focus on this point using a scaled representation of a seismic subduction zone and a monitoring technique similar to Global Navigation Satellite System. We identify the most useful surface regions to be monitored and the parameter that most strongly influences prediction accuracy for the timing of upcoming laboratory earthquakes. The routine implemented in this study could be used to predict the onset and extent of slow earthquakes.Key Points:


We investigate the performances of a binary classifier predicting slip‐event imminence in analog models of megathrust seismic cycling.
A 70–85 km‐wide coastal swath is the region producing the most important information for the imminence classification.
Length of time that we consider an event imminent plays a primary role in tuning the performances of a binary classifier predicting the imminence of analog earthquakes.DAAD‐Prim",space,961
10.3389/frcmn.2021.739414,filtered,core,'Frontiers Media SA',2021-10-01 00:00:00,core,entropy-driven stochastic federated learning in non-iid 6g edge-ran,https://core.ac.uk/download/490561694.pdf,"Scalable and sustainable AI-driven analytics are necessary to enable large-scale and heterogeneous service deployment in sixth-generation (6G) ultra-dense networks. This implies that the exchange of raw monitoring data should be minimized across the network by bringing the analysis functions closer to the data collection points. While federated learning (FL) is an efficient tool to implement such a decentralized strategy, real networks are generally characterized by time- and space-varying traffic patterns and channel conditions, making thereby the data collected in different points non independent and identically distributed (non-IID), which is challenging for FL. To sidestep this issue, we first introduce a new a priori metric that we call dataset entropy, whose role is to capture the distribution, the quantity of information, the unbalanced structure and the “non-IIDness” of a dataset independently of the models. This a priori entropy is calculated using a multi-dimensional spectral clustering scheme over both the features and the supervised output spaces, and is suitable for classification as well as regression tasks. The FL aggregation operations support system (OSS) server then uses the reported dataset entropies to devise 1) an entropy-based federated averaging scheme, and 2) a stochastic participant selection policy to significantly stabilize the training, minimize the convergence time, and reduce the corresponding computation cost. Numerical results are provided to show the superiority of these novel approaches",space,962
10.1155/2021/5533884,filtered,core,'Hindawi Limited',2021-01-01 00:00:00,core,machine learning-based multitarget tracking of motion in sports video,,"In this paper, we track the motion of multiple targets in sports videos by a machine learning algorithm and study its tracking technique in depth. In terms of moving target detection, the traditional detection algorithms are analysed theoretically as well as implemented algorithmically, based on which a fusion algorithm of four interframe difference method and background averaging method is proposed for the shortcomings of interframe difference method and background difference method. The fusion algorithm uses the learning rate to update the background in real time and combines morphological processing to correct the foreground, which can effectively cope with the slow change of the background. According to the requirements of real time, accuracy, and occupying less video memory space in intelligent video surveillance systems, this paper improves the streamlined version of the algorithm. The experimental results show that the improved multitarget tracking algorithm effectively improves the Kalman filter-based algorithm to meet the real-time and accuracy requirements in intelligent video surveillance scenarios",space,963
10.2514/6.2020-4219,filtered,core,Modeling and Simulation of a Spacecraft Payload Hardware Using Machine Learning Techniques,2020-11-02 00:00:00,core,https://core.ac.uk/download/362647068.pdf,'American Institute of Aeronautics and Astronautics (AIAA)',"Space systems are complex and consist of multiple subsystems. Research and development teams of such complex systems are usually distributed among various institutions and space agencies. This affects the quality of the On-board Software (OBSW) since testing it without having all required subsystems at the software development site can be troublesome. In this paper, we present a data-driven method which can be used to synthesize parts of a system or even an entire system as a black-box model. We exploit the data collected from the real hardware to derive a model using a Machine Learning (ML) algorithm. The proposed model can easily be distributed among development teams and is dedicated to emulate the system for testing the OBSW",space,964
10.4230/lipics.itcs.2020.48,filtered,core,The Computational Cost of Asynchronous Neural Communication,2020-01-01 00:00:00,core,https://core.ac.uk/download/287883823.pdf,LIPIcs - Leibniz International Proceedings in Informatics. 11th Innovations in Theoretical Computer Science Conference (ITCS 2020),"Biological neural computation is inherently asynchronous due to large variations in neuronal spike timing and transmission delays. So-far, most theoretical work on neural networks assumes the synchronous setting where neurons fire simultaneously in discrete rounds. In this work we aim at understanding the barriers of asynchronous neural computation from an algorithmic perspective. We consider an extension of the widely studied model of synchronized spiking neurons [Maass, Neural Networks 97] to the asynchronous setting by taking into account edge and node delays.
- Edge Delays: We define an asynchronous model for spiking neurons in which the latency values (i.e., transmission delays) of non self-loop edges vary adversarially over time. This extends the recent work of [Hitron and Parter, ESA\u2719] in which the latency values are restricted to be fixed over time. Our first contribution is an impossibility result that implies that the assumption that self-loop edges have no delays (as assumed in Hitron and Parter) is indeed necessary. Interestingly, in real biological networks self-loop edges (a.k.a. autapse) are indeed free of delays, and the latter has been noted by neuroscientists to be crucial for network synchronization. 
To capture the computational challenges in this setting, we first consider the implementation of a single NOT gate. This simple function already captures the fundamental difficulties in the asynchronous setting. Our key technical results are space and time upper and lower bounds for the NOT function, our time bounds are tight. In the spirit of the distributed synchronizers [Awerbuch and Peleg, FOCS\u2790] and following [Hitron and Parter, ESA\u2719], we then provide a general synchronizer machinery. Our construction is very modular and it is based on efficient circuit implementation of threshold gates. The complexity of our scheme is measured by the overhead in the number of neurons and the computation time, both are shown to be polynomial in the largest latency value, and the largest incoming degree ? of the original network.
- Node Delays: We introduce the study of asynchronous communication due to variations in the response rates of the neurons in the network. In real brain networks, the round duration varies between different neurons in the network. Our key result is a simulation methodology that allows one to transform the above mentioned synchronized solution under edge delays into a synchronized under node delays while incurring a small overhead w.r.t space and time",space,965
10.1103/physrevapplied.13.034075,filtered,core,Auto-tuning of double dot devices in situ with machine learning,2020-04-01 00:00:00,core,http://arxiv.org/abs/1909.08030,'American Physical Society (APS)',"The current practice of manually tuning quantum dots (QDs) for qubit
operation is a relatively time-consuming procedure that is inherently
impractical for scaling up and applications. In this work, we report on the
{\it in situ} implementation of a recently proposed autotuning protocol that
combines machine learning (ML) with an optimization routine to navigate the
parameter space. In particular, we show that a ML algorithm trained using
exclusively simulated data to quantitatively classify the state of a double-QD
device can be used to replace human heuristics in the tuning of gate voltages
in real devices. We demonstrate active feedback of a functional double-dot
device operated at millikelvin temperatures and discuss success rates as a
function of the initial conditions and the device performance. Modifications to
the training network, fitness function, and optimizer are discussed as a path
toward further improvement in the success rate when starting both near and far
detuned from the target double-dot range.Comment: 9 pages, 7 figure",space,966
10.1051/0004-6361/201833390,filtered,core,A new method for unveiling Open Clusters in Gaia: new nearby Open Clusters confirmed by DR2,2020-07-13 00:00:00,core,https://core.ac.uk/download/328873796.pdf,'EDP Sciences',"Context. The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in astronomy. It includes precise astrometric data (positions, proper motions, and parallaxes) for more than 1.3 billion sources, mostly stars. To analyse such a vast amount of new data, the use of data-mining techniques and machine-learning algorithms is mandatory. Aims. A great example of the application of such techniques and algorithms is the search for open clusters (OCs), groups of stars that were born and move together, located in the disc. Our aim is to develop a method to automatically explore the data space, requiring minimal manual intervention. Methods. We explore the performance of a density-based clustering algorithm, DBSCAN, to find clusters in the data together with a supervised learning method such as an artificial neural network (ANN) to automatically distinguish between real OCs and statistical clusters. Results. The development and implementation of this method in a five-dimensional space (l, b, ϖ, μα*, μδ) with the Tycho-Gaia Astrometric Solution (TGAS) data, and a posterior validation using Gaia DR2 data, lead to the proposal of a set of new nearby OCs. Conclusions. We have developed a method to find OCs in astrometric data, designed to be applied to the full Gaia DR2 archive",space,967
10.20998/2079-0023.2020.01.13,filtered,core,Технологія управління знаннями про віртуальне просування,2020-01-01 00:00:00,core,https://core.ac.uk/download/333611276.pdf,'National Technical University Kharkiv Polytechnic Institute',"The article presents a new concept of “Management of knowledge about virtual promotion” on the Internet. Usually a real product or service is being divided into four components (product, price, promotion and place) in accordance with the theory of marketing. One of the components is a product promotion. But now this element is becoming a fully virtual tool. It is necessary to consider product promotion as an image or a copy of a real product in a virtual space that lives in parallel on the network. Therefore, the objective of the paper is the presentation of a new object of research based on the experience of more than thirty real projects performed in Ukraine, USA, Europe and Canada. We regard the promotion as a software product, which works according to principles of knowledge management and machine learning. It is proposed that virtual promotion is characterized by four views: customer or user, data, technology and marketing. Thus, the structure of virtual promotion business process was presented. It includes four steps: selection of hypertext sources, knowledge representation and extraction, semantic kernel building and quality criterion evaluation to stop the process. Based on the process structure the research tasks were identified. The central task is semantic kernel forming. Then the software architecture was developed. IT solution contains CRM system as accounting tool and Web site as an image of virtual promotion. CRM plays main role as a commander center. Here we form semantic kernel and then send it via marketing channels such as Web site, telegram or viber accounts. Another part of IT solution is Web service such as Bing API or Google API. They help us to build the kernel. Also the paper demonstrates the list of future tasks that should be solved and the example of real project of proposed approach.У статті представлена нова технологія «управління знаннями про віртуальне просування» в мережі Інтернет. Звичайно реальний продукт або послуга характеризуються наступними компонентами: продукт, ціна, просування і місце, згідно з теорією маркетингу. Одним з компонентів є просування товару. Однак зараз цей елемент стає повністю віртуальним інструментом. Необхідно розглядати просування продукту як відображення або копію реального продукту у віртуальному просторі. Це відображення існує паралельно у мережі та безпосередньо впливає на реальний продукт чи послугу. Тому ціллю статті є презентація нового об’єкта дослідження, поява якого основана на досвіді виконання більш ніж тридцяти реальних проектів в Україні, США, Європі та Канаді. Ми працюємо відповідно до принципів управління знаннями і машинного навчання. Передбачається, що віртуальне просування характеризується чотирма репрезентаціями: клієнт або користувач, дані, технологія та маркетинг. Далі була представлена структура бізнес–процесу віртуального просування. Він включає чотири етапи: вибір джерел гіпертексту, подання та витяг знань, побудова семантичного ядра і оцінка критерію якості для зупинки процесу. На основі структури процесу були визначені задачі дослідження. Центральна задача – формування семантичного ядра. Потім була розроблена архітектура програмного забезпечення. ІТ рішення містить CRM систему в якості інструменту обліку та Веб сайт як образ віртуального просування. CRM грає роль командного центру. Тут формується семантичне ядро і потім відправляється через маркетингові канали, такі як Веб сайт, Телеграм канали або профілі в Вайбері. Інша частина ІТ рішення – це Веб сервіс, такий як Bing API або Google API. Вони допомагають нам побудувати ядро. Також в статті наведено список майбутніх завдань, які необхідно вирішити, і приклад реальних проектів в рамках запропонованого підходу",space,968
10.1088/1538-3873/ab936e,filtered,core,Design and operation of the ATLAS Transient Science Server,2020-06-02 00:00:00,core,http://arxiv.org/abs/2003.09052,'IOP Publishing',"The Asteroid Terrestrial impact Last Alert System (ATLAS) system consists of
two 0.5m Schmidt telescopes with cameras covering 29 square degrees at plate
scale of 1.86 arcsec per pixel. Working in tandem, the telescopes routinely
survey the whole sky visible from Hawaii (above $\delta > -50^{\circ}$) every
two nights, exposing four times per night, typically reaching $o < 19$
magnitude per exposure when the moon is illuminated and $c < 19.5$ per exposure
in dark skies. Construction is underway of two further units to be sited in
Chile and South Africa which will result in an all-sky daily cadence from 2021.
Initially designed for detecting potentially hazardous near earth objects, the
ATLAS data enable a range of astrophysical time domain science. To extract
transients from the data stream requires a computing system to process the
data, assimilate detections in time and space and associate them with known
astrophysical sources. Here we describe the hardware and software
infrastructure to produce a stream of clean, real, astrophysical transients in
real time. This involves machine learning and boosted decision tree algorithms
to identify extragalactic and Galactic transients. Typically we detect 10-15
supernova candidates per night which we immediately announce publicly. The
ATLAS discoveries not only enable rapid follow-up of interesting sources but
will provide complete statistical samples within the local volume of 100 Mpc. A
simple comparison of the detected supernova rate within 100 Mpc, with no
corrections for completeness, is already significantly higher (factor 1.5 to 2)
than the current accepted rates.Comment: 27 pages, 12 figures. Accepted for publication in PASP on 2020 May 1",space,969
https://core.ac.uk/download/232648119.pdf,filtered,core,ECI Digital Archives,2019-06-05 00:00:00,core,"too far, too small, too dark, too foggy: on the use of machine learning for computational imaging problems",,"Computational imaging system design involves the joint optimization of hardware and software to deliver high fidelity images to a human user or artificial intelligence (AI) algorithm. For example, in medical tomography CAT scanners produce non-invasively cross-sectional images of the patient’s organs and then medical professionals or, increasingly, automated recognition systems perform diagnosis and decide upon a course of treatment. We refer to this operation of AI as image interpretation.
 
This talk is about a different paradigm where machine learning (ML) is used at the step of image formation itself, i.e. for image reconstruction rather than interpretation. The ML algorithm, typically implemented as a deep neural network (DNN), is programmed using physically generated or rigorously simulated examples of objects and their associated signals produced on the sensor (or camera.) The training phase consists of adjusting the connection weights of the DNN until it becomes possible, given the sensor signal from a hitherto unseen object, for the DNN to yield an accurate estimate of the object’s spatial structure.
The ML approach to solving inverse problems in such fashion has its roots in optimization methods employed long before in computational imaging, compressed sensing and dictionaries in particular. By replacing the proximal gradient step of the optimization with a DNN [K. Gregor & Y. LeCun, ICML 2010], it becomes possible to learn priors other than sparsity, and restrict the object class almost arbitrarily to facilitate the solution of “hard” inverse problems, e.g. highly ill-posed and highly noisy at the same time. Moreover, execution becomes very fast because pre-trained DNNs mostly consist of forward computations which can easily be run at real time, whereas traditional compressed sensing optimization routines are generally iterative. DNN training is time consuming too, but it is only run up front while developing the algorithm; it is not a burden during operation. Unfortunately, however, with the DNN approach some of the nice properties of compressed sensing are lost, most notably convexity.
In this talk we will review these basic developments and then discuss in detail their application to the specific problem of phase retrieval in lensless (free-space propagation) or defocused imaging systems. More specifically, we will investigate the impact of the power spectral density of the training example database on the quality of the reconstructions. We will review a sequence of papers where we first ignored this problem [A. Sinha et al, Optica 4:1117, 2017], then improved it in an ad hoc way by pre-modulation of the training examples [Li Shuai et al, Opt. Express 26:29340, 2018] and finally devised a dual-band approach where the signal is first separated into its low- and high-frequency components, their respective reconstructions are obtained by two DNNs trained separately and then re-composed by a third “synthesizer” DNN [Deng Mo et al, arXiv:1811.07945]. We will explain why each new attempt improves resolution and overall fidelity through progressively more balanced treatment of the spatial frequency spectrum.
We will also discuss implications of this method for phase retrieval under extremely low-photon (too dark) conditions [A. Goy et al, Phys. Rev. Lett. 121:243902, 2018] other related inverse problems, e.g. super resolution (too far or too small), and imaging through diffusers (too foggy.",space,970
https://core.ac.uk/download/234708035.pdf,filtered,core,'Knowledge Kingdom Publishing',2019-01-05 00:00:00,core,automatic  human  sperm concentrartion in microscopic videos,10.26415/2572-004X-vol2iss4p301-307,"&nbsp;
Background: Human sperm cell counting analysis is of significant interest to biologists studying sperm function and to medical practitioners evaluating male infertility. Currently the analysis of this assessment is done manually by looking at the sperm samples through a phase-contrast microscope using expert knowledge to do a subjective judgement of the quality.
Aims: to eliminate the subjective and error prone of the manual semen analysis and to avoid inter and intra-laboratory inconsistencies in semen analysis test results
Methods: In this paper we introduce a technique for human sperm concentration. Its principle is based on the execution of three steps: The first step in unavoidable. It concerns the pretreatment of the human sperm microscopic videos which consists of a conversion of the RGB color space into the YCbCr space, the “Gaussian filtering” and the “discrete wavelet filtering”. The second step is devoted to the segmentation of the image into two classes: spermatozoas and the background. To achieve this, we used an edge detection technique “Sobel Contour detector”. The third step is to separate true sperm from false ones. It uses a machine learning technique of type decision trees that consist on two classes classification based on invariant characteristics that are the dimensions of the bounding ellipse of the spermatozoid head as well as its surface.
Results: To test the robustness of our system, we compared our results with those performed manually by andrologists. After results analysis, we can conclude that our system brings a real improvement of precision as well as treatment time which make it might be useful for groups who intend to design new CASA systems.
Conclusion: In this study, we designed and implemented a system for automatic concentration assessment based on machine learning method and image processing techniques",space,971
https://core.ac.uk/download/286729627.pdf,filtered,core,"eScholarship, University of California",2019-01-01 00:00:00,core,"scheduling, characterization and prediction of hpc workloads for distributed computing environments",,"As High Performance Computing (HPC) has grown considerably and is expected to grow even more, effective resource management for distributed computing sys- tems is motivated more than ever. As the computational workloads grow in quantity, it is becoming more crucial to apply efficient resource management and workload scheduling to use resources efficiently while keeping the computational performance reasonably good. The problem of efficiently scheduling workloads on resources while meeting performance standards is hard. Additionally, non-clairvoyance of job dimen- sions makes resource management even harder in real-world scenarios. Our research methodology investigates the scheduling problem compliant for HPC and researches the challenges for deploying the scheduling in real world-scenarios using state of the art machine learning and data science techniques.To this end, this Ph.D. dissertation makes the following core contributions: a) We perform a theoretical analysis of space-sharing, non-preemptive scheduling: we studied this scheduling problem and proposed scheduling algorithms with polyno- mial computation time. We also proved constant upper-bounds for the performance of these algorithms. b) We studied the sensitivity of scheduling algorithms to the accuracy of runtime and devised a meta-learning approach to estimate prediction accuracy for newly submitted jobs to the HPC system. c) We studied the runtime prediction problem for HPC applications. For this purpose, we studied the distri- bution of available public workloads and proposed two different solutions that can predict multi-modal distributions: switching state-space models and Mixture Density Networks. d) We studied the effectiveness of recent recurrent neural network models for CPU usage trace prediction for individual VM traces as well as aggregate CPU usage traces. In this dissertation, we explore solutions to improve the performance of scheduling workloads on distributed systems.We begin by looking at the problem from the theoretical perspective. Modeling the problem mathematically, we first propose a scheduling algorithm that finds a constant approximation of the optimal solution for the problem in polynomial time. We prove that the performance of the algorithm (average completion time is the constant approximation of the performance of the optimal scheduling. We next look at the problem in real-world scenarios. Considering High-Performance Computing (HPC) workload computing environments as the most similar real-world equivalent of our mathematical model, we explore the problem of predicting application runtime. We propose an algorithm to handle the existing uncertainties in the real world and show-case our algorithm with demonstrative effectiveness in terms of response time and resource utilization. After looking at the uncertainty problem, we focus on trying to improve the accuracy of existing prediction approaches for HPC application runtime. We propose two solutions, one based on Kalman filters and one based on deep density mixture networks. We showcase the effectiveness of our prediction approaches by comparing with previous prediction approaches in terms of prediction accuracy and impact on improving scheduling performance. In the end, we focus on predicting resource usage for individual applications during their execution. We explore the application of recurrent neural networks for predicting resource usage of applications deployed on individual virtual machines. To validate our proposed models and solutions, we performed extensive trace-driven simulation and measured the effectiveness of our approaches",space,972
https://riunet.upv.es/bitstream/10251/166901/7/perez-bento%3bgarca-gomez%3bnavarro%20-%20communty%20detecton-based%20deep%20neural%20network%20archtectures%3a%20a%20ful....pdf,filtered,core,'Wiley',2020-09-30 00:00:00,core,community detection-based deep neural network architectures: a fully automated framework based on likert-scale data,10.1002/mma.6567,"[EN] Deep neural networks (DNNs) have emerged as a state-of-the-art tool in very different research fields due to its adaptive power to the decision space since they do not presuppose any linear relationship between data. Some of the main disadvantages of these trending models are that the choice of the network underlying architecture profoundly influences the performance of the model and that the architecture design requires prior knowledge of the field of study. The use of questionnaires is hugely extended in social/behavioral sciences. The main contribution of this work is to automate the process of a DNN architecture design by using an agglomerative hierarchical algorithm that mimics the conceptual structure of such surveys. Although the train had regression purposes, it is easily convertible to deal with classification tasks. Our proposed methodology will be tested with a database containing socio-demographic data and the responses to five psychometric Likert scales related to the prediction of happiness. These scales have been already used to design a DNN architecture based on the subdimension of the scales. We show that our new network configurations outperform the previous existing DNN architectures.The authors thank the support of the project Analysis, quality, and variability of medical data funded by Universitat Politècnica de València. JMGG and JAC acknowledge the support of the H2020 project CrowdHealth (Collective Wisdom Driving Public Health Policies - 727560) funded by the European Comission. JMGG acknowledge and to the In Advance project (Patient-Centred Pathways of Early Palliative Care, Supportive Ecosystems and Appraisal Standard - 825750) funded by the European Comission, too.Perez-Benito, FJ.; Garcia-Gomez, JM.; Navarro Pardo, E.; Conejero, JA. (2020). Community detection-based deep neural network architectures: A fully automated framework based on Likert-scale data. Mathematical Methods in the Applied Sciences. 43(14):8290-8301. https://doi.org/10.1002/mma.6567S829083014314LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444. doi:10.1038/nature14539Antonov, V., Tarkhov, D., & Vasilyev, A. (2018). Unified approach to constructing the neural network models of real objects. Part 1. Mathematical Methods in the Applied Sciences, 41(18), 9244-9251. doi:10.1002/mma.5205Arifovic, J., & Gençay, R. (2001). Using genetic algorithms to select architecture of a feedforward artificial neural network. Physica A: Statistical Mechanics and its Applications, 289(3-4), 574-594. doi:10.1016/s0378-4371(00)00479-9IslamB‐U BaharudinZ RazaM‐Q NallagowndenP.Optimization of neural network architecture using genetic algorithm for load forecasting. In: 5th International Conference on Intelligent and Advanced Systems (ICIAS) 2014;2014:1‐6.KoutníkJ SchmidhuberJ GomezF.Evolving deep unsupervised convolutional networks for vision‐based reinforcement learning. In: Proceedings of the 2014 annual conference on genetic and evolutionary computation ACM;2014:541‐548.VidnerováP NerudaR.Evolving keras architectures for sensor data analysis Federated Conference on Computer Science and Information Systems (FedCSIS) 2017IEEE;2017:109‐112.Albert, R., & Barabási, A.-L. (2002). Statistical mechanics of complex networks. Reviews of Modern Physics, 74(1), 47-97. doi:10.1103/revmodphys.74.47Newman, M., Barabási, A.-L., & Watts, D. J. (2011). The Structure and Dynamics of Networks. doi:10.1515/9781400841356Albert, R., Jeong, H., & Barabási, A.-L. (1999). Diameter of the World-Wide Web. Nature, 401(6749), 130-131. doi:10.1038/43601Redner, S. (1998). How popular is your paper? An empirical study of the citation distribution. The European Physical Journal B, 4(2), 131-134. doi:10.1007/s100510050359Ito, T., Chiba, T., Ozawa, R., Yoshida, M., Hattori, M., & Sakaki, Y. (2001). A comprehensive two-hybrid analysis to explore the yeast protein interactome. Proceedings of the National Academy of Sciences, 98(8), 4569-4574. doi:10.1073/pnas.061034498BarabásiA‐L.Network medicine ‐ from obesity to the diseasom:Mass Medical Soc;2007.Jian, F., & Dandan, S. (2016). Complex Network Theory and Its Application Research on P2P Networks. Applied Mathematics and Nonlinear Sciences, 1(1), 45-52. doi:10.21042/amns.2016.1.00004FortunatoS CastellanoC.Community structure in graphs. In: Computational complexity;2012:490‐512.Kernighan, B. W., & Lin, S. (1970). An Efficient Heuristic Procedure for Partitioning Graphs. Bell System Technical Journal, 49(2), 291-307. doi:10.1002/j.1538-7305.1970.tb01770.xScott, J. (2017). Social Network Analysis. doi:10.4135/9781529716597Amaral, L. A. N., Scala, A., Barthelemy, M., & Stanley, H. E. (2000). Classes of small-world networks. Proceedings of the National Academy of Sciences, 97(21), 11149-11152. doi:10.1073/pnas.200327197Marchiori, M., & Latora, V. (2000). Harmony in the small-world. Physica A: Statistical Mechanics and its Applications, 285(3-4), 539-546. doi:10.1016/s0378-4371(00)00311-3Luo, W., Lu, N., Ni, L., Zhu, W., & Ding, W. (2020). Local community detection by the nearest nodes with greater centrality. Information Sciences, 517, 377-392. doi:10.1016/j.ins.2020.01.001YanardagP VishwanathanS‐V‐N.Deep graph kernels. In: Proceedings of the 21th acm sigkdd international conference on knowledge discovery and data mining;2015:1365‐1374.LiJ ZhangH HanZ RongY ChengH HuangJ.Adversarial attack on community detection by hiding individuals. In: Proceedings of the web conference 2020;2020:917‐927.Khodayar, M., & Wang, J. (2019). Spatio-Temporal Graph Deep Neural Network for Short-Term Wind Speed Forecasting. IEEE Transactions on Sustainable Energy, 10(2), 670-681. doi:10.1109/tste.2018.2844102Pérez-Benito, F. J., Villacampa-Fernández, P., Conejero, J. A., García-Gómez, J. M., & Navarro-Pardo, E. (2019). A happiness degree predictor using the conceptual data structure for deep learning architectures. Computer Methods and Programs in Biomedicine, 168, 59-68. doi:10.1016/j.cmpb.2017.11.004Spector, P. (1992). Summated Rating Scale Construction. doi:10.4135/9781412986038Cacioppo, J. T., & Berntson, G. G. (1994). Relationship between attitudes and evaluative space: A critical review, with emphasis on the separability of positive and negative substrates. Psychological Bulletin, 115(3), 401-423. doi:10.1037/0033-2909.115.3.401Newman, M. E. J., & Girvan, M. (2004). Finding and evaluating community structure in networks. Physical Review E, 69(2). doi:10.1103/physreve.69.026113Clauset, A., Newman, M. E. J., & Moore, C. (2004). Finding community structure in very large networks. Physical Review E, 70(6). doi:10.1103/physreve.70.066111Blondel, V. D., Guillaume, J.-L., Lambiotte, R., & Lefebvre, E. (2008). Fast unfolding of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment, 2008(10), P10008. doi:10.1088/1742-5468/2008/10/p10008Arenas, A., Duch, J., Fernández, A., & Gómez, S. (2007). Size reduction of complex networks preserving modularity. New Journal of Physics, 9(6), 176-176. doi:10.1088/1367-2630/9/6/176Traag, V. A. (2015). Faster unfolding of communities: Speeding up the Louvain algorithm. Physical Review E, 92(3). doi:10.1103/physreve.92.032801HagbergAric SwartPieter S ChultDaniel.Exploring network structure dynamics and function using networkx  Los Alamos National Lab.(LANL) Los Alamos NM (United States);2008.AbadiM BarhamP ChenJ et al.Tensorflow: a system for large‐scale machine learning. In: 12th USENIX symposium on operating systems design and implementation (OSDI 16);2016:265‐283.Joseph, S., Linley, P. A., Harwood, J., Lewis, C. A., & McCollam, P. (2004). Rapid assessment of well-being: The Short Depression-Happiness Scale (SDHS). Psychology and Psychotherapy: Theory, Research and Practice, 77(4), 463-478. doi:10.1348/1476083042555406Carver, C. S. (1997). You want to measure coping but your protocol’ too long: Consider the brief cope. International Journal of Behavioral Medicine, 4(1), 92-100. doi:10.1207/s15327558ijbm0401_6Francis, L. J., Brown, L. B., & Philipchalk, R. (1992). The development of an abbreviated form of the revised Eysenck personality questionnaire (EPQR-A): Its use among students in England, Canada, the U.S.A. and Australia. Personality and Individual Differences, 13(4), 443-449. doi:10.1016/0191-8869(92)90073-xSherbourne, C. D., & Stewart, A. L. (1991). The MOS social support survey. Social Science & Medicine, 32(6), 705-714. doi:10.1016/0277-9536(91)90150-bLawrenceS GilesC‐L TsoiA‐C.Lessons in neural network training: overfitting may be harder than expected. In: AAAI/IAAI;1997:540‐545",space,973
https://core.ac.uk/download/346440943.pdf,filtered,core,Western CEDAR,2020-05-18 00:00:00,core,vikingbot: the starcraft artificial intelligence,,"VikingBot is an automated AI that plays StarCraft by using a combination of machine learning and artificial intelligence. High level strategies are planned using the Brown-UMBC Reinforcement Learning and Planning (BURLAP), library which implements planning algorithms and provides interfaces for defining a domain and models of that domain for planning. For the planning, we used the BURLAP implementation of the sparse sampling algorithm because the time complexity is independent of the size of the state space, and we have to plan quickly in real time. SARSA reinforcement learning is used for a machine learning model that controls combat units. Various other helping functions are distributed to agent classes that aid in the AI in the different areas of the game. These agents are categorized as strategy, economy, combat, and intelligence. By using these parts in tandem VikingBot aims to use less training resources and still be able to play games at a high enough level to beat a human player",space,974
https://core.ac.uk/download/357554258.pdf,filtered,core,,2020-04-24 00:00:00,core,enhancing robot perception using human teammates * (extended abstract),,"ABSTRACT In robotics research, perception is one of the most challenging tasks. In contrast to existing approaches that rely only on computer vision, we propose an alternative method for improving perception by learning from human teammates. To evaluate, we apply this idea to a door detection problem. A set of preliminary experiments has been completed using software agents with real vision data. Our results demonstrate that information inferred from teammate observations significantly improves the perception precision. Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Intelligent agents General Terms Human Factors Keywords Robot perception, robot-human hybrid teams BACKGROUND Robot perception is generally formulated as a problem of analyzing and interpreting various sensory inputs, e.g., camera feeds. In this paper, we approach robot perception from a completely different direction. Our approach utilizes a team setting where a robot collaborates with human teammates. Motivated by the fact that humans possess superior perception skills relative to their robotic counterparts, we investigate how a robot can take advantage of its teammate&apos;s perfect vision. In general, an agent acquires new information through perception, and in turn, the agent chooses actions based on the information acquired. Let us suppose that a robot has a mental model of its human teammate such that a causal relationship is specified between information and actions. Then, by understanding the human mental model of such decision making (or planning), the robot can infer what the human teammate has seen based on the human&apos;s behavior. In other words, an observation of a human teammate can be * This work was conducted (in part) through collaborative participation in the Robotics Consortium sponsored by the U. used as evidence to infer the information perceived by the human. This, in turn, can be used to reduce uncertainty in robot perception. In this paper, we specifically focus on a motivating problem of door detection in the following scenario. Consider a team consisting of a robot and a human performing a military operation in a hostile environment. According to intelligence, armed insurgents are hiding in an urban street. The team is deployed to cover the buildings in the surrounding area, focusing on doors from which the insurgents may try to egress. This is a stealth operation. We make two specific assumptions that are reasonable in a team context. First, observing a teammate is generally more manageable than perceiving an unfamiliar environment. Second, team members share common objectives in reaching the team&apos;s goals. PERCEPTION USING VISION This section describes a purely camera-based approach. First, we find a likely semantic image segmentation using a computer vision technique called stacked hierarchical labeling  It is not constrained by shape grammars and can model a more general class of objects, but its method of constructing a hierarchical segmentation does not convey semantic meaning at a finer detail, as would be necessary to detect doors on a building. It is, however, reliable in detecting buildings as a whole, significantly reducing the search space for detecting doors in the next step. Once buildings are identified, we can apply a broad feature detector to detect likely openings on the façade of the building. As i",space,975
https://riunet.upv.es/bitstream/handle/10251/166659/salcedo-gonz%c3%a1lez%3bsuarez-paez%3besteve%20-%20a%20novel%20method%20of%20spatotemporal%20dynamc%20geo-vsualzaton%20of%20cr....pdf?sequence=1&isallowed=y,filtered,core,'MDPI AG',2020-03-10 00:00:00,core,"a novel method of spatiotemporal dynamic geo-visualization of criminal data, applied to command and control centers for public safety",10.3390/ijgi9030160,"[EN] This article shows a novel geo-visualization method of dynamic spatiotemporal data that allows mobility and concentration of criminal activity to be study. The method was developed using, only and significantly, real data of Santiago de Cali (Colombia), collected by the Colombian National Police (PONAL). This method constitutes a tool that allows criminal influx to be analyzed by concentration, zone, time slot and date. In addition to the field experience of police commanders, it allows patterns of criminal activity to be detected, thereby enabling a better distribution and management of police resources allocated to crime deterrence, prevention and control. Additionally, it may be applied to the concepts of safe city and smart city of the PONAL within the architecture of Command and Control System (C2S) of Command and Control Centers for Public Safety. Furthermore, it contributes to a better situational awareness and improves the future projection, agility, efficiency and decision-making processes of police officers, which are all essential for fulfillment of police missions against crime. Finally, this was developed using an open source software, it can be adapted to any other city, be used with real-time data and be implemented, if necessary, with the geographic software of any other C2S.This work was co-funded by the European Commission as part of H2020 call SEC-12-FCT-2016-thrtopic3 under the project VICTORIA (No. 740754). This publication reflects the views only of the authors, and the Commission cannot be held responsible for any use which may be made of the information contained therein. The authors would like to thank Colombian National Police and its Office of Telematics for their support on development of this project.Salcedo-González, ML.; Suarez-Paez, JE.; Esteve Domingo, M.; Gomez, J.; Palau Salvador, CE. (2020). A Novel Method of Spatiotemporal Dynamic Geo-Visualization of Criminal Data, Applied to Command and Control Centers for Public Safety. ISPRS International Journal of Geo-Information. 9(3):1-17. https://doi.org/10.3390/ijgi9030160S11793Lacinák, M., & Ristvej, J. (2017). Smart City, Safety and Security. Procedia Engineering, 192, 522-527. doi:10.1016/j.proeng.2017.06.090Neumann, M., & Elsenbroich, C. (2016). Introduction: the societal dimensions of organized crime. Trends in Organized Crime, 20(1-2), 1-15. doi:10.1007/s12117-016-9294-zPhillips, P., & Lee, I. (2012). Mining co-distribution patterns for large crime datasets. Expert Systems with Applications, 39(14), 11556-11563. doi:10.1016/j.eswa.2012.03.071Linning, S. J. (2015). Crime seasonality and the micro-spatial patterns of property crime in Vancouver, BC and Ottawa, ON. Journal of Criminal Justice, 43(6), 544-555. doi:10.1016/j.jcrimjus.2015.05.007Spicer, V., & Song, J. (2017). The impact of transit growth on the perception of crime. Journal of Environmental Psychology, 54, 151-159. doi:10.1016/j.jenvp.2017.09.002Beland, L.-P., & Brent, D. A. (2018). Traffic and crime. Journal of Public Economics, 160, 96-116. doi:10.1016/j.jpubeco.2018.03.002Newspaper of National Circulation in Colombia, E.T. Robos en Trancones en El Tintal—Bogotá—.ELTIEMPO.COM https://www.eltiempo.com/bogota/robos-en-trancones-en-el-tintal-168226Nueva Modalidad de Atraco a Conductores en Los Trancones de Bogotá|ELESPECTADOR.COM http://www.elespectador.com/noticias/bogota/nueva-modalidad-de-atraco-conductores-en-los-trancones-de-bogota-articulo-697716Carrillo, P. E., Lopez-Luzuriaga, A., & Malik, A. S. (2018). Pollution or crime: The effect of driving restrictions on criminal activity. Journal of Public Economics, 164, 50-69. doi:10.1016/j.jpubeco.2018.05.007Twinam, T. (2017). Danger zone: Land use and the geography of neighborhood crime. Journal of Urban Economics, 100, 104-119. doi:10.1016/j.jue.2017.05.006Sadler, R. C., Pizarro, J., Turchan, B., Gasteyer, S. P., & McGarrell, E. F. (2017). Exploring the spatial-temporal relationships between a community greening program and neighborhood rates of crime. Applied Geography, 83, 13-26. doi:10.1016/j.apgeog.2017.03.017Roth, R. E., Ross, K. S., Finch, B. G., Luo, W., & MacEachren, A. M. (2013). Spatiotemporal crime analysis in U.S. law enforcement agencies: Current practices and unmet needs. Government Information Quarterly, 30(3), 226-240. doi:10.1016/j.giq.2013.02.001Sustainable Development Goals|UNDP https://www.undp.org/content/undp/en/home/sustainable-development-goals.htmlGiménez-Santana, A., Caplan, J. M., & Drawve, G. (2018). Risk Terrain Modeling and Socio-Economic Stratification: Identifying Risky Places for Violent Crime Victimization in Bogotá, Colombia. European Journal on Criminal Policy and Research, 24(4), 417-431. doi:10.1007/s10610-018-9374-5Kim, S., Jeong, S., Woo, I., Jang, Y., Maciejewski, R., & Ebert, D. S. (2018). Data Flow Analysis and Visualization for Spatiotemporal Statistical Data without Trajectory Information. IEEE Transactions on Visualization and Computer Graphics, 24(3), 1287-1300. doi:10.1109/tvcg.2017.2666146Kounadi, O., & Leitner, M. (2014). Spatial Information Divergence: Using Global and Local Indices to Compare Geographical Masks Applied to Crime Data. Transactions in GIS, 19(5), 737-757. doi:10.1111/tgis.12125Khalid, S., Shoaib, F., Qian, T., Rui, Y., Bari, A. I., Sajjad, M., … Wang, J. (2017). Network Constrained Spatio-Temporal Hotspot Mapping of Crimes in Faisalabad. Applied Spatial Analysis and Policy, 11(3), 599-622. doi:10.1007/s12061-017-9230-xLopez-Cuevas, A., Medina-Perez, M. A., Monroy, R., Ramirez-Marquez, J. E., & Trejo, L. A. (2018). FiToViz: A Visualisation Approach for Real-Time Risk Situation Awareness. IEEE Transactions on Affective Computing, 9(3), 372-382. doi:10.1109/taffc.2017.2741478Xue, Y., & Brown, D. E. (2006). Spatial analysis with preference specification of latent decision makers for criminal event prediction. Decision Support Systems, 41(3), 560-573. doi:10.1016/j.dss.2004.06.007Nakaya, T., & Yano, K. (2010). Visualising Crime Clusters in a Space-time Cube: An Exploratory Data-analysis Approach Using Space-time Kernel Density Estimation and Scan Statistics. Transactions in GIS, 14(3), 223-239. doi:10.1111/j.1467-9671.2010.01194.xAnuar, N. B., & Yap, B. W. (2018). Data Visualization of Violent Crime Hotspots in Malaysia. Soft Computing in Data Science, 350-363. doi:10.1007/978-981-13-3441-2_27Malik, A., Maciejewski, R., Towers, S., McCullough, S., & Ebert, D. S. (2014). Proactive Spatiotemporal Resource Allocation and Predictive Visual Analytics for Community Policing and Law Enforcement. IEEE Transactions on Visualization and Computer Graphics, 20(12), 1863-1872. doi:10.1109/tvcg.2014.2346926Arietta, S. M., Efros, A. A., Ramamoorthi, R., & Agrawala, M. (2014). City Forensics: Using Visual Elements to Predict Non-Visual City Attributes. IEEE Transactions on Visualization and Computer Graphics, 20(12), 2624-2633. doi:10.1109/tvcg.2014.2346446Hu, Y., Wang, F., Guin, C., & Zhu, H. (2018). A spatio-temporal kernel density estimation framework for predictive crime hotspot mapping and evaluation. Applied Geography, 99, 89-97. doi:10.1016/j.apgeog.2018.08.001Yang, D., Heaney, T., Tonon, A., Wang, L., & Cudré-Mauroux, P. (2017). CrimeTelescope: crime hotspot prediction based on urban and social media data fusion. World Wide Web, 21(5), 1323-1347. doi:10.1007/s11280-017-0515-4ToppiReddy, H. K. R., Saini, B., & Mahajan, G. (2018). Crime Prediction & Monitoring Framework Based on Spatial Analysis. Procedia Computer Science, 132, 696-705. doi:10.1016/j.procs.2018.05.075Devia, N., & Weber, R. (2013). Generating crime data using agent-based simulation. Computers, Environment and Urban Systems, 42, 26-41. doi:10.1016/j.compenvurbsys.2013.09.001Kuo, P.-F., Lord, D., & Walden, T. D. (2013). Using geographical information systems to organize police patrol routes effectively by grouping hotspots of crash and crime data. Journal of Transport Geography, 30, 138-148. doi:10.1016/j.jtrangeo.2013.04.006Camacho-Collados, M., & Liberatore, F. (2015). A Decision Support System for predictive police patrolling. Decision Support Systems, 75, 25-37. doi:10.1016/j.dss.2015.04.012Kagawa, T., Saiki, S., & Nakamura, M. (2019). Analyzing street crimes in Kobe city using PRISM. International Journal of Web Information Systems, 15(2), 183-200. doi:10.1108/ijwis-04-2018-0032Jentner, W., Sacha, D., Stoffel, F., Ellis, G., Zhang, L., & Keim, D. A. (2018). Making machine intelligence less scary for criminal analysts: reflections on designing a visual comparative case analysis tool. The Visual Computer, 34(9), 1225-1241. doi:10.1007/s00371-018-1483-0Suarez-Paez, J., Salcedo-Gonzalez, M., Esteve, M., Gómez, J. A., Palau, C., & Pérez-Llopis, I. (2018). Reduced computational cost prototype for street theft detection based on depth decrement in Convolutional Neural Network. Application to Command and Control Information Systems (C2IS) in the National Police of Colombia. International Journal of Computational Intelligence Systems, 12(1), 123. doi:10.2991/ijcis.2018.25905186Suarez-Paez, J., Salcedo-Gonzalez, M., Climente, A., Esteve, M., Gómez, J. A., Palau, C. E., & Pérez-Llopis, I. (2019). A Novel Low Processing Time System for Criminal Activities Detection Applied to Command and Control Citizen Security Centers. Information, 10(12), 365. doi:10.3390/info10120365Esteve, M., Perez-Llopis, I., & Palau, C. E. (2013). Friendly Force Tracking COTS solution. IEEE Aerospace and Electronic Systems Magazine, 28(1), 14-21. doi:10.1109/maes.2013.6470440Esteve, M., Perez-Llopis, I., Hernandez-Blanco, L. E., Palau, C. E., & Carvajal, F. (2007). SIMACOP: Small Units Management C4ISR System. Multimedia and Expo, 2007 IEEE International Conference on. doi:10.1109/icme.2007.4284862OpenStreetMap http://www.openstreetmap.or",space,976
https://core.ac.uk/download/212853933.pdf,filtered,core,'Wiley',2018-01-01 00:00:00,core,towards robust and domain invariant feature representations in deep learning,10.13016/M2MK65C49,"A fundamental problem in perception-based systems is to define

and learn representations of the scene that are more robust and adaptive to several nuisance

factors. Over the recent past, for a variety of tasks involving images, learned representations  have been empirically shown to outperform handcrafted ones.

However, their inability to generalize across varying data distributions poses the following

question: Do representations learned using deep networks just fit a given data distribution or do

they sufficiently model the underlying structure of the problem ? This question could be

understood using a simple example: If a learning algorithm is shown a number of images of a simple handwritten digit, then the representation learned should be generic enough to identify the same digit in

a different form. With regards to deep networks, although the learned representation has been shown

to be robust to various forms of synthetic distortions such as random noise, they fail in the presence of

more implicit forms of naturally occurring distortions. In this dissertation, we

propose approaches to mitigate the effect of such distortions and in the process, study some

vulnerabilities of deep networks to small imperceptible changes that occur in the given input.

The research problems that comprise this dissertation lie in the cross section of two open topics: (1)

Studying and developing methods that enable neural networks learn robust representations (2) Improving

generalization of neural nets across domains.  The first part of the dissertation  approaches the problem of robustness from two broad viewpoints:  Robustness to external nuisance factors that occur in the data and robustness

(or a lack thereof) to perturbations of the learned feature space.  In the second part, we focus on learning representations that are invariant to external covariate

shift, which is more commonly termed as domain shift. 

Towards learning representations robust

to external nuisance factors, we propose an approach that couples a deep convolutional neural

network with a low-dimensional discriminative embedding learned using triplet probability

constraints to solve the unconstrained face analysis problem. While previous approaches in this area have proposed scalable yet ad-hoc solutions to this problem, we propose a principled and parameter free formulation which is based on maximum likelihood estimation. In addition, we employ the principle of transfer learning to realize a deep network architecture that can train faster and on lesser data yet significantly outperforms existing approaches on the unconstrained face verification task. We demonstrate the robustness

of the approach to challenges including age, pose, blur and clutter by performing clustering

experiments on challenging benchmarks.

Recent seminal works have shown that deep neural networks are susceptible to visually imperceptible perturbations of the input. In this dissertation, we build on their ideas in two unique ways: (a) We show that neural networks that perform pixel-wise semantic segmentation tasks also suffer from this vulnerability, despite being trained with more extra information compares to simple classification tasks.  In addition, we present a novel self correcting mechanism in segmentation networks and provide an efficient way to generate such perturbations (b) We present a novel approach to regularize deep neural networks by perturbing intermediate layer activations in an efficient manner, thereby exploring the trade-off between conventional regularization and adversarial robustness within the context of very deep networks. Both of these works provide interesting directions towards understanding the secure nature of deep learning algorithms. 

While humans find it extremely simple to generalize their knowledge across domains, machine learning algorithms including deep neural networks suffer from the problem of domain shift across what are commonly termed as 'source' (S) 

and 'target' (T) distributions. Let the data that a learning algorithm

is trained on be sampled from  S. If the real data used to evaluate the model is

then sampled from T, then the learnt model under-performs on the target data. This inability to generalize is characterized as domain shift. 

Our attempt to address this problem involves learning a common

feature subspace, where distance between source and target distributions are minimized. Estimating the distance between different domains is highly non-trivial and is an open research

problem in itself. In our approach we parameterize the distance measure by using a Generative

Adversarial Network (GAN). A GAN involves a two player game between two mappings com-

monly termed as generator and discriminator. These mappings are learned simultaneously by

employing an adversarial game, i.e. by letting the generator fool the discriminator and enabling

the discriminator to outperform the generator. This adversarial game can be formulated as a

minimax problem. In our approach, we learn three mappings simultaneously: the generator,

discriminator and a feature mapping that contains information about both the content and the

domain of the input. We deploy a two-level minimax game, where the first level is a competition

between the generator and a discriminator similar to a GAN; the second level game is where the

feature mapping attempts to fool the discriminator thereby introducing domain invariance in

the learned feature representation. We have extensively evaluated this approach for different

tasks such as object classification and semantic segmentation, where we achieve state of the

art results across several real datasets. In addition to the conceptual novelty, our approach

presents a more efficient and scalable solution compared to other approaches that attempt to

solve the same problem.

In the final part of this dissertation, we describe some ongoing efforts and future directions of research. Inspired from the study of perturbations described above, we propose a novel metric on how to effectively choose pixels to label given an image, for a pixel-wise segmentation task. This has the potential to significantly reduce the labeling effort and our preliminary results for the task of semantic segmentation are encouraging.  While the domain adaptation approach proposed above considered static images, we propose an extension to video data aided by the use of recurrent neural networks.  Use of full temporal information, when available, provides the perceptual system additional context to disambiguate among smaller object classes  that commonly occur in real scenes",space,977
https://core.ac.uk/download/195379882.pdf,filtered,core,,2018-01-01 00:00:00,core,hardware acceleration of deep convolutional neural networks on fpga,,"abstract: The rapid improvement in computation capability has made deep convolutional neural networks (CNNs) a great success in recent years on many computer vision tasks with significantly improved accuracy. During the inference phase, many applications demand low latency processing of one image with strict power consumption requirement, which reduces the efficiency of GPU and other general-purpose platform, bringing opportunities for specific acceleration hardware, e.g. FPGA, by customizing the digital circuit specific for the deep learning algorithm inference. However, deploying CNNs on portable and embedded systems is still challenging due to large data volume, intensive computation, varying algorithm structures, and frequent memory accesses. This dissertation proposes a complete design methodology and framework to accelerate the inference process of various CNN algorithms on FPGA hardware with high performance, efficiency and flexibility. 

As convolution contributes most operations in CNNs, the convolution acceleration scheme significantly affects the efficiency and performance of a hardware CNN accelerator. Convolution involves multiply and accumulate (MAC) operations with four levels of loops. Without fully studying the convolution loop optimization before the hardware design phase, the resulting accelerator can hardly exploit the data reuse and manage data movement efficiently. This work overcomes these barriers by quantitatively analyzing and optimizing the design objectives (e.g. memory access) of the CNN accelerator based on multiple design variables. An efficient dataflow and hardware architecture of CNN acceleration are proposed to minimize the data communication while maximizing the resource utilization to achieve high performance.

Although great performance and efficiency can be achieved by customizing the FPGA hardware for each CNN model, significant efforts and expertise are required leading to long development time, which makes it difficult to catch up with the rapid development of CNN algorithms. In this work, we present an RTL-level CNN compiler that automatically generates customized FPGA hardware for the inference tasks of various CNNs, in order to enable high-level fast prototyping of CNNs from software to FPGA and still keep the benefits of low-level hardware optimization. First, a general-purpose library of RTL modules is developed to model different operations at each layer. The integration and dataflow of physical modules are predefined in the top-level system template and reconfigured during compilation for a given CNN algorithm. The runtime control of layer-by-layer sequential computation is managed by the proposed execution schedule so that even highly irregular and complex network topology, e.g. GoogLeNet and ResNet, can be compiled. The proposed methodology is demonstrated with various CNN algorithms, e.g. NiN, VGG, GoogLeNet and ResNet, on two different standalone FPGAs achieving state-of-the art performance.

Based on the optimized acceleration strategy, there are still a lot of design options, e.g. the degree and dimension of computation parallelism, the size of on-chip buffers, and the external memory bandwidth, which impact the utilization of computation resources and data communication efficiency, and finally affect the performance and energy consumption of the accelerator. The large design space of the accelerator makes it impractical to explore the optimal design choice during the real implementation phase. Therefore, a performance model is proposed in this work to quantitatively estimate the accelerator performance and resource utilization. By this means, the performance bottleneck and design bound can be identified and the optimal design option can be explored early in the design phase.Dissertation/ThesisDoctoral Dissertation Electrical Engineering 201",space,978
https://core.ac.uk/download/186564469.pdf,filtered,core,"НТУ ""ХПІ""",2018-01-01 00:00:00,core,optimization of hierarchical data structure of intelligent system of functional diagnosis of technical condition of complex machines,,"Розглядається метод інформаційно-екстремального машинного навчання системи функціонального діагностування технічного стану складної машини з оптимізацією ієрархічної структури вхідних даних. Показано, що на функціональну ефективність машинного навчання системи функціонального діагностування суттєво впливає розміщення в ієрархічній структурі класів розпізнавання, які характеризують
технічний стан машини та її вузлів. При цьому для кожної страти ієрархічної структури накладаються обмеження на кількість класів розпізнавання, що дозволяє зменшити ступінь їх перетину в просторі діагностичних ознак. Оптимізація ієрархічної структури здійснюється в процесі інформаційно-екстремального машинного навчання системи функціонального діагностування, що дозволяє максимізувати
інформаційну спроможність системи. Як критерій оптимізації параметрів машинного навчання розглядається модифікована інформаційна міра Кульбака, яка є функціоналом точнісних характеристик діагностичних рішень. При цьому алгоритм машинного навчання представляв собою багатоциклічну ітераційну процедуру пошуку максимального глобального значення інформаційного критерію оптимізації параметрів машинного навчання в робочій (допустимій) області визначення його функції. В результаті для страт всіх ярусів ієрархічної структури сформовано алфавіти класів розпізнавання, які забезпечили максимальну функціональну ефективність машинного навчання. За отриманими в процесі машинного навчання оптимальними геометричними параметрами контейнерів класів розпізнавання побудовано вирішальні правила, які дозволяють приймати діагностичні рішення в реальному темпі часу. Крім того, вирішальні правила, побудовані в рамках геометричного підходу, є практично інваріантними до багатовимірності вхідних даних, що є їх суттєвою перевагою перед штучними нейронними мережами. Як приклад реалізації запропонованого методу розглядалося машинне навчання системи функціонального діагностування шахтної підйомної машини з оптимізацією структури вхідних даних.The conclusions about the strata of society, various parties are supported by, have been made. The method of information-extreme machine learning of the system of functional diagnosis of the technical state of a complex machine with the optimization of the hierarchical data structure is considered. It is shown that the functional efficiency of machine learning of the system of functional diagnosis is significantly influenced by the location in the hierarchical structure of the recognition classes characterizing the technical state of the machine and its nodes. At the same time, for each level of the hierarchical structure under consideration, a restriction on the number of recognition classes is imposed, which makes it possible to reduce the degree of their intersection in the space of diagnostic features. Optimization of the hierarchical structure was carried out in the process of information-extreme machine learning of the system of functional diagnosis, which allows to maximize the information capacity of the system. As a criterion for optimizing the parameters of machine learning, we considered a modi fied information measure of Kulbak, which is a functional of the accurate characteristics of diagnostic solutions. In this case, the algorithm of machine learning represented a multi-cycle iterative procedure of finding the maximum global value of the information criterion for optimizing learning parameters in the working (permissible) domain of determining its function. Based on the optimal geometric parameters of recognition class containers obtained in the course of machine learning, decision rules have been constructed that allow making diagnostic decisions in a real time. As an example of the implementation of the method of optimization the structure of input data, the machine learning of the system for the functional diagnosis of a mine hoist was considered. As a result, alphabets of recognition classes have been created for strata of all tiers of the hierarchical structure, providing the maximum functional efficiency of machine learnin",space,979
https://core.ac.uk/download/211248000.pdf,filtered,core,,2019-06-19 00:00:00,core,criminal data analysis based on low rank sparse representation,,"FINDING effective clustering methods for a high dimensional dataset is challenging due to the curse of dimensionality. These challenges can usually make the most of basic common algorithms fail in highdimensional spaces from tackling problems such as large number of groups, and overlapping. Most domains uses some parameters to describe the appearance, geometry and dynamics of a scene. This has motivated the implementation of several techniques of a high-dimensional data for finding a low-dimensional space. Many proposed methods fail to overcome the challenges, especially when the data input is high-dimensional, and the clusters have a complex.



REGULARLY in high dimensional data, lots of the data dimensions are not related and might hide the existing clusters in noisy data. High-dimensional data often reside on some low dimensional subspaces. The problem of subspace clustering algorithms is to uncover the type of relationship of an objects from one dimension that are related in different subsets of another dimensions. The state-of-the-art methods for subspace segmentation which included the Low Rank Representation (LRR) and Sparse Representation (SR). The former

seeks the global lowest-rank representation but restrictively assumes the independence among subspaces, whereas the latter seeks the clustering of disjoint or overlapped subspaces through locality measure, which, however, causes failure in the case of large noise.



THIS thesis aims are to identify the key problems and obstacles that have challenged the researchers in recent years in clustering high dimensional data, then to implement an effective subspace clustering methods for solving high dimensional crimes domains for both real events and synthetic data which has complex data structure with 168 different offence crimes. As well as to overcome the disadvantages of existed subspace algorithms techniques. To this end, a Low-Rank Sparse Representation (LRSR) theory, the future will refer to as Criminal Data Analysis Based on LRSR will be examined, then to be used to recover and segment embedding subspaces. The results of these methods will be discussed and compared with what already have been examined on previous approaches such as K-mean and PCA segmented based on K-means. The previous approaches have helped us to chose the right subspace clustering methods. The Proposed method based on subspace segmentation method named Low Rank subspace Sparse Representation (LRSR) which not only recovers the low-rank subspaces but also gets a relatively sparse segmentation with respect to disjoint subspaces or even overlapping subspaces.



BOTH UCI Machine Learning Repository, and crime database are the best to find and compare the best subspace clustering algorithm that fit for high dimensional space data. We used many Open-Source Machine Learning Frameworks and Tools for both employ our machine learning tasks and methods including preparing, transforming, clustering and visualizing the high-dimensional crime dataset, we precisely have used the most modern and powerful Machine Learning Frameworks data science that known as SciKit-Learn for library for the Python programming language, as well as we have used R, and Matlab in previous experiment",space,980
https://core.ac.uk/download/211242274.pdf,filtered,core,,2019-05-01 00:00:00,core,shortest path algorithms for dynamic transportation networks,,"Over the last decade, many interesting route planning problems can be solved by finding the shortest path in a weighted graph that represents a transportation network. Such networks are private transport networks or timetabled public transportation networks. In the shortest path problem, every network type requires different algorithms to compute one or more than one shortest path. However, routing in a public transportation network is completely different and is much more complex than routing in a private transport network, and therefore different algorithms are required.

 

For large networks, the standard shortest path algorithms - Dijkstra's algorithm (1959) and Bellman's algorithm (1958)- are too slow. Consequently, faster algorithms have been designed to speed up the search. However, these algorithms often consider only the simplest scenario of finding an optimal route on a graph with static real edge costs. But real map routing problems are often not that simple – it is often necessary to consider time-dependent edge costs. For example, in public transportation routing, consideration of the time-dependent model of these networks is mandatory.

 

However, there are a number of transportation applications that use informed search algorithms (where the algorithm uses heuristics that guide the search toward the destination), rather than one of the standard static shortest path algorithms. This is primarily due to shortest paths needing to be rapidly identified either because an immediate response is required. For example, the A* algorithm (Nilsson, 1971) is widely used in artificial intelligence. Heuristic information (in the form of estimated distance to the destination) is used to focus the search towards the destination node. This results in finding the shortest path faster than the standard static search algorithms.

 

Road traffic congestion has become an increasingly significant problem in a modern society. In a dynamic traffic environment, traffic conditions are time-dependent. For instance, when travelling from home to the work, although an optimal route can be planned prior to departure based on the traffic conditions at that time, it may be necessary to adjust the route while en route because traffic conditions change all the time. In some cases, it is necessary to modify the travelling route from time to time and re-plan a new route from the current location to the destination, based on the real-time traffic information. The challenge lies in the fact that any modification to the optimal route to adapt to the dynamic environment necessitates speeding up of the search efforts. Among the algorithms suggested for the dynamic shortest path problem is the algorithm of Lifelong Planning A* algorithm (LPA*) (Koenig, Likhachev and Furcy, 2004). This algorithm has been given this name because of its ability to reuse information from previous searches. It is used to adjust a shortest path to adapt to the dynamic transportation network.

 

Search space and fast shortest path queries can be used for finding fastest updated route on road and bus networks. Consequently, the efficient processing of both types of queries is of first-rate significance. However, most search methods focus only on one type of query and do not efficiently support the other. To address this challenge, this research presents the first novel approach; an Optimised Lifelong Planning A* (OLPA*) algorithm. The OLPA* used an appropriate data structure to improve the efficiency of the dynamic algorithms implementation making it capable of improving the search performance of the algorithm to solve the dynamic shortest path problem, which is where the traveller may have to re-compute the shortest path while travelling in a dynamic transportation environment.

 

This research has also proposed bi-directional LPA* (BLPA*) algorithm. The proposed algorithm BLPA* used bi-directional search strategy and the main idea in this strategy is to divide the search problem into two separate problems. One search proceeds forwards from the start node, while the other search proceeds backwards from the end node. The solution requires the two search problems to meet at one middle node. The BLPA* algorithm has the same overall structure as the LPA* algorithm search, with some differences that the BLPA* contains a priority queue for each direction.

 

This research presented another algorithm that designed to adaptively derive the shortest path to the desired destination by making use of previous search results and reducing the total execution time by using the benefits of a bi-directional search strategy . This novel algorithm has been called the bi-directional optimised Lifelong A* algorithm (BiOLPA*). It was originally proposed for road transport networks and later also applied to public transportation networks. For the road transport network, the experimental results demonstrate that the proposed incremental search approach considerably outperforms the original approach method, which recomputed the shortest path from scratch each time without utilization of the previous search results. However, for public transportation, the significant problem is that it is not possible to apply a bi-directional search backwards using estimated arrival time. This has been further investigated and a better understanding of why this technique fails has been documented. While the OLPA* algorithms give an impressive result when applied on bus network compared with original A* algorithms, and our experimental results demonstrate that the BiOLPA* algorithm on road network is significantly faster than the LPA*, OLPA* and the A* algorithms, not only in terms of number of expansion nodes but also in terms of computation time",space,981
https://core.ac.uk/download/286618536.pdf,filtered,core,'National Aviation University',2019-01-01 00:00:00,core,modification of jet fuels composition with renewable bio-additives,10.18372/37895,"1.	Abu-Taieh C., Evon J.: Technology Engineering and Management in Aviation: Advancements and Discoveries. Information Science Reference, 2011.
2.	Ajam M, Woolard C, Wiljoen CL. Biomass pyrolysis oil as a renewable feedstock for bio-jet fuel. In: Proceedings of the 13th international conference on stability, handling and use of liquid fuels (IASH2013), Rhodes, Greece; October 2013. p. 6–10.
3.	Аnnual report to Parliament on the renewable transport fuel obligation. Renewable Fuels Agency. The Stationery Office, 2011.
4.	Agarwal S., Chhibber V. K., Bhatnagar A. K.:Tribological behavior of diesel fuels and the effect of anti-wear additives. Fuel. Vol. 106, 2013, p. 21–29, 
5.	Alves S. M., Barros B.S., Trajano M.F.: Tribological behavior of vegetable oil-based lubricants with nanoparticles of oxides in boundary lubrication conditions. Tribology International. Vol. 65, 2013, p. 28–36.
6.	Asgari H., Chen X., Sainudiin R.: Modelling and simulation of gas turbines. International Journalof Modelling, Identification and Control, Vol.25, No.3, 2013, p. 1–15.
7.	Bartis James T. LaTourrette T., Dixon L.: Oil Shale Development in the United States: Prospects and Policy Issues. Santa Monica, Calif.: RAND Corporation, MG-414-NETL, 2005.
8.	Bassam N. El.: Handbook of Bioenergy Crops: A Complete Reference to Species. Development and Applications Earthscan, 2010.
9.	Bazazzadeh M., Badihi H., Shahriari A.: Gas Turbine Engine Control Design Using Fuzzy Logic and Neural Networks. International Journal of Aerospace Engineering. Vol. 1, 2011, p. 1–13. 
10.	Blakey S, Rye L, Wilson C.W.: Aviation gas turbine alternative fuels: A review.  P Combust Inst, No. 33, 2011, p. 2863–2885.
11.	Boichenko S., Iakovlieva A., Vovk O.: Traditional and alternative jet fuels: problems of quality standardization. Journal of Petroleum & Environmental Biotechnology. Vol. 4. Iss. 3, 2013.
12.	Boichenko S., Shkilniuk I., Turchak V.. The problems of biopollution with jet fuels and the way of achieving solution. Transport. 23, 2008; p. 253–257.
13.	Boichenko S., Yakovleva A. Prospects of biofuels introduction into aviation. Transport engineering and management: Proceedings of the 15-th conference for Lithuania Junior researchers. Science – future of Lithuania, 4 May 2012. Vilnius: Technika. p. 90–94.
14.	Boichenko S., Yakovlieva A., Gryshchenko O., Zinchuk A. Prospects of using different generations biofuels for minimizing impact of modern aviation on environment, Энерготехнологии и ресурсосбережение, № 1, 2018, p. 10–20.
15.	Boichenko S., Lejda K., Yakovlieva A., Vovk O. Comparative characteristics of low-temperature properties of jet fuels modified with bio-additives, International Automotive Conference (KONMOT2018). IOP Conf. Series: Materials Science and Engineering 421, 2018.
16.	Breil C., Meullemiestre A., Vian M., Chemat F.: Bio-Based Solvents for Green Extraction of Lipids from Oleaginous Yeast Biomass for Sustainable Aviation Biofuel. Molecules. Iss. 21(196), 2016, p. 1–14.
17.	Carels N., Sujatha M., Bahadur B.: Jatropha, Challenges for a New Energy Crop. Vol. 1: Farming, Economics and Biofuel. Springer Science & Business Media, 2012.
18.	Cavani F., Albonetti S., Basile F., Gandini A.: Chemicals and Fuels from Bio-Based Building Blocks. John Wiley & Sons, 2015.
19.	Cermak S. C., Evangelista R. L., Kenar J. A.: Distillation of Natural Fatty Acids and Their Chemical Derivatives, Distillation - Advances from Modeling to Applications, Dr. Sina Zereshki (Ed.), InTech, 2012. – р. 5. – 140. 
20.	Chai M. Thermal Decomposition of Methyl Esters in Biodiesel Fuel: Kinetics, Mechanisms and Products, Ph.D. Thesis, University оf Cincinnati, 2012.
21.	Chiaramonti D, Bonini M, Fratini E, Tondi G, Gartner K, Bridgwater AV, et al. Development of emulsion from biomass pyrolysis liquid and diesel and their use in engines – Part 1: emulsion production. Biomass Bioenergy, No. 25, 2003, p. 85–99.
22.	Chiaramonti D, Bonini M, Fratini E, Tondi G, Gartner K, Bridgwater AV, et al. Development of emulsion from biomass pyrolysis liquid and diesel and their use in engines – Part 2: tests in diesel engines. Biomass Bioenergy, No. 25, 2003, p. 101–11.
23.	Chuck C.J., Donnelly J.: The compatibility of potential bioderived fuels with Jet A-1 aviation kerosene. Applied Energy. Vol. 118, 2014, p. 83–91.
24.	Cleveland C.J., Morris C. G.: Handbook of energy. Volume II: Cronologies, top ten lists, and words clouds. Elsvier Inc., 2014.
25.	Cushion E., Whiteman A., Dieterle G.: Bioenergy Development: Issues and Impacts for Poverty and Natural Resource Management. World Bank Publications, 2010.
26.	Daggett D. L., Hendricks R.C., Walther R., Corporan E.: Alternative fuels for use in commercial aircrafts. The Boeing Company, 2007.
27.	Dahlquist E.: Biomass as Energy Source. Resources, Systems and Applications. CRC Press, 2013.
28.	Delmon B., Grange P., Froment G.F.: Hydrotreatment and Hydrocracking of Oil Fractions. Elsevier, 1999.
29.	Doc 9889 Airport Air Quality Manual. International Civil Aviation Organization, 2011. 
30.	Doc 9977. Manual on Civil Aviation Jet Fuel Supply, 2012.
31.	Edwards T.: Advancements in Gas Turbine Fuels from 1943 to 2005. J Eng Gas Power, No. 129, 2007, p. 13–20.
32.	Firrisa M. T., Van Duren I., Voinov A.: Energy efficiency for rapeseed biodiesel production in different farming systems. Energy Efficiency, 2013.
33.	Garcia-Anton J., Monzo J., Guninon J.L.: Study of corrosion on copper strips by petroleum naphtha in the ASTM D-130 test by means of electronic microscopy (SEM) and energy dispersive X-ray (EDX). Fresenius Journal of Analytical Chemistry. Iss. 337, 1990, p. 382–388.
34.	Garcia Santander C.M., Gymez Rueda S.M., de Lima da Silva N.: Measurements of normal boiling points of fatty acid esters and triacylglycerols by thermogravimetric analysis, Fuel, Iss. 92, 2012, p. 158–161.
35.	Geller D. P., Goodrum J.: W. Effects of speciﬁc fatty acid methyl esters on diesel fuel lubricity, Fuel, Vol. 83, 2004, p. 2351–2356.
36.	Gupta, K. K, Rehman A, Sarviya R. M.: Bio-fuels for the gas turbine: A review. Renew. Sust. Energ. Rev. No. 14, 2010, p. 2946–2955.
37.	Harvey B. G, Merriman W.W., Koontz T.A.: High-Density Renewable Diesel and Jet Fuels Prepared from Multicyclic Sesquiterpanes and a 1‑Hexene-Derived Synthetic Paraffinic Kerosene, Energy Fuels, 2013.
38.	Hemighaus G., Boval T., Bosley C.: Alternative Jet Fuels. Addendum 1 to Aviation Fuels Technical Review (FTR-3/A1). Chevron Corporation, 2006.
39.	Hileman J.I., Stratton R.W.: Alternative jet fuel feasibility. Transport Policy. Vol. 34, 2014, p. 52–62.
40.	Hileman J.I., Wong H.M., Waitz I.: Near-Term Feasibility of Alternative Jet Fuels. Santa Monica, California: RAND Corporation, 2009.
41.	Hileman, J. Ortiz D., Bartis J.: Near-Term Feasibility of Alternative Jet Fuels. Jointly published by the RAND Corporation (Report No. TR-554-FAA) and the Partnership for Air Transportation Noise and Emissions Reduction, 2009.
42.	Honga T.D., Soerawidjajab T.H., Reksowardojoa I.K.: A study on developing aviation biofuel for the Tropics: Production process – Experimental and theoretical evaluation of their blends with fossil kerosene, Chemical Engineering and Processing: Process Intensification, Vol. 74, 2013, p. 124–130.
43.	Hristova M., Tchaoushev S.: Сalculation of flash points and flammability limits of substances and mixtures. Journal of the University of Chemical Technology and Metallurgy, Iss. 41(3), p. 291–296, 2006.
44.	Hu J., Du Z., Li C., Min E.: Study on the lubrication properties of biodiesel as fuel lubricity enhancers, Fuel. Vol. 84, 2005. p. 1601–1606. 
45.	Iakovlieva A., Boichenko S., Vovk O.: Investigation of the fractional composition of rape oil-derived aviation biofuels. Aviation in the XXI-st century. Safety in aviation and space technologies: the fifth world congress, 25–27 September 2012: abstracts. Kyiv, Vol. 3, 2012, p. 5.41–5.43.
46.	Iakovlieva A.V. Boichenko S.V., Vovk O.O.: Overview of innovative technologies for aviation fuels production. Journal of Chemistry and chemical technology, Vol. 7. Iss. 3, 2013, p. 305–312.
47.	Iakovlieva A., Lejda K., Vovk O., Boichenko S.: Peculiarities of the development and implementation of aviation biofuels in Ukraine. World Congress on Petrochemistry and Chemical Engineering. Journal of Petroleum & Environmental Biotechnology. November 2013, San Antonio. Vol.4. Iss. 6, 2013, p. 47.
48.	Iakovlieva A., Boichenko S., Gay A.: Cause-Effect Analysis of the Modern State in Production of Jet Fuels. Journal of Сhemistry & Chemical Technology. Vol. 8. No 1, 2014, p. 107–116.
49.	Iakovlieva A., Boichenko S., Vovk O., Lejda K.: Potential of jet biofuels production and application in Ukraine and Poland. International Journal of Sustainable Aviation. Vol. 1. No.4, 2015, p. 314–323.
50.	Iakovlieva A., Boichenko S., Lejda K.: Impact of rape oil ethyl esters additives on some characteristics of jet fuel. Проблеми хіммотології. Теорія та практика раціонального використання традиційних і альтернативних паливно -мастильних матеріалів: V міжнар. наук.-техн. конф., 6–10 жовт. 2014. Київ, c. 286 – 289. 
51.	Iakovlieva A., Lejda K., Vovk O., Boichenko S., Skilniuk I.: Vacuum Distillation of Rapeseed Oil Esters for Production of Jet Fuel Bio-Additives, Procedia Engineering, Vol. 187, 2017, p. 363 – 370. 
52.	Iakovlieva A., Lejda K., Vovk O., Boichenko S.: Рotential of jet biofuels production and application in Ukraine and Poland. Proceedings of the 1st International Simposium on Sustainable Aviation.–31 May–03 June 2015, Isntanbul, p. 137.
53.	Iakovlieva A., Boichenko S., Lejda K.: Experimental study on antiwear properties for blends of jet fuel with biocomponents derived from rapeseed oil. Eastern-European journal of enterprise technologies. No. 5/8(77), 2015, p. 20–28.
54.	Iakovlieva A., Vovk O., Boichenko S.: Еxperimental study of rape oil esters influence on physical-chemical properties of jet fuels. Proceedings of the 19th Conference for Junior Researchers ‘Science – Future of Lithuania’ Тransport engineering and management, 6 May 2016, Vilnius. p. 85–89.
55.	Iakovlieva A., Lejda K., Vovk O., Boichenko S., Kuszewski H. Improvement of technological scheme of fatty acids ethyl esters production for use as jet fuels biocomponents. International Journal of Theoretical and Applied Science. Iss. 11(19), 2014, p. 44–55.
56.	International Air Transport organization. Vision 2050. Report. Montreal. Geneva, 2011.
57.	Jansen R. A.: Second Generation Biofuels and Biomass: Essential Guide for Investors, Scientists and Decision Makers. Wiley. 2012.
58.	Jenkins R.W., Munro M., Christopher S.N., Chuck C.: Potential renewable oxygenated biofuels for the aviation and road transport sectors. Fuel, Vol. 103, 2013, p. 593–599.
59.	Jacyna M., Żak J., Jacyna-Gołda I., Merkisz J., Merkisz-Guranowska A., Pielecha J.: Selected aspects of the model of proecological transport system. Journal of KONES Powertrain and Transport, Vol. 20, No. 3, 2013, p. 193 – 202.
60.	Kallio P., Pasztor A., Akhtar M.K., Jones P.R.: Renewable jet fuel. Current Opinion in Biotechnology. Vol. 26, 2014, p. 50–55.
61.	Kandaramath Hari T., Yaakob Z., Binitha N.N.: Aviation biofuel from renewable resources: Routes, opportunities and challenges. Renewable and Sustainable Energy Reviews. Vol. 42, 2015, p. 1234–1244. 
62.	Kinder J. D., Rahmes T.: Evaluation of Bio-Derived Synthetic Paraffinic Kerosene (Bio-SPK). The Boeing Company Sustainable Biofuels Research&Technology Program, 2009.
63.	Kirklin P.W., David. P.: Aviation Fuel: Thermal Stability. ASTM International, 1992.
64.	Lapuerta M., Rodriguez-Fernandeza J., Estevez C., Bayarri N.: Properties of fatty acid glycerol formal ester (FAGE) for use as a component in blends for diesel engines. Biomass and bioenergy. Vol. 76, 2015, p. 130–140. 
65.	Lebedevas S., Vaicekauskas A.: Research into the application of biodiesel in the transport sector of Lithuania. Transport. Vol. 21, Iss. 2, 2006, p. 80–87.
66.	Liu G., Yan B., Chen G.: Technical review on jet fuel production. Renewable and Sustainable Energy Reviews. Vol. 25, 2013, p. 59–70. 
67.	Lu M., Chai M.: Experimental Investigation of the Oxidation of Methyl Oleate: One of the Major Biodiesel Fuel Components Synthetic Liquids Production and Refining. Chapter 13, P. 289–312. American Chemical Society. 2011
68.	Merkisz J., Merkisz-Guranowska, A., Pielecha J., Nowak M., Jacyna M., Lewczuk K., Żak J.: Exhaust emission measurements in the development of sustainable road transport. Journal of KONES Powertrain and Transport, Vol. 20, No. 4 2013, p. 277 – 284.
69.	Maksimuk Yu., Antonova Z., Fes’ko V., Kursevich V.: Diesel biofuel viscosity and heat of combustion. Chemistry and technology of fuels and oils. Iss. 45, 2009, p. 343–346.
70.	Maru M. M., Trommer R.M., Cavalcanti K.F.: The Stribeck curve as a suitable characterization method of the lubricity of biodiesel and diesel blends. Energy. Vol. 69, 2014, p. 673–681.
71.	Maurice L.Q., Lander H., Edwards T., Harrison W.E.: Advanced aviation fuels: a look ahead via a historical perspective. Fuel. Vol. 80, Iss. 5, 2001, p. 747–756.
72.	Merkisz J., Markowski J., Pielecha J. Emission tests of the AI-14RA aircraft engine under real operating conditions of PZL-104"" Wilga"" plane. Silniki Spalinowe. No. 3, 2009, p. 64–70.
73.	Merkisz J., Galant M., Karpiński D., Kubiak, K. Evaluation of possibility to use the LTO cycle for emission test on example of the model turbine engine GTM-120 Journal of Mechanical and Transport Engineering. Vol. 66, No. 2, 2014, p. 25—33.
74.	Murphy D.J., Hall C.A.S.: Year in review—EROI or energy return on (energy) invested. Annals of the New York academy of sciences. Issue: Ecological Economics Reviews. Iss. 1185, 2010, p. 102–118.
75.	Murphy D.J., Hall C.A.S., Powers B.:New perspectives on the energy return on (energy) investment (EROI) of corn ethanol. Environment, Development and Sustainability. Vol. 13, Iss. 1, 2011, p. 179–202.
76.	Naik S.N., Goud V.V., Rout P.K., Dalai A.K.: Production of first and second generation biofuels: A comprehensive review. Renew. Sust. Energ. Rev., No. 14, 2010, p. 578–597.
77.	Nollet Leo M. L.: Handbook of Food Analysis: Physical characterization and nutrient analysis. CRC Press, 2004.
78.	Orszulik S.: Environmental Technology in the Oil Industry. Springer Science & Business Media, 2013.
79.	Pandey A.: Biofuels: Alternative Feedstocks and Conversion Processes. Academic Press, 2011.
80.	Pearlson M.N.: A techno-economic and environmental assessment of hydroprocessed renewable distillate fuels. Master of Science in Technology and Policy. Massachiussets Institute of Technology. June 2011.
81.	Prag P.: Renewable Energy in the Countryside. Taylor & Francis, 2014.
82.	Prussi M, Chiaramonti D, Recchia L, Martelli F, Guidotti F, Pari L.: Alternative feedstock for the biodiesel and energy production: the OVEST project. Energy Journal, No. 58, 2013, p. 2–8.
83.	Rahmes T.F., Kinder J.D., Henry T.M., etc.: Sustainable Bio-Derived Synthetic Paraffinic Kerosene (BioSPK) Jet Fuel Flights and Engine Tests Program Results. American Institute of Aeronautics and Astronautics, 2009.
84.	Rajagopal D., Zilberman D.: Environmental, Economic and Policy Aspects of Biofuels. Nеw Publishers Inc., 2008.
85.	Report on alternative fuels. International Air Transport Association IATA. http://www.iata.org/publications/Documents/2012-report-alternativefuels. pdf; 2012
86.	Rosillo Calle F, Trhan D, Seiffert M, Teeluckingh S. The potential and role of biofuels in commercial air transport – biojetfuels. Task 40 sustainable international bioenergy trade. IEA Bioenergy
87.	Sarin R., Kumar R., Srivastav B., etc.: Biodiesel surrogates: Achieving performance demands. Bioresource Technology. Vol. 100, Iss. 12, 2009, p. 3022–3028. 
88.	Shen Y.. Аn experimental study on thermal stability of FAEE biodiesel fuel with ethanol. Master Thesis, 2015.
89.	Shepherd J.E., Nuyt C.D., Lee J.J.: Flash Point and Chemical Composition of Aviation Kerosene (Jet A). National Transportation Safety Board, 2000.
90.	Singh B.: Biofuel Crops: Production, Physiology and Genetics. CABI, 2013.
91.	Singh B.: Biofuel Crop Sustainability. John Wiley & Sons, 2013.
92.	Sperling D., Cannon J.S.: Reducing Climate Impacts in the Transportation Sector. Springer Science & Business Media, 2011.
93.	Szczerek M., Tuszyсski W. Tribological researches – scuffing. Radom: Institute for Sustainable Technologies – National Research Institute, 2000.
94.	The jet engine. Rolls-Royce plc. Renault Printing Co Ltd., 1996.
95.	T-02U. Aparat czterokulowy – instrukcja obsługi. Radom: Wydawnictwo Instytutu Technologii Eksploatacji, 2011.
96.	Wcisło G.: Determination of the impact of FAME biocomponent on the fractional composition of diesel engine fuels. Combustion Engines. Iss. 154(3), 2013, p. 1098–1103.
97.	Xu Y., Wang Q., Hu X.: Characterization of the lubricity of bio-oil/diesel fuel blends by high frequency reciprocating test rig. Energy. Vol. 35, Iss. 1, 2010, p. 283–287. 
98.	Yakovleva A.V., Boichenko S.V., Lejda K, Vovk O.O., Kuszewski H.: Antiwear Properties of Plant—Mineral-Based Fuels for Airbreathing Jet Engines, Chemistry and Technology of Fuels and Oils, Vol. 53, Iss. 1, 2017, p. 1–9. 
99.	Yakovlieva A.V., Boichenko S.V., Leida K., Vovk O.A., Kuzhevskii Kh.. Influence of Rapeseed Oil Ester Additives on Fuel Quality Index for Air Jet Engines, Chemistry and Technology of Fuels and Oils, Vol. 53, Iss. 3, 2017. p. 308–317.
100.	Yakovlieva A., Boichenko S., Vovk O., Lejda K., Gryshchenko O.. Case Study of Alternative Jet Fuel Production with Bio-additives from Plant Oils in Ukraine and Poland. Advances in Sustainable Aviation. Springer International Publishing, 2018. Chapter 4.
101.	Yakovlieva A., Boshkov V. Experimental study of low-temperature properties of alternative aviation fuels, Proceedings of the 21th Conference for Junior Researchers ‘Science – Future of Lithuania’ Transport Engineering and Management, 4-5 May 2018, Vilnius, Lithuania. 2018. p. 130 – 134.
102.	Yildirim U, Abanteriba S.: Manufacture, qualification and approval of new aviation turbine fuels and additives, proceedia Engineering, No. 49, 2012, p. 310 – 315.
103.	Yutko B. and Hansman J., Approaches to Representing Aircraft Fuel Efficiency Performance for the Purpose of a Commercial Aircraft Certification Standard, MITInternational Center for Air Transportation, Cambridge, Mass, 2011.
104.	Zhu Y.: An Experimental Study on Thermal Stability of Biodiesel Fuel. Master Thesis. – 2012. – 160 p.
105.	Авиационный турбореактивный двигатель РУ 19A-300, руководство по эксплуатации и техническому обслуживанию, ЗАО «АНТЦ Технолог», 2001.
106.	Азев В.С., Середа А.В.: Влияние соединений серы на противоизносные свойства дизельных топлив, Химия и технология топлив и масел. № 3, 2009, c. 23–27.
107.	Андіїшин М.П., Марчук Я.С., Бойченко С.В., Рябоконь Л.А.: Газ природний, палива та оливи. Одеса: Астропринт, 2010.
108.	Бойченко С.В., Спіркін В.Г. Вступ до хіммотології палив та олив: навч. посіб.: у 2-х ч. Одеса: Астропринт, Ч.1., 2009.
109.	Бойченко С.В., Любінін Й.А., Спіркін В.Г.: Вступ до хіммотології палив та олив: навч. посіб.: у 2-х ч. Одеса: Астропринт. Ч.2., 2010.
110.	Бойченко С.В., Черняк Л.М., Яковлєва А.В.: Традиційні технології виробництва палив для повітряно-реактивних двигунів. Вісник Національного авіаційного університету. № 2 (55), 2013, с. 195–209.
111.	Бойченко С. В., Яковлева А. В., Волошинец В. А., Лейда К. Модифицирование эфиров рапсового масла вакуумным фракционированием, Технологии нефти и газа, №5, 2018, c. 15–20
112.	Братичак М.М.: Основи промислової нафтохімії, Львів: Вид-во НУ «Львівська політехніка», 2008.
113.	Васильев И.П.: Влияние топлив растительного происхождения на экологические и экономические показатели дизеля, Луганск: Изд-во ВНУ им. В. Даля, 2009.
114.	Волошинець В.А. Фізична та колоїдна хімія: Фізико-хімія дисперсних систем та полімерів: навч.посіб. Львів : Вид-во Львів. політехніки, 2013. – 200 с.
115.	Голоскоков А.Н. Критерии сравнения эффективности традиционных и альтернативных энергоресурсов. Нефтегазовое дело. № 1, 2011, c. 285–301.
116.	Голоскоков А.Н. Пик добычи нефти и начало мирового энергетического кризиса. Нефтегазовое дело. 2010, c. 1–13.
117.	Данилов А.М., Каминский Э.Ф., Хавкин В.А.: Альтернативные топлива: достоинства и недостатки. Проблемы применения. Российский химический журнал (Журнал Российского химического общества им. Д.И. Менделеева). Т. XLVII. № 6, 2003, c. 4–11.
118.	Дворецкий С.И., Нагорнов С.А., Романцова С.В. и др.: Производство биодизельного топлива из органического сырья. Вопросы современной науки и практики. № 39, 2012, c. 126– 35.
119.	Девянин С.Н., Марков В.А., Семенов В.Г.: Растительные масла и топлива на их основе для дизельных двигателей. Харьков: Новое слово. 2007.
120.	Ергин Д.: Добыча: Всемирная история борьбы за нефть, деньги и власть. Москва,: Альпина Паблишер, 2011.
121.	Запорожець А.О.: Дослідження стехіометричної суміші «повітря ‒ паливо» органічних сполук. Частина 1. Алкани. Наукоємні технології. № 2(22), 2014, c. 163–167.
122.	Кириченко В., Бойченко С., Кириченко В., Нездоровин В.: Комплексная переработка технических растительных масел: концепция, методы и технологи. «Systems and means of motor transport» Seria: Transport. Monografia. № 4, 2013, p. 357–370.
123.	Колодницька Р.В., Семенов В.Г.: Моделювання низькотемпературних властивостей біодизельних палив. Вісник СевНТУ. Серія: Машиноприладобудування та транспорт. № 134, 2012, c. 135–138.
124.	Коллоидная химия нефти и нефтепродуктов: Сборник материалов, посвященных научной деятельности проф. Г.И. Фукса. Москва: Изд-во «Техника». ООО «Тума Групп», 2001.
125.	Крылов И.Ф., Емельянов В.Е.: Альтернативные моторные топлива. Производство, применение",space,982
https://riunet.upv.es/bitstream/10251/150623/1/marco%3bripoll%20-%20sspfa%3a%20effective%20stack%20smashing%20protection%20for%20android%20os.pdf,filtered,core,'Springer Science and Business Media LLC',2019-08-01 00:00:00,core,sspfa: effective stack smashing protection for android os,10.1007/s10207-018-00425-8,"[EN] In this paper, we detail why the stack smashing protector (SSP), one of the most effective techniques to mitigate stack bufferoverflow attacks, fails to protect the Android operating system and thus causes a false sense of security that affects all Androiddevices. We detail weaknesses of existing SSP implementations, revealing that current SSP is not secure. We propose SSPFA,the first effective and practical SSP for Android devices. SSPFA provides security against stack buffer overflows withoutchanging the underlying architecture. SSPFA has been implemented and tested on several real devices showing that it is notintrusive, and it is binary-compatible with Android applications. Extensive empirical validation has been carried out over theproposed solution.This work was partially funded by Universitat Politecnica de Valencia (Grant No. 20160251-ASLR-NG).Marco Gisbert, H.; Ripoll Ripoll, JI. (2019). SSPFA: Effective Stack Smashing Protection for Android OS. International Journal of Information Security. 18(4):519-532. https://doi.org/10.1007/s10207-018-00425-8S519532184Buchanan, W.J., Chiale, S., Macfarlane, R.: A methodology for the security evaluation within third-party android marketplaces. Digit. Investig. 23(Supplement C), 88–98 (2017). https://doi.org/10.1016/j.diin.2017.10.002Tian, D., Jia, X., Chen, J., Hu, C., Xue, J.: A practical online approach to protecting kernel heap buffers in kernel modules. China Commun. 1, 143–152 (2016)One, A.: Smashing the stack for fun and profit. Phrack, 7(49) (1996)Younan, Y., Pozza, D., Piessens, F., Joosen, W.: Extended protection against stack smashing attacks without performance loss. In: In Proceedings of ACSAC (2006)Abadi, M., Budiu, M., Erlingsson, U., Ligatti, J.: Control-flow Integrity. In: Proceedings of the 12th ACM Conference on Computer and Communications Security, Series CCS ’05, pp. 340–353. ACM, New York (2005). https://doi.org/10.1145/1102120.1102165Wartell, R., Mohan, V., Hamlen, K.W., Lin, Z.: Binary stirring: self-randomizing instruction addresses of legacy x86 binary code. In: Proceedings of the 2012 ACM Conference on Computer and Communications Security, Series CCS ’12, pp. 157–168. ACM, New York (2012). https://doi.org/10.1145/2382196.2382216Roglia, G.F., Martignoni, L., Paleari, R., Bruschi, D.: Surgically returning to randomized lib(c). In: Proceedings of the 2009 Annual Computer Security Applications Conference, Series ACSAC ’09, pp. 60–69. IEEE Computer Society, Washington (2009). https://doi.org/10.1109/ACSAC.2009.16Roemer, R., Buchanan, E., Shacham, H., Savage, S.: Return-oriented programming: systems, languages, and applications. ACM Trans. Inf. Syst. Secur. 15(1), 2:1–2:34 (2012). https://doi.org/10.1145/2133375.2133377Pappas, V., Polychronakis, M., Keromytis, A.: Smashing the gadgets: hindering return-oriented programming using in-place code randomization. In: 2012 IEEE Symposium on Security and Privacy (SP), pp. 601–615 (2012)S. R. to Thwart Return Oriented Programming in Embedded Systems, Stack Redundancy to Thwart Return Oriented Programming in Embedded Systems, IEEE Embedded Systems Letters, vol. (first on-line), pp. 1–1 (2018)Moula, V., Niksefat, S.: ROPK++: an enhanced ROP attack detection framework for Linux operating system. In: International Conference on Cyber Security And Protection Of Digital Services (Cyber Security). IEEE (2017)Das, S., Zhang, W., Liu, Y.: A fine-grained control flow integrity approach against runtime memory attacks for embedded systems. IEEE Trans. Very Large Scale Integr. VLSI Syst. 25, 3193–3207 (2016)Alam, M., Roy, D.B., Bhattacharya, S., Govindan, V., Chakraborty, R.S., Mukhopadhyay, D.: SmashClean: a hardware level mitigation to stack smashing attacks in OpenRISC. In: ACM/IEEE International Conference on Formal Methods and Models for System Design (MEMOCODE), pp. 1–4. IEEE (2016)Kananizadeh, S., Kononenko, K.: Development of dynamic protection against timing channels. Int. J. Inf. Secur. 16, 641–651 (2017)Bhatkar, S., DuVarney, D.C., Sekar, R.: Address obfuscation: an efficient approach to combat a board range of memory error exploits. In: Proceedings of the 12th Conference on USENIX Security Symposium—volume 12, Series SSYM’03, p. 8. USENIX Association, Berkeley (2003). http://dl.acm.org/citation.cfm?id=1251353.1251361 . Accessed 18 Jan 2019Snow, K.Z., Monrose, F., Davi, L., Dmitrienko, A., Liebchen, C., Sadeghi, A.-R.: Just-in-time code reuse: on the effectiveness of fine-grained address space layout randomization. In: 2013 IEEE Symposium on Security and Privacy (SP), pp. 574–588. IEEE (2013)Kumar, K.S., Kisore, N.R.: Protection against buffer overflow attacks through runtime memory layout randomization. In: International Conference on Information Technology (ICIT). IEEE (2014)Oberheide, J.: A look at ASLR in Android ice cream sandwich 4.0 (2012). https://www.duosecurity.com/blog/a-look-at-aslr-in-android-ice-cream-sandwich-4-0 . Accessed 18 Jan 2019Zabrocki, A.P.: Scraps of notes on remote stack overflow exploitation (2010). http://www.phrack.org/issues.html?issue=67&id=13#article . Accessed 18 Jan 2019Saito, T., Watanabe, R., Kondo, S., Sugawara, S., Yokoyama, M.: A survey of prevention/mitigation against memory corruption attacks. In: 19th International Conference on Network-Based Information Systems (NBiS). IEEE (2016)Meike, G.B.: Inside the Android OS: Building, Customizing, Managing and Operating Android System Services, illustrated ed., P. Education, Ed. Pearson Education, vol. 1 (2018). https://www.amazon.com/Inside-Android-OS-Customizing-Operating/dp/0134096347?SubscriptionId=0JYN1NVW651KCA56C102&tag=techkie-20&linkCode=xm2&camp=2025&creative=165953&creativeASIN=0134096347 . Accessed 18 Jan 2019Cowan, C., Pu, C., Maier, D., Hintongif, H., Walpole, J., Bakke, P., Beattie, S., Grier, A., Wagle, P., Zhang, Q.: StackGuard: automatic adaptive detection and prevention of buffer-overflow attacks. In: Proceedings of the 7th USENIX Security Symposium, pp. 63–78 (1998)’xorl’: Linux GLibC stack canary values (2010). http://xorl.wordpress.com/2010/10/14/linux-glibc-stack-canary-values/ . Accessed 18 Jan 2019Lee, B., Lu, L., Wang, T., Kim, T., Lee, W.: From zygote to morula: fortifying weakened ASLR on Android. In: Proceedings of the 2014 IEEE Symposium on Security and Privacy, Series SP ’14, pp. 424–439. IEEE Computer Society, Washington (2014). https://doi.org/10.1109/SP.2014.34Miller, D.: Security measures in OpenSSH (2007). http://www.openbsd.org/papers/openssh-measures-asiabsdcon2007-slides.pdf . Accessed 18 Jan 2019Molnar, I.: Exec shield, new Linux security feature (2003). https://lwn.net/Articles/31032/ . Accessed 18 Jan 2019Wagle, P., Cowan, C.: StackGuard: simple stack smash protection for GCC. In: Proceedings of the GCC Developers Summit, pp. 243–256 (2003)Etoh, H.: GCC extension for protecting applications from stack-smashing attacks (ProPolice) (2003). http://www.trl.ibm.com/projects/security/ssp/ . Accessed 18 Jan 2019Erb, C., Collins, M., Greathouse, J. L.: Dynamic buffer overflow detection for GPGPUs. In: IEEE/ACM International Symposium on Code Generation and Optimization (CGO), pp. 61–73 IEEE (2017)Molnar, I.: Stackprotector updates for v3.14 (2014). https://lwn.net/Articles/584278/Shen, H.: Add a new option “-fstack-protector-strong” (2012). http://gcc.gnu.org/ml/gcc-patches/2012-06/msg00974.html . Accessed 18 Jan 2019Guan, X., Ji, J., Jiang, J., Zhang, S.: Stack overflow protection device, method, and related compiler and computing device, August 22 2013, uS Patent App. 13/772,858. https://www.google.com/patents/US20130219373 . Accessed 18 Jan 2019Backes, M., Bugiel, S., Derr, E.: Reliable third-party library detection in Android and its security applications. In: Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, Series. CCS ’16, pp. 356–367. ACM, New York (2016)Greenberg, A.: SC magazine: trojanized Android apps steal authentication tokens, put accounts at risk (2014). www.scmagazine.com/trojanized-android-apps-steal-authentication-tokens-put-accounts-at-risk/article/342208/Enck, W., Octeau, D., McDaniel, P., Chaudhuri, S.: A study of android application security. In: Proceedings of the 20th USENIX Conference on Security, Series SEC’11, pp. 21–21. USENIX Association, Berkeley (2011) http://dl.acm.org/citation.cfm?id=2028067.2028088 . Accessed 18 Jan 2019Poll: How often do you reboot? (2014). http://www.androidcentral.com/poll-how-often-do-you-reboot . Accessed 18 Jan 2019Wang, H., Li, H., Li, L., Guo, Y., Xu, G.: Why are android apps removed from Google play? A large-scale empirical study. In Proceedings of the 15th International Conference on Mining Software Repositories, Series MSR ’18, pp. 231–242. ACM, New York (2018). http://doi.acm.org/10.1145/3196398.3196412Marco-Gisbert, H., Ripoll, I.: Preventing brute force attacks against stack canary protection on networking servers. In: 12th International Symposium on Network Computing and Applications, pp. 243–250 (2013)Petsios, T., Kemerlis, V.P., Polychronakis, M., Keromytis, A.D.: DynaGuard: armoring canary-based protections against brute-force attacks. In: Proceedings of the 31st Annual Computer Security Applications Conference, Series ACSAC 2015, pp. 351–360. ACM, New York (2015). http://doi.acm.org/10.1145/2818000.281803",space,983
https://core.ac.uk/download/304995515.pdf,filtered,core,Georgia Institute of Technology,2019-08-21 00:00:00,core,"statistical inference, modeling, and learning of point processes",,"Complex systems, such as healthcare systems, cities, and information networks, often produce a large volume of time series data, along with ordered event data, which are discrete in time and space, and rich in other features (e.g., markers or texts). We model the asynchronous event data as point processes. It is essential to understand and model the complex dynamics of these time series and event data so that accurate prediction, reliable detection, or smart intervention can be carried out for social goods. Specifically, my thesis focuses on the following aspects: (1) new statistical models and effective learning algorithms for complex dynamics exhibited in event data; (2) new inference algorithms for change-point detection, and temporal logic reasoning involving time series and event data. In Chapter 1, we propose a kernel-based nonparametric change-point detection method for high-dimensional streaming data. Change-point detection is an essential topic in modern complex systems. For example, wearable sensors are nowadays common in healthcare systems, which make it possible to monitor patients' health status in real time. Early event detection of deterioration is helpful and can even save patients' lives. However, it is challenging to aggregate measurements from different sensors to form one indicator, and it is not clear how to define pre- and post- change-point distributions. To tackle this problem, in Chapter 1, we propose a distribution-free and computationally efficient kernel-based nonparametric change-point detection method, which enjoys fewer assumptions on the distributions and can handle high-dimensional streaming data. Theoretical tail probability approximation of the nonparametric statistic is also proposed, which provides a statistically principled way to determine the detection thresholds. The proposed nonparametric method shows excellent performance on real human-activity detection dataset and speech dataset. In Chapter 2, we model networked asynchronous event data as point processes and propose a continuous-time change-point detection framework to detect dynamic changes in networks. We cast the problem into a sequential hypothesis test, and derive the generalized likelihood-ratio (GLR) statistic for networked point processes by considering the network topology. The constructed statistic can achieve weak signal detection by aggregating local statistics over time and networks. We further propose to evaluate the proposed GLR statistic via an efficient EM-like algorithm which can be implemented in a distributed fashion across dimensions. Similarly, we obtain a highly accurate theoretical threshold characterization for the proposed GLR statistic and demonstrate the excellent performance of our method on real social media datasets, such as Twitter and Memetracker. In Chapter 3, we propose an expressive model for the event data and further propose an adversarial learning framework to uncover the temporal dynamics. When modeling event data as point processes, instead of hand-crafting the occurrence intensity function by a parametric form, we leverage recent advances in deep learning and parameterize the intensity function as a recurrent neural network (RNN). RNN is a composition of a series of highly flexible nonlinear functions, which allows the model to capture complex dynamics in event data and make the generative process mimic the real data much better than the prior art. Fitting neural network models for even data is challenging. We develop a novel adversarial learning framework to address this challenge and further avoid model-misspecification. Our method provides a novel connection of such event data fitting method to inverse reinforcement learning, where a stochastic policy and the associated reward function are learned simultaneously. The proposed framework has been evaluated on real crime, social network, and healthcare datasets, and outperforms the state-of-the-art methods in data description. In Chapter 4, we propose a unified framework to integrate first-order temporal logic rules into point process models for event data. The proposed modeling framework excels in small data regime and has the ability to incorporate domain knowledge. The proposed temporal logic point processes model the intensity function of the event starts and ends via a set of first-order temporal logic rules. Using softened representation of temporal relations, and a weighted combination of logic rules, our framework can also deal with uncertainty in event data. Furthermore, many existing point process models can be interpreted as special cases of our framework given simple temporal logic rules. We derive a maximum likelihood estimation procedure for the proposed temporal logic point processes, and show that it can lead to accurate predictions when data are sparse and domain knowledge is critical. The proposed framework has been evaluated on real healthcare datasets, and outperforms the neural network models in event predication on small data and is easy to interpret.Ph.D",space,984
'institute of electrical and electronics engineers (ieee)',filtered,core,Learning of a tracker model from multi-radar data for performance prediction of air surveillance system,2017-06-01 00:00:00,core,10.1109/cec.2017.7969562,https://core.ac.uk/download/417852328.pdf,"A valid model of the air surveillance system performance is highly valued when making decisions related to the optimal control of the system. We formulate a model for a multi-radar tracker system by combining a radar performance model with a tracker performance model. A tracker as a complex software system is hard to model mathematically and physically. Our novel approach is to utilize machine learning to create a tracker model based on measurement data from which the input and target output for the model are calculated. The measured data comprises the time series of 3D coordinates of cooperative aircraft flights, the corresponding target detection recordings from multiple radars, and the related multi-radar track recordings. The collected data is used to calculate performance measures for the radars and the tracker at specific locations in the air space. We apply genetic programming to learning such rules from radar performance measures that explain tracker performance. The easily interpretable rules are intended to reveal the real behavior of the system providing comprehension for its control and further development. The learned rules allow predicting tracker performance level for the system control in all radar geometries, modes, and conditions at any location. In the experiments, we show the feasibility of our approach to learning a tracker model and compare our rule learner with two tree classifiers, another rule learner, a neural network, and an instance-based classifier using the real air surveillance data. The tracker model created by our rule learner outperforms the models by the other methods except for the neural network whose prediction performance is equal.acceptedVersionPeer reviewe",space,985
'elsevier bv',filtered,core,Quantitative PET image reconstruction employing nested expectation-maximization deconvolution for motion compensation,2017-09-01 00:00:00,core,10.1016/j.compmedimag.2016.11.006,,"Bulk body motion may randomly occur during PET acquisitions introducing blurring, attenuation-emission mismatches and, in dynamic PET, discontinuities in the measured time activity curves between consecutive frames. Meanwhile, dynamic PET scans are longer, thus increasing the probability of bulk motion. In this study, we propose a streamlined 3D PET motion-compensated image reconstruction (3D-MCIR) framework, capable of robustly deconvolving intra-frame motion from a static or dynamic 3D sinogram. The presented 3D-MCIR methods need not partition the data into multiple gates, such as 4D MCIR algorithms, or access list-mode (LM) data, such as LM MCIR methods, both associated with increased computation or memory resources. The proposed algorithms can support compensation for any periodic and non-periodic motion, such as cardio-respiratory or bulk motion, the latter including rolling, twisting or drifting. Inspired from the widely adopted point-spread function (PSF) deconvolution 3D PET reconstruction techniques, here we introduce an image-based 3D generalized motion deconvolution method within the standard 3D maximum-likelihood expectation-maximization (ML-EM) reconstruction framework. In particular, we initially integrate a motion blurring kernel, accounting for every tracked motion within a frame, as an additional MLEM modeling component in the image space (integrated 3D-MCIR). Subsequently, we replaced the integrated model component with a nested iterative Richardson-Lucy (RL) image-based deconvolution method to accelerate the MLEM algorithm convergence rate (RL-3D-MCIR). The final method was evaluated with realistic simulations of whole-body dynamic PET data employing the XCAT phantom and real human bulk motion profiles, the latter estimated from volunteer dynamic MRI scans. In addition, metabolic uptake rate Ki parametric images were generated with the standard Patlak method. Our results demonstrate significant improvement in contrast-to-noise ratio (CNR) and noise-bias performance in both dynamic and parametric images. The proposed nested RL-3D-MCIR method is implemented on the Software for Tomographic Image Reconstruction (STIR) open-source platform and is scheduled for public release",space,986
'springer science and business media llc',filtered,core,CoMet: a workflow using contig coverage and composition for binning a metagenomic sample with high precision,2017-12-01 00:00:00,core,10.1186/s12859-017-1967-3,,"Abstract Background In metagenomics, the separation of nucleotide sequences belonging to an individual or closely matched populations is termed binning. Binning helps the evaluation of underlying microbial population structure as well as the recovery of individual genomes from a sample of uncultivable microbial organisms. Both supervised and unsupervised learning methods have been employed in binning; however, characterizing a metagenomic sample containing multiple strains remains a significant challenge. In this study, we designed and implemented a new workflow, Coverage and composition based binning of Metagenomes (CoMet), for binning contigs in a single metagenomic sample. CoMet utilizes coverage values and the compositional features of metagenomic contigs. The binning strategy in CoMet includes the initial grouping of contigs in guanine-cytosine (GC) content-coverage space and refinement of bins in tetranucleotide frequencies space in a purely unsupervised manner. With CoMet, the clustering algorithm DBSCAN is employed for binning contigs. The performances of CoMet were compared against four existing approaches for binning a single metagenomic sample, including MaxBin, Metawatt, MyCC (default) and MyCC (coverage) using multiple datasets including a sample comprised of multiple strains. Results Binning methods based on both compositional features and coverages of contigs had higher performances than the method which is based only on compositional features of contigs. CoMet yielded higher or comparable precision in comparison to the existing binning methods on benchmark datasets of varying complexities. MyCC (coverage) had the highest ranking score in F1-score. However, the performances of CoMet were higher than MyCC (coverage) on the dataset containing multiple strains. Furthermore, CoMet recovered contigs of more species and was 18 - 39% higher in precision than the compared existing methods in discriminating species from the sample of multiple strains. CoMet resulted in higher precision than MyCC (default) and MyCC (coverage) on a real metagenome. Conclusions The approach proposed with CoMet for binning contigs, improves the precision of binning while characterizing more species in a single metagenomic sample and in a sample containing multiple strains. The F1-scores obtained from different binning strategies vary with different datasets; however, CoMet yields the highest F1-score with a sample comprised of multiple strains",space,987
'cri electronics',filtered,core,METHOD OF CLASSIFICATION  OF COMPLEX STRUCTURED IMAGES  ON THE BASIS OF SELF-ORGANIZED  NEURAL NETWORK STRUCTURES,2017-01-01 00:00:00,core,10.21778/2413-9599-2016-4-57-65,https://core.ac.uk/download/201032537.pdf,"The relevance in development of intelligent systems for classification of complex structured images occurs during processing of images taken from cameras of UAV used for navigational purposes where there is no communication with artificial Earth satellites, or in the analysis of images in real time by the operator. The developed method provides high requirements to quality of classification of objects on images, as well as to fast selection and classification of investigated segments of images. For classification of such images the appropriate computer technologies based on the boosting methodology are offered. The space of Informative features is formed by spectral windows obtained by scanning of the original image. Spectral windows belonging to different classes, are arranged in the form of clusters on Kohonen plane. To form a cluster, the rules of correction of vectors of weights are used, and such rules make it possible to reduce the values of insignificant components of the vectors and the coordinates of the clusters centers are identified. Strong classifiers are built on the basis of the cluster structure of Kohonen plane. There has been designed and demonstrated the structure of the strong classifier on neural networks of direct distribution referred to block type, which was implemented for classification of chest X-ray images",space,988
'national documentation centre (ekt)',filtered,core,Modeling big data applications and operators in cloud environments,2018-01-01 00:00:00,core,10.12681/eadd/45004,,"The Big Data revolution has created new requirements for the design of applications and operators that are able to handle the volume of the data sources. The adoption of distributed architectures and the increasing popularity of the Cloud paradigm has complexed their structure, making the problem of modeling their behavior increasingly difficulty. Moreover, the wide variety of the existing datasets have complicated the problem of selecting the appropriate inputs for a given operator, since the examination of the data utility for a given workflow is a largely manual process that requires exhaustive execution for the entirety of the available datasets. This thesis attempts to model the behavior of an arbitrary Big Data operator from two different viewpoints.First, we wish to model the operator’s performance when deployed under different resource configurations. To this end, we present an adaptive performance modeling methodology that relies on recursively partitioning the configuration space in disjoint regions, distributing a predefined number of samples to each region based on different region characteristics (i.e., size, modeling error) and deploying the given operator for the selected samples. The performance is, then, approximated for the entire space using a combination of linear models for each subregion. Intuitively, this approach attempts to compromise the contradicting aspects of exploring the configuration space and exploiting the obtained knowledge through focusing on areas with higher approximation error.Second and in order to accelerate data analysis, we wish to model the operator’s output when deployed over different datasets. Based on the observation that similar datasets tend to affect the operators that are applied to them similarly, we propose a content-based methodology that models the output of a provided operator for all datasets. Our approach measures the similarity between the different datasets in the light of some fundamental properties commonly used in data analysis tasks, i.e., the statistical distribution, the dataset size and the tuple ordering. These similarities are, next, projected to a low dimensional metric space that is utilized as an input domain by Neural Networks in order to approximate the operator’s output for all datasets, given the actual operator output for a mere subset of them. Our evaluation, conducted using several real-world operators applied for real and synthetic datasets, indicated that the introduced methodologies manage to accurately model the operator’s behavior from both angles. The adoption of a divide-and-conquer approach that equally respects space exploration and knowledge exploitation for the performance modeling part, proved to be the main reason that our scheme outperforms other state-of-the-art methodologies. On the same time, the construction of a low dimensional dataset metric space for the second part, proved to be particularly informative in order to allow Machine Learning models to approximate operator output for a wide variety of operators with diverse characteristics.Η επανάσταση των Μεγάλων Δεδομένων έχει δημιουργήσει νέες απαιτήσεις για το σχεδιασμό εφαρμογών και τελεστών έτσι ώστε να μπορούν να διαχειρίζονται τον τεράστιο όγκο δεδομένων. Η υιοθέτηση κατανεμημένων τεχνικών και η ολοένα αυξανόμενη δημοτικότητα των Υπολογιστικών Νεφών έχουν συντελέσει στην αύξηση της πολυπλοκότητας της αρχιτεκτονικής των τελεστών Μεγάλων Δεδομένων, κάνοντας το πρόβλημα της μοντελοποίησης της συμπεριφοράς τους ολοένα και πιο δύσκολο. Παράλληλα, η μεγάλη ποικιλομορφία των διαφορετικών πηγών δεδομένων έχει περιπλέξει το πρόβλημα της επιλογής των κατάλληλων εισόδων για έναν τελεστή, καθώς η εξέταση της χρησιμότητας των δεδομένων εισόδου για αυτόν είναι μια μη αυτοματοποιημένη διαδικασία που στηρίζεται στην εξαντλητική εκτέλεση του τελεστή για το σύνολο των διαθέσιμων δεδομένων. Η διατριβή αυτή προσπαθεί να μοντελοποιήσει τη συμπεριφορά ενός δοθέντος τελεστή Μεγάλων Δεδομένων, υπό το πρίσμα δύο διαφορετικών κατευθύνσεων.Πρώτον, η διατριβή αυτή ασχολείται με το πρόβλημα της μοντελοποίησης της απόδοσης ενός τελεστή όταν αυτός εγκαθίσταται με διαφορετικές παραμέτρους. Για το σκοπό αυτό, παρουσιάζεται μια προσαρμοστική μεθοδολογία μοντελοποίησης της απόδοσης μιας εφαρμογής, που στηρίζεται: (α) στην αναδρομική διαμέριση του χώρου παραμέτρων, (β) στην κατανομή ενός προαποφασισμένου αριθμού δειγμάτων σε κάθε υποπεριοχή σύμφωνα με διαφορετικά χαρακτηριστικά της (π.χ., το μέγεθός της, το σφάλμα μοντελοποίησης, κλπ) και (γ) στην φυσική εγκατάσταση της εφαρμογής για τα επιλεχθέντα σύνολα παραμέτρων. Η απόδοση προσεγγίζεται για ολόκληρο το χώρο χρησιμοποιώντας ένα συνδυασμό γραμμικών μοντέλων που εφαρμόζεται σε κάθε υποπεριοχή. Διαισθητικά, η προσέγγιση αυτή προσπαθεί να συμβιβάσει τις αντίρροπες κατευθύνσεις της εξερεύνησης του χώρου παραμέτρων και της εκμετάλλευσης της αποκτηθείσας γνώσης (μέσω των δειγμάτων που έχουν επιλεγεί προηγούμενα) διαμέσου της συγκέντρωσης σε περιοχές με υψηλό σφάλμα προσέγγισης.Δεύτερον και με σκοπό την επιτάχυνση της ανάλυσης των δεδομένων, η διατριβή αυτή προτείνει μια μεθοδολογία για τη μοντελοποίηση της εξόδου ενός τελεστή όταν αυτός εφαρμόζεται σε διαφορετικά σύνολα δεδομένων εισόδου. Με βάση την παρατήρηση ότι όμοια σύνολα δεδομένων τείνουν να επηρεάζουν έναν τελεστή με παρόμοιο τρόπο, προτείνεται μια βασισμένη στο περιεχόμενο μεθοδολογία που μοντελοποιεί την έξοδο ενός τελεστή για όλα τα δεδομένα εισόδου. Η προσέγγιση αυτή ποσοτικοποιεί την ομοιότητα μεταξύ των διαφορετικών συνόλων δεδομένων υπό το πρίσμα τριών θεμελιωδών ιδιοτήτων: (α) τη στατιστική κατανομή τους, (β) το μέγεθος τους και (γ) τη σειρά εμφάνισης των πλειάδων τους. Η ομοιότητα μεταξύ των διαφορετικών συνόλων προβάλλεται, εν συνεχεία, σε ένα μετρικό χώρο χαμηλής διάστασης και χρησιμοποιείται σαν σύνολο ορισμού από ένα Νευρωνικό Δίκτυο που έχει ως σκοπό την προσέγγιση της εξόδου του τελεστή για όλα τα σύνολα, δοθέντων την πραγματικών τιμών εξόδου για ένα μικρό υποσύνολο τους.Η πειραματική αξιολόγηση, που πραγματοποιήθηκε χρησιμοποιώντας μεγάλη πληθώρα πραγματικών τελεστών που εκτελούνται τόσο για πραγματικά όσο και συνθετικά δεδομένα εισόδου, έδειξε ότι οι προτεινόμενες μεθοδολογίες μπορούν μοντελοποιήσουν με υψηλή ακρίβεια τη συμπεριφορά ενός τελεστή Μεγάλων Δεδομένων και από τις δυο εξεταζόμενες σκοπιές. Η υιοθέτηση της τεχνικής “διαίρει και βασίλευε” που σέβεται εξίσου την εξερεύνηση του χώρου παραμέτρων και την εκμετάλλευση της παραγόμενης γνώσης σχετικά με τη μοντελοποίησης της απόδοσης, είναι ο κύριος λόγος που εξηγεί την υψηλότερη ακρίβεια που πετυχαίνει η προταθείσα μεθοδολογία, εν συγκρίσει με άλλες, παρεμφερείς μεθοδολογίες μοντελοποίησης απόδοσης. Παράλληλα, ο μετρικός χώρος χαμηλής διάστασης, που κατασκευάζεται σχετικά με το δεύτερο κομμάτι της διατριβής, περιέχει αρκετή πληροφορία για να επιτρέψει μοντέλα Μηχανικής Μάθησης να προσεγγίσουν την έξοδο ενός μεγάλου αριθμού τελεστών με διαφορετικά χαρακτηριστικά",space,989
frontiers media s.a.,filtered,core,A Spiking Neural Model of HT3D for Corner Detection,2018-06-01 00:00:00,core,10.3389/fncom.2018.00037/full,,"Obtaining good quality image features is of remarkable importance for most computer vision tasks. It has been demonstrated that the first layers of the human visual cortex are devoted to feature detection. The need for these features has made line, segment, and corner detection one of the most studied topics in computer vision. HT3D is a recent variant of the Hough transform for the combined detection of corners and line segments in images. It uses a 3D parameter space that enables the detection of segments instead of whole lines. This space also encloses canonical configurations of image corners, transforming corner detection into a pattern search problem. Spiking neural networks (SNN) have previously been proposed for multiple image processing tasks, including corner and line detection using the Hough transform. Following these ideas, this paper presents and describes in detail a model to implement HT3D as a Spiking Neural Network for corner detection. The results obtained from a thorough testing of its implementation using real images evince the correctness of the Spiking Neural Network HT3D implementation. Such results are comparable to those obtained with the regular HT3D implementation, which are in turn superior to other corner detection algorithms",space,990
'american society for engineering education',filtered,core,Icarus: the development of a voluntary research program to increase engineering students' engagement,2018-01-01 00:00:00,core,10.18260/1-2--29637,,"In order to find ways to address problems of motivation and engagement in civil engineering students, and provide students with a space to develop sense of belonging and engage with their peers through a co-curricular experience, the School of Civil Engineering at [BLINDED FOR REVIEW] in 2015 developed the Icarus program. The purpose of this exploratory study is to present preliminary information about the implementation of Icarus, as an engineering education experiment. The program's goal was to provide students with a different space to develop the competencies and skills desired while simultaneously they form their identity as engineers. The sample was 116 civil engineering students, 49 of them enrolled in the Icarus program in its first semester. Results showed that the main motivation to join the Icarus program was to apply theory from class into engineering real world issues, and to work and engage with peers. In addition, Icarus students have higher levels of aspirations on how well they will do in their engineering courses, and higher levels of deep learning when compared to other non-Icarus engineering students in the same year. Further Implications are provided",space,991
sapienza università editrice,filtered,core,Nanoseismic monitoring for detection of rockfalls. Experiments in quarry areas,2018-01-01 00:00:00,core,10.4408/ijege.2018-01.o-03,https://core.ac.uk/download/159636017.pdf,"Le frane per crollo da ammassi rocciosi fratturati sono tra i processi di instabilità gravitativa che più frequentemente interessano opere antropiche quali tagli su versanti naturali o artificiali, pareti di cava, trincee stradali, autostradali o ferroviarie, sia per ciò che attiene le aree di distacco che per quelle di accumulo. Nell’ambito dell’applicazione di sistemi di early warning per la gestione del rischio geologico legato a queste tipologie di frana, una sperimentazione della tecnica del monitoraggio nanosismometrico è stata effettuata presso due siti estrattivi non più in attività: le “Pirrere” della Baia di Cala Rossa sull’isola di Favignana (Trapani), in Sicilia, e la cava dismessa di Acuto (Frosinone), in Italia Centrale. Il monitoraggio nanosismometrico è una tecnica di indagine che consente di individuare e localizzare deboli eventi sismici, fino a magnitudo locale (ML) nell’ordine di -3, attraverso l’impiego di quattro sensori sismometrici disposti secondo una specifica geometria di array detta SNS (Seismic Navigation System).

Nel presente lavoro, mediante il software NanoseismicSuite sono stati analizzati 73 eventi di crollo indotti artificialmente attraverso la caduta controllata di blocchi di roccia nei due siti estrattivi abbandonati; sono stati lanciati, simulando fenomeni di rockfalls, rispettivamente 47 blocchi di roccia nella cava di Acuto e 26 eventi in quattro diverse cave a cielo aperto presenti nel settore occidentale di Cala Rossa. Tali eventi, avendo punto epicentrale noto, hanno permesso di determinare il miglior modello di sottosuolo in termini di valori di velocità delle onde P ed S attraverso un’operazione di back analysis. L’analisi è stata, infatti, effettuata variando i valori di velocità e scegliendo quelli relativi all’epicentro teorico ottenuto dall’analisi dell’evento che fosse il più vicino possibile al punto reale di impatto del blocco di roccia. Al fine di valutare la sensibilità della geometria dell’array SNS e l’influenza del sito di installazione sulla capacità di individuare e localizzare gli eventi, le sperimentazioni sono state condotte sia variando il raggio di apertura che la zona di installazione degli array: presso Acuto le acquisizioni di segnale sono state condotte prima con un array SNS con apertura di 20 m e successivamente con un array di apertura 10 m, mentre presso Cala Rossa l’array è stato installato alternativamente all’aperto in un’area di plateau roccioso ed in una galleria facente parte dell’area di cava abbandonata.

Analizzando i dati si è ottenuta una precisione dell’ubicazione epicentrale compresa tra il 10 ed il 22% della distanza che intercorre tra la sorgente e l’array nanosismometrico. Il miglior modello di sottosuolo ottenuto per entrambi i casi di studio è risultato avere una velocità delle onde P pari a 900 m/s ed un rapporto VP/VS pari a 1.73, valori in accordo con le condizioni di intenso stato di fratturazione delle rocce carbonatiche affioranti nelle due zone di cava. Per gli eventi di crollo indotti la magnitudo ML è risultata essere compresa tra -2.8 e -1.3; considerando l’energia sviluppata dall’impatto, legata alla massa del blocco ed all’altezza e alla velocità di caduta, non è stato possibile definire una relazione tra magnitudo ed energia, probabilmente a causa delle differenti caratteristiche del punto di impatto dei diversi blocchi. In generale, si è osservato che la precisione di ubicazione degli eventi, in termini di azimuth e distanza dal reale epicentro, è risultata paragonabile sia variando l’apertura dell’array che variando il sito di installazione. Per il sito sperimentale di Acuto, il processo di picking manuale del tempo di primo arrivo delle onde P è risultato essere più affidabile nel caso di array con apertura pari a 10 m. La sperimentazione effettuata a Cala Rossa ha permesso, invece, di osservare una migliore capacità di individuazione degli eventi nelle tracce relative all’array posizionato in galleria a causa della minore rumorosità di base del sito di installazione.

Tra le registrazioni sismometriche sono state identificate varie tipologie di segnali, oltre a quelli generati dal lancio dei blocchi, alcune riconducibili ad eventi naturali di crollo altre a deboli terremoti. L’analisi dei segnali riferibili alla prima tipologia di eventi naturali, effettuata tenendo in considerazione i modelli di sottosuolo precedentemente calibrati, ha portato all’identificazione in ambedue i siti di aree aventi maggiore suscettibilità a frane per crollo. In definitiva, si può ritenere che i risultati ottenuti in questo studio siano incoraggianti rispetto all’efficacia della tecnica di monitoraggio nanosismometrico nell’individuazione e nell’ubicazione di fenomeni di crollo in roccia e portano a ritenere questa tecnica potenzialmente applicabile in aree in cui tali eventi possono interferire con infrastrutture antropiche.In the frame of early warning and risk mitigation studies for landslide processes involving rock masses, two quarry areas (Cala Rossa Bay in Sicily and Acuto in Central Italy) were monitored with SNS (Seismic Navigation System) arrays. In this study, 73 rockfalls were simulated by launches of rock blocks. This allowed to perform a back analysis for defining the best seismic velocity model of the subsoil half-space; the records related to each impact caused by the rock block launch were managed by the nanoseismic monitoring approach, varying the velocity model to obtain a theoretical epicentre as close as possible to the actual location of the impact point. In order to evaluate the sensibility of the SNS array, the results obtained by different array apertures and positions were compared in terms of azimuth and distance error with respect to the real epicentres. On the other hand, several natural rockfalls were detected; their analysis allowed to identify areas having higher susceptibility to rockfalls by using the previously calibrated subsoil half-space model. Further studies are required to better define the areas prone to rockfall generation in the considered test sites; nevertheless, the here obtained results show an encouraging perspective about the application of the nanoseismic monitoring with respect to vulnerable infrastructures in rockfall prone areas",space,992
'oxford university press (oup)',filtered,core,"On the realistic validation of photometric redshifts, or why Teddy will
  never be Happy",2017-03-20 00:00:00,core,10.1093/mnras/stx687,http://arxiv.org/abs/1701.08748,"Two of the main problems encountered in the development and accurate
validation of photometric redshift (photo-z) techniques are the lack of
spectroscopic coverage in feature space (e.g. colours and magnitudes) and the
mismatch between photometric error distributions associated with the
spectroscopic and photometric samples. Although these issues are well known,
there is currently no standard benchmark allowing a quantitative analysis of
their impact on the final photo-z estimation. In this work, we present two
galaxy catalogues, Teddy and Happy, built to enable a more demanding and
realistic test of photo-z methods. Using photometry from the Sloan Digital Sky
Survey and spectroscopy from a collection of sources, we constructed datasets
which mimic the biases between the underlying probability distribution of the
real spectroscopic and photometric sample. We demonstrate the potential of
these catalogues by submitting them to the scrutiny of different photo-z
methods, including machine learning (ML) and template fitting approaches.
Beyond the expected bad results from most ML algorithms for cases with missing
coverage in feature space, we were able to recognize the superiority of global
models in the same situation and the general failure across all types of
methods when incomplete coverage is convoluted with the presence of photometric
errors - a data situation which photo-z methods were not trained to deal with
up to now and which must be addressed by future large scale surveys. Our
catalogues represent the first controlled environment allowing a
straightforward implementation of such tests. The data are publicly available
within the COINtoolbox (https://github.com/COINtoolbox/photoz_catalogues).Comment: 19 pages, 10 figures. Minor revision accepted by MNRAS on 2017 March
  1",space,993
"lausanne, epfl",filtered,core,Parameter Estimation with GNSS-Reflectometry and GNSS Synthetic Aperture Techniques,2018-02-19 00:00:00,core,10.5075/epfl-thesis-8350,,"Aside from intentional interference, multipath is the most significant error source for Global Navigation Satellite Systems (GNSS) receivers in many operational scenarios. In this thesis, we study the multipath estimation from two different perspectives: to retrieve useful information from it using GNSS-Reflectometry (GNSS-R) techniques; and to mitigate its effects or to estimate its direction-of-arrival (DOA) as well as the line-of-sight (LOS) signal¿s using synthetic aperture (SA) processing.

The first part of the thesis focuses on precision bounds for GNSS-R techniques for ground-based receivers, in scenarios where a single antenna simultaneously receives the LOS signal and a specular reflection. First, we derive the Cramér-Rao bound (CRB) of the receiver¿s height and the reflection coefficient, with the latter depending on the surface¿s electrical properties. More specifically, we propose a CRB derivation applicable to GNSS-R techniques that make use of the phase information and long observation times, such as the interference pattern technique (IPT). The derivation is based on the parameter transformation of the Fisher information matrix. We study the dependence of the computed CRB on the scenario and the receiver bandwidth. The CRB results for the simulated scenarios are consistent with the precision reported for many GNSS-R techniques used in these scenarios. The proposed CRB is meant to benchmark and compare new and existing techniques.

Besides the derived CRB, we propose an algorithm to obtain the maximum-likelihood (ML) estimator of the parameters of interest with the IPT: the segmented ML estimator (SML). The SML transforms a complex multivariate optimization problem into multiple simpler ones by dividing the parameter search space taking advantage of the cost function¿s particular structure. The SML is validated with simulated signal and asymptotically cross-validates the CRB results.

The second part of the thesis is devoted to the study of the SA processing of GNSS signals. The goal is to estimate the DOA of the signals received, and mitigate errors in the navigation solution caused by interfering signals, such as multipath. We start by deriving the CRB for the SA context, as a function of the antenna trajectory. This CRB considers the effect of the antenna complex gain, and we show in simulations that it is possible to achieve meaningful DOA estimation only by changing the antenna¿s orientation. We continue by proposing a development framework built upon a signal tracking architecture integrating SA processing. Before any SA processing, it is necessary to estimate and compensate any carrier phase contribution not related to the antenna motion. To do so, we propose two new sequential techniques based on the extended Kalman filter (EKF): EKF1 and EKF2. Also, we develop an open-loop version of the proposed SA tracking architecture, more robust than its closed-loop counterpart. Finally, we validate the proposed architecture and SA-based techniques with synthetic GPS signals at first, and then with real signals, recorded using an antenna mounted on a mechanical rotating arm. The obtained results validate the implemented techniques and show how the proposed SA architecture can ultimately mitigate the position bias error observed in environments with severe multipath interference",space,994
'edp sciences',filtered,core,A new method for unveiling open clusters in Gaia New nearby open clusters confirmed by DR2,2018-10-16 00:00:00,core,10.1051/0004-6361/201833390,,"International audienceContext. The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in astronomy. It includes precise astrometric data (positions, proper motions, and parallaxes) for more than 1.3 billion sources, mostly stars. To analyse such a vast amount of new data, the use of data-mining techniques and machine-learning algorithms is mandatory.Aims. A great example of the application of such techniques and algorithms is the search for open clusters (OCs), groups of stars that were born and move together, located in the disc. Our aim is to develop a method to automatically explore the data space, requiring minimal manual intervention.Methods. We explore the performance of a density-based clustering algorithm, DBSCAN, to find clusters in the data together with a supervised learning method such as an artificial neural network (ANN) to automatically distinguish between real OCs and statistical clusters.Results. The development and implementation of this method in a five-dimensional space (l, b, ϖ, μα*, μδ) with the Tycho-Gaia Astrometric Solution (TGAS) data, and a posterior validation using Gaia DR2 data, lead to the proposal of a set of new nearby OCs. Conclusions. We have developed a method to find OCs in astrometric data, designed to be applied to the full Gaia DR2 archive",space,995
'copernicus gmbh',filtered,core,Ten years of MIPAS measurements with ESA Level 2 processor V6 – Part 1: Retrieval algorithm and diagnostics of the products,2016-06-16 00:00:00,core,10.5194/amt-6-2419-2013,https://core.ac.uk/download/45450607.pdf,"The MIPAS (Michelson Interferometer for Passive Atmospheric Sounding) instrument on the Envisat (Environmental satellite) satellite has provided vertical profiles of the atmospheric composition on a global scale for almost ten years. The MIPAS mission is divided in two phases: the full resolution phase, from 2002 to 2004, and the optimized resolution phase, from 2005 to 2012, which is characterized by a finer vertical and horizontal sampling attained through a reduction of the spectral resolution.  While the description and characterization of the products of the ESA processor for the full resolution phase has been already described in previous papers, in this paper we focus on the performances of the latest version of the ESA (European Space Agency) processor, named ML2PP V6 (MIPAS Level 2 Prototype Processor), which has been used for reprocessing the entire mission. The ESA processor had to perform the operational near real time analysis of the observations and its products needed to be available for data assimilation. Therefore, it has been designed for fast, continuous and automated analysis of observations made in quite different atmospheric conditions and for a minimum use of external constraints in order to avoid biases in the products.  The dense vertical sampling of the measurements adopted in the second phase of the MIPAS mission resulted in sampling intervals finer than the instantaneous field of view of the instrument. Together with the choice of a retrieval grid aligned with the vertical sampling of the measurements, this made ill-conditioned the retrieval problem of the MIPAS operational processor. This problem has been handled with minimal changes to the original retrieval approach but with significant improvements nonetheless. The Levenberg-Marquardt method, already present in the retrieval scheme for its capability to provide fast convergence for nonlinear problems, is now also exploited for the reduction of the ill-conditioning of the inversion. An expression specifically designed for the regularizing Levenberg-Marquardt method has been implemented for the computation of the covariance matrices and averaging kernels of the retrieved products. The regularization of the Levenberg-Marquardt method is controlled by the convergence criteria and is deliberately kept weak. The resulting oscillations of the retrieved profile are a posteriori damped by an innovative self-adapting Tikhonov regularization. The convergence criteria and the weakness of the self-adapting regularization ensure that minimum constraints are used and the best vertical resolution obtainable from the measurements is achieved in all atmospheric conditions.  Random and systematic errors, as well as vertical and horizontal resolution are compared in the two phases of the mission for all products, namely: temperature, H2O, O3, HNO3, CH4, N2O, NO2, CFC-11, CFC-12, N2O5 and ClONO2. The use in the two phases of the mission of different optimized sets of spectral intervals ensures that, despite the different spectral resolutions, comparable performances are obtained in the whole MIPAS mission in terms of random and systematic errors, while the vertical resolution and the horizontal resolution are significantly better in the case of the optimized resolution measurements. © Author(s) 2013.This work has been performed under the ESA study >Support to MIPAS Level 2 product validation>, contract ESA-ESRIN no. 21719/08/I-OL. The authors are grateful to the Astrium team that developed the industrial prototype ML2PP using the ORM code as reference, to Thorsten Fehr and Rolf von Kulhmann for the coordination of the MIPAS Quality Working Group activities and to Michael Kiefer for the work done within the MIPAS Quality Working Group.Peer Reviewe",space,996
"'universidade de sao paulo, agencia usp de gestao da informacao academica (aguia)'",filtered,core,Temporal unsupervised neural networks for identification and control of dynamical systems,2015-11-25 00:00:00,core,10.11606/t.18.2003.tde-25112015-115752,,"A pesquisa em redes neurais artificiais (RNAs) está atualmente experimentando um crescente interesse por modelos que utilizem a variável tempo como um grau de liberdade extra a ser explorado nas representações neurais. Esta ênfase na codificação temporal (temporal coding) tem provocado debates inflamados nas neurociências e áreas correlatas, mas nos últimos anos o surgimento de um grande volume de dados comportamentais e fisiológicos vêm dando suporte ao papel-chave desempenhado por este tipo de representação no cérebro [BALLARD et al. (1998)]. Contribuições ao estudo da representação temporal em redes neurais vêm sendo observadas nos mais diversos tópicos de pesquisa, tais como sistemas dinâmicos não-lineares, redes oscilatórias, redes caóticas, redes com neurônios pulsantes e redes acopladas por pulsos. Como conseqüência, várias tarefas de processamento da informação têm sido investigada via codificação temporal, a saber: classificação de padrões, aprendizagem, memória associativa, controle sensório-motor, identificação de sistemas dinâmicos e robótica. Freqüentemente, porém, não fica muito claro até que ponto a modelagem dos aspectos temporais de uma tarefa contribui para aumentar a capacidade de processamento da informação de modelos neurais. Esta tese busca apresentar, de uma maneira clara e abrangente, os principais conceitos e resultados referentes à proposição de dois modelos de redes neurais não-supervisionadas (RNATs), e como estas lançam mão da codificação temporal para desempenhar melhor a tarefa que lhes é confiada. O primeiro modelo, chamado rede competitiva Hebbiana temporal (competitive temporal Hebbian - CTH), é aplicado especificamente em tarefas de aprendizagem e reprodução de trajetórias do robô manipulador PUMA 560. A rede CTH é uma rede neural competitiva cuja a principal característica é o aprendizado rápido, em apenas uma época de treinamento, de várias trajetórias complexas contendo ) elementos repetidos. As relações temporais da tarefa, representadas pela ordem temporal da trajetória, são capturadas por pesos laterais treinados via aprendizagem hebbiana. As propriedades computacionais da rede CTH são avaliadas através de simulações, bem como através da implementação de um sistema de controle distribuído para o robô PUMA 560 real. O desempenho da rede CTH é superior ao de métodos tabulares (look-up table) tradicionais de aprendizagem de trajetórias robóticas e ao de outras técnicas baseadas em redes neurais, tais como redes recorrentes supervisionadas e modelos de memória associativa bidirecional (BAM). O segundo modelo, chamado rede Auto-Organizável NARX (Self-Organizing NARX-SONARX), é baseado no conhecido algoritmo SOM, proposto por KOHONEN (1997). Do ponto de vista computacional, as propriedades de rede SONARX são avaliadas em diferentes domínios de aplicação, tais como predição de séries temporais caóticas, identificação de um atuador hidráulico e no controle preditivo de uma planta não-linear. Do ponto de vista teórico, demonstra-se que a rede SONARX pode ser utilizada como aproximador assintótico de mapeamentos dinâmicos não-lineares, graças a uma nova técnica de modelagem neural, chamada Memória Associativa Temporal via Quantização Vetorial (MATQV). A MATQV, assim como a aprendizagem hebbiana da rede CTH, é uma técnica de aprendizado associativo temporal. A rede SONARX é comparada com modelos NARX supervisionados, implementados a partir das redes MLP e RBF. Em todos os testes realizados para cada uma das tarefas citadas no parágrafo anterior, a rede SONARX tem desempenho similar ou melhor do que o apresentado por modelos supervisionados tradicionais, com um custo computacional consideravelmente menor. A rede SONARX é também comparada com a rede CTH na parendizagem de trajetórias robóticas complexas, com o intuito de destacar as principais diferenças entre os dois ) tipos de aprendizado associativo. Esta tese também propõe uma taxonomia matemática, baseada na representação por espaço de estados da teoria de sistemas, que visa classificar redes neurais não-supervisionadas temporais com ênfase em suas propriedades computacionais. Esta taxonomia tem como principal objetivo unificar a descrição de modelos neurais dinâmicos, facilitando a análise e a comparação entre diferentes arquiteturas, contrastando suas características representacionais e operacionais. Como exemplo, as redes CTH e a SONARX serão descritas usando a taxonomia proposta.Neural network research is currently witnessing a significant shift of emphasis towards temporal coding, which uses time as an extra degree of freedom in neural representations. Temporal coding is passionately debated in neuroscience and related fields, but in the last few years a large volume of physiological and behavioral data has emerged that supports a key role for temporal coding in the brain [BALLARD et al. (1998)]. In neural networks, a great deal of research is undertaken under the topics of nonlinear dynamics, oscillatory and chaotic networks, spiking neurons, and pulse-coupled networks. Various information processing tasks are investigated using temporal coding, including pattern classification, learning, associative memory, inference, motor control, dynamical systems identification and control, and robotics. Progress has been made that substantially advances the state-of-the-art of neural computing. In many instances, however, it is unclear whether, and to what extent, the temporal aspects of the models contribute to information processing capabilities. This thesis seeks to present, in a clear and collective way, the main issues and results regarding the proposal of two unsupervised neural models, emphasizing how these networks make use of temporal coding to perform better in the task they are engaged in. The first model, called Competitive Temporal Hebbian (CTH) network, is applied specifically to learning and reproduction of trajectories of a PUMA 560 robot. The CTH model is a competitive neural network whose main characteristic is the fast learning, in just one training epoch, of multiple trajectories containing repeated elements. The temporal relationships within the task, represented by the temporal order of the elements of a given trajectory, are coded in lateral synaptic trained with hebbian learning. The computational properties of the CTH network are assessed through simulations, as well ) as through the practical implementation of a distributed control system for the real PUMA 560 robot. The CTH performs better than conventional look-up table methods for robot trajectory learning, and better than other neural-based techniques, such as supervised recurrent networks and bidirectional associative memory models. The second model, called Self-Organizing NARX (SONARX) network, is based on the well-known SOM algorithm by KOHONEN (1997). From the computational view-point, the properties of the SONARX model are evaluated in different application domains, such as prediction of chaotic time series, identification of an hydraulic actuator and predictive control of a non-linear plant. From the theoretic viewpoint, it is shown that the SONARX model can be seen as an asymptotic approximator for nonlinear dynamical mappings, thanks to a new neural modelling technique, called Vector-Quantized Temporal Associative Memory (VQTAM). This VQTAM, just like the hebbian learning rule of the CTH network, is a temporal associative memory techniques. The SONARX network is compared with supervised NARX models which based on the MLP and RBF networks. For all simulations, in each one of the forementioned application domains, the SONARX network had a similar and sometimes better performance than those observed for standard supervised models, with the additional advantage of a lower computational cost. The SONARX model is also compared with the CTH network in trajectory reproduction tasks, in order to contrast the main differences between these two types of temporal associative learning models. In this thesis, it is also proposed a mathematical taxonomy, based on the state-space representation of dynamical systems, for classification of unsupervised temporal neural networks with emphasis in their computational properties. The main goal of this taxonomy is to unify the description of dynamic neural models, ) facilitating the analysis and comparison of different architectures by constrasting their representational and operational characteristics. Is is shown how the CTH and SONARX models can be described using the proposed taxonomy",space,997
'ai access foundation',filtered,core,Robots in Retirement Homes: Applying Off-the-Shelf Planning and Scheduling to a Team of Assistive Robots,2016-08-01 00:00:00,core,10.1613/jair.5306,https://core.ac.uk/download/85123517.pdf,"This paper investigates three different technologies for solving a planning and scheduling problem of deploying multiple robots in a retirement home environment to assist elderly residents. The models proposed make use of standard techniques and solvers developed in AI planning and scheduling, with two primary motivations. First, to find a planning and scheduling solution that we can deploy in our real-world application. Second, to evaluate planning and scheduling technology in terms of the ``model-and-solve'' functionality that forms a major research goal in both domain-independent planning and constraint programming. Seven variations of our application are studied using the following three technologies: PDDL-based planning, time-line planning and scheduling, and constraint-based scheduling. The variations address specific aspects of the problem that we believe can impact the performance of the technologies while also representing reasonable abstractions of the real world application. We evaluate the capabilities of each technology and conclude that a constraint-based scheduling approach, specifically a decomposition using constraint programming, provides the most promising results for our application. PDDL-based planning is able to find mostly low quality solutions while the timeline approach was unable to model the full problem without alterations to the solver code, thus moving away from the model-and-solve paradigm. It would be misleading to conclude that constraint programming is ``better'' than PDDL-based planning in a general sense, both because we have examined a single application and because the approaches make different assumptions about the knowledge one is allowed to embed in a model. Nonetheless, we believe our investigation is valuable for AI planning and scheduling researchers as it highlights these different modelling assumptions and provides insight into avenues for the application of AI planning and scheduling for similar robotics problems. In particular, as constraint programming has not been widely applied to robot planning and scheduling in the literature, our results suggest significant untapped potential in doing so.California Institute of Technology. Keck Institute for Space Studie",space,998
'persee program',filtered,core,Intelligence artificielle sans données ontologiques sur une réalité présupposée,2016-01-01 00:00:00,core,10.3406/intel.2016.1793,,"Artificial Intelligence without Using Ontological Data about a Presupposed Reality. This paper introduces an original model to provide software agents and robots with the capacity of learning by interpreting regularities in their stream of sensorimotor experience rather than by exploiting data that would give them ontological information about a predefined domain. Specifically, this model pulls inspiration from : a) the movement of embodied cognition, b) the philosophy of knowledge, c) constructivist epistemology, and d) the theory of enaction. Respectively to these four influences : a) Our agents discover their environment through their body’s active capacity of experimentation. b) They do not know their environment “ as such” but only “ as they can experience it”. c) They construct knowledge from regularities of sensorimotor experience. d) They have some level of constitutive autonomy. Technically, this model differs from the traditional perception / cognition/ action model in that it rests upon atomic sensorimotor experiences rather than separating percepts from actions. We present algorithms that implement this model, and we describe experiments to validate these algorithms. These experiments show that the agents exhibit a certain form of intelligence through their behaviors, as they construct proto-ontological knowledge of the phenomena that appear to them when they observe persistent possibilities of sensorimotor experiences in time and space. These results promote a theory of artificial intelligence without ontological data about a presupposed reality. An application includes a more robust way of creating robots capable of constructing their own knowledge and goals in the real world, which could be initially unknown to them and un-modeled by their designers.Cet article propose un modèle original pour doter des agents informatiques ou des robots de la capacité d’apprendre en interprétant des régularités dans leur flux d’expériences sensorimotrices plutôt qu’en exploitant des données qui leur apporteraient des informations ontologiques sur un domaine prédéfini. Ce modèle s’inspire en particulier de : a) le courant de la cognition incarnée, b) la philosophie de la connaissance, c) l’épistémologie constructiviste, et d) la théorie de l’énaction. Respectivement à ces quatre influences : a) Nos agents découvrent leur environnement à travers les capacités expérimentales actives de leur corps. b) Ils ne connaissent pas leur environnement «en soi » mais uniquement «en ce qu’ils peuvent en faire l’expérience » . c) Ils construisent leurs connaissances à partir de régularités d’expériences sensorimotrices. d) Ils disposent d’une certaine autonomie constitutive.
Techniquement, ce modèle se distingue du modèle perception/cognition/action classique par le fait qu’il considère des expériences sensorimotrices atomiques au lieu de séparer les percepts et les actions. Nous présentons des algorithmes qui implémentent ce modèle, et décrivons des expérimentations permettant de les valider. Les expérimentations montrent que les agents exhibent une certaine forme d’intelligence dans leurs comportements en construisant une connaissance protoontologique des phénomènes qui apparaissent à eux quand ils constatent des possibilités d’expériences sensorimotrices persistantes dans l’espace et le temps. Ces résultats promeuvent une théorie de l’intelligence artificielle sans données ontologiques sur une réalité présupposée, avec, comme perspectives applicatives, des robots capables de construire leurs propres connaissances et objectifs dans le monde réel, initialement inconnu d’eux et non modélisé par leur concepteur.Georgeon Olivier, Mille Alain, Gay Simon. Intelligence artificielle sans données ontologiques sur une réalité présupposée. In: Intellectica. Revue de l'Association pour la Recherche Cognitive, n°65, 2016/1. Nouvelles approches en Robotique Cognitive. pp. 143-168",space,999
"[{'title': none, 'identifiers': ['2340-1079', 'issn:2340-1079']}]",filtered,core,The implementation of mobile location based-games and Qr codes : the case of MobiGeo,2014-03-01 00:00:00,core,https://core.ac.uk/download/55628774.pdf,"'Associated Management Consultants,  PVT., Ltd.'","Education is being revolutionized by the introduction, of mobile technologies in the teaching and learning process. However, studies that focus in the application of mobile technologies to informal
learning environments is scarce and not systematized [1]. This is the reason for conducting a research
project that involved a urban game MobiGeo, designed in to take better advantage of the flexibility and
ubiquity offered by the Mobile Learning (ML) but also taking into account the importance of motivation
and interaction to enhance students learning.
The definition of ML has been a complicated task for researchers, but there are assumptions that can
not be neglected: the mobility, portability and ubiquity [2], these are features that will drive new learning spaces and thus motivate students. This idea is supported by [2] that introduces the concepts
of ""just in time"", ""just enough"" and ""just-for-me"" and [4] that speaks of the triad ""location
independence”, ""independence time"" and ""meaningful content”.
These principles of ""anytime"" and ""anywhere"" consolidated by mobile technologies came to renew the
variety of educational activities available to teachers and in this context arises the concept of mobile
location-based games. According to [5] ""these games are played in physical space, but at the same
time, they are supported by actions and events in an interconnected virtual space"", which can be
classified into three categories: ludic, pedagogic and hybrid. By being in direct contact with the
contents to assimilate and move in a real context, students will have a more significant learning [6]
and this will result in the mobilization of knowledge in different contexts. To make the connection
between the physical and the virtual world, our research has made use of Qr codes as these devices provide information in real time and in a dynamically way.
For this research was idealized an urban game called “MobiGeo”, that respect the principles
suggested by [7] and that has as common thread the history of the European Union. To measure
results the researchers developed a questionnaire that was adapted from a proposal of [8] which
created a ""Model to evaluate Educational Games”, so our proposal was built taking into account the
motivational model of Kirkpatrick (level1) and encompassing three major dimensions:
Motivation/Interest, Interaction and Perceived Learning. To assess the Motivation/Interest was used
the Model ARCS (Relevance, Confidence and Satisfaction) and items of Fun, Immersion and
Challenge of “Game User Experience”. On the other hand the interaction was evaluated by items of
the Social Interaction dimension of the “Game of User Experience”, the Learning Perceptions were
evaluated by Bloom's Taxonomy (Knowledge category).
In this paper we present the design and implementation of the MobiGeo outdoor learning activity with
a group of 173 students from the 7th grade of a basic school in the north of Portugal. Initial results
show that this urban game with Qr codes was an adequate activity to use in informal learning
environments that could engage students in gaming with high degrees of motivation and interaction in order to solve the tasks presented to them and so consolidate and acquired new knowledge about the European Union.CIEC – Research Centre on Child Studies, UM (FCT R&D 317",space,1000
10.1007/s12369-011-0113-z,filtered,core,Learning the selection of actions for an autonomous social robot by reinforcement learning based on motivations,2011-11-01 00:00:00,core,https://core.ac.uk/download/29405426.pdf,'Springer Science and Business Media LLC',"Autonomy is a prime issue on robotics field and it is closely related to decision making. Last researches on decision making for social robots are focused on biologically inspired mechanisms for taking decisions. Following this approach, we propose a motivational system for decision making, using internal (drives) and external stimuli for learning to choose the right action. Actions are selected from a finite set of skills in order to keep robot's needs within an acceptable range. The robot uses reinforcement learning in order to calculate the suitability of every action in each state. The state of the robot is determined by the dominant motivation and its relation to the objects presents in its environment. The used reinforcement learning method exploits a new algorithm called Object Q-Learning. The proposed reduction of the state space and the new algorithm considering the collateral effects (relationship between different objects) results in a suitable algorithm to be applied to robots living in real environments. In this paper, a first implementation of the decision making system and the learning process is implemented on a social robot showing an improvement in robot's performance. The quality of its performance will be determined by observing the evolution of the robot's wellbeing.The funds provided by the Spanish Government through the project called “Peer
to Peer Robot-Human Interaction” (R2H), of MEC (Ministry of Science and Education), the project “A new approach to social robotics” (AROS), of MICINN (Ministry of Science and Innovation), and the RoboCity2030-II-CM project (S2009/DPI-1559), funded by Programas de Actividades I+D en la Comunidad de Madrid and cofunded by Structural Funds of the EU",space,1001
10.1002/nbm.2895,filtered,core,Extracting MRS discriminant functional features of brain tumors,2013-05-01 00:00:00,core,http://hdl.handle.net/10251/34457,'Wiley',"The current challenge in automatic brain tumor classification based on MRS is the improvement of the robustness of the classification models that explicitly account for the probable breach of the independent and identically distributed conditions in the MRS data points. To contribute to this purpose, a new algorithm for the extraction of discriminant MRS features of brain tumors based on a functional approach is presented. Functional data analysis based on region segmentation (RSFDA) is based on the functional data analysis formalism using nonuniformly distributed B splines according to spectral regions that are highly correlated. An exhaustive characterization of the method is presented in this work using controlled and real scenarios. The performance of RSFDA was compared with other widely used feature extraction methods. In all simulated conditions, RSFDA was proven to be stable with respect to the number of variables selected and with respect to the classification performance against noise and baseline artifacts. Furthermore, with real multicenter datasets classification, RSFDA and peak integration (PI) obtained better performance than the other feature extraction methods used for comparison. Other advantages of the method proposed are its usefulness in selecting the optimal number of features for classification and its simplified functional representation of the spectra, which contributes to highlight the discriminative regions of the MR spectrum for each classification task. © 2012 John Wiley & Sons, Ltd.The authors gratefully acknowledge former INTERPRET and eTUMOUR European project partners. Data providers: Professor B. Celda (Physical Chemistry, University of Valencia, Burjassot, Valencia, Spain); Dr F. A. Howe (St George's University of London, London, UK); Dr D. Monleon (Fundacion Investigacion HCUV-University of Valencia, Valencia, Spain); Professor A. Heerschap (Radboud University, Nijmegen, the Netherlands.); Dr. W. Gajewicz, Professor L. Stefanczyk and Dr J. Fortuniak (Uniwersytet Medycznyw Lodz, Lodz, Poland); Professor J. Griffiths (CR UK Cambridge Research Institute, Cambridge, UK); Professor A. C. Peet (Academic Department of Paediatrics and Child Health, University of Birmingham, Birmingham, UK); Professor W. Semmler (Department of Medical Physics in Radiology, German Cancer Research Center, Heidelberg, Germany); Dr. J. Calvar (Fundacion para la Lucha contra las Enfermedades Neurologicas de la Infancia, Buenos Aires, Argentina); Dr. J. Capellades (Hospital Universitari Germans Trias i Pujol, Badalona, Spain); and Dr. C. Majos (Hospital Universitari de Bellvitge, L'Hospitalet de Llobregat, Barcelona, Spain). Data curators: Professor C. Arus, Dr M. Julia-Sape, Dr A. P. Candiota, Dr I. Olier, Ms T. Delgado, Ms J. Martin, Ms M. Camison and Mr A. Perez [all from Grup d'Aplicacions Biomediques de la Ressonancia Magestica Nuclear (GABRMN), Universitat Aut noma de Barcelona (UAB) (GABRMN-UAB) and CIBER-BBN]; and Professor B. Celda, Dra. M. C. Martinez-Bisbal and Dra. B. Martinez-Granados (all from Physical Chemistry, University of Valencia, Burjassot, Valencia, Spain). The authors would also like to thank Dr F. A. Howe and F. Raschke for their suggestions and comments. This work was partially funded by the European Commission: eTUMOUR (contract no. FP6-2002-LIFESCIHEALTH 503094), the HEALTHAGENTS EC project (HEALTHAGENTS) (contract no. FP6-2005-IST 027213).Fuster García, E.; Tortajada Velert, S.; Vicente Robledo, J.; Robles Viejo, M.; García Gómez, JM. (2013). Extracting MRS discriminant functional features of brain tumors. NMR in Biomedicine. 26(5):578-592. doi:10.1002/nbm.2895S578592265Tate, A. R., Underwood, J., Acosta, D. M., Julià-Sapé, M., Majós, C., Moreno-Torres, À., … Arús, C. (2006). Development of a decision support system for diagnosis and grading of brain tumours usingin vivo magnetic resonance single voxel spectra. NMR in Biomedicine, 19(4), 411-434. doi:10.1002/nbm.1016Pérez-Ruiz, A., Julià-Sapé, M., Mercadal, G., Olier, I., Majós, C., & Arús, C. (2010). The INTERPRET Decision-Support System version 3.0 for evaluation of Magnetic Resonance Spectroscopy data from human brain tumours and other abnormal brain masses. BMC Bioinformatics, 11(1), 581. doi:10.1186/1471-2105-11-581Poullet, J.-B., Sima, D. M., Simonetti, A. W., De Neuter, B., Vanhamme, L., Lemmerling, P., & Van Huffel, S. (2007). An automated quantitation of short echo time MRS spectra in an open source software environment: AQSES. NMR in Biomedicine, 20(5), 493-504. doi:10.1002/nbm.1112Ratiney, H., Sdika, M., Coenradie, Y., Cavassila, S., Ormondt, D. van, & Graveron-Demilly, D. (2005). Time-domain semi-parametric estimation based on a metabolite basis set. NMR in Biomedicine, 18(1), 1-13. doi:10.1002/nbm.895Luts, J., Poullet, J.-B., Garcia-Gomez, J. M., Heerschap, A., Robles, M., Suykens, J. A. K., & Huffel, S. V. (2008). Effect of feature extraction for brain tumor classification based on short echo time1H MR spectra. Magnetic Resonance in Medicine, 60(2), 288-298. doi:10.1002/mrm.21626Menze, B. H., Lichy, M. P., Bachert, P., Kelm, B. M., Schlemmer, H.-P., & Hamprecht, F. A. (2006). Optimal classification of long echo timein vivo magnetic resonance spectra in the detection of recurrent brain tumors. NMR in Biomedicine, 19(5), 599-609. doi:10.1002/nbm.1041Provencher, S. W. (2001). Automatic quantitation of localizedin vivo1H spectra with LCModel. NMR in Biomedicine, 14(4), 260-264. doi:10.1002/nbm.698Wilson, M., Reynolds, G., Kauppinen, R. A., Arvanitis, T. N., & Peet, A. C. (2010). A constrained least-squares approach to the automated quantitation of in vivo1H magnetic resonance spectroscopy data. Magnetic Resonance in Medicine, 65(1), 1-12. doi:10.1002/mrm.22579Simonetti, A. W., Melssen, W. J., Edelenyi, F. S. de, van Asten, J. J. A., Heerschap, A., & Buydens, L. M. C. (2005). Combination of feature-reduced MR spectroscopic and MR imaging data for improved brain tumor classification. NMR in Biomedicine, 18(1), 34-43. doi:10.1002/nbm.919Raschke, F., Fuster-Garcia, E., Opstad, K. S., & Howe, F. A. (2011). Classification of single-voxel 1H spectra of brain tumours using LCModel. NMR in Biomedicine, 25(2), 322-331. doi:10.1002/nbm.1753Opstad, K. S., Ladroue, C., Bell, B. A., Griffiths, J. R., & Howe, F. A. (2007). Linear discriminant analysis of brain tumour1H MR spectra: a comparison of classification using whole spectra versus metabolite quantification. NMR in Biomedicine, 20(8), 763-770. doi:10.1002/nbm.1147Fuster-Garcia, E., Navarro, C., Vicente, J., Tortajada, S., García-Gómez, J. M., Sáez, C., … Robles, M. (2011). Compatibility between 3T 1H SV-MRS data and automatic brain tumour diagnosis support systems based on databases of 1.5T 1H SV-MRS spectra. Magnetic Resonance Materials in Physics, Biology and Medicine, 24(1), 35-42. doi:10.1007/s10334-010-0241-8García-Gómez, J. M., Luts, J., Julià-Sapé, M., Krooshof, P., Tortajada, S., Robledo, J. V., … Robles, M. (2008). Multiproject–multicenter evaluation of automatic brain tumor classification by magnetic resonance spectroscopy. Magnetic Resonance Materials in Physics, Biology and Medicine, 22(1), 5-18. doi:10.1007/s10334-008-0146-yTate, A. R., Majós, C., Moreno, A., Howe, F. A., Griffiths, J. R., & Arús, C. (2002). Automated classification of short echo time in in vivo1H brain tumor spectra: A multicenter study. Magnetic Resonance in Medicine, 49(1), 29-36. doi:10.1002/mrm.10315Julià-Sapé, M., Acosta, D., Mier, M., Arùs, C., & Watson, D. (2006). A Multi-Centre, Web-Accessible and Quality Control-Checked Database of in vivo MR Spectra of Brain Tumour Patients. Magnetic Resonance Materials in Physics, Biology and Medicine, 19(1), 22-33. doi:10.1007/s10334-005-0023-xLukas, L., Devos, A., Suykens, J. A. K., Vanhamme, L., Howe, F. A., Majós, C., … Van Huffel, S. (2004). Brain tumor classification based on long echo proton MRS signals. Artificial Intelligence in Medicine, 31(1), 73-89. doi:10.1016/j.artmed.2004.01.001Vanhamme, L., Sundin, T., Hecke, P. V., & Huffel, S. V. (2001). MR spectroscopy quantitation: a review of time-domain methods. NMR in Biomedicine, 14(4), 233-246. doi:10.1002/nbm.695Van der Graaf, M., Julià-Sapé, M., Howe, F. A., Ziegler, A., Majós, C., Moreno-Torres, A., … Heerschap, A. (2008). MRS quality assessment in a multicentre study on MRS-based classification of brain tumours. NMR in Biomedicine, 21(2), 148-158. doi:10.1002/nbm.1172Sáez C García-Gómez JM Vicente J Tortajada S Esparza M Navarro A Fuster-Garcia E Robles M Martí-Bonmatí L Arús C A generic decision support system featuring an assembled view of predictive models for magnetic resonance and clinical data 25th Annual Scientific Meeting ESMRMBKlose, U. (1990). In vivo proton spectroscopy in presence of eddy currents. Magnetic Resonance in Medicine, 14(1), 26-30. doi:10.1002/mrm.1910140104Van den Boogaart A Van Hecke P Van Huffel S Graveron-Demilly S Van Ormondt D de Beer R MRUI: a graphical user interface for accurate routine MRS data analysis 13th Annual Scientific Meeting ESMRMB 1996 319FISHER, R. A. (1936). THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS. Annals of Eugenics, 7(2), 179-188. doi:10.1111/j.1469-1809.1936.tb02137.xDe Boor, C. (1978). A Practical Guide to Splines. Applied Mathematical Sciences. doi:10.1007/978-1-4612-6333-3Ramsay, J. (2005). Functional Data Analysis. Encyclopedia of Statistics in Behavioral Science. doi:10.1002/0470013192.bsa239Craven, P., & Wahba, G. (1978). Smoothing noisy data with spline functions. Numerische Mathematik, 31(4), 377-403. doi:10.1007/bf01404567Pearson, K. (1901). LIII. On lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11), 559-572. doi:10.1080/14786440109462720FISHER, R. A. (1936). THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS. Annals of Eugenics, 7(2), 179-188. doi:10.1111/j.1469-1809.1936.tb02137.xGarcía-Gómez, J. M., Tortajada, S., Vidal, C., Julià-Sapé, M., Luts, J., Moreno-Torres, À., … Robles, M. (2008). The effect of combining two echo times in automatic brain tumor classification by MRS. NMR in Biomedicine, 21(10), 1112-1125. doi:10.1002/nbm.1288Devos, A., Lukas, L., Suykens, J. A. K., Vanhamme, L., Tate, A. R., Howe, F. A., … Van Huffel, S. (2004). Classification of brain tumours using short echo time 1H MR spectra. Journal of Magnetic Resonance, 170(1), 164-175. doi:10.1016/j.jmr.2004.06.010Daubechies, I. (1992). Ten Lectures on Wavelets. doi:10.1137/1.9781611970104Hyvärinen, A., Karhunen, J., & Oja, E. (2001). Independent Component Analysis. Adaptive and Learning Systems for Signal Processing, Communications, and Control. doi:10.1002/047122131",space,1002
10.5614/itbj.eng.sci.2011.43.2.1,filtered,core,Combination of Minimum-Maximum (M-m) Attribute and Zero-INTENS-Difference (Z-i-d) Attribute for Estimating Seismically Thin-Bed Thickness,2011-01-01 00:00:00,core,https://media.neliti.com/media/publications/72383-en-combination-of-minimum-maximum-m-m-attri.pdf,Bandung Institute of Technology,"This  paper  demonstrates  a  new  alternative  way  in  estimating seismically thin-bed (below-tuning) thickness. Initial thickness is built by bandpass filtering the amplitude display of a zero-phase seismic. The filter removes the  non  minimum  and  or  non  maximum  and  left  the  maximum  and  or  the minimum of seismic amplitude. The unresolved below-tuning thickness is then corrected  by  zero-INTENS-difference  (z-i-d)  attribute.  INTENS  is  integrated energy  spectra,  an  attribute  which  can  be  derived  from  spectral  analysis.  z-i-d attribute is zero difference of INTENS between the seismic and its synthetic. The method  generates  INTENS  difference  profile  by  subtracting  seismic  INTENS and its synthetic INTENS iteratively. The iteration is controlled by dipole space shifting from  distance to closer or  vice  versa.  The true thickness is derived  by locating z-i-d which laid in INTENS different profile. It has found that, for free noise  true  seismic  and  perfect-wavelet  (a  wavelet  which  only  approximately similar  with  wavelet  which  constructing  the  true  seismic)  synthetic  seismic,  in INTENS  different  profile,  the  z-i-d  location  always  corresponds  to  true  dipole space or thickness. The method could resolve all thickness of a wedge-modeled seismic with three different dominant frequencies. When the synthetic seismic is constructed with imperfect wavelet, slightly different analysis is needed to locate z-i-d  attribute  and  the  result  is  not  as  perfect  as  when  perfect  wavelet constructing synthetic seismic. A quiet similar result is got when the method is implemented  for  noisy  wedge-modeled  seismic.  Bad  thickness  estimation  is resulted  for  20%  noise  seismic.  The  method  algorithm  is  extended  for  similar dipole polarity model and multilayer model to bring the method to real seismic data  nearer.  The  extension  is  done  by  estimating  thickness  of  every  layer  of  a stacked-wedge-modeled  seismic. The algorithm then generalized for estimating layers  thickness  with  several  thickness  combinations.  The  method  was  able  to delineate shallow channel of Stratton Field by providing good pseudo-acousticimpedance (pseudo AI) map",space,1003
10.18418/978-3-96043-008-7,filtered,core,3D people detection in domestic environments,2012-03-15 00:00:00,core,https://core.ac.uk/download/35168175.pdf,Hochschule Bonn-Rhein-Sieg,"The ability of detecting people has become a crucial subtask, especially in robotic systems which aim an application in public or domestic environments. Robots already provide their services e.g. in real home improvement markets and guide people to a desired product. In such a scenario many robot internal tasks would benefit from the knowledge of knowing the number and positions of people in the vicinity. The navigation for example could treat them as dynamical moving objects and also predict their next motion directions in order to compute a much safer path. Or the robot could specifically approach customers and offer its services. This requires to detect a person or even a group of people in a reasonable range in front of the robot. Challenges of such a real-world task are e.g. changing lightning conditions, a dynamic environment and different people shapes. In this thesis a 3D people detection approach based on point cloud data provided by the Microsoft Kinect is implemented and integrated on mobile service robot. A Top-Down/Bottom-Up segmentation is applied to increase the systems flexibility and provided the capability to the detect people even if they are partially occluded. A feature set is proposed to detect people in various pose configurations and motions using a machine learning technique. The system can detect people up to a distance of 5 meters. The experimental evaluation compared different machine learning techniques and showed that standing people can be detected with a rate of 87.29% and sitting people with 74.94% using a Random Forest classifier. Certain objects caused several false detections. To elimante those a verification is proposed which further evaluates the persons shape in the 2D space. The detection component has been implemented as s sequential (frame rate of 10 Hz) and a parallel application (frame rate of 16 Hz). Finally, the component has been embedded into complete people search task which explorates the environment, find all people and approach each detected person",space,1004
10.1007/s11269-006-0326-3,filtered,core,An intelligent decision support system for management of floods,2006-01-01 00:00:00,core,https://digitalscholarship.unlv.edu/fac_articles/108,Digital Scholarship@UNLV,"Integrating human knowledge with modeling tools, an intelligent decision support system (DSS) is developed to assist decision makers during different phases of flood management. The DSS is developed as a virtual planning tool and can address both engineering and non-engineering issues related to flood management. Different models (hydrodynamic, forecasting, and economic) that are part of the DSS share data and communicate with each other by providing feedback. The DSS is able to assist in: selecting suitable flood damage reduction options (using an expert system approach); forecasting floods (using artificial neural networks approach); modeling the operation of flood control structures; and describing the impacts (area flooded and damage) of floods in time and space. The proposed DSS is implemented for the Red River Basin in Manitoba, Canada. The results from the test application of DSS for 1997 flood in the Red River Basin are very promising. The DSS is able to predict the peak flows with 2% error and reveals that with revised operating rules the contribution of Assiniboine River to the flooding of Winnipeg city can be significantly reduced. The decision support environment allows a number of “what-if” type questions to be asked and answered, thus, multiple decisions can be tried without having to deal with the real life consequences",space,1005
10.48780/publications.aston.ac.uk.00028743,filtered,core,Computationsl modelling of dimethyl ether separation and steam reforming in fluidized bed reactors,,core,https://core.ac.uk/download/78898986.pdf,,"This study presents a computational fluid dynamic (CFD) study of Dimethyl Ether (DME) gas adsorptive separation and steam reforming (DME-SR) in a large scale Circulating Fluidized Bed (CFB) reactor. The CFD model is based on Eulerian-Eulerian dispersed flow and solved using commercial software (ANSYS FLUENT). Hydrogen is currently receiving increasing interest as an alternative source of clean energy and has high potential applications, including the transportation sector and power generation. Computational fluid dynamic (CFD) modelling has attracted considerable recognition in the engineering sector consequently leading to using it as a tool for process design and optimisation in many industrial processes. In most cases, these processes are difficult or expensive to conduct in lab scale experiments. The CFD provides a cost effective methodology to gain detailed information up to the microscopic level. The main objectives in this project are to: (i) develop a predictive model using ANSYS FLUENT (CFD) commercial code to simulate the flow hydrodynamics, mass transfer, reactions and heat transfer in a large scale dual fluidized bed system for combined gas separation and steam reforming processes (ii) implement a suitable adsorption models in the CFD code, through a user defined function, to predict selective separation of a gas from a mixture (iii) develop a model for dimethyl ether steam reforming (DME-SR) to predict hydrogen production (iv) carry out detailed parametric analysis in order to establish ideal operating conditions for future industrial application. The project has originated from a real industrial case problem in collaboration with the industrial partner Dow Corning (UK) and jointly funded by the Engineering and Physical Research Council (UK) and Dow Corning. The research examined gas separation by adsorption in a bubbling bed, as part of a dual fluidized bed system. The adsorption process was simulated based on the kinetics derived from the experimental data produced as part of a separate PhD project completed under the same fund. The kinetic model was incorporated in FLUENT CFD tool as a pseudo-first order rate equation; some of the parameters for the pseudo-first order kinetics were obtained using MATLAB. The modelling of the DME adsorption in the designed bubbling bed was performed for the first time in this project and highlights the novelty in the investigations. The simulation results were analysed to provide understanding of the flow hydrodynamic, reactor design and optimum operating condition for efficient separation. Bubbling bed validation by estimation of bed expansion and the solid and gas distribution from simulation agreed well with trends seen in the literatures. Parametric analysis on the adsorption process demonstrated that increasing fluidizing velocity reduced adsorption of DME. This is as a result of reduction in the gas residence time which appears to have much effect compared to the solid residence time. The removal efficiency of DME from the bed was found to be more than 88%. Simulation of the DME-SR in FLUENT CFD was conducted using selected kinetics from literature and implemented in the model using an in-house developed user defined function. The validation of the kinetics was achieved by simulating a case to replicate an experimental study of a laboratory scale bubbling bed by Vicente et al [1]. Good agreement was achieved for the validation of the models, which was then applied in the DME-SR in the large scale riser section of the dual fluidized bed system. This is the first study to use the selected DME-SR kinetics in a circulating fluidized bed (CFB) system and for the geometry size proposed for the project. As a result, the simulation produced the first detailed data on the spatial variation and final gas product in such an industrial scale fluidized bed system. The simulation results provided insight in the flow hydrodynamic, reactor design and optimum operating condition. The solid and gas distribution in the CFB was observed to show good agreement with literatures. The parametric analysis showed that the increase in temperature and steam to DME molar ratio increased the production of hydrogen due to the increased DME conversions, whereas the increase in the space velocity has been found to have an adverse effect. Increasing temperature between 200 oC to 350 oC increased DME conversion from 47% to 99% while hydrogen yield increased substantially from 11% to 100%. The CO2 selectivity decreased from 100% to 91% due to the water gas shift reaction favouring CO at higher temperatures. The higher conversions observed as the temperature increased was reflected on the quantity of unreacted DME and methanol concentrations in the product gas, where both decreased to very low values of 0.27 mol% and 0.46 mol% respectively at 350 °C. Increasing the steam to DME molar ratio from 4 to 7.68 increased the DME conversion from 69% to 87%, while the hydrogen yield increased from 40% to 59%. The CO2 selectivity decreased from 100% to 97%. The decrease in the space velocity from 37104 ml/g/h to 15394 ml/g/h increased the DME conversion from 87% to 100% while increasing the hydrogen yield from 59% to 87%. The parametric analysis suggests an operating condition for maximum hydrogen yield is in the region of 300 oC temperatures and Steam/DME molar ratio of 5. The analysis of the industrial sponsor’s case for the given flow and composition of the gas to be treated suggests that 88% of DME can be adsorbed from the bubbling and consequently producing 224.4t/y of hydrogen in the riser section of the dual fluidized bed system. The process also produces 1458.4t/y of CO2 and 127.9t/y of CO as part of the product gas. The developed models and parametric analysis carried out in this study provided essential guideline for future design of DME-SR at industrial level and in particular this work has been of tremendous importance for the industrial collaborator in order to draw conclusions and plan for future potential implementation of the process at an industrial scale",space,1006
10.1080/17445760.2017.1390098,filtered,core,A self-aware paradigm for autonomous architectural systems within the Internet of Things,,core,https://core.ac.uk/download/96575110.pdf,'Informa UK Limited',"This research explores a newfangled approach for the notion that architecture is

no longer concerned with the organization of space and matter, but rather a

system of systems with self-organizational behavior and progressively complex

programmatic interventions. Such system represents an Internet of Things (IoT)

paradigm that has the capacity to self-organize a pre-defined architecture

acclimating dynamically to the demands of the environment through a process of

self-awareness and re-configurability.

This paper is an attempt to develop spaces that bring computation into the

physical world by introducing artificial intelligence into building systems so as to

communicate, exchange information, and allow right responses and decisions

within a sustainable manner. Environments with embedded computational

systems and adaptive reconfiguration behavior will be precisely studied.

With an intensive focus on smart, self reconfiguring architecture that has

embraced kinetic motion as an approach for environmental adaptation and

responsiveness. This study addresses the networked organization of sensory

systems that incorporate computational platforms in relation to user’s desires,

where architecture turns into an interactive intermediary between human and

computation.

Correspondingly, the authors are particularly aiming to propose three conceptual

frameworks for delivering a smart system at the architectural scale which is

capable of reconfiguring and interacting constantly in real time, precisely

focusing on an initial implementation for the first framework utilizing an

experimental customized prototype, while the other two frameworks would be

posteriorly studied through future work.

Thus the IoT will foster a rapid development in intelligent systems which would

directly introduce an advanced technological leap forward in accommodating a

new way of demonstrating human-computer interaction within a novel

architectural design approach",space,1007
10.1109/icmla.2015.183,filtered,2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA),IEEE,2015-12-11 00:00:00,ieeexplore,comparative evaluation of top-n recommenders in e-commerce: an industrial perspective,https://ieeexplore.ieee.org/document/7424455/,"We experiment on two real e-commerce datasets and survey more than 30 popular e-commerce platforms to reveal what methods work best for product recommendations in industrial settings. Despite recent academic advances in the field, we observe that simple methods such as best-seller lists dominate deployed recommendation engines in e-commerce. We find our empirical findings to be well-aligned with those of the survey, where in both cases simple personalized recommenders achieve higher ranking than more advanced techniques. We also compare the traditional random evaluation protocol to our proposed chronological sampling method, which can be used for determining the optimal time-span of the training history for optimizing the performance of algorithms. This performance is also affected by a proper hyperparameter tuning, for which we propose golden section search as a fast alternative to other optimization techniques.",e-commerce,1008
10.1109/icacsis.2018.8618144,filtered,2018 International Conference on Advanced Computer Science and Information Systems (ICACSIS),IEEE,2018-10-28 00:00:00,ieeexplore,protagoras: a service for tagging e-commerce products at scale,https://ieeexplore.ieee.org/document/8618144/,"Despite widespread adoption of machine learning to solve real world problems, the implementation of ML solutions in production environment is more complicated than it seems. It is quite straightforward to write machine learning codes these days but they are not designed to be deployed in production scale where millions of requests per day is a norm. In this paper, we describe our implementation of a ML service for large scale product tagging in e-commerce called Protagoras. The problem of tagging products can be seen as multi-label classification where the labels are product tags. By performing the classification within each product category, the precision can be increased and the inference can be performed faster. Protagoras combined the scalability and speed of microservice implementation in Golang and robust machine learning implementation in Python. We present the architecture of the system with all its components including API endpoints, job queue, database, and monitoring. The benchmark shows that, even with 1000 classifiers in one category, the average latency for online inference is below 300 millisecond. The throughput can be further maximized by replicating the service into multiple servers.",e-commerce,1009
10.1109/ic4e.2010.136,filtered,"2010 International Conference on e-Education, e-Business, e-Management and e-Learning",IEEE,2010-01-24 00:00:00,ieeexplore,towards an ethical sales-agent in e-commerce,https://ieeexplore.ieee.org/document/5432428/,"For simulating a shop on cyberspace with complete properties of a real shop, we need to model a shop's clerks as virtual clerks with complete capabilities of a real clerk. For this purpose we need an autonomous entity which is called autonomous sales-agent. As autonomy and human-like capabilities of these agents, such as predicting customer's preference, have been developing, they can perform unethical behaviors besides their normal behaviors. So the capability of them to make moral decisions becomes an important concern. In this paper we adapted our previous proposed general framework for creating an artificial ethical agent which can be integrated into the reasoning structure of sales-agent in order to add the capability of performing ethical behaviors to autonomous sales-agent. Some implementation guidelines are also presented for implementation of proposed framework by JColibri and JACK.",e-commerce,1010
10.1109/icais50930.2021.9395759,filtered,2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS),IEEE,2021-03-27 00:00:00,ieeexplore,a comprehensive survey on movie recommendation systems,https://ieeexplore.ieee.org/document/9395759/,"Internet technology has occupied an important part of human lives. Users often face the problem of the available excessive information. Recommandation system (RS) are deployed to help users cope up with the information explosion. RS is mostly used in digital entertainment, such as Netflix, prime video, and IMDB, and e-commerce portals such as Amazon, Flipkart, and eBay. The two traditional methods namely, collaborative filtering (CF) and content-based approaches consist of few limitations individually. However, any hybrid system, which utilizes the advantage of both the systems to leverage better results. Some fundamental issues faced by movie recommendation systems such as scalability, cold start problem, data sparsity and practical usage feedback and verification based on real implementation are still neglected. Other issues that require significant research attention are accuracy and time complexity problem, which could make RS, a bad candidate for real-world recommendation systems. This literature survey aims to consolidate and structurally categorize all the major drawbacks present in the most common and popular commercial movie recommendation systems.",e-commerce,1011
10.1109/isda.2009.48,filtered,2009 Ninth International Conference on Intelligent Systems Design and Applications,IEEE,2009-12-02 00:00:00,ieeexplore,a jade-based art-inspired ontology and protocols for handling trust and reputation,https://ieeexplore.ieee.org/document/5364833/,"Trust and reputation management play an important role in agent-based recommender systems. Although several protocols and ontologies of agents using trust and reputation has been proposed, none of them has been so extensively used and implicitly accepted by research community as those from agent reputation and trust (ART in advance) testbed. The motivation of this adaptation is to facilitate the use of ART principles in real distributed applications instead of a centralized testbed for experimentation. This paper presents an adaptation of the protocols proposed by ART testbed to a codification for the most popular agent platform: JADE. This implementation follows a coherent API with the FIPA protocols included in JADE distribution for an easy use. We also complement the behaviours of corresponding initiators and responders of the protocols with an ontology formed by a collection of concepts, predicates and agent actions that may represent as the ART application domain as any other service-oriented domain. The proposal has been designed to be applied in domains where multi-agent e-commerce solutions are needed. Future work includes the integration of this ontology and protocols in context-aware scenarios such as an airport.",e-commerce,1012
10.1109/citisia53721.2021.9719909,filtered,2021 6th International Conference on Innovative Technology in Intelligent System and Industrial Applications (CITISIA),IEEE,2021-11-26 00:00:00,ieeexplore,a sentiment analysis of amazon review data using machine learning model,https://ieeexplore.ieee.org/document/9719909/,"Nowadays everything is digitalized in the world. In the digitalization world E-commerce take a unique place for people. People are not going anywhere and buy all the thing at home using this E-commerce platform. For selecting the platform generally used the reviews of the people which are already buy from there. The paper proposes a sentiment analysis of the large amazon real dataset based on the counter vectorizer (CV) and term frequency inverse document frequency (TF-IDF) and logistic regressor. Firstly, take the dataset from the amazon E-commerce into JSON format and load the dataset and split the dataset into train test model. Secondly, take out the features using the counter vectorizer and term frequency inverse document frequency (TF-IDF). Finally, logistic regressor (LR) is used and measure the positive and negative sentiment of the review. simulation result represents the model accuracy score, precision, recall, confusion matrix of the implemented approach.",e-commerce,1013
10.1109/iemcon51383.2020.9284818,filtered,"2020 11th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)",IEEE,2020-11-07 00:00:00,ieeexplore,an adapter for ibm streams and apache spark to facilitate multi-level data analytics,https://ieeexplore.ieee.org/document/9284818/,"Data analytics with unsupervised clustering of data streams has provided revolutionary breakthroughs in fields like healthcare, and E-commerce. IBM Streams and Apache Spark are among the most useful and popular data analytics tools that help engineers and researchers extend the abilities to store, analyze, transform, and visualize data for business use. IBM Streams is capable of ingesting, filtering, analyzing, and associating massive volumes of continuous data streams and the Streams Processing Language (SPL) enables coding custom stream graphs to process data and handle real-time events. Apache Spark has unified analytics edge for large scale data processing with high performance for both batch and streaming data. We developed adapters without using third party tools to facilitate data transfer between IBM Streams and Apache Spark to support new and legacy data analytic systems. An example use case would be IBM Streams ingesting and processing realtime data streams, and then passing the data to Spark to train or update machine learning algorithms in real time that can be re-deployed in the IBM Streams data processing pipeline. This paper provides an overview of the structure of the data processing pipeline, describes the implementation details and the principle behind the design.",e-commerce,1014
10.1109/wcse.2009.251,filtered,2009 WRI World Congress on Software Engineering,IEEE,2009-05-21 00:00:00,ieeexplore,an ontology-based system for semantic query over heterogeneous databases,https://ieeexplore.ieee.org/document/5319712/,"With an exponential growth in the amount of information available in diverse domains, the traditional information retrieval (IR) approaches which based on keywords can not meet the semantic needs of users. Semantic query, as an application of semantic Web, has shown significant potential in improving the performance of IR. However, many current semantic query engines focus only on the ontology query, while in real world lots of data is stored in heterogeneous relational databases, not ontologies. So how to use semantic query to search the real information in databases has become an issue. In this paper, we explore a novel semantic search approach based on ontology, which uses SPARQL to query the Global Ontology, and realizes the final search of information in the heterogeneous relational databases through a series of query rewrite. A prototype system named 'OSEQ' has been implemented in light of this approach. And it is illustrated by a case study in the e-commerce domain.",e-commerce,1015
10.1109/icinpro43533.2018.9096796,filtered,2018 Fourteenth International Conference on Information Processing (ICINPRO),IEEE,2018-12-23 00:00:00,ieeexplore,aspect based sentiment analysis on product reviews,https://ieeexplore.ieee.org/document/9096796/,"With the popularity and growing availability of opinion rich sources such as reviews from e-commerce sites, choosing the right product from huge product brands have difficult for the user. In order to enhance the sales and customer satisfaction, most of the sites provide opportunity for the user to write review aspects about the product. These reviews are in text format and increases day by day. It is difficult for the user and manufacturer to understand likes and dislikes of a customer about the product. In this situation sentiment analysis helps the people to analyze the reviews and come to conclusion whether it is good or bad. Sentiment Analysis which also known as opinion mining is one of the subsection in Natural Language processing in which it learns about Sentiment or subjectivity from reviews. The main purpose of the project is to develop a system to extract the reviews from e-commerce site, extract aspect from the reviews and categorize reviews into positive and negative. We have implemented Sentiment analysis with unsupervised machine learning technique like uni-gram Lexicon, bi-gram Lexicon and Supervised technique like Support vector machine (SVM). These techniques are experimented on real time e-commerce dataset. Out of three techniques, SVM outperformed with an accuracy of 84%.",e-commerce,1016
10.1109/rivf48685.2020.9140770,filtered,2020 RIVF International Conference on Computing and Communication Technologies (RIVF),IEEE,2020-10-15 00:00:00,ieeexplore,building filters for vietnamese chatbot responses,https://ieeexplore.ieee.org/document/9140770/,"Automatic Chatbot is a developing trend in the world because of its convenience, efficiency, applicability in many fields such as E-commerce, customer care, and so on. Through Chatbot, we can cut down human resources at the place that Chatbot can replace, increase the convenience for both customer and company. Some good chatbot systems in the world primarily used in English. With Vietnamese, however, the outputs of these models are often not so good due to different structures and grammar. Some output (i.e. chatbot's responses) is incorrect or even misleading to the user. This can cause some serious problems in some legal or financial applications. In this paper, we proposed an approach to reevaluate outputs of general chatbot to reduce incorrect responses. Specifically, we firstly deployed well-known approaches in English chatbot into Vietnamese chatbot. After that, we build filters to assess chatbot responses based on the characteristics of the Vietnamese language. If the response is not suitable, the system will not give back to the user. In this case, the system can require more information to find a better response or at least return announcement instead of an incorrect response. We also developed a web-based application with chatbot integration to evaluate our approach in real life scenario.",e-commerce,1017
10.1109/icmcce51767.2020.00464,filtered,"2020 5th International Conference on Mechanical, Control and Computer Engineering (ICMCCE)",IEEE,2020-12-27 00:00:00,ieeexplore,edge-based fashion detection by transfer learning,https://ieeexplore.ieee.org/document/9421705/,"Fashion detection is a technology of detection, classification, and localization that is applied to fashion attributes. This technology is usually implemented by object detection which is becoming especially popular recently. It can be applied in real life, such as extracting fashion attribute of fashion in a certain picture, which is the first step to achieve fashion retrieval, or used as a detection module of the dress-changing application, which helps consumer transform their garment to allow them to know whether this costume suits them or not and further boosts e-commerce platform's sales volume. Therefore, fashion detection has become a new emerging promising research field that has captured many researchers' attention. Transfer-learning is a recently developed new technology and extensively applied in multiple fields. This paper applies transfer-learning-based object detection on a fashion dataset, which is fashion detection. Specifically, first the operation is packing training and testing data into a specific file format which is “.tfrecord”, then uploading them and its' corresponding bounding box labels on Google Cloud Storage. After configuring the pre-trained model and training parameter, the training process follows with Tensorflow Object Detection which is Google's deep learning open-source API. Finally, under particular conditions, this model achieves good prediction performance on Color-Fashion Dataset. Besides, this work includes employing a light-weight model on the Android platform with Android Studio, which allows visualization of this work.",e-commerce,1018
10.1109/dsdis.2015.75,filtered,2015 IEEE International Conference on Data Science and Data Intensive Systems,IEEE,2015-12-13 00:00:00,ieeexplore,i shopping: intelligent shopping and predicate analysis system using data mining,https://ieeexplore.ieee.org/document/7396474/,"E-commerce has become a major fact that affects the rapid development of the global economy for past few decades. With the growth of the information technology sector, internet and the mobile phone usage have increased greatly, which prompts retailers to attract more and more consumers to their shopping malls. As retailers tend to reach and keep the constant interaction, consumers no longer feel a peculiarity between e-shopping and offline shopping. What they all need is ""Shopping"". We emphasize revolutionary concept of connecting the consumers and the retailer real time and allow them to exchange important information. The ""I Shopping: Intelligent Shopping and Predicate Analysis System"" is a unique software bridge, which comforts to keep the constant connectivity among the retailers and the consumers. The important information gathered from the customer purchase behaviors are supplied to the retailers where they can analyze it to discover new patterns and trends using data mining. The system is able to do effective predicates on day-to-day sales and to improve the business in many different ways, with the aid of data forecasting. Also the newest and essential updates of the price and other retailer details are provided to the customer to ease their shopping troubles. The most suitable items for the customers are suggested by using a sensitively designed ontology. The most cost effective, nearest and a shopping mall which has all the requested shopping items is suggested to the customer to do their shopping using newly designed efficient filtering algorithms. So that the customers are totally released from the nuisance of choosing the most suitable shopping malls.",e-commerce,1019
10.1109/spices.2017.8091310,filtered,"2017 IEEE International Conference on Signal Processing, Informatics, Communication and Energy Systems (SPICES)",IEEE,2017-08-10 00:00:00,ieeexplore,implementation of a self-adaptive real time recommendation system using spark machine learning libraries,https://ieeexplore.ieee.org/document/8091310/,"Real time recommendation systems have become an essential component of e-commerce web applications. With increasing volume and velocity of data handled by these applications, known as the bigdata problem, traditional recommendation systems that analyze data and update models at regular time intervals would not be able to satisfy this requirement. With the evolution of technologies for processing bigdata in real time, it has become fairly easy to implement real time recommendation systems. Stream-computing is a new computing paradigm for handling the velocity attribute of bigdata which makes it possible to develop real time bigdata applications. This paper gives the details of implementation of a real time recommendation system using Apache Spark, a widely used platform for stream computing. This system is implemented for recommending TV channels to viewers in real time. This becomes a challenging task due to continuous changes in the set of available channels and the context dependent preference of viewers. In channel recommendation scenario, characterized by its dynamic nature, volume of data, and tight time constraints, traditional approaches cannot be used. We have implemented a highly scalable TV channel recommendation system optimized for the processing of real-time data streams originating from set-top boxes. The proposed system implements a self-adaptive approach for model building. The system effectively uses distributed processing power of Apache Spark to make recommendations in real time with scalability to meet the real time constraints with increasing load. The Spark Machine Learning Libraries (Spark MLLib) provide several algorithms which were used for developing the proposed recommendation system. The large amount of data in the system is efficiently managed by the data processing method of Lambda Architecture.",e-commerce,1020
10.1109/ecai.2018.8679045,filtered,"2018 10th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",IEEE,2018-06-30 00:00:00,ieeexplore,intelligent conversational agent for online sales,https://ieeexplore.ieee.org/document/8679045/,"A conversational agent is one of emerging technologies that will take place in several of life domains, including e-commerce. The common features of any conversational agent in any domain is its ability to have a dialog with a human user and 24/7 available. Thus, including a conversational agent in an online business will give benefits not only to online retailers, but also e-customers. However, a conversational agent for online sales may have a different feature than a conversational agent in other domain such as health care. The main task of a conversational agent in an online sales domain is to engage and persuade e-commerce customers to buy a product. In a real life, persuasion is considered as an abstract process, which only can be achieved through a series of dialogs. In a virtual world, a conversational agent for online sales is considered intelligent if it is able to demonstrate this feature. The aim of this paper is to propose a method to demonstrate persuasion feature through recommendation and negotiation dialogs to meet e-customers' needs. A prototype of the proposed system has been implemented and preliminary results indicate the proposed method is successful.",e-commerce,1021
10.1109/icrtac.2018.8679129,filtered,2018 International Conference on Recent Trends in Advance Computing (ICRTAC),IEEE,2018-09-11 00:00:00,ieeexplore,tensorflow based website click through rate (ctr) prediction using heat maps,https://ieeexplore.ieee.org/document/8679129/,"Web Heat Maps are used to identify the click patterns and activities visited by the users of the website. Using heat maps, one can make a manual decision based on the user's click activity. This paper proposes a framework using TensorFlow to identify and detect the users click activity in real time. Tensor Flow also suggest or take business decisions predicted through users clicks. This paper models Tensor Flow's machine learning library to take automated decisions like placement of suitable products, placement of advertisements and others based on the highest clicks recorded by the users. The results predict that the future businesses like e-commerce, fashion and retail industry can benefit more if this framework is deployed in such applications.",e-commerce,1022
10.1109/access.2021.3134330,filtered,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,performance evaluation of machine learning methods for credit card fraud detection using smote and adaboost,https://ieeexplore.ieee.org/document/9651991/,"The advance in technologies such as e-commerce and financial technology (FinTech) applications have sparked an increase in the number of online card transactions that occur on a daily basis. As a result, there has been a spike in credit card fraud that affects card issuing companies, merchants, and banks. It is therefore essential to develop mechanisms that ensure the security and integrity of credit card transactions. In this research, we implement a machine learning (ML) based framework for credit card fraud detection using a real world imbalanced datasets that were generated from European credit cardholders. To solve the issue of class imbalance, we re-sampled the dataset using the Synthetic Minority over-sampling TEchnique (SMOTE). This framework was evaluated using the following ML methods: Support Vector Machine (SVM), Logistic Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), Decision Tree (DT), and Extra Tree (ET). These ML algorithms were coupled with the Adaptive Boosting (AdaBoost) technique to increase their quality of classification. The models were evaluated using the accuracy, the recall, the precision, the Matthews Correlation Coefficient (MCC), and the Area Under the Curve (AUC). Moreover, the proposed framework was implemented on a highly skewed synthetic credit card fraud dataset to further validate the results that were obtained in this research. The experimental outcomes demonstrated that using the AdaBoost has a positive impact on the performance of the proposed methods. Further, the results obtained by the boosted models were superior to existing methods.",e-commerce,1023
10.1109/access.2021.3104472,filtered,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,transfer learning strategies for credit card fraud detection,https://ieeexplore.ieee.org/document/9512084/,"Credit card fraud jeopardizes the trust of customers in e-commerce transactions. This led in recent years to major advances in the design of automatic Fraud Detection Systems (FDS) able to detect fraudulent transactions with short reaction time and high precision. Nevertheless, the heterogeneous nature of the fraud behavior makes it difficult to tailor existing systems to different contexts (e.g. new payment systems, different countries and/or population segments). Given the high cost (research, prototype development, and implementation in production) of designing data-driven FDSs, it is crucial for transactional companies to define procedures able to adapt existing pipelines to new challenges. From an AI/machine learning perspective, this is known as the problem of <i>transfer learning</i>. This paper discusses the design and implementation of transfer learning approaches for e-commerce credit card fraud detection and their assessment in a real setting. The case study, based on a six-month dataset (more than 200 million e-commerce transactions) provided by the industrial partner, relates to the transfer of detection models developed for a European country to another country. In particular, we present and discuss 15 transfer learning techniques (ranging from naive baselines to state-of-the-art and new approaches), making a critical and quantitative comparison in terms of precision for different transfer scenarios. Our contributions are twofold: (i) we show that the accuracy of many transfer methods is strongly dependent on the number of labeled samples in the target domain and (ii) we propose an ensemble solution to this problem based on self-supervised and semi-supervised domain adaptation classifiers. The thorough experimental assessment shows that this solution is both highly accurate and hardly sensitive to the number of labeled samples.",e-commerce,1024
10.1109/icodse.2018.8705834,filtered,2018 5th International Conference on Data and Software Engineering (ICoDSE),IEEE,2018-11-08 00:00:00,ieeexplore,microtask crowdsourcing marketplace for social network,https://ieeexplore.ieee.org/document/8705834/,"Crowdsourcing is a powerful way to process and collect data that needs human's logic and perception. Crowdsourcing can take part in many hard to compute problems such as data entry, multimedia transcriptions and many case of artificial intelligence. While this is a powerful approach, crowdsourcing needs a relatively large marketplace to works optimum. In context of Indonesia, we hardly hear about task crowdsourcing even though there are some marketplace like poin-web.co.id. In this paper, we propose a social network for crowdsourcing marketplace to penetrate the market. People tends to waste most of their time on social network and game rather than other mobile application in case of mobile usage. The idea is to make crowdsourcing as a filler while people are using social networks like waiting for a chat or scrolling the timeline. And by applying microtask as the task, people will not have much burden on doing the task. On the other hand, he/she will get additional income. We implemented LINE as a basis of our social network marketplace. LINE does provide the most interactive way of provide message. Additionally, the user of LINE in Indonesia is growing in a fast pace. We conduct an experiment focused on worker's coverage and ease of use. By using usability testing as a basis of ease of use evaluation, we received good feedbacks as 90.9% of the users feel easier to answer through LINE and is excited to use the platform in case it goes with real money.",e-commerce,1025
10.1109/iscon52037.2021.9702502,filtered,2021 5th International Conference on Information Systems and Computer Networks (ISCON),IEEE,2021-10-23 00:00:00,ieeexplore,airlines based twitter sentiment analysis using deep learning,https://ieeexplore.ieee.org/document/9702502/,"The airlines industry has been a competitive marketplace that has grown rapidly over the last few decades. Mostly, customers like family members, businessman, sportsman and youngsters are traveling through Airlines. If people are participating, their feedback is extremely important. Customer direct feedback may be favorable or negative, but understanding their Tweets is critical for improvement. This serves as a conduit for open-source inactive communication between promoters and customers coincidentally exercising their time and duties on the same platform for a variety of reasons. Sentiment Analysis on the social networking sites like Twitter or Facebook that bridge the gap between information and real time feedback, has become an amazing method for finding out about a user's feelings and has a wide scope of utilizations. It focuses on polarity (positive, negative, and neutral), sentiments and emotions (urgent, not urgent), and even intents (interested not interested). In this paper, the idea is to analyses the tweets emerging from social site such as Twitter, necessarily focused around the airline industry, its customers and employees, current as well as imminent. So ultimately, the objective is to deploy the deep learning algorithms on dataset of 14641 total tweets regarding U.S airlines, collected from Kaggle repository. The similar data set is utilized for both training and testing because there is more possibility for errors, which raises the likelihood of inaccurate predictions. Therefore, train_test_split function of scikit-learn python library has been used. It allows to breaking a dataset with ease while pursuing an ideal model. To prevent over fitting associated with the co-adaptation of feature detectors, the dropout learning algorithm has been called on to remarkable results.",e-commerce,1026
10.1109/eidwt.2011.15,filtered,2011 International Conference on Emerging Intelligent Data and Web Technologies,IEEE,2011-09-09 00:00:00,ieeexplore,a relational recommender system based on domain ontology,https://ieeexplore.ieee.org/document/6076418/,"Product recommendation on electronic commerce Web sites becomes more important with the widespread use of Internet-based shopping. Collaborative filtering and content based filtering methods have been commonly used for this task by electronic commerce Web sites. These methods have several shortcomings, such as cold start problem, biased ratings problem or inaccurate recommendations. In order to produce effective and accurate recommendations, recent approaches utilize the semantic properties of data by integrating the domain ontology into the recommendation process. In these studies, the domain ontology covering only the types and properties of the product to be recommended is considered where the relational nature of the product data is omitted. However, the domain ontology of the features related to the product may also provide useful information during recommendation process. In this study, we focus on integrating the domain ontology of relational data into the recommendation process. We design a framework for an easy implementation of a recommendation system on relational data. Using this framework, we implement as a case study a recommendation model that recommends books to the users. We evaluated the performance of our model on real data obtained from a Turkish Internet book store. Our experimental results show that our proposed method can be effectively used for recommending items in relational data.",e-commerce,1027
10.1109/icssit48917.2020.9214285,filtered,2020 Third International Conference on Smart Systems and Inventive Technology (ICSSIT),IEEE,2020-08-22 00:00:00,ieeexplore,evaluation of information retrieval performance metrics using real estate ontology,https://ieeexplore.ieee.org/document/9214285/,"With an ever-increasing data over the internet, efficient information retrieval of data for its users has always been on stake. The issue is deeper when it comes to e-governance based real estate scenario, where there is a lack of interoperability found in buying and selling of property formats and loads of miscellaneous legal terminology that fetches inappropriate legal documentation for the users. Hence in today's Semantic web scenario, a Real estate ontology has been proposed under the Real Estate Information Retrieval Model and the model is evaluated by applying various information retrieval measures. This paper shows the implementation and analysis of various popular information retrieval metrics, be it a set retrieval or rank retrieval that can be used for performance evaluation of web retrieval of data. The proposed system depicts better results in almost all the metrics as compared to the initial user query set.",e-commerce,1028
10.1109/icoict.2017.8074710,filtered,2017 5th International Conference on Information and Communication Technology (ICoIC7),IEEE,2017-05-19 00:00:00,ieeexplore,indonesia infrastructure and consumer stock portfolio prediction using artificial neural network backpropagation,https://ieeexplore.ieee.org/document/8074710/,"Artificial Neural Network (ANN) method is increasingly popular to build predictive model that generated small error prediction. To have a good model, ANN needs large dataset as an input. ANN backpropagation is a gradient decrease method to minimize the output error squared. Stock price movements are suitable with ANN requirement: it is a large data set because stock price is recorded up to every seconds, usually called high frequency data. The implementation of stock price prediction using ANN approach is quite new. The predictive model help investor in building stock portfolio and their decision making process. Buying some stocks in portfolio decrease diversified risk and increases the chance of higher return. In this paper, we show how to generate prediction model using artificial neural network backpropagation of stock price and forming portfolio with predicted price that bring prediction of the portfolio with the smallest error. The data set we use is historical stock price data from ten different company stocks of infrastructure and consumer sector Indonesia Stock Exchage. The results is for lower risk condition, ANN predictive model gives higher expected return than the return from real condition, while for higher risk, the return from the real condition is higher than the ANN predictive model.",e-commerce,1029
10.1109/compsac51774.2021.00034,filtered,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE,2021-07-16 00:00:00,ieeexplore,intelligent probabilistic forecasts of day-ahead electricity prices in a highly volatile power market,https://ieeexplore.ieee.org/document/9529702/,"Electricity price forecasting plays an important role in decision making on bidding strategies of selling and buying electricity. This paper computes one day-ahead (DA) quantile forecasts of electricity prices in a highly volatile market by applying regression models to a pool of point forecasts. Three data-driven forecasting methods are implemented to generate DA point forecasts of the Ontario market’s electricity prices. In order to generate the three sets of point forecasts, we use: i) the triple exponential smoothing (TES) method, ii) a neural network (NN) that combines layers of Convolutional neurons and gradient recurrent units (GRU), iii) an extreme gradient boosted (XGB) non-linear regression approach. Performance of the three models compared against a benchmark which considers the forecast of electricity prices as the average price of the same hour and day during the last four weeks. The TES method decreases the mean absolute error (MAE) of the benchmark model from 10.29 to 9.42. The Convolutional GRU (ConvGRU) model and XGB regression also reduce the MAE to 8.20 and 7.06, respectively. Finally, quantile regression averaging (QRA) is applied to the pool of point forecasts obtained by TES, ConvGRU, and XGB methods to compute DA quantile forecasts of electricity prices. Moreover, the QRA method is further developed in this work by employing gradient boosting non-linear regression (GBR). Our analysis using real data reveals that the GBR method provides more reliable quantiles as well as tighter prediction intervals with smaller forecasting errors than QRA.",e-commerce,1030
10.1109/icter51097.2020.9325501,filtered,2020 20th International Conference on Advances in ICT for Emerging Regions (ICTer),IEEE,2020-11-07 00:00:00,ieeexplore,mixed reality supermarket: a modern approach into day - to - day grocery shopping,https://ieeexplore.ieee.org/document/9325501/,"In the modern world where there are massive trends in development and implementation of new technologies, combination of Virtual Reality and Augmented Reality is one which has key potential in an everyday developing world. The main concept behind Virtual Reality is simply immersing the user in a virtual environment at the comfort of their own place. This is done by creating a computer-generated 3D environment with hand gestured navigation system combined with concepts of voice recognition, image processing and machine learning that explores intense human interactions. As we are in the 21st century, where technological transformations are most certainly creating blurry lines between fiction and reality, more and more people have the need to fulfill their daily requirements easily without wasting their valuable time. Buying day to day needs from a supermarket is one of the main activities that each one of us struggle to go through during the day. Targeting the above simple daily activities, we are making an effort to apply VR Technology to this area through this research and thus trying to provide a rather new technological experience for purchasing items from a supermarket. This can be beneficial to the consumers to minimize their valuable time wasted, and also, they will be able to get the real experience of shopping while getting exposure to marketing.",e-commerce,1031
10.1109/bigdataservice49289.2020.00034,filtered,2020 IEEE Sixth International Conference on Big Data Computing Service and Applications (BigDataService),IEEE,2020-08-06 00:00:00,ieeexplore,zenden - a personalized house searching application,https://ieeexplore.ieee.org/document/9179601/,"A method directly linking buyers and sellers in the housing market would benefit both parties in both the renting and purchasing domains. The real estate mobile applications available today, such as Zillow and Redfin, are highly dependent on filter based searches (location, price, bedrooms, etc) and contain many screens that require zooming in and out of map-based interfaces. Furthermore, the student market and lower-income demographics that are more likely renting a house instead of buying are usually not the focus of these applications. Described in this paper is a novel mobile application that will change the way people in the renting market, e.g., university students, find lodging. The implementation contains a streamlined swipe-based user interface backed by a user-house recommender system to manage content. Deep learning techniques are used in building the recommender system that recommends houses to users based on their view history. For image classification, we build convolutional neural networks (CNN) for analyzing house images. The goal is to create a personalized, easy to use application that will reduce the effort and time required for people in the renting market to find housing.",e-commerce,1032
10.1109/jiot.2016.2556006,filtered,IEEE Internet of Things Journal,IEEE,2017-04-01 00:00:00,ieeexplore,optimized day-ahead pricing with renewable energy demand-side management for smart grids,https://ieeexplore.ieee.org/document/7457198/,"Internet of Things (IoT) has recently emerged as an enabling technology for context-aware and interconnected “smart things.” Those smart things along with advanced power engineering and wireless communication technologies have realized the possibility of next generation electrical grid, smart grid, which allows users to deploy smart meters, monitoring their electric condition in real time. At the same time, increased environmental consciousness is driving electric companies to replace traditional generators with renewable energy sources which are already productive in user's homes. One of the most incentive ways is for electric companies to institute electricity buying-back schemes to encourage end users to generate more renewable energy. Different from the previous works, we consider renewable energy buying-back schemes with dynamic pricing to achieve the goal of energy efficiency for smart grids. We formulate the dynamic pricing problem as a convex optimization dual problem and propose a day-ahead time-dependent pricing scheme in a distributed manner which provides increased user privacy. The proposed framework seeks to achieve maximum benefits for both users and electric companies. To our best knowledge, this is one of the first attempts to tackle the time-dependent problem for smart grids with consideration of environmental benefits of renewable energy. Numerical results show that our proposed framework can significantly reduce peak time loading and efficiently balance system energy distribution.",e-commerce,1033
http://arxiv.org/abs/2012.10853v1,filtered,arxiv,arxiv,2020-12-20 00:00:00,arxiv,etree: learning tree-structured embeddings,http://arxiv.org/abs/2012.10853v1,"Matrix factorization (MF) plays an important role in a wide range of machine
learning and data mining models. MF is commonly used to obtain item embeddings
and feature representations due to its ability to capture correlations and
higher-order statistical dependencies across dimensions. In many applications,
the categories of items exhibit a hierarchical tree structure. For instance,
human diseases can be divided into coarse categories, e.g., bacterial, and
viral. These categories can be further divided into finer categories, e.g.,
viral infections can be respiratory, gastrointestinal, and exanthematous viral
diseases. In e-commerce, products, movies, books, etc., are grouped into
hierarchical categories, e.g., clothing items are divided by gender, then by
type (formal, casual, etc.). While the tree structure and the categories of the
different items may be known in some applications, they have to be learned
together with the embeddings in many others. In this work, we propose eTREE, a
model that incorporates the (usually ignored) tree structure to enhance the
quality of the embeddings. We leverage the special uniqueness properties of
Nonnegative MF (NMF) to prove identifiability of eTREE. The proposed model not
only exploits the tree structure prior, but also learns the hierarchical
clustering in an unsupervised data-driven fashion. We derive an efficient
algorithmic solution and a scalable implementation of eTREE that exploits
parallel computing, computation caching, and warm start strategies. We showcase
the effectiveness of eTREE on real data from various application domains:
healthcare, recommender systems, and education. We also demonstrate the
meaningfulness of the tree obtained from eTREE by means of domain experts
interpretation.",e-commerce,1034
http://arxiv.org/abs/1805.05491v1,filtered,arxiv,arxiv,2018-05-14 00:00:00,arxiv,"crowdbreaks: tracking health trends using public social media data and
  crowdsourcing",http://arxiv.org/abs/1805.05491v1,"In the past decade, tracking health trends using social media data has shown
great promise, due to a powerful combination of massive adoption of social
media around the world, and increasingly potent hardware and software that
enables us to work with these new big data streams. At the same time, many
challenging problems have been identified. First, there is often a mismatch
between how rapidly online data can change, and how rapidly algorithms are
updated, which means that there is limited reusability for algorithms trained
on past data as their performance decreases over time. Second, much of the work
is focusing on specific issues during a specific past period in time, even
though public health institutions would need flexible tools to assess multiple
evolving situations in real time. Third, most tools providing such capabilities
are proprietary systems with little algorithmic or data transparency, and thus
little buy-in from the global public health and research community. Here, we
introduce Crowdbreaks, an open platform which allows tracking of health trends
by making use of continuous crowdsourced labelling of public social media
content. The system is built in a way which automatizes the typical workflow
from data collection, filtering, labelling and training of machine learning
classifiers and therefore can greatly accelerate the research process in the
public health domain. This work introduces the technical aspects of the
platform and explores its future use cases.",e-commerce,1035
http://arxiv.org/abs/1709.08920v1,filtered,arxiv,arxiv,2017-09-26 00:00:00,arxiv,"scarff: a scalable framework for streaming credit card fraud detection
  with spark",http://arxiv.org/abs/1709.08920v1,"The expansion of the electronic commerce, together with an increasing
confidence of customers in electronic payments, makes of fraud detection a
critical factor. Detecting frauds in (nearly) real time setting demands the
design and the implementation of scalable learning techniques able to ingest
and analyse massive amounts of streaming data. Recent advances in analytics and
the availability of open source solutions for Big Data storage and processing
open new perspectives to the fraud detection field. In this paper we present a
SCAlable Real-time Fraud Finder (SCARFF) which integrates Big Data tools
(Kafka, Spark and Cassandra) with a machine learning approach which deals with
imbalance, nonstationarity and feedback latency. Experimental results on a
massive dataset of real credit card transactions show that this framework is
scalable, efficient and accurate over a big stream of transactions.",e-commerce,1036
http://arxiv.org/abs/1702.03488v2,filtered,arxiv,arxiv,2017-02-12 00:00:00,arxiv,octopus: a framework for cost-quality-time optimization in crowdsourcing,http://arxiv.org/abs/1702.03488v2,"We present Octopus, an AI agent to jointly balance three conflicting task
objectives on a micro-crowdsourcing marketplace - the quality of work, total
cost incurred, and time to completion. Previous control agents have mostly
focused on cost-quality, or cost-time tradeoffs, but not on directly
controlling all three in concert. A naive formulation of three-objective
optimization is intractable; Octopus takes a hierarchical POMDP approach, with
three different components responsible for setting the pay per task, selecting
the next task, and controlling task-level quality. We demonstrate that Octopus
significantly outperforms existing state-of-the-art approaches on real
experiments. We also deploy Octopus on Amazon Mechanical Turk, showing its
ability to manage tasks in a real-world dynamic setting.",e-commerce,1037
10.1016/j.procs.2022.01.187,filtered,Procedia Computer Science,scopus,2021-01-01,sciencedirect,lost in translation: what linguistic measurements best measure text quality of online listings,https://api.elsevier.com/content/abstract/scopus_id/85124965543,"Ecommerce websites are filled with international sellers. Product descriptions on these sites are often written in English by non-native speakers. Linguistic imperfections in these descriptions confuse consumers, which may further attenuate their purchase intentions. How descriptive quality/efficacy can be defined and then improved shall be of great interest to all sellers and their consumers. In this research, we attempt to evaluate online product description quality using lexical measurements from linguistics studies. Linguistics measurements of writing quality were mostly developed in pure academic settings. We test and analyze these measurements’ applicability in defining and contrasting business description quality using Amazon.com data. Modern classification techniques in the artificial intelligence and machine learning field are deployed in identifying measurement applicability and assessing computational efficiency. Our findings enable automatic identification of descriptive efficacy through artificial intelligence methods on real ecommerce text data.",e-commerce,1038
10.1016/j.ins.2018.11.028,filtered,Information Sciences,scopus,2019-04-01,sciencedirect,machine learning based privacy-preserving fair data trading in big data market,https://api.elsevier.com/content/abstract/scopus_id/85056879362,"In the era of big data, the produced and collected data explode due to the emerging technologies and applications that pervade everywhere in our daily lives, including internet of things applications such as smart home, smart city, smart grid, e-commerce applications and social network. Big data market can carry out efficient data trading, which provides a way to share data and further enhances the utility of data. However, to realize effective data trading in big data market, several challenges need to be resolved. The first one is to verify the data availability for a data consumer. The second is privacy of a data provider who is unwilling to reveal his real identity to the data consumer. The third is the payment fairness between a data provider and a data consumer with atomic exchange. In this paper, we address these challenges by proposing a new blockchain-based fair data trading protocol in big data market. The proposed protocol integrates ring signature, double-authentication-preventing signature and similarity learning to guarantee the availability of trading data, privacy of data providers and fairness between data providers and data consumers. We show the proposed protocol achieves the desirable security properties that a secure data trading protocol should have. The implementation results with Solidity smart contract demonstrate the validity of the proposed blockchain-based fair data trading protocol.",e-commerce,1039
10.1016/j.inffus.2017.09.005,filtered,Information Fusion,scopus,2018-05-01,sciencedirect,scarff: a scalable framework for streaming credit card fraud detection with spark,https://api.elsevier.com/content/abstract/scopus_id/85029580394,"The expansion of the electronic commerce, together with an increasing confidence of customers in electronic payments, makes of fraud detection a critical factor. Detecting frauds in (nearly) real time setting demands the design and the implementation of scalable learning techniques able to ingest and analyse massive amounts of streaming data. Recent advances in analytics and the availability of open source solutions for Big Data storage and processing open new perspectives to the fraud detection field. In this paper we present a Scalable Real-time Fraud Finder (SCARFF) which integrates Big Data tools (Kafka, Spark and Cassandra) with a machine learning approach which deals with imbalance, nonstationarity and feedback latency. Experimental results on a massive dataset of real credit card transactions show that this framework is scalable, efficient and accurate over a big stream of transactions.",e-commerce,1040
10.1016/j.chb.2017.02.064,filtered,Computers in Human Behavior,scopus,2017-12-01,sciencedirect,shopping with a robotic companion,https://api.elsevier.com/content/abstract/scopus_id/85015734591,"In this paper, we present a robotic shopping assistant, designed with a cognitive architecture, grounded in machine learning systems, in order to study how the human-robot interaction (HRI) is changing the shopping behavior in smart technological stores. In the software environment of the NAO robot, connected to the Internet with cloud services, we designed a social-like interaction where the robot carries out actions with the customer. In particular, we focused our design on two main skills the robot has to learn: the first is the ability to acquire social input communicated by relevant clues that humans provide about their emotional state (emotions, emotional speech), or collected in the Social Media (such as, information on the customer's tastes, cultural background, etc.). The second is the skill to express in turn its own emotional state, so that it can affect the customer buying decision, refining in the user the sense of interacting with a human-like companion. By combining social robotics and machine learning systems the potential of robotics to assist people in real life situations will increase, providing a gentle customers' acceptance of advanced technologies.",e-commerce,1041
10.11606/d.55.2019.tde-21082019-165653,filtered,core,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",2019-08-21 00:00:00,core,analysis of the network of products bought together in electronic commerce,,"Este trabalho aborda as áreas de teoria dos grafos, sistemas de recomendação, e comércio eletrônico, que já foram tema de diversas publicações ao longo das últimas décadas. Entretanto, o estudo da importância da utilização de medidas de centralidade de redes como atributos preditivos de modelos de aprendizado de máquina é um assunto que ainda não foi explorado pela literatura. Neste trabalho, além de relatarmos resultados que sugerem que essas medidas de centralidade podem aumentar a precisão dos modelos preditivos, também apresentamos os principais conceitos teóricos de redes complexas, como tipos de redes, caracterização, métricas de distância, além de propriedades de redes reais. Também apresentamos as ferramentas e metodologia utilizadas para o desenvolvimento de um webcrawler próprio, software necessário para a construção da rede de produtos comprados em conjunto no comércio eletrônico. Modelos de aprendizado de máquina foram treinados utilizando a base de produtos obtida pelo webcrawler, possibilitando a obtenção de modelos preditivos de estimativa de preços de produtos, e de previsão de probabilidade de ligação entre produtos da rede. A performance dos modelos preditivos obtidos são apresentadas.This work approaches areas such as graph theory, recommendation systems, and electronic commerce, which have been chosen as topics for several publications over the last decades. Although, studying the importance of using network centrality measures as predictive features within machine learning models is a topic which was not yet explored on literature. In this work, besides reporting results which suggest that those centrality measures can increase the precision of predictive models, we also present the main theoretical concepts of complex networks, such as network types, characterization, distance metrics, besides some properties of real networks. We also present the tools and methodology used on the development of our own webcrawler, a software required for the generation of the network of products bought together in the electronic commerce. Machine learning models were trained using the product database obtained using the webcrawler, allowing the achievement of predictive models for product price estimation, and also link prediction between products of the network. The performance of the predictive models are also presented",e-commerce,1042
10.3390/su10093142,filtered,core,'MDPI AG',2018-09-03 00:00:00,core,"a systematic review of smart real estate technology: drivers of, and barriers to, the use of digital disruptive technologies and online platforms",http://www.mdpi.com/2071-1050/10/9/3142,"Real estate needs to improve its adoption of disruptive technologies to move from traditional to smart real estate (SRE). This study reviews the adoption of disruptive technologies in real estate. It covers the applications of nine such technologies, hereby referred to as the Big9. These are: drones, the internet of things (IoT), clouds, software as a service (SaaS), big data, 3D scanning, wearable technologies, virtual and augmented realities (VR and AR), and artificial intelligence (AI) and robotics. The Big9 are examined in terms of their application to real estate and how they can furnish consumers with the kind of information that can avert regrets. The review is based on 213 published articles. The compiled results show the state of each technology’s practice and usage in real estate. This review also surveys dissemination mechanisms, including smartphone technology, websites and social media-based online platforms, as well as the core components of SRE: sustainability, innovative technology and user centredness. It identifies four key real estate stakeholders—consumers, agents and associations, government and regulatory authorities, and complementary industries—and their needs, such as buying or selling property, profits, taxes, business and/or other factors. Interactions between these stakeholders are highlighted, and the specific needs that various technologies address are tabulated in the form of a what, who and how analysis to highlight the impact that the technologies have on key stakeholders. Finally, stakeholder needs as identified in the previous steps are matched theoretically with six extensions of the traditionally accepted technology adoption model (TAM), paving the way for a smoother transition to technology-based benefits for consumers. The findings pertinent to the Big9 technologies in the form of opportunities, potential losses and exploitation levels (OPLEL) analyses highlight the potential utilisation of each technology for addressing consumers’ needs and minimizing their regrets. Additionally, the tabulated findings in the form of what, how and who links the Big9 technologies to core consumers’ needs and provides a list of resources needed to ensure proper information dissemination to the stakeholders. Such high-quality information can bridge the gap between real estate consumers and other stakeholders and raise the state of the industry to a level where its consumers have fewer or no regrets. The study, being the first to explore real estate technologies, is limited by the number of research publications on the SRE technologies that has been compensated through incorporation of online reports",e-commerce,1043
10.1016/s1571-0661(05)80598-5,filtered,core,Published by Elsevier B.V.,2002-10-31 00:00:00,core,preface ,,"AbstractOver recent years, the notion of agency has claimed a major role in defining the trends of modern research. Influencing a broad spectrum of disciplines such as Sociology, Psychology, among others, the agent paradigm virtually invaded every sub-field of Computer Science, not least because of the Internet and Robotics.Multi-agent Systems (MAS) are communities of problem-solving entities that can perceive and act upon their environments to achieve their individual goals as well as joint goals. The work on such systems integrates many technologies and concepts in artificial intelligence and other areas of computing. There is a full spectrum of MAS applications that have been and are being developed; from search engines to educational aids to electronic commerce and trade.Although commonly implemented by means of imperative languages, mainly for reasons of efficiency, the agent concept has recently increased its influence in the research and development of computational logic based systems.Computational Logic, by virtue of its nature both in substance and method, provides a well-defined, general, and rigorous framework for systematically studying computation, be it syntax, semantics, procedures, or attending implementations, environments, tools, and standards. Computational Logic approaches problems, and provides solutions, at a sufficient level of abstraction so that they generalise from problem domain to problem domain, afforded by the nature of its very foundation in logic, both in substance and method, which constitutes one of its major assets.The purpose of this workshop is to discuss techniques, based on computational logic, for representing, programming and reasoning about multi-agent systems in a formal way. This is clearly a major challenge for computational logic, to deal with real world issues and applications.The first workshop in this series took place in Las Cruces, New Mexico, USA, in 1999, under the designation Multi-Agent Systems in Logic Programming (MASLP'99), and affiliated with ICLP'99. In the following year, the name of the workshop changed to Computational Logic in Multi-Agent Systems (CLIMA'00), taking place in London, UK, and affiliated with CL'2000. The subsequent edition, CLIMA'01, took place in Paphos, Cyprus, affiliated with ICLP'01. The present edition, CLIMA'02, takes place in Copenhagen, Denmark, on August the1st of 2002, and is affiliated with ICLP'02 and part of FLOC'02.We would like to thank the authors of the submitted papers, the members of the program committee and the additional reviewers for their contribution to both the meeting and this volume. We would also like to thank Michael Mislove for his help with the editing of the proceedings.Programme Committee
				Jürgen Dix (The University of Manchester, UK)Thomas Eiter (Vienna University of Technology, Austria)Klaus Fischer (DFKI, Germany)Michael Fisher (University of Liverpool, UK)James Harland (Royal Melbourne Institute of Technology, Australia)Wiebe van der Hoek (Utrecht University, The Netherlands)Katsumi Inoue (Kobe University, Japan)João Alexandre Leite (New University of Lisbon, Portugal)Luís Moniz Pereira (New University of Lisbon, Portugal)Ken Satoh (National Institute of Informatics, Japan)V. S. Subrahmanian (University of Maryland, USA)Francesca Toni (Imperial College, UK)Paolo Torroni (University of Bologna, Italy)Additional Reviewers
				José AlferesKoji IwanumaAndrea SchalkTadashi AraragiGerhard LakemeyerMichael SchroederAlastair BurtWei LiuKenji TaguchiAnna CiampoliniSeng LokeHans TompitsPierangelo Dell'AcquaHidetomo NabeshimaMirosaw TruszczynskiUwe EglyNaoyuki NideMathijs de WeerdtMichael FinkInna PivkinnaMichael WinikoffChiara GhidiniFabrizio RiguzziCees WitteveenHisashi HayashiChiaki SakamaFor this edition of CLIMA, we have received 25 submissions of which 12 were selected for presentation, after a careful review process where each paper was independently reviewed by three members of the Program Committee.The workshop consisted of five sessions: four devoted to the oral presentation of the selected papers and subsequent discussion; and one devoted to a panel discussion, Paolo Torroni being the invited moderator. There follows a brief overview of the workshop.Session 1 - Agents: Arguments and UpdatesSchroeder and Schweimeier present a framework based on logic programming with 3-valued multi-agent argumentation and fuzzy unification, for knowledge representation and reasoning in agents, to accommodate arguments for negotiating agents when agent communication is subject to uncertainty.Leite et al. extend the language LUPS introducing MLUPS, an update command language designed for specifying the flexible evolution of hierarchically related groups of agents, based on logic programming, thus assigning them declarative semantics.Kakas and Moraïtis present a modular argumentation framework for modelling agent deliberation, where object level arguments can be made conditional on agents' roles and the priority relation amongst such roles can, in turn, be made conditional on contents, on top of which a simple form of abduction allows dealing with incomplete knowledge.Session 2 - Logics for AgentsToyama et al. introduce a translation of multi-agent autoepistemic logic (MAEL), a logic for multi-agent systems based on Moore's autoepistemic logic, into logic programming, and show the correspondence between MAEL extensions and the stable models of the corresponding logic program.Dell'Acqua et al. extend their previous work on abductive logic programming based multi-agent systems, in which agents can update themselves and each other, eliminate contradictory update rules, abduce hypotheses to explain observations, and use them to generate actions, with asynchronous based communication through the use of buffers.Harland and Winikoff discuss the formalisation and implementation issues of BDI-type agents, using a Linear Logic based calculus that allows a mixture of forward- and backward-chaining techniques.Session 3 - BDI Agent SystemsBordini and Moreira investigate how far the Asymmetry Thesis Principles formulated by Rao and Georgeff are actually met by the abstract agent specification language AgentSpeak(L), hence contributing to the reconciliationBetween practice and theory of BDI-based agents.Araragi et al. formalise and propose a method to solve a verification problem that arises in implementing a commitment strategy for the BDI architecture, namely the verification of the suitability and/or feasibility of the intentions of an agent.Nide et al. extend with mental state consistency features their previously presented deduction system for CTL-based propositional BDI Logics using sequent calculus, as a step towards the use of the expressive power of BDI Logics as executable specification languages of rational agents.Session 4 - Agents: Speculative Computation and IntrospectionHayashi et al. address the issue of integrating speculative computation and action execution through logic programming, namely by devising a method for plan modification when speculative computation fails or actions are executed.Iwanuma and Inoue refine the first-order consequence-finding procedure based on clausal tableaux SOL, with conditional answer computation and skip-preference, to formalise speculative computation in a master-slave multi-agent system.Bolander investigates on finding consistent classes of formulas under the syntactical treatment of knowledge and belief, identifying three maximal sets of introspective beliefs that strong introspective agents can consistently maintain so as to avoid the paradoxes of self-reference.Session 5 - Panel DiscussionTorroni moderates a panel discussion entitled ""Logics and Multi-agents: towards a new symbolic model of cognition"".This volume constitutes the proceedings of CLIMA'02.September 2002, Jürgen Dix, João Alexandre Leite and Ken Satoh (Guest Editors",e-commerce,1044
10.1145/1363686.1363695,filtered,core,,2015-11-26 00:00:00,core,electricity market simulation: multiagent system approach,,"This paper suggests a multiagent system (MAS) approach for market simulation. This is achieved through analysis, modeling, implementation and simulation of artificial markets populated by software agents that represent economic self interested agents. Software agents are the constructs of a complex system, an artificial market that model a real existing market or an outline of a market design. The interest in simulating a market is multiple: exploiting existing market rules, searching for market design flaws and loopholes, and supporting decision making during a market mechanism design process. The main aim of the suggested approach is to analyze the behavior that emerges from the interaction of self interested agents acting in an artificial market. AEMAS (Artificial Economy MultiAgent System), a multiagent system architecture inspired by the Market Oriented Programming (MOP) approach is defined. In different economical sectors, e.g. energy markets, there is no consensus about which structures lead to social welfare maximization outcomes. An approach to find adequate architectures allows different market structure instances to be created and simulated, to ease the design and analysis of alternative structures. These alternatives can then be compared and potential design flaws eventually risen by simulation identified. Taking the electricity market as an example, two instances of the proposed architecture are presented, corresponding to the centralized dispatch arrangement common to non restructured markets, and the auction based pool, common to restructured markets. Copyright 2008 ACM.3438Al-Agtash, S., Evolutionary negotiation strategies in emerging electricity markets (2004) Lecture Notes in Artificial Intelligence, 3070, pp. 1099-1104. , ICAISC 2004Batten, D.F., (2000) Discovering Artificial Economics: How Agents Learn and Economies Evolve, , Westview Press, Boulder, ColoradoBower, J., Bunn, D.W., Experimental analysis of the efficiency of uniform-price versus discriminatory auctions in the England and Wales electricity market (2001) Journal of Economic Dynamics and Control, 25 (3-4), pp. 561-592. , MarBunn, D.W., Oliveira, F.S., Agent-based simulation - An application to the New Electricity Trading Arrangements of England and Wales (2001) IEEE Transactions on Evolutionary Computing, 5 (5), pp. 493-503. , OctF. S. Carvalho and C. D. N. Vinhal. Temporal difference methods applied to thermoelectric energy markets: A distributed multi-agents approach. In XV Congresso Brasileiro de Automática (CBA 2004), Gramado, RS, Sept.21-24 2004(2000) FIPA 2000, , http://www.fipa.org/repository/fipa2000.html, Foundation for Intelligent Physical AgentsGenoud, C., Regulation as a game: The role of independent regulatory agencies in the regulatory process (2003) CARR Risk and Regulation Research Student Conference, , London, UK, Sept, London School of Economics and Political ScienceM. Griss and R. Letsinger. Games at work - Agent mediated e-commerce simulation. In Autonomous Agents 2000, Barcelona, Spain, June 2000. HP Laboratories Technical Report HPL-2000-52Harp, S.A., Brignone, S., Wollenberg, B.F., Samad, T., SEPIA: A simulator for electric power industry agents (2000) IEEE Control Systems Magazine, 20 (4), pp. 53-69. , AugLima, W., Freitas, E.N.A., A multi agent based simulator for Brazilian wholesale electricity energy market (2006) Lecture Notes in Computer Science, 4140, pp. 68-77. , X Ibero-American Artificial Intelligence Conference, XVIII Brazilian Artificial Intelligence Symposium, IBERAMIA '2006/SBIA '2006, of, Ribeirão Preto, SP, Oct, SpringerMonclar, F.-R., Quatrain, R., Simulation of electricity markets: A multi-agent approach (2001) International Conference on Intelligent System Application to Power Systems, pp. 207-212. , Budapest, Hungary, June, IEEE Power Engineering SocietyPraça, I., Ramos, C., Vale, Z., Cordeiro, M., MASCEM: A multiagent system that simulates competitive electricity markets (2003) IEEE Intelligent Systems, 18 (6), pp. 54-60. , Nov-DecSimon, H.A., From substantive to procedural rationality (1979) Philosophy and Economic Theory, pp. 65-86. , F. Hahn and M. Hollis, editors, Oxford University PressSycara, K., Decker, K., Williamson, M., Matchmaking and brokering (1996) Proceedings of the Second International Conference on Multi-Agent SystemsTesfatsion, L., Agent-based computational economics: Growing economies from the bottom up (2002) Artificial Life, 8 (1), pp. 55-82Handbook of Computational Economics: Agent-Based Computational Economics (2006) Handbooks in Economics, 2. , L. Tesfatsion and K. L. Judd, editors, of, North HollandVeit, D., Matchmaking in electronic markets: An agent-based approach towards matchmaking in electronic negotiations (2003) Lecture Notes in Artificial Intelligence, 2882. , J. G. Carbonell and J. Siekmann, editors, SpringerWalter, I., (2006) Sistemas Multiagentes em Mercados de Energia Elétrica, , PhD thesis, Faculdade de Engenharia Elétrica e de Computação, Universidade Estadual de Campinas, DecWalter, I., Gomide, F., Simulação de mercados de energia elétrica: Abordagem multi-agentes (2005) VII SBAI Simpósio Brasileiro de Automação Inteligente, , O. R. Saavedra et al, editors, São Luís, MA, SeptI. Walter and F. Gomide. Design of coordination strategies in multiagent systems via genetic fuzzy systems. Soft Computing, 10(10):903-915, Aug. 2006. Special Issue: New Trends in the Design of Fuzzy SystemsWalter, I., Gomide, F., Genetic fuzzy systems to evolve coordination strategies in multiagent systems (2007) International Journal of Intelligent Systems, 22 (9), pp. 971-991. , Special Issue on Genetic Fuzzy SystemsWellman, M.P., A market-oriented programming environment and its application to distributed multicommodity flow problems (1993) Journal of Artificial Intelligence Research, 1 (1), pp. 1-23. , AugWooldridge, M., Jennings, N.R., Intelligent agents: Theory and practice (1995) The Knowledge Engineering Review, 10 (2), pp. 115-15",e-commerce,1045
10.1007/978-3-642-28499-1_1,filtered,core,,2012-05-07 00:00:00,core,co-learning segmentation in marketplaces,,"Abstract. We present the problem of automatic co-niching in which potential suppliers of some product or service need to determine which offers to make to the marketplace at the same time as potential buyers need to determine which offers (if any) to purchase. Because both groups typically face incomplete or uncertain information needed for these decisions, participants in repeated market interactions engage in a learning process, making tentative decisions and adjusting these in the light of experiences they gain. Perhaps surprisingly, real markets typically then exhibit a form of parallel clustering: buyers cluster into segments of similar preferences and buyers into segments of similar offers. For computer scientists, the interesting question is whether such co-niching behaviours can be automated. We report on the first simulation experiments showing automated co-niching is possible using reinforcement learning in a multi-attribute product model. The work is of relevance to designers of online marketplaces, of computational resource allocation systems, and of automated software trading agents. ",e-commerce,1046
10.1007/978-3-030-68154-8_75,filtered,core,'Springer Science and Business Media LLC',2021-01-01 00:00:00,core,recommendation system for e-commerce using alternating least squares (als) on apache spark,,"Recommendation system can predict the ratings of users to items by leveraging machine learning algorithms. The use of recommendation systems is common in e-commerce websites now-a-days. Since enormous amounts of data including users’ click streams, purchase history, demographics, social networking comments and user-item ratings are stored in e-commerce systems databases, the volume of the data is getting bigger at high speed, and the data is sparse. However, the recommendations and predictions must be made in real time, enabling to bring enormous benefits to human beings. Apache spark is well suited for applications which require high speed query of data, transformation and analytics results. Therefore, the recommendation system developed in this research is implemented on Apache Spark. Also, the matrix factorization using Alternating Least Squares (ALS) algorithm which is a type of collaborative filtering is used to solve overfitting issues in sparse data and increases prediction accuracy. The overfitting problem arises in the data as the user-item rating matrix is sparse. In this research a recommendation system for e-commerce using alternating least squares (ALS) matrix factorization method on Apache Spark MLlib is developed. The research shows that the RMSE value is significantly reduced using ALS matrix factorization method and the RMSE is 0.870. Consequently, it is shown that the ALS algorithm is suitable for training explicit feedback data set where users provide ratings for items.ISBN för värdpublikation: 978-3-030-68153-1,  978-3-030-68154-8</p",e-commerce,1047
10.1007/s10115-010-0349-1,filtered,core,'Springer Science and Business Media LLC',2011-11-01 00:00:00,core,an abstract architecture for virtual organizations: the thomas approach,http://hdl.handle.net/10251/37655,"Today, the need for architectures and computational models for large-scale open multi-agent systems is considered to be a key issue for the success of agent technology in real-world scenarios. This paper analyzes the significant unsolved problems that must be taken into account in order to develop real, open multi-agent systems. It identifies requirements and related open issues, discusses how some of these requirements have been tackled by current technologies, and explains how the THOMAS architecture is able to give support to these open issues. This paper also describes the THOMAS abstract architecture and computational model for large-scale open multi-agent systems based on a service-oriented approach that specifically addresses the design of virtual organizations. An application example for the management of a travel agency system, which demonstrates the new features of the proposal, is also presented. © 2010 Springer-Verlag London Limited.This work is supported by the Spanish government grants CONSOLIDER-INGENIO 2010 CSD2007-00022, TIN2008-04446 and TIN2009-13839-C03-01 and by the GVA project PROMETEO 2008/051.Argente Villaplana, E.; Botti Navarro, VJ.; Carrascosa Casamayor, C.; Giret Boggino, AS.; Julian Inglada, VJ.; Rebollo Pedruelo, M. (2011). An abstract architecture for virtual organizations: The THOMAS approach. Knowledge and Information Systems. 29(2):379-403. doi:10.1007/s10115-010-0349-1S379403292Agent-Oriented-Software (2004) JACK intelligent agents: JACK teams manual, Release 4.1Aguero J, Rebollo M, Carrascosa C, Julian V (2009) Mdd-based agent-oriented software engineering for ubiquitous deployment. In: The sixth annual international conference MobiQuitous 2009Albers M, Jonker C, Karami M, Treur J (2004) Agent models and different user ontologies for an electronic market place. Knowl Inf Syst 6(1): 1–41Argente E, Criado N, Botti V, Julian V (2008) Norms for agent service controlling. In: EUMAS, pp 1–15Argente E, Giret A, Valero S, Julian V, Botti V (2004) Survey of MAS Methods and Platforms focusing on organizational concepts. Frontiers in Artificial Intelligence and Applications, pp 309–316Argente E, Julian V, Botti V (2006) Multi-agent system development based on organizations. Electron Notes Theor Comput Sci 150: 55–71Argente E, Julian V, Botti V (2008) Mas modelling based on organizations. In: 9th international workshop on agent oriented software engineering (AOSE08). SpringerArgente E, Palanca J, Aranda G, Julian V, Botti V, García-Fornes A, Espinosa A (2007) Supporting agent organizations. In: Proceedings of CEEMAS’07’, pp 236–245Barman R, Tennenholtz M (1996) On partially controlled multi-agent systems. J Artif Intell Res 4: 477–507Baumer G, Breugst M, Choy S, Magedanz T (2000) Grasshopper: A universal agent platform based on OMG MASIF and FIPA standards. In: Agents Technology in EuropeBellifemine F, Poggi A, Rimassa G (2001) Developing multi-agent systems with JADE. In: Castelfranchi C, Lesperance Y (eds) Intelligent agents VII 1571’, pp 89–103Boella G, Caire P, der Torre LV (2009) Norm negotiation in online multi-player games. Knowl Inf Syst 18(2): 137–156Boissier O, Padget J, Dignum V, Lindemann G, Matson E, Ossowski S, Sichman J, Vazquez-Salceda J (2006) Coordination, organizations, institutions and norms in multi-Agent systems, vol 3913Brena RF, Aguirre JL, Chesnevar CI, Ramirez E, Garrido L (2007) Knowledge and information distribution leveraged by intelligent agents. Knowl Inf Syst (KAIS), Springer 12(2):203–227Broersen J, Dignum F, Dignum V, Meyer J (2004) Designing a deontic logic for deadlines. In: Proceedings of 7th international workshop on deontic logic in computer scienceBrogi A, Corfini S, Popescu R (2003) Composition-oriented service discovery. In: Proceedings of 5th international conference on autonomous agents and multi-agent systems (AAMAS)Caceres C, Fernandez A, Ossowski S, Vasirani M (2006), Role-based service description and discovery. In: International joint conference on autonomous agents and multi-agent systemsCarrascosa C, Giret A, Julian V, Rebollo M, Argente E, Botti V (2009) Service oriented multi-agent systems: an open architecture. In: Autonomous Agents and Multiagent Systems (AAMAS), pp 1291–1292Cicortas A, Iordan V (2006) A multi-agent framework for execution of complex applications. Acta Polytechnica Hungarica, J Appl Sci 3(3): 97–119Criado N, Julian V, Botti V, Argente E (2009) A Norm-based Organization Management System. In: Proceedings of COIN’09, pp 1–16Dang J, Hungs M (2006) Concurrent multiple-issue negotiation for internet-based services. In: IEEE internet computing number vol 10–6, pp 42–49de Broek E, Jonker C, Sharpanskykh A, Treur J, Yolum P (2005) Formal modeling and analysis of organizations. In: AAMAS Workshops, vol 3913 of Lecture Notes in Computer Science, Springer, pp 18–34Dignum V, Dignum F (2006) A landscape of agent systems for the real world, Technical report 44-cs-2006-061, Institute of Information and Computing Sciences, Utrecht UniversityDignum V, Dignum F (2007) A logic for agent organization, In: ‘Proc. FAMAS@Agents’007’Dignum V, Meyer J, Weigand H, Dignum F (2002) An organization-oriented model for agent societies. In: International workshop on regulated agent-based social systems: theory and applications (RASTA’02) pp 31–50Dignum V, Vazquez-Salceda J, Dignum F (2005) Omni: Introducing social structure, norms and ontologies into agent organizations, LNAI 3346Erol, K. (1996) Cybele Agent Infrastructure Users guide, http://www.opencybele.org/Escriva M, Palanca J, Aranda G, García-Fornes A, Julian V, Botti V (2006) A Jabber-based multi-agent system platform. In: Proceedings of the fifth international joint conference on autonomous agents and multiagent systems (AAMAS06), ACM Press, pp 1282–1284Esteva M, Rodriguez-Aguilar J, Sierra C, Arcos J, Garcia P (2001) On the formal specification of electronic institutions. Lecture notes in artificial intelligence 1991, Springer, pp 126–147Bellifemine F, Caire G, Greenwood D (2007) Developing multi-agent systems with JADE. Wiley, LondonFerber J, Gutknecht O (1998) A meta-model for the analysis and design of organizations in multi-agent systems. In: Proceedings of the third international conference on multi-agent systems (ICMAS’98). IEEE Computer Society, pp 128–135Ferber J, Gutknecht O, Michel F (2004) From agents to organizations: an organizational view of multi-agent systems. In: Giorgini P, Muller J, Odell J (eds) Agent-oriented software engineering VI, Vol LNCS 2935, pp 214–230Gasser L (2001) Perspectives on organizations in multi-agent systems. Springer New York, Inc., New York, NY, USA, pp 1–16Giorgini P, Kolp M, Mylopoulos J (2006) Multi-agent architectures as organizational structures. Auton Agents Multi-Agent Syst 13(1): 3–25Giret A, Julian V, Rebollo M, Argente E, Carrascosa C, Botti V (2010) An open architecture for service-oriented virtual organizations. In: PROMAS 2009 Post-Proceedings, Springer, pp 1–15Greenwood D, Calisti M (2004) Engineering web service—agent integration. In: IEEE international conference on systems, man and cybernetics, number vol 2, pp 1918–1925Greenwood D, Lyell M, Mallya A, Suguri H (2007) The IEEE FIPA approach to integrating software agents and web services. In: AAMAS ’07: proceedings of the 6th international joint conference on autonomous agents and multiagent systems, ACM pp 1–7Gutknecht O, Ferber J (1997) Madkit: organizing heterogeneity with groups in a platform for multiple multi-agent systems. In: Technical Report 97188 LIRMMHahn C, Nesbigall S, Warwas S, Zinnikus I, Fischer K, Klusch M (2008) Integration of multiagent systems and semantic web services on a platform independent level, In: Proceedings of the 2008 IEEE/WIC/ACM international conference on web intelligence and intelligent agent technology, IEEE Computer Society, Washington, DC, USA, pp 200–206Heep M (2006) Semantic web and semantic web services. In: IEEE internet computing, number vol 10–2, pp 85–88Horling B, Lesser V (2004) A survey of multiagent organizational paradigms. Knowl Eng Rev 19: 281–316Horling B, Lesser V (2005) Using ODML to model multi-agent organizations. In: IAT05: Proceedings of the IEEE/WIC/ACM international conference on intelligent agent technologyHowden N, Rönnquist R, Hodgson A, Lucas A (2001) JACK intelligent agents-summary of an agent infrastructure. In: Proceedings of the 5th ACM international conference on autonomous agentsHubner J, Sichman J, Boissier O (2006) S-Moise+: A middleware for developing organised multi-agent systems. In: International workshop on organizations in multi-agent systems vol 3913 of LNCS pp 64–78Huhns M, Singh M (2005) Service-oriented computing: key concepts and principles. IEEE Internet Comput 9(1): 75–81Jennings NR, Wooldridge M (2002) Agent-oriented software engineering. Handbook of Agent TechnologyJennings, N, Wooldridge, M (eds) (1998) Agent technology. Foundations, applications and markets. Springer, New York. ISBN 3-540-63591-2Klusch M, Fries B, Sycara K (2006) Automated semantic web service discovery with owls-mx. In: Proceedings of 5th international conference on autonomous agents and multi-agent systems (AAMAS). Hakodate, JapanLopez F, Luck M, d’Inverno M (2006) A normative framework for agent-based systems. Comput Math Organ Theor 12: 227–250Luck M, McBurney P (2008) Computing as interaction: agent and agreement technologies. In: IEEE SMC conference on distributed human-machine systems, pp 1–6Luck M, McBurney P, Shehory O, Willmott S (2005) Agent technology: computing as interaction (A roadmap for agent based computing), AgentLinkMao X, Yu E (2005) Organizational and social concepts in agent oriented software engineering. In:AOSE IV, vol 3382 of lecture notes in artificial intelligence, pp 184–202Negri A, Poggi A, Tomaiuolo M, Turci P (2006) Dynamic grid tasks composition and distribution through agents: research articles. Concurr Comput Pract Exper 18(8): 875–885Nguyen NT, Katarzyniak RP (2009) Actions and social interactions in multi-agent systems. Knowl Inf Syst 18(2): 133–136Nguyen T, Kowalczyk R (2005) Ws2jade: integrating web service with jade agents, technical report SUTICT-TR2005.03, Centre for Intelligent Agents and Multi-Agent Systems, Swinburne University of TechnologyOdell J, Nodine M, Levy R (2005) A metamodel for agents, roles, and groups. In: James Odell JM, Giorgini P (eds) Agent-Oriented Software Engineerin (AOSE) V. Lecture notes in computer science. SpringerPathak J, Koul N, Caragea D, Honavar VG (2005) A framework for semantic web services discovery. In: WIDM ’05: proceedings of the 7th annual ACM international workshop on Web information and data management, ACM, pp 45–50Piunti M, Ricci A, Santi A (2009) Soa/ws applications using cognitive agents working in cartago environments. In: Proceedings of 10th joint conference AI*IA TABOO From Objects to Agents (WOA 2009)Poggi A, Tomaiuolo M, Turci P (2007) An agent-based service oriented architecture. In: 8th AI*IA/TABOO joint workshop “from objects to agents”: Agents and Industry, pp 157–165Rebollo M, Giret A, Argente E, Carrascosa C, Corchado J, Fernandez A, Julian V (2009) On the road to an abstract architecture for open Virtual organizations. In: 10th international work-conference on artificial neural networks, vol 5517 of LNCS, pp 642–649Ricci A, Viroli M, Omicini A (2008) The A&A programming model and technology for developing agent environments in MAS. In: programming multi-agent systems, vol 4908 of LNCS, Springer, pp 89–106Ricci A, Viroli M, Piancastelli G (2007) Simpa: A simple agent-oriented java extension for developing concurrent applications. In: ‘LADS’, vol 5118 of Lecture Notes in Computer Science, Springer, pp 261–278Sensoy M, Pembe C, Zirtiloglu H, Yolum P, Bener A (2007) Experience-based service provider selection in agent-mediated e-commerce. In: Engineering applications of artificial intelligence, vol 3, pp 325–335Shafiq MO, Ali A, Ahmad HF, Suguri H (2005) Agentweb gateway—a middleware for dynamic integration of multi agent system and web services framework. In: 14th IEEE international workshops on enabling technologies (WETICE 2005), 13–15 June 2005, Linköping, Sweden, IEEE Computer Society, pp 267–270Sycara K, Paolucci M, Soudry J, Srinivasan N (2004) Dynamic discovery and coordination of agent-based semantic web services. In: IEEE internet computing number, vol 8–3, pp 66–73Sycara K, Widoffand S, Klusch M, Lu J (1982) Larks: dynamic matchmaking among heterogeneous software agents in cyberspace. J Auton Agents Multi-Agent Syst 5(2): 173–203Tapia DI, Rodríguez S, Bajo J, Corchado JM (2009) FUSION@, a SOA-based multi-agent architecture. In: International symposium on distributed computing and artificial intelligence, DCAI 2008, University of Salamanca, Spain, 22th–24th October 2008, vol 50 of Advances in soft computing. Springer, pp 99–107Val ED, Criado N, Carrascosa C, Julian V, Rebollo M, Argente E, Botti V (2010) THOMAS: a service-oriented framework for virtual organizations. In: 9th international conference on autonomous agents and multiagent systems (AAMAS 2010)Varga LZ, Hajnal Á (2003) Engineering web service invocations from agent systems In: 3rd international central and eastern european conference on multi-agent systems, CEEMAS 2003 vol LNCS 2691, pp 626–635Zambonelli F, Parunak H (2002) From design to intention: signs of a revolution. In: Proceedings of 1st international joint conference on autonomous agents and multiAgent systems, pp 455–456Zinnikus I, Hahn C, Fischer K (2008) A model-driven, agent-based approach for the integration of services into a collaborative business process. In: AAMAS ’08: 7th international joint conference on autonomous agents and multiagent systems, pp 241–24",e-commerce,1048
10.1007/978-3-030-79150-6_61,filtered,core,'Springer Science and Business Media LLC',2021-06-25 00:00:00,core,validation and verification of data marketplaces,,"Part 18: Smart Blockchain Applications/CybersecurityInternational audienceThis paper presents a Validation and Verification (V&V) model of Data Marketplaces. Data is extracted from the sensors embedded within the Smart city, infrastructure, or building via Application Programming Interfaces (APIs) and inserted into a Data Marketplace. The technology is based on smart contracts deployed on a private ethereum blockchain. Current issues with data in Smart cities, infrastructure, buildings, or any real estate, are the difficulty of its access and retrieval, therefore integration; its quality in terms of meaningful information; its large quantity with a reduced coverage in terms of systems and finally its authenticity, as data can be manipulated for economic advantage. In order to address these issues, this paper proposes a Data Marketplace model with a hierarchical process for data validation and verification where each stage adds a layer of data abstraction, value-added services and authenticity based on Artificial Intelligence. By using a blockchain, this presented approach is based on a decentralised method where each stakeholder stores the data. The proposed model is validated in a real application with live data: Newcastle urban observatory smart city project",e-commerce,1049
10.36930/40290822,filtered,core,'Ukrainian National Forestry University',2019-10-31 00:00:00,core,ансамбль мереж grnn для розв'язання задач регресії з підвищеною точністю,https://core.ac.uk/download/270168619.pdf,"The effective solution of regression problems is an important task for e-commerce, medicine, business analytics and for many other industries. In recent years, the demand to use artificial intelligence methods for solving regression problems has been rapidly increased. This can be explained by the need to work with large datasets, or the complex interrelationships between multiple independent variables. General regression neural network is one of the options for solving this problem. However, the use of this computational intelligence method does not provide high accuracy of the result, which imposes a number of limitations. The new method based on a general regression neural network ensemble for increasing prediction task accuracy is developed. The main advantages and disadvantages of neural networks of this type are described in detail. A brief description of the operation of the general regression neural network is given. An algorithmic implementation of the developed ensemble is provided. An increased prediction accuracy using developed ensemble has been received. A software solution for the implementation of the described method with use libraries of Python programming language is developed. Experimental modeling of the method is conducted on real data of the regression problem. High efficiency of solving the problem is established using the developed method on the basis of both the mean absolute error in percentage and using the standard error. The method is compared with the existing ones: the Wiener polynomial approximation based on Stochastic Gradient Descent, the general regression neural network, and the modified AdaBoost algorithm. The highest accuracy of the solution of the problem by the developed method is proved experimentally based on both indicators of accuracy among all the methods considered in the work. In particular, it provides accuracy more than 3.4&nbsp;%, 4.3&nbsp;% and 8.3&nbsp;% (MAPE) compared to existing methods. The method developed can be used to obtain high-precision solutions for solving applications of e-commerce, medicine, materials science, business analytics and others. The plan for further researches is to develop a hybrid high-speed computational intelligence system based on the combination of the developed method and the successive geometric transformations model (SGTM).Розроблено метод ансамблювання нейронних мереж узагальненої регресії для підвищення точності розв'язання задачі прогнозування. Описано базові положення функціонування нейронної мережі узагальненої регресії. На основі цього подано алгоритмічну реалізацію розробленого ансамблю. Аналітично доведено можливість підвищення точності прогнозу із використанням розробленого ансамблю. Із використанням бібліотек мови Python, розроблено програмне рішення для реалізації описаного методу. Проведено експериментальне моделювання роботи методу на реальних даних задачі регресії. Встановлено високу ефективність розв'язання поставленої задачі із застосуванням розробленого методу на основі як середньої абсолютної похибки у відсотках, так і з використанням середньоквадратичної похибки. Здійснено порівняння роботи методу із наявними: апроксимацією поліномом Вінера на основі Стохастичного Градієнтного спуску, нейронною мережею узагальненої регресії та модифікованим алгоритмом AdaBoost. Експериментальним шляхом доведено найвищу точність розв'язання поставленої задачі розробленим методом на основі обох показників точності серед усіх розглянутих у роботі методів. Зокрема, він забезпечує більш ніж на 3,4, 4,3&nbsp;та 8,3&nbsp;% (MAPE) вищу точність порівняно із наявними методами відповідно. Розроблений метод можна використовувати для отримання розв'язків підвищеної точності під час вирішення прикладних завдань електронної комерції, медицини, матеріалознавства, бізнес-аналітики та інших",e-commerce,1050
,filtered,core,,,core,iot virtualization for animal simulations,,"IoT, also known as Internet of Things, is a popular topic nowadays. Devices of various scales,
complexities, and functionalities are deployed for different tasks. However, there exist technical
difficulties in many circumstances where physical deployments are challenging or require
verifications beforehand. For example, it would be hard for a group of high school students, who
have no prior knowledge of circuits, to physically implement even the simplest circuit design.
Devices that would be massively deployed without human supervision, such as ocean quality
sensors, will also require verification in a simulation. Professor Matthew has been conducting
research on virtualizing IoT devices, where he aimed to provide a platform to construct virtual
circuits and deploy them in the virtual world. This platform allows users to monitor simulated inputs
and outputs through virtual consoles and change parameters and components of the circuits in a
virtual setup. To drive the virtual world, there should be an engine that simulates different aspects
of the real world, such as movement, weather, etc. Taking the example of monitoring animals,
there should be simulators that generate both the animals and environment. To simulate animals,
several attempts were made to accurately model different animals like zebra and lions. We initially used
an FSM-like model with fixed states to describe different actions and conditions of an animal,
then moved forward to a learning-based model using different machine learning techniques, like linear regressions and neural networks. With a learning-based, or black-box model, animals act
with more flexibility, instead of only allowing to be in the several fixed states. Circuit emulation is
also an important feature in the overall virtualization process. There were two steps to create a
virtual device. The first step involved physical testing and modeling of the devices, and the second
step involved mapping them into scripts that describe their behaviors. These two components are
essential parts for the entire IoT virtualization project since they generate all the simulations in the
virtual world, and they will be actively updated as we introduce more features to the project.U of I OnlyUndergraduate senior thesis not recommended for open acces",oceanology,1051
,filtered,core,Delft Cluster,2003-06-01 00:00:00,core,"data mining, knowledge discovery and data-driven modelling",,"The project was aimed at exploring the possibilities of a new paradigm in modelling - data-driven modelling, often referred as ""data mining"". Several application areas were considered: sedimentation problems in the Port of Rotterdam, automatic soil classification on the basis of cone penetration tests, prediction of water levels in the ocean, and others. The methods applied included: artificial neural networks, fuzzy systems, M5 model trees, chaos theory. The knowledge was disseminated using the following means: several publications at the conferences and peer-reviewed journals, Symposium for 80 Dutch participants from various civil engineering organisations (April 2002), Web-based platform with the main project results, software and knowledge base of best practices and publications. The developed research methodologies were practically used in the Delft Cluster research Themes and by external partners. Two PhD researches are underway and 4 MSc projects were successfully completed. A comprehensive Knowledge Platform ( http://datamining.ihe.nl) was built; it includes the educational components, links to the news and events in the area of data-driven modelling, knowledge base with the publications, software and best practices, and software with the examples of real case studies. The achieved results make possible to conclude that the data-driven methods can be effectively used in a wide range of civil engineering problems, complementing and sometimes replacing more traditional simulation models",oceanology,1052
,filtered,core,J. A. Ririe,2002-07-03 00:00:00,core,"magrath store news (july 3, 2002)",,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.1
I
MAGRATH NEWS 1 •>
Published Weekly Since 1932 by
The Magrath Trading Company
50c o ... July 3,2002
r J
la .t-.k.
TOWN OF MAGRATH
“the garden crrr
DISASTER SERVICES
COORDINATOR
The Magrath and District Emergency Services is seeking an
extraordinary volunteer to serve as Disaster Services Coordinator for
Magrath and area. ... t:
This is a volunteer position to assist in the delivery of disaster services for
the Town of Magrath and the eastern half of Cardston County during
emergencies. This individual will be responsible for the following:
1. The development and implementation of a disaster plan in
consultation with the municipalities, local agencies and disaster
preparedness group such as Red Cross..
2. The reassessment of the disaster plan on an annual basis.
3. Coordinate training for disaster response personnel.
4. During emergency incidents, through a consultative process with
other emergency services, municipalities and agencies, ensure that
disaster needs are provided.
The initial development process would require a considerable time
commitment, however, once the disaster plan is in place the time
involved would be significantly less. If you require further information,
contact Rod Bly at 758-3212. Interested parties may submit their intent
in writing to the Town of Magrath, Box 520, Magrath, AB TOK 1J0, .
Attention: Rod Bly
*
r
Points of Interest
♦ Employment
* Grocery Specials
Home Hardware
Inside this Issue:
Community 1-3
Sports 4
Classified Ads
Calendar
Library News
Grocery Specials
6
8
8
9-11
^Hardware Specials 12y
!
DONT FORGET
Magrath Parade - Saturday July 27th
If you would like to participate in the parade, we would love to see you.
Please remember the date and get your decorations ready.
li
2
¿Samudfornii, ^ctbiâ'ul
have chosen to be married
Saturday, July 20,2002
at the Coaldale LDS Church
2216 - 21A Avenue
Together with their parents
they invite you to join the celebration
Wedding Ceremony 1:30 in the afternoon
Calling Reception 2:30 - 4:30
Parents of the Groom
Carol J. Patey
the late Clark Dainard '
Parents of the Bride Parents of the Bride
Tammy J. Noble William A Knopf
Leo Trenlær Beverly J. Knopf
h , I
Community No Cost Clothing
Exchange
Magrath Arena
July 30 - Evening only
Seniors & Disabled 5-6 p.m.
Open to public 6-9 p.m.
July 31
Open 9 a.m. - 4 p.m.
Donations of CLEAN clothing can be
dropped off at Wendy Coleman’s garage,
157 S. 1st St. W. before July 19th.
u,. • J •
^DESIGN
We specialize in:
-Scanning: slides, photos, negatives, text..
-Photo repair, retouching, restoration &
photo collages
-Memory videos (weddings, reunions, sports...
All of the above can be preserved
on VHS, CD and
(phone for details)
We also do private tutoring, troubleshooting,
desktop publishing & more!
Phone Bonny/Brenda: 758-3844 eve-/voicemail (day)
Duane & Carma Thomson are
pleased to announce the birth
of their daughter,
Keltie
She was born June 20,2002 and
weighed 9 lbs. 4 oz.
She is a baby sister to a very
happy Nathan, Cara and
Benjamin.
î -
MINI-GUARD
The Spirit of Alberta Barid i$ sponsoring a
one Week camp for all girls grades K-6,
July 22-26 (Magrath Celebration Week).
The camp will be held at the school from
10:00 to 11:30 each morning with
instruction according to age level in
dance, flag, and precision drill. The camp
will feature a special performance for
parents on the last day, and all students
may march with the Spirit of Alberta Band
in the Magrath Days parade on Saturday.
The cost of the camp is $39.00 which
includes a camp T-shirt. Registration
forms have been sent home with all girls
in elementary school (except
Kindergarten girls, which were missed).
Extra registration forms are available at
the Trading Co. Office or till. Also
welcome are next fall’s kindergarten girls
as well as friends and cousins here for
the celebration.
For additional information, contact
Joanne Dever (758-3631), Nikki Bogdan
(758-3217), or Jerry Chatwin (758-3765).
ATTENTION!!
QJtADE_ /2 $TUDENT± NOTE
Students entering Nursing
Magrath Hospital Auxiliaiy present each year
a “Margaret Long"" Scholarship
to the student with the highest marks entering
Nursing.
To apply, send a letter and copy of final marks
before September I, 2002 to:
Magrath Hospital Auxiliaiy
Box 639
Magrath, AB TOK I JO
Now Booking Appointments!!
'Wakeup With Make-up!
Permanent Cosmetics
SENIOR’S NEWS
Suppers will resume Sept. 4th
Cornbust date will be announced in August
*******
Contact Hazel Rasmussen for rental of the
Senior’s Centre. Fee $75.00 Phone 758-3545
*******
Garage Sale September 20th.
All donations gladly accepted.
Phone Grace Navratil at 758-3291
or Bob Clifton at 758-6733
HAVE A WONDERFUL SUMMER!
Denturist
* Complete denture service
* Soft Liners & stabilizing wings
for problems, sore lowers
George
Torre Alba
JULY 15 - 6:00 p.m.
Diamond Willow Terrace
In Your Town
MOBILE
Tina. D. Reidcpct
Phone: (403) 758-3930
758-3936
Call for your Free Consultation
MUSEUM NEWS
Il Darryl Passey has kindly loaned us his
(taxidermy collection which is a display of
local animals. These items are currently on
(display and can be viewed during the
! extended
|{ Summer Hours:
1 Monday 11:00 a.m. - 8:00 p.m.
I Tuesday - Friday 9:00 a.m. - 5:00 p.m.
(The Museum will be open from 2-4 p.m. on
July 27th (Celebration day) for family and
II friends to enjoy the displays and remember
1 the good ol’ days.
Make your dream come true
Have your home built by
JENSEN ENTERPRISES
New construction:
Additions:
Patio Decks:
403-758-3669
Ken Jensen, Owner
Fine woodwork:
Hardwood furniture:
Renovations:
Box 93
Magrath, AB TOK 1 JO
MR. DANIEL J. WIPF
beloved husband of Mrs. Sarah Wipf of Magrath,
passed away at the Magrath Hospital on Monday,
July 1, 2002 at the age of 68 years. .
He is survived by his wife Sarah Wipf of Magrath;
two sons, Tim (Judy) Wipf of Magrath, Joe Wipf of
Lethbridge; three daughters, Judy, Sarah, and Annie
Wipf all of Magrath; and 5 grandchildren.
A Funeral Service will be held at Deerfield Colony
Church on Wednesday, July 3rd, 2002 at 12:00
p.m.
Interment to follow at Deerfield Colony Cemetery.
4. 'BASKETBALL -
I---------- -----u.
’F J ; to
—----- —-----—SO »Fr.T1 BAL■ L u < •
MAGRATH
FAMILY/COMMUN1TY -
‘ softball TOURNEY w ❖ * . * 4 ♦
July 25-27,2002
All families/community members wanting
play in the Celebration' Softball Toiifnament
need to get their names in to either the town
office (758-3212) or Bill Alston (758^895).
Any community or district member^j^; are""'
not playing on a family team, may put their,
names on aijisf ht the Town Hall. No hand-p.
icked teams, please.
• -in? r-vi:<r •& ^ MqfnoO ■»
.'IHIdI*
..-»a • , .'
*
I
rI
... CLOGGING flaggy
■.... Krxikvd New cXoggiwg St*
* .....Opgkuuvg iw
* September 2002 *
GANAIMAN
-- intermediate arvdy^va*Aced/
Classes znoiubbr
- 45 mii/vwte Lessors 1 day a weetetl 7 c’1
- 4^5.00 -for 1 session. (12 lessors)
i- starting middle of «September td'*""'1'11 ”
December ai/vd will commen.ee iiA<__.. __
Jan.uary oi/vce again, .’¿M
- Competition. in, ApriL @ i^exburg, • rj*v/ *
- Filled with Fun,,jazzy steps autd-lots
of LearlA-luvg ■ -*nn
- Instructors: wt.uk <i
Paige Hudson,,jillei/ve Hudson,,
Cheriess H-LtdsoiA. § Noelle Wolsey s f ¡-v,-
P^AS^^LL:
Paige,jilleiA-e or cberiess @ y-5^.-3±^
on Noelle @ 7^2-3992
----------------- "" i' ■ 1 •—jfc.-. .
i Ji
i
r* I DANNY BALDERSON
BASKETBALL CAMP
.JULY 29 - AUGUST 2, 2002
Tom Karren Gym Magrath, Alberta
for BOYS and GIRLS Grades 3-9
Head Instructor: DANNY BALDERSON
Guest Instructors: DIANE SMITH
JIMMY BALDERSON
.. z Camp Includes:
Free Camp T-Shirt
5 Hours of Skill Instruction/Day (9-12, 1-3)
Skills, One-on-One and Team Competitions
Daily Awards and Prizes
Camp Tuition: $105.00 ($90'.00 for each additional
> ?fimiTy-member)
Forms can be found at the Trading Co. Office,
Magrath High School Offioe, or by contactin Wes
or Debbie Baldersoti (758-6380). 1
Because of Limited Space - Register Early!!
I
I
J
ÌI
I
REMEMBER TO CALL
TAI
Heating, Air Conditioning,
Refrigeration and Appliance Service
Gas and Electrical Service
and Trenching
752-3866
TAI HANCOCK - MGR.
BRIDGE SEFTIC SERVICE u -
?!
A DIVISION OF CHINOOK WATER WAGON ~'
'! r'-j, ""TiO""lJ
: •» vl'mel vv?w
RR 8-20-18 ■ ’
LETHBRIDGE, AB •’ .
T1J4P4
STEELE SHERIDAN - PROPRIETOR
PHONE (403) 328-2460
CELL (403) 330-8066
FAX (403) 327-3337
5
REMEMBER WHEN ....
Close your eyes... A nd go back in time Before semi automatics and crack... Before SEGA dr Super Nintendo... Way back...I'm talking about Hide and seek at dusk. Red light, green light.
The corner Store.
Hopscotch, butterscotch, doubledutch, jacks, kickball, dodgeball.
Mother May I... Red Rover arid Roly Poly. ...... Hula Hoops.
Running through the sprinkler.
An ice cream coneion a warm summer night... Chocolate or vanilla or strawberry or maybe butter pecan.
• Wait...
Watching Saturday Morning cartoons... Short commercials.
Fat Albert, Road Runner, The Three Stooges, and Bugs.
Or back f urther...
i When around the comer seemed far away, And going downtown seemed like going somewhere.
Cops and Robbers, Cowboys and Indians, Zorro. Climbing trees, building igloos out of snow banks
. Running till you were out of breath. Laughing so hard that your stomach hurt. ..
Jumping on the bed. , ""a Pillow fights.
Spinning around, getting dizzy, and falling
down...
Being tired from playing...Remember that?
The worst embarrassment was being picked last for a team.
War was a card game.
Water balloonsweretheultimate weapon; < Baseball cards in the spokes transformed any bike
into a motorcycle. :
I'm not finished just yet...
When you'd reach into a muddy gutter for a
! penny.
When you got your windshield cleaned, oil checked,
and gas pumped without asking, for free, every ; time...
and, you didn't pay for air.
if When nearly everyone's mom was at home . when the kids got there.
When it took five minutes for the TV to warm up.
if you even had one.
It was magic when dad would ""remove* his thumb.
When it was considered a great privilege to be taken out to dinner at a real restaurant with your parents.
When girls neither dated nor kissed until late high school, if then. When they threatened to keep kids back a grade if they failed...and did!
When being sent to the principal's office was nothing compared to the fate that awaited a misbehaving student at home. Basically, we were in fear for our lives but it wasn't because of drive-by shootings, drugs, gangs, etc.
Our parents and grandparents were a much bigger threat!
Didn't that feel good? Just to go back and say,
""Yeah, I remember that!"" Remember when... Decisions were made by going ""eeny-meeny- miney-mo.""
Mistakes were corrected by simply exclaiming, ""Do over!""
""Race issue"" meant arguing about who ran the fastest.
. The worst thing you could catch from the opposite sex was cooties.
It was unbelievable that dodgeball wasn't an . Olympic event.
Having a weapon in school meant being caught with a slingshot.
Scrapes and bruises were kissed and made better.
Taking drugs meant orange-flavored chewable aspirin.
Abilities were discovered because of a ""double­dog-dare.""
If you can remember most or all of these, then you have LIVED!!!6
CLASSIFIED ADS
FREE MARKET
❖
FOR SALE - Dark Blue Toddler Stroller. Good condition/clean ~ $25.00. Lightweight Pull Golf Cart 2
*******
yrs. old ~ $15.00. Never used Bike Rack for vehicle
❖
ALL FORMER GUARD GIRLS or their parents. Please check your closets for any sailor outfits or any other guard costumes you may have. Please contact Jerry Chatwin or Joanne Dever if you find any.
❖
(front or back). Holds 2 bikes ~ $20.00. Never used Heavy Hay Tarp. 25’ x 14’ ~ $90.00 o.b.o. RonYoshihara 758-3556.
HORSES FOR SALE-
❖
FOUND - Border Collie Dog, south of Magrath. Male with brown markings. 758-6632
1 - 5 yr. old Quarter Gelding
1 - older Appaloosa Gelding, great kids horse.
CaU Craig at 758-3188.
GARAGE SALES
❖
WANTED - Good used 3-wheel Bike. Call George Fyfe at 394-3663 ■ < .. . .
*******,; ,,.v.
MOVING SALE 0
Everything Must Go! !; Microbe,.:T;y.’s. furniture, bedroom states, videos* ..
AUTOMOTIVE (
.. ‘ .''A'.
******* ._ -
284 South 2nd St West
: Saturday, July'6
❖
FOR S ALE - 1988 Plymouth Sundance - 5 speed
9 am. until everything’s gone
standard. CD player, great gas mileage, good condition,
.c
excellent car for student or small family. $1500.00 Call
❖
GARAGE SALE
Dustin at 317-1682 or 758-3188.
S. J^St.tW. (Wendy Coleman’s) .. . < < ■.
Saturday, July 6th
❖
FOR SALE - 1987 Chev % ton Customized Van
9 am. - 2 p.m.
(pacemaker package). Loaded, no rust, good condition.
Pinky' Coleman - Avbn Sale, too! '1
runs great. $4000.00 Call Craig at 758-3188.
❖
YARDS^LE:;j .-2, ,,-,.(3,,- . -
Saturday, July 6
9 am.-1p.m. 'ISdmsfn:.-: •
BUSINESS
****-***••■•’■
159 W. la Ave. NHll(,.-. •>,- i.: :
(M. Schad) '
❖
Looking for WEEKLY HOUSE CLEANER to clean my house near Spring Coulee. Mature person preferred. Transportation costs will be covered. 653-3461.
•. i ...
BUY & SELL
❖
CUSTOM CUTTING & BAILING
*******
Call Cody Ririe 758-6785
❖
FOR SALE - Moores Pure Virgin Wool Black Suit.
• .. , ,■
Regular 42 ~ $100.00. Moores Pure Virgin Wool Dark
JUST RIGHT ROTOTILLING by David & Mark
Blue Jacket Regular 42 ~ $45.00. Jessica black, green,
HcUKCi. 758-3009.
gold Dress. Size 14 ~ $30.00. J.R. Petite black sleeveless c/w % length royal bluejacket ~ dress ~ size
❖
BENNETT LAWN MOWING - Jonathan, Lauren &
12 ~ $45.00. 758-6024.
Russell - Riding mower and push mower. 758-6222.
❖
FOR SALE - Compac Laptop Presario 1694-AMD 450
❖
MIKE HARKER is back and ready to rototill your
MHZ - 64 MB - 6 GB - 4xDVD - 14 with warranty until Dec. 17,2002. Programs. Never used as computer
garden. Call me whenever you’re ready at 758-6664.
- like new - $2000.00.
Water cont - holding tank for R.V.’s - $50.00 758-6024
❖
DO YOU HAVE AN ELDERLY OR ILL LOVED ONE? Medical Pendants can save lives. They call for help when you’re away. Call Canadian Security
❖
FOR SALE - Cell Phone. Nokia 5160 c/w 12V charger, leather holder, hands free ear buds, window mount
Systems at 758-3945.
antenna, 3 watt booster, home charger. $400.00
❖
For all your cleaning needs from hospital clean to a
758-6024
touch-up, carpet to ceiling & everything in between. No job too big or too small.
Call Wayne’s Carpet & Upholstery Cleaning 758-6414.CLASSIFIED ADS
♦
REALESTATE *******
FOR RENT
2 bedroom house for rent in Magrath. 2 houses east of
the Credit Union. Available July 1* 758-6725
❖ House for rent. 2 bedroom home in Welling. No
smoking unit for abstainers. No pets. Fridge, stove,
washer & dryer in eluded in rent. Call 752*3848.
FOR SALE
NEWER MOBILE HOME FOR SALE - 3 bedroom, 2
bathroom, new carpet, new lino, fridge, stove, dishwasher.
On lot in Coalhurst — can be moved. $36,000. 758-6277.
❖ HOME FOR SALE -Completely Renovated 4 bedroom
home. Good location close to school. Asking $72,000.
758-3731 or 758-6826.
ili
: :
J J
c? HomeLife
Higher Standards
M.L.S.
Jim Anderson
agent
Residential ~ Farm
Acreage ~ Commercial
in Magrath and Area
2 Houses for Sale in Del Bonita
2 teacherages for sale in Del Bonita
Comparative Market Analysis
(No Charge) - For people interested in getting an
evaluation of marketability of your property.
Phone: 758-6725 (leave message)
331 -8882 (cellular)
• t •
•<
Z
.15
Therapy
can help you
call KIM at
Z58-3210
to book your
appointment today
ALSO SPECIALIZING IN
MATERNITY MASSAGE
COBH) By MOST HEALTH CMC HUMS
i s- •)
AMAZING UiZLEii FACTS
Almonds are not really nuts, but a member of
the peach family.
Eskimos use refrigerators to keep food
FROM freezing.
The name Wendy was made up for the book
“Peter Pan”. . ,.A
... •.u Ti .. i .
The reason firehouses have circular stairways
is from the days when the engines were pulled
by horses. They kept the horses from walking
up the stairs.
An estimated 35 percent of people who use
personal ads for dating are already married!
The Bible has been translated in to Klingon.
Butterflies taste with their feet!
According to surveys, on average people fear
spiders more than they fear death.
«i
I■I4
JMfiRAÎHATWS
Im Our Community... July 2002
Sunday
Monday
Tuesday
Wednesday
Thursday
Friday
Saturday
30
/
Mobile Denturist 6:00 Library closed
2
3
4
5
Mo
<
Yar V
6
ving Sale 28^ >. 2nd St. W.
dSale 15<
1.Is* Ave. N.
»
7
8
Co-ed Slo-pitch 6:30
9
*
¡0
II
12
!3
LIBRARY NEWS
The Summer Reading Program starts Thursday, July 4, 2002 from 2 p.m. to 3 p.m. for children from Grade 1 to Grade 12. The theme is “Sailing the 7 Seas”. This program promises to have fim activities, games, stories and treats. The summer reading program will run every Thursday for the month of July. For more information do not hesitate to call the library. 758-6498 ™ ;
Movies that are loaned from the Magrath Library now have a two day renewal period instead of one day renewal. We hope that this will be more convenient for .everyone.
We are selling Chinook Arch 10 Year Anniversary Quilt Book Bags. They are made with strong canvas and have the picture of the 10 year anniversary quilt. We are selling them for $12.00.
. . • • ‘.'..it’d ""b..
We will be closed Saturday, June 29 and Monday, July 1st for the long weekend.
Adult Fiction
To Trust a Stranger by Karen Robards 1
An Execution of Honor by Thomas L. Muldoon Looking Back by Belva Plain v’
The Edge of Town by Dorothy Garlock , •
The Summerhouse by Jude Deveraux
Seals Sub Rescue: Operation Endurance by S.M. Gunn
Juvenile Fiction
The Bear Dance by Chris Riddell ''
A Letter to Grandma by Paul Rogers
Disney’s The Hunchback of Notre Dame
J ' Non-Fiction
■ Redefining Beauty by Victoria Jackson ,: .
Cooking for Jack by Tommy Baratta ’
- ■: ; Other Creations by Christopher Manes s ’?
. t Pheasants and Their Breeding and Management by K.C.R. Howman
The Diabetes Holiday Cookbook by Carolyn Leontos
Videos ;
Dudley Do-Right
-
My Life
Please note that the deadline for submissions to the pajjer is MONDAY at 6:00 p.m. Entries submitted after the deadline will be published the following week. Phone 758-6377, email to tidmarsh@telusplanet.net, fax 758­6888 or drop off your submissions at the Magrath Trading Company Office.•if
9
- f MagTath Trading Company ■ -
3 5>. J,
jQ’
GROCERY SPECIALS
“From Our Family To Yours, v”
Daily Delights and Frozen Favorites!’
'•hi viy.iv ir. ■ •'
Kraft Single Slices or Cheez Whiz-select varieties
1 kg
$6.98
Kraft Philadelphia Cream Cheese-select varieties
j? 250gii.
$2.98
Western Family Margarine - select varieties
>- ,,454g. .
.98
Armstrong Aged Cheddar - select varieties .
750 g
$8.98
Armstrong Cheddar Cheese or Mozzarella - select varieties
750 g
$7.98
Dairyland Yogurt - regular or fat free - select varieties
750 g
$2.68
Dairyland Cottage Cheese -select varieties
750 g
$3.68
i DairyiaRd Sour Cream ?*.
750 ml
$2.88 A i
BreyersiAII Natural or Blends ice Cream - select varieties
2 litre
$5.98 - ;
i Tropicana or Tropics Juice, Cocktails or Blends-select varieties
... . 1.89 litre
2 for $7.00V; 1
Minute Maid Orange Juice, frozen-select varieties
355 ml
3 for $3.00* .
Minute Maid Punch or Nestea Iced Tea. frozen - select varieties
355 ml
5 for $4.00 O 1
Five Alive or Fruitopia Punch or Blends-select varieties
1.89 litre
2 for $5.00 J *
McCain Rising Crust or International Pizza, frozen - select var.
464-835 g
$6.98
McCain Diep n Delicious or Triple Chill Cakes, frozen
510-530 g
'•Jp- $3:98
Groceries...
Kraft Pourable Dressing-select varieties
..,475 ml
:■ 2 for $5.00 '
Western Family Apple Blend .. :,1G7
1 -litre- ':j •'
5 for $4.00 j.
Kraft Jam - strawberry or raspberry
500 ml
J . $3.98 . rih *
Kraft Peanut Butter-select varieties
1 kg-
$4.98
V8 Splash or Vegetable Cocktail - select varieties
1.89 litre .....
$3.68
Minute Maid, Bibo or Five-Alive-select varieties
10 pack
$3.48
Ocean Spray Cranberry Cocktail-select varieties
1.89 litre
$3.98
Nabob Tradition Coffee - select varieties
300 g
2 for $5.98
. . - ------------------------------------
10
More Grocery Specials...
Quaker Granola Bars or Dipps - select varieties
187-225 g
$2.28
General Mills Cereal-select varieties
385-525 g
2 for $7.00
Aunt Jemima Pancake Syrup - select varieties
750 ml
$2.98
Aunt Jemima Pancake Mix or Snackery-select varieties
9O5g- 1 kg
$2.68
DelMonte Fruit Cups-select varieties
4 pack
$2.98
Chef Boyardee Pasta Dinners-select varieties
212-425 g
3 for $3.00
Kraft Mayonnaise or Miracle Whip - select varieties
750 ml - 1 litre
$3.98
Dempster’s Original Bread - white or 100% whole wheat
675 g
$1.68
Dempster’s Hot Dog or Hamburger Buns
I2’s
$1.68
Lipton Sidekicks, Noodles or Rice & sauce - select varieties
120-142 g
3 for $3.99
Hunt’s Thick & Rich Pasta Sauce-select varieties
680 ml
3 for $4.98
Kraft Dinner
225 g
.78
Campbell’s Chunky Soup-select varieties
540 ml
$1.88
Puritan Flaked Ham, Turkey or Chicken
184 g
3 for $3.00
Swift Canned Ham
680 g
$4.98
Cadbuiy or Neilson Bars - select varieties
Singles
3 for $1.89
Cadbury or Neilson Bars-select varieties
Family Size
3 for $3.99
Tetley Iced Tea-select varieties
3 x 250 ml
2 for $3.00
Goodhost or Nestea Iced Tea Mix-select varieties
640 g - I kg
$4.48
DoritOS or FritOS Nachos - select varieties
300-370 g
2 for $5.00
Christie Snack Crackers - select varieties
185-300 g
2 for $5.00
Quaker Rice Cakes or Crispy Minis-select varieties
100-199 g 1
2 for $4.00
Pringles Potato Chips-select varieties
145 -170 g
$1.78
Rave Potato Chips - select varieties
170 g
2 for $3.00
Dare Cookies - select varieties
325-350 g
2 for $5.00
Kraft Handi Snacks - select varieties
87 g
3 for $4.98
Western Family Freeze Pops - kid s size
110 pack
$4.48Ill'I-
11
Market Fresh Produce!
Fresh Nectarines - u.s. grown
$l.94Ag
,88/lb
Fresh Broccoli - u.s. grown
$l.94Ag
.884b
Fresh Strawberries - California grown, #1 grade
1 lb. Package
$2.48 each
Premium Celery Stalks - Cal",oceanology,1053
,filtered,core,,2014-12-12 00:00:00,core,"6 who’s smart and who’s lucky? inferring trading strategy, learning and adaptation in financial markets through data mining",,"Summary. Trading “profits ” can be obtained by luck or by the implementation of a superior trading strategy. In this chapter we discuss the difficulties of distinguishing between the two. First, a suitable characterization of profit that distinguishes between trading gains and market gains is required. Secondly, one needs to be able to characterize trading “strategies”. To achieve this, we introduce the notion of a genotype-phenotype map to finance, where the genotype is as-sociated with the information set and associated decision rules that lead to a given set of trading decisions for a given trader, while the phenotype is described by the set of observable trading de-cisions themselves. In AI based systems, such as agent-based markets, a strategy is implemented algorithmically and so the genotype is explicitly known. In real markets however, the genotypic trading strategy of one trader is hidden from the rest. The phenotype however, is, in principle, observable. A microscopic description at the level of the set of individual trades, however, is not sufficient to understand or characterize the strategies at a more macroscopic and intuitive one. By introducing a set of coarse grained variables that can be used to classify strategy types, we show how these variables can then be data mined to understand what differs between an intelligent and a lucky strategy. We show that these variables can be used to distinguish between different strat-egy types and can be further used to infer the presence of learning and adaptation in the market. We illustrate all of the above using data from an experimental political market. 6.",finance,1054
,filtered,core,ДВНЗ «Приазовський державний технічний університет»,2020-02-20 00:00:00,core,дослідження використання методів розпізнавання обличчя людини в системах ідентифікації,,"Computer vision is used in recognition, identification, identification, analysis of images. But the disadvantage of advanced software is that it does not perceive the surrounding world as a person. Smart machines cannot yet see, but can learn, like the formation of neural connections inthe human brain. The relevance of the study of human face recognition methods is manifested due to the popularity of human image processing and the need to improve human interaction and technology.The result of the research is to determine the advantages and disadvantages of existing systems and methods, as well as simplifying the process of recognizing a person's face in images and increasing indicators when recognizing using the convolutional neural network method. The resulting system based on neural network methods makes a decision similarly to humans. To make a decision, this system needs information about the object, which is received at the entrance bytracking the special properties of the object. When a person is the object under investigation, the most special properties can be obtained by tracking his personality. In this case, the system has todeal with sometimes low-quality images, noise, angles of the head position, poor lighting and the like. Accuracy and speed criteria are factors in the success of a face recognition system. Theoriginal product shows improved recognition rates.Studies have shown the versatility of neural networks and their effectiveness in solving problems of facial recognition in real time and in photography. The system of automatic tracking and recognition of a person’s face using artificial intelligence and convolutional neural networks can be used at checkpoints, customs control, for identification in banking systems, government institutions, educational institutions (attendance control, registration of passers-by of unauthorized persons in the premises, identification personswhen writing control or examination papers).Компьютерное зрение применяется при распознавании, идентификации, выявлении, анализе изображения. Но недостатком продвинутого программного обеспечения является то, что оно не воспринимает окружающий мир как человек. Умные машины пока не могут видеть, но могут учиться, подобно формированию нейронных связей в мозгу человека. Актуальность исследования методов распознавания лица человека проявляется из-за популярности обработки изображения человека и необходимости улучшениявзаимодействия человека и технологий. Результатом исследований является определение преимуществ и недостатков существующих систем и методов, а также упрощение процесса распознавания лицачеловека на изображениях и повышение показателей при распознавании с помощью метода сверточных нейронных сетей. Полученная система на основе нейросетевых методовпринимает решение аналогично человеку. Для принятия решения данной системе необходима информация об объекте, которую получают на входе благодаря отслеживаниюособых свойств объекта. Когда исследуемым объектом является человек, наиболее особые свойства можно получить благодаря отслеживанию его личности. При этом системе приходится иметь дело с иногда некачественными изображениями, шумом, ракурсамиположения головы, плохим освещением и тому подобное.  Критерии точности и скорости являются факторами успешной работы системы распознавания лица. Исходный продукт показывает улучшенные показатели распознавания. Проведенные исследования показали универсальность нейронных сетей и их эффективность в решении задач распознавания лица человека в реальном времени и по фотографии. Система автоматического отслеживания и распознавания лица человека с использованием искусственного интеллекта и сверточных нейронных сетей может быть использована в контрольно-пропускных пунктах, таможенном контроле, дляидентификации в банковских системах, государственных  учреждениях, учебных заведениях (контроль посещаемости, учет проходимости посторонних лиц в помещениях, идентификация лица при написании контрольных или экзаменационных работ).Комп'ютерний зір застосовується при розпізнаванні, ідентифікації, виявленні, аналізу зображення. Але недоліком просунутого програмного забезпечення є те, що воно не сприймає навколишній світ як людина. Найрозумніші машини поки що не можуть бачити,але можуть навчатися, подібно формуванню нейронних зв’язків у мозку людини. Актуальність дослідження методів розпізнавання обличчя людини проявляється через популярність обробки зображення людини та необхідність покращення взаємодії людини і технологій. Результатом досліджень є визначення переваг та недоліків існуючих систем та методів, а також спрощення процесу розпізнавання обличчя людини на зображеннях та підвищення показників при розпізнаванні за допомогою методу згорткових нейронних мереж. Отримана система на основі нейромережевих методів приймає рішення аналогічно людині. Для прийняття рішення даній системі необхідна інформація про об’єкт, яку отримують на вході завдяки відстеженню особливих властивостей об’єкта. Коли досліджуваним об’єктом є людина, найбільш особливі властивості можна одержати завдяки відстеженню його особи. При цьому системі доводиться мати справу з інодінеякісними зображеннями, шумом, ракурсами положення голови, погане освітлення і тощо. Критерії точності та швидкості є факторами успішної роботи системи розпізнавання обличчя. Вихідний продукт показує поліпшені показники розпізнавання. Проведені дослідження показали універсальність нейронних мереж та їх ефективність у вирішенні задач розпізнавання обличчя людини у реальному часі та по фотографії. Систему автоматичного відстеження та розпізнавання обличчя людини із використанням штучного інтелекту та згорткових нейронних мереж може бути використано в контрольно-пропускних пунктах, митному контролі, для ідентифікації у банківських системах, державних установах, навчальних закладах (контроль відвідуваності,облік проходимості сторонніх осіб у приміщеннях, ідентифікація особи при написанні контрольних або екзаменаційних робіт)",finance,1055
,filtered,core,,2013-07-23 00:00:00,core,mobile code distributed systems a new development,,"Abstract: The paper presents an introduction in the Mobile Agents Systems and describes how this technology can be used in wireless applications. Also it is shown the possibility of securing wireless applications that use mobile agents and distributed computing. Wireless networks are a relatively new technology in the LAN market. With the weak encryption and security defined in the IEEE standards, wireless LANs, when improperly deployed or administered, can provide a significant risk to those economic sectors. These sectors include health-care, government, and banking in particular. Increasingly diverse heterogeneous wireless infrastructures in combination with more narrowly defined roles of parties participating in the delivery of applications to mobile users pose new challenges for support for delivering these applications. Key-Words: mobile agents, artificial intelligence, intelligent agents, wireless applications. 1. Mobile and intelligent agents Mobile software agents are a new concept used in distributed systems and this concept is based on human agents idea – real estate agent, travel agent. Figure 1 presents a new vision about intelligent agents",finance,1056
,filtered,core,,2018-05-23 00:00:00,core,matching de faces de diferentes domínios para sistemas de segurança bancário,,"Dissertação (mestrado)—Universidade de Brasília, Faculdade de Tecnologia, Departamento de Engenharia Elétrica, 2018.Um dos principais desafios enfrentados pelo sistema bancário é garantir a segurança das transações financeiras. Devido à conveniência e aceitação, o uso de caracterı́sticas faciais para autenticação biométrica de usuários em sistemas bancários está se tornando uma tendência mundial. Essa abordagem de autenticação de usuários está atraindo grandes investimentos de instituições bancárias e financeiras, especialmente em cenários de diferentes domı́nios, nos quais imagens faciais tiradas de documentos de identificação são comparadas com autorretratos digitais (selfies) tiradas com câmeras de dispositivos móveis. Neste estudo, coletamos das bases de dados do maior banco público brasileiro um grande dataset, chamado FaceBank, com 27.002 imagens de selfies e fotos de documentos de identificação de 13.501 sujeitos. Em seguida, avaliamos os desempenhos de dois modelos de Redes Neurais Convolucionais bem referenciados (VGG-Face e OpenFace) para extração de caracterı́sticas profundas, bem como os desempenhos de quatro classificadores (SVM Linear, SVM Power Mean, Random Forest e Random Forest com o Ensemble Vote) para autenticação robusta de face em diferentes domı́nios. Com base nos resultados obtidos (precisões superiores a 90%, em geral), é possı́vel concluir que a abordagem de matching de faces profundas avaliada neste estudo é adequada para autenticação de usuários em aplicações bancárias entre domı́nios. Até onde sabemos, este é o primeiro trabalho que usa um grande conjunto de dados composto por imagens bancárias reais para avaliar a abordagem de autenticação de face entre domı́nios. Além disso, este trabalho apresenta um estudo sobre as reais necessidades na implementação futura de um sistema biométrico, propondo um sistema de nuvem para permitir a adoção de tecnologias biométricas. Por fim, propõe também um modelo seguro e integrado de subsistema ABIS de transmissão de dados. Toda a análise e implementação leva em conta a total aderência e compatibilidade com padrões e especificações propostos pelo governo brasileiro.Ensuring the security of transactions is currently one of the major challenges facing banking systems. The use of facial features for biometric authentication of users in banking systems is becoming a worldwide trend, due to the convenience and acceptability of this form of identification, and also because computers and mobile devices already have built-in cameras. This user authentication approach is attracting large investments from banking and financial institutions especially in cross-domain scenarios, in which facial images taken from ID documents are compared with digital self-portraits (selfies) taken with mobile device cameras. In this study, from the databases of the largest public Brazilian bank we collected a large dataset, called FaceBank, with 27,002 images of selfies and ID document photos from 13,501 subjects. Then, we assessed the performances of two well-referenced Convolutional Neural Networks models (VGG-Face and OpenFace) for deep face features extraction, as well as the performances of four effective classifiers (Linear SVM, Power Mean SVM, Random Forest and Random Forest with Ensemble Vote) for robust cross-domain face authentication. Based on the results obtained (authentication accuracies higher than 90%, in general), it is possible to conclude that the deep face matching approach assessed in this study is suitable for user authentication in cross-domain banking applications. To the best of our knowledge, this is the first study that uses a large dataset composed of real banking images to assess the cross-domain face authentication approach to be used in banking systems. As an additional, this work presents a study on the real needs in the future implementation of a biometric system proposing a cloud system to enable the adoption of biometrics technologies, creating a new model of service delivery. Besides that, proposes a secure and integrated ABIS Data Transmission subsystem model. All the analysis and implementation takes into account the total adherence and compatibility with the standards and specifications proposed by the Brazilian government, at the same time, establish mechanisms and controls to ensure the effective protection of data",finance,1057
,filtered,core,,2010-10-06 00:00:00,core,policy iteration for learning an exercise policy for american options,,"Abstract. Options are important financial instruments, whose prices are usually determined by computational methods. Computational finance is a compelling application area for reinforcement learning research, where hard sequential decision making problems abound and have great practical significance. In this paper, we investigate reinforcement learning methods, in particular, least squares policy iteration (LSPI), for the problem of learning an exercise policy for American options. We also investigate TVR, another policy iteration method. We compare LSPI, TVR with LSM, the standard least squares Monte Carlo method from the finance community. We evaluate their performance on both real and synthetic data. The results show that the exercise policies discovered by LSPI and TVR gain larger payoffs than those discovered by LSM, on both real and synthetic data. Furthermore, for LSPI, TVR and LSM, policies learned from real data generally gain larger payoffs than policies learned from simulated samples. Our work shows that solution methods developed in reinforcement learning can advance the state of the art in an important and challenging application area, and demonstrates furthermore that computational finance remains an under-explored area for deployment of reinforcement learning methods. ",finance,1058
,filtered,core,,2021-08-14 00:00:00,core,blockchain for securing ai applications and open innovations,,"Nowadays, open innovations such as intelligent automation and digitalization are being adopted by every industry with the help of powerful technology such as Artificial Intelligence (AI). This evolution drives systematic running processes, involves less overhead of managerial activities and increased production rate. However, it also gave birth to different kinds of attacks and security issues at the data storage level and process level. The real-life implementation of such AI-enabled intelligent systems is currently plagued by the lack of security and trust levels in system predictions. Blockchain is a prevailing technology that can help to alleviate the security risks of AI applications. These two technologies are complementing each other as Blockchain can mitigate vulnerabilities in AI, and AI can improve the performance of Blockchain. Many studies are currently being conducted on the applicability of Blockchains for securing intelligent applications in various crucial domains such as healthcare, finance, energy, government, and defense. However, this domain lacks a systematic study that can offer an overarching view of research activities currently going on in applying Blockchains for securing AI-based systems and improving their robustness. This paper presents a bibliometric and literature analysis of how Blockchain provides a security blanket to AI-based systems. Two well-known research databases (Scopus and Web of Science) have been examined for this analytical study and review. The research uncovered that idea proposals in conferences and some articles published in journals make a major contribution. However, there is still a lot of research work to be done to implement real and stable Blockchain-based AI systems",finance,1059
,filtered,core,ConTexto,2009-10-26 00:00:00,core,use of artificial neural networks for estimations of rates on bovespa petrobras pn,,"A crescente sofisticação das operações no mercado financeiro aumentou muito a exposição do risco de algumas operações. Isso requer a utilização de tecnologias avançadas para modelar e estimar a série de preços dos ativos. Uma das abordagens que vem ganhando importância na modelagem de preços e de volatilidade é a de Redes Neurais Artificiais (RNA). O presente artigo tem por objetivo estimar, por meio de RNA, os preços para as ações da Petrobrás PN, utilizando uma série de preços diários compreendida no período de 2 de janeiro de 2001 até 9 de maio de 2008, representando 1821 observações diárias. Após a estruturação e o treinamento da RNA no software Matlab 7.6.0. R2008a, calculou-se o erro quadrado médio (RMSE) e estimaram-se as cotações da Petrobrás PN, comparando as estimações com os preços efetivamente ocorridos. Os resultados obtidos pela estimação, quando comparados com a cotação efetiva da Petrobrás para os dias seguintes, demonstraram um alto grau de aderência do modelo com RNA para períodos curtos de estimação. No entanto, esses resultados merecem outros testes com janelas de tempo diferentes, incluindo momentos de maior stress nos preços, e com outras ações menos aderentes à carteira de mercado, visando aumentar o nível confiabilidade do modelo.The increasing sophistication of some operations in the finance market had increased a lot the exposure of the risk from some operations. So, it requests the use of some advanced technologies to model and to esteem series of asset prices. One of the approaches that are receiving importance in the modeling of prices and volatility is the Artificial Neural Networks. The main purpose of this article is to esteem, through ANN, the prices of Petrobrás PN using a series of daily prices between January 2, 2001 and May 9, 2008. This series represents 1821 daily observations. After the ANN was modeled in the software Matlab 7.6.0. R2008a, it was calculated the RMSE, estimated the Petrobrás PN prices and compared the estimated and real prices of the selected asset. The conclusion was that, comparing the estimated prices to the real prices of Petrobrás in the analyzed period, there is a high degree of adherence of the model to ANN in short term. However these studies could have other tests where they will consider different windows of time, including moments with huge stress in the prices and others with fewer adherences in the market, aiming to increase the reliability level of the model",finance,1060
,filtered,core,,2007-11-22 00:00:00,core,(al) and an application program (known as an,,"The development and deployment of multi-agent systems in real world settings raises a number of important research issues and problems which must be overcome if Distributed AI (DAI) is to become a widespread solution technology. Work undertaken in the context of the ARCHON project has provided a number of important insights into these issues. By providing an in depth analysis of ARCHON&apos;s electricity transportation management application, this paper draws together many of the experiences obtained when building one of the world&apos;s first operational DAI systems.  Introduction  In many industrial applications a substantial amount of time, effort and finance has been devoted to developing complex and sophisticated software systems. These systems are often viewed in a piecemeal manner as isolated islands of automation, when, in reality, they should be seen as components of a much larger business function (Jennings, 1994a). The main benefit of taking a holistic perspective is that the partial ..",finance,1061
,filtered,core,UW Tacoma Digital Commons,2018-01-01 00:00:00,core,exact and consistent interpretation for piecewise linear neural networks: a closed form solution,,"Strong intelligent machines powered by deep neural networks are increasingly deployed as black boxes to make decisions in risk-sensitive domains, such as finance and medical. To reduce potential risk and build trust with users, it is critical to interpret how such machines make their decisions. Existing works interpret a pre-trained neural network by analyzing hidden neurons, mimicking pre-trained models or approximating local predictions. However, these methods do not provide a guarantee on the exactness and consistency of their interpretations. In this paper, we propose an elegant closed form solution named $OpenBox$ to compute exact and consistent interpretations for the family of Piecewise Linear Neural Networks (PLNN). The major idea is to first transform a PLNN into a mathematically equivalent set of linear classifiers, then interpret each linear classifier by the features that dominate its prediction. We further apply $OpenBox$ to demonstrate the effectiveness of non-negative and sparse constraints on improving the interpretability of PLNNs. The extensive experiments on both synthetic and real world data sets clearly demonstrate the exactness and consistency of our interpretation",finance,1062
,filtered,core,,2012-10-25 00:00:00,core,desenvolvimento de um modelo de sistema multiagente para previsão de retorno sobre indices de ações,https://core.ac.uk/download/30375150.pdf,"Dissertação (mestrado) - Universidade Federal de Santa Catarina, Centro Tecnológico, Programa de Pós-Graduação em Ciência da Computação, Florianópolis, 2010No mundo das finanças, a Teoria dos Mercados Eficientes (TME) afirma que a flutuação do preço dos ativos financeiros é aleatória, sendo assim, não existem maneiras de proteger o investidor, prevendo os futuros movimentos do mercado. Contudo, várias iniciativas empíricas têm demonstrado que a afirmação da TME não é totalmente correta. Entre as frentes de pesquisa que buscam prever os movimentos de ativos financeiros, pode-se destacar a área com um ponto de vista econométrico, que tenta prever movimentos mediante métodos matemáticos e estatísticos, como regressão linear e regressão não linear, bem como as redes neurais. Além disso, em outra área de pesquisa, há a Teoria Multiagente de Modelagem de Mercado (Theory of Multi-Agent Market Modeling) que foca sua atenção na microestrutura do mercado, partindo do princípio de que os movimentos de preços emergem da interação de muitos agentes individuais do mercado. Contudo, esses modelos financeiros baseados em agentes têm algumas limitações. Não é possível adequar os agentes a dados reais do mercado para gerar previsões futuras, pois na maioria dos modelos a tomada de decisão é feita por meio de funções ad-hoc ou mecanismos que não podem ser ajustados a dados externos. Para obter uma previsão real do modelo, é preciso adaptar o mecanismo de decisão dos agentes com modelos econométricos, como as redes neurais, que podem ser ajustadas a séries de dados reais. Dessa forma, por intermédio da interação dos agentes, o modelo de mercado resultante pode capturar a dinâmica oculta do mercado e prever os movimentos futuros com uma eficácia maior do que faria um sistema de regressão não linear isoladamente. Este trabalho propõe um modelo computacional baseado na utilização do comportamento emergente de uma comunidade de agentes de software com mecanismos de decisão cognitiva, baseados em redes neurais, com o objetivo de realizar previsões do comportamento do Índice da Bolsa de Valores de São Paulo # Índice Bovespa. Os agentes da comunidade interagem com um mecanismo de coleta dos valores das 65 ações mais negociadas da Bovespa que compõem o Índice Bovespa, utilizados para computar as previsões de evolução do índice. A fim de validar a hipótese da pesquisa, é feita uma comparação entre os resultados obtidos pelo modelo implementado, com uma abordagem tradicional de previsão baseada exclusivamente em redes neurais. Os resultados obtidos demonstram que as previsões do modelo proposto são mais performáticas do que a previsão isolada do índice baseada exclusivamente em redes neurais, já que o modelo proposto captura melhor a microestrutura do mercado, prevendo um passo a frente de maneira mais eficaz.In finance world , the efficient-market hypothesis (EMH) states that the financial assets have random price fluctuation, so, there are not ways to protect the investor forecasting the future market movements. However, several initiatives have shown that empirical assertion of EMH is not entirely correct. Among the research fronts that seek to predict the movements of financial assets, we can highlight the area with an econometric point of view, which attempts to predict movements through mathematical and statistical methods such as linear regression and nonlinear regression, as well as neural networks. Moreover, in another area of research, we have the Theory of Multi-Agent Market Modeling that focuses attention on the micro-structure of the market, assuming that the price movements emerge from the interaction of many actors in the market. However, these financial models based on agents have some limitations. It is not possible to match agents to actual market data to forecast a step ahead, as in most models of decision making is done through ad-hoc functions or mechanisms that can not be adjusted to external data. For a preview of the real model, you need to adapt the decision-making mechanism of the agents with econometric models such as neural networks, which can be adjusted to actual data sets. Thus, through the interaction of the agents, the market model results can capture the hidden dynamics of the market and predict future movements with greater effectiveness than would a system of non-linear regression alone. This paper proposes a computational model based on the use of emergent behavior of a community of software agents with cognitive decision-making mechanisms base on neural networks in order to make predictions of the behavior of the index of He Stock Exchange of São Paulo # Bovespa Index. The agents of the community interact with one engine collects the values of the 65 most actively traded stocks that comprise the Bovespa Bovespa Index, used to compute the projected trend of the index. In order to validate the hypothesis of the study, a comparison is made between the results obtained by the model implemented with a traditional approach to forecasting based solely on neural networks. The results show that by the proposed model are more performing than the forecast index alone based solely on neural networks, since the proposed model better captures the microstructure of the market, and one step ahead more effectively",finance,1063
,filtered,core,Kluwer Academic Publishers,1998-01-01 00:00:00,core,analysing rule sets for the calculation of banking fees by a theorem prover with constraints,,"Theorem proving, logic programming and constraint solving can be combined in a straightforward manner. This is shown not only by setting up a theoretical framework, but also by a real world application: the calculation of banking fees. We tackle the problems of deciding whether such a rule set is total and deterministic, i.e. does it permit calculation of a fee for every business deal, and can only one fee be computed in each case. Although these questions are undecidable in general, the restricted form of the investigated rule sets makes them decidable and even tractable in practise. Experiences with our system implemented in Prolog as well as some questions one may have with the application are discussed.  Keywords: banking applications; constraint logic programming; finite domain constraints; theorem proving. 1 Introduction  One of the most traditional disciplines in artificial intelligence research is theorem proving. In the early days, it was concentrated mainly on developing gene..",finance,1064
,filtered,core,Sudan University of Science and Technology,2020-01-03 00:00:00,core,a type-2 fuzzy logic based system for decision support to minimize financial default in the sudanese banking sector,https://core.ac.uk/download/323246048.pdf,"The recent global financial-economic crisis has led to the collapse of several companies from all over the world. This has created the need for powerful frameworks which can predict and reduce the potential risks in financial applications. Such frameworks help organizations to enhance their services quality and productivity as well as reducing the financial risk. The widely used techniques to build predictive models in the financial sector are based on statistical regression, which is deployed in many financial applications such as risk forecasting, customers’ loan default and fraud detection. However, in the last few years, the use of Artificial Intelligence (AI) techniques has increased in many financial institutions because they can provide powerful predictive models. However, the vast majority of the existing AI techniques employ black box models like Support Vector Machine (SVMs) and Neural Network (NNs) which are not able to give clear and transparent reasoning to explain the extracted decision. However, nowadays transparent reasoning models are highly needed for financial applications. This paper presents a type-2 fuzzy logic system for predicting default in financial systems. the researchers used a real dataset collected from the banking sector in Sudan. The proposed system resulted in transparent outputs which could be easily understood, analyzed and augmented by the human stakeholders. Besides, the proposed system resulted in an average recall of 83.5%, which outperformed its type-1 counterpart by 20.66%",finance,1065
,filtered,core,IOP Publishing Ltd,2019-06-17 00:00:00,core,utilizing big data for enhancing passenger safety in railway stations,https://core.ac.uk/download/226755775.pdf,"In light of the increasing demand and capacity in the railway industry, it is imperative to maintain safety in relation to the complexities of the substantial railway stations. Thus, it is important to take note of the time where investments in new technologies directed at the safety of the railway enable safety and protection in this area. Novel technological techniques such as big data analysis (BDA), data mining or machine learning (ML) have been developed and applied in many areas such as sales, banking and healthcare. The development of such methods has important benefits within the context of railway safety, however, these new methods need to be implemented and developed with consideration of whether these operational models can help to solve the various difficulties that currently exist in the risk analysis of railway stations. Moreover, as the adoption of the Internet of thing (IoT) grows, it is expected that analytical needs for handling data will also increase. It has been shown that the progression towards automation and applying such innovative new technologies such as BDA may be a powerful tool for integration in the future of transportation in general and the railway industry in particular, whereby analytical predictions can aid in the development of safer railway stations which have greater potential for ensuring the safety of passengers. In this paper a Bow Tie (BT) framework model has been created to combine BDA into the risk assessment process. The BDA can be beneficial to the risk assessment, support the decision makers in real time, and reduce human errors. This method can be fully integrated into passenger data and the business model for the railway station. Employing the existing safety records utilizing BDA is expected to mitigate risks, predict hazards, raise safety and security efficiency and reduce the cost",finance,1066
,filtered,core,ScholarWorks@UMass Amherst,2015-11-09 00:00:00,core,physically equivalent intelligent systems for reasoning under uncertainty at nanoscale,https://core.ac.uk/download/32441784.pdf,"Machines today lack the inherent ability to reason and make decisions, or operate in the presence of uncertainty. Machine-learning methods such as Bayesian Networks (BNs) are widely acknowledged for their ability to uncover relationships and generate causal models for complex interactions. However, their massive computational requirement, when implemented on conventional computers, hinders their usefulness in many critical problem areas e.g., genetic basis of diseases, macro finance, text classification, environment monitoring, etc. We propose a new non-von Neumann technology framework purposefully architected across all layers for solving these problems efficiently through physical equivalence, enabled by emerging nanotechnology. The architecture builds on a probabilistic information representation and multi-domain mixed-signal circuit style, and is tightly coupled to a nanoscale physical layer that spans magnetic and electrical domains. Based on bottom-up device-circuit-architecture simulations, we show up to four orders of magnitude performance improvement (using computational resolution of 0.1) vs. best-of-breed multi-core machines with 100 processors, for BNs with about a million variables. Smaller problem sizes of ~100 variables can be realized at 20 mW power consumption and very low area around a few tenths of a mm2. Our vision is to enable solving complex Bayesian problems in real time, as well as enable intelligence capabilities at a small scale everywhere, ushering in a new era of machine intelligence",finance,1067
,filtered,core,,2018-01-01 00:00:00,core,how can kbc securities and kbc markets stay relevant in the future?,,"In chapter 1, an introduction to KBC Securities and KBC Markets is provided. It is mentioned that KBC Securities plays a significant role in the Benelux capital markets and has been rewarded with the title of Equity Finance House of the Year and Cash Market Brokerage House of the Year in 2017. Nevertheless, KBC Securities and KBC Markets operate in a changing environment and are facing a series of challenges: new regulation, advances in innovative technology, strong competitors, new entrants and higher demanding clients. As a result, the following research question was formulated: ""How can KBC Securities and KBC Markets stay relevant in the future?"". Additionally, we researched several subquestions such as: ""What are the reasons clients (do not) choose for KBC Securities? What activities do clients expect and want, now and in the future? Are KBC Securities clients satisfied with the service level they receive? How does KBC Securities' client experience compare to competitors?"". In chapter 2, an overview of the selected methodologies is given in order to tackle the research question. This study is exploratory and has an inductive approach using qualitative research in order to fulfil the purpose of the paper. Semi-structured interviews with clients, non-clients, industry experts, academics and employees were chosen as a way to gather empirical data for research. An overview of the interview questions can be found in this chapter. Additionally, we supplemented our findings using secondary research: journals, books, articles and working papers conducted by academics and global professional management firms. In chapter 3, several conceptual strategy frameworks are presented which are used to tackle the research question in a structured way. First, a strategic framework of Bain & Company is given which involves making deliberate choices in three areas: ambition, where-to-play and how-to-win. The main focus of this paper will be on the ""how-to-win"" question. Second, the three value disciplines of Treacy & Wiersema are used to answer the ""how-to-win"" question and it is concluded that ""customer intimacy"" is the appropriate strategy for KBC Securities. This strategy focuses on offering unique customer services that allow tailored solutions to meet different customer demand. Companies who pursue this strategy bundle services and products into a solution designed specifically for the customer's problem. As a result of this strategy, KBC Securities will face a new key challenge: switch from being product-centric (the so called 'product-factory) to being client-centric. Thirdly, a framework of prof. dr. Kurt Verweire is used to implement customer intimacy based on seven building blocks: (1) strategic positioning, (2) customer relationship management (CRM), (3) client segmentation, (4) client profitability, (5) key account management, (6) tracking client satisfaction and (7) a dynamic and proactive approach. In chapter 4, the paper elaborates on the seven building blocks that are needed to achieve customer intimacy: (1) Strategic positioning: KBC Securities should position itself as a local specialist. A local specialist is an investment bank with deep capabilities in individual markets or regions, and strong local knowledge, networks and connectivity. (2) Customer relationship management: In context of achieving customer intimacy, CRM systems are indispensable. Two elements are essential to successfully implement CRM, namely CRM data collection and CRM data usage. The benefits of a good CRM system are listed and several recommendations for KBC Securities are given. (3) Client segmentation: We suggest implementing a value-driven segmentation system based on the client's current value, potential value and behaviour. This results in three client segments: priority, specialised and self service. For each segment, a distinct differentiation approach and clear rules for coverage structure, content access and execution and platform capabilities should be developed. (4) Client profitability: Client relationships should be managed in terms of profitability, not revenues. KBC Securities should consider both client revenues as well the cost-to-serve these clients. (5) Key Account Management (KAM): KAM is an approach to act as a real business partner for clients. This enables to build loyalty, customer commitment and give the client a single point of contact. We believe an appropriate person to fulfil the key account manager's position is the senior corporate banker. Another option would be to develop the function of a key account manager in-house at KBC Securities. This person could then join client meetings with the corporate banker and proactively approach clients of KBC Securities to identify business opportunities. (6) Tracking client satisfaction: We believe it is essential for KBC Securities to track client satisfaction more rigorously. This happens in three simple stages: gather input data, analyse the input data and adjust the services based on the outcome of this analysis. Internally, senior management's involvement and the documenting of outputs are important. (7) Dynamic and proactive approach: Based on client interviews, it seems that KBC Securities sometimes lacks dynamism and proactivity in its business approach when compared to competitors. We elaborate on this concept and emphasise the importance of staying sharp in every business interaction. In chapter 5, we elaborate on some additional findings which are relevant for KBC Securities, but not directly related to implementing customer intimacy. More specifically, it covers the following topics: innovation & technology and some suggestions for starting activities. Concerning innovation and technology, we identify some critical areas where IT investments should be focused on: dealing with legacy IT and customer-centric solutions. We also elaborate on the implementation of a 'Technology Plan' which is based on three elements: a budget, a time-period and an 'IT Innovation Officer'. The IT Innovation Officer should be responsible for developing a cohesive innovation and technology strategy for KBC Securities. Related to the suggestions for starting activities, we elaborate on (1) starting in-house private equity activities, (2) cross-selling in KBC Securities, (3) the organisational set-up of the corporate finance team and (4) convertible bonds. (1) Starting of in-house private equity activities: Having in-house private equity activities might be beneficial for KBC Securities, as well as for KBC bank. Potential advantages could be the realisation of informational synergies, cross-selling opportunities, better serving of the upper wealth management clients, reducing agency problems and a better brand image. Additionally, it would be in line with KBC Securities' vision of serving companies in every stage of their lifecycle. (2) Cross-selling in KBC Securities: We focus on how private, corporate and investment bankers should work together to generate more revenues as a whole. We elaborate on the importance of knowledge training, good incentive systems, alignment of every party involved, proper internal communication systems and the support of senior management to effectively realise cross-selling opportunities within the bank. (3) Organisational set-up of the corporate finance team: For the reason that ECM activities and M&A activities serve different type of clients, require different skills and networks to grow, we believe (after performing research and conducting interviews with industry experts) that it would be better to split up the corporate finance team into two separate teams: an ECM team and a M&A team. (4) Convertible bonds: As KBC Securities currently has no in-house expertise regarding convertible bonds, we want to encourage KBC Securities' management to consider if it is worth starting this activity. Chapter 6 is the conclusion of the In-Company Project. Moreover, the research question which was presented in chapter 1, is answered. KBC Securities should implement customer intimacy as a strategy in order to stay relevant. Furthermore, the following limitations of this project were discussed in this chapter, namely: the limited number of conducted interviews, the limited amount of available academic research regarding the research question and the time constraint. We emphasised that this project is a purely strategic exercise (formulation), with limited focus on strategy execution. As a result, compliance, risk, regulation and operation-related matters are out of the scope of this project. This chapter finishes with some actions for KBC Securities going forward. The formulation of a sound strategy, as presented in this report, was only a first step in becoming a customer intimate bank that puts the client at the centre of everything it does. However, there still is a long way to go. Next steps for KBC Securities should focus on the actual execution and implementation of this strategy across the investment bank, with impact on its day-to-day operations. In chapter 7, additional research is conducted on the sell-side research function. MiFID II will have a major impact on sell-side research firms. Buy-side firms will become much more selective in deciding which research reports to buy and the budgets for research will likely decline, which means quality becomes the key differentiator. Additionally, competition is expected to increase. We conclude that KBC Securities should position itself as a regional champion where they should leverage their home field advantage in the Benelux to compete with other sell-side firms covering the same stocks. Lastly, we conclude that artificial intelligence and technology will be important in the future and that research analysts should focus more on client-facing activities instead of performing basic written research. KBC Securities should take actions to make sure analysts' time is spent on activities that are valued by the client and activities that the client wants to pay for, instead of spending most of their time on less-valued activities",finance,1068
,filtered,core,HAL CCSD,2003-09-01 00:00:00,core,two expert diagnosis systems for smes: from database- only technologies to the unavoidable addition of ai techniques,,"International audienceIn this application-oriented paper, we describe two expert diagnosis systems we have developed for SMEs. Both systems are fully implemented and operational, and both have been put to use on data from actual SMEs. Although both systems are packed with knowledge and expertise on SMEs, neither has been implemented with AI techniques. We explain why and how both systems relate to knowledge-based and expert systems. We identify aspects of both systems that will benefit from the addition of AI techniques in future developments. 1. Expertise for Small and Medium-sized Enterprises (SMEs) The work we describe here takes place within the context of the Research Institute for SMEs 1. The specific lab in which we have conducted the research projects we refer to in this paper is the LaRePE 2 (LAboratoire de REcherche sur la Performance des Entreprises). This lab is mainly concerned with the development of scientific expertise on the study and modeling of SMEs' performance, including a variety of interrelated subjects such as finance, management , information systems, production, technology, etc. The vast majority of research projects carried out at the LaRePE involves both theoretical and practical aspects, often necessitating in-field studies with SMEs. As a result, our research projects always attempt to provide practical solutions to real problems confronting SMEs. In this application-oriented paper we briefly describe two expert diagnosis systems we have developed for SMEs. Both can be considered as decision support systems (Turban and Aronson, 2001). The first is the PDG system: a benchmarking software that evaluates production and management activities, and the results of these activities in terms of productivity, profitability, vulnerability and efficiency. The second is the eRisC system: a software that helps identify, measure and manage the main risk factors that could compromise the success of SME development projects. Both systems are fully implemented and operational. Moreover, both have been put to use on data from actual SMEs. What is of particular interest here, especially from a knowledge-based systems perspective, is the fact that although both the PDG and the eRisC systems are packed with knowledge and 1 The core mission of the Institute (http://www.uqtr.ca/inrpme/anglais/index.html), which supports fundamental and applied research, is to foster the advancement of knowledge on SMEs to contribute to their development. ",finance,1069
,filtered,core,McGraw-Hill Education,2015-01-01 00:00:00,core,modellizzazione e gestione dei rischi finanziari attraverso un approccio di portafoglio,,"Questo volume vuole evidenziare l’importanza di una modellizzazione

dei rischi bancari, in particolare di quelli associati all’attività di

credito e di negoziazione in titoli da parte della banca, che sia coerente

con le caratteristiche empiriche dei dati finanziari che, attualmente, non

sembrano confermare l’ipotesi tradizionale di normalità dei rendimenti.

Se la validità di un modello finanziario poggia sulla veridicità delle sue

ipotesi sottostanti, è anche vero che il contributo operativo di un modello

teorico è legato alla sua facilità d’implementazione e al suo basso sforzo

computazionale. Nella prima parte del volume si presentano, quindi,

dei modelli avanzati ma, allo stesso tempo, semplici da implementare

finalizzati ad una stima più realistica e “coerente” dell’ammontare dei

rischi di credito e di mercato dell’attività bancaria; nella seconda parte del

volume si descrivono dei modelli di gestione degli stessi rischi basati su un

approccio di portafoglio in cui le soluzioni del problema di minimizzazione

del rischio possono suggerire alla banca come riallocare coerentemente

il proprio capitale tra le diverse attività migliorando allo stesso tempo il

proprio profilo di rischio e rendimento. Successivamente questi modelli

sono stati implementati ai dati finanziari, reali e simulati, per testare il

grado di performance degli stessi in un’ottica comparativa con i modelli

più tradizionali utilizzati dall’industria finanziaria internazionale.This book aims to highlight the importance of a banking risk modeling, in particular of the financial risks associated with the activity of lending and trading in securities from the bank, which is consistent with the empirical characteristics of financial data that currently do not seem to confirm the traditional hypothesis of normality distribution. If the validity of a financial model is based on the truthfulness of his assumptions, it is also true that its operational contribution is related to the ease of implementation and its low computational efforts.

Therefore, in the first part of this volume we present some advanced models but, at the same time, simple to implement aimed at a more realistic and “coherent” estimate of the amount of credit and market risks of the banking activity. In the second part of this volume, we describe some advanced risk management models based on a portfolio approach. Le solutions of the portfolio risk minimization problem may suggest to the banking managers how consistently reallocate the total capital among the various financial assets improving the banking risk-return profile. Subsequently these same models have been implemented to financial data, real and simulated, in order to test the degree of performance of these models in a comparative perspective with some more traditional approaches used currently by the international financial industry.

In conclusion, we find in our experiments that the CVaR measure is a risk metric superior to the traditional VaR, overall in a perspective of financial risk management. In fact, only the CVaR is able to taking into account the diversification effects in terms of portfolio risk mitigation. In addition, we underline the utility of using copula functions for modeling the dependence structure among the assets in portfolio. In particular, this is true in the case of the Student’s t-copulas that by means of the tail dependence property are able to capture the “extreme” events of joint defaults. We think that the adoption of these new mathematical tools and of the advanced models based on these properties may enforce the grade of stability of the banking sector",finance,1070
,filtered,core,Georgia Institute of Technology,2021-01-11 00:00:00,core,evaluating the effects of model simplifications on the transference of policies learned in simulation,https://core.ac.uk/download/395002254.pdf,"Both the military and civil worlds are being transformed by the development and deployment of unmanned systems to a wider range of scenarios. As the field of unmanned systems has grown and matured, it has continuously advanced towards increasing levels of autonomy. As an example, the cars of today have gone from ""maintain this speed"" to ""drive on this highway."" As this push towards greater levels of autonomy continues, new methods for developing policies for control of these systems are required. Recent breakthroughs in reinforcement learning hope to address this problem. The primary advantage of reinforcement learning based systems is their focus on goal driven behavior. In developing reinforcement learning based policies, there is a need for significant exploration of a system’s possible state-action space. As such, modeling and simulation has been an indispensable tool. However, transferring policies from a simulated world to the real world presents its own challenges. This work develops a method for evaluating the relative importance of different possible simplifications that can be taken during the modeling process. This relies on a sampling-based method to explore the possible simplification space of a given referent model. Experiments show that this method compares favorably to a number of baseline model development strategies and can lead to significantly simplified system models that maintain similar transference properties. Additional experiments are presented that evaluate different choices within this sampling-based strategy and the effects of imperfect referent models on the resulting evaluations.Ph.D",space,1071
,filtered,core,DigitalCommons@USU,2021-08-09 00:00:00,core,an ai-based goal-oriented agent for advanced on-board automation,https://core.ac.uk/download/478906302.pdf,"In the context of fierce competition arising in the space economy, the number of satellites and constellations that will be placed in orbit is set to increase considerably in the upcoming years. In such a dynamic environment, raising the autonomy level of the next space missions is key to maintaining a competitive edge in terms of the scientific, technological, and commercial outcome.
We propose the adoption of an AI-based autonomous agent aiming to fully enable spacecraft’s goal-oriented autonomy. The implemented cognitive architecture collects input starting from the sensing of the surrounding operating environment and defines a low-level schedule of tasks that will be carried out throughout the specified horizon. Furthermore, the agent provides a planner module designed to find optimal solutions that maximize the outcome of the pursued objective goal. The autonomous loop is closed by comparing the expected outcome of these scheduled tasks against the real environment measurements.
The entire algorithmic pipeline was tested in a simulated operational environment, specifically developed for replicating inputs and resources relative to Earth Observation missions. The autonomous reasoning agent was evaluated against the classical, non-autonomous, mission control approach, considering both the quantity and the quality of collected observation data in addition to the quantity of the observation opportunities exploited throughout the simulation time. The preliminary simulation results point out that the adoption of our software agent enhances dramatically the effectiveness of the entire mission, increasing and optimizing in-orbit activities, on the one hand, reducing events\u27 response latency (opportunities, failures, malfunctioning, etc.) on the other.
In the presentation, we will cover the description of the high-level algorithmic structure of the proposed goal-oriented reasoning model, as well as a brief explanation of each internal module’s contribution to the overall agent’s architecture. Besides, an overview of the parameters processed as input and the expected algorithms\u27 output will be provided, to contextualize the placement of the proposed solution. Finally, an Earth Observation use case will be used as the benchmark to test the performances of the proposed approach against the classical one, highlighting promising conclusions regarding our autonomous agent’s adoption",space,1072
,filtered,core,ScholarlyCommons,2021-01-01 00:00:00,core,scalable learning in distributed robot teams,https://core.ac.uk/download/479136994.pdf,"Mobile robots are already in use for mapping, agriculture, entertainment, and the delivery of goods and people. As robotic systems continue to become more affordable, large numbers of mobile robots may be deployed concurrently to accomplish tasks faster and more efficiently. Practical deployments of very large teams will require scalable algorithms to enable the distributed cooperation of autonomous agents. This thesis focuses on the three main algorithmic obstacles to the scalability of robot teams: coordination, control, and communication. To address these challenges, we design graph-based abstractions that allow us to apply Graph Neural Networks (GNNs).First, a team of robots must continually coordinate to divide up mission requirements among all agents. We focus on the case studies of exploration and coverage to develop a spatial GNN controller that can coordinate a team of dozens of agents as they visit thousands of landmarks. A routing problem of this size is intractable for existing optimization-based approaches. Second, a robot in a team must be able to execute the trajectory that will accomplish its given sub-task. In large teams with high densities of robots, planning and execution of safe, collision-free trajectories requires the joint optimization over all agent trajectories, which may be impractical in large teams. We present two approaches to scalable control: a) a controller for flocking that uses delayed communication formalized via a GNN; and b) an inverse optimal planning method that learns from real air traffic data. Third, robot teams may need to operate in harsh environments without existing communication infrastructure, requiring the formation of ad-hoc networks to exchange information. Many algorithms for control of multi-robot teams operate under the assumption that low-latency, global state information necessary to coordinate agent actions can readily be disseminated among the team. Our approach leverages GNNs to control the connectivity within the ad-hoc network and to provide the data distribution infrastructure necessary for countless multi-robot algorithms. Finally, this thesis develops a framework for distributed learning to be used when centralized information is unavailable during training. Our approach allows robots to train controllers independently and then share their experiences by composing multiple models represented in a Reproducing Kernel Hilbert Space",space,1073
,filtered,core,HAL CCSD,2021-01-28 00:00:00,core,traffic refinery: cost-aware traffic representation for machine learning in networks,,"Ever more frequently network management tasks apply machine learning on network traffic. Both the accuracy of a machine learning model and its effectiveness in practice ultimately depend on the representation of raw network traffic as features. Often, the representation of the traffic is as important as the choice of the model itself; furthermore, the features that the model relies on will ultimately determine where (and even whether) the model can be deployed in practice. This paper develops a new framework and system that enables a joint evaluation of both the conventional notions of machine learning performance (e.g., model accuracy) and the systems-level costs of different representations of network traffic. We highlight these two dimensions for a practical network management task, video streaming quality inference, and show that the appropriate operating point for these two dimensions depends on the deployment scenario. We demonstrate the benefit of exploring a range of representations of network traffic and present Traffic Refinery, a proof-of-concept reference implementation that both monitors network traffic at 10 Gbps and transforms the traffic in real time to produce a variety of feature representations for machine learning models. Traffic Refinery both highlights this design space and makes it possible for network operators to easily explore different representations for learning, balancing systems costs related to feature extraction and model training against the resulting model performance",space,1074
,filtered,core,法政大学大学院デザイン工学研究科,2021-03-24 00:00:00,core,development of an object position and attitude sharing networked ar game system using omni directional laser scanner,https://core.ac.uk/download/426984051.pdf,"In recent years, the development environment for mobile applications using Augmented Reality (AR) technology, which superimposes virtual objects on real space, has improved dramatically, enabling the implementation of advanced applications by combining image processing using artificial intelligence (AI) and space sharing using network communication. In particular, the technology for simultaneous presentation of virtual objects to multiple users through space sharing is important for applications such as product demonstrations, design meetings, surgical support, or competitive games. However, such space-sharing technology alone can share virtual objects in the same space, but it cannot share remote objects as virtual objects with each other.In this research, as a proposal for the use of technology to mutually share remote objects as virtual objects, I will develop an AR game system that can communicate and compete with each other in the same way as real competitive games even in remote areas, by using a 360-degree omni-directional laser scanner to acquire the position and posture of a real radio-controlled car in each user\u27s space, and using network communication technology to mutually share this information and reproduce it as a virtual object in real time",space,1075
,filtered,core,"Department of Computer Science, Lund University, Sweden",2021-01-01 00:00:00,core,learning impedance actions for safe reinforcement learning in contact-rich tasks,,"Reinforcement Learning (RL) has the potential of solving complex continuous control tasks, with direct applications to robotics. Nevertheless, current state-of-the-art methods are generally unsafe to learn directly on a physical robot as exploration by trial-and-error can cause harm to the real world systems. In this paper, we leverage a framework for learning latent action spaces for RL agents from demonstrated trajectories. We extend this framework by connecting it to a variable impedance Cartesian space controller, allowing us to learn contact-rich tasks safely and efficiently. Our method learns from trajectories that incorporate both positional, but also crucially impedance-space information. We evaluate our method on a number of peg-in-hole task variants with a Franka Panda arm and demonstrate that learning variable impedance actions for RL in Cartesian space can be safely deployed on the real robot directly, without resorting to learning in simulation and a subsequent policy transfer",space,1076
,filtered,core,'Pisa University Press',2045-05-23 00:00:00,core,"la collaborazione in ""emergenza"". come sviluppare competenze collaborative da utilizzare in contesti anche ""fuori dai margini"".",https://core.ac.uk/download/14690759.pdf,"Gli uomini vivono in società. Un campo di analisi vasto, complesso, un universo di pianeti. Le modalità per studiarlo molteplici, intersecanti, interdisciplinari: una “rete”.
Gli uomini vivono in una società e interagiscono per ottenere dal “sistema società” il massimo profitto “personale”. Il tutto avviene in un determinato “spazio vitale reale o virtuale”, quello dove i nostri affari vengono svolti. Ed è qui che l’uomo cerca un equilibrio vantaggioso, o certamente positivo per ciò che “rappresenta”. Ma non può fare tutto da sé. Il mondo oggi è ormai ad un grado di complessità che è utile, efficace, efficiente collaborare per tendere a sviluppare l’eccellenza e non solo.
Collaborare diventa poi fondamentale in alcune situazioni, come quelle stra–ordinarie, dove “lo stato di non normalità”, diventa “emergenza”. La riflessione ci porterà attraverso una lettura della normalità su ciò che è possibile fare, quello che succede mentre comunichiamo e come meglio porci in dinamica, ricavando dalla sinergia interdisciplinare, o meglio transdisciplinare, lo stimolo a non disperdere energie.
Per far questo, proporrò dopo aver chiarito cosa s’intenda per emergenza e per collaborazione ed aver fatto un quadro generale della ricerca nel primo capitolo, un’analisi sull’organizzazione nell’emergenza: quali presupposti educativi macrosociali, quale specificità potenziale per gli addetti ai lavori nella nuova figura professionale del Disaster Manager, quale urgenza di una progettazione oltre la pianificazione attraverso ad esempio la tecnica logoterapica ed infine quale sviluppo fiduciario di comunità spinga le motivazioni.
Non solo. Nel capitolo terzo, mi misurerò con altre scoperte di ricerca sulla necessità di un gruppo guida e su come creare l’idea della partecipazione, quale mezzo per ricostruire un’intesa spesso dispersa, facendo accenno allo sviluppo ed all’efficacia del software Haria–2 sempre più innovativo e da noi implementato.
Infine, per completare la visione globale della ricerca mi sono sentito in dovere di accennare all’evoluzione di tecniche di gruppo, personalmente sperimentate, che centrano le competenze collaborative, proponendo così una formazione partecipata.
Si conosce sé stessi. Si sviluppa cioè la consapevolezza graduale di ciò di cui sentiamo di aver bisogno e anche di cosa possiamo mettere a servizio degli altri e per noi stessi. L’educazione deve essere vissuta, elaborata, come strada maestra per un’auto consapevolezza di cammino con…, ma curando il “peso” delle relazioni.
Per essere collaborativi, per diventare gruppo integrato e integrante in situazioni rischiose, questo “essere sé stessi” è immerso in un “essere sé stessi con gli altri” e va saputo gestire. Ecco la Maratona, lo psicodramma, lo stare in rete, l’educazione dei piccoli, ma anche tecniche relazionali come il Focus Group ed il T–Group.
INGLESE (vedi Indice Inglese)
People live in society. A wide, complex, field of analysis, a universe of planets . The ways to study it are various, intersecting, interdisciplinary: a “net”.
People live in society and they interact to obtain by the “society system” the greatest “personal” profit. Everything happens in a determined “vital real or virtual space”, the one in which our affairs are performed. Here people search for a favourable, or certainly positive balance, for his “role”. But they can’t act alone. Today the world is so complex that it is useful, efficacious, efficient, to collaborate to develop excellence and not only it.
Collaborating becomes then very important in several situations, as for example the extra–ordinary ones, where “the not–normality condition”, becomes “emergency”. The reflection will lead us through a reading of normality to what is possible to do, what happens while we are speaking and how to better relate, taking from the interdisciplinary synergy, or better transdisciplinary , the incentive not to dissipate energies.
For this purpose, after clearing what I mean by emergency and by collaboration and after a general view of the research in the first chapter, I will propose an analysis about the organization in emergency: what macrosocial educational assumptions, what potential specificity for people in charge of works in the new professional figure of the Disaster Manager, what urgency of a planning over the project through for example the ‘logotherapeutic’ technique and finally which trust development of community pushes motivations.
Not only. In the third chapter, I will compete with other discoveries of research about the necessity of a guide group and about how to create the idea of the participation, as a means to rebuild an often dispersed accord, referring to the development and the efficaciousness of the software Haria–2 more and more innovative and improved by us.
Finally, to complete the global vision of the research I felt bound to speak about the evolution of group techniques, personally experimented, that centre the competence of collaboration, so proposing a participated formation.
We know ourselves. That is we develop gradual awareness of what we feel we need and also what we can put at the service of the others and of ourselves. Education has to be lived, elaborated, like a main road for a self–awareness of way with…, but caring the “weight” of relationships.
To collaborate, to become an integrated group and integrating in dangerous situations, this “being ourselves” is immersed in a “being ourselves with the others” and we must be able to manage it. Here is the ‘Marathon’, the ‘psychodrama’, staying in the net, children education, but also connection techniques like the Focus Group and the T–Group",space,1077
,filtered,core,'Pisa University Press',2024-03-26 00:00:00,core,hybrid machine learning - monte carlo approach to petrophysical seismic inversion,,"We implement a machine-learning inversion approach that uses a convolutional neural network (CNN) to solve the petrophysical seismic inversion. A discrete Cosine Transform (DCT) is used to compress both the input and output response of the network, and hence the network is trained to predict the nonlinear mapping between the DCT-transformed seismic data and the DCT-transformed petrophysical model. This transformation is used as an additional feature extraction technique that not only reduces the dimensionality of the input and output of the network but also guarantees the preservation of the assumed temporal and spatial continuity pattern in the estimated model. A theoretical rock-physics model (RPM) based on granular media models constitutes the link between the elastic and the petrophysical space, whereas the exact Zoeppritz equations map the elastic properties onto the seismic pre-stack domain. A direct sequential co-simulation with joint probability distribution generates the training and validation sets under the assumption of a stationary non-parametric prior and a Gaussian variogram model. We apply a Monte Carlo simulation strategy to propagate onto the final estimates both the uncertainties associated to the noise contamination in the data and the modeling error introduced by the network approximation. We discuss synthetic inversions to a realistic subsurface model that simulates a real gas-saturated reservoir hosted in a turbiditic sequence. We assess the robustness and stability of our trained CNN in case of erroneous assumptions about the noise statistics, errors in the calibrated RPM, and errors in the estimated source wavelet. The outcomes of the proposed approach are compared with those achieved by a more standard linearized inversion in which each seismic gather is inverted separately. Lastly, we demonstrate that transfer learning avoids retraining the network from scratch when the statistical properties of the training and test sets differ. Our experiments confirm that the implemented CNN inversion successfully solves the petrophysical seismic inversion and guarantees more stable and accurate predictions with respect to the standard inversion approach. In particular, once the network has been trained, the implemented inversion retrieves petrophysical properties and associated uncertainties in near real-time",space,1078
,filtered,core,'Pisa University Press',2091-02-19 00:00:00,core,exploiting generative adversarial networks for data augmentation: the european space agency phisat-1 mission case study,,"The dramatic increase in the number of satellites in orbit in recent years has brought progressive interest even from companies that did not operate in sectors directly related to space. To fully exploit this area, however, years of flight heritage are required, which not all companies can afford, because of that many rely on services offered by others. Deploying a new remote sensing service, however, is not simple, especially considering that the advancing interest does not keep pace with the advancement of artificial intelligence for space applications, which although on earth have had incredible improvements in recent times, on satellites had never been used by european missions. For this reason, several companies in collaboration with the European Space Agency and the University of Pisa have created the PhiSat-1 mission, demonstrating that artificial intelligence could be used on-board even for sensors never adopted in orbit, using a training dataset composed of images from the Sentinel-2 mission. Thanks to systems like the Intel Myriad Movidius Stick 2 it has been possible to reduce the time to market of neural networks for embedded satellite systems, allowing to meet the strict requirements of size and power consumption, still obtaining good performances. The use case examined in this thesis is the application of Artificial Intelligence to filter out images that are too cloudy to be usable, in order to send on-ground only those with a cloud presence considered acceptable. Given that the neural network deployed on PhiSat-1 had great limitations on its training set, as it was not very large in size and with a very unbalanced distribution in the cloudiness of the images, it has been used an architecture of Generative Adversarial Network partly customized to produce artificial images to join the real ones during a new training of the classifier network employed on PhiSat-1, both to increase the size of the dataset and to rebalance it. A custom Frechet Inception Distance has been defined to assess the quality of the produced samples. Several scenarios were defined to test in which situation the artificial images were useful. In particular, there was a statistical increase in the ability of the network to detect cloudy images when artificial samples were used to rebalance the cloudiness of the training set",space,1079
,filtered,core,,2021-04-23 00:00:00,core,kessler : a machine learning library for spacecraft collision avoidance,,"As megaconstellations are launched and the space sector grows, space debris pollution is posing an increasing threat to operational spacecraft. Low Earth orbit is a junkyard of dead satellites, rocket bodies, shrapnels, and other debris that travel at very high speed in an uncontrolled manner. Collisions at orbital speeds can generate fragments and potentially trigger a cascade of more collisions endangering the whole population, a scenario known since the late 1970s as the Kessler syndrome. In this work we present Kessler: an open-source Python package for machine learning (ML) applied to collision avoidance. Kessler provides functionalities to import and export conjunction data messages (CDMs) in their standard format and predict the evolution of conjunction events based on explainable ML models. In Kessler we provide Bayesian recurrent neural networks that can be trained with existing collections of CDM data and then deployed in order to predict the contents of future CDMs in a given conjunction event, conditioned on all CDMs received up to now, with associated uncertainty estimates about all predictions. Furthermore Kessler includes a novel generative model of conjunction events and CDM sequences implemented using probabilistic programming, simulating the CDM generation process of the Combined Space Operations Center (CSpOC). The model allows Bayesian inference and also the generation of large datasets of realistic synthetic CDMs that we believe will be pivotal to enable further ML approaches given the sensitive nature and public unavailability of real CDM data",space,1080
,filtered,core,'Pisa University Press',2048-02-27 00:00:00,core,ottimizzazione sistematica di impronte di contatto in ingranaggi spiroconici,https://core.ac.uk/download/14695018.pdf,"La presente tesi propone un metodo sistematico per l'ottimizzazione di impronte di contatto in ingranaggi ipoidi e spiroconici: rispetto alla condizione non ottimizzata, si determinano le variazioni dei parametri macchina del pignone necessarie per ottenere un'impronta di contatto la cui distanza, misurata attraverso un'opportuna metrica, è minima rispetto a quella ottimale.

Queste sono le fasi in cui si sviluppa la procedura.
• Determinare le superfici B-Spline che interpolano i punti campionati sui denti di una ruota, ottenuti applicando l'approccio invariante della teoria dell'ingranamento al metodo face milling della Gleason®. La superficie del pignone è generata sovrapponendo alla superficie di base quella di spoglia, che viene modificata durante l'ottimizzazione.
• Effettuare l'analisi del contatto rigido (TCA) e quella del contatto esteso simulato (Simulated LTCA); quest'ultima consiste nell'approssimare l'impronta di contatto sotto carico come l'intersezione tra la superficie del pignone e quella della corona con sovrapposto uno strato di pasta.
• Individuare la superficie di spoglia ottima che minimizza la distanza tra l'impronta di contatto target e quella attuale attraverso il metodo di Nelder-Mead.
• Determinare i settaggi macchina che minimizzano la distanza tra la superficie di spoglia obiettivo e quella attuale, utilizzando un metodo non lineare ai minimi quadrati, quale l'algoritmo di Levenberg-Marquardt con strategia trust-region.

Il metodo è stato implementato in C++ e validato testando alcune coppie impiegate in campo aerospaziale; sono riportati la struttura del codice con i file di input e i risultati ottenuti. Sono inoltre stati discussi lo stato dell'arte, i possibili sviluppi futuri di questo lavoro e i riferimenti alla normativa vigente.

------------------------------------------------

This thesis work presents a systematic and robust methodology to optimize tooth contact patterns in hypoid and spiral bevel gears. The main goals are: (1) to find the shape of the ease off surface that minimizes the distance between the current contact pattern and the target one; (2) to identify the machine setting corrections required to obtain the previously identified ease off surface.

The proposed method consists in the following steps.
• The surfaces of the teeth, generated with the face milling process (Gleason®), are modeled using the invariant approach and then interpolated by B-Spline surfaces (the pinion surface is considered as the superposition of the base surface and the ease off surface along the local normals).
• The Tooth Contact Analysis and the Simulated Loaded Tooth Contact Analysis are carried out; in the latter, a marking compound layer is applied on the gear surface and the portion of the compound removed during the simulation of meshing is taken as a valid approximation of the tooth contact pattern.
• The best ease off minimizing the distance between the current contact pattern and the target one is identified employing the Nelder-Mead algorithm.
• The feasible setting machine variations, that allow to achieve the prescribed ease off surface, are detected employing the Levenberg-Marquardt algorithm (a nonlinear least square method) with a trust region strategy.

The method was implemented in C++ language and tested in the optimization of the contact pattern properties of real aerospace drives. The structure of the program along with the input files and the results obtained are also presented. The state of art, some possible future developments with reference to the current standards are discussed.<br",space,1081
,filtered,core,'Pisa University Press',2061-03-26 00:00:00,core,applications of convolutional and residual neural networks to solve the electrical resistivity tomography inversion,,"This thesis assesses the applicability of a machine learning approach to solve the Electrical Resistivity Tomography (ERT). The ERT is a non-linear and ill-conditioned problem usually solved through gradient-based methods. These approaches ensure a rapid convergence toward a best-fitting model but suffer from the local linearization, and hence are prone to get trapped into local minima of the error function. Here we implement and apply an alternative approach in which a convolutional neural network is trained to learn the non-linear mapping between the pseudo resistivity (data) domain and the resistivity space. The computational cost of the training procedure is highly dependent on the dimensionality of the input and output of the network and for this reason, we also assess the applicability of the Discrete Cosine Transform (DCT) (an orthogonal transformation) to compress the number of data points and model parameters. The aim of the DCT is twofold: not only it reduces the dimensionality of the input and output of the network, but it also acts as a regularization operator in the model space that mitigates the ill-conditioning of the ERT inversion problem, while preserving the assumed spatial continuity pattern in the recovered solution. In addition to standard CNNs, we also apply a Residual Neural Network (RNN), a special kind of CNN that uses skip connections to avoid the so-called vanishing gradient problem that arises when training a deep network.
The implemented machine-learning ERT inversion includes four steps:
1) Generation phase: define an ensemble of 2D resistivity models and compute the associated pseudosections, which constitute the network output and input responses, respectively. The models are created according to a previously defined prior distribution and spatial variability pattern, while a finite-elements code constitutes the forward modeling engine.
2) Network Design: define a network architecture to approximate the non-linear mapping between the data and the model spaces.
3) Training phase: train the network by minimizing the differences between the predicted and desired output.
4) Prediction phase: once trained, use the network to project an observed pseudosection onto the associated resistivity values.
For what concerns the network design we analyze how different hyperparameter settings (i.e., number of layers, size of the convolutional filters, number of filters) affect the network performances that are evaluated on the training, validation, and test sets. We also assess the robustness and stability of the network predictions in case of erroneous assumptions about the noise and prior model statistics assumed during the learning stage. In this regard, we also demonstrate that transfer learning avoids retraining the network from scratch when the trained convolutional neural network is applied to a different scenario (i.e., a test model with different statistical properties). This technique is routinely employed in machine learning applications and consists of an additional training process with a small portion of target data, thereby allowing for a quick transfer of the learned features to a new task.
The implemented machine learning approaches for ERT inversion are first tested on synthetic data and then applied to real data measured along a river embankment. Our tests confirm the suitability of the proposed approach, opening the possibility to estimate the subsurface resistivity values in near real-time",space,1082
,filtered,core,The Random Feature Model for Input-Output Maps between Banach Spaces,2020-05-20 00:00:00,core,https://core.ac.uk/download/323787593.pdf,,"Well known to the machine learning community, the random feature model, originally introduced by Rahimi and Recht in 2008, is a parametric approximation to kernel interpolation or regression methods. It is typically used to approximate functions mapping a finite-dimensional input space to the real line. In this paper, we instead propose a methodology for use of the random feature model as a data-driven surrogate for operators that map an input Banach space to an output Banach space. Although the methodology is quite general, we consider operators defined by partial differential equations (PDEs); here, the inputs and outputs are themselves functions, with the input parameters being functions required to specify the problem, such as initial data or coefficients, and the outputs being solutions of the problem. Upon discretization, the model inherits several desirable attributes from this infinite-dimensional, function space viewpoint, including mesh-invariant approximation error with respect to the true PDE solution map and the capability to be trained at one mesh resolution and then deployed at different mesh resolutions. We view the random feature model as a non-intrusive data-driven emulator, provide a mathematical framework for its interpretation, and demonstrate its ability to efficiently and accurately approximate the nonlinear parameter-to-solution maps of two prototypical PDEs arising in physical science and engineering applications: viscous Burgers' equation and a variable coefficient elliptic equation",space,1083
,filtered,core,"The Photoswitch Dataset: A Molecular Machine Learning Benchmark for the
  Advancement of Synthetic Chemistry",2020-06-28 00:00:00,core,http://arxiv.org/abs/2008.03226,,"The space of synthesizable molecules is greater than $10^{60}$, meaning only
a vanishingly small fraction of these molecules have ever been realized in the
lab. In order to prioritize which regions of this space to explore next,
synthetic chemists need access to accurate molecular property predictions.
While great advances in molecular machine learning have been made, there is a
dearth of benchmarks featuring properties that are useful for the synthetic
chemist. Focussing directly on the needs of the synthetic chemist, we introduce
the Photoswitch Dataset, a new benchmark for molecular machine learning where
improvements in model performance can be immediately observed in the throughput
of promising molecules synthesized in the lab. Photoswitches are a versatile
class of molecule for medical and renewable energy applications where a
molecule's efficacy is governed by its electronic transition wavelengths. We
demonstrate superior performance in predicting these wavelengths compared to
both time-dependent density functional theory (TD-DFT), the incumbent first
principles quantum mechanical approach, as well as a panel of human experts.
Our baseline models are currently being deployed in the lab as part of the
decision process for candidate synthesis. It is our hope that this benchmark
can drive real discoveries in photoswitch chemistry and that future benchmarks
can be introduced to pivot learning algorithm development to benefit more
expansive areas of synthetic chemistry.Comment: Prior version accepted to the 2020 ICLR Workshop on Fundamental
  Science in the Era of AI. First two authors contributed equall",space,1084
,filtered,core,Heterogeneous CPU+GPU Stochastic Gradient Descent Algorithms,2020-04-19 00:00:00,core,http://arxiv.org/abs/2004.08771,,"The widely-adopted practice is to train deep learning models with specialized
hardware accelerators, e.g., GPUs or TPUs, due to their superior performance on
linear algebra operations. However, this strategy does not employ effectively
the extensive CPU and memory resources -- which are used only for
preprocessing, data transfer, and scheduling -- available by default on the
accelerated servers. In this paper, we study training algorithms for deep
learning on heterogeneous CPU+GPU architectures. Our two-fold objective --
maximize convergence rate and resource utilization simultaneously -- makes the
problem challenging. In order to allow for a principled exploration of the
design space, we first introduce a generic deep learning framework that
exploits the difference in computational power and memory hierarchy between CPU
and GPU through asynchronous message passing. Based on insights gained through
experimentation with the framework, we design two heterogeneous asynchronous
stochastic gradient descent (SGD) algorithms. The first algorithm -- CPU+GPU
Hogbatch -- combines small batches on CPU with large batches on GPU in order to
maximize the utilization of both resources. However, this generates an
unbalanced model update distribution which hinders the statistical convergence.
The second algorithm -- Adaptive Hogbatch -- assigns batches with continuously
evolving size based on the relative speed of CPU and GPU. This balances the
model updates ratio at the expense of a customizable decrease in utilization.
We show that the implementation of these algorithms in the proposed CPU+GPU
framework achieves both faster convergence and higher resource utilization than
TensorFlow on several real datasets and on two computing architectures -- an
on-premises server and a cloud instance",space,1085
,filtered,core,"Deep Learning Techniques to make Gravitational Wave Detections from Weak
  Time-Series Data",2021-04-12 00:00:00,core,http://arxiv.org/abs/2007.05889,,"Gravitational waves are ripples in the space time fabric when high energy
events such as black hole mergers or neutron star collisions take place. The
first Gravitational Wave (GW) detection (GW150914) was made by the Laser
Interferometer Gravitational-wave Observatory (LIGO) and Virgo Collaboration on
September 14, 2015. Furthermore, the proof of the existence of GWs had
countless implications from Stellar Evolution to General Relativity.
Gravitational waves detection requires multiple filters and the filtered data
has to be studied intensively to come to conclusions on whether the data is a
just a glitch or an actual gravitational wave detection. However, with the use
of Deep Learning the process is simplified heavily, as it reduces the level of
filtering greatly, and the output is more definitive, even though the model
produces a probabilistic result. Our technique, Deep Learning, utilizes a
different implementation of a one-dimensional convolutional neural network
(CNN). The model is trained by a composite of real LIGO noise, and injections
of GW waveform templates. The CNN effectively uses classification to
differentiate weak GW time series from non-gaussian noise from glitches in the
LIGO data stream. In addition, we are the first study to utilize fine-tuning as
a means to train the model with a second pass of data, while maintaining all
the learned features from the initial training iteration. This enables our
model to have a sensitivity of 100%, higher than all prior studies in this
field, when making real-time detections of GWs at an extremely low
Signal-to-noise ratios (SNR), while still being less computationally expensive.
This sensitivity, in part, is also achieved through the use of deep signal
manifolds from both the Hanford and Livingston detectors, which enable the
neural network to be responsive to false positives.Comment: 9 pages, 4 figure",space,1086
,filtered,core,"Traffic Refinery: Cost-Aware Data Representation for Machine Learning on
  Network Traffic",2021-06-07 00:00:00,core,http://arxiv.org/abs/2010.14605,,"Network management often relies on machine learning to make predictions about
performance and security from network traffic. Often, the representation of the
traffic is as important as the choice of the model. The features that the model
relies on, and the representation of those features, ultimately determine model
accuracy, as well as where and whether the model can be deployed in practice.
Thus, the design and evaluation of these models ultimately requires
understanding not only model accuracy but also the systems costs associated
with deploying the model in an operational network. Towards this goal, this
paper develops a new framework and system that enables a joint evaluation of
both the conventional notions of machine learning performance (e.g., model
accuracy) and the systems-level costs of different representations of network
traffic. We highlight these two dimensions for two practical network management
tasks, video streaming quality inference and malware detection, to demonstrate
the importance of exploring different representations to find the appropriate
operating point. We demonstrate the benefit of exploring a range of
representations of network traffic and present Traffic Refinery, a
proof-of-concept implementation that both monitors network traffic at 10 Gbps
and transforms traffic in real time to produce a variety of feature
representations for machine learning. Traffic Refinery both highlights this
design space and makes it possible to explore different representations for
learning, balancing systems costs related to feature extraction and model
training against model accuracy",space,1087
,filtered,core,Constellation Forecasting Tools for Autonomous Operations,2021-08-11 00:00:00,core,https://core.ac.uk/download/478906382.pdf,DigitalCommons@USU,"Large constellations are quickly becoming the norm in small satellite missions. These constellations are being designed and developed faster than ever before through the utilization of smaller, heterogeneous spacecraft. Often, these constellations provide increased resiliency and capabilities over their heritage, highly tailored counterparts. The ability to replace on-orbit assets quickly and with lower costs is an advantageous feature of these large smallsat constellations. With the advent of these new architectures, though, come increased complexity in mission operations. The management and monitoring of potentially hundreds of heterogeneous space assets can be extremely challenging and negate much of the cost savings using current operational approaches. Additional complexity is added with the loss expectancy of some number of assets inherent to the design within these constellations. Rather than tasking individual assets to complete missions on behalf of the system, ideal operation would be conducted through tasking of the constellation as a whole. This approach requires tasking of the individual assets by the constellation using machine-to-machine (M2M) data sharing and on-orbit autonomous decision making. Recent advances in machine learning (ML) and artificial intelligence (AI) have now set the stage for the state of the possible in this regard.
The authors of this paper are part of a research and development team aiming to develop solutions and tools to support this operational approach. The ideas presented involve a procedural and technical implementation of using forecasted operational effects developed by a combination of state machines and ML tools. First, the system’s state is gathered, time-synced, and produced into a “Dynamic Relative Telemetry Calculator.” This is presented as an NxN matrix documenting each node’s state relative to all other nodes in the system. Next, a desired operational command can be loaded into the system. Multiple possible operational scenarios and effects can be propagated. For each propagation, each asset must be capable of reporting the “cost” of performing a certain task within a certain operational scenario. By itself, this still requires a human in the loop to analyze the results and determine a command decision. However, the secondary and tertiary effects of these decisions are still unknown. To this front, the authors are developing a method of wrapping ML capability around the system\u27s state machine and propagators to create a forecaster capable of autonomously determining optimal decisions within a system. The forecaster operates in real time, improving its predictions as more data is produced by each subsystem. Generated operational forecasts, and their effects, are validated with log data from a simulation. This data is being applied to proprietary mission scenarios, but could also be applied to historical open/mission data for validation or operational lessons learned. Over time, this forecasting tool could optimize large constellation management by reserving human in the loop for only the most severe/impactful decision thresholds. This paper will present current progress of the integrated solution, next steps in the research and development roadmap, and, most importantly, the current technical hurdles still to overcome to achieve true spaceflight autonomy",space,1088
,filtered,core,HSSMATCH: um modelo híbrido para semantic schema matching em arquiteturas orientadas a microsserviços,2020-02-06 00:00:00,core,https://core.ac.uk/download/389140546.pdf,Engenharia Elétrica,"Os ecossistemas de software da atualidade possuem estilo arquitetural de

microsserviços e características específicas, sistemas e dados distribuídos em

diferentes fontes, o que dificulta o gerenciamento de dados. Como os modelos

conceituais do mundo real são diferentes entre os sistemas, há problemas para

integrar os dados e realizar a comunicação entre esses microsserviços, o que

implica na necessidade de matching entre os esquemas e mensagens. A

literatura evidencia problemas de matching como o tamanho do espaço de

busca, a heterogeneidade semântica dos dados, e as atualizações pelas quais

os esquemas passam constantemente, e mostra como lacunas a inadequação

das interfaces de usuários, a acomodação de alterações nas estruturas de

dados, e, ainda, a escassez de abordagens para uso prático. Assim, o presente

estudo teve por objetivo é apresentar um modelo híbrido de semantic schema

matching para microsserviços com capacidade de identificar similaridades entre

os elementos de dois esquemas em larga escala, que suporte a atualização dos

esquemas e seus dados e, considere os resultados da validação humana para

reuso. Para tanto, foi apresentada a arquitetura do modelo HSSMatch e

implementado o protótipo do HSSMatch System que permite ao usuário, por

meio de uma interface gráfica Web, gerenciar o processo de schema matching

de microsserviços. A avaliação desse protótipo, no que se refere à sua

adequação de design de interação, foi feita por meio de experimentos e

questionários aplicados a usuários que atuam na área de integração de dados e

comunicação entre sistemas de software. A avaliação também foi realizada em

experimentos com dois datasets, e mostrou aspectos que confirmam a hipótese

deste estudo, pois verificou-se melhoria na eficiência e eficácia do processo de

schema matching utilizando a abordagem híbrida que acomoda alterações nos

dados, reduz o espaço de busca e combina matchers em nível de esquema e de

instâncias. Como trabalhos futuros, podem ser explorados métodos

supervisionados de aprendizado de máquina para configuração semiautomática

de estratégias de schema matching, e ainda outras técnicas de particionamento

de esquemas, ontologias de domínio em prol da melhoria da qualidade do

resultado de schema matching para domínios específicos.Today's software ecosystems have an architectural style of microservices and

specific characteristics, systems and data distributed across different sources,

making data management difficult. Because real-world conceptual models differ

across systems, there are problems integrating data and carrying out

communication between these microservices, which implies the need for

matching between schemas and messages. The literature highlights matching

problems such as search space size, semantic heterogeneity of data, and

updates that schemas constantly go through, and shows as gaps the inadequacy

of the user interfaces, the accommodation of changes in data structures, and the

scarcity of approaches for practical use. Thus, the present study aimed to present

a hybrid model of semantic schema matching for microservices with the ability to

identify similarities between the elements of two large-scale schemes that

supports the updating of the schemes and their data and considers the results of

human validation for reuse. For this, the architecture of the HSSMatch model was

presented, and the prototype of the HSSMatch System was implemented,

allowing the user, through a web graphical interface, to manage the

microservices schema matching process. The evaluation of this prototype,

regarding its suitability of interaction design, was made through experiments and

questionnaires applied to users working in the area of data integration and

communication between software systems. The evaluation was also performed

in experiments with two datasets, and showed aspects that confirm the

hypothesis of this study, since there was an improvement in the efficiency and

effectiveness of the schema matching process using the hybrid approach that

accommodates data changes, reduces search space and combines matchers at

schema and instance levels. As future work, supervised machine learning

methods for semiautomatic configuration of schema matching strategies, as well

as other schema partitioning techniques, domain ontologies for improving the

quality of schema matching results for specific domains can be explored.Instituto Presbiteriano Mackenzi",space,1089
,filtered,core,"FNS: an event-driven spiking neural network simulator based on the LIFL
  neuron model",2020-07-16 00:00:00,core,http://arxiv.org/abs/1801.00864,,"Limitations in processing capabilities and memory of today's computers make
spiking neuron-based (human) whole-brain simulations inevitably characterized
by a compromise between bio-plausibility and computational cost. It translates
into brain models composed of a reduced number of neurons and a simplified
neuron's mathematical model, leading to the search for new simulation
strategies. Taking advantage of the sparse character of brain-like computation,
the event-driven technique could represent a way to carry out efficient
simulation of large-scale Spiking Neural Networks (SNN). The recent Leaky
Integrate-and-Fire with Latency (LIFL) spiking neuron model is event-driven
compatible and exhibits some realistic neuronal features, opening new avenues
for brain modelling. In this paper we introduce FNS, the first LIFL-based
spiking neural network framework, which combines spiking/synaptic neural
modelling with the event-driven approach, allowing us to define heterogeneous
neuron modules and multi-scale connectivity with delayed connections and
plastic synapses. In order to allow multi-thread implementations a novel
parallelization strategy is also introduced. This paper presents mathematical
models, software implementation and simulation routines on which FNS is based.
Finally, a brain subnetwork is modeled on the basis of real brain structural
data, and the resulting simulated activity is compared with associated brain
functional (source-space MEG) data, demonstrating a good matching between the
activity of the model and that of the experimetal data. This work aims to lay
the groundwork for future event-driven based personalised brain models.Comment: Changed title, added references, corrected typo",space,1090
,filtered,core,"Not All Datasets Are Born Equal: On Heterogeneous Data and Adversarial
  Examples",2020-10-07 00:00:00,core,http://arxiv.org/abs/2010.03180,,"Recent work on adversarial learning has focused mainly on neural networks and
domains where they excel, such as computer vision. The data in these domains is
homogeneous, whereas heterogeneous tabular data domains remain underexplored
despite their prevalence. Constructing an attack on models with heterogeneous
input spaces is challenging, as they are governed by complex domain-specific
validity rules and comprised of nominal, ordinal, and numerical features. We
argue that machine learning models trained on heterogeneous tabular data are as
susceptible to adversarial manipulations as those trained on continuous or
homogeneous data such as images. In this paper, we introduce an optimization
framework for identifying adversarial perturbations in heterogeneous input
spaces. We define distribution-aware constraints for preserving the consistency
of the adversarial examples and incorporate them by embedding the heterogeneous
input into a continuous latent space. Our approach focuses on an adversary who
aims to craft valid perturbations of minimal l_0-norms and apply them in real
life. We propose a neural network-based implementation of our approach and
demonstrate its effectiveness using three datasets from different content
domains. Our results suggest that despite the several constraints heterogeneity
imposes on the input space of a machine learning model, the susceptibility to
adversarial examples remains unimpaired",space,1091
,filtered,core,VARCLUST: clustering variables using dimensionality reduction,2020-12-18 00:00:00,core,https://core.ac.uk/download/370420110.pdf,HAL CCSD,"VARCLUST algorithm is proposed for clustering variables under the assumption that variables in a given cluster are linear combinations of a small number of hidden latent variables, corrupted by the random noise. The entire clustering task is viewed as the problem of selection of the statistical model, which is defined by the number of clusters, the partition of variables into these clusters and the 'cluster dimensions', i.e. the vector of dimensions of linear subspaces spanning each of the clusters. The ""optimal"" model is selected using the approximate Bayesian criterion based on the Laplace approximations and using a non-informative uniform prior on the number of clusters. To solve the problem of the search over a huge space of possible models we propose an extension of the ClustOfVar algorithm of [29, 7] which was dedicated to subspaces of dimension only 1, and which is similar in structure to the K-centroid algorithm. We provide a complete methodology with theoretical guarantees, extensive numerical experi-mentations, complete data analyses and implementation. Our algorithm assigns variables to appropriate clusterse based on the consistent Bayesian Information Criterion (BIC), and estimates the dimensionality of each cluster by the PEnalized SEmi-integrated Likelihood Criterion (PESEL) of [24], whose consistency we prove. Additionally, we prove that each iteration of our algorithm leads to an increase of the Laplace approximation to the model posterior probability and provide the criterion for the estimation of the number of clusters. Numerical comparisons with other algorithms show that VARCLUST may outperform some popular machine learning tools for sparse subspace clustering. We also report the results of real data analysis including TCGA breast cancer data and meteorological data, which show that the algorithm can lead to meaningful clustering. The proposed method is implemented in the publicly available R package varclust. Keywords variable clustering · Bayesian approach · k-means · dimensionality reduction · subspace clustering 2 P. Sobczyk, S. Wilczyński, M. Bogdan et al",space,1092
,filtered,core,Multi-function RF for Situational Awareness,2020-08-23 00:00:00,core,https://core.ac.uk/download/401852046.pdf,SURFACE at Syracuse University,"Radio frequency (RF) communications are an integral part of many situational awareness applications. Sensing data need to be processed in a timely manner, making it imperative to have a robust and reliable RF link for information dissemination. Moreover, there is an increasing need for exploiting RF communication signals directly for sensing, leading to the notion of multi-function RF.
In the first part of this dissertation, we investigate the development of a robust Multiple-Input Multiple-Output (MIMO) communication system suitable for airborne platforms.Three majors challenges in realizing MIMO capacity gain in airborne environment are addressed: 1) antenna blockage due largely to the orientation of the antenna array; 2) the presence of unknown interference inherent to the intended application; 3) the lack of channel state information (CSI) at the transmitter. Built on the Diagonal Bell-Labs Layered Space-Time (D-BLAST) MIMO architecture, the system integrates three key design approaches: spatial spreading to counter antenna blockage; temporal spreading to mitigate signal to interference and noise ratio degradation due to intended or unintended interference; and a simple low rate feedback scheme to enable real time adaptation in the absence of full transmitter CSI. Extensive experiment studies using a fully functioning $4\times 4$ MIMO system validate the developed system.
In the second part, ambient RF signals are exploited to extract situational awareness information directly. Using WiFi signals as an example, we demonstrate that the CSI obtained at the receiver contains rich information about the propagation environment. Two distinct learning systems are developed for occupancy detection using passive WiFi sensing. The first one is based on deep learning where a parallel convolutional neural network (CNN) architecture is designed to extract useful information from both magnitude and phase of the CSI. Pre-processing steps are carefully designed to preserve human motion induced channel variation while insulating against other impairments and post-processing is applied after CNN to infer presence information for instantaneous motion outputs. To alleviate the need of tedious training efforts involved in deep learning based system, a novel learning problem with contaminated sampling is formulated. This leads to a second learning system: a two-stage solution for motion detection using support vector machines (SVM). A one-class SVM model is first evaluated whose training data are from human free environment only. Decontamination of human presence data using the one-class SVM is done prior to motion detection through a two-class support vector classifier. Extensive experiments using commercial off-the-shelf WiFi devices are conducted for both systems. The results demonstrate that the learning based RF sensing provides a viable and promising alternative for occupancy detection as they are much more sensitive to human motion than passive infrared sensors which are widely deployed in commercial and residential buildings",space,1093
,filtered,core,Deep Learning-based Vessel Detection from Very High and Medium Resolution Optical Satellite Images as Component of Maritime Surveillance Systems,2020-01-01 00:00:00,core,https://core.ac.uk/download/364708500.pdf,,"Today vessel detection from remote sensing images is increasingly becoming a crucial component in maritime surveillance applications. The increasing number of very high and medium resolution (VHR and MR) optical satellites shortens the revisit time as it was never before. This makes the technology especially attractive for a variety of maritime monitoring tasks. Nevertheless, it is quite a challenge to perform object detection on enormous large satellite images that cover several hundreds of square kilometers and derive results under near real time constraints.

This thesis presents an end-to-end multiclass vessel detection method from optical satellite images. The proposed workflow covers the complete processing chain and involves rapid image enhancement techniques, the fusion with automatic identification system (AIS) data, and the detection algorithm based on convolutional neural networks (CNN). To train the CNNs, two versions of training datasets were generated. The VHR training dataset was produced from the set of WorldView-[1-3] and GeoEye-1 images and contains about 40 000 of uniquely annotated vessels divided into 14 different classes. The MR training dataset was generated from the set of Landsat-8 images and contains about 14 000 of uniquely annotated vessels of 7 different classes.

The algorithms presented are implemented in the form of independent software processors and integrated in an automated processing chain as part of the Earth Observation Maritime Surveillance System (EO-MARISS). The solution developed from the methods presented has proven its usability within different projects and is used and further developed at the ground station of the German Aerospace Center (DLR) in Neustrelitz",space,1094
,filtered,core,Deep Learning-based Vessel Detection from Very High and Medium Resolution Optical Satellite Images as Component of Maritime Surveillance Systems,2020-12-01 00:00:00,core,https://core.ac.uk/download/395064651.pdf,,"Today vessel detection from remote sensing images is increasingly becoming a crucial component in maritime surveillance applications. The increasing number of very high and medium resolution (VHR and MR) optical satellites shortens the revisit time as it was never before. This makes the technology especially attractive for a variety of maritime monitoring tasks. Nevertheless, it is quite a challenge to perform object detection on enormous large satellite images that cover several hundreds of square kilometers and derive results under near real time constraints.

This thesis presents an end-to-end multiclass vessel detection method from optical satellite images. The proposed workflow covers the complete processing chain and involves rapid image enhancement techniques, the fusion with automatic identification system (AIS) data, and the detection algorithm based on convolutional neural networks (CNN). To train the CNNs, two versions of training datasets were generated. The VHR training dataset was produced from the set of WorldView-[1-3] and GeoEye-1 images and contains about 40 000 of uniquely annotated vessels divided into 14 different classes. The MR training dataset was generated from the set of Landsat-8 images and contains about 14 000 of uniquely annotated vessels of 7 different classes.

The algorithms presented are implemented in the form of independent software processors and integrated in an automated processing chain as part of the Earth Observation Maritime Surveillance System (EO-MARISS). The solution developed from the methods presented has proven its usability within different projects and is used and further developed at the ground station of the German Aerospace Center (DLR) in Neustrelitz",space,1095
,filtered,core,"CorGAN: Correlation-Capturing Convolutional Generative Adversarial
  Networks for Generating Synthetic Healthcare Records",2020-03-04 00:00:00,core,http://arxiv.org/abs/2001.09346,,"Deep learning models have demonstrated high-quality performance in areas such
as image classification and speech processing. However, creating a deep
learning model using electronic health record (EHR) data, requires addressing
particular privacy challenges that are unique to researchers in this domain.
This matter focuses attention on generating realistic synthetic data while
ensuring privacy. In this paper, we propose a novel framework called
correlation-capturing Generative Adversarial Network (CorGAN), to generate
synthetic healthcare records. In CorGAN we utilize Convolutional Neural
Networks to capture the correlations between adjacent medical features in the
data representation space by combining Convolutional Generative Adversarial
Networks and Convolutional Autoencoders. To demonstrate the model fidelity, we
show that CorGAN generates synthetic data with performance similar to that of
real data in various Machine Learning settings such as classification and
prediction. We also give a privacy assessment and report on statistical
analysis regarding realistic characteristics of the synthetic data. The
software of this work is open-source and is available at:
https://github.com/astorfi/cor-gan.Comment: Accepted to be published in the 33rd International FLAIRS Conference,
  AI in Healthcare Informatic",space,1096
,filtered,core,Generation of refactoring algorithms through grammatical evolution,2020-01-01 00:00:00,core,https://core.ac.uk/download/346536233.pdf,,"Orientadora: Silvia Regina VergilioCoorientador: Marouane KessentiniTese (doutorado) - Universidade Federal do Paraná, Setor de Ciências Exatas, Programa de Pós-Graduação em Informática. Defesa : Curitiba, 24/04/2020Inclui referências: p. 61-66Área de concentração: Ciência da ComputaçãoResumo: A atividade de refatoração tem como principal objetivo aplicar um conjunto de transformações em um artefato de software para melhorar sua estrutura sem alterar sua funcionalidade. Alguns estudos recentes, apresentam bons resultados ao gerarem modelos de predição de refatorações. Além disso, os estudos mostram que refatorações similares são aplicadas em diferentes contextos e podem ser aprendidas. Neste sentido, a maioria dos trabalhos existentes utiliza técnicas de aprendizado de máquina para gerar modelos que predizem se um dado trecho de código deve ser refatorado. Entretanto, essas abordagens possuem limitações. Elas buscam por refatorações específicas e exatamente como aplicadas por desenvolvedores, o que limita que outras refatorações sejam encontradas. Dada a natureza subjetiva da atividade de refatoração de software, a exploração por refatorações com base em outros critérios também é vantajosa. Existem trabalhos na área conhecida como Refatoração de Software Baseada em Busca (SBR) (do inglês, Search Based Software Refactoring), em que algoritmos de busca são utilizados para encontrar refatorações em um grande espaço de busca e visando a melhorar diversos aspectos. Recentemente, trabalhos em SBR começaram a utilizar exemplos de refatorações já aplicadas por desenvolvedores para incorporar aprendizado na busca. Entretanto, essas abordagens são limitadas em termos de generalização dos resultados, uma vez que não geram um modelo que possa ser utilizado para diferentes programas. Desse modo, abordagens existentes de SBR devem ser configuradas e executadas a cada novo programa. Neste contexto, este trabalho visa a incorporar os benefícios encontrados na área de aprendizado de máquina e na área de SBR, apresentando uma abordagem chamada Gorgeous (do inglês, Generation of Refactoring Algorithms through Grammatical Evolution). Gorgeous tem como objetivo gerar algoritmos de refatoração compostos por regras, que quando executados, determinam trechos de código que devem ser refatorados e refatorações a serem aplicadas. Os algoritmos são criados de forma que as refatorações sugeridas sejam similares a refatorações aplicadas na prática e que também melhorem a qualidade do software. Os algoritmos são criados utilizando um processo de aprendizado que primeiro extrai padrões de refatoração de programas agrupando elementos que foram refatorados de maneira similar. Após isso, uma evolução gramatical é executada para gerar algoritmos de refatoração com base nos padrões extraídos. Gorgeous é avaliada utilizando dados de refatoração extraídos de 40 programas Java do repositório GitHub. Como resultado, os algoritmos gerados foram capazes de obter bons resultados para diferentes programas, melhorando em média 60% a qualidade do programa e obtendo 50% de similaridade com refatorações aplicadas na prática. Palavras-chave: Refatoração, Engenharia de Software Baseada em Busca,Agrupamento, Evolução GramaticalAbstract: The refactoring activity addresses the application of a set of transformations in software artifacts to improve their structure while preserving their functionality. Recent studies present promising results generating prediction models for refactoring. Furthermore, they provide evidences that similar refactoring operations are applied in different contexts and they can be learned using Machine Learning (ML). Most works on ML based refactoring generate models to predict if a piece of code should be refactored. Despite the capability of prediction, existing works are limited to learn specific refactoring operations as applied by developers. However, to explore refactoring operations possibilities based on other criteria is also beneficial, mainly by the subjective context of refactoring. In this context, the Search-Based Software Refactoring (SBR) area addresses studies using search algorithms to find refactoring operations in a huge search space, aiming at improving several other aspects. However, existing SBR approaches do not support generalization of results since they do not generate a model as ML studies. In this way, a SBR approach needs to be configured and executed for each program in need of refactoring. In this context, this work introduces a SBR learning approach aiming at taking most advantage of both fields. Gorgeous (Generation of Refactoring Algorithms through Grammatical Evolution) generates refactoring algorithms composed by several rules determining pieces of code that should be refactored and the refactoring types to be used. A refactoring algorithm provides as solution a set of refactoring operations to be applied in a program. In this respect, the algorithm is generated with the goal of increasing similarity of the refactoring operations with the ones applied in practice, and also improving program quality. To do this, a learning process first extracts refactoring patterns from programs by grouping their elements that were refactored in similar ways. After that, a Grammatical Evolution (GE) is executed to generate the algorithms based on the extracted patterns. Gorgeous is evaluated using refactoring data from 40 Java programs of GitHub repository. The refactoring algorithms are capable of obtaining good results to different programs, obtaining around 60% of program quality improvement and 50% of similarity with real refactoring applications. Keywords: Refactoring, Search-Based Software Engineering, Clustering, Grammatical Evolutio",space,1097
,filtered,core,Optimization and Training of Generational Garbage Collectors,2020-03-24 00:00:00,core,https://core.ac.uk/download/288433288.pdf,ScholarWorks@UMass Amherst,"Garbage collectors are nearly ubiquitous in modern programming languages, and we want to minimize the cost they impose in terms of time and space. Generally, a collector waits until its space is full and then performs a collection to reclaim needed memory. However, this is not the only option; a collection could be performed early when some free space remains. For copying collectors, which are what we consider here, the system must traverse the graph of live objects and copy them, so the cost of a collection is proportional to the volume of objects that are live. Since this value fluctuates during a program\u27s execution, a collector can minimize its cost by carefully choosing the points at which it collects.
We help to realize this goal in two ways. First, by developing an algorithm that analyzes after-the-fact traces and computes optimal collection points, we can explore the theoretical limits of garbage collectors. This gives insight into what performance gains are possible, and can guide future collector development into areas that could be most fruitful.
Second, we use techniques from machine learning to find improved garbage collection policies that could be implemented in real systems. The optimal collection points provide ground truth from which a model can learn",space,1098
,filtered,core,Unsupervised Anomaly Detection in High-Dimensional Flight Data Using Convolutional Variational Auto-Encoder,2020-01-01 00:00:00,core,https://core.ac.uk/download/pdf/323104269.pdf,,"The modern National Airspace System (NAS) is an extremely safe system and the aviation industry has experienced a steady decrease in fatalities over the years. This can be attributed to both improved flight critical systems with redundant hardware and software protections, as well as an increased focus on active monitoring and response to real time and historically identified vulnerabilities by implementing more resilient procedures and protocols. The main approach for identifying vulnerabilities in operations leverages domain expertise using knowledge about how the system should behave within the expected tolerances to known safety margins. This approach works well when the system has a well-defined operating condition. However, the operations in the NAS can be highly complex with various nuances that render it difficult to clearly pre-define all known safety vulnerabilities. With the advancement of data science and machine learning techniques, the potential to automatically identify emerging vulnerabilities in the observed operations has become more practical in recent years. The state-of-the-art anomaly detection approaches in aerospace data usually rely on supervised or semi-supervised learning. However, in many real-world problems such as flight safety, creating labels for the data requires huge amount of effort and is largely impractical. To address this challenge, we developed a Convolutional Variational Auto-Encoder (CVAE), which is an unsupervised learning approach for anomaly detection in high-dimensional heterogeneous time-series data. We validate performance of CVAE compared to the state-of-the-art supervised learning approach as well as unsupervised clustering-based approach using KMeans++ and kernel-based approach using One-Class Support Vector Machine (OC-SVM) on Yahoo!'s benchmark time series anomaly detection data. Finally, we showcase performance of CVAE on a case study of identifying anomalies in the first 60 seconds of commercial flights' take-offs using Flight Operational Quality Assurance (FOQA) data",space,1099
,filtered,core,"Improving the Performance of Fine-Grain Image Classifiers via Generative
  Data Augmentation",2020-08-12 00:00:00,core,http://arxiv.org/abs/2008.05381,,"Recent advances in machine learning (ML) and computer vision tools have
enabled applications in a wide variety of arenas such as financial analytics,
medical diagnostics, and even within the Department of Defense. However, their
widespread implementation in real-world use cases poses several challenges: (1)
many applications are highly specialized, and hence operate in a \emph{sparse
data} domain; (2) ML tools are sensitive to their training sets and typically
require cumbersome, labor-intensive data collection and data labelling
processes; and (3) ML tools can be extremely ""black box,"" offering users little
to no insight into the decision-making process or how new data might affect
prediction performance. To address these challenges, we have designed and
developed Data Augmentation from Proficient Pre-Training of Robust Generative
Adversarial Networks (DAPPER GAN), an ML analytics support tool that
automatically generates novel views of training images in order to improve
downstream classifier performance. DAPPER GAN leverages high-fidelity
embeddings generated by a StyleGAN2 model (trained on the LSUN cars dataset) to
create novel imagery for previously unseen classes. We experimentally evaluate
this technique on the Stanford Cars dataset, demonstrating improved vehicle
make and model classification accuracy and reduced requirements for real data
using our GAN based data augmentation framework. The method's validity was
supported through an analysis of classifier performance on both augmented and
non-augmented datasets, achieving comparable or better accuracy with up to 30\%
less real data across visually similar classes. To support this method, we
developed a novel augmentation method that can manipulate semantically
meaningful dimensions (e.g., orientation) of the target object in the embedding
space",space,1100
,filtered,core,Efficient physics signal selectors for the first trigger level of the Belle II experiment based on machine learning,2020-10-21 00:00:00,core,https://core.ac.uk/download/395059242.pdf,Ludwig-Maximilians-Universität München,"A neural network based z-vertex trigger is developed for the first level trigger of the upgraded flavor physics experiment Belle II at the high luminosity B factory SuperKEKB in Tsukuba, Japan. Using the hit and drift time information from the central drift chamber, a pool of expert neural networks estimates the 3D track parameters of the single tracks found by a 2D Hough finder. The neural networks are already implemented on parallel FPGA hardware for real time data processing and running pipelined in the online first level trigger of Belle II. Due to the anticipated high luminosity of up to 8 × 10³⁵ cm⁻²s⁻¹, Belle II will have to face severe levels of background tracks with vertices displaced along the beamline. The neural z-vertex algorithm presented in this thesis allows to reject displaced background tracks such that the requirements of the standard track trigger can be strongly relaxed. Especially for physics decay channels with a low track multiplicity in the final states, like τ pair production, or initial state radiation events with reduced center of mass energies, the trigger efficiencies can be significantly increased.



As an upgrade of the present 2D Hough finder in the neural network preprocessing, a model independent 3D track finder is developed that uses the additional stereo hit information of the drift chamber. Thus, the trigger efficiencies improve for tracks in the phase space of low transverse momenta and shallow polar angles. Since the cross sections of the physics signal events typically increase towards shallow polar angles, this enlarged acceptance of the track trigger provides a substantial gain in the signal efficiencies. By using an adapted pool of expert networks, the enlarged phase space provided by the 3D finder can be efficiently covered.



Studies on simulated MC background, on simulated initial state radiation events, and on recorded data from early Belle II runs demonstrate the high performance of the novel trigger algorithms. With the 3D finder an increase of the track finding rate of about 50 % is confirmed for signal tracks, while displaced background tracks are actively suppressed prior to the neural network. Based on z-vertex cuts on the tracks processed by the neural networks, a two track event efficiency of more than 99 % can be achieved with a purity of around 80 %",space,1101
,filtered,core,"Collective Risk Minimization via a Bayesian Model for Statistical
  Software Testing",2020-05-15 00:00:00,core,http://arxiv.org/abs/2005.07460,,"In the last four years, the number of distinct autonomous vehicles platforms
deployed in the streets of California increased 6-fold, while the reported
accidents increased 12-fold. This can become a trend with no signs of subsiding
as it is fueled by a constant stream of innovations in hardware sensors and
machine learning software. Meanwhile, if we expect the public and regulators to
trust the autonomous vehicle platforms, we need to find better ways to solve
the problem of adding technological complexity without increasing the risk of
accidents. We studied this problem from the perspective of reliability
engineering in which a given risk of an accident has severity and probability
of occurring. Timely information on accidents is important for engineers to
anticipate and reuse previous failures to approximate the risk of accidents in
a new city. However, this is challenging in the context of autonomous vehicles
because of the sparse nature of data on the operational scenarios (driving
trajectories in a new city). Our approach was to mitigate data sparsity by
reducing the state space through monitoring of multiple-vehicles operations. We
then minimized the risk of accidents by determining proper allocation of tests
for each equivalence class. Our contributions comprise (1) a set of strategies
to monitor the operational data of multiple autonomous vehicles, (2) a Bayesian
model that estimates changes in the risk of accidents, and (3) a feedback
control-loop that minimizes these risks by reallocating test effort. Our
results are promising in the sense that we were able to measure and control
risk for a diversity of changes in the operational scenarios. We evaluated our
models with data from two real cities with distinct traffic patterns and made
the data available for the community.Comment: 12 pages, 14 figures, 15th International Symposium on Software
  Engineering for Adaptive and Self-Managing Systems (SEAMS2020",space,1102
,filtered,core,Global Robustness Verification Networks,2020-06-08 00:00:00,core,http://arxiv.org/abs/2006.04403,,"The wide deployment of deep neural networks, though achieving great success
in many domains, has severe safety and reliability concerns. Existing
adversarial attack generation and automatic verification techniques cannot
formally verify whether a network is globally robust, i.e., the absence or not
of adversarial examples in the input space. To address this problem, we develop
a global robustness verification framework with three components: 1) a novel
rule-based ``back-propagation'' finding which input region is responsible for
the class assignment by logic reasoning; 2) a new network architecture Sliding
Door Network (SDN) enabling feasible rule-based ``back-propagation''; 3) a
region-based global robustness verification (RGRV) approach. Moreover, we
demonstrate the effectiveness of our approach on both synthetic and real
datasets",space,1103
,filtered,core,'Association for Computing Machinery (ACM)',2019-01-01 00:00:00,core,scalpel-cd: leveraging crowdsourcing and deep probabilistic modeling for debugging noisy training data,10.1145/3308558.3313599,"This paper presents Scalpel-CD, a first-of-its-kind system that leverages both human and machine intelligence to debug noisy labels from the training data of machine learning systems. Our system identifies potentially wrong labels using a deep probabilistic model, which is able to infer the latent class of a high-dimensional data instance by exploiting data distributions in the underlying latent feature space. To minimize crowd efforts, it employs a data sampler which selects data instances that would benefit the most from being inspected by the crowd. The manually verified labels are then propagated to similar data instances in the original training data by exploiting the underlying data structure, thus scaling out the contribution from the crowd. Scalpel-CD is designed with a set of algorithmic solutions to automatically search for the optimal configurations for different types of training data, in terms of the underlying data structure, noise ratio, and noise types (random vs. structural). In a real deployment on multiple machine learning tasks, we demonstrate that Scalpel-CD is able to improve label quality by 12.9% with only 2.8% instances inspected by the crowd",space,1104
,filtered,core,,2019-01-01 00:00:00,core,the influence of the future on the past (criminal law aspect),,"The article attempts to offer answers to some provocative questions.
Is there in all cases a linearity in the deployment of certain mandatory features of the objective side of a specific corpus delicti (formal components), in particular, space as a crime scene, time, method, etc.? Could some of the mandatory attributes of the objective side of a specific criminal offense continue to be realized in time and space after the moment of its finish?
Is it possible in the future to influence the nature and degree of public danger of a crime committed in the past after a considerable period of time of its completion?
The author concludes that time in criminal law is a relational category.
This allows the dependence of the characteristics of space and time on the nature and method of interaction of objects, events, properties and relations.
Matter, space and time form a system of certain interactions. They depend on mutual movement. Time can be multidimensional in the same way as space. Thus, time can be determined using not one, but several quantities.
In certain cases, time in criminal law can be isotropic, that is, equal in all its possible directions of movement. In some cases, other mandatory attributes of the objective side of a specific criminal offense may continue to be realized in time and space even after the moment of its completion.
The rules of the current Criminal Code of Ukraine and the practice of their application indicate on possibility in the future to influence the nature and degree of public danger of a crime committed in the past, including after a significant period of time after its completion.
Thus, the author concludes that the future in criminal law can influence the past.
In the examples considered in the article with smuggling and escalation of theft into robbery, the author admits the possibility of the existence of a slight curvature of time-space, or a more or less long loop of time.
However, these cases are not time travel.
It is possible to return to the past only if history has already recorded such a return in any way. For example, an incomprehensible event occurred, which could not be properly explained, in due time. The possibility of such a return would continue to be rather inconvenient for criminal law and other branches shattering the tenet of human being’ free will, on which all theories of legal responsibility are based. Nowadays, such freedom is declared only because we cannot fully predict subsequent behavior of human being.
However, with the uprising of artificial intelligence, which is capable of processing huge contents of information regarding a specific subject, as well as due to significant achievements in the field of neurobiology, there is a real perspective not only to predict the future behavior of any person, but also to manage it (for example, when consuming goods and services, making political choices in a referendum, etc.). This creates new challenges to the legal system and human civilization as a whole.В статті здійснено спробу надати відповіді на питання, чи у всіх випадках має місце лінійність розгортання окремих обов’язкових ознак об’єктивної сторони конкретного складу злочину, зокрема, простору як місця вчинення злочину, часу, способу тощо, чи можуть продовжувати реалізуватися у часі та просторі інші, крім діяння та наслідку, обов’язкові ознаки об’єктивної сторони конкретного кримінального правопорушення вже після того, як настав момент його закінчення, чи є можливим у майбутньому вплинути на характер і ступінь суспільної небезпечності вчиненого у минулому злочину, в тому числі через значний проміжок часу після його закінчення.
Автор приходить до висновку про те, що час у кримінальному праві є реляційною категорією, і це допускає залежність характеристик простору і часу від характеру та способу взаємодії об’єктів, подій, властивостей та відносин. Матерія, простір і час утворюють систему певних відносин, вони залежать від взаємного руху, час може бути багатовимірним так само, як і простір. Таким чином, час може бути визначений за допомогою не однієї, а декількох величин. Час у кримінальному праві у певних випадках може бути ізотропним, тобто рівноправним у всіх своїх можливих напрямках руху. В окремих випадках можуть продовжувати реалізуватися у часі та просторі інші, крім діяння та наслідку, обов’язкові ознаки об’єктивної сторони конкретного кримінального правопорушення і це може мати місце вже після того, як настав момент його закінчення. Положення чинного КК України та практика їх застосування вказують на можливість у майбутньому вплинути на характер і ступінь суспільної небезпечності злочину, який був вчинений у минулому, в тому числі через значний проміжок часу після його закінчення. 
Таким чином, автор приходить до висновку про те, що майбутнє у кримінальному праві може впливати на минуле. В розглянутих у статті прикладах з контрабандою та переростанням крадіжки у розбій, автор припускає можливість існування незначного викривлення часу-простору, або більш чи менш тривалої петлі часу. При цьому згадані випадки не є подорожами у часі.
Повернутися у минуле можливо лише у тому випадку, якщо історія вже зафіксувала будь-яким чином таке повернення (наприклад, сталася незрозуміла подія, яку на той час не змогли належним чином пояснити). Можливість такого повернення продовжувала би доволі незручно для кримінального права та інших галузей розхитувати постулат про свободу волі людини, на якому базуються всі теорії юридичної відповідальності. Сьогодні про таку свободу стверджується лише тому, що ми не можемо повною мірою передбачити наступну поведінку тієї чи іншої особи. Втім, за появу штучного інтелекту, який є здатним обробляти величезні обсяги інформації відносно конкретної особи, а так само завдяки значним проривам у нейробіології та біоінженерії, виникає реальна перспектива не тільки передбачати майбутню поведінку будь-якої людини, але й керувати нею (наприклад, під час споживання товарів і послуг, здійснення політичного вибору на референдумі тощо), що утворює нові виклики правовій системі та людський цивілізації в цілому.В статье предпринята попытка предложить ответы на такие вопросы как, во всех ли случаях имеет место линейность развертывания отдельных обязательных признаков объективной стороны конкретного состава преступления, в частности, пространства как места совершения преступления, времени, способа и т.д., могут ли продолжать реализоваться во времени и пространстве иные, кроме деяния и его последствия, обязательные признаки объективной стороны конкретного уголовного правонарушения уже после того, как наступил момент его окончания, возможно ли в будущем повлиять на характер и степень общественной опасности совершенного в прошлом преступления, в том числе через значительный промежуток времени после его окончания.
Автор приходит к выводу о том, что время в уголовном праве является реляционной категорией, и это допускает зависимость характеристик пространства и времени от характера и способа взаимодействия объектов, событий, свойств и отношений. Материя, пространство и время образуют систему определенных взаимодействий, они зависят от взаимного движения, время может быть многомерным так же, как и пространство. Таким образом, время может быть определено с помощью не одной, а нескольких величин. Время в уголовном праве в определенных случаях может быть изотропным, то есть равноправным во всех своих возможных направлениях движения. В отдельных случаях могут продолжать реализоваться во времени и пространстве иные, кроме деяния и последствия, обязательные признаки объективной стороны конкретного уголовного преступления и это может иметь место уже после того, как наступил момент его окончания. Положения действующего УК Украины и практика их применения указывают на возможность в будущем повлиять на характер и степень общественной опасности преступления, совершенного в прошлом, в том числе через значительный промежуток времени после его окончания.
Таким образом, автор приходит к выводу о том, что будущее в уголовном праве может влиять на прошлое. В рассмотренных в статье примерах с контрабандой и перерастанием кражи в разбой автор допускает возможность существования незначительного искривления времени-пространства, или более или менее длительной петли времени. При этом упомянутые случаи не являются путешествиями во времени.
Вернуться в прошлое возможно лишь в том случае, если история уже зафиксировала любым способом такое возвращение (например, произошло непонятное событие, которое в свое время не смогли должным образом объяснить). Возможность такого возвращения продолжила бы довольно неудобно для уголовного права и других отраслей расшатывать постулат о свободе воли человека, на чем базируются все теории юридической ответственности. Сегодня о такой свободе утверждается только потому, что мы не можем в полной мере предсказать последующее поведение того или иного лица. Впрочем, с появлением искусственного интеллекта, способного обрабатывать огромные объемы информации в отношении конкретного субъекта, а так же, благодаря значительным достижениям в области нейробиологии и биоинженерии, возникает реальная перспектива не только предвидеть будущее поведение любого человека, но и управлять им (например, при потреблении товаров и услуг, осуществлении политического выбора на референдуме и т.д.), что создает новые вызовы правовой системе и человеческой цивилизации в целом",space,1105
,filtered,core,HAL CCSD,2019-07-08 00:00:00,core,milling diagnosis using machine learning approaches,,"International audienceThe Industry 4.0 framework needs new intelligent approaches. Thus, the manufacturing industries more and more pay close attention to artificial intelligence (AI). For example, smart monitoring and diagnosis, real time evaluation and optimization of the whole production and raw materials management can be improved by using machine learning and big data tools. An accurate milling process implies a high quality of the obtained material surface (roughness, flatness). With the involvement of AI-based algorithms, milling process is expected to be more accurate during complex operations.In this work, a smart milling diagnosis has been developed for composite sandwich structures based on honey-comb core. The use of such material has grown considerably in recent years, especially in the aeronautic, aerospace, sporting and automotive industries. But the precise milling of such material presents many difficulties. The objective of this work is to develop a data-driven industrial surface quality diagnosis for the milling of honey-comb material, by using supervised machine learning methods. Therefore, cutting forces and workpiece material vibrations are online measured in order to predict the resulting surface flatness.The workpiece material studied in this investigation is Nomex® honeycomb cores with thin cell walls. The Nomex® honeycomb machining presents several defects related to its composite nature (uncut fiber, tearing of the walls), the cutting conditions and to the alveolar geometry of the structure which causes vibration on the different components of the cutting effort.Given the low level of cutting forces, the quality of the obtained machined surface allows to establish criteria for determining the machinability of the honeycomb structures. Nearly 40 features are calculated in time domain and frequency domain from the raw signal in steady state behavior (transient zones are not taken into account). The features are then normalized. The input parameters for each experiment are: the tool rotation speed, the cutting speed and the depth of cut. It is then necessary to make a dimensional reduction of that feature table in order to avoid overfitting and to reduce the computing time of the learning algorithm.In this work, several classification algorithms have been implemented such as : k-nearest neighbor (kNN), Decision trees (DT), Support Vector Machine (SVM). The different supervised learning algorithms have been implemented and compared. Each AI-based model has been applied to a set of features. From the prediction results, SVM algorithm seems to be the most efficient algorithm in this application",space,1106
,filtered,core,'Wiley',2019-01-28 00:00:00,core,room temperature commensurate charge density wave in epitaxial strained tite 2 multilayer films,10.1002/admi.201801850,"International audienceDespite a large number of studies [2,3] over the years since the first discovery [7] and a couple of comprehensive reviews [8,9] the actual mechanism for PLD/CDW formation is still under debate. The most recent experimental [10-13] and theoretical [14] works focus on the large area growth of the CDW phase [13] the thickness dependence , and the possible unconventional behavior in the ultimate 2D limit of a single layer TiSe 2. [10-12,14] On the other hand, the other Ti dichalcogenides namely TiS 2 and TiTe 2 did not show any clear evidence until very recently when a CDW state was reported only for 1 monolayer (ML)-thin TiTe 2 at temperatures lower than 92 K. [15] It is surprising that the CDW in TiTe 2 was found to be totally suppressed for films thicker than 1 ML, [15] unlike the case of other TMDs where 1 ML and bulk-like films both make the transition to a CDW at nearly the same temperature. The interest about TiTe 2 is continuously increasing in view of theoretical predictions [16] and more recent experimental evidence [17] about pressure induced topological phase transitions in TiTe 2. The possibility to also manipulate superconduc-tivity by external pressure as predicted [18] and more recently evidenced [19] in bulk TiTe 2 creates the prospect to explore the emergence of topological superconductivity in this material. In the latter work [19] it has been shown that under nonhydro-static pressure, a CDW-like state with estimated transition temperature above room temperature (RT) appears in bulk TiTe 2 at around 0.5-1.8 GPa. These results call for a re-examination of the possibility to obtain a CDW in multilayer TiTe 2 and indeed at RT with good potential for real world applications utilizing the properties of the CDW state. These applications include a voltage-controlled oscillator device operating at room temperature , [20] fast electronic resistance switching for nonvolatile memories, [21,22] and field-effect transistor devices potentially suitable for implementation of non-Boolean logic. [23] In this paper it is shown that multilayer films (50 ML ≈ 32 nm), as well as single layer TiTe 2 epitaxially grown on InAs(111)/ Si(111) substrates by molecular beam epitaxy exhibit, in ambient pressure conditions, a CDW distortion at room temperature which is sustained up to higher temperatures, at least 400 °C, as evidenced by reflection high energy electron diffrac-tion (RHEED) (Figure S1, Supporting Information). The results are explained in terms of anisotropic strain imposed by the substrate. The group IVB 2D transition metal dichalcogenides are considered to be stable in the high symmetry trigonal octahedral structure due to the lack of unpaired d-electrons on the metal site. It is found that multilayer epitaxial TiTe 2 is an exception adopting a commensurate 2 × 2 × 2 charge density wave (CDW) structure at room temperature with an ABA type of stacking as evidenced by direct lattice imaging and reciprocal space mapping. The CDW is stabilized by highly anisotropic strain imposed by the substrate with an out-off-plane compression which reduces the interlayer van der Waals gap increasing the coupling between TiTe 2 layers",space,1107
,filtered,core,,2019-01-01 00:00:00,core,"optimization of the memory subsystem of a coarse
grained reconfigurable hardware accelerator",,"Fast and energy efficient processing of data has always been a key requirement in processor design. The latest developments in technology emphasize these requirements even further.
The widespread usage of mobile devices increases the demand of energy efficient solutions. Many new applications like advanced driver assistance systems focus more and more on machine learning algorithms and have to process large data sets in hard real time.
Up to the 1990s the increase in processor performance was mainly achieved by new and better manufacturing technologies for processors. That way, processors could operate at higher clock frequencies, while the processor microarchitecture was mainly the same. At the beginning of the 21st century this development stopped. New manufacturing technologies made it possible to integrate more processor cores onto one chip, but almost no improvements were achieved anymore in terms of clock frequencies. This required new approaches in both processor microarchitecture and software design. Instead of improving the performance of a single processor, the current problem has to be divided into several subtasks that can be executed in parallel on different processing elements which speeds up the application.

One common approach is to use multi-core processors or GPUs (Graphic Processing Units) in which each processing element calculates one subtask of the problem. This approach requires new programming techniques and legacy software has to be reformulated.
Another approach is the usage of hardware accelerators which are coupled to a general purpose processor. For each problem a dedicated circuit is designed which can solve the problem fast and efficiently. The actual computation is then executed on the accelerator and not on the general purpose processor. The disadvantage of this approach is that a new circuit has to be designed for each problem. This results in an increased design effort and typically the circuit can not be adapted once it is deployed.

This work covers reconfigurable hardware accelerators. They can be reconfigured during runtime so that the same hardware is used to accelerate different problems. During runtime, time consuming code fragments can be identified and the processor itself starts a process that creates a configuration for the hardware accelerator. This configuration can now be loaded and the code will then be executed on the accelerator faster and more efficient.
A coarse grained reconfigurable architecture was chosen because creating a configuration for it is much less complex than creating a configuration for a fine grained reconfigurable architecture like an FPGA (Field Programmable Gate Array). Additionally, the smaller overhead for the reconfigurability results in higher clock frequencies.
One advantage of this approach is that programmers don't need any knowledge about the underlying hardware, because the acceleration is done automatically during runtime. It is also possible to accelerate legacy code without user interaction (even when no source code is available anymore).

One challenge that is relevant for all approaches, is the efficient and fast data exchange between processing elements and main memory.
Therefore, this work concentrates on the optimization of the memory interface between the coarse grained reconfigurable hardware accelerator and the main memory. To achieve this, a simulator for a Java processor coupled with a coarse grained reconfigurable hardware accelerator was developed during this work.
Several strategies were developed to improve the performance of the memory interface. The solutions range from different hardware designs to software solutions that try to optimize the usage of the memory interface during the creation of the configuration of the accelerator.
The simulator was used to search the design space for the best implementation. With this optimization of the memory interface a performance improvement of  22.6% was achieved.

Apart from that, a first prototype of this kind of accelerator was designed and implemented on an FPGA to show the correct functionality of the whole approach and the simulator",space,1108
,filtered,core,'Institute of Electrical and Electronics Engineers (IEEE)',2019-11-20 00:00:00,core,incremental and transfer learning of contextual skill model for robots,10.1109/HUMANOIDS.2016.7803277,"The thesis studies building blocks for robot skill learning. Using these key components, learning frameworks can be constructed which provide robots with the capability to acquire a motion and manipulation skill autonomously. We study skill learning in two contexts: in-contact and free-space motions. In brief, this thesis investigates how to: (1) learn a policy for in-contact tasks; (2) generalize a free-space motion policy to new situations using a contextual skill model (CSM); and (3) transfer the CSM from simulation to real world. 

Learning an in-contact task such as wood planing from scratch can be time-consuming and dangerous. This problem can be avoided by imitating a policy from a human demonstration. However, a mere imitation may not satisfy the objective of the corresponding in-contact task. The thesis proposes a reinforcement learning (RL) framework for improving the performance of an imitated in-contact policy. The policy search for in-contact tasks has been achieved by making the motion compliant which allows for exploration in the force profile. 

Generalizing a policy to new situations is fundamental to skill learning as it alleviates the need to learn a new policy in every novel situation. Generalizing a policy refers to synthesizing a function mapping the policy to new situations. The function is referred to as a contextual policy or contextual skill model (CSM). The thesis proposes a parametric CSM. Experiments demonstrated that the parametric CSM can extract a global pattern from a database (DB) of policy parameters leading to significantly better extrapolation capability than with non-parametric CSMs. Furthermore, the underlying model of the CSM is fitted to the DB using a novel model selection approach to better represent the underlying regularities of the task. In order to speed the process of learning, the prediction uncertainty of the CSM is calculated using empirical Bayes (EB) and employed for guiding the exploration process of a model-free policy search. In addition, the most promising task is selected using a novel task manager, allowing for better future generalization performance achieved with minimum effort. In essence, the thesis presents an incremental learning framework,the main components of which are as follows: CSM, policy search, model selection, DB, EB, and a task manager implemented using active learning.

Learning a policy in a simulated environment and transferring it to the real world will alleviate the need to learn from scratch or from a demonstration. The thesis proposes to transfer a CSM instead of transferring a single control policy. We developed a simulation-to-real transfer framework which learns a source CSM in simulation incrementally and transfers it to the real world incrementally. Transference of the source CSM has been achieved using sample policies from the target environment. Experiments indicated that one sample policy is sufficient to transfer a CSM to the target environment. The target CSM improved the extrapolation capability significantly better than zero-shot transfer",space,1109
,filtered,core,International Foundation for Telemetering,2019-10-01 00:00:00,core,multi-stage attack detection using layered hidden markov model intrusion detection system,,"Intrusion Detection Systems (IDS) based on Artificial Intelligence can be deployed to protect telemetry networks against intruders. As security solutions which encrypt radio links do not accommodate the ever evolving network attacks and vulnerabilities, new defense mechanisms using machine learning and artificial intelligence can play a significant role for telemetry networks. This paper proposes a multi-layered Hidden Markov Model (HMM) IDS that addresses multi-stage attacks. This is due to the fact that intrusions are increasingly being launched through multiple phases instead of single stage intrusion. This layered model divides the problem space into smaller manageable pieces reducing the curse of dimensionality associated with HMMs. To verify the application of this model for real network, the NSL-KDD dataset is used to train and test the model.International Foundation for TelemeteringProceedings from the International Telemetering Conference are made available by the International Foundation for Telemetering and the University of Arizona Libraries. Visit http://www.telemetry.org/index.php/contact-us if you have questions about items in this collection",space,1110
,filtered,core,"eScholarship, University of California",2020-01-01 00:00:00,core,machine learning in compiler optimization,,"The end of Moore's law is driving the search for new techniques to improve system performance as applications continue to evolve rapidly and computing power demands continue to rise. One promising technique is to build more intelligent compilers.Compilers map high-level programs to lower-level primitives that run on hardware. During this process, compilers perform many complex optimizations to boost the performance of the generated code. These optimizations often require solving NP-Hard problems and dealing with an enormous search space. To overcome these challenges, compilers currently use hand-engineered heuristics that can achieve good but often far-from-optimal performance. Alternatively, software engineers resort to manually writing the optimizations for every section in the code, a burdensome process that requires prior experience and significantly increases the development time.In this thesis, novel approaches for automatically handling complex compiler optimization tasks are explored. End-to-end solutions using deep reinforcement learning and other machine learning algorithms are proposed. These solutions dramatically reduce the search time while capturing the code structure, different instructions, dependencies, and data structures to enable learning a sophisticated model that can better predict the actual performance cost and determine superior compiler optimizations. The proposed techniques can outperform existing state-of-the-art solutions while requiring shorter search time. Furthermore, unlike existing solutions, the deep reinforcement learning solutions are shown to generalize well to real benchmarks",space,1111
,filtered,core,'Hindawi Limited',2020-01-01 00:00:00,core,visual object multimodality tracking based on correlation filters for edge computing,10.1155/2020/8891035,"In recent years, visual object tracking has become a very active research field which is mainly divided into the correlation filter-based tracking and deep learning (e.g., deep convolutional neural network and Siamese neural network) based tracking. For target tracking algorithms based on deep learning, a large amount of computation is required, usually deployed on expensive graphics cards. However, for the rich monitoring devices in the Internet of Things, it is difficult to capture all the moving targets in each device in real time, so it is necessary to perform hierarchical processing and use tracking based on correlation filtering in insensitive areas to alleviate the local computing pressure. In sensitive areas, upload the video stream to a cloud computing platform with a faster computing speed to perform an algorithm based on deep features. In this paper, we mainly focus on the correlation filter-based tracking. In the correlation filter-based tracking, the discriminative scale space tracker (DSST) is one of the most popular and typical ones which is successfully applied to many application fields. However, there are still some improvements that need to be further studied for DSST. One is that the algorithms do not consider the target rotation on purpose. The other is that it is a very heavy computational load to extract the histogram of oriented gradient (HOG) features from too many patches centered at the target position in order to ensure the scale estimation accuracy. To address these two problems, we introduce the alterable patch number for target scale tracking and the space searching for target rotation tracking into the standard DSST tracking method and propose a visual object multimodality tracker based on correlation filters (MTCF) to simultaneously cope with translation, scale, and rotation in plane for the tracked target and to obtain the target information of position, scale, and attitude angle at the same time. Finally, in Visual Tracker Benchmark data set, the experiments are performed on the proposed algorithms to show their effectiveness in multimodality tracking",space,1112
,filtered,core,,2020-03-27 00:00:00,core,εξερεύνηση αρχιτεκτονικών προγραμματιζόμενης λογικής για αποδοτική επιτάχυνση εφαρμογών υψηλών επιδόσεων,,"Οι Διατάξεις Προγραμματιζόμενης Λογικής (Field Programmable Gate Arrays -
FPGAs) έχουν αυξανόμενο αντίκτυπο σε όλο και περισσότερες εφαρμογές, από τα
νευρωνικά δίκτυα (Deep Neural Networks) έως επεκτάσεις του ρεπερτορίου εντολών
(Instruction Set Extensions) σε στενώς συνεζευγμένα συστήματα με ενσωματωμένες
FPGAs (eFPGAs). Καθώς οι εφαρμογές αποκλίνουν στην πολυπλοκότητα, τις επιδόσεις,
τις ανάγκες μνήμης και τους περιορισμούς σε έκταση, υπάρχει ανάγκη για ένα ευρύτερο
φάσμα αρχιτεκτονικών FPGA. Ωστόσο, η ανάπτυξη και υλοποίηση των αρχιτεκτονικών
αυτών παραμένει δύσκολη και απαιτεί πολύ χρόνο, λόγω των υψηλών απαιτήσεών τους
σε ειδικευμένες σχεδιάσεις (custom layout) και της ανάγκης ανάπτυξης λογισμικού 
προσαρμοσμένου για τον προγραμματισμό κάθε αρχιτεκτονικής, οδηγώντας στην
παραγωγή προϊόντων πιο γενικού σκοπού.
Πολλές ακαδημαϊκές εργασίες επικεντρώνονται στην αυτοματοποιημένη
διαδικασία παραγωγής αρχιτεκτονικών FPGA, σε μια προσπάθεια να προωθηθεί η
εξατομίκευση και να μειωθεί η χρονική περίοδος μέχρι την αγορά. Σε άλλες
προσεγγίσεις, οι ερευνητές στοχεύουν στη διαδικασία εξερεύνησης, στην οποία
αναζητούν τη βέλτιστη αρχιτεκτονική για ένα συγκεκριμένο σενάριο, χρησιμοποιώντας
μοντέλα εκτιμήσεων μεγέθους και καθυστέρησης.
Στην εργασία αυτή επιλέγουμε να συνδυάσουμε τις δύο αυτές προσεγγίσεις.
Αναπτύσσουμε μια επέκταση για το δημοφιλές εργαλείο ανοιχτού κώδικα Verilog-toRouting (VTR) προκειμένου να εξάγουμε σε Verilog την αναπαράσταση των
αρχιτεκτονικών FPGA που έχουν οριστεί από τον χρήστη, να υποστηρίξουμε ζητούμενες
μονάδες ειδικού σκοπού (hard blocks - RAMs, DSPs, FP Units) και να παράξουμε αρχεία
προγραμματισμού της FPGA (Bitstreams) για δοθέντα benchmarks. Στόχος μας είναι να
δημιουργήσουμε συνθέσιμο κώδικα RTL ανεξάρτητο από τεχνολογία, ικανό να συντεθεί
με οποιαδήποτε βιβλιοθήκη Standard Cells. Ανακαλύπτουμε τις πραγματικές
σχεδιαστικές ιδιότητες μιας αρχιτεκτονικής FPGA χρησιμοποιώντας μια προτεινόμενη
ροή σχεδιασμού υλικού (ASIC flow) και ανακτούμε πραγματικές μετρήσεις μεγέθους και
καθυστέρησης και τελικά προχωρούμε στην εξερεύνηση των βέλτιστων αρχιτεκτονικών
FPGA για συγκεκριμένα σύνολα από benchmakrs.
Χρησιμοποιώντας την επέκτασή μας, εξερευνούμε το χώρο σχεδιασμού των FPGA
για ένα σύνολο από benchmakrs υψηλών επιδόσεων που εξάγονται από την πλατφόρμα
High Level Synthesis (HLS) της Xilinx. Η εξερεύνηση μας αρχίζει με τον εντοπισμό των
βέλτιστων αρχιτεκτονικών FPGA, ξεκινώντας από το μέγεθος των προγραμματιζόμενων
πυλών (Lookup Tables - LUTs) και τον αριθμό τους ανά ομάδα (Configurable Logic Block -
CLB) και έπειτα εξετάζοντας το μέγεθος και μήκος των καναλιών διασύνδεσης τους.
Συγκρίνουμε επίσης τις βέλτιστες αρχιτεκτονικές FPGA που προκύπτουν κατά τη χρήση
των benchmakrs υψηλών επιδόσεων με τις αντίστοιχες αρχιτεκτονικές που προκύπτουν
όταν χρησιμοποιούμε τα MCNC benchmarks γενικού σκοπού.
Τέλος, δημιουργούμε σύνολα εντολών TCL για τη σύνθεση και την υλοποίηση της
τοποθέτησης και διασύνδεσης (place and route) που μπορούν να προσαρμοστούν σε
οποιοδήποτε μέγεθος και χαρακτηριστικό αρχιτεκτονικής και να αυτοματοποιήσουν τη
ροή ASIC για νέα τσιπ FPGAField Programmable Gate Arrays (FPGAs) have an ever-expanding impact to more
and more applications, ranging from Deep Neural Networks to High-Performance
Computing (HPC) and other uses such as customization of Instruction Set Extensions and
computation offloading in systems with tightly coupled embedded FPGAs (eFPGAs). As
applications diverge in complexity, performance, memory needs and area limitations,
there is a need for a wider variety of FPGA architectures. However, developing and
implementing new FPGA architectures remains challenging and requires a lot of time, due
to their high content in custom layout designs and the need for design software and flows
tailored for each specific architecture, leading to the production of more generic
products.
Many academic works are focusing on the automated FPGA design generation
process, in an attempt to promote customizability and reduce time-to-market. In other
approaches, researchers target only the exploration process, in which they seek for the
optimal architecture for a specific case scenario, using area and delay estimation models.
In this thesis we choose to combine the two approaches. We develop an extension
for the popular open-source tool Verilog-to-Routing (VTR) in order to export in Verilog
the representation of user-specified FPGA architectures, develop support for custom user 
hard-blocks (RAMs, DSPs, FP Units), and generate Bitstreams for given benchmarks. Our
objective is to create synthesizable and technology independent RTL design code, able to
be synthesized with any standard cell library. We discover the real design properties of
an FPGA architecture using our proposed ASIC flow and retrieve real area and delay
measurements and eventually proceed with the exploration of optimal FPGA
architectures for given sets of benchmarks.
Using our VTR extension, we perform FPGA design-space exploration for a set of
HPC oriented benchmarks that are derived using Xilinx's High Level Synthesis (HLS). Our
exploration starts by identifying pareto-optimal FPGA architectures starting with the size
of Lookup Tables (LUTs) and the number of LUTs per Configurable Logic Block (CLB) and
then explore the size of routing channels and wire segments' configurations. We also
compare the optimal FPGA architectures derived when using the HPC benchmarks with
the respective architectures derived when we use the generic MCNC benchmarks.
Finally, we create TCL scripts for synthesis and back-end implementation (place
and route) which can adjust to any architectural characteristic and size and automate the
ASIC flow for new FPGA chips",space,1113
,filtered,core,'Pisa University Press',2020-02-16 00:00:00,core,fpga extension of a battery management system for fail-operational control of lithium-ion batteries in safety-critical applications,,"La crescente attenzione riguardo alle tematiche ambientali sta portando sempre più all’attenzione i problemi legati all’inquinamento, specialmente per quanto riguarda l’emissione dei gas serra. Una delle principali cause dell’aumento di gas serra è senza dubbio l’utilizzo dei combustibili fossili, anche nel campo della mobilità. Per questo motivo, negli ultimi decenni, sono state cercate alternative più ecologiche: la tendenza attuale è senza dubbio quella di muoversi in direzione della trazione elettrica, e in special modo a guida autonoma. Il lavoro oggetto di tesi è appunto inquadrato all’interno del progetto europeo AutoDrive, il quale mira alla progettazione di componenti elettronici e architetture di tipo Fail-Operational, ovvero che continuano a eseguire un insieme definito delle loro funzioni anche in presenza di guasti, che permettano l’introduzione della guida autonoma in autoveicoli di tutte le categorie, con l’intento di contribuire a una mobilità più efficiente e sicura. Infatti, dato il crescente numero di implementazioni software e meccatroniche all’interno delle automobili, sono presenti sempre più rischi dovuti a loro possibili guasti, sia sistematici che aleatori, che devono quindi essere tenuti in considerazione al fine della sicurezza funzionale. Poiché quest’ultima venga garantita è necessario introdurre all’interno del sistema, in modo controllato, un qualche tipo di ridondanza che permetta di mascherare o individuare i guasti che si possono verificare.
La possibilità di realizzare veicoli a trazione elettrica è strettamente legata ai progressi conseguiti nel campo delle batterie, specialmente per quanto riguarda le tecnologie basate sugli ioni di Litio. Infatti, quest’ultime presentano una maggiore densità di energia e di potenza, una tensione di cella più elevata, la mancanza di effetto memoria e una minore corrente di auto scarica se confrontate con le altre chimiche esistenti. Grazie a questa serie di vantaggi, le tecnologie basate sugli ioni di Litio, rappresentano l’unico vero candidato per il futuro della mobilità elettrica. Questa tecnologia, tuttavia, presenta degli svantaggi in quanto è necessario l’inserimento di un sistema elettronico che monitori la batteria, il Battery Management System (BMS), il quale ha, tra gli altri, il compito di garantirne il corretto funzionamento in termini di range operativi di tensione, temperatura e corrente. Difatti, la fuoriuscita di queste grandezze dalla loro Safe Operating Area (SOA), oltre a portare un degradamento delle prestazioni della batteria, può provocare l’innescarsi di condizioni ben più gravi quali fughe termiche interne alle celle o persino l’esplosione delle celle stesse. Qualora il BMS identifichi una situazione critica per l’operatività della batteria, questo sistema interviene attraverso un sistema di feedback sul circuito, andando ad esempio a distaccare il carico. Il BMS misura costantemente tutte le grandezze fisiche delle celle che compongono la batteria, quali corrente, tensione e temperatura. A partire da queste, oltre a verificare se i dati sono all’interno della loro SOA, questo sistema esegue degli algoritmi di stima dello stato della batteria andando a ricavare dei parametri tipici quali lo stato di carica (SoC) e lo stato di salute (SoH) e, qualora fosse richiesto, si occupa di loggare e comunicare queste informazioni agli altri blocchi che compongono il sistema.
Nel caso in cui si voglia realizzare un intero sistema avente una batteria agli ioni litio e che presenti un comportamento di tipo Fail-Operational, come quello in esame nel progetto Autodrive, anche il BMS deve avere questa caratteristica. Per questo motivo, partendo dunque dalla struttura convenzionale del BMS, sono state aggiunte strutture ridondanti al fine di raggiungere l’obbiettivo preposto. La ridondanza può essere classificata in due grandi categorie, spaziale e temporale: la prima involve l’introduzione all’interno del sistema di componenti, o funzioni, che sarebbero inutili in ambienti privi di guasti, mentre la seconda è basata sulla ripetizione delle operazioni eseguite e il confronto con il risultato precedente. Specialmente in applicazioni safety-critical, quali quella automotive, la ridondanza di tipo spaziale è necessaria al fine di raggiungere i massimi livelli di sicurezza stabiliti dagli standard, quali ISO 26262.
Per questo motivo la struttura convenzionale del BMS è stata replicata e un’estensione del BMS stesso, che ricopre un ruolo decisionale, è stata sviluppata su una piattaforma FPGA. La scelta di utilizzare un FPGA invece di un microcontrollore, porta numerosi vantaggi, tra i quali un incremento delle capacità computazionali e una maggiore flessibilità e riconfigurabilità del sistema sviluppato. Nel dettaglio è stato scelto di modificare la struttura del BMS andando a sviluppare un sistema in triplice ridondanza nel quale, come suggerisce il nome, si ha una triplicazione parallela del sistema. I tre flussi di dati ottenuti vengono sottoposti a un sistema di voting a maggioranza, dal quale si ottiene un unico flusso di uscita, permettendo dunque di mascherare un guasto su uno dei tre ingressi. A questo scopo sono state sviluppate delle periferiche hardware su FPGA per eseguire le operazioni di voting sui dati e per stimare la regione di lavoro della batteria. È stata inoltre sviluppata una interfaccia grafica su PC, la quale permette di configurare opportunamente le periferiche implementate e mostra in tempo reale tutte le informazioni riguardo alla batteria, tra le quali i valori di tensione delle celle, le temperature e lo stato del sistema. Il sistema è stato infine testato andando a triplicare virtualmente il BMS convenzionale a nostra disposizione e, manipolando i dati su ciascuna linea in modo controllato, è stato verificato il corretto funzionamento delle periferiche sviluppate e validata l’architettura proposta.
#english version#
The growing awareness about the environmental issues is bringing to the attention the pollution topics, especially the greenhouse gas emission. One of the main causes is fossil fuel consumption for the energy production, even in the automotive systems. For this reason, during the last decades greener alternatives have been sought, and the actual trend is the one that leads to the electrical traction, especially towards the autonomous driving system. The thesis work is part of the European project AutoDrive, which aims at designing Fail-Operational electronic components and system architectures, that enables the introduction of automated driving in all car categories to make future mobility more efficient and safer. It is said that a system presents a Fail-Operational behaviour if it continues to execute a defined set of its function even in presence of faults. Given the increasing number of software and mechatronic implementations within the automotive systems, there are increasing risks from systematic failures and random hardware failures that must be considered within the scope of functional safety. Since this last one must be guaranteed, the introduction within the system of some kind of redundancy is mandatory in order to detect or mask the possible faults.
The possibility of developing electrical traction vehicles is closely related to the progress made in the battery field, especially with regards to the Lithium-ion (Li-ion) based technologies. In fact, these types of cells present a higher energy and power density, an higher cell voltage, no memory effect and a lower auto discharge current compared to the other chemistries. Thanks to these advantages, Li-ion based technologies are the only real candidate for the future electrical mobility. However, this chemistry also brings some disadvantages since a battery monitoring system, the Battery Management System (BMS), is mandatory. This system has to ensure the maintenance of the all cells composing the battery pack within their operative ranges in terms of voltages, temperatures and current. In fact, the coming out of one or more of these measures from its Safe Operating Area (SOA) brings a degradation of the cell’s performance, and could also lead to hazardous conditions, such us thermal runaway within the cell itself or even explosions. If the BMS identifies a critical condition for the battery functionality, it acts on the circuit though a feedback system, for example disconnecting the load. The BMS also uses the acquired measures in order to estimate typical battery parameters, such as State of Charge (SoC) and State of Health (SoH), and, if required, it also provides logging functionality and communicates to the other blocks composing the system.
In the case of a system containing a Li-ion battery with Fail-Operational behaviour has to be developed, such as the project AutoDrive one, even the BMS must show this characteristic. For this reason, starting from a conventional BMS, redundant structures have been added to the system in order to reach the responsible goal.
Redundancy can be classified into two main categories: space redundancy and time redundancy. The former involves the introduction within the system of components, or functions, that would be useless in a fault-free environment, while the latter is based on the repetition of the tasks and the comparison of the results to a stored copy of the previous ones. Especially in safety-critical applications, such as automotive, space redundancy is mandatory in order to ensure the safety level required by standards, such as ISO 26262. Therefore, the conventional structure of BMS has been replicated and an extension of BMS itself, which acts as decisional unit, has been developed on a FPGA platform. The FPGA approach, compared to a microcontroller-based one, brings several advantages, such as an increased computational capability and a higher flexibility and reconfigurability of the developed system. More specifically, the conventional structure of the BMS has been modified by using a Triple Modular Redundancy (TMR) approach. As the name suggests, TMR involves the triplication of the components to perform the same computation in parallel. The three obtained data flows are then subjected to a majority voting unit, which provides a single data flow as output, allowing to mask a fault in one of the inputs. For this purpose, hardware peripherals within the FPGA fabric have been developed in order to execute voting and operating area estimation algorithms. Furthermore, a PC graphical user interface has been developed and it allows to configure the hardware peripherals and to show real-time information about the battery pack, such as cell voltages, temperatures and current. The system has been finally tested by virtually triplicating the conventional BMS at our disposal and, manipulating each data flow in a controlled manner, the proper functioning of the developed peripherals has been verified and the proposed architecture has been validated",space,1114
,filtered,core,,2020-02-09 00:00:00,core,solving a multi-attribute vehicle routing problem in the freight delivery industry,,"Freight transportation industry is characterized by several decisional problems that operations managers have to cope with. Not only the routes planning must be realized before their execution, but also other types of decisions must be taken, in order to answer events that may dynamically occur during operations, as for instance road network congestion or vehicle

failures. Each decision can involve different aspects: for instance, the price negotiation of a just-in-time order should take into consideration the current routes status and planning. Off-the-shelf decision support software, although able to independently support the decision makers in each area, tend to keep tasks compartmentalized.

Trans-Cel, a small trucking company in Padova (Italy), has a Research and Development branch developing a cloud-based platform, called Chainment, able to host different decision support tools that can communicate through a data sharing system. These tools rely on an algorithmic engine that includes a routing optimization algorithm and artificial intelligence systems. In particular, the routing problem combines express couriers requirements, generally studied in urban contexts, with routes and vehicle features typical of medium- and long-haul trips, showing interesting characteristics that are worth of study in the Operation Research field.

In this thesis, we focus on the design of an optimization algorithm able to provide a solution to a Vehicle Routing Problem (VRP) inspired by the Trans-Cel scenario, that we name Express Pickup and Delivery in freight Trucking problem (EPDT).

The classical VRP definition includes a set of customers and a fleet of vehicles and aims to define a set of routes such that all customers are visited exactly once while minimizing the overall distance traveled. In the scientific literature, the basic definition of the problem has been generalized in order to consider additional attributes, often rising from real-world scenarios, as for instance capacity of vehicles, time windows and orders with both pickup and delivery operations. Often, in real-world cases, decision makers must simultaneously deal with a large number of attributes, thus defining a class of routing problems called Multi-Attribute VRP (MAVRP), which includes EPDT.

The thesis proposes a meta-heuristic algorithm for the solution of EPDT, with the aim of embedding it in the algorithmic engine of Chainment. In order to comply with the platform  requirements, the algorithm is designed so that a solution is returned within few seconds.

The solution method we propose consists of a two-level heuristic: at the first level, a Tabu Search algorithm hybridized with a Variable Neighborhood Descent explores the order-to-vehicle assignments, while, at the second level, it makes use of a Local Search to determine the sequence of customers visited and obtain an evaluation of routes.

The algorithm efficiency is enhanced by the use of a granular exploration, by procedures for fast evaluation of solutions in the neighborhoods, and parallel implementation of specific algorithmic components. These elements are adapted to the specific attributes of EPDT and represent some of the thesis contributions. The improvement in computational times have been validated by the experimental results, verifying the desired requirements for the platform integration.

The quality of the solutions obtained with the proposed meta-heuristic algorithm has been assessed both on the field, by comparison with Trans-Cel operations managers, and through bounds obtained with mathematical programming methods. To this purpose, the thesis proposes an Integer Linear Programming formulation for EPDT and a solution method for its continuous

relaxation based on Column Generation. In particular, the thesis presents new pricing procedures suitable for the specific EPDT attributes. The available bounds show optimality or near-optimality of solutions provided by the heuristic algorithm for real instances. Moreover, the algorithm has been tested on literature benchmarks related to the Pickup and Delivery

Problem with Time Windows (PDPTW), providing solutions that are competitive with the state-of-the-art.

The thesis also proposes a preliminary study of new approaches for vehicle routing problems in dynamic contexts. In particular, the thesis explores the possibility of taking advantage from the availability of historical data on orders by means of anticipatory strategies. The first strategy is based on clustering methods that are applied to the orders to define space-time

points that aggregate the information on future demand. A second strategy is based on the concept of accessibility, as defined in the discrete choice theory and urban logistic, to represent the route capability of intercepting future orders.

The heuristic algorithm proposed for EPDT has been integrated in the algorithmic engine of the Chainment platform at Trans-Cel. The thesis describes integration and the adaptation of the proposed optimization algorithms for a proper interaction with the different modules in the operational context handled by the platform, as, for instance, initial routes planning, reacting to dynamic events or order price negotiation",space,1115
,filtered,core,,2020-03-06 00:00:00,core,"getting humanoids to move and imitate this behavior-based approach to structuring and controlling complex robotic systems uses imitation for interaction and learning. the use of basis behaviors, or primitives, lets these humanoid robots dance the macarena",,"AS ROBOTS INCREASINGLY BE- come part of our everyday lives, they will serve as caretakers for the elderly and disabled, assistants in surgery and rehabilitation, and educational toys. But for this to happen, programming and control must become simpler and human-robot interaction more natural. Both challenges are particularly relevant to humanoid robots, which are highly difficult to control yet most natural for interaction with people and operation in human environments. As this article shows, we have used biologically inspired notions of behavior-based control to address these challenges at the University of Southern California&apos;s Interaction Lab, part of the USC Robotics Research Labs. By endowing robots with the ability to imitate, we can program and interact with them through human demonstration, a natural human-humanoid interface. The human ability to imitate-to observe and repeat behaviors performed by a teacher-is a poorly understood but powerful form of skill learning. Two fundamental open problems in imitation involve interpreting and understanding the observed behavior and integrating the visual perception and movement control systems to reconstruct what was observed. Our research has a similarly twofold goal: we are developing methods for segmenting and classifying visual input for recognizing human behavior as well as methods for structuring the motor control system for general movement and imitation-learning capabilities. Our approach brings these two pursuits together much as the evolutionary process brought them together in biological systems. 1,2 We structure the motor system into a collection of movement primitives, which then serve both to generate the humanoid&apos;s movement repertoire and to provide prediction and classification capabilities for visual perception and interpretation of movement. This way, what the humanoid can do helps it understand what it sees and vice versa. The more it sees, the more it learns to do, and thus the better it gets at understanding what it sees for further learning; this is the imitation process. Behavior-based robotics Our work over the last 15 years has focused on developing distributed, behavior-based methods for controlling groups of mobile robots and, most recently, humanoids. Behavior-based control involves the design of control systems consisting of a collection of behaviors.  THIS BEHAVIOR-BASED APPROACH TO STRUCTURING IEEE INTELLIGENT SYSTEMS The inspiration for behavior-based control comes from biology, where natural systems are believed to be similarly organized, from spinal reflex movements up to more complex behaviors such as flocking and foraging.  Basis behaviors and primitives. Several methods for principled behavior design and coordination are possible.  Collections of behaviors are a natural representation for controlling collections of robots. But how can we use the same idea in the humanoid control domain, where the body&apos;s individual degrees of freedom are more coupled and constrained? For this, we have combined the notion of primitives with another line of evidence from neurosciencemirror neurons-to structure humanoid motor control into a general and robust system capable of a variety of skills and learning by imitation. 6 Humanoid control and imitation. Robot control is a complex problem, involving sensory and effector limitations and various forms of uncertainty. The more complex the system to be controlled, the more we must modularize the approach to make control viable and efficient. Humanoid agents and robots are highly complex; a human arm has seven degrees of freedom (DOF), the hand has 23, and the control of an actuated human spine is beyond current consideration. Yet humans display complex dynamic behaviors in real time and learn various motor skills throughout life, often through imitation. Methods for automating robot programming are in high demand. Reinforcement learning, which lets a robot improve its behavior based on trial-and-error feedback, is very popular. However, reinforcement learning is slow, as the robot must repeatedly try various behaviors in different situations. It can also jeopardize the robot. In contrast, learning by imitation is particularly appealing because it lets the designer specify entire behaviors by demonstration, instead of using low-level programming or trial and error by the robot. In biological systems, imitation appears to be a complex learning mechanism that involves an intricate interaction between visual perception and motor control, both of which are complex in themselves. Although various animals demonstrate simple mimicry, only a very few species, including humans, chimps, and dolphins, are capable of so-called true imitation, which involves the ability to learn arbitrary new skills by observation.  Neuroscience inspiration Evidence from neuroscience studies in animals points to two neural structures we find of key relevance to imitation: spinal fields and mirror neurons. Spinal fields, found in frogs and rats so far, code for complete primitive movements (or behaviors), such as reaching and wiping.  Investigators recently found neurons with so-called mirror properties in monkeys and humans. They appear to directly connect the visual and motor control systems by mapping observed behaviors, such as reaching and grasping, to motor structures that produce them.  We combine these two lines of evidence, spinal basis fields and mirror neurons, into a more sophisticated notion of behaviors, or perceptual-motor primitives. These let a complex system, such as a humanoid, recognize, reproduce, and learn motor skills. The primitives serve as the basis set for generating movements, but also as a vocabulary for classifying observed movements into executable ones. Thus, primitives can classify, predict, and act. In our approach to imitation, the vision system continually matches any observed human movements onto its own set of motor primitives. The primitive, or combination of primitives, that best approximates the observed input also provides the best predictor of what the robot expects to observe next. This expectation facilitates visual segmentation and interpretation of the observed movement. Imitation, then, is a process of matching, classification, and prediction. Learning by imitation, in turn, creates new skills as novel sequences and superpositions of the matched and classified primitives. The hierarchical structure of our imitation approach lets the robot initially observe and imitate a skill, then perfect it through repetition, so that the skill becomes a routine and itself a primitive. The set of primitives can thus adapt over time, to allow for learning arbitrary new skills-that is, for &quot;true&quot; imitation.  Choosing the primitives Movement primitives or behaviors are the unifying mechanism between visual perception and motor control in our approach, and choosing the right ones is a research challenge, driven by several constraints. On the one hand, the motor control system imposes physical bottom-up limitations, based on its kinematic and dynamic properties. It also provides topdown constraints from the type of movements the system is expected to perform, because the primitives must be sufficient for the robot&apos;s entire movement repertoire. On the other hand, the visual system&apos;s structure and inputs influence the choice of primitives for mapping the various observed movements onto its own executable repertoire. To serve as a general and parsimonious basis set, the primitives encode groups or classes of stereotypical movements, invariant to exact position, rate of motion, size, and perspective. Thus, they represent the generic building blocks of motion that can be implemented as parametric motor controllers. Consider a primitive for reaching. Its most important parameter is the goal position of the end point-that is, the hand or held object. It might be further parametrized by a default posture for the entire arm. Such a primitive lets a robot reach toward various goals within a multitude of tasks, from grasping objects and tools, to dancing, to writing and drawing. We used just such a reaching primitive in our experiments to reconstruct the popular dance, the Macarena.  What constitutes a good set of primitives? We have experimented with two types: innate and learned. Innate primitives are userselected and preprogrammed. We have demonstrated the effectiveness of a basis set consisting of three types: • discrete straight-line movements of subsets of degrees of freedom, accounting for reaching-type motions; • continuous oscillatory movements of subsets of DOFs, accounting for repetitive motions; 9 and • postures, accounting for large subsets of the body&apos;s DOFs. 2 Our approach computes the learned primitives directly from human movement data. We gather different types of such data, using the following methods: vision-based motion tracking of the human upper body (using our tracking system), magnetic markers on the arm (using the FastTrak system), and fullbody joint angle data (using the Sarcos Sensuit). We first reduce the dimensionality of the movement data by employing principal components analysis, wavelet compression, and correlation across multiple DOF. Next, we use clustering techniques to extract patterns of similar movements in the data. These clusters or patterns form the basis for the primitives; the movements in the clusters are generalized and parameterized, to result in primitives for producing a variety of similar movements. Visual classification into primitives. Visual perception is also an important constraint on the primitives and a key component of the imitation process. Because the human (and humanoid) visual attention is resource-limited, it must select the visual features that are most relevant to the given imitation task. Determining what those features are for a given demonstration is a challenging problem. Our previous work showed that people watching videos of arm movements displayed no difference in attention whether they were just watching or intending to subsequently imitate. In both cases, they fixated at the end point-the hand or a held object.  Consequently, we can effectively bias the visual perception mechanism toward recognizing movements that it can execute, especially those movements it performs most frequently. The motor control system&apos;s structure, and its underlying set of movement primitives, provides key constraints for visual movement recognition and classification. Our primitive classifier uses the primitives&apos; descriptions to segment a given motion based on the movement data. In the experiments described below, we used end-point data for both arms as input for the vector quantization-based classifier. 11 Again, a key issue in classification is representing the primitives such that they account for significant invariances, such as position, rotation, and scaling. Our classification approach forms the original motion into a vector of relative end-point movements between successive frames, then smoothes and normalizes it. At the classification level, we ignore all other information about the movement, such as global position and arm configuration, enabling a small set of high-level primitive representations instead of a potentially prohibitively large set of detailed ones. Other information necessary for correct imitation serves for parameterizing the selected primitives at the level of movement reconstruction and execution. To simplify matching, our approach describes primitives themselves in the same normalized form. For each time step of the observed motion, we compare a fixed- IEEE INTELLIGENT SYSTEMS WE CAN EFFECTIVELY BIAS THE VISUAL PERCEPTION MECHANISM TOWARD RECOGNIZING MOVEMENTS THAT IT CAN EXECUTE, ESPECIALLY THOSE MOVEMENTS IT PERFORMS MOST FREQUENTLY. horizon window to every primitive and select the one that best matches the input. Adjacent windows with identical classifications connect to form continuous segments. For any segments that fail to match the given primitives, our approach uses the reaching primitive to move the end point frame by frame. Because the horizon window is of fixed size, the perception of a distinct match of a primitive applies only for the given timescale. We are currently working on addressing classification at multiple timescales. To validate our approach, we implemented various examples of imitation, including reaching, ball throwing, aerobics moves, and dance, all on humanoid testbeds, taking human demonstrations as input.  We also used 3D magnetic marker data from the human arm, gathered from subjects imitating videos of arm movements while wearing FastTrak markers for position recording. (These data were gathered at the National Institutes of Health Resource for the Study of Neural Models of Behavior at the University of Rochester.) We used four markers: near the shoulder, the elbow, the wrist, and the start of the middle finger. The movement data resulting from this experiment serve as input into our imitation system, as well as for automatically learning the primitives. Finally, we used full-body joint angle data gathered with the Sarcos Sensuit, a wearable exoskeleton that simultaneously records the joint positions of 35 DOF: the shoulders, elbows, wrists, hips, knees, ankles, and waist. (These data are obtained through a collaboration with the ATR Dynamic Brain Project at the Human Information Processing Labs in Kyoto, Japan.) We are currently focusing on reproducing the upper-body movements from those data on our testbeds, described next. Evaluation testbeds To properly validate our approach to humanoid motor control and imitation, we use different experimental testbeds. Most of our work so far has been done on Adonis (developed at the Georgia Institute of Technology Animation Lab), a 3D rigid-body simulation of a human with full dynamics  As the project progresses, we plan to use physical humanoid robots as the ultimate testbeds for evaluating our imitation architecture. The NASA Robonaut is one candidate, through collaboration with the Johnson Space Center. The Sarcos full-body humanoid robot is another, through collaboration with the ATR Dynamic Brain Project. Both robots are highly complex, built to approximate human body structure as faithfully as practically possible, and feature binocular cameras for embodied visual perception critical for imitation. OUR APPROACH TO HUMANOID motor control and imitation relies on the use of a set of movement primitives. We have experimented with different types of such primitives on different humanoid simulation testbeds. Specifically, we have implemented two versions of the spinal fields found in animals. One closely modeled the frog data, and used a joint-space representation-it controlled individual joints of Adonis&apos;s arms.  We tested both types of primitives on a sequential motor task, dancing the Macarena. Both proved effective, but each had limitations for particular types of movements. This led us to propose and explore a combination approach, where multiple types of primitives can be sequenced and combined. For example, we constructed a basis behavior set consisting of three types of primitives: • discrete straight-line movements using impedance control; • continuous oscillatory movements using coupled oscillators (or a collection of piece-wise linear segments using impedance control); and • postures, using PD-servos to directly control the joints. We also added a forth type of primitive, for avoidance, implemented as a repulsive vector field. The continuously active fourth primitive combined with whatever other primitive was executing to prevent any collisions 22 IEEE INTELLIGENT SYSTEMS  between body parts. In the Macarena, for example, this is necessary for arm movements around and behind the head (  Our goal is not to achieve perfect, completely precise, high-fidelity imitation. While that might be possible, through the use of exact quantitative measurements of the observed movement using signal-processing techniques, it is not what happens in imitation in nature and it is neither necessary nor helpful for our main goals: natural interaction and programming of robots. For those purposes, we aim for an approximation of the observed behavior, one that allows any necessary freedom of interpretation by the humanoid robot, but achieves the task and effectively communicates and interacts with the human. Our goal is also distinct from task-level imitation, which only achieves the demonstration&apos;s goal, but does not imitate the behaviors involved. This problem has been studied in assembly robotics, where investigators used a robotic arm to record, segment, interpret, and then repeat a series of visual images of a human performing an object-stacking task",space,1116
,filtered,core,'National Documentation Centre (EKT)',2020-01-01 00:00:00,core,methodology for the design of compatible and performing restoration mortars for the earthquake protection of monuments and historical structures,10.12681/eadd/47665,"The aim of this doctoral thesis is the development of an original methodology for the design of compatible and performing restoration mortars for the earthquake protection of monuments and historic structures. In order to achieve this aim, the study of three important historic structures is utilized. Specifically, the Catholicon of Kaisariani Monastery, an important mid-byzantine church and the Plaka Bridge of Arachthos river, a traditional historic bridge, located in the seismic areas of Athens and Epirus respectively, are studied, as well as the Holy Aedicule of the Holy Sepulchre in Jerusalem, the most important monument of Christianity, whose structural integrity was at risk. These three monument were selected, as they present different construction materials and different structural systems, while they are also subjected to different environmental loads, thus guaranteeing the development of a universal approach which can be applied on any monument, taking its specific characteristics into account. Most importantly, these monuments were selected as study cases, on behalf of the interdisciplinarity employed during their study, which provides, within the framework of this doctoral thesis, the possibility of defining the critical thresholds of mechanical properties, which the restoration mortars must comply with in order to ensure their structural integrity and their seismic protection. The monument itself is at the core of the developed methodology. Each monument is studied in the current state and the results are taken into account in order to specify and prioritize design criteria for compatible and performing restoration mortars. Within this process, emphasis is given to the critical thresholds, which the physicochemical, microstructural and mechanical characteristics of the restoration mortar must abide to. It is obvious that the demands of the design criteria and the assignment of critical thresholds regarding the restoration mortar characteristics, create a defined space of acceptability, where the restoration mortar is both compatible and performing. Regarding the compatibility design criteria, acceptability limits set for different types of restoration mortars, related to compositional characteristics, as measured through thermogravimetric analysis and microstructural characteristics, as measured through mercury intrusion porosimetry, are utilized as critical thresholds regarding physicochemical compatibility of the restoration mortar. Characterization and decay diagnosis of the historical materials is at the center of both the design and assessment of any restoration mortar in terms of compatibility. All instrumental techniques which have already been established in the Materials Science and Engineering Laboratory, are utilized for the study of both historical materials and restoration materials, and the results allow for the definition of critical thresholds regarding additional restoration mortar characteristics, where compatibility is ensured.Regarding the performance design criteria, the mechanical properties of the restoration mortars are taken into account and assessed in relation to the critical thresholds set by the finite element model analysis (in the case of the Holy Aedicule of the Holy Sepulchre) or by the fragility analysis (in the case of the Kaisariani Monastery Catholicon) results, corresponding to the demanded value of the restoration mortars compressive strength, capable of ensuring an adequate response of the monument under static and dynamic loads. In both cases the real material data were incorporated within the computation models. Thus, structural analysis results reveal the minimum demanded compressive strength that the restoration mortar must present. In the case of the Holy Aedicule, where the proposed restoration mortar was applied during the restoration project, non-destructive techniques were utilized to assess the effectiveness of their application in terms of compatibility and performance, thus leading to a methodological approach regarding the assessment of mortar application in situ, in real scale and time. The design criteria are parameterized in order to define an area that delimits the characteristics that a restoration mortar must present in order to be compatible and performing, and which allows the selection of the most suitable raw materials from a wider range of materials. At the level of processing the big data that emerges, an original system for evaluating the restoration mortars is proposed, based on the above criteria and using importance weights, which differ for each monument, thus offering the possibility of choosing the optimal mortar for each case. The use of principal component analysis (PCA) proved particularly useful for the study of the restoration mortars examined in this thesis, as it managed to correlate their characteristics and distinguish restoration mortars according to their characteristics, thus facilitating the simultaneous investigation of both their compatibility and performance. At the same time, it is possible to use the PCA method as a tool to assess the compatibility of historical and restoration mortars in terms of composition and microstructural characteristics. In the last section of this thesis, artificial neural networks (ANNs) are developed and trained, aiming to facilitate the design of restoration mortars and creating an added, necessary innovation in the field of mortar design. It is the first time that ANNs are implemented within this framework, and the results highlight their potential as a tool in mortar design. In addition to revealing the effect of each parameter of synthesis (binder to sand ratio, water to binder ratio, maximum aggregate diameter and age of specimen) on the different mortar characteristics, ANNs can also reveal a recommended area of design, regarding mortar synthesis parameters, within which the restoration mortar is expected to develop characteristics as set by the compatibility and design criteria of each monument. Thus, while discrimination analysis within PCA facilitates the management and correlation of experimental data, the use of ANNs allows the examination of a wider area of mortar design, even of synthesis parameter combinations where no experimental data exists, allowing the study and design of the material in the multidimensional space defined by the parameters of mortar synthesis and allowing research to surpass the experimental process. The mortar design methodology developed in the current doctoral thesis, is conducted in a wide design framework, also at the level of correlation and simulation of mortar characteristics, thus configuring an integrated design framework. Mortar design is thus conducted within a continuous and integrated space of compatibility and performance, thus marking the developed methodology –parameterization of criteria, correlation of independent variables, discrimination analysis and simulation with ANNs– as an important tool for the design of compatible and performing restoration mortars, which can contribute to the earthquake protection of monuments and historic structures, as well ensure their structural integrity.Σκοπός της παρούσης διδακτορικής διατριβής είναι η ανάπτυξη μιας πρωτότυπης μεθοδολογίας σχεδιασμού συμβατών και επιτελεστικών κονιαμάτων αποκατάστασης για την αντισεισμική προστασία μνημείων και ιστορικών κατασκευών. Για τον σκοπό αυτό, αξιοποιείται η μελέτη τριών σημαντικών μνημείων και ιστορικών κατασκευών. Συγκεκριμένα, μελετώνται η Μονή Καισαριανής και το Γεφύρι της Πλάκας, μια σημαντική μεσοβυζαντινή εκκλησία και ένα ιστορικό γεφύρι που βρίσκονται στις σεισμογενείς περιοχές της Αττικής και της Ηπείρου αντίστοιχα, καθώς και το Ιερό Κουβούκλιο του Παναγίου Τάφου στην περιοχή των Ιεροσολύμων, το σημαντικότερο μνημείο της χριστιανοσύνης, του οποίου η δομική ακεραιότητα παρουσίαζε προβλήματα. Τα τρία αυτά μνημεία επιλέχθηκαν, καθώς παρουσιάζουν διαφορετικά δομικά υλικά και τρόπο δόμησης, ενώ βρίσκονται και σε διαφορετικό περιβάλλον, ως εκ τούτου διασφαλίζοντας μια καθολική προσέγγιση, που μπορεί να εφαρμοστεί σε κάθε μνημείο, λαμβάνοντας υπόψη τα ιδιαίτερα χαρακτηριστικά του. Κυρίως όμως, τα μνημεία αυτά επιλέχθηκαν γιατί η εξέτασή τους ήταν διεπιστημονική και παρέχει, στο πλαίσιο της παρούσης διδακτορικής διατριβής, τη δυνατότητα γνώσης των κρίσιμων κατωφλίων των μηχανικών απαιτήσεων των κονιαμάτων αποκατάστασης για τη διασφάλιση της δομικής τους ακεραιότητας και την αντισεισμική τους θωράκιση. Η μεθοδολογία που αναπτύσσεται έχει ως πυρήνα το μνημείο και τα αποτελέσματα της μελέτης του, ώστε να προτεραιοποιηθούν και να συγκεκριμενοποιηθούν κριτήρια σχεδιασμού συμβατών και επιτελεστικών κονιαμάτων αποκατάστασης, ενώ ιδιαίτερη έμφαση δίνεται στα κρίσιμα κατώφλια των φυσικοχημικών και μηχανικών τους χαρακτηριστικών. Είναι σαφές ότι οι απαιτήσεις και η θέσπιση κρίσιμων κατωφλίων όσον αφορά και τη συμβατότητα και την επιτελεστικότητα, δημιουργούν έναν χώρο αποδοχής, όπου το κονίαμα αποκατάστασης πληροί και τις δυο αυτές σημαντικές απαιτήσεις. Σε σχέση με τα κριτήρια συμβατότητας των κονιαμάτων αποκατάστασης, αξιοποιούνται τα όρια αποδοχής κονιαμάτων αποκατάστασης ως κρίσιμα κατώφλια των φυσικοχημικών τους χαρακτηριστικών, ενώ ο χαρακτηρισμός των ιστορικών υλικών αποτελεί πυρήνα σχεδιασμού και αποτίμησης της συμβατότητας των κονιαμάτων αποκατάστασης. Για τον χαρακτηρισμό των ιστορικών υλικών και την αποτίμηση των κονιαμάτων αποκατάστασης, χρησιμοποιούνται όλες οι ενόργανες τεχνικές που έχουν προτυποποιηθεί στο Εργαστήριο Επιστήμης και Τεχνικής των Υλικών, τα αποτελέσματα των οποίων επίσης προσφέρουν τη δυνατότητα ορισμού κρίσιμων κατωφλίων για τη διασφάλιση της συμβατότητας. Σε σχέση με τα κριτήρια επιτελεστικότητας αξιοποιούνται μετρήσεις μηχανικών αντοχών των κονιαμάτων αποκατάστασης και λαμβάνονται υπόψη τα κρίσιμα κατώφλια απαιτούμενων αντοχών, όπως αυτά προκύπτουν από την μελέτη πεπερασμένων στοιχείων (Ιερό Κουβούκλιο του Παναγίου Τάφου) και τις καμπύλες θραυστότητας (Καθολικό Μονής Καισαριανής), ενώ και στις δυο περιπτώσεις ενσωματώνονται στα υπολογιστικά μοντέλα τα πραγματικά δεδομένα που προέκυψαν από τη μελέτη των υλικών. Τα αποτελέσματα της δομικής ανάλυσης υποδεικνύουν την ελάχιστη απαιτούμενη αντοχή σε θλίψη που πρέπει να παρουσιάζει το κονίαμα αποκατάστασης, ενώ, στην περίπτωση του Ιερού Κουβουκλίου του Παναγίου Τάφου, όπου και εφαρμόστηκαν τα προτεινόμενα κονιάματα αποκατάστασης, οι μετρήσεις μη καταστρεπτικού ελέγχου επιτρέπουν την αποτίμηση της συμπεριφοράς και της εφαρμογής στους στο επίπεδο του μνημείου. Τα κριτήρια σχεδιασμού παραμετροποιούνται ώστε να διαμορφώνουν ένα πεδίο τιμών που οριοθετεί τα χαρακτηριστικά που πρέπει να εμφανίζει ένα κονίαμα αποκατάστασης, και το οποίο επιτρέπει την επιλογή των πλέον κατάλληλων πρώτων υλών από ένα ευρύτερο φάσμα υλικών. Σε επίπεδο επεξεργασίας των μεγάλων δεδομένων που προκύπτουν, προτείνεται σύστημα αξιολόγησης κονιαμάτων αποκατάστασης, βάσει των ως άνω κριτηρίων με τη χρήση βαρών σημαντικότητας στο κάθε μνημείο, το οποίο προσφέρει τη δυνατότητα επιλογής του βέλτιστου κονιάματος. Η χρήση κυρίων συνιστωσών για την μελέτη των κονιαμάτων αποκατάστασης είναι ιδιαιτέρως χρήσιμη, καθώς υποδεικνύει τις συσχετίσεις των χαρακτηριστικών τους και διακριτοποιεί τα κονιάματα αποκατάστασης αναλόγως των χαρακτηριστικών τους, διευκολύνοντας τη ταυτόχρονη διερεύνηση της συμβατότητας και της επιτελεστικότητάς τους. Παράλληλα, διαφαίνεται η δυνατότητα χρήσης της μεθόδου ως εργαλείο για την αποτίμηση της συμβατότητας ιστορικών κονιαμάτων και κονιαμάτων αποκατάστασης από άποψη σύστασης και μικροδομής. Στο τελευταίο τμήμα της διδακτορικής διατριβής, γίνεται ανάπτυξη και χρήση τεχνητών νευρωνικών δικτύων, για πρώτη φορά στον σχεδιασμό κονιαμάτων αποκατάστασης, δημιουργώντας μια επιπλέον απαραίτητη καινοτομία στο πεδίο του σχεδιασμού. Όπως αναδεικνύεται στο πλαίσιο αυτής της διδακτορικής διατριβής, η χρήση τους μπορεί να υποδείξει, βάσει των κριτηρίων σχεδιασμού για το εκάστοτε μνημείο και την οριοθέτηση των χαρακτηριστικών του κατάλληλου συμβατού και επιτελεστικού κονιάματος αποκατάστασης, μια προτεινόμενη περιοχή σχεδιασμού κονιαμάτων αποκατάστασης, όσον αφορά στις παραμέτρους της σύνθεσης (αναλογία κονίας/αδρανών, λόγος νερού κονίας). Έτσι, ενώ η ανάλυση διάκρισης επιτρέπει τη διαχείριση των πειραματικών δεδομένων, η χρήση των τεχνητών νευρωνικών δικτύων επιτρέπει και τη μελέτη του υλικού στον πολυδιάστατο χώρο που ορίζουν οι παράμετροι της σύνθεσής του, ενώ προσφέρει τη δυνατότητα διερεύνησης επιπλέον περιοχών σχεδιασμού, όπου δεν υπάρχουν διαθέσιμα πειραματικά δεδομένα, επιτρέποντας την υπέρβαση της πειραματικής διαδικασίας. Η μεθοδολογία σχεδιασμού που αναπτύσσεται στην παρούσα διδακτορική διατριβή λαμβάνει χώρα σε ένα ευρύτερο πεδίο σχεδιασμού, τόσο στο επίπεδο της συσχέτισης, όσο και στο επίπεδο της προσομοίωσης των ιδιοτήτων του κονιάματος αποκατάστασης, ώστε να διαμορφώσει ένα ολοκληρωμένο πεδίο σχεδιασμού. Ο σχεδιασμός ανάγεται ως εκ τούτου σε έναν ενιαίο και συνεχή χώρο συμβατότητας και επιτελεστικότητας, καθιστώντας την ολοκληρωμένη μεθοδολογία που αναπτύσσεται –παραμετροποίηση, συσχέτιση ανεξάρτητων μεταβλητών, ανάλυση διάκρισης και προσομοίωση με τεχνητά νευρωνικά δίκτυα– ένα σημαντικό εργαλείο σχεδιασμού συμβατών και επιτελεστικών κονιαμάτων αποκατάστασης για την αντισεισμική προστασία μνημείων και ιστορικών κατασκευών και τη διασφάλιση της δομικής τους ακεραιότητας",space,1117
,filtered,core,,2020-03-27 00:00:00,core,μάθηση μέσω παρατήρησης για την επίτευξη ρομποτικών δράσεων χειρισμού,,"Η παρούσα διδακτορική διατριβή αφορά τη μελέτη, ανάπτυξη και εφαρμογή, μεθόδων
Μηχανικής Μάθησης μέσω Παρατήρησης (Learning from Demonstration) με στόχο την
ρομποτική αναπαραγωγή δράσεων χειρισμού. Η μεθοδολογία αυτή στηρίζεται στην
δημιουργία μιας αντιστοίχισης (mapping) μεταξύ της κινηματικής του ανθρώπινου χεριού και
ενός ρομποτικού βραχίονα, ή πιο συγκεκριμένα μεταξύ του πολυδιάστατου χώρου των
κινήσεων του ανθρώπου (human actor) με τον επίσης πολυδιάστατο χώρο δράσης του
ρομπότ. Η συσχέτιση των ανθρώπινων ενεργειών με αντίστοιχες ρομποτικές, επιτυγχάνεται
μέσω μιας άδηλης αναπαράστασης, που ονομάζεται λανθάνουσα απεικόνιση χώρου (latent
space). Πιο συγκεκριμένα, μελετάμε την αμοιβαία αλληλεπίδραση της αντίληψης και της 
δράσης, προκειμένου να διδάξουμε τα ρομπότ μια ποικιλία από νέες κινήσεις χειρός. Ως εκ
τούτου, υλοποιήθηκε ένα μεθοδολογικό πλαίσιο μάθησης μέσω παρατήρησης, το οποίο
ονομάζεται IMFO (Imitation Framework by Observation), που διευκολύνει την αναπαραγωγή
μαθημένων και νέων κινήσεων χειρισμού από ένα ρομπότ (manipulation tasks) και,
παράλληλα, έχει ευρεία εφαρμογή σε σενάρια αλληλεπίδρασης ανθρώπου-ρομπότ (HRI) σε
καθημερινά περιβάλλοντα.
Επιπλέον, σε αυτή τη διατριβή, εξετάζουμε το ρόλο της χρονικής διάρκειας εκτέλεσης μιας
κίνησης μέσα από τη διαδικασία μάθησης από παρατήρηση, ενισχύοντας το διαμορφωμένο
πλαίσιο IMFO με την δυνατότητα αναπαράστασης και αναπαραγωγής τόσο των χωρικών όσο
και των χρονικών χαρακτηριστικών των ανθρώπινων κινήσεων. Σε αντίθεση με άλλες
μεθόδους μάθησης μέσω παρατήρησης (LfD) που περιγράφουν την εκτελούμενη δράση μόνο
με βάση τα χωρικά χαρακτηριστικά της, η προτεινόμενη μεθοδολογία ενισχύει την
αναπαραγωγή των χωροχρονικών πτυχών μιας κίνησης επιτρέποντας την αποτελεσματική
εφαρμογή της σε πιο σύνθετα σενάρια HRI, όπου η χρονική αλληλουχία των δράσεων είναι
σημαντική. Επιπρόσθετα, εισάγεται ένα σύνολο καλά καθορισμένων μετρικών αξιολόγησης
(evaluation metrics) για να αποτιμηθεί η εγκυρότητα της προτεινόμενης προσέγγισης
λαμβάνοντας υπόψη τη χρονική και χωρική συνέπεια των αναπαραγόμενων συμπεριφορών.
Μια αξιοσημείωτη επέκταση του προαναφερθέντος πλαισίου αναφέρεται στην εκμάθηση
της δύναμης που επιβάλλεται από τον χρήστη για την επιτυχημένη εκτέλεση λεπτών
χειρισμών. Αυτή η διαδικασία παρουσιάζεται επίσης στην παρούσα διατριβή μέσω ενός
νέου πλαισίου εποπτευόμενης μάθησης, το οποίο ονομάζεται SLF (Supervised Learning
scheme for Force-based manipulation). Το SLF διατυπώνεται ως μία διαδικασία τριών
σταδίων: (α) επιβλεπόμενη διαδικασία εκτέλεσης κινήσεων χειρισμού σε προσομοίωση για
την απόκτηση επαρκών δεδομένων, (β) διαδικασία εκπαίδευσης (training) για τη
διευκόλυνση της μάθησης κινήσεων χειρισμού με την κατάλληλη προσαρμογή του καρπού
και της δύναμη πιασίματος και μεταφοράς και (γ) εκτέλεση της κίνησης από ρομποτικό
βραχίονα σε προσομοίωση. Στη συνέχεια, με τη χρήση της μεθόδου sim-to-real transfer,
επιτυγχάνεται αναπαραγωγή των μαθημένων δράσεων σε πραγματικά περιβάλλοντα
γενικεύοντας την εφαρμογή του πλαισίου μάθησης σε επιπλέον συνθήκες χειρισμού
εύθραυστων αντικειμένων. Τα αποτελέσματα με τη χρήση του ρομποτικού βραχίονα YuMi,
σε πειράματα με διαφορετικά αντικείμενα με παρόμοιους συντελεστές τριβής, και
εναλλακτικές πόζες πιασίματος, αποδεικνύουν ότι το ρομπότ είναι σε θέση να αναπαράγει
αποτελεσματικά απαιτητικές κινήσεις μεταφοράς και χειρισμού μετά την ολοκλήρωση της
διαδικασίας μάθησης.
Συνοπτικά, η παρούσα διατριβή μελετά την διαδικασία μάθησης μέσω παρατήρησης
συνεισφέροντας με μια νέα προσέγγιση που εισάγει την μελέτη δράσεων χειρισμού
αντικειμένων μέσα από έναν χώρο μειωμένων διαστάσεων, για την εύκολη και συμπαγή
κωδικοποίηση των επιμέρους χαρακτηριστικών των δράσεων. Ταυτόχρονα μελετώνται τα
χρονικά χαρακτηριστικά των κινήσεων ώστε να ενισχυθεί η εφαρμογή της μεθόδου σε
σύνθετες, πραγματικές συνθήκες που απαιτούν χρονική ακρίβεια αναπαραγωγής. Τέλος, η
διαμόρφωση μιας γενικευμένης διαδικασίας εποπτευόμενης μάθησης για τον χειρισμό
εύθραυστων αντικείμενων αναβαθμίζει περαιτέρω το αρχικό πλαίσιο μάθησης.The current PhD thesis addresses the formulation and implementation of a methodological
framework for robot Learning from Demonstration (LfD). The latter refers to methodologies
that develop behavioral policies from example state-to-action mappings. To this
end, we study the reciprocal interaction of perception and action, in order to teach robots
a repertoire of novel action behaviors. Based on that, we design, develop and implement
a robust imitation framework, termed IMFO (IMitation Framework by Observation), that
facilitates imitation learning and relevant applications in human-robot interaction (HRI)
tasks. IMFO can cope with the reproduction of learned (i.e. previously observed) actions,
aswell as novel ones. Mapping of human actions to the respective robotic ones is achieved
via an indeterminate depiction, termed latent space representation. The latter accomplishes
a compact, yet precise abstraction of action trajectories, effectively representing
high dimensional raw actions in a low dimensional space.
Moreover, throughout this thesis, we examine the role of time in LfD by enhancing
the aforementioned framework with the notion of learning both the spatial and temporal
characteristics of human motions. Accordingly, learned actions can be subsequently reproduced
in the context of more complex time-informed HRI scenarios. Unlike previous
LfD methods that cope only with the spatial traits of an action, the formulated scheme
effectively encompasses spatial and temporal aspects. Extensive experimentation with a
variety of real robotic platforms demonstrates the robustness and applicability of the introduced
integrated LfD scheme.
Learned actions are reproduced under the high level control of a time-informed task
planner. During the implementation of the studied scenarios, temporal and physical constraints
may impose speed adaptations in the performed actions. The employed latent
space representation readily supports such variations, giving rise to novel actions in the
temporal domain. Experimental results demonstrate the effectiveness of the proposed
enhanced imitation scheme in the implementation of HRI scenarios. Additionally, a set
of well defined evaluation metrics are introduced to assess the validity of the proposed
approach considering the temporal and spatial consistency of the reproduced behaviors.
A noteworthy extension of the above regards force-based object grasping for executing
sensitive manipulation tasks. This is also treated in the current thesis via a novel supervised
learning scheme, termed SLF (Supervised Learning for Force-based manipulation).
SLF is formulated as a three-stage process: (a) supervised trial-execution in simulation
to acquire sufficient training data; (b) training to facilitate grasp learning with suitable
robot-arm pose and lifting force; (c) grasp execution in simulation. Subsequently, following
sim-to-real transfer, operation in real environments is achieved in addition to simulated
ones, generalizing also for objects not included in the trial sessions. The proposed
learning scheme is demonstrated in object lifting tasks where the applied force varies for
different objects with similar contact friction coefficients, and likewise the grasping pose.
Experimental results on the manipulator YuMi show that the robot is able to effectively
reproduce demanding lifting and manipulation tasks after learning is accomplished.
In summary, our thesis has studied LfD and has contributed with a novel approach that
introduced latent space representations to encode the action characteristics. A framework
implementation (IMFO) of our approach allowed extensive experimentation and also conduction
of HRI scenarios. The inclusion of temporal aspects in our approach enhanced it
to cope with complex, real-life interactions. Finally, the extension of IMFO with forcebased
grasping facilitated manipulation tasks with sensitive objects",space,1118
,filtered,core,,2020-01-01 00:00:00,core,generation of refactoring algorithms through grammatical evolution,,"Orientadora: Silvia Regina VergilioCoorientador: Marouane KessentiniTese (doutorado) - Universidade Federal do Paraná, Setor de Ciências Exatas, Programa de Pós-Graduação em Informática. Defesa : Curitiba, 24/04/2020Inclui referências: p. 61-66Área de concentração: Ciência da ComputaçãoResumo: A atividade de refatoração tem como principal objetivo aplicar um conjunto de transformações em um artefato de software para melhorar sua estrutura sem alterar sua funcionalidade. Alguns estudos recentes, apresentam bons resultados ao gerarem modelos de predição de refatorações. Além disso, os estudos mostram que refatorações similares são aplicadas em diferentes contextos e podem ser aprendidas. Neste sentido, a maioria dos trabalhos existentes utiliza técnicas de aprendizado de máquina para gerar modelos que predizem se um dado trecho de código deve ser refatorado. Entretanto, essas abordagens possuem limitações. Elas buscam por refatorações específicas e exatamente como aplicadas por desenvolvedores, o que limita que outras refatorações sejam encontradas. Dada a natureza subjetiva da atividade de refatoração de software, a exploração por refatorações com base em outros critérios também é vantajosa. Existem trabalhos na área conhecida como Refatoração de Software Baseada em Busca (SBR) (do inglês, Search Based Software Refactoring), em que algoritmos de busca são utilizados para encontrar refatorações em um grande espaço de busca e visando a melhorar diversos aspectos. Recentemente, trabalhos em SBR começaram a utilizar exemplos de refatorações já aplicadas por desenvolvedores para incorporar aprendizado na busca. Entretanto, essas abordagens são limitadas em termos de generalização dos resultados, uma vez que não geram um modelo que possa ser utilizado para diferentes programas. Desse modo, abordagens existentes de SBR devem ser configuradas e executadas a cada novo programa. Neste contexto, este trabalho visa a incorporar os benefícios encontrados na área de aprendizado de máquina e na área de SBR, apresentando uma abordagem chamada Gorgeous (do inglês, Generation of Refactoring Algorithms through Grammatical Evolution). Gorgeous tem como objetivo gerar algoritmos de refatoração compostos por regras, que quando executados, determinam trechos de código que devem ser refatorados e refatorações a serem aplicadas. Os algoritmos são criados de forma que as refatorações sugeridas sejam similares a refatorações aplicadas na prática e que também melhorem a qualidade do software. Os algoritmos são criados utilizando um processo de aprendizado que primeiro extrai padrões de refatoração de programas agrupando elementos que foram refatorados de maneira similar. Após isso, uma evolução gramatical é executada para gerar algoritmos de refatoração com base nos padrões extraídos. Gorgeous é avaliada utilizando dados de refatoração extraídos de 40 programas Java do repositório GitHub. Como resultado, os algoritmos gerados foram capazes de obter bons resultados para diferentes programas, melhorando em média 60% a qualidade do programa e obtendo 50% de similaridade com refatorações aplicadas na prática. Palavras-chave: Refatoração, Engenharia de Software Baseada em Busca,Agrupamento, Evolução GramaticalAbstract: The refactoring activity addresses the application of a set of transformations in software artifacts to improve their structure while preserving their functionality. Recent studies present promising results generating prediction models for refactoring. Furthermore, they provide evidences that similar refactoring operations are applied in different contexts and they can be learned using Machine Learning (ML). Most works on ML based refactoring generate models to predict if a piece of code should be refactored. Despite the capability of prediction, existing works are limited to learn specific refactoring operations as applied by developers. However, to explore refactoring operations possibilities based on other criteria is also beneficial, mainly by the subjective context of refactoring. In this context, the Search-Based Software Refactoring (SBR) area addresses studies using search algorithms to find refactoring operations in a huge search space, aiming at improving several other aspects. However, existing SBR approaches do not support generalization of results since they do not generate a model as ML studies. In this way, a SBR approach needs to be configured and executed for each program in need of refactoring. In this context, this work introduces a SBR learning approach aiming at taking most advantage of both fields. Gorgeous (Generation of Refactoring Algorithms through Grammatical Evolution) generates refactoring algorithms composed by several rules determining pieces of code that should be refactored and the refactoring types to be used. A refactoring algorithm provides as solution a set of refactoring operations to be applied in a program. In this respect, the algorithm is generated with the goal of increasing similarity of the refactoring operations with the ones applied in practice, and also improving program quality. To do this, a learning process first extracts refactoring patterns from programs by grouping their elements that were refactored in similar ways. After that, a Grammatical Evolution (GE) is executed to generate the algorithms based on the extracted patterns. Gorgeous is evaluated using refactoring data from 40 Java programs of GitHub repository. The refactoring algorithms are capable of obtaining good results to different programs, obtaining around 60% of program quality improvement and 50% of similarity with real refactoring applications. Keywords: Refactoring, Search-Based Software Engineering, Clustering, Grammatical Evolutio",space,1119
,filtered,core,,2018-06-30 00:00:00,core,software and system health management for autonomous robotics missions,10.1184/r1/6710654.v1,"Advanced autonomous robotics space missions rely heavily on the flawless interaction of complex hardware, multiple sensors, and a mission-critical software system.  This software system consists of an operating system, device drivers, controllers, and executives; recently highly complex AI-based autonomy software have also been introduced. Prior to launch, this software has to undergo rigorous verification and validation (V&V).  Nevertheless, dormant software bugs, failing sensors, unexpected hardware-software interactions, and unanticipated environmental conditions—likely on a space exploration mission—can cause major software faults that can endanger the entire mission.

Our Integrated Software Health Management (ISWHM) system continuously monitors the hardware sensors and the software in real-time. The ISWHM uses Bayesian networks, compiled to arithmetic circuits, to model software and hardware interactions. Advanced reasoning algorithms using arithmetic circuits not only enable the ISWHM to handle large, hierarchical models that are necessary in the realm of complex autonomous systems, but also enable efficient execution on small embedded processors. The latter capability is of extreme importance for small (mobile) autonomous units with limited computational power and low telemetry bandwidth.  In this paper, we discuss the requirements of ISWHM.  As our initial demonstration platform, we use a primitive Lego rover. A Lego 
Mindstorms microcontroller is used to implement a highly simplified autonomous rover driving system, running on the OSEK real-time operating system. We demonstrate that our ISWHM, running on this small embedded microcontroller, can perform fault detection as well as on-board reasoning for advanced diagnosis and root-cause detection in real time",space,1120
,filtered,core,'Institute of Electrical and Electronics Engineers (IEEE)',2018-06-29 00:00:00,core,"navigating the landscape for real-time localisation and mapping for robotics, virtual and augmented reality",10.1109/JPROC.2018.2856739,"Visual understanding of 3-D environments in real time, at low power, is a huge computational challenge. Often referred to as simultaneous localization and mapping (SLAM), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, and virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are: 1) tools and methodology for systematic quantitative evaluation of SLAM algorithms; 2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives; 3) end-to-end simulation tools to enable optimization of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches; and 4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context",space,1121
,filtered,core,University of Southern California. Libraries,2018-10-15 00:00:00,core,speech and language understanding in the sigma cognitive architecture,,"2018-10-16Cognitive architectures model fixed structures underlying intelligence and seek to heed the original goal of AI—a working implementation of a full cognitive system in aid of creating synthetic agents with human capabilities. Sigma is a cognitive architecture, developed with the immediate aim of supporting real time needs of intelligent agents, robots and virtual humans. In Sigma, this requirement manifests as a system whose development heuristically is guided by knowledge about human cognition with the ultimate desire to explain human intelligence at an appropriate level of abstraction. ❧ Spoken language processing is an important cognitive capability and yet not addressed by existing cognitive architectures. This is indicative of the mixed—symbolic and probabilistic—nature of the speech problem. Sigma, guided in its development by a core set of desiderata that are an evolution of the desiderata implicit in Newell’s Unified Theories of Cognition, presents a unique opportunity to attempt the integration of spoken language understanding in a cognitive architecture. Such attempt is an exercise to push cognitive architectures beyond what they are capable of, taking a first step towards enabling an architecturally based theory of spoken language understanding—deconstructed in terms of the interplay between various cognitive and sub-cognitive capabilities that play an important role in the comprehension process. ❧ This dissertation investigates the issues involved in integration of incremental speech and language processing, with cognition, in aid of spoken language understanding, guided by the desiderata driving Sigma’s development. The space of possibilities this integration enables is explored and a suitable spoken language understanding task is chosen to evaluate the key properties of the theory of spoken language understanding developed in Sigma. Speech signal obtained from an external speech front end is combined with linguistic knowledge in the form of phonetic, lexical and semantic knowledge sources. The linguistic input is converted into meaning using a Natural Language Understanding (NLU) scheme implemented on top of the architecture. ❧ In addition to phonetic, lexical and semantic processing, language processing involves a syntactic component. Probabilistic context free grammar parsing is an important form of grammar processing that has not been possible to realize in cognitive architectures. Probabilistic context free grammar parsing poses a challenge to Sigma’s grounding in graphical models. Sigma is shown to be able to perform syntactic processing via Sum Product Networks (SPNs), a new kind of deep architecture that allows efficient, tractable and exact inference in a wide class of problems, including grammar parsing. It is shown that Sigma’s cognitive language is sufficient to specify any arbitrary valid SPN, with the tractability and exactness expected of them. This shows Sigma’s ability to efficiently specify a wide range of problems. The implications of this are discussed, along with Sigma mechanisms that allow for specifying SPNs. This leads to a novel relationship between neural networks and SPNs in the context of Sigma",space,1122
,filtered,core,'Frontiers Media SA',2018-01-01 00:00:00,core,oncilla robot: a versatile open-source quadruped research robot with compliant pantograph legs,10.3389/frobt.2018.00067,"Sprowitz AT, Tuleu A, Ajallooeian M, et al. Oncilla Robot: A Versatile Open-Source Quadruped Research Robot With Compliant Pantograph Legs. FRONTIERS IN ROBOTICS AND AI. 2018;5: 18.We present Oncilla robot, a novel mobile, quadruped legged locomotion machine. This large-cat sized, 5.1 kg robot is one of a kind of a recent, bioinspired legged robot class designed with the capability of model-free locomotion control. Animal legged locomotion in rough terrain is clearly shaped by sensor feedback systems. Results with Oncilla robot show that agile and versatile locomotion is possible without sensory signals to some extend, and tracking becomes robust when feedback control is added (Ajallooeian, 2015). By incorporating mechanical and control blueprints inspired from animals, and by observing the resulting robot locomotion characteristics, we aim to understand the contribution of individual components. Legged robots have a wide mechanical and control design parameter space, and a unique potential as research tools to investigate principles of biomechanics and legged locomotion control. But the hardware and controller design can be a steep initial hurdle for academic research. To facilitate the easy start and development of legged robots, Oncilla-robot's blueprints are available through open-source. The robot's locomotion capabilities are shown in several scenarios. Specifically, its spring-loaded pantographic leg design compensates for overdetermined body and leg postures, i.e., during turning maneuvers, locomotion outdoors, or while going up and down slopes. The robot's active degree of freedom allow tight and swift direction changes, and turns on the spot. Presented hardware experiments are conducted in an open-loop manner, with little control and computational effort. For more versatile locomotion control, Oncilla-robot can sense leg joint rotations, and leg-trunk forces. Additional sensors can be included for feedback control with an open communication protocol interface. The robot's customized actuators are designed for robust actuation, and efficient locomotion. It trots with a cost of transport of 3.2 J/(Nm),at a speed of 0.63 m s(-1) (Froude number 0.25). The robot trots inclined slopes up to 10 degrees, at 0.25 m s(-1). The multi-body Webots model of Oncilla robot, and Oncilla robot's extensive software architecture enables users to design and test scenarios in simulation. Controllers can directly be transferred to the real robot. Oncilla robot's blueprints are open-source published (hardware GLP v3, software LGPL v3)",space,1123
,filtered,core,Proceedings of Machine Learning Research,2018-01-01 00:00:00,core,learning deployable navigation policies at kilometer scale from a single traversal,,"Model-free reinforcement learning has recently been shown to be effective at learning navigation policies from complex image input. However, these algorithms tend to require large amounts of interaction with the environment, which can be prohibitively costly to obtain on robots in the real world. We present an approach for efficiently learning goal-directed navigation policies on a mobile robot, from only a single coverage traversal of recorded data. The navigation agent learns an effective policy over a diverse action space in a large heterogeneous environment consisting of more than 2km of travel, through buildings and outdoor regions that collectively exhibit large variations in visual appearance, self-similarity, and connectivity. We compare pretrained visual encoders that enable precomputation of visual embeddings to achieve a throughput of tens of thousands of transitions per second at training time on a commodity desktop computer, allowing agents to learn from millions of trajectories of experience in a matter of hours. We propose multi- ple forms of computationally efficient stochastic augmentation to enable the learned policy to generalise beyond these precomputed embeddings, and demonstrate successful deployment of the learned policy on the real robot without fine tuning, despite environmental appearance differences at test time. The dataset and code required to reproduce these results and apply the technique to other datasets and robots is made publicly available at rl-navigation.github.io/deployable ",space,1124
,filtered,core,'American Astronomical Society',2018-05-01 00:00:00,core,machine-learning-based brokers for real-time classification of the lsst alert stream,10.3847/1538-4365/aab781,"The unprecedented volume and rate of transient events that will be discovered by the Large Synoptic Survey Telescope (LSST) demand that the astronomical community update its follow-up paradigm. Alert-brokers-automated software system to sift through, characterize, annotate, and prioritize events for follow-up-will be critical tools for managing alert streams in the LSST era. The Arizona-NOAO Temporal Analysis and Response to Events System (ANTARES) is one such broker. In this work, we develop a machine learning pipeline to characterize and classify variable and transient sources only using the available multiband optical photometry. We describe three illustrative stages of the pipeline, serving the three goals of early, intermediate, and retrospective classification of alerts. The first takes the form of variable versus transient categorization, the second a multiclass typing of the combined variable and transient data set, and the third a purity-driven subtyping of a transient class. Although several similar algorithms have proven themselves in simulations, we validate their performance on real observations for the first time. We quantitatively evaluate our pipeline on sparse, unevenly sampled, heteroskedastic data from various existing observational campaigns, and demonstrate very competitive classification performance. We describe our progress toward adapting the pipeline developed in this work into a real-time broker working on live alert streams from time-domain surveys.Lasker Fellowship at the Space Telescope Science Institute; SKA SA; NRF; AIMS; NSF INSPIRE grant [CISE AST-1344204]; NOAO Community Science Data Center (CSDC)This item from the UA Faculty Publications collection is made available by the University of Arizona with support from the University of Arizona Libraries. If you have questions, please contact us at repository@u.library.arizona.edu",space,1125
,filtered,core,'EDP Sciences',2018-10-16 00:00:00,core,a new method for unveiling open clusters in,10.1051/0004-6361/201833390,"Context. The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in astronomy. It includes precise astrometric data (positions, proper motions, and parallaxes) for more than 1.3 billion sources, mostly stars. To analyse such a vast amount of new data, the use of data-mining techniques and machine-learning algorithms is mandatory.
				Aims. A great example of the application of such techniques and algorithms is the search for open clusters (OCs), groups of stars that were born and move together, located in the disc. Our aim is to develop a method to automatically explore the data space, requiring minimal manual intervention.
				Methods. We explore the performance of a density-based clustering algorithm, DBSCAN, to find clusters in the data together with a supervised learning method such as an artificial neural network (ANN) to automatically distinguish between real OCs and statistical clusters.
				Results. The development and implementation of this method in a five-dimensional space (l, b, ϖ, μα*, μδ) with the Tycho-Gaia Astrometric Solution (TGAS) data, and a posterior validation using Gaia DR2 data, lead to the proposal of a set of new nearby OCs. Conclusions. We have developed a method to find OCs in astrometric data, designed to be applied to the full Gaia DR2 archive",space,1126
,filtered,core,'Khmelnytskyi National University',2019-01-01 00:00:00,core,бібліометричний аналіз досліджень кіберзлочинності в умовах цифровізації фінансового сектору економіки держави,10.31891/2307-5740-2019-276-6(2)-253-259.,"Розвиток інформаційних технологій змінює всі усталені процеси функціонування як економіки в цілому, так і фінансового сектору зокрема. У той же час, цифровізація фінансового сектори створила сприятливі умови для розвитку кіберзлочинності, так здійснення фінансових злочинів перейшло з реального у віртуальний простір та зосередилось на викраданні інформації про банківські рахунки й банківські карти, зломи паролів, шахрайство з банкоматами та інше. У статті проведено бібліометричний аналіз наукових публікацій, присвячених проблемі кіберзлочинності в умовах цифровізації фінансового сектору економіки України. В результаті побудови графіків динаміки публікацій бази даних Scopus встановлено зростаючи тенденції публікативної активності науковців в сфері кіберзлочинності. Найвищу активність у вирішенні проблем пов’язаних з кіберзлочинами проявляють науковці США та Великобританії, як передових країн в сфері електронної комерції.Развитие информационных технологий меняет все устоявшиеся процессы функционирования как экономики в целом, так и финансового сектора в частности. В то же время, цифровизация финансового сектора создала благоприятные условия для развития киберпреступности, так осуществление финансовых преступлений перешло из реальной в виртуальное пространство и сосредоточилось на похищении информации о банковских счетах и ​​банковские карты, взломы паролей, мошенничество с банкоматами и прочее. В статье проведен библиометрические анализ научных публикаций, посвященных проблеме киберпреступности в условиях цифровизации финансового сектора экономики Украины. В результате построения графиков динамики публикаций базы данных Scopus установлено вырастая тенденции публикативнои активности ученых в сфере киберпреступности. Наивысшую активность в решении проблем связанных с киберпреступностью проявляют ученые США и Великобритании, как передовых стран в сфере электронной коммерции.The development of information technology changes the long-standing processes of functioning of both the economy in general and the financial sector in particular. Automation of business processes of financial institutions and digitalization of products of financial intermediaries create new conditions for the implementation of economic activities of all economic agents. Meanwhile, the digitalization of the financial sector has created favorable conditions for the development of cybercrime. Thus, financial crimes moved from the real to the virtual space and focused on the theft of information about bank accounts and bank cards, password cracking, ATM fraud, etc. The purpose of this article is to conduct a bibliometric analysis of scientific publications that deal with the issue of cybercrime in the context of digitalization of the financial sector of the Ukrainian economy. The research was carried out based on publications indexed in the Scopus database during the last 7 years. The plotting of the dynamics of publications in the Scopus database showed an increase in the general trend of publication activity in the field of cybercrime and the field of its interaction with the financial sector. The histogram of the geography of studies dealing with the issue of cybercrime in the context of digitalization of the financial sector of the economy showed the highest activity in solving the problem by scientists from the USA and Great Britain, which are leaders in the field of e-commerce. The bibliometric map of keywords of publications, built using the VOSviewer software, allowed identifying 7 clusters representing the areas of research: computer security, analysis of large databases, digital data storage, artificial intelligence, e-commerce, digital and mobile forensics. The results obtained can be used to identify the most potential areas for the spread of cybercrimes in the digital financial space, as well as tools to counter these types of crimes in Ukraine",space,1127
,filtered,core,'Pisa University Press',2019-04-05 00:00:00,core,a generative adversarial network approach for the attenuation correction in pet-mr hybrid imaging,,"Positron emission tomography (PET) provides functional images useful to track metabolic processes in the body and enables the diagnosis of several diseases. The technique is based on the use of radiotracers that emit positrons whose annihilation with electrons in the human body produces photons which travel away in almost anti-parallel directions. A ring of detectors is used to detect them and an event is counted when two detectors are activated within a time window (coincidence window). Each couple of detectors defines a line of response (LOR) to which events are associated. After the scan, a reconstruction algorithm transforms the acquired data into a map of activity in the patient’s body. Photons do not travel in vacuum but in human body, thus a correction for their attenuation is required.
PET images are characterized by limited spatial resolution. In order to get morphological details to combine to functional ones, PET-CT (PET and computed tomography) and PET-MR (PET and magnetic resonance) systems have been developed. Linear attenuation coefficient maps are obtainable directly from the CT scan in the case of PET-CT by means of an accurate energy rescaling to 511 keV.
Unfortunately, there is no straightforward technique to be used in PET-MR to derive the attenuation properties of tissues from MR signals. Plenty of techniques have been developed to address such kind of problem and in this work we explore an original approach based on deep neural networks. These could provide a boost in the direction of a data-driven algorithm for attenuation correction by using structural, T 1 weighted, MR images transformed into pseudo-CTs, i.e. images whose intensity values are similar to the ones expected in a CT image.
Already implemented deep learning techniques to this purpose require paired data.
Unfortunately, it is quite hard to obtain a big dataset of paired medical images, i.e. MR and CT images belonging to the same patient. To overcome this limitation, we chose to develop an approach based on a Generative Adversarial Network (GAN) trained on unpaired data.
A GAN is a deep learning architecture composed by two neural networks, a generator and a discriminator, fighting against each other: the generator tries to map the input to the desired output and the discriminator tells if the generated output is good or not. In the training phase, the generator has to maximize the similarity to the desired output and the score provided by discriminator; the discriminator instead has to distinguish between the fakes, produced by the generator, and the original data. After the training phase, the generator is capable of mapping any point in the input space (MR images) to a point in the output space (pseudo-CT images). The generation of pseudo-CTs from MRs with an unpaired training set in the case here proposed has been approached by using a CycleGAN (with some ad-hoc developed modifications), characterized by the presence of four networks: two generators, the transformations from MR to CT domain (MR2CT) and vice versa (CT2MR) and two discriminators (fake CT vs. real CT, fake MR vs. real MR). A cyclic consistency constraint imposes that the whole cycle is the identity operator: MR ≈ MR2CT(CT2MR(MR)). This requirement, introduced in the loss function, guides the network training to generate not just an image but an image of the specific input patient.
We collected a dataset of structural MR brain images coming from the Autism Brain Imaging Data Exchange (ABIDE: http://fcon_1000.projects.nitrc.org/indi/abide/) project and CT scans provided by the NeuroAnatomy and image Processing LABoratory (NAPLAB) of the IRCCS SDN (Naples, IT). We used these unpaired examples to train a CycleGAN-like network.
Prior implementations of deep learning models for the generation of medical images require working on single slices of the acquired images, due to the availability of algorithms developed for 2D natural images and limitations in computing power. The proposed approach has been developed to work directly on 3D data. A registration step that aligns all images to approximately the same orientation has proved to be necessary due to the low number of training examples. In fact, no paired data is required, but in any case retrieving a brain CT from a MR is a major issue which needs a simplification at first stage.
Structural similarity index computed between the generated output and the expected one shows satisfactory results. Despite a validation on a more populated dataset is needed to release the current requirements on the initial image alignment, the proposed approach opens to the perspective of using data driven methods to several processing pipelines on medical images, including data augmentation, segmentation and classification. Further investigation on the behaviour of the network in case of abnormalities in the images is required.
An advantage of this technique with respect to other currently available procedures for attenuation correction in PET-MR is that it does not need any extra MR acquisition: only the standard diagnostic T 1 -weighted image is used and, due to the low computational cost, images are translated from the MR to the CT domain in a couple of seconds. Building a large collection of publicly available images could undoubtedly lead to avoiding some preprocessing steps and to achieve better overall results",space,1128
,filtered,core,Digital Commons @ University of South Florida,2019-11-08 00:00:00,core,algorithms for multi-objective mixed integer programming problems,,"This thesis presents a total of 3 groups of contributions related to multi-objective optimization.  The first group includes the development of a new algorithm and an open-source user-friendly package for optimization over the efficient set for bi-objective mixed integer linear programs.  The second group includes an application of a special case of optimization over the efficient on conservation planning problems modeled with modern portfolio theory. Finally, the third group presents a machine learning framework to enhance criterion space search algorithms for multi-objective binary linear programming.
In the first group of contributions, this thesis presents the first (criterion space search) algorithm for optimizing a linear function over the set of efficient solutions of bi-objective mixed integer linear programs. The proposed algorithm is developed based on the triangle splitting method (Boland et al.), which can find a full representation of the nondominated frontier of any bi-objective mixed integer linear program. The proposed algorithm is easy to understand and implement, and converges quickly to an optimal solution. An extensive computational study shows the efficacy of the algorithm. Is numerically shown in this thesis that the proposed algorithm can be used to quickly generate a provably high-quality approximate solution because it maintains a lower and an upper bound on the optimal value of the linear function at any point in time. Additionally, this thesis presents OOESAlgorithm.jl, a comprehensive julia package based on the proposed algorithm. The proposed package ex- tends the first implementation of the algorithm by adding two main features: (a) in addition to CPLEX, the package allows employing any single-objective solver supported by Math- ProgBase.jl, for example, GLPK, CPLEX, and SCIP; (b) the package supports execution
on multiple processors and is compatible with the JuMP modeling language. An extensive computational study shows the efficacy of the package and its features.
In the  second group of contributions, this thesis presents a Nash bargaining solu- tion approach for spatial conservation planning problems modeled with modern portfolio theory.  The proposed modern portfolio optimization formulation corresponds to a spatial conservation planning problem involving two conflicting objectives: maximizing return and minimizing risk. A Nash bargaining solution approach is presented in this thesis to directly compute a desirable Pareto-optimal (nondominated) solution for the proposed bi-objective optimization formulation in natural resource management problems. Numerical examples in this thesis show that to directly compute a Nash bargaining solution, a Binary Quadratically Constrained Quadratic Program (BQCQP) can be solved. This thesis also shows that the proposed approach (implementable with commercial  solvers such as CPLEX) can effectively solve the proposed BQCQP for much larger problems than previous approaches published in the ecological literature.  The new approach expands considerably the applicability of such optimization methods to address real spatial conservation planning problems.
In the third group of contributions, this thesis investigates the possibility of improving the performance of multi-objective optimization solution approaches using machine learning techniques. Specifically, this thesis focus on multi-objective binary linear programs and employs one of the most effective and recently developed criterion space search algorithms, the so-called KSA, during our study.  This algorithm computes all nondominated points of a problem with p objectives by searching on a projected criterion space, i.e., a (p − 1)- dimensional criterion space.  This thesis presents an effective and fast learning approach to identify on which projected space the KSA should work, and also presents several generic features that can be used in machine learning techniques for identifying the best-projected space. Finally, a bi-objective optimization-based heuristic for selecting the best subset of the features to overcome the issue of overfitting in learning is presented. Through an extensive computational study, the performance of the proposed learning approach is tested",space,1129
,filtered,core,,2019-05-10 00:00:00,core,multi-level control architecture for bionic handling assistant robot augmented by learning from demonstration for apple-picking,10.6084/m9.figshare.8107610.v1,"<p>The control of soft continuum robots is challenging owing to their mechanical elasticity and complex dynamics. An additional challenge emerges when we want to apply Learning from Demonstration (LfD) and need to collect necessary demonstrations due to the inherent control difficulty. In this paper, we provide a multi-level architecture from low-level control to high-level motion planning for the Bionic Handling Assistant (BHA) robot. We deploy learning across all levels to enable the application of LfD for a real-world manipulation task. To record the demonstrations, an actively compliant controller is used. A variant of dynamical systems' application that are able to encode both position and orientation then maps the recorded 6D end-effector pose data into a virtual attractor space. A recent LfD method encodes the pose attractors within the same model for point-to-point motion planning. In the proposed architecture, hybrid models that combine an analytical approach and machine learning techniques are used to overcome the inherent slow dynamics and model imprecision of the BHA. The performance and generalization capability of the proposed multi-level approach are evaluated in simulation and with the real BHA robot in an apple-picking scenario which requires high accuracy to control the pose of the robot's end-effector.</p",space,1130
,filtered,core,'Copernicus GmbH',2019-01-01 00:00:00,core,a neural network radiative transfer model approach applied to the tropospheric monitoring instrument aerosol height algorithm,10.5194/amt-12-6619-2019,"To retrieve aerosol properties from satellite measurements of the oxygen A-band in the near-infrared, a line-by-line radiative transfer model implementation requires a large number of calculations. These calculations severely restrict a retrieval algorithm's operational capability as it can take several minutes to retrieve the aerosol layer height for a single ground pixel. This paper proposes a forward modelling approach using artificial neural networks to speed up the retrieval algorithm. The forward model outputs are trained into a set of neural network models to completely replace line-by-line calculations in the operational processor. Results comparing the forward model to the neural network alternative show an encouraging outcome with good agreement between the two when they are applied to retrieval scenarios using both synthetic and real measured spectra from TROPOMI (TROPOspheric Monitoring Instrument) on board the European Space Agency (ESA) Sentinel-5 Precursor mission. With an enhancement of the computational speed by 3 orders of magnitude, TROPOMI's operational aerosol layer height processor is now able to retrieve aerosol layer heights well within operational capacity.Atmospheric Remote Sensin",space,1131
,filtered,core,,2019-12-30 00:00:00,core,learning algorithms for robotics systems,,"Robotics systems are now increasingly widespread in our day-life. For instance, robots have been successfully used in several fields, like, agriculture, construction, defense, aerospace, and hospitality. However, there are still several issues to be addressed for allowing the large scale deployment of robots. Issues related to security, and manufacturing and operating costs are particularly relevant. Indeed, differently from industrial applications, service robots should be cheap and capable of operating in unknown, or partially-unknown environments, possibly with minimal human intervention. To deal with these challenges, in the last years the research community focused on deriving learning algorithms capable of providing flexibility and adaptability to the robots. In this context, the application of Machine Learning and Reinforcement Learning techniques turns out to be especially useful. In this manuscript, we propose different learning algorithms for robotics systems. In Chapter 2, we propose a solution for learning the geometrical model of a robot directly from data, combining proprioceptive measures with data collected with a 2D camera. Besides testing the accuracy of the kinematic models derived with real experiments, we validate the possibility of deriving a kinematic controller based on the model identified. Instead, in Chapter 3, we address the robot inverse dynamics problem. Our strategy relies on the fact that the robot inverse dynamics is a polynomial function in a particular input space. Besides characterizing the input space, we propose a data-driven solution based on Gaussian Process Regression (GPR). Given the type of each joint, we define a kernel named Geometrically Inspired Polynomial (GIP) kernel, which is given by the product of several polynomial kernels. To cope with the dimensionality of the resulting polynomial, we use a variation of the standard polynomial kernel, named Multiplicative Polynomial kernel, further discussed in Chapter 6. Tests performed on simulated and real environments show that, compared to other data-driven solutions, the GIP kernel-based estimator is more accurate and data-efficient.

In Chapter 4, we propose a proprioceptive collision detection algorithm based on GPR. Compared to other proprioceptive approaches, we closely inspect the robot behaviors in quasi-static configurations, namely, configurations in which joint velocities are null or close to zero. Such configurations are particularly relevant in the Collaborative Robotics context, where humans and robots work side-by-side sharing the same environment. Experimental results performed with a UR10 robot confirm the relevance of the problem and the effectiveness of the proposed solution.

Finally, in Chapter 5, we present MC-PILCO, a model-based policy search algorithm inspired by the PILCO algorithm. As the original PILCO algorithm, MC-PILCO models the system evolution relying on GPR, and improves the control policy minimizing the expected value of a cost function. However, instead of approximating the expected cost by moment matching, MC-PILCO approximates the expected cost with a Monte Carlo particle-based approach; no assumption about the type of GPR model is necessary. Thus, MC-PILCO allows more freedom in designing the GPR models, possibly leading to better models of the system dynamics. Results obtained in a simulated environment show consistent improvements with respect to the original algorithm, both in terms of speed and success rate",space,1132
,filtered,core,'MDPI AG',2019-04-01 00:00:00,core,object tracking for a smart city using iot and edge computing,10.3390/s19091987,"As the Internet-of-Things (IoT) and edge computing have been major paradigms for distributed data collection, communication, and processing, smart city applications in the real world tend to adopt IoT and edge computing broadly. Today, more and more machine learning algorithms would be deployed into front-end sensors, devices, and edge data centres rather than centralised cloud data centres. However, front-end sensors and devices are usually not so capable as those computing units in huge data centres, and for this sake, in practice, engineers choose to compromise for limited capacity of embedded computing and limited memory, e.g., neural network models being pruned to fit embedded devices. Visual object tracking is one of many important elements of a smart city, and in the IoT and edge computing context, high requirements to computing power and memory space severely prevent massive and accurate tracking. In this paper, we report on our contribution to object tracking on lightweight computing including (1) using limited computing capacity and memory space to realise tracking; (2) proposing a new algorithm region proposal correlation filter fitting for most edge devices. Systematic evaluations show that (1) our techniques can fit most IoT devices; (2) our techniques can keep relatively high accuracy; and (3) the generated model size is much less than others",space,1133
,filtered,core,'MDPI AG',2019-02-01 00:00:00,core,static and dynamic activity detection with ambient sensors in smart spaces,10.3390/s19040804,"Convergence of Machine Learning, Internet of Things, and computationally powerful single-board computers has boosted research and implementation of smart spaces. Smart spaces make predictions based on historical data to enhance user experience. In this paper, we present a low-cost, low-energy smart space implementation to detect static and dynamic human activities that require simple motions. We use low-resolution (4 &#215; 16) and non-intrusive thermal sensors to collect data. We train six machine learning algorithms, namely logistic regression, naive Bayes, support vector machine, decision tree, random forest and artificial neural network (vanilla feed-forward) on the dataset collected in our lab. Our experiments reveal a very high static activity detection rate with all algorithms, where the feed-forward neural network method gives the best accuracy of 99.96%. We also show how data collection methods and sensor placement plays an important role in the resulting accuracy of different machine learning algorithms. To detect dynamic activities in real time, we use cross-correlation and connected components of thermal images. Our smart space implementation, with its real-time properties, can be used in various domains and applications, such as conference room automation, elderly health-care, etc",space,1134
,filtered,core,Learning and Reasoning in Logic Tensor Networks: Theory and Application to Semantic Image Interpretation,2017-01-01 00:00:00,core,10.1145/3019612.3019642,,"This paper presents a revision of Real Logic and its implementation with Logic Tensor Networks to address the problem of Semantic Image Interpretation. Real Logic is a framework where learning from numerical data and logical reasoning are integrated using first order logic syntax. The symbols of the signature of Real Logic are interpreted in the data-space, i.e, on the domain of real numbers. The integration of learning and reasoning obtained in Real Logic allows us to formalize learning as approximate satisfiability in the presence of logical constraints, and to perform inference on symbolic and numerical data. After introducing a refined version of the formalism, we describe its implementation into Logic Tensor Networks which uses deep learning within Google's Tensorflow. We evaluate LTN on the task of classifying objects and their parts in images, where we combine state-of-the-art-object detectors with a part-of ontology. LTN outperforms the state-of-the-art on object classification, and improves the performances on part-of relation detection with respect to a rule-based baseline",space,1135
,filtered,core,"Data_Sheet_1_A Neuro-Inspired System for Online Learning and Recognition of Parallel Spike Trains, Based on Spike Latency, and Heterosynaptic STDP.PDF",2018-10-31 00:00:00,core,10.3389/fnins.2018.00780.s001,,"<p>Humans perform remarkably well in many cognitive tasks including pattern recognition. However, the neuronal mechanisms underlying this process are not well understood. Nevertheless, artificial neural networks, inspired in brain circuits, have been designed and used to tackle spatio-temporal pattern recognition tasks. In this paper we present a multi-neuronal spike pattern detection structure able to autonomously implement online learning and recognition of parallel spike sequences (i.e., sequences of pulses belonging to different neurons/neural ensembles). The operating principle of this structure is based on two spiking/synaptic neurocomputational characteristics: spike latency, which enables neurons to fire spikes with a certain delay and heterosynaptic plasticity, which allows the own regulation of synaptic weights. From the perspective of the information representation, the structure allows mapping a spatio-temporal stimulus into a multi-dimensional, temporal, feature space. In this space, the parameter coordinate and the time at which a neuron fires represent one specific feature. In this sense, each feature can be considered to span a single temporal axis. We applied our proposed scheme to experimental data obtained from a motor-inhibitory cognitive task. The results show that out method exhibits similar performance compared with other classification methods, indicating the effectiveness of our approach. In addition, its simplicity and low computational cost suggest a large scale implementation for real time recognition applications in several areas, such as brain computer interface, personal biometrics authentication, or early detection of diseases.</p",space,1136
,filtered,core,GenderTron: exploring implicit gender bias,2018-03-29 00:00:00,core,10.4225/03/5abc50e48db39,,"How can we use machine learning to provoke new conversations about gender? <br>This talk will discuss a project that questioned the role of technology and design within complex social issues like gender inequality. Drawing from feminist theory and linguistics algorithms, this project developed a research device that monitored spoken ‘gendered language’ within a space and revealed these patterns back to users in real time. This device was implemented in a variety of settings and assessed through the interviews and documentary film. In addition to serving as a heuristic learning tool to generate complex discussions about gender identity, this device explored what additional research value can be gained when turning quantitative data into a qualitative experience.<br",space,1137
,filtered,core,Governance and Assessment of Future Spaces: A Discussion of Some Issues Raised by the Possibilities of Human-Machine Mergers,2017-09-19 00:00:00,core,10.5281/zenodo.896109,https://core.ac.uk/download/pdf/144821921.pdf,"‘In faith, I do not love thee with mine eyes, For they in thee a thousand errors note; But ‘tis my heart that loves what they despise …’ 1 This sonnet and the ancient Japanese notion of wabi-sabi view aesthetics or beauty as imperfect, impermanent and incomplete. Rather than celebrating the human diversity created by our ‘imperfections’, today's society increasingly focuses on them as ‘areas for improvement’, often via a doctor’s scalpel or the latest gadget. Developments in science, technology, engineering, mathematics and medicine (STEMM) promise a tomorrow where ‘errors’ or ‘deficiencies’ in an organism’s genetic and/or phenotypic makeup can be modulated, enhanced, corrected, redefined or eradicated by, for instance, networks of biological nanomachines. Upgraded organisms will be convolutions of organic parts, electronic components, microchips, and biomechanotronic devices. Humans 1.0, Humans 2.0 and transhumans will live in new fully immersive worlds (virtual reality), inhabit a modified real world (augmented reality), and exist with an altered body schema (mixed-reality). This future world could be a place of total technological convergence, where it may not be possible to ensure privacy of an individual’s thoughts. It could also be a place where people can be subjected to Social Engineering and manipulation, including the potential for viruses and malware infecting the brain or body, as well as new forms of external control of individuals by third parties. In this discussion paper, we will explore the potential privacy, security, and ethical issues raised by humanmachine mergers. The focus is on research, development and products at the intersection of robotics, artificial intelligence, Big Data, and smart computing. We suggest that there is a need for a more holistic approach to the assessment of technology and its governance. Additionally, we suggest that in order to determine how the law will need to respond to this particular future space, it is necessary to understand the full impacts of human-machine mergers on societies and our planet – to go beyond these three issues. Since STEMM-related activities are promising a cornucopia of future spaces, we will propose that the problems of governance and assessment require a new conception of ‘responsible research and innovation’, one that is fulfilled by our recently proposed FLE5 SH framework.2 To some extent the FLE5 SH framework can be seen as allowing the formation of a social contract, whereby all stakeholders are required to engage in a review of this wider spectrum of the possible impacts of technologies. We suggest that a Precautionary Principle approach may be of assistance in considering the impacts of technologies, remembering that especially in the context of software based systems, it is always useful to think first and bugfix later",space,1138
,filtered,core,"Il minore come persona digitale.

Regole, tutele e privacy dei minori sul Web",2017-01-01 00:00:00,core,10.15160/2038-1034/1417,,"– Nel corso di questo lavoro ho inteso approfondire e discutere le seguenti tesi: 1. il code, ossia il

software e l’hardware che costituiscono il cyberspazio, impone un assetto normativo sul comportamento individuale

e collettivo nel Web. Con ciò non si nega la funzione regolativa del diritto sul cyberspazio; si pensi ad

esempio alle sanzioni previste dalle leggi sul copyright, sul diritto contrattuale, sulla diffamazione e sull’oscenità.

Pur tuttavia, Internet rappresenta un universo di flussi e di attriti restio all’imposizione top-down di qualsivoglia

governance estranea ai propri utenti. Il code è performativo: “ciò che dice fa”; 2. il cyberspazio è un luogo ambivalente,

ove molte attività si sovrappongono alle attività del mondo reale e molte attività gli sono peculiari, denotando

la plasticità che gli è propria. Il tasso di crescita dell’innovazione tecnologica continua ad aumentare ed è

accompagnato da nuove insidiose forme di invasione della sfera privata delle persone. È forte la tentazione di rinunciare

ai propri diritti per godere del paradiso tecnologico che ci viene offerto. Nell’ecosistema digitale compare

una nuova entità – la persona digitale – quale esito tecnologico della riconfigurazione della nozione classica di

persona; 3. lo sviluppo delle applicazioni 2.0 permette la connessione tra persone e loro corrispondenti identità

digitali: soggetti che formano legami senza vincoli di spazio e di compresenza fisica diventano parte di uno sciame

digitale. La nostra privacy (e in particolar modo quella dei minori, nativi digitali) è progressivamente erosa dalla

nostra crescente accondiscendenza, apatia, indifferenza o supporto esplicito a misure che ci sono presentate

come indispensabili o innocue. Se è ancora prematuro affermare che la privacy è ormai morta, le giovani generazioni

sono protagoniste, volenti o no, di una prassi culturale e di una socializzazione primaria lontane dal concetto

di privacy; 4. la quarta tesi, con cui concludo questo contributo, approfondisce la questione se si possono 

ancora ricavare spazi per forme di governance efficaci a tutelare la persona digitale: il suo diritto alla dignità,

all’habeas data e alla riservatezza dei dati personali in Internet.– The purpose of this work is to discuss and investigate the following theses: 1. the code, that is to say

the software and the hardware cyberspace is made of, imposes a normative set-up both on Web collective and

individual conduct. By that, law’s regulative task on Web can’t be denied; for instance, think about sanctions imposed

by laws about copyright, contracts, slander and obscenity. Yet Internet represents a universe made of

flows and frictions, unwilling to whatever top-down governance alien to its users. Code is performative: “what it

says, it does”; 2. cyberspace is an ambivalent place, where many activities are laid on top of real world’s ones

and many others are peculiar to it, showing its own plasticity. Technological innovation’s growth rate is still increasing,

together with new insidious forms of invasion of people’s private life. It is still strong the temptation to

waive one’s own rights only to enjoy the technological paradise we are offered. A new entity – digital person –

makes its appearance on digital ecosystem, as technological outcome of classical person concept’s reconfiguration;

3. the development of 2.0 applications allows to put into connection people and their digital identities: individuals

get involved in a digital swarm, forming links free from any territorial and simultaneous physical presence’s

chains. Our privacy (in particular, that of the minors, born in the digital era) is progressively eaten away by

our rising indulgency, apathy, unconcern and explicit support to measures, shown us as necessary and harmless.

If it is yet untimely to say that privacy is almost death, new generations, whether they like it or not, are playing

a leading role in a cultural praxis and in a primary socialization which is far from the concept of privacy; 4. the

fourth thesis, summing up this note, investigates if it is yet possible or not to give space to new effective forms of

governance, appointed to defend the digital person: its rights to dignity, habeas data and personal data privacy",space,1139
,filtered,core,HTRU2,2016-03-01 00:00:00,core,10.6084/m9.figshare.3080389.v1,,"<div>1. Overview</div><div><br></div><div>HTRU2 is a data set which describes a sample of pulsar candidates collected during the High Time Resolution Universe Survey (South) [1]. </div><div><br></div><div></div><div>Pulsars are a rare type of Neutron star that produce radio emission detectable here on Earth. They are of considerable scientific interest as probes of space-time, the inter-stellar medium, and states of matter (see [2] for more uses). </div><div><br></div><div>As pulsars rotate, their emission beam sweeps across the sky, and when this crosses our line of sight, produces a detectable pattern of broadband radio emission. As pulsars</div><div>rotate rapidly, this pattern repeats periodically. Thus pulsar search involves looking for periodic radio signals with large radio telescopes.</div><div><br></div><div></div><div>Each pulsar produces a slightly different emission pattern, which varies slightly with each rotation (see [2] for an introduction to pulsar astrophysics to find out why). Thus a  potential signal detection known as a 'candidate', is averaged over many rotations of the pulsar, as determined by the length of an observation. In the absence of additional info, each candidate could potentially describe a real pulsar. However in practice almost all detections are caused by radio frequency interference (RFI) and noise, making legitimate signals hard to find.</div><div><br></div><div></div><div>Machine learning tools are now being used to automatically label pulsar candidates to facilitate rapid analysis. Classification systems in particular are being widely adopted,</div><div>(see [4,5,6,7,8,9]) which treat the candidate data sets  as binary classification problems. Here the legitimate pulsar examples are a minority positive class, and spurious examples the majority negative class. At present multi-class labels are unavailable, given the costs associated with data annotation.</div><div><br></div><div></div><div>The data set shared here contains 16,259 spurious examples caused by RFI/noise, and 1,639 real pulsar examples. These examples have all been checked by human annotators. Each candidate is described by 8 continuous variables. The first four are simple statistics obtained from the integrated pulse profile (folded profile). This is an array of continuous variables that describe a longitude-resolved version of the signal that has been averaged in both time and frequency (see [3] for more details). The remaining four variables are similarly obtained from the DM-SNR curve (again see [3] for more details). These are summarised below:</div><div><br></div><div></div><div>1. Mean of the integrated profile.</div><div>2. Standard deviation of the integrated profile.</div><div>3. Excess kurtosis of the integrated profile.</div><div>4. Skewness of the integrated profile.</div><div>5. Mean of the DM-SNR curve.</div><div>6. Standard deviation of the DM-SNR curve.</div><div>7. Excess kurtosis of the DM-SNR curve.</div><div>8. Skewness of the DM-SNR curve.</div><div><br></div><div></div><div>HTRU 2 Summary</div><div></div><div>17,898 total examples.</div><div>1,639 positive examples.</div><div>16,259 negative examples.</div><div><br></div><div></div><div></div><div>The data is presented in two formats: CSV and ARFF (used by the WEKA data mining tool). Candidates are stored in both files in separate rows. Each row lists the variables first, and the class label is the final entry. The class labels used are 0 (negative) and 1 (positive).</div><div><br></div><div></div><div>Please note that the data contains no positional information or other astronomical details. It is simply feature data extracted from candidate files using the PulsarFeatureLab tool (see [10]).</div><div><br></div><div>2. Citing our work</div><div><br></div><div></div><div>If you use the dataset in your work please cite us using the DOI of the dataset, and the paper: </div><div><br></div><div>R. J. Lyon, B. W. Stappers, S. Cooper, J. M. Brooke, J. D. Knowles, Fifty Years of Pulsar Candidate Selection: From simple filters to a new principled real-time classification approach MNRAS, 2016.</div><div><br></div><div></div><div>3. Acknowledgements</div><div><br></div><div>This data was obtained with the support of grant EP/I028099/1 for the University of Manchester  Centre for Doctoral Training in Computer Science, from the UK Engineering and Physical Sciences Research Council (EPSRC). The raw observational data was collected by the High Time Resolution Universe Collaboration using the Parkes Observatory, funded by the Commonwealth of Australia and managed by the CSIRO.</div><div><br></div><div></div><div>4. References</div><div><br></div><div>[1] M.~J. Keith et al., ""The High Time Resolution Universe Pulsar Survey - I. System Configuration and Initial Discoveries"",2010, Monthly Notices of the Royal Astronomical Society, vol. 409,  pp. 619-627. DOI: 10.1111/j.1365-2966.2010.17325.x</div><div><br></div><div></div><div>[2] D. R. Lorimer and M. Kramer, ""Handbook of Pulsar Astronomy"", Cambridge University Press, 2005.</div><div><br></div><div></div><div>[3] R. J. Lyon, ""Why Are Pulsars Hard To Find?"", PhD Thesis, University of Manchester, 2015.</div><div><br></div><div></div><div>[4] R. J. Lyon et al., ""Fifty Years of Pulsar Candidate Selection: From simple filters to a new principled real-time classification approach"", Monthly Notices of the Royal Astronomical Society, submitted.</div><div><br></div><div></div><div>[5] R. P. Eatough et al., ""Selection of radio pulsar candidates using artificial neural networks"", Monthly Notices of the Royal Astronomical Society, vol. 407, no. 4, pp. 2443-2450, 2010.</div><div><br></div><div></div><div>[6] S. D. Bates et al., ""The high time resolution universe pulsar survey vi. an artificial neural network and timing of 75 pulsars"", Monthly Notices of the Royal Astronomical Society, vol. 427, no. 2, pp. 1052-1065, 2012.</div><div><br></div><div>[7] D. Thornton, ""The High Time Resolution Radio Sky"", PhD thesis, University of Manchester, Jodrell Bank Centre for Astrophysics School of Physics and Astronomy, 2013.</div><div><br></div><div></div><div>[8] K. J. Lee et al., ""PEACE: pulsar evaluation algorithm for candidate extraction a software package for post-analysis processing of pulsar survey candidates"", Monthly Notices of the Royal Astronomical Society, vol. 433, no. 1, pp. 688-694, 2013.</div><div><br></div><div></div><div>[9] V. Morello et al., ""SPINN: a straightforward machine learning solution to the pulsar candidate selection problem"", Monthly Notices of the Royal Astronomical Society, vol. 443, no. 2, pp. 1651-1662, 2014.</div><div><br></div><div></div><div>[10] R. J. Lyon, ""PulsarFeatureLab"", 2015, https://dx.doi.org/10.6084/m9.figshare.1536472.v1.</div><div><br></div",space,1140
,filtered,core,Graph-based inter-subject pattern analysis of fMRI data,2014-07-01 00:00:00,core,https://core.ac.uk/download/49608797.pdf,'Public Library of Science (PLoS)',"International audienceIn brain imaging, solving learning problems in multi-subjects settings is difficult because of the differences that exist across individuals. Here we introduce a novel classification framework based on group-invariant graphical representations, allowing to overcome the inter-subject variability present in functional magnetic resonance imaging (fMRI) data and to perform multivariate pattern analysis across subjects. Our contribution is twofold: first, we propose an unsupervised representation learning scheme that encodes all relevant characteristics of distributed fMRI patterns into attributed graphs; second, we introduce a custom-designed graph kernel that exploits all these characteristics and makes it possible to perform supervised learning (here, classification) directly in graph space. The well-foundedness of our technique and the robustness of the performance to the parameter setting are demonstrated through inter-subject classification experiments conducted on both artificial data and a real fMRI experiment aimed at characterizing local cortical representations. Our results show that our framework produces accurate inter-subject predictions and that it outperforms a wide range of state-of-the-art vector- and parcel-based classification methods. Moreover, the genericity of our method makes it is easily adaptable to a wide range of potential applications. The dataset used in this study and an implementation of our framework are available at http://dx.doi.org/10.6084/m9.figshare.1086317",space,1141
,filtered,core,Combination of Minimum-Maximum (m-m) Attribute and Zero-INTENS-Difference (z-i-d) Attribute for Estimating Seismically Thin-Bed Thickness,2013-09-13 00:00:00,core,https://core.ac.uk/download/481314527.pdf,"Institute for Research and Community Services, Institut Teknologi Bandung","This  paper  demonstrates  a  new  alternative  way  in  estimating seismically thin-bed (below-tuning) thickness. Initial thickness is built by bandpass filtering the amplitude display of a zero-phase seismic. The filter removes the  non  minimum  and  or  non  maximum  and  left  the  maximum  and  or  the minimum of seismic amplitude. The unresolved below-tuning thickness is then corrected  by  zero-INTENS-difference  (z-i-d)  attribute.  INTENS  is  integrated energy  spectra,  an  attribute  which  can  be  derived  from  spectral  analysis.  z-i-d attribute is zero difference of INTENS between the seismic and its synthetic. The method  generates  INTENS  difference  profile  by  subtracting  seismic  INTENS and its synthetic INTENS iteratively. The iteration is controlled by dipole space shifting from  distance to closer or  vice  versa.  The true thickness is derived  by locating z-i-d which laid in INTENS different profile. It has found that, for free noise  true  seismic  and  perfect-wavelet  (a  wavelet  which  only  approximately similar  with  wavelet  which  constructing  the  true  seismic)  synthetic  seismic,  in INTENS  different  profile,  the  z-i-d  location  always  corresponds  to  true  dipole space or thickness. The method could resolve all thickness of a wedge-modeled seismic with three different dominant frequencies. When the synthetic seismic is constructed with imperfect wavelet, slightly different analysis is needed to locate z-i-d  attribute  and  the  result  is  not  as  perfect  as  when  perfect  wavelet constructing synthetic seismic. A quiet similar result is got when the method is implemented  for  noisy  wedge-modeled  seismic.  Bad  thickness  estimation  is resulted  for  20%  noise  seismic.  The  method  algorithm  is  extended  for  similar dipole polarity model and multilayer model to bring the method to real seismic data  nearer.  The  extension  is  done  by  estimating  thickness  of  every  layer  of  a stacked-wedge-modeled  seismic. The algorithm then generalized for estimating layers  thickness  with  several  thickness  combinations.  The  method  was  able  to delineate shallow channel of Stratton Field by providing good pseudo-acousticimpedance (pseudo AI) map",space,1142
,filtered,core,Combination of Minimum-Maximum (m-m) Attribute and Zero-INTENS-Difference (z-i-d) Attribute for Estimating Seismically Thin-Bed Thickness,2013-09-13 00:00:00,core,https://core.ac.uk/download/427084953.pdf,'The Institute for Research and Community Services (LPPM) ITB',"This  paper  demonstrates  a  new  alternative  way  in  estimating seismically thin-bed (below-tuning) thickness. Initial thickness is built by bandpass filtering the amplitude display of a zero-phase seismic. The filter removes the  non  minimum  and  or  non  maximum  and  left  the  maximum  and  or  the minimum of seismic amplitude. The unresolved below-tuning thickness is then corrected  by  zero-INTENS-difference  (z-i-d)  attribute.  INTENS  is  integrated energy  spectra,  an  attribute  which  can  be  derived  from  spectral  analysis.  z-i-d attribute is zero difference of INTENS between the seismic and its synthetic. The method  generates  INTENS  difference  profile  by  subtracting  seismic  INTENS and its synthetic INTENS iteratively. The iteration is controlled by dipole space shifting from  distance to closer or  vice  versa.  The true thickness is derived  by locating z-i-d which laid in INTENS different profile. It has found that, for free noise  true  seismic  and  perfect-wavelet  (a  wavelet  which  only  approximately similar  with  wavelet  which  constructing  the  true  seismic)  synthetic  seismic,  in INTENS  different  profile,  the  z-i-d  location  always  corresponds  to  true  dipole space or thickness. The method could resolve all thickness of a wedge-modeled seismic with three different dominant frequencies. When the synthetic seismic is constructed with imperfect wavelet, slightly different analysis is needed to locate z-i-d  attribute  and  the  result  is  not  as  perfect  as  when  perfect  wavelet constructing synthetic seismic. A quiet similar result is got when the method is implemented  for  noisy  wedge-modeled  seismic.  Bad  thickness  estimation  is resulted  for  20%  noise  seismic.  The  method  algorithm  is  extended  for  similar dipole polarity model and multilayer model to bring the method to real seismic data  nearer.  The  extension  is  done  by  estimating  thickness  of  every  layer  of  a stacked-wedge-modeled  seismic. The algorithm then generalized for estimating layers  thickness  with  several  thickness  combinations.  The  method  was  able  to delineate shallow channel of Stratton Field by providing good pseudo-acousticimpedance (pseudo AI) map",space,1143
,filtered,core,Ten years of MIPAS measurements with ESA Level 2 processor V6-Part 1: Retrieval algorithm and diagnostics of the products,2013-09-23 00:00:00,core,http://hdl.handle.net/2381/32625,,"The MIPAS (Michelson Interferometer for Passive Atmospheric Sounding) instrument on the Envisat (Environmental satellite) satellite has provided vertical profiles of the atmospheric composition on a global scale for almost ten years. The MIPAS mission is divided in two phases: the full resolution phase, from 2002 to 2004, and the optimized resolution phase, from 2005 to 2012, which is characterized by a finer vertical and horizontal sampling attained through a reduction of the spectral resolution.

While the description and characterization of the products of the ESA processor for the full resolution phase has been already described in previous papers, in this paper we focus on the performances of the latest version of the ESA (European Space Agency) processor, named ML2PP V6 (MIPAS Level 2 Prototype Processor), which has been used for reprocessing the entire mission. The ESA processor had to perform the operational near real time analysis of the observations and its products needed to be available for data assimilation. Therefore, it has been designed for fast, continuous and automated analysis of observations made in quite different atmospheric conditions and for a minimum use of external constraints in order to avoid biases in the products.

The dense vertical sampling of the measurements adopted in the second phase of the MIPAS mission resulted in sampling intervals finer than the instantaneous field of view of the instrument. Together with the choice of a retrieval grid aligned with the vertical sampling of the measurements, this made ill-conditioned the retrieval problem of the MIPAS operational processor. This problem has been handled with minimal changes to the original retrieval approach but with significant improvements nonetheless. The Levenberg–Marquardt method, already present in the retrieval scheme for its capability to provide fast convergence for nonlinear problems, is now also exploited for the reduction of the ill-conditioning of the inversion. An expression specifically designed for the regularizing Levenberg–Marquardt method has been implemented for the computation of the covariance matrices and averaging kernels of the retrieved products. The regularization of the Levenberg–Marquardt method is controlled by the convergence criteria and is deliberately kept weak. The resulting oscillations of the retrieved profile are a posteriori damped by an innovative self-adapting Tikhonov regularization. The convergence criteria and the weakness of the self-adapting regularization ensure that minimum constraints are used and the best vertical resolution obtainable from the measurements is achieved in all atmospheric conditions.

Random and systematic errors, as well as vertical and horizontal resolution are compared in the two phases of the mission for all products, namely: temperature, H[subscript: 2]O, O[subscript: 3], HNO[subscript: 3], CH[subscript: 4], N[subscript: 2]O, NO[subscript: 2], CFC-11, CFC-12, N[subscript: 2]O[subscript: 5] and ClONO[subscript: 2]. The use in the two phases of the mission of different optimized sets of spectral intervals ensures that, despite the different spectral resolutions, comparable performances are obtained in the whole MIPAS mission in terms of random and systematic errors, while the vertical resolution and the horizontal resolution are significantly better in the case of the optimized resolution measurements",space,1144
,filtered,core,Automated Real-Time Classification and Decision Making in Massive Data Streams from Synoptic Sky Surveys,2014-10-01 00:00:00,core,https://core.ac.uk/download/33116415.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',"The nature of scientific and technological data collection is evolving rapidly: data volumes and rates grow exponentially, with increasing complexity and information content, and there has been a transition from static data sets to data streams that must be analyzed in real time. Interesting or anomalous phenomena must be quickly characterized and followed up with additional measurements via optimal deployment of limited assets. Modern astronomy presents a variety of such phenomena in the form of transient events in digital synoptic sky surveys, including cosmic explosions (supernovae, gamma ray bursts), relativistic phenomena (black hole formation, jets), potentially hazardous asteroids, etc. We have been developing a set of machine learning tools to detect, classify and plan a response to transient events for astronomy applications, using the Catalina Real-time Transient Survey (CRTS) as a scientific and methodological testbed. The ability to respond rapidly to the potentially most interesting events is a key bottleneck that limits the scientific returns from the current and anticipated synoptic sky surveys. Similar challenge arise in other contexts, from environmental monitoring using sensor networks to autonomous spacecraft systems. Given the exponential growth of data rates, and the time-critical response, we need a fully automated and robust approach. We describe the results obtained to date, and the possible future developments",space,1145
,filtered,core,Copper nanoparticle heterogeneous catalytic 'click' cycloaddition confirmed by single-molecule spectroscopy,2014-08-01 00:00:00,core,https://riunet.upv.es/bitstream/10251/61400/2/ncomms5612.pdf,'Springer Science and Business Media LLC',"Colloidal or heterogeneous nanocatalysts can improve the range and diversity of Cu(I)catalysed click reactions and facilitate catalyst separation and reuse. Catalysis by metal nanoparticles raises the question as to whether heterogeneous catalysts may cause homogeneous catalysis through metal ion leaching, since the catalytic process could be mediated by the particle, or by metal ions released from it. The question is critical as unwanted homogeneous processes could offset the benefits of heterogeneous catalysis. Here, we combine standard bench scale techniques with single-molecule spectroscopy to monitor single catalytic events in real time and demonstrate that click catalysis occurs directly at the surface of copper nanoparticles; this general approach could be implemented in other systems. We use 'from the mole to the molecule' to describe this emerging idea in which mole scale reactions can be optimized through an intimate understanding of the catalytic process at the single-molecule-single catalytic nanoparticle level.The Natural Sciences and Engineering Research Council of Canada supported this work through its Discovery and CREATE programs, while the Canadian Foundation for Innovation enabled the purchase of the instrumentation used in this work. M. L. M. thanks the Spanish Ministerio de Educacion, Cultura y Deporte (Programa Salvador de Madariaga) for its financial support.Decan, MR.; Impellizzeri, S.; Marín García, ML.; Scaiano, J. (2014). Copper nanoparticle heterogeneous catalytic 'click' cycloaddition confirmed by single-molecule spectroscopy. Nature Communications. 5:4612-4615. doi:10.1038/ncomms5612S461246155Rostovtsev, V. V., Green, L. G., Fokin, V. V. & Sharpless, K. B. A stepwise Huisgen cycloaddition process: copper(I)-catalyzed regioselective ‘ligation’ of azides and terminal alkynes. Angew. Chem. Int. Ed. 41, 2596–2599 (2002).Tornøe, C. W., Christensen, C. & Meldal, M. Peptidotriazoles on solid phase: [1,2,3]-triazoles by regiospecific copper(I)-catalyzed 1,3-dipolar cycloadditions of terminal alkynes to azides. J. Org. Chem. 67, 3057–3064 (2002).Hein, J. & Fokin, V. Copper-catalyzed azide–alkyne cycloaddition (CuAAC) and beyond: new reactivity of copper(I) acetylides. Chem. Soc. Rev. 39, 1302–1315 (2010).Kolb, H. C., Finn, M. G. & Sharpless, K. B. Click chemistry: diverse chemical function from a few good reactions. Angew. Chem. Int. Ed. 40, 2004 (2001).Meldal, M. & Tornoe, C. W. Cu-catalyzed azide-alkyne cycloaddition. Chem. Rev. 108, 2952–3015 (2008).Moses, J. E. & Moorhouse, A. D. The growing applications of click chemistry. Chem. Soc. Rev. 36, 1249–1262 (2007).Thirumurugan, P., Matosiuk, D. & Jozwiak, K. Click chemistry for drug development and diverse chemical—biology applications. Chem. Rev. 113, 4905–4979 (2013).Cintas, P., Barge, A., Tagliapietra, S., Boffa, L. & Cravotto, G. Alkyne–azide click reaction catalyzed by metallic copper under ultrasound. Nat. Protoc. 5, 607–616 (2010).Hong, V., Presolski, S., Ma, C. & Finn, M. Analysis and optimization of copper-catalyzed azide-alkyne cycloaddition for bioconjugation. Angew. Chem. Int. Ed. 48, 9879–9883 (2009).Kappe, C. & Van Der Eycken, E. Click chemistry under non-classical reaction conditions. Chem. Soc. Rev. 39, 1280–1290 (2010).Pachón, L., Van Maarseveen, J. & Rothenberg, G. Click chemistry: copper clusters catalyse the cycloaddition of azides with terminal alkynes. Adv. Synth. Catal. 347, 811–815 (2005).Adzima, B. et al. Spatial and temporal control of the alkyne–azide cycloaddition by photoinitiated Cu(II) reduction. Nat. Chem. 3, 258–261 (2011).Jin, T., Yan, M. & Yamamoto, Y. Click chemistry of alkyne-azide cycloaddition using nanostructured copper catalysts. ChemCatChem 4, 1217–1229 (2012).Woo, H. et al. Azide-alkyne Huisgen [3+2] cycloaddition using CuO nanoparticles. Molecules 17, 13235–13252 (2012).Rance, G., Solomonsz, W. & Khlobystov, A. Click chemistry in carbon nanoreactors. Chem. Commun. 49, 1067–1069 (2013).Alonso, F., Moglie, Y., Radivoy, G. & Yus, M. Unsupported copper nanoparticles in the 1,3-dipolar cycloaddition of terminal alkynes and azides. Eur. J. Org. Chem. 10, 1875–1884 (2010).Alonso, F., Moglie, Y., Radivoy, G. & Yus, M. Multicomponent synthesis of 1,2,3-triazoles in water catalyzed by copper nanoparticles on activated carbon. Adv. Synth. Catal. 352, 3208–3214 (2010).Raut, D. et al. Copper nanoparticles in ionic liquids: Recyclable and efficient catalytic system for 1,3-dipolar cycloaddition reaction. Catal. Commun. 10, 1240–1243 (2009).Kumar, B. S. P. A., Reddy, K. H. V., Madhav, B., Ramesh, K. & Nageswar, Y. V. D. Magnetically separable CuFe2O4 nano particles catalyzed multicomponent synthesis of 1,4-disubstituted 1,2,3-triazoles in tap water using ‘click chemistry’. Tetrahedron Lett. 53, 4595–4599 (2012).Hudson, R., Li, C. & Moores, A. Magnetic copper–iron nanoparticles as simple heterogeneous catalysts for the azide–alkyne click reaction in water. Green Chem. 14, 622–624 (2012).Sarkar, A., Mukherjee, T. & Kapoor, S. PVP-stabilized copper nanoparticles: a reusable catalyst for ‘click’ reaction between terminal alkynes and azides in nonaqueous solvents. J. Phys. Chem. C 112, 3334–3340 (2008).Pacioni, N. L., Filippenko, V., Presseau, N. & Scaiano, J. C. Oxidation of copper nanoparticles in water: mechanistic insights revealed by oxygen uptake and spectroscopic methods. Dalton Trans. 42, 5832–5838 (2013).Davies, I. W., Matty, L., Hughes, D. L. & Reider, P. J. Are heterogeneous catalysts precursors to homogeneous catalysts? J. Am.Chem. Soc. 123, 10139–10140 (2001).Witham, C. A. et al. Converting homogeneous to heterogeneous in electrophilic catalysis using monodisperse metal nanoparticles. Nat. Chem. 2, 36–41 (2010).Schmidt, A. F. & Kurokhtina, A. A. Distinguishing between the homogeneous and heterogeneous mechanisms of catalysis in the Mizoroki-Heck and Suzuki-Miyaura reactions: problems and prospects. Kinet. Catal. 53, 714–730 (2012).Nishina, Y., Miyata, J., Kawai, R. & Gotoh, K. Recyclable Pd-graphene catalyst: mechanistic insights into heterogeneous and homogeneous catalysis. RSC Advances 2, 9380–9382 (2012).Esfandiari, N. M. & Blum, S. A. Homogeneous vs heterogeneous polymerization catalysis revealed by single-particle fluorescence microscopy. J. Am.Chem. Soc. 133, 18145–18147 (2011).Hensle, E. M. & Blum, S. A. Phase separation polymerization of dicyclopentadiene characterized by in operando fluorescence microscopy. J. Am.Chem. Soc. 135, 12324–12328 (2013).De Cremer, G. et al. High-resolution single-turnover mapping reveals intraparticle diffusion limitation in Ti-MCM-41-catalyzed epoxidation. Angew. Chem. Int. Ed. 49, 908–911 (2010).Roeffaers, M. B. J. et al. Super-resolution reactivity mapping of nanostructured catalyst particles. Angew. Chem. Int. Ed. 48, 9285–9289 (2009).Roeffaers, M. B. J. et al. Spatially resolved observation of crystal-face-dependent catalysis by single turnover counting. Nature 439, 572–575 (2006).Xu, W., Kong, J. S., Yeh, Y.-T.E. & Chen, P. Single-molecule nanocatalysis reveals heterogeneous reaction pathways and catalytic dynamics. Nat. Mater. 7, 992–996 (2008).Zhou, X., Xu, W., Liu, G., Panda, D. & Chen, P. Size-dependent catalytic activity and dynamics of gold nanoparticles at the single-molecule level. J. Am.Chem. Soc. 132, 138–146 (2010).Zhou, X. et al. Quantitative super-resolution imaging uncovers reactivity patterns on single nanocatalysts. Nat. Nanotechnol. 7, 237–241 (2012).Chen, P. et al. Single-molecule fluorescence imaging of nanocatalytic processes. Chem. Soc. Rev. 39, 4560–4570 (2010).Chen, P. et al. Spatiotemporal catalytic dynamics within single nanocatalysts revealed by single-molecule microscopy. Chem. Soc. Rev. 43, 1107–1117 (2014).Buurmans, I. L. C. & Weckhuysen, B. M. Heterogeneities of individual catalyst particles in space and time as monitored by spectroscopy. Nat. Chem. 4, 873–886 (2012).Cordes, T. & Blum, S. A. Opportunities and challenges in single-molecule and single-particle fluorescence microscopy for mechanistic studies of chemical reactions. Nat. Chem. 5, 993–999 (2013).Zhou, X., Choudhary, E., Andoy, N. M., Zou, N. & Chen, P. Scalable parallel screening of catalyst activity at the single-particle level and subdiffraction resolution. ACS Catal. 3, 1448–1453 (2013).Esfandiari, N. M. et al. Single-molecule imaging of platinum ligand exchange reaction reveals reactivity distribution. J. Am. Chem. Soc. 132, 15167–15169 (2010).Wee, T., Schmidt, L. C. & Scaiano, J. C. Photooxidation of 9-anthraldehyde catalyzed by gold nanoparticles: solution and single nanoparticle studies using fluorescence lifetime imaging. J. Phys. Chem. C 116, 24373–24379 (2012).Fiolka, R., Belyaev, Y., Ewers, H. & Stemmer, A. Even illumination in total internal reflection fluorescence microscopy using laser light. Microsc. Res. Tech. 71, 45–50 (2007).Canham, S. M. et al. Toward the single-molecule investigation of organometallic reaction mechanisms: single-molecule imaging of fluorophore-tagged palladium(II) complexes. Organometallics 27, 2172–2175 (2008).Jares-Erijman, E. & Jovin, T. FRET imaging. Nat. Biotechnol. 21, 1387–1395 (2003).Roy, R., Hohng, S. & Ha, T. A practical guide to single-molecule FRET. Nat. Methods 5, 507–516 (2008).Kasper, L. et al. Probing the free-energy surface for protein folding with single-molecule spectroscopy. Nature 419, 743–747 (2002).Chung, H., Louis, J. & Eaton, W. Distinguishing between protein dynamics and dye photophysics in single-molecule FRET experiments. Biophys. J. 98, 696–706 (2010).Hohlbein, J., Craggs, T. & Cordes, T. Alternating-laser excitation: single-molecule FRET and beyond. Chem. Soc. Rev. 43, 1156–1171 (2014).Greenleaf, W. J., Woodside, M. T. & Block, S. M. High-resolution, single-molecule measurements of biomolecular motion. Annu. Rev. Biophys. Biomol. Struct. 36, 171–190 (2007).Di Fiori, N. & Meller, A. The effect of dye-dye interactions on the spatial resolution of single-molecule FRET measurements in nucleic acids. Biophys. J. 98, 2265–2272.Ahlquist, M. r. & Fokin, V. Enhanced reactivity of dinuclear copper(I) acetylides in dipolar cycloadditions. Organometallics 26, 4389–4391 (2007).Straub, B. μ-Acetylide and μ-alkenylidene ligands in ‘click’ triazole syntheses. Chem. Commun. 37, 3868–3870 (2007)",space,1146
,filtered,core,Genetic programming and serial processing for time series classification,2014-06-01 00:00:00,core,https://riunet.upv.es/bitstream/handle/10251/67540/alfaro%3bsharman%3besparcia%20-%20genetic%20programming%20and%20serial%20processing%20for%20time%20series%20classification.pdf?sequence=1&isallowed=y,'MIT Press - Journals',"This work describes an approach devised by the authors for time series classification. In our approach genetic programming is used in combination with a serial processing of data, where the last output is the result of the classification. The use of genetic programming for classification, although still a field where more research in needed, is not new. However, the application of genetic programming to classification tasks is normally done by considering the input data as a feature vector. That is, to the best of our knowledge, there are not examples in the genetic programming literature of approaches where the time series data are processed serially and the last output is considered as the classification result. The serial processing approach presented here fills a gap in the existing literature. This approach was tested in three different problems. Two of them are real world problems whose data were gathered for online or conference competitions. As there are published results of these two problems this gives us the chance to compare the performance of our approach against top performing methods. The serial processing of data in combination with genetic programming obtained competitive results in both competitions, showing its potential for solving time series classification problems. The main advantage of our serial processing approach is that it can easily handle very large datasets.Alfaro Cid, E.; Sharman, KC.; Esparcia Alcázar, AI. (2014). Genetic programming and serial processing for time series classification. Evolutionary Computation. 22(2):265-285. doi:10.1162/EVCO_a_00110S265285222Adeodato, P. J. L., Arnaud, A. L., Vasconcelos, G. C., Cunha, R. C. L. V., Gurgel, T. B., & Monteiro, D. S. M. P. (2009). The role of temporal feature extraction and bagging of MLP neural networks for solving the WCCI 2008 Ford Classification Challenge. 2009 International Joint Conference on Neural Networks. doi:10.1109/ijcnn.2009.5178965Alfaro-Cid, E., Merelo, J. J., de Vega, F. F., Esparcia-Alcázar, A. I., & Sharman, K. (2010). Bloat Control Operators and Diversity in Genetic Programming: A Comparative Study. Evolutionary Computation, 18(2), 305-332. doi:10.1162/evco.2010.18.2.18206Alfaro-Cid, E., Sharman, K., & Esparcia-Alcazar, A. I. (s. f.). Evolving a Learning Machine by Genetic Programming. 2006 IEEE International Conference on Evolutionary Computation. doi:10.1109/cec.2006.1688316Arenas, M. G., Collet, P., Eiben, A. E., Jelasity, M., Merelo, J. J., Paechter, B., … Schoenauer, M. (2002). A Framework for Distributed Evolutionary Algorithms. Lecture Notes in Computer Science, 665-675. doi:10.1007/3-540-45712-7_64Blankertz, B., Muller, K.-R., Curio, G., Vaughan, T. M., Schalk, G., Wolpaw, J. R., … Birbaumer, N. (2004). The BCI Competition 2003: Progress and Perspectives in Detection and Discrimination of EEG Single Trials. IEEE Transactions on Biomedical Engineering, 51(6), 1044-1051. doi:10.1109/tbme.2004.826692Borrelli, A., De Falco, I., Della Cioppa, A., Nicodemi, M., & Trautteur, G. (2006). Performance of genetic programming to extract the trend in noisy data series. Physica A: Statistical Mechanics and its Applications, 370(1), 104-108. doi:10.1016/j.physa.2006.04.025Eads, D. R., Hill, D., Davis, S., Perkins, S. J., Ma, J., Porter, R. B., & Theiler, J. P. (2002). Genetic Algorithms and Support Vector Machines for Time Series Classification. Applications and Science of Neural Networks, Fuzzy Systems, and Evolutionary Computation V. doi:10.1117/12.453526Eggermont, J., Eiben, A. E., & van Hemert, J. I. (1999). A Comparison of Genetic Programming Variants for Data Classification. Lecture Notes in Computer Science, 281-290. doi:10.1007/3-540-48412-4_24Holladay, K. L., & Robbins, K. A. (2007). Evolution of Signal Processing Algorithms using Vector Based Genetic Programming. 2007 15th International Conference on Digital Signal Processing. doi:10.1109/icdsp.2007.4288629Kaboudan, M. A. (2000). Computational Economics, 16(3), 207-236. doi:10.1023/a:1008768404046Kishore, J. K., Patnaik, L. M., Mani, V., & Agrawal, V. K. (2000). Application of genetic programming for multicategory pattern classification. IEEE Transactions on Evolutionary Computation, 4(3), 242-258. doi:10.1109/4235.873235Kishore, J. K., Patnaik, L. M., Mani, V., & Agrawal, V. K. (2001). Genetic programming based pattern classification with feature space partitioning. Information Sciences, 131(1-4), 65-86. doi:10.1016/s0020-0255(00)00081-5Langdon, W. B., McKay, R. I., & Spector, L. (2010). Genetic Programming. International Series in Operations Research & Management Science, 185-225. doi:10.1007/978-1-4419-1665-5_7Yi Liu, & Khoshgoftaar, T. (s. f.). Reducing overfitting in genetic programming models for software quality classification. Eighth IEEE International Symposium on High Assurance Systems Engineering, 2004. Proceedings. doi:10.1109/hase.2004.1281730Luke, S. (2000). Two fast tree-creation algorithms for genetic programming. IEEE Transactions on Evolutionary Computation, 4(3), 274-283. doi:10.1109/4235.873237Luke, S., & Panait, L. (2006). A Comparison of Bloat Control Methods for Genetic Programming. Evolutionary Computation, 14(3), 309-344. doi:10.1162/evco.2006.14.3.309Mensh, B. D., Werfel, J., & Seung, H. S. (2004). BCI Competition 2003—Data Set Ia: Combining Gamma-Band Power With Slow Cortical Potentials to Improve Single-Trial Classification of Electroencephalographic Signals. IEEE Transactions on Biomedical Engineering, 51(6), 1052-1056. doi:10.1109/tbme.2004.827081Muni, D. P., Pal, N. R., & Das, J. (2006). Genetic programming for simultaneous feature selection and classifier design. IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics), 36(1), 106-117. doi:10.1109/tsmcb.2005.854499Oltean, M., & Dioşan, L. (2009). An autonomous GP-based system for regression and classification problems. Applied Soft Computing, 9(1), 49-60. doi:10.1016/j.asoc.2008.03.008Otero, F. E. B., Silva, M. M. S., Freitas, A. A., & Nievola, J. C. (2003). Genetic Programming for Attribute Construction in Data Mining. Genetic Programming, 384-393. doi:10.1007/3-540-36599-0_36Poli, R. (2010). Genetic programming theory. Proceedings of the 12th annual conference comp on Genetic and evolutionary computation - GECCO  ’10. doi:10.1145/1830761.1830905Tsakonas, A. (2006). A comparison of classification accuracy of four genetic programming-evolved intelligent structures. Information Sciences, 176(6), 691-724. doi:10.1016/j.ins.2005.03.012Wolpaw, J. R., Birbaumer, N., Heetderks, W. J., McFarland, D. J., Peckham, P. H., Schalk, G., … Vaughan, T. M. (2000). Brain-computer interface technology: a review of the first international meeting. IEEE Transactions on Rehabilitation Engineering, 8(2), 164-173. doi:10.1109/tre.2000.84780",space,1147
,filtered,core,Adaptive multi-classifier systems for face re-identification applications,2015-01-26 00:00:00,core,https://core.ac.uk/download/pdf/33802013.pdf,École de technologie supérieure,"In video surveillance, decision support systems rely more and more on face recognition (FR) to rapidly determine if facial regions captured over a network of cameras correspond to individuals of interest. Systems for FR in video surveillance are applied in a range of scenarios, for instance in watchlist screening, face re-identification, and search and retrieval. The focus of this Thesis is video-to-video FR, as found in face re-identification applications, where facial models are designed on reference data, and update is archived on operational captures from video streams. Several challenges emerge from the task of recognizing individuals of interest from faces captured with video cameras. Most notably, it is often assumed that the facial appearance of target individuals do not change over time, and the proportions of faces captured for target and non-target individuals are balanced, known a priori and remain fixed. However, faces captured during operations vary due to several factors, including illumination, blur, resolution, pose expression,  and camera interoperability. In addition, facial models used matching are commonly not representative since they are designed a priori, with a limited amount of reference samples that are collected and labeled at a high cost. Finally, the proportions of target and non-target individuals continuously change during operations.



In literature, adaptive multiple classifier systems (MCSs) have been successfully applied to video-to-video FR, where  the facial model for each target individual is designed using an ensemble of 2-class classifiers (trained using target vs. non-target reference samples). Recent approaches employ ensembles of 2-class Fuzzy ARTMAP classifiers, with a DPSO strategy to generate a pool of classifiers with optimized hyperparameters, and Boolean combination to merge their responses in the ROC space. Besides, the skew-sensitive ensembles were recently proposed to adapt the fusion function of an ensemble according to class imbalance measured on operational data. These active approaches estimate target vs. non-target proportions periodically during operations distance, and the fusion of classifier ensembles are adapted to such imbalance. Finally, face tracking can be used to regroup the system responses linked to a facial trajectory (facial captures from a single person in the scene) for robust spatio-temporal recognition, and to update facial models over time using operational data.



In this Thesis, new techniques are proposed to adapt the facial models for individuals enrolled to a video-to-video FR system. Trajectory-based self-updating is proposed to update the system, considering gradual and abrupt changes in the classification environment. Then, skew-sensitive ensembles are proposed to adapt the system to the operational imbalance.



In Chapter 2, an adaptive framework is proposed for partially-supervised learning of facial models over time based on facial trajectories. During operations, information from a face tracker and individual-specific ensembles is integrated for robust spatio-temporal recognition and for self-update of facial models. The tracker defines a facial trajectory for each individual in video. Recognition of a target individual is done if the positive predictions accumulated along a trajectory surpass a detection threshold for an ensemble. If the accumulated positive predictions surpass a higher update threshold, then all target face samples from the trajectory are combined with non-target samples (selected from the cohort and universal models) to update the corresponding facial model. A learn-and-combine strategy is employed to avoid knowledge corruption during self-update of ensembles. In addition, a memory management strategy based on Kullback-Leibler divergence is proposed to rank and select the most relevant target and non-target reference samples to be stored in memory as the ensembles evolves. The proposed system was validated with synthetic data and real videos from Face in Action dataset, emulating a passport checking scenario. Initially, enrollment trajectories were used for supervised learning of ensembles, and videos from three capture sessions were presented to the system for FR and self-update.  Transaction-level analysis shows that the proposed approach outperforms baseline systems that do not adapt to new trajectories, and provides comparable performance to ideal systems that adapt to all relevant target trajectories, through supervised learning. Subject-level analysis reveals the existence of individuals for which self-updated ensembles provide a considerable benefit. Trajectory-level analysis indicates that the proposed system allows for robust spatio-temporal video-to-video FR.



In Chapter 3, an extension and a particular implementation of the ensemble-based system for spatio-temporal FR is proposed, and is characterized in scenarios with gradual and abrupt changes in the classification environment. Transaction-level results show that the proposed system allows to increase AUC accuracy by about 3% in scenarios with abrupt changes, and by about 5% in scenarios with gradual changes. Subject-based analysis reveals the difficulties of FR with different poses, affecting more significantly the lamb- and goat-like individuals. Compared to reference spatio-temporal fusion approaches, the proposed accumulation scheme produces the highest discrimination.



In Chapter 4, adaptive skew-sensitive ensembles are proposed to combine classifiers trained by selecting data with varying levels of imbalance and complexity, to sustain a high level the performance for video-to-video FR. During operations, the level of imbalance is periodically estimated from the input trajectories using the HDx quantification method, and pre-computed histogram representations of imbalanced data distributions. Ensemble scores are accumulated of trajectories for robust skew-sensitive spatio-temporal recognition. Results on synthetic data show that adapting the fusion function with the proposed approach can significantly improve performance. Results on real data show that the proposed method can outperform reference techniques in imbalanced video surveillance environments",space,1148
,filtered,core,New trends on soft computing models in industrial and environmental applications,2013-01-01 00:00:00,core,https://core.ac.uk/download/211485892.pdf,'Elsevier BV',"The twelve papers included in this special issue represent a selection of extended contributions presented at the Sixth International Conference on Soft Computing Models in Industrial and Environmental Applications, held in Salamanca, Spain, 6–8th April, 2011. Papers were selected on the basis of fundamental ideas and concepts rather than the direct usage of well-established techniques. This special issue is then aimed at practitioners, researchers and post-graduate students, who are engaged in developing and applying advanced Soft Computing Models to solving real-world problems in the Industrial and Environmental fields. The papers are organized as follows. In the first contribution, Graña and Gonzalez-Acuña develop a formulation of dendritic classifiers based on lattice kernels and train them using a direct Monte Carlo approach and a Sparse Bayesian Learning. The results of both kinds of training are compared with the Relevance Vector Machines on a collection of benchmark datasets. In the second contribution by Irigoyen and Miñano, the authors present the results of the identification of the relationship in time, between the required exercise (machine resistance) and the heart rate of the patient in medical effort tests, using a NARX neural network model. In the experimental stage, test data have been obtained by exercising with a cyclo-ergometer in two different tests: Power Step Response and Conconi. Carneiro et al. in the third contribution present a biologically inspired method to deal with the problem in which genetic algorithms are used to create possible solutions for a given dispute. The approach presented is able to generate a broad number of diverse solutions that cover virtually the whole search space for a given problem. The results of this work are being applied in a negotiation tool that is part of the UMCourt conflict resolution platform. In the fourth contribution by Donate et al., they propose a novel Evolutionary Artificial Neural Networks (EANN) approach, where a weighted n-fold validation fitness scheme is used to build an ensemble of neural networks, under four different combination methods: mean, median, softmax and rank-based combinations. Several experiments were held, using six real-world time series with different characteristics and from distinct domains. Overall, the proposed approach achieved competitive results when compared with non weighted n-fold EANN ensembles, the simpler 0-fold EANN and also the popular Holt–Winters statistical method. Dan Burdescu et al. in the fifth contribution, present a system used in the medical domain for three distinct tasks: image annotation, semantic based image retrieval and content based image retrieval. An original image segmentation algorithm based on a hexagonal structure was used to perform the segmentation of medical images. Image's regions are described using a vocabulary of blobs generated from image features using the K-means clustering algorithm. The annotation and semantic based retrieval task is evaluated for two annotation models: Cross Media Relevance Model and Continuous-space Relevance Model. Semantic based image retrieval is performed using the methods provided by the annotation models. The ontology used by the annotation process was created in an original manner starting from the information content provided by the Medical Subject Headings (MeSH). The experiments were made using a database containing colour images retrieved from medical domain using an endoscope and related to digestive diseases. In sixth paper by Pedraza et al., they develop a face recognition system based on soft computing techniques, which complies with privacy-by-design rules and defines a set of principles that context-aware applications (including biometric sensors) should contain to conform to European and US law. This research deals with the necessity to consider legal issues concerning privacy or human rights in the development of biometric identification in ambient intelligence systems. Clearly, context-based services and ambient intelligence (and the most promising research area in Europe, namely ambient assisted living, ALL) call for a major research effort on new identification procedures. The aim of the research by Redel-Macías et al. in paper seven is to develop a novel model which can be used in pass-by noise test in vehicles based on ensembles of hybrid Evolutionary Product Unit or Radial Basis function Neural Networks (EPUNN or ERBFNNs) at high frequencies. Statistical models and ensembles of hybrid EPUNN and ERBFNN approaches have been used to develop different noise identification models. The results obtained using different ensembles of hybrid EPUNNs and ERBFNNs show that the functional model and the hybrid algorithms proposed provide a very accurate identification compared to other statistical methodologies used to solve this regression problem. In the eighth paper, Wu et al. analyse the existence criterion of loop strategies, and then present some corollaries and theorems, by which the loop strategies and chain strategies can be found, also superfluous strategies and inconsistent strategies. It presents a ranking model that indicates the weak node in strategy set and it also introduces a probability-based model which is the basis of evaluation of strategy. Additionally, this research proposes a method to generate offensive strategy, and the statistic results of simulation game prove the validity of the method. Pop et al. in the ninth paper present an efficient hybrid heuristic algorithm obtained by combining a genetic algorithm (GA) with a local–global approach to the generalized vehicle routing problem (GVRP) and a powerful local search procedure. The computational experiments on several benchmarks instances show that the hybrid algorithm is competitive to all of the known heuristics published to date. In the tenth paper Kramer et al. illustrate how methods from neural computation can serve as forecasting, and monitoring techniques, contributing to a successful integration of wind into sustainable, and smart energy grids. The study is based on the application of kernel methods like support vector regression and kernel density estimation as prediction methods. Furthermore, dimension reduction techniques like self-organizing maps for monitoring of high-dimensional wind time series are applied. The methods are briefly introduced, related work is presented, and experimental case studies are exemplarily described. The experimental parts are based on real wind energy time series data from the NREL western wind resource dataset. Vera et al. in the eleventh contribution present a novel soft computing procedure based on the application of artificial neural networks, genetic algorithms and identification systems, which makes it possible to optimise the implementation conditions in the manufacturing process of high precision parts, including finishing precision, while saving both time and financial costs and/or energy. The novel proposed approach was tested under real dental milling processes using a high precision machining centre with five axes, requiring high finishing precision of measures in micrometres with a large number of process factors to analyse. The results of the experiment, which validate the performance of the proposed approach, are presented in this study. The final contribution, by Sakalauskas and Kriksciuniene, presents a research about financial market efficiency and to recognize major reversal points of long-term trend of stock market index, which could indicate forthcoming crisis or market raise periods. The study suggests a computational model of financial time series analysis, which combines several approaches of soft computing, including information efficiency evaluation methods (Shannon's entropy, Hurst exponent), neural networks and sensitivity analysis. The model aims to derive the aggregated measure for evaluating efficiency of the financial market and to find its interrelationships with the reversal of long-term trend. The radial basis function neural network was designed for forecasting moments of cardinal changes in stock market behaviour, expressed by its entropy values derived from the symbolized time series of stock market index. The performance of neural network model is explored by applying sensitivity analysis and resulted in selecting smoothing parameters of the input variables. The experimental research investigates behaviour of the long-term trend of the three emerging financial markets within NASDAQ OMX Baltic stock exchange. Introduction of information efficiency measures improve ability of the model to recognize the approaching reversal of long-term trend from temporary market “nervousness” and can be useful for calibrating stock trading strategy. First, we would like to thank all the authors for their valuable contributions, which made this special issue possible. We also like to thank our peer-reviewers for their timely diligent work and efficient efforts. We are also grateful to the Editor-in-Chief of Neurocomputing Journal, Prof. Tom Heskes, for his continued support for the SOCO series of conferences and for this Special Issue on this prestigious journal. Finally, we hope the reader will share our joy and find this special issue very useful",space,1149
,filtered,core,Automated Real-Time Classification and Decision Making in Massive Data Streams from Synoptic Sky Surveys,2014-10-01 00:00:00,core,https://core.ac.uk/download/216204075.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',"The nature of scientific and technological data collection is evolving rapidly: data volumes and rates grow exponentially, with increasing complexity and information content, and there has been a transition from static data sets to data streams that must be analyzed in real time. Interesting or anomalous phenomena must be quickly characterized and followed up with additional measurements via optimal deployment of limited assets. Modern astronomy presents a variety of such phenomena in the form of transient events in digital synoptic sky surveys, including cosmic explosions (supernovae, gamma ray bursts), relativistic phenomena (black hole formation, jets), potentially hazardous asteroids, etc. We have been developing a set of machine learning tools to detect, classify and plan a response to transient events for astronomy applications, using the Catalina Real-time Transient Survey (CRTS) as a scientific and methodological testbed. The ability to respond rapidly to the potentially most interesting events is a key bottleneck that limits the scientific returns from the current and anticipated synoptic sky surveys. Similar challenge arise in other contexts, from environmental monitoring using sensor networks to autonomous spacecraft systems. Given the exponential growth of data rates, and the time-critical response, we need a fully automated and robust approach. We describe the results obtained to date, and the possible future developments",space,1150
,filtered,core,Multicore and FPGA implementations of emotional-based agent architectures,2015-02-01 00:00:00,core,https://riunet.upv.es/bitstream/handle/10251/85419/dom%c3%adnguez%2c%20c.%20et%20al.%20-%20multicore%20and%20fpga%20implementations%20of%20emotional-based....pdf?sequence=2&isallowed=y,'Springer Science and Business Media LLC',"The final publication is available at Springer via http://dx.doi.org/10.1007/s11227-014-1307-6.Control architectures based on Emotions are becoming promising solutions for the implementation of future robotic agents. The basic controllers of the architecture are the emotional processes that decide which behaviors of the robot must activate to fulfill the objectives. The number of emotional processes increases (hundreds of millions/s) with the complexity level of the application, reducing the processing capacity of the main processor to solve complex problems (millions of decisions in a given instant). However, the potential parallelism of the emotional processes permits their execution in parallel on FPGAs or Multicores, thus enabling slack computing in the main processor to tackle more complex dynamic problems. In this paper, an emotional architecture for mobile robotic agents is presented. The workload of the emotional processes is evaluated. Then, the main processor is extended with FPGA co-processors through Ethernet link. The FPGAs will be in charge of the execution of the emotional processes in parallel. Different Stratix FPGAs are compared to analyze their suitability to cope with the proposed mobile robotic agent applications. The applications are set up taking into account different environmental conditions, robot dynamics and emotional states. Moreover, the applications are run also on Multicore processors to compare their performance in relation to the FPGAs. Experimental results show that Stratix IV FPGA increases the performance in about one order of magnitude over the main processor and solves all the considered problems. Quad-Core increases the performance in 3.64 times, allowing to tackle about 89 % of the considered problems. Quad-Core has a lower cost than a Stratix IV, so more adequate solution but not for the most complex application. Stratix III could be applied to solve problems with around the double of the requirements that the main processor could support. Finally, a Dual-Core provides slightly better performance than stratix III and it is relatively cheaper.This work was supported in part under Spanish Grant PAID/2012/325 of ""Programa de Apoyo a la Investigacion y Desarrollo. Proyectos multidisciplinares"", Universitat Politecnica de Valencia, Spain.Domínguez Montagud, CP.; Hassan Mohamed, H.; Crespo, A.; Albaladejo Meroño, J. (2015). Multicore and FPGA implementations of emotional-based agent architectures. Journal of Supercomputing. 71(2):479-507. doi:10.1007/s11227-014-1307-6S479507712Malfaz M, Salichs MA (2010) Using MUDs as an experimental platform for testing a decision making system for self-motivated autonomous agents. Artif Intell Simul Behav J 2(1):21–44Damiano L, Cañamero L (2010) Constructing emotions. Epistemological groundings and applications in robotics for a synthetic approach to emotions. In: Proceedings of international symposium on aI-inspired biology, The Society for the Study of Artificial Intelligence, pp 20–28Hawes N, Wyatt J, Sloman A (2009) Exploring design space for an integrated intelligent system. Knowl Based Syst 22(7):509–515Sloman A (2009) Some requirements for human-like robots: why the recent over-emphasis on embodiment has held up progress. Creat Brain Like Intell 2009:248–277Arkin RC, Ulam P, Wagner AR (2012) Moral decision-making in autonomous systems: enforcement, moral emotions, dignity, trust and deception. In: Proceedings of the IEEE, Mar 2012, vol 100, no 3, pp 571–589iRobot industrial robots website. http://www.irobot.com/gi/ground/ . Accessed 22 Sept 2014Moravec H (2009) Rise of the robots: the future of artificial intelligence. Scientific American, March 2009. http://www.scientificamerican.com/article/rise-of-the-robots/ . Accessed 14 Oct 2014.Thu Bui L, Abbass HA, Barlow M, Bender A (2012) Robustness against the decision-maker’s attitude to risk in problems with conflicting objectives. IEEE Trans Evolut Comput 16(1):1–19Pedrycz W, Song M (2011) Analytic hierarchy process (AHP) in group decision making and its optimization with an allocation of information granularity. IEEE Trans Fuzzy Syst 19(3):527–539Lee-Johnson CP, Carnegie DA (2010) Mobile robot navigation modulated by artificial emotions. IEEE Trans Syst Man Cybern Part B 40(2):469–480Daglarli E, Temeltas H, Yesiloglu M (2009) Behavioral task processing for cognitive robots using artificial emotions. Neurocomputing 72(13):2835–2844Ventura R, Pinto-Ferreira C (2009) Responding efficiently to relevant stimuli using an emotion-based agent architecture. Neurocomputing 72(13):2923–2930Arkin RC, Ulam P, Wagner AR (2012) Moral decision-making in autonomous systems: enforcement, moral emotions, dignity, trust and deception. Proc IEEE 100(3):571–589Salichs MA, Malfaz M (2012) A new approach to modeling emotions and their use on a decision-making system for artificial agents. Affect Comput IEEE Trans 3(1):56–68Altera Corporation (2011) Stratix III device handbook, vol 1–2, version 2.2. http://www.altera.com/literature/lit-stx3.jsp . Accessed 14 Oct 2014.Altera Corporation (2014) Stratix IV device handbook, vol 1–4, version 5.9. http://www.altera.com/literature/lit-stratix-iv.jsp . Accessed 14 Oct 2014.Naouar MW, Monmasson E, Naassani AA, Slama-Belkhodja I, Patin N (2007) FPGA-based current controllers for AC machine drives: a review. IEEE Trans Ind Electr 54(4):1907–1925Intel Corporation (2014) Desktop 4th generation Intel Core Processor Family, Desktop Intel Pentium Processor Family, and Desktop Intel Celeron Processor Family, Datasheet, vol 1, 2March JL, Sahuquillo J, Hassan H, Petit S, Duato J (2011) A new energy-aware dynamic task set partitioning algorithm for soft and hard embedded real-time systems. Comput J 54(8):1282–1294Del Campo I, Basterretxea K, Echanobe J, Bosque G, Doctor F (2012) A system-on-chip development of a neuro-fuzzy embedded agent for ambient-intelligence environments. IEEE Trans Syst Man Cybern Part B 42(2):501–512Pedraza C, Castillo J, Martínez JI, Huerta P, Bosque JL, Cano J (2011) Genetic algorithm for Boolean minimization in an FPGA cluster. J Supercomput 58(2):244–252Orlowska-Kowalska T, Kaminski M (2011) FPGA implementation of the multilayer neural network for the speed estimation of the two-mass drive system. IEEE Trans Ind Inf 7(3):436–445Cassidy AS, Merolla P, Arthur JV, Esser SK, Jackson B, Alvarez-icaza R, Datta P, Sawada J, Wong TM, Feldman V, Amir A, Ben-dayan D, Mcquinn E, Risk WP, Modha DS (2013) Cognitive computing building block: a versatile and efficient digital neuron model for neurosynaptic cores. In: Proceedings of international joint conference on neural networks, IEEE (IJCNN’2013)IBM Cognitive Computing and Neurosynaptic chips website. http://www.research.ibm.com/cognitive-computing/neurosynaptic-chips.shtml . Accessed 22 Sept 2014Seo E, Jeong J, Park S, Lee J (2008) Energy efficient scheduling of real-time tasks on multicore processors. IEEE Trans Parallel Distrib Syst 19(11):1540–1552Lehoczky J, Sha L, Ding Y (1989) The rate monotonic scheduling algorithm: exact characterization and average case behavior. In: Proceedings of real time systems symposium, IEEE 1989, pp 166–171Ng-Thow-Hing V, Lim J, Wormer J, Sarvadevabhatla RK, Rocha C, Fujimura K, Sakagami Y (2008) The memory game: creating a human-robot interactive scenario for ASIMO. In: Proceedings of intelligent robots and systems, 2008, IROS 2008, IEEE/RSJ international conference, pp 779–78",space,1151
,filtered,core,Automated Real-Time Classification and Decision Making in Massive Data Streams from Synoptic Sky Surveys,2014-10-01 00:00:00,core,https://core.ac.uk/download/379197212.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',"The nature of scientific and technological data collection is evolving rapidly: data volumes and rates grow exponentially, with increasing complexity and information content, and there has been a transition from static data sets to data streams that must be analyzed in real time. Interesting or anomalous phenomena must be quickly characterized and followed up with additional measurements via optimal deployment of limited assets. Modern astronomy presents a variety of such phenomena in the form of transient events in digital synoptic sky surveys, including cosmic explosions (supernovae, gamma ray bursts), relativistic phenomena (black hole formation, jets), potentially hazardous asteroids, etc. We have been developing a set of machine learning tools to detect, classify and plan a response to transient events for astronomy applications, using the Catalina Real-time Transient Survey (CRTS) as a scientific and methodological testbed. The ability to respond rapidly to the potentially most interesting events is a key bottleneck that limits the scientific returns from the current and anticipated synoptic sky surveys. Similar challenge arise in other contexts, from environmental monitoring using sensor networks to autonomous spacecraft systems. Given the exponential growth of data rates, and the time-critical response, we need a fully automated and robust approach. We describe the results obtained to date, and the possible future developments",space,1152
,filtered,core,"Automated Real-Time Classification and Decision Making in Massive Data
  Streams from Synoptic Sky Surveys",2014-07-13 00:00:00,core,http://arxiv.org/abs/1407.3502,'Institute of Electrical and Electronics Engineers (IEEE)',"The nature of scientific and technological data collection is evolving
rapidly: data volumes and rates grow exponentially, with increasing complexity
and information content, and there has been a transition from static data sets
to data streams that must be analyzed in real time. Interesting or anomalous
phenomena must be quickly characterized and followed up with additional
measurements via optimal deployment of limited assets. Modern astronomy
presents a variety of such phenomena in the form of transient events in digital
synoptic sky surveys, including cosmic explosions (supernovae, gamma ray
bursts), relativistic phenomena (black hole formation, jets), potentially
hazardous asteroids, etc. We have been developing a set of machine learning
tools to detect, classify and plan a response to transient events for astronomy
applications, using the Catalina Real-time Transient Survey (CRTS) as a
scientific and methodological testbed. The ability to respond rapidly to the
potentially most interesting events is a key bottleneck that limits the
scientific returns from the current and anticipated synoptic sky surveys.
Similar challenge arise in other contexts, from environmental monitoring using
sensor networks to autonomous spacecraft systems. Given the exponential growth
of data rates, and the time-critical response, we need a fully automated and
robust approach. We describe the results obtained to date, and the possible
future developments.Comment: 8 pages, IEEE conference format, to appear in the refereed
  proceedings of the IEEE e-Science 2014 conf., eds. C. Medeiros et al., IEEE,
  in press (2014). arXiv admin note: substantial text overlap with
  arXiv:1209.1681, arXiv:1110.465",space,1153
,filtered,core,A new tracking approach for multipath mitigation based on antenna array,2011-01-01 00:00:00,core,https://core.ac.uk/download/12042846.pdf,,"In Global Navigation Satellites Systems (GNSS), multipaths (MP) are still one of the major error sources. The additional signal replica due to reflection will introduce a bias in conventional Delay Lock Loops (DLL) which will finally cause a strong positioning error. Several techniques, based on Maximum Likelihood estimation (ML), have been developed for multipaths mitigation/estimation such as the Narrow correlator spacing [1] or the Multipath Estimating Delay-Lock-Loop  (MEDLL) [2] algorithm. These techniques try to discriminate the MP from the Line Of Sight Signal (LOSS) on the time and frequency domains and thus, short delay multipaths (<0.1Chips) can not be completely mitigated. Antenna array perform a spatial sampling of the wave front what makes possible the discrimination of the sources on the space domain (azimuth and elevation). As the time-delay domain and space domain can be assumed independent, we can expect to mitigate/estimate very short delay MP by using an antenna array. However, we don't want to increase too much the size, the complexity and the cost of the receivers and thus, we focus our study on small arrays with a small number of antennas: typically a square 2x2 array. Consequently, conventional beamforming (space Fast Fourier Transform) is not directive enough to assure the mitigation of the multipaths, and then this first class of solutions was rejected. In order to improve the resolution, adaptive beamformers have also been tested. However, the LOSS and the MP signal are strongly correlated and thus, classical adaptive algorithms [3] are not able to discriminate the sources. These preliminary studies have shown that the mitigation/estimation of multipaths based on the space domain will exhibit limited performances in presence of close sources. Then, in order to propose robust algorithms, we decided to investigate a space-time-frequency estimation of the sources. Space Alternating Generalized Expectation maximisation (SAGE) algorithm [4], which is a low-complexity generalization of the Expectation Maximisation (EM) algorithm, has been considered. The basic concept of the SAGE algorithm is the hidden data space [4]. Instead of estimating the parameters of all impinging waves in parallel in one iteration step as done by the EM algorithm, the SAGE algorithm estimates the parameters of each signal sequentially. Moreover, SAGE algorithm breaks down the multi-dimensional optimization problem into several smaller problems. In [5], it can be seen that SAGE algorithm is efficient for any multipaths configurations (small relative delays, close DOAs) and space-time-frequency approach is clearly outperforming classical time-frequency approaches. Notwithstanding, SAGE algorithm is a post processing algorithm. Thus, it's necessary to memorise in the receiver the incoming signal in order to apply SAGE estimation. For example, if we want to process 10ms of signal with a 10MHz sampling rate, we need to store a matrix of m*105 with m the number of antennas. In such condition, we can understand than SAGE algorithm is hardly implemented in real time. The challenge is then to find a new type of algorithms that reach the efficiency of the SAGE algorithms, but with a reduced complexity in order to enable real time processing.

Furthermore, the implementation should be compatible with conventional GNSS tracking loops (DLL and PLL). To cope with these two constraints, we propose to apply the SAGE algorithm on the post-correlated signal. Indeed, the correlation step can be seen as a compression step and thus, the size of the studied signal is strongly reduced. In such a way, SAGE algorithm is able to provide estimates of the relative delay and Doppler of the received signals with respect to the local replicas. Thus, a post correlation implementation of SAGE can be seen as a discriminator for both the DLL and the PLL",space,1154
,filtered,core,Data mining based learning algorithms for semi-supervised object identification and tracking,2011-01-01 00:00:00,core,https://core.ac.uk/download/236621381.pdf,Louisiana Tech Digital Commons,"Sensor exploitation (SE) is the crucial step in surveillance applications such as airport security and search and rescue operations. It allows localization and identification of movement in urban settings and can significantly boost knowledge gathering, interpretation and action. Data mining techniques offer the promise of precise and accurate knowledge acquisition techniques in high-dimensional data domains (and diminishing the “curse of dimensionality” prevalent in such datasets), coupled by algorithmic design in feature extraction, discriminative ranking, feature fusion and supervised learning (classification). Consequently, data mining techniques and algorithms can be used to refine and process captured data and to detect, recognize, classify, and track objects with predictable high degrees of specificity and sensitivity.
Automatic object detection and tracking algorithms face several obstacles, such as large and incomplete datasets, ill-defined regions of interest (ROIs), variable scalability, lack of compactness, angular regions, partial occlusions, environmental variables, and unknown potential object classes, which work against their ability to achieve accurate real-time results. Methods must produce fast and accurate results by streamlining image processing, data compression and reduction, feature extraction, classification, and tracking algorithms. Data mining techniques can sufficiently address these challenges by implementing efficient and accurate dimensionality reduction with feature extraction to refine incomplete (ill-partitioning) data-space and addressing challenges related to object classification, intra-class variability, and inter-class dependencies.
A series of methods have been developed to combat many of the challenges for the purpose of creating a sensor exploitation and tracking framework for real time image sensor inputs. The framework has been broken down into a series of sub-routines, which work in both series and parallel to accomplish tasks such as image pre-processing, data reduction, segmentation, object detection, tracking, and classification. These methods can be implemented either independently or together to form a synergistic solution to object detection and tracking.
The main contributions to the SE field include novel feature extraction methods for highly discriminative object detection, classification, and tracking. Also, a new supervised classification scheme is presented for detecting objects in urban environments. This scheme incorporates both novel features and non-maximal suppression to reduce false alarms, which can be abundant in cluttered environments such as cities. Lastly, a performance evaluation of Graphical Processing Unit (GPU) implementations of the subtask algorithms is presented, which provides insight into speed-up gains throughout the SE framework to improve design for real time applications.
The overall framework provides a comprehensive SE system, which can be tailored for integration into a layered sensing scheme to provide the war fighter with automated assistance and support. As more sensor technology and integration continues to advance, this SE framework can provide faster and more accurate decision support for both intelligence and civilian applications",space,1155
,filtered,core,Installation of a very broad band borehole seismic station in Ferrara (Emilia),2012-11-21 00:00:00,core,https://core.ac.uk/download/pdf/41152694.pdf,OGS,"The Istituto Nazionale di Geofisica e Vulcanologia (INGV) is the Italian agency devoted to monitor in real time the seismicity on the Italian territory.  The seismicity in Italy is of course variable in time and space, being also very much dependant on local noise conditions. Specifically, monitoring seismicity in an alluvial basin like the Po one is a challenge, due to consistent site effects induced by soft alluvial deposits and bad coupling with the deep bedrock (Steidl et al., 1996). This problem was tackled by INGV first with the Cavola experiment (Bordoni et al., 2007), where a landslide was seismically characterized using a seismic array and also down-hole logging of P- and S-wave travel times at a borehole drilled within the array; later, with an ad hoc project in 2000-2001, with the first installation of a broad band seismic station nearby Ferrara in a borehole of 135 meters depth. Comparison of recordings with a surface seismic station indicated a noise reduction of 2 decades in power spectral density at frequencies larger than 1.0 Hz (Cocco et al., 2001). The instrumentation in Ferrara has been working for several months but after that the seismic station was discontinued due to lack of maintenance manpower.
The Centro di Ricerche Sismologiche (CRS, Seismological Research Center) of the Istituto Nazionale di Oceanografia e di Geofisica Sperimentale (OGS, Italian National Institute for Oceanography and Experimental Geophysics) in Udine (Italy) after the strong earthquake of magnitude M=6.4 occurred in 1976 in the Italian Friuli-Venezia Giulia region, started to operate the Northeastern Italy (NI) Seismic Network: it currently consists of 15 very sensitive broad band and 21 simpler short period seismic stations, all telemetered to and acquired in real time at the OGS-CRS data center in Udine (Fig. 1).
Real time data exchange agreements in place with other Italian, Slovenian, Austrian and Swiss seismological institutes lead to a total number of about 100 seismic stations acquired in real time, which makes the OGS the reference institute for seismic monitoring of Northeastern Italy. Since 2002 OGS-CRS is using the Antelope software suite on several workstations plus a SUN cluster as the main tool for collecting, analyzing, archiving and exchanging seismic data, initially in the framework of the EU Interreg IIIA project “Trans-national seismological networks in the South-Eastern Alps”. SeisComP is also used as a real time data exchange server tool (Bragato et al., 2011).
Among the various Italian institution with which OGS is cooperating for real time monitoring of local seismicity there is the Regione Veneto (Barnaba et al., 2012). The Southern part of the Veneto Region stands on the Po alluvial basin: earthquake localization and characterization is here again affected in this area by the presence of soft alluvial deposits. OGS ha already experience in running a local seismic network in difficult noise conditions making use of borehole installations (Priolo et al., 2012) in the case of the monitoring of a local storage site for the Italian national electricity company ENEL. Following the ML=5.9 earthquake that struck the Emilia region around Ferrara in Northern Italy on May 20, 2012 at 02:03:53 UTC, a cooperation of INGV, OGS, the Comune di Ferrara and the University of Ferrara lead to the reinstallation of the very broad band borehole seismic station in Ferrara. The aim of the OGS intervention was on one hand to extend its real time seismic monitoring capabilities toward South-East (Fig. 1), including Ferrara and its surroundings, and on the other hand to evaluate the seismic  response at the site.
As concerns the superficial geology of the area where the borehole seismic station  has been installed, the outcropping materials are represented by alluvial deposits of different environments, like channel and proximal levee, inter-fluvial, meander and swamps deposits. As  a consequence, the outcropping deposits are everywhere Holocene in age substantially loose or poorly compacted in the first meters-decameters and granulometrically could vary from clay to coarse sand.
Two preliminary reports prepared by the Italian Department of Civil Defense (Dipartimento Nazionale di Protezione Civile) in collaboration with other institutions describe the data  recorded by the national accelerometric network and complemented by additional data recorded by a number of temporary stations (Dolce et al., 2012a; Dolce et al., 2012b). These reports bear witness of strong ground motion values with an acceleration peak of about 0.9 g in the vertical component recorded during the ML=5.8 earthquake of May 29, 2012 by the Mirandola station, located at about 2 km from the epicentre. The analysis of the seismic noise recorded at some stations shows a quite pronounced peak of the horizontal-to-vertical spectral ratio (H/V) in the frequency range of 0.6 – 0.9 Hz common to all stations. Finally, strong evidence of liquefaction phenomena are reported at several sites (e.g.: S. Carlo, S. Agostino and Mirabello), most of which have been attributed to the occurrence of saturated sandy layer(s) at shallow depth deposited along an abandoned reach of the Reno River (Papathanassiou et al., 2012).
Details of the station configuration and installation will be outlined, with first results",space,1156
,filtered,core,Restricting Supervised Learning: Feature Selection and Feature Space Partition,2012-01-01 00:00:00,core,https://core.ac.uk/download/288062662.pdf,eGrove,"Many supervised learning problems are considered difficult to solve either because of the redundant features or because of the structural complexity of the generative function. Redundant features increase the learning noise and therefore decrease the prediction performance. Additionally, a number of problems in various applications such as bioinformatics or image processing, whose data are sampled in a high dimensional space, suffer the curse of dimensionality, and there are not enough observations to obtain good estimates. Therefore, it is necessary to reduce such features under consideration. Another issue of supervised learning is caused by the complexity of an unknown generative model. To obtain a low variance predictor, linear or other simple functions are normally suggested, but they usually result in high bias. Hence, a possible solution is to partition the feature space into multiple non-overlapping regions such that each region is simple enough to be classified easily.  In this dissertation, we proposed several novel techniques for restricting supervised learning problems with respect to either feature selection or feature space partition. Among different feature selection methods, 1-norm regularization is advocated by many researchers because it incorporates feature selection as part of the learning process. We give special focus here on ranking problems because very little work has been done for ranking using L1 penalty.  We present here a 1-norm support vector machine method to simultaneously find a linear ranking function and to perform feature subset selection in ranking problems. Additionally, because ranking is formulated as a classification task when pair-wise data are considered, it increases the computational complexity from linear to quadratic in terms of sample size. We also propose a convex hull reduction method to reduce this impact. The method was tested on one artificial data set and two benchmark real data sets, concrete compressive strength set and Abalone data set. Theoretically, by tuning the trade-off parameter between the 1-norm penalty and the empirical error, any desired size of feature subset could be achieved, but computing the whole solution path in terms of the trade-off parameter is extremely difficult. Therefore, using 1-norm regularization alone may not end up with a feature subset of small size. We propose a recursive feature selection method based on 1-norm regularization which can handle the multi-class setting effectively and efficiently. The selection is performed iteratively. In each iteration, a linear multi-class classifier is trained using 1-norm regularization, which leads to sparse weight vectors, i.e., many feature weights are exactly zero. Those zero-weight features are eliminated in the next iteration. The selection process has a fast rate of convergence. We tested our method on an earthworm microarray data set and the empirical results demonstrate that the selected features (genes) have very competitive discriminative power. Feature space partition separates a complex learning problem into multiple non-overlapping simple sub-problems. It is normally implemented in a hierarchical fashion. Different from decision tree, a leaf node of this hierarchical structure does not represent a single decision, but represents a region (sub-problem) that is solvable with respect to linear functions or other simple functions. In our work, we incorporate domain knowledge in the feature space partition process.  We consider domain information encoded by discrete or categorical attributes. A discrete or categorical attribute provides a natural partition of the problem domain, and hence divides the original problem into several non-overlapping sub-problems. In this sense, the domain information is useful if the partition simplifies the learning task. However it is not trivial to select the  discrete or categorical attribute that maximally simplify the learning task. A naive approach exhaustively searches all the possible restructured problems. It is computationally prohibitive when the number of discrete or categorical attributes is large. We describe a metric to rank attributes according to their potential to reduce the uncertainty of a classification task. It is quantified as a conditional entropy achieved using a set of optimal classifiers, each of which is built for a sub-problem defined by the attribute under consideration. To avoid high computational cost, we approximate the solution by the expected minimum conditional entropy with respect to random projections. This approach was tested on three artificial data sets, three cheminformatics data sets, and two leukemia gene expression data sets. Empirical results demonstrate that our method is capable of selecting a proper discrete or categorical attribute to simplify the problem, i.e., the performance of the classifier built for the restructured problem always beats that of the original problem. Restricting supervised learning is always about building simple learning functions using a limited number of features. Top Selected Pair (TSP) method builds simple classifiers based on very few (for example, two) features with simple arithmetic calculation. However, traditional TSP method only deals with static data. In this dissertation, we propose classification methods for time series data that only depend on a few pairs of features. Based on the different comparison strategies, we developed the following approaches: TSP based on average, TSP based on trend, and TSP based on trend and absolute difference amount. In addition, inspired by the idea of using two features, we propose a time series classification method based on few feature pairs using dynamic time warping and nearest neighbor",space,1157
,filtered,core,Recursive learning of image parameters and restoration of images using EM based learning algorithm,2011-05-02 00:00:00,core,https://core.ac.uk/download/230195551.pdf,İTÜDERGİSİ/d,"Bir&ccedil;ok klasik g&ouml;r&uuml;nt&uuml; onarım tekniği bulanıklık işlevinin bilindiği varsayımı altında &ccedil;alışır. Ancak, ger&ccedil;ek hayat problemlerinde sadece g&ouml;zlem verisi elde edilebilmekte bozucu sistemler hakkında yeterli bilgi sağlanamamaktadır. Bu y&uuml;zden g&ouml;r&uuml;nt&uuml; onarımının ilk adımı bozucunun &ouml;ğrenilmesi (tanınması) işlemidir. Ge&ccedil;mişte, g&ouml;r&uuml;nt&uuml; ve bulanıklık parametrelerinin &ouml;ğrenilmesi Enb&uuml;y&uuml;k Olabilirlik (EO) problemi olarak ele alınmış ve Beklenti Enb&uuml;y&uuml;kleme (BE) yordamı ile &ccedil;&ouml;z&uuml;lm&uuml;şt&uuml;r. &Ouml;zellikle BE yordamının E adımında kapalı yapıda bir &ccedil;&ouml;z&uuml;m bulunması bu yordamı daha cazip bir hale getirmektedir. G&ouml;r&uuml;nt&uuml; ve bulanıklık parametrelerinin t&uuml;m g&ouml;r&uuml;nt&uuml; verisi kullanılarak &ouml;ğrenilmesi ge&ccedil;mişte &ccedil;alışılmış olmakla birlikte, parametrelerin yinelemeli BE&rsquo;ye dayalı &ouml;ğrenilmesi daha &ouml;nce &ccedil;alışılmamıştır. Yinelemeli teknikler dinamik işlem yetenekleri sayesinde t&uuml;m veri &uuml;zerinde işlem yapan y&ouml;ntemlere nispetle &ccedil;ok daha az bellek ihtiyacı duyarlar. Daha az bellek ihtiyacı ise &ouml;zellikle g&ouml;r&uuml;nt&uuml; işleme alanında &ccedil;ok &ouml;nemlidir. Bu &ccedil;alışmada yeni bir eşzamanlı yinelemeli parametre &ouml;ğrenme ve g&ouml;r&uuml;nt&uuml; onarım y&ouml;ntemi sunulmuştur. Dinamik Bayes&ccedil;i Ağ (DBA) yapısında yeni bir &ccedil;&ouml;z&uuml;m &ouml;nerilmiştir. Sunulan y&ouml;ntem EO parametre tanıma ve durum kestirimi i&ccedil;in en iyi Kalman yumuşatma ifadelerini i&ccedil;erir. Kalman yumuşatma ifadelerinin yoğun hesaplama gerektirmesi sebebi ile Kalman s&uuml;zge&ccedil; yaklaşıklığı kullanılmıştır. Aynı zamanda, onarılmış g&ouml;r&uuml;nt&uuml; eş zamanlı olarak bu s&uuml;zge&ccedil; &ccedil;ıkışından elde edilmektedir. G&ouml;r&uuml;nt&uuml; ve bulanıklık parametrelerinin BE &ouml;ğrenme problemi kapalı yapıda &ccedil;&ouml;z&uuml;mlenmesi başarılmıştır.&nbsp; Y&ouml;ntemin başarımı ger&ccedil;ek g&ouml;r&uuml;nt&uuml;ler &uuml;zerinde yapılan benzetim ve denemeler ile verilmiştir. &nbsp;Anahtar Kelimeler: Beklenti enb&uuml;y&uuml;kleme, bulanıklık ve g&ouml;r&uuml;nt&uuml; tanıma, yinelemeli işleme, kalman yumuşatma ve s&uuml;zge&ccedil;leme.The image restoration problem can be defined as the general problem of estimating the ideal image from its blurred and noisy version. Many classical image restoration techniques have been reported under the assumption that the blur operation is exactly known. In real life applications, the corruption mechanism of any system is not known because only observed data is available, so it is necessary to handle uncertain events and observations. The image restoration problem is in general ill-posed; a small perturbation on the given data produces large deviations in the solution. The direct inversion of the blur transfer function usually has a large magnitude at high frequencies, therefore excessive amplification of noise results at those frequencies. Clearly, this is not an acceptable solution for noisy images. To overcome the noise sensitivity problem of the inverse filter, some filters have been developed based on the least-squares structure. The Wiener filter is based on batch processing which is usually implemented in the frequency domain. The Kalman filter is based on recursive processing which is usually implemented in the spatial domain. Both solutions only work when blur, image and noise parameters are known. The first step for image restoration is the identification of degradation. Consequently, modeling uncertain relationships among many kinds of variables and learning (identification) such variables are important topics. The blur and image parameter identification problem was formerly formulated as a constrained Maximum Likelihood (ML) estimation procedure which was based on optimizing the probability density function (pdf) of the observed image with respect to the unknown parameters. But, the direct optimization of the likelihood function is not feasible, because of its highly nonlinear character. The Expectation Maximization (EM) algorithm is a very popular and widely used algorithm for the computation of ML estimates. There are two steps in EM algorithm, as E (Expectation) and M (Maximization). The EM algorithm finds the conditional expectation of the log-likelihood of complete data given the observed incomplete data. In the E-step, the conditional expectation of the ""hidden variables"" is calculated.  In the M-step, this expectation is maximized with respect to the parameters. The advantage of the EM method is such that it avoids operating directly on the nonlinear likelihood function. The EM algorithm becomes more attractive if its maximization step can be formulated analytically. Even though batch processing of the EM based blur identification and restoration problem needs large memory size, recursive techniques allow dynamic processing with modest storage requirements. Although the EM learning was applied to learning of unknown image and blur parameters based on batch image processing before, recursive EM learning of unknown image and blur parameters has not been studied as much as necessary. Many time series models, including the Hidden Markov Models (HMM) and Kalman Filter Models (KFM) used in filtering and control applications, can be viewed as examples of Dynamic Bayesian Network DBNs. Since, a Bayesian Network is a graphical way to represent a particular factorization of joint distribution; we propose that state space image model can be represented as a DBN. In this work, we introduce a new simultaneous recursive parameter learning and image restoration method based on the ML parameter identification and state estimation for images. We present a new formulation which is given in a Dynamic Bayesian Network (DBN) framework. We focus on the problem of learning the parameters of a Bayesian network. This technique incorporates optimal Kalman smoothing equations for ML parameter identification and state estimation. The use of Kalman filtering instead of Kalman smoothing is employed because of the computationally extensive processing of smoothing. In addition, a restored image is obtained simultaneously as the output of the Kalman filter. We manage to solve the EM learning problem for image and blur parameters in closed form. Although our proposed method processes huge data, because of the recursive structure it does not need large size storage. Performance evaluation of the method is given based on experiments carried out upon real images.  Keywords: Expectation-Maximization, Blur and Image Identification, Recursive Processing,  Kalman Smoothing and Filtering",space,1158
,filtered,core,"A framework for simulating and estimating the state and functional
  topology of complex dynamic geometric networks",2010-06-22 00:00:00,core,http://arxiv.org/abs/0908.3934,,"We present a framework for simulating signal propagation in geometric
networks (i.e. networks that can be mapped to geometric graphs in some space)
and for developing algorithms that estimate (i.e. map) the state and functional
topology of complex dynamic geometric net- works. Within the framework we
define the key features typically present in such networks and of particular
relevance to biological cellular neural networks: Dynamics, signaling,
observation, and control. The framework is particularly well-suited for
estimating functional connectivity in cellular neural networks from
experimentally observable data, and has been implemented using graphics
processing unit (GPU) high performance computing. Computationally, the
framework can simulate cellular network signaling close to or faster than real
time. We further propose a standard test set of networks to measure performance
and compare different mapping algorithms.Comment: Revised following initial peer review. Current version 28 pages and 7
  figures. A slightly modified version has been accepted to Neural Computation
  and is now in pres",space,1159
,filtered,core,Analysis of interplanetary solar sail trajectories with attitude dynamics,2012-03-01 00:00:00,core,https://core.ac.uk/download/19609981.pdf,Univelt Inc,"We present a new approach to the problem of optimal control of solar sails for low-thrust trajectory optimization. The objective was to find the required control torque magnitudes in order to steer a solar sail in interplanetary space. A new steering strategy, controlling the solar sail with generic torques applied about the spacecraft body axes, is integrated into the existing low-thrust trajectory optimization software InTrance. This software combines artificial neural networks and evolutionary algorithms to find steering strategies close to the global optimum without an initial guess. Furthermore, we implement a three rotational degree-of-freedom rigid-body attitude dynamics model to represent the solar sail in space. Two interplanetary transfers to Mars and Neptune are chosen to represent typical future solar sail mission scenarios. The results found with the new steering strategy are compared to the existing reference trajectories without attitude dynamics. The resulting control torques required to accomplish the missions are investigated, as they pose the primary requirements to a real on-board attitude control system",space,1160
,filtered,core,"Click fraud : how to spot it, how to stop it?",2011-08-01 00:00:00,core,https://core.ac.uk/download/143830668.pdf,ThinkIR: The University of Louisville\u27s Institutional Repository,"Online search advertising is currently the greatest source of revenue for many Internet giants such as Google™, Yahoo!™, and Bing™. The increased number of specialized websites and modern profiling techniques have all contributed to an explosion of the income of ad brokers from online advertising. The single biggest threat to this growth is however click fraud. Trained botnets and even individuals are hired by click-fraud specialists in order to maximize the revenue of certain users from the ads they publish on  their websites, or to launch an attack between competing businesses. Most academics and consultants who study online advertising estimate that 15% to 35% of ads in pay per click (PPC) online advertising systems are not authentic. In the first two quarters of 2010, US marketers alone spent $5.7 billion on PPC ads, where PPC ads are between 45 and 50 percent of all online ad spending. On average about $1.5 billion is wasted due to click-fraud. These fraudulent clicks are believed to be initiated by users in poor countries, or botnets, who are trained to click on specific ads. For example, according to a 2010 study from Information Warfare Monitor, the operators of Koobface, a program that installed malicious software to participate in click fraud, made over $2 million in just over a year. The process of making such illegitimate clicks to generate revenue is called click-fraud. Search engines claim they filter out most questionable clicks and either not charge for them or reimburse advertisers that have been wrongly billed. However this is a hard task, despite the claims that brokers\u27 efforts are satisfactory. In the simplest scenario, a publisher continuously clicks on the ads displayed on his own website in order to make revenue. In a more complicated scenario. a travel agent may hire a large, globally distributed, botnet to click on its competitor\u27s ads, hence depleting their daily budget. We analyzed those different types of click fraud methods and proposed new methodologies to detect and prevent them real time. While traditional commercial approaches detect only some specific types of click fraud, Collaborative Click Fraud Detection and Prevention (CCFDP) system, an architecture that we have implemented based on the proposed methodologies, can detect and prevents all major types of click fraud. The proposed solution analyzes the detailed user activities on both, the server side and client side collaboratively to better describe the intention of the click. Data fusion techniques are developed to combine evidences from several data mining models and to obtain a better estimation of the quality of the click traffic. Our ideas are experimented through the development of the Collaborative Click Fraud Detection and Prevention (CCFDP) system. Experimental results show that the CCFDP system is better than the existing commercial click fraud solution in three major aspects: 1) detecting more click fraud especially clicks generated by software; 2) providing prevention ability; 3) proposing the concept of click quality score for click quality estimation. In the CCFDP initial version, we analyzed the performances of the click fraud detection and prediction model by using a rule base algorithm, which is similar to most of the existing systems. We have assigned a quality score for each click instead of classifying the click as fraud or genuine, because it is hard to get solid evidence of click fraud just based on the data collected, and it is difficult to determine the real intention of users who make the clicks. Results from initial version revealed that the diversity of CF attack Results from initial version revealed that the diversity of CF attack types makes it hard for a single counter measure to prevent click fraud. Therefore, it is important to be able to combine multiple measures capable of effective protection from click fraud. Therefore, in the CCFDP improved version, we provide the traffic quality score as a combination of evidence from several data mining algorithms. We have tested the system with a data from an actual ad campaign in 2007 and 2008.  We have compared the results with Google Adwords reports for the same campaign. Results show that a higher percentage of click fraud present even with the most popular search engine. The multiple model based CCFDP always estimated less valid traffic compare to Google. Sometimes the difference is as high as 53%. Detection of duplicates, fast and efficient, is one of the most important requirement in any click fraud solution. Usually duplicate detection algorithms run in real time. In order to provide real time results, solution providers should utilize data structures that can be updated in real time. In addition, space requirement to hold data should be minimum. In this dissertation, we also addressed the problem of detecting duplicate clicks in pay-per-click streams. We proposed a simple data structure, Temporal Stateful Bloom Filter (TSBF), an extension to the regular Bloom Filter and Counting Bloom Filter. The bit vector in the Bloom Filter was replaced with a status vector. Duplicate detection results of TSBF method is compared with Buffering, FPBuffering, and CBF methods. False positive rate of TSBF is less than 1% and it does not have false negatives. Space requirement of TSBF is minimal among other solutions. Even though Buffering does not have either false positives or false negatives its space requirement increases exponentially with the size of the stream data size. When the false positive rate of the FPBuffering is set to 1% its false negative rate jumps to around 5%, which will not be tolerated by most of the streaming data applications. We also compared the TSBF results with CBF. TSBF uses only half the space or less than standard CBF with the same false positive probability. One of the biggest successes with CCFDP is the discovery of new mercantile click bot, the Smart ClickBot. We presented a Bayesian approach for detecting the Smart ClickBot type clicks. The system combines evidence extracted from web server sessions to determine the final class of each click. Some of these evidences can be used alone, while some can be used in combination with other features for the click bot detection. During training and testing we also addressed the class imbalance problem. Our best classifier shows recall of 94%. and precision of 89%, with F1 measure calculated as 92%. The high accuracy of our system proves the effectiveness of the proposed methodology. Since the Smart ClickBot is a sophisticated click bot that manipulate every possible parameters to go undetected, the techniques that we discussed here can lead to detection of other types of software bots too. Despite the enormous capabilities of modern machine learning and data mining techniques in modeling complicated problems, most of the available click fraud detection systems are rule-based. Click fraud solution providers keep the rules as a secret weapon and bargain with others to prove their superiority. We proposed validation framework to acquire another model of the clicks data that is not rule dependent, a model that learns the inherent statistical regularities of the data. Then the output of both models is compared. Due to the uniqueness of the CCFDP system architecture, it is better than current commercial solution and search engine/ISP solution. The system protects Pay-Per-Click advertisers from click fraud and improves their Return on Investment (ROI). The system can also provide an arbitration system for advertiser and PPC publisher whenever the click fraud argument arises. Advertisers can gain their confidence on PPC advertisement by having a channel to argue the traffic quality with big search engine publishers. The results of this system will booster the internet economy by eliminating the shortcoming of PPC business model. General consumer will gain their confidence on internet business model by reducing fraudulent activities which are numerous in current virtual internet world",space,1161
,filtered,core,RNA: REUSABLE NEURON ARCHITECTURE FOR ON-CHIP ELECTROCARDIOGRAM CLASSIFICATION AND MACHINE LEARNING,2010-06-25 00:00:00,core,https://core.ac.uk/download/12206366.pdf,,"Artificial neural networks (ANN) offer tremendous promise in classifying electrocardiogram (ECG) for detection and diagnosis of cardiovascular diseases. In this thesis, we propose a reusable neuron architecture (RNA) to enable an efficient and cost-effective ANN-based ECG processing by multiplexing the same physical neurons for both feed-forward and back-propagation stages. RNA further conserves the area and resources of the chip and reduces power dissipation by coalescing different layers of the neural network into a single layer. Moreover, the microarchitecture of each RNA neuron has been optimized to maximize the degree of hardware reusability by fusing multiple two-input multipliers and a multi-input adder into one two-input multiplier and one two-input adder. With RNA, we demonstrated a hardware implementation of a three-layer 51-30-12 artificial neural network using only thirty physical RNA neurons.A quantitative design space exploration in area, power dissipation, and speed between the proposed RNA and three other implementations representative of different reusable hardware strategies is presented and discussed. An RNA ASIC was implemented using 45nm CMOS technology and verified on a Xilinx Virtex-5 FPGA board. Compared with an equivalent software implementation in C executed on a mainstream embedded microprocessor, the RNA ASIC improves both the training speed and the energy efficiency by three orders of magnitude, respectively. The real-time and functional correctness of RNA was verified using real ECG signals from the MIT-BIH arrhythmia database",space,1162
,filtered,core,cytonGrasp: Cyton Alpha Controller via GraspIt! Simulation,2011-12-01 00:00:00,core,https://core.ac.uk/download/268809713.pdf,TRACE: Tennessee Research and Creative Exchange,"This thesis addresses an expansion of the control programs for the Cyton Alpha 7D 1G arm. The original control system made use of configurable software which exploited the arm’s seven degrees of freedom and kinematic redundancy to control the arm based on desired behaviors that were configured off-line. The inclusions of the GraspIt! grasp planning simulator and toolkit enables the Cyton Alpha to be used in more proactive on-line grasping problems, as well as, presenting many additional tools for on-line learning applications. In short, GraspIt! expands what is possible with the Cyton Alpha to include many machine learning tools and opportunities for future research. Noteworthy features of GraspIt!:
• A 3D user interface allowing the user to see and interact virtual objects, obstacles, and robots, in addition to a 3D representation of the Cyton Alpha
• A collision detection and contact determination system within simulation • On-line grasp analysis routines
• Visualization methods for determining the weak points within a grasp, as well as, creating projections of grasp quality and ability to resist dynamic forces.
• Computation of numerical grasp quality metrics and visualization methods for proposed grasps
• Dynamics engine
• Support for lower-dimensional hand posture subspaces
• Interaction with sensors (Flock of Birds tracker) and hardware (Pioneer robot) within simulation
• GraspIt! can generate huge databases of labeled grasp data, which can be used for data-driven grasp-planning algorithms and has built in support for the Columbia Grasp Database.
By making use of the GraspIt! simulator, it is possible to test algorithms for grasp manipulation, grasp planning, or grasp synthesis more quickly and with greater repeatability than would be possible on the real robot. Contributions of this system include:
1. A joint based 3D rendering of the Cyton Alpha 7D 1G arm
2. Simulated bodies for several objects in the DI Lab
3. Support for multiple representations of joint data within three-dimensional space
• Euler Angles
• Quaternions
• Denavit-Hartenberg Parameters
4. Framework for future work in grasp-planning, grasp synthesis, cooperative grasping tasks, and transfer learning applications with the Cyton Alpha arm",space,1163
,filtered,core,Software Analyzes Complex Systems in Real Time,2008-01-01 00:00:00,core,https://core.ac.uk/download/pdf/10546711.pdf,,"Expert system software programs, also known as knowledge-based systems, are computer programs that emulate the knowledge and analytical skills of one or more human experts, related to a specific subject. SHINE (Spacecraft Health Inference Engine) is one such program, a software inference engine (expert system) designed by NASA for the purpose of monitoring, analyzing, and diagnosing both real-time and non-real-time systems. It was developed to meet many of the Agency s demanding and rigorous artificial intelligence goals for current and future needs. NASA developed the sophisticated and reusable software based on the experience and requirements of its Jet Propulsion Laboratory s (JPL) Artificial Intelligence Research Group in developing expert systems for space flight operations specifically, the diagnosis of spacecraft health. It was designed to be efficient enough to operate in demanding real time and in limited hardware environments, and to be utilized by non-expert systems applications written in conventional programming languages. The technology is currently used in several ongoing NASA applications, including the Mars Exploration Rovers and the Spacecraft Health Automatic Reasoning Pilot (SHARP) program for the diagnosis of telecommunication anomalies during the Neptune Voyager Encounter. It is also finding applications outside of the Space Agency",space,1164
,filtered,core,Corroborating Emotion Theory with Role Theory and Agent Technology: a Framework for Designing Emotional Agents as Motivational Tutoring Entities,2008-03-19 00:00:00,core,https://core.ac.uk/download/33799208.pdf,,"Nowadays, more and more applications require systems that can interact with humans. Agents can be perceived as computing services that humans, or even other agents, can request in order to accomplish their tasks. Some services may be simple and others rather complex. A way to determine the best agents (services) to be implemented is to identify who the actors are in the object of study, which roles they play, and (if possible) what kind of knowledge they use. 
Socially Intelligent Agents (SIAs) are agent systems that are able to connect and interface with humans, i.e. robotic or computational systems that show aspects of human-style social intelligence. In addition to their relevance in application areas such as e-commerce and entertainment, building artefacts in software and hardware has been recognized as a powerful tool for establishing a science of social minds which is a constructive approach toward understanding social intelligence in humans and other animals.
Social intelligence in humans and other animals has a number of fascinating facets and implications for the design of SIAs. Human beings are biological agents that are embodied members of a social environment and are autobiographic agents who have a unique personality. They are situated in time and space and interpret new experiences based on reconstructions of previous experiences. Due to their physical embodiment, they have a unique perspective on the world and a unique history: an autobiography. Also, humans are able to express and recognize emotions, that are important in regulating individual survival and problem-solving as well as social interactions.
Like artificial intelligence research trend, SIA research trend can be pursued with different goals in mind. A deep AI approach seeks to simulate real social intelligence and processes. A shallow AI approach, which will be highlighted also within this thesis, aims to create artefacts that are not socially intelligent per se, but rather appear socially intelligent to a given user. The shallow approach does not seek to create social intelligence unless it is meaningful social intelligence vis-à-vis some user situation
In order to develop believable SIAs we do not have to know how beliefs-desires and intentions actually relate to each other in the real minds of the people. If one wants to create the impression of an artificial social agent driven by beliefs and desires, it is enough to draw on investigations on how people with different cultural background, develop and use theories of mind to understand the behaviours of others. Therefore, SIA technology needs to model the folk-theory reasoning rather than the real thing. To a shallow AI approach, a model of mind based on folk-psychology is as valid as one based on cognitive theory. 
Distance education is understood as online learning that is technology-based training which encompasses both computer-assisted and Web-based training. These systems, which appear to offer something for everyone at any time, in any place, do not always live up to the great promise they offer.
The usage of social intelligent agents in online learning environments can enable the design of “enhanced-learning environments” that allow for the development and the assessment of social competences as well as the common professional competences. 

Within this thesis it is shown how to corroborate affective theory with role theory with agent technology in a synchronous virtual environment in order to overcome several inconveniences of distance education systems. This research embraces also the shallow approach of SIA and aims to provide the first steps of a method for creating a believable life-like tutor agent which can partially replace human-teachers and assist the students in the process of learning. The starting point for this research came from the fact: anxious, angry or depressed students do not learn; people in these conditions do not absorb information efficiently, consequentially it is an illusion to think that learning environments that do not consider motivational and emotional factors are adequate",space,1165
,filtered,core,Scene Monitoring With A Forest Of Cooperative Sensors,2005-01-01 00:00:00,core,https://core.ac.uk/download/236257331.pdf,'Information Bulletin on Variable Stars (IBVS)',"In this dissertation, we present vision based scene interpretation methods for monitoring of people and vehicles, in real-time, within a busy environment using a forest of co-operative electro-optical (EO) sensors. We have developed novel video understanding algorithms with learning capability, to detect and categorize people and vehicles, track them with in a camera and hand-off this information across multiple networked cameras for multi-camera tracking. The ability to learn prevents the need for extensive manual intervention, site models and camera calibration, and provides adaptability to changing environmental conditions. For object detection and categorization in the video stream, a two step detection procedure is used. First, regions of interest are determined using a novel hierarchical background subtraction algorithm that uses color and gradient information for interest region detection. Second, objects are located and classified from within these regions using a weakly supervised learning mechanism based on co-training that employs motion and appearance features. The main contribution of this approach is that it is an online procedure in which separate views (features) of the data are used for co-training, while the combined view (all features) is used to make classification decisions in a single boosted framework. The advantage of this approach is that it requires only a few initial training samples and can automatically adjust its parameters online to improve the detection and classification performance. Once objects are detected and classified they are tracked in individual cameras. Single camera tracking is performed using a voting based approach that utilizes color and shape cues to establish correspondence in individual cameras. The tracker has the capability to handle multiple occluded objects. Next, the objects are tracked across a forest of cameras with non-overlapping views. This is a hard problem because of two reasons. First, the observations of an object are often widely separated in time and space when viewed from non-overlapping cameras. Secondly, the appearance of an object in one camera view might be very different from its appearance in another camera view due to the differences in illumination, pose and camera properties. To deal with the first problem, the system learns the inter-camera relationships to constrain track correspondences. These relationships are learned in the form of multivariate probability density of space-time variables (object entry and exit locations, velocities, and inter-camera transition times) using Parzen windows. To handle the appearance change of an object as it moves from one camera to another, we show that all color transfer functions from a given camera to another camera lie in a low dimensional subspace. The tracking algorithm learns this subspace by using probabilistic principal component analysis and uses it for appearance matching. The proposed system learns the camera topology and subspace of inter-camera color transfer functions during a training phase. Once the training is complete, correspondences are assigned using the maximum a posteriori (MAP) estimation framework using both the location and appearance cues. Extensive experiments and deployment of this system in realistic scenarios has demonstrated the robustness of the proposed methods. The proposed system was able to detect and classify targets, and seamlessly tracked them across multiple cameras. It also generated a summary in terms of key frames and textual description of trajectories to a monitoring officer for final analysis and response decision. This level of interpretation was the goal of our research effort, and we believe that it is a significant step forward in the development of intelligent systems that can deal with the complexities of real world scenarios",space,1166
,filtered,core,INDUCTIVE SYSTEM HEALTH MONITORING WITH STATISTICAL METRICS,2005-01-01 00:00:00,core,https://core.ac.uk/download/pdf/10516158.pdf,,"Model-based reasoning is a powerful method for performing system monitoring and diagnosis. Building models for model-based reasoning is often a difficult and time consuming process. The Inductive Monitoring System (IMS) software was developed to provide a technique to automatically produce health monitoring knowledge bases for systems that are either difficult to model (simulate) with a computer or which require computer models that are too complex to use for real time monitoring. IMS processes nominal data sets collected either directly from the system or from simulations to build a knowledge base that can be used to detect anomalous behavior in the system. Machine learning and data mining techniques are used to characterize typical system behavior by extracting general classes of nominal data from archived data sets. In particular, a clustering algorithm forms groups of nominal values for sets of related parameters. This establishes constraints on those parameter values that should hold during nominal operation. During monitoring, IMS provides a statistically weighted measure of the deviation of current system behavior from the established normal baseline. If the deviation increases beyond the expected level, an anomaly is suspected, prompting further investigation by an operator or automated system. IMS has shown potential to be an effective, low cost technique to produce system monitoring capability for a variety of applications. We describe the training and system health monitoring techniques of IMS. We also present the application of IMS to a data set from the Space Shuttle Columbia STS-107 flight. IMS was able to detect an anomaly in the launch telemetry shortly after a foam impact damaged Columbia's thermal protection system",space,1167
,filtered,core,A comparison of fixed final time optimal control computational methods with a view to closed loop implementation using artificial neural networks,2009-01-01 00:00:00,core,https://core.ac.uk/download/219372139.pdf,,"The purpose of this paper is to lay the foundations of a new generation of closed loop
optimal control laws based on the plant state space model and implemented using artificial neural
networks. The basis is the long established open loop methods of Bellman and Pontryagin, which
compute optimal controls off line and apply them subsequently in real time. They are therefore open
loop methods and during the period leading up to the present century, they have been abandoned by
the mainstream control researchers due to a) the fundamental drawback of susceptibility to plant
modelling errors and external disturbances and b) the lack of success in deriving closed loop versions
in all but the simplest and often unrealistic cases. The recent energy crisis, however, has promoted the
authors to revisit
the classical optimal control methods with a view to deriving new practicable
closed loop optimal control laws that could save terawatts of electrical energy by replacement of
classical controllers throughout industry. First Bellman’s and Pontryagin’s methods are compared
regarding ease of computation. Then a new optimal state feedback controller is proposed based on the
training of artificial neural networks with the computed optimal controls",space,1168
,filtered,core,Lyapunov Based Stability Analysis for Metro Lines,2008-06-30 00:00:00,core,https://core.ac.uk/download/268932471.pdf,,"In this work a direct method to measure the stability of metro system lines with respect to a previously constructed time schedule is presented. For this purpose we first model saturation effects using a real time discrete space state representation and then apply a Lyapunov-based stability analysis considering time delays of trains as disturbances. As a result we have been able to define a new set of indexes that relate time delays with the validity of the actual time schedule when falling inside a particular ‘stability area’. Results obtained in a simulated environment show that the new stability indexes are able to evaluate quantitatively and qualitatively the effects of saturation in metro lines as well as predict the need for rescheduling Keywords: metro system, stability, planning, genetic algorithm, artificial intelligence. 1 Introduction The dynamics of metro line systems have been deeply studied by several researchers [1–5,7–10]. Most of these dynamical models are based on the Sasama and Ohkawa [9] linear model. It is well known that such kind of linear models, usually yield simple formulation, implementation and simulation. As a result, some dynamic traffic linear controllers [1–3,7] and real time simulators [3,4] have been proposed.In this work a direct method to measure the stability of metro system lines with respect to a previously constructed time schedule is presented. For this purpose we first model saturation effects using a real time discrete space state representation and then apply a Lyapunov-based stability analysis considering time delays of trains as disturbances. As a result we have been able to define a new set of indexes that relate time delays with the validity of the actual time schedule when falling inside a particular ‘stability area’. Results obtained in a simulated environment show that the new stability indexes are able to evaluate quantitatively and qualitatively the effects of saturation in metro lines as well as predict the need for rescheduling Keywords: metro system, stability, planning, genetic algorithm, artificial intelligence. 1 Introduction The dynamics of metro line systems have been deeply studied by several researchers [1–5,7–10]. Most of these dynamical models are based on the Sasama and Ohkawa [9] linear model. It is well known that such kind of linear models, usually yield simple formulation, implementation and simulation. As a result, some dynamic traffic linear controllers [1–3,7] and real time simulators [3,4] have been proposed",space,1169
,filtered,core,Efficient feature reduction and classification methods,2009-01-01 00:00:00,core,https://core.ac.uk/download/11589081.pdf,,"Durch die steigende Anzahl verfügbarer Daten in unterschiedlichsten Anwendungsgebieten nimmt der Aufwand vieler Data-Mining Applikationen signifikant zu.  Speziell hochdimensionierte Daten (Daten die über viele verschiedene Attribute beschrieben werden) können ein großes Problem für viele Data-Mining Anwendungen darstellen. Neben höheren Laufzeiten können dadurch sowohl für überwachte (supervised), als auch nicht überwachte (unsupervised) Klassifikationsalgorithmen weitere Komplikationen  entstehen (z.B. ungenaue Klassifikationsgenauigkeit, schlechte Clustering-Eigenschaften, …). Dies führt zu einem Bedarf an effektiven und effizienten Methoden zur Dimensionsreduzierung. 

Feature Selection (die Auswahl eines Subsets von Originalattributen) und Dimensionality Reduction (Transformation von Originalattribute in (Linear)-Kombinationen der Originalattribute) sind zwei wichtige Methoden um die Dimension von Daten zu reduzieren. Obwohl sich in den letzten Jahren vielen Studien mit diesen Methoden beschäftigt haben, gibt es immer noch viele offene Fragestellungen in diesem Forschungsgebiet. Darüber hinaus ergeben sich in vielen Anwendungsbereichen durch die immer weiter steigende Anzahl an verfügbaren und verwendeten Attributen und Features laufend neue Probleme. Das Ziel dieser Dissertation ist es, verschiedene Fragenstellungen in diesem Bereich genau zu analysieren und Verbesserungsmöglichkeiten zu entwickeln.



Grundsätzlich, werden folgende Ansprüche an Methoden zur Feature Selection und Dimensionality Reduction gestellt: Die Methoden sollten effizient (bezüglich ihres Rechenaufwandes) sein und die resultierenden Feature-Sets sollten die Originaldaten möglichst kompakt repräsentieren können. Darüber hinaus ist es in vielen Anwendungsgebieten wichtig, die Interpretierbarkeit der Originaldaten beizubehalten. Letztendlich sollte der Prozess der Dimensionsreduzierung keinen negativen Effekt auf die Klassifikationsgenauigkeit haben - sondern idealerweise, diese noch verbessern. 



Offene Problemstellungen in diesem Bereich betreffen unter anderem den Zusammenhang zwischen Methoden zur Dimensionsreduzierung und der resultierenden Klassifikationsgenauigkeit, wobei sowohl eine möglichst kompakte Repräsentation der Daten, als auch eine hohe Klassifikationsgenauigkeit erzielt werden sollen. Wie bereits erwähnt, ergibt sich durch die große Anzahl an Daten auch ein erhöhter Rechenaufwand, weshalb schnelle und effektive Methoden zur Dimensionsreduzierung entwickelt werden müssen, bzw. existierende Methoden verbessert werden müssen. Darüber hinaus sollte natürlich auch der Rechenaufwand der verwendeten Klassifikationsmethoden möglichst gering sein. Des Weiteren ist die Interpretierbarkeit von Feature Sets zwar möglich, wenn Feature Selection Methoden für die Dimensionsreduzierung verwendet werden, im Fall von Dimensionality Reduction sind die resultierenden Feature Sets jedoch meist Linearkombinationen der Originalfeatures. Daher ist es schwierig zu überprüfen, wie viel Information einzelne Originalfeatures beitragen. 



Im Rahmen dieser Dissertation konnten wichtige Beiträge zu den oben genannten Problemstellungen präsentiert werden: Es wurden neue, effiziente Initialisierungsvarianten für die Dimensionality Reduction Methode Nonnegative Matrix Factorization (NMF) entwickelt, welche im Vergleich zu randomisierter Initialisierung und im Vergleich zu State-of-the-Art Initialisierungsmethoden zu einer schnelleren Reduktion des Approximationsfehlers führen. Diese Initialisierungsvarianten können darüber hinaus mit neu entwickelten und sehr effektiven Klassifikationsalgorithmen basierend auf NMF kombiniert werden. Um die Laufzeit von NMF weiter zu steigern wurden unterschiedliche Varianten von NMF Algorithmen auf Multi-Prozessor Systemen vorgestellt, welche sowohl Task- als auch Datenparallelismus unterstützen und zu einer erheblichen Reduktion der Laufzeit für NMF führen. Außerdem wurde eine effektive Verbesserung der Matlab Implementierung des ALS Algorithmus vorgestellt. Darüber hinaus wurde eine Technik aus dem Bereich des Information Retrieval -- Latent Semantic Indexing -- erfolgreich als Klassifikationsalgorithmus für Email Daten angewendet. Schließlich wurde eine ausführliche empirische Studie über den Zusammenhang verschiedener Feature Reduction Methoden (Feature Selection und Dimensionality Reduction) und der resultierenden Klassifikationsgenauigkeit unterschiedlicher Lernalgorithmen präsentiert. 



Der starke Einfluss unterschiedlicher Methoden zur Dimensionsreduzierung auf die resultierende Klassifikationsgenauigkeit unterstreicht dass noch weitere Untersuchungen notwendig sind um das komplexe Zusammenspiel von Dimensionsreduzierung und Klassifikation genau analysieren zu können.The sheer volume of data today and its expected growth over the next years are some of the key challenges in data mining and knowledge discovery applications. Besides the huge number of data samples that are collected and processed, the high dimensional nature of data arising in many applications causes the need to develop effective and efficient techniques that are able to deal with this massive amount of data. In addition to the significant increase in the demand of computational resources, those large datasets might also influence the quality of several data mining applications (especially if the number of features is very high compared to the number of samples). As the dimensionality of data increases, many types of data analysis and classification problems become significantly harder. This can lead to problems for both supervised and unsupervised learning. Dimensionality reduction and feature (subset) selection methods are two types of techniques for reducing the attribute space. While in feature selection a subset of the original attributes is extracted, dimensionality reduction in general produces linear combinations of the original attribute set. In both approaches, the goal is to select a low dimensional subset of the attribute space that covers most of the information of the original data. During the last years, feature selection and dimensionality reduction techniques have become a real prerequisite for data mining applications. 



There are several open questions in this research field, and due to the often increasing number of candidate features for various application areas (e.\,g., email filtering or drug classification/molecular modeling) new questions arise. In this thesis, we focus on some open research questions in this context, such as the relationship between feature reduction techniques and the resulting classification accuracy and the relationship between the variability captured in the linear combinations of dimensionality reduction techniques (e.\,g., PCA, SVD) and the accuracy of machine learning algorithms operating on them.  Another important goal is to better understand new techniques for dimensionality reduction, such as nonnegative matrix factorization (NMF), which can be applied for finding parts-based, linear representations of nonnegative data.  This ``sum-of-parts'' representation is especially useful if the interpretability of the original data should be retained. Moreover, performance aspects of feature reduction algorithms are investigated. As data grow, implementations of feature selection and dimensionality reduction techniques for high-performance parallel and distributed computing environments become more and more important. 



In this thesis, we focus on two types of open research questions: methodological advances without any specific application context, and application-driven advances for a specific application context. Summarizing, new methodological contributions are the following: The utilization of nonnegative matrix factorization in the context of classification methods is investigated.  In particular, it is of interest how the improved interpretability of NMF factors due to the non-negativity constraints (which is of central importance in various problem settings) can be exploited. Motivated by this problem context two new fast initialization techniques for NMF based on feature selection are introduced. It is shown how approximation accuracy can be increased and/or how computational effort can be reduced compared to standard randomized seeding of the NMF and to state-of-the-art initialization strategies suggested earlier. For example, for a given number of iterations and a required approximation error a speedup of 3.6 compared to standard initialization, and a speedup of 3.4 compared to state-of-the-art initialization strategies could be achieved. Beyond that, novel classification methods based on the NMF are proposed and investigated. We can show that they are not only competitive in terms of classification accuracy with state-of-the-art classifiers, but also provide important advantages in terms of computational effort (especially for low-rank approximations). Moreover, parallelization and distributed execution of NMF is investigated. Several algorithmic variants for efficiently computing NMF on multi-core systems are studied and compared to each other. In particular, several approaches for exploiting task and/or data-parallelism in NMF are studied. We show that for some scenarios new algorithmic variants clearly outperform existing implementations. Last, but not least, a computationally very efficient adaptation of the implementation of the ALS algorithm in Matlab 2009a is investigated. This variant reduces the runtime significantly (in some settings by a factor of 8) and also provides several possibilities to be executed concurrently.



In addition to purely methodological questions, we also address questions arising in the adaptation of feature selection and classification methods to two specific application problems: email classification and in silico screening for drug discovery. Different research challenges arise in the contexts of these different application areas, such as the dynamic nature of data for email classification problems, or the imbalance in the number of available samples of different classes for drug discovery problems. Application-driven advances of this thesis comprise the adaptation and application of latent semantic indexing (LSI) to the task of email filtering. Experimental results show that LSI achieves significantly better classification results than the widespread de-facto standard method for this special application context. In the context of drug discovery problems, several groups of well discriminating descriptors could be identified by utilizing the ``sum-of-parts`` representation of NMF. The number of important descriptors could be further increased when applying sparseness constraints on the NMF factors",space,1170
,filtered,core,"Vision-based reinforcement learning using approximate policy

iteration",2009-01-01 00:00:00,core,https://core.ac.uk/download/55463.pdf,,"A major issue for reinforcement learning (RL) applied to robotics is the time required to learn a new skill. While RL has been used to learn mobile robot control in many simulated domains, applications involving learning on real

robots are still relatively rare. In this paper, the Least-Squares Policy Iteration (LSPI) reinforcement learning algorithm and a new model-based algorithm Least-Squares Policy Iteration with Prioritized Sweeping (LSPI+), are implemented on a mobile robot to acquire new skills quickly and efficiently. LSPI+ combines the benefits of LSPI and prioritized sweeping, which uses all previous experience to focus the computational effort on the most “interesting” or dynamic parts of the state space. 

The proposed algorithms are tested on a household vacuum

cleaner robot for learning a docking task using vision as the only sensor modality. In experiments these algorithms are compared to other model-based and model-free RL algorithms. The results show that the number of trials required to learn the docking task is significantly reduced using LSPI compared to the other RL algorithms investigated, and that LSPI+ further improves on the performance of LSPI",space,1171
,filtered,core,Real-Time Adaptive Color Segmentation by Neural Networks,2004-11-01 00:00:00,core,https://core.ac.uk/download/pdf/10563765.pdf,,"Artificial neural networks that would utilize the cascade error projection (CEP) algorithm have been proposed as means of autonomous, real-time, adaptive color segmentation of images that change with time. In the original intended application, such a neural network would be used to analyze digitized color video images of terrain on a remote planet as viewed from an uninhabited spacecraft approaching the planet. During descent toward the surface of the planet, information on the segmentation of the images into differently colored areas would be updated adaptively in real time to capture changes in contrast, brightness, and resolution, all in an effort to identify a safe and scientifically productive landing site and provide control feedback to steer the spacecraft toward that site. Potential terrestrial applications include monitoring images of crops to detect insect invasions and monitoring of buildings and other facilities to detect intruders. The CEP algorithm is reliable and is well suited to implementation in very-large-scale integrated (VLSI) circuitry. It was chosen over other neural-network learning algorithms because it is better suited to realtime learning: It provides a self-evolving neural-network structure, requires fewer iterations to converge and is more tolerant to low resolution (that is, fewer bits) in the quantization of neural-network synaptic weights. Consequently, a CEP neural network learns relatively quickly, and the circuitry needed to implement it is relatively simple. Like other neural networks, a CEP neural network includes an input layer, hidden units, and output units (see figure). As in other neural networks, a CEP network is presented with a succession of input training patterns, giving rise to a set of outputs that are compared with the desired outputs. Also as in other neural networks, the synaptic weights are updated iteratively in an effort to bring the outputs closer to target values. A distinctive feature of the CEP neural network and algorithm is that each update of synaptic weights takes place in conjunction with the addition of another hidden unit, which then remains in place as still other hidden units are added on subsequent iterations. For a given training pattern, the synaptic weight between (1) the inputs and the previously added hidden units and (2) the newly added hidden unit is updated by an amount proportional to the partial derivative of a quadratic error function with respect to the synaptic weight. The synaptic weight between the newly added hidden unit and each output unit is given by a more complex function that involves the errors between the outputs and their target values, the transfer functions (hyperbolic tangents) of the neural units, and the derivatives of the transfer functions",space,1172
,filtered,core,Combinatorial optimization and metaheuristics,2006-01-01 00:00:00,core,https://core.ac.uk/download/29139966.pdf,Brunel University,"Today, combinatorial optimization is one of the youngest and most active areas of discrete mathematics. It is a branch of optimization in applied mathematics and computer science, related to operational research, algorithm theory and computational complexity theory. It sits at the intersection of several fields, including artificial intelligence, mathematics and software engineering. Its increasing interest arises for the fact that a large number of scientific and industrial problems can be formulated as abstract combinatorial optimization problems, through graphs and/or (integer) linear programs. Some of these problems have polynomial-time (“efficient”) algorithms, while most of them are NP-hard, i.e. it is not proved that they can be solved in polynomial-time. Mainly, it means that it is not possible to guarantee that an exact solution to the problem can be found and one has to settle for an approximate solution with known performance guarantees. Indeed, the goal of approximate methods is to find “quickly” (reasonable run-times), with “high” probability, provable “good” solutions (low error from the real optimal solution). In the last 20 years, a new kind of algorithm commonly called metaheuristics have emerged in this class, which basically try to combine heuristics in high level frameworks aimed at efficiently and effectively exploring the search space. This report briefly outlines the components, concepts, advantages and disadvantages of different metaheuristic approaches from a conceptual point of view, in order to analyze their similarities and differences. The two very significant forces of intensification and diversification, that mainly determine the behavior of a metaheuristic, will be pointed out. The report concludes by exploring the importance of hybridization and integration methods",space,1173
,filtered,core,A framework for predicting oil-palm yield from climate data,2006-05-01 00:00:00,core,https://core.ac.uk/download/11779721.pdf,,"Intelligent systems based on machine learning techniques, such as classification, clustering, are gaining wide spread popularity in real world applications. This paper presents work on developing a software system for predicting crop yield, for example oil-palm yield, from climate and plantation data. At the core of our system is a method for unsupervised partitioning of data for finding spatio-temporal patterns in climate data using kernel methods which offer strength to deal with complex data. This work gets inspiration from the notion that a non-linear data transformation into some high dimensional feature space increases the possibility of linear separability of the patterns in the transformed space. Therefore, it simplifies exploration of the associated structure in the data. Kernel methods implicitly perform a non-linear mapping of the input data into a high dimensional feature space by replacing the inner products with an appropriate positive definite function. In this paper we present a robust weighted kernel k-means algorithm incorporating spatial constraints for clustering the data. The proposed algorithm can effectively handle noise, outliers and auto-correlation in the spatial data, for effective and efficient data analysis by exploring patterns and structures in the data, and thus can be used for predicting oil-palm yield by analyzing various factors affecting the yield",space,1174
,filtered,core,Object oriented fault diagnosis system for space shuttle main engine redlines,1990-01-01 00:00:00,core,https://core.ac.uk/download/pdf/42822513.pdf,,"A great deal of attention has recently been given to Artificial Intelligence research in the area of computer aided diagnostics. Due to the dynamic and complex nature of space shuttle red-line parameters, a research effort is under way to develop a real time diagnostic tool that will employ historical and engineering rulebases as well as a sensor validity checking. The capability of AI software development tools (KEE and G2) will be explored by applying object oriented programming techniques in accomplishing the diagnostic evaluation",space,1175
,filtered,core,Development of a Computer Architecture to Support the Optical Plume Anomaly Detection (OPAD) System,1996-10-01 00:00:00,core,https://core.ac.uk/download/pdf/10473436.pdf,,"The NASA OPAD spectrometer system relies heavily on extensive software which repetitively extracts spectral information from the engine plume and reports the amounts of metals which are present in the plume. The development of this software is at a sufficiently advanced stage where it can be used in actual engine tests to provide valuable data on engine operation and health. This activity will continue and, in addition, the OPAD system is planned to be used in flight aboard space vehicles. The two implementations, test-stand and in-flight, may have some differing requirements. For example, the data stored during a test-stand experiment are much more extensive than in the in-flight case. In both cases though, the majority of the requirements are similar. New data from the spectrograph is generated at a rate of once every 0.5 sec or faster. All processing must be completed within this period of time to maintain real-time performance. Every 0.5 sec, the OPAD system must report the amounts of specific metals within the engine plume, given the spectral data. At present, the software in the OPAD system performs this function by solving the inverse problem. It uses powerful physics-based computational models (the SPECTRA code), which receive amounts of metals as inputs to produce the spectral data that would have been observed, had the same metal amounts been present in the engine plume. During the experiment, for every spectrum that is observed, an initial approximation is performed using neural networks to establish an initial metal composition which approximates as accurately as possible the real one. Then, using optimization techniques, the SPECTRA code is repetitively used to produce a fit to the data, by adjusting the metal input amounts until the produced spectrum matches the observed one to within a given level of tolerance. This iterative solution to the original problem of determining the metal composition in the plume requires a relatively long period of time to execute the software in a modern single-processor workstation, and therefore real-time operation is currently not possible. A different number of iterations may be required to perform spectral data fitting per spectral sample. Yet, the OPAD system must be designed to maintain real-time performance in all cases. Although faster single-processor workstations are available for execution of the fitting and SPECTRA software, this option is unattractive due to the excessive cost associated with very fast workstations and also due to the fact that such hardware is not easily expandable to accommodate future versions of the software which may require more processing power. Initial research has already demonstrated that the OPAD software can take advantage of a parallel computer architecture to achieve the necessary speedup. Current work has improved the software by converting it into a form which is easily parallelizable. Timing experiments have been performed to establish the computational complexity and execution speed of major components of the software. This work provides the foundation of future work which will create a fully parallel version of the software executing in a shared-memory multiprocessor system",space,1176
,filtered,core,Current research activities at the NASA-sponsored Illinois Computing Laboratory of Aerospace Systems and Software,1994-01-01 00:00:00,core,https://core.ac.uk/download/pdf/42783398.pdf,,"The Illinois Computing Laboratory of Aerospace Systems and Software (ICLASS) was established to: (1) pursue research in the areas of aerospace computing systems, software and applications of critical importance to NASA, and (2) to develop and maintain close contacts between researchers at ICLASS and at various NASA centers to stimulate interaction and cooperation, and facilitate technology transfer. Current ICLASS activities are in the areas of parallel architectures and algorithms, reliable and fault tolerant computing, real time systems, distributed systems, software engineering and artificial intelligence",space,1177
,filtered,core,Efficiently modeling neural networks on massively parallel computers,1993-01-01 00:00:00,core,https://core.ac.uk/download/pdf/42808011.pdf,,"Neural networks are a very useful tool for analyzing and modeling complex real world systems. Applying neural network simulations to real world problems generally involves large amounts of data and massive amounts of computation. To efficiently handle the computational requirements of large problems, we have implemented at Los Alamos a highly efficient neural network compiler for serial computers, vector computers, vector parallel computers, and fine grain SIMD computers such as the CM-2 connection machine. This paper describes the mapping used by the compiler to implement feed-forward backpropagation neural networks for a SIMD (Single Instruction Multiple Data) architecture parallel computer. Thinking Machines Corporation has benchmarked our code at 1.3 billion interconnects per second (approximately 3 gigaflops) on a 64,000 processor CM-2 connection machine (Singer 1990). This mapping is applicable to other SIMD computers and can be implemented on MIMD computers such as the CM-5 connection machine. Our mapping has virtually no communications overhead with the exception of the communications required for a global summation across the processors (which has a sub-linear runtime growth on the order of O(log(number of processors)). We can efficiently model very large neural networks which have many neurons and interconnects and our mapping can extend to arbitrarily large networks (within memory limitations) by merging the memory space of separate processors with fast adjacent processor interprocessor communications. This paper will consider the simulation of only feed forward neural network although this method is extendable to recurrent networks",space,1178
,filtered,core,A Robotic System for Learning Visually-Driven Grasp Planning (Dissertation Proposal),1992-03-22 00:00:00,core,https://core.ac.uk/download/214170588.pdf,ScholarlyCommons,"We use findings in machine learning, developmental psychology, and neurophysiology to guide a robotic learning system\u27s level of representation both for actions and for percepts. Visually-driven grasping is chosen as the experimental task since it has general applicability and it has been extensively researched from several perspectives. An implementation of a robotic system with a gripper, compliant instrumented wrist, arm and vision is used to test these ideas. Several sensorimotor primitives (vision segmentation and manipulatory reflexes) are implemented in this system and may be thought of as the  innate  perceptual and motor abilities of the system.
Applying empirical learning techniques to real situations brings up such important issues as observation sparsity in high-dimensional spaces, arbitrary underlying functional forms of the reinforcement distribution and robustness to noise in exemplars. The well-established technique of non-parametric projection pursuit regression (PPR) is used to accomplish reinforcement learning by searching for projections of high-dimensional data sets that capture task invariants.
We also pursue the following problem: how can we use human expertise and insight into grasping to train a system to select both appropriate hand preshapes and approaches for a wide variety of objects, and then have it verify and refine its skills through trial and error. To accomplish this learning we propose a new class of Density Adaptive reinforcement learning algorithms. These algorithms use statistical tests to identify possibly  interesting  regions of the attribute space in which the dynamics of the task change. They automatically concentrate the building of high resolution descriptions of the reinforcement in those areas, and build low resolution representations in regions that are either not populated in the given task or are highly uniform in outcome.
Additionally, the use of any learning process generally implies failures along the way. Therefore, the mechanics of the untrained robotic system must be able to tolerate mistakes during learning and not damage itself. We address this by the use of an instrumented, compliant robot wrist that controls impact forces",space,1179
,filtered,core,Parallelization of Rocket Engine Simulator Software (PRESS),1998-01-01 00:00:00,core,https://core.ac.uk/download/pdf/42769102.pdf,,"We have outlined our work in the last half of the funding period. We have shown how a demo package for RESSAP using MPI can be done. However, we also mentioned the difficulties with the UNIX platform. We have reiterated some of the suggestions made during the presentation of the progress of the at Fourth Annual HBCU Conference. Although we have discussed, in some detail, how TURBDES/PUMPDES software can be run in parallel using MPI, at present, we are unable to experiment any further with either MPI or PVM. Due to X windows not being implemented, we are also not able to experiment further with XPVM, which it will be recalled, has a nice GUI interface. There are also some concerns, on our part, about MPI being an appropriate tool. The best thing about MPr is that it is public domain. Although and plenty of documentation exists for the intricacies of using MPI, little information is available on its actual implementations. Other than very typical, somewhat contrived examples, such as Jacobi algorithm for solving Laplace's equation, there are few examples which can readily be applied to real situations, such as in our case. In effect, the review of literature on both MPI and PVM, and there is a lot, indicate something similar to the enormous effort which was spent on LISP and LISP-like languages as tools for artificial intelligence research. During the development of a book on programming languages [12], when we searched the literature for very simple examples like taking averages, reading and writing records, multiplying matrices, etc., we could hardly find a any! Yet, so much was said and done on that topic in academic circles. It appears that we faced the same problem with MPI, where despite significant documentation, we could not find even a simple example which supports course-grain parallelism involving only a few processes. From the foregoing, it appears that a new direction may be required for more productive research during the extension period (10/19/98 - 10/18/99). At the least, the research would need to be done on Windows 95/Windows NT based platforms. Moreover, with the acquisition of Lahey Fortran package for PC platform, and the existing Borland C + + 5. 0, we can do work on C + + wrapper issues. We have carefully studied the blueprint for Space Transportation Propulsion Integrated Design Environment for the next 25 years [13] and found the inclusion of HBCUs in that effort encouraging. Especially in the long period for which a map is provided, there is no doubt that HBCUs will grow and become better equipped to do meaningful research. In the shorter period, as was suggested in our presentation at the HBCU conference, some key decisions regarding the aging Fortran based software for rocket propellants will need to be made. One important issue is whether or not object oriented languages such as C + + or Java should be used for distributed computing. Whether or not ""distributed computing"" is necessary for the existing software is yet another, larger, question to be tackled with",space,1180
,filtered,core,Thunderstorm Hypothesis Reasoner,1994-01-01 00:00:00,core,https://core.ac.uk/download/pdf/42787396.pdf,,"THOR is a knowledge-based system which incorporates techniques from signal processing, pattern recognition, and artificial intelligence (AI) in order to determine the boundary of small thunderstorms which develop and dissipate over the area encompassed by KSC and the Cape Canaveral Air Force Station. THOR interprets electric field mill data (derived from a network of electric field mills) by using heuristics and algorithms about thunderstorms that have been obtained from several domain specialists. THOR generates two forms of output: contour plots which visually describe the electric field activity over the network and a verbal interpretation of the activity. THOR uses signal processing and pattern recognition to detect signatures associated with noise or thunderstorm behavior in a near real time fashion from over 31 electrical field mills. THOR's AI component generates hypotheses identifying areas which are under a threat from storm activity, such as lightning. THOR runs on a VAX/VMS at the Kennedy Space Center. Its software is a coupling of C and FORTRAN programs, several signal processing packages, and an expert system development shell",space,1181
,filtered,core,Experiments in Neural-Network Control of a Free-Flying Space Robot,1995-03-01 00:00:00,core,https://core.ac.uk/download/pdf/42772710.pdf,,"Four important generic issues are identified and addressed in some depth in this thesis as part of the development of an adaptive neural network based control system for an experimental free flying space robot prototype. The first issue concerns the importance of true system level design of the control system. A new hybrid strategy is developed here, in depth, for the beneficial integration of neural networks into the total control system. A second important issue in neural network control concerns incorporating a priori knowledge into the neural network. In many applications, it is possible to get a reasonably accurate controller using conventional means. If this prior information is used purposefully to provide a starting point for the optimizing capabilities of the neural network, it can provide much faster initial learning. In a step towards addressing this issue, a new generic Fully Connected Architecture (FCA) is developed for use with backpropagation. A third issue is that neural networks are commonly trained using a gradient based optimization method such as backpropagation; but many real world systems have Discrete Valued Functions (DVFs) that do not permit gradient based optimization. One example is the on-off thrusters that are common on spacecraft. A new technique is developed here that now extends backpropagation learning for use with DVFs. The fourth issue is that the speed of adaptation is often a limiting factor in the implementation of a neural network control system. This issue has been strongly resolved in the research by drawing on the above new contributions",space,1182
,filtered,core,Neural networks: Application to medical imaging,1994-01-01 00:00:00,core,https://core.ac.uk/download/pdf/42781573.pdf,,"The research mission is the development of computer assisted diagnostic (CAD) methods for improved diagnosis of medical images including digital x-ray sensors and tomographic imaging modalities. The CAD algorithms include advanced methods for adaptive nonlinear filters for image noise suppression, hybrid wavelet methods for feature segmentation and enhancement, and high convergence neural networks for feature detection and VLSI implementation of neural networks for real time analysis. Other missions include (1) implementation of CAD methods on hospital based picture archiving computer systems (PACS) and information networks for central and remote diagnosis and (2) collaboration with defense and medical industry, NASA, and federal laboratories in the area of dual use technology conversion from defense or aerospace to medicine",space,1183
,filtered,core,A Robotic System for Learning Visually-Driven Grasp Planning (Dissertation Proposal),1992-03-22 00:00:00,core,https://core.ac.uk/download/76360605.pdf,ScholarlyCommons,"We use findings in machine learning, developmental psychology, and neurophysiology to guide a robotic learning system\u27s level of representation both for actions and for percepts. Visually-driven grasping is chosen as the experimental task since it has general applicability and it has been extensively researched from several perspectives. An implementation of a robotic system with a gripper, compliant instrumented wrist, arm and vision is used to test these ideas. Several sensorimotor primitives (vision segmentation and manipulatory reflexes) are implemented in this system and may be thought of as the  innate  perceptual and motor abilities of the system.
Applying empirical learning techniques to real situations brings up such important issues as observation sparsity in high-dimensional spaces, arbitrary underlying functional forms of the reinforcement distribution and robustness to noise in exemplars. The well-established technique of non-parametric projection pursuit regression (PPR) is used to accomplish reinforcement learning by searching for projections of high-dimensional data sets that capture task invariants.
We also pursue the following problem: how can we use human expertise and insight into grasping to train a system to select both appropriate hand preshapes and approaches for a wide variety of objects, and then have it verify and refine its skills through trial and error. To accomplish this learning we propose a new class of Density Adaptive reinforcement learning algorithms. These algorithms use statistical tests to identify possibly  interesting  regions of the attribute space in which the dynamics of the task change. They automatically concentrate the building of high resolution descriptions of the reinforcement in those areas, and build low resolution representations in regions that are either not populated in the given task or are highly uniform in outcome.
Additionally, the use of any learning process generally implies failures along the way. Therefore, the mechanics of the untrained robotic system must be able to tolerate mistakes during learning and not damage itself. We address this by the use of an instrumented, compliant robot wrist that controls impact forces",space,1184
,filtered,core,Performance results of cooperating expert systems in a distributed real-time monitoring system,1994-01-01 00:00:00,core,https://core.ac.uk/download/pdf/42782006.pdf,,"There are numerous definitions for real-time systems, the most stringent of which involve guaranteeing correct system response within a domain-dependent or situationally defined period of time. For applications such as diagnosis, in which the time required to produce a solution can be non-deterministic, this requirement poses a unique set of challenges in dynamic modification of solution strategy that conforms with maximum possible latencies. However, another definition of real time is relevant in the case of monitoring systems where failure to supply a response in the proper (and often infinitesimal) amount of time allowed does not make the solution less useful (or, in the extreme example of a monitoring system responsible for detecting and deflecting enemy missiles, completely irrelevant). This more casual definition involves responding to data at the same rate at which it is produced, and is more appropriate for monitoring applications with softer real-time constraints, such as interplanetary exploration, which results in massive quantities of data transmitted at the speed of light for a number of hours before it even reaches the monitoring system. The latter definition of real time has been applied to the MARVEL system for automated monitoring and diagnosis of spacecraft telemetry. An early version of this system has been in continuous operational use since it was first deployed in 1989 for the Voyager encounter with Neptune. This system remained under incremental development until 1991 and has been under routine maintenance in operations since then, while continuing to serve as an artificial intelligence (AI) testbed in the laboratory. The system architecture has been designed to facilitate concurrent and cooperative processing by multiple diagnostic expert systems in a hierarchical organization. The diagnostic modules adhere to concepts of data-driven reasoning, constrained but complete nonoverlapping domains, metaknowledge of global consequences of anomalous data, hierarchical reporting of problems that extend beyond a single domain, and shared responsibility for problems that overlap domains. The system enables efficient diagnosis of complex system failures in real-time environments with high data volumes and moderate failure rates, as indicated by extensive performance measurements",space,1185
,filtered,core,Prototype space station automation system delivered and demonstrated at NASA,1987-01-01 00:00:00,core,https://core.ac.uk/download/pdf/42833627.pdf,,"The Automated Subsystem Control for Life Support System (ASCLSS) program has successfully developed and demonstrated a generic approach to the automation and control of Space Station subsystems. The hierarchical and distributed real time controls system places the required controls authority at every level of the automation system architecture. As a demonstration of the automation technique, the ASCLSS system automated the Air Revitalization Group (ARG) of the Space Station regenerative Environmental Control and Life Support System (ECLSS) using real-time, high fidelity simulators of the ARG processess. This automation system represents an early flight prototype and an important test bed for evaluating Space Station controls technology including future application of ADA software in real-time control and the development and demonstration of embedded artificial intelligence and expert systems (AI/ES) in distributed automation and controls systems",space,1186
,filtered,core,Efficient techniques for soft tissue modeling and simulation,,core,https://core.ac.uk/download/76971.pdf,,"Performing realistic deformation simulations in real time is a challenging problem in computer graphics. Among numerous proposed methods including Finite Element

Modeling and ChainMail, we have implemented a mass spring system because of its acceptable accuracy and speed. Mass spring systems have, however, some drawbacks such as, the determination of simulation coefficients with their iterative nature. Given the correct parameters, mass spring systems can accurately simulate tissue deformations but choosing parameters that capture nonlinear deformation behavior is extremely difficult. Since most of the applications require a large number of elements

i. e. points and springs in the modeling process it is extremely difficult to reach realtime performance with an iterative method. We have developed a new parameter

identification method based on neural networks. The structure of the mass spring system is modified and neural networks are integrated into this structure. The input

space consists of changes in spring lengths and velocities while a ""teacher"" signal is chosen as the total spring force, which is expressed in terms of positional changes and

applied external forces. Neural networks are trained to learn nonlinear tissue characteristics represented by spring stiffness and damping in the mass spring algorithm. The learning algorithm is further enhanced by an adaptive learning rate, developed particularly for mass spring systems. In order to avoid the iterative approach in deformation simulations we have developed a new deformation algorithm. This algorithm defines the relationships between points and springs and specifies a set of rules on spring movements and deformations. These rules result in a deformation surface, which is called the search space. The

deformation algorithm then finds the deformed points and springs in the search space with the help of the defined rules. The algorithm also sets rules on each element i. e.

triangle or tetrahedron so that they do not pass through each other. The new algorithm is considerably faster than the original mass spring systems algorithm and provides an

opportunity for various deformation applications.

We have used mass spring systems and the developed method in the simulation of craniofacial surgery. For this purpose, a patient-specific head model was generated

from MRI medical data by applying medical image processing tools such as, filtering, the segmentation and polygonal representation of such model is obtained using a

surface generation algorithm. Prism volume elements are generated between the skin and bone surfaces so that different tissue layers are included to the head model. Both

methods produce plausible results verified by surgeons",space,1187
,filtered,core,Instrumental Radiation Patterns as Models for Corpus-Based Spatial Sound Synthesis: Cosmologies for Piano and 3D Electronics,,core,https://core.ac.uk/download/435142828.pdf,,"The Cosmologies project aims to situate the listener inside a virtual grand piano by enabling computer processes to learn from the spatial presence of the live instrument and performer. We propose novel techniques that leverage mea- surements of natural acoustic phenomena to inform spatial sound composition and synthesis. Measured radiation pat- terns of acoustic instruments are applied interactively in response to a live input to synthesize spatial forms in real time. We implement this with software tools for the first time connecting audio descriptor analysis and corpus-based syn- thesis to spatialization using Higher-Order Ambisonics and machine learning. The resulting musical work, Cosmologies for piano and 3D electronics, explodes the space inside the grand piano out to the space of the concert hall, allowing the listener to experience its secret inner life",space,1188
,filtered,core,Automatic Detection of Electric Power Troubles (ADEPT),1988-01-01 00:00:00,core,https://core.ac.uk/download/pdf/42829070.pdf,,"Automatic Detection of Electric Power Troubles (A DEPT) is an expert system that integrates knowledge from three different suppliers to offer an advanced fault-detection system. It is designed for two modes of operation: real time fault isolation and simulated modeling. Real time fault isolation of components is accomplished on a power system breadboard through the Fault Isolation Expert System (FIES II) interface with a rule system developed in-house. Faults are quickly detected and displayed and the rules and chain of reasoning optionally provided on a laser printer. This system consists of a simulated space station power module using direct-current power supplies for solar arrays on three power buses. For tests of the system's ablilty to locate faults inserted via switches, loads are configured by an INTEL microcomputer and the Symbolics artificial intelligence development system. As these loads are resistive in nature, Ohm's Law is used as the basis for rules by which faults are located. The three-bus system can correct faults automatically where there is a surplus of power available on any of the three buses. Techniques developed and used can be applied readily to other control systems requiring rapid intelligent decisions. Simulated modeling, used for theoretical studies, is implemented using a modified version of Kennedy Space Center's KATE (Knowledge-Based Automatic Test Equipment), FIES II windowing, and an ADEPT knowledge base",space,1189
,filtered,core,Artificial intelligence and the space station software support environment,1986-01-01 00:00:00,core,https://core.ac.uk/download/pdf/42829617.pdf,,"In a software system the size of the Space Station Software Support Environment (SSE), no one software development or implementation methodology is presently powerful enough to provide safe, reliable, maintainable, cost effective real time or near real time software. In an environment that must survive one of the most harsh and long life times, software must be produced that will perform as predicted, from the first time it is executed to the last. Many of the software challenges that will be faced will require strategies borrowed from Artificial Intelligence (AI). AI is the only development area mentioned as an example of a legitimate reason for a waiver from the overall requirement to use the Ada programming language for software development. The limits are defined of the applicability of the Ada language Ada Programming Support Environment (of which the SSE is a special case), and software engineering to AI solutions by describing a scenario that involves many facets of AI methodologies",space,1190
,filtered,core,"Monterey, California.  Naval Postgraduate School",2017-03-01 00:00:00,core,data consolidation of disparate procurement data sources for correlated performance-based acquisition decision support,https://core.ac.uk/download/343432594.pdf,"Frank Kendall, then Under Secretary of Defense for Acquisition, Technology and Logistics, released the first defense acquisition system performance report in June 2013. This report focused primarily on performance related to the collective outcomes of Major Defense Acquisition Programs (MDAPs), but additionally explored various descriptive dimensions and acquisition approaches of the same (Kendall, 2013). Each annual report builds on the work previously conducted, and focuses on data-driven analysis relying on statistical techniques to identify trends that improve the defense acquisition communityﾒs insights into how contract incentives are motivating better contractor/vendor performance (Kendal, 2016). Nevertheless, large amounts of data (in modern jargon, ﾓBig Dataﾔ) are now available for research in the area of defense acquisition. Over the past several years, changes in electronic commerce have increased the amounts of both structured and unstructured data availableﾗboth in runtime and archived environments. This electronic data, from a variety of different acquisition agencies, can be obtained by a variety of means and used for a multitude of purposes (Snider et al., 2014). Traditional statistical and trend analysis methods thus far have been primarily relied upon to explore trends and test metrics in the sets of acquisition data at hand. Sometimes, spreadsheets of linear regression correlation are employed, or, in some more modern applications, multivariate structural equation models via scientific applications such as SPSS and AMOS are leveraged for their ability to evaluate complex variable relationships, such as nested or recursive if-then patterns (Byrne, 2016). However, not only are todayﾒs modern datasets large in magnitude, they are also large in variety and complexity (Gartner, 2013). Furthermore, to address this state of data, new statistical modeling techniques, more powerful than before, have had to be created. This is due to the older methods finding difficulty with some of the size problems Big Data represents, such as privacy and security concerns (Parms, 2017). Thankfully, computer power necessary to employ the modern techniques is less expensive today, the software near free, and the storage capacities available now yield bewildering capacities at a fingertip, and with amazingly fast access speed. In fact, these performance parameters appear to continue along a Mooreﾒs trend line against critical opposition (Magee, Basnet, Funk, & Benson, 2015). Presently, one of the more interesting of the new statistical modeling techniques is neural network algorithm machine learning. Neural network modeling involves utilizing a ﾓpowerful computational data model that is able to capture and represent input/output relationships.ﾔ This model was developed out of the desire to create artificial intelligence systems capable of completing functions that were previously executed solely by the human brain. One benefit of using neural network modeling lies with its capacity to display and comprehend both linear and non-linear relationships from the data to which it is supplied (NeuroSolutions, 2015). Research Question Because ﾓBig Dataﾔ is present in the Defense Acquisition Business space, and, because the demand to critically understand real cause-and-effect relationships between variables within that data is persistent from the Acquisition community, this paperﾒs research question is, Can a neural network modeling technique be confidently relied upon to meaningfully explore variable relationships within acquisition business datasets? Because, if it is, then any question may be reasonably asked by anyone of such a dataset; and, via the neural network-enabled tool, the answers they receive will come with scientific statistical confidence as to whether they can be trusted as interesting or useful answers.1 In order to explore this research question, the study opted to use business data on contractor performance and attempted to isolate predictive variables from past performance information predictive of good performance.Naval Postgraduate School Acquisition Research Progra",e-commerce,1191
,filtered,core,,2008-03-19 00:00:00,core,corroborating emotion theory with role theory and agent technology: a framework for designing emotional agents as motivational tutoring entities,https://core.ac.uk/download/33799208.pdf,"Nowadays, more and more applications require systems that can interact with humans. Agents can be perceived as computing services that humans, or even other agents, can request in order to accomplish their tasks. Some services may be simple and others rather complex. A way to determine the best agents (services) to be implemented is to identify who the actors are in the object of study, which roles they play, and (if possible) what kind of knowledge they use. 
Socially Intelligent Agents (SIAs) are agent systems that are able to connect and interface with humans, i.e. robotic or computational systems that show aspects of human-style social intelligence. In addition to their relevance in application areas such as e-commerce and entertainment, building artefacts in software and hardware has been recognized as a powerful tool for establishing a science of social minds which is a constructive approach toward understanding social intelligence in humans and other animals.
Social intelligence in humans and other animals has a number of fascinating facets and implications for the design of SIAs. Human beings are biological agents that are embodied members of a social environment and are autobiographic agents who have a unique personality. They are situated in time and space and interpret new experiences based on reconstructions of previous experiences. Due to their physical embodiment, they have a unique perspective on the world and a unique history: an autobiography. Also, humans are able to express and recognize emotions, that are important in regulating individual survival and problem-solving as well as social interactions.
Like artificial intelligence research trend, SIA research trend can be pursued with different goals in mind. A deep AI approach seeks to simulate real social intelligence and processes. A shallow AI approach, which will be highlighted also within this thesis, aims to create artefacts that are not socially intelligent per se, but rather appear socially intelligent to a given user. The shallow approach does not seek to create social intelligence unless it is meaningful social intelligence vis-à-vis some user situation
In order to develop believable SIAs we do not have to know how beliefs-desires and intentions actually relate to each other in the real minds of the people. If one wants to create the impression of an artificial social agent driven by beliefs and desires, it is enough to draw on investigations on how people with different cultural background, develop and use theories of mind to understand the behaviours of others. Therefore, SIA technology needs to model the folk-theory reasoning rather than the real thing. To a shallow AI approach, a model of mind based on folk-psychology is as valid as one based on cognitive theory. 
Distance education is understood as online learning that is technology-based training which encompasses both computer-assisted and Web-based training. These systems, which appear to offer something for everyone at any time, in any place, do not always live up to the great promise they offer.
The usage of social intelligent agents in online learning environments can enable the design of “enhanced-learning environments” that allow for the development and the assessment of social competences as well as the common professional competences. 

Within this thesis it is shown how to corroborate affective theory with role theory with agent technology in a synchronous virtual environment in order to overcome several inconveniences of distance education systems. This research embraces also the shallow approach of SIA and aims to provide the first steps of a method for creating a believable life-like tutor agent which can partially replace human-teachers and assist the students in the process of learning. The starting point for this research came from the fact: anxious, angry or depressed students do not learn; people in these conditions do not absorb information efficiently, consequentially it is an illusion to think that learning environments that do not consider motivational and emotional factors are adequate",e-commerce,1192
,filtered,core,"Univerzita Karlova, Fakulta sociálních věd",2020-01-01 00:00:00,core,předpovědi spotřebitelského chování v eshopech,,"This thesis analyzes behavior of customers on an e-commerce website in order to predict whether the customer is willing to buy something or is just window shopping. In addition the secondary model predicts, if the customer is going to leave the e-commerce website in next few clicks. To answer this questions different frameworks are tested. The base model used is the Logit model. The base model is compared with more sophisticated methods in machine learning - with neural networks. The best results were yielded by Recurrent neural network - the Long Short-Term Memory (LSTM). The results of the analysis confirm importance of the click stream data and calculated features that track user behavior on the e-commerce website, type of the page (product, category, information), product variance and category variance. The thesis emphasizes practical implications of this models. Two possible practical implementations are presented. The models are tested in novel ways to see how would they perform if implemented on the real e-commerce website.Tento článek analyzuje chování zákazníků na webových stránkách elektron- ického obchodu s cílem předpovědět, zda je zákazník ochoten si něco koupit nebo se jen dívá. Kromě toho sekundární model předpovídá, zda zákazník během několika málo kliknutí opustí web elektronického obchodu. Pro zod- povězení těchto otázek jsou testovány různé metody řešení. Použitý základní model je Logit. Základní model je porovnán se sofistikovanějšími metodami strojového učení - s neuronovými sítěmi. Nejlepší výsledky byly dosaženy po- mocí rekurentní neuronové sítě - Long Short-Term Memory (LSTM). Výsledky analýzy potvrzují důležitost údajů o tocích kliknutí a napočtených proměnných, které sledují chování uživatelů na webové stránce elektronického obchodu, typ stránky (produkt, kategorie, informace), variance produktu a varianci kate- gorie. Práce zdůrazňuje praktické využití těchto modelů. Jsou představeny dvě možné praktické implementace. Modely jsou testovány novými způsoby, aby se zjistilo, jak by fungovaly, kdyby byly implementovány na skutečné we- bové stránce elektronického obchodu.Institute of Economic StudiesInstitut ekonomických studiíFakulta sociálních vědFaculty of Social Science",e-commerce,1193
,filtered,core,'Information Bulletin on Variable Stars (IBVS)',2020-01-01 00:00:00,core,assessing the potential of implementing blockchain in supply chains using agent-based simulation and deep learning,,"In this decade, with the rise of data science accompanying the growth of e-commerce, many technologies have been developed. An example of these technologies is Blockchain, which has appeared to overcome security problems potentially. This research assesses Blockchain\u27s implementation in supply chains through a methodology that uses deep learning and agent-based simulation. A case study was utilized to observe and validate research developments. The unique method predicts intrusions by using deep learning, and agent-based modeling reproduces artificial but convincing agents (e.g., customers, companies, hackers, and cyber pirates) in a computer-generated market. Trust and other relationships are systematically captured to represent Blockchain additions. Once again, the agent-based simulation model\u27s environment permits hypothetical interactions and emergent features by coordinating supply and demand for business-to-consumer e-commerce events. The case study based on a real environment shows that the proposed method can determine the feasibility of the business model and Blockchain implementation\u27s potential contributions",e-commerce,1194
,filtered,core,"Monterey, California.  Naval Postgraduate School",2017-03-01 00:00:00,core,data consolidation of disparate procurement data sources for correlated performance-based acquisition,https://core.ac.uk/download/343439754.pdf,"Frank Kendall, then Under Secretary of Defense for Acquisition, Technology and Logistics, released the first defense acquisition system performance report in June 2013. This report focused primarily on performance related to the collective outcomes of Major Defense Acquisition Programs (MDAPs), but additionally explored various descriptive dimensions and acquisition approaches of the same (Kendall, 2013). Each annual report builds on the work previously conducted, and focuses on data-driven analysis relying on statistical techniques to identify trends that improve the defense acquisition communityﾒs insights into how contract incentives are motivating better contractor/vendor performance (Kendal, 2016). Nevertheless, large amounts of data (in modern jargon, ﾓBig Dataﾔ) are now available for research in the area of defense acquisition. Over the past several years, changes in electronic commerce have increased the amounts of both structured and unstructured data availableﾗboth in runtime and archived environments. This electronic data, from a variety of different acquisition agencies, can be obtained by a variety of means and used for a multitude of purposes (Snider et al., 2014). Traditional statistical and trend analysis methods thus far have been primarily relied upon to explore trends and test metrics in the sets of acquisition data at hand. Sometimes, spreadsheets of linear regression correlation are employed, or, in some more modern applications, multivariate structural equation models via scientific applications such as SPSS and AMOS are leveraged for their ability to evaluate complex variable relationships, such as nested or recursive if-then patterns (Byrne, 2016). However, not only are todayﾒs modern datasets large in magnitude, they are also large in variety and complexity (Gartner, 2013). Furthermore, to address this state of data, new statistical modeling techniques, more powerful than before, have had to be created. This is due to the older methods finding difficulty with some of the size problems Big Data represents, such as privacy and security concerns (Parms, 2017). Thankfully, computer power necessary to employ the modern techniques is less expensive today, the software near free, and the storage capacities available now yield bewildering capacities at a fingertip, and with amazingly fast access speed. In fact, these performance parameters appear to continue along a Mooreﾒs trend line against critical opposition (Magee, Basnet, Funk, & Benson, 2015). Presently, one of the more interesting of the new statistical modeling techniques is neural network algorithm machine learning. Neural network modeling involves utilizing a ﾓpowerful computational data model that is able to capture and represent input/output relationships.ﾔ This model was developed out of the desire to create artificial intelligence systems capable of completing functions that were previously executed solely by the human brain. One benefit of using neural network modeling lies with its capacity to display and comprehend both linear and non-linear relationships from the data to which it is supplied (NeuroSolutions, 2015). Research Question Because ﾓBig Dataﾔ is present in the Defense Acquisition Business space, and, because the demand to critically understand real cause-and-effect relationships between variables within that data is persistent from the Acquisition community, this paperﾒs research question is, Can a neural network modeling technique be confidently relied upon to meaningfully explore variable relationships within acquisition business datasets? Because, if it is, then any question may be reasonably asked by anyone of such a dataset; and, via the neural network-enabled tool, the answers they receive will come with scientific statistical confidence as to whether they can be trusted as interesting or useful answers.1 In order to explore this research question, the study opted to use business data on contractor performance and attempted to isolate predictive variables from past performance information predictive of good performance.Naval Postgraduate School Acquisition Research Progra",e-commerce,1195
,filtered,core,DigitalCommons@University of Nebraska - Lincoln,2017-05-01 00:00:00,core,"can public diplomacy survive the internet? bots, echo chambers, and disinformation",https://core.ac.uk/download/428380032.pdf,"Report from a meeting held on the topic of disinformation, the Internet, and public diplomacy held at the Hoover Institution, Stanford University, in 2017.
Executive Summary
Scientific progress continues to accelerate, and while we’ve witnessed a revolution in communication technologies in the past ten years, what proceeds in the next ten years may be far more transformative. It may also be extremely disruptive, challenging long held conventions behind public diplomacy (PD) programs and strategies. In order to think carefully about PD in this ever and rapidly changing communications space, the Advisory Commission on Public Diplomacy (ACPD) convened a group of private sector, government, and academic experts at Stanford University’s Hoover Institution to discuss the latest trends in research on strategic communication in digital spaces. The results of that workshop, refined by a number of follow-on interviews and discussions, are included in this report. I encourage you to read each of the fourteen essays that follow, which are divided into three thematic sections: Digital’s Dark Side, Disinformation, and Narratives.
Digital’s Dark Side focuses on the emergence of social bots, artificial intelligence, and computational propaganda. Essays in this section aim to raise awareness regarding how technology is transforming the nature of digital communication, offer ideas for competing in this space, and raise a number of important policy and research questions needing immediate attention. The Disinformation section confronts Oxford English Dictionary’s 2016 word of the year – “post-truth” – with a series of compelling essays from practitioners, a social scientist, and philosopher on the essential roles that truth and facts play in a democratic society. Here, theory, research, and practice neatly align, suggesting it is both crucial and effective to double-down on fact-checking and evidence-based news and information programming in order to combat disinformation campaigns from our adversaries. The Narrative section concludes the report by focusing on how technology and facts are ultimately part of, and dependent on, strategic narratives. Better understanding how these narratives form, and what predicts their likely success, is necessary to think through precisely how PD can, indeed, survive the Internet. Below are some key takeaways from the report.
In Defense of Truth
• We are not living in a “post-truth” society. Every generation tends to think that the current generation is less honest than the previous generation. This is an old human concern, and should be seen today as a strategic narrative (see Hancock, p. 49; Roselle, p. 77). Defending the value and search for truth is crucial. As Jason Stanley notes (p. 71), “without truth, there is just power.”
• Humans are remarkably bad at detecting deception. Studies show that people tend to trust what others say, an effect called the truth bias. This bias is actually quite rational—most of the messages that a person encounters in a day are honest, so being biased toward the truth is almost always the correct response (see Hancock, p.49).
• At the same time people are also continuously evaluating the validity of their understanding of the world. This process is called “epistemic vigilance,” a continuous process checking that the information that a person believes they know about the world is accurate. While we have a difficult time detecting deception from interpersonal cues, people can detect lies when they have the time, resources, and motivation. Lies are often discovered through contradicting information from a third source, or evidence that challenges a deceptive account (see Hancock, p. 49).
• Fact checking can be effective, even in hyper-partisan settings (see Porter, p. 55), and is crucial for sustained democratic dialogue (Bennett, p. 61; Stanley, p. 71). Moreover, it is possible, using digital tools, to detect and effectively combat disinformation campaigns in real time (Henick and Walsh, p. 65).
Computational Propaganda
• Computational propaganda refers to the coordinated use of social media platforms, autonomous agents and big data directed towards the manipulation of public opinion.
• Social media bots (or “web robots”) are the primary tools used in the dissemination of computational propaganda. In their most basic form, bots provide basic answers to simple questions, publish content on a schedule or disseminate stories in response to triggers (e.g. breaking news). Bots can have a disproportionate impact because it is easy to create a lot of them and they can post a high-volume content at a high frequency (see Woolley, p.13).
• Political bots aim to automate political engagement in an attempt to manipulate public opinions. They allow for massive amplification of political views and can empower a small group of people to set conversation agenda’s online. Political bots are used over social media to manufacture trends, game hashtags, megaphone particular content, spam opposition and attack journalists. The noise, spam and manipulation inherent in many bot deployment techniques threaten to disrupt civic conversations and organization worldwide (see Chessen, p.19).
• Advances in artificial intelligence (AI) – an evolving constellation of technologies enabling computers to simulate cognitive processes – will soon enable highly persuasive machine-generated communications. Imagine an automated system that uses the mass of online data to infer your personality, political preferences, religious affiliation, demographic data and interests. It knows which news websites and social media platforms you frequent and it controls multiple user accounts on those platforms. The system dynamically creates content specifically designed to plug into your particular psychological frame and achieve a particular outcome (see Chessen, p. 39).
• Digital tools have tremendous advantages over humans. Once an organization creates and configures a sophisticated AI bot, the marginal cost of running it on thousands or millions of user accounts is relatively low. They can operate 24/7/365 and respond to events almost immediately. AI bots can be programmed to react to certain events and create content at machine speed, shaping the narrative almost immediately. This is critical in an information environment where the first story to circulate may be the only one that people recall, even if it is untrue (see Chessen, p. 39).
• PD practitioners need to consider the question of how they can create and sustain meaningful conversations and engagements with audiences if the mediums typically relied upon are becoming less trusted, compromised and dominated by intelligent machines.
• Challenging computational propaganda should include efforts to ensure the robustness and integrity of the marketplace of information online. Defensively, this strategy would focus on producing patterns of information exchange among groups that would make them difficult to sway using techniques of computational propaganda. Offensively, the strategy would seek to distribute the costs of counter-messaging broadly, shaping the social ecosystem to enable alternative voices to effectively challenge campaigns of misinformation (see Hwang, p. 27). In the persuasive landscape formed by social media and computational propaganda, it may be at times more effective to build tools, rather than construct a specific message.
• Practitioners are not alone in their concern about the escalating use of social bots by adversarial state actors. The private sector is, too. Social media platforms see this trend as a potentially existential threat to their business models, especially if the rise of bots and computational propaganda weakens users’ trust in the integrity of the platforms themselves. Coordination with private sector is key, as their policies governing autonomous bots will adapt and, thus, shape what is and isn’t feasible online.
Moving Past Folk Theories
• Folk theories, or how people think a particular process works, are driving far too many digital strategies. One example of a folk theory is in the prevalence of echo chambers online, or the idea that people are increasingly digitally walled off from one another, engaging only with content that fits cognitive predispositions and preferences.
• Research suggests that the more users rely on digital platforms (e.g. Twitter and Facebook) for their news and information, the more exposure they have to a multitude of sources and stories. This remains true even among partisans (though to a lesser extent than non-partisans). It turns out we haven’t digitally walled ourselves off after all (see Henick and Walsh, p. 65).
• Despite increased exposure to a pluralistic media ecosystem, we are becoming more and more ideological and partisan, and becoming more walled off at the interpersonal and physical layers. For example, marriages today are twice as likely to be between two people with similar political views than they were in 1960.
• Understanding this gap between a robustly diverse news environment and an increasingly “siloed” physical environment is crucial to more effectively engaging with target audiences around the world. Interpersonal and in-person engagement, including exchange programs, remain crucial for effective PD moving forward (see Wharton, p. 7).
• Despite this growing ideological divide, people are increasingly willing to trust one another, even complete strangers, when their goals are aligned (see the sharing economy, for example). This creates interesting opportunities for PD practitioners. Targeting strategies based on political attitudes or profiles may overshadow the possibility of aligned goals on important policy and social issues (see Hancock, p. 49).
Rethinking Our Digital Platforms and Metrics
Virality – the crown jewel in the social media realm – is overemphasized often at the expense of more important metrics like context and longevity. Many of the metrics used to measure the effectiveness of social media campaigns are vulnerable to manipulation, and more importantly, don’t measure engagement in any meaningful way. These metrics were built for an industry reliant on advertising for revenue generation, and as a result, may not be well-suited when applied to the context of PD (see Ford, p. 33; Woolley, p. 13).
• Overemphasizing certain metrics, such as reach or impressions, fails to account for the risks created by relaying on the same portals as other, less truthful and more nefarious actors. We need to be cautious and aware of the various ways in which the digital media business industries are shaping PD content, be aware of the risks, and think carefully about safeguarding the credibility U.S. Department of State PD programs operating in this space (see Wharton, p. 7; Ford, p. 33).
Strategic Narratives
• Strategic narratives—a means for political actors to construct a shared meaning of the past, present and future of politics in order to shape the behavior of other actors.” They provide the ideological backdrop for how audiences assess the meaning and significance of current events and breaking news. Put another way, they help people make sense of what would otherwise be a dizzying onslaught of news they are exposed to on a daily basis (see Roselle, p. 77; Kounalakis, p. 91).
• Crafting effective narratives require a genuine consensus--even if limited or temporary--on our policy priorities and their underlying values, as well as a detailed understanding and appreciation of local grievances and concerns about the related policy issue (see Wharton, p. 7; Roselle. P. 77). As such, effective strategic narratives must be mutually constructed.
• Rather than focusing on trending news topics and stories alone, we need to develop greater capacity to understand competing public narratives in foreign contexts and track how they adapt over time. Understanding distinctions between system (or governance), value, and identity narratives would allow PD practitioners to construct policy narratives that speak to, or at least acknowledge, the underlying pillars of belief in a given community (see Walker, p. 83; Roselle, p. 77).
• Every new administration creates new opportunities for foreign engagement. A shift towards a more transactional approach to PD, focused less on values but more on shared policy priorities, could allow for improved relations and cooperation with a number of countries previously hostile to American PD efforts and programs (see Kounalakis, p. 91)",e-commerce,1196
,filtered,core,,2004-07-06 00:00:00,core,"autonomous agents in bargaining games: an evolutionary investigation of fundamentals, strategies, and business applications",,"Bargaining is becoming increasingly important due to developments within the field of electronic commerce, especially the development of autonomous software agents. Software agents are programs which, given instructions from a user, are capable of autonomously and intelligently realise a given task. By means of such agents, the bargaining process can be automated, allowing products and services together with related conditions, such as warranty and delivery time, to be flexible and tuned to the individual preferences of the people concerned. In this theses we concentrate on both fundamental aspects of bargaining as well as business-related applications of automated bargaining using software agents. The fundamental part investigates bargaining outcomes within a stylised world, and the factors that influence these outcomes. This can provide insights for the production of software agents, strategies, and setting up bargaining rules for practical situations. We study these aspects using computational simulations of bargaining agents. Hereby we consider adaptive systems, i.e., where agents learn to adjust their bargaining strategy given past experience. This learning behaviour is simulated using evolutionary algorithms. These algorithms originate from the field of artificial intelligence, and are inspired by the biological theory of evolution. Originally, evolutionary algorithms were designed for solving optimisation problems, but they are now increasingly being used within economics for modelling human learning behaviour. Besides computational simulations, we also consider mathematical solutions from game theory for relatively simple cases. Game theory is mainly concerned with the “rational man”, that is, with optimal outcomes within an stylised setting (or game) where people act rationally. We use the game-theoretic outcomes to validate the computational experiments. The advantage of computer simulations is that less strict assumptions are necessary, and that more complex interactions that are closer to real-world settings can be investigated. First of all, we study a bargaining setting where two players exchange offers and counter offers, the so-called alternating-offers game. This game is frequently used for modelling bargaining about for instance the price of a product or service. It is also important, however, to allow other product- and service-related aspects to be negotiated, such as quality, delivery time, and warranty. This enables compromises by conceding on less important issues and demanding a higher value for relatively important aspects. This way, bargaining is less competitive and the resulting outcome can be mutually beneficial. Therefore, we investigate using computational simulations an extended version of the alternating-offers game, where multiple aspects are negotiated concurrently. Moreover, we apply game theory to validate the results of the computational experiments. The simulation shows that learning agents are capable of quickly finding optimal compromises, also called Pareto-efficient outcomes. In addition, we study the effects of time pressure that arise if negotiations are broken off with a small probability, for example due to external eventualities. In absence of time pressure and a maximum number of negotiation rounds, outcomes are very unbalanced: the player that has the opportunity to make a final offer proposes a take-it-or-leave-it offer in the last round, which leaves the other player with a deal that is only slightly better than no deal at all. With relatively high time pressure, on the other hand, the first offer is most important and almost all agreements are reached in the first round. Another interesting result is that the simulation outcomes after a long period of learning in general coincide with the results from game theory, in spite of the fact that the learning agents are not “rational”. In reality, not only the final outcome is important, but also other factors play a role, such as the fairness of an offer. Using the simulation we study the influence of such fairness norms on the bargaining outcomes. The fairness norms result in much more balanced outcomes, even with no time pressure, and seem to be closer outcomes in the real world. Negotiations are rarely isolated, but can also be influenced by external factors such as additional bargaining opportunities. We therefore also consider bargaining within a market-like setting, where both buyers and sellers can bargain with several opponents before reaching an agreement. The negotiations are executed consecutively until an agreement is reached or no more opportunities are available. Each bargaining game is reduced to a single round, where player 1 makes an offer and player 2 can only respond by rejecting or accepting this offer. Using an evolutionary simulation we study several properties of this market game. It appears that the outcomes depend on the information that is available to the players. If players are informed about the bargaining opportunities of their opponents, the first player in turn has the advantage and always proposes a take-it-or-leave-it deal that leaves the other player with a relatively poor outcome. This outcome is consistent with a game-theoretic analysis which we also present in this thesis. If this information is not available, a theoretical analysis is very hard. The evolutionary simulation, however, shows that in this case the responder obtains a better deal. This occurs because the first player can no longer anticipate the response of the other player, and therefore bids lower to avoid a disagreement. In this thesis, we additionally consider other factors that influence the outcomes of the market game, such as negotiation over multiple issues simultaneously, search costs, and break off probabilities. Besides fundamental issues, this thesis presents a number of business-related applications of automated bargaining, as well as generic bargaining strategies for agents that can be employed in related areas. As a first application, we introduce a framework where negotiation is used for recommending shops to customers, for example on a web page of an electronic shopping mall. Through a market-driven auction a relevant selection of shops is determined in a distributed fashion. This is achieved by selling a limited number of banner spaces in an electronic auction. For each arriving customer on the web page, shops can automatically place bids for this “customer attention space” through their shop agents. These software agents bid based on a customer profile, containing personal data of the customer, such as age, interests, and/or keywords in a search query. The shop agents are adaptive and learn, given feedback from the customers, which profiles to target and how much to bid in the auction. The highest bidders are then selected and displayed to the customer. The feasibility of this distributed approach for matching shops to customers is demonstrated using an evolutionary simulation. Several customer models and auction mechanisms are studied, and we show that the market-based approach results in a proper selection of shops for the customers. Bargaining can be especially beneficial if not only the price, but other aspects are considered as well. This allows for example to customise products and services to the personal preferences of a user. We developed a system makes use of these properties for selling and personalising so-called information goods, such as news articles, software, and music. Using the alternating-offers protocol, a seller agent negotiates with several buyers simultaneously about a fixed price, a per-item price, and the quality of a bundle of information goods. The system is capable of taking into account important business-related conditions such as the fairness of the negotiation. The agents combine a search strategy and a concession strategy to generate offers in the negotiations. The concession strategy determines the amount the agent will concede each round, whereas the search strategy takes care of the personalisation of the offer. We introduce two search strategies in this thesis, and show through computer experiments that the use of these strategies by a buyer and seller agent, result in personalised outcomes, also when combined with various concession strategies. The search strategies presented here can be easily applied to other domains where personalisation is important. In addition, we also developed concession strategies for the seller agent that can be used in settings where a single seller agent bargains with several buyer agents simultaneously. Even if bargaining itself is bilateral (i.e., between two parties), a seller agent can actually benefit from the fact that several such negotiations occur concurrently. The developed strategies are focussed on domains where supply is flexible and can be adjusted to meet demand, like for information goods. We study fixed strategies, time-dependent strategies and introduce several auction-inspired strategies. Auctions are often used when one party negotiates with several opponents simultaneously. Although the latter strategies benefit from the advantages of auctions, the actual negotiation remains bilateral and consists of exchanging offers and counter offers. We developed an evolutionary simulation environment to evaluate the seller agent’s strategies. We especially consider the case where buyers are time-impatient and under pressure to reach agreements early. The simulations show that the auction-inspired strategies are able to obtain almost maximum profits from the negotiations, given sufficient time pressure of the buyers",e-commerce,1197
,filtered,core,Technická univerzita v Liberci,2008-01-01 00:00:00,core,data mining with clustering,,"Data mining is a new discipline lying at the interface of statistics, database technology, pattern 
recognition, machine learning, and other areas. It is concerned with the secondary analysis of lar-
ge databases in order to find previously unsuspected relationships which are of interest or value 
to the database owners. There are two keys to success in data mining. First is coming up with 
a precise formulation of the problem you are trying to solve. A focused statement usually results 
in the best payoff. The second key is using the right data. After choosing from the data available 
to you, or perhaps buying external data, you may need to transform and combine it in significant 
ways. New problems arise, partly as a consequence of the sheer size of the data sets involved, and 
partly because of issues of pattern matching. H
owever, since statistics 
provides the intellectual 
glue underlying the effort, it is important for statisticians to become involved. There are very real 
opportunities for statisticians to make significant contributions. 
The main definition of data mining and the special data mining tasks are mentioned in the first 
part of this paper. The data mining problem was also discussed in previous issues of E+M. One 
method (clustering) was chosen to be a subject of this article. 
One of the opportunities to gain knowledge from data is a use of clustering analysis. Clustering 
analysis belongs to unsupervised methods of data mining. We put here a focus on this method. 
Some basic principles are described in the second part of this paper. This method is examined on 
two examples from the marketing field. In the first example is used software Statgraphics 5.0Plus 
(www.statgraphics.com) to solve clustering problem (nearest neighbour algorithm and Eucleidi-
an distance), and in the second example is used Statistica 6.0Cz software (from Statoft, Inc., 
www.statsoft.com or www.statsoft.cz).
But the building models is only one step in knowledge discovery. It is vital to properly collect and 
prepare the data, and to check the models against the real world. The „best“ model is often found 
after building models of several different types, or by trying different technologies or algorithms",e-commerce,1198
,filtered,core,,2015-03-17 00:00:00,core,natural language sales assistant: a web-based dialog system for online sales,,"This paper describes a web-based dialog system – Natural Language Sales Assistant (NLSA)  – that helps users find relevant information about products and services in e-commerce sites. The system leverages technologies in natural language processing and human computer interaction to create a faster and more intuitive way of interacting with websites. By combining traditional AI rule-based technology with taxonomy mapping, the system is able to accommodate both customer and business requirements. Our user studies have demonstrated that, in the context of e-commerce, users preferred the natural language enabled navigation over menu-driven navigation (79 % to 21 % users). In addition, compared to a menu driven system, the average number of clicks used in the natural language system was reduced by 63.2 % and the average time was reduced by 33.3%. The NLSA system is currently deployed by IBM as a live pilot and we are collecting real user feedback. We believe that conversational interfaces like that of NLSA offer the ultimate personalization and can greatly enhance the user experience for websites. ",e-commerce,1199
,filtered,core,HAL CCSD,2020-01-29 00:00:00,core,intégration des séries temporelles dans les a/b-tests,,"International audienceAn A/B test evaluates the impact of a new technology by running it in a real production environment and testing its performance on a set of items. Recently, promising new methods are optimizing A/B tests with dynamic allocation. They allow for a quicker result regarding which variation (A or B) is the best, saving money for the user. However, dynamic allocation by traditional methods requires certain assumptions, which are not always verified in reality. This is mainly due to the fact that the populations tested are not homogeneous. This document reports on the new reinforcement learning methodology which has been deployed by the commercial A/B testing platform AB Tasty. We provide a new method that not only builds homogeneous groups for a user, but also allows to find the best variation for these groups in a short period of time. This paper provides numerical results on AB Tasty data, but also on public data sets, to demonstrate an improvement in A/B testing over traditional methods.Récemment en e-commerce, de nouvelles méthodes promet-teuses optimisent les A/B-Tests en utilisant une allocation dynamique des items aux variations permettant ainsi de déterminer plus rapidement la meilleure variation et donc de réduire les coûts du test. Cependant, ces méthodes qui peuvent s'apparenter à un apprentissage par renforcement, restent limitées à des données statiques et ne peuvent prendre en compte des données temporelles évolutives. Nous présentons ici deux nouvelles méthodes basées sur une approche commune, qui permettent d'intégrer des séries temporelles dans le profil des visiteurs. Nous montrons dans cet article qu'elles améliorent l'allocation dynamique des A/B-Tests en présentant des résultats obtenus sur des données issues de tests réels",e-commerce,1200
,filtered,core,,2001-01-01 00:00:00,core,2001. natural language sales assistant - a web-based dialog system for online sales,,"This paper describes a web-based dialog system – Natural Language Sales Assistant (NLSA)  – that helps users find relevant information about products and services in ecommerce sites. The system leverages technologies in natural language processing and human computer interaction to create a faster and more intuitive way of interacting with websites. By combining traditional AI rulebased technology with taxonomy mapping, the system is able to accommodate both customer and business requirements. Our user studies have demonstrated that, in the context of e-commerce, users preferred the natural language enabled navigation over menu-driven navigation (79 % to 21 % users). In addition, compared to a menu driven system, the average number of clicks used in the natural language system was reduced by 63.2 % and the average time was reduced by 33.3%. The NLSA system is currently deployed by IBM as a live pilot and we are collecting real user feedback. We believe that conversational interfaces like that of NLSA offer the ultimate personalization and can greatly enhance the user experience for websites",e-commerce,1201
,filtered,core,,2021-01-01 00:00:00,core,transfer learning strategies for credit card fraud detection.,https://core.ac.uk/download/491346803.pdf,"Credit card fraud jeopardizes the trust of customers in e-commerce transactions. This led in recent years to major advances in the design of automatic Fraud Detection Systems (FDS) able to detect fraudulent transactions with short reaction time and high precision. Nevertheless, the heterogeneous nature of the fraud behavior makes it difficult to tailor existing systems to different contexts (e.g. new payment systems, different countries and/or population segments). Given the high cost (research, prototype development, and implementation in production) of designing data-driven FDSs, it is crucial for transactional companies to define procedures able to adapt existing pipelines to new challenges. From an AI/machine learning perspective, this is known as the problem of transfer learning. This paper discusses the design and implementation of transfer learning approaches for e-commerce credit card fraud detection and their assessment in a real setting. The case study, based on a six-month dataset (more than 200 million e-commerce transactions) provided by the industrial partner, relates to the transfer of detection models developed for a European country to another country. In particular, we present and discuss 15 transfer learning  techniques (ranging from naive baselines to state-of-the-art and new approaches), making a critical and quantitative comparison in terms of precision for different transfer scenarios. Our contributions are twofold: (i) we show that the accuracy of many transfer methods is strongly dependent on the number of labeled samples in the target domain and (ii) we propose an ensemble solution to this problem based on self-supervised and semi-supervised domain adaptation classifiers. The thorough experimental assessment shows that this solution is both highly accurate and hardly sensitive to the number of labeled samples",e-commerce,1202
,filtered,core,,2014-01-22 00:00:00,core,finding additive biclusters with random background,,"Abstract. The biclustering problem has been extensively studied in many areas including e-commerce, data mining, machine learning, pattern recognition, statistics, and more recently in computational biology. Given an n m matrix A (n � m), the main goal of biclustering is to identify a subset of rows (called objects) and a subset of columns (called properties) such that some objective function that specifies the quality of the found bicluster (formed by the subsets of rows and of columns of A) is optimized. The problem has been proved or conjectured to be NP-hard under various mathematical models. In this paper, we study a probabilistic model of the implanted additive bicluster problem, where each element in the n m background matrix is a random number from [0 � L 1], and a k k implanted additive bicluster is obtained from an error-free additive bicluster by randomly changing each element to a number in [0 � L 1] with probability �. We propose an O(n2m) time voting algorithm to solve the 2 1 problem. We show that for any constant Æ such that (1 Æ)(1 �)  � 0, L when k � max 8 n log n � 8logn log(2L),wherecisaconstant number, the c voting algorithm can correctly find the implanted bicluster with probability at 9 least 1 n2. We also implement our algorithm as a software tool for finding novel biclusters in microarray gene expression data, called VOTE. The implementation incorporates several nontrivial ideas for estimating the size of an implanted bicluster, adjusting the threshold in voting, dealing with small biclusters, and dealing with multiple (and overlapping) implanted biclusters. Our experimental results on both simulated and real datasets show that VOTE can find biclusters with a high accuracy and speed",e-commerce,1203
,filtered,core,"Mary Ann Liebert, Inc.",,core,an efficient voting algorithm for finding additive biclusters with random background,,"The biclustering problem has been extensively studied in many areas, including e-commerce, data mining, machine learning, pattern recognition, statistics, and, more recently, computational biology. Given an n × m matrix A (n ≥ m), the main goal of biclustering is to identify a subset of rows (called objects) and a subset of columns (called properties) such that some objective function that specifies the quality of the found bicluster (formed by the subsets of rows and of columns of A) is optimized. The problem has been proved or conjectured to be NP-hard for various objective functions. In this article, we study a probabilistic model for the implanted additive bicluster problem, where each element in the n × m background matrix is a random integer from [0, L − 1] for some integer L, and a k × k implanted additive bicluster is obtained from an error-free additive bicluster by randomly changing each element to a number in [0, L − 1] with probability θ. We propose an O (n2m) time algorithm based on voting to solve the problem. We show that when \documentclass{aastex}\usepackage{amsbsy}\usepackage{amsfonts}\usepackage{amssymb}\usepackage{bm}\usepackage{mathrsfs}\usepackage{pifont}\usepackage{stmaryrd}\usepackage{textcomp}\usepackage{portland, xspace}\usepackage{amsmath, amsxtra}\pagestyle{empty}\DeclareMathSizes{10}{9}{7}{6}\begin{document}$$k \geq \Omega (\sqrt{n \log n})$$\end{document}, the voting algorithm can correctly find the implanted bicluster with probability at least \documentclass{aastex}\usepackage{amsbsy}\usepackage{amsfonts}\usepackage{amssymb}\usepackage{bm}\usepackage{mathrsfs}\usepackage{pifont}\usepackage{stmaryrd}\usepackage{textcomp}\usepackage{portland, xspace}\usepackage{amsmath, amsxtra}\pagestyle{empty}\DeclareMathSizes{10}{9}{7}{6}\begin{document}$$1 - {\frac {9} {n^ {2}}}$$\end{document}. We also implement our algorithm as a C++ program named VOTE. The implementation incorporates several ideas for estimating the size of an implanted bicluster, adjusting the threshold in voting, dealing with small biclusters, and dealing with overlapping implanted biclusters. Our experimental results on both simulated and real datasets show that VOTE can find biclusters with a high accuracy and speed",e-commerce,1204
48c6c90f708013906e4fb9e82635a94f1db386db,filtered,semantic_scholar,2021 IEEE Intelligent Vehicles Symposium (IV),2021-01-01 00:00:00,semantic_scholar,digimobot: digital twin for human-robot collaboration in indoor environments,https://www.semanticscholar.org/paper/48c6c90f708013906e4fb9e82635a94f1db386db,"Human-robot collaboration and cooperation are critical for Autonomous Mobile Robots (AMRs) in order to use them in indoor environments, such as offices, hospitals, libraries, schools, factories, and warehouses. Since a long transition period might be required to fully automate such facilities, we have to deploy AMRs while improving safety in the mixed environments of human and mobile robots. In addition, human behaviors in such environments might be difficult to predict. In this paper, we present a Digital Twin for Autonomous Mobile Robots system named DigiMobot to support, manage, monitor, and validate AMRs in indoor environments. First, DigiMobot can simulate human behaviors and robot movements to verify and validate AMRs to improve safety in a virtual world. Secondly, DigiMobot can monitor and manage AMRs in the physical world by collecting sensor data from each robot in real-time. Since DigiMobot enables us to test the robot systems in the virtual world, we can deploy and implement AMRs in each facility without any modifications. To show the feasibility of DigiMobot, we develop a software framework and two different types of autonomous mobile robots. Finally, we conduct real-world experiments in a warehouse located in Saitama, Japan, in which more than 400, 000 items are stored for commercial purposes.",oceanology,1205
334c96fd3595cf14477eaa52455f14b17d2b8ffc,filtered,semantic_scholar,J. Intell. Robotic Syst.,2020-01-01 00:00:00,semantic_scholar,pie: a tool for data-driven autonomous uav flight testing,https://www.semanticscholar.org/paper/334c96fd3595cf14477eaa52455f14b17d2b8ffc,"In this paper, a novel technique is presented to test the flight of an unmanned aerial vehicle autonomously in a real-world scenario using a data-driven technique without intervening with its onboard software. With the growing applications of such vehicles, testing of autonomous flight is a very important task for rapid deployment. There are different tools for modeling and simulating unmanned vehicles in virtual worlds such as Gazebo, MATLAB, Simulink, and Webots to name a few. None of these simulation tools are able to model all possible physical parameters of a real-world environment. Hence, the flight controller or mission planning software has to be tested in the physical world in the presence of an expert before deployment for a specific task. A Perception Inference Engine evaluation tool is presented that can infer internal states of the autonomous system from external observations only. The Gazebo simulation platform is used to collect data to develop the perception model. For real-time data collection, a VICON motion capture system is used to observe the autonomous flight of a small unmanned aerial vehicle. A state-of-the-art decision tree algorithm is used to implement the data-driven approach. The technique was tested using simulation data and verified with real-time data from Intel Aero Ready to Fly and Parrot AR. 2.0 drones. Moreover, we analyzed the robustness of the proposed system by introducing noise in sensor measurement and ambiguity in the testing scenario. We compared the performance of the decision tree classifier with Naïve bayes and support vector machine classifiers. It is shown that the developed system can be used for the performance evaluation of a UAV operating in the physical world by significantly reducing uncertainty in mission failure due to environmental parameters.",oceanology,1206
fab176450e9142a31f47c0dbe1990f1e647cb66d,filtered,semantic_scholar,International Journal of Parallel Programming,2019-01-01 00:00:00,semantic_scholar,guest editorial: special issue on emerging technology for software define network enabled internet of things,https://www.semanticscholar.org/paper/fab176450e9142a31f47c0dbe1990f1e647cb66d,"The Internet of Things (IoT) has been considered as a technology, which takes the first step towards a smarter world bridging the physical world with the cyber world. Even though IoT notion has gained much attention during the last few decades, real-world implementation of large-scale IoT network is still evolving in its infancy. Since deployment lags far behind the theoretic notion, comprehensive research efforts on innovative applications, architecture, and a network of IoT are required to promote the implementation of large-scale, high-quality, efficient, and secure IoT scenarios. Software Defined Networks (SDN) facilitates a variety of opportunities for network evolution. The key feature of SDN is to decouple data and control planes, which removes control plane from network hardware. As a result, it offers a remarkable resilience in programming, while providing a broad range of opportunities to optimize the utilization of network resources. Owing to the characteristics of SDN, experts in both industry and academia claims SDN as one of the ideal technologies to bridge the gaps and to overcome drawbacks of IoT deployment. Exploiting the benefits of scalable and adaptable network devices, SDN is considered as highly promising in empowering smart, powerful, and open IoT services and communication functionalities. In fact, SDN is capable of addressing numerous challenges in IoT varying from innumerable service requests and responses, the enormous data flow of IoT sensors, devices, and appli-",oceanology,1207
10.1109/scc.2017.68,filtered,2017 IEEE International Conference on Services Computing (SCC),IEEE,2017-06-30 00:00:00,ieeexplore,synadapt: automated synthesis of adaptive agents,https://ieeexplore.ieee.org/document/8035021/,"Distributed autonomous multi-agent reasoning and classification systems have been thought of to be the basis of intelligence and have wide applications in the space of operational intelligence in closing the loop between sensing, analytics, and actions. This paper targets multi-agent systems that employ rulebased logics (i.e., rules that determine the output/response of an agent depending on the range of the input values) with pre-defined rules to accurately perceive the environment, and provide associated reactions. Such rule-based systems do not perform well in scenarios, where human generated rules cannot adapt to dynamic variations in the data distribution arising due to dynamic changes in the environment, especially if data dimensionality is very high. Examples of such scenarios exist wherever the sensed data arrives from the physical world - such as weather data, physical sensor data, human behaviour controlled data, etc. Clearly, to meet the adaptivity requirements of such scenarios we require the agents to possess adaptive reasoning capability such that they can adapt the underlying rules with respect to the changing environment. Developing such adaptive agents requires the developer to additionally possess considerable expertise of state-of-the-art machine learning techniques, apart from possessing knowledge of the agent's target domain. To address the above issues, we automate the process of development and deployment of adaptive agents. We present the fundamental design concepts behind the development of SynAdapt: a new adaptive meta-learning based multi-agent synthesis framework, that automates the synthesis of adaptive multi-agent systems from high-level user specifications. SynAdapt provides the following key features: a) Automated synthesis and deployment of adaptive agents from high-level user specification, b) Agents synthesised by SynAdapt can select a learning strategy that is particularly suited for given user specifications and input dataset, and c) Agents synthesised by SynAdapt can leverage adaptive ensemble learning techniques to deal with concept drift.",space,1208
