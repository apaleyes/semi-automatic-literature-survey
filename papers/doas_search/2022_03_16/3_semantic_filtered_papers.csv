doi,type,publication,publisher,publication_date,database,title,url,abstract,domain,id,status
cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,to_check,semantic_scholar,,2018-01-01 00:00:00,semantic_scholar,comparing human-robot proxemics between virtual reality and the real world,https://www.semanticscholar.org/paper/cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,"Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of Human-Robot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. Comparing Human-Robot Proxemics between Virtual Reality and the Real World Rui Li KTH Royal Institute of Technology Stockholm, Sweden Rui3@kth.se ABSTRACT Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other.Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. INTRODUCTION Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI) [1][2][3][4]. VR has been used to test teleoperation and collect demonstration data to train machine learning algorithms, which showcased the effectiveness of learning visuomotor skills using data collected by consumer-grade devices [1]. VR teleoperation systems were proposed to crowdsource robotic demonstrations at scale [2]. A VR simulation framework was also proposed to replace the physical robot, as VR can enable high level abstraction in embodiment and multimodal interaction [3]. VR has also been used as a rapid prototyping tool to design in-vehicle interactions and interfaces for self-driving cars, which showed the evocation to genuine responses from test participants [4]. Compared to other HRI experiment methods, VR as an emerging interactive media provides unique advantages. VR HRI has the potential of having higher immersion and fidelity than picture based HRI, video-based HRI and simulated HRI. In situations where the perception of the robot is challenging, compared to on-screen viewing, VR display showed significant improvement on collaborative tasks [5]. When comparing VR HRI to the physical, realworld interaction (Live HRI), there is a trade-off between the two. VR experiences still cannot replace physical experiences due to system limitation, and limited interaction modalities etc. [6]. For example, system limitations such as limited field of view and low display resolution could reduce immersion and presence of the VR experience, resulting in different behaviors from Live experiments. Limited interaction modalities, such as the absence of touch, means that the participant could not feel the robot or even go through the robot, which could potentially break the entire interaction. Figure 1: Photograph of the Live experiment setting However, with the help of the distribution of consumer-grade VR devices and online crowdsourcing platforms, VR HRI has the potential to gain massive data for training robotic behavior and studying HRI related issues. Data collection through VR can also reduce noise and improve the data quality [1], which help to ease data processing and algorithm training. Furthermore, VR HRI experiments can test concepts and interactions without physical robots, making it more resource efficient and less expensive than Live HRI. Less hardware also means that the experiment will be less cumbersome to set up, easier to be reproduced and to ensure experiment quality. In this study, HRI Proxemics (the preferred personal space between a human and a robot) was compared to give a better justification and more basic understanding of the relationship between Live and VR. Proxemics preferences rely on lower level intuition [7], therefore, reflect the differences in the perceptions between Live and VR better. Compared to other HRI subject such as conversational (audio) or gaze behavior (visual), which are more modality dependent, proxemics can give a comprehensive understanding of the human responses. In addition, variations of modalities in VR can greatly influence human perception. For example, a higher visual familiarity of the physical environment in VR can decrease the effect of distance distortion [8]. Auditory inputs play another important role in VR, the addition of spatial sound can increase the sense of presence in VR and provide sound localization [9]. Thus, this work also compares VR settings with variance in modalities to evaluate the impacts of visual familiarity and spatial sound on VR HRI experiments. A 2 x 3 mixed design experiment was conducted to evaluate the differences between Live and VR HRI, as well as the influence of visual familiarity and spatial sound in VR. For the Live HRI, the pepper robot from Softbank Robotics was used (Figure 1). In the VR HRI, a 3D model of the same robot was used. To measure visual familiarity, the VR scene was created in Blender based on a 3D scan of the physical lab. The spatial sound was created by enabling the movement of the physical robot, due to the difficulties of engineering spatial sound. The interaction was implemented in Unity. As an objective measurement for proxemics preference, the minimum comfort distance (MCD) was measured. In addition, for the psychological perception of the experience, the feeling of presence was measured with the SUS questionnaire. For the perception of the robot, two relevant factors, competence and discomfort was measured with the ROSAS questionnaire.",oceanology,1,unknown
59e10d1d4cd454635914cfd0ac5160a318fd0473,to_check,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,ub09 session 9,https://www.semanticscholar.org/paper/59e10d1d4cd454635914cfd0ac5160a318fd0473,"In the domain of Wireless Sensor Networks (WSN), providing an effective security solution to protect the motes and their communications is challenging. Due to the hard constraints on performance, storage and energy consumption, normal network-security related techniques cannot be applied. Focusing on the ""Intrusion Detection"" problem, we propose a realworld application of our WSN Intrusion Detection System (WIDS). WIDS exploits the Weak Process Models to classify potential security issues in the WSN and to notify the operators when an attack tentative is detected. In this demonstration, we show how our IDS works, how it detects some basic attacks and how the IDS can evolve to fullfil the needs of secure WSN deployments. Download Paper (PDF) UB09.2 RESCUE: EDA TOOLSET FOR INTERDEPENDENT ASPECTS OF RELIABILITY, SECURITY AND QUALITY IN NANOELECTRONIC SYSTEMS DESIGN Authors: Cemil Cem GÃ¼rsoy1, Guilherme Cardoso Medeiros2, Junchao Chen3, Nevin George4, Josie Esteban Rodriguez Condia5, Thomas Lange6, Aleksa Damljanovic5, Raphael Segabinazzi Ferreira4, Aneesh Balakrishnan6, Xinhui Anna Lai1, Shayesteh Masoumian7, Dmytro Petryk3, Troya Cagil Koylu2, Felipe Augusto da Silva8, Ahmet Cagri Bagbaba8 and Maksim Jenihhin1 1Tallinn University of Technology, EE; 2Delft University of Technology, NL; 3IHP, DE; 4BTU Cottbus-Senftenberg, DE; 5Politecnico di Torino, IT; 6IROC Technologies, FR; 7Intrinsic ID B.V., NL; 8Cadence Design Systems GmbH, DE Abstract The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF)The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF) UB09.3 ASAM: AUTOMATIC SYNTHESIS OF ALGORITHMS ON MULTI CHIP/FPGA WITH COMMUNICATION CONSTRAINTS Authors: Amir Masoud Gharehbaghi, Tomohiro Maruoka, Yukio Miyasaka, Akihiro Goda, Amir Masoud Gharehbaghi and Masahiro Fujita, The University of Tokyo, JP Abstract Mapping of large systems/computations on multiple chips/multiple cores needs sophisticated compilation methods. In this demonstration, we present our compiler tools for multi-chip and multi-core systems that considers communication architecture and the related constraints for optimal mapping. Specifically, we demonstrate compilation methods for multi-chip connected with ring topology, and multi-core connected with mesh topology, assuming fine-grained reconfigurable cores, as well as generalization techniques for large problems size as convolutional neural networks. We will demonstrate our mappings methods starting from data-flow graphs (DFGs) and equations, specifically with applications to convolutional neural networks (CNNs) for convolution layers as well as fully connected layers. Download Paper (PDF) UB09.4 HEPSYCODE-MC: ELECTRONIC SYSTEM-LEVEL METHODOLOGY FOR HW/SW CO-DESIGN OF MIXED-CRITICALITY EMBEDDED SYSTEMS Authors: Luigi Pomante1, Vittoriano Muttillo1, Marco Santic1 and Emilio Incerto2 1UniversitÃ  degli Studi dell'Aquila DEWS, IT; 2IMT Lucca, IT Abstract Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF)Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF) UB09.5 CS: CRAZYSQUARE Authors: Federica Caruso1, Federica Caruso1, Tania Di Mascio1, Alessandro D'Errico1, Marco Pennese2, Luigi Pomante1, Claudia Rinaldi1 and Marco Santic1 1University of L'Aquila, IT; 2Ministry of Education, IT Abstract CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF)CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF) UB09.6 LABSMILING: A SAAS FRAMEWORK, COMPOSED OF A NUMBER OF REMOTELY ACCESSIBLE TESTBEDS AND RELATED SW TOOLS, FOR ANALYSIS, DESIGN AND MANAGEMENT OF LOW DATA-RATE WIRELESS PERSONAL AREA NETWORKS BASED ON IEEE 802.15.4 Authors: Carlo Centofanti, Luigi Pomante, Marco Santic and Walter Tiberti, University of L'Aquila, IT Abstract Low data-rate wireless personal area networks (LR-WPANs) are constantly increasing their presence in the fields of IoT, wearable, home automation, health monitoring. The development, deployment and testing of SW based on IEEE 802.15.4 standard (and derivations, e.g. 15.4e), require the exploitation of a testbed as the network grows in complexity and heterogeneity. This demo shows LabSmiling: a SaaS framework which connects testbeds deployed in a real-world-environment and the related SW tools that make available a meaningful (but still scalable) number of physical devices (sensor nodes) to developers. It provides a comforta",oceanology,2,unknown
5e171bf6603a03fbfdf3434abcd29496a2327100,to_check,semantic_scholar,Multimodal Technol. Interact.,2021-01-01 00:00:00,semantic_scholar,a learning analytics conceptual framework for augmented reality-supported educational case studies,https://www.semanticscholar.org/paper/5e171bf6603a03fbfdf3434abcd29496a2327100,"The deployment of augmented reality (AR) has attracted educatorsâ interest and introduced new opportunities in education. Additionally, the advancement of artificial intelligence has enabled educational researchers to apply innovative methods and techniques for the monitoring and evaluation of the teaching and learning process. The so-called learning analytics (LA) discipline emerged with the promise to revolutionize traditional instructional practices by introducing systematic and multidimensional ways to improve the effectiveness of the instructional process. However, the implementation of LA methods is usually associated with web-based platforms, which offer direct access to learnersâ data with minimal effort or adjustments. On the other hand, the complex nature of immersive technologies and the diverse instructional approaches which are utilized in different scientific domains have limited the opportunities for research and development in this direction. Within these research contexts, we present a conceptual framework that describes the elements of an LA process tailored to the information that can be gathered from the use of educational applications, and further provide an indicative case study for AR-supported educational interventions. The current work contributes by elucidating and concretizing the design elements of AR-supported applications and provides researchers and designers with guidelines on how to apply instructional strategies in (augmented) real-world projects.",oceanology,3,unknown
83c855157830ae26e3420a8bf44877660451c8e4,to_check,semantic_scholar,,2021-01-01 00:00:00,semantic_scholar,on the suitability of current augmented reality head-mounted devices,https://www.semanticscholar.org/paper/83c855157830ae26e3420a8bf44877660451c8e4,"Simulation is a recognized and much-appreciated tool in healthcare and education. Advances in simulation have led to the burgeoning of various technologies. In recent years, one such technological advancement has been Augmented Reality (AR). Augmented Reality simulations have been implemented in healthcare on various fronts with the help of a plethora of devices including cellphones, tablets, and wearable AR headsets. AR headsets offer the most immersive experience of the AR simulation as they are head-mounted and offer a stereoscopic view of the superimposed 3D models through the attached goggles overlaid on real-world surfaces. To this effect, it is important to understand the performance capabilities of the AR headsets based on workload. In this paper, our objective is to compare the performances of two prominent AR headsets of today, the Microsoft Hololens and the Magic Leap One. We use surgical AR software that allows the surgeons to show internal structures, such as the rib cage, to assist in the surgery as a reference application to obtain performance numbers for those AR devices. Based on our research, there are no performance measurements and recommendations available for these types of devices in general yet. Introduction In an attempt to measure the feasibility and effectiveness of using AR in surgery and nursing education, we developed an application titled ARiSE (Augmented Reality in Surgery and Education) [39]. We incorporated two facets of this application, one to be used during surgery in the Operating Room (OR), and another to assist in the education of nursing students. Surgeons would use this application in the OR during rib-plating surgery and be able to visualize an accurate model of the patientâs rib cage derived from computerized tomography (CT) scans outside their body. The nursing education application would be used by nursing students during the training of fundamental cardiopulmonary physical assessment skills. The students will be able to visualize stock models of various human organs overlaid on manikins along with visual guides to correct auscultation assessment. The aforementioned applications were developed and deployed into the first generation Microsoft Hololens and Magic Leap One AR headsets for practical use. While the application was deployed successfully and demonstrated accurate usability in both devices, our objective in this paper was to measure and compare the performances between the two AR headsets to derive recommendations for when to use which of these devices. The contributions of this paper are as follows: 1. Direct comparison of head-mounted augmented reality devices from the major brands, namely Microsoft and Magic Leap. 2. Expert evaluation based on real-world application tested with the help of domain specialists. 3. Recommendations for head-mounted Augmented Reality devices. Related Work In this section, we discuss some of the other works that relate to our project and use augmented reality techniques and devices. The related work is split into separated subsections with AR being the common theme applied to different medical areas. Augmented Reality in Mobile Devices for Medical Learning Required clinical content cannot always be imparted in live settings due to various restrictions. Educators have instead started using simulation to enhance clinical education. AR simulations have been used to assist the teaching of emergency situations, procedural training, and anatomy [35]. One such AR simulation is described by Von Jan et al. [1] in their paper. The researchers present an application that may be implemented on cellphones and tablet devices that present life-like scenarios which are overlaid on real-world objects. The trainee would visualize these scenarios through their mobile or tablet devices. This application is called mARble, and the researchers report their findings that indicate that AR enhances learning in medical education settings, specifically for subjects that are visually oriented. Augmented Reality Used in Education In their paper, Steve Chi-Yin Yuen et al. [13] have implemented AR in education and training and evaluated its efficiency. The researchers discuss the applications of AR in various fields including architecture, advertising, entertainment, medicine, gaming, books, travel, and the military. With respect to medical education, their results show AR enhancing surgical procedures and aiding clinical procedures by enhancing efficiency, reducing cost, and improving safety. The researchers also state that AR ahs the potential to invent new clinical and surgical procedures. AR has been integrated with existing medical equipment by Fischer et al. in their research [17]. There has also been research that claims AR to have the potential to make surgery minimally invasive [13] and also to enhance the learning experience in educational settings [29]. Chien et al. have used AR to assist in teaching students the anatomy of a 3D skull [16]. Researchers have also demonstrated that AR may enhance the teaching of human anatomy [19]. AR in Nursing Education Wuller et. al. have reviewed existing AR research to assist nursing education [30]. Foronda et. al. have described 3 types of AR applications used to supplement nursing education [6]. Researchers use the Microsoft Hololens to overlay muscles and bones of the human anatomy on manikins. Rahn et. al., overlay 3D models of human organs in real-time on students using iPads[31]. AR was also used with the help of iPads by Abersold et. al. in their study to assist in the training of the placement of the nasogastric tube(NGT) [32]. Ferguson et. al. have claimed game-based AR applications as having the potential to enhance nursing education [33]. This is also supported by Garrett et. al. who demonstrate improved nursing and clinical skills acquisition in students who participated in AR training scenarios [34]. Simulating Surgeries Scott Delp et al. have reviewed the shortcomings of educating medical personnel in providing appropriate emergency care [2]. In their research Samset et al. [14] developed AR tools for minimal invasive therapies (MIT). Scenarios presented by them include liver surgery, liver tumors, and cardiac surgery. With the help of AR the researchers superimpose real-world objects with 3D models obtained from CT scans. Results demonstrated improved surgical procedures and hence the potential of AR to improve healthcare in terms of utility, quality, and cost-effectiveness. Other research has also been conducted with regards to using AR during surgery. Kawamata et al. describe an AR application in their research that assists in the surgery of pituitary tumors [18]. Their results demonstrated this type of AR navigation allowing surgeons to perform accurate and safe endoscopic operations on these tumors. Memory Retention While Using AR Using Steady State Topography (SST) brain imaging to examine the brain activity of people who participated in AR and non-AR tasks, Heather Andrew et. al. [12] found that the visual attention is almost double when performing AR tasks when compared to non-AR tasks. The author also found that what is stored in memory is 70% higher for AR experiences [12]. Other studies show that the long-term memory of the learner can be enhanced by using multiple media interactions in the learning process [11]. Adedukon-Shittu et. al. have also demonstrated the effectiveness of AR technology with regards to enhancing memory retention and performance [23]. Other studies have also demonstrated the enhanced knowledge acquisition and retention of adequate memory when using AR as a supplemental tool in the education process [24]. Feasibility of Using AR to Train Resuscitation Steve Balian et al. [8] introduced a method of testing the feasibility of using augmented reality to educate healthcare providers about administration of Cardio Pulmonary Resuscitation (CPR). Using the Microsoft Hololens to provide users with audio and visual feedback, the blood flow in the human body was superimposed in real time onto a manikin. The study deployed 51 volunteers for this study. The volunteering health care providers were asked to perform CPR using only the Hololens for two minutes. The chest compression parameters were then recorded for this test. The participants generally responded positively to the system. The approach was perceived to be realistic and the AR was considered a helpful tool for training in medical education. Among the volunteers, 94% stated that they would be willing to use this application for CPR training in the future. The further support the notion of ARâs usefulness in education, Balien et al. successfully demonstrated another augmented reality tool that proved to be valuable for existing education approaches in medical training[16]. Menon et al. [37] developed an augmented reality application to improve the training of nursing students that showed a measurable improvement in student outcomes. Time and again augmented reality has proven to be advantageous when integrated into education in terms of novelty, memory retention, and knowledge gained [14] [12] [23] [24] . AR Triage Training for Multi-Casualty Scenarios The order in which patients are treated can have a detrimental effect on the survival rate of a group of patients. Hence, triage, i.e. selecting the most critical patients based on their chance of survival is crucial. John Hendricks et al. [4] devised a virtual reality simulation that assists medical personnel in their training and military field medics in making appropriate decisions in triage training environments. Their model deploys a scene in which users encounter a virtual patient with multiple injury scenarios. The virtual patients can vary with respect to their injuries as well physiological conditions and these conditions can evolve with timebased on their injuries. The injuries are visually supported by animations, such as bleeding and seizures. Augmented rea",oceanology,4,unknown
57165b0eb61847bec87cdd6df7a4eb37bd92fc59,to_check,semantic_scholar,Surgical innovation,2020-01-01 00:00:00,semantic_scholar,commercially available head-mounted displays are unsuitable for augmented reality surgical guidance: a call for focused research for surgical applications,https://www.semanticscholar.org/paper/57165b0eb61847bec87cdd6df7a4eb37bd92fc59,"Recent advances in portable computational units, optics, and photonics devices have enabled the scientific community to open many new fronts in biomedical research, with the development of innovative augmented reality (AR) applications exploiting the potentialities offered by head-mounted display (HMD) technology. Such technology has reached the maturity to be translated into commercial products, and published works on HMDs provide glimpses of how AR will disrupt the surgical field, allowing for an ergonomic, intuitive, and 3-dimensional fruition of preoperative and intraoperative information. Nowadays several commercial HMDs, such as Microsoft HoloLens, Meta or Magic Leap, integrate tracking and registration technology, and the deployment of software development kits has reduced technical complexity of custom application development, allowing for a wide range of users to easily create AR applications and attracting researchers to explore their potentialities for the implementation of surgical navigators. The above-mentioned HMDs are designed following an optical see-through (OST) approach, which augments the natural view through the projection of virtual reality information on semitransparent displays in front of the userâs eyes. The OST approach fits well in the surgical domain as it offers an instantaneous full-resolution view of the real world, allowing the natural synchronization of visual and proprioceptive information, and a complete situation awareness. Ongoing research is aimed at the goal of providing a device âconceived as a transparent interface between the user and the environment, a personal and mobile window that fully integrates real and virtual information.â1 Commercial companies are rapidly improving HMD ergonomic aspects, for example, HoloLens 2 features an improved field of view (52Â° diagonal), which includes eye tracking, and offers more comfortable wearability. However, maximizing surgical accuracy remains a challenge for manufacturers and researchers. Together with ergonomics, the achievement of precision objectives must be addressed to develop a visor suitable for guiding surgical operations, not to mention compliance with medical device regulations. An increasing number of research studies propose the use of commercial HMDs to guide surgical interventions.2 To the best of our knowledge, these works are principally focused on the need to strengthen virtual/real patient registration (eg, use of an external localization system),3 improve virtual content stability,4 and solve calibration issues, and they underestimate the contribution of perceptual issues to the user accuracy. One of the largest obstacles to obtain a perceptually correct augmentation is the inability to render proper focus cues in HMDs; indeed, the majority of systems offers the AR content at a fixed focal distance, failing to stimulate natural eye accommodation and retinal blur effects.5 Our recent work2 suggests to avoid the use of existing HMD-OST, which are not specifically designed for performing tasks in peripersonal space (<1 m), to guide manual tasks requiring a high level of precision, since perceptual issues, particularly âfocal rivalryâ (ie, inability to see simultaneously in focus the virtual and real content), can affect user performance.5 Most commercial systems (HoloLens, Lumus, Meta, Ora2) indeed have a fixed focal plane at 2 m or more (often infinite). Thus, during manual tasks, virtual content is 903197 SRIXXX10.1177/1553350620903197Surgical InnovationCarbone et al editorial2020",oceanology,5,not included
944ad40d87fce6566211eeca78fe0b08eee1e34b,to_check,semantic_scholar,Frontiers in Robotics and AI,2020-01-01 00:00:00,semantic_scholar,trustable environmental monitoring by means of sensors networks on swarming autonomous marine vessels and distributed ledger technology,https://www.semanticscholar.org/paper/944ad40d87fce6566211eeca78fe0b08eee1e34b,"The article describes a highly trustable environmental monitoring system employing a small scalable swarm of small-sized marine vessels equipped with compact sensors and intended for the monitoring of water resources and infrastructures. The technological foundation of the process which guarantees that any third party can not alter the samples taken by the robot swarm is based on the Robonomics platform. This platform provides encrypted decentralized technologies based on distributed ledger tools, and market mechanisms for organizing the work of heterogeneous multi-vendor cyber-physical systems when automated economical transactions are needed. A small swarm of robots follows the autonomous ship, which is in charge of maintaining the secure transactions. The swarm implements a version of Reynolds' Boids model based on the Belief Space Planning approach. The main contributions of our work consist of: (1) the deployment of a secure sample certification and logging platform based on the blockchain with a small-sized swarm of autonomous vessels performing maneuvers to measure chemical parameters of water in automatic mode; (2) the coordination of a leader-follower framework for the small platoon of robots by means of a Reynolds' Boids model based on a Belief Space Planning approach. In addition, the article describes the process of measuring the chemical parameters of water by using sensors located on the vessels. Both technology testing on experimental vessel and environmental measurements are detailed. The results have been obtained through real world experiments of an autonomous vessel, which was integrated as the âleaderâ into a mixed reality simulation of a swarm of simulated smaller vessels.The design of the experimental vessel physically deployed in the Volga river to demonstrate the practical viability of the proposed methods is shortly described.",oceanology,6,unknown
506e55521ddd1442ae6eae58534aa971946acc3f,to_check,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,distributed heterogeneous tracking for augmented reality,https://www.semanticscholar.org/paper/506e55521ddd1442ae6eae58534aa971946acc3f,"Augmented reality (AR) is a technique in which a userâs view of the real world is enhanced or augmented with additional information generated from a computer model (Azuma et al., 2001). The enhancement may consist of virtual artifacts to be fitted into the environment or a display of non-geometric information about existing real objects. Mobile AR (MAR) systems implement this interaction paradigm in an environment in which the user moves, possibly over wide areas (Feiner, MacIntyre, Hoellerer, & Webster, 1997). This is in contrast to non-mobile AR systems that are utilized in limited spaces such as a computer-aided surgery or by a technicianâs aid in a repair shop. There are a number of challenges to implementing successful AR systems. These include a proper calibration of the optical properties of cameras and display systems (Tuceryan et al., 1995; Tuceryan, Genc, & Navab, 2002), and an accurate registration of threedimensional objects with their physical counterparts and environments (Breen, Whitaker, Rose, & Tuceryan, 1996; Whitaker, Crampton, Breen, Tuceryan, & Rose, 1995). In particular, as the observer (or an object of interest) moves over time, the 3D graphics need to be properly updated so that the realism of the resulting scene and/or alignment of necessary objects and graphics are maintained. Furthermore, this has to be done in real time and with high accuracy. The technology that allows this real-time update of the graphics as users and objects move is a tracking system that measures the position and orientation of the tracked objects (Koller et al., 1997). The ability to track objects, therefore, is one of the big challenges in MAR systems. This article describes a software framework for realizing such a distributed tracking environment by discovering independently deployed, possibly heterogeneous trackers and fusing the data from them while roaming over a wide area. In addition to the MAR domain, this kind of a tracking capability would also be useful in other domains such as robotics and locationaware applications. The novelty of this research lies in the amalgamation of the theoretical principles from the domains of AR/VR, data fusion, and the distributed software systems to create a sensor-based, wide-area tracking environment. BACKGROUND",oceanology,7,unknown
26960f55ddb24b2338a5eae012c63cd13abb7bdd,to_check,semantic_scholar,2017 IEEE International Symposium on Parallel and Distributed Processing with Applications and 2017 IEEE International Conference on Ubiquitous Computing and Communications (ISPA/IUCC),2017-01-01 00:00:00,semantic_scholar,performance evaluation for wifi dcf networks from theory to testbed,https://www.semanticscholar.org/paper/26960f55ddb24b2338a5eae012c63cd13abb7bdd,"Distributed Coordination Function (DCF) is a basic MAC protocol used in the world-wide WiFi networks and plays a key role in determining the network performance, especially in situations with a large number of users and high-density Access Point (AP) deployed. To achieve a better understanding of the real-world performance of 802.11 DCF networks, we have constructed an emulation platform and a prototype testbed for performance evaluation. The design and implementation of these two platforms are discussed in this paper. The key DCF parameters, i.e., the initial contention window size ($CW_{min}$) and the maximum contention window size ($CW_{max}$), are tuneable so that we are able to study the impact of these DCF parameters on the network performance. These experiment results are compared against with a recently proposed unified analytical framework to examine the model assumptions and system performance bottlenecks. Our results demonstrate that by adapting the values of $CW_{min}$ based on WiFi traffic load, the maximal network throughput can be achieved, and the optimal value of $CW_{min}$ varies when the network size changes. As a reality check, the emerging software defined WiFi network architecture can be optimized for performance enhancement guided by this unified performance model.",oceanology,9,unknown
4dd4afbb17999bdf9e218001e3a6ae2252c10f8f,to_check,semantic_scholar,Defense + Security,2017-01-01 00:00:00,semantic_scholar,visualizing uas-collected imagery using augmented reality,https://www.semanticscholar.org/paper/4dd4afbb17999bdf9e218001e3a6ae2252c10f8f,"One of the areas where augmented reality will have an impact is in the visualization of 3-D data. 3-D data has traditionally been viewed on a 2-D screen, which has limited its utility. Augmented reality head-mounted displays, such as the Microsoft HoloLens, make it possible to view 3-D data overlaid on the real world. This allows a user to view and interact with the data in ways similar to how they would interact with a physical 3-D object, such as moving, rotating, or walking around it. A type of 3-D data that is particularly useful for military applications is geo-specific 3-D terrain data, and the visualization of this data is critical for training, mission planning, intelligence, and improved situational awareness. Advances in Unmanned Aerial Systems (UAS), photogrammetry software, and rendering hardware have drastically reduced the technological and financial obstacles in collecting aerial imagery and in generating 3-D terrain maps from that imagery. Because of this, there is an increased need to develop new tools for the exploitation of 3-D data. We will demonstrate how the HoloLens can be used as a tool for visualizing 3-D terrain data. We will describe: 1) how UAScollected imagery is used to create 3-D terrain maps, 2) how those maps are deployed to the HoloLens, 3) how a user can view and manipulate the maps, and 4) how multiple users can view the same virtual 3-D object at the same time.",oceanology,10,unknown
83c19e91c197218df688172968455ff9d4efc7fe,to_check,semantic_scholar,,2017-01-01 00:00:00,semantic_scholar,enhancing liveness testing for transferring data packets through using automatic test packet generation,https://www.semanticscholar.org/paper/83c19e91c197218df688172968455ff9d4efc7fe,"-Networks are getting bigger and more complex, yet administrators rely on incomplete tools such as and to debug problems. We propose an automated and systematic approach for testing and rectify networks called âAutomatic evaluates Package Generationâ (ATPG). ATPG reads router configurations and generates a device-independent model. The model is used to generate a minimum set of test packets to minimally exerting every link in the network or maximally exerting every rule in the network. Test packets are sent periodically, and detected failures trigger a separate mechanism to localize the revoke. ATPG can detect both functional and renderings problems. ATPG complements but goes beyond earlier work in static checking for which cannot detect liveness or performance faults or fault localization which only localize revoke given liveness results. We describe our prototype ATPG implementation and results on two real-world data sets: Stanford Universityâs backbone network and Internet. We find that a small number of test packets suffice to test all rules in these networks. A sending 4000 test packet 10 times per second consumes less than 1% of link capacity. ATPG code and the datasets are publicly available. Keyword: ATPG, liveness, Networks. ________________________________________________________________________________________________________ I.INTRODUCTION Networking is the word fundamentally cogitates to computers and their property. It is very often used in the world of computers and their use in different connections. The term networking express the link between two or more computers and their tendency, with the vital purpose of sharing the data stored in the computers, with each other. The networks between the computing tendencies are very public these days due to the launch of assorted hardware and computer software which aid in making the activity much more convenient to build and use. Fig: 1.1Structure of Networking between the different computers The discuss about Figure: 1.1 Structure of Networking between the different computer. Its main process of share the Internet to different things and devices. General Network Techniques When computers communicate on a network, they send out information packets without knowing if anyone is listening. Computers in a network all have an attached to the network and that is called to be attached to a network bus. What one computer sends out will reach the other computer on the local area network. Â© 2017 IJEDR | Volume 5, Issue 1 | ISSN: 2321-9939 IJEDR1701073 International Journal of Engineering Development and Research (www.ijedr.org) 476 Fig: 1.2 the clear idea about the networking functions The discus about figure: 1.2 clear ideas of network function and different computers to be able to distinguish between each other, every computer have unique ID called MAC-address Media Access Control Address. This address is not only unique on your network but unique for all tendencies that can be aquiline up to a network. The MAC-address is tied to the hardware and has nothing to do with IP-addresses. Since all computers on the network graduate inversion that is sent out from all other computers the MACaddresses is primarily used by the computers to filter out incoming network traffic that is addressed to the scratcher computer. When a computer expostulation with another computer on the network, it sends out both the other computers MAC-address and the MACaddress of its own. In that way the receiving computer will not only realize that this parcel is for me but also, who sent this data packet so a return response can be sent to the sender. Ethernet network as delineate here, all computers hear all network aggregation since they are attached to the same bus. This network structure is called multi-drop. One problem with this network structure is that when you have, let say ten computers on a network and they expostulation attendance and due to that they sends out there data packets randomly, collisions occur when two or more computers sends data at the same time. When that happens data gets imperfect and has to be resent. On a network that is heavy loaded even the resent packets collide with other packets and have to be resent again. In reality this soon takes affect an information problem. If respective computers communicate with each other at high speed they may not be able to utilize more than 25% of the total network information measure since the rest of the bandwidth is used for regressive antecedently corrupted packets. The way to minimize this problem is to use network switches. II.RELATED NETWORK Detecting the occurrence and location of performance anomalies is critical to ensuring the effective operation of network infrastructures. In this paper we present a framework for detecting and apposition performance anomalies based on using an active investigate measurement infrastructure deployed on the periphery of a network. Our framework has three components: an algorithm for detection performance oddball on a path, an algorithm for environs which paths to probe at a given time in order to detect performance anomalies where a path is defined as the set of links between two sampling nodes, and an algorithm for designation the links that are causing an identified anomaly on a path The path selection algorithm is designed to enable a interchange between insure that all links in a network are of times monitored to detect performance anomalies, while minimizing probing overhead.[1] This paper, we develop failure-resilient techniques for monitoring link detain and imbecility in a Service Provider or endeavor IP network. Our two-phased approach attempts to minimize both the monitoring infrastructure costs as well as the additional aggregation due to probe messages. In the first phase, we compute the particular point of a minimal set of monitoring stations such Â© 2017 IJEDR | Volume 5, Issue 1 | ISSN: 2321-9939 IJEDR1701073 International Journal of Engineering Development and Research (www.ijedr.org) 477 that all network links are covered, even in the bigness of several link reverting. Afterwards, in the second phase, we compute a minimal set of probe messages that are transmitted by the stations to measure link delays and isolate network faults these approximation ratios are provably very close to the best possible bounds for any algorithm. [2]We present a new symbolic execution tool, KLEE, capable of automatically induce tests that wangle high coverage on a diverse set of complex and environmentallyintense programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment position on millions of UNIX systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage on average over 90% per tool. [3] The emergence of Open Flow-capable switches enables exciting new network functionality, at the risk of programming errors that make communication less reliable. The centralized programming model, where a single accountant program manages the network, seems to reduce the likelihood of bugs. However, the system is inherently scattered and asynchronous, with events happening at different switches and end hosts, and inevitable delays affecting communication with the controller. In this paper, we present economic, regular techniques for testing unqualified controller programs. Our NICE tool applies model checking to explore the state space of the entire system the controller, the switches, and the hosts. [4] Network performance tomography, characteristics of the network interior, such as link loss and packet latency, is inferred from unrelated end-to-end measurements. Most work to date is based on employ packet level correlations. However, these methods are often limited in scope-multicast is not widely deployed-or require deployment of additional hardware or software system. Some recent work has been successful in reaching a less detailed goal: identifying the lossiest network links using only unrelated end-toend sampling. In this paper, we abstract the properties of network performance that allow this to be done and exploit them with a quick and simple deduction algorithm that, with high likely, separate the worst performing links. [5] III. SYSTEM ANALAYS Automatic Test Packet Generation (ATPG) framework that self-loading generates a minimal set of collection to test the liveness of the underlying topology and the congruence between data plane state and redundancy description. The tool can also automatically generate packets to test performance assertions such as packet latency. It can also be specialized to generate a minimal set of packets that merely test every link for network liveness. ï§ A survey of network operators revealing common failures and root causes. ï§ A test packet generation algorithm. ï§ A revoke localization algorithm to isolate faulty devices and rules. ï§ ATPG use cases for functional and performance testing. ï§ Evaluation of a prototype ATPG system using rule sets collected from the Stanford and Internet2 backbones.",oceanology,11,not included
2b4cfc0b672aafc526bda6f7e35cf204ece0d34c,to_check,semantic_scholar,,,semantic_scholar,ac 2009-1203: a novel interdisciplinary sensor networks laboratory,https://www.semanticscholar.org/paper/2b4cfc0b672aafc526bda6f7e35cf204ece0d34c,"Today, networks of legacy and newer sophisticated sensors and actuators that combine reconfigurable gigascale semiconductor technology with emerging micro-mechanical systems (MEMS) and nanotechnology subsystems (i.e. bio-systems/chemical/fluidics/photonics/ etc) are being designed and deployed in almost every area of technology that impacts human endeavor and commerce. These smart sensors/actuators are being networked together through: either standards based or industry specific, proprietary, wired networks or newly emerging wireless networking technologies. Presently, at the twoand four-year college level, technicians and technologists in a wide variety of impacted disciplines are not receiving an adequate education about: fundamental sensor theory, basic sensor operation, sensor system deployment planning, appropriate data-transport and networking connectivity schemes, applications software, and impending system maintenance support needs of these increasingly more sophisticated and complex, smart sensor/actuator based systems. This paper will report on the development, organization, and use of a novel interdisciplinary sensor networks laboratory. The heart of the laboratory is a dedicated data-network (SensorNet) that emulates a wide area network or WAN. The SensorNet WAN nodes and other network access points allow for the interconnection of numerous types of industry specific and standard âarea networksâ typically utilized for the gathering of sensor data and directing other sensor functions, as well as, the associated PCâs and servers used to direct the sensor systems and warehouse the gathered data. This laboratory environment lends itself to real world case-study and problem-based type student-centered learning experiences that can be themselves integrated into established fields of technology that do not normally include this type of activity as part of the fieldâs traditional educational experience at the undergraduate level. I. Overview Although it is not uncommon for several different technology fields to converge together, it is somewhat unexpected to observe such an amalgamation rapidly triggering other technologic innovations that have widespread potential to change our relationship with the environment and our daily endeavors. However, this is just what is happening today. Only a short time ago, the Internet, the result of a convergence of several technologies, spawned the development of what is commonly known as the âInformation Economy.â Today another innovative and important convergence of technologies has recently gained critical mass and recognition by business and industry, government, academia, and professional societies. It is the deployment of intricate systems involving complex sensors with embedded (ambient) intelligence and advanced actuators coupled with modern data-transport and networking technologies and applicationenabling software with data fusion capabilities. This rapidly evolving convergence of technologies, which allows us to implement sensor systems that gather in situ (remote), realtime, statistically relevant information and interpret it in new and novel ways, has already started to transform automation and process control systems. The technology of networked sensor systems has the very genuine potential to significantly impact almost every aspect of human endeavor by increasing system efficiency, reducing energy consumption, permitting the real-time monitoring of the âhealthâ of the nationâs infrastructure and environment, and improving public health and safety. Applications are limitless! P ge 1.77.3 On a global level, the NSF has been calling this âgrand convergence,â cyberinfrastructure. One may find many references to this concept, forecasts of potential future applications, reports on inprogress test projects such as HPWREN, NIMS, and ROADnet, and potential research funding opportunities on the NSFâs Web site [1] . However, most of this current, enthusiastic attention and promotion of cyberinfrastructure by the NSF is aimed at senior, graduate-level research institutions. Not surprisingly, most of the NSFâs recent Requests for Proposals (RFPs) in this area have been targeted at basic research about wireless sensor networks and systems and applications of these systems to infrastructure and environmental monitoring and other technology areas. While many applications of networked sensor systems are yet to be even thought of, the reality is that they are being deployed today and will continue to proliferate for many years to come until they eventually become as commonplace as a typical public utility like electricity. This paper describes aspects of an NSF funded CCLI project (DUE 0736888), titled, âThe Sensor Networks Education Projectâ (SNEP) that seeks to develop materials and a model teaching laboratory that will be useful for other faculty and organizations at the twoand even the four-year college level to emulate. This project looks at this evolving convergence on a more practical level and speaks to the lack of engineering technology faculty expertise and teaching materials needed to infuse the newly recognized, exponentially growing knowledge base of networked sensor technology into the curricula and hence into the skill-sets of todayâs twoand four-year technical college graduates â the technicians and technologists of tomorrow. This is the community of workers that will most likely deal with the design, deployment, updating, and maintenance of these systems. Today, networks of legacy and newer sophisticated sensors that combine reconfigurable gigascale semiconductor technology with emerging micro-electromechanical systems (MEMS) and nanotechnology subsystems [2] (i.e. bio-systems/chemical/molecular/photonic) are being designed and deployed in almost every area of technology that impacts human endeavor and commerce (i.e. Aerospace, Agriculture, Automotive, Biomedical, Building Automation, Energy Exploration and Production, Environmental Monitoring, Healthcare, Homeland Security, Industrial Automation, Infrastructure Monitoring, Information Technology, Manufacturing, Military, Pharmaceutical, Telecomm, Transportation, Weather Forecasting, etc). These sensors are being networked together through: either standards based or industry proprietary wired networks or emerging wireless networking technologies. Presently, at the twoand four-year college level, technologists and technicians in a wide variety of impacted disciplines are not receiving an adequate education about: fundamental sensor theory, basic sensor operation, sensor system deployment planning, appropriate data-transport and networking connectivity schemes, applications software, and impending system maintenance support needs of these increasingly more sophisticated sensor based systems. Recently, there has been a great deal of public dialogue about the out-sourcing of American manufacturing jobs and the effect of this reality on the nationâs future. Dealing with an ever increasing base of physical sensor networks in all areas of endeavor will not be something that can be done through a call to a help desk located in a foreign country. The apparent curriculum shortcoming regarding these topics within todayâs associate and bachelors degree technology oriented programs is primarily due to the extremely rapid evolution and convergence of several key areas of electronics, computer, and MEMS technology (i.e. embedded processing, smart P ge 1.77.4 sensors/actuators, wired and wireless networking, etc), the lack of appropriate up-to-date educational materials, and a lack of appropriate faculty expertise in this rapidly expanding and remarkably cross-disciplinary field. II. Project Overview Over its two-year life-span, this CCLI Phase I project has as its primary goals the creation and testing of interdisciplinary student-centered learning materials primarily designed for a âfield laboratoryâ type environment, the dissemination of these materials, and the development of faculty expertise in the multi-disciplinary field of networked sensors and modern active-learner teaching techniques. To accomplish these goals the project will: (1) develop and deploy a model, innovative, replicate-able, multi-interdisciplinary, case-study and problem-based oriented, networked, distributed sensor laboratory, (2) develop basic and advanced instructional materials and standard and âhybridâ laboratory activities related to the sensor laboratory that can be utilized for introductory courses in sensor technology or more advanced courses in networked sensor systems for use by both twoand four-year technology programs, (3) develop several prototype multifaceted educational modules that integrate traditional science and math based theory, practical real-world laboratory exercises, and science based, high-resolution, interactive simulation software, applicable to several of the major technology areas employing networked sensor technology (i.e. building automation and infrastructure monitoring and industrial automation), and (4) provide on-going local, regional, and national dissemination of these developed materials and laboratory experiences through hands-on faculty workshops and webbased distribution technologies including the National Science Digital Library (NSDL). In addition, for the duration of the project, continuous on-going professional development in the principles and applications of student-centered and active learner techniques will be provided to the recruited college faculty that will take part in the project. Research has shown that long term professional development programs are more effective than short-term workshops. For this project to be successful, the participating faculty must learn how to effectively integrate content and pedagogy in a way that actively engages students in individual and collaborative problem solving, analysis, synthesis, critical thinking, reasoning, and skillfully applying knowledge in real-w",oceanology,12,unknown
0c177e2385aad71584840a21b23ebd4b58c00132,to_check,semantic_scholar,,2012-01-01 00:00:00,semantic_scholar,service virtualization: reality is overrated,https://www.semanticscholar.org/paper/0c177e2385aad71584840a21b23ebd4b58c00132,"Software drives innovation and success in todays business world. Yet critical software projects consistently come in late, defective, and way over budget. So whats the problem? Get ready for a shock, because the answer to the problem is to avoid reality altogether. A new IT practice and technology called Service Virtualization (SV) is industrializing the process of simulating everything in our software development and test environments. Yes, fake systems are even better than the real thing for most of the design and development lifecycle, and SV is already making a huge impact at some of the worlds biggest companies. Service Virtualization:Reality Is Overratedis the first book to present this powerful new method for simulating the behavior, data, and responsiveness of specific components in complex applications. By faking out dependency constraints, SV delivers dramatic improvements in speed, cost, performance, and agility to the development of enterprise application software. Writing for executive and technical readers alike, SV inventor John Michelsen and Jason English capture lessons learned from the first five years of applying this game-changing practice in real customer environments. Other industriesfrom aviation to medicinealready understand the power of simulation to solve real-world constraints and deliver new products to market better, faster, and cheaper. Now its time to apply the same thinking to our software. For more information, see servicevirtualization.com. What youll learnYou will learn why, when, where, and how to deploy service virtualization (SV) solutions to mitigate or eliminate the constraints of an unavailable or unready service system by simulating its dependent components in order to deliver better enterprise software faster and at lower cost. In particular, you will learn step-by-step why, when, where, and how to deploy the following SV solutions: shift-left infrastructure availability performance readiness test scenario management Who this book is for This book is not only for IT practitioners on engineering, testing, and environments teams engaged in the development and delivery of enterprise software, but also for executives of companies in all sectors who need to understand and implement emergent opportunities to improve the time to market and overall competitiveness of any outward-facing business strategy that has a software application component. Table of ContentsForeword by Burt Klein Chapter 1. Introduction Service Virtualization Briefly Defined Key Practices Enabled by SV Shift-Left Infrastructure Availability Performance Readiness Test Scenario Management Navigating This Book Chapter 2. The Business Imperative: Innovate or Die Consumers Have No Mercy Business Demands Agile Software Delivery Increased Change and Complexity for IT Simulation Is Not Just for Other Industries Chapter 3. How We Got Here From Monolithic to Composite Apps Todays Complex Service Environments From Waterfall to Agile Development Chapter 4. Constraints: The Enemy of Agility Unavailable Systems and Environments Conflicting Delivery Schedules Data Management and Volatility Third Party Costs and Control Chapter 5. What is Service Virtualization? The Opposite of Server Virtualization Creation of a Virtual Service Maintaining Virtual Services What Kinds of Things You Can virtualize Virtual Service Environments (VSEs) Chapter 6. Where to Start with SV? Pick a Hairy Problem Identify Stakeholders Set Real Value Goals for Releases Avoid Inappropriate Technologies Chapter 7. Capabilities of Service Virtualization Technology Live-Like Development Environment Automation Eliminates Manual Stubbing and Maintenance Enables Parallel Dev and Test No more Availability Problem Platform-Neutrality Chapter 8. Best Practice #1: Shift-Left Reducing Wait Time Early Component and System Testing Define SV from Capture Define Incomplete SV from Requirements Expected Results Customer Example Chapter 9. Best Practice #2: Infrastructure Availability Finding Over-Utilized Resources Virtualizing Mainframes Avoiding Big IT Outlays Expected Results Customer Example Chapter 10. Best Practice #3: Performance Readiness Virtualizing Performance Environments Informing Performance from Production Expected Results Customer Example Chapter 11. Best Practice #4: Test Scenario Management Managing Big Data Shielding Teams from Volatility Massively Parallel Regression Testing Expected Results Customer Example Chapter 12. Rolling out Service Virtualization Who Pays for Service Virtualization? Overcoming Organizational Challenges Who Manages a VSE? Should I Have More Than One? Key Skills and Roles in a Virtual IT World Chapter 13. Service Virtualization in the DevTest Cloud Constraints of Cloud Dev and Test Achieving Elastic Cloud Environments Chapter 14. Assessing the Value Key Metrics for Success Areas for Improvement Chapter 15. Conclusion The Industrialized Software Supply Chain Innovate and Thrive Whats Next for SV? Glossary About the Authors",oceanology,13,not included
eebc85dcbbf0b5904316256d57cf7e9c3488d024,to_check,semantic_scholar,,2012-01-01 00:00:00,semantic_scholar,e-science approaches in molecular science,https://www.semanticscholar.org/paper/eebc85dcbbf0b5904316256d57cf7e9c3488d024,"Computer simulations of the properties of processes and materials are becoming increasingly necessary in several technological and environmental studies. This implies a growing demand of computing resources that severely exploits computational environments in terms of sustainability and reliability of the infrastructure.
 The developments in computing hardware and software, in particular the deployment of world-wide reliable Grid Computing infrastructures, the adoption of innovative computing approaches like the General Purpose Graphic Processing Unit (GPGPU) Computing and the High Performance Network environments, stimulate the exploitation of new approaches and methodologies in Computational Sciences. Furthermore the advances made in the World Wide Web, allow the implementation of Web sites from which the simulation of elementary chemical processes at molecular level is performed combining various techniques and computational approaches which are executed on High Throughput Computing (HTC) and/or High Performance Computing (HPC) platforms.
 The ubiquity of information and computing resources has impacted on the researchers? productivity, in a similar way the same technologies impacted everyone?s daily life. The E-science technologies facilitate the exchange of information among researchers, enhance the collaborative work and increase the quality of dissemination of results. Several European initiatives are devoted to facilitate researchers? work and to establish networks among researchers of the various disciplines, enabling some European research groups to reach leading positions in their disciplines. We have got the support of the EU COST (COllaboration in Science and Technology) Initiative in two Actions devoted to the facilitation of adoption of Grid and Distributed Computing technologies in Molecular and Matter Sciences. In particular I participated to the COST D23 Action: Metachem - Metalaboratories for Complex Computational Applications in Chemistry (2000-2005), and I coordinated the Working Group: Simbex: a metalaboratory for the a priori simulation of crossed molecular Beam Experiments. Furthermore I participated to the COST D37 Action: Grid Computing in Chemistry: GRIDCHEM (2006-2009), and I coordinated the Working Group: ELAMS: E-science and Learning Approaches in Molecular Science.
 The outcomes of both COST Actions contributed significantly to establish an active group of Computational Chemistry and Molecular and Matter Science laboratories which adopted the Grid Computing as an innovative computing paradigm for performing massive computational campaigns.
 Since 2004 the researchers of such laboratories joined the Virtual Organization (VO) CompChem established on the EGEE Grid Infrastructure, the largest distributed computing environment ever established worldwide, and coordinated by CERN (Conseil Europeen pour la Recherche Nucleaire). I served as VO Manager since the VO was established, under the coordination of Prof. Antonio Lagana, Department of Chemistry, University of Perugia.
 The present thesis covered a long period of research work focused on implementing some e-science instruments to the computational chemistry community, in particular the community of users belonging to the COMPCHEM Virtual Organization active in the EGEE/EGI European Grid Initiative.
 The Thesis describes some tools and approaches the author adopted to provide innovative tools to the Computational Chemistry community based on two main pillars:
1. approaches for running large computational campaigns on Grid Infrastructures 2. adopting virtual reality techniques for making more intuitive the interaction with nanoscale computing approaches and simplifying the definition of the initial conditions of the molecular simulations
 The research work originated 10 research papers, several of them produced as a joint work with European laboratories interested in the implementation of e-science tools for a smart, curious and demanding community like the Computational Chemistry one.
 The author has been able to provide a useful view of the molecular world through the use of virtual reality techniques, combined with the most advanced Web technologies, in particular using the ISO standard X3D for the 3D visualization and interaction with a virtual world. These innovative tools enabled the researchers to set up the environment for carrying out complex molecular simulations (as in the case of the Dl-Poly software package) in a intuitive and visual way. Once defined the species interacting in the considered molecular system, represented in a virtual world, the system produce the input file for the simulation and the Dl-Poly program may be launched, possibly on a Grid infrastructure to take benefit of the powerful available computational resources.
 In chapters 1, 2 and 3 the various steps toward the implementation of an a-priori molecular simulator on the EGEE/EGI Grid, for the COMPCHEM VO users, are reported.
 Molecular Virtual Reality applications are really useful as e-learning support tools for Chemistry students. To this purpose we implemented a Learning Management System based on a semantic web approach, described in chapter 7, and an assessment system, described in chapter 8, which have been used several times to assess the competences of students participating to the Erasmus Mundus Master of Science in Theoretical Chemistry and Computational Modeling. The system enables the coordinators of the Master to monitor the progresses made by the students an a daily basis.
 The author has shown how it is possible to use virtual reality approaches to describe a chemical experiment at both human and molecular level using a virtual reality approach. To this end a multi-scale virtual reality approach has been adopted to deal with the description of the physical environment,HVR. The main features of the virtual reality representation of the experiments and the potentiality of associating VRML and X3D with Java engine calculator are outlined in chapter 4.
 In chapter 5 an X3D Molecular Virtual Reality environment in which the researcher is able to interact with it by using immersive devices and dynamic gestures is described. By using the gestures the researcher is able to modify the composition of the molecular system by adding or subtracting functions and the molecular properties of the new species are evaluated in real time by invoking a Web Service implementing the simulation environment. This has required the assemblage of an innovative approach coupling the management of immersive devices with Web Services and molecular dynamics packages.
 In chapter 6 the author has presented an X3D Molecular Virtual Reality environment which makes usage of the most recent and powerful HTML and Web technologies. The approach implemented takes into account the modern approaches followed in implementing Social Networking environments and showed how useful these approaches are also in implementing scientific environments. We think such type of work is important also in consideration of how our lifestyle is changing, thanks to the ubiquity of the information, the availability of an increasing availability of storage and computing power. The social networking showed us how deep may be the impact of the computing and networking facility in the daily life and similarly the computational science, and the computational chemistry in particular, has to reshape the classical approaches and methodologies in order to gain advantage of the modern computing platforms and the powerfulness of the networking, distributed and mobile environments.",oceanology,14,unknown
d70a02826a51efc7d133f688a820e73e6a1a1b44,to_check,semantic_scholar,SSW,2010-01-01 00:00:00,semantic_scholar,semantic high level querying in sensor networks,https://www.semanticscholar.org/paper/d70a02826a51efc7d133f688a820e73e6a1a1b44,"The quick development and deployment of sensor technology within the general frame of the Internet of Things poses relevant opportunity and challenges. The sensor is not a pure data source, but an entity (Semantic Sensor Web) with associated metadata and it is a building block of a âworldwide distributedâ real time database, to be processed through real-time queries. Important challenges are to achieve interoperability in connectivity and processing capabilities (queries) and to apply âintelligenceâ and processing capabilities as close as possible to the source of data. This paper presents the extension of a general architecture for data integration in which we add capabilities for processing of complex queries and discuss how they can be adapted to, and used by, an application in the Semantic Sensor Web, presenting a pilot study in environment and health domains. 1 Background and Motivation The rapid development and deployment of sensor technology involves many different types of sensors, both remote and in situ, with such diverse capabilities as range, modality, and manoeuvrability. It is possible today to utilize networks with multiple sensors to detect and identify objects of interest up close or from a great distance. Connected Objects â or the Internet of Things â is expected to be a significant new market and encompass a large variety of technologies and services in different domains. Transport, environmental management, health, agriculture, domestic appliances, building automation, energy efficiency will benefit of real-time reality mining, personal decision support capabilities provided by the growing information shadow (i.e. data traces) of people, goods and objects supplied by the huge data available from the emerging sensor Web [1]. Vertical applications can be developed to connect to and communicate with objects tailored for specific sub domains, service enablement to face fragmented connectivity, device standards, application information protocols etc. and device management. Building extending connectivity, connectivity tailored for object communication â with regards to business model, service level, billing etc, are possible exploitation areas of the Internet Connected Objects. Important challenges are to achieve interoperability in connectivity and processing capabilities (queries, etc.), to distribute âintelligenceâ and processing capabilities as close as possible to the source of data (the Giordani I., Toscani D., Archetti F. and Cislaghi M.. Semantic High Level Querying in Sensor Networks. DOI: 10.5220/0003116600720084 In Proceedings of the International Workshop on Semantic Sensor Web (SSW-2010), pages 72-84 ISBN: 978-989-8425-33-1 Copyright c 2010 SCITEPRESS (Science and Technology Publications, Lda.) sensor or mobile device), in order to avoid massive data flows and bottlenecks on the connectivity side. The sensor is not a pure data source, but an entity (Semantic Sensor Web) with associated domain metadata, capable of autonomous processing and it is a building block of a âworldwide distributedâ real time database, to be processed through realtime queries. The vision of the Semantic Sensor Web promises to unify the real and the virtual world by integrating sensor technologies and Semantic Web technologies. Sensors and their data will be formally described and annotated in order to facilitate the common integration, discovery and querying of information. Since this semantic information ultimately needs to be communicated by the sensors themselves, one may wonder whether existing techniques for processing, querying and modeling sensor data are still applicable under this increased load of transmitted data. In the following of this paper we introduce the state of the art in data querying over network of data providers. In Sect. 2 we present the software architecture of a data integration system in which we added complex query processing features. Sect. 3 introduces the case study in which we deployed our system: the study of short term effect of air pollution on health. Sect. 4 presents the detailed implementation of the querying features together with results on real data sets. Finally, Sect 5 presents the conclusions and future work. 1.1 State of the Art This paper stems from the work presented in [12], in which is presented a software system aimed at forecasting the demand of patient admissions on health care structures due to environmental pollution. The target users of this decision sup-port tool are health care managers and public administrators, which need help in resource allocation and policies implementation. The key feature of that system was the algorithmic kernel, to perform time series analysis through Autoregressive Hidden Markov Models (AHMM) [7]. The scenario in which the system has been deployed is the research project LENVIS1, which is aimed to create a network of services for data and information sharing based on heterogeneous and distributed data sources and modeling. One of the innovations brought by LENVIS is the âservice oriented business intelligenceâ, i.e. an approach to Business Intelligence in which the information presented to the user comes from data processing that is performed online, i.e. data are extracted under request of the applications, and on the basis of data availability, i.e. data are exchanged through web services, which does not guarantee response time neither availability. Such a complex environment, in which data sources are distributed over the internet, is common to several problems and has been faced by different approaches. One of them is that of [13], in which âmonitoring queriesâ continuously collect data about spatially-related physical phenomena. An algorithm, called Adaptive Pocket Driven Trajectories, is used to select data collection paths based on the spatial layout of sen1 LENVIS Localised environmental and health information services for all. FP7-ICT-2007-2. Project number 223925. www.lenvis.eu 73",oceanology,15,not included
44ce98492713648fef9446f779de56029f432763,to_check,semantic_scholar,,2006-01-01 00:00:00,semantic_scholar,effectiveness of collaborative learning in online teaching,https://www.semanticscholar.org/paper/44ce98492713648fef9446f779de56029f432763,"This paper describes how e-learning is becoming popular and used as an alternative means of solving problems in education. E-learning is usually used in distance learning and may be used to replace conventional classroom teaching. Many educational institutions use Internet for collaborative learning in a distributed educational process. It has been known that traditional communications media can be replaced by electronic communication for the whole educational process and in particular, to assess the role of collaborative learning in a distributed education environment. It has been found that a distributed educational process naturally supports collaborative learning environments in which students and tutors interact and provide essential support for students studying at a distance. The tutorâs feedback to students help the learning process and there is indication that tutors are happy to work in the new environment. It is therefore suggested that âblendedâ online teaching â a combination of the use of the Internet as a medium of instruction and tutors to do face-to-face teaching via a collaborative learning approach â may be implemented to achieve enhanced distance-student performance. INTRODUCTION There is a number of definitions for e-learning. For example, Soekartawi et al. (2002) defined elearning as: â... a generic term for all technologically supported learning using an array of teaching and learning tools as phone bridging, audio and videotapes, teleconferencing, satellite transmissions, and the more recognized web-based training or computer aided instruction also commonly referred to as online courses...â. Today, e-learning has become an extremely popular alternative means of delivering educational services worldwide mainly because it is seen as a means of resolving significant educational problems that cannot be solved by conventional means. E-learning is particularly used in open and distance learning (ODL) as it can provide flexible education for those who cannot attend regular schooling particularly because they are unable to leave their work to attend regular conventional classes. Additionally, in an archipelagic country like Indonesia, where the bulk of the population is spread over thousands of islands, educational services are made more accessible through ODL. Most ODL programmes in Southeast Asia, particularly in Indonesia, deploy any one or a combination of the following media: print, radio, television, audiocassettes, videocassettes and computers. Generally, the course materials of ODL programmes in the region are largely print-based. It is likely that this will continue for a long time. However, by increasing the use of computers and information and communication technology (ICT), the role of the printed-based course materials will be replaced gradually by e-learning. MOJIT Effectiveness Of Collaborative Learning In Online Teaching* 69 The development of ICT is moving rapidly because of its pertinent role in providing education to the masses. In fact, by effectively deploying ICT, the educational institutions concerned can cope with, and cater to, the expanding student population. In general, it has been observed that the use of ICT in education in Indonesia has resulted in relatively successful outcomes. There have been numerous problems, however, but there is one significant issue that may be cited here because of its implications to future endeavours in education in Indonesia and in the Southeast Asian region as well. This issue has direct influence over the learning process. It is called transactional distance. The physical separation of teacher and student is no longer a problem given the developments in ICT. However, with ICT use, transactional distance can easily result in a misunderstanding and miscomprehension of the concepts to be learned. In fact, it can even lead to the mal-education of people, following a lack of appropriate communication between learner and teacher. If there is no communication, however, this transactional distance between learner and teacher becomes wider. THE NEW LEARNING PARADIGM In 1995, the International Council on Distance Education (ICDE) conducted what it called an anecdotal, worldwide survey to determine the nature, reality and pace of the shift in the learning paradigm (ICDE, 1996). The survey noted the following clear signs: o A shift from objective knowledge to constructed knowledge. o A shift from an industrial-based to a knowledge-based society. o A shift in education missions from providing instruction to providing learning. o A shift in technologically-mediated procedures of communication and learning. o A shift from âcurrent college and university models to as yet undetermined structures.â Over the last five years, the fifth observation of ICDE has become very clear. The âundetermined structureâ at the time of the ICDE survey has come to be known as the virtual learning structure. Experts made similar observations from the ICTsector at about the same time period (Soekartawi et al., 2002). The importance of the virtual campus in the Indonesian context has been reported by Soekartawi (2002, 2002a, 2003) and Haryono & Alatas (2002). According to Garmer & Firestone (1996), due to the development of ICT, the paradigm for learning is shifting away from the traditional notion that ââknowledgeâ is transferred from teacher to student within the confines of the classroom where the focus of the teaching-learning process before was the teacher; now, it is shifting to the learner. A new understanding of learning places the learner at the centre of the learning process, with the teacher serving an important supporting role in facilitating the process (Garmer & Firestone, 1996). In this new paradigm, successful learning is measured by the individualâs ability to apply appropriate tools and information to solve problems in real life. There is a challenge in this new concept of learning. It is necessary to unlearn old habits and notions of how learning should be structured and to develop new habits of instruction that motivate learners to take greater control over their own education (Garmer & Firestone, 1996). This is what generally known by the âfirst concernâ which should be to understand the learnerâs motivations and goals in order that we can expand opportunities for learning. This new paradigm has critical implications for the use of ICT (e-learning) in education, particularly for ODL. For one, it requires that the learner take on more responsibility and autonomy in his learning. This is something that the learner may not be comfortable with or prepared to assume completely at this time. For another, teachers will have to give up control over the learning process and take on a new role similar to that of educational coaches who spend more time on the sidelines MOJIT Effectiveness Of Collaborative Learning In Online Teaching* 70 watching and maybe making plans for a more effective learning environment. The new paradigm has also highlighted an important issue. There is a need for all learners to upgrade their skills, particularly skills to learn on their own. This largely requires an ability to seek, understand and use information, which, in turn, requires the ability to use technology. In todayâs world, for example, one must be computer literate to gain access to information and new knowledge. COLLABORATIVE LEARNING One of the critical factors in online learning is the quality issue. Many efforts can be used to meet this issue, among them obtaining feedback evaluation from the student. Further, a crucial aspect of successful distance education is the quality of feedback on student assignments. The electronic assignment handling system pioneered in these trials has now been adopted by the Open University at large and will serve a big number of students. Collaborative learning, therefore, can be used to improve the issue of quality in distance learning. Collaborative learning is an essential ingredient in the recipe to create an ""effective learning environment"" as it provides learners with the opportunity to discuss, argue, negotiate and reflect upon existing beliefs and knowledge. The learner is ""...involved in constructing knowledge through a process of discussion and interaction with learning peers and experts...."" Harasim (in Thomas, 1999, p.51). To facilitate collaboration so that personal knowledge can be constructed, there needs to be a purpose for the collaboration and the purpose needs to be meaningful to the learner. Thus, it is important that an appropriate context is set for the collaborative activity, for example, assigning a ""real world"" task for learners or a problem to which all learners can relate. In addition to setting the context, there needs to be a vehicle through which collaboration can take place. In traditional faceto-face educational settings, collaboration mostly occurs through conversation, that is, individuals interacting with one another via the use of language. Therefore, in terms of creating an effective learning environment, four attributes surface as being paramount: â¢ Providing opportunities to foster personal construction of knowledge â¢ Setting an appropriate context for learning to create the opportunities. â¢ Facilitating collaboration amongst learners. â¢ Using conversation to facilitate collaboration. Petre (1998) argued that while superficially it might appear that distance learning/education is the domain of the distance educator, the aims of educational institutions are similar in ideology; according to Thomas (1999), only the implementations differ. However, what distance educators brings to this arena is the experience of how to interact with students and tutors who are geographically and temporally remote from a campus as well as from one another. The ultimate objective is to diminish and even remove the barriers that remoteness erects. In general, the learning environment in the Open University can be described",oceanology,16,unknown
e61e75ec370207ed0dca2e81307e8901d09c0493,to_check,semantic_scholar,,2013-01-01 00:00:00,semantic_scholar,enabling customer experience and front-office transformation through business process engineering,https://www.semanticscholar.org/paper/e61e75ec370207ed0dca2e81307e8901d09c0493,"In the past, the scope of business processes has been circumscribed to the industrialization of enterprise operations. Indeed, Business Process Management (BPM) has focused on relatively mature operations, with the goal of improving performance through automation. However, in todayâs world of customer-centricity and individualized services, the richest source of economic value-creation comes from enterprise-customer contacts beyond transactions. Consequently, process has recently moved out of its traditional court and is becoming prevalent in less traditional competences such as marketing operations, customer-relationship management, campaign creation and monitoring, brand management, sales and advisory services, multi-channel management, service innovation and management life-cycle, among others. These competences host customerenterprise co-creation activities characterized by innovation, human creativity, and new technologies. Above all, these work-practices call for continuous differentiation, instead of âpouring concreteâ on emerging business processes. While BPM will continue to make important contributions to the factory of enterprises, Business Process Engineering (BPE) is chartered to provide a holistic approach to new opportunities related to the life-cycle of enterprise customers and the transformation of so-called Front-Office Operations. More broadly, Business Process Engineering fosters a new space for the multidisciplinary study of process, integrating individuals, information and technology, and it does so with the goal of engineering (i.e., designing and running) innovative enterprise operations to serve customers and improve their experiences. Furthermore, given past challenges in the Back-Office, it is imperative that managers focus on processes in the Front-Office where the software industry has jumped into with solutions that bury key processes within applications, thus making differentiation and agility very difficult. BPE stresses the critical importance of the integration of Information and Behavior and it is this goal that links it with Business Informatics: the information process in organizations and society. Since behavior and information are complementary and inseparable domains of concern, current approaches to decision making based on data-only evidence should be reexamined holistically: it may be catastrophic to explicate or predict the behavior of organizations or individuals meaningfully by insisting on the ongoing divorce across the two domains. In particular, Business Informatics and Business Process Engineering offer an opportunity to address potential benefits of âbig dataâ and âbusiness analyticsâ beyond the IT domain. Having IEEE lead these directions means an opportunity for stimulating new research and practice on the most fundamental problems that enterprises and customers face today in dealing with each other. Keywordsâ business process engineering; customer experience; business process management; business informatics; enterprise engineering I. PROCESS IS OUT OF THE INDUSTRIALIZATION BOX Business process has been at the center of the stage in both research and industry for several decades. Under the brand of Business Process Management (BPM), business process has attracted a great deal of attention from many practitioners and scholars. BPM has been defined as the analysis, design, implementation, optimization and monitoring of business processes [70], [219], [79], [229], [230]. In [266], Van der Aalst defined some targets of BPM: â ... supports business processes using methods, techniques, and software to design, enact, control and analyze operational processes involving humans, organizations, applications, documents and other sources of informationâ . While the above definitions are quite comprehensive and broad, in reality most BPM research and industry activity has grwon upon the motivation of reducing operating costs through automation, optimization and outsourcing. There are a several schools of thought and practice (such as lean, lean sixsigma, and others [172], [6], [4], [5]) and a myriad of related literature in the last 40 years that serve to illustrate the focus on cost contention. Around the middle of the past decade, T. Davenport stated in a celebrated Harvard Business Review paper [54] that processes were being âanalyzed, standardized, and quality checkedâ, and that this phenomenon was happening for all sort of activities, stated in Davenportâs own terms: âfrom making a mouse trap to hiring a CEOâ. The actual situation is that industry investment and consequential research have stayed much more on âtrapping the mouseâ than in differentiating customer services through innovative and more intelligent processes, let alone hiring CEOs. This may be explained partly from Davenportâs own statements: âProcess standards could revolutionize how businesses work. They could dramatically increase the level and breadth of outsourcing and reduce the number of processes that organizations decide to perform for themselvesâ (bold face is added here for emphasis). With the advent of different technologies such as mobile, cloud, social media, and related capabilities that have empowered consumers, the classical approach and scope of business process have begun to change quickly. Organizations are adopting new operating models [100] that will drastically affect the way processes are conceived and deployed. As stated by many authors in the last four decades, business process work is supposed to cover all competences in an organization, irrespective of the specific skills from human beings participating in such operations. However, in an unpublished inspection of about 1,300 papers conducted by 1 Van der Aalst excludes strategy processes from BPM, a remarkable point that will be revisited in more depth later in this paper. the author and some of his collaborators 2 , most process examples shown in the literature deal with rather simple forms of coordination of work, mostly exhibiting a flow structure and addressing administrative tasks (like those captured in early works on office information systems). Furthermore, the examples provided usually deal with rather idealized operations, probably offered as simple examples with the only purpose of illustrating theoretical or foundational research results. Thus, radically simplified versions of âmanaging an orderâ, âapproving a formâ, âprocessing a claimâ, âpaying a providerâ, âdelivering an orderâ etc. are among the most popular examples of processes found in the literature. The lack of public documentation of substantial collections of real-world processes is remarkable. The authors in [106] both confirmed the dominant focus on simple business processes and also suggested potential practical consequences of related research: â... there is a growing and very active research community looking at process modeling and analysis, reference models, workflow flexibility, process mining and process-centric service-oriented architecture (SOA). However, it is clear that existing approaches have problems dealing with the enormous challenges real-life BPM projects are facing [ ... ] Conventional BPM research seems to focus on situations with just a few isolated processes ...â. Of course, the list of available real-world processes would be a lot richer if one included the set defined by enterprise packaged applications [219]. However, this comprehensive collection is proprietary because it constitutes a key piece of intellectual capital coming from software vendors or integrators in the industry. The traditional focus on process has also raised much controversy. At the S-BPM ONE Conference in 2010, a keynote speaker [176] remarked: âLet me be as undiplomatic as I possibly can be without being offensive [...] The academic community is as much to blame [...] as the vendors of BPM systems, who continue to reduce the task of managing business processes to a purely technological and automation-oriented levelâ. While other authors in the same conference debated âwho is to blameâ very animatedly [78], [234] it is important to highlight that the statement from Olbrich (in bold face above for emphasis) reinforces that BPM has mostly followed the obsession of automation and optimization by means of Information Technology. A detailed inspection of the extant literature confirms that business process work has been devoted to a rather small fraction of the actual variety and complexity found in enterprise behavior. This behavior enacts many valuegenerating capabilities that organizations cultivate based on skills provided by their own workforces and through rich interactions with other enterprise stakeholders, particularly customers. The following points offer a simplified summary: 2 At the time of this publication in IEEE, the mentioned work still remains unpublished. The co-authors are L. Flores and V. Becker both from IBM. (1) Business process research in Computer Science has been traditionally focused on certain classes of enterprise operations, mostly involving simple coordination mechanisms across tasks. This type of coordination and the overall behavior represented in underlying models reflect very much an âassembly lineâ where work is linearly synchronized to deliver a desired artifact or outcome. Simplicity of the choreography is ensured by removing any form of overhead in communication when moving from one stage to the next. Unlike other more complex business processes, many software applications do have this simplified structure. In fact, a trend since the early 2000âs is to separate the specific application logic from the coordination / choreography needed across modules, and both of them from the actual data contained in a data-base management system. Different foundations and a plethora of languages have been created to capture this semantics of coordination such as Business Process Modeling Notation (BPMN), Business Process Execution Languag",oceanology,17,unknown
635c01ca9f63bac692d2b9e4e6f4bdaf9aef4ce7,to_check,semantic_scholar,,2003-01-01 00:00:00,semantic_scholar,tactical insertion mission planning and rehearsal using virtual reality simulation,https://www.semanticscholar.org/paper/635c01ca9f63bac692d2b9e4e6f4bdaf9aef4ce7,"Systems Technology, Inc. (STI) has developed a versatile new system for parachute mission planning and rehearsal, combining the validated technology of STI's PC-based PARASIMï£ª parachute simulation system with real-time interactive networking, powerful scene generation graphics tools, and terrain-correlated wind fields. This Tactical Insertion Mission Planning and Rehearsal Simulator (TIMPARS) was developed under the SBIR program, funded by the US Special Operations Command (SOCOM). The TIMPARS system rests on four cornerstones: the PARASIM ï£ª simulation software, the real-time interactive network, the scene generation toolkit, and the terrain-correlated wind generation module. These elements combine to produce a system with which users can utilize geospecific terrain data and imagery to recreate a real-world site as a simulation scene, input actual or forecasted wind speeds and directions at altitude above the chosen location to generate a terrain-correlated wind field specific to the simulation scene, and then plan and rehearse a mission in a real-time simulation environment with multiple live participants interacting in the same virtual space. BACKGROUND STI's original parachute simulator was developed for use by smokejumpers, US Forestry Service airborne firefighters. Designed to teach round and ramair canopy control, this early version employed rudimentary graphics with a fixed monitor; users stood before the display monitor and pulled simple toggle lines to maneuver in the simulation. Despite the austere configuration, this version provided the minimum cues required to teach parachute flight safely at low cost. In 1996, STI launched a major development effort to incorporate new photo- realistic graphics and head-mounted display/virtual reality technology into the simulator. Subsequent development efforts produced malfunctions procedures software, riser controls, harness switches, and additional simulator improvements. The implementation of these enhanced simulators by the US Marine Corps (USMC) and the Military Freefall School in Yuma, AZ, resulted in a drastic drop in the rate of training injuries. In particular, the USMC First Force Reconnaissance Company experienced a 75% reduction in main canopy cutaways after implementing the enhanced simulator in the MC-5 static line deployed ram-air parachute system (SLDRAPS) transition course at Camp Pendleton, CA. TIMPARS PROJECT",oceanology,18,not included
ee2be26c5e2bfab3885adc0e6c70e4583435bd3e,to_check,semantic_scholar,,2003-01-01 00:00:00,semantic_scholar,how to harness the grid with ogsa - tutorial proposal,https://www.semanticscholar.org/paper/ee2be26c5e2bfab3885adc0e6c70e4583435bd3e,"Summary and Conclusion As a conclusion to this tutorial, we will share lessons learnt from our experience developing with OGSA and highlight the limitations as well as the benefits of deploying OGSA middleware. In particular, we will examine the behaviour of the crystal polymorph prediction system operating in a Grid environment and consider how effectively the implementation exploits available resources. We also discuss practical and political considerations that arise from âreal worldâ Grid environments where technical arguments are often compromised by the needs and preferences of different users, organizations and domain administrators. Conduct of tutorial Delivery The tutorial will consist mostly of a talk support by a Powerpoint presentation. We also intend to illustrate some concepts with live software demonstrations: the first outlining the process of creating a simple OGSA service using the Globus Toolkit 3.0 (GT3) from simple interface description through to stub generation and wrapping an implementation using the delegation model; the second demonstrating the crystal polymorph application running in a simulated Grid environment. This will enable participants to experience Grid middleware from a developerâs perspective and lend some reality to the concepts and mechanisms the tutorial covers.",oceanology,19,not included
0fb400e80dcda882fc293327c5c12e59fd18af93,to_check,semantic_scholar,IWUC,2006-01-01 00:00:00,semantic_scholar,on-demand loading of pervasive-oriented applications using mass-market camera phones,https://www.semanticscholar.org/paper/0fb400e80dcda882fc293327c5c12e59fd18af93,"Camera phones are the first realistic platform for the development of pervasive computing applications: they are personal, ubiquitous, and the builtin camera can be used as a context-sensing equipment. Unfortunately, currently available systems for pervasive computing, emerged from both academic and industrial research, can be adopted only on a small fraction of the devices already deployed or in production in the next future. In this paper we present an extensible programming infrastructure that turns mass-market phones into a platform for pervasive computing. 1 Mobile phone: a platform for pervasive computing Pervasive computing tries to make M. Weiserâs vision [1] a reality by saturating the environment with computing and communication devices: the most of the infrastructure is often invisible and supports userâs activities with an interaction model that is strongly human-centric. Today, almost fifteen years later, despite significant progresses in both hardware and software technologies, this vision is still not completely realizable or economically convenient. Supporting the interaction between users and the environment can be greatly simplified if we relax the interaction model and include a personal device as the access medium. Mobile phones are the most obvious candidates: they are in constant reach of their users, have wireless connectivity capabilities, and are provided with increasing computing power [2]. Even better results can be achieved with those phones that are equipped with a camera. Instead of manually getting information or editing configurations, users can point physical objects to express their will of using them: taking a picture of the objects would suffice to setup the link with the offered services. Relaying on an image acquisition device does not impose a strict limit to the share of possible users, since an always growing number of commercially available mobile phone is equipped with an integrated camera: according to recent studies [3], over 175 million camera phones were shipped in 2004 and, by the end of the decade, the global population of camera phones is expected to surpass 1 billion. ? This work is partially supported by the Italian Ministry for Education and Scientific Research (MIUR) in the framework of the FIRB-VICOM project. However, the acquisition of context-related information through images is not a trivial task, especially with resource-constrained devices. To ease the recognition process, objects can be labeled with visual tags readable by machines. Once decoded, visual tags either directly provide information about the resource they are attached to or, if the amount of information is too large, they act as resource identifiers that can be used to gather information from the network. In this paper, we describe the design and the implementation of POLPO 1 (Polpo is On-demand Loading of Pervasive-Oriented applications), a software system that turns mass-market phones into a platform for the development of pervasive applications. With POLPO, a phone with a built-in camera and compatible with the Java 2 Micro Edition (J2ME) platform is able to get context information by decoding visual tags attached to real-world objects. POLPO supports dynamic loading and installation of custom applications used to interact with the desired resources. 2 Background and contribution In this section we summarize the most relevant solutions based on visual tags and the contribution of our system in this field. Cybercode [4] is a visual tagging system based on a two-dimensional barcode technology. The system has been used to develop several augmented reality applications where the physical world is linked to the digital space trough the use of visual tags. Cybercode is one of the first systems where visual tags can be recognized by low-cost CCD or CMOS cameras, without the need for separate and dedicated readers. Each Cybercode symbol is able to encode 24 or 48 bits of information. The system has been tested with notebook PCs and PDAs. In [5] the author presents a system that turns camera-phones into mobile sensors for two-dimensional visual tags. By recognizing a visual tag, the device can determine the coded value, as well as additional parameters, such as the viewing angle of the camera. The system includes a movement detection scheme which enables to use the mobile phone as a mouse (this is achieved by associating a coordinate scheme to visual tags). The communication capability of the mobile phone is used to retrieve information related to the selected tag and to interact with the corresponding resource. Tag recognition and motion detection algorithms were implemented in C++ for Symbian OS. The Mobile Service Toolkit (MST) [6] is a client-server framework for developing site-specific services that interact with usersâ smart phones. Services are advertised by means of machine-readable visual tags, which encode the Bluetooth device address of the machine that hosts the service (Internet protocols addressing could be supported as well). Visual tags also include 15 bits of application-specific data. Once the connection has been established, MST servers can request personal information to the client to provide personalized services. Site-specific services can push user interfaces, expressed with a markup language similar to WML (Wireless Markup Language), to smart phones. MST also provide thin-client functionality: servers can push arbitrary graphics 1 The Italian name for the octopus vulgaris, a cephalopod of the order octopoda, probably the most intelligent of the invertebrates. to the phoneâs display which in turn forwards all keypress events to the server. The client-side is written in C++ and requires Symbian OS. A similar approach is described in [7], where the authors propose an architecture for a platform that supports ubiquitous services. Real-world objects are linked to services on the network through visual tags based on geometric invariants that do not depend on the viewing direction [8]. But differently from other solutions, image processing does not take place on the userâs device: pictures are sent to a server where they are elaborated and converted into IDs. Instead of using two-dimensional barcodes, an alternative way of performing object recognition is the one based on radio frequency identification (RFID): small tags, attached to or incorporated into objects, that respond to queries from a reader. However this solution, that can be useful in many pervasive computing scenarios, is not particularly suitable when the interaction is mediated by mobile phones, that lack the capability of reading RFIDs. In our opinion, currently available solutions present two major drawbacks: i) they are limited to specific hw/sw platforms (i.e. Symbian OS), excluding most of the models of mobile phones already shipped and in production in the near future; ii) the software needed to interact with the environment is statically installed onto the mobile phone and cannot be dynamically expanded, e.g. to interact with new classes of resources. We designed and developed a system for pervasive computing based on visual tags that overcomes these constraints as follows. Compatibility with J2ME The system runs on devices compatible with the J2ME platform. This environment is quite limited in terms of both memory and execution speed, but also extremely popular (nearly all mobile phones produced). This required the implementation of a pure Java decoder of visual tags for the J2ME environment. Downloadable applications Our system is based on the idea that the interaction with a given class of resources, e.g. printers, public displays, etc., takes place through a custom application. New custom applications can be downloaded from the network and installed onto the userâs device as needed. This brings two advantages: i) the classes of resources that can be used do not have to be known a priori; ii) the userâs device, that is resource constrained, includes only the software needed to interact with the services actually used. The J2ME platform comprises two configurations, few profiles, and several optional packages. The J2ME configurations identify different classes of devices: the Connected Device Configuration (CDC) is a framework that supports the execution of Java application on embedded devices such as network equipment, set-top boxes, and personal digital assistants; the Connected Limited Device Configuration (CLDC) defines the Java runtime for resource constrained devices as mobile phones and pagers. Our systems runs on top of the version 1.1 of the CLDC, that provides support for floating point arithmetics (unavailable in version 1.0). The adopted profile is the Mobile Information Device Profile (MIDP) that, together with CLDC, provides a complete Java application environment for mobile phones. 3 System architecture POLPO requires that physical resources are labeled with visual tags, and that a program providing access to POLPO functionalities is installed onto the userâs device. This program has the following primary functions: â Decoding of visual tags. The image captured with the built-in camera is processed to extract the data contained into the visual tag. â Management of custom applications. The program downloads and installs the custom application required to interact with a resource. Usually, resources of the same kind share the same custom application (i.e., a single application is used to interact with all printers, another is used with public displays, etc). â Management of userâs personal data. In many cases, applications need information about the user to provide customized services. For this reason, the software installed on mobile phones includes a module that manages userâs personal data and stores them into the persistent memory. Managed data comprise userâs name, telephone number, email address, homepage, etc. Each resource is identified and described by a Data Matrix visual tag. Da",oceanology,20,unknown
cdd8028ef1569a9c7191e806839ed2bb261efdb9,to_check,semantic_scholar,,2002-01-01 00:00:00,semantic_scholar,title: an integrated design environment to evaluate power/performance tradeoffs for sensor network applications,https://www.semanticscholar.org/paper/cdd8028ef1569a9c7191e806839ed2bb261efdb9,"Networks of inexpensive, low-power sensing nodes that can monitor the environment, perform limited processing on the samples, and detect events of interest in a collaborative fashion are fast becoming a reality. Examples of such monitoring and detection include target tracking based on acoustic signatures and line-of-bearing estimation, climate control, intrusion detection, etc. The advances in low-power radio technology are making wireless communication within sensor networks an attractive option. However, it is typically difficult or impossible to replenish energy resources available to a portable sensor node, once it is deployed. Maximizing the life of sensor nodes is an overriding priority, and different energy optimization techniques are being developed to addresses computation/communication tradeoffs. A large number of research efforts are focusing on different aspects of the general problem of designing efficient sensor network-based systems where the metrics to measure efficiency vary from system to system. With technological advancements such as silicon-based radios expected to become a reality in a few years, designers of sensor network-based systems will be faced with an extremely large set of design decisions. Each choice will affect the overall system performance in ways that might not always be cleanly modeled. In addition to the research challenges in design and optimization, the practical aspects of designing real-world sensor networks will become equally important. For example, the ability of the design framework to allow rapid specification and evaluation of a particular network configuration is crucial for a more exhaustive exploration of the design space. A design environment for future sensor networks should provide tools and formal methodologies that will allow designers to model, analyze, optimize, and simulate such systems. In the context of our work, design and optimization of a sensor network application involves determining the task allocation to different sensor nodes and the inter-node communication mechanism. Design of the sensor node hardware itself is also an area of active research. However, we assume that a set of node architectures is already available to our enduser, and the design problem is restricted to using the available hardware (with flexibilities, if any, such as dynamic voltage scaling) to efficiently implement the target application. We take a simple, seven-node wireless sensor network for acoustic detection [5] (Automatic Target Recognition) as the case study and demonstrate (i) a modeling and simulation methodology for a class of sensor networks, and (ii) a software framework that implements our methodology. Our formal application model is illustrated in Fig. 1. We use a data flow graph representation to model the computing tasks and their data dependencies. The end-to-end application consists of two types of such data flow graphs: the first type denotes the processing that has to be performed for each sample before it is ready to be âfusedâ with results from other sensors, and the second type represents the computing involved in data fusion. Specifically , in our case study, a Fast Fourier Transform (FFT) operation is the only task that is performed on each block of sampled data. The outputs of FFTs from all seven nodes are provided as an input to the collaborative computing part, which consists of delay and sum beamforming (BF), and lineof-bearing (LOB) estimation. The result of collaborative computation in such a cluster model of sensor networks has to be transmitted to some observer. This is accomplished by designating one of the nodes as the cluster-head, which could be equipped with more powerful communication facilities than other sensor nodes. All communication within our cluster is one-hop, and processing of a particular data sample (FFT/BF/LOB) occurs either on its home node, or the cluster-head, or partly on both. Simulating a completely specified instance of the above class of sensor networks involves many challenges. None of the existing network simulators to our knowledge models the internal architecture of the processing nodes in the network. This is because the focus of most network simulators is on protocol development and empirical analysis. Except in areas such as high-speed router design, the node internals have little or no impact on decisions related to protocol design. Also, processor simulators do not model the environment outside the chip boundary. Therefore, to obtain detailed and accurate performance estimates for the entire system, we propose a technique to automatically generate network scenarios based on results from low-level node simulations. The network simulator is configured using the generated scenarios, and the individual simulation results are merged and presented to the end-user as a whole. Such a âhorizontalâ simulation is accomplished through the use of a central data repository for model information, which means that the simulators never have to directly interact with each other. The simulators we integrate provide estimates about energy consumption, thereby assisting in a power/performance analysis of a specific system configuration. Our design framework facilitates multi-granular simulation, i.e., simulating the same system configuration by using simulation models at different levels of abstraction. Typically, coarse-grained models provide rapid estimates, but need to make approximations about system behavior that might not be very accurate. For such a scenario, we demonstrate a form of analytical model refinement (see Fig. 2), i.e. the data from low-level simulations can be automatically processed to âdistillâ parameter values used by high-level simulators. Naturally, the exact processing has to be specified by someone with knowledge of the analytical model semantics. Our design environment provides the following capabilities to the user: â¢ To graphically describe the target application, node architecture, network configuration, and task-to-node mapping. â¢ To change (reconfigure) the system model to explore alternate designs. Some of the parameters that can be manipulated by the designer include receive/transmit power of the radio, voltage/frequency setting of the processor, cluster geometry, propagation models, etc. â¢ To automatically simulate a design using a coarse system model. â¢ To automatically configure and execute low-level simulators for the node (Wattch [3]) and the network (ns-2 [4]) and obtain system-wide energy and latency estimates. â¢ To automatically update high-level model parameters using low-level simulation statistics. â¢ To graphically visualize simulation results and facilitate (manual) identification of power/performance bottlenecks in the design. This work is an illustration of the general approach of the MILAN [1] project. A modeling and simulation framework based on the first version of the 7-node ATR system model was implemented in [2]. The primary focus of that work was a prototype demonstration of simulator integration and model refinement. Therefore, the system model itself lacked generality. Also, we use a relatively more detailed version of the high-level estimator implemented for [2]. This work represents a significant step towards the ultimate goal of a design environment for automatic optimization and synthesis of sensor network applications.",oceanology,21,unknown
336d84f6e7a7a398c70e924b4677c1fee7f3b81d,to_check,semantic_scholar,,2001-01-01 00:00:00,semantic_scholar,the advantages of micro simulation in traffic modelling with reference to the n4 platinum toll road,https://www.semanticscholar.org/paper/336d84f6e7a7a398c70e924b4677c1fee7f3b81d,"Micro simulation has been used to a limited extend in the past in South Africa, despite major advantages of this tool above static modelling and itâs popularity oversees. The main advantages are dynamic modelling and visual interpretation of the traffic conditions. This tool is ideal to test geometric designs, traffic controls and a variety of traffic management measures. These include incident and congestion management, road works, ramp metering, VMS, etc. It is an extremely suitable tool to use when low cost solutions must be found because of severely limited infrastructure resources. Times that micro simulation was not able to calculate and show reliable traffic situations is over, various traffic simulation models have developed and have reached high quality standards. Micro simulation is about to gain a real market share all around the world; South Africa is following. Modelling toll plazas at interchanges on the N4 Platinum Toll Road is used to illustrate the advantages of micro simulation. Geometric design options, measures effecting toll throughput and traffic control options were evaluated in this example as well as the estimation of the expected life span of various options within a congested network. The package used in this study is AIMSUN2, an advanced micro simulation package widely used internationally that can interact with TRANSYT, SCOOT, EMME/2 and SATURN. AIMSUN2 has been applied to traffic impact analysis, traffic control measures, HOV-lanes, tolling and geometric design within the last three years in South Africa. It has also been used successfully to convey results of investigations to nontechnical people. 1. MICRO SIMULATION 1.1 Background on the development of micro simulation The microscopic traffic simulation models are based on the reproduction of the traffic flows simulating the behavior of the individual vehicles, this not only enables them to capture the full dynamics of time dependent traffic phenomena, but also to deal with behavioral models accounting for driversâ reactions. The underlying hypothesis is that the dynamics of a stream of traffic is the result of a series of driversâ attempts to regulate their speed and acceleration accordingly with information received. The driverâs actions resulting from the interpretation of the information received will consist on the control of the acceleration (braking and accelerating), the control of heading (steering) and the decision of overtaking the precedent vehicle either to increase the speed or to position themselves in the right lane to perform a maneuver (i.e. a turning). The origins of microscopic traffic simulation can be traced back to the early stages of digital computers. Although the basic principles were set up many years ago, with the seminal work of, among others, Robert Hermann and the General Motors Group in the early fifties, the computing requirements made them impractical until hardware and software developments made them affordable even on todayâs laptop computers. Most of the currently existing microscopic traffic simulators are based on the family of carfollowing, lane changing and gap acceptance models to model the vehicleâs behavior. Carfollowing models are a form of stimulus-response model, where the response is the reaction of the driver (follower) to the motion of the vehicle immediately preceding him (the leader) in the traffic stream. The response of the follower is to accelerate or decelerate in proportion to the magnitude of the stimulus at time t after a reaction time T. The generic form of the conceptual model is: response (t+T) = sensitivity * stimulus (t) Among the most used models are Hellyâs model (1), implemented in SITRA-B+, (2), Hermanâs model (3), or its improved version by Wicks (4), implemented in MITSIM, (5), the psycho-physical model of Wiedemann, (6), used in VISSIM (7), or the ad hoc version of Gipps (8), used in AIMSUN2 (9, 10). Other microscopic simulators such as INTEGRATION (11) and PARAMICS employ heuristic or other modeling not publicly available in analytic form. A common drawback of most of these models is that the model parameters are global i.e. constant for the entire network whereas it is well know that driverâs behavior is affected by traffic conditions. Therefore a more realistic car-following modeling for microscopic simulation should account for local behavior. This implies that some of the model parameters must be local depending on local geometric and traffic conditions. 1.2 What micro simulation is and the advantages thereof compared to static models The deployment of Intelligent Traffic Systems (ITS) requires support of complementary studies clearly showing the feasibility of the systems and what benefits should be expected from their operation. The large investments required have to be justified in a robust way. That means feasibility studies that validate the proposed systems, assess their expected impacts and provide the basis for sound cost benefit analyses. Microscopic traffic simulation has proven to be a useful tool to achieve these objectives. This is not only due to its ability to capture the full dynamics of time dependent traffic phenomena, but also being capable of dealing with behavioral models accounting for driversâ reactions when exposed to ITS systems. The advent of ITS has created new objectives and requirements for micro-simulation models. Quoting from Deliverable D3 of the European Commission Project SMARTEST [12]: âThe objective of micro-simulation models is essentially, from the model designers point of view, to quantify the benefits of Intelligent Transportation Systems (ITS), primarily Advanced Traveler Information Systems (ATIS) and Advanced Traffic Management Systems (ATMS). Micro-simulation is used for evaluation prior to or in parallel with on-street operation. This covers many objectives such as the study of dynamic traffic control, incident management schemes, real-time route guidance strategies, adaptive intersection signal controls, ramp and mainline metering, etc. Furthermore some models try to assess the impact and sensitivity of alternative design parametersâ. The analysis of traffic systems and namely ITS systems, is beyond the capabilities of traditional static transport planning models. Microscopic simulation is then the suitable analysis tool to achieve the required objectives. An example from a real case study where microscopic simulation was used to complement static modeling will help us to understand better how both levels may help the decisionmaker. The city of San Sebastian, in the North of Spain, completed recently a new urban freeway connecting two separated neighborhoods. Figure 1 shows the typical result of the planning study with an close up of the Amara neighborhood. The road network and the demand were modeled using the EMME/2 package. The figure displays the expected impacts of the new infrastructure highlighting in green the average flow reduction due to the redistribution of flows enabled by the new paths on the network, and in red the increase of flows attracted by these paths. A significant discharge in the level of congestion in the main road network was the foreseen impact of the new infrastructure, but the access to the new freeway in the East-West direction shows some undesirable side effects in the neighborhood (Amara) inside the rectangle. The solutions to these problems demands a close up to the subnetwork and take decisions at the level of traffic control and traffic management schemes, not excluding even a partial reshaping of part of the street network. This type of decision requires a more detailed modeling, able of reproducing in a very accurate way the traffic conditions, accounting for the interactions between the vehicular flows and the infrastructure, and obviously including the influence of the traffic lights, objective that can only be achieved by a microscopic traffic simulation model. Figure 1: Expected impact of the new infrastructure in San Sebastian with an close up of the Amara neighborhood Figure 2 displays the corresponding model built with the AIMSUN2 traffic simulation software, the EMME/2 sub model has been built automatically from the AIMSUN2 model by means of an interface between both systems. Figure 2: AIMSUN2 micro simulation model of the Amara neighborhood The type of information that micro simulation can provide for a further analysis is beyond the capabilities of traditional static models. The average flows from sections to sections turning movements) for the allowed movements at selected intersections in the model, speeds and delays for every simulated time interval can be obtained. The dynamic analysis for a time period is completed with values for other traffic variables or indicators of the quality of service as number of stops, time delayed at stops, average queue lengths, etc. Figure 3 provide a further insight on the capacity of analysis provided by dynamic simulation software. The graphic in this figure describes the evolution over time of average flows. The same type of graph can be produced for average queue lengths on a subset of selected sections in the model. Figure 3: The evolution of average flows over time 1.3 The ease of model building and data input (AIMSUN2) The recent evolution of the microscopic simulators has taken advantages of the state-of-theart in the development of object-oriented simulators, and graphical user interfaces, as well as the new trends in software design and the available tools that support it adapted to traffic modeling requirements. A proper achievement of the basic requirements of a microscopic simulator implies building models as close to reality as possible. The closer the model is to reality the more data demanding it become. This has been traditionally the main barrier Section Volumes (Veh/h) 0 200 400 600 80",oceanology,22,not included
9566723d1b3075cd14cabc8519a24a7eaa00cd5a,to_check,semantic_scholar,,2008-01-01 00:00:00,semantic_scholar,"using simulation tools for embedded software development class # 410 , embedded systems conference , silicon valley 2008",https://www.semanticscholar.org/paper/9566723d1b3075cd14cabc8519a24a7eaa00cd5a,"ion vs. Detail A key insight in building simulations is that you must always make a trade-off between simulator detail and the scope of the simulated system. Looking at some extreme cases, you cannot use the same level of abstraction when simulating the evolution of the universe on a grand scale as when simulating protein folding. You can always trade execution time for increased detail or scope, but assuming you want a result in a reasonable time scale, compromises are necessary. A corollary to the abstraction rule is that simulation is a workload that can always use maximum computer performance (unless it is limited by the speed of interaction from the world or users). A faster computer or less detailed model lets you scale up the size of the system simulated or reduce simulation run times. In general, if the processor in your computer is not loaded to 100%, you are not making optimal use of simulation. The high demands for computer power used to be a limiting factor for the use of simulation, requiring large, expensive, and rare supercomputers to be used. Today, however, even the cheapest PC has sufficient computation power to perform relevant simulations in reasonable time. Thus, the availability of computer equipment is not a problem anymore, and simulation should be a tool considered for deployment to every engineer in a development project. Simulating the Environment Simulation of the physical environment is often done for its own sake, without regard for the eventual use of the simulation model by embedded software developers. It is standard practice in mechanical and electrical engineering to design with computer aided tools and simulation. For example, control engineers developing control algorithms for physical systems such as engines or processing plants often build models of the controlled system in tools such as MatLab/Simulink and Labview. These models are then combined with a model of the controller under development, and control properties like stability and performance evaluated. From a software perspective, this is simulating the specification of the embedded software along with the controlled environment. For a space probe, the environment simulation could comprise a model of the planets, the sun, and the probe itself. This model can be used to evaluate proposed trajectories, since it is possible to work through missions of years in length in a very short time. In conjunction with embedded computer simulations, such a simulator would provide data on the attitude and distance to the sun, the amount of power being generated from solar panels, and the positions of stars seen by the navigation sensors. When the mechanical component of an embedded system is potentially dangerous or impractical to work with, you absolutely want to simulate the effects of the software before committing to physical hardware. For example, control software for heavy machinery or military vehicles are best tested in simulation. Also, the number of physical prototypes available is fairly limited in such circumstances, and not something every developer will have at their desk. Such models can be created using modeling tools, or written in C or C++ (which is quite popular in practice). In many cases, environment simulations can be simple data sequences captured from a real sensor or simply guessed by a developer. It should be noted that a simulated environment can be used for two different purposes. One is to provide âtypicalâ data to the computer system simulation, trying to mimic the behavior of the final physical system under normal operating conditions. The other is to provide âextremeâ data, corresponding to boundary cases in the system behavior, and âfaultyâ data corresponding to broken sensors or similar cases outside normal operating conditions. The ability to inject extreme and faulty cases is a key benefit from simulation. Simulating the Human User Interface The human interface portion of an embedded device is often also simulated during its development. For testing user interface ideas, rapid prototyping and simulation is very worthwhile and can be done in many different ways. One creative example is how the creator of the original Palm Pilot used a wooden block to simulate the effect of carrying the device. Instead of building complete implementations of the interface of a TV, mobile phone, or plant control computer, mockups are built in specialized user interface (UI) tools, in Visual Studio GUI builder on a PC, or even PowerPoint or Flash. Sometimes such simulations have complex behaviors implemented in various scripts or even simple prototype software stacks. Only when the UI design is stable do you commit to implementing it in real code for your real device, since this typically implies a greater programming effort. In later phases of development, when the hardware user interface and most of the software user interface is done, a computer simulation of a device needs to provide input and output facilities to make it possible to test software for the device without hardware. This kind of simulation runs the gamut from simple text consoles showing the output from a serial port to graphical simulations of user interface panels where the user can click on switches, turn knobs, and watch feedback on graphical dials and screens. A typical example is Nokiaâs Series 60 development kit, which provides a virtual mobile phone with a keypad and small display. Another example is how virtual PC tools like VmWare and Parallels map the display, keyboard, and mouse of a PC to a target system. In consumer electronics, PC peripherals are often used to provide live test data approximating that of a real system. For example, a webcam is a good test data generator for a simulated mobile phone containing a camera. Even if the optics and sensors are different, it still provides something better than static predetermined images. Same for sound capture and playback â you want to hear the sound the machine is making, not just watch the waveform on a display. Simulating the Network Most embedded computers today are connected to one or more networks. These networks can be internal to a system; for example, in a rack-based system, VME, PCI, PCI Express, RapidIO, Ethernet, IC, serial lines, and ATM can be used to connect the boards. In cars, CAN, LIN, FlexRay, and MOST buses connect body electronics, telematics, and control systems. Aircraft control systems communicate over special bus systems like MIL-STD-1553, ARINC 429, and AFDX. Between the external interfaces of systems, Ethernet running internet standards like UDP and TCP is common. Mobile phones connect to headsets and PCs over Bluetooth, USB, and IR, and to cellular networks using UMTS, CDMA2000, GSM, and other standards. Telephone systems have traffic flowing using many different protocols and physical standards like SS7, SONET, SDH, and ATM. Smartcards connect to card readers using contacts or contact-less interfaces. Sensor nodes communicate over standard wireless networks or lower-power, lower-speed interfaces like Zigbee. Thus, existing in an internal or external network is a reality for most embedded systems. Due to the large scale of a typical network, the network part is almost universally simulated to some extent. You simply cannot test a phone switch or router inside its real deployment network, so you have to provide some kind of simulation for the external world. You donât want to test mobile phone viruses in the live network for very practical reasons. Often, many other nodes on the network are being developed at the same time. Or you might just want to combine point simulations of several networked systems into a single simulated network. Network simulation can be applied at many levels of the networking stack. The picture below shows the most common levels at which network simulation is being performed. The two levels highlighted in green are the ones that are most useful for embedded software work on a concrete target model. Physical signalling Bit stream Packet transmission Network protocol Application protocol High-level application actions Analog signals, bit errors, radio modeling Clocked zeros and ones, CAN with contention, Ethernet with CSMA model Ethernet packets with MAC address, CAN packets, serial characters, VME data read/write TCP/IP etc. FTP, DHCP, SS7, CANopen Load software, configure node, restart Hardware/software boundary r r / ft r r The most detailed modeling level is the physical signal level. Here, the analog properties of the transmission medium and how signals pass through it is modeled. This makes it possible to simulate radio propagation, echoes, and signal degradation, or the electronic interference caused by signals on a CAN bus. It is quite rarely used in the setting of developing embedded systems software, since it complex and provides more details than strictly needed. Bit stream simulation looks at the ones and zeroes transmitted on a bus or other medium. It is possible to detect events like transmission collisions on Ethernet and the resulting back-off, priorities being clocked onto a CAN bus, and signal garbling due to simultaneous transmissions in radio networks. An open example of such a simulator is the VMNet simulator for sensor networks. Considering the abstraction levels for computer system simulation discussed below, this is at an abstraction level similar to cycle-accurate simulation. Another example is the simulation of the precise clock-by-clock communication between units inside a system-on-a-chip. Packet transmission passes entire packets around, where the definition of a packet depends on the network type. In Ethernet, packets can be up to 65kB large, while serial lines usually transmit single bytes in each âpacketâ. It is the network simulation equivalent of transaction-level modeling, as discussed below for computer boards. The network simulation has no knowledge of the meaning of the packets. It just passes opaqu",oceanology,23,unknown
79dd4e21811c1399a4525d82e16c8fbf23db3d51,to_check,semantic_scholar,Encyclopedia of Database Systems,1993-01-01 00:00:00,semantic_scholar,human-computer interaction,https://www.semanticscholar.org/paper/79dd4e21811c1399a4525d82e16c8fbf23db3d51,"Contents Foreword Preface to the third edition Preface to the second edition Preface to the first edition Introduction Part 1 Foundations Chapter 1 The human 1.1 Introduction 1.2 Input-output channels Design Focus: Getting noticed Design Focus: Where's the middle? 1.3 Human memory Design Focus: Cashing in Design Focus: 7 +- 2 revisited 1.4 Thinking: reasoning and problem solving Design Focus: Human error and false memories 1.5 Emotion 1.6 Individual differences 1.7 Psychology and the design of interactive systems 1.8 Summary Exercises Recommended reading Chapter 2 The computer 2.1 Introduction Design Focus: Numeric keypads 2.2 Text entry devices 2.3 Positioning, pointing and drawing 2.4 Display devices Design Focus: Hermes: a situated display 2.5 Devices for virtual reality and 3D interaction 2.6 Physical controls, sensors and special devices Design Focus: Feeling the road Design Focus: Smart-Its - making sensors easy 2.7 Paper: printing and scanning Design Focus: Readability of text 2.8 Memory 2.9 Processing and networks Design Focus: The myth of the infinitely fast machine 2.10 Summary Exercises Recommended reading Chapter 3 The interaction 3.1 Introduction 3.2 Models of interaction Design Focus: Video recorder 3.3 Frameworks and HCI 3.4 Ergonomics Design Focus: Industrial interfaces 3.5 Interaction styles Design Focus: Navigation in 3D and 2D 3.6 Elements of the WIMP interface Design Focus: Learning toolbars 3.7 Interactivity 3.8 The context of the interaction Design Focus: Half the picture? 3.9 Experience, engagement and fun 3.10 Summary Exercises Recommended reading Chapter 4 Paradigms 4.1 Introduction 4.2 Paradigms for interaction 4.3 Summary Exercises Recommended reading Part 2 Design process Chapter 5 Interaction design basics 5.1 Introduction 5.2 What is design? 5.3 The process of design 5.4 User focus Design Focus: Cultural probes 5.5 Scenarios 5.6 Navigation design Design Focus: Beware the big button trap Design Focus: Modes 5.7 Screen design and layout Design Focus: Alignment and layout matter Design Focus: Checking screen colors 5.8 Iteration and prototyping 5.9 Summary Exercises Recommended reading Chapter 6 HCI in the software process 6.1 Introduction 6.2 The software life cycle 6.3 Usability engineering 6.4 Iterative design and prototyping Design Focus: Prototyping in practice 6.5 Design rationale 6.6 Summary Exercises Recommended reading Chapter 7 Design rules 7.1 Introduction 7.2 Principles to support usability 7.3 Standards 7.4 Guidelines 7.5 Golden rules and heuristics 7.6 HCI patterns 7.7 Summary Exercises Recommended reading Chapter 8 Implementation support 8.1 Introduction 8.2 Elements of windowing systems 8.3 Programming the application Design Focus: Going with the grain 8.4 Using toolkits Design Focus: Java and AWT 8.5 User interface management systems 8.6 Summary Exercises Recommended reading Chapter 9 Evaluation techniques 9.1 What is evaluation? 9.2 Goals of evaluation 9.3 Evaluation through expert analysis 9.4 Evaluation through user participation 9.5 Choosing an evaluation method 9.6 Summary Exercises Recommended reading Chapter 10 Universal design 10.1 Introduction 10.2 Universal design principles 10.3 Multi-modal interaction Design Focus: Designing websites for screen readers Design Focus: Choosing the right kind of speech Design Focus: Apple Newton 10.4 Designing for diversity Design Focus: Mathematics for the blind 10.5 Summary Exercises Recommended reading Chapter 11 User support 11.1 Introduction 11.2 Requirements of user support 11.3 Approaches to user support 11.4 Adaptive help systems Design Focus: It's good to talk - help from real people 11.5 Designing user support systems 11.6 Summary Exercises Recommended reading Part 3 Models and theories Chapter 12 Cognitive models 12.1 Introduction 12.2 Goal and task hierarchies Design Focus: GOMS saves money 12.3 Linguistic models 12.4 The challenge of display-based systems 12.5 Physical and device models 12.6 Cognitive architectures 12.7 Summary Exercises Recommended reading Chapter 13 Socio-organizational issues and stakeholder requirements 13.1 Introduction 13.2 Organizational issues Design Focus: Implementing workflow in Lotus Notes 13.3 Capturing requirements Design Focus: Tomorrow's hospital - using participatory design 13.4 Summary Exercises Recommended reading Chapter 14 Communication and collaboration models 14.1 Introduction 14.2 Face-to-face communication Design Focus: Looking real - Avatar Conference 14.3 Conversation 14.4 Text-based communication 14.5 Group working 14.6 Summary Exercises Recommended reading Chapter 15 Task analysis 15.1 Introduction 15.2 Differences between task analysis and other techniques 15.3 Task decomposition 15.4 Knowledge-based analysis 15.5 Entity-relationship-based techniques 15.6 Sources of information and data collection 15.7 Uses of task analysis 15.8 Summary Exercises Recommended reading Chapter 16 Dialog notations and design 16.1 What is dialog? 16.2 Dialog design notations 16.3 Diagrammatic notations Design Focus: Using STNs in prototyping Design Focus: Digital watch - documentation and analysis 16.4 Textual dialog notations 16.5 Dialog semantics 16.6 Dialog analysis and design 16.7 Summary Exercises Recommended reading Chapter 17 Models of the system 17.1 Introduction 17.2 Standard formalisms 17.3 Interaction models 17.4 Continuous behavior 17.5 Summary Exercises Recommended reading Chapter 18 Modeling rich interaction 18.1 Introduction 18.2 Status-event analysis 18.3 Rich contexts 18.4 Low intention and sensor-based interaction Design Focus: Designing a car courtesy light 18.5 Summary Exercises Recommended reading Part 4 Outside the box Chapter 19 Groupware 19.1 Introduction 19.2 Groupware systems 19.3 Computer-mediated communication Design Focus: SMS in action 19.4 Meeting and decision support systems 19.5 Shared applications and artifacts 19.6 Frameworks for groupware Design Focus: TOWER - workspace awareness Exercises Recommended reading Chapter 20 Ubiquitous computing and augmented realities 20.1 Introduction 20.2 Ubiquitous computing applications research Design Focus: Ambient Wood - augmenting the physical Design Focus: Classroom 2000/eClass - deploying and evaluating ubicomp 20.3 Virtual and augmented reality Design Focus: Shared experience Design Focus: Applications of augmented reality 20.4 Information and data visualization Design Focus: Getting the size right 20.5 Summary Exercises Recommended reading Chapter 21 Hypertext, multimedia and the world wide web 21.1 Introduction 21.2 Understanding hypertext 21.3 Finding things 21.4 Web technology and issues 21.5 Static web content 21.6 Dynamic web content 21.7 Summary Exercises Recommended reading References Index",oceanology,24,unknown
f77d59740dfeb10e9b650ec8b1baba91fca70279,to_check,semantic_scholar,,2011-01-01 00:00:00,semantic_scholar,navibeam: indoor assistance and navigation for shopping malls through projector phones,https://www.semanticscholar.org/paper/f77d59740dfeb10e9b650ec8b1baba91fca70279,"We present our concept of an indoor assistance and navigation system for pedestrians that leverages projector phones. Digitally enhanced guides have many advantages over traditional paper-based indoor guides, most of all that they can be aware of their current context and display dynamic information. That is why particularly shopping malls recently started deploying indoor assistance applications for mobile phones, which also include support for navigation. Moreover, as we show in the paper, projected interfaces offer additional distinct advantages over static guides and even traditional or augmented reality mobile applications. We describe five concepts for a shopping mall indoor assistance system based on projector phones, comprising support for shop selection, precise way finding, âvirtual fittingâ of clothes, and context-aware and ambient advertisements. We then apply the concepts to a typical shopping scenario, where users wear the phone at their belt and constantly project the interface in front of them. Expected benefits of our system are that people find their way quicker, easier, and less distracted from their usual shopping experience. Finally, we discuss the technical feasibility of our envisioned implementation and research questions we are particularly interested in. INTRODUCTION Navigation and location-based services for pedestrians recently gained a lot of attention and are becoming rapidly adopted. Very popular among these are applications for location-based places recommendations and turn-by-turn navigation. While these applications mostly focus on outdoor navigation, less attention has been paid to the opportunities for providing indoor assistance with mobile devices. Especially in large complex buildings, e.g. shopping centers, in most cases static signs, You-Are-Here maps, or paper flyer maps are still the only available navigation assistance for visitors. Preliminary observations and interviews we conducted in nearby shopping centers show that, at least in Germany, available navigational aids are still as insufficient as Levine reported them to be almost 30 years ago [8]. Despite a lot of research projects that explored indoor assistance over the last decade, it was not before the beginning of 2011 that we saw the first mobile shopping applications reaching consumer markets, such as the Samâs Club mobile application [1], which provides indoor navigation to specific items and/or shops in some selected American shopping malls. Similarly, some shopping centers in Asia introduced mobile AR applications for shopping assistance [2]. In our research group we study future application areas of projector phones, i.e. mobile phones with integrated projectors (see [11] for a detailed survey). We found projector phones to provide some distinguished advantages for indoor navigation assistance, e.g., that interaction can be handsfree and the projection can serve as ambient display, thereby not contradicting the traditional shopping experience. Further that the surrounding world can be directly augmented, freeing the user from mapping between mobile display and real world and that the output space is much larger than on mobile displays. And finally, that bystander can see and attach to the projected output. RELATED WORK We present relevant work dealing with mobile shopping, recommender systems, indoor positioning and navigation. One of the first works on digital mobile shopping assistants has been done by Asthana et al. [3]. They presented main usage scenarios, e.g., telling people where to find certain products or informing them on discounts and special offers. Similar can be recognized in aforementioned mobile shopping applications and as well in recently filed patents, e.g. from Apple Inc. (US 2010/0198626 A1, US 2010/0191578 A1), which include navigation, service interactions (e.g., parking tickets), and support for social networking. Yang et al. [15] developed a location-aware recommender system. It learns a customerâs interests from previously visited product websites and continuously presents a list of products around the customerâs current location, that are likely to interest him. The software also takes into account the distance to shops and is able to learn customerâs preference between highly interesting products and far distances. Butz et al. [4] present a hybrid indoor navigation system that is able to adapt route instructions to different output devices (screen resolution, device resources) and based on the precision of available location information. Results from [7,14] indicate that intelligent fusion of infrastructure techniques, e.g. GPS, GSM triangulation, and sensors like accelerometers, magnetometers, and gyroscopes, enables precise indoor location tracking with current commercially available smartphones in unaltered environments. Kray et al. [6] explore the design space of routing instructions for pedestrians on mobile devices. Narzt et al. present a specific mobile application for augmented reality (AR) [10]. Alternative systems for pedestrian navigation are the Rotating Compass by Rukzio et al. [12], showing personalised navigation information on public displays and the CabBoots system by Frey [5], which guides users by means of tactile output in the shoes. Aforementioned techniques, with the exception of the last two, rely on holding a handset device. However, we feel strongly inclined that holding a device in hand for a longer time does not fit well the traditional shopping experience. Negative side effects, e.g., arm fatigue, regular switching of the field of view, do not allow using the shopping assistance application as constant companion. CONCEPT In our envisioned prototype, people wear their projector phone on their belt, projecting a display right in front of their feet (Figure 1). In the following we present five concepts for mobile shopping assistance that are enabled by projector phones. Later we apply these concepts to a typical shopping scenario. Shadow Interaction Since our concept expects people to wear the projector fixed to the belt most of the time, direct interaction with the mobile device would not be sufficient as the only interaction technique. Audio is not an alternative because of the noisy environment in a shopping mall. This leaves users with the option to directly interact with the projection, either by feet or hands (or gaze in the future). In preliminary studies we discovered that foot-based interaction is not well suited due to the fact that foot movement inherently involves movement of the body at the same time, which makes interaction cumbersome. We found interacting with the shadow of a finger in front of the projection a much more promising solution. Figure 2 shows the stroke of the resulting shadow on the projection. With the tip of the shadow, all points on the projection can be reached. Items should be highlighted once the shadow reaches their bounding box to give adequate feedback to the user. Although the tracking of the fingerâs height to enable traditional press/release behaviors would be possible, this would require the user to maintain a complex mental model of different finger height levels. Instead, our concept builds on the fact, that the tip of the index finger can be moved well without changing the shape of the rest of the hand or even the middle knuckle of the index finger. Thus, to select an item, a user moves the index finger to point on the desired item and then bends the index finger and unbends it again. Alternatively, one could use the fingerâs dwell time as in Microsoftâs Kinect. To the best of our knowledge, shadow interaction with projections has not been reported before. Radar of Recommendations Another concept that is particularly useful with projected navigation is a radar of recommendations. Building on [15], we want to constantly show and update a personal radar of products the user might be interested in (Figure 2). Based on the information available from accounts with online stores (e.g., Amazon) and items recently explored with our system (see fourth concept), users see offers of nearby stores in front of them and can select these items to start a navigation in the middle of the circle. The size of an item conveys the expected interest of the user, the distance from the middle depicts the walking distance (not air path) from the userâs current location. The size of the radius of interest (distance to shops and items) can be adjusted by slightly changing the angle of the projector, similarly to looking further ahead. Different from [15], the projection provides a much larger output space and the radar serves as ambient display in the userâs periphery. Projected (augmented) Navigation When the user selected an item or shop he is interested in, the assistance system starts a projected navigation. In outdoor navigation, turn-by-turn navigation is still the most prominent, though we have seen augmented reality been used in research [10]. Especially for indoor navigation, where there are more tight and subsequent turns or small decision spaces (take the left stairs up, not the right stairs down) our experience is that turn-by-turn navigation does not work well. Therefore, we want the user not to follow Figure 1: The projector is worn at different locations on the belt (left and middle) and can optionally be taken into hand to change the angle of the projection (right). Figure 2: The user interacts with his radar of recommendations through the shadow of his finger. The orange outline shows the shadow, the left red circle the fingertip that, for clicking, can be changed independently of the fingerâs middle knuckle (right circle) or the rest of the handâs shape. this type of directions, but instead simply follow a blue line projected in front of her (see Figure 3). Since the projector phone is spatially-aware, both in terms of location and orientation, the blue line is projected as a static augmentation of the real world, i",oceanology,25,unknown
59304bc73b6d24f18d23404e0d408c462b66c4d0,to_check,semantic_scholar,Softwaretechnik-Trends,2008-01-01 00:00:00,semantic_scholar,simulationsbasierte analyse und entwicklung von peer-to-peer-systemen,https://www.semanticscholar.org/paper/59304bc73b6d24f18d23404e0d408c462b66c4d0,"Peer-to-Peer (P2P) systems are distributed systems composed of up to millions of functionally equivalent entities (peers), which form P2P overlay networks on top of physical networks to communicate. The functionality of a peer is implemented by a P2P application which de nes the behavior of the whole P2P system. The equivalence of peers is realized by providing client functionality as well as server functionality. Implementing a P2P system with speci ed behavior is a di cult task because the behavior depends on many factors, such as the used P2P search methods and the underlying physical network. Some factors cannot be taken into account completely because of their complexity or unknown or not understood parts. For instance, the prospective user behavior may only be estimated based on observed data. When engineering complex, dynamic software systems such as P2P systems, simulation is often used to analyze the properties of these systems based on simulation models in an early development phase. With simulations in natural sciences, the separation of reality and (simulation) model is clear: the reality exists in nature, while the model exists as software within some computer system. When simulating software systems, this separation is not so obvious: the simulated model is itself a software system. With P2P systems, for instance, a simpli ed P2P system is modeled and simulated for predicting properties of real P2P systems. The new software engineering contribution of this work is the Peer Software Engineering (PeerSE) method, which allows a controlled transition from simulation models to real-world software systems. The method starts with a comparative analysis of simulation models for P2P systems and proceeds iteratively toward the experimental implementation in a laboratory setting and nally a real-world P2P system deployed in a target environment. The method includes a simulation model for P2P systems and a tool supporting the execution of simulation and laboratory experiments. Simulation is an essential part of the PeerSE method used to identify and to compare models ful lling given requirements. When an appropriate model has been found, model components can be reused and further re ned to implement a laboratory P2P system. To allow for a controlled transition of model components to laboratory components, the results of simulation and laboratory experiments are directly compared using the same metrics. The applicability of the PeerSE method has been successfully evaluated by analyzing and realizing a P2P system for distributed software development.",oceanology,26,unknown
10.1109/icas.2009.2,to_check,2009 Fifth International Conference on Autonomic and Autonomous Systems,IEEE,2009-04-25 00:00:00,ieeexplore,[title page iii],https://ieeexplore.ieee.org/document/4976566/,The following topics are dealt with: real-time chain-structured synchronous dataflow; memory requirement formal determination; linear singular descriptor differential system; execution-optimized paths; greedy strategy; load trend evaluation; self-managed P2P streaming; context-aware ambient assisted living application; self-adaptive distributed model; autonomic systems; wireless sensor networks; topology control; learning based method; self-recovery method; mobile data sharing; heterogeneous QoS resource manager; component-based self-healing; NGN mobility; interactive user activity; .NET Windows service agent technology; agent based Web browser; resource-definition policies; autonomic computing; autonomic system administration; automatic database performance tuning; knowledge management; adaptive reinforcement learning; VoIP services; autonomic RSS: distributed virtual reality simulations; virtual machines resources allocation; multi-lier distributed systems; network I/O extensibility; virtual keyboards; self-configuring smart homes; legged underwater vehicles; particle filters; reusable semantic components; multi-agent systems; fixed-wing unmanned aerial vehicles; fuzzy inference system; robot swarms; mobile robots; optimization architecture; autonomous unmanned helicopter landing system design; heterogeneous multi-database environments; autonomic software license management system; Web server crashes prediction; laser range finder; video quality; wireless networks; ITU-T G.1030; open IMS core; context-aware data mining methodology; supply chain finance cooperative systems; autonomous pervasive environments; distributed generic stress tool; dynamic adaptive systems; multisensory media effects and user preference.,finance,27,unknown
10.1109/robot.2000.844768,to_check,Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065),IEEE,2000-04-28 00:00:00,ieeexplore,application of automatic action planning for several work cells to the german ets-vii space robotics experiments,https://ieeexplore.ieee.org/document/844768/,"Experiences in space robotics show, that the user normally has to cope with a huge amount of data. So, only robot and mission specialists are able to control the robot arm directly in teleoperation mode. By means of an intelligent robot control in cooperation with virtual reality methods, it is possible for non-robot specialists to generate tasks for a robot or an automation component intuitively. Furthermore, the intelligent robot control improves the safety of the entire system. The on-ground robot control and command station for the robot arm ERA onboard the satellite ETS-VII builds on a new resource-based action planning approach to manage robot manipulators and other automation components. In the case of ERA, the action planning system also takes care of the ""real"" robot onboard the satellite and the ""virtual"" robot in the simulation system. By means of the simulation system, the user can plan tasks ahead as well as analyze and visualize different strategies. The paper describes the mechanism of resource-based action planning, its application to different work cells, the practical experiences gained from the implementation for the on-ground robot control and command station for the robot arm ERA developed in the GETEX project as well as the services it provides to support VR-based man machine interfaces.",space,28,included
10.1109/snpd.2012.99,to_check,"2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",IEEE,2012-08-10 00:00:00,ieeexplore,evaluation of realism of dynamic sound space using a virtual auditory display,https://ieeexplore.ieee.org/document/6299338/,"We can perceive a sound position from binaural signals using mainly head-related transfer functions (HRTFs). Using the theorem presented herein, we can display a sound image to a specific position in virtual auditory space by HRTFs. However, HRTF is defined commonly in a free-field, and a virtual sound image is perceived as a dry source without reflection, reverberation, or ambient noise. Therefore, the virtual sound space might be unnatural. The authors developed a software-based virtual auditory display (VAD) that outputs audio signals for a set of headphones with a three-dimensional position sensor. The VAD software can display a dynamic virtual auditory space that is responsive to a listener's head movement. Subjective evaluations were conducted to clarify the relation between the perceived reality of virtual sound space and ambient sound. Evaluation results of the reality of the virtual sound space displayed by the VAD software are introduced.",space,29,unknown
10.1109/robio49542.2019.8961870,to_check,2019 IEEE International Conference on Robotics and Biomimetics (ROBIO),IEEE,2019-12-08 00:00:00,ieeexplore,probabilistic inferences on quadruped robots: an experimental comparison,https://ieeexplore.ieee.org/document/8961870/,"Due to the reality gap, computer software cannot fully model the physical robot in its environment, with noise, ground friction, and energy consumption. Consequently, a limited number of researchers work on applying machine learning in real-world robots. In this paper, we use two intelligent black-box optimization algorithms, Bayesian Optimization (BO) and Covariance Matrix Adaptation Evolution Strategy (CMA-ES), to solve a quadruped robot gait's parametric search problem in 10 dimensions, and compare these two methods to find which one is more suitable for legged robots' controller parameters tuning. Our results show that both methods can find an optimal solution in 130 iterations. BO converges faster than CMA-ES within its constrained range, while CMA-ES finds the optimum in the continuous space. Compared with the specific controller parameters of two methods, we also find that for quadruped robot's oscillators, the angular amplitude is the most important parameter. Thus, it is very beneficial for the quick parametric search of legged robots' controllers and avoids time-consuming manual tuning.",space,30,unknown
10.23919/mipro.2019.8756928,to_check,"2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",IEEE,2019-05-24 00:00:00,ieeexplore,utilizing appleâs arkit 2.0 for augmented reality application development,https://ieeexplore.ieee.org/document/8756928/,"When it comes to practical augmented reality applications, mobile platform tools are the most deserving. Thanks to the nature of mobile devices and their everyday usage, the ideal basis for this kind of content has inadvertently formed itself. Consequently, within the iOS development environment, Apple's Xcode program enables application development using the ARKit library which delivers a host of benefits. Amongst the plethora of advantages, this paper focuses on utilizing features such as the ability to measure distances between two points in space, horizontal and vertical plane detection, the ability to detect three-dimensional objects and utilize them as triggers, and the consolidated implementation of ARKit and MapKit libraries in conjunction with the Google Places API intended for displaying superimposed computer-generated content on iOS 11 and later iterations of Apple's mobile operating system.",space,31,not included
http://arxiv.org/abs/1912.06321v2,to_check,arxiv,arxiv,2019-12-13 00:00:00,arxiv,"sim2real predictivity: does evaluation in simulation predict real-world
  performance?",http://arxiv.org/abs/1912.06321v2,"Does progress in simulation translate to progress on robots? If one method
outperforms another in simulation, how likely is that trend to hold in reality
on a robot? We examine this question for embodied PointGoal navigation,
developing engineering tools and a research paradigm for evaluating a simulator
by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy),
a library for seamless execution of identical code on simulated agents and
robots, transferring simulation-trained agents to a LoCoBot platform with a
one-line code change. Second, we investigate the sim2real predictivity of
Habitat-Sim for PointGoal navigation. We 3D-scan a physical lab space to create
a virtualized replica, and run parallel tests of 9 different models in reality
and simulation. We present a new metric called Sim-vs-Real Correlation
Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as
used for the CVPR19 challenge is low (0.18 for the success metric), suggesting
that performance differences in this simulator-based challenge do not persist
after physical deployment. This gap is largely due to AI agents learning to
exploit simulator imperfections, abusing collision dynamics to 'slide' along
walls, leading to shortcuts through otherwise non-navigable space. Naturally,
such exploits do not work in the real world. Our experiments show that it is
possible to tune simulation parameters to improve sim2real predictivity (e.g.
improving $SRCC_{Succ}$ from 0.18 to 0.844), increasing confidence that
in-simulation comparisons will translate to deployed systems in reality.",space,32,unknown
https://core.ac.uk/download/322366596.pdf,to_check,core,'Lviv State University of Life Safety',2018-12-31 00:00:00,core,hard-soft-ÑÐµÑÐ½Ð¾Ð»Ð¾Ð³ÑÑ ÑÐ½ÑÐ¾ÑÐ¼Ð°ÑÑÐ¹Ð½Ð¾Ð³Ð¾ ÑÑÐ¿ÑÐ¾Ð²Ð¾Ð´Ñ  Ð¿ÑÐ¾ÑÐµÑÑ Ð¼Ð¾Ð´ÐµÐ»ÑÐ²Ð°Ð½Ð½Ñ ÑÐµÐ¿Ð»Ð¾ÑÐ²Ð¾ÑÐµÐ½Ð½Ñ/ÑÐµÐ¿Ð»Ð¾ÑÐ¿Ð¾Ð¶Ð¸Ð²Ð°Ð½Ð½Ñ  Ð² Ð´Ð²Ð¸Ð³ÑÐ½Ñ Ð²Ð½ÑÑÑÑÑÐ½ÑÐ¾Ð³Ð¾ Ð·Ð³Ð¾ÑÑÐ½Ð½Ñ,10.32447/20784643.18.2018.01,"Deterministic and, in a certain sense, ""linear"" interpretation of the world often leads to the recognition of the fact that the more accurate model we need, the more complex it must be (as in case of a formalized reproduction of the real system, or the implementation of the desired system properties in the process of formal synthesis of something new). Instead, following the principle of synergy leads to the conviction that there is always a certain model of optimal complexity e.g. in the synthesis of the new system, and in the analysis of real system peculiarities. However, the model of reality could be a part of this reality that is included to the carefully structured formal description.  Since we cannot penetrate into the working space of the serial engine while testing, we should use a test engine of a special construction when the working space corresponds to the laws of similarity and this engine will serve as a model of the working space of the serial engine.
&nbsp;
&nbsp;
The study illustrates the effectiveness of hard-soft technology while investigating the peculiarities of heat generation and heat consumption in the internal combustion engine, which will combine mathematic and algorithmic means of modelling as well as the means of real simulation. The necessity of hard-soft technology introduction arises from the excessive complexity of thermal phenomena occurring in the internal combustion engine (ICE), and the inability to fully subordinate these phenomena to existing analytical models.
The combination of original and analytical properties, reality and virtual reality while modelling the processes in internal combustion engines allows us to substantially improve the quality of information in the process of design and engine construction. Taking this into consideration, there are some natural grounds to apply principles of heuristic self-organization, self-learning, means of the neural networks, etc. in the design implementation.
The study demonstrates the example of modelling the real working space of ICE with the forced start that serves as a supplement to the mathematical algorithmic two-zone model of heat generation / heat consumption / heat extraction.
The basic information that can be obtained by means of hard-soft technology in the framework of, for example, the two-zone model of the work process in the gasoline engine, is the variability with the change in the angle of rotation of the crankshaft of the engine: absolute pressure (indicative diagram); absolute temperature; heat transmitted inside the cylinder between zones; coefficient of excess air; coefficient of heat transfer; intensity of heat extraction in the process of combustion of fuel; intensity of heat transfer through the walls of the cylindeÐÐµÑÐµÑÐ¼ÑÐ½ÑÑÑÐ¸ÑÐ½Ðµ Ñ Ð² Ð¿ÐµÐ²Ð½Ð¾Ð¼Ñ ÑÐµÐ½ÑÑ Â«Ð»ÑÐ½ÑÐ¹Ð½ÐµÂ» ÑÑÐ°ÐºÑÑÐ²Ð°Ð½Ð½Ñ ÑÐ²ÑÑÑ ÑÐ°ÑÑÐ¾ Ð²ÐµÐ´Ðµ Ð´Ð¾ Ð²Ð¸Ð·Ð½Ð°Ð½Ð½Ñ ÑÐ¾Ð³Ð¾, ÑÐ¾ ÑÐ¸Ð¼ ÑÐ¾ÑÐ½ÑÑÐ¾Ñ Ð¿Ð¾ÑÑÑÐ±Ð½Ð° Ð¹Ð¾Ð³Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ñ, ÑÐ¸Ð¼ ÑÐºÐ»Ð°Ð´Ð½ÑÑÐ¾Ñ Ð²Ð¾Ð½Ð° Ð¼Ð°Ñ Ð±ÑÑÐ¸ (ÑÐº Ñ ÑÐ°Ð·Ñ ÑÐ¾ÑÐ¼Ð°Ð»ÑÐ·Ð¾Ð²Ð°Ð½Ð¾Ð³Ð¾ Ð²ÑÐ´ÑÐ²Ð¾ÑÐµÐ½Ð½Ñ ÑÐµÐ°Ð»ÑÐ½Ð¾Ñ ÑÐ¸ÑÑÐµÐ¼Ð¸, ÑÐ°Ðº Ñ Ñ ÑÐ°Ð·Ñ Ð²ÑÑÐ»ÐµÐ½Ð½Ñ Ð±Ð°Ð¶Ð°Ð½Ð¸Ñ ÑÐ¸ÑÑÐµÐ¼Ð½Ð¸Ñ Ð²Ð»Ð°ÑÑÐ¸Ð²Ð¾ÑÑÐµÐ¹ Ñ Ð¿ÑÐ¾ÑÐµÑÑ ÑÐ¾ÑÐ¼Ð°Ð»ÑÐ·Ð¾Ð²Ð°Ð½Ð¾Ð³Ð¾ ÑÐ¸Ð½ÑÐµÐ·Ñ ÑÐ¾Ð³Ð¾ÑÑ Ð½Ð¾Ð²Ð¾Ð³Ð¾). ÐÐ°ÑÐ¾Ð¼ÑÑÑÑ Ð´Ð¾ÑÑÐ¸Ð¼Ð°Ð½Ð½Ñ Ð¿ÑÐ¸Ð½ÑÐ¸Ð¿Ñ ÑÐ¸Ð½ÐµÑÐ³ÐµÑÐ¸ÑÐ½Ð¾ÑÑÑ Ð²ÐµÐ´Ðµ Ð´Ð¾ Ð¿ÐµÑÐµÐºÐ¾Ð½Ð°Ð½Ð½Ñ, ÑÐ¾ Ð·Ð°Ð²Ð¶Ð´Ð¸ ÑÑÐ½ÑÑ ÑÐºÐ°ÑÑ Ð¼Ð¾Ð´ÐµÐ»Ñ Ð¾Ð¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½Ð¾Ñ ÑÐºÐ»Ð°Ð´Ð½Ð¾ÑÑÑ â Ñ ÑÐ¾Ð´Ñ, ÐºÐ¾Ð»Ð¸ Ð¹Ð´ÐµÑÑÑÑ Ð¿ÑÐ¾ ÑÐ¸Ð½ÑÐµÐ· Ð½Ð¾Ð²Ð¾Ñ ÑÐ¸ÑÑÐµÐ¼Ð¸, Ñ ÑÐ¾Ð´Ñ, ÐºÐ¾Ð»Ð¸ Ð¿ÑÐ¾Ð²Ð°Ð´Ð¸ÑÑÑÑ Ð°Ð½Ð°Ð»ÑÐ· Ð²Ð»Ð°ÑÑÐ¸Ð²Ð¾ÑÑÐµÐ¹ ÑÐµÐ°Ð»ÑÐ½Ð¾Ñ ÑÐ¸ÑÑÐµÐ¼Ð¸. ÐÐ»Ðµ Ð¶ Ð¼Ð¾Ð´ÐµÐ»Ð»Ñ ÑÐµÐ°Ð»ÑÐ½Ð¾ÑÑÑ Ð¼Ð¾Ð¶Ðµ ÑÐ»ÑÐ³ÑÐ²Ð°ÑÐ¸ ÑÐ°ÐºÐ¾Ð¶ Ñ ÑÐºÐ°ÑÑ ÑÐ°ÑÑÐ¸Ð½Ð° ÑÑÑÑ ÑÐµÐ°Ð»ÑÐ½Ð¾ÑÑÑ, Ð´Ð¾Ð»ÑÑÐµÐ½Ð° Ð´Ð¾ ÑÐµÑÐµÐ»ÑÐ½Ð¾ ÑÑÑÑÐºÑÑÑÐ¾Ð²Ð°Ð½Ð¾Ð³Ð¾ ÑÐ¾ÑÐ¼Ð°Ð»ÑÐ½Ð¾Ð³Ð¾ Ð¾Ð¿Ð¸ÑÑ. ÐÑÐºÑÐ»ÑÐºÐ¸ Ð´Ð¾ÑÐ»ÑÐ´Ð½Ð¸Ð¼Ð¸ Ð·Ð°ÑÐ¾Ð±Ð°Ð¼Ð¸ Ð¿ÑÐ¾Ð½Ð¸ÐºÐ½ÑÑÐ¸ Ð² ÑÐ¾Ð±Ð¾ÑÐ¸Ð¹ Ð¿ÑÐ¾ÑÑÑÑ ÑÐµÑÑÐ¹Ð½Ð¾Ð³Ð¾ Ð´Ð²Ð¸Ð³ÑÐ½Ð° Ð½ÐµÐ¼Ð° Ð·Ð¼Ð¾Ð³Ð¸, ÑÐ¾ Ð´Ð¾Ð²Ð¾Ð´Ð¸ÑÑÑÑ Ð²Ð¸ÐºÐ¾ÑÐ¸ÑÑÐ¾Ð²ÑÐ²Ð°ÑÐ¸ Ð´Ð¾ÑÐ»ÑÐ´Ð½Ð¸Ð¹ Ð´Ð²Ð¸Ð³ÑÐ½ Ð¾ÑÐ¾Ð±Ð»Ð¸Ð²Ð¾Ñ ÐºÐ¾Ð½ÑÑÑÑÐºÑÑÑ, ÑÐ¾Ð±Ð¾ÑÐ¸Ð¹ Ð¿ÑÐ¾ÑÑÑÑ ÑÐºÐ¾Ð³Ð¾ Ð²ÑÐ´Ð¿Ð¾Ð²ÑÐ´Ð°Ñ Ð·Ð°ÐºÐ¾Ð½Ð°Ð¼ Ð¿Ð¾Ð´ÑÐ±Ð½Ð¾ÑÑÑ Ñ ÑÐ»ÑÐ³ÑÐ²Ð°ÑÐ¸Ð¼Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð»Ñ-Ð°Ð½Ð°Ð»Ð¾Ð³Ð¾Ð¼ ÑÐ¾Ð±Ð¾ÑÐ¾Ð³Ð¾ Ð¿ÑÐ¾ÑÑÐ¾ÑÑ ÑÐµÑÑÐ¹Ð½Ð¾Ð³Ð¾ Ð´Ð²Ð¸Ð³ÑÐ½Ð°.
ÐÐµÑÐ° ÑÐ¾Ð±Ð¾ÑÐ¸ â Ð¾Ð±ÒÑÑÐ½ÑÑÐ²Ð°ÑÐ¸ ÐµÑÐµÐºÑÐ¸Ð²Ð½ÑÑÑÑ hard-soft-ÑÐµÑÐ½Ð¾Ð»Ð¾Ð³ÑÑ Ð´Ð¾ÑÐ»ÑÐ´Ð¶ÐµÐ½Ð½Ñ Ð¾ÑÐ¾Ð±Ð»Ð¸Ð²Ð¾ÑÑÐµÐ¹ ÑÐµÐ¿Ð»Ð¾ÑÐ²Ð¾ÑÐµÐ½Ð½Ñ Ñ ÑÐµÐ¿Ð»Ð¾ÑÐ¿Ð¾Ð¶Ð¸Ð²Ð°Ð½Ð½Ñ Ð² Ð´Ð²Ð¸Ð³ÑÐ½Ñ Ð²Ð½ÑÑÑÑÑÐ½ÑÐ¾Ð³Ð¾ Ð·Ð³Ð¾ÑÑÐ½Ð½Ñ, ÑÐºÐ° Ð± ÑÐ¸ÑÑÐµÐ¼Ð½Ð¾ Ð¿Ð¾ÑÐ´Ð½ÑÐ²Ð°Ð»Ð° Ð² ÑÐ¾Ð±Ñ Ð·Ð°ÑÐ¾Ð±Ð¸ Ð¼Ð°ÑÐµÐ¼Ð°ÑÐ¸ÑÐ½Ð¾Ð³Ð¾ Ð¹ Ð°Ð»Ð³Ð¾ÑÐ¸ÑÐ¼ÑÑÐ½Ð¾Ð³Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑÐ²Ð°Ð½Ð½Ñ ÑÐ° Ð·Ð°ÑÐ¾Ð±Ð¸ Ð½Ð°ÑÑÑÐ½Ð¾Ð³Ð¾ ÑÐ¸Ð¼ÑÐ»ÑÐ²Ð°Ð½Ð½Ñ. ÐÐµÐ¾Ð±ÑÑÐ´Ð½ÑÑÑÑ Ð²Ð¿ÑÐ¾Ð²Ð°Ð´Ð¶ÐµÐ½Ð½Ñ hard-soft-ÑÐµÑÐ½Ð¾Ð»Ð¾Ð³ÑÑ Ð²Ð¸Ð¿Ð»Ð¸Ð²Ð°Ñ Ð· Ð½Ð°Ð´Ð¼ÑÑÐ½Ð¾Ñ ÑÐºÐ»Ð°Ð´Ð½Ð¾ÑÑÑ ÑÐµÐ¿Ð»Ð¾Ð²Ð¸Ñ ÑÐ²Ð¸Ñ, ÑÐ¾ Ð¿ÐµÑÐµÐ±ÑÐ³Ð°ÑÑÑ Ñ Ð´Ð²Ð¸Ð³ÑÐ½Ñ Ð²Ð½ÑÑÑÑÑÐ½ÑÐ¾Ð³Ð¾ Ð·Ð³Ð¾ÑÑÐ½Ð½Ñ, ÑÐ° Ð½ÐµÐ¼Ð¾Ð¶Ð»Ð¸Ð²Ð¾ÑÑÑ ÑÐ¿Ð¾Ð²Ð½Ñ Ð¿ÑÐ´Ð¿Ð¾ÑÑÐ´ÐºÑÐ²Ð°ÑÐ¸ ÑÑ ÑÐ²Ð¸ÑÐ° ÑÑÐ½ÑÑÑÐ¸Ð¼ Ð°Ð½Ð°Ð»ÑÑÐ¸ÑÐ½Ð¸Ð¼ Ð¼Ð¾Ð´ÐµÐ»ÑÐ½Ð¸Ð¼ ÑÑÐ²Ð»ÐµÐ½Ð½ÑÐ¼.
ÐÐ¾ÑÐ´Ð½Ð°Ð½Ð½Ñ Ð½Ð°ÑÑÑÐ½Ð¾ÑÑÑ ÑÐ° Ð°Ð½Ð°Ð»ÑÑÐ¸ÑÐ½Ð¾ÑÑÑ, ÑÐµÐ°Ð»ÑÐ½Ð¾ÑÑÑ ÑÐ° Ð²ÑÑÑÑÐ°Ð»ÑÐ½Ð¾ÑÑÑ Ð² Ð¼Ð¾Ð´ÐµÐ»ÑÐ²Ð°Ð½Ð½Ñ Ð¿ÑÐ¾ÑÐµÑÑÐ² Ñ Ð´Ð²Ð¸Ð³ÑÐ½Ð°Ñ Ð²Ð½ÑÑÑÑÑÐ½ÑÐ¾Ð³Ð¾ Ð·Ð³Ð¾ÑÑÐ½Ð½Ñ Ð´Ð¾Ð·Ð²Ð¾Ð»ÑÑ Ð¿ÑÐ¸Ð½ÑÐ¸Ð¿Ð¾Ð²Ð¾ Ð¿ÑÐ´Ð²Ð¸ÑÐ¸ÑÐ¸ ÑÐºÑÑÑÑ ÑÐ½ÑÐ¾ÑÐ¼Ð°ÑÑÐ¹Ð½Ð¾Ð³Ð¾ Ð·Ð°Ð±ÐµÐ·Ð¿ÐµÑÐµÐ½Ð½Ñ Ð¿ÑÐ¾ÑÐµÑÑ Ð¿ÑÐ¾ÐµÐºÑÑÐ²Ð°Ð½Ð½Ñ Ð¹ ÐºÐ¾Ð½ÑÑÑÑÑÐ²Ð°Ð½Ð½Ñ Ð´Ð²Ð¸Ð³ÑÐ½ÑÐ². ÐÑÐ¸ ÑÑÐ¾Ð¼Ñ Ð²Ð¸Ð½Ð¸ÐºÐ°ÑÑÑ Ð¿ÑÐ¸ÑÐ¾Ð´Ð½Ñ Ð¿ÑÐ´ÑÑÐ°Ð²Ð¸ Ð´Ð»Ñ Ð²ÑÑÐ»ÐµÐ½Ð½Ñ Ñ Ð¼Ð¾Ð´ÐµÐ»ÑÐ²Ð°Ð½Ð½Ñ Ð¿ÑÐ¸Ð½ÑÐ¸Ð¿ÑÐ² ÐµÐ²ÑÐ¸ÑÑÐ¸ÑÐ½Ð¾Ñ ÑÐ°Ð¼Ð¾Ð¾ÑÐ³Ð°Ð½ÑÐ·Ð°ÑÑÑ, ÑÐ°Ð¼Ð¾Ð½Ð°Ð²ÑÐ°Ð½Ð½Ñ, Ð·Ð°ÑÐ¾Ð±ÑÐ² ÑÑÐ¸Ð±Ñ Ð½ÐµÐ¹ÑÐ¾Ð½Ð½Ð¸Ñ Ð¼ÐµÑÐµÐ¶ ÑÐ¾ÑÐ¾.
ÐÐ°Ð²Ð¾Ð´Ð¸ÑÑÑÑ Ð¿ÑÐ¸ÐºÐ»Ð°Ð´ ÑÐ¾ÑÐ¼ÑÐ²Ð°Ð½Ð½Ñ ÑÐµÐ°Ð»ÑÐ½Ð¾Ð³Ð¾ ÑÐ¾Ð±Ð¾ÑÐ¾Ð³Ð¾ Ð¿ÑÐ¾ÑÑÐ¾ÑÑ Ð´Ð²Ð¸Ð³ÑÐ½Ð° Ð²Ð½ÑÑÑÑÑÐ½ÑÐ¾Ð³Ð¾ Ð·Ð³Ð¾ÑÑÐ½Ð½Ñ Ð· Ð¿ÑÐ¸Ð¼ÑÑÐ¾Ð²Ð¸Ð¼ Ð·Ð°Ð¿Ð°Ð»ÐµÐ½Ð½ÑÐ¼, Ð¿Ð¾ÐºÐ»Ð¸ÐºÐ°Ð½Ð¾Ð³Ð¾ Ð´Ð¾Ð¿Ð¾Ð²Ð½Ð¸ÑÐ¸ Ð¼Ð°ÑÐµÐ¼Ð°ÑÐ¸ÑÐ½Ð¾-Ð°Ð»Ð³Ð¾ÑÐ¸ÑÐ¼ÑÑÐ½Ñ Ð´Ð²Ð¾Ð·Ð¾Ð½Ð½Ñ Ð¼Ð¾Ð´ÐµÐ»Ñ ÑÐµÐ¿Ð»Ð¾ÑÐ²Ð¾ÑÐµÐ½Ð½Ñ/ÑÐµÐ¿Ð»Ð¾ÑÐ¿Ð¾Ð¶Ð¸Ð²Ð°Ð½Ð½Ñ/ÑÐµÐ¿Ð»Ð¾Ð²ÑÐ´Ð²ÐµÐ´ÐµÐ½Ð½Ñ.
ÐÑÐ½Ð¾Ð²Ð½Ð¾Ñ ÑÐ½ÑÐ¾ÑÐ¼Ð°ÑÑÑÑ, ÑÐºÑ Ð¼Ð¾Ð¶Ð½Ð° Ð´Ð¾Ð±ÑÐ²Ð°ÑÐ¸ Ð·Ð°ÑÐ¾Ð±Ð°Ð¼Ð¸ hard-soft-ÑÐµÑÐ½Ð¾Ð»Ð¾Ð³ÑÑ Ð² ÑÐ°Ð¼ÐºÐ°Ñ, Ð¿ÑÐ¸Ð¼ÑÑÐ¾Ð¼, Ð´Ð²Ð¾Ð·Ð¾Ð½Ð½Ð¾Ñ Ð¼Ð¾Ð´ÐµÐ»Ñ ÑÐ¾Ð±Ð¾ÑÐ¾Ð³Ð¾ Ð¿ÑÐ¾ÑÐµÑÑ Ð² Ð±ÐµÐ½Ð·Ð¸Ð½Ð¾Ð²Ð¾Ð¼Ñ Ð´Ð²Ð¸Ð³ÑÐ½Ñ, Ñ Ð·Ð¼ÑÐ½ÑÐ²Ð°Ð½ÑÑÑÑ Ð·Ñ Ð·Ð¼ÑÐ½Ð¾Ñ ÐºÑÑÐ° Ð¿Ð¾Ð²Ð¾ÑÐ¾ÑÑ ÐºÐ¾Ð»ÑÐ½ÑÐ°ÑÑÐ¾Ð³Ð¾ Ð²Ð°Ð»Ð° Ð´Ð²Ð¸Ð³ÑÐ½Ð°: Ð°Ð±ÑÐ¾Ð»ÑÑÐ½Ð¾Ð³Ð¾ ÑÐ¸ÑÐºÑ (ÑÐ½Ð´Ð¸ÐºÐ°ÑÐ¾ÑÐ½Ð° Ð´ÑÐ°Ð³ÑÐ°Ð¼Ð°); Ð°Ð±ÑÐ¾Ð»ÑÑÐ½Ð¾Ñ ÑÐµÐ¼Ð¿ÐµÑÐ°ÑÑÑÐ¸; ÑÐµÐ¿Ð»Ð¾ÑÐ¸, ÑÐ¾ Ð¿ÐµÑÐµÑÐ¸Ð»Ð°ÑÑÑÑÑ Ð²ÑÐµÑÐµÐ´Ð¸Ð½Ñ ÑÐ¸Ð»ÑÐ½Ð´ÑÐ° Ð¼ÑÐ¶ Ð·Ð¾Ð½Ð°Ð¼Ð¸; ÐºÐ¾ÐµÑÑÑÑÑÐ½ÑÐ° Ð½Ð°Ð´Ð»Ð¸ÑÐºÑ Ð¿Ð¾Ð²ÑÑÑÑ; ÐºÐ¾ÐµÑÑÑÑÑÐ½ÑÐ° ÑÐµÐ¿Ð»Ð¾Ð²ÑÐ´Ð´Ð°ÑÑ; ÑÐ½ÑÐµÐ½ÑÐ¸Ð²Ð½Ð¾ÑÑÑ ÑÐµÐ¿Ð»Ð¾Ð²Ð¸Ð´ÑÐ»ÐµÐ½Ð½Ñ Ñ Ð¿ÑÐ¾ÑÐµÑÑ Ð·Ð³Ð¾ÑÑÐ½Ð½Ñ Ð¿Ð°Ð»Ð¸Ð²Ð°; ÑÐ½ÑÐµÐ½ÑÐ¸Ð²Ð½ÑÑÑÑ ÑÐµÐ¿Ð»Ð¾Ð²ÑÐ´Ð²ÐµÐ´ÐµÐ½Ð½Ñ ÑÐµÑÐµÐ· ÑÑÑÐ½ÐºÐ¸ ÑÐ¸Ð»ÑÐ½Ð´ÑÐ°",space,33,not included
5d248aace06704234de122ac354a51f1630182b8,to_check,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,augmented worlds: a proposal for modelling and engineering pervasive mixed reality smart environments,https://www.semanticscholar.org/paper/5d248aace06704234de122ac354a51f1630182b8,"In recent years, the remarkable technological advancement has revolutionised the ICT panorama offering nowadays the opportunity to exploit several technological convergences to reduce the gulf existing between the physical and the digital matter, between the physical real world and every computational software system or application. Along with Pervasive Computing - entered in the mainstream with the concept of Internet of Things (IoT) - Mixed Reality (MR) is going to be an essential ingredient for the design and development of next future smart environments. In particular, in such environments is feasible to imagine that the computation will drive the augmentation of the physical space, and software will also be executed in a cyber-physical world, eventually populated with of (interactive) holograms. 
 
After an initial exploration of the state of the art about augmentation technologies both for humans and the environment, in this dissertation we present the vision of Augmented Worlds (AW), a conceptual and a practical proposal for modelling and engineering next future pervasive mixed reality smart environments as distribute, multi-user and cooperative complex software systems. 
On the one hand, a meta-model is formalised and opportunely discussed to offer to the literature a conceptual tool for modelling AWs. On the other hand, also a concrete infrastructure - called MiRAgE - is designed and developed to produce a platform for engineering and deploy such innovative smart environments. 
 
The work carried out in this dissertation fits into the scientific literature and research of Pervasive Computing and Mixed Reality fields. Furthermore, part of the contribution is related also to the area of Cognitive Agents and Multi-Agent Systems, due to the AWs orientation to be deeply connected to a layer involving autonomous agents able to observe and act proactively in the smart environment.",space,34,unknown
10.1109/aimv53313.2021.9670978,to_check,2021 International Conference on Artificial Intelligence and Machine Vision (AIMV),IEEE,2021-09-26 00:00:00,ieeexplore,voice controlled augmented reality for real estate,https://ieeexplore.ieee.org/document/9670978/,"As technology advances, augmented reality is becoming more prevalent in every business. The most frequent usage of AR is to project real things onto the user, which is usually done via an image target. In the real estate industry, no one can deny AR's capacity to improve the buying and selling experience. AR can help real estate developers expand their marketing methods and give clients a more memorable home experience. It has already been used in apps for house design and land hunting, and the industry's strike proves that Augmented Reality has a lot more to give. Everyone nowadays has a smartphone or tablet with which to access the internet, and the technology utilized in these devices is improving every day. As a result, using AR tools in everyday life and having a comfortable AR experience on mobile devices is becoming more convenient. The amount of time spent touring each site with consumers and not having appropriate resources to impress them is a common challenge that real estate developers confront. Augmented reality software is frequently the seal of approval that realtors receive in order to grow their business and overcome these obstacles. This paper proposes a method for projecting a home onto an image target and allowing the user to explore the interior of the house. Voice controllers incorporated into AR can control the interior.",e-commerce,35,not included
,to_check,core,Digital Commons @ University of South Florida,2021-08-01 00:00:00,core,advances in global services and retail management: volume 2,,"This is the second volume of the Advances in Global Services and Retail Management Book Series. This volume has the following parts:   Part 1: Hospitality and Tourism Part 2: Marketing, E-marketing, and Consumer Behavior Part 3: Management Part 4: Human Resources Management Part 5: Retail Management Part 6: Economics Part 7: Accounting and Finance Part 8: Sustainability and Environmental Issues Part 9: Information Technology 
ISBN: 978-1-955833-03-5  Hospitality and Tourism  Significance of VR in the spa: A spatial analysis  Irini Lai Fun Tang, Schultz Zhi Bin Xu, and Eric Chan   Social media marketing in rural hospitality and tourism destination research  Samuel Adeyinka-Ojo and Shamsul Kamariah Abdullah   All aboard! Is space tourism still a fantasy or a reality: An investigation on Turkish market  Emrah Tasarer, Vahit Oguz Kiper, Orhan Batman, and Oguz Turkay   Strategic consciousness and business performance relationship of open innovation strategies in food and beverage businesses  Muhsin Halis, Kazim Ozan Ozer, Hasan Cinnioglu, and Zafer Camlibel   The effects of COVID-19 epidemic on guided tours and alternative tour samples from Turkey  Bayram Akay   The effect of COVID-19 phobia on holiday intention  Halil Akmese and Ali Ilgaz    The effect of the usage of virtual reality in tourism education on learning motivation  Sarp Tahsin Kumlu and Emrah Ozkul   The impact of effective implementation of customer relationship management to the success of hotels in Afikpo North local government of Ebonyi State, Nigeria  Ogboagha Callister and Managwu Lilian   The influence of study travel on quality-oriented education: The case of Handan, China  Wang Jingya and Alaa Nimer Abukhalifeh   The impact of U.S. Cuba policies on Cuban tourism industry: Focus on the Obama and Trump Administration  Jukka M. Laitamaki, Antonio Diaz Medina, and Lisandra Torres Hechavarria   Determination of studentsâ characteristics and perspectives about social entrepreneurship: A case of Anadolu University  Muhammed Kavak, Ipek Itir Can, and Emre Ozan Aksoz   The place of Kazakhstan tourism sector in the countries of the region in terms of transportation infrastructure  Maiya Myrzabekova, Muhsin Halis, and Zafer Camlibel   What are tour guides most praised for? A sharing economy perspective  Derya Demirdelen-Alrawadieh and Ibrahim Cifci   An examination of representations for USA in tourism brochures for Chinese market  Yasong Wang   An exploratory study on cognitive internship perception of tourism students  Ozge Buyuk and Gulsah Akkus   Are you afraid to travel during COVID-19?  Gulsum Tabak, Sibel Canik, and Ebru Guneren   Destination management during the health emergency: A bibliometric analysis  Valentina Della Corte, Giovanna Del Gaudio, Giuliana Nevola, Enrico Di Taranto, and Simone Luongo   Determination of food neophobia levels of International Mersin Citrus Festival participants  Sevda Sahilli Birdir, Nurhayat Iflazoglu, and Kemal Birdir   Analysis of effectiveness of industrial exposure training undertaken by students of hospitality management in star hotels  G. Saravana Kumar   Conceptualization of ecotourism service experiences framework from the dimensions of motivation and quality of experiences: Four realms of experience approach  Jennifer Kim Lian Chan   Does Coronavirus (COVID-19) transform travel and tourism to automation (robots)?  M. Omar Parvez, Ali Ozturen, and Cihan Cobanoglu   Efficiency of internal control systems and the effect of organizational structure and culture on internal control systems in accommodation industry  Kadriye Alev Akmese and Ali Ilgaz   Ethical perceptions of housekeeping department employees: A study in Izmir Province  Tuba Turkmendag and Bayram Sahin   Factors that prevent participation of tourists in online co-creation activities  Resat Arica, Feridun Duman, and Abdulkadir Corbaci   Health sector after COVID-19: Salt thermal facilities example  Azize Serap Tuncer and Sinan Bulut   PRISMA statement and thematic analysis framework in hospitality and tourism research  Samuel Adeyinka-Ojo   Evaluation of Turkish nights as a tourism product: The case of Cappadocia  Meral Buyukkuru, Eda Ozgul Katlav, and Firdevs Yonet Eren   Customer perceptions against COVID-19 precautionary measures of the restaurants: The case of Istanbul-Turkey  Elif Kaymaz and Sevki Ulema   Analysis of e-complaints regarding hotel restaurants during COVID-19 process: The case of Antalya  Sevim Usta and Serkan Sengul    Marketing, E-marketing, and Consumer Behavior  Materialistic social consumption amidst COVID-19 pandemic: Terror management theory in the Malaysia context  Seong-Yuen Toh and Siew-Wai Yuan   A conceptual framework for the mediating role of the flow experience between destination brand experience and destination loyalty  Ipek Kazancoglu and Taskin Dirsehan   Investigating drivers influencing choice behaviour of Islamic investment products  Hanudin Amin   Local food festivals within the scope of destination branding  Hatice Akturk and Atilla Akbaba   Marketing a destination on social media: Case of three municipalities of Izmir  Huseyin Ozan Altin and Ige Pirnar   Perceived usefulness, ease of use, online trust and online purchase intention: Mediating role of attitude towards online purchase  Muhammed Yazeed, Mohammed Aliyu Dantsoho, and Adamu Ado Abubakar    Social media framework for businesses  Nawel Amrouche   Social media marketing the African door of return experience in Badagry-Nigeria  Huseyin Arasli, Maryam Abdullahi, and Tugrul Gunay   The effect of corporate social responsibility on consumer-based brand equity: A research on automobile brands  Ali Koroglu and Ibrahim Avci   The effect of superstitions on consumer luck, horoscope and evil eye-oriented purchasing behavior: A study in Turkey  Ibrahim Avci and Salih Yildiz   The evaluation of S-D orientation on service innovation and performance of airline  Inci Polat and Ozlem Atalik   Brand new leisure constraint: COVID-19  Guliz Coskun    The impact of consumers price level perception on emotions towards supermarkets  Abdulcelil Cakici and Sena Tekeli   The impact of TikTokâs plastic surgery content on adolescentsâ self-perception and purchase intention  Markus Rach   Accelerated modernity: What are the social media stories undergraduate students engage with?  Pericles Asher Rospigliosi and Sebastian Raza-Mejia   Virtual influencer as celebrity endorsers  Fanny Cheung and Wing-Fai Leung   Does millennial shopping orientation using augmented reality enabled mobile applications really impact product purchase intention?  Anil Kumar   Exposure to e-cigarette marketing and product use among highly educated adults  Onur Sahin   Extending the theory of planned behavior to explain intention to use online food delivery services in the context of COVID -19 pandemic  Ahmed Chemseddine Bouarar, Smail Mouloudj, and Kamel Mouloudj   Factors affecting investorsâ buying decision in real estate market in Northern Cyprus  Gurkan Arslan and Karen Howells   From home to the store: Combined effect of music and traffic on consumers shopping behaviour  Luigi Piper, Lucrezia Maria de Cosmo, Maria Irene Prete, and Gianluigi Guido   Market expansion and business growth from the perspective of resources and capabilities: The case of a micro-enterprise  Jose G. Vargas-Hernandez and Omar C. Vargas-Gonzalez   How learning style interacts with voice-assisted technology (VAT) in consumer task evaluation  Bonnie Canziani and Sara MacSween     Effect of brand credibility and innovation on customer based brand equity and overall brand equity in Turkey: An investigation of GSM operators  Suphan Nasir and Ozge Guvendik   Value chain for a B school in India  Vimal Chandra Verma and Devashish Das Gupta    Management  AI as a boost for startups companies: Evidence from Italy  Irene Di Bernardo, Marco Tregua, Greco Fabio, and Ruggiero Andrea   The role of quality management applications for corporate reputations  Ibrahim Sapaloglu and Isik Cicek   Toxicity in organizations: A sample study on the perceived toxicity in Turkish academicians  Mustafa Hakan Atasoy and Muhsin Halis    Which resources are matter to healthcare performance? A case study on Bahrain  Mahmood Asad Ali and Mohamed Sayed Abou Elseoud   Case study: HereWay Inc. European expansion: A facility location problem  Mikhail M. Sher, Michael T. Paz, and Donald R. (Bob) Smith   In search of the effective mission statement: Structural support of the firmâs culture to augment financial performance  Seong-Yuen Toh   Innovation labs to support tourism organization in transforming crisis into opportunities: Insight from a case study  Francesco Santarsiero, Daniela Carlucci, and Giovanni Schiuma   Novelty and success of healthcare service innovation: A comparison between China and the Netherlands  Yu Mu, Rujun Wang and Ying Huang   Public private partnership in selected countries: A comparative analysis   Bekir Parlak and Abdullahi Suleiman Hashi   Strategic orientation of service enterprises towards customers  Korhan Arun and Saniye Yildirim Ozmutlu   The effects of organizational culture on information sharing attitude  Mohammadi Lanbaran Nasrin and Cicek Isik   The impact of industry 4.0 strategy on the work-life balance of employees  Ali Sukru Cetinkaya   The mediating effect of psychological empowerment on inclusive leadership and innovative work behaviour: A research in hotels  Emete Toros, Ahmet Maslakci, and Lutfi Surucu   Assessment of industry 4.0 on manufacturing enterprises: Demographic perspective  Ali Sukru Cetinkaya and M. Kemal Unsacar    Human Resources Management  Affective commitment in new hiresâ onboarding? The role of organizational socialization in the fashion retail industry  Pui Sze Chan, Ho Ching Ching, Pui Yi Ng, and Annie Ko   Do burnout perception levels of nurses working in the health sector differ according to demographic characteristics?  Irfan Akkoc and Korhan Arun   Examining a moderating effect of employee turnover between recruitment and selection practice and organizational performance in Maldives civil service sector  Fathmath Muna, Azam S. M. Ferdous, and Ahmad Albattat   Personnel relationships in the workplace  Ali Sukru Cetinkaya, Shafiq Habibi, and Umut Yavuz   The evolution of human resources empowerment theory: A literature review (1970â2020)  Theodoros Stavrinoudis and Moschos Psimoulis   Teamwork, satisfaction and mediating effect of affective, continuance and normative commitments on employeeâs loyalty  Thalita Aparecida Costa Nicolleti, Eduardo Roque Mangini, Leonardo Aureliano-Silva, Cristiane Sales Pires, and Carolina Aparecida de Freitas Dias   Perceptions of teachers in educational institutions regarding the principles of teaching professional ethics  Gulsah Aki, Nejat Ira, and Hasan Arslan   Influence of psychological empowerment on employee competence in Nigerian universal basic education system: The mediating role of work engagement  Isah Sani, Rashidah Binti Mohammad Ibrahim, and Fazida Karim    Retail Management  Artificial intelligence in retailing  Ibrahim Kircova, Munise Hayrun Saglam, and Sirin Gizem Kose    Customer value in retailing (2000-2020): A narrative review and future research directions  Rajat Gera and Ashish Pruthi   Effect of social media marketing on online retail performance of Konga Nigeria LTD  Abubakar Ado Adamu, Muhammed Yazeed, Mohammed Aliyu Dantsoho, Jamilu Abdulkadir, and Aliyu Audu Gemu   Employment of blue-collar workers in organized retail sector: The case of Turkey  Inci Kayhan-Kuzgun   Saving grace: Digitization to stay or address crisis?   Smitha Vasudevan   Inclusion of disabled consumers in online retail landscape: Web accessibility conformance of Turkish organized food retailersâ web sites  Asiye Ayben Celik   A customer segmentation model proposal for retailers: RFM-V  Pinar Ozkan and Ipek Deveci Kocakoc    Economics  Nigeriaâs economic management: Reflections through monthly interest rate movement from 1996 to 2020 and beyond  Job Nmadu, Halima Sallawu, and Yebosoko Nmadu   A qualitative study of perceptions of the residents of Sidon, Lebanon regarding the economic effect on Sidon with reference to repatriation of the Palestinian refugees  Raja El Majzoub and Karen Howells   Three keys of development: Knowledge, efficiency and innovative entrepreneurship  Irfan Kalayci, Ali Soylu, and Baris Aytekin   Tourism and women empowerment: Empirical findings from past experience and predictions for the post-COVID era  Burcu Turkcan   COVID-19 effect on FDI motivation and their impact on service sector: Case of Georgia  Vakhtang Charaia and Mariam Lashkhi   Economic cooperation between Central Caucasus, China, and EU, under COVID-19 challenges  Vakhtang Charaia and Mariam Lashkhi   Effect of real exchange rate and income on international tourist arrivals for Turkey  Erhan Aslanoglu, Oral Erdogan, and Yasin Enes Aksu   Innovative entrepreneurship in Turkey: Micro and macro perspectives  Irfan Kalayci, Baris Aytekin, and Ali Soylu   Optimal fiscal and price stability in Germany: Autoregressive distributed lags (ARDL) cointegration relationship  Ergin Akalpler and Dahiru Alhaji Birnintsabas   Struggle with COVID-19 crisis within the scope of financial national security: The example of the Republic of Turkey  Silacan Karakus   The nexus between fiscal freedom and investment freedom: The case of E7 countries  Mehmet Bolukbas   To be or not to be a female entrepreneur in the Mexicali Valley  Roberto Burgueno Romero and Jose David Ledezma Torrez    Accounting and Finance  Comparative measurement of working capital efficiency for Borsa Istanbul restaurants and hotels for the COVID-19 period and previous quarters  Fatih Gunay and Gary Cokins   Relationship between business confidence index and non-financial firms foreign exchange assets and liabilities: Evidence from ARDL bound approach  Ilkut Elif Kandil-Goker   The impact of RTGS on internal control - A comparative study between some Iraqi banks  Salowan H. Al Taee and Noor A. Radhi   The impact of working capital on cash management under IAS 7 framework: An examination of tourism listed companies in Indonesia and Turkey  Tri Damayanti and Tuba Derya Baskan     A nexus between mergers & acquisitions and financial performance of firms: A study of industrial sector of Pakistan  Fiza Quareshi, Mukhtiar Ali, and Salar Hussain   Decentralized approach to deep-learning based asset allocation  Sarthak Sengupta, Priyanshu Priyam, and Anurika Vaish    Sustainability and Environmental Issues  Blockchain technology applied to the Consortium Etna DOC to avoid counterfeiting  Matarazzo Agata, Edoardo Carmelo Spampinato, Sergio Arfo, Ugo Sinigaglia, Antonino Bajeli, and Salvino Benanti   Eco-label certification, hotel performance and customer satisfaction: Analysis of a case study and future developments  Michele Preziosi, Alessia Acampora, Roberto Merli, and Maria Claudia Lucchetti   The integration of circular economy in the tourism industry: A framework for the implementation of circular hotels  Martina Sgambati, Alessia Acampora, Olimpia Martucci, and Maria Claudia Lucchetti   Using the theory of planned behavior to explore green food purchase intentions  Katrina Anna Auza and Kamel Mouloudj   Survey on purchasing methods of food products in Tarragona and Catania  Matarazzo Agata, Vazzano Tommaso Alberto, and Squillaci Carmelo    Information Technology  Comparative analysis of tools for matching work-related skill profiles with CV data and other unstructured data  Florian Beuttiker, Stefan Roth, Tobias Steinacher, and Thomas Hanne   State-of-the-art next generation open innovation platforms  Murielle De Roche, Monika Blaser, Patrick Hollinger, and Thomas Hanne   The coverage of AIOT based functional service: Case study of Asian futuristic hotel  Gege Wang, Irini Lai Fun Tang, Eric Chan, and Wai Hung Wilco Chan   The effect of the blockchain technology on service companies and food retailers: An overview of the blockchain use cases and applications  Gokhan Kirbac and Erkut Ergenc   The regulation problem of cryptocurrencies  Lamiha Ozturk and Ece Sulungur   Understanding information technology acceptance by physicians: Testing technology acceptance model  Anuruddha Indika Jagod",finance,36,unknown
,to_check,core,'MDPI AG',2018-12-01 00:00:00,core,hyperparameter optimization for image recognition over an ar-sandbox based on convolutional neural networks applying a previous phase of segmentation by colorâspace,10.3390/sym10120743,"Immersive techniques such as augmented reality through devices such as the AR-Sandbox and deep learning through convolutional neural networks (CNN) provide an environment that is potentially applicable for motor rehabilitation and early education. However, given the orientation towards the creation of topographic models and the form of representation of the AR-Sandbox, the classification of images is complicated by the amount of noise that is generated in each capture. For this reason, this research has the purpose of establishing a model of a CNN for the classification of geometric figures by optimizing hyperparameters using Random Search, evaluating the impact of the implementation of a previous phase of color&#8315;space segmentation to a set of tests captured from the AR-Sandbox, and evaluating this type of segmentation using similarity indexes such as Jaccard and S&#248;rensen&#8315;Dice. The aim of the proposed scheme is to improve the identification and extraction of characteristics of the geometric figures. Using the proposed method, an average decrease of 39.45% to a function of loss and an increase of 14.83% on average in the percentage of correct answers is presented, concluding that the selected CNN model increased its performance by applying color&#8315;space segmentation in a phase that was prior to the prediction, given the nature of multiple pigmentation of the AR-Sandbox",space,37,unknown
,to_check,core,'MDPI AG',2019-02-26 00:00:00,core,hyperparameter optimization for image recognition over an ar-sandbox based on convolutional neural networks applying a previous phase of segmentation by color-space,10.3390/sym10120743,"Immersive techniques such as augmented reality through devices such as the AR-Sandbox and deep learning through convolutional neural networks (CNN) provide an environment that is potentially applicable for motor rehabilitation and early education. However, given the orientation towards the creation of topographic models and the form of representation of the AR-Sandbox, the classification of images is complicated by the amount of noise that is generated in each capture. For this reason, this research has the purpose of establishing a model of a CNN for the classification of geometric figures by optimizing hyperparameters using Random Search, evaluating the impact of the implementation of a previous phase of color-space segmentation to a set of tests captured from the AR-Sandbox, and evaluating this type of segmentation using similarity indexes such as Jaccard and Sorensen-Dice. The aim of the proposed scheme is to improve the identification and extraction of characteristics of the geometric figures. Using the proposed method, an average decrease of 39.45% to a function of loss and an increase of 14.83% on average in the percentage of correct answers is presented, concluding that the selected CNN model increased its performance by applying color-space segmentation in a phase that was prior to the prediction, given the nature of multiple pigmentation of the AR-Sandbox",space,38,unknown
10.23919/ecc.2019.8795916,to_check,2019 18th European Control Conference (ECC),IEEE,2019-06-28 00:00:00,ieeexplore,towards reinforcement learning-based control of an energy harvesting pendulum,https://ieeexplore.ieee.org/document/8795916/,"Harvesting energy from the environment, e.g. ocean waves, is a key capability for the long-term operation of remote electronic systems where standard energy supply is not available. Rotating pendulums can be used as energy converters when excited close to their eigenfrequency. However, to ensure robust operation of the harvester, the energy of the dynamic system has to be controlled. In this study, we deploy a light-weight reinforcement learning algorithm to drive the energy of an Acrobot pendulum towards a desired value. We analyze the algorithm in an extensive series of simulations. Moreover, we explore the real world application of our energy-based reinforcement learning algorithm using a computationally constrained hardware setup based on low-cost components, such as the Raspberry Pi platform.",oceanology,39,unknown
10.5194/npg-21-521-2014,to_check,core,'Copernicus GmbH',2014-04-23 00:00:00,core,full-field and anomaly initialization using a low-order climate model: a comparison and proposals for advanced formulations,https://core.ac.uk/download/305109079.pdf,"Initialization techniques for seasonal-to-decadal climate predictions fall into two main categories; namely full-field initialization (FFI) and anomaly initialization (AI). In the FFI case the initial model state is replaced by the best possible available estimate of the real state. By doing so the initial error is efficiently reduced but, due to the unavoidable presence of model deficiencies, once the model is let free to run a prediction, its trajectory drifts away from the observations no matter how small the initial error is. This problem is partly overcome with AI where the aim is to forecast future anomalies by assimilating observed anomalies on an estimate of the model climate.



The large variety of experimental setups, models and observational networks adopted worldwide make it difficult to draw firm conclusions on the respective advantages and drawbacks of FFI and AI, or to identify distinctive lines for improvement. The lack of a unified mathematical framework adds an additional difficulty toward the design of adequate initialization strategies that fit the desired forecast horizon, observational network and model at hand.



Here we compare FFI and AI using a low-order climate model of nine ordinary differential equations and use the notation and concepts of data assimilation theory to highlight their error scaling properties. This analysis suggests better performances using FFI when a good observational network is available and reveals the direct relation of its skill with the observational accuracy. The skill of AI appears, however, mostly related to the model quality and clear increases of skill can only be expected in coincidence with model upgrades.



We have compared FFI and AI in experiments in which either the full system or the atmosphere and ocean were independently initialized. In the former case FFI shows better and longer-lasting improvements, with skillful predictions until month 30. In the initialization of single compartments, the best performance is obtained when the stabler component of the model (the ocean) is initialized, but with FFI it is possible to have some predictive skill even when the most unstable compartment (the extratropical atmosphere) is observed.



Two advanced formulations, least-square initialization (LSI) and exploring parameter uncertainty (EPU), are introduced. Using LSI the initialization makes use of model statistics to propagate information from observation locations to the entire model domain. Numerical results show that LSI improves the performance of FFI in all the situations when only a portion of the system's state is observed. EPU is an online drift correction method in which the drift caused by the parametric error is estimated using a short-time evolution law and is then removed during the forecast run. Its implementation in conjunction with FFI allows us to improve the prediction skill within the first forecast year.



Finally, the application of these results in the context of realistic climate models is discussed",oceanology,40,not included
7db240ccd5e4bbb8a520d24318e8523eda544e9a,to_check,semantic_scholar,ELIV 2021,2021-01-01 00:00:00,semantic_scholar,ansys real time physics based radar simulation â an enabler for machine learning in the context of autonomous driving,https://www.semanticscholar.org/paper/7db240ccd5e4bbb8a520d24318e8523eda544e9a,"Throughout the evolution of Advanced Driver Assistance Systems (ADAS), the correct perception of the environment has always been a decisive success factor. Capturing and defining scenarios/edge cases, various and heterogenous datasets, multiple sensors/sensorfusion architectures, and perception algorithms are just a few of the many challenges we are facing when implementing such systems. To cope with such levels of complexity, modular approaches are required. Such approaches target flexibility and standardized interfaces between data provided by various sensor modules/models and driving functions. In the Artificial Intelligence (AI) domain, and more precisely when dealing with supervised training of Neural Networks (NN), obtaining valid and accurately labeled datasets is essential. By enabling Machine Learning (ML) in electromagnetic applications, Ansys physics-based Real Time Radar (RTR) introduces a new paradigm for sensor development and integration that leverages GPU hardware and new algorithms to accelerate simulation by orders of magnitude without compromising accuracy. In this paper, a comprehensive workflow for the generation of virtual datasets using the Open Simulation Interface (OSI) will be presented. This workflow will illustrate how the scenario variation process coupled with RTR facilitates the creation of heterogenous/labeled datasets that are ready for training object detection NN. Finally, this presentation will also show the preliminary results obtained when implementing this process. Introduction Machine Learning (ML) is gradually taking over âconventionalâ algorithms that were previously designed to help make Autonomous Vehicles (AVs) a reality. Several auto giants like BMW, VDI-Berichte Nr. 2384, 2021 83 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulÃ¤ssig. Volkswagen, and Volvo are at least partially relying on ML algorithms to solve various parts of the sense-plan-act paradigm. Others, like Tesla [1] are solely relying on ML and in some cases Artificial Intelligence (AI) to provide an end-to-end solution. AVs usually rely on several sensors of different types to perceive their surrounding environment. As such sensors continuously scan the environment and generate raw data, the perception stack processes this data and generates a meaningful virtual map of the surrounding environment. In the area of Computer Vision (CV), usually relying on optical systems, ML algorithms are already successfully deployed in commercial systems [2] â [4]. In addition to optical systems, and due to their superior performances in bad weather conditions and dark environments, radars have also made their way into the AVâs sensor stacks. Though not frequently encountered, ML has also been used to replace some of the traditional radar signal processing algorithms. For example, in [5] the author demonstrates how a fully convolutional network can be used for object detection and 3D estimation using a Frequency-Modulated ContinuousWave (FMCW) radar. Contrary to [5], which is using real data for the training process, the author in [6] illustrates the power of physics-based simulation to also demonstrate the feasibility of using ML approaches to solve radar-based perception problems. As the training process of ML algorithms highly relies on labeled training data, Ansysâ Real Time Radar (RTR) automates generation of labeled data sets by shifting data generation and labeling from the real world to the virtual world. In addition, having an API which is compatible with the Open Simulation Interface (OSI) [7] ensures that a standardized interface is being deployed to describe the virtual environment in which generated scenarios are executed. Finally, this paper illustrates how, with the help of Ansys optiSlang [8], a tool chain is developed to orchestrate the scenario variation process and the simulation workflow to automatically generate labeled datasets for training a Neural Network (NN) based object detection algorithm. Ansys Real Time Radar RTR is an all-GPU implementation of the shooting and bouncing rays (SBR) method optimized for the automotive radar application to simulate a scenario in real time/faster than real time. The simulation, which is based on an arbitrary 3D scene/actor geometry, electrical material VDI-Berichte Nr. 2384, 2021 84 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulÃ¤ssig. properties, including transmissive and reflective dielectrics, 3D polarized antenna patterns, directly calculates the scattered electro-magnetic fields as observed by the radar. As an output, RTR can generate raw I and I+Q A/D data for multi-channel radars in dynamically changing driving scenarios. Objects (e.g., vehicles, pedestrians, road, infrastructure, etc.) can be assigned arbitrary positions, orientations, linear and angular velocities in a scene graph hierarchy through a light-weight API to characterize complex traffic scenarios with negligible simulation overhead. To measure Doppler velocity, automotive radars transmit, receive, and process hundreds of chirps over each Coherent Processing Interval (CPI). Fast Fourier Transformations (FFT) and several post processing algorithms are then applied to hundreds of samples from each chirp/CPI to obtain range-Doppler (RD) images, which will be used for Neural Network (NN) training. These images, as represented in Fig. 1, give a visualization of all scattered fields in terms of relative velocity (Doppler) and distance from the radar (range). Fig. 1: Example of Range-Doppler image. As previously mentioned, RTR includes a lightweight C++ and Python API, enabling it to be integrated into nearly any driving simulator available on the market. In Fig. 2 the APIâs main interfaces are depicted. VDI-Berichte Nr. 2384, 2021 85 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulÃ¤ssig. Fig. 2: RTR's API representing its main inputs and outputs. Starting from left to right, the API give the user access to the following: 1. âRadar Configâ enables the user to configure radar waveforms, radar modes, and antenna patterns. Radar waveforms are defined by parameters such as center frequency, bandwidth, number of frequency samples, CPI duration, number of chirps, number of transmit and receive antennas, and relative antenna positions. 2. âObject and Materialsâ helps the user build the 3D environment to be simulated and assign dielectric material properties. For example, as presented in Fig. 3, a vehicle can be imported as a set of subcomponents. Users can assign appropriate material properties for each component. VDI-Berichte Nr. 2384, 2021 86 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulÃ¤ssig. Fig. 3: Vehicle subcomponents featuring adequate assignment of material properties. 3. âObject Velocitiesâ represents scene dynamics where position and velocity updates are provided at each simulation time step. Such data is usually obtained from any driving simulator or from a set of pre-recorded/measured GPS/IMU data. At each time step, RTR executes a physics-based radar simulation of the scene and returns either the RD data per channel or the raw I/Q channel data (post A/D conversion). Open Simulation Interface To ensure modularity and interchangeability, RTRâs inputs have been adapted to support OSI. Focusing on environment perception and automated driving functions, OSI is an interface specification for models and components of distributed simulation. It defines a generic interface that ensures modularity, interoperability, and integration of simulation frameworkâs individual components. In addition, OSI was developed to address and align with the emerging standard for communication interfaces of real sensors, ISO 23150 [9]. This will eventually ensure a better correlation between communication interfaces used in both virtual and real worlds. Corresponding to OSIâs message description, the OSI:SensorView message represented in Fig. 4 contains the ground truth data that can be generated by any 3rd party driving simulator. The message includes information about the states of dynamic and static actors. VDI-Berichte Nr. 2384, 2021 87 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulÃ¤ssig. Fig. 4: Driving Simulator and RTR connection via OSI. Scenario Variation Neural Networks (NN) are designed to behave as low bias and high variance machines that can perform extremely well on training data. To generalize such machines to new environments, heterogeneous datasets are essential for the training process. Using Ansys optiSlang, scenario variations were generated based on a predefined set of parameters. As represented in Table 1, a set of three parameters were chosen. Table 1: Scenario Variation Parameters Parameter Description Model Name Describes the 3D geometry of the vehicle. Initial Speed A range between 0 and 80 km/h Driver Behavior Aggressive, Normal, Cautious 1. âModel Nameâ defines whether a traffic participant is a car, bus, or a motorcycle. It also describes what vehicle model to use, ensuring a large variety of traffic participants within each generated scenario. 2. âInitial Speedâ may randomly vary between 0 and 80 km/h. Considering that the NN is being trained on RD images, this parameter will ensure that RD scattered fields are randomly distributed in the RD image space along the Doppler velocity axis. 3. âDriver Behaviorâ takes in three different values: Aggressive, Normal and Cautious. Each behaviour ",oceanology,41,unknown
b42fd5d2959d327a2eaa28784b744f74f6b4e6b7,to_check,semantic_scholar,,2010-01-01 00:00:00,semantic_scholar,configuration management best practices: practical methods that work in the real world,https://www.semanticscholar.org/paper/b42fd5d2959d327a2eaa28784b744f74f6b4e6b7,"Successfully Implement High-Value Configuration Management Processes in Any Development Environment As IT systems have grown increasingly complex and mission-critical, effective configuration management (CM) has become critical to an organizations success. Using CM best practices, IT professionals can systematically manage change, avoiding unexpected problems introduced by changes to hardware, software, or networks. Now, todays best CM practices have been gathered in one indispensable resource showing you how to implement them throughout any agile or traditional development organization. Configuration Management Best Practices is practical, easy to understand and apply, and fully reflects the day-to-day realities faced by practitioners. Bob Aiello and Leslie Sachs thoroughly address all six pillars of CM: source code management, build engineering, environment configuration, change control, release engineering, and deployment. They demonstrate how to implement CM in ways that support software and systems development, meet compliance rules such as SOX and SAS-70, anticipate emerging standards such as IEEE/ISO 12207, and integrate with modern frameworks such as ITIL, COBIT, and CMMI. Coverage includes Using CM to meet business objectives, contractual requirements, and compliance rules Enhancing quality and productivity through lean processes and just-in-time process improvement Getting off to a good start in organizations without effective CM Implementing a Core CM Best Practices Framework that supports the entire development lifecycle Mastering the people side of CM: rightsizing processes, overcoming resistance, and understanding workplace psychology Architecting applications to take full advantage of CM best practices Establishing effective IT controls and compliance Managing tradeoffs and costs and avoiding expensive pitfalls Configuration Management Best Practices is the essential resource for everyone concerned with CM: from CTOs and CIOs to development, QA, and project managers and software engineers to analysts, testers, and compliance professionals. Praise for Configuration Management Best Practices Understanding change is critical to any attempt to manage change. Bob Aiello and Leslie Sachss Configuration Management Best Practices presents fundamental definitions and explanations to help practitioners understand change and its potential impact. Mary Lou A. Hines Fritts, CIO and Vice Provost Academic Programs, University of Missouri-Kansas City Few books on software configuration management emphasize the role of people and organizational context in defining and executing an effective SCM process. Bob Aiello and Leslie Sachss book will give you the information you need not only to manage change effectively but also to manage the transition to a better SCM process. Steve Berczuk, Agile Software Developer, and author of Software Configuration Management Patterns: Effective Teamwork, Practical Integration Bob Aiello and Leslie Sachs succeed handsomely in producing an important book, at a practical and balanced level of detail, for this topic that often goes without saying (and hence gets many projects into deep trouble). Their passion for the topic shows as they cover a wonderful range of topicseven culture, personality, and dealing with resistance to changein an accessible form that can be applied to any project. The software industry has needed a book like this for a long time! Jim Brosseau, Clarrus Consulting Group, and author of Software Teamwork: Taking Ownership for Success A must read for anyone developing or managing software or hardware projects. Bob Aiello and Leslie Sachs are able to bridge the language gap between the myriad of communities involved with successful Configuration Management implementations. They describe practical, real world practices that can be implemented by developers, managers, standard makers, and even Classical CM Folk. Bob Ventimiglia, Bobev Consulting A fresh and smart review of todays key concepts of SCM, build management, and related key practices on day-to-day software engineering. From the voice of an expert, Bob Aiello and Leslie Sachs offer an invaluable resource to success in SCM. Pablo Santos Luaces, CEO of Codice Software Bob Aiello and Leslie Sachs have a gift for stimulating the types of conversation and thought that necessarily precede needed organizational change. What they have to say is always interesting and often important. Marianne Bays, Business Consultant, Manager and Educator",oceanology,42,unknown
c3d3d7b56fcc775c96777b1c6123b46a452b4aee,to_check,semantic_scholar,,2010-01-01 00:00:00,semantic_scholar,data-centric programming best practices: using dds to integrate real-world systems,https://www.semanticscholar.org/paper/c3d3d7b56fcc775c96777b1c6123b46a452b4aee,".................................................................................................................................... 3 Real-World Systems Programming ........................................................................................... 4 Defining a Data Model .............................................................................................................. 4 DDS Maintains the State of the World as Defined by the Data Model ..................................... 6 About DDS ................................................................................................................................ 8 Best Practices in DDS Programming ........................................................................................ 9 G1. Start by defining a data model, then map the data model to DDS domains, data types and Topics. .............................................................................................................. 9 G2. Fully define your DDS Types, do not rely on opaque bytes or other custom encapsulations ................................................................................................................ 11 G3. Isolate subsystems into DDS Domains. Use mediation, such as RTI Routing Service, to bridge Domains ............................................................................................. 12 G4. Use keyed Topics. For each data type, indicate to DDS the fields that uniquely identify the data-object .................................................................................................... 13 G5. Large teams should create a targeted application platform with system-wide QoS profiles and limited access to the DDS APIs. .................................................................. 16 G6. Configure QoS using the XML Profiles .................................................................... 17 Conclusions ............................................................................................................................. 18 References .............................................................................................................................. 18 Best-Practices Data-Centric Programming: Using DDS to Integrate Real-World Systems November 2010 3 Â© 2010 Real-Time Innovations Abstract Systems are often implemented by teams using a variety of technologies, programming languages, and operating systems. Integrating and evolving these systems becomes complex. Traditional approaches rely on low-level messaging technologies, delegating much of the message interpretation and information management services to application logic. This complicates system integration because different applications could use inconsistent interpretations and implementations of information-management services, such as detecting component presence, state management, reliability and availability of the information, handling of component failures, etc. Integrating modern systems requires a new, modular network-centric approach that avoids these historic problems by relying on standard APIs and protocols that provide stronger information-management services. For example, many of these systems are heterogeneous, mixing a variety of computer hardware, operating systems, and programming languages. Developers often use Java, .NET, or web-scripting to develop consoles and other GUI-oriented applications, and C or C++ for specialized hardware, device drivers, and performanceor time-critical applications. The end system might mix computers running Windows, Linux, and other operating systems, such as Mac OS X, Android, or real-time operating systems like VxWorks and INTEGRITY. The use of standard APIs and interoperable protocols allows all these systems to be easily integrated and deployed. Today, these systems are typically developed using a service-oriented approach and integrated using standards-based middleware APIs such as DDS, JMS, and CORBA, and protocols such as DDS-RTPS, Web-Services/SOAP, REST/HTTP, AMQP, and CORBA/IIOP. This whitepaper focuses on âreal-worldâ systems, that is, systems that interact with the external physical world and must live within the constraints imposed by real-world physics. Good examples include air-traffic control systems, real-time stock trading, command and control (C2) systems, unmanned vehicles, robotic and vetronics, and Supervisory Control and Data Acquisition (SCADA) systems. More and more these âreal-worldâ systems are integrated using a Data-Centric PublishSubscribe approach, specifically the programming model defined by the Object Management Group (OMG) Data Distribution Service (DDS) specification. This whitepaper describes the basic characteristics of real-world systems programming, reasons why DDS is the best standard middleware technology to use to integrate these systems, and a set of âbest practicesâ guidelines that should be applied when using DDS to implement these systems. Best-Practices Data-Centric Programming: Using DDS to Integrate Real-World Systems November 2010 4 Â© 2010 Real-Time Innovations Real-World Systems Programming Real-World systems refer to a class of software systems that operate continuously and interact directly with real-world objects, such as aircraft, trains, stock transactions, weapons, robotic and manufacturing equipment, etc. Unlike systems involving only humans and computers, real-world systems have to live within the constraints imposed by the physics of the external world. Notably, time cannot be slowed, paused, or reversed. The implication is that these systems must be able to handle the information at the pace it arrives at, as well as be robust to changes in the operating environment. In addition to these environmental considerations, the nature of typical real-world applications also places demands on their availability and need to continue operating even in the presence of partial failures. In order to interact with the real world, software must include a reasonable, if simplified, model of the external world. This model typically includes aspects of the âstate of the worldâ relevant to system operations. Here the word âstateâ is used in the normal sense in software modeling and programming. State summarizes the past inputs to the system from its initial state and contains all the information necessary for a system or program to know how it should react to future events or inputs. Imagine that a new component or application starts and joins a system. The âstate of the systemâ contains the information that this new component needs to acquire before it is ready to start performing its function. A typical component would normally only need access to a subset of that state, the portion that directly affects its operation. For example, in an air-traffic management problem, the relevant aspects of the state of the world might include the current location and trajectory of every aircraft, the flight plans of all flights within a 24-hour window, specific details on each aircraft (type, airline, crew), etc. Once a software component or subsystem is running, it interacts with other components by exposing part of its state, notifying other components when its state changes, and invoking operations on (or sending messages to) other components. Each component reacts to these information exchanges by updating its internal model of the world and using that to perform its necessary actions. Defining a Data Model A data model is simply an organized description of the state of the system. Thus, it includes data types, processes for transferring and updating those types, and methods for accessing the data. It does not typically include functions that can alter the data or (importantly) the application-level logic that affects the data. Governance organizations and system integrators often start their design by designing the system data-model. There are good reasons for this approach: â¢ A data model provides governance across disparate teams and organizations, allowing components developed at different points in time by different organizations to be integrated. This makes it an ideal starting point for a central design or governance authority. Best-Practices Data-Centric Programming: Using DDS to Integrate Real-World Systems November 2010 5 Â© 2010 Real-Time Innovations â¢ A data model represents the better understood, more invariant aspects of the system. Typically the data model is grounded in the âphysics of the system.â That is, it describes the kinds of objects and sensors it manages (like aircraft locations, flight plans, and vehicle positions). The data model is not strongly tied to applicationspecific use cases (e.g., the possible fields in a flight plan are a consequence of the nature of aircraft flight); this makes the data model a good starting point, since the full set of use cases might not be well known in advance or might be the responsibility of a different team. â¢ A data model increases decoupling between systems and components. The data model is grounded in the essential information present in the system and it does not depend so much on the use cases that access the information. For example, an airtraffic control model might include a definition of a âflight plan,â but not whether it is automatically generated using an optimization algorithm, checked for collisions, or altered in mid-flight. Using the data model as the basis for the integration avoids over-constraining the design, leaving it open to allow future evolution and use cases. Contrast this with a design based on defining service invocation APIs which are intimately tied to the details of each service and are likely to change as new use cases are incorporated Example Data Model Imagine designing a simple âchatâ application. The underlying Data-Model could be defined to contain four kinds of objects summarized in the table below: Object Kind Key Fields Other Fields Description Person EmailAddress Name, Loc",oceanology,43,unknown
036a7c42a459eb751bba2f8badec3b62d6328c10,to_check,semantic_scholar,,2010-01-01 00:00:00,semantic_scholar,editorial wireless sensor networks: design for real-life deployment and deployment experiences wireless sensor networks: design for real-life deployment and deployment experiences,https://www.semanticscholar.org/paper/036a7c42a459eb751bba2f8badec3b62d6328c10,"Wireless sensor networks (WSNs) are among the most promising technologies of the new millennium. The opportunities afforded by being able to program networks of small, lightweight, low-power, computation- and bandwidth-limited nodes have attracted a large community of researchers and developers. However, the unique set of capabilities offered by the technology produces an exciting but complex design space, which is often difficult to negotiate in an application context. Deploying sensing physical environments produces its own set of challenges, and can push systems into failure modes, thus revealing problems that can be difficult to discover or reproduce in simulation or the laboratory. Sustained efforts in the area of wireless networked sensing over the last 15 years have resulted in a large number of theoretical developments, substantial practical achievements, and a wealth of lessons for the future. It is clear that in order to bridge the gap between (on the one hand) visions of very large scale, autonomous, randomly deployed networks and (on the other) the actual performance of fielded systems, we need to view deployment as an essential component in the process of developing sensor networks: a process that includes hardware and software solutions that serve specific applications and end-user needs. Incorporating deployment into the design process reveals a new and different set of requirements and considerations, whose solutions require innovative thinking, multidisciplinary teams and strong involvement from end-user communities. This special feature uncovers and documents some of the hurdles encountered and solutions offered by experimental scientists when deploying and evaluating wireless sensor networks in situ, in a variety of well specified application scenarios. The papers specifically address issues of generic importance for WSN system designers: (i) data quality, (ii) communications availability and quality, (iii) alternative, low-energy sensing modalities and (iv) system solutions with high end-user added value and cost benefits. The common thread is deployment and deployment evaluation. In particular, satisfaction of application requirements, involvement of the end-user in the design and deployment process, satisfactory system performance and user acceptance are concerns addressed in many of the contributions. The contributions form a valuable set, which help to identify the priorities for research in this burgeoning area: Robust, reliable and efficient data collection in embedded wireless multi-hop networks are essential elements in creating a true deploy-and-forget user experience. Maintaining full connectivity within a WSN, in a real world environment populated by other WSNs, WiFi networks or Bluetooth devices that constitute sources of interference is a key element in any application, but more so for those that are safety-critical, such as disaster response. Awareness of the effects of wireless channel, physical position and line-of-sight on received signal strength in real-world, outdoor environments will shape the design of many outdoor applications. Thus, the quantification of such effects is valuable knowledge for designers. Sensors' failure detection, scalability and commercialization are common challenges in many long-term monitoring applications; transferable solutions are evidenced here in the context of pollutant detection and water quality. Innovative, alternative thinking is often needed to achieve the desired long-lived networks when power-hungry sensors are foreseen components; in some instances, the very problems of wireless technology, such as RF irregularity, can be transformed into advantages. The importance of an iterative design and evaluation methodologyâfrom analysis to simulation to real-life deploymentâshould be well understood by all WSN developers. The value of this is highlighted in the context of a challenging WPAN video-surveillance application based on a novel Nomadic Access Mechanism. Cost benefits to be drawn from devising a WSN based solution to classic application areas such as surveillance are often a prime motivator for WSN designers; an example is offered here based on the use of intelligent agents for intrusion monitoring. Last but not least, the practicality and usability of the WSN solutions found for novel applications is key to their adoption. This is particularly true when the end-users of the developed technology are medical patients. The importance of feedback, elegant hardware encapsulation and extraction of meaning from data is presented in the context of novel orthopedic rehabilitation aids. Overall, this feature offers wide coverage of most issues encountered in the process of design, implementation and evaluation of deployable WSN systems. We trust that designers and developers of WSN systems will find much work of value, ranging from lessons learned, through solutions to known hurdles, to novel developments that enhance applications. Finally, we would like to thank all authors for their valuable contributions!",oceanology,44,not included
3ee6c2e1aebf16c7b62774700b99a4320ebfefea,to_check,semantic_scholar,IEEE Internet of Things Journal,2019-01-01 00:00:00,semantic_scholar,mc-sdn: supporting mixed-criticality real-time communication using software-defined networking,https://www.semanticscholar.org/paper/3ee6c2e1aebf16c7b62774700b99a4320ebfefea,"Despite recent advances, there still remain many problems to design reliable cyber-physical systems. One of the typical problems is to achieve a seemingly conflicting goal, which is to support timely delivery of real-time flows while improving resource efficiency. Recently, the concept of mixed-criticality (MC) has been widely accepted as useful in addressing the goal for real-time resource management. However, it has not been yet studied well for real-time communication. In this paper, we present the first approach to support MC flow scheduling on switched Ethernet networks leveraging an emerging network architecture, software-defined networking (SDN). Though SDN provides flexible and programmatic ways to control packet forwarding and scheduling, it yet raises several challenges to enable real-time MC flow scheduling on SDN, including: 1) how to handle (i.e., drop or re-prioritize) out-of-mode packets in the middle of the network when the criticality mode changes and 2) how the mode change affects end-to-end transmission delays. Addressing such challenges, we develop MC-SDN that supports real-time MC flow scheduling by extending SDN-enabled switches and OpenFlow protocols. It manages and schedules MC packets in different ways depending on the system criticality mode. To this end, we carefully design the mode change protocol that provides analytic mode change delay bound, and then resolve implementation issues for system architecture. For evaluation, we implement a prototype of MC-SDN on top of Open vSwitch, and integrate it into a real world network testbed as well as a 1/10 autonomous vehicle. Our extensive evaluations with the network testbed and vehicle deployment show that MC-SDN supports MC flow scheduling with minimal delays on forwarding rule updates and it brings a significant improvement in safety in a real-world application scenario.",oceanology,45,unknown
cd51bbfd51cde0c089d9dfb7d30bfc124d9b7c55,to_check,semantic_scholar,,2020-01-01 00:00:00,semantic_scholar,"summary for cife seed proposals for academic year 2020-21 proposal number: 2020-04 proposal title: hybrid physical-digital spaces: transforming the design, operation, and experience of built environments to promote health and wellbeing",https://www.semanticscholar.org/paper/cd51bbfd51cde0c089d9dfb7d30bfc124d9b7c55,"up to 150 words) Increasing evidence suggests built office features (e.g., lighting, materials, and ventilation) have substantial impacts on occupant wellbeing. A key next direction is field studies at industry partner sites to examine real-world workplaces. We propose to develop innovative Internet of Things (IoT) techniques that integrate data from building instrumentation, personal device sensors, and self-report interfaces and then deploy this platform in-the-wild to capture rich, longitudinal, ecologically-valid data about the status of office workers and the spaces they occupy. Insights will advance scientific knowledge of how buildings impact wellbeing as well as produce practical implications for building designers and operators. A timely component will explore how covid-19 has temporally or fundamentally changed occupant behaviors and operational decisions (e.g., physical distancing desks and ventilation settings that reduce pathogen spread). Overall, our proposed research has the potential to transform the industryâs thinking on how built environments can be designed, operated, and experienced. Hybrid Physical-Digital Spaces: Transforming the Design, Operation, and Experience of Built Environments to Promote Health and Wellbeing Problem and Significance Considering that people in the U.S. spend 87% of their time in indoor spaces , we assert that 1 buildings are powerful yet underleveraged loci for promoting human wellbeing. Imagine an intelligent office that could adapt soundscape systems to manage noise in open floor plans, optimize space reservation or utilization to foster collaborations and save energy, or provide digital information displays that promote employee connectedness and physical activity. Towards actualizing our vision of such hybrid physical-digital spaces, our proposal strives to develop, apply, and evaluate novel scientific and engineering approaches that will transform the industryâs thinking around how built environments can be designed, operated, and experienced. Increasingly, hypotheses suggest that built features of indoor environments (e.g., lighting, materials, and ventilation) have substantial impacts on occupants (e.g., employee recruitment and retention, absenteeism, cognition, creativity, productivity, social interactions, physical activity and health, and psychological wellbeing). In turn, these individual outcomes also drive pivotal organizational outcomes such as product innovation, workforce diversity, employee turnover, market share, and profitability. Examples illustrate how building interventions can have huge impacts : enhancing employee exposure to daylight can save businesses ~$2,000/yr per capita 2 , better air quality can raise cognitive scores of workers by 101% 3 , and increasing indoor access to biophilic elements could recoup $23 billion considering 10% of workplace absenteeism (a $226 billion dollar problem) is attributable to architecture that inadequately connects to nature 4 . However, few of these hypotheses have been tested at scale, over time, and in real world conditions . Instead, most prior efforts are small sample, short-term correlational studies based on potentially biased and sparse self-reported data. A more rigorous, scientific, and human-centered approach to study and engineer buildings that promote wellbeing can have major implications at individual, organizational, and societal levels (see Figure 1), offering both foundational theoretical knowledge as well as practical strategies for building designers and operators. Figure 1. Relations among building features and human outcomes at various levels. Further, âsmart buildingsâ today typically focus on basic sensing and control for energy savings, thermal comfort, and security. Connecting to CIFEâs Vision for the Future of Building Users, we argue buildings of the future can go beyond such bottom line outcomes to be more interactive and human-centered: aware of and responsive to occupantsâ cognitive, mental, and physical feelings and needs, while respecting privacy and promoting positive indoor experiences . 1 Klepeis, et al., 2001; 2 Heschong & Mahone, 2003; 3 Allen et al., 2016; 4 Elzeyadi, 2011. <Landay-Billington> < Hybrid Physical-Digital Spaces> 1 Theoretical and Practical Points of Departure It is imperative to increase understanding of exactly what built attributes have what impacts and on whom, in a scalable, longitudinal, and inclusive manner. Thus through technology-driven assessment and hybrid physical-digital interventions, we aim to (a) fundamentally advance the science on how built environments impact human wellbeing and, in turn, (b) generate guidelines that can revolutionize the way spaces are designed, operated, and experienced . Our current scope focuses on office spaces and workers; though an overarching goal is for our developed approaches and insights to establish a foundation that enables future research with additional populations and environments (e.g., physicians and patients in clinical settings, students and teachers in classrooms, and traditionally marginalized shift and temporary workers). In particular, our reusable platform will help others study this wider range of buildings and occupants; and combining these approaches with emerging endeavors such as biophilic design and precision interventions provides a novel opportunity to not only more deeply investigate but also address long-running public health challenges and systemic inequities facing society. In these ways, we hope to positively impact a broad cross-section of stakeholders at individual, organizational, and institutional levels. Moreover, this project will support interdisciplinary fertilization across engineering, computing, psychology, law, and medicine . Research Methods and Work Plan Our research agenda is to support the design and operation of built facilities that augment human capabilities and wellbeing â and have a fundamental positive change on the way indoor spaces are experienced by the people that occupy them. By introducing intelligent systems capable of gathering and interpreting building and occupant data as well as delivering adaptive interventions in response, novel roles will also emerge for managing buildings and the activities that take place inside them. To achieve these goals, our research will comprise three main activities: 1. Developing an extensible and secure data collection and machine learning platform . A key aim of this research is scientifically examining how built spaces impact human wellbeing. To pursue this investigation and develop methods that enable buildings to be more aware of occupantsâ states and needs, we have been developing pattern detection software that integrates data from (a) personal devices (smartphones, smartwatches, fitness trackers), (b) building instrumentation or portable environmental sensors (light levels, air quality), and (c) experience sampling interfaces that prompt occupants for subjective information through quick, validated self-report techniques. Figure 2 illustrates examples of these assessment components. This work involves addressing a number of technical challenges, such as selecting sampling rates and window sizes to maximize efficiency, developing methods for analyzing asynchronous and sparse sensor data, and developing privacy-sensitive feature engineering strategies for detecting and predicting wellbeing outcomes of interest. We also plan to package our platform as a reusable toolkit that can be applied by other researchers and building operators. This work is ongoing and a basic version will be ready by summer. Once development is complete, CIFE support would allow us to move onto the next critical phase: moving out of the lab and into the field. <Landay-Billington> < Hybrid Physical-Digital Spaces> 2 Figure 2. Platform to integrate data from personal devices, building sensors, and subjective self-report. 2. Deploying the platform through a mixed-method study with industry partners . The next step in our research is to deploy this platform at field sites in partnership with View, Inc. (specifically, at TIAA offices in Manhattan, this summer/fall) to capture rich, longitudinal, ecologically-valid data about behavioral, psychological, and physiological states of occupants and their everyday work environments. Our plan is to recruit a sample of approximately 150 employees for a period of 18 weeks, which will involve a baseline phase followed by systematic variation of built features (Views/No Views, Plants/No Plants, and Diversity/No Diversity in artwork) and measurement of indicators hypothesized to promote both personal wellbeing and organizational performance, based on the literature and our formative online and lab studies, described below. In combination with the engineering-focused activities to implement and install the platform, deployment will occur in tandem with ethnographic work (e.g., observations, interviews, and surveys) to manually validate reliability of the systemâs automated inferences as well as gain a more qualitative portrait of occupant experiences in various spaces. Privacy-centric engagements will additionally investigate stakeholdersâ attitudes regarding the capture of various types of information to derive implications about informed consent and personal data management. Along similar lines, it will be critical to responsibly manage captured data, especially potentially sensitive and exploitable data about wellness or performance. Therefore all studies will be conducted with oversight and approval from the Stanford Institutional Review Board (IRB). In addition to obtaining participantsâ informed consent, we will also design sensor and data collection mechanisms to use an opt-in model, including partial participation. Our data management systems can also allow individuals to view and delete their personal data, including if purging is desired in the event of study withdrawal. Our research team has exp",oceanology,46,unknown
b8f7fa8d93c5ee4c3df988ba7c7499b1db51706e,to_check,semantic_scholar,2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),2020-01-01 00:00:00,semantic_scholar,towards autonomous smart sensing systems,https://www.semanticscholar.org/paper/b8f7fa8d93c5ee4c3df988ba7c7499b1db51706e,"Since the 1990's, researchers in both academia and industry have been exploring ways to exploit the potential for Wireless Sensor Networks (WSNs) to revolutionize our understanding of - and interaction with - the world around us. WSNs have therefore been a major focus of research over the past 20 years. While WSNs offer a persuasive solution for accurate real-time sensing of the physical world, they are yet to be as ubiquitous as originally predicted when the technology was first envisaged. Technical difficulties exist which have inhibited the anticipated uptake in WSN technologies. The most challenging of these have been identified as system reliability, battery lifetime, maintenance requirements, node size and ease of use. Over the past decade, the Wireless Sensor Networks (WSN) group at the Tyndall National Institute, has been at the forefront of driving the vision of ubiquitously deployed, extended lifetime, low power consumption embedded systems providing information rich data streams wirelessly in (close to) real-time. In this time, the WSN group has developed multiple novel, first of kind, wireless multi-sensor systems and deployed these in the world around us, overcoming the technical challenges associated with ensuring robust and reliable long-term data sets from our environment. This work is focused on investigating and addressing these challenges through the development of the new technologies and system integration methodologies required to facilitate and implement WSNs and validate these in real deployments. Specifically, discussed are the development and deployment of novel WSN systems in the built environment, environmental monitoring and fitness and health monitoring systems.The key research challenges identified and discussed are:a)The development of resource-constrained, extremely low power consumption systems incorporating energy-efficient hardware and software algorithms.b)The development of highly reliable extremely long duration deployments which through the use of appropriate energy harvesting solutions facilitate (near) zero maintenance sensor networks.c)The development of low power consumption miniaturized wearable microsysteThe development of technologies to address these challenges in terms of cost, size, power consumption and reliability which need to be tested and validated in real world deployments of wireless sensing systems is discussed. It is clear that when looking at the scale up of deployments of novel WSNs, that to be successful, such systems need to ""be invisible, last forever, cost nothing and work out of the box"". This paper describes these relevant technologies and associated project demonstrators",oceanology,47,unknown
577a8528c2dd27d9c36d9cb1e63c6667c9c3370d,to_check,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,challenges and opportunities in the future applications of iot technology,https://www.semanticscholar.org/paper/577a8528c2dd27d9c36d9cb1e63c6667c9c3370d,"The advent of internet of things (IoT) has influenced and revolutionized the information systems and computing technologies. A computing concept where physical objects used in daily life, will identify themselves by getting connected to the internet is called IoT. Physical objects embedded with electronic, radio-frequency identification, software, sensors, actuators and smart objects converge with the internet to accumulate and share data in IoT. IoT is expected to bring in extreme changes and solutions to most of the daily problems in the real world. Thus, IoT provides connectivity for everyone and everything at any time. The IoT embeds some intelligence in Internet connected objects to communicate, exchange information, take decisions, invoke actions and provide amazing services. It has an imperative economic and societal impact for the future construction of information, network, and communication technology. In the upcoming years, the IoT is expected to bridge various technologies to enable new applications by connecting physical objects together to support the intelligent decision making. As the most cost-effective and performant source of positioning and timing information in outdoor environments, the global navigation satellite systems(GNSS) has become an essential element of major contemporary technology developments notably including the IoT, Big Data, Smart Cities and Multimodal Logistics. By 2020, there will be more than 20 billion interconnected IoT devices, and its market size may reach $1.5 trillion. Projections for the impact of IoT on the Internet and economy are impressive, with some anticipating as many as 100 billion connected IoT devices and a global economic impact of more than $11 trillion by 2025. Regulators can play a role in encouraging the development and adoption of the IoT, by preventing abuse of market dominance, protecting users and protecting Internet networks while promoting efficient markets and the public interest. Regulators can consider and identify some measures to foster development of the IoT. Encourage development of LTEâA and 5G wireless networks, and keep need for IoTâspecific spectrum under review. Universal IPv6 adoption by governments in their own services and procurements, and other incentives for private sector adoption. Increasing interoperability through competition law and give users a right to easy access to personal data. Support global standardization and deployment of remotely provisioned SIMs for greater machine to machine competition. Particular attention will be needed from regulators to IoT privacy and security issues, which are key to encouraging public trust in and adoption of the technology. This paper focuses specifically on the essential technologies that enable the implementation of IoT and the general layered architecture of IoT, the market of IoT and GNSS technologies and their impact of the world economy, application domain of IoT and finally the Policy and regulatory implications and best practices.",oceanology,48,unknown
8002fff47f40e6126bf3f9f7fabea1ac9e1cbb4e,to_check,semantic_scholar,Transportation Research Record: Journal of the Transportation Research Board,2018-01-01 00:00:00,semantic_scholar,hardware-in-the-loop testing of connected and automated vehicle applications: a use case for queue-aware signalized intersection approach and departure,https://www.semanticscholar.org/paper/8002fff47f40e6126bf3f9f7fabea1ac9e1cbb4e,"Most existing studies on connected and automated vehicle (CAV) applications apply simulation to evaluate system effectiveness. Model accuracy, limited data for calibration, and simulation assumptions limit the validity of evaluation results. One alternative approach is to use emerging hardware-in-the-loop (HIL) testing methods. HIL test environments enable physical test vehicles to interact with virtual vehicles from traffic simulation models, providing an evaluation environment that can replicate deployment conditions at early stages of CAV technology implementation without incurring excessive costs related to large field tests. In this study, a HIL testing system for vehicle-to-infrastructure (V2I) CAV applications is developed. The involved software and hardware includes a physical CAV controlled in real time, a traffic signal controller, communication devices, and a traffic simulator (VISSIM). Such HIL systems increase validity by considering the physical vehicleâs trajectoriesâwhich are constrained by real-world factors such as GPS accuracy, communication delay, and vehicle dynamicsâin a simulated traffic environment. The developed HIL system is applied to test a representative early deployment CAV application: queue-aware signalized intersection approach and departure (Q-SIAD). The Q-SIAD algorithm generates recommended speed profiles based on the vehicleâs status, signal phase and timing (SPaT), downstream queue length, and system constraints and parameters (e.g., maximum acceleration and deceleration). The algorithm also considers the status of other vehicles in designing the speed profiles. The experiment successfully demonstrated this functionality with one test CAV driving through one intersection controlled by a fixed-timing traffic signal under various simulated traffic conditions.",oceanology,49,not included
ef8e465f80f41ec5cb3dbdb527c8509e66abaf5c,to_check,semantic_scholar,,2019-01-01 00:00:00,semantic_scholar,a framework to secure applications with isa heterogeneity,https://www.semanticscholar.org/paper/ef8e465f80f41ec5cb3dbdb527c8509e66abaf5c,"Software security attacks are evolving from exploiting common code vulnerabilities to exploiting micro architecture side-channels. Traditional software diversity or code randomization techniques diversify the code memory layout and make it difficult for potential attackers to pinpoint the precise location of the target vulnerability. However, those approaches may not be sufficient enough for the new micro architecture attacks (e.g., Spectre). While some architecture researchers have proposed using diverse ISA configurations to defeat code injection or code reuse attacks, most of these works remain in the simulation stage due to legal, licensing, and verification costs involved in bringing a heterogeneous chip design into physical hardware [39]. In this paper, we report our on-going work of HeterSec, a framework to secure applications utilizing real world heterogeneous ISA machines. HeterSec runs on top of the commodity x86_64 and ARM64 machines. It gives the process the ability to dynamically select its underlying ISA environment. Therefore, the protected process would hide the vulnerable targets with the diversified instruction set, or would detect the abnormal behavior by comparing the execution results step-by-step from multiple ISA-diversified instances. To demonstrate the effectiveness of such software framework, we implemented HeterSec on Linux and showed its deployability by running it on a x86_64 and ARM64machine pair, connected using InfiniBand. We then conduct two case studies with HeterSec. In the first case, we timely randomize the process execution path across the ISA, which achieves similar security guarantees as the existing architecture based solutions. In the second case, we implement a multi-ISA based multi-version execution (MVX) system, providing a stronger security guarantee than current homogeneousISA MVX designs.",oceanology,50,unknown
2ad3366962d249b7b63c4986ebb0cb22ea212a75,to_check,semantic_scholar,,2005-01-01 00:00:00,semantic_scholar,"service-oriented architecture compass: business value, planning, and enterprise roadmap",https://www.semanticscholar.org/paper/2ad3366962d249b7b63c4986ebb0cb22ea212a75,"Praise for Service-Oriented Architecture Compass""A comprehensive roadmap to Service-Oriented Architecture (SOA). SOA is, in reality, a business architecture to be used by those enterprises intending to prosper in the 21st century. Decision makers who desire that their business become flexible can jumpstart that process by adopting the best practices and rules of thumb described in SOA Compass.""iÂ¾ÂBob Laird, MCI IT Chief Architect""The book Service-Oriented Architecture Compass shows very clearly by means of real projects how agile business processes can be implemented using Service-Oriented Architectures. The entire development cycle from planning through implementation is presented very close to practice and the critical success factors are presented very convincingly.""iÂ¾ÂProfessor Dr. Thomas Obermeier, Vice Dean of FHDW Bergisch Gladbach, Germany""This book is a major improvement in the field. It gives a clear view and all the key points on how to really face a SOA deployment in today's organizations.""iÂ¾ÂMario Moreno, IT Architect Leader, Generali France""Service-Oriented Architecture enables organizations to be agile and flexible enough to adopt new business strategies and produce new services to overcome the challenges created by business dynamism today. CIOs have to consider SOA as a foundation of their Enterprise Applications Architecture primarily because it demonstrates that IT aligns to business processes and also because it positions IT as a service enabler and maximizes previous investments on business applications.To understand and profit from SOA, this book provides CIOs with the necessary concepts and knowledge needed to understand and adapt it into their IT organizations.""iÂ¾ÂSabri Hamed Al-Azazi, CIO of Dubai Holding, Sabri""I am extremely impressed by the depth and scale of this book! The title is perfectiÂ¾Âwhen you know where you want to go, you need a compass to guide you there! After good IT strategy leads you to SOA, this book is the perfect vehicle that will drive you from dream to reality. We in DSK Bank will use it as our SOA bible in the ongoing project.""iÂ¾ÂMiro Vichev, CIO, DSK Bank, Bulgaria, member of OTP Group""Service-Oriented Architecture offers a pathway to networking of intra- and inter-corporate business systems. The standards have the potential to create far more flexible and resilient business information systems than have been possible in the past. This book is a must-read for those who care about the future of business IT.""iÂ¾ÂElizabeth Hackenson, CIO, MCI""Service-Oriented Architecture is key to help customers become on demand businessesiÂ¾Âa business that can quickly respond to competitive threats and be first to take advantage of marketplace opportunities. SOA Compass is a must-read for those individuals looking to bridge the gap between IT and business in order to help their enterprises become more flexible and responsive.""iÂ¾ÂMichael Liebow, Vice President, Web Services and Service-Oriented Architecture, IBM Business Consulting Services""This book is a welcome addition to SOA literature. It articulates the business case and provides practical proven real-world advice, guidance, tips, and techniques for organizations to make the evolution from simple point-to-point web services to true SOA by addressing such topics as planning, organization, analysis and design, security, and systems management.""iÂ¾ÂDenis O'Sullivan, Fireman's Fund Enterprise ArchitectMaximize the business value and flexibility of your SOA deploymentIn this book, IBM Enterprise Integration Team experts present a start-to-finish guide to planning, implementing, and managing Service-Oriented Architecture. Drawing on their extensive experience helping enterprise customers migrate to SOA, the authors share hard-earned lessons and best practices for architects, project managers, and software development leaders alike.Well-written and practical, Service-Oriented Architecture Compass offers the perfect blend of principles and ""how-to"" guidance for transitioning your infrastructure to SOA. The authors clearly explain what SOA is, the opportunities it offers, and how it differs from earlier approaches. Using detailed examples from IBM consulting engagements, they show how to deploy SOA solutions that tightly integrate with your processes and operations, delivering maximum flexibility and value. With detailed coverage of topics ranging from policy-based management to workflow implementation, no other SOA book offers comparable value to workingIT professionals.Coverage includes SOA from both a business and technical standpointiÂ¾Âand how to make the business case Planning your SOA project: best practices and pitfalls to avoid SOA analysis and design for superior flexibility and value Securing and managing your SOA environment Using SOA to simplify enterprise application integration Implementing business processes and workflow in SOA environments Case studies in SOA deployment After you've deployed: delivering better collaboration, greater scalability, and more sophisticated applicationsThe IBM Press developerWorksÂ® Series is a unique undertaking in which print books and the Web are mutually supportive. The publications in this series are complemented by resources on the developerWorks Web site on ibm.com. Icons throughout the book alert the reader to these valuable resources.",oceanology,51,unknown
7654c04648178149dad73ae3b1a93d404e631774,to_check,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,openstack in action,https://www.semanticscholar.org/paper/7654c04648178149dad73ae3b1a93d404e631774,"In the cloud computing model, a cluster of physical computers hosts an environment that provides shared services (public and private) and offers the flexibility to easily add, remove, and expand virtual servers and applications. OpenStack is an open source framework that can be installed on individual physical servers to a cloud platform and enables the building of custom infrastructure (IaaS), platform (PaaS), and software (SaaS) services without the high cost and vendor lock-in associated with proprietary cloud platforms. OpenStack in Action offers real world use cases and step-by-step instructions to develop cloud platforms from inception to deployment. It explains the design of both the physical hardware cluster and the infrastructure services needed to create a custom cloud platform. It shows how to select and set up virtual and physical servers, implement software-defined networking, and the myriad other technical details required to design, deploy, and operate an OpenStack cloud in an enterprise. It also discusses the cloud operation techniques needed to establish security practices, access control, efficient scalability, and day-to-day DevOps practices. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications.",oceanology,52,unknown
497ed558a130464edf5ae4b974e35cb6b374a54d,to_check,semantic_scholar,,2017-01-01 00:00:00,semantic_scholar,an active learning environment to improve first-year mechanical engineering retention rates and software skills,https://www.semanticscholar.org/paper/497ed558a130464edf5ae4b974e35cb6b374a54d,"This work proposes a foundational change from traditional lecture to an active learning environment in the Colorado State University First-Year Introduction to Mechanical Engineering course of 145 students. The goal of this approach is to improve computational capabilities in Mechanical Engineering and long-term retention rates with a single broad emphasis. Major and minor changes were implemented in the course, from specific day to day in-class activities to the addition of laboratory sessions to replace traditional classroom lecture. These laboratories of no more than fifteen students were delivered by Learning Assistants, which were upper-level undergraduate peer educators. To evaluate proficiency, a MATLAB post-test was delivered to students who were instructed through lecture only (âLectureâ) and those who were instructed with the above changes (âActiveâ). A survey was also provided upon completion of the course to the Active group for student reflection on their perceived software capability and the usefulness of approaches. Post-test results suggest that the Active group was more proficient in MATLAB than the Lecture group. Survey results suggest that the Active group recognize they had not achieved expert use of the software but that they were likely to use it throughout their careers and that all approaches were useful, in particular the use of Learning Assistants. Future longterm retention statistics will shed light on the possible effectiveness of this approach, which are currently unavailable. Introduction Colorado State University has a total student enrollment in excess of 33,000. As a land grant university, the historic mission of the institution is to provide students with an education in practical fields such as agriculture and engineering. The College of Engineering has a growing student cohort, with an increase from ~450 first-year students Fall 2010 to ~600 students Fall 2015 [1]. However, persistence and graduation rates have remained fairly steady over the last fifteen years. The current six year persistence rate within the college is only ~45% and the six year graduation rate within the college is similar at ~43%. Many students do not remain within the college for even a full year, as the second fall persistence rate is only 70-75% [1]. These data show a significant portion of enrolled first-year engineering students do not remain within the program long enough to be exposed to foundational engineering content, which starts in the sophomore year with engineering specific courses. A current goal of the college is to improve these retention statistics. Additionally, many students do not develop the necessary software skills required to use computational tools such as MATLAB, which are integral to success in the curriculum. Students who do not develop these skills during introductory coursework must âcatch upâ in later courses, where the technical content is more challenging. We hypothesize this can lead to unpreparedness for challenging content or careers as an engineer and can negatively impact academic standing, leading to decreased retention. Thus, the goals of this work were to 1) improve retention rates for first-year engineering students, specifically mechanical engineering, and 2) improve computational and software skills of first-year students, specifically MATLAB and Microsoft Excel. MATLAB is a common computational package which can be used for a broad range of engineering problems throughout a curriculum [2]. However, learning Excel and MATLAB through lecture is challenging, as these tools are best understood through utilization, not observation [3]. MATLAB and other computational tools are often taught in classrooms with computational equipment, however this is can be a challenge with a large classroom [4]. Some have utilized computer based tutorials which students can complete on their own time [5], while others implemented a large scale deployment of personal computers equipped with MATLAB and other software [6]. Additionally, the use of peer-educators can be an effective approach to facilitating MATLAB development [7]. Thus, we have chosen to employ an approach which utilizes an active environment to learn MATLAB and other introductory content through the use of laboratory sessions and peer-educators, in this case the Learning Assistant model [8]. Similar to previous approaches, we have utilized classroom lectures, hands on in-class activities, and laboratory sessions [9]. The Introduction to Mechanical Engineering Course (MECH 103) was developed to provide students with an overview of the mechanical engineering discipline and as an introduction to the computational packages MATALB [10] and Microsoft Excel. The course consists of between 140 and 250 first-year students and was previously delivered using traditional lecture. While this approach was most efficient for a single instructor due to the enrollment size, this resulted in a static learning environment for a course which should excite students about mechanical engineering and provide foundational technical skills. The overall approach to this work was to thus create an active environment for students within the course, which had an enrollment of 145 students for the Fall 2016 semester. The rationale to this approach was that by providing students with hands-on experiences working with mechanical engineering problems and computational software, the understanding of course content will improve [11,12] whereby improving retention [13]. While some immediate test and survey data were acquired and are shown in this work, it is important to note that the true impact on retention is not currently recognizable and will require future analysis. In-Class Sessions Class sessions were varied throughout the semester and the week, as they typically included lectured course material, guest lectures or panels, and activities. The course met Monday, Wednesday, and Friday from 9-9:50 AM in a large lecture hall with individual stadium seating. Friday lecture was often cancelled and this time was spent in weekly laboratory sessions instead, which are outlined in the next section. Monday class time was assigned to covering course content through lecture, teamwork activities, and in class problems. The content of the course included general introductory material such as teamwork, communication, and design, commonly used units and unit conversions, mathematical models and systems, and an introduction to Microsoft Excel and MATLAB. Active engagement in the class included a teamwork design problem, requiring students to break into groups of three. Due to the theater seating layout of the classroom, groups of four or more made successful teamwork and communication difficult. Each group of students were provided one piece of 8.5â x 11â blank printer paper, one paperclip, and two pieces of scotch tape. The design problem was simple: build the tallest free standing structure possible using only the given materials. This was an inexpensive and simple approach to teamwork design activity. In place of a lecture or even a discussion on how to use design techniques for a simple problem such as this, students were able to actively engage in this process despite the difficulties of class size and layout. While students typically have an excellent understanding of units such as a pound (lb), their physical understanding of units such as a Joule or Watt are less developed within the context of everyday life. To provide students with a meaningful representation of energy (Joule) and power (Watt), they were provided a common object â in this case a softball â and asked to calculate how high they would have to raise the object to exert one Joule of energy â in this case roughly a foot and a half. While simple and inexpensive, this activity provided students with useful knowledge they can apply without a calculator and helps them relate coursework to the real world. For example, if they can place a Joule into real-world context, they could then answer the question âCan I launch a rocket into space using a thousand Joules?â. Wednesday lecture sessions were commonly used for guest lecturers and panels. These class sessions included the College of Engineering Dean, faculty members and graduate students in mechanical engineering, industry panelists, entrepreneurs and small business owners, and an interactive teamwork theatre troupe. The goal of these sessions was to provide students with a broad overview of different disciplines within mechanical engineering and what skills are necessary to succeed in various professional roles. While emphasizing an active learning environment is inherently difficult with each and every guest, student engagement was addressed by delivering variability in all of the presentations and strongly encouraging students to ask questions. For example, the theater troupe was an interactive experience where students were able to act as a team member within a group that mocked to show a diverse team struggling with communication. This session involved humor, discussion, and lively responses from students in place of a traditional static lecture. Laboratory Sessions In place of Friday lecture, students were asked to attend laboratory sessions for one hour [14,3]. A total of eleven sessions were provided throughout the week to accommodate all schedules. Sessions included one instructor, 13-16 students, and were held in laboratories with individual workstations with Microsoft Excel and MATLAB software. Laboratory instructors included a Graduate Teaching Fellow and Undergraduate Learning Assistants (LAs). Laboratory sessions involved a short (<5 minutes) lecture briefly reviewing content from class before students began working on assigned problems. These problems implemented course content such as the use of Excel or MATLAB to analyze and display data through real-world applications. An example of utilizing MATLAB to simulate rolling a die is p",oceanology,53,unknown
55b4107c8d37629d0378671324f56f9e801a6d4e,to_check,semantic_scholar,KDD,2015-01-01 00:00:00,semantic_scholar,efficient long-term degradation profiling in time series for complex physical systems,https://www.semanticscholar.org/paper/55b4107c8d37629d0378671324f56f9e801a6d4e,"The long term operation of physical systems inevitably leads to their wearing out, and may cause degradations in performance or the unexpected failure of the entire system. To reduce the possibility of such unanticipated failures, the system must be monitored for tell-tale symptoms of degradation that are suggestive of imminent failure. In this work, we introduce a novel time series analysis technique that allows the decomposition of the time series into trend and fluctuation components, providing the monitoring software with actionable information about the changes of the system's behavior over time. We analyze the underlying problem and formulate it to a Quadratic Programming (QP) problem that can be solved with existing QP-solvers. However, when the profiling resolution is high, as generally required by real-world applications, such a decomposition becomes intractable to general QP-solvers. To speed up the problem solving, we further transform the problem and present a novel QP formulation, Non-negative QP, for the problem and demonstrate a tractable solution that bypasses the use of slow general QP-solvers. We demonstrate our ideas on both synthetic and real datasets, showing that our method allows us to accurately extract the degradation phenomenon of time series. We further demonstrate the generality of our ideas by applying them beyond classic machine prognostics to problems in identifying the influence of news events on currency exchange rates and stock prices. We fully implement our profiling system and deploy it into several physical systems, such as chemical plants and nuclear power plants, and it greatly helps detect the degradation phenomenon, and diagnose the corresponding components.",oceanology,54,unknown
e1184cc1e4725f7736d9944a33ada01a626cedc3,to_check,semantic_scholar,,2006-01-01 00:00:00,semantic_scholar,learning in robotics,https://www.semanticscholar.org/paper/e1184cc1e4725f7736d9944a33ada01a626cedc3,"For a robot, the ability to get from one place to another is one of the most basic skills. However, locomotion on legged robots is a challenging multidimensional control problem. This paper presents a machine learning approach to legged locomotion, with all training done on the physical robots. The main contributions are a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithms for learning quadrupedal locomotion. The resulting learned walk is considerably faster than all previously reported hand-coded walks for the same robot platform. Introduction The ability to deploy a fully autonomous robot in an unstructured, dynamic environment (the proverbial real world) over an extended period of time remains an open challenge in the field of robotics. Considerable progress is being made towards many components of this task including physical agility, power management, and on-board sensor technology. One such component that has drawn considerable interest recently is the ability for a robot to autonomously learn to improve its own performance (Ng et al. 2004; Bagnell & Schneider 2001; Zhang & Vadakkepat 2003). Despite this interest, considerable work remains due to the di fficulties associated with machine learning in the real world . Compared to other machine learning scenarios such as classification or action learning in simulation, learning o n physical robots presents several formidable challenges, i ncluding the following. Sparse Training Data: It is often prohibitively difficult to generate large amounts of data due to the maintenance required on robots, such as battery changes, hardware repairs, and, usually, constant human supervision. Thus, learning methods designed for physical robots must be effective with small amounts of data. Dynamical Complexity: The dynamics of many robotic control tasks are too complex for faithful simulation to be possible. Furthermore, robots are inherently situated in an unstructured environment with unpredictable sensor and actuator noise, namely the real world. Thus, even when off-line simulation is possible, it can never be fully reflective of the target environment. In this paper, we overcome these challenges for one concrete complex robot task, namely legged locomotion. Using a commercially available quadruped robot, we fully automate the training process (other than battery changes) and Copyright c Â© 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. employ machine learning algorithms that are sufficiently data efficient to enable productive learning on physical robots in a matter of hours. The resulting learned walk is considerably faster than all previously reported hand-cod ed walks for the same robot platform. This paper contributes both a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithms for learning quadrupedal locomotion. The remainder of the paper is organized as follows. First, we introduce the parameterized walk which our learning process seeks to optimize. We then specify our four learning approaches, and follow with detailed empirical results. We close with a discussion of their implications and possible avenues for future work. A Parameterized Walk The Sony Aibo ERS-210A is a commercially available robot that is equipped with a color CMOS camera and an optional ethernet card that can be used for wireless communication. The Aibo is a quadruped robot, and has three degrees of freedom in each of its four legs (Sony 2004). At the lowest level, the Aiboâs gait is determined by a series of joint positions for the three joints in each of its leg s. An early attempt to develop a gait by Hornby et al. (1999) involved using a genetic algorithm to learn a set of lowlevel parameters that described joint velocities and body p osition.1 More recent attempts to develop gaits for the Aibo have involved adopting a higher-level representation that deals with the trajectories of the Aiboâs four feet through three-dimensional space. An inverse kinematics calculati on is then used to convert these trajectories into joint angles . Among higher-level approaches, most of the differences between gaits that have been developed for the Aibo stem from the shape of the loci through which the feet pass and the exact parameterizations of those loci. For example, a team from the University of New South Wales achieved the fastest known hand-tuned gait using the high-level approach described above with trapezoidal loci. They subsequently generated an even faster walk via learning (Kim & Uther 2003). A team from Germany created a flexible gait implementation that allows them to use a variety of different shapes of loci (Rofer et al. 2003), and the team from the University of Newcastle was able to generate highvelocity gaits using a genetic algorithm and loci of arbitra ry shape (Quinlan, Chalup, & Middleton 2003). Our team (UT Austin Villa, Stone t al. 2004) first approached the gait optimization problem by hand-tuning Developed on an earlier version of the Aibo. a gait described by half-elliptical loci. This gait performed comparably to those of other teams participating in RoboCup 2003. The work reported in this paper uses the hand-tuned UT Austin Villa walk as a starting point for learning. Figure 1 compares the reported speeds of the gaits mentioned above, both hand-tuned and learned, including that of our starting point, the UT Austin Villa walk. The latter walk is described fully in a team technical report (Stone et al. 2004). The remainder of this section describes those details of the UT Austin Villa walk that are important to understand for the purposes of this paper. Hand-tuned gaits Learned gaits CMU Austin Villa UNSW Hornby UNSW NUBots (2002) (2003) (2003) (1999) (2003) (2003) 200 245 254 170 270 296 Figure 1: Maximum forward velocities of the best gaits (in mm/s) for different teams, both learned and hand-tuned. The half-elliptical locus used by our team is shown in Figure 2. By instructing each foot to move through a locus of this shape, with each pair of diagonally opposite legs in phase with each other and perfectly out of phase with the other two (a gait known as a trot), we enable the Aibo to walk. Four parameters define this elliptical locus: 1. The length of the ellipse; 2. The height of the ellipse; 3. The position of the ellipse on the x axis; and 4. The position of the ellipse on the y axis. Since the Aibo is roughly symz",oceanology,55,unknown
6e7e52c8f59ec975cb9b850cef1ecf8470b9c28a,to_check,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,physical hardware-in-the-loop modelling and simulation,https://www.semanticscholar.org/paper/6e7e52c8f59ec975cb9b850cef1ecf8470b9c28a,"It is too risky to install a newly-designed device, component, or controller, directly into a real power system without rigorous testing. To help to de-risk the system integration, and to assist in the design process, computer simulation is an accepted and widely-adopted tool. However, in a simulation-only environment, many real-world issues such as noise, randomness of event timings, and hardware design issues are not well explored. In addition, there are limits on the size and fidelity of system which can be simulated, due to the required computational intensity, and because control systems for devices often contain software which is proprietary and cannot be modelled accurately. Physical Hardware in the Loop Simulation provides an interim stage between purely computer-based simulation, and real device deployment. Part of the power system (or âSmart Gridâ) is simulated, but specific components are implemented in actual hardware. The hardware may consist of instrumentation, relays or controllers, carrying no primary current. Such testing is termed âSecondary Hardware-in-the-Loopâ, as the signals exchanged between the simulation and hardware consist only of measurements and control values. A more advanced environment is created where primary power flow is exchanged with the hardware. This is termed âPrimary Hardware-in-the-Loopâ or âPower Hardware-in-the-Loopâ testing. In addition to measurement and control signals being exchanged with the simulation, an interface is required at which primary power is exchanged between the simulation and the hardware, at the voltage and current levels suitable for the hardware under test. Creation of such environments is complex, but allows steady-state, dynamic, and worst-case scenarios to be re-created in a controlled environment. Therefore hardware-in-the-loop testing offers a cheaper, safer, faster and more comprehensive de-risking process than trying the hardware for the first time on a real network. The complexity and interconnected nature of the Smart Grid means that such Hardware in the Loop based testing is becoming even more critical to understanding the behaviour of systems and schemes, and consequently the safe and secure introduction of new technologies.",oceanology,56,unknown
88aa14a159f0fad0a2b07445c3f091558ffbda62,to_check,semantic_scholar,FHPC '14,2014-01-01 00:00:00,semantic_scholar,ziria: wireless programming for hardware dummies,https://www.semanticscholar.org/paper/88aa14a159f0fad0a2b07445c3f091558ffbda62,"Software-defined radio (SDR) brings the flexibility of software to the domain of wireless protocol design, promising both an ideal platform for research and innovation and the rapid deployment of new protocols on existing hardware. Most existing SDR platforms require careful hand-tuning of low-level code to be useful in the real world. In this talk I will describe Ziria, an SDR platform that is both easily programmable and performant. Ziria introduces a programming model that builds on ideas from functional programming and that is tailored to wireless physical layer tasks. The model captures the inherent and important distinction between data and control paths in this domain. I will describe the programming model, give an overview of the execution model, compiler optimizations, and current work. We have used Ziria to produce an implementation of 802.11a/g and a partial implementation of LTE.",oceanology,57,not included
16af2f3a6d1f07c46bd851aa2899731136fab73e,to_check,semantic_scholar,MobiCom,2014-01-01 00:00:00,semantic_scholar,poster: ziria: language for rapid prototyping of wireless phy,https://www.semanticscholar.org/paper/16af2f3a6d1f07c46bd851aa2899731136fab73e,"Software-defined radio (SDR) brings the flexibility of software to the domain of wireless protocol design, promising an ideal platform both for research and innovation and rapid deployment of new protocols on existing hardware. However, existing SDR programming platforms require either careful hand-tuning of low-level code, negating many of the advantages of software, or are too slow to be useful in the real world. We present Ziria, the first software-defined radio programming platform that is both easily programmable and performant. Ziria introduces a novel programming model tailored to wireless physical layer tasks and captures the inherent and important distinction between data and control paths in this domain. Ziria provides the capability of implementing a real-time WiFi PHY running at 20 MHz.",oceanology,58,unknown
949b01c64ba61c94ba0982ffd6abc50658d53874,to_check,semantic_scholar,SRIF@SIGCOMM,2014-01-01 00:00:00,semantic_scholar,"demo: 802.11 a/g phy implementation in ziria, domain-specific language for wireless programming",https://www.semanticscholar.org/paper/949b01c64ba61c94ba0982ffd6abc50658d53874,"Software-defined radio (SDR) brings the flexibility of software to the domain of wireless protocol design, promising an ideal platform both for research and innovation and the rapid deployment of new protocols on existing hardware. However, existing SDR programming platforms require either careful hand-tuning of low-level code, negating many of the advantages of software, or are too slow to be useful in the real world. In this demo we present Ziria, the first software-defined radio programming platform that is both easily programmable and performant. Ziria introduces a novel programming model tailored to wireless physical layer tasks and captures the inherent and important distinction between data and control paths in this domain. We show the capabilities of Ziria by demonstrating a real-time implementation of WiFi PHY running at 20 MHz.",oceanology,59,unknown
74fab23e3fd31db77d22074ede9de7e8c8a40c38,to_check,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,improving vehicular networking reliability and efficiency in the context of platooning applications,https://www.semanticscholar.org/paper/74fab23e3fd31db77d22074ede9de7e8c8a40c38,"Vehicular networking is a technology that enables vehicles communication system. A joint effort from the automobile industry, transportation industry, and government offices is driving the adoption of this technology to build intelligent transportation systems that consist of smart vehicles. This study attempts to improve the reliability and efficiency of vehicular networking. The study assumes the context of platooning applications, but the contributions of this study can be applied to other vehicular applications as well. There are two contributions in this study. First, a wireless emulator is designed and implemented to emulate IEEE 802.11 networks in real-time using the Ethernet infrastructure. The emulator replaces the MAC layer and physical layer of IEEE 802.11 networking stack with a real-time CSMA/CA model, thus reduces the cost of experiments. It provides upper layers the same interfaces as on a real device. As a result, the testing targets in the emulation are real-world software components as opposed to simulation scripts in a discrete event simulator. These software components can be routing protocols, transport protocols, or applications, and are the same code that can be deployed in the real world. Second, an Interframe Compression Transmission Layer is designed and implemented, to provide efficient transmission of periodical messages in vehicular environments. The transmission layer compresses the difference between frames instead of frames themselves, and reduces bandwidth consumption significantly. To improve the behaviors of the transmission layer under different scenarios and configurations studied, an adaptive version of the algorithm is designed, which achieves more than 50% in reduction of bandwidth consumption using real-world platooning data trace. With lower bandwidth consumption, delivery ratio is vastly improved in congested networking environments.",oceanology,60,unknown
6220b68cd721512098b9b14851afe5e660c0e565,to_check,semantic_scholar,,2015-01-01 00:00:00,semantic_scholar,crypto-day campeon a8,https://www.semanticscholar.org/paper/6220b68cd721512098b9b14851afe5e660c0e565,"Using the properties of a wireless channel is an alternative approach for securing the channel besides pre-shared keys or asymmetric cryptography. Numerous experiments have recently demonstrated that channel-based key establishment (CBKE) is a promising alternative to well-known symmetric/asymmetric approaches. Their run-times for establishing a symmetric key suggest that such methods are highly suitable for real-world applications that operate in a dynamic mobile environment with peer-to-peer association. CBKE is a new paradigm for generating shared secret keys. The approach is based on the estimation of the wireless transmission channel by both the sender and receiver, where the shared secret key is derived from channel parameters. The commonness of the randomness of the secret key relies on the principle of channel reciprocity. Specifically, this means that the channel from Alice to Bob is the same than the channel from Bob to Alice. This symmetry of practical channels is usually sufficiently high, as well as its entropy of spatial, temporal, and spectral characteristics. Security is given if an attackerâs distance to the two communicating nodes is high enough, so that the observed channel parameters to each node are uncorrelated and independent from each other. Typically, in real environments this is given if the distance is greater than about half of the carrier wavelength. For instance, for the frequency used in 2.4 GHz WiFi, this translates to a distance of 6.25 cm. So far, high usability and dynamic key management are very difficult to achieve for wireless devices, which operate under strict resource constraints. CBKE has the potential to significantly reduce the cost of securing small embedded devices, and hence make mass production and deployment more viable. Until now, no research has addressed the requirements for performance evaluation of real-world implementations of CBKE systems. We present a wireless CBKE security system built with standard components, e.g., quantization scheme and error correction codes, presented in recent publications. We introduce necessary implementation properties and requirements of CBKE systems. In order to validate the performance of the key generation algorithms, we define a set of metrics. Finally, we describe an end-to-end implementation on an ARM-Cortex M3 microcontroller to demonstrate the practical feasibility of channel-based key estimation using current embedded hardware. Comparative analysis of pseudorandom generators Aleksei Burlakov, Johannes vom Dorp, Joachim von zur Gathen, Sarah Hillmann, Michael Link, Daniel Loebenberger, Jan LÃ¼hr, Simon Schneider & Sven Zemanek {burlakov,dorp,luehr,schneid,zemanek}@cs.uni-bonn.de {sarah.hillmann,michael.link}@uni-bonn.de {gathen,daniel}@bit.uni-bonn.de Bonn-Aachen International Center for Information Technology Dahlmannstr. 2, Bonn We compare random generators (RGs) under controlled conditions regarding their efficiency and statistical properties. For this purpose, we distinguish between physical RGs and software RGs, which can be further subdivided into cryptographically secure and insecure RGs. Physical RGs covered by our study are the hardware generator PRG310-4 and /dev/random as implemented in the Linux kernel. Since /dev/random is fed by system events, we analyze both an idle lab environment and a server hosting several virtual machines. As examples for cryptographically secure RGs our analysis compares the RSA generator and the Blum-Blum-Shub generator, both for 3000-bit moduli. Additionally, we compare them to the Nisan-Wigderson construction with suitably selected parameters. We include two cryptographically insecure RGs, namely a linear congruential generator (LCG) and the Littlewood generator. In order to obtain repeatable and comparable results, our implementations of the software RGs were all run on the same machine and produced 512 kB of output each, using AES post-processed output of the generator PRG310-4 as source for random seed bits. We compare the results in terms of byte entropy and throughput excluding initialization. For further statistical analysis â not shown in the table â we apply the NIST test suite on the outputs. The most important finding is that in our scenarios, number-theoretic generators compete very well against hardware-based ones. byte entropy runtime throughput throughput [bit] [Î¼s] [kB/s] normalized PRG310-4, no post-processing 7.99963 16308400 31.39486 4.34492 AES post-processing 7.99963 36524300 14.01806 1.94004 /dev/random, in the field 7.99979 9.169Ã 10 5.584Ã 10â3 7.728Ã 10â4 in the lab 7.99948 2.671Ã 10 1.917Ã 10â4 2.653Ã 10â5 Littlewood 6.47244 15206550 33.66970 4.61011 Linear congruential generator 7.99969 2644039 193.64313 26.51392 Blum-Blum-Shub 7.99962 17708350 28.91291 3.95880 RSA, e = 2 + 1, 1400 bit/round 7.99966 267604 1913.27484 261.96857 e = 3, 1 bit/round 7.99963 70103838 7.30345 1 Nisan-Wigderson 7.99961 2731227 187.46153 25.66753 Table 1: Overview of the results for generating 512 kB of output.",oceanology,61,unknown
08ae139d6890717bea0e6243549d66caf24fa78e,to_check,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,teaching embedded systems in a mooc format,https://www.semanticscholar.org/paper/08ae139d6890717bea0e6243549d66caf24fa78e,"We have designed and implemented a Massive Open Online Class (MOOC) with a substantial lab component within the edX platform. We deployed this MOOC three times with a total enrollment of over 100,000 students. If MOOCs are truly going to transform engineering education, then they must be able to deliver classes with laboratory components. Our offering goes a long way in unraveling the perceived complexities in delivering a laboratory experience to thousands of students from around the globe. We believe the techniques developed in this class will significantly transform the MOOC environment. Effective education requires students to learn by doing. In the traditional academic setting this active learning is achieved through a lab component. Translating this to the online environment is a non-trivial task that required several important factors to come together. First, we have significant support from industrial partners ARM Inc. [1] and Texas Instruments [2]. Second, the massive growth of embedded microcontrollers has made the availability of lost-cost development platforms feasible. Third, we have assembled a team with the passion, patience, and experience of delivering quality lab experiences to large classes. Fourth, online tools now exist that allow students to interact and support each other. We used edX for the delivery of videos, interactive animations, text, and quizzes [3]. We used Piazza [4] for discussion boards and Zyante [5] for a programming reference. We partnered with element-14 [6], Digi-Key [7], and Mouser [8] to make the lab kit available and low-cost. Even though there was a $40-$70 cost to purchase the lab kit, the course completion numbers were slightly better than a typical MOOC. 7.3% of the students completed enough of the class to receive a certificate. Students completing end of the course surveys report a 95% overall satisfaction. Demographics show a world-wide reach with India, US, and Egypt being the countries with the most students. In this paper we will present best practices, successes and limitations of teaching a substantial lab across the globe. Background An embedded system combines mechanical, electrical, and chemical components along with a computer, hidden inside, to serve a single dedicated purpose [9-11]. There are over 50 billion processors based on the ARM architecture delivered into products, and most of these computers are single-chip microcontrollers that are the brains of an embedded system. Embedded systems are a ubiquitous component of our everyday lives. We interact with hundreds of tiny computers every day that are embedded into our houses, our cars, our bridges, our toys, and our work. As our world has become more complex, so have the capabilities of the microcontrollers embedded into our devices. Therefore the world needs a trained workforce to develop and manage products based on embedded microcontrollers. Review Other online classes have delivered laboratory experiences. Hesselink at Stanford University developed iLabs as a means to deliver science experiments to online learning. Their lab-in-a-box involves simulations and animations [12]. OâMalley et al. from the University of Manchester developed a Chemistry MOOC with a lab component using virtual labs and simulations [13-14]. University of Washington presented a hardware/software MOOC on Coursera [15]. This course is primarily a programming class without graded physical labs. Ferri et al. from Georgia Institute of Technology created a MOOC for linear circuits [16]. This class had activities to perform with NIâs myDAC, but graded lab circuits were not part of the online experience. Connor, and Huettel at Duke created a Virtual Community of Practice for electric circuits [17]. Cherner et al. created a virtual multifunctional X-Ray diffractometer for teaching science and engineering [18]. Saterbak et al. at Rice University developed online materials to teach freshman design, with the goal to free-up class time for more interactive learning experiences [19]. Harris from University of California at Irvine has a six-course sequence on Introduction to the Internet of Things and Embedded Systems where students build actual embedded devices [20]. Grading for this course uses peer assessment. Lee et al. at Berkeley developed an introduction to embedded systems MOOC with laboratory exercises. The lab itself was a robotic controller in a virtual laboratory environment. Completion of the labs themselves does have an automatic grading component based on the studentâs written software [21-22]. All this work emphasizes the need for hands on learning. Pedagogy The overall educational objective of this class is to allow students to discover how computers interact with the environment. The class provides hands-on experiences of how an embedded system could be used to solve problems. The focus of this introductory course is understanding and analysis rather than design, where students learn new techniques by doing them. We feel we have solved the dilemma in learning a laboratory-based topic like embedded systems, where there is a tremendous volume of details that first must be learned before hardware and software systems can be designed. The approach taken in this course is to learn by doing in a bottom-up fashion. One of the advantages of a bottom-up approach to learning is that the student begins by mastering simple concepts. Once the student truly understands simple concepts, he or she can embark on the creative process of design, which involves putting the pieces together to create a more complex system. True creativity involves solving complex problems using effective combinations of simple components. Embedded systems afford an effective platform to teach new engineers how to program for three reasons. First, there is no operating system. Thus, in a bottom-up fashion the student can see, write, and understand all software running on a system that actually does something. Second, embedded systems involve real input/output that is easy for the student to touch, hear, and see. Many engineering students struggle with abstraction. We believe many students learn effectively by using their sense of touch, hearing and sight to first understand and internalize difficult concepts, and then they will be able to develop and appreciate abstractions. Third, embedded systems are employed in many everyday products, motivating students to see firsthand, how engineering processes can be applied in the real world. This course is intended for beginning college students with some knowledge of electricity as would have been taught in an introductory college physics class. Secondly, it is expected students will have some basic knowledge of programming and logic design. No specific language will be assumed as prior knowledge but this class could be taken as their second programming class. We hoped experienced engineers could also use this course to train or retrain in the field of embedded systems. Learning objectives of the course Although the students are engaged with a fun and rewarding lab experience, our educational pedagogy is centered on fundamental learning objectives. After the successful conclusion of this class, students should be able to understand the basic components of a computer, write C language programs that perform input/output interfacing, implement simple data structures, manipulate numbers in multiple formats, and understand how software uses global memory to store permanent information and the stack to store temporary information. Our goal is for students to learn these concepts: 0) How the computer stores and manipulates data; 1) Embedded systems using modular design and abstraction; 2) Design tools like requirements documents, data flow graphs, and call graphs; 3) C programming: considering both function and style; 4) Debugging and verification using a simulator and the real microcontroller; 5) Debugging tools like voltmeters, oscilloscopes, and logic analyzers; 6) How to input/output using switches, LEDs, DACs, ADCs, and serial ports; 7) Implementation of an I/O driver, multithreaded programming, and interrupts; 8) Analog to digital conversion (ADC), periodic sampling, and the Nyquist Theorem; 9) Stepper motors, brushed DC motors, and simple digital controllers; 10) Digital to analog conversion (DAC), used to make simple sounds; 11) Simple distributed systems that connect two microcontrollers; 12) Internet of things, connecting the embedded system to the internet; 13) System-level design that combine multiple components together. Laboratory Kit Active learning requires a platform for the student to learn by doing. Figure 1 shows the components of the basic lab kit. There are two difficulties with a physical lab kit deployed in a world-wide open classroom environment. The first problem is availability of components. We partnered with companies and distributors six months in advance of the course launch to guarantee availability. The companies wanted us to specify the number of students who would buy the kit. In this regard, we were very lucky. Six months prior to our first launch, we estimated 2000 people would register for the class and 1000 would buy the kit. In turns out Texas Instruments produced 10,000 microcontroller boards just in case. Much to our surprise 40,000 people registered and we estimate 11,000 purchased the kit during this first delivery of the course. The second solution to the problem of availability was to have three world-wide distributors (element-14, Mouser, and Digi-Key). Working with these distributors, we created one-click landing pages for students to buy the kit. Furthermore, for each component in the kit (other than the microcontroller board), we had three or more possible parts. The third solution was to design the course with flexible deadlines and pathways. Each lab had a simulation and a real-board requirement. Students who were waiting for the parts to be shipped could proceed with ",oceanology,62,unknown
3f826f1f349ee707639c39d231259498b14215c5,to_check,semantic_scholar,,2011-01-01 00:00:00,semantic_scholar,ac 2011-2689: smart grid development in electrical dis- tribution network,https://www.semanticscholar.org/paper/3f826f1f349ee707639c39d231259498b14215c5,"This paper will focus on smart grid project design and implementation. The project was developed by students and demonstrates new ideas and teamwork. This project was successfully completed and has been developed, implemented and assessed. Topics covered are: how to build a smart gird by utilizing computer application software tools, design, simulation, and diagnoses of electrical distribution systems. All the real world components in electrical distribution network such as residential, commercial and industrial building are modeled in this project. Background The purpose of this project is to design and implement a small scale electric power network by a team of seven students, supervised by a faculty member. The studentsâ background is in electrical engineering with emphasis in electric power system. The students conducted a study in the field of Smart Grid technologies for history and background information. This work led to designing and implementing a small model of a smart gird power distribution network. The power grid represents the real world aspirations of both government and private industry geared toward building a more reliable, responsive, and overall efficient network of residential, industrial and commercial buildings. Since the concept of a smart grid is very vague, students chose to implement a time tested and proven aspect of such technology known as smart meters. The smart meter is a wireless device connected to every house, industrial and commercial buildings to provide essential feedback in real time to the power companies. This feedback could be in the form of a fault occurring at that said location, or illegal energy usage. This feedback in real time is very useful to the power companies, given the fact that most rely on feedback via a phone call from the consumer before they know whether or not there is a fault in the system. Another purpose of implementing the grid was to simulate metering technology at the residential, industrial and commercial level. These meters would send data to a computer which is a simulated control room in order to read where certain faults occur in the system. In turn one could control which areas of the grid would be supplying the power. This represents a simulation of the power companyâs ability to read and send vital information throughout the grid, thus improving the responsiveness and reliability of the network. Figure 1 illustrates the completed model after it was built and during testing. The lifecycle of this project was implemented in three different phases and started in September of 2009 and it was completed in May of 2010. Planning and analysis was completed in phase I, design and implementation in phase II, and documentation and studentsâ assessment in phase III. Figure 1. A model of smart grid in electrical distribution system Phase I: Planning and Analysis Initially, each team member worked on individual research on the concepts of smart grid its purpose. Later on, a decision was made as to what the team wanted to demonstrate with the project. The decision was made to show specifically how smart meters would work and help in fault detection as well as saving money by removing the need for meter readers to read the power meters every month. A project leader was elected by the team members to coordinate the team work. Meetings were then set up by the project leader, to brainstorm on how the actual implementation was going to be planned. Microsoft Project software was very instrumental to organize the work of the team. Tasks were assigned with specific due dates to keep the project on schedule and under budget. Requirements Since this project was spread over three quarters, students had many deadlines and task that had to be met in order to have a successful project. There were three phases to this project, research, background study and planning during the 1 quarter, design and building during the 2 quarter and the final stage of testing and troubleshooting during the 3 quarter along with final oral presentation, simulation documentation and assessment of the project. The students made documents and recorded each steps of the project down to each task and timeline by using Microsoft Project software. The project advisor coordinated the project steps and students were required to present a weekly progress report. This step insured that the project was moving smooth and on the track. The group was divided into two teams, one in software teams which consisted of two members and a hardware team which consisted of other five group members. The software team was in charge of all the coding and GUI implementation so the actual grid can communicate back and forth with the computer. The hardware team was in charge of the physical grid which consisted of the circuit that was built using logic chips such as MUXs, and Flip-Flops, wiring, creating a map on the grid with houses, roads, school, power stations, sub-stations, transmission lines, and distributions lines. The commercial site consists of shopping area, factories, stadium, school and so forth. In final stages of the project, testing, debugging and troubleshooting was performed in order to assure that hardware components and related software can communicate back and forth in a proper sequence. Much of the requirements had the made along the way since this was very new to all students. Phase II: Design and Implementation The design started immediately after the clear definition of the project requirement and purpose. To lower the cost and improve the safety, the design would be a DC (Direct Current) representation of an AC (Alternate Current) system. The system was designed by drawing out the model of a city and the specific buildings to exist in that city. The design was based on what took place in the planning stage which defined how the city and buildings will receive their power and the power. Figure 2. The process of building a smart grid The next challenge in the design process was solving the problem of switches and smart meters. Figure 2 shows the design of the smart meters and placement of LEDs (Light Emitting Diode). The LEDs will represent whether a particular house, building or transformer has power on or off. If for any reason an LED was not lit, then that particular item does not have power. The faults were determined by voltages because even if the building wasnât drawing power, then there still would be a voltage on the line. This voltage was then sent to a 64 to 1 multiplexor which was then sent to the microcontroller to determine faults. To turn the power of buildings âonâ and âoffâ a common NPN transistor (2n2222) and the base current was provided by a flip flop integrated circuit. Flip flops were used due to I/Oâs limitations of the PIC. Figure 3 was duplicated for every transformer, with the only difference being the number of buildings being fed from the transformer which is the first LED after the 12V source. Figure 3. Circuit diagram for buildings Implementation A Smart Grid system includes a power meter which enables the communication systems to update the utility about its condition and the electronics to control the meter. The old electromechanical meters that were used are becoming obsolete since they cannot support the features that the utilities desire to have such as monitoring and controlling power supplied to its customers. Utilities wishes to monitor power consumption so that they can accurately predict how much power will be used during peak and down times. This information is helpful in producing sufficient energy and better efficiency in power waste. It can also help to pinpoint locations of power outages leading to a quicker recovery time. Challenges in implementation of the system are based on a couple of issues. First is the cost. It could cost upwards of $1000 for each smart meter, depending on features to be installed for each house or business. The costs can add up quickly, and the utilities don't see any immediate savings or incentive to deploy the smart grid in a very near future. The system designed in this project is using smart meters with a simulated wireless connection to the central servers at the utilities. The meters would send a signal to the central computer to update its status, power consumption, and other things. It can be designed in a way that it will have a battery backup for when the power is interrupted, or have the central computer assume it is off when it doesn't send a signal at the regular time intervals. Obviously the latter option would be the most cost effective and would use less power to run. But having power to the smart meter could also be beneficial because diagnostics could be run to determine if the power went out or if the meter is having its own internal hardware problems. A wireless signal was simulated for this project, but in real world application one can use either wireless, normal phone lines, or communications over power line. Most utilities already have communications systems set up through their power lines and using this method would be most cost effective. Having wireless, on the other hand, frees up usage of the power lines reducing their stress and prolonging the cables life. Companies are developing and testing their own systems using one of those options. In any case, it is based on hardware availability and cost effectiveness. Figure 4 illustrates communication with the smart grid. Figure 4. Communication with the smart grid Phase III: Documentation and Studentsâ Assessment In phase III of the project, the students provided a detail documentation of the project which includes cost analysis and different phases of the design. An electronic copy of this documentation and demo presentation was produced in a DVD. The following assessment and lessons learned was observed during the life cycle of the project: 1) When the main board that was used in the final project was constructed, the problems of wiring of all o",oceanology,63,not included
a2ca591957d1081bbf4b1a04c565b8f365c014d8,to_check,semantic_scholar,,2017-01-01 00:00:00,semantic_scholar,literature survey: a design approach to smart system based on internet of thing (iot) for intelligent transportation,https://www.semanticscholar.org/paper/a2ca591957d1081bbf4b1a04c565b8f365c014d8,"Recent years, the transportation efficiency and related issues have become one of the main focuses of the global world. Along this line, intelligent transportation systems (ITS) based on Internet of Things (IoT) provided a promising chance to resolve the challenges caused by the increasing transportation problems, such as traffic prediction, road status evaluation, traffic accident detection, etc. In this, The Internet of Things is based on the Internet, network wireless sensing and detection technologies to realize the intelligent recognition on the tagged traffic object, tracking, monitoring, managing and processed automatically. IoT based intelligent transportation systems are designed to support the Smart City vision, which aims at employing the advanced and powerful communication technologies for the administration of the city and the citizens. KeywordsâIoT, transportation. I. LITERATURE SURVEY K.Ashokkumar, Baron Sam, R.Arshadprabhu, Britto [1] proposes the advances in cloud computing and web of things (IoT) have provided a promising chance to resolve the challenges caused by the increasing transportation problems. They tend to gift a unique multilayered conveyance knowledge cloud platform by exploitation cloud computing and IoT technologies to resolve the challenges caused by the increasing transportation issues. They present a novel multilayered vehicular data cloud platform by using cloud computing and IoT technologies. Two innovative vehicular data cloud services, an intelligent parking cloud service and a vehicular data mining cloud service in the IoT environment are also presented reviews. Amir-Mohammad Rahmani, Nanda Kumar Thanigaivelan, Tuan Nguyen Gia, Jose Granados, Behailu Negash, Pasi Liljeberg, and Hannu Tenhunen [2] proposes the strategic position of gateways to offer several higherlevel services such as local storage, real-time local data processing, embedded data mining, etc., proposing thus a Smart e-Health Gateway. By taking responsibility for handling some burdens of the sensor network and a remote healthcare center, a Smart e-Health Gateway can cope with many challenges in ubiquitous healthcare systems such as energy efficiency, scalability, and reliability issues. Michele Nitti, Luigi Atzori, and Irena Pletikosa Cvijikj [3] addressed the issue by analyzing possible strategies for the benefit of overall network navigability.They first propose five heuristics, which are based on local network properties and that are expected to have an impact on the overall network structure. Thet then perform extensive experiments, which are intended to analyze the performance in terms of giant components, average degree of connections, local clustering, and average path length. Jianli Pan, Raj Jain, Subharthi Paul, Tam Vu, Abusayeed Saifullah, Mo Sha [4] proposes an IoT framework with smart location-based automated and networked energy control, which uses smartphone platform and cloud-computing technologies to enable multiscale energy proportionality including building-, user-, and organizational-level energy proportionality. They further build a proof-of-concept IoT network and control system prototype and carried out real-world experiments, which demonstrate the effectiveness of the proposed solution. They envision that the broad application of the proposed solution has not only led to significant economic benefits in term of energy saving, improving home/office network intelligence, but also bought in a huge social implication in terms of global sustainability Catarinucci, L. , de Donno, D. , Mainetti, L. , Palano, L. [5] proposes a novel, IoT-aware, smart architecture for automatic monitoring and tracking of patients, personnel, and biomedical devices within hospitals and nursing institutes. Staying true to the IoT vision, they propose a smart hospital system (SHS), which relies on different, yet complementary, technologies, specifically RFID, WSN, and smart mobile, interoperating with each other through a Constrained Application Protocol (CoAP)/IPv6 over lowpower wireless personal area network (6LoWPAN)/representational state transfer (REST) network International Conference on Science and Engineering for Sustainable Development (ICSESD-2017) (www.jit.org.in) International Journal of Advanced Engineering, Management and Science (IJAEMS) Special Issue-1 https://dx.doi.org/10.24001/icsesd2017.49 ISSN : 2454-1311 www.ijaems.com Page | 195 infrastructure. The SHS is able to collect, in real time, both environmental conditions and patients' physiological parameters via an ultra-low-power hybrid sensing network (HSN) composed of 6LoWPAN nodes integrating UHF RFID functionalities. Sensed data are delivered to a control center where an advanced monitoring application (MA) makes them easily accessible by both local and remote users via a REST web service. Al-Fuqaha, A., Kalamazoo, MI, Guizani, M. , Mohammadi, M., Aledhari, M. [6] provides a more thorough summary of the most relevant protocols and application issues to enable researchers and application developers to get up to speed quickly on how the different protocols fit together to deliver desired functionalities without having to go through RFCs and the standards specifications. They also provides an overview of some of the key IoT challenges presented in the recent literature and provide a summary of related research work. Moreover, they explore the relation between the IoT and other emerging technologies including big data analytics and cloud and fog computing. They also presents the need for better horizontal integration among IoT services. Stecca, M., Moiso, C., Fornasa, M., Baglietto, P. [7] presents app execution platform (AEP), a platform that supports the design, deployment, execution, and management of IoT applications in the domain of smart home, smart car, and smart city. AEP was designed to coherently fulfill a set of requirements covered only partially or in a fragmented way by other IoT application platforms. AEP focuses on SO virtualization and on composite application (CA) orchestration and supports dynamic object availability. Yi-Bing Lin, Yun-Wei Lin, Chang-Yen Chih, Tzu-Yi Li [8] proposes an IoT device which is characterized by its âfeaturesâ (e.g., temperature, vibration, and display) that are manipulated by the network applications. If a network application handles the individual device features independently, then we can write a software module for each device feature, and the network application can be simply constructed by including these brick-like device feature modules. Based on the concept of device feature, brick-like software modules can provide simple and efficient mechanism to develop IoT device applications and interactions. Ganz, F. , Puschmann, D. , Barnaghi, P. , Carrez, F. [9] provides a survey of the requirements and solutions and describes challenges in the area of information abstraction and presents an efficient workflow to extract meaningful information from raw sensor data based on the current stateof-the-art in this area and also identifies research directions at the edge of information abstraction for sensor data. To ease the understanding of the abstraction workflow process, they introduce a software toolkit that implements the introduced techniques and motivates to apply them on various data sets. Aijaz, A. , Aghvami, A.H.[10] provides the state of the art in cognitive M2M communications from a protocol stack perspective, covers the emerging standardization efforts and the latest developments on protocols for cognitive M2M networks which includes a centralized cognitive medium access control (MAC) protocol, a distributed cognitive MAC protocol, and a specially designed routing protocol for cognitive M2M networks. These protocols explicitly account for the peculiarities of cognitive radio environments. Performance evaluation demonstrates that the proposed protocols not only ensure protection to the primary users (PUs) but also fulfil the utility requirements of the secondary M2M networks. Tsirmpas, C., Anastasiou, A., Bountris, P., Koutsouris, D. [11] proposes a new methodology based on self organizing maps (SOMs) and fuzzy C-means (FCM) algorithms for profile generation as regards the activities of the user and their correlation with the available sensors. Moreover, we utilize the provided context to assign the generated profiles to more contextually complex activities. This methodology is being evaluated into an AAL structure equipped with several sensors. More precisely, they assess the proposed method in a data set generated by accelerometers and its performance over a number of everyday activities Mainetti, L., Lecce, Mighali, V. ; Patrono, L. [12] proposes a software architecture to easily mash-up constrained application protocol (CoAP) resources. It is able to discover the available devices and to virtualize them outside the physical network. These virtualizations are then exposed to the upper layers by a REpresentational State Transfer (REST) interface, so that the physical devices interact only with their own virtualization. Furthermore, the system provides simplified tools allowing the development of mash-up applications to different-skilled users. Finally, the architecture allows not only to monitor but also to control the devices, thus establishing a bidirectional communication channel. Hasan Omar Al-Sakran [13] presents a novel intelligent traffic administration system, based on Internet of Things, which is featured by low cost, high scalability, high compatibility, easy to upgrade, to replace traditional traffic management system and the proposed system can improve road traffic tremendously. The Internet of Things is based on the Internet, network wireless sensing and detection technologies to realize the intelligent recognition on the tagged traffic object, tracking, monitoring, managing and processed automatically. The paper proposes an architecture that integrates internet of things with",oceanology,64,unknown
a2524b60d8c51af32ee36f9d3ccb0761c8b593f6,to_check,semantic_scholar,,2003-01-01 00:00:00,semantic_scholar,indexing and retrieving semantic web resources: the rdfstore model,https://www.semanticscholar.org/paper/a2524b60d8c51af32ee36f9d3ccb0761c8b593f6,"The Semantic Web is a logical evolution of the existing Web. It is based on a common conceptual data model of great generality that allows both humans and machines to work with interrelated, but disjoint, information as if it was a single global database. The design and implementation of a general, scalable, federated and flexible data storage and indexing model, which corresponds to the data model of the Semantic Web, is fundamental for the success and deployment of such a system. The generality of the RDF data model presents unique challenges to efficient storage, indexing and querying engines. This paper presents our experience and work related to RDFStore which implements a new flexible indexing and query model. The model is tailored to RDF data and is designed around the Semantic Web from the ground up. The paper describes the underlying indexing algorithm, together with comparisons to other existing RDF storage and query strategies. Towards a lightweight database architecture The generality of the RDF data model presents unique challenges to efficient storage, indexing and querying software. Even if the Entity-Relational (ER) data model [1] is the dominant technology for database management systems today, it has limitations in modeling RDF constructs. RDF being unbounded, the resulting data structures are irregular, expressed using different data granularity, deeply nested or even cyclic. As a consequence, it is not possible to easily fix the ""structural view"" of a piece of information (object), which is instead one of the fundaments of traditional RDBMS systems trying to be much narrower and precise as possible and where an update not conforming to a single static schema is rejected. Database systems also optimize data storage and retrieval by knowing ahead of time how records are structured and interrelated and tend to use very inefficient nested SQL SELECT statements to process nested and cyclic structures. All this is too restrictive for RDF data. Like most semi-structured formalisms [2][3] RDF is self-describing. This means that the schema information is embedded with the data, and no a priori structure can be assumed, giving a lot of flexibility to manage any data and deal with changes in the data's structure seamlessly at the application level. The only basic structure available is the RDF graph itself, which allows describing RDF vocabularies as groups of related resources and the relationships between these resources [4]. All new data can be ""safely"" accepted, eventually at the cost of tailoring the queries to the data. On the other side, RDF data management systems must be much more generic and polymorphic like most of dynamically-bound object-oriented systems [5]; changes to the schema are expected to be as frequent as changes to the data itself and could happen while the data is being processed or ingested. A drawback of RDF heterogeneity is that the schema is relatively large compared to the data itself [6]; this in contrast to traditional RDBMS where the data schema is generally several orders of magnitude smaller than the data. This also implies that RDF queries over the schema information are as important as queries on the data. Another problem is that most RDF data (e.g. metadata embedded into an HTML page or RSS1.0 news feed) might exist independently of the vocabulary schemas used to mark-up the data, further complicating data structure ""validation"" (RDF Schema validation). This de-coupling aspect also makes the data ""de-normalization"" more difficult [7][8][9]. ""De-normalization"" is needed in RDBMS to overcome query performance penalties caused by the very general ""normalized"" schemas. De-normalization must be done taking into account to how the database will be used and how data is initially structured. In RDF this is not generally possible, unless all the RDF Schema definitions of the classes and properties used are known a-priori and available to the software application. Even if that might be the case, it is not a general rule and it would be too restrictive and make RDF applications extremely fragile. In the simplest and most general case, RDF software must associate the semantics to a given property exclusively using the unique string representation of its URIs. This will not stop of course more advanced and intelligent software to go a step further and retrieve, if available, the schema of the associated namespace declarations for validation, optimization or inference purposes. It is interesting to point out that a large part of queries foreseen for Web applications are information discovery and retrieval queries (e.g. Google) that can ""ignore"" the data schema taxonomy. Simple browsing through the RDF data itself or searching for some sub-string into literals, or using common patterns is generally enough for a large family of RDF applications. On the other hand, we strongly believe that RDBMS has proven to be a very effective and efficient technology to manage large quantities of well-structured data. This will continue to be true for the foreseeable future. We thus see RDF and similar less rigid, or semi-structured data technologies as complementary to traditional RDBMS systems. We expect to see RDF increasingly appear in the middle layer where lightweight systems that focus on interoperability, flexibility and a certain degree of decoupling of rigid formats are desired. We believe that a fundamentally different storage and query architecture is required to support the efficiently and the flexibility of RDF and its query languages. At a minimum such storage system needs to be: Lightweight Native implementation of the graph Fundamentally independent from data structure Allow for very wide ranges in value sizes; where the size distribution is not known in advance, most certainly is not Gaussian and will fluctuate wildly. Be efficient it should not be necessary to retrieve very large volumes of data in order to reconstruct part of the graph. Allow built support for arbitrary complex regular-path-expressions on the graph to match RDF queries like RDQL [50] statement triple-patterns. Have some free-text support Context/provenance/scope or flavoring of triples Furthermore given that RDF and the Semantic Web are relatively new, and will require significant integration and experimentation it is important that its technology matches that of the Internet: Easy to interface to C, Perl and Java at the very least. Ruby, Python, Visual Basic and .NET are a pre. Easy to distribute (part of) the solution across physical machines or locations in order match scaling and operational habits of existing key Internet infrastructure. Very resistant to ""missing links"" and other noise. Contexts and provenance A RDF statement represents a fact that is asserted as true in a certain context space time, situation, scope, etc. The circumstances where the statement has been stated represent its ""contextual"" information [10][11]. For example, it may be useful to track the origin of triples added to the graph, e.g. the URI of the source where triples are defined, e.g. in an RDF/XML file, when and by whom they where added and the expiration date (if any) for the triples. Such context and provenance information can be thought of as an additional and orthogonal dimension to the other components of a triple. The concept is called in the literature ""statement reification"". Context and provenance are currently not included in the RDF standardisation process [48][49], but will hopefully adressed in a next release of the specification. From the application developer point of view there is a clear need for such primitive constructs to layer different levels of semantics on top of RDF which can not be represented in the RDF triples space. Applications normally need to build meta-levels of abstraction over triples to reduce complexity and provide an incremental and scaleable access to information. For example, if a Web robot is processing and syndicating news coming from various on-line newspapers, there will be overlap. An application may decide to filter the news based not only on a timeline or some other property, but perhaps select sources providing only certain information with unique characteristics. This requires the flagging of triples as belonging to different contexts and then describing in the RDF itself the relationships between the contexts. At query time such information can then be used by the application to define a search scope to filter the results. Another common example of the usage of provenance and contextual information is about digital signing RDF triples to provide a basic level of trust over the Semantic. In that case triples could be flagged for example with a PGP key to uniquely identify the source and its properties. There have been several attempts [12][13][14][15] trying to formalize and use contexts and provenance information in RDF but a common agreement has not been reached yet. However, context and provenance information come out as soon as a real application is built using RDF. Some first examples are presented below. Our approach to model contexts and provenance has been simpler and motivated by real-world RDF applications we have developed [16a][16b][16c]. We found that an additional dimension to the RDF triple can be useful or even essential. Given that the usage of full-blown RDF reification is not feasible due to its verbosity and inefficiency we developed a different modeling technique that flags or mark a given statement as belonging to a specific context. First example considers subjective assertions. The Last Minute News (LMN) [16b] and The News Blender (NB) [16c] demos allow an user rating and qualifying the source newspapers. The user can ""say"" that a newspaper is ""liberal"" or ""conservative"". Of course, two users, X and Y, will show two different opinions. Without considering the context, this will result in two triples: Newspaper A -> Quality -> ""liberal"" Newspaper A -> Qu",oceanology,65,unknown
c55e4acdf98b1931bf1e00280639348d51a6283a,to_check,semantic_scholar,,2002-01-01 00:00:00,semantic_scholar,a hierarchical collective agents network for real-time sensor fusion and decision support,https://www.semanticscholar.org/paper/c55e4acdf98b1931bf1e00280639348d51a6283a,"This research addresses a problem of how to make effective use of real-time information acquired from multiple sensor and heterogeneous data resources, and reasoning on the gathered information for situation assessment and impact assessment (SA/IA), thus to provide reliable decision support for time-critical operations. A hierarchical collective agents network (HCAN) is employed as a solution to this problem. The agents network supports multi-sensor registration, real-time sensor/platform cueing, level-2 and level-3 information fusion, and has an arm toward the level-4 fusion objectives. An agent component assembly and decision-support-system-development environment, the 21 Century systemsâ AEDGE software package, is used for the design and implementation of a HCAN-DSS system. The ability to integrate and correlate a vast amounts of disparate information from multiple sensor and heterogeneous data resources with varying degrees of uncertainty in real-time is an impediment issue for missioncritical decision support systems (DSS). For example, in crucial military operations command officers need real-time information and intelligences from various sensors/data resources in a theater of reconnaissance and surveillance to build a whole picture of the battle-space. It is critical for the commanders to know and to understand the relationships among the information collected. Questions are asked: what are the physical and functional constituencies among the objects in a given geographic sector? Are there sequential or temporal dependencies of the objects and what will trigger them? What are the possible consequences of the action and re-actions? Decision making based on these situation assessment and impact assessment (SA/IA) are particularly important for identifying and prioritizing âgapsâ between the operation planning and the real-time interactions. To support making effective SA/IA, a data fusion and decision support system is required to use a set of coherent patterns derived from the available data sets and infer the implications (e.g., causal relations) toward the real world situations. The attribute coherence that is critical to the formation of the meaningful knowledge patterns is often obscure in the data sets obtained from heterogeneous resources. The data collections are often incomplete, imprecise, and inconsistent due to various natural constraints and human faults. Decision makers naturally desire to access large quantities of information expressed in diverse forms. However, as new sensor technology and various information sources have combined to create quantity and diversity, it has become increasingly difficult to provide decision makers with the right information at the right time and in the right quantity and format. Real-time computerized decision support systems are constructed by integrating a number of diverse components from a variety of software modules. Software developers have come to a numerous ways of querying the local and centralized data resources to access and distill the large and diverse information for the purpose of providing effective decision support. Meanwhile DSS are becoming more and more complex in terms of knowing which data resources to connect, how to keep track of the data dynamics, and assess the reliability of information from each resources. These tying links make the use of intelligent agent architecture necessary and desirable for allowing real-time responsibility and adaptive control of the DSS. Many popular agent systems of today deploy agents in a uniform level of operation. The agents respond to the same calls and cooperate at the same time toward the same goal of operation. The architecture endues some difficulties in agent communications and task control. When applied in complex real-time DSS with intensive human and system interactions, the cooperative nature makes the system less robust because the disability of one agent would affect the successive operations of the entire agent assembly. The collective nature of the agents in a HCAN paradigm overcomes some of these difficulties, for example, relieving the burden of data-exchanges between fellow agents by limiting agent communication to vertical layers of the assembly only. The hierarchical architecture simplifies the functional design of the agent interactions and enhances the security and efficiency of the process. The HCAN architecture also strikes a balance between the centralized control and distributed computation by allowing distributive agent operations within layers of the hierarchy and enforcing centralized control between the layers of the hierarchy, thus creating a federated agents integration structure. Basically, the HCAN has the functionalities of. 1). A flexible software architecture for accommodating system augmentation and evolutions; 2). A powerful representation schema for accommodating heterogeneous forms of information; 3). A diverse interface for various input resources, output formats, and human interactions; From: AAAI Technical Report WS-02-15. Compilation copyright Â© 2002, AAAI (www.aaai.org). All rights reserved. 4). An ability of reasoning on incomplete and inconsistent information, and extracting useful knowledge from the data of heterogeneous resources; 5). An ability of incorporating real-time dynamics of information resources into system at time of operation, and promptly adjusting the reasoning mechanisms; 6). An ability of summarizing and refining knowledge extracted, and distinguishing mission and time critical knowledge from insignificant and redundant ones; 7). A capability of supplying meaningful and accurate explanations, both qualitatively and quantitatively, of the automated system actions; and 8). A capability of providing adequate control and scrutiny of the system operations w.r.t. environment constrains. There are many sources of uncertainty at different levels of the decision support. For example, even if a situationassessor is aware of the presence of certain objects in the operation space, such as the type of contact, intention, reaction rational, etc., the exact dynamics of the object is still uncertain to the decision maker. While the knowledge about the object dynamics is critical in constructing an optimal strategy of action, various statistical methods and knowledge discovery techniques are applied in the reasoning module. The level of uncertainty forces the reasoning agents to operate with different decision strategies. The 21 Century Systems, Inc. has developed the AEDGE as an open DII-COE and CORBA compliant agentbased environment that enables the development of components-based agent systems. The system is implemented in JavaTM, with Java Database ConnectivityTM for DB access, Java Swing, AWT, and Java3D for visual interfaces, Java Media Framework and Java Speech API for audio/speech interface. AEDGE defines Agents, Entities, Avatars and their interactions with each other and with external sources of information. This standardized architecture allows additional components, such as servicespecific DSS tools, to be efficiently built upon the core functionality. Common interfaces and data structures can be exported to interested parties who wish to extend the architecture with new components, agents, servers, or clients. When the core AEDGE components are bundled with customer-specific components, a clean separation of those components, through APIs, is provided. The AEDGE is based on an extensible multi-component DSS architecture (EMDA, also referred to as the AEDGETM Architecture). The architectures describe the data objects, interfaces, communication mechanisms, component interactions, and integration mechanisms for the AEDGE and its extensions. In the AEDGE architecture, components communicate among each other via the Service Provider/Service Requester Protocol (SPSR). Service providers are components that implement an algorithm or need to share their data (data sources). Service requesters are the components that need a function performed for them by some other component or need to import data from another component. Both service requesters and service providers implement remote interfaces, which enables such components to communicate over a TCP/IP network. The remote interface implementation is currently based on Java RMI (remote method invocation), though the Architecture is not dependent on this implementation. AEDGE provides multiple levels of customization. The subject-matter users are able to build scenarios and scripts or to automatically generate them using the AEDGE-based Scenario Editor. Rules and triggers for agent behaviors can be created and modified by the advanced user. AEDGE also provides APIs for custom extensions of agents, data bridges, and the entity framework. The practical user will enjoy AEDGEâs versatile data connectivity and its near-real-time execution and monitoring of DSS functions. As a built-in bonus, AEDGE provides connections to a number of simulators and data formats, including HLA, DIS, DTED, DBDB2, XML, as well as support for multiple modes of distribution (CORBA, RMI, TCP/IP). As an example of the HCAN design using AEDGE for data fusion and DSS applications, the Advanced Battlestation with Decision Support System (ABS/DSS) which was developed as an operational agent-based C2 team decision support platform for command and control centers aboard aircraft carriers.. The ABS/DSS is based on AEDGEâs implementation of HCAN, and provides consolidated situational awareness through real-time, interactive, agent based decision support coupled with a linked 2D/3D battlespace visualization. Additionally, the ABS/DSS supports shipboard distributed training, train-asyou-fight, with a built-in scenario construction and emulation of friendly and hostile entities. Whether the watchstander is in live-feed mode or in training mode the operation of the agent-based decision support system and the 2D/3D visualization is identical. ",oceanology,66,unknown
4a99aa2ca1dd85ac7b82d79bf8cec1a09c9d8488,to_check,semantic_scholar,,2012-01-01 00:00:00,semantic_scholar,energy efficient protocol design in wireless sensor networks â contributions to make the ubiquitous platform greener,https://www.semanticscholar.org/paper/4a99aa2ca1dd85ac7b82d79bf8cec1a09c9d8488,"s all hardware resources as components. For example, calling the getData() command on a sensor component will cause it to later signal a dataReady() event when the hardware interrupt fires. While many components are entirely software based, the combination of split-phase operations and tasks makes this distinction transparent to the programmer. In both cases an event signals that the encryption operation is complete. ADC, ClockC, UART, SlavePin and SpiByteFifo are example hardware abstraction components. TinyOS commands and events are very short, due to limited code space and a finite state machine style of decomposition. The rich event processing model means an event or command call path can traverse several components. The TinyOS component model allows us to easily change the target platform from mote hardware to simulation by only replacing a small number of low-level components. The event-driven execution model can be exploited for efficient eventdriven simulation, and the whole program compilation process can be re-targeted for the simulatorâs storage model and native instruction set. The static component memory model of TinyOS simplifies state management for these large collections. Setting the right level of simulation abstraction can accurately capture the behavior and interactions of TinyOS applications. Figure 1.3: TinyOS Structure (Consist of scheduler and graph of components) 2.3 TOSSIM: A Simulator for TinyOS Sensor Networks The Necessity of Network Simulation: The emergence of wireless sensor networks brought many open issues to network designers. Traditionally, the three main techniques for analyzing the performance of wired and wireless networks are analytical methods, computer simulation, and physical measurement. However, because of many constraints imposed on sensor networks, such as energy limitation, decentralized collaboration and fault tolerance, algorithms for sensor networks tend to be quite complex and usually defy analytical methods that have been proved to be fairly effective for traditional networks. Furthermore, few sensor networks have come into existence, for there are still many unsolved research problems, so measurement is virtually impossible. It appears that simulation is the only feasible approach to the quantitative analysis of sensor networks. The event-driven nature of sensor networks means that testing an individual mote is insufficient. Programs must be tested at scale and in complex and rich conditions to capture a wide range of interactions. Deploying hundreds of motes is a daunting task, the focus of work shifts from research to maintenance, which is time-consuming due to the failure rate of individual motes. A simulator can deal with these difficulties, by providing controlled, reproducible environments, by enabling access to tools such as debuggers, and by postponing deployment until code is well tested and algorithms are understood. TOSSIM: TOSSIM is a discrete event simulator for TinyOS sensor networks. Instead of compiling a TinyOS application for a mote, users can compile it into the TOSSIM framework, which runs on a PC. This allows users to debug, test, and analyze algorithms in a controlled and repeatable environment. As TOSSIM runs on a PC, users can examine their TinyOS code using debuggers and other development tools. TOSSIMâs primary goal is to provide a high fidelity simulation of TinyOS applications. For this reason, it focuses on simulating TinyOS and its execution, rather than simulating the real world. While TOSSIM can be used to understand the causes of behavior observed in the real world, it does not capture all of them, and should not be used for absolute evaluations. Related Publication: Swarup Kumar Mitra, Ayon Chakraborty, Subhajit Mandal and M.K.Naskar, Simulation of Wireless Sensor Networks using TinyOS A Case Study, In the Proceedings of the National Conference on Modern Trends in Electrical Engineering, pages EC 23 EC 26, Hooghly, West Bengal, July 2009. 3 Data Gathering Schemes in WSNs Data gathering is by far one of the most important aspects of research considering energy efficiency in the routing protocols for wireless sensor networks. Wireless sensor networks have emerged as a ubiquitous platform recently, and issues regarding the efficiency of energy usage by these devices play a very important role. These devices are equipped with negligible or less amount of battery power to sustain for a long time. Not only that, in most of the scenarios, where these networks are deployed it is infeasible or impossible sometimes to replace the battery power of the sensor nodes. One of the most fundamental aspects for energy consumption in sensor nodes is communication, other than sensing and computation costs. Optimization of communication costs is thus essential, which is a direct consequence of betterment of routing techniques in this type of wireless networks. A major portion of my contribution in this project deals with designing data gathering schemes for wireless sensor networks and optimization of routing techniques, described below. The first in this queue was the HDS or âHybrid Data Gathering Schemeâ. Published in the International Conference of Distributed Computing and Internet Technology (ICDCITâ10), this work is a novel approach in minimizing not only the communication / energy overhead but also guarantees a minimal energy-latency product. It also distributes the energy consumption by the nodes by rotating the leader node, so as to increase the uniformity of energy content in the nodes. The uniform distribution of energy content in the nodes also helps to lessen the chances of a black hole or a sinkhole problem. The HDS protocol is based on the hybrid combination of two algorithms, SHORT and LBERRA. The LBERRA scheme is used to subdivide the sensor field into predefined clusters, and SHORT is applied to form a binary tree spanning the nodes. There are two types of leader nodes: one for each cluster, forming the root of the tree and the other one is the âsinkâ communicating the gathered data to the Base Station. In each of the data gathering rounds the leader node is changed The second work was related to optimization of routing chain through heuristic techniques. Firstly, I applied Particle Swarm Optimization to create the most energy efficient paths for communication in the sensor field. Then, I investigated the use of Genetic Algorithms (hybridized with simulated annealing) in solving the same problem. In these works, I not only devised the algorithm for the minimum-energy path formation, but also coded it in nesC discussed in the earlier section. The implementation and simulation in nesC guarantees the hardware feasibility of the algorithm in sensor nodes. Packet loss rates were also studied with varying network topology and signal strengths in communication between particular sensor nodes. In all the cases, a standard background noise was considered. This work followed a series of publications including three international conferences and two international journals. An extension of this work was to create energy efficient data gathering trees. Most algorithms developed in literature used greedy algorithms to construct routing trees which in most of the cases did not result in near-optimal energy usage. âROOTâ or âROuting through Optimized Treesâ was an answer to this need. Related Publications: International Conference: 1. Ayon Chakraborty, Kaushik Chakraborty, Swarup Mitra and Mrinal Naskar, An Optimized Lifetime Enhancement Scheme for Data Gathering in Wireless Sensor Networks, in the proceedings of The Fifth IEEE Conference on Wireless Communication and Sensor Networks, WCSN'09 Allahabad, India, (December, 2009). 2. Ayon Chakraborty, Swarup Mitra and Mrinal Naskar, An Efficient Hybrid Data Gathering Scheme in Wireless Sensor Networks, in the proceedings of The Sixth International Conference on Distributed Computing and Internet Technology, ICDCIT'10, Bhubaneswar, India. (February, 2010). 3. Ayon Chakraborty, Swarup K. Mitra and M.K. Naskar, Energy Efficient Routing in Wireless Sensor Networks: A Genetic Approach, in the Proceedings of the International Conference on Computer Communications and Devices (ICCCD 2010), IIT Kharagpur (December, 2010) 4. Kaushik Chakraborty, Ayon Chakraborty, Swarup Mitra and Mrinal Naskar, ROOT: Energy Efficient Routing through Optimized Tree in Sensor Networks, in the proceedings of The International Conference on Computer Communications and Devices â ICCCD'10, Kharagpur, India. (December, 2010).",oceanology,67,not included
d0f2b863b1af919d08620efe960f708aabe63199,to_check,semantic_scholar,,2005-01-01 00:00:00,semantic_scholar,secure design and implementation of distributed and interoperable information systems based on overlap knowledge pattern,https://www.semanticscholar.org/paper/d0f2b863b1af919d08620efe960f708aabe63199,"New architectural and technical forms of information systems add a more significant level of complexity due to the decentralization of the constraints, treatment and data. These architectures increase the deployment and the runtime possibilities because of the number of existing sites. Indeed, the simple separation of the various functional levels as it is done in a classical architecture (Data, Treatment, Presentation) is not enough and the choice of the site of deployment or runtime becomes significant for the optimization of the production of the system. In these architectures, these decisions of distribution are generally made during the implementation phase. The conceptual structures offered to designers to allow them to express their needs for distribution (concepts of packages, business component,..) do not match with the rules used by developers for building their distributed components [Snene04B]. In fact, software components represent a single and autonomous concept of real world. They encapsulate all the data concerning this concept including name, goal, behavior and all other information with regard to them. In fact, a software component is a set of objects that can be physically deployed on two or several sites. It is usually made up of one or of several distributed components that offer together the various aspects of distribution necessary to the software component. The distributed components represent the physical modules used for application assembling. They encapsulate given data and treatments and provide their services through well-defined interfaces.",oceanology,68,not included
3d80085c2bc6e289c5620bbf06b0878f4b3be001,to_check,semantic_scholar,,2010-01-01 00:00:00,semantic_scholar,reliable middleware framework for rfid system,https://www.semanticscholar.org/paper/3d80085c2bc6e289c5620bbf06b0878f4b3be001,"The reliability of RFID systems depends on a number of factors including: RF interference, deployment environment, configuration of the readers, and placement of readers and tags. While RFID technology is improving rapidly, a reliable deployment of this technology is still a significant challenge impeding wide-spread adoption. This research investigates system software solutions for achieving a highly reliable deployment that mitigates inherent unreliability in RFID technology. 
We have considered two different problem domains for large scale RFID deployment. One is item tracking and the other is guidance-monitoring. 
The basic contribution of our work is providing novel middleware solution that is able to serve the application taking into account the inherent unreliability of RFID technology. Our path abstraction that uses the physical flow of data as an ally to generate a logical system level flow enhances the performance in many ways. The contributions of this dissertation are summarized below: 
Defining novel system architecture for item tracking applications: We have defined a system architecture referred to as Reliable Framework for RFID (RF2ID) that takes into account the unreliability of RFID devices and provides a scalable, reliable system architecture for item tracking applications. It uses a distributed system abstraction named Virtual Reader (VR) that handles RFID data in different geographic locations. Virtual Path (VPath) is the abstraction that creates channels among the VRs and facilitates a data flow oriented data management in the system. 
Implementation of RF2ID: We have implemented RF2ID that is able to incorporate physical RFID devices as well as emulated devices for scalability study taking into account various real world challenges of large scale RFID deployment. 
Load Shedding Based Resource Management: RF2ID requires a mechanism to handle unexpected system load in the presence of asynchronous arrival of data items. Space based load shedding and time based load shedding techniques are used in RF2ID. The basic idea is to exploit the VR and Vpath abstraction to intelligently share the load among the VRs in the presence of high system load, and yet provide some guaranteed Quality of Service (QoS). 
Architecture for GuardianAngel: We define an architecture for an indoor pervasive environment which provides novel system abstraction and communication framework. The layered architecture has distributed computational elements known as the virtual station (VS) that are in charge of serving different regions of the environment. The Mobile Objects (MO) are the physical and logical entities that use sensing device and traverse the environment. The environment itself is tagged with RFID. The MO uses its sensing device to make guidance decisions locally. The VS keeps status information of MOs and keeps coarse grained information of the MO over time and space providing a virtual location for each MO. 
Implementation of GuardianAngel: We have implemented the GuardianAngle system as defined by the architecture. We have used a testbed that uses real RFID readers and tags in the pervasive environment in a limited laboratory setup. We have also developed a distributed system setup using emulated tags for a scalability study of the proposed architecture. We have also implemented a prototype application, to test its feasibility in the real world. 
Evaluation of the system: We have conducted extensive evaluation using the real RFID testbed as well as scalability study using emulated readers and tags. The evaluation using the real RFID tags and readers gives us the credibility of the system under various environmental considerations. The large scale experimentations provide us with scalability and feasibility study to strengthen our limited resource study using real RFID testbed. (Abstract shortened by UMI.)",oceanology,69,unknown
a72d2df90c87909159ee5f59811f722b7f6ad4ad,to_check,semantic_scholar,,2003-01-01 00:00:00,semantic_scholar,"usenix association proceedings of bsdcon â 03 san mateo , ca , usa september",https://www.semanticscholar.org/paper/a72d2df90c87909159ee5f59811f722b7f6ad4ad,"The ever increasing mobility of computers has made protection of data on digital storage media an important requirement in a number of applications and situations. GBDE is a strong cryptographic facility for denying unauthorised access to data stored on a ââcoldââ disk for decades and longer. GBDE operates on the disk(-partition) level allowing any type of file system or database to be protected. A significant focus has been put on the practical aspects in order to make it possible to deploy GBDE in the real world. 1 1. Losing data left and right In the last couple of years, gentlemen of the press have repeatedly been able to expose how laptop computers containing highly sensitive or very valuable information have been lost to carelessness, theft and in some cases espionage. [THEREG] The scope of the problem is very hard to gauge, since it is not a subject which the involved persons and, in particular, institutions are at all keen on having exposed. However, a few data points have been uncovered, revealing that the U.S. Federal Bureau of Investigation loses, on average, one laptop every three days. [DOJ0227] When a computer is lost, stolen or misplaced, it is very often the case that the computer hardware represents a value which is insignificant compared to the value of the disk contents. More often than not, the only reason the press heard about it was that the material on the disk was ââhotââ enough to make the loss of control rattle people at government level. While it is easy to blame these incidents on ââuser errorââ, as is generally done, doing so makes it a very hard problem to fix. Human nature being what it is, seems to remain just that. In the absence of technical counter measures, administrative measures have been applied, generally with abysmal results. In one case, a bureaucracy has handled the problem according to what could easily be 1 This software was developed for the FreeBSD Project by Poul-Henning Kamp and NAI Labs, the Security Research Division of Network Associates, Inc. under DARPA/SPAWAR contract N66001-01-C-8035 (ââCBOSSââ), as part of the DARPA CHATS research program. mistaken for the plot from a classic Buster Keaton movie: First a laptop was forgotten and lost in a taxi-cab. New policy: always drive your own car if you bring your laptop. Then a car was stolen, including the laptop in the trunk. New policy: always bring your laptop with you. The next laptop was stolen from a pub while the owner was bowing to the pressures of nature. New policy: employees are not to carry their own laptops outside the office at any time. Laptops will be transported from and to the employees home address by the agency security force and will be chained and locked to a ring in the wall installed by the company janitors. All requests must be filed 3 days in advance on form ##-#. [PRIV] 2. Protecting disk contents Protecting the contents of a computerâs disk can in practice be done in two ways: by physically securing the disk or by encrypting its contents. Physical protection is increasingly impossible to implement. It used to be that disk drives could only be moved by forklift, but these days a gigabyte disk is the size, but not quite yet the thickness, of a postage stamp. While computers can be tied down with wires and bars can be put in front of windows, such measures are generally not acceptable, or at least not judged economically justified in any but the most sensitive operations. That leaves encryption of the disk contents as the only practical and viable mode of protection, and both the practicality and the viability has been somewhat in doubt. Until recently, nearly all aspects of cryptography were a highly political issue, this has eased a lot in the last couple of years and there now ââonlyââ remain a number of rather fundamental questions in the area of law enforcement and human rights, which are still unsettled. With the political issues mostly out of the way, the next roadblock is practical: While use of cryptography can never be entirely transparent, the overhead and workload it brings must be reasonable. 2.1. Application level encryption Encryption at the application level has been available for a number of years, primarily in the form of the PGP [PGP] program. This is about as intrusive and demanding as things can get: the user is explicitly responsible for doing both encryption and decryption and must enter the pass-phrase for every operation. 2 Apart from the inconvenience of this extra workload, many org anisations would trust their users neither to get this right nor even to want to get it right. From an institutional point of view it is important that cryptographic data protection can be made mandatory. 2.2. Filesystem level encryption Encryption at the file system level is a tried and acknowledged method of providing protection, but it suffers from a number of drawbacks, mainly because no mainstream file systems offer encryption. Encrypting file systems are speciality items, which means increased cost and system administration problems of all sorts. And since practically all operating systems use their own file system format, cross platform fully functional file systems are very rare. This means that a typical organisation will have to operate with a handful of different methods of encryption, which translates to system administration overhead, user confusion and extra effort to pass security and ISO9000 audits. A secondary, but increasingly important issue is that data which are stored in databases on raw disk, operating system paging areas and other such data are not protected by a cryptographic file system. To protect these would mean adding yet another set of encryption methods, which leads to a situation which is very hard to handle practically and administratively. Finally, file systems have a complex programming interface to the operating system, which traditionally 2 Interestingly, this is so impractical in real world use that various applications with PGP support resort to caching the pass-phrase at the application level, thereby weakening the protection a fair bit. has been subject to both version skew and compatibility problems. 2.3. Disk level encryption Encryption at the disk level can protect all data, no matter how they are stored, file system, database or otherwise. To a user, encryption at the disk level would require authentication before the computer can be used, everything functioning transparently thereafter, with all disk content automatically protected. Given that the programming interface for a disk device is very simple and practically identical between operating systems, there are no technical reasons why the same implementation could not be used across several operating systems. All in all this is a close to ideal solution from an operational point of view. There are significant implementation issues however. In difference from the higher levels, encryption at the disk level has no way of knowing a priori which sectors contain data and which sectors do not; neither is knowledge available about access patterns or relationships between individual sectors. Where application level or file system based encryption schemes can key each file individually, a disk based encryption must key each and every sector individually, ev en if it is not currently used to hold data. It has been argued that the encryption ideally should happen in the disk-drive, and while there are steps in this direction, they do unfortunately seem to have been made for the wrong reasons by the wrong people [CPRM], and have consequently not gained acceptance. Provided the owner of the computer remains in control of the encryption, I see no reason why encryption in the disk drives should not gain acceptance in the future. 3. Why this is not quite simple Several implementations have been produced which implement a disk encryption feature by running the user provided passphrase through a good quality one-way hash function and used the output as a key to encrypt all the sectors using a standard block cipher in CBC mode. A per sector IV for the encryption is typically derived from the passphrase and sector address using a one-way hash function. Tw o typical examples are [CGD] and [LOOPAES]. Unfortunately this approach suffers from a number of significant drawbacks, both in terms of cryptographic strength and deployability. For data to stay protected for decades or even lifetimes, sufficient margin must exist not only for technological advances in brute force technology, but also for theoretical advances in cryptoanalytical attacks on the algorithms used. Protecting a modern disk, typically having a few hundred millions of sectors, with the same single 128 or 256 bits of key material offers an incredibly large amount of data for statistical, differential or probabilistic attacks in the future. Worse, because the sectors contain file system or database data and meta data which are optimised for speed, the plaintext sector data typically have both a high degree of structure and a high predictability, offering ample opportunities for statistical and known plaintext attacks. This author would certainly not trust data so protected to be kept secret for more than maybe fiv e or ten years against a determined attacker. But far more damning to this method is that there can only be one single passphrase for the disk. This effectively rules out the ability for an organisation to implement any kind of per-user or multilevel key management scheme: the only possible scheme is ââone key per diskââ. Add to this that to change the passphrase the entire disk would have to be decrypted and re-encrypted, and we have a model which may work in theory, and can be made to work in practice for a determined individual, but which would fast become an operational liability for any org anisation. 4. Designing GBDE The initial design phase of GBDE focused on determining a set of features which would make it both possible and",oceanology,70,not included
3a984a19f86877947561b4613b8b40b2efa01d26,to_check,semantic_scholar,,2009-01-01 00:00:00,semantic_scholar,understanding storage system problems and diagnosing them through log analysis,https://www.semanticscholar.org/paper/3a984a19f86877947561b4613b8b40b2efa01d26,"Nowadays, over 90% new information produced are stored on hard disk drives. The explosion of data is making storage system a strategic investment priority in the enterprise world. The revenue created by storage system industry steadily increases from $14.2 Billion in 2004 to over $18.4 Billion in 2007. As a key component of enterprise systems, reliable storage systems are critical. However, despite the efforts put into building robust storage systems, as the size and complexity of storage systems have grown to an unprecedented level, storage system problems are common. Unfortunately, many aspects of storage system problems are still not well understood, and most of previous studies only focus on one component - disk drives. 
To better understand storage system problems, we analyzed the failure characteristics of the core part of storage system - the storage subsystem, which contains disks and all components providing connectivity and usage of disk to the entire storage system. More specifically, we analyzed the storage system logs collected from about 39,000 storage systems commercially deployed at various customer sites. The data set covers a period of 44 months and includes about 1,800,000 disks hosted in about 155,000 storage shelf enclosures. Our study reveals many interesting findings, providing useful guideline for designing reliable storage systems. Some of the major findings include: (1) In addition to disk failures that contribute to 20â55% of storage subsystem failures, other components such as physical interconnects and protocol stacks also account for significant percentages of storage subsystem failures. (2) Each individual storage subsystem failure type and storage subsystem failure as a whole exhibit strong self-correlations. In addition, these failures exhibit bursty patterns. (3) Storage subsystems configured with dual-path interconnects experience 30â40% lower failure rates than those with a single interconnect. (4) Spanning disks of a RAID group across multiple shelves provides a more resilient solution for storage subsystems than within a single shelf. 
As we found out that storage subsystem problems are far beyond disk failures, we extend the scope of study to various storage system problems, and study the characteristics of storage system problem troubleshooting from various dimensions. Using a large set (636,108) of real world customer problem cases reported from 100,000 commercially deployed storage systems in the last two years, the analysis show that while some problems are either benign, or resolved automatically, many others can take hours or days of manual diagnosis to fix. For modern storage systems, hardware failures and misconfigurations dominate customer cases, but software failures take longer time to resolve. Interestingly, a relatively significant percentage of cases are because customers lack sufficient knowledge about the system. We also evaluate the potential of using storage system logs to resolve these problems. Our analysis shows that a failure message alone is a poor indicator of root cause, and that combining failure messages with multiple log events can improve problem root cause prediction by a factor of three. 
One key finding is that storage system logs contain useful information for narrowing down the root cause, while they are challenging to analyze manually because they are noisy and the useful log events are often separated by hundreds of irrelevant log events. Motivated by this finding, we designed and implemented an automatic tool, called Log Analyzer, to improve problem troubleshooting process. By applying statistical analysis techniques, the Log Analyzer can automatically infer the dependency relationship between log events, and identify the key log events that capture the essential system states related to storage system problems. By combining classic unsupervised classification techniques - hierarchical clustering with the event ranking techniques, the Log Analyzer can also identify recurrent storage system problems based on similar log patterns, so that previous diagnosis efforts can be systematically retrieved and leveraged. We train the Log Analyze with 18,878 week-long storage system logs and evaluate it with 164 real-world problem cases. The evaluation indicates that the Log Analyzer can effectively reduce the log event number to 3.4%. For most of the 16 real-world problem cases manually annotated with 1â3 key log events, the Log Analyzer accurately ranked the key log events within top 3 without a priori knowledge on how important the events are. For the other 148 problem cases with diagnosis and with root cause information, the Log Analyzer effectively grouped problem cases with the same root cause together with 63â93% accuracy, significantly outperforming other three alternative solutions which only achieve 30â46% accuracy.",oceanology,71,unknown
df7fb406e31a6057e2ea8f4997c3b71895e44093,to_check,semantic_scholar,,2007-01-01 00:00:00,semantic_scholar,design and implementation of a microprocessor-based sequencer for a small-scale groundnut oil production plant,https://www.semanticscholar.org/paper/df7fb406e31a6057e2ea8f4997c3b71895e44093,"A microprocessor-based Sequencer for a small-scale groundnut oil production plant was designed and a test model was implemented. The microprocessor-based Sequencer is meant to replace traditional, electromechanical sequencers, which are based on relays, contactors, limit switches and other similar devices. The INTEL 8085A microprocessor, combined with interface chips like the AD7575 ADC, the MAX378 multiplexer was used to implement the sequencer. Software was programmed into 2716 EPROM. Actuators and signal conditioning circuits were also designed and implemented. The implemented system was tested and the performance was found to be satisfactory. Introduction Background to the problem : According to Nwachuku[1] microprocessors are the state of the art in electronics digital systemsâ design and the Nigerian Engineer, like his counterpart the world over, has no choice but to become interested in them. He contended further that, because of the nature of the microprocessor and its versatility, it becomes possible for the Nigerian engineer to device products to meet local needs using imported chips. This according to him, is the direction of technological development ..In the advanced industrialized and newly industrialized countries, the last couple of decades have seen the extensive application of microprocessors and computers to automate production processes. Specifically, the microprocessor acts as a micro-controller with a fixed program. Here, the microprocessor application system in most cases, involves the determination of values of physical parameters like temperature, pressure, and so on. In Nigeria, not much seems to have been achieved in this area. Some big manufacturing companies in both the public and private sectors of the Nigerian economy have had to change from along monitoring and control of plant operations to microprocessor/computer-based systems. Most of them are based on the programmable logic controllers (PLCs). But all these analog to digital (microprocessor /computer-based) implementations are mostly carried out by foreign firms and personnel, bringing along with them, their designed hardware and software. Even at that, not much is known to have been achieved in the area of deploying this latest-in-technology to improve the production processes of small-scale industries. This project was conceived as a result of a realization of this obvious short-coming. The nature of the problem: All over the world, countries have come to recognize the leading role which small-scale industries play in their economic development. They furnish over forty-percent (40%) of a nationâs output of goods and they also provide a substantial amount of total employment in an economy. A realization of this obvious fact has made the Federal Government of Nigeria to lay emphasis on the need to support Small and Medium Scale Enterprises in order that they act as catalyst for Nigeriaâs industrial and economic growth. For example, the Federal Government of Nigeria has floated a Bank of Industries and has set up a Small and Medium Enterprises Development Agency of Nigeria (SMEDAN) with the objective of improving the performance of Small and Medium Enterprises (SMEs) towards achieving rapid industrialization of Nigeria and for the reversal of its over dependence on imports. Advanced Materials Research Online: 2007-06-15 ISSN: 1662-8985, Vols. 18-19, pp 107-110 doi:10.4028/www.scientific.net/AMR.18-19.107 Â© 2007 Trans Tech Publications Ltd, Switzerland All rights reserved. No part of contents of this paper may be reproduced or transmitted in any form or by any means without the written permission of Trans Tech Publications Ltd, www.scientific.net. (Semanticscholar.org-19/03/20,17:18:59) However, it must be realized that, the success of these initiatives and of Small and Medium Scale Industries (SMIs) themselves, would depend on indigenous locally developed production processes and technologies. Failures of this class of industries in the past have largely been attributed to their over dependence on imported production processes, technologies, and by extension, on imported plants, equipments and machines. The Raw Materials Research and Development Council (RMRDC) of Nigeria have identified the dearth of process equipment and machinery as the bane of Nigeriaâs under-utilization of her agricultural and mineral raw materials. This now places a huge burden on the Nigerian engineer to now start to design, implement, and practicalize new production processes, machines, equipment and plants that can be deployed by the old and emerging SMIs. The Federal Government of Nigeria seems to have set the ball rolling by the establishment of a National Office for Technology Acquisition and Promotion (NOTAP). The literature is replete with works on interfacing microprocessors with real life situations which is exemplified by the following: Hosier[2] reviewed briefly, the issues that has to be addressed when interfacing a microprocessor to perform real world monitoring and control operations. Anazia [3] gave an overview of the status of microprocessor applications in industrial process control as it relates to the Guinness Plant in Benin-City, Nigeria. A PC-Based Data Acquisition and Supervisory Control system for a Small-Scale Industry was designed and implemented by [4]. Mansfield[5] described the use of transducers for industrial measurement purposes. Program and Data Stores design and Input/Output interfacing were well treated by Short[6]. Gregory[7] explained in good details signal conditioning circuits design. General description of the microprocessor based sequencer The groundnut-oil production plant operates as a series of logically controlled sequence of states; for example, agitator ON, crusher OFF and so on; the duration of each state being determined by sensor signals, for example, temperature or pressure; these sequence of states were transferred into software. The software part (application program), in consonance with the process-sequential flowchart (SFC) that describes the operation of the plant was written in assembler. A major part of the plant consists of electrical and mechanical components like motors which are controlled by electro-mechanical relays, these in turn are operated by signals from the microprocessor; therefore, OUTPUT INTERFACE circuits were designed. Other signals are fed to the microprocessor from temperature, pressure, and other sensors; INPUT INTERFACE circuits were therefore, also designed. The microprocessor system must have memory for storing the application program and for implementing intermediate computations and such other operations. INPUT transducersâ outputs are memory-mapped. The Y2 output of the 74ALS138 decoder that was used addresses a memory address range of 1000Hex to 17FFHex. Therefore, the six (6) input transducers reside at addresses 1000H, 1001H, 1002H, 1003H, 1004H and 1005H respectively. The analog-to-digital converter converts input voltages of between 0volts and 5volts to binary outputs of between 00000000 and 11111111, that is, between 00Hex to FFHex. The output ports (74LS373s) were assigned to a separate address space different from that occupied by main memory; hence, they were isolated or standard I/O. Therefore, the designed and implemented system is as shown in block diagrammatic form, in Fig. 1. Description of the Designed and Implemented System In Fig. 1, block 1 represents the transducers. Block 2 represents the signal conditioning circuits that condition the transducersâ signals to match the multiplexerâs-MAX 378(block 3) inputs. The combination of the multiplexer and the sample and hold-AD781 JN(block 4) selects and holds an input for the analog-to-digital converter-AD7575 JN (block 5) to convert for the input ports (block 6). Block 7 (INTEL 8085A) is the microprocessor subsystem. Block 8 is an address/data bus decoder (74ALS373), while block 9 is an address decoder (74ALS138) for the memory subsystem(blocks 10-4118 and 11-2716). Block 12 represents the output ports (74ALS373) and 108 Advances in Materials and Systems Technologies",oceanology,72,unknown
7dbe61b811bfc91e247dd4c949543468c394a1fd,to_check,semantic_scholar,,1999-01-01 00:00:00,semantic_scholar,integration of corba and web technologies in the vega dis,https://www.semanticscholar.org/paper/7dbe61b811bfc91e247dd4c949543468c394a1fd,"Distributed client/server architectures nowadays appear as a must for the wide information systems of the future virtual enterprise. At the same time, the continued growth of the Internet/WEB and its associated standard developments leads to new ways of world-wide information communication, distribution and access to information. This paper introduces to the various developments undertaken in the VEGA 1 project for a tighter integration of STEP, CORBA and WEB technologies within a DIS . Thus, the VEGA platform will allow both the support of distributed heterogeneous and interoperable client/server information systems (through CORBA) and the support of WEB based access to information and services through an Internet based navigation, building upon CGI and Java technologies. 1. Context and problematical issues The Large Scale Engineering (LSE) and Manufacturing universe is nowadays facing an increasing competitive environment where flexibility and adaptability to change are the bound paths to success, leading companies to renew their way of working. This is due to economical and technological drivers. Indeed, industrial enterprises are now devoted to the specification, design and construction of still larger and more complex manufacturing products: it has become no more possible to a single even-wide enterprise to take in charge the realisation of the whole products, both for financial reasons and because the required skills are not all within the enterprise. Thus, companies and SMEs ( Small and Medium Enterprises ) are now on the way of constituting virtual enterprises (VEs) for each new project. In a VE, contractors, partners, suppliers and customers form a temporary aggregation of non co-located actors dealing with the same product, but focusing on their core domain of competence for the shared profitability of industrial projects. At the same time, current progress in IT , providing more reliable and relevant mechanisms and software tools, the development of sophisticated new frameworks in client/server applications, and the continued growth of the Internet enable improved business processes and provide organisations with new business opportunities. Companies are now widely deploying their applications in Internet and Intranet 4 environments, assembling advanced IT based architectures encompassing among others networking, distributed information systems and concurrent engineering. In such a context, the VEGA project is developing a mandatory infrastructure for the support of the LSE business processes, particularly on the base of main standards for information modelling and exchange, with STEP (ISO 10303) or the IFC developed by the IAI , distribution and interoperability mechanisms with the OMG 6 CORBA and IIOP de facto standards, and WEB technology based on HTTP /CGI and Java. VEGA intends to cover the general needs of companies in VEs and Intranets, with a focus on the Building & Construction sector. Its main objective is to provide an information management architecture to guaranty interoperability among various software components running on different platforms, targeting STEP distributed technologies as an approach to bridge the gap between multiple and delocalised software systems in the construction industry. Within VEGA, the development of the DIS is a first approach towards an end-user oriented service for access to information through various forms. In the AEC field, large projects require the involvement of many body entities (client, architect, design engineers, various technical engineers, etc.) sitting at various locations, with different views and needs on the project, and managing different forms of documents like textual documents, structured documents, drawings, and so on. Dealing with all these kinds of information representations on the client side lead to consider, as far as possible, standardised front-end services. Access to information can be related to EDI 9 too. Until now, EDI has always been considered as a technology typically based on EDIFACT syntax and rules. But EDI can be considered as a kind of generic term, including all aspects of technical and commercial information exchange without mandatory requirements with respect to specific communication technology. Indeed, EDI can be considered from two different points of view: â¢ the first one is a rather conceptual one: the EDI is a structured electronic exchange of data of any type between computer applications of parties involved in a transaction ; â¢ the second is an operational one, where we consider means to realise the task as announced in the first point of view. With respect to these operational means, the Internet is nowadays more and more acknowledged as the common medium to support communication facilities within the development of client/server applications that deserve the larger audience. The WEB technology builds upon HTTP to provide the users with high-level graphical applications independent of the underlying client platform. Furthermore, the Java language now simplifies the development of WEB applications through the power and flexibility of a real object-oriented language. The Intranet perspective enfavours the use of WEB technologies on LANs 10 on a company scale, whether it be real or virtual. Therefore, a different approach has been applied in the VEGA DIS, considering EDI in the broad sense and consequently managing EDI messages through distributed networked infrastructures, emerging WEB standards (mainly HTML and VRML formats), and WEB technologies like CGI and Java. 2. Integration of new technologies in the VEGA project 2.1 Overview of the VEGA project As previously mentioned, the VEGA 11 project objective is to develop an IT platform enabling companies in a VE to work together. VEGA leans on available technology, and extend their capabilities as needed for engineering collaboration in a flexible distributed environment. To address the problem of information sharing, VEGA deals with 3 different technologies: â¢ product-data modelling for the specification of meaningful project information; â¢ middleware technology for the distribution of project information; â¢ workflow management for the control of the flow of information and work in the VE. The VEGA platform relies on high level open standards for the three technologies listed above, including STEP and particularly EXPRESS for the neutral specification of product model data, CORBA for communication between distributed applications and distribution of objects over networks, workflow technology as defined by the WfMC 12 for design of process control, and information standards like SGML 13 or various WEB de facto standards for the support of valueadded distributed information services (exchange of administrative messages, document handling, presentation of information). Thus, VEGA is currently elaborating the fundamental grounds for distributed architectures, defining a service layered on top of CORBA (the COAST â COrba Access to STEP models [2]), for remote data access and manipulation of information models defined by explicit meta-models (or schemata) satisfying the STEP EXPRESS semantics. 2.2 STEP and the IAI If different software applications need to communicate and inter-operate, they first need to share the same information, without misunderstanding or loss of semantics. This implies a common way to represent and exchange the semantics. Such issues have led to dramatic research efforts to achieve effective product data exchanges, standardisation of methodologies, languages and technologies, especially in the context of PDT , among them: â¢ STEP ([3], [4]), developed in ISO TC184/SC4 16 for the product data representation and exchange. It allows the expression in a uniform and complete way of all the information required for a product life-cycle (especially through the EXPRESS language [5]), and supplies means for exchanging data physical files [6] and sharing product databases [7] with models and applications independent mechanisms. STEP is today deeply used for real world product information modelling, communication and interpretation. â¢ IFC [8], another major effort currently undertaken by a non-profit alliance (IAI) of the building industry including architects and engineers, building clients, software vendors, and so on. The main IAI objective is to specify the IFC as a universal model to be a basis for collaborative work in the building industry and consequently to improve communication, productivity, delivery time, cost, and quality throughout the design, construction, operation and maintenance life cycle of buildings. STEP and IAI share the same goals, i.e. application interoperability; data exchange and actor cooperation, but differ in their respective processes. The IAI promote a bottom-up approach, with an iterative and incremental development for fast implementation. STEP is a long-term project, with a top-down approach and is concerned with broad standardisation. The IAI, having a formal liaison status with STEP has partially adopted the STEP technology, mainly through an EXPRESS representation of the IFC. In the future, an integration of the IFC within STEP is planned. As an initiative driven by leading companies, the IAI is pushing the IFC as a de facto standard in the Building industry in a very near future. A key point of the VEGA infrastructure is to use PDT and especially EXPRESS to describe and store meaningful product information. Implementing the VEGA vision requires a tight coupling between STEP models and all the various components of the VEGA platform (including distribution layer, workflow, data storage). Eventually, the current IFC 1.5 release will be used in the final VEGA demonstration . 2.3 The CORBA standard CORBA ([9], [10], [11]) is an OMG specification for application interoperability and portability in distributed architectures, allowing objects described in any language to be shared across heterogeneous operating system",oceanology,73,unknown
1890380a9e14e0a82dc105c9b8ce251107af9ddf,to_check,semantic_scholar,,2020-01-01 00:00:00,semantic_scholar,2nd cfp: 7th ieee international conference on self-adaptive and self-organizing systems (saso2013),https://www.semanticscholar.org/paper/1890380a9e14e0a82dc105c9b8ce251107af9ddf,"submission: May 3, 2013 Paper submission: May 10, 2013 Notification: June 21, 2013 Camera ready copy due: July 19, 2013 Early registration: August 21, 2013 Conference: September 9-13, 2013 ------------------------------Topics of Interest ------------------------------The topics of interest to SASO include, but are not limited to: Self-* systems theory: theoretical frameworks and models; biologicallyand socially-inspired paradigms; inter-operation of self-* mechanisms; Self-* systems engineering: hardware, software and middleware development frameworks and methods, platforms and toolkits; self-* materials; Self-* system properties: robustness, resilience and stability; emergence; computational awareness and self-awareness; reflection; Self-* cyber-physical and socio-technical systems: human factors and visualization; self-* social computers; crowdsourcing and collective awareness; Applications and experiences of self-* systems: cyber security, transportation, computational sustainability, big data and creative commons, power systems. --------------------------------------Submission Instructions --------------------------------------All submissions should be 10 pages and formatted according to the IEEE Computer Society Press proceedings style guide and submitted electronically in PDF format. Please register as authors and submit your papers using the SASO 2013 conference management system. The proceedings will be published by IEEE Computer Society Press, and made available as a part of the IEEE digital library. Note that a separate call for poster submissions has also been issued. ---------------------------Review Criteria ---------------------------Papers should present novel ideas in the cross-disciplinary research context described in this call, clearly motivated by problems from current practice or applied research. We expect both theoretical and empirical contributions to be clearly stated, substantiated by formal analysis, animation or simulation, experimental evaluations, comparative studies, and so on. Appropriate reference must be made to related work. Because SASO is a cross-disciplinary conference, papers must be intelligible and relevant to researchers who are not members of the same specialized sub-field. Authors are also encouraged to submit papers describing applications. Application papers are expected to provide an indication of the real world relevance of the problem that is solved, including a description of the deployment domain, and some form of evaluation of performance, usability, or comparison to alternative approaches. Experience papers are also welcome but they must clearly state the insight into any aspect of design, implementation or management of self-* systems which is of benefit to practitioners and the SASO community. ---------------------------Program Chairs ---------------------------Tom Holvoet KU Leuven, Belgium Jeremy Pitt Imperial College London, England Ichiro Satoh National Institute of Informatics, Tokyo, Japan ? CFP RSP-2013 at ESWeek CfP: Workshop on Model-Driven and Agile Engineering for the Web (MDWE) @ ICWE 2013 ? Calls for Papers CPS Domains Architectures Secure Control Systems Multi-models Communication Embedded Software Model Integration Platforms Systems Engineering Modeling Science of Security Transportation CPS Technologies Announcement",oceanology,74,unknown
5516eea2b00bba86118be6f90d7c678b4c022226,to_check,semantic_scholar,,2020-01-01 00:00:00,semantic_scholar,developments in the united kingdom road transport from a smart cities perspective,https://www.semanticscholar.org/paper/5516eea2b00bba86118be6f90d7c678b4c022226,"Purpose: Smart city is a city which functions in a sustainable and intelligent way, by integrating all of its infrastructures and services in a cohesive way using intelligent devices for monitoring and control, to ensure efficiency and better quality of life for its citizens. As other countries globally, UK is keen for economic development and investment in smart city missions to create interest in monetary environment and inward investment. This paper explores the driving forces of smart road transport transformation and implementation in the UK. Design/methodology/approach: The study involved interviews with sixteen professionals from the UK road transport sector. A semi-structured interview technique was used to collect expertsâ perception, which was then examined using content analysis. Findings: The results of the study revealed that the technological advancement is a key driver. The main challenges faced for the implementation of smart city elements in the UK road network are: lack of investment; maintenance; state of readiness and the awareness of the smart road transport concept. The study concludes that an understanding of the concept of smart cities from a road transport perspective is very important to create awareness of the benefits and the way it works. A wider collaboration between every sector is crucial to create a successful smart city. Originality/value: The study contributes to the field of digitalisation of road transport sector. This paper reveals the key driving forces of smart road transport transformation, the current status of smart road transport implementation in UK and challenges of the smart road transport development in the UK. Engineering, Construction and Architectural Management http://mc.manuscriptcentral.com/ecaam 2 Introduction Enormous global urbanisation and growth has caused migration of people in urban areas and spatial development of urban infrastructure. According to Obaidat and Nicopolitidis (2016), 85-90% of the world population evolution is a result of a 21 st century cities. Therefore, smart cities are being created from scratch or being developed gradually by improving the prevailing cities infrastructure and primary resources. A study of McKinsey Global Instituteâs by Department for Business innovation and Skills (2013) shows that more than 50% of the global GDP (Gross Domestic Product) is generated in the 190 major cities in the developed countries compare to 22 largest cities in the developing countries. However, the increase of growth in the developed countries is healthy but this phenomena also set a high expectation on public services and the quality of the urban infrastructure and environment. Due to the urbanisation, more problems such as overpopulation, pollutions, scarcity of natural resources, public and private services are being created (Yigitcanlar et. al. 2020, Dameri, 2014). Smart cities are an emerging strategy to mitigate the problems generated by the rapid urban population growth and rapid urbanization (Prakash, 2019). A âsmart cityâ is defined as an urban area that is highly innovative in terms of overall facilities, ecological real estate, communication and market feasibility (Chirisa et al, 2016). Also, smart cities is defined as âa place where the traditional networks and services are made more efficient with the use of digital and telecommunication technologies, for the benefits of its inhabitants and businessesâ (Kumar et al., 2018). Whereas BSI (2014) noted that smart cities as an effective integration of physical, digital and human systems in a built environment to deliver sustainable, prosperous and inclusive future for its citizens. From the above three definitions, it could be inferred that smart cities are cities that make use of physical, digital and human systems in order to enable sustainability and efficiency for the citizens within that city. The economic growth of any country is supported by its good infrastructure. The United Kingdom (UK) has strategic roads, railway and airports; however, it is one of the top 10 most congested country in the world (Korosec, 2018). According to ONS (2017), the population in the UK in 2016 was 65.6 million which was the largest ever. It is also projected that the population in the UK would grow, reaching over 74 million by 2039. Due to the population rise amalgamated with increase of cars on the road, which has increased by 4.6% higher than the previous peak in 2016 (Department of Transport, 2017), the present UK transport system faces many challenges. The UK road transportation system is gradually taking steps to overcome the problems. As road transport is a significant source of both safety and environmental concerns. This research aims to explore the driving forces of smart road transport transformation, and implementation in the UK along with the challenges faced. In doing so the next section delves into the relevant literature review followed by methodology and findings. The paper finally concludes with recommendations. 2. Literature review An extensive review of literature on drivers, current status of smart cities and the challenges are discussed in this section. Three main drivers include: technology, environment, and socioeconomy (See Table 1). The technological advancement is clearly seen as strong impact on the cities in the recent years. It can be seen clearly that, the communication technologies (ICT) develops city management, enhances technical and social networks, and urban affordability. Within the technology as a driver, the review of literature revealed that technologies such as Big Data, Internet of Things and Artificial Intelligence are widely Engineering, Construction and Architectural Management http://mc.manuscriptcentral.com/ecaam 3 implemented within the smart cities context. However, there is a lack of studies focusing on the UK smart road transport sector. Table 1: Literature on classification of drivers of smart cities development Drivers Description Source Technology Big data Big volume of digital data contents through online services such as Facebook, Twitter, You Tube, Instagram and SnapChat Olshannikova et al , 2017 Data is transmitted to various smart applications through sensor devices or other cloud computing integrated devices Hashem et al , 2016 Big data development highlights information and communication tools in the cyber physical farm management cycle and it identifies the interconnection related to socio-economic challenges Wolfert et al , 2017 Internet of Things IoT is widely in use for many multimedia application, smart transportation system and smart city design and deployment issues KeertiKumar et al , 2016 The implementation of IoT in transportation system means underlying technology and creating smart environments to increase their efficiency in tackling the transportation and environmental issues Kyriazis et al , 2013 Artificial Intelligence Artificial Intelligence (AI) is a technology that is influencing every pace of life which enable people to reconsider how to integrate information, analyse data, and real time data usage for the purpose of improve decision making West and Allen , 2018 Artificial neutral networks, expert system and hybrid intelligent system are computer based modelling tools that have recently aroused and found extensive recognition in many discipline for modelling complex real-world problem. Bahrammirzaee , 2010 China is a leading competitor and primarily focusing in the use of AI in military vehicles. While, Russia actively looking for opportunity in AI development and focused on robots in military Hoadley and Lucas , 2018 Environment Songdo, the Korean model of smart city was subjected to become one of the sustainable smart cities around the world Yigitcanlar , et al, 2018 Songdo, consist 40% of parks and green spaces and waste processing centre placed by the underground system and to recycle Benedikt , 2016 ; Shwayri , 2013 European Union aimed to reduce greenhouse gas emission in urban design through the implementation of innovation technologies Ahvenniemi et al , 2017 Engineering, Construction and Architectural Management http://mc.manuscriptcentral.com/ecaam 4 Cities have been setting high expectation on reaching the target of creating a clean future as shared by Covenant of Mayorsâ vision for 2050 to accelerate the decarbonisation Covenant of Mayors for Climate & Energy , 2018 Socioeconomy By 2050, six hundred cities will generate almost 65% of world economic growth by contribute to a higher global GDP Dobbs et al, 2012 Smart cities indicated as the next evolution of ânew community management where urban problems converted into opportunities for business investment and profit earning Anand and Marcco , 2018 Global smart cities market size to grow reaching USD 2.57 trillion by 2050 Grand View Research , 2018 Governments are obligated to protect citizen and their control over the active connection of local public groups, the police force, and the citizen such as the senior and disabled Neirotti et al ,2014 Level of observing has been focused by an increasing safety and security that desires to manage risks Kitchin , 2014 Safety and security features implemented with help of big data and data controls centres that joined and bind data stream collectively for example the installation of CCTV and street monitored camera are to monitor activity remotely Firmino and Duarte, 2014 Big data is a trend of utilising software tools to store data and share data collected with the use of technology. Nowadays, big data has been a tool that facilitating individual, businesses and government to discover new solutions to certain problems. Data is a crucial aspect as when an asset is built, asset management goes on and the more data collected in, the asset is being constructed, the more the asset can be maintained in an efficient manner (Loshin, 2018). Furthermore, KeertiKumar et al (2016) mention that IoT is widely used for many multim",oceanology,75,unknown
877faa6486db570bdbe7aa24d5b40cac6017843d,to_check,semantic_scholar,,2016-01-01 00:00:00,semantic_scholar,software and system engineering for cyber-physical systems: technical challenges and collaboration opportunities,https://www.semanticscholar.org/paper/877faa6486db570bdbe7aa24d5b40cac6017843d,"of presentations Holger Pfeifer (FORTISS) â The European Smart Anything Everywhere initiative and funding opportunities by CPSE-Labs experiments The European âSmart Anything Everywhereâ (SAE) initiative supports the innovation on smart digital systems thanks to networks of competence centres. The ecosystems built under these initiatives are based on collaboration between researchers, large industries and SMEs which will help to transfer knowledge and resources available to a much wider group of companies. SMEs and middle size companies can experiment with new technologies, try them out in their processes and work together with the suppliers of the technology to adapt it to their specific needs. CPSELabs is one SAE innovation action which provides an open forum for sharing platforms, architectures and SW tools for the engineering of dependable and trustworthy CPS. It provides funding for focussed experiments (36 partners) and fast-track (12-18 months) with innovation objective. Next call for experiment will be published in Spring 2016 http://www.cpse-labs.eu/calls.php Holger Pfeifer (FORTISS) CPSE-Labs experiments of Germany South centre: Model-driven engineering for industrial automation systems The importance of software in industrial automation is continuously increasing. New approaches to the development and maintenance are needed to cope with the growing complexity of control software for future automation systems. 4DIAC Framework for Distributed Industrial Automation & Control is an open source solution for the programming of programmable logic controllers (PLCs) according to the standard IEC 61499. With this standard it provides higher level modelling means and better support for networked control devices. The main components of 4DIAC are the Eclipse-based integrated development environment 4DIAC-IDE and the real-time capable run-time environment FORTE. Martin Grimheden (KTH) â CPSE-Labs experiments of Sweden centre: Overcoming thresholds for data integration in CPS engineering environments The talk will describe the Kth model-based approach to data integration based on the OSLC interoperability standard. Patrick Leserf (ESTACA) Trade-off analysis with SysML and Papyrus : a drone application Obtaining the set of trade-off architectures from a SysML model is an important objective for the system designer. To achieve this goal, we propose a methodology combining SysML with the variability concept and multiobjectives optimization techniques. An initial SysML model is completed with variability information to show up the different alternatives for component redundancy and selection from a library. The constraints and objective functions are also added to the initial SysML model, with an optimization context. Then a representation of a constraint satisfaction problem (CSP) is generated with an algorithm from the optimization context and solved with an existing solver. The presentation will illustrate our methodology by designing an Embedded Cognitive Safety System (ECSS). From a component repository and redundancy alternatives, the best design alternatives are generated in order to minimize the total cost and maximize the estimated system reliability. BenoÃ®t Combemale (IRISA) Using models for a broader engagement in smart systems. Various disciplines use models for different purposes. While engineering models, including software engineering models, are often developed to guide the construction of a non-existent system, scientific models, in contrast, are created to better understand a natural phenomenon (i.e., an already existing system). An engineering model may incorporate scientific models to build a smart system. This talk proposes a vision promoting an approach that synergistically combines engineering and scientific models to enable broader engagement of end users in smart systems, informed decision-making based on more-accessible scientific models and data, and automated feedback to the engineering models to support dynamic adaptation of smart systems. To support this vision, we identify a number of challenges to be addressed with particular emphasis on the socio-technical benefits of modeling. Claire Ingram (Newcastle University) CPSE-Labs experiments of UK centre: Pragmatic techniques for modelbased Engineering of Cyber-Physical Systems Newcastle University's Cyber-Physical Systems Lab conducts research into pragmatic techniques for model-based engineering of Cyber-Physical Systems (CPSs). In this talk I will introduce some platforms supported by Newcastle's CPS Lab, including an approach for co-modelling which allows separate design teams working with discrete-event and continuous-time formalisms to develop CPS designs collaboratively. I will also introduce an experiment which has been funded previously under the CPSE Labs initiative. Michael Siegel (OFFIS) CPSE-Labs experiments of Germany North centre: The Maritime Architecture Framework (MAF) and eMaritime Integrated Reference Platform (eMIR) The Maritime Architecture Framework (MAF) is a stakeholder-oriented CPS architecture framework for existing and future maritime CPS and services. MAF provides the conceptual basis, methods, tools and technologies to define, document, and align existing or future CPS architectures and architectural reference models for e-navigation and e-maritime applications. The MAF is a key enabler in the maritime domain for system harmonization, interoperability and standardization. The MAF is also a reference for the definition and design of testbeds for enavigation and e-maritime systems and services. It helps to define the context, to check completeness and provides a semantic basis to discuss the outcome and results. Additionally it offers a semantic basis for integration of testbeds e.g. for larger demonstrators. To support the development of maritime CPS â i.e. the integration of heterogeneous systems of the maritime transportation space , the Design Centre North provides the eMaritime Integrated Reference Platform (eMIR) for rapid prototyping in simulation environments and testing in real world environments. This talk gives an overview about the background, context and concepts of the MAF and why testbed environments (e.g. eMIR) for the development, integration testing and demonstration for CPS must take into account and support the design aspects of such an architecture framework. Andre Pierre Mattei (SCA-ITA) SysML Design of an observation satellite for agriculture surveillance in Brazil FranÃ§ois Fouquet (SnT, Interdisciplinary Centre for Security, Reliability and Trust)Models for managing IoT data Internet of Things applications analyze our past habits through sensor measurements to anticipate future trends. To yield accurate predictions, intelligent systems not only rely on single numerical values, but also on structured models aggregated from different sensors. Computation theory, based on the discretization of observable data into timed events, can easily lead to millions of values. Time series and similar database structures can efficiently index the mere data, but quickly reach computation and storage limits when it comes to structuring and processing IoT data. During this talk, I will present various results presented at Modelsâ15 and SmartGridComâ15 that tackles this complexity by exploiting IoT data characteristics. Notably, I will present a concept of continuous models that can handle high-volatile IoT data by defining a meta type for continuous attributes. In addition to traditional discrete object-oriented modeling APIs, we enable models to represent very large sequences of sensor values by inferring mathematical models that can efficiently replace raw values. We show on various IoT datasets that sequences of polynomials can significantly improve storage and reasoning efficiency. I will present an application of this IoT model extension for suspicious value detection in the SmartGrid domain. We proposed a method to learn and store a profile of âtypicalâ values and their probability in IoT context models. We show that using such profiles together with a contextual checker we can improve suspicious value detection, both in terms of efficiency and effectiveness. Juan Garbajosa (UPM) Experiments of Spain centre: Open CPS platform for building and deploying smart city services Bran Selic (Simula) Modeling uncertainty in cyber-physical systems For the past year, we have been working on a core model of Uncertainty and its application to requirements specification and system testing in the context of the European Commission's H2020 UTEST project. (More information on this project, which involves a number of industrial and research partners from Europe, can be found at: http://certus-sfi.no/u-test-a-horizon-2020-funding-recipient/). Fabien Peureux Fabien Peureux (Femto-st/EGM/Smartesting S&S) Model-Based Testing for Internet of Things and Cyber-Physical Systems Testing Cyber-Physical Systems (CPS) is challenging due to the various uncertainties in their behaviour. The purpose of this talk is to present our ongoing work on a model-based testing framework for automatic generation of executable test cases for CPS in the presence of various uncertainties. Basically, uncertainties can be described as a lack of certainty about the current state or about the future outcome of the system. Within CPS, it can be due to the stimulus and data sent from the user environment to the physical units (application level), to the interactions between the physical units and the network services (infrastructure level), or a combination of the both (integration level). To test such issues, the proposed model-based testing approach is implemented on the EMF framework, and based on the test generation tool Smartesting CertifyIt1. It relies on a UML behavioral model of the system under test, from which abstract test cases are automatically generated by applying dedicated coverage strategies focusing on uncertainty testing. Afterwards, ",oceanology,76,unknown
30f346032407b43e811107a46bf07a38f30527d7,to_check,semantic_scholar,,2005-01-01 00:00:00,semantic_scholar,the build master: microsoft's software configuration management best practices,https://www.semanticscholar.org/paper/30f346032407b43e811107a46bf07a38f30527d7,"""Wow, what can I say? Chapter 4, 'The Build Lab and Personnel,' by itself is enough justification to purchase the book! Vince is obviously a 'Dirty Finger Nails' build meister and there is a lot we can all learn from how he got them dirty! There are so many gems of wisdom throughout this book it's hard to know where to start describing them! It starts where SCM should start, at the end, and works its way forward. This book is a perfect complement to the 'Follow the Files' approach to SCM that I espouse. I will recommend that every software lead and software configuration management person I work with be required to read this book!""-Bob Ventimiglia, autonomic logistics software configuration manager, Lockheed Martin Aeronautics""The Build Master contains some truly new information; most of the chapters discuss points that many people in the industry don't have a full understanding of and need to know. It's written in a way that is easy to read and will help a reader fill holes in their vision regarding software build management. I especially liked Vince's use of Microsoft stories to make his points throughout the book. I will purchase the book and make certain chapters mandatory reading for my build manager consultants.""-Steve Konieczka, SCM consultant""Vince does a great job of providing the details of an actual working build process. It can be very useful for those who must tackle this task within their own organization. Also the 'Microsoft Notes' found throughout the book provide a very keen insight into the workings of Microsoft. This alone is worth purchasing this book.""-Mario E. Moreira, author of Software Configuration Management Implementation Roadmap and columnist at CM Crossroads""Software configuration management professionals will find this book presents practical ideas for managing code throughout the software development and deployment lifecycles. Drawing on lessons learned, the author provides real-world examples and solutions to help you avoid the traps and pitfalls common in today's environments that require advanced and elegant software controls.""-Sean W. Sides, senior technical configuration manager, Great-West Healthcare Information Systems""If you think compiling your application is a build process, then this book is for you. Vince gives us a real look at the build process. With his extensive experience in the area at Microsoft, a reader will get a look in at the Microsoft machine and also how a mature build process should work. This is a must read for anyone doing serious software development.""-Jon Box, Microsoft regional director, ProTech Systems Group""Did you ever wonder how Microsoft manages to ship increasingly complex software? In The Build Master, specialist Vince Maraia provides an insider's look.""-Bernard Vander Beken, software developer, jawn.net""This book offers an interesting look into how Microsoft manages internal development of large projects and provides excellent insight into the kinds of build/SCM things you can do for your large-scale projects.""-Lance Johnston, vice president of Software Development, SCM Labs, Inc.""The Build Master provides an interesting insight into how large software systems are built at Microsoft covering the set up of their build labs and the current and future tools used. The sections on security, globalization, and versioning were quite helpful as these areas tend to be overlooked.""-Chris Brown, ThoughtWorks, consultant""The Build Master is a great read. Managing builds is crucial to the profitable delivery of high-quality software. Until now, the build process has been one of the least-understood stages of the entire development lifecycle. This book helps you implement a smoother, faster, more effective build process and use it to deliver better software.""-Robert J. Shimonski, Networking and Security Expert, http://www.rsnetworks.netThe first best-practice, start-to-finish guide for the software build processManaging builds is crucial to the profitable delivery of high-quality software; however, the build process has been one of the least-understood stages of the entire development lifecycle. Now, one of Microsoft's leading software build experts introduces step-by-step best practices for maximizing the reliability, effectiveness, timeliness, quality, and security of every build you create.Drawing on his extensive experience working with Microsoft's enterprise and development customers, Vincent Maraia covers all facets of the build process-introducing techniques that will work on any platform, on projects of any size. Maraia places software builds in context, showing how they integrate with configuration management, setup, and even customer support. Coverage includes How Microsoft manages builds: process flows, check-in windows, reporting status, and more Understanding developer and project builds, pre- and post-build steps, clean builds, incremental builds, continuous integration builds, and more Choosing the right build tools for your projects Configuring source trees and establishing your build environment-introducing Virtual Build Labs (VBLs) Planning builds for multiple-site development projects or teams Determining what should (and shouldn't) be kept under source control Managing versioning, including build, file, and .NET assembly versions Using automation as effectively as possible Securing builds: a four layer approach-physical, tracking sources, binary/release bits assurance, and beyondBuilds powerfully impact every software professional: developers, architects, managers, project leaders, configuration specialists, testers, and release managers. Whatever your role, this book will help you implement a smoother, faster, more effective build process-and use it to deliver better software.Â© Copyright Pearson Education. All rights reserved.",oceanology,77,unknown
e349556d5302749ee79643792e60b192020a42b2,to_check,semantic_scholar,,2009-01-01 00:00:00,semantic_scholar,special issue on wireless sensor network: theory and practice,https://www.semanticscholar.org/paper/e349556d5302749ee79643792e60b192020a42b2,"Wireless sensor network (WSN) is an emergent multi-disciplinary science, and it may be considered as the foundation of pervasive computing, mobile computing and wearable computing. WSN is a very active and competitive research area due to its diverse and unlimited potential applications: air, underground and underwater. In spite of its young age, economic impact of WSN is important, for examples the industrial control segment market will be worth $5.3B by 2010 and the smart home market will be worth $2.8 billion worldwide by 2012 (Source: Stamatis Karnouskos, EU-US 08 Workshop). WSN is a set of wireless nodes. On one hand, each wireless node (WN) has similar hardware and software functionalities as a PC: CPU, memory, operating system, and communication protocol to fulfill a specific task. On the other hand, a WN has a limited power supply (embedded battery) and consumes approximately 1 million less power (~100Î¼W instead of ~100W) than a PC. Due to resources constraints: energy consumption and form factor, the approaches applied in general purpose computer systems are not adapted to the requirements of WSN. When it comes to the design of energy efficient oriented hardware and software components of WSN, cross-layering optimization approaches are generally adopted such as application-specific unified hardware and software by taking into account the following criteria: trade-off between complexity, efficiency and resource consumption, and application context (context-aware) etc. Currently two main hardware development and design trends are carried out to implement the WN: Commercial OffThe-Shelf âCOTSâ and System on Chip âSoCâ. The first and second generations of WN were designed by using low power 8-bit or 16-bit microcontroller processor core, Bluetooth and non standard wireless access medium (MICA Mote). The current trend of WN design such as Tmotesky, iMote and LiveNode are based on low power 16-bit or 32-bit RISC microcontroller, and full compliance IEEE802.14.5 standard. However the ultimate goal of all the researchers in the world is the implementation of long life, low cost and invisible WN integrated and embedded into environment. Three key technologies make possible to achieve this objective: MEMS âMicroElectroMEchanical systemsâ, UWB âUltra-Wide Bandâ and low power CMOS technology. Different WN prototypes are realized by Intel (iMOTE2), University of Michigan (MOTE: Michigan Uni Prototype) and University of Berkeley (Pico-Mote). WN hardware seems easier to solve than embedded software for diverse WSN applications. The main questions which are related to WSN basic software design is how to keep modularity, high level abstraction and reliability to enable to implement complex massively distributed WSNs to meet resource constraint requirements. Real-time operating system (RTOS) plays a key role to support high level abstraction and distributed collaborative processing. Currently four categories of WSNâs RTOS are developed: Event driven (TinyOS), Multitask (RETOS, tKernel, NutOS, MANTIS), Data-Centric (AmbientRT), and Hybrid (Contiki, LIMOS). Note that TinyOS is very popular but it not adapted to complex hard real-time application. The development challenges of the WSN RTOS are energy-efficiency (context aware, configurable, small footprint), robustness, fault tolerance, support hard real-time constraint, and support component based model (high level of abstraction to ease the integration of high level SW such as protocol, middleware, application, and simulator). Furthermore, for WSN applications, message sending is energy consuming. Thus it is important to implement embedded energy efficient wireless routing protocol to increase WSN lifetime. It is clear that general purpose MANET routing protocol such as AODV (active), OLSR (proactive) and ZBR (hybrid) etc. are not suitable for WSN due to resource constraints. For example optimal routing path is well adapted to general purpose MANET but not suitable for WSN because the repetitive use of the same path will exhaust the battery of WNs belonging to the optimal routing path (black hole). Many WSN dedicated protocols are implemented (spin, cougar, gear, leach, speed ...) but it is currently very difficult to have a clear idea concerning their performance (energy consumption, scalability, connectivity, lifetime ...) because of the lack of large scale WSN real world experimentation results and because the simulation model does not reflect the real-world ones (physical layer). Note that, there is no standard scenario and the application program (with a known number of WNs) enables to evaluate rationally the performance of wireless routing protocols. In addition routing protocol relies on the WSN topology. On one hand, an optimal WSN topology facilitates the implementation of routing and administration protocols. On the other hand, the deployment of large scale WS nodes in a large area is random and its topology is a priori unknown. Then, it is important to investigate the auto-configuration algorithms to increase the efficiency of routing and administration protocols. However the frontier between the administration protocol and the routing protocol is not as clearly defined as in a classical network (e.g. TCP/IP and SNMP) due to cross-layering approach. Moreover WSN security, reliability, and fault tolerance are still an open problem. In this special issue 5 papers are selected among 40 submitted papers for the NTMS workshop on wireless sensor network: theory and practice, held at Tangier at Morocco in 2008, the rest of the papers are selected from an open call for paper. WSN is a multi-disciplinary science. It impossible to present all its topics but this special issue addresses most of the WSN embedded software problems dealing with real-world applications (EU NeT-ADDED FP6 project, French ANR research project and industrial projects). JOURNAL OF NETWORKS, VOL. 4, NO. 6, AUGUST 2009 379",oceanology,78,not included
368687003560b21e53865cd604aae8c00dc62c4b,to_check,semantic_scholar,2009 6th IEEE Consumer Communications and Networking Conference,2009-01-01 00:00:00,semantic_scholar,passiton: an opportunistic messaging prototype on mobile devices,https://www.semanticscholar.org/paper/368687003560b21e53865cd604aae8c00dc62c4b,"With the increasing popularity of mobile handheld devices and the growing capability of these devices, it is becoming possible that information sharing/dissemination is carried out through human networks, as a complement to the traditional computer networks. In such human networks, people come across one another, while their mobile devices exchange and store information in a spontaneous and transparent way. Such an encounter could be established through direct device-todevice connectivity when two devices come into each otherâs communication range, or be enabled by, e.g., a Wi-Fi access point, when the devices both enter its coverage. A new form of dissemination, which we call opportunistic messaging, is such an application that is based on human encounters and mobilities. When human encounters are exploited for communications, the reliance on network infrastructure access is eliminated; communications can be performed even where infrastructure is absent or infrastructure access is intermittent. By leveraging human mobilities, data delivery does not require an end-to-end path from the source to a recipient; instead, people carrying mobile devices serve as relays â they cache othersâ data and forward/deliver the data when appropriate. Thus, the propagation of information is tied to peopleâs physical proximity when they move around, and incorporates the social aspects of communications as people tend to spend more time co-locating with their social relations. Opportunistic messaging is applicable anywhere, and is especially appealing where network infrastructure access is limited or intermittent (e.g., on cruise ships, in national parks, after disasters). Another intriguing characteristic of it is its ease of deployment â no central server is needed, but only a single piece of software on usersâ mobile devices. However, as human encounters and mobilities are unpredictable, when used for social applications, opportunistic messaging is most suited for disseminating user-generated information that is non-formal, less important, and thus not time-critical. In recent years, a considerable amount of efforts have been invested in the research on opportunistic networking and delay-tolerant networking (which encompasses opportunistic networking but is a broader concept). A large portion of prior work has focused on routing issues, e.g., through whom as intermediate carriers to deliver a message to the destinations [1] [2] [3] [4]. The routing issues have been further explored in various contexts, such as in vehicular networks [5] [6] [7] and in social networking applications [8] [9] [10] [11]. However, serious real-world application development, deployment and evaluation of the opportunistic networking concepts still fall behind [12], in which many challenging issues remain to be addressed (to name a few, location-awareness, user incentives and preferences, power preservation, encounter controls, etc.). In this work, we design and prototype PassItOn, a fully distributed opportunistic messaging system. Our goal is to build up a proof-of-concept platform on real mobile devices, and thus show the feasibility and potentials of utilizing human movements for dissemination applications. Meanwhile, we seek to shed lights on the design, implementation and deployment issues in building such systems, and thus stimulate new ideas and perspectives on addressing these issues. Moreover, we aim to offer a real testbed on which new mechanisms, protocols and use cases can be tested and evaluated.",oceanology,79,unknown
d0270dcfa7ca058cea59512b832be6c91408676f,to_check,semantic_scholar,Scalable Comput. Pract. Exp.,2005-01-01 00:00:00,semantic_scholar,challenges concerning symbolic computations on grids,https://www.semanticscholar.org/paper/d0270dcfa7ca058cea59512b832be6c91408676f,"Challenges concerning symbolic computations on grids Symbolic and algebraic computations are currently ones of fastest growing areas of scientific computing. For a long time, the numerical approach to computational solution of mathematical problems had an advantage of being capable of solving a substantially larger set of problems than the other approach, the symbolic one. Only recently the symbolic approach gained more recognition as a viable tool for solving large-scale problems from physics, engineering or economics, reasoning, robotics or life sciences. Developments in symbolic computing were lagging relative to numerical computing, mainly due to the inadequacy of available computational resources, most importantly computer memory, but also processor power. Continuous growth in the capabilities of computer hardware led naturally to an increasing interest in symbolic calculations and resulted, among others things, in development of sophisticated Computer Algebra Systems (CASs). CASs allow users to study computational problems on the basis of their mathematical formulations and to focus on the problems themselves instead of spending time transforming the problems into forms that are numerically solvable. While their major purpose is to manipulate formulas symbolically, many systems have substantially extended their capabilities, offering nowadays functionalities like graphics allowing a comprehensive approach to problem solving. While, typically, CAS systems are utilized in an interactive mode, in order to solve large problems they can be also used in a batch mode and programmed using languages that are close to common mathematical notation. As CASs become capable of solving large problems, they follow the course of development that has already been taken by numerical software: from sequential computers to parallel machines to distributed computing and finally to the grid. It is particularly the grid that has the highest potential as a discovery accelerator. Currently, its widespread adoption is still impeded by a number of problems, one of which is difficulty of developing and implementing grid-enabled programs. That it is also the case for grid-enabled symbolic computations. There are several classes of symbolic and algebraic algorithms that can perform better in parallel and distributing computing environments. For example for multiprecision integer arithmetic, that appears among others in factorizations, were developed already twenty years ago systolic algorithms and implementations on massive parallel processors, and more recently, on the Internet. Another class that utilize significant amount of computational resources is related to the implementations of polynomial arithmetic: knowledge based algorithms such as symbolic differentiation, factorization of polynomials, greatest common divisor, or, more complicated, Groebner base computations. For example, in the latest case, the size of the computation and the irregular data structures make the parallel or distributed implementation not only an attractive option for improving the algorithm performance, but also a challenge for the computational environment. A third class of algorithms that can benefit from multiple resources in parallel and distributed environments is concerning the exact solvers of large systems of equations. The main reason driving the development of parallel and distributed algorithms for symbolic computations is the ability to solve problems that are memory bound, i.e. that cannot fit into memory of a single computer. An argument for this statement relies on the observation that the input size of a symbolic or algebraic computation can be small, but the memory used in the intermediate stages of the computation may grow considerably. Modern CASs increase their utility not only through new symbolic capabilities, but also expending their applicability using visualization or numerical modules and becoming more than only specific computational kernels. They are real problem solving environments based on interfaces to a significant number of computational engines. In this context it appears also the need to address the ability to reduce the wall-clock time by using parallel or distributed computing environment. A simple example is the case of rendering the images for a simulation animation. Several approaches can be identified in the historical evolution of parallel and distributed CASs: developing versions for shared memory architectures, developing computer algebra hardware, adding facilities for communication and cooperation between existing CASs, or building distributed systems for distributed memory parallel machines or even across Internet. Developing completely new parallel or distributed systems, although efficient, in most cases is rather difficult. Only a few parallel or distributed algorithms within such a system are fully implemented and tested. Still there are several successful special libraries and systems falling in this category: ParSac-2 system, the parallel version of SAC-2, Paclib system, the parallel extension of Saclib, FLATS based on special hardware, STAR/MPI, the parallel version of GAP, ParForm, the parallel version of Form, Cabal, MuPAD, or the recent Givaro, for parallel computing environments, FoxBox or DSC, for distributed computing environments. An alternative approach to build parallel and distributed CASs is to add the new value, the parallelism or the distribution, to an existing system. The number of parallel and distributed versions of most popular CASs is impressive and it can be explained by the different requirements or targeted architectures. For example, for Maple there are several implementations on parallel machines, like the one for Intel Paragon or ||Maple||, and several implementations on networks of workstations, like Distributed Maple or PVMaple. For Mathematica there is a Parallel Computing Toolkit, a Distributed Mathematica and a gridMathematica (for dedicated clusters). Matlab that provides a Symbolic Math Toolbox based on a Maple kernel has more than twenty different parallel or distributed versions: DP-Toolbox, MPITB/PVMTB, MultiMatlab, Matlab Parallelization Toolkit, ParMatlab, PMI, MatlabMPI, MATmarks, Matlab*p, Conlab, Otter and others. More recent web-enabled systems were proved to be efficient in number theory for finding large prime numbers, factoring large numbers, or finding collisions on known encryption algorithms. Online systems for complicated symbolic computations were also built: e.g. OGB for Groebner basis computations. A framework for description and provision of web-based mathematical services was recently designed within the Monet project and a symbolic solver wrapper was build to provide an environment that encapsulates CASs and expose their functionalities through symbolic services (Maple and Axiom were chosen as computing engines). Another platform is MapleNet build on client-server architecture: the server manages concurrent Maple instances launched to server client requests for mathematical computations. WebMathematica is a similar system that offers access to Mathematica applications through a web browser. Grid-oriented projects that involve CASs were only recent initiated. The well-known NetSolve system was one of the earliest grid system developed. Version 2 released in 2003 introduces GridSolve for interoperability with the grid based on agent technologies. APIs are available for Mathematica, Octave and Matlab. The Genss project (Grid Enabled Numerical and Symbolic Services) follows the ideas of the Monet project and intends also to combine grid computing and mathematical web services using a common agent-based framework. Several projects are porting Matlab on grids: from small ones, like Matlab*g, to very complex ones, like Geodise. Maple2g and MathGridLink are two different approaches for grid-enabled version of Maple and Mathematica. Simple to use front-end were recently build in projects like Gemlca and Websolve to deploy legacy code applications as grid services and to allows the submission of computational requests. The vision of grid computing is that of a simple and low cost access to computing resources without artificial barriers of physical location or ownership. Unfortunately, none of the above mentioned grid-enabled CAS is responding simultaneously to some elementary requirements of a possible implementation of this vision: deploy grid symbolic services, access within CAS to available grid services, and couple different grid symbolic services. Moreover a number of major obstacles remain to be addressed. Amongst the most important are mechanisms for adapting to dynamic changes in either computations or systems. This is especially important for symbolic computations, which may be highly irregular in terms of data and general computational demands. Such demands received until now relatively little attention from the research community. In the context of a growing interest in symbolic computations, powerful computer algebra systems are required for complex applications. Freshly started projects shows that porting a CAS to a current distributed environment like a grid is not a trivial task not only from technological point of view but also from algorithmic point of view. Already existing tools are allowing experimental work to be initiated, but a long way is still to be cross until real-world problems will be solved using symbolic computations on grids. Dana Petcu, Western University of Timisoara",oceanology,80,unknown
10.1109/itnt52450.2021.9649038,to_check,2021 International Conference on Information Technology and Nanotechnology (ITNT),IEEE,2021-09-24 00:00:00,ieeexplore,"digital twin of rice as a decision-making service for precise farming, based on environmental datasets from the fields",https://ieeexplore.ieee.org/document/9649038/,"In this paper a ready-to-use software component, which simulates real state of rice crop in the field and called âDigital Twin of riceâ (DT), is studied. DT uses ontology-based knowledge base of plant cultivation to execute the rules of plant growth. The software provides real-time data collection from the fields and distributed decision making to find the optimum solution in planning process of rice growth stages. Rice DT is developed as an autonomous service and can be integrated to any existing digital agricultural platform. A pilot integration with cyber-physical system (CPS) for precise farming is described in the paper. The CPS has a number of services to provide digital transformation in plant cultivation enterprises and big farms. The system performs adaptive scheduling of resources, such as fertilizers, protection agents, vehicles, personnel and finance. Results of DT implementation shows adequate decision-making of the service compared to experiments on the pilot farms. So, DT of plant could be a next step in digital transformation of agriculture, providing improvement of ROI from precision farming, automate decision-making processes for farmers and service companies and make their business smarter, more flexible, and more cost-efficient, providing better productivity of plant cultivation and sustainability of agriculture under global climate changes.",finance,81,unknown
10.1109/kcic.2018.8628512,to_check,2018 International Electronics Symposium on Knowledge Creation and Intelligent Computing (IES-KCIC),IEEE,2018-10-30 00:00:00,ieeexplore,estimating adaptive individual interests and needs based on online local variational inference for a logistic regression mixture model,https://ieeexplore.ieee.org/document/8628512/,"In real companies engaged in economic activities through transactions involving consumer items, such as retail, distribution, finance, and information materials, supplying an opportunity to customers to choose specialized items is an important factor that can improve customer satisfaction and convenience allowing their diverse and time-dependent needs to be met. However, capturing the specialized needs of customers accurately is a difficult task because their needs depend on time, context, situation, and meaning. Recently, physical computational environments have been developing rapidly, thereby allowing easy implementation to sense a customer's action and deal with it sequentially. In this paper, we propose a personalized method to predict individual interests and demands appropriately. In particular, the system learns the customers' situation, meaning, and action from their action history, and reflects a feedback of the result to predict the next action. To realize this method, we utilize the following two methodologies: the mathematical model of meaning (MMM), which is a semantic associative search technology; and the local variational inference (LVI), which is an approximation of the Bayesian inference. A numerical experiment shows that the proposed method performed better than a typical method.",finance,82,unknown
10.2498/iti.2012.0379,to_check,Proceedings of the ITI 2012 34th International Conference on Information Technology Interfaces,IEEE,2012-06-28 00:00:00,ieeexplore,model of the new sales planning optimization and sales force deployment erp business intelligence module for direct sales of the products and services with temporal characteristics,https://ieeexplore.ieee.org/document/6307985/,"Considering that direct sales today has significant share in the whole sales niche and that existing travelling salesman problem solutions still do not provide a comprehensive resolution for sales force deployment when selling products and services with temporal or periodic recurring, here we present a new method that combines business intelligence and easy-to-use graphical user interface to give the answers. Module is self-learning due to implemented statistics with tendency for more accurate sales predictions as time goes by. Presented model is applicable in various industries like telecommunications, finance, pharmaceuticals etc. and capable of solving large-scale real-world problems in real time.",finance,83,unknown
10.1109/ccwc47524.2020.9031269,to_check,2020 10th Annual Computing and Communication Workshop and Conference (CCWC),IEEE,2020-01-08 00:00:00,ieeexplore,an automated framework for real-time phishing url detection,https://ieeexplore.ieee.org/document/9031269/,"An increasing number of services, including banking and social networking, are being integrated with world wide web in recent years. The crux of this increasing dependence on the internet is the rise of different kinds of cyberattacks on unsuspecting users. One such attack is phishing, which aims at stealing user information via deceptive websites. The primary defense against phishing consists of maintaining a black list of the phishing URLs. However, a black list approach is reactive and cannot defend against new phishing websites. For this reason, a number of research have been done on using machine learning techniques to detect previously unseen phishing URLs. While they show promising results, any such implementation is yet to be seen. This is because 1) little work has been done on developing a complete end-to-end framework for phishing URL detection 2) it is prohibitively slow to detect phishing URLs using machine learning algorithms. In this work we address these two issues by formulating a robust framework for fast and automated detection of phishing URLs. We have validated our framework with a real dataset achieving 87% accuracy in a real-time setup.",finance,84,unknown
10.1109/sist50301.2021.9465891,to_check,2021 IEEE International Conference on Smart Information Systems and Technologies (SIST),IEEE,2021-04-30 00:00:00,ieeexplore,information - education milieu: methodology of forming knowledge content and practice oriented training of it-disciplines,https://ieeexplore.ieee.org/document/9465891/,"The article herein considers the innovative methodology of forming the knowledge content, based on ontological model. The basic methodology concepts are project-oriented technology of CDIO training and graduate's competence model, and implementation tool is information - education milieu. Information - education milieu allows implement adaptive, in compliance with production requirements, knowledge trend planning and knowledge content forming both for specialty degree programs disciplines and individual training trajectory, using for those aims the smart-contract.For the discipline Technologies of processing the distributed applications there have been shown examples of using the technologies on modeling the knowledge, connected with designing the real time systems of parallel and distributed applications, where as a project there has been used the banking system.The methodology's software implementation has been fulfilled in the form of a web-application and currently it is going through approbation at the chair Computer and software engineering of Turan university.",finance,85,unknown
10.1109/comitcon.2019.8862225,to_check,"2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)",IEEE,2019-02-16 00:00:00,ieeexplore,stock market analysis using supervised machine learning,https://ieeexplore.ieee.org/document/8862225/,"Stock market or Share market is one of the most complicated and sophisticated way to do business. Small ownerships, brokerage corporations, banking sector, all depend on this very body to make revenue and divide risks; a very complicated model. However, this paper proposes to use machine learning algorithm to predict the future stock price for exchange by using open source libraries and preexisting algorithms to help make this unpredictable format of business a little more predictable. We shall see how this simple implementation will bring acceptable results. The outcome is completely based on numbers and assumes a lot of axioms that may or may not follow in the real world so as the time of prediction.",finance,86,unknown
http://arxiv.org/abs/2202.02751v1,to_check,arxiv,arxiv,2022-02-06 00:00:00,arxiv,pipe overflow: smashing voice authentication for fun and profit,http://arxiv.org/abs/2202.02751v1,"Recent years have seen a surge of popularity of acoustics-enabled personal
devices powered by machine learning. Yet, machine learning has proven to be
vulnerable to adversarial examples. Large number of modern systems protect
themselves against such attacks by targeting the artificiality, i.e., they
deploy mechanisms to detect the lack of human involvement in generating the
adversarial examples. However, these defenses implicitly assume that humans are
incapable of producing meaningful and targeted adversarial examples. In this
paper, we show that this base assumption is wrong. In particular, we
demonstrate that for tasks like speaker identification, a human is capable of
producing analog adversarial examples directly with little cost and
supervision: by simply speaking through a tube, an adversary reliably
impersonates other speakers in eyes of ML models for speaker identification.
Our findings extend to a range of other acoustic-biometric tasks such as
liveness, bringing into question their use in security-critical settings in
real life, such as phone banking.",finance,87,unknown
http://arxiv.org/abs/2106.12563v2,to_check,arxiv,arxiv,2021-06-23 00:00:00,arxiv,feature attributions and counterfactual explanations can be manipulated,http://arxiv.org/abs/2106.12563v2,"As machine learning models are increasingly used in critical decision-making
settings (e.g., healthcare, finance), there has been a growing emphasis on
developing methods to explain model predictions. Such \textit{explanations} are
used to understand and establish trust in models and are vital components in
machine learning pipelines. Though explanations are a critical piece in these
systems, there is little understanding about how they are vulnerable to
manipulation by adversaries. In this paper, we discuss how two broad classes of
explanations are vulnerable to manipulation. We demonstrate how adversaries can
design biased models that manipulate model agnostic feature attribution methods
(e.g., LIME \& SHAP) and counterfactual explanations that hill-climb during the
counterfactual search (e.g., Wachter's Algorithm \& DiCE) into
\textit{concealing} the model's biases. These vulnerabilities allow an
adversary to deploy a biased model, yet explanations will not reveal this bias,
thereby deceiving stakeholders into trusting the model. We evaluate the
manipulations on real world data sets, including COMPAS and Communities \&
Crime, and find explanations can be manipulated in practice.",finance,88,unknown
10.31590/ejosat.598036,to_check,core,'European Journal of Science and Technology',2019-01-01 00:00:00,core,detection of fake websites by classification algorithms,,"GÃ¼nÃ¼mÃ¼zde kimlik avÄ± yapan sahte web sitelerinin sayÄ±sÄ± oldukÃ§a artmÄ±ÅtÄ±r. Bu web sitelerinin amaÃ§larÄ± genel anlamda kiÅilerin,kiÅisel bilgilerini ele geÃ§irerek Ã§Ä±kar saÄlamaktÄ±r. Sosyal medya hesaplarÄ±mÄ±zdaki kimlik ve parola bilgilerimiz, alÄ±ÅveriÅ sitelerindekikimlik ve adres bilgilerimiz bize ait kiÅisel bilgilerimizdir. Bu tÃ¼r bilgiler istenmeyen kiÅilerin eline geÃ§mesi durumunda, tahmin bileedemeyeceÄimiz kÃ¶tÃ¼ sonuÃ§lar doÄurabilmektedir. AyrÄ±ca online bankacÄ±lÄ±k iÅlemlerimiz gibi finansal iÅlemlerimizin Ã¶nemli birkÄ±smÄ±nÄ± internet ortamÄ±nda yapÄ±yor olmamÄ±z bu tÃ¼r sitelerden korunmamÄ±z aÃ§Ä±sÄ±ndan Ã¶nemli bir sorun teÅkil etmektedir. Bu amaÃ§laantivÃ¼rÃ¼s yazÄ±lÄ±m firmalarÄ±, tarayÄ±cÄ±lar, arama motorlarÄ± daha iyi kullanÄ±cÄ± hizmeti ve memnunniyet saÄlamak aÃ§Ä±sÄ±ndan bu tÃ¼r zararlÄ±sitelerden kullanÄ±cÄ±larÄ±nÄ± korumak iÃ§in Ã§alÄ±Åmalar yapmaktadÄ±rlar. AyrÄ±ca sahte web sayfalarÄ±nÄ±n kullanÄ±cÄ±larÄ±n Ã¶nÃ¼ne gelmeden tespitedilip engellenmesi gÃ¼nÃ¼mÃ¼z yapay zeka Ã§alÄ±ÅmalarÄ±nÄ±nda Ã¶nemli bir Ã§alÄ±Åma alanÄ± olmaktadÄ±r. HergÃ¼n milyarlarca insanÄ±n gezindiÄiinternet ortamÄ±nda bu sahte sitelerden korunmasÄ±nÄ±n en kolay yÃ¶ntemi, sahte web sayfalarÄ±nÄ±n otomatik olarak tespit edilipengellenmesi olacaktÄ±r. Makine Ã¶Ärenmesi sÄ±nÄ±flandÄ±rma algoritmalarÄ± ile bir sayfaya ait bilgilere bakarak sistem tarafÄ±ndan otomatikolarak sahte veya gerÃ§ek olarak tespit edilmesi yapay zeka Ã§alÄ±ÅmalarÄ±nÄ±n sunduÄu Ã¶nemli avantajlarÄ±n baÅÄ±nda gelmektedir. BuÃ§alÄ±Åma ile bir web sitesi adresine ait belirlenmiÅ 10 Ã¶zellik kullanÄ±larak; bu adresin sahte mi, yoksa gerÃ§ek bir adres mi olduÄu tespitedilmeye Ã§alÄ±ÅÄ±lmaktadÄ±r. ÃalÄ±Åmada kullanÄ±lan veriler Machine Learning Repository (UCI)âdan alÄ±nmÄ±ÅtÄ±r. Verilerin analizi ÃaprazEndÃ¼stri Standart SÃ¼reÃ§ Modeli(CRISP-DM) baz alÄ±narak gerÃ§ekleÅtirilmiÅtir. Veri setinde web sitelerinin durumunu belirleyen nitelik(Class, Kimlik AvÄ±=-1, ÅÃ¼pheli=0 ve MeÅru=1) olarak etiketlenmiÅtir. ÃalÄ±Åma da RStudio kullanÄ±larak R programlama dili ileanalizler yapÄ±lmÄ±ÅtÄ±r. KullanÄ±lan sÄ±nÄ±flandÄ±rma algoritmalarÄ± Rastgele Orman (RF), Destek VektÃ¶r Makineleri (SVM), J48, K-En YakÄ±nKomÅu (KNN) ve Naive Bayes algoritmalarÄ±dÄ±r. YapÄ±lan deÄerlendirmeler sonucunda Rastgele Orman algoritmasÄ± ile en yÃ¼ksekdoÄruluk performansÄ± elde edilmiÅtir.Nowadays, phishing web sites have been increased. The purpose of these sites is to obtain benefits by acquiring personal information of people in general. Our identity and password information in our social media accounts and identity and address information on shopping sites are our personal information. If such information is received by unwanted people, it can have bad unpredictable consequences. In addition, the fact that we carry out a significant portion of our financial transactions such as our online banking transactions on the internet constitutes an important problem in terms of protection from such sites. For this purpose, antivirus software companies, browsers, search engines are working to protect users from such harmful sites in terms of providing better user service and satisfaction. In addition, the detection and prevention of fake web pages before the users is an important area of work in today's artificial intelligence studies. The easiest method of protecting these fraudulent sites in the internet environment where billions of people are browsing every day will be to detect and block fake web pages automatically. Machine learning classification algorithms are automatically identified as fake or real by the system by looking at the information of a page and this is one of the important advantages offered by artificial intelligence studies. With this study, using 10 properties determined for a website address; it is attempted to determine whether this address is a fake or a real address. The data used in this study were taken from Machine Learning Repository (UCI). Data analysis was performed based on the Cross Industry Standard Process Model (CRISP-DM). In the data set, it is labeled as the attribute that determines the status of websites (Class, Phishing = -1, Suspicious = 0 and Legitimate = 1). The study was also done by using RStudio analysis with R programming language. The classification algorithms used are Random Forest (RF), Support Vector Machines (SVM), J48, K-Nearest Neighbor (KNN) and Naive Bayes algorithms. The highest accuracy performance was obtained by Random Forest algorithm",finance,89,unknown
10.32702/2307-2105-2020.10.50,to_check,core,'DKS Center',2020-01-01 00:00:00,core,ÑÐµÐ¾ÑÑÑ ÑÐ° Ð¿ÑÐ°ÐºÑÐ¸ÐºÐ° Ð·Ð°Ð±ÐµÐ·Ð¿ÐµÑÐµÐ½Ð½Ñ ÐºÑÐ±ÐµÑÑÑÑÐ¹ÐºÐ¾ÑÑÑ Ð±Ð°Ð½ÐºÑÐ²,https://core.ac.uk/download/395139328.pdf,"Ð£ ÑÑÐ°ÑÑÑ Ð´Ð¾Ð²ÐµÐ´ÐµÐ½Ð¾ Ð²Ð°Ð¶Ð»Ð¸Ð²ÑÑÑÑ ÑÐ¾ÑÐ¼ÑÐ²Ð°Ð½Ð½Ñ ÐºÐ¾Ð½ÑÐµÐ¿ÑÑÑ Ð·Ð°Ð±ÐµÐ·Ð¿ÐµÑÐµÐ½Ð½Ñ ÐºÑÐ±ÐµÑÑÑÑÐ¹ÐºÐ¾ÑÑÑ Ð±Ð°Ð½ÐºÑÐ² Ð½Ð° ÑÑÑÐ°ÑÐ½Ð¾Ð¼Ñ ÐµÑÐ°Ð¿Ñ ÑÐ¾Ð·Ð²Ð¸ÑÐºÑ ÑÐ¸ÑÑÐ¾Ð²Ð¾Ñ ÐµÐºÐ¾Ð½Ð¾Ð¼ÑÐºÐ¸ ÐºÑÐ°ÑÐ½Ð¸, Ð·Ð²Ð°Ð¶Ð°ÑÑÐ¸ Ð½Ð° Ð½ÐµÐ³Ð°ÑÐ¸Ð²Ð½Ð¸Ð¹ ÑÑÐ½Ð°Ð½ÑÐ¾Ð²Ð¸Ð¹ ÑÐ° Ð½ÐµÑÑÐ½Ð°Ð½ÑÐ¾Ð²Ð¸Ð¹ Ð²Ð¿Ð»Ð¸Ð² ÐºÑÐ±ÐµÑÐ°ÑÐ°Ðº Ð½Ð° Ð±Ð°Ð½ÐºÑÐ²ÑÑÐºÑ ÑÐ¸ÑÑÐµÐ¼Ñ ÑÐ° ÐµÐºÐ¾Ð½Ð¾Ð¼ÑÐºÑ ÐºÑÐ°ÑÐ½Ð¸ Ð² ÑÑÐ»Ð¾Ð¼Ñ. ÐÐ²ÑÐ¾ÑÐ¾Ð¼ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ñ ÑÐ·Ð°Ð³Ð°Ð»ÑÐ½ÐµÐ½Ð½Ñ Ð´Ð¾ÑÐ»ÑÐ´Ð¶ÐµÐ½Ñ Ð· ÑÑÑÑ ÑÐµÐ¼Ð°ÑÐ¸ÐºÐ¸ ÑÑÐ¾ÑÐ½ÐµÐ½Ð¾ Ð·Ð¼ÑÑÑ Ð¿Ð¾Ð½ÑÑÑÑ âÐºÑÐ±ÐµÑÑÑÑÐ¹ÐºÑÑÑÑ Ð±Ð°Ð½ÐºÑâ ÑÐ° Ð²Ð¸Ð·Ð½Ð°ÑÐµÐ½Ð¾ Ð¹Ð¾Ð³Ð¾ ÑÑÑÐ½ÑÑÐ½Ñ ÑÐ°ÑÐ°ÐºÑÐµÑÐ¸ÑÑÐ¸ÐºÐ¸ Ð·Ð° ÑÐºÑÑÐ½Ð¸Ð¼ ÑÐ° ÐºÑÐ»ÑÐºÑÑÐ½Ð¸Ð¼ Ð¿ÑÐ´ÑÐ¾Ð´Ð°Ð¼Ð¸. Ð ÑÑÐ°ÑÑÑ Ð¿ÑÐ¾Ð²ÐµÐ´ÐµÐ½Ð¾ Ð´Ð¾ÑÐ»ÑÐ´Ð¶ÐµÐ½Ð½Ñ ÑÐµÐ¾ÑÐµÑÐ¸ÑÐ½Ð¸Ñ Ð¿ÑÐ´ÑÐ¾Ð´ÑÐ² Ð´Ð¾ Ð·Ð°Ð±ÐµÐ·Ð¿ÐµÑÐµÐ½Ð½Ñ ÐºÑÐ±ÐµÑÑÑÑÐ¹ÐºÐ¾ÑÑÑ Ð±Ð°Ð½ÐºÑÐ² ÑÐ° Ð½Ð° ÑÑÐ¹ Ð¾ÑÐ½Ð¾Ð²Ñ ÑÐ¾Ð·ÑÐ¾Ð±Ð»ÐµÐ½Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ñ Ð¼ÐµÑÐ°Ð½ÑÐ·Ð¼Ñ Ð·Ð°Ð±ÐµÐ·Ð¿ÐµÑÐµÐ½Ð½Ñ ÐºÑÐ±ÐµÑÑÑÑÐ¹ÐºÐ¾ÑÑÑ, Ð°Ð´ÐµÐºÐ²Ð°ÑÐ½Ñ ÑÑÑÐ°ÑÐ½Ð¾Ð¼Ñ ÑÑÐ°Ð½Ñ ÑÐ° ÑÐ¼Ð¾Ð²Ð°Ð¼, Ð² ÑÐºÐ¸Ñ ÑÑÐ½ÐºÑÑÐ¾Ð½ÑÑÑÑ Ð±Ð°Ð½ÐºÐ¸ Ð£ÐºÑÐ°ÑÐ½Ð¸. ÐÐ° ÑÐµÐ·ÑÐ»ÑÑÐ°ÑÐ°Ð¼Ð¸ Ð´Ð¾ÑÐ»ÑÐ´Ð¶ÐµÐ½Ð½Ñ Ð²Ð¸Ð·Ð½Ð°ÑÐµÐ½Ð¾, ÑÐ¾ ÐµÑÐµÐºÑÐ¸Ð²Ð½Ðµ ÑÑÐ½ÐºÑÑÐ¾Ð½ÑÐ²Ð°Ð½Ð½Ñ Ð¼ÐµÑÐ°Ð½ÑÐ·Ð¼Ñ Ð·Ð°Ð±ÐµÐ·Ð¿ÐµÑÐµÐ½Ð½Ñ ÐºÑÐ±ÐµÑÑÑÑÐ¹ÐºÐ¾ÑÑÑ Ð¿Ð¾ÑÑÐµÐ±ÑÑ Ð²ÑÐ´Ð¿Ð¾Ð²ÑÐ´Ð½Ð¾Ð³Ð¾ Ð¾ÑÐ³Ð°Ð½ÑÐ·Ð°ÑÑÐ¹Ð½Ð¾Ð³Ð¾ Ð·Ð°Ð±ÐµÐ·Ð¿ÐµÑÐµÐ½Ð½Ñ, Ð·Ð¾ÐºÑÐµÐ¼Ð° ÑÑÐ²Ð¾ÑÐµÐ½Ð½Ñ Ð¦ÐµÐ½ÑÑÐ° ÐºÑÐ±ÐµÑÑÑÑÐ¹ÐºÐ¾ÑÑÑ Ð±Ð°Ð½ÐºÑ.Ð ÑÑÐ°ÑÑÐµ Ð´Ð¾ÐºÐ°Ð·Ð°Ð½Ð° Ð²Ð°Ð¶Ð½Ð¾ÑÑÑ ÑÐ¾ÑÐ¼Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ ÐºÐ¾Ð½ÑÐµÐ¿ÑÐ¸Ð¸ Ð¾Ð±ÐµÑÐ¿ÐµÑÐµÐ½Ð¸Ñ ÐºÐ¸Ð±ÐµÑÑÑÑÐ¾Ð¹ÑÐ¸Ð²Ð¾ÑÑÐ¸ Ð±Ð°Ð½ÐºÐ¾Ð² Ð½Ð° ÑÐ¾Ð²ÑÐµÐ¼ÐµÐ½Ð½Ð¾Ð¼ ÑÑÐ°Ð¿Ðµ ÑÐ°Ð·Ð²Ð¸ÑÐ¸Ñ ÑÐ¸ÑÑÐ¾Ð²Ð¾Ð¹ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÐºÐ¸ ÑÑÑÐ°Ð½Ñ, Ð½ÐµÑÐ¼Ð¾ÑÑÑ Ð½Ð° Ð¾ÑÑÐ¸ÑÐ°ÑÐµÐ»ÑÐ½Ð¾Ðµ ÑÐ¸Ð½Ð°Ð½ÑÐ¾Ð²Ð¾Ðµ Ð¸ Ð½ÐµÑÐ¸Ð½Ð°Ð½ÑÐ¾Ð²Ð¾Ðµ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ ÐºÐ¸Ð±ÐµÑÐ°ÑÐ°Ðº Ð½Ð° Ð±Ð°Ð½ÐºÐ¾Ð²ÑÐºÑÑ ÑÐ¸ÑÑÐµÐ¼Ñ Ð¸ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÐºÑ ÑÑÑÐ°Ð½Ñ Ð² ÑÐµÐ»Ð¾Ð¼. ÐÐ²ÑÐ¾ÑÐ¾Ð¼ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¾Ð±Ð¾Ð±ÑÐµÐ½Ð¸Ñ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ð¹ Ð¿Ð¾ ÑÑÐ¾Ð¹ ÑÐµÐ¼Ð°ÑÐ¸ÐºÐµ ÑÑÐ¾ÑÐ½ÐµÐ½Ð¾ ÑÐ¾Ð´ÐµÑÐ¶Ð°Ð½Ð¸Ðµ Ð¿Ð¾Ð½ÑÑÐ¸Ñ ""ÐºÐ¸Ð±ÐµÑÑÑÑÐ¾Ð¹ÑÐ¸Ð²Ð¾ÑÑÑ Ð±Ð°Ð½ÐºÐ°"" Ð¸ Ð¾Ð¿ÑÐµÐ´ÐµÐ»ÐµÐ½Ñ ÐµÐ³Ð¾ ÑÑÑÐ½Ð¾ÑÑÐ½ÑÐµ ÑÐ°ÑÐ°ÐºÑÐµÑÐ¸ÑÑÐ¸ÐºÐ¸ ÑÐ¾Ð³Ð»Ð°ÑÐ½Ð¾ ÐºÐ°ÑÐµÑÑÐ²ÐµÐ½Ð½Ð¾Ð¼Ñ Ð¸ ÐºÐ¾Ð»Ð¸ÑÐµÑÑÐ²ÐµÐ½Ð½Ð¾Ð¼Ñ Ð¿Ð¾Ð´ÑÐ¾Ð´Ð°Ð¼. Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐ¾Ð²ÐµÐ´ÐµÐ½Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÐµÐ¾ÑÐµÑÐ¸ÑÐµÑÐºÐ¸Ñ Ð¿Ð¾Ð´ÑÐ¾Ð´Ð¾Ð² Ðº Ð¾Ð±ÐµÑÐ¿ÐµÑÐµÐ½Ð¸Ñ ÐºÐ¸Ð±ÐµÑÑÑÑÐ¾Ð¹ÑÐ¸Ð²Ð¾ÑÑÐ¸ Ð±Ð°Ð½ÐºÐ¾Ð² Ð¸ Ð½Ð° ÑÑÐ¾Ð¹ Ð¾ÑÐ½Ð¾Ð²Ðµ ÑÐ°Ð·ÑÐ°Ð±Ð¾ÑÐ°Ð½Ð° Ð¼Ð¾Ð´ÐµÐ»Ñ Ð¼ÐµÑÐ°Ð½Ð¸Ð·Ð¼Ð° Ð¾Ð±ÐµÑÐ¿ÐµÑÐµÐ½Ð¸Ñ ÐºÐ¸Ð±ÐµÑÑÑÑÐ¾Ð¹ÑÐ¸Ð²Ð¾ÑÑÐ¸, Ð°Ð´ÐµÐºÐ²Ð°ÑÐ½Ð°Ñ ÑÑÐ»Ð¾Ð²Ð¸ÑÐ¼, Ð² ÐºÐ¾ÑÐ¾ÑÑÑ ÑÑÐ½ÐºÑÐ¸Ð¾Ð½Ð¸ÑÑÑÑ Ð±Ð°Ð½ÐºÐ¸ Ð£ÐºÑÐ°Ð¸Ð½Ñ. Ð ÑÐµÐ·ÑÐ»ÑÑÐ°ÑÐµ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ ÑÑÑÐ°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾, ÑÑÐ¾ ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾Ðµ ÑÑÐ½ÐºÑÐ¸Ð¾Ð½Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð¼ÐµÑÐ°Ð½Ð¸Ð·Ð¼Ð° Ð¾Ð±ÐµÑÐ¿ÐµÑÐµÐ½Ð¸Ñ ÐºÐ¸Ð±ÐµÑÑÑÑÐ¾Ð¹ÑÐ¸Ð²Ð¾ÑÑÐ¸ ÑÑÐµÐ±ÑÐµÑ ÑÐ¾Ð¾ÑÐ²ÐµÑÑÑÐ²ÑÑÑÐµÐ³Ð¾ Ð¾ÑÐ³Ð°Ð½Ð¸Ð·Ð°ÑÐ¸Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÐµÑÐ¿ÐµÑÐµÐ½Ð¸Ñ, Ð² ÑÐ°ÑÑÐ½Ð¾ÑÑÐ¸ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¦ÐµÐ½ÑÑÐ° ÐºÐ¸Ð±ÐµÑÑÑÑÐ¾Ð¹ÑÐ¸Ð²Ð¾ÑÑÐ¸ Ð±Ð°Ð½ÐºÐ°.The article proves the importance of forming the concept of ensuring the cyber resilience of banks at the present stage of development of the country's digital economy in the transition to the sixth technological mode and the associated use of industry 4.0 technologies, such as artificial intelligence, Â«cloudÂ» and Â«foggyÂ» computing, IoT / IIoT, Big Data, Blockchain, VR / AR. This leads to a significant complication of the cyber threat landscape, an increase in the number of cyberattacks with a significant increase in the negative financial and non-financial consequences that cyberattacks have on the banking system and the economy as a whole. The author, based on the generalization of research on this topic, clarified the content of the concept of ""cyber resilience of a bank"". Its essential characteristics were determined in the context of qualitative and quantitative approaches. The article studies theoretical approaches to ensuring the cyber resilience of banks, which made it possible to develop a conceptual model of the mechanism for ensuring cyber resilience, adequate to the current state and conditions in which the banks of Ukraine operate. The developed mechanism for ensuring cyber resilience allows for: 1) the formalization of the landscape of real and potential cyber threats; 2) ensures the consistency of mechanisms and tools for countering them, adapting and/or recovering from cyber incidents; 3) allows not only to adequately respond to existing cyber threats but also to identify negative factors that can lead to the emergence and implementation of new cyber threats and cyber-attacks. According to the results of the study, it was found that the effective functioning of the mechanism for ensuring the cyber resilience of the bank requires appropriate organizational support. For this, the author substantiated the need to create a Bank Cyber Resilience Center. It should include representatives from the departments responsible for banking business continuity, cybersecurity, cyber risk management and IT quality. This will allow obtaining a synergistic effect by creating a single interconnected process-based model, including metrics of the bank's cyber resilience level and KPIs, as well as tools for monitoring, controlling and resisting external and internal cyber threats, adaptation and/or recovery after them",finance,90,unknown
10.1109/isncc.2018.8530988,to_check,"2018 International Symposium on Networks, Computers and Communications (ISNCC)",IEEE,2018-06-21 00:00:00,ieeexplore,building an intelligent and efficient smart space to detect human behavior in common areas,https://ieeexplore.ieee.org/document/8530988/,"Smart spaces have become an integral part of our daily routines to improve quality of life for many different groups of people. The use of embedded systems to build these smart spaces, in combination with data analytics, can provide real-time information about the environment and how it interacts with the people in it. In this paper, we demonstrate how one embedded system that acquires data based on a 2-dimensional positional-grid, movement, temperature and vibration is used to build a smart and pervasive space. Data collected from these sensors is used for real time localization in conjunction with machine learning mechanisms to analyze human activities. We evaluate five machine learning algorithms, namely Logistic Regression, Support Vector Machine, Decision Tree, Random Forest, Naive Bayes and Artificial Neural Network applied on a dataset collected in our lab. Results show high classification performance for all methods giving up-to 99.95% classification accuracy. These patterns provide useful information about occupancy patterns, movement patterns, etc., which will be later used to allocate computational resources in the smart space accordingly. Furthermore, our implementation does not use any camera or microphone deployment, hence addressing potential privacy issues.",space,91,unknown
10.1109/allerton.2009.5394528,to_check,"2009 47th Annual Allerton Conference on Communication, Control, and Computing (Allerton)",IEEE,2009-10-02 00:00:00,ieeexplore,highly parallel decoding of space-time codes on graphics processing units,https://ieeexplore.ieee.org/document/5394528/,Graphics processing units (GPUs) with a few hundred extremely simple processors represent a paradigm shift for highly parallel computations. We use this emergent GPU architecture to provide a first demonstration of the feasibility of real time ML decoding (in software) of a high rate space-time block code that is representative of codes incorporated in 4th generation wireless standards such as WiMAX and LTE. The decoding algorithm is conditional optimization which reduces to a parallel calculation that is a natural fit to the architecture of low cost GPUs. Experimental results demonstrate that asymptotically the GPU implementation is more than 700 times faster than a standard serial implementation. These results suggest that GPU architectures have the potential to improve the cost / performance tradeoff of 4th generation wireless base stations. Additional benefits might include reducing the time required for system development and the time required for configuration and testing of wireless base stations.,space,92,unknown
10.1109/vrais.1995.512487,to_check,Proceedings Virtual Reality Annual International Symposium '95,IEEE,1995-03-15 00:00:00,ieeexplore,human figure synthesis and animation for virtual space teleconferencing,https://ieeexplore.ieee.org/document/512487/,"Human figure animation is it widely researched area with many applications. This paper addresses specific issues that deal with the synthesis, animation and environmental interaction of human figures within a virtual space teleconferencing system. A layered representation of the human figure is adopted. Skeletal posture is determined from magnetic sensors on the body, using heuristics and inverse kinematics. This paper describes the use of implicit function techniques in the synthesis and animation of a polymesh geometric skin over the skeletal structure. Implicit functions perform detection and handling of collisions with an optimal worst case time complexity that is linear in the number polymesh vertices. Body deformations resulting from auto-collisions are handled elegantly and homogeneously as part of the environment. Further, implicit functions generate precise collision contact surfaces and have the capability to model the physical characteristics of muscles in systems that employ force feedback. The real time implementation within a virtual space teleconferencing system, illustrates this new approach, coupling polymesh and implicit surface based modeling and animation techniques.",space,93,unknown
10.1109/icassp39728.2021.9414951,to_check,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",IEEE,2021-06-11 00:00:00,ieeexplore,kernel regression on graphs in random fourier features space,https://ieeexplore.ieee.org/document/9414951/,"This work proposes an efficient batch-based implementation for kernel regression on graphs (KRG) using random Fourier features (RFF) and a low-complexity online implementation. Kernel regression has proven to be an efficient learning tool in the graph signal processing framework. However, it suffers from poor scalability inherent to kernel methods. We employ RFF to overcome this issue and derive a batch-based KRG whose model size is independent of the training sample size. We then combine it with a stochastic gradient-descent approach to propose an online algorithm for KRG, namely the stochastic-gradient KRG (SGKRG). We also derive sufficient conditions for convergence in the mean sense of the online algorithms. We validate the performance of the proposed algorithms through numerical experiments using both synthesized and real data. Results show that the proposed batch-based implementation can match the performance of conventional KRG while having reduced complexity. Moreover, the online implementations effectively learn the target model and achieve competitive performance compared to the batch implementations.",space,94,unknown
10.1109/access.2021.3064928,to_check,IEEE Access,IEEE,2021-01-01 00:00:00,ieeexplore,deep space network scheduling via mixed-integer linear programming,https://ieeexplore.ieee.org/document/9373338/,"NASAâs Deep Space Network (DSN) is a globally-spanning communications network responsible for supporting the interplanetary spacecraft missions of NASA and other international users. The DSN is a highly utilized asset, and the large demand for itsâ services makes the assignment of DSN resources a daunting computational problem. In this paper we study the DSN scheduling problem, which is the problem of assigning the DSNâs limited resources to its users within a given time horizon. The DSN scheduling problem is oversubscribed, meaning that only a subset of the activities can be scheduled, and network operators must decide which activities to exclude from the schedule. We first formulate this challenging scheduling task as a Mixed-Integer Linear Programming (MILP) optimization problem. Next, we develop a sequential algorithm which solves the resulting MILP formulation to produce valid schedules for large-scale instances of the DSN scheduling problem. We use real world DSN data from week 44 of 2016 in order to evaluate our algorithmâs performance. We find that given a fixed run time, our algorithm outperforms a simple implementation of our MILP model, generating a feasible schedule in which 17% more activities are scheduled by the algorithm than by the simple implementation. We design a non-MILP based heuristic to further validate our results. We find that our algorithm also outperforms this heuristic, scheduling 8% more activities and 20% more tracking time than the best results achieved by the non-MILP implementation.",space,95,unknown
10.1109/icisce48695.2019.00014,to_check,2019 6th International Conference on Information Science and Control Engineering (ICISCE),IEEE,2019-12-22 00:00:00,ieeexplore,a deep reinforcement learning malware detection method based on pe feature distribution,https://ieeexplore.ieee.org/document/9107644/,"Existing anti-virus software and malware detection methods, including signature-based and the machine learning-based malware detection methods, are unable to update the virus database in real time, resulting in poor resistance to malware variants. To solve this problem, this paper proposes a novel malware detection method based on deep reinforcement learning, which combines the advantages of Q-learning and neural network. Q-learning action selection strategy is adopted while solving the problem of high dimensional state space. Theoretical analysis and experimental results show that the proposed method can not only detect malware variants efficiently, but also perform well in many well-known anti-virus software, which is a new direction in the field of malware detection.",space,96,unknown
10.1109/aero50100.2021.9438232,to_check,2021 IEEE Aerospace Conference (50100),IEEE,2021-03-13 00:00:00,ieeexplore,a pipeline for vision-based on-orbit proximity operations using deep learning and synthetic imagery,https://ieeexplore.ieee.org/document/9438232/,"Deep learning has become the gold standard for image processing over the past decade. Simultaneously, we have seen growing interest in orbital activities such as satellite servicing and debris removal that depend on proximity operations between spacecraft. However, two key challenges currently pose a major barrier to the use of deep learning for vision-based on-orbit proximity operations. Firstly, efficient implementation of these techniques relies on an effective system for model development that streamlines data curation, training, and evaluation. Secondly, a scarcity of labeled training data (images of a target spacecraft) hinders creation of robust deep learning models. This paper presents an open-source deep learning pipeline, developed specifically for on-orbit visual navigation applications, that addresses these challenges. The core of our work consists of two custom software tools built on top of a cloud architecture that interconnects all stages of the model development process. The first tool leverages Blender, an open-source 3D graphics toolset, to generate labeled synthetic training data with configurable model poses (positions and orientations), lighting conditions, backgrounds, and commonly observed in-space image aberrations. The second tool is a plugin-based framework for effective dataset curation and model training; it provides common functionality like metadata generation and remote storage access to all projects while giving complete independence to project-specific code. Time-consuming, graphics-intensive processes such as synthetic image generation and model training run on cloud-based computational resources which scale to any scope and budget and allow development of even the largest datasets and models from any machine. The presented system has been used in the Texas Spacecraft Laboratory with marked benefits in development speed and quality. Remote development, scalable compute, and automatic organization of data and artifacts have dramatically decreased iteration time while increasing reproducibility and system comprehension. Diverse, high-fidelity synthetic images that more closely replicate the real environment have improved model performance against real-world data. These results demonstrate that the presented pipeline offers tangible benefits to the application of deep learning for vision-based on-orbit proximity operations.",space,97,unknown
10.1109/ijcnn.2000.857872,to_check,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,IEEE,2000-07-27 00:00:00,ieeexplore,a hippocampal model of visually guided navigation as implemented by a mobile agent,https://ieeexplore.ieee.org/document/857872/,"Visually guided landmark navigation is based on space coding by hippocampal place cells (PC). A biologically realistic architecture of cooperative-competitive associative networks (implemented as a control system for mobile agents) emulates PC activity during local navigation in exploration and goal-retrieval. The system builds and stores panoramic views from landmarks and compares them with current inputs. Mismatch-induced low levels of recognition trigger a vigilance burst, which favors either the recognition of an alternative place category or the creation of a new category. The sole implementation of visual ""What"" and ""Where"" information does not restrain the generality of the model since several modalities could cooperate to give rise to more robust place field spatial categories. Providing the system with real visual inputs automatically extracted from a natural environment demonstrates that interspecies differences in PC coding result more from characteristics of the visual systems than from differences in processing. Conversely, differences in PC multiple codes within a system result from different levels of processing and/or different degrees of multimodality. Each code could be used within different navigational strategies. A control system derived from the model allows a mobile agent to learn a few places and the associated actions required to reach a goal. Generalisation property of the model provides the capacity to join the goal from any place in the learned environment.",space,98,unknown
10.1109/cnna.2010.5430245,to_check,2010 12th International Workshop on Cellular Nanoscale Networks and their Applications (CNNA 2010),IEEE,2010-02-05 00:00:00,ieeexplore,a multi-fpga distributed embedded system for the emulation of multi-layer cnns in real time video applications,https://ieeexplore.ieee.org/document/5430245/,"This paper describes the design and the implementation of an embedded system based on multiple FPGAs that can be used to process real time video streams in standalone mode for applications that require the use of large Multi-Layer CNNs (ML-CNNs). The system processes video in progressive mode and provides a standard VGA output format. The main features of the system are determined by using a distributed computing architecture, based on Independent Hardware Modules (IHM), which facilitate system expansion and adaptation to new applications. Each IHM is composed by an FPGA board that can hold one or more CNN layers. The total computing capacity of the system is determined by the number of IHM used and the amount of resources available in the FPGAs. Our architecture supports traditional cloned templates, but also the (simultaneous) use of time-variant and space-variant templates.",space,99,unknown
10.1109/icsmc.2003.1244605,to_check,"SMC'03 Conference Proceedings. 2003 IEEE International Conference on Systems, Man and Cybernetics. Conference Theme - System Security and Assurance (Cat. No.03CH37483)",IEEE,2003-10-08 00:00:00,ieeexplore,a novel approach for person authentication and content-based tracking in videos using kernel methods and active appearance models,https://ieeexplore.ieee.org/document/1244605/,"A novel integration of methods for person authentication and tracking is proposed for real time security systems. The implementation of the idea for this real time implementation follows a three step procedure-face detection, recognition and content-based tracking. Instead of analyzing continuous videos we sample the frame based on a method derived from Shannon's information theory model. The Face-detector detects multi-viewed faces in a video using feature-based kernel methods in a reduced feature space obtained using ICA. The identified ""face regions"" are then passed on to the face recognition system which is based on Active Appearance Models (AAM). Once the subject is recognized, it can be tracked in the video using kernel based object tracking method. Several space reduction techniques have been used like ICA, PCA and skin-color segmentation.",space,100,unknown
10.1109/icpp.2000.876164,to_check,Proceedings 2000 International Conference on Parallel Processing,IEEE,2000-08-24 00:00:00,ieeexplore,a scalable parallel subspace clustering algorithm for massive data sets,https://ieeexplore.ieee.org/document/876164/,"Clustering is a data mining problem which finds dense regions in a sparse multi-dimensional data set. The attribute values and ranges of these regions characterize the clusters. Clustering algorithms need to scale with the data base size and also with the large dimensionality of the data set. Further, these algorithms need to explore the embedded clusters in a subspace of a high dimensional space. However the time complexity of the algorithm to explore clusters in subspaces is exponential in the dimensionality of the data and is thus extremely compute intensive. Thus, parallelization is the choice for discovering clusters for large data sets. In this paper we present a scalable parallel subspace clustering algorithm which has both data and task parallelism embedded in it. We also formulate the technique of adaptive grids and present a truly unsupervised clustering algorithm requiring no user inputs. Our implementation shows near linear speedups with negligible communication overheads. The use of adaptive grids results in two orders of magnitude improvement in the computation time of our serial algorithm over current methods with much better quality of clustering. Performance results on both real and synthetic data sets with very large number of dimensions on a 16 node IBM SP2 demonstrate our algorithm to be a practical and scalable clustering technique.",space,101,unknown
10.1109/ipdpsw52791.2021.00012,to_check,2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),IEEE,2021-06-21 00:00:00,ieeexplore,adaptive stochastic gradient descent for deep learning on heterogeneous cpu+gpu architectures,https://ieeexplore.ieee.org/document/9460628/,"The widely-adopted practice is to train deep learning models with specialized hardware accelerators, e.g., GPUs or TPUs, due to their superior performance on linear algebra operations. However, this strategy does not employ effectively the extensive CPU and memory resources - which are used only for preprocessing, data transfer, and scheduling - available by default on the accelerated servers. In this paper, we study training algorithms for deep learning on heterogeneous CPU+GPU architectures. Our two-fold objective - maximize convergence rate and resource utilization simultaneously - makes the problem challenging. In order to allow for a principled exploration of the design space, we first introduce a generic deep learning framework that exploits the difference in computational power and memory hierarchy between CPU and GPU through asynchronous message passing. Based on insights gained through experimentation with the framework, we design two heterogeneous asynchronous stochastic gradient descent (SGD) algorithms. The first algorithm - CPU+GPU Hogbatch - combines small batches on CPU with large batches on GPU in order to maximize the utilization of both resources. However, this generates an unbalanced model update distribution which hinders the statistical convergence. The second algorithm - Adaptive Hogbatch - assigns batches with continuously evolving size based on the relative speed of CPU and GPU. This balances the model updates ratio at the expense of a customizable decrease in utilization. We show that the implementation of these algorithms in the proposed CPU+GPU framework achieves both faster convergence and higher resource utilization than TensorFlow on several real datasets.",space,102,not included
10.1109/icra40945.2020.9196582,to_check,2020 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2020-08-31 00:00:00,ieeexplore,adversarial skill networks: unsupervised robot skill learning from video,https://ieeexplore.ieee.org/document/9196582/,"Key challenges for the deployment of reinforcement learning (RL) agents in the real world are the discovery, representation and reuse of skills in the absence of a reward function. To this end, we propose a novel approach to learn a task-agnostic skill embedding space from unlabeled multi-view videos. Our method learns a general skill embedding independently from the task context by using an adversarial loss. We combine a metric learning loss, which utilizes temporal video coherence to learn a state representation, with an entropy-regularized adversarial skill-transfer loss. The metric learning loss learns a disentangled representation by attracting simultaneous viewpoints of the same observations and repelling visually similar frames from temporal neighbors. The adversarial skill-transfer loss enhances re-usability of learned skill embeddings over multiple task domains. We show that the learned embedding enables training of continuous control policies to solve novel tasks that require the interpolation of previously seen skills. Our extensive evaluation with both simulation and real world data demonstrates the effectiveness of our method in learning transferable skills from unlabeled interaction videos and composing them for new tasks. Code, pretrained models and dataset are available at http://robotskills.cs.uni-freiburg.de.",space,103,unknown
10.1109/cbd.2016.066,to_check,2016 International Conference on Advanced Cloud and Big Data (CBD),IEEE,2016-08-16 00:00:00,ieeexplore,allocation of resources after disaster based on big data from sns and spatial scan,https://ieeexplore.ieee.org/document/7815232/,"After a disaster such as earthquakes, debris flows, forest fires, or landslides, etc., a lot of people have to be away from their home and gather in a shelter. In addition, the refugees suffer from the shortage of necessary resources due to impaired life infrastructure, such as damaged roads and communication networks. The degree of reducing damage depends on the amount of food, water, daily necessities and communication resources required by each shelter. How to effectively and efficiently allocate resources according to grasp the exact need of a disaster situation will be an important issue. We estimate the degree of the disaster by collecting and analyzing big data from the SNS, and building a platform for the communication resources to be efficiently and effectively allocated. In order to achieve this goal, we are challenging the following issues A) Understanding situations (user requirements) after disaster occur The SNS streams large scale semantic information about real time situation in society, especially during and after disaster. It is both domain-specific and computational challenge in processing the heterogeneous large data set to extract the exact situational content with reduced semantic uncertainty. The machine learning (ML) and natural language processing (NPL) tool kits are useful in semantic analysis, but still needs domain-specific implementation and computational improvement for the situation understanding from the SNS big data. B) Understanding distribution patterns of situations/users' requirements The disaster related situation is spatiotemporally correlated, and varies dynamically in space and time. It is also domain-specific and computational challenge in estimating the spatiotemporal distribution patterns of the disaster affect based on the spatial big data from SNS. The scan statistics such as the spatial scan have provided well tested mathematical tools and software for spatial data mining. However, new methodologies are necessary since the assumptions have to be different when it meets the spatial big data in SNS. And the computational complexity in spatial big data is also a bottleneck for real-time processing. C) Solving uncertainty of big crowd data One of the major features in big crowd data, e.g., SNS data, is uncertainty behind the data. Especially in a disaster scenario, the collecting time period cannot be long enough to smooth the data automatically. How to efficiently solve uncertainty problem in the big crowd data in a disaster scenario becomes a new and big challenge for disaster management.",space,104,unknown
10.1109/hpcs.2018.00034,to_check,2018 International Conference on High Performance Computing & Simulation (HPCS),IEEE,2018-07-20 00:00:00,ieeexplore,an ensemble-based p2p framework for the detection of deviant business process instances,https://ieeexplore.ieee.org/document/8514339/,"The problem of discriminating ""deviant"" traces (i.e. traces diverging from normal/desired outcomes, such as frauds, faults) in the execution log of a business process can be faced by extracting a classification model for the traces, after mapping them onto some suitable feature space. An ensemble-learning approach was recently proposed that trains multiple base learners on different vector-space views of the given log, and a probabilistic meta-model that combines the predictions of the discovered base classifiers. However, the sequential centralised implementation of this learning approach makes it unsuitable for real applications, where large volumes of traces are produced continuously, while both deviant and normal behaviours tend to change over the time. We here propose an online deviance detection framework that leverages a novel incremental learning scheme, which extracts different base models from different chunks of a trace stream, and dynamically combines them in an ensemble model. Notably, the system is based on a p2p architecture that allows it to distribute the entire learning procedure among multiple nodes and to exploit the power of HPC resources (e.g. cloud computing environments). Preliminary tests on a real-life log confirmed the validity of the approach, in terms of both effectiveness and efficiency.",space,105,unknown
10.1109/dsc50466.2020.00029,to_check,2020 IEEE Fifth International Conference on Data Science in Cyberspace (DSC),IEEE,2020-07-30 00:00:00,ieeexplore,anobeat: anomaly detection for electrocardiography beat signals,https://ieeexplore.ieee.org/document/9172828/,"Electrocardiography signals are composed of variform heartbeats which could indicate the condition of the heart and reveal the risk of heart attacks. Many existing classification-based works for abnormal beats detection are limited by the class-imbalanced data or labor-intensive manual annotation bias. A promising trend to address the issue is to identify the abnormal data that differs from the normal data by utilizing normal (oneclass) data to learn the manifold and detect the anomaly to the unseen and unlabeled data in an/aunsupervised/semi-supervised manner. In this paper, we propose Anobeat, a semi-supervised approach, to perform the abnormal beat detection by facilitating adversarial regularized autoencoders constrained with multifeature and reconstruction error. In order to obtain a robust and reasonable latent coding, we deploy two discriminators in the latent space and visual space to distinguish real and fake features and minimize the distance between two features to train the visual discriminator in alternate steps. Meanwhile, we minimize the reconstruction error and maximum distance between input and noise features to improve the decoder. The adversarial multi-feature constraints enable the generator to learn the latent representations of the target normal data and reconstruct the beats properly. Experiments showed that Anobeat achieved ROC-AUC of 0.960 and 0.894 in the MIT-BIH intrapatient and inter-patient dataset respectively, which outperforms the most competitive baseline by 1.61% and 0.62% respectively. Anobeat also performs comparative robustness and shows good interpretability in the European ST-T and MIT-BIH Arrhythmia Database.",space,106,unknown
10.1109/infocom42981.2021.9488865,to_check,IEEE INFOCOM 2021 - IEEE Conference on Computer Communications,IEEE,2021-05-13 00:00:00,ieeexplore,can you fix my neural network? real-time adaptive waveform synthesis for resilient wireless signal classification,https://ieeexplore.ieee.org/document/9488865/,"Due to the sheer scale of the Internet of Things (IoT) and 5G, the wireless spectrum is becoming severely congested. For this reason, wireless devices will need to continuously adapt to current spectrum conditions by changing their communication parameters in real-time. Therefore, wireless signal classification (WSC) will become a compelling necessity to decode fast-changing signals from dynamic transmitters. Thanks to its capability of classifying complex phenomena without explicit mathematical modeling, deep learning (DL) has been demonstrated to be a key enabler of WSC. Although DL can achieve a very high accuracy under certain conditions, recent research has unveiled that the wireless channel can disrupt the features learned by the DL model during training, thus drastically reducing the classification performance in real-world live settings. Since retraining classifiers is cumbersome after deployment, existing work has leveraged the usage of carefully-tailored Finite Impulse Response (FIR) filters that, when applied at the transmitter's side, can restore the features that are lost because of the the channel actions, i.e., waveform synthesis. However, these approaches compute FIRs using offline optimization strategies, which limits their efficacy in highly-dynamic channel settings. In this paper, we improve the state of the art by proposing Chares, a Deep Reinforcement Learning (DRL)-based framework for channel-resilient adaptive waveform synthesis. Chares adapts to new and unseen channel conditions by optimally computing through DRL the FIRs in real time. Chares is a DRL agent whose architecture is based upon the Twin Delayed Deep Deterministic Policy Gradients (TD3), which requires minimal feedback from the receiver and explores a continuous action space for best performance. Chares has been extensively evaluated on two well-known datasets with an extensive number of channels. We have also evaluated the real-time latency of Chares with an implementation on field-programmable gate array (FPGA). Results show that Chares increases the accuracy up to 4.1x when no waveform synthesis is performed, by 1.9x with respect to existing work, and can compute new actions within 41 Î¼s.",space,107,unknown
10.1109/iceccme52200.2021.9591113,to_check,"2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)",IEEE,2021-10-08 00:00:00,ieeexplore,cobots for fintech,https://ieeexplore.ieee.org/document/9591113/,"Embedded devices enabling payments transaction processing in Financial Services industry cannot have any margin for error. These devices need to be tested &amp; validated by replicating production like environment to the extent possible. This means literally handling payments related events like swiping a credit card, tapping a mobile phone or pressing buttons amongst many other things like in real world. Embedded Software development is time consuming as it involves multiple man-machine interactions and dependencies such as managing and handling embedded devices, operating devices (Push buttons, interpret display panels, read receipt printouts etc.) and sharing devices for collaboration within team. During the current pandemic, it was impossible for software teams to travel to office, share devices or even procure necessary devices on time for project related tasks. This caused delay to project delivery and increased Time to market. The paper describes how the team used Capgemini's flexible Robotics as a Service (RaaS) platform that helped during pandemic to automate feasible man-machine interactions using Robotic arms. The paper provides details of the work done by the team that involves internet of things (IoT), Artificial Intelligence (AI) to remotely handle and operate hardware and devices thereby completing embedded software development life cycles faster and well within budget while ensuring superior product quality and importantly ensuring team's health and safety. This is novel in Financial Services space.",space,108,unknown
10.1109/itnt52450.2021.9649145,to_check,2021 International Conference on Information Technology and Nanotechnology (ITNT),IEEE,2021-09-24 00:00:00,ieeexplore,comparison of reinforcement learning algorithms for motion control of an autonomous robot in gazebo simulator,https://ieeexplore.ieee.org/document/9649145/,"This article compares various implementations of deep Q learning as it is one of the most efficient reinforcement learning algorithms for discrete action space systems. The efficiency of the implementations for the classical Cartpole problem ported to the Gazebo environment is investigated. Then, these algorithms are compared for a self-created bipedal robot problem. Since the creation and configuration of a real robotic system is a laborious process, the initial debugging of the robot can be performed using the appropriate software that simulates the real environment. In our case, the Gazebo simulator was used. Using the simulator allows you to conduct research without having a real robotic system. In this case, it is possible to transfer the results from the simulator to the real system. The result of the study is the conclusion about the greatest efficiency of deep Q-learning with the experience reproduction mechanism. Also, the conclusion is that even for a robot with two degrees of freedom, Q-learning algorithms are not effective enough, and a comparative study with other families of reinforcement learning algorithms is needed.",space,109,not included
10.1109/qrs.2015.17,to_check,"2015 IEEE International Conference on Software Quality, Reliability and Security",IEEE,2015-08-05 00:00:00,ieeexplore,cross-project aging related bug prediction,https://ieeexplore.ieee.org/document/7272913/,"In a long running system, software tends to encounter performance degradation and increasing failure rate during execution, which is called software aging. The bugs contributing to the phenomenon of software aging are defined as Aging Related Bugs (ARBs). Lots of manpower and economic costs will be saved if ARBs can be found in the testing phase. However, due to the low presence probability and reproducing difficulty of ARBs, it is usually hard to predict ARBs within a project. In this paper, we study whether and how ARBs can be located through cross-project prediction. We propose a transfer learning based aging related bug prediction approach (TLAP), which takes advantage of transfer learning to reduce the distribution difference between training sets and testing sets while preserving their data variance. Furthermore, in order to mitigate the severe class imbalance, class imbalance learning is conducted on the transferred latent space. Finally, we employ machine learning methods to handle the bug prediction tasks. The effectiveness of our approach is validated and evaluated by experiments on two real software systems. It indicates that after the processing of TLAP, the performance of ARB bug prediction can be dramatically improved.",space,110,unknown
10.1109/icbaie52039.2021.9389950,to_check,"2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",IEEE,2021-03-28 00:00:00,ieeexplore,design of cloud computing platform based accurate measurement for structure monitoring using fiber bragg grating sensors,https://ieeexplore.ieee.org/document/9389950/,"The efficient integration of distributed fiber Bragg grating (FBG) sensors and cloud computing platform is used to achieve accurate measurement and evaluation of physical quantities, which solves the problems of traditional fiber Bragg grating sensing technology for health structure monitoring system, such as the cost and space constraints, it is difficult to deploy enough servers to deal with data collection, transmission and storage in real time. The cloud platform using fiber Bragg grating sensors adopts the structure of erbium-doped fiber cascaded Bragg grating, reasonably configures the FBG demodulator acquisition and analysis software, deploys the health monitoring system in the cloud, constructs the cloud platform of high-efficiency health monitoring optical fiber sensor network, improves the scalability of the system, flexibly deploys applications and services, and ensures the security and reliability of various real-time monitoring data and professional data. It can meet the needs of some specific or wide application fields for the automation technology, structural mechanics, computer technology, Internet architecture, cloud deployment and interdisciplinary practical comprehensive valuable application research.",space,111,unknown
10.1109/emctech49634.2020.9261512,to_check,2020 International Conference on Engineering Management of Communication and Technology (EMCTECH),IEEE,2020-10-22 00:00:00,ieeexplore,digital socio-political communication and its transformation in the technological evolution of artificial intelligence and neural network algorithms,https://ieeexplore.ieee.org/document/9261512/,"The study aims to analyze the specifics of determining the subjects of digital social and political communication in the context of the development of artificial intelligence technologies and neural network algorithms. The work uses a case-study design. As a research methodology, the method of critical analysis of the digital communications practice in the socio-political sphere, as well as discourse analysis of modern scientific research in the field of the development of artificial intelligence and neural network algorithms, are used. It is concluded that the implementation of technological solutions based on artificial intelligence and neural network algorithms into the processes of socio-political communications creates a problem of defining the subject of communication acts in the socio-political sphere. Society may face such communication practices in which hybrid subjectness is realized. In the conditions of hybrid subjectness, both real subjects and programmed neural network algorithms acting as real subjects (but only imitating own subjectivity) interact in common communication space. The originality of the work lies in the formulation of the author's hypothesis about the emergence of the phenomenon of hybrid subjectness in the space of modern socio-political communications and its potential in the aspect of influencing the mass consciousness of citizens.",space,112,not included
10.1109/dac.2005.193910,to_check,"Proceedings. 42nd Design Automation Conference, 2005.",IEEE,2005-06-17 00:00:00,ieeexplore,efficient sat solving: beyond supercubes,https://ieeexplore.ieee.org/document/1510430/,"SAT (Boolean satisfiability) has become the primary Boolean reasoning engine for many EDA applications, so the efficiency of SAT solving is of great practical importance. Recently, Goldberg et al introduced supercubing, a different approach to search-space pruning, based on a theory that unifies many existing methods. Their implementation reduced the number of decisions, but no speedup was obtained. In this paper, we generalize beyond supercubes, creating a theory we call B-cubing, and show how to implement B-cubing in a practical solver. On extensive benchmark runs, using both real problems and synthetic benchmarks, the new technique is competitive on average with the newest version of ZChaff, is much faster in some cases, and is more robust.",space,113,unknown
10.1109/icnc.2010.5584314,to_check,2010 Sixth International Conference on Natural Computation,IEEE,2010-08-12 00:00:00,ieeexplore,emotion generation for virtual human using cognitive map,https://ieeexplore.ieee.org/document/5584314/,"Affective computing is an indispensable aspect in harmonious human-computer interaction and artificial intelligence. Making computers have the ability of generating emotions is a challenging task of affective computing. The paper first introduces the basic affective elements, and the representation of affections in a computer. An emotion generation model using Cognitive Map is proposed. The model is used to generate emotion based on the evaluation of the overall influences of the mood, the personality, the previous emotion and the external stimulations. Then the paper describes a method to build a mapping from the emotion space to the facial expression space with a competitive network, and the implementation of facial expression animation. Finally, the paper constructs an intelligent virtual human system with facial expression, voice and vision communication. Experimental results show that the emotion generation model using Cognitive Map can produce realistic emotions similar to those of a real human.",space,114,not included
10.1109/humanoids.2014.7041373,to_check,2014 IEEE-RAS International Conference on Humanoid Robots,IEEE,2014-11-20 00:00:00,ieeexplore,footstep planning on uneven terrain with mixed-integer convex optimization,https://ieeexplore.ieee.org/document/7041373/,"We present a new method for planning footstep placements for a robot walking on uneven terrain with obstacles, using a mixed-integer quadratically-constrained quadratic program (MIQCQP). Our approach is unique in that it handles obstacle avoidance, kinematic reachability, and rotation of footstep placements, which typically have required non-convex constraints, in a single mixed-integer optimization that can be efficiently solved to its global optimum. Reachability is enforced through a convex inner approximation of the reachable space for the robot's feet. Rotation of the footsteps is handled by a piecewise linear approximation of sine and cosine, designed to ensure that the approximation never overestimates the robot's reachability. Obstacle avoidance is ensured by decomposing the environment into convex regions of obstacle-free configuration space and assigning each footstep to one such safe region. We demonstrate this technique in simple 2D and 3D environments and with real environments sensed by a humanoid robot. We also discuss computational performance of the algorithm, which is currently capable of planning short sequences of a few steps in under one second or longer sequences of 10-30 footsteps in tens of seconds to minutes on common laptop computer hardware. Our implementation is available within the Drake MATLAB toolbox [1].",space,115,unknown
10.1109/fuzz-ieee.2016.7737911,to_check,2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),IEEE,2016-07-29 00:00:00,ieeexplore,fuzzyfication of principle component analysis for data dimensionalty reduction,https://ieeexplore.ieee.org/document/7737911/,"Principal component analysis (PCA) extracts small uncorrelated data from original high dimensional data space and is widely used for data analysis. The methodology of classical PCA is based on orthogonal projection defined in convex vector space. Thus, a norm between two projected vectors is unavoidably smaller than the norm between any two objects before implementation of PCA. Due to this, in some cases when the PCA cannot capture the data structure, its implementation does not necessarily confirm the real similarity of data in the higher dimensional space, making the results unacceptable. In order to avoid this problem for the purposes of high dimensional data clustering, we propose new Fuzzy PCA (FPCA) algorithm. The novelty is in the extracted similarity structures of objects in high-dimensional space and dissimilarities between objects based on their cluster structure and dimension when the algorithm is implemented in conjunction with known fuzzy clustering algorithms as FCM, GK or GG algorithms. This is done by using the fuzzy membership functions and modification of the classical PCA approach by considering the similarity structures during the construction of projections in smaller dimensional space. The effectiveness of the proposed algorithm is tested on several benchmark data sets. We also evaluate the clustering efficiency by implementing validation measures.",space,116,not included
10.1109/cogann.1992.273945,to_check,[Proceedings] COGANN-92: International Workshop on Combinations of Genetic Algorithms and Neural Networks,IEEE,1992-06-06 00:00:00,ieeexplore,genetic sparse distributed memory,https://ieeexplore.ieee.org/document/273945/,"Kanerva's 'sparse distributed memory' (SDM) is a type of self-organizing neural network which is able to extract a statistical summary from large volumes of data as it is being processed online. Genetic algorithms have been used to optimize the 'location address space' which corresponds to the mapping from the input layer to the hidden units in the neural network implementation of the sparse distributed memory. If treated as a global optimization problem, the genetic algorithm will attempt to optimize the sparse distributed memory so as to extract a single best statistical predictor. However, the real objective is to obtain not just a single global optimum, but to extract information about as many local optima as possible, since each local optimum in this particular definition of the search space represents a different and distinct data pattern that correlates with some output in which we may be interested. The implementation details of a genetic sparse distributed memory as well as modified algorithm designed to deal better with multiple data patterns are presented.&lt;<ETX>&gt;</ETX>",space,117,unknown
10.1109/isuvr.2010.25,to_check,2010 International Symposium on Ubiquitous Virtual Reality,IEEE,2010-07-10 00:00:00,ieeexplore,high-performance real-time face-detection architecture for hci applications,https://ieeexplore.ieee.org/document/5557933/,"This paper proposes a novel hardware structure and FPGA implementation method for real-time detection of multiple human faces with robustness against illumination variations and Rotated faces. These are designed to greatly improve face detection in various environments, using the Adaboost learning algorithm and MCT techniques, Rotation Transformation, which is robust against variable illumination and rotated faces. The overall structure of proposed hardware is composed of a Color Space Converter, Noise Filter, Memory Controller Interface, Rotation Transformation, MCT (Modified Census Transform), Candidate Detector/Confidence Mapper, Position Resizer, Data Grouper, Overlay Processor and Color Overlay Processor. The experiment was conducted in various environments using a QVGA Camera, LCD Display and Virtext5 XC5VLX330 FF1760 FPGA, made by Xilinx. Implementation and verification results showed that it is possible to detect at least 32 faces in a wide variety of sizes at a maximum speed of 149 frames per second in real time.",space,118,not included
10.1109/iccairo.2018.00041,to_check,"2018 International Conference on Control, Artificial Intelligence, Robotics & Optimization (ICCAIRO)",IEEE,2018-05-21 00:00:00,ieeexplore,iterative algorithm for solving a split common null point problem for demicontractive operators,https://ieeexplore.ieee.org/document/8698385/,"In this paper, first we introduce an iterative algorithm which does not require prior knowledge of operator norm and prove strong convergence theorem for approximating a solution of split common null point problem of demicontractive mappings in a real Hilbert space. Widely known the computation of algorithms involving the operator norm for solving split common null point problem may be difficult and for this reason, authors have recently started constructing iterative algorithms with a way of selecting the step-sizes such that the implementation of the algorithm does not require the calculation or estimation of the operator norm. We introduce a new algorithm for solving the split common null point problem for demicontractive mappings with a way of selecting the step-sizes such that the implementation of the algorithm does not require the calculation or estimation of the operator norm and then prove strong convergence of the sequence in real Hilbert spaces. Finally, we give some numerical examples to illustrate our main result.",space,119,not included
10.1109/ictai.2019.00220,to_check,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),IEEE,2019-11-06 00:00:00,ieeexplore,learning to drive via apprenticeship learning and deep reinforcement learning,https://ieeexplore.ieee.org/document/8995417/,"With the implementation of reinforcement learning (RL) algorithms, current state-of-art autonomous vehicle technology have the potential to get closer to full automation. However, most of the applications have been limited to game domains or discrete action space which are far from the real world driving. Moreover, it is very tough to tune the parameters of reward mechanism since the driving styles vary a lot among the different users. For instance, an aggressive driver may prefer driving with high acceleration whereas some conservative drivers prefer a safer driving style. Therefore, we propose an apprenticeship learning in combination with deep reinforcement learning approach that allows the agent to learn the driving and stopping behaviors with continuous actions. We use gradient inverse reinforcement learning (GIRL) algorithm to recover the unknown reward function and employ REINFORCE as well as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal policy. The performance of our method is evaluated in simulation-based scenario and the results demonstrate that the agent performs human like driving and even better in some aspects after training.",space,120,not included
10.1109/etfa.2018.8502485,to_check,2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA),IEEE,2018-09-07 00:00:00,ieeexplore,linear classification of badly conditioned data,https://ieeexplore.ieee.org/document/8502485/,"We present a method for the fast and robust linear classification of badly conditioned data. In our considerations, badly conditioned data are such data which are numerically difficult to handle. Due to, e.g. a large number of features or a large number of objects representing classes as well as noise, outliers or incompleteness, the common software computation of the discriminating linear combination of features between classes fails or is extremely time consuming. The theoretical foundations of our approach are based on the single feature ranking, which allows fast calculation of the approximative initial classification boundary. For the increasing of classification accuracy of this boundary, the refinement is performed in the lower dimensional space. Our approach is tested on several datasets from UCI Reposi-tiory. Experimental results indicate high classification accuracy of the approach. For the modern real industrial applications such a method is especially suitable in the Cyber-Physical-System environments and provides a part of the workflow for the automated classifier design.",space,121,unknown
10.1109/ijcnn.2014.6889658,to_check,2014 International Joint Conference on Neural Networks (IJCNN),IEEE,2014-07-11 00:00:00,ieeexplore,long-term learning behavior in a recurrent neural network for sound recognition,https://ieeexplore.ieee.org/document/6889658/,"In this paper, the long-term learning properties of an artificial neural network model, designed for sound recognition and computational auditory scene analysis in general, are investigated. The model is designed to run for long periods of time (weeks to months) on low-cost hardware, used in a noise monitoring network, and builds upon previous work by the same authors. It consists of three neural layers, connected to each other by feedforward and feedback excitatory connections. It is shown that the different mechanisms that drive auditory attention emerge naturally from the way in which neural activation and intra-layer inhibitory connections are implemented in the model. Training of the artificial neural network is done following the Hebb principle, dictating that ""Cells that fire together, wire together"", with some important modifications, compared to standard Hebbian learning. As the model is designed to be on-line for extended periods of time, also learning mechanisms need to be adapted to this. The learning needs to be strongly attention- and saliency-driven, in order not to waste available memory space for sounds that are of no interest to the human listener. The model also implements plasticity, in order to deal with new or changing input over time, without catastrophically forgetting what it already learned. On top of that, it is shown that also the implementation of short-term memory plays an important role in the long-term learning properties of the model. The above properties are investigated and demonstrated by training on real urban sound recordings.",space,122,unknown
10.1109/icra40945.2020.9196540,to_check,2020 IEEE International Conference on Robotics and Automation (ICRA),IEEE,2020-08-31 00:00:00,ieeexplore,meta reinforcement learning for sim-to-real domain adaptation,https://ieeexplore.ieee.org/document/9196540/,"Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.",space,123,included
10.1109/ictai.2011.89,to_check,2011 IEEE 23rd International Conference on Tools with Artificial Intelligence,IEEE,2011-11-09 00:00:00,ieeexplore,multi-agent simulation design driven by real observations and clustering techniques,https://ieeexplore.ieee.org/document/6103379/,"The multi-agent simulation consists in using a set of interacting agents to reproduce the dynamics and the evolution of the phenomena that we seek to simulate. It is considered now as an alternative to classical simulations based on analytical models. But, its implementation remains difficult, particularly in terms of behaviors extraction and agents modelling. This task is usually performed by the designer who has some expertise and available observation data on the process. In this paper, we propose a novel way to make use of the observations of real world agents to model simulated agents. The modelling is based on clustering techniques. Our approach is illustrated through an example in which the behaviors of agents are extracted as trajectories and destinations from video sequences analysis. This methodology is investigated with the aim to apply it, in particular, in a retail space simulation for the evaluation of marketing strategies. This paper presents experiments of our methodology in the context of a public area modelling.",space,124,not included
10.1109/fpa.1994.636106,to_check,Proceedings of PerAc '94. From Perception to Action,IEEE,1994-09-09 00:00:00,ieeexplore,rcs: a reference model architecture for intelligent control,https://ieeexplore.ieee.org/document/636106/,"The Real-time Control System (RCS) is a reference model architecture for intelligent real time control systems. It partitions the control problem into four basic elements: task decomposition, world modeling, sensory processing, and value judgment. It clusters these elements into computational nodes that have responsibility for specific subsystems and arranges these nodes in hierarchical layers such that each layer has characteristic functionality and timing. The RCS architecture has a systematic regularity, and recursive structure that suggests a canonical form. Systems based on the RCS architecture have been implemented more or less for a wide variety of applications that include loading and unloading of parts and tools in machine tools, controlling machining workstations, performing robotic deburring and chamfering, and controlling space station telerobots, multiple autonomous undersea vehicles, unmanned land vehicles, coal mining automation systems. postal service mail handling systems, and submarine operational automation systems. Software developers accustomed to using RCS for building control systems have found it provides a structured design approach that makes it possible to reuse a great deal of software.",space,125,unknown
10.1109/smc-it.2009.40,to_check,2009 Third IEEE International Conference on Space Mission Challenges for Information Technology,IEEE,2009-07-23 00:00:00,ieeexplore,rapid prototyping of planning &amp; scheduling tools,https://ieeexplore.ieee.org/document/5226820/,"The Advanced Planning and Scheduling Initiative, or APSI, is an ESA programme to design and implement an Artificial Intelligence (AI) software infrastructure for planning and scheduling that can generically support different types and classes of space mission operations. The goal of the APSI is twofold: (1)~creating a software framework to improve the cost-effectiveness and flexibility of mission planning support tool development; (2)~bridging the gap between AI planning and scheduling technology and the world of space mission planning. A key aspect of the success of this project is the presence of a flexible timeline representation module that allows to exploit alternatives in the modeling of mission features. This paper shows an example of such a flexibility by using a real problem in the space realm - the HERSCHEL Science Long Term Planning process.",space,126,included
10.1109/icit.2006.372319,to_check,2006 IEEE International Conference on Industrial Technology,IEEE,2006-12-17 00:00:00,ieeexplore,real time classifier for industrial wireless sensor network using neural networks with wavelet preprocessors,https://ieeexplore.ieee.org/document/4237641/,"Wireless sensor node is embedded of computation unit, sensing unit and a radio unit for communication. Amongst three units communication is the largest consumer of energy. Energy is the prime source for wireless sensor node to function. Hence every aspects of sensor node are designed with energy constraints. Neural Networks in particular the combination of ART1 and FuzzyART(FA) can be used very efficiently for developing Real time Classifier. Wireless sensor networks demand for the real time classification of sensor data. In this paper classification technique using ART1 and Fuzzy ART is discussed. ART1 and FA have very good architectural strategy, which makes it simple for VLSI implementation. The VLSI implementation of the proposed classifier can be a part of embedded microsensor. The paper discusses classification technique, which can reduce the energy need for communication and improves communications bandwidth. The proposed sensor clustering architecture can give distributed storage space for the sensor networks. Wavelet Transform is used as preprocessor for denoising the real word data from sensor node, this makes it much suitable for industrial environment. Many methods of wavelet transforms are available. Simplest Haar 1D transform is used for preprocessing and smoothing the sensor signals. The discrete wavelet transform implemented here helps to extract important feature in the sensor data like sudden changes at various scales.",space,127,not included
10.1109/aero47225.2020.9172454,to_check,2020 IEEE Aerospace Conference,IEEE,2020-03-14 00:00:00,ieeexplore,semi-supervised machine learning for spacecraft anomaly detection &amp; diagnosis,https://ieeexplore.ieee.org/document/9172454/,"This paper describes Anomaly Detection via Topological-feature Map (ADTM), a data-driven approach to Integrated System Health Management (ISHM) for monitoring the health of spacecraft and space habitats. Developed for NASA Ames Research Center, ADTM leverages proven artificial intelligence techniques for rapidly detecting and diagnosing anomalies in near real-time. ADTM combines Self-Organizing Maps (SOMs) as the basis for modeling system behavior with supervised machine learning techniques for localizing detected anomalies. A SOM is a two-layer artificial neural network (ANN) that produces a low-dimensional representation of the training samples. Once trained on normal system behavior, SOMs are adept at detecting behavior previously not encountered in the training data. Upon detecting anomalous behavior, ADTM uses a supervised classification approach to determine a subset of measurands that characterize the anomaly. This allows it to localize faults and thereby provide extra insight. We demonstrate the effectiveness of our approach on telemetry data collected from a lab-stationed CubeSat (the âLabSatâ) connected to software that gave us the ability to trigger several real hardware faults. We include an analysis and discussion of ADTM's performance on several of these fault cases. We conclude with a brief discussion of future work, which contains investigation of a hierarchical SOM-architecture as well as a Case-Based Reasoning module for further assisting astronauts in diagnosis and remediation activities.",space,128,unknown
10.1109/aero47225.2020.9172439,to_check,2020 IEEE Aerospace Conference,IEEE,2020-03-14 00:00:00,ieeexplore,"smart &amp; integrated management system - smart cities, epidemiological control tool using drones",https://ieeexplore.ieee.org/document/9172439/,"This paper describes the development of a real application using Drones over urban regions to help the authorities at epidemiological control through a disruptive solutions based on a customizable Smart &amp; Integrated Management System (SIGI), devices and software based on the Enterprise Resource Planning (ERP) concept. Compound by management software, Drones and specific IoT devices, both referred to as sensors, the sensors collect the data of the interest areas in real time, creating a specified database. Based on the data collected from the interest areas, SIGI software has the ability to show real-time situational analysis of these areas and allows that the administrator can optimize resources (material and human) improving the efficiency of resource allocation in these areas. In addition to the development of the management software, the development of sensors to collect the information in the field and update these information to the database of the management software, are considered. The sensors will be recognized as IoT devices for the collection of meteorological data, images and command / control Drones. Initially the system will be customized, using an Artificial Intelligence tool, to collect data and identify the outbreaks of the dengue mosquito, zika and Chikungunya, nominee by risk areas. After the definition of the potential risk areas, in a complementary way, a totally customized Drone will be used to map these areas of interest, generating aerial photographs, identifying and geotagging the potential âtargetsâ, which will allow the agents to identify potential mosquito breeding sites. After the identification of breeding areas, the next step will be the effective combat of the vectors, using the Drones to fly over the areas of interest, where biological defenses will be âdroppedâ over the targets to combat mosquitoes. Due some Drone flight restrictions over the cities, the whole process will be monitored by a situation room, that will be able to control the Drone remotely, access the air space controller, reads the sensors installed in the city (field), that will measure, for example, rainfall through weather stations installed in risk areas and subsequently processed by Intelligent System Integrated Management (SIGI), which will result to the information public official reflecting the situational analysis of the areas, which will enable a better management of available resources, helping the public agent, preventively in the decision making.",space,129,unknown
10.1109/syscon47679.2020.9275860,to_check,2020 IEEE International Systems Conference (SysCon),IEEE,2020-09-20 00:00:00,ieeexplore,the role of attribute ranker using classification for software defect-prone data-sets model: an empirical comparative study,https://ieeexplore.ieee.org/document/9275860/,"Feature selection, is an issue firmly identified with size decrease of data-sets model. The target of feature selection is to recognize features in the data-set as significant, and dispose of some other feature as unimportant and repetitive data. Since feature selection diminishes the dimensionality of the data-sets model, it holds out the probability of increasingly successful and quick activity of data mining algorithm which can be worked quicker and all the more adequately by utilizing feature selection. In this research paper we will investigate feature extraction Principal Component Analysis (PCA) with attribute ranker search technique. In practice, Principal Component Analysis with attribute ranker search strategy isnât just used to improve extra storage space or the computational accuracy and efficiency of the classification algorithm, however can likewise enhance the prescient presentation by diminishing the scourge of dimensionality â particularly on the off chance that we are working with software defect-prone or non-defected data-sets models. We have used 10 datasets models, these datasets models basically REPOSITORY model of NASA which contain binary class defected and non-defected datasets models. We have also used 6 classifiers for comparatively analysis between objective model and real datasets model. We illustrated the comparatively analysis between PCA Ranker Search method with No-PCA. We have also compared classifiers efficiency with each other. The most efficient and useful classifier are the Bagging and Multilayer Perceptron at all attribute ranking search method. But the comparatively analysis between the classifiers that NaÃ¯ve bayes and MultiLayer Perceptron have well increased the correctly classified instances percentage in overall software fault forecast. One more comparison between the PCA Ranker Search Method and No-PCA that is attribute ranker search method is really good for increasing accuracy and efficiency for software fault forecast dataset model as compare the no-PCA method.",space,130,unknown
10.1109/ijcnn.2000.861451,to_check,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,IEEE,2000-07-27 00:00:00,ieeexplore,training neural networks with threshold activation functions and constrained integer weights,https://ieeexplore.ieee.org/document/861451/,"Evolutionary neural network training algorithms are presented. These algorithms are applied to train neural networks with weight values confined to a narrow band of integers. We constrain the weights and biases in the range [-2/sup k+1/+1, 2/sup k-1/-1], for k=3, 4, 5, thus they can be represented by just k bits. Such neural networks are better suited for hardware implementation than the real weight ones. Mathematical operations that are easy to implement in software might often be very burdensome in the hardware and therefore more costly. Hardware-friendly algorithms are essential to ensure the functionality and cost effectiveness of the hardware implementation. To this end, in addition to the integer weights, the trained neural networks use threshold activation functions only, so hardware implementation is even easier. These algorithms have been designed keeping in mind that the resulting integer weights require less bits to be stored and the digital arithmetic operations between them are easier to be implemented in hardware. Obviously, if the network is trained in a constrained weight space, smaller weights are found and less memory is required. On the other hand, as we have found here, the network training procedure can be more effective and efficient when larger weights are allowed. Thus, for a given application a trade off between effectiveness and memory consumption has to be considered. Our intention is to present results of evolutionary algorithms on this difficult task. Based on the application of the proposed class of methods on classical neural network benchmarks, our experience is that these methods are effective and reliable.",space,131,unknown
10.1109/ictai.2014.37,to_check,2014 IEEE 26th International Conference on Tools with Artificial Intelligence,IEEE,2014-11-12 00:00:00,ieeexplore,triangulation versus graph partitioning for tackling large real world qualitative spatial networks,https://ieeexplore.ieee.org/document/6984473/,"There has been interest in recent literature in tackling very large real world qualitative spatial networks, primarily because of the real datasets that have been, and are to be, offered by the Semantic Web community and scale up to millions of nodes. The proposed techniques for tackling such large networks employ the following two approaches for retaining the sparseness of their underlying graphs and reasoning with them: (i) graph triangulation and sparse matrix implementation, and (ii) graph partitioning and parallelization. Regarding the latter approach, an implementation has been offered recently, presented in [AAAI, 2014]. However, although the implementation looks promising and with space for improvement, an improper use of competing solvers in the evaluation process resulted in the wrong conclusion that it is able to provide fast consistency for very large qualitative spatial networks with respect to the state-of-the-art. In this paper, we review the two aforementioned approaches and provide new results that are different to the results presented in [AAAI, 2014] by properly re-evaluating them with the benchmark dataset of that paper. Thus, we establish a clear view on the state-of-the-art solutions for reasoning with large real world qualitative spatial networks efficiently, which is the main result of this paper.",space,132,unknown
10.1109/spawc51858.2021.9593170,to_check,2021 IEEE 22nd International Workshop on Signal Processing Advances in Wireless Communications (SPAWC),IEEE,2021-09-30 00:00:00,ieeexplore,turning channel noise into an accelerator for over-the-air principal component analysis,https://ieeexplore.ieee.org/document/9593170/,"T In recent years, the attempts on distilling mobile data into useful knowledge have led to the deployment of machine learning algorithms at the network edge. Principal component analysis (PCA) is a classic technique for extracting the linear structure of a dataset, which is useful for feature extraction and data compression. In this work, we propose the deployment of distributed PCA over a multi-access channel based on the algorithm of stochastic gradient descent to learn the dominant feature space of a distributed dataset at multiple devices. Over- the-air aggregation is adopted to reduce the multi-access latency, giving the name over-the-air PCA. The novelty of this design lies in exploiting channel noise to accelerate the descent in the region around each saddle point encountered by gradient descent, thereby increasing the convergence speed of over-the-air PCA. The idea is materialized by proposing a power-control scheme controlling the level of channel noise accordingly. The scheme is proved to achieve faster convergence than in the case without power control by experiments on real datasets.",space,133,not included
10.1109/isic.2000.882942,to_check,Proceedings of the 2000 IEEE International Symposium on Intelligent Control. Held jointly with the 8th IEEE Mediterranean Conference on Control and Automation (Cat. No.00CH37147),IEEE,2000-07-19 00:00:00,ieeexplore,two suggestions for efficient implementation of cmac's,https://ieeexplore.ieee.org/document/882942/,"The CMAC (Cerebellar Model Articulation Controller) suffers from two important problems: the huge amount of memory needed for its implementation in many common situations, and the lack of a systematic way for selecting appropriate values for its parameters, particularly number of quantization intervals. This paper presents two proposals for addressing these difficulties: 1) a dynamic implementation that requires memory only for those weights needed to represent the training data set, and that performs linear interpolation when a query using other weights is requested; and 2) consists of the definition of an index of correlation from which the optimal number of quantization intervals that should be assigned to each dimension of the input space that can be found. Experiments are performed for two synthetic cases and for one set of real data. These are used to model the dynamic behaviour of a real sensor-based car. Figures are given to show the memory savings and mean squared error obtained.",space,134,unknown
10.1109/ccnc49033.2022.9700501,to_check,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,unsupervised root-cause identification of software bugs in 5g ran,https://ieeexplore.ieee.org/document/9700501/,"Developers of complex system like 5G Radio Access Networks (RAN) need algorithms that can automatically locate the root causes of software bugs. Existing methods mainly use supervised learning to track down root causes and only a few of these provide enough information to identify the function in which a software bug occurs. Supervised learning methods work well when scenarios can be repeated, and the normal behavior is somewhat similar. In RAN, where thousands of different configurations are used, software is updated frequently, and each node has its own traffic intensity, using unsupervised learning that does not require any pre-training can be more suitable. The few existing methods that use unsupervised learning to locate the root cause of software bugs can only detect delays or software hangs, and are not able to identify the many types of bugs that occur in RAN. We propose a multi-step method that uses unsupervised learning to analyze kernel and user space traces in system logs. The methods can guide developers by suggesting top-k candidate functions that are likely to contain a software bug. Our methods, MultiSpace and CallGraph were evaluated using an advanced 5G testbed in which many different software bugs that are common in RAN, were injected. The results shows that MultiSpace and CallGraph, can detect a wider range of software bugs than previous methods and only adds an average CPU load of 1.3% on the testbed. An important aspect is also that our methods scale well with large amount of data produced by real time systems, like RAN, and can analyze the data much faster.",space,135,not included
10.1109/icnnsp.2008.4590383,to_check,2008 International Conference on Neural Networks and Signal Processing,IEEE,2008-06-11 00:00:00,ieeexplore,video object matching based on sift algorithm,https://ieeexplore.ieee.org/document/4590383/,"SIFT (Scale Invariant Feature Transform) is used to solve visual tracking problem, where the appearances of the tracked object and scene background change during tracking. The implementation of this algorithm has five major stages: scale-space extrema detection; keypoint localization; orientation assignment; keypoint descriptor; keypoint matching. From the beginning frame, object is selected as the template, its SIFT features are computed. Then in the following frames, the SIFT features are computed. Euclidean distance between the objectâs SIFT features and the framesâ SIFT features can be used to compute the accurate position of the matched object. The experimental results on real video sequences demonstrate the effectiveness of this approach and show this algorithm is of higher robustness and real-time performance. It can solve the matching problem with translation, rotation and affine distortion between images. It plays an important role in video object tracking and video object retrieval.",space,136,unknown
10.1109/iciap.2003.1234077,to_check,"12th International Conference on Image Analysis and Processing, 2003.Proceedings.",IEEE,2003-09-19 00:00:00,ieeexplore,visual self-localisation using automatic topology construction,https://ieeexplore.ieee.org/document/1234077/,"The paper proposes a machine learning method for self-localising a mobile agent, using the images supplied by a single omni-directional camera. The images acquired by the camera may be viewed as an implicit topological representation of the environment. The environment is a priori unknown and the topological representation is derived by unsupervised neural network architecture. The architecture includes a self-organising neural network, and is constituted by a growing neural gas, which is well known for its topology preserving quality. The growth depends on the topology that is not a priori defined, and on the need of discovering it, by the neural network, during the learning. The implemented system is able to recognise correctly the input frames and to reconstruct a topological map of the environment. Each node of the neural network identifies a single zone of the environment and the connections between the nodes correspond to the real space connections in the environment.",space,137,unknown
10.1109/idea49133.2020.9170686,to_check,"2nd International Conference on Data, Engineering and Applications (IDEA)",IEEE,2020-02-29 00:00:00,ieeexplore,âerror back propagation based recurrent neural networks for intrusion detection systemâ,https://ieeexplore.ieee.org/document/9170686/,"Internet usage is increasing daily, which adds to the system's vulnerability. Network security is always a major problem for network administrators and is constantly changing due to the additional application space and requirements of a smart and efficient network. Simple and more efficient software tools include victims on the security side of the protocol that hackers use to perform various types of attacks on the network. The purpose of this review is to establish and coordinate processes to prevent one member against new and known attacks, and to act as a separate security system or an independent network. The neural network connects the body's immune system to receive memory, errors, and synchronized learning. This paper discusses interrelated processes that are largely based on the application of artificial intelligence for intrusion detection and learning processes.Proposed approach finds the type of session i.e. either normal or intrusion where if intrusion found than class of intrusion was detected. Here artificial neural network was used for finding the patterns in the input data. In this work Back propagation is used for the ANN in recursive manner. Proposed algorithm gives an effective framework which has been a promising one for distinguishing interruption of various kind where, one can get the detail of the class of attack also. Experiment has been conduced on real data set where various set of testing data were pass for comparison on different evaluation parameters. Proposed approach detects all sort of attacks applied on the network such as DoS (Denial of service), (R2L) Remote to local, (U2R) User to remote, Probe etc. In this work a Random Forest Tree is used for the detection of intrusion in network. Proposed approach improved 6.87% accuracy, 12.06% Precision and 1.15% recall.",space,138,not included
10.1109/tnsre.2017.2697415,to_check,IEEE Transactions on Neural Systems and Rehabilitation Engineering,IEEE,2017-12-01 00:00:00,ieeexplore,a 128-channel fpga-based real-time spike-sorting bidirectional closed-loop neural interface system,https://ieeexplore.ieee.org/document/7908970/,"A multichannel neural interface system is an important tool for various types of neuroscientific studies. For the electrical interface with a biological system, high-precision high-speed data recording and various types of stimulation capability are required. In addition, real-time signal processing is an important feature in the implementation of a real-time closed-loop system without unwanted substantial delay for feedback stimulation. Online spike sorting, the process of assigning neural spikes to an identified group of neurons or clusters, is a necessary step to make a closed-loop path in real time, but massive memory-space requirements commonly limit hardware implementations. Here, we present a 128-channel field-programmable gate array (FPGA)-based real-time closed-loop bidirectional neural interface system. The system supports 128 channels for simultaneous signal recording and eight selectable channels for stimulation. A modular 64-channel analog front-end (AFE) provides scalability and a parameterized specification of the AFE supports the recording of various electrophysiological signal types with 1.59 Â± 0.76 $\mu {V}$ root-mean-square noise. The stimulator supports both voltage-controlled and current-controlled arbitrarily shaped waveforms with the programmable amplitude and duration of pulse. An empirical algorithm for online real-time spike sorting is implemented in an FPGA. The spike-sorting is performed by template matching, and templates are created by an online real-time unsupervised learning process. A memory saving technique, called dynamic cache organizing, is proposed to reduce the memory requirement down to 6 kbit per channel and modular implementation improves the scalability for further extensions.",space,139,unknown
10.1109/taslp.2014.2375572,to_check,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",IEEE,2015-01-01 00:00:00,ieeexplore,a recursive dialogue game for personalized computer-aided pronunciation training,https://ieeexplore.ieee.org/document/6971195/,"Learning languages in addition to the native language is very important for all people in the globalized world today, and computer-aided pronunciation training (CAPT) is attractive since the software can be used anywhere at any time, and repeated as many times as desired. In this paper, we introduce the immersive interaction scenario offered by spoken dialogues to CAPT by proposing a recursive dialogue game to make CAPT personalized. A number of tree-structured sub-dialogues are linked sequentially and recursively as the script for the game. The system policy at each dialogue turn is to select in real-time along the dialogue the best training sentence for each specific individual learner within the dialogue script, considering the learner's learning status and the future possible dialogue paths in the script, such that the learner can have the scores for all pronunciation units considered reaching a predefined standard in a minimum number of turns. The purpose here is that those pronunciation units poorly produced by the specific learner can be offered with more practice opportunities in the future sentences along the dialogue, which enables the learner to improve the pronunciation without having to repeat the same training sentences many times. This makes the learning process for each learner completely personalized. The dialogue policy is modeled by Markov decision process (MDP) with high-dimensional continuous state space, and trained with fitted value iteration using a huge number of simulated learners. These simulated leaners have the behavior similar to real learners, and were generated from a corpus of real learner data. The experiments demonstrated very promising results and a real cloud-based system is also successfully implemented.",space,140,unknown
10.1109/tase.2020.3032075,to_check,IEEE Transactions on Automation Science and Engineering,IEEE,2021-10-01 00:00:00,ieeexplore,a virtual mechanism approach for exploiting functional redundancy in finishing operations,https://ieeexplore.ieee.org/document/9246671/,"We propose a new approach to programming by the demonstration of finishing operations. Such operations can be carried out by industrial robots in multiple ways because an industrial robot is typically functionally redundant with respect to a finishing task. In the proposed system, a human expert demonstrates a finishing operation, and the demonstrated motion is recorded in the Cartesian space. The robotâs kinematic model is augmented with a virtual mechanism, which is defined according to the applied finishing tool. This way, the kinematic model is expanded with additional degrees of freedom that can be exploited to compute the optimal joint space motion of the robot without altering the essential aspects of the Cartesian space task execution as demonstrated by the human expert. Finishing operations, such as polishing and grinding, occur in contact with the treated workpiece. Since information about the contact point position is needed to control the robot during the operation, we have developed a novel approach for accurate estimation of contact points using the measured forces and torques. Finally, we applied iterative learning control to refine the demonstrated operations and compensate for inaccurate calibration and different dynamics of the robot and human demonstrator. The proposed method was verified on real robots and real polishing and grinding tasks. <i>Note to Practitioners</i>âThis work was motivated by the need for automation of finishing operations, such as polishing and grinding, on contemporary industrial robots. Existing approaches are both too complex and too time-consuming to be applied in flexible and small-scale production, which often requires the frequent deployment of new applications. Our approach is based on programming by demonstration and enables the programming of finishing operations also for users who are not specialists in robot programming. Programming by demonstration is especially useful for teaching finishing operations because it enables the transfer of expert knowledge about finishing skills to robots without providing lengthy task descriptions or manual coding. Besides the human demonstration of the desired operation, the proposed approach also requires the availability of the kinematic model for the machine tool applied to carry out the finishing operation. We provide several practical examples of grinding and polishing tools and how to integrate them into our approach. Another feature of the proposed system is that user demonstrations of finishing operations can be transferred between different combinations of robots and machine tools.",space,141,included
10.1109/42.563667,to_check,IEEE Transactions on Medical Imaging,IEEE,1997-04-01 00:00:00,ieeexplore,a focus-of-attention preprocessing scheme for em-ml pet reconstruction,https://ieeexplore.ieee.org/document/563667/,"The expectation-maximization maximum-likelihood (EM-ML) algorithm belongs to a family of algorithms that compute positron emission tomography (PET) reconstructions by iteratively solving a large linear system of equations. The authors describe a preprocessing scheme for automatically focusing the attention, and thus the computational resources, on a subset of the equations and unknowns. Experimental work with a CM-5 parallel computer implementation using a simulated phantom as well as real data obtained from an ECAT 921 PET scanner indicates that quite significant savings can be obtained with respect to both time and space requirements of the EM-ML algorithm without compromising the quality of the reconstructed images.",space,142,unknown
10.1109/access.2020.3005513,to_check,IEEE Access,IEEE,2020-01-01 00:00:00,ieeexplore,analysis and design of computational news angles,https://ieeexplore.ieee.org/document/9127417/,"A key skill for a journalist is the ability to assess the newsworthiness of an event or situation. To this purpose journalists often rely on news angles, conceptual criteria that are used both i) to assess whether something is newsworthy and also ii) to shape the structure of the resulting news item. As journalism becomes increasingly computer-supported, and more and more sources of potentially newsworthy data become available in real time, it makes sense to try and equip journalistic software tools with operational versions of news angles, so that, when searching this vast data space, these tools can both identify effectively the events most relevant to the target audience, and also link them to appropriate news angles. In this paper we analyse the notion of news angle and, in particular, we i) introduce a formal framework and data schema for representing news angles and related concepts and ii) carry out a preliminary analysis and characterization of a number of commonly used news angles, both in terms of our formal model and also in terms of the computational reasoning capabilities that are needed to apply them effectively to real-world scenarios. This study provides a stepping stone towards our ultimate goal of realizing a solution capable of exploiting a library of news angles to identify potentially newsworthy events in a large journalistic data space.",space,143,unknown
10.1109/tnnls.2018.2854796,to_check,IEEE Transactions on Neural Networks and Learning Systems,IEEE,2019-03-01 00:00:00,ieeexplore,asymptotically optimal contextual bandit algorithm using hierarchical structures,https://ieeexplore.ieee.org/document/8424511/,"We propose an online algorithm for sequential learning in the contextual multiarmed bandit setting. Our approach is to partition the context space and, then, optimally combine all of the possible mappings between the partition regions and the set of bandit arms in a data-driven manner. We show that in our approach, the best mapping is able to approximate the best arm selection policy to any desired degree under mild Lipschitz conditions. Therefore, we design our algorithm based on the optimal adaptive combination and asymptotically achieve the performance of the best mapping as well as the best arm selection policy. This optimality is also guaranteed to hold even in adversarial environments since we do not rely on any statistical assumptions regarding the contexts or the loss of the bandit arms. Moreover, we design an efficient implementation for our algorithm using various hierarchical partitioning structures, such as lexicographical or arbitrary position splitting and binary trees (BTs) (and several other partitioning examples). For instance, in the case of BT partitioning, the computational complexity is only log-linear in the number of regions in the finest partition. In conclusion, we provide significant performance improvements by introducing upper bounds (with respect to the best arm selection policy) that are mathematically proven to vanish in the average loss per round sense at a faster rate compared to the state of the art. Our experimental work extensively covers various scenarios ranging from bandit settings to multiclass classification with real and synthetic data. In these experiments, we show that our algorithm is highly superior to the state-of-the-art techniques while maintaining the introduced mathematical guarantees and a computationally decent scalability.",space,144,unknown
10.1109/tro.2021.3084374,to_check,IEEE Transactions on Robotics,IEEE,2022-02-01 00:00:00,ieeexplore,cat-like jumping and landing of legged robots in low gravity using deep reinforcement learning,https://ieeexplore.ieee.org/document/9453856/,"In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we train a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of 3-D (re)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for 2-D microgravity experiments. The experimental results demonstrate that repetitive controlled jumping and landing with natural agility is possible.",space,145,unknown
10.1109/tsmc.2020.2967936,to_check,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",IEEE,2021-12-01 00:00:00,ieeexplore,deep q-learning with q-matrix transfer learning for novel fire evacuation environment,https://ieeexplore.ieee.org/document/8989970/,"Deep reinforcement learning (RL) is achieving significant success in various applications like control, robotics, games, resource management, and scheduling. However, the important problem of emergency evacuation, which clearly could benefit from RL, has been largely unaddressed. Indeed, emergency evacuation is a complex task that is difficult to solve with RL. An emergency situation is highly dynamic, with a lot of changing variables and complex constraints that make it challenging to solve. Also, there is no standard benchmark environment available that can be used to train RL agents for evacuation. A realistic environment can be complex to design. In this article, we propose the first fire evacuation environment to train RL agents for evacuation planning. The environment is modeled as a graph capturing the building structure. It consists of realistic features like fire spread, uncertainty, and bottlenecks. The implementation of our environment is in the OpenAI gym format, to facilitate future research. We also propose a new RL approach that entails pretraining the network weights of a DQN-based agent [DQN/Double-DQN (DDQN)/Dueling-DQN] to incorporate information on the shortest path to the exit. We achieved this by using tabular <inline-formula> <tex-math notation=""LaTeX"">$Q$ </tex-math></inline-formula>-learning to learn the shortest path on the building modelâs graph. This information is transferred to the network by deliberately overfitting it on the <inline-formula> <tex-math notation=""LaTeX"">$Q$ </tex-math></inline-formula>-matrix. Then, the pretrained DQN model is trained on the fire evacuation environment to generate the optimal evacuation path under time varying conditions due to fire spread, bottlenecks, and uncertainty. We perform comparisons of the proposed approach with state-of-the-art RL algorithms like DQN, DDQN, Dueling-DQN, PPO, VPG, state-action-reward-state-action (SARSA), actorâcritic method, and ACKTR. The results show that our method is able to outperform state-of-the-art models by a huge margin including the original DQN-based models. Finally, our model is tested on a large and complex real building consisting of 91 rooms, with the possibility to move to any other room, hence giving 8281 actions. In order to reduce the action space, we propose a strategy that involves one step simulation. That is, an action importance vector is added to the final output of the pretrained DQN and acts like an attention mechanism. Using this strategy, the action space is reduced by 90.1%. In this manner, the model is able to deal with large action spaces. Hence, our model achieves near optimal performance on the real world emergency environment.",space,146,unknown
10.1109/access.2017.2768405,to_check,IEEE Access,IEEE,2017-01-01 00:00:00,ieeexplore,hand-mouse interface using virtual monitor concept for natural interaction,https://ieeexplore.ieee.org/document/8091117/,"The growing interest in human-computer interaction has prompted research in this area. In addition, research has been conducted on a natural user interface/natural user experience (NUI/NUX), which utilizes a user's gestures and voice. In the case of NUI/NUX, a recognition algorithm is needed for the gestures or voice. However, such recognition algorithms have weaknesses because their implementation is complex, and they require a large amount of time for training. Therefore, steps that include pre-processing, normalization, and feature extraction are needed. In this paper, we designed and implemented a hand-mouse interface that introduces a new concept called a âvirtual monitorâ, to extract a user's physical features through Kinect in real time. This virtual monitor allows a virtual space to be controlled by the hand mouse. It is possible to map the coordinates on the virtual monitor to the coordinates on the real monitor accurately. A hand-mouse interface based on the virtual monitor concept maintains the outstanding intuitiveness that is the strength of the previous study and enhances the accuracy of mouse functions. In order to evaluate the intuitiveness and accuracy of the interface, we conducted an experiment with 50 volunteers ranging from teenagers to those in their 50s. The results of this intuitiveness experiment showed that 84% of the subjects learned how to use the mouse within 1 min. In addition, the accuracy experiment showed the high accuracy level of the mouse functions [drag (80.9%), click (80%), double-click (76.7%)]. This is a good example of an interface for controlling a system by hand in the future.",space,147,unknown
10.1109/3477.499796,to_check,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",IEEE,1996-06-01 00:00:00,ieeexplore,hidden state and reinforcement learning with instance-based state identification,https://ieeexplore.ieee.org/document/499796/,"Real robots with real sensors are not omniscient. When a robot's next course of action depends on information that is hidden from the sensors because of problems such as occlusion, restricted range, bounded field of view and limited attention, we say the robot suffers from the hidden state problem. State identification techniques use history information to uncover hidden state. Some previous approaches to encoding history include: finite state machines, recurrent neural networks and genetic programming with indexed memory. A chief disadvantage of all these techniques is their long training time. This paper presents instance-based state identification, a new approach to reinforcement learning with state identification that learns with much fewer training steps. Noting that learning with history and learning in continuous spaces both share the property that they begin without knowing the granularity of the state space, the approach applies instance-based (or ""memory-based"") learning to history sequences-instead of recording instances in a continuous geometrical space, we record instances in action-percept-reward sequence space. The first implementation of this approach, called Nearest Sequence Memory, learns with an order of magnitude fewer steps than several previous approaches.",space,148,unknown
10.1109/access.2017.2690987,to_check,IEEE Access,IEEE,2017-01-01 00:00:00,ieeexplore,nc-link: a new linkage method for efficient hierarchical clustering of large-scale data,https://ieeexplore.ieee.org/document/7891959/,"In various disciplines, hierarchical clustering (HC) has been an effective tool for data analysis due to its ability to summarize hierarchical structures of data in an intuitive and interpretable manner. A run of HC requires multiple iterations, each of which needs to compute and update the pairwise distances between all intermediate clusters. This makes the exact algorithm for HC inevitably suffer from quadratic time and space complexities. To address large-scale data, various approximate/parallel algorithms have been proposed to reduce the computational cost of HC. However, such algorithms still rely on conventional linkage methods (such as single, centroid, average, complete, or Ward's) for defining pairwise distances, mostly focusing on the approximation/parallelization of linkage computations. Given that the choice of linkage profoundly affects not only the quality but also the efficiency of HC, we propose a new linkage method named NC-link and design an exact algorithm for NC-link-based HC. To guarantee the exactness, the proposed algorithm maintains the quadratic nature in time complexity but exhibits only linear space complexity, thereby allowing us to address million-object data on a personal computer. To underpin the extensibility of our approach, we showthat the algorithmic nature of NC-link enables single instruction multiple data (SIMD) parallelization and subquadratic-time approximation of HC. To verify our proposal, we thoroughly tested it with a number of large-scale real and synthetic data sets. In terms of efficiency, NC-link allowed us to perform HC substantially more space efficiently or faster than conventional methods: compared with average and complete linkages, using NC-link incurred only 0.7%-1.75% of the memory usage, and the NC-link-based implementation delivered speedups of approximately 3.5 times over the centroid and Ward's linkages. With regard to clustering quality, the proposed method was able to retrieve hierarchical structures from input data as faithfully as in the popular average and centroid linkage methods. We anticipate that the existing approximation/parallel algorithms will be able to benefit from adopting NC-link as their linkage method for obtaining better clustering results and reduced time and space demands.",space,149,unknown
10.1109/jproc.2018.2856739,to_check,Proceedings of the IEEE,IEEE,2018-11-01 00:00:00,ieeexplore,navigating the landscape for real-time localization and mapping for robotics and virtual and augmented reality,https://ieeexplore.ieee.org/document/8436423/,"Visual understanding of 3-D environments in real time, at low power, is a huge computational challenge. Often referred to as simultaneous localization and mapping (SLAM), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, and virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are: 1) tools and methodology for systematic quantitative evaluation of SLAM algorithms; 2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives; 3) end-to-end simulation tools to enable optimization of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches; and 4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",space,150,unknown
10.1109/tnnls.2015.2451535,to_check,IEEE Transactions on Neural Networks and Learning Systems,IEEE,2016-07-01 00:00:00,ieeexplore,near-optimal controller for nonlinear continuous-time systems with unknown dynamics using policy iteration,https://ieeexplore.ieee.org/document/7174547/,"This paper presents a single-network adaptive critic-based controller for continuous-time systems with unknown dynamics in a policy iteration (PI) framework. It is assumed that the unknown dynamics can be estimated using the Takagi-Sugeno-Kang fuzzy model with arbitrary precision. The successful implementation of a PI scheme depends on the effective learning of critic network parameters. Network parameters must stabilize the system in each iteration in addition to approximating the critic and the cost. It is found that the critic updates according to the Hamilton-Jacobi-Bellman formulation sometimes lead to the instability of the closed-loop systems. In the proposed work, a novel critic network parameter update scheme is adopted, which not only approximates the critic at current iteration but also provides feasible solutions that keep the policy stable in the next step of training by combining a Lyapunov-based linear matrix inequalities approach with PI. The critic modeling technique presented here is the first of its kind to address this issue. Though multiple literature exists discussing the convergence of PI, however, to the best of our knowledge, there exists no literature, which focuses on the effect of critic network parameters on the convergence. Computational complexity in the proposed algorithm is reduced to the order of (F<sub>z</sub>)<sup>n-1</sup>, where n is the fuzzy state dimensionality and F<sub>z</sub> is the number of fuzzy zones in the states space. A genetic algorithm toolbox of MATLAB is used for searching stable parameters while minimizing the training error. The proposed algorithm also provides a way to solve for the initial stable control policy in the PI scheme. The algorithm is validated through real-time experiment on a commercial robotic manipulator. Results show that the algorithm successfully finds stable critic network parameters in real time for a highly nonlinear system.",space,151,unknown
10.1109/tcad.2012.2188401,to_check,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,IEEE,2012-07-01 00:00:00,ieeexplore,on-chip network-enabled multicore platforms targeting maximum likelihood phylogeny reconstruction,https://ieeexplore.ieee.org/document/6218233/,"In phylogenetic inference, which aims at finding a phylogenetic tree that best explains the evolutionary relationship among a given set of species, statistical estimation approaches such as maximum likelihood (ML) and Bayesian inference provide more accurate estimates than other nonstatistical approaches. However, the improved quality comes at a higher computational cost, as these approaches, even though heuristic driven, involve optimization over multidimensional real continuous space. The number of possible search trees in ML is at least exponential, thereby making runtimes on even modest-sized datasets to clock up to several million CPU hours. Evaluation of these trees, involving node-level likelihood vector computation and branch-length optimization, can be partitioned into tasks (or kernels), providing the application with the potential to benefit from hardware acceleration. The range of hardware acceleration architectures tried so far offer limited degree of fine-grain parallelism. Network-on-chip (NoC) is an emerging paradigm that can efficiently support integration of massive number of cores on a chip. In this paper, we explore the design and performance evaluation of 2-D and 3-D NoC architectures for RAxML, which is one of the most widely used ML software suites. Specifically, we implement the computation kernels of the top three functions consuming more than 85% of the total software runtime. Simulations show that through appropriate choice of NoC architecture, and novel core design, allocation and placement strategies, our NoC-based implementation can achieve individual function-level speedups of 390x to 847x, speed up the targeted kernels in excess of 6500x, and provide end-to-end runtime reductions up to 5x over state-of-the-art multithreaded software.",space,152,unknown
10.1109/access.2018.2875677,to_check,IEEE Access,IEEE,2018-01-01 00:00:00,ieeexplore,patient2vec: a personalized interpretable deep representation of the longitudinal electronic health record,https://ieeexplore.ieee.org/document/8490816/,"The wide implementation of electronic health record (EHR) systems facilitates the collection of large-scale health data from real clinical settings. Despite the significant increase in adoption of EHR systems, these data remain largely unexplored, but present a rich data source for knowledge discovery from patient health histories in tasks, such as understanding disease correlations and predicting health outcomes. However, the heterogeneity, sparsity, noise, and bias in these data present many complex challenges. This complexity makes it difficult to translate potentially relevant information into machine learning algorithms. In this paper, we propose a computational framework, Patient2Vec, to learn an interpretable deep representation of longitudinal EHR data, which is personalized for each patient. To evaluate this approach, we apply it to the prediction of future hospitalizations using real EHR data and compare its predictive performance with baseline methods. Patient2Vec produces a vector space with meaningful structure, and it achieves an area under curve around 0.799, outperforming baseline methods. In the end, the learned feature importance can be visualized and interpreted at both the individual and population levels to bring clinical insights.",space,153,unknown
10.1109/tim.2021.3092518,to_check,IEEE Transactions on Instrumentation and Measurement,IEEE,2021-01-01 00:00:00,ieeexplore,pipeline safety early warning by multifeature-fusion cnn and lightgbm analysis of signals from distributed optical fiber sensors,https://ieeexplore.ieee.org/document/9541184/,"Energy pipelines are the backbones of global energy systems. Monitoring their safety and automatically identifying and locating third-party damage events are crucial to energy supply. However, most traditional methods lack in-depth consideration of distributed fiber signals and have not been tested on real-world long-distance pipelines, making it difficult to deploy them in operating long-distance pipelines. In this study, we utilize a novel real-time machine-learning method based on phase-sensitive optical time domain reflectometer technology to monitor the safety of oil and gas pipelines. Specifically, we build a multifeature-fusion convolutional neural network and LightGBM fusion model based on two novel complementary spatiotemporal features. The method was applied to a large amount of data collected from real-world oilâgas transportation pipelines of the China National Petroleum Corporation. The proposed method could accurately locate and identify third-party damage events in real-time under conditions of strong noise and various types of system hardware, and could effectively handle signal drift in the time and space dimensions. Our methodology has been deployed at real long-distance energy pipeline sites and our work will contribute to energy pipeline safety and energy supply security. Furthermore, the proposed solution could be generalized to other fields, such as industrial inspection, measurement, and monitoring.",space,154,unknown
10.1109/tcyb.2013.2275291,to_check,IEEE Transactions on Cybernetics,IEEE,2013-10-01 00:00:00,ieeexplore,real-time multiple human perception with color-depth cameras on a mobile robot,https://ieeexplore.ieee.org/document/6583249/,"The ability to perceive humans is an essential requirement for safe and efficient human-robot interaction. In real-world applications, the need for a robot to interact in real time with multiple humans in a dynamic, 3-D environment presents a significant challenge. The recent availability of commercial color-depth cameras allow for the creation of a system that makes use of the depth dimension, thus enabling a robot to observe its environment and perceive in the 3-D space. Here we present a system for 3-D multiple human perception in real time from a moving robot equipped with a color-depth camera and a consumer-grade computer. Our approach reduces computation time to achieve real-time performance through a unique combination of new ideas and established techniques. We remove the ground and ceiling planes from the 3-D point cloud input to separate candidate point clusters. We introduce the novel information concept, depth of interest, which we use to identify candidates for detection, and that avoids the computationally expensive scanning-window methods of other approaches. We utilize a cascade of detectors to distinguish humans from objects, in which we make intelligent reuse of intermediary features in successive detectors to improve computation. Because of the high computational cost of some methods, we represent our candidate tracking algorithm with a decision directed acyclic graph, which allows us to use the most computationally intense techniques only where necessary. We detail the successful implementation of our novel approach on a mobile robot and examine its performance in scenarios with real-world challenges, including occlusion, robot motion, nonupright humans, humans leaving and reentering the field of view (i.e., the reidentification challenge), human-object and human-human interaction. We conclude with the observation that the incorporation of the depth information, together with the use of modern techniques in new ways, we are able to create an accurate system for real-time 3-D perception of humans by a mobile robot.",space,155,unknown
10.1109/jiot.2018.2812210,to_check,IEEE Internet of Things Journal,IEEE,2018-10-01 00:00:00,ieeexplore,sdcor: software defined cognitive routing for internet of vehicles,https://ieeexplore.ieee.org/document/8306876/,"The Internet of Vehicles (IoV) is a subapplication of the Internet of Things in the automotive field. Large amounts of sensor data require to be transferred in real-time. Most of the routing protocols are specifically targeted to specific situations in IoV. But communication environment of IoV usually changes in the space-time dimension. Unfortunately, the traditional vehicular networks cannot select the optimal routing policy when facing the dynamic environment, due to the lack of abilities of sensing the environment and learning the best strategy. Sensing and learning constitute two key steps of the cognition procedure. Thus, in this paper, we present a software defined cognitive network for IoV (SDCIV), in which reinforcement learning and software defined network technology are considered for IoV to achieve cognitive capability. To the best of our knowledge, this paper is the first one that can give the optimal routing policy adaptively through sensing and learning from the environment of IoV. We perform experiments on a real vehicular dataset to validate the effectiveness and feasibility of the proposed algorithm. Results show that our algorithm achieves better performance than several typical protocols in IoV. We also show the feasibility and effectiveness of our proposed SDCIV.",space,156,unknown
10.1109/tsc.2017.2777478,to_check,IEEE Transactions on Services Computing,IEEE,2021-02-01 00:00:00,ieeexplore,solar: services-oriented deep learning architectures-deep learning as a service,https://ieeexplore.ieee.org/document/8119814/,"Deep learning has been an emerging field of machine learning during past decades. However, the diversity and large scale data size have posed significant challenge to construct a flexible and high performance implementations of deep learning neural networks. In order to improve the performance as well to maintain the scalability, in this paper we present SOLAR, a services-oriented deep learning architecture using various accelerators like GPU and FPGA. SOLAR provides a uniform programming model to users so that the hardware implementation and the scheduling is invisible to the programmers. At runtime, the services can be executed either on the software processors or the hardware accelerators. To leverage the trade-offs between the metrics among performance, power, energy, and efficiency, we present a multitarget design space exploration. Experimental results on the real state-of-the-art FPGA board demonstrate that the SOLAR is able to provide a ubiquitous framework for diverse applications without increasing the burden of the programmers. Moreover, the speedup of the GPU and FPGA hardware accelerator in SOLAR can achieve significant speedup comparing to the conventional Intel i5 processors with great scalability.",space,157,unknown
10.1109/robot.1986.1087518,to_check,Proceedings. 1986 IEEE International Conference on Robotics and Automation,IEEE,1986-04-10 00:00:00,ieeexplore,architecture and early experience with planning for the alv,https://ieeexplore.ieee.org/document/1087518/,"This paper describes the software architecture and the initial algorithms that have proved to be effective for a real time robot planning system. The architecture is designed to incorporate planning technology from research on artificial intelligence while at the same time supporting the high performance decision making needed to control a fast-moving autonomous vehicle. The symbolic representation of the vehicle's plan is a key element in this architecture. Our initial algorithms use an especially efficient version of dynamic programming to find the best routes. The route is then translated into a symbolic plan. Replanning happens at several levels with the cost of replanning proportionate to the scope of the changes. This software is currently running in an environment which simulates the vehicle and perception systems, but it will be transferred to the DARPA Autonomous Land Vehicle built by Martin Marietta Denver Aerospace [Lowrie 86].",space,158,unknown
10.1109/aiam54119.2021.00049,to_check,2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture (AIAM),IEEE,2021-10-25 00:00:00,ieeexplore,design of online measurement system for molten pool morphology in selective laser melting (slm) process,https://ieeexplore.ieee.org/document/9724859/,"Selective Laser Melting (SLM), as the processing technology with the highest forming accuracy and the best processing quality in additive manufacturing, is widely used in aerospace fields, etc. Laser molten pool is the basic unit of SLM processing, and online monitoring of the laser molten pool morphology can reflect the SLM processing accuracy and process stability in real time. In this paper, an online measurement system for side-axis laser molten pool morphology is developed inside the SLM molding cabin. Temperature isolation and sealing protection is designed for the online measurement system under complex high-temperature environments. It uses high-speed COMS camera and image acquisition card equipped with FPGA chips to collect and transmit the molten pool image information in real time. The software design of the online measurement system includes the development of the camera SDK based on Visual Studio, the design of real-time image processing algorithms, the evaluation of the molten pool, etc., which aims to evaluate the image acquisition, transmission and storage of the molten pool. Through simulation analysis and experimental verification, it is proved that the system can allow on-line measurement of molten pool morphology.",space,159,unknown
10.1109/ijcnn.2013.6706957,to_check,The 2013 International Joint Conference on Neural Networks (IJCNN),IEEE,2013-08-09 00:00:00,ieeexplore,optimized neuro genetic fast estimator (ongfe) for efficient distributed intelligence instantiation within embedded systems,https://ieeexplore.ieee.org/document/6706957/,"The Optimized Neuro Genetic Fast Estimator (ONGFE) is a software tool that allows for embedding system, subsystem, and component failure detection, identification, and prognostics (FDI&amp;P) capability by using Intelligent Software Elements (ISE) based upon Artificial Neural Networks (ANN). With an Application Programming Interface (API), highly innovative algorithms are compiled for efficient distributed intelligence instantiation within embedded systems. The original design had the purpose of providing a real time kernel to deploy health monitoring functions for Condition Based Maintenance (CBM) and Real Time Monitoring (RTM) systems in a broad variety of applications (such as aerospace, structural, and widely distributed support systems). The ONGFE contains embedded fast and on-line training for designing ANNs to perform several high performance FDI&amp;P functions. A key advantage of this technology is an optimization block based upon pseudogenetic algorithms which compensate for effects due to initial weight values and local minimums without the computational burden of genetic algorithms. The ONGFE also provides a synchronization block for communication with secondary diagnostic modules. The algorithms are designed for a distributed, scalar, and modular deployment. Based on this technology, a scheme for conducting sensor data validation has been embedded in Smart Sensors.",space,160,unknown
10.1109/dasc52595.2021.9594351,to_check,2021 IEEE/AIAA 40th Digital Avionics Systems Conference (DASC),IEEE,2021-10-07 00:00:00,ieeexplore,seda: a self-explaining decision architecture implemented using deep learning for on-board command and control,https://ieeexplore.ieee.org/document/9594351/,"Machine learning (ML) is a powerful tool for solving stochastic optimization problems. The aerospace and defense sectors have a number of stochastic optimization problems that would benefit from the application of ML; however, people often have difficulties interpreting solutions arrived at via ML, which undermines trust, producing an obstacle to widespread adoption in these sectors. This paper introduces the Self-Explaining Decision Architecture (SEDA) for ML-based decision-making systems capable of generating intuitive explanations for their decisions in real time. SEDA makes use of a feature extraction subsystem and a sequence interpretation subsystem to identify patterns in data followed by a decision generation subsystem that determines appropriate actions based on those patterns. Internal state information from each of these subsystems is used to generate explanations of the systemâs decisions. Using this information to create explanations provides insight as to the data elements the system focused on when making decisions as well as the reasoning that was used. As a proof-of-concept, we present a first implementation of SEDA using start-of-the-art deep learning components including a combined convolutional neural network and long short-term memory network with attention mechanisms and demonstrate its use on both standard and custom datasets.",space,161,included
10.1109/rast.2005.1512560,to_check,"Proceedings of 2nd International Conference on Recent Advances in Space Technologies, 2005. RAST 2005.",IEEE,2005-06-11 00:00:00,ieeexplore,autonomous onboard computer systems using real time trace models,https://ieeexplore.ieee.org/document/1512560/,"In the paper some particularities of the autonomous spacecraft computer systems are discussed. The ways to reach the autonomy are described - high reliability and adaptability of the system, reached by application of AI elements formal structural models (specification, verification and real time control) and dynamic reconfiguration. A method using trace models In real time for testing of system operations is described. The method is suitable for systems specified by process algebras (CSP - Communicating Sequential Processes, Timed CSP, ASM - Abstract State Machine, and TAM - Temporal Agent Model) where process tracts are used. The computing processes and these in the hardware are observed. Special control processes (tracers) are specified. They allow to control in real time the correspondence of the real system traces and preliminary computed traces (the formal specification) and send messages to the other subsystems when no correspondence (e.g. to specification and dynamic reconfiguration subsystems). The method allows finding in real, time the software and hardware incorrectly functioning (disparity of the traces) based on rigorous mathematical specification. In the paper are described: an autonomous control system structural model, an algorithm for real time traces control, software simulations (laboratory models) of autonomous systems, the faults and reaction of the tracer-processes, all based on CPPCSP - C++ Communicating Sequential Processes library. The application of trace models in real time allows find any system incorrectly operating and this is one of the ways for,increasing the onboard computer systeme reliability and to achieve full operation autonomy.",space,162,unknown
http://arxiv.org/abs/2111.07171v1,to_check,arxiv,arxiv,2021-11-13 00:00:00,arxiv,"deep reinforcement learning with shallow controllers: an experimental
  application to pid tuning",http://arxiv.org/abs/2111.07171v1,"Deep reinforcement learning (RL) is an optimization-driven framework for
producing control strategies for general dynamical systems without explicit
reliance on process models. Good results have been reported in simulation. Here
we demonstrate the challenges in implementing a state of the art deep RL
algorithm on a real physical system. Aspects include the interplay between
software and existing hardware; experiment design and sample efficiency;
training subject to input constraints; and interpretability of the algorithm
and control law. At the core of our approach is the use of a PID controller as
the trainable RL policy. In addition to its simplicity, this approach has
several appealing features: No additional hardware needs to be added to the
control system, since a PID controller can easily be implemented through a
standard programmable logic controller; the control law can easily be
initialized in a ""safe'' region of the parameter space; and the final product
-- a well-tuned PID controller -- has a form that practitioners can reason
about and deploy with confidence.",space,163,included
http://arxiv.org/abs/2110.06196v1,to_check,arxiv,arxiv,2021-10-12 00:00:00,arxiv,grape: fast and scalable graph processing and embedding,http://arxiv.org/abs/2110.06196v1,"Graph Representation Learning methods have enabled a wide range of learning
problems to be addressed for data that can be represented in graph form.
Nevertheless, several real world problems in economy, biology, medicine and
other fields raised relevant scaling problems with existing methods and their
software implementation, due to the size of real world graphs characterized by
millions of nodes and billions of edges. We present GraPE, a software resource
for graph processing and random walk based embedding, that can scale with large
and high-degree graphs and significantly speed up-computation. GraPE comprises
specialized data structures, algorithms, and a fast parallel implementation
that displays everal orders of magnitude improvement in empirical space and
time complexity compared to state of the art software resources, with a
corresponding boost in the performance of machine learning methods for edge and
node label prediction and for the unsupervised analysis of graphs.GraPE is
designed to run on laptop and desktop computers, as well as on high performance
computing clusters",space,164,unknown
http://arxiv.org/abs/2110.04003v2,to_check,arxiv,arxiv,2021-10-08 00:00:00,arxiv,learning to centralize dual-arm assembly,http://arxiv.org/abs/2110.04003v2,"Robotic manipulators are widely used in modern manufacturing processes.
However, their deployment in unstructured environments remains an open problem.
To deal with the variety, complexity, and uncertainty of real-world
manipulation tasks, it is essential to develop a flexible framework with
reduced assumptions on the environment characteristics. In recent years,
reinforcement learning (RL) has shown great results for single-arm robotic
manipulation. However, research focusing on dual-arm manipulation is still
rare. From a classical control perspective, solving such tasks often involves
complex modeling of interactions between two manipulators and the objects
encountered in the tasks, as well as the two robots coupling at a control
level. Instead, in this work, we explore the applicability of model-free RL to
dual-arm assembly. As we aim to contribute towards an approach that is not
limited to dual-arm assembly, but dual-arm manipulation in general, we keep
modeling efforts at a minimum. Hence, to avoid modeling the interaction between
the two robots and the used assembly tools, we present a modular approach with
two decentralized single-arm controllers which are coupled using a single
centralized learned policy. We reduce modeling effort to a minimum by using
sparse rewards only. Our architecture enables successful assembly and simple
transfer from simulation to the real world. We demonstrate the effectiveness of
the framework on dual-arm peg-in-hole and analyze sample efficiency and success
rates for different action spaces. Moreover, we compare results on different
clearances and showcase disturbance recovery and robustness, when dealing with
position uncertainties. Finally we zero-shot transfer policies trained in
simulation to the real world and evaluate their performance.",space,165,unknown
http://arxiv.org/abs/2110.03979v2,to_check,arxiv,arxiv,2021-10-08 00:00:00,arxiv,"millitrace-ir: contact tracing and temperature screening via mm-wave and
  infrared sensing",http://arxiv.org/abs/2110.03979v2,"Social distancing and temperature screening have been widely employed to
counteract the COVID-19 pandemic, sparking great interest from academia,
industry and public administrations worldwide. While most solutions have dealt
with these aspects separately, their combination would greatly benefit the
continuous monitoring of public spaces and help trigger effective
countermeasures. This work presents milliTRACE-IR, a joint mmWave radar and
infrared imaging sensing system performing unobtrusive and privacy preserving
human body temperature screening and contact tracing in indoor spaces.
milliTRACE-IR combines, via a robust sensor fusion approach, mmWave radars and
infrared thermal cameras. It achieves fully automated measurement of distancing
and body temperature, by jointly tracking the subjects's faces in the thermal
camera image plane and the human motion in the radar reference system.
Moreover, milliTRACE-IR performs contact tracing: a person with high body
temperature is reliably detected by the thermal camera sensor and subsequently
traced across a large indoor area in a non-invasive way by the radars. When
entering a new room, a subject is re-identified among several other individuals
by computing gait-related features from the radar reflections through a deep
neural network and using a weighted extreme learning machine as the final
re-identification tool. Experimental results, obtained from a real
implementation of milliTRACE-IR, demonstrate decimeter-level accuracy in
distance/trajectory estimation, inter-personal distance estimation (effective
for subjects getting as close as 0.2 m), and accurate temperature monitoring
(max. errors of 0.5{\deg}C). Furthermore, milliTRACE-IR provides contact
tracing through highly accurate (95%) person re-identification, in less than 20
seconds.",space,166,unknown
http://arxiv.org/abs/2108.01846v2,to_check,arxiv,arxiv,2021-08-04 00:00:00,arxiv,"learning barrier certificates: towards safe reinforcement learning with
  zero training-time violations",http://arxiv.org/abs/2108.01846v2,"Training-time safety violations have been a major concern when we deploy
reinforcement learning algorithms in the real world. This paper explores the
possibility of safe RL algorithms with zero training-time safety violations in
the challenging setting where we are only given a safe but trivial-reward
initial policy without any prior knowledge of the dynamics model and additional
offline data. We propose an algorithm, Co-trained Barrier Certificate for Safe
RL (CRABS), which iteratively learns barrier certificates, dynamics models, and
policies. The barrier certificates, learned via adversarial training, ensure
the policy's safety assuming calibrated learned dynamics model. We also add a
regularization term to encourage larger certified regions to enable better
exploration. Empirical simulations show that zero safety violations are already
challenging for a suite of simple environments with only 2-4 dimensional state
space, especially if high-reward policies have to visit regions near the safety
boundary. Prior methods require hundreds of violations to achieve decent
rewards on these tasks, whereas our proposed algorithms incur zero violations.",space,167,unknown
http://arxiv.org/abs/2106.09357v1,to_check,arxiv,arxiv,2021-06-17 00:00:00,arxiv,"cat-like jumping and landing of legged robots in low-gravity using deep
  reinforcement learning",http://arxiv.org/abs/2106.09357v1,"In this article, we show that learned policies can be applied to solve legged
locomotion control tasks with extensive flight phases, such as those
encountered in space exploration. Using an off-the-shelf deep reinforcement
learning algorithm, we trained a neural network to control a jumping quadruped
robot while solely using its limbs for attitude control. We present tasks of
increasing complexity leading to a combination of three-dimensional
(re-)orientation and landing locomotion behaviors of a quadruped robot
traversing simulated low-gravity celestial bodies. We show that our approach
easily generalizes across these tasks and successfully trains policies for each
case. Using sim-to-real transfer, we deploy trained policies in the real world
on the SpaceBok robot placed on an experimental testbed designed for
two-dimensional micro-gravity experiments. The experimental results demonstrate
that repetitive, controlled jumping and landing with natural agility is
possible.",space,168,unknown
http://arxiv.org/abs/2104.02306v1,to_check,arxiv,arxiv,2021-04-06 00:00:00,arxiv,binary neural network for speaker verification,http://arxiv.org/abs/2104.02306v1,"Although deep neural networks are successful for many tasks in the speech
domain, the high computational and memory costs of deep neural networks make it
difficult to directly deploy highperformance Neural Network systems on
low-resource embedded devices. There are several mechanisms to reduce the size
of the neural networks i.e. parameter pruning, parameter quantization, etc.
This paper focuses on how to apply binary neural networks to the task of
speaker verification. The proposed binarization of training parameters can
largely maintain the performance while significantly reducing storage space
requirements and computational costs. Experiment results show that, after
binarizing the Convolutional Neural Network, the ResNet34-based network
achieves an EER of around 5% on the Voxceleb1 testing dataset and even
outperforms the traditional real number network on the text-dependent dataset:
Xiaole while having a 32x memory saving.",space,169,unknown
http://arxiv.org/abs/2103.07541v1,to_check,arxiv,arxiv,2021-03-12 00:00:00,arxiv,"machine learning aided k-t sense for fast reconstruction of highly
  accelerated pcmr data",http://arxiv.org/abs/2103.07541v1,"Purpose: We implemented the Machine Learning (ML) aided k-t SENSE
reconstruction to enable high resolution quantitative real-time phase contrast
MR (PCMR). Methods: A residual U-net and our U-net M were used to generate the
high resolution x-f space estimate for k-t SENSE regularisation prior. The
networks were judged on their ability to generalise to real undersampled data.
The in-vivo validation was done on 20 real-time 18x prospectively undersmapled
GASperturbed PCMR data. The ML aided k-t SENSE reconstruction results were
compared against the free-breathing Cartesian retrospectively gated sequence
and the compressed sensing (CS) reconstruction of the same data. Results: In
general, the ML aided k-t SENSE generated flow curves that were visually
sharper than those produced using CS. In two exceptional cases, U-net M
predictions exhibited blurring which propagated to the extracted velocity
curves. However, there were no statistical differences in the measured peak
velocities and stroke volumes between the tested methods. The ML aided k-t
SENSE was estimated to be ~3.6x faster in processing than CS. Conclusion: The
ML aided k-t SENSE reconstruction enables artefact suppression on a par with CS
with no significant differences in quantitative measures. The timing results
suggest the on-line implementation could deliver a substantial increase in
clinical throughput.",space,170,unknown
http://arxiv.org/abs/2012.08364v1,to_check,arxiv,arxiv,2020-12-13 00:00:00,arxiv,gap-net for snapshot compressive imaging,http://arxiv.org/abs/2012.08364v1,"Snapshot compressive imaging (SCI) systems aim to capture high-dimensional
($\ge3$D) images in a single shot using 2D detectors. SCI devices include two
main parts: a hardware encoder and a software decoder. The hardware encoder
typically consists of an (optical) imaging system designed to capture
{compressed measurements}. The software decoder on the other hand refers to a
reconstruction algorithm that retrieves the desired high-dimensional signal
from those measurements. In this paper, using deep unfolding ideas, we propose
an SCI recovery algorithm, namely GAP-net, which unfolds the generalized
alternating projection (GAP) algorithm. At each stage, GAP-net passes its
current estimate of the desired signal through a trained convolutional neural
network (CNN). The CNN operates as a denoiser that projects the estimate back
to the desired signal space. For the GAP-net that employs trained
auto-encoder-based denoisers, we prove a probabilistic global convergence
result. Finally, we investigate the performance of GAP-net in solving video SCI
and spectral SCI problems. In both cases, GAP-net demonstrates competitive
performance on both synthetic and real data. In addition to having high
accuracy and high speed, we show that GAP-net is flexible with respect to
signal modulation implying that a trained GAP-net decoder can be applied in
different systems. Our code is at https://github.com/mengziyi64/ADMM-net.",space,171,unknown
http://arxiv.org/abs/2008.05381v1,to_check,arxiv,arxiv,2020-08-12 00:00:00,arxiv,"improving the performance of fine-grain image classifiers via generative
  data augmentation",http://arxiv.org/abs/2008.05381v1,"Recent advances in machine learning (ML) and computer vision tools have
enabled applications in a wide variety of arenas such as financial analytics,
medical diagnostics, and even within the Department of Defense. However, their
widespread implementation in real-world use cases poses several challenges: (1)
many applications are highly specialized, and hence operate in a \emph{sparse
data} domain; (2) ML tools are sensitive to their training sets and typically
require cumbersome, labor-intensive data collection and data labelling
processes; and (3) ML tools can be extremely ""black box,"" offering users little
to no insight into the decision-making process or how new data might affect
prediction performance. To address these challenges, we have designed and
developed Data Augmentation from Proficient Pre-Training of Robust Generative
Adversarial Networks (DAPPER GAN), an ML analytics support tool that
automatically generates novel views of training images in order to improve
downstream classifier performance. DAPPER GAN leverages high-fidelity
embeddings generated by a StyleGAN2 model (trained on the LSUN cars dataset) to
create novel imagery for previously unseen classes. We experimentally evaluate
this technique on the Stanford Cars dataset, demonstrating improved vehicle
make and model classification accuracy and reduced requirements for real data
using our GAN based data augmentation framework. The method's validity was
supported through an analysis of classifier performance on both augmented and
non-augmented datasets, achieving comparable or better accuracy with up to 30\%
less real data across visually similar classes. To support this method, we
developed a novel augmentation method that can manipulate semantically
meaningful dimensions (e.g., orientation) of the target object in the embedding
space.",space,172,not included
http://arxiv.org/abs/2006.04403v1,to_check,arxiv,arxiv,2020-06-08 00:00:00,arxiv,global robustness verification networks,http://arxiv.org/abs/2006.04403v1,"The wide deployment of deep neural networks, though achieving great success
in many domains, has severe safety and reliability concerns. Existing
adversarial attack generation and automatic verification techniques cannot
formally verify whether a network is globally robust, i.e., the absence or not
of adversarial examples in the input space. To address this problem, we develop
a global robustness verification framework with three components: 1) a novel
rule-based ``back-propagation'' finding which input region is responsible for
the class assignment by logic reasoning; 2) a new network architecture Sliding
Door Network (SDN) enabling feasible rule-based ``back-propagation''; 3) a
region-based global robustness verification (RGRV) approach. Moreover, we
demonstrate the effectiveness of our approach on both synthetic and real
datasets.",space,173,unknown
http://arxiv.org/abs/2004.08771v1,to_check,arxiv,arxiv,2020-04-19 00:00:00,arxiv,heterogeneous cpu+gpu stochastic gradient descent algorithms,http://arxiv.org/abs/2004.08771v1,"The widely-adopted practice is to train deep learning models with specialized
hardware accelerators, e.g., GPUs or TPUs, due to their superior performance on
linear algebra operations. However, this strategy does not employ effectively
the extensive CPU and memory resources -- which are used only for
preprocessing, data transfer, and scheduling -- available by default on the
accelerated servers. In this paper, we study training algorithms for deep
learning on heterogeneous CPU+GPU architectures. Our two-fold objective --
maximize convergence rate and resource utilization simultaneously -- makes the
problem challenging. In order to allow for a principled exploration of the
design space, we first introduce a generic deep learning framework that
exploits the difference in computational power and memory hierarchy between CPU
and GPU through asynchronous message passing. Based on insights gained through
experimentation with the framework, we design two heterogeneous asynchronous
stochastic gradient descent (SGD) algorithms. The first algorithm -- CPU+GPU
Hogbatch -- combines small batches on CPU with large batches on GPU in order to
maximize the utilization of both resources. However, this generates an
unbalanced model update distribution which hinders the statistical convergence.
The second algorithm -- Adaptive Hogbatch -- assigns batches with continuously
evolving size based on the relative speed of CPU and GPU. This balances the
model updates ratio at the expense of a customizable decrease in utilization.
We show that the implementation of these algorithms in the proposed CPU+GPU
framework achieves both faster convergence and higher resource utilization than
TensorFlow on several real datasets and on two computing architectures -- an
on-premises server and a cloud instance.",space,174,unknown
http://arxiv.org/abs/2001.09346v2,to_check,arxiv,arxiv,2020-01-25 00:00:00,arxiv,"corgan: correlation-capturing convolutional generative adversarial
  networks for generating synthetic healthcare records",http://arxiv.org/abs/2001.09346v2,"Deep learning models have demonstrated high-quality performance in areas such
as image classification and speech processing. However, creating a deep
learning model using electronic health record (EHR) data, requires addressing
particular privacy challenges that are unique to researchers in this domain.
This matter focuses attention on generating realistic synthetic data while
ensuring privacy. In this paper, we propose a novel framework called
correlation-capturing Generative Adversarial Network (CorGAN), to generate
synthetic healthcare records. In CorGAN we utilize Convolutional Neural
Networks to capture the correlations between adjacent medical features in the
data representation space by combining Convolutional Generative Adversarial
Networks and Convolutional Autoencoders. To demonstrate the model fidelity, we
show that CorGAN generates synthetic data with performance similar to that of
real data in various Machine Learning settings such as classification and
prediction. We also give a privacy assessment and report on statistical
analysis regarding realistic characteristics of the synthetic data. The
software of this work is open-source and is available at:
https://github.com/astorfi/cor-gan.",space,175,not included
http://arxiv.org/abs/2001.03864v1,to_check,arxiv,arxiv,2020-01-12 00:00:00,arxiv,"learning to drive via apprenticeship learning and deep reinforcement
  learning",http://arxiv.org/abs/2001.03864v1,"With the implementation of reinforcement learning (RL) algorithms, current
state-of-art autonomous vehicle technology have the potential to get closer to
full automation. However, most of the applications have been limited to game
domains or discrete action space which are far from the real world driving.
Moreover, it is very tough to tune the parameters of reward mechanism since the
driving styles vary a lot among the different users. For instance, an
aggressive driver may prefer driving with high acceleration whereas some
conservative drivers prefer a safer driving style. Therefore, we propose an
apprenticeship learning in combination with deep reinforcement learning
approach that allows the agent to learn the driving and stopping behaviors with
continuous actions. We use gradient inverse reinforcement learning (GIRL)
algorithm to recover the unknown reward function and employ REINFORCE as well
as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal
policy. The performance of our method is evaluated in simulation-based scenario
and the results demonstrate that the agent performs human like driving and even
better in some aspects after training.",space,176,not included
http://arxiv.org/abs/2001.09938v1,to_check,arxiv,arxiv,2019-10-22 00:00:00,arxiv,"autonomous discovery of battery electrolytes with robotic
  experimentation and machine-learning",http://arxiv.org/abs/2001.09938v1,"Innovations in batteries take years to formulate and commercialize, requiring
extensive experimentation during the design and optimization phases. We
approached the design and selection of a battery electrolyte through a
black-box optimization algorithm directly integrated into a robotic test-stand.
We report here the discovery of a novel battery electrolyte by this experiment
completely guided by the machine-learning software without human intervention.
Motivated by the recent trend toward super-concentrated aqueous electrolytes
for high-performance batteries, we utilize Dragonfly - a Bayesian
machine-learning software package - to search mixtures of commonly used lithium
and sodium salts for super-concentrated aqueous electrolytes with wide
electrochemical stability windows. Dragonfly autonomously managed the robotic
test-stand, recommending electrolyte designs to test and receiving experimental
feedback in real time. In 40 hours of continuous experimentation over a
four-dimensional design space with millions of potential candidates, Dragonfly
discovered a novel, mixed-anion aqueous sodium electrolyte with a wider
electrochemical stability window than state-of-the-art sodium electrolyte. A
human-guided design process may have missed this optimal electrolyte. This
result demonstrates the possibility of integrating robotics with
machine-learning to rapidly and autonomously discover novel battery materials.",space,177,unknown
http://arxiv.org/abs/1908.00754v1,to_check,arxiv,arxiv,2019-08-02 00:00:00,arxiv,"a visual technique to analyze flow of information in a machine learning
  system",http://arxiv.org/abs/1908.00754v1,"Machine learning (ML) algorithms and machine learning based software systems
implicitly or explicitly involve complex flow of information between various
entities such as training data, feature space, validation set and results.
Understanding the statistical distribution of such information and how they
flow from one entity to another influence the operation and correctness of such
systems, especially in large-scale applications that perform classification or
prediction in real time. In this paper, we propose a visual approach to
understand and analyze flow of information during model training and serving
phases. We build the visualizations using a technique called Sankey Diagram -
conventionally used to understand data flow among sets - to address various use
cases of in a machine learning system. We demonstrate how the proposed
technique, tweaked and twisted to suit a classification problem, can play a
critical role in better understanding of the training data, the features, and
the classifier performance. We also discuss how this technique enables
diagnostic analysis of model predictions and comparative analysis of
predictions from multiple classifiers. The proposed concept is illustrated with
the example of categorization of millions of products in the e-commerce domain
- a multi-class hierarchical classification problem.",space,178,included
http://arxiv.org/abs/1902.06824v2,to_check,arxiv,arxiv,2019-02-18 00:00:00,arxiv,"autonomous airline revenue management: a deep reinforcement learning
  approach to seat inventory control and overbooking",http://arxiv.org/abs/1902.06824v2,"Revenue management can enable airline corporations to maximize the revenue
generated from each scheduled flight departing in their transportation network
by means of finding the optimal policies for differential pricing, seat
inventory control and overbooking. As different demand segments in the market
have different Willingness-To-Pay (WTP), airlines use differential pricing,
booking restrictions, and service amenities to determine different fare classes
or products targeted at each of these demand segments. Because seats are
limited for each flight, airlines also need to allocate seats for each of these
fare classes to prevent lower fare class passengers from displacing higher fare
class ones and set overbooking limits in anticipation of cancellations and
no-shows such that revenue is maximized. Previous work addresses these problems
using optimization techniques or classical Reinforcement Learning methods. This
paper focuses on the latter problem - the seat inventory control problem -
casting it as a Markov Decision Process to be able to find the optimal policy.
Multiple fare classes, concurrent continuous arrival of passengers of different
fare classes, overbooking and random cancellations that are independent of
class have been considered in the model. We have addressed this problem using
Deep Q-Learning with the goal of maximizing the reward for each flight
departure. The implementation of this technique allows us to employ large
continuous state space but also presents the potential opportunity to test on
real time airline data. To generate data and train the agent, a basic
air-travel market simulator was developed. The performance of the agent in
different simulated market scenarios was compared against theoretically optimal
solutions and was found to be nearly close to the expected optimal revenue.",space,179,unknown
http://arxiv.org/abs/1810.04793v3,to_check,arxiv,arxiv,2018-10-10 00:00:00,arxiv,"patient2vec: a personalized interpretable deep representation of the
  longitudinal electronic health record",http://arxiv.org/abs/1810.04793v3,"The wide implementation of electronic health record (EHR) systems facilitates
the collection of large-scale health data from real clinical settings. Despite
the significant increase in adoption of EHR systems, this data remains largely
unexplored, but presents a rich data source for knowledge discovery from
patient health histories in tasks such as understanding disease correlations
and predicting health outcomes. However, the heterogeneity, sparsity, noise,
and bias in this data present many complex challenges. This complexity makes it
difficult to translate potentially relevant information into machine learning
algorithms. In this paper, we propose a computational framework, Patient2Vec,
to learn an interpretable deep representation of longitudinal EHR data which is
personalized for each patient. To evaluate this approach, we apply it to the
prediction of future hospitalizations using real EHR data and compare its
predictive performance with baseline methods. Patient2Vec produces a vector
space with meaningful structure and it achieves an AUC around 0.799
outperforming baseline methods. In the end, the learned feature importance can
be visualized and interpreted at both the individual and population levels to
bring clinical insights.",space,180,not included
http://arxiv.org/abs/1807.05211v1,to_check,arxiv,arxiv,2018-07-11 00:00:00,arxiv,"learning deployable navigation policies at kilometer scale from a single
  traversal",http://arxiv.org/abs/1807.05211v1,"Model-free reinforcement learning has recently been shown to be effective at
learning navigation policies from complex image input. However, these
algorithms tend to require large amounts of interaction with the environment,
which can be prohibitively costly to obtain on robots in the real world. We
present an approach for efficiently learning goal-directed navigation policies
on a mobile robot, from only a single coverage traversal of recorded data. The
navigation agent learns an effective policy over a diverse action space in a
large heterogeneous environment consisting of more than 2km of travel, through
buildings and outdoor regions that collectively exhibit large variations in
visual appearance, self-similarity, and connectivity. We compare pretrained
visual encoders that enable precomputation of visual embeddings to achieve a
throughput of tens of thousands of transitions per second at training time on a
commodity desktop computer, allowing agents to learn from millions of
trajectories of experience in a matter of hours. We propose multiple forms of
computationally efficient stochastic augmentation to enable the learned policy
to generalise beyond these precomputed embeddings, and demonstrate successful
deployment of the learned policy on the real robot without fine tuning, despite
environmental appearance differences at test time. The dataset and code
required to reproduce these results and apply the technique to other datasets
and robots is made publicly available at rl-navigation.github.io/deployable.",space,181,included
http://arxiv.org/abs/1805.03045v2,to_check,arxiv,arxiv,2018-05-08 00:00:00,arxiv,"a new method for unveiling open clusters in gaia: new nearby open
  clusters confirmed by dr2",http://arxiv.org/abs/1805.03045v2,"The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in
Astronomy. It includes precise astrometric data (positions, proper motions and
parallaxes) for more than $1.3$ billion sources, mostly stars. To analyse such
a vast amount of new data, the use of data mining techniques and machine
learning algorithms are mandatory. The search for Open Clusters, groups of
stars that were born and move together, located in the disk, is a great example
for the application of these techniques. Our aim is to develop a method to
automatically explore the data space, requiring minimal manual intervention. We
explore the performance of a density based clustering algorithm, DBSCAN, to
find clusters in the data together with a supervised learning method such as an
Artificial Neural Network (ANN) to automatically distinguish between real Open
Clusters and statistical clusters. The development and implementation of this
method to a $5$-Dimensional space ($l$, $b$, $\varpi$, $\mu_{\alpha^*}$,
$\mu_\delta$) to the Tycho-Gaia Astrometric Solution (TGAS) data, and a
posterior validation using Gaia DR2 data, lead to the proposal of a set of new
nearby Open Clusters. We have developed a method to find OCs in astrometric
data, designed to be applied to the full Gaia DR2 archive.",space,182,unknown
http://arxiv.org/abs/1804.04756v1,to_check,arxiv,arxiv,2018-04-13 00:00:00,arxiv,machine learning peeling and loss modelling of time-domain reflectometry,http://arxiv.org/abs/1804.04756v1,"A fundamental pursuit of microwave metrology is the determination of the
characteristic impedance profile of microwave systems. Among other methods,
this can be practically achieved by means of time-domain reflectometry (TDR)
that measures the reflections from a device due to an applied stimulus.
Conventional TDR allows for the measurement of systems comprising a single
impedance. However, real systems typically feature impedance variations that
obscure the determination of all impedances subsequent to the first one. This
problem has been studied previously and is generally known as scattering
inversion or, in the context of microwave metrology, time-domain ""peeling"". In
this article, we demonstrate the implementation of a space-time efficient
peeling algorithm that corrects for the effect of prior impedance mismatch in a
nonuniform lossless transmission line, regardless of the nature of the
stimulus. We generalize TDR measurement analysis by introducing two tools: A
stochastic machine learning clustering tool and an arbitrary lossy transmission
line modeling tool. The former mitigates many of the imperfections typically
plaguing TDR measurements (except for dispersion) and allows for an efficient
processing of large datasets; the latter allows for a complete transmission
line characterization including both conductor and dielectric loss.",space,183,unknown
http://arxiv.org/abs/1712.02294v4,to_check,arxiv,arxiv,2017-12-06 00:00:00,arxiv,joint 3d proposal generation and object detection from view aggregation,http://arxiv.org/abs/1712.02294v4,"We present AVOD, an Aggregate View Object Detection network for autonomous
driving scenarios. The proposed neural network architecture uses LIDAR point
clouds and RGB images to generate features that are shared by two subnetworks:
a region proposal network (RPN) and a second stage detector network. The
proposed RPN uses a novel architecture capable of performing multimodal feature
fusion on high resolution feature maps to generate reliable 3D object proposals
for multiple object classes in road scenes. Using these proposals, the second
stage detection network performs accurate oriented 3D bounding box regression
and category classification to predict the extents, orientation, and
classification of objects in 3D space. Our proposed architecture is shown to
produce state of the art results on the KITTI 3D object detection benchmark
while running in real time with a low memory footprint, making it a suitable
candidate for deployment on autonomous vehicles. Code is at:
https://github.com/kujason/avod",space,184,not included
http://arxiv.org/abs/1702.06329v1,to_check,arxiv,arxiv,2017-02-21 00:00:00,arxiv,"towards a common implementation of reinforcement learning for multiple
  robotic tasks",http://arxiv.org/abs/1702.06329v1,"Mobile robots are increasingly being employed for performing complex tasks in
dynamic environments. Reinforcement learning (RL) methods are recognized to be
promising for specifying such tasks in a relatively simple manner. However, the
strong dependency between the learning method and the task to learn is a
well-known problem that restricts practical implementations of RL in robotics,
often requiring major modifications of parameters and adding other techniques
for each particular task. In this paper we present a practical core
implementation of RL which enables the learning process for multiple robotic
tasks with minimal per-task tuning or none. Based on value iteration methods,
this implementation includes a novel approach for action selection, called
Q-biased softmax regression (QBIASSR), which avoids poor performance of the
learning process when the robot reaches new unexplored states. Our approach
takes advantage of the structure of the state space by attending the physical
variables involved (e.g., distances to obstacles, X,Y,{\theta} pose, etc.),
thus experienced sets of states may favor the decision-making process of
unexplored or rarely-explored states. This improvement has a relevant role in
reducing the tuning of the algorithm for particular tasks. Experiments with
real and simulated robots, performed with the software framework also
introduced here, show that our implementation is effectively able to learn
different robotic tasks without tuning the learning method. Results also
suggest that the combination of true online SARSA({\lambda}) with QBIASSR can
outperform the existing RL core algorithms in low-dimensional robotic tasks.",space,185,unknown
http://arxiv.org/abs/1610.07862v2,to_check,arxiv,arxiv,2016-10-24 00:00:00,arxiv,intelligence in artificial intelligence,http://arxiv.org/abs/1610.07862v2,"The elusive quest for intelligence in artificial intelligence prompts us to
consider that instituting human-level intelligence in systems may be (still) in
the realm of utopia. In about a quarter century, we have witnessed the winter
of AI (1990) being transformed and transported to the zenith of tabloid fodder
about AI (2015). The discussion at hand is about the elements that constitute
the canonical idea of intelligence. The delivery of intelligence as a
pay-per-use-service, popping out of an app or from a shrink-wrapped software
defined point solution, is in contrast to the bio-inspired view of intelligence
as an outcome, perhaps formed from a tapestry of events, cross-pollinated by
instances, each with its own microcosm of experiences and learning, which may
not be discrete all-or-none functions but continuous, over space and time. The
enterprise world may not require, aspire or desire such an engaged solution to
improve its services for enabling digital transformation through the deployment
of digital twins, for example. One might ask whether the ""work-flow on
steroids"" version of decision support may suffice for intelligence? Are we
harking back to the era of rule based expert systems? The image conjured by the
publicity machines offers deep solutions with human-level AI and preposterous
claims about capturing the ""brain in a box"" by 2020. Even emulating insects may
be difficult in terms of real progress. Perhaps we can try to focus on worms
(Caenorhabditis elegans) which may be better suited for what business needs to
quench its thirst for so-called intelligence in AI.",space,186,unknown
http://arxiv.org/abs/1609.08018v1,to_check,arxiv,arxiv,2016-09-26 00:00:00,arxiv,"small near-earth asteroids in the palomar transient factory survey: a
  real-time streak-detection system",http://arxiv.org/abs/1609.08018v1,"Near-Earth asteroids (NEAs) in the 1-100 meter size range are estimated to be
$\sim$1,000 times more numerous than the $\sim$15,000 currently-catalogued
NEAs, most of which are in the 0.5-10 kilometer size range. Impacts from 10-100
meter size NEAs are not statistically life-threatening but may cause
significant regional damage, while 1-10 meter size NEAs with low velocities
relative to Earth are compelling targets for space missions. We describe the
implementation and initial results of a real-time NEA-discovery system
specialized for the detection of small, high angular rate (visually-streaked)
NEAs in Palomar Transient Factory (PTF) images. PTF is a 1.2-m aperture,
7.3-deg$^2$ field-of-view optical survey designed primarily for the discovery
of extragalactic transients (e.g., supernovae) in 60-second exposures reaching
$\sim$20.5 visual magnitude. Our real-time NEA discovery pipeline uses a
machine-learned classifier to filter a large number of false-positive streak
detections, permitting a human scanner to efficiently and remotely identify
real asteroid streaks during the night. Upon recognition of a streaked NEA
detection (typically within an hour of the discovery exposure), the scanner
triggers follow-up with the same telescope and posts the observations to the
Minor Planet Center for worldwide confirmation. We describe our ten initial
confirmed discoveries, all small NEAs that passed 0.3-15 lunar distances from
Earth. Lastly, we derive useful scaling laws for comparing
streaked-NEA-detection capabilities of different surveys as a function of their
hardware and survey-pattern characteristics. This work most directly informs
estimates of the streak-detection capabilities of the Zwicky Transient Facility
(ZTF, planned to succeed PTF in 2017), which will apply PTF's current
resolution and sensitivity over a 47-deg$^2$ field-of-view.",space,187,included
10.1016/j.future.2022.02.008,to_check,Future Generation Computer Systems,scopus,2022-07-01,sciencedirect,hatch: self-distributing systems for data centers,https://api.elsevier.com/content/abstract/scopus_id/85125223395,"Designing and maintaining distributed systems remains highly challenging: there is a high-dimensional design space of potential ways to distribute a systemâs sub-components over a large-scale infrastructure; and the deployment environment for a system tends to change in unforeseen ways over time. For engineers, this is a complex prediction problem to gauge which distributed design may best suit a given environment. We present the concept of self-distributing systems, in which any local system built using our framework can learn, at runtime, the most appropriate distributed design given its perceived operating conditions. Our concept abstracts distribution of a systemâs sub-components to a list of simple actions in a reward matrix of distributed design alternatives to be used by reinforcement learning algorithms. By doing this, we enable software to experiment, in a live production environment, with different ways in which to distribute its software modules by placing them in different hosts throughout the systemâs infrastructure. We implement this concept in a framework we call Hatch, which has three major elements: (i) a transparent and generalized RPC layer that supports seamless relocation of any local component to a remote host during execution; (ii) a set of primitives, including relocation, replication and sharding, from which to create an action/reward matrix of possible distributed designs of a system; and (iii) a decentralized reinforcement learning approach to converge towards more optimal designs in real time. Using an example of a self-distributing web-serving infrastructure, Hatch is able to autonomously select the most suitable distributed design from among 
                        â
                     700,000 alternatives in about 5Â min.",space,188,unknown
10.1016/j.conengprac.2021.105046,to_check,Control Engineering Practice,scopus,2022-04-01,sciencedirect,deep reinforcement learning with shallow controllers: an experimental application to pid tuning,https://api.elsevier.com/content/abstract/scopus_id/85122624409,"Deep reinforcement learning (RL) is an optimization-driven framework for producing control strategies for general dynamical systems without explicit reliance on process models. Good results have been reported in simulation. Here we demonstrate the challenges in implementing a state of the art deep RL algorithm on a real physical system. Aspects include the interplay between software and existing hardware; experiment design and sample efficiency; training subject to input constraints; and interpretability of the algorithm and control law. At the core of our approach is the use of a PID controller as the trainable RL policy. In addition to its simplicity, this approach has several appealing features: No additional hardware needs to be added to the control system, since a PID controller can easily be implemented through a standard programmable logic controller; the control law can easily be initialized in a âsafeâ region of the parameter space; and the final productâa well-tuned PID controllerâhas a form that practitioners can reason about and deploy with confidence.",space,189,not included
10.1016/j.softx.2021.100854,to_check,SoftwareX,scopus,2021-12-01,sciencedirect,microvip: microscopy image simulation on the virtual imaging platform,https://api.elsevier.com/content/abstract/scopus_id/85119052033,"MicroVIP is an open source software that assembles, in a unified web-application running on distributed computing resources, simulators of the main fluorescent microscopy imaging modalities (with existing codes or newly developed). MicroVIP provides realistic simulated images including several sources of noise (microfluidic blur effect, diffraction, Poisson noise, camera read out noise). MicroVIP also includes a module which simulates single cells with fluorescent markers and a module to analyze the simulated images with textural and pointillist feature spaces. MicroVIP is shown to be of value for supervised machine learning. It allow to automatically generate large sets of training images and virtual instrumentation to optimize the optical parameters before realizing real experiments.",space,190,unknown
10.1016/j.scs.2021.103071,to_check,Sustainable Cities and Society,scopus,2021-09-01,sciencedirect,a stochastic machine learning based approach for observability enhancement of automated smart grids,https://api.elsevier.com/content/abstract/scopus_id/85107608455,"This paper develops a machine learning aggregated integer linear programming approach for the full observability of the automated smart grids by positioning of micro-synchrophasor units, taking into account the reconfigurable structure of the distribution systems. The proposed stochastic approach presents a strategy occurring in several stages to micro-synchrophasor unit positioning based on the load level and demand in the system and based on the pre-determined sectionalizing and tie switches. Such a technique can also deploy the zero-injection limitations of the model and reduce the search space of the problem. Moreover, a novel method based on whale optimization method (WOM) is introduced to simultaneously enhance the reliability indices in order to specify the optimum topology for each phase and reduce the costs of power losses and customer interruptions. Although the problem of micro-synchrophasor placement is formulated in an integer linear programming framework, the restructuring technique is resolved on the basis of the WOM heuristic approach. Considering the uncertainty due to the metering devices or forecast errors, a stochastic framework based on point estimation is deployed to handle the uncertainty effects. The simulation and numerical results on a real system verify that the proposed method assures visibility of the distribution network pre and post reconfiguration in the time horizon of the planning. Furthermore, the results show that the system observability can be guaranteed at different load levels even though the system experiences different reconfiguration and topologies.",space,191,unknown
10.1016/j.jmsy.2021.04.005,to_check,Journal of Manufacturing Systems,scopus,2021-07-01,sciencedirect,learningadd: machine learning based acoustic defect detection in factory automation,https://api.elsevier.com/content/abstract/scopus_id/85106283308,"Defect inspection of glass bottles in the beverage industrial is of significance to prevent unexpected losses caused by the damage of bottles during manufacturing and transporting. The commonly used manual methods suffer from inefficiency, excessive space consumption, and beverage wastes after filling. To replace the manual operations in the pre-filling detection with improved efficiency and reduced costs, this paper proposes a machine learning based Acoustic Defect Detection (LearningADD) system. Moreover, to realize scalable deployment on edge and cloud computing platforms, deployment strategies especially partitioning and allocation of functionalities need to be compared and optimized under realistic constraints such as latency, complexity, and capacity of the platforms. In particular, to distinguish the defects in glass bottles efficiently, the improved Hilbert-Huang transform (HHT) is employed to extend the extracted feature sets, and then Shuffled Frog Leaping Algorithm (SFLA) based feature selection is applied to optimize the feature sets. Five deployment strategies are quantitatively compared to optimize real-time performances based on the constraints measured from a real edge and cloud environment. The LearningADD algorithms are validated by the datasets from a real-life beverage factory, and the F-measure of the system reaches 98.48 %. The proposed deployment strategies are verified by experiments on private cloud platforms, which shows that the Distributed Heavy Edge deployment outperforms other strategies, benefited from the parallel computing and edge computing, where the Defect Detection Time for one bottle is less than 2.061âs in 99 % probability.",space,192,unknown
10.1016/j.actaastro.2021.01.048,to_check,Acta Astronautica,scopus,2021-04-01,sciencedirect,a transfer learning approach to space debris classification using observational light curve data,https://api.elsevier.com/content/abstract/scopus_id/85100001105,"This paper presents a data driven approach to space object characterisation through the application of machine learning techniques to observational light curve data. One-dimensional convolutional neural networks are shown to be effective at classifying the shape of objects from both simulated and real light curve data. To the best of the authorsâ knowledge this is the first generalised attempt to classify the shape of space objects using real observational light curve data.
                  It is also demonstrated that transfer learning is successful in improving the overall classification accuracy on real light curve datasets. The authors develop a simulated light curve dataset using a high fidelity three-dimensional ray-tracing software. The simulator takes in a textured geometric model of a Resident Space Object as well as its ephemeris and uses ray-tracing software to generate photo-realistic images of the object that are then processed to extract the light curve. Models that are pre-trained on the simulated dataset and then fine-tuned on the real datasets are shown to outperform models purely trained on the real datasets. This result indicates that transfer learning will allow organisations to effectively utilise deep learning techniques without the requirement to build up large real light curve datasets for training.",space,193,unknown
10.1016/j.ifacol.2021.10.504,to_check,IFAC-PapersOnLine,scopus,2021-01-01,sciencedirect,advanced state fuzzy cognitive maps applied on nearly zero energy building model,https://api.elsevier.com/content/abstract/scopus_id/85120711263,"Fuzzy Cognitive Maps method combines the advantages of Fuzzy Logic, such as their human reasoning and linguistic features, with the advantages of Neural Networks, such as their low mathematical calculation requirements, in order to model complex dynamic systems on a wide variety of applications. The system variables and their interconnections are described using a graph and a weight matrix. Application of expertsâ knowledge leads towards more realistic system models. In addition, the implementation of state-space theory in combination with learning algorithms, lead to a new generation of Fuzzy Cognitive Maps, the Advanced State Fuzzy Cognitive Maps. All the above are implemented on a nearly Zero Energy Building model, using real weather data and presenting its annual energy response.",space,194,unknown
10.1016/j.ymssp.2020.107061,to_check,Mechanical Systems and Signal Processing,scopus,2021-01-01,sciencedirect,recovering compressed images for automatic crack segmentation using generative models,https://api.elsevier.com/content/abstract/scopus_id/85086994715,"In a structural health monitoring (SHM) system that uses digital cameras to monitor cracks of structural surfaces, techniques for reliable and effective data compression are essential to ensure a stable and energy-efficient crack images transmission in wireless devices, e.g., drones and robots with high definition cameras installed. Compressive sensing (CS) is a signal processing technique that allows accurate recovery of a signal from a sampling rate much smaller than the limitation of the Nyquist sampling theorem. Different from the popular approach of simultaneously training encoder and decoder using neural network models, the CS theory ensures a high probability of accurate signal reconstruction based on random measurements that is shorter than the length of the original signal under a sparsity constraint. Such method is particularly useful when measurements are expensive, such as wireless sensing of civil structures, because its hardware implementation allows down sampling of signals during the sensing process. Hence, CS methods can achieve significant energy saving for the sensing devices. However, the strong assumption of the signals being highly sparse in an invertible space is relatively hard to guarantee for many real images, such as image of cracks. In this paper, we present a new approach of CS that replaces the sparsity regularization with a generative model that is able to effectively capture a low dimension representation of targeted images. We develop a recovery framework for automatic crack segmentation of compressed crack images based on this new CS method. We demonstrate the remarkable performance of our method that takes advantage of the strong capability of generative models to capture the necessary features required in the crack segmentation task even the backgrounds of the generated images are not well reconstructed. The superior performance of our recovery framework is illustrated by comparisons to three existing CS algorithms. Furthermore, we show that our framework is potentially extensible to other common problems in automatic crack segmentation, such as defect recovery from motion blurring and occlusion.",space,195,unknown
10.1016/j.physd.2020.132615,to_check,Physica D: Nonlinear Phenomena,scopus,2020-11-01,sciencedirect,a reduced order deep data assimilation model,https://api.elsevier.com/content/abstract/scopus_id/85087917230,"A new Reduced Order Deep Data Assimilation (RODDA) model combining Reduced order models (ROM), Data Assimilation (DA) and Machine Learning is proposed in this paper. The RODDA model aims to improve the accuracy of Computational Fluid Dynamics (CFD) simulations. The DA model ingests information from observed data in the simulation provided by the CFD model. The results of the DA are used to train a neural network learning a function which predicts the misfit between the results of the CFD model and the DA model. Thus, the trained function is combined with the original CFD model in order to generate forecasts with implicit DA given by neural network. Due to the time complexity of the numerical models used to implement DA and the neural network, and due to the scale of the forecasting area considered for forecasting problems in real case scenarios, the implementation of RODDA mandated the introduction of opportune reduced spaces. Here, RODDA is applied to a CFD simulation for air pollution, using the CFD software Fluidity, in South London (UK). We show that, using this framework, the data forecasted by the coupled model CFD+RODDA are closer to the observations with a gain in terms of execution time with respect to the classic predictionâcorrection cycle given by coupling CFD with a standard DA. Additionally, RODDA predicts future observations, if not available, since these are embedded in the data assimilated state in which the network is trained on. The RODDA framework is not exclusive to air pollution, Fluidity, or the study area in South London, and therefore the workflow could be applied to different physical models if enough temporal data are available.",space,196,not included
10.1016/j.physa.2019.123151,to_check,Physica A: Statistical Mechanics and its Applications,scopus,2020-02-15,sciencedirect,early warning system: from face recognition by surveillance cameras to social media analysis to detecting suspicious people,https://api.elsevier.com/content/abstract/scopus_id/85074532417,"Surveillance security cameras are increasingly deployed in almost every location for monitoring purposes, including watching people and their actions for security purposes. For criminology, images collected from these cameras are usually used after an incident occurs to analyze who could be the people involved. While this usage of the cameras is important for a post crime action, there exists the need for real time monitoring to act as an early warning to prevent or avoid an incident before it occurs. In this paper, we describe the development and implementation of an early warning system that recognizes people automatically in a surveillance camera environment and then use data from various sources to identify these people and build their profile and network. The current literature is still missing a complete workflow from identifying people/criminals from a video surveillance to building a criminal information extraction framework and identifying those people and their interactions with others We train a feature extraction model for face recognition using convolutional neural networks to get a good recognition rate on the Chokepoint dataset collected using surveillance cameras. The system also provides the function to record people appearance in a location, such that unknown people passing through a scene excessive number of times (above a threshold decided by a security expert) will then be further analyzed to collect information about them. We implemented a queue based system to record people entrance. We try to avoid missing relevant individuals passing through as in some cases it is not possible to add every passing person to the queue which is maintained using some cache handling techniques. We collect and analyze information about unknown people by comparing their images from the cameras to a list of social media profiles collected from Facebook and intelligent services archives. After locating the profile of a person, traditional news and other social media platforms are crawled to collect and analyze more information about the identified person. The analyzed information is then presented to the analyst where a list of keywords and verb phrases are shown. We also construct the personâs network from individuals mentioned with him/her in the text. Further analysis will allow security experts to mark this person as a suspect or safe. This work shows that building a complete early warning system is feasible to tackle and identify criminals so that authorities can take the required actions on the spot.",space,197,not included
10.1016/j.cogsys.2019.09.015,to_check,Cognitive Systems Research,scopus,2020-01-01,sciencedirect,multi-agent neurocognitive models of semantics of spatial localization of events,https://api.elsevier.com/content/abstract/scopus_id/85072851037,"The purpose of the study is to develop a learning system for internal representation of the events localization space to realize orientation and navigation of autonomous mobile systems. The task of the research is the development of simulation models of the semantics of the event localization space based on multi-agent neurocognitive architectures. The paper proves that the multi-agent neurocognitive architecture is an effective formalism for describing the semantics of the spatial localization of events. Main theoretical foundations have been developed for the simulation of spatial relations using the so-called multi-agent facts, consisting of software agents-concepts, reflecting semantic categories corresponding to parts of speech. It is shown that locative software agents that describe the spatial location of objects and events, forming homogeneous connections, compose the so-called field locations. The latter describes a holistic view of the intellectual agent about the environment. The paper defines conceptual foundations of multi-agent modeling of the semantics of subjective reflexive mapping of the interaction between real objects, space and time.",space,198,not included
10.1016/j.neucom.2018.06.095,to_check,Neurocomputing,scopus,2019-08-18,sciencedirect,speeding up k-nearest neighbors classifier for large-scale multi-label learning on gpus,https://api.elsevier.com/content/abstract/scopus_id/85065140025,"Multi-label classification is one of the most dynamically growing fields of machine learning, due to its numerous real-life applications in solving problems that can be described by multiple labels at the same time. While most of works in this field focus on proposing novel and accurate classification algorithms, the issue of the computational complexity on growing dataset sizes is somehow marginalized. Owning to the ever-increasing capabilities of data capturing, we are faced with the problem of large-scale data mining that forces learners to be not only highly accurate, but also fast and scalable on high-dimensional spaces of instances, features, and labels. In this paper, we propose a highly efficient parallel approach for computing the multi-label k-Nearest Neighbor classifier on GPUs. While this method is highly effective due to its accuracy and simplicity, its computational complexity makes it prohibitive for large-scale data. We propose a four-step implementation that takes an advantage of the GPU architecture, allowing for an efficient execution of the multi-label k-Nearest Neighbors classifier without any loss of accuracy. Experiments carried out on a number of real and artificial benchmarks show that we are able to achieve speedups up to 200 times when compared to a sequential CPU execution, while efficiently scaling up to varying number of instances and features.",space,199,unknown
10.1016/j.eswa.2017.11.011,to_check,Expert Systems with Applications,scopus,2018-06-15,sciencedirect,towards a common implementation of reinforcement learning for multiple robotic tasks,https://api.elsevier.com/content/abstract/scopus_id/85035079318,"Mobile robots are increasingly being employed for performing complex tasks in dynamic environments. Those tasks can be either explicitly programmed by an engineer or learned by means of some automatic learning method, which improves the adaptability of the robot and reduces the effort of setting it up. In this sense, reinforcement learning (RL) methods are recognized as a promising tool for a machine to learn autonomously how to do tasks that are specified in a relatively simple manner. However, the dependency between these methods and the particular task to learn is a well-known problem that has strongly restricted practical implementations in robotics so far. Breaking this barrier would have a significant impact on these and other intelligent systems; in particular, having a core method that requires little tuning effort for being applicable to diverse tasks would boost their autonomy in learning and self-adaptation capabilities. In this paper we present such a practical core implementation of RL, which enables the learning process for multiple robotic tasks with minimal per-task tuning or none. Based on value iteration methods, we introduce a novel approach for action selection, called Q-biased softmax regression (QBIASSR), that takes advantage of the structure of the state space by attending the physical variables involved (e.g., distances to obstacles, robot pose, etc.), thus experienced sets of states accelerate the decision-making process of unexplored or rarely-explored states. Intensive experiments with both real and simulated robots, carried out with the software framework also introduced here, show that our implementation is able to learn different robotic tasks without tuning the learning method. They also suggest that the combination of true online SARSA(Î») (TOSL) with QBIASSR can outperform the existing RL core algorithms in low-dimensional robotic tasks. All of these are promising results towards the possibility of learning much more complex tasks autonomously by a robotic agent.",space,200,unknown
10.1016/j.compind.2018.03.014,to_check,Computers in Industry,scopus,2018-06-01,sciencedirect,real-time object detection in agricultural/remote environments using the multiple-expert colour feature extreme learning machine (mec-elm),https://api.elsevier.com/content/abstract/scopus_id/85044151304,"It is necessary for autonomous robotics in agriculture to provide real time feedback, but due to a diverse array of objects and lack of landscape uniformity this objective is inherently complex. The current study presents two implementations of the multiple-expert colour feature extreme learning machine (MEC-ELM). The MEC-ELM is a cascading algorithm that has been implemented along side a summed area table (SAT) for fast feature extraction and object classification, for a fully functioning object detection algorithm. The MEC-ELM is an implementation of the colour feature extreme learning machine (CF-ELM), which is an extreme learning machine (ELM) with a partially connected hidden layer; taking three colour bands as inputs. The colour implementation used with the SAT enable the MEC-ELM to find and classify objects quickly, with 84% precision and 91% recall in weed detection in the YâUV colour space and in 0.5â¯s per frame. The colour implementation is however limited to low resolution images and for this reason a colour level co-occurrence matrix (CLCM) variant of the MEC-ELM is proposed. This variant uses the SAT to produce a CLCM and texture analyses, with texture values processed as an input to the MEC-ELM. This enabled the MEC-ELM to achieve 78â85% precision and 81â93% recall in cattle, weed and quad bike detection and in times between 1 and 2â¯s per frame. Both implementations were benchmarked on a standard i7 mobile processor. Thus the results presented in this paper demonstrated that the MEC-ELM with SAT grid and CLCM makes an ideal candidate for fast object detection in complex and/or agricultural landscapes.",space,201,not included
10.1016/j.comcom.2016.12.015,to_check,Computer Communications,scopus,2017-06-01,sciencedirect,imola: a decentralised learning-driven protocol for multi-hop white-fi,https://api.elsevier.com/content/abstract/scopus_id/85028254628,"In this paper we tackle the digital exclusion problem in developing and remote locations by proposing Imola, an inexpensive learning-driven access mechanism for multi-hop wireless networks that operate across TV white-spaces (TVWS). Stations running Imola only rely on passively acquired neighbourhood information to achieve scheduled-like operation in a decentralised way, without explicit synchronisation. Our design overcomes pathological circumstances such as hidden and exposed terminals that arise due to carrier sensing and are exceptionally problematic in low frequency bands. We present a prototype implementation of our proposal and conduct experiments in a real test bed, which confirms the practical feasibility of deploying our solution in mesh networks that build upon the IEEE 802.11af standard. Finally, the extensive system level simulations we perform demonstrate that Imola achieves up to 4Ã more throughput than the channel access protocol defined by the standard and reduces frame loss rate by up to 100%.",space,202,unknown
10.1016/j.cmpb.2016.04.005,to_check,Computer Methods and Programs in Biomedicine,scopus,2016-07-01,sciencedirect,a mapreduce approach to diminish imbalance parameters for big deoxyribonucleic acid dataset,https://api.elsevier.com/content/abstract/scopus_id/84964534094,"Background
                  In the age of information superhighway, big data play a significant role in information processing, extractions, retrieving and management. In computational biology, the continuous challenge is to manage the biological data. Data mining techniques are sometimes imperfect for new space and time requirements. Thus, it is critical to process massive amounts of data to retrieve knowledge. The existing software and automated tools to handle big data sets are not sufficient. As a result, an expandable mining technique that enfolds the large storage and processing capability of distributed or parallel processing platforms is essential.
               
                  Method
                  In this analysis, a contemporary distributed clustering methodology for imbalance data reduction using k-nearest neighbor (K-NN) classification approach has been introduced. The pivotal objective of this work is to illustrate real training data sets with reduced amount of elements or instances. These reduced amounts of data sets will ensure faster data classification and standard storage management with less sensitivity. However, general data reduction methods cannot manage very big data sets. To minimize these difficulties, a MapReduce-oriented framework is designed using various clusters of automated contents, comprising multiple algorithmic approaches.
               
                  Results
                  To test the proposed approach, a real DNA (deoxyribonucleic acid) dataset that consists of 90 million pairs has been used. The proposed model reduces the imbalance data sets from large-scale data sets without loss of its accuracy.
               
                  Conclusions
                  The obtained results depict that MapReduce based K-NN classifier provided accurate results for big data of DNA.",space,203,not included
10.1016/j.neucom.2015.02.096,to_check,Neurocomputing,scopus,2016-01-22,sciencedirect,building feature space of extreme learning machine with sparse denoising stacked-autoencoder,https://api.elsevier.com/content/abstract/scopus_id/84940061662,"The random-hidden-node extreme learning machine (ELM) is a much more generalized cluster of single-hidden-layer feed-forward neural networks (SLFNs) which has three parts: random projection, non-linear transformation, and ridge regression (RR) model. Networks with deep architectures have demonstrated state-of-the-art performance in a variety of settings, especially with computer vision tasks. Deep learning algorithms such as stacked autoencoder (SAE) and deep belief network (DBN) are built on learning several levels of representation of the input. Beyond simply learning features by stacking autoencoders (AE), there is a need for increasing its robustness to noise and reinforcing the sparsity of weights to make it easier to discover interesting and prominent features. The sparse AE and denoising AE was hence developed for this purpose. This paper proposes an approach: SSDAE-RR (stacked sparse denoising autoencoder â ridge regression) that effectively integrates the advantages in SAE, sparse AE, denoising AE, and the RR implementation in ELM algorithm. We conducted experimental study on real-world classification (binary and multiclass) and regression problems with different scales among several relevant approaches: SSDAE-RR, ELM, DBN, neural network (NN), and SAE. The performance analysis shows that the SSDAE-RR tends to achieve a better generalization ability on relatively large datasets (large sample size and high dimension) that were not pre-processed for feature abstraction. For 16 out of 18 tested datasets, the performance of SSDAE-RR is more stable than other tested approaches. We also note that the sparsity regularization and denoising mechanism seem to be mandatory for constructing interpretable feature representations. The fact that a SSDAE-RR approach often has a comparable training time to ELM makes it useful in some real applications.",space,204,unknown
10.1016/j.sigpro.2012.10.020,to_check,Signal Processing,scopus,2013-06-01,sciencedirect,3d cbir with sparse coding for image-guided neurosurgery,https://api.elsevier.com/content/abstract/scopus_id/84875248258,"This research takes an application-specific approach to investigate, extend and implement the state of the art in the fields of both visual information retrieval and machine learning, bridging the gap between theoretical models and real world applications. During an image-guided neurosurgery, path planning remains the foremost and hence the most important step to perform an operation and ensures the maximum resection of an intended target and minimum sacrifice of health tissues. In this investigation, the technique of content-based image retrieval (CBIR) coupled with machine learning algorithms are exploited in designing a computer aided path planning system (CAP) to assist junior doctors in planning surgical paths while sustaining the highest precision. Specifically, after evaluation of approaches of sparse coding and K-means in constructing a codebook, the model of sparse codes of 3D SIFT has been furthered and thereafter employed for retrieving, The novelty of this work lies in the fact that not only the existing algorithms for 2D images have been successfully extended into 3D space, leading to promising results, but also the application of CBIR that is mainly in a research realm, to a clinical sector can be achieved by the integration with machine learning techniques. Comparison with the other four popular existing methods is also conducted, which demonstrates that with the implementation of sparse coding, all methods give better retrieval results than without while constituting the codebook, implying the significant contribution of machine learning techniques.",space,205,not included
10.1016/j.procs.2013.05.187,to_check,Procedia Computer Science,scopus,2013-01-01,sciencedirect,comparing support vector machines and artificial neural networks in the recognition of steering angle for driving of mobile robots through paths in plantations,https://api.elsevier.com/content/abstract/scopus_id/84896966222,"The use of mobile robots turns out to be interesting in activities where the action of human specialist is difficult or dangerous. Mobile robots are often used for the exploration in areas of difficult access, such as rescue operations and space missions, to avoid human experts exposition to risky situations. Mobile robots are also used in agriculture for planting tasks as well as for keeping the application of pesticides within minimal amounts to mitigate environmental pollution. In this paper we present the development of a system to control the navigation of an autonomous mobile robot through tracks in plantations. Track images are used to control robot direction by pre-processing them to extract image features. Such features are then submitted to a support vector machine and an artificial neural network in order to find out the most appropriate route. A comparison of the two approaches was performed to ascertain the one presenting the best outcome. The overall goal of the project to which this work is connected is to develop a real time robot control system to be embedded into a hardware platform. In this paper we report the software implementation of a support vector machine and of an artificial neural network, which so far presented respectively around 93% and 90% accuracy in predicting the appropriate route.",space,206,not included
10.1016/j.actaastro.2012.06.003,to_check,Acta Astronautica,scopus,2012-11-01,sciencedirect,"robotic mission to mars: hands-on, minds-on, web-based learning",https://api.elsevier.com/content/abstract/scopus_id/84865224510,"Problem-based learning has been demonstrated as an effective methodology for developing analytical skills and critical thinking. The use of scenario-based learning incorporates problem-based learning whilst encouraging students to collaborate with their colleagues and dynamically adapt to their environment. This increased interaction stimulates a deeper understanding and the generation of new knowledge. The Victorian Space Science Education Centre (VSSEC) uses scenario-based learning in its Mission to Mars, Mission to the Orbiting Space Laboratory and Primary Expedition to the M.A.R.S. Base programs. These programs utilize methodologies such as hands-on applications, immersive-learning, integrated technologies, critical thinking and mentoring to engage students in Science, Technology, Engineering and Mathematics (STEM) and highlight potential career paths in science and engineering. The immersive nature of the programs demands specialist environments such as a simulated Mars environment, Mission Control and Space Laboratory, thus restricting these programs to a physical location and limiting student access to the programs. To move beyond these limitations, VSSEC worked with its university partners to develop a web-based mission that delivered the benefits of scenario-based learning within a school environment. The Robotic Mission to Mars allows students to remotely control a real rover, developed by the Australian Centre for Field Robotics (ACFR), on the VSSEC Mars surface. After completing a pre-mission training program and site selection activity, students take on the roles of scientists and engineers in Mission Control to complete a mission and collect data for further analysis. Mission Control is established using software developed by the ACRI Games Technology Lab at La Trobe University using the principles of serious gaming. The software allows students to control the rover, monitor its systems and collect scientific data for analysis. This program encourages students to work scientifically and explores the interaction between scientists and engineers. This paper presents the development of the program, including the involvement of university students in the development of the rover, the software, and the collation of the scientific data. It also presents the results of the trial phase of this program including the impact on student engagement and learning outcomes.",space,207,unknown
10.1016/b978-0-444-59520-1.50104-4,to_check,Computer Aided Chemical Engineering,scopus,2012-01-01,sciencedirect,intelligent automation platform for bioprocess development,https://api.elsevier.com/content/abstract/scopus_id/84862870640,"High throughput technology has been increasingly adapted for drug screen and bioprocess development, due to the small amount of processing materials and reagents required and parallel experiment execution. It allows a wide design space to be explored in order to discover novel bioprocess solutions. Currently, the high throughput experiments for bioprocess development are implemented in a sequential fashion in which liquid handling system will perform the web lab experiment to prepare the samples; standalone analysis devices detect the data such as protein concentration; and specific software is used to realise the data analysis for process design or further experimentation.
                  The aim of this paper is to show how the efficiency of the high throughput bioprocess development approach can be enhanced by creating an intelligent automation platform that systematically drives liquid handling system, analysis devices and data analysis to perform a closed-loop learning. The first generation prototype has been established which consists of three parts: automated devices, design algorithms and database. In order to prove the concept of prototype, both simulation and real experiments studies have been established. In this case study, the platform is used to investigate the solubility of lysozyme at various ion strengths and pH values. Tecan liquid handling system for experimentation as well as buffer preparation and a plate reader for uv absorption measurement to determine protein concentration were used as the automated devices. The simplex search algorithm and artificial neural network modelling were utilised as design algorithm to iteratively select the experiments to execute and determine the optimal design solution. An entity-relationship database with Tecan system configuration information and experimental data was established. The results demonstrate this integrated approach can implement experiments and data analysis automatically to provide specific bioprocess design solutions in a closed loop strategy at first time. It is a promising approach that may significant increase the level of lab automation to release the engineer from the labour intensive R&D activities and provides the base for sophisticated artificial intelligent learning in the future.",space,208,unknown
10.1016/j.advwatres.2009.01.001,to_check,Advances in Water Resources,scopus,2009-04-01,sciencedirect,pumping optimization of coastal aquifers based on evolutionary algorithms and surrogate modular neural network models,https://api.elsevier.com/content/abstract/scopus_id/62349136438,"Pumping optimization of coastal aquifers involves complex numerical models. In problems with many decision variables, the computational burden for reaching the optimal solution can be excessive. Artificial Neural Networks (ANN) are flexible function approximators and have been used as surrogate models of complex numerical models in groundwater optimization. However, this approach is not practical in cases where the number of decision variables is large, because the required neural network structure can be very complex and difficult to train. The present study develops an optimization method based on modular neural networks, in which several small subnetwork modules, trained using a fast adaptive procedure, cooperate to solve a complex pumping optimization problem with many decision variables. The method utilizes the fact that salinity distribution in the aquifer, depends more on pumping from nearby wells rather than from distant ones. Each subnetwork predicts salinity in only one monitoring well, and is controlled by relatively few pumping wells falling within certain control distance from the monitoring well. While the initial control area is radial, its shape is adaptively improved using a Hermite interpolation procedure. The modular neural subnetworks are trained adaptively during optimization, and it is possible to retrain only the ones not performing well. As optimization progresses, the subnetworks are adapted to maximize performance near the current search space of the optimization algorithm. The modular neural subnetwork models are combined with an efficient optimization algorithm and are applied to a real coastal aquifer in the Greek island of Santorini. The numerical code SEAWAT was selected for solving the partial differential equations of flow and density dependent transport. The decision variables correspond to pumping rates from 34 wells. The modular subnetwork implementation resulted in significant reduction in CPU time and identified an even better solution than the original numerical model.",space,209,not included
10.1039/d1nr01109j,to_check,core,'Royal Society of Chemistry (RSC)',2021-01-01 00:00:00,core,enabling autonomous scanning probe microscopy imaging of single molecules with deep learning,,"Scanning probe microscopies allow investigating surfaces at the nanoscale, in real space and with unparalleled signal-to-noise ratio. However, these microscopies are not used as much as it would be expected considering their potential. The main limitations preventing a broader use are the need of experienced users, the difficulty in data analysis and the time-consuming nature of experiments that require continuous user supervision. In this work, we addressed the latter and developed an algorithm that controlled the operation of an Atomic Force Microscope (AFM) that, without the need of user intervention, allowed acquiring multiple high-resolution images of different molecules. We used DNA on mica as a model sample to test our control algorithm, which made use of two deep learning techniques that so far have not been used for real time SPM automation. One was an object detector, YOLOv3, which provided the location of molecules in the captured images. The second was a Siamese network that could identify the same molecule in different images. This allowed both performing a series of images on selected molecules while incrementing the resolution, as well as keeping track of molecules already imaged at high resolution, avoiding loops where the same molecule would be imaged an unlimited number of times. Overall, our implementation of deep learning techniques brings SPM a step closer to full autonomous operation",space,210,unknown
10.1051/0004-6361/201833390,to_check,core,A new method for unveiling Open Clusters in Gaia: new nearby Open Clusters confirmed by DR2,2020-07-13 00:00:00,core,https://core.ac.uk/download/328873796.pdf,'EDP Sciences',"Context. The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in astronomy. It includes precise astrometric data (positions, proper motions, and parallaxes) for more than 1.3 billion sources, mostly stars. To analyse such a vast amount of new data, the use of data-mining techniques and machine-learning algorithms is mandatory. Aims. A great example of the application of such techniques and algorithms is the search for open clusters (OCs), groups of stars that were born and move together, located in the disc. Our aim is to develop a method to automatically explore the data space, requiring minimal manual intervention. Methods. We explore the performance of a density-based clustering algorithm, DBSCAN, to find clusters in the data together with a supervised learning method such as an artificial neural network (ANN) to automatically distinguish between real OCs and statistical clusters. Results. The development and implementation of this method in a five-dimensional space (l, b, Ï, Î¼Î±*, Î¼Î´) with the Tycho-Gaia Astrometric Solution (TGAS) data, and a posterior validation using Gaia DR2 data, lead to the proposal of a set of new nearby OCs. Conclusions. We have developed a method to find OCs in astrometric data, designed to be applied to the full Gaia DR2 archive",space,211,unknown
https://core.ac.uk/download/346440943.pdf,to_check,core,Western CEDAR,2020-05-18 00:00:00,core,vikingbot: the starcraft artificial intelligence,,"VikingBot is an automated AI that plays StarCraft by using a combination of machine learning and artificial intelligence. High level strategies are planned using the Brown-UMBC Reinforcement Learning and Planning (BURLAP), library which implements planning algorithms and provides interfaces for defining a domain and models of that domain for planning. For the planning, we used the BURLAP implementation of the sparse sampling algorithm because the time complexity is independent of the size of the state space, and we have to plan quickly in real time. SARSA reinforcement learning is used for a machine learning model that controls combat units. Various other helping functions are distributed to agent classes that aid in the AI in the different areas of the game. These agents are categorized as strategy, economy, combat, and intelligence. By using these parts in tandem VikingBot aims to use less training resources and still be able to play games at a high enough level to beat a human player",space,212,included
https://core.ac.uk/download/212853933.pdf,to_check,core,'Wiley',2018-01-01 00:00:00,core,towards robust and domain invariant feature representations in deep learning,10.13016/M2MK65C49,"A fundamental problem in perception-based systems is to define

and learn representations of the scene that are more robust and adaptive to several nuisance

factors. Over the recent past, for a variety of tasks involving images, learned representations  have been empirically shown to outperform handcrafted ones.

However, their inability to generalize across varying data distributions poses the following

question: Do representations learned using deep networks just fit a given data distribution or do

they sufficiently model the underlying structure of the problem ? This question could be

understood using a simple example: If a learning algorithm is shown a number of images of a simple handwritten digit, then the representation learned should be generic enough to identify the same digit in

a different form. With regards to deep networks, although the learned representation has been shown

to be robust to various forms of synthetic distortions such as random noise, they fail in the presence of

more implicit forms of naturally occurring distortions. In this dissertation, we

propose approaches to mitigate the effect of such distortions and in the process, study some

vulnerabilities of deep networks to small imperceptible changes that occur in the given input.

The research problems that comprise this dissertation lie in the cross section of two open topics: (1)

Studying and developing methods that enable neural networks learn robust representations (2) Improving

generalization of neural nets across domains.  The first part of the dissertation  approaches the problem of robustness from two broad viewpoints:  Robustness to external nuisance factors that occur in the data and robustness

(or a lack thereof) to perturbations of the learned feature space.  In the second part, we focus on learning representations that are invariant to external covariate

shift, which is more commonly termed as domain shift. 

Towards learning representations robust

to external nuisance factors, we propose an approach that couples a deep convolutional neural

network with a low-dimensional discriminative embedding learned using triplet probability

constraints to solve the unconstrained face analysis problem. While previous approaches in this area have proposed scalable yet ad-hoc solutions to this problem, we propose a principled and parameter free formulation which is based on maximum likelihood estimation. In addition, we employ the principle of transfer learning to realize a deep network architecture that can train faster and on lesser data yet significantly outperforms existing approaches on the unconstrained face verification task. We demonstrate the robustness

of the approach to challenges including age, pose, blur and clutter by performing clustering

experiments on challenging benchmarks.

Recent seminal works have shown that deep neural networks are susceptible to visually imperceptible perturbations of the input. In this dissertation, we build on their ideas in two unique ways: (a) We show that neural networks that perform pixel-wise semantic segmentation tasks also suffer from this vulnerability, despite being trained with more extra information compares to simple classification tasks.  In addition, we present a novel self correcting mechanism in segmentation networks and provide an efficient way to generate such perturbations (b) We present a novel approach to regularize deep neural networks by perturbing intermediate layer activations in an efficient manner, thereby exploring the trade-off between conventional regularization and adversarial robustness within the context of very deep networks. Both of these works provide interesting directions towards understanding the secure nature of deep learning algorithms. 

While humans find it extremely simple to generalize their knowledge across domains, machine learning algorithms including deep neural networks suffer from the problem of domain shift across what are commonly termed as 'source' (S) 

and 'target' (T) distributions. Let the data that a learning algorithm

is trained on be sampled from  S. If the real data used to evaluate the model is

then sampled from T, then the learnt model under-performs on the target data. This inability to generalize is characterized as domain shift. 

Our attempt to address this problem involves learning a common

feature subspace, where distance between source and target distributions are minimized. Estimating the distance between different domains is highly non-trivial and is an open research

problem in itself. In our approach we parameterize the distance measure by using a Generative

Adversarial Network (GAN). A GAN involves a two player game between two mappings com-

monly termed as generator and discriminator. These mappings are learned simultaneously by

employing an adversarial game, i.e. by letting the generator fool the discriminator and enabling

the discriminator to outperform the generator. This adversarial game can be formulated as a

minimax problem. In our approach, we learn three mappings simultaneously: the generator,

discriminator and a feature mapping that contains information about both the content and the

domain of the input. We deploy a two-level minimax game, where the first level is a competition

between the generator and a discriminator similar to a GAN; the second level game is where the

feature mapping attempts to fool the discriminator thereby introducing domain invariance in

the learned feature representation. We have extensively evaluated this approach for different

tasks such as object classification and semantic segmentation, where we achieve state of the

art results across several real datasets. In addition to the conceptual novelty, our approach

presents a more efficient and scalable solution compared to other approaches that attempt to

solve the same problem.

In the final part of this dissertation, we describe some ongoing efforts and future directions of research. Inspired from the study of perturbations described above, we propose a novel metric on how to effectively choose pixels to label given an image, for a pixel-wise segmentation task. This has the potential to significantly reduce the labeling effort and our preliminary results for the task of semantic segmentation are encouraging.  While the domain adaptation approach proposed above considered static images, we propose an extension to video data aided by the use of recurrent neural networks.  Use of full temporal information, when available, provides the perceptual system additional context to disambiguate among smaller object classes  that commonly occur in real scenes",space,213,not included
sapienza universitÃ  editrice,to_check,core,Nanoseismic monitoring for detection of rockfalls. Experiments in quarry areas,2018-01-01 00:00:00,core,10.4408/ijege.2018-01.o-03,https://core.ac.uk/download/159636017.pdf,"Le frane per crollo da ammassi rocciosi fratturati sono tra i processi di instabilitÃ  gravitativa che piÃ¹ frequentemente interessano opere antropiche quali tagli su versanti naturali o artificiali, pareti di cava, trincee stradali, autostradali o ferroviarie, sia per ciÃ² che attiene le aree di distacco che per quelle di accumulo. Nellâambito dellâapplicazione di sistemi di early warning per la gestione del rischio geologico legato a queste tipologie di frana, una sperimentazione della tecnica del monitoraggio nanosismometrico Ã¨ stata effettuata presso due siti estrattivi non piÃ¹ in attivitÃ : le âPirrereâ della Baia di Cala Rossa sullâisola di Favignana (Trapani), in Sicilia, e la cava dismessa di Acuto (Frosinone), in Italia Centrale. Il monitoraggio nanosismometrico Ã¨ una tecnica di indagine che consente di individuare e localizzare deboli eventi sismici, fino a magnitudo locale (ML) nellâordine di -3, attraverso lâimpiego di quattro sensori sismometrici disposti secondo una specifica geometria di array detta SNS (Seismic Navigation System).

Nel presente lavoro, mediante il software NanoseismicSuite sono stati analizzati 73 eventi di crollo indotti artificialmente attraverso la caduta controllata di blocchi di roccia nei due siti estrattivi abbandonati; sono stati lanciati, simulando fenomeni di rockfalls, rispettivamente 47 blocchi di roccia nella cava di Acuto e 26 eventi in quattro diverse cave a cielo aperto presenti nel settore occidentale di Cala Rossa. Tali eventi, avendo punto epicentrale noto, hanno permesso di determinare il miglior modello di sottosuolo in termini di valori di velocitÃ  delle onde P ed S attraverso unâoperazione di back analysis. Lâanalisi Ã¨ stata, infatti, effettuata variando i valori di velocitÃ  e scegliendo quelli relativi allâepicentro teorico ottenuto dallâanalisi dellâevento che fosse il piÃ¹ vicino possibile al punto reale di impatto del blocco di roccia. Al fine di valutare la sensibilitÃ  della geometria dellâarray SNS e lâinfluenza del sito di installazione sulla capacitÃ  di individuare e localizzare gli eventi, le sperimentazioni sono state condotte sia variando il raggio di apertura che la zona di installazione degli array: presso Acuto le acquisizioni di segnale sono state condotte prima con un array SNS con apertura di 20 m e successivamente con un array di apertura 10 m, mentre presso Cala Rossa lâarray Ã¨ stato installato alternativamente allâaperto in unâarea di plateau roccioso ed in una galleria facente parte dellâarea di cava abbandonata.

Analizzando i dati si Ã¨ ottenuta una precisione dellâubicazione epicentrale compresa tra il 10 ed il 22% della distanza che intercorre tra la sorgente e lâarray nanosismometrico. Il miglior modello di sottosuolo ottenuto per entrambi i casi di studio Ã¨ risultato avere una velocitÃ  delle onde P pari a 900 m/s ed un rapporto VP/VS pari a 1.73, valori in accordo con le condizioni di intenso stato di fratturazione delle rocce carbonatiche affioranti nelle due zone di cava. Per gli eventi di crollo indotti la magnitudo ML Ã¨ risultata essere compresa tra -2.8 e -1.3; considerando lâenergia sviluppata dallâimpatto, legata alla massa del blocco ed allâaltezza e alla velocitÃ  di caduta, non Ã¨ stato possibile definire una relazione tra magnitudo ed energia, probabilmente a causa delle differenti caratteristiche del punto di impatto dei diversi blocchi. In generale, si Ã¨ osservato che la precisione di ubicazione degli eventi, in termini di azimuth e distanza dal reale epicentro, Ã¨ risultata paragonabile sia variando lâapertura dellâarray che variando il sito di installazione. Per il sito sperimentale di Acuto, il processo di picking manuale del tempo di primo arrivo delle onde P Ã¨ risultato essere piÃ¹ affidabile nel caso di array con apertura pari a 10 m. La sperimentazione effettuata a Cala Rossa ha permesso, invece, di osservare una migliore capacitÃ  di individuazione degli eventi nelle tracce relative allâarray posizionato in galleria a causa della minore rumorositÃ  di base del sito di installazione.

Tra le registrazioni sismometriche sono state identificate varie tipologie di segnali, oltre a quelli generati dal lancio dei blocchi, alcune riconducibili ad eventi naturali di crollo altre a deboli terremoti. Lâanalisi dei segnali riferibili alla prima tipologia di eventi naturali, effettuata tenendo in considerazione i modelli di sottosuolo precedentemente calibrati, ha portato allâidentificazione in ambedue i siti di aree aventi maggiore suscettibilitÃ  a frane per crollo. In definitiva, si puÃ² ritenere che i risultati ottenuti in questo studio siano incoraggianti rispetto allâefficacia della tecnica di monitoraggio nanosismometrico nellâindividuazione e nellâubicazione di fenomeni di crollo in roccia e portano a ritenere questa tecnica potenzialmente applicabile in aree in cui tali eventi possono interferire con infrastrutture antropiche.In the frame of early warning and risk mitigation studies for landslide processes involving rock masses, two quarry areas (Cala Rossa Bay in Sicily and Acuto in Central Italy) were monitored with SNS (Seismic Navigation System) arrays. In this study, 73 rockfalls were simulated by launches of rock blocks. This allowed to perform a back analysis for defining the best seismic velocity model of the subsoil half-space; the records related to each impact caused by the rock block launch were managed by the nanoseismic monitoring approach, varying the velocity model to obtain a theoretical epicentre as close as possible to the actual location of the impact point. In order to evaluate the sensibility of the SNS array, the results obtained by different array apertures and positions were compared in terms of azimuth and distance error with respect to the real epicentres. On the other hand, several natural rockfalls were detected; their analysis allowed to identify areas having higher susceptibility to rockfalls by using the previously calibrated subsoil half-space model. Further studies are required to better define the areas prone to rockfall generation in the considered test sites; nevertheless, the here obtained results show an encouraging perspective about the application of the nanoseismic monitoring with respect to vulnerable infrastructures in rockfall prone areas",space,214,unknown
'edp sciences',to_check,core,A new method for unveiling open clusters in Gaia New nearby open clusters confirmed by DR2,2018-10-16 00:00:00,core,10.1051/0004-6361/201833390,,"International audienceContext. The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in astronomy. It includes precise astrometric data (positions, proper motions, and parallaxes) for more than 1.3 billion sources, mostly stars. To analyse such a vast amount of new data, the use of data-mining techniques and machine-learning algorithms is mandatory.Aims. A great example of the application of such techniques and algorithms is the search for open clusters (OCs), groups of stars that were born and move together, located in the disc. Our aim is to develop a method to automatically explore the data space, requiring minimal manual intervention.Methods. We explore the performance of a density-based clustering algorithm, DBSCAN, to find clusters in the data together with a supervised learning method such as an artificial neural network (ANN) to automatically distinguish between real OCs and statistical clusters.Results. The development and implementation of this method in a five-dimensional space (l, b, Ï, Î¼Î±*, Î¼Î´) with the Tycho-Gaia Astrometric Solution (TGAS) data, and a posterior validation using Gaia DR2 data, lead to the proposal of a set of new nearby OCs. Conclusions. We have developed a method to find OCs in astrometric data, designed to be applied to the full Gaia DR2 archive",space,215,not included
10.1080/17445760.2017.1390098,to_check,core,A self-aware paradigm for autonomous architectural systems within the Internet of Things,,core,https://core.ac.uk/download/96575110.pdf,'Informa UK Limited',"This research explores a newfangled approach for the notion that architecture is

no longer concerned with the organization of space and matter, but rather a

system of systems with self-organizational behavior and progressively complex

programmatic interventions. Such system represents an Internet of Things (IoT)

paradigm that has the capacity to self-organize a pre-defined architecture

acclimating dynamically to the demands of the environment through a process of

self-awareness and re-configurability.

This paper is an attempt to develop spaces that bring computation into the

physical world by introducing artificial intelligence into building systems so as to

communicate, exchange information, and allow right responses and decisions

within a sustainable manner. Environments with embedded computational

systems and adaptive reconfiguration behavior will be precisely studied.

With an intensive focus on smart, self reconfiguring architecture that has

embraced kinetic motion as an approach for environmental adaptation and

responsiveness. This study addresses the networked organization of sensory

systems that incorporate computational platforms in relation to userâs desires,

where architecture turns into an interactive intermediary between human and

computation.

Correspondingly, the authors are particularly aiming to propose three conceptual

frameworks for delivering a smart system at the architectural scale which is

capable of reconfiguring and interacting constantly in real time, precisely

focusing on an initial implementation for the first framework utilizing an

experimental customized prototype, while the other two frameworks would be

posteriorly studied through future work.

Thus the IoT will foster a rapid development in intelligent systems which would

directly introduce an advanced technological leap forward in accommodating a

new way of demonstrating human-computer interaction within a novel

architectural design approach",space,216,unknown
10.1109/icacsis.2018.8618144,to_check,2018 International Conference on Advanced Computer Science and Information Systems (ICACSIS),IEEE,2018-10-28 00:00:00,ieeexplore,protagoras: a service for tagging e-commerce products at scale,https://ieeexplore.ieee.org/document/8618144/,"Despite widespread adoption of machine learning to solve real world problems, the implementation of ML solutions in production environment is more complicated than it seems. It is quite straightforward to write machine learning codes these days but they are not designed to be deployed in production scale where millions of requests per day is a norm. In this paper, we describe our implementation of a ML service for large scale product tagging in e-commerce called Protagoras. The problem of tagging products can be seen as multi-label classification where the labels are product tags. By performing the classification within each product category, the precision can be increased and the inference can be performed faster. Protagoras combined the scalability and speed of microservice implementation in Golang and robust machine learning implementation in Python. We present the architecture of the system with all its components including API endpoints, job queue, database, and monitoring. The benchmark shows that, even with 1000 classifiers in one category, the average latency for online inference is below 300 millisecond. The throughput can be further maximized by replicating the service into multiple servers.",e-commerce,217,unknown
10.1109/isda.2009.48,to_check,2009 Ninth International Conference on Intelligent Systems Design and Applications,IEEE,2009-12-02 00:00:00,ieeexplore,a jade-based art-inspired ontology and protocols for handling trust and reputation,https://ieeexplore.ieee.org/document/5364833/,"Trust and reputation management play an important role in agent-based recommender systems. Although several protocols and ontologies of agents using trust and reputation has been proposed, none of them has been so extensively used and implicitly accepted by research community as those from agent reputation and trust (ART in advance) testbed. The motivation of this adaptation is to facilitate the use of ART principles in real distributed applications instead of a centralized testbed for experimentation. This paper presents an adaptation of the protocols proposed by ART testbed to a codification for the most popular agent platform: JADE. This implementation follows a coherent API with the FIPA protocols included in JADE distribution for an easy use. We also complement the behaviours of corresponding initiators and responders of the protocols with an ontology formed by a collection of concepts, predicates and agent actions that may represent as the ART application domain as any other service-oriented domain. The proposal has been designed to be applied in domains where multi-agent e-commerce solutions are needed. Future work includes the integration of this ontology and protocols in context-aware scenarios such as an airport.",e-commerce,218,not included
10.1109/icssit48917.2020.9214285,to_check,2020 Third International Conference on Smart Systems and Inventive Technology (ICSSIT),IEEE,2020-08-22 00:00:00,ieeexplore,evaluation of information retrieval performance metrics using real estate ontology,https://ieeexplore.ieee.org/document/9214285/,"With an ever-increasing data over the internet, efficient information retrieval of data for its users has always been on stake. The issue is deeper when it comes to e-governance based real estate scenario, where there is a lack of interoperability found in buying and selling of property formats and loads of miscellaneous legal terminology that fetches inappropriate legal documentation for the users. Hence in today's Semantic web scenario, a Real estate ontology has been proposed under the Real Estate Information Retrieval Model and the model is evaluated by applying various information retrieval measures. This paper shows the implementation and analysis of various popular information retrieval metrics, be it a set retrieval or rank retrieval that can be used for performance evaluation of web retrieval of data. The proposed system depicts better results in almost all the metrics as compared to the initial user query set.",e-commerce,219,unknown
10.1109/icoict.2017.8074710,to_check,2017 5th International Conference on Information and Communication Technology (ICoIC7),IEEE,2017-05-19 00:00:00,ieeexplore,indonesia infrastructure and consumer stock portfolio prediction using artificial neural network backpropagation,https://ieeexplore.ieee.org/document/8074710/,"Artificial Neural Network (ANN) method is increasingly popular to build predictive model that generated small error prediction. To have a good model, ANN needs large dataset as an input. ANN backpropagation is a gradient decrease method to minimize the output error squared. Stock price movements are suitable with ANN requirement: it is a large data set because stock price is recorded up to every seconds, usually called high frequency data. The implementation of stock price prediction using ANN approach is quite new. The predictive model help investor in building stock portfolio and their decision making process. Buying some stocks in portfolio decrease diversified risk and increases the chance of higher return. In this paper, we show how to generate prediction model using artificial neural network backpropagation of stock price and forming portfolio with predicted price that bring prediction of the portfolio with the smallest error. The data set we use is historical stock price data from ten different company stocks of infrastructure and consumer sector Indonesia Stock Exchage. The results is for lower risk condition, ANN predictive model gives higher expected return than the return from real condition, while for higher risk, the return from the real condition is higher than the ANN predictive model.",e-commerce,220,not included
10.1109/bigdataservice49289.2020.00034,to_check,2020 IEEE Sixth International Conference on Big Data Computing Service and Applications (BigDataService),IEEE,2020-08-06 00:00:00,ieeexplore,zenden - a personalized house searching application,https://ieeexplore.ieee.org/document/9179601/,"A method directly linking buyers and sellers in the housing market would benefit both parties in both the renting and purchasing domains. The real estate mobile applications available today, such as Zillow and Redfin, are highly dependent on filter based searches (location, price, bedrooms, etc) and contain many screens that require zooming in and out of map-based interfaces. Furthermore, the student market and lower-income demographics that are more likely renting a house instead of buying are usually not the focus of these applications. Described in this paper is a novel mobile application that will change the way people in the renting market, e.g., university students, find lodging. The implementation contains a streamlined swipe-based user interface backed by a user-house recommender system to manage content. Deep learning techniques are used in building the recommender system that recommends houses to users based on their view history. For image classification, we build convolutional neural networks (CNN) for analyzing house images. The goal is to create a personalized, easy to use application that will reduce the effort and time required for people in the renting market to find housing.",e-commerce,221,unknown
10.1109/jiot.2016.2556006,to_check,IEEE Internet of Things Journal,IEEE,2017-04-01 00:00:00,ieeexplore,optimized day-ahead pricing with renewable energy demand-side management for smart grids,https://ieeexplore.ieee.org/document/7457198/,"Internet of Things (IoT) has recently emerged as an enabling technology for context-aware and interconnected âsmart things.â Those smart things along with advanced power engineering and wireless communication technologies have realized the possibility of next generation electrical grid, smart grid, which allows users to deploy smart meters, monitoring their electric condition in real time. At the same time, increased environmental consciousness is driving electric companies to replace traditional generators with renewable energy sources which are already productive in user's homes. One of the most incentive ways is for electric companies to institute electricity buying-back schemes to encourage end users to generate more renewable energy. Different from the previous works, we consider renewable energy buying-back schemes with dynamic pricing to achieve the goal of energy efficiency for smart grids. We formulate the dynamic pricing problem as a convex optimization dual problem and propose a day-ahead time-dependent pricing scheme in a distributed manner which provides increased user privacy. The proposed framework seeks to achieve maximum benefits for both users and electric companies. To our best knowledge, this is one of the first attempts to tackle the time-dependent problem for smart grids with consideration of environmental benefits of renewable energy. Numerical results show that our proposed framework can significantly reduce peak time loading and efficiently balance system energy distribution.",e-commerce,222,unknown
http://arxiv.org/abs/1702.03488v2,to_check,arxiv,arxiv,2017-02-12 00:00:00,arxiv,octopus: a framework for cost-quality-time optimization in crowdsourcing,http://arxiv.org/abs/1702.03488v2,"We present Octopus, an AI agent to jointly balance three conflicting task
objectives on a micro-crowdsourcing marketplace - the quality of work, total
cost incurred, and time to completion. Previous control agents have mostly
focused on cost-quality, or cost-time tradeoffs, but not on directly
controlling all three in concert. A naive formulation of three-objective
optimization is intractable; Octopus takes a hierarchical POMDP approach, with
three different components responsible for setting the pay per task, selecting
the next task, and controlling task-level quality. We demonstrate that Octopus
significantly outperforms existing state-of-the-art approaches on real
experiments. We also deploy Octopus on Amazon Mechanical Turk, showing its
ability to manage tasks in a real-world dynamic setting.",e-commerce,223,included
10.1016/j.ins.2018.11.028,to_check,Information Sciences,scopus,2019-04-01,sciencedirect,machine learning based privacy-preserving fair data trading in big data market,https://api.elsevier.com/content/abstract/scopus_id/85056879362,"In the era of big data, the produced and collected data explode due to the emerging technologies and applications that pervade everywhere in our daily lives, including internet of things applications such as smart home, smart city, smart grid, e-commerce applications and social network. Big data market can carry out efficient data trading, which provides a way to share data and further enhances the utility of data. However, to realize effective data trading in big data market, several challenges need to be resolved. The first one is to verify the data availability for a data consumer. The second is privacy of a data provider who is unwilling to reveal his real identity to the data consumer. The third is the payment fairness between a data provider and a data consumer with atomic exchange. In this paper, we address these challenges by proposing a new blockchain-based fair data trading protocol in big data market. The proposed protocol integrates ring signature, double-authentication-preventing signature and similarity learning to guarantee the availability of trading data, privacy of data providers and fairness between data providers and data consumers. We show the proposed protocol achieves the desirable security properties that a secure data trading protocol should have. The implementation results with Solidity smart contract demonstrate the validity of the proposed blockchain-based fair data trading protocol.",e-commerce,224,unknown
10.11606/d.55.2019.tde-21082019-165653,to_check,core,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",2019-08-21 00:00:00,core,analysis of the network of products bought together in electronic commerce,,"Este trabalho aborda as Ã¡reas de teoria dos grafos, sistemas de recomendaÃ§Ã£o, e comÃ©rcio eletrÃ´nico, que jÃ¡ foram tema de diversas publicaÃ§Ãµes ao longo das Ãºltimas dÃ©cadas. Entretanto, o estudo da importÃ¢ncia da utilizaÃ§Ã£o de medidas de centralidade de redes como atributos preditivos de modelos de aprendizado de mÃ¡quina Ã© um assunto que ainda nÃ£o foi explorado pela literatura. Neste trabalho, alÃ©m de relatarmos resultados que sugerem que essas medidas de centralidade podem aumentar a precisÃ£o dos modelos preditivos, tambÃ©m apresentamos os principais conceitos teÃ³ricos de redes complexas, como tipos de redes, caracterizaÃ§Ã£o, mÃ©tricas de distÃ¢ncia, alÃ©m de propriedades de redes reais. TambÃ©m apresentamos as ferramentas e metodologia utilizadas para o desenvolvimento de um webcrawler prÃ³prio, software necessÃ¡rio para a construÃ§Ã£o da rede de produtos comprados em conjunto no comÃ©rcio eletrÃ´nico. Modelos de aprendizado de mÃ¡quina foram treinados utilizando a base de produtos obtida pelo webcrawler, possibilitando a obtenÃ§Ã£o de modelos preditivos de estimativa de preÃ§os de produtos, e de previsÃ£o de probabilidade de ligaÃ§Ã£o entre produtos da rede. A performance dos modelos preditivos obtidos sÃ£o apresentadas.This work approaches areas such as graph theory, recommendation systems, and electronic commerce, which have been chosen as topics for several publications over the last decades. Although, studying the importance of using network centrality measures as predictive features within machine learning models is a topic which was not yet explored on literature. In this work, besides reporting results which suggest that those centrality measures can increase the precision of predictive models, we also present the main theoretical concepts of complex networks, such as network types, characterization, distance metrics, besides some properties of real networks. We also present the tools and methodology used on the development of our own webcrawler, a software required for the generation of the network of products bought together in the electronic commerce. Machine learning models were trained using the product database obtained using the webcrawler, allowing the achievement of predictive models for product price estimation, and also link prediction between products of the network. The performance of the predictive models are also presented",e-commerce,225,unknown
10.3390/su10093142,to_check,core,'MDPI AG',2018-09-03 00:00:00,core,"a systematic review of smart real estate technology: drivers of, and barriers to, the use of digital disruptive technologies and online platforms",http://www.mdpi.com/2071-1050/10/9/3142,"Real estate needs to improve its adoption of disruptive technologies to move from traditional to smart real estate (SRE). This study reviews the adoption of disruptive technologies in real estate. It covers the applications of nine such technologies, hereby referred to as the Big9. These are: drones, the internet of things (IoT), clouds, software as a service (SaaS), big data, 3D scanning, wearable technologies, virtual and augmented realities (VR and AR), and artificial intelligence (AI) and robotics. The Big9 are examined in terms of their application to real estate and how they can furnish consumers with the kind of information that can avert regrets. The review is based on 213 published articles. The compiled results show the state of each technologyâs practice and usage in real estate. This review also surveys dissemination mechanisms, including smartphone technology, websites and social media-based online platforms, as well as the core components of SRE: sustainability, innovative technology and user centredness. It identifies four key real estate stakeholdersâconsumers, agents and associations, government and regulatory authorities, and complementary industriesâand their needs, such as buying or selling property, profits, taxes, business and/or other factors. Interactions between these stakeholders are highlighted, and the specific needs that various technologies address are tabulated in the form of a what, who and how analysis to highlight the impact that the technologies have on key stakeholders. Finally, stakeholder needs as identified in the previous steps are matched theoretically with six extensions of the traditionally accepted technology adoption model (TAM), paving the way for a smoother transition to technology-based benefits for consumers. The findings pertinent to the Big9 technologies in the form of opportunities, potential losses and exploitation levels (OPLEL) analyses highlight the potential utilisation of each technology for addressing consumersâ needs and minimizing their regrets. Additionally, the tabulated findings in the form of what, how and who links the Big9 technologies to core consumersâ needs and provides a list of resources needed to ensure proper information dissemination to the stakeholders. Such high-quality information can bridge the gap between real estate consumers and other stakeholders and raise the state of the industry to a level where its consumers have fewer or no regrets. The study, being the first to explore real estate technologies, is limited by the number of research publications on the SRE technologies that has been compensated through incorporation of online reports",e-commerce,226,not included
10.1145/1363686.1363695,to_check,core,,2015-11-26 00:00:00,core,electricity market simulation: multiagent system approach,,"This paper suggests a multiagent system (MAS) approach for market simulation. This is achieved through analysis, modeling, implementation and simulation of artificial markets populated by software agents that represent economic self interested agents. Software agents are the constructs of a complex system, an artificial market that model a real existing market or an outline of a market design. The interest in simulating a market is multiple: exploiting existing market rules, searching for market design flaws and loopholes, and supporting decision making during a market mechanism design process. The main aim of the suggested approach is to analyze the behavior that emerges from the interaction of self interested agents acting in an artificial market. AEMAS (Artificial Economy MultiAgent System), a multiagent system architecture inspired by the Market Oriented Programming (MOP) approach is defined. In different economical sectors, e.g. energy markets, there is no consensus about which structures lead to social welfare maximization outcomes. An approach to find adequate architectures allows different market structure instances to be created and simulated, to ease the design and analysis of alternative structures. These alternatives can then be compared and potential design flaws eventually risen by simulation identified. Taking the electricity market as an example, two instances of the proposed architecture are presented, corresponding to the centralized dispatch arrangement common to non restructured markets, and the auction based pool, common to restructured markets. Copyright 2008 ACM.3438Al-Agtash, S., Evolutionary negotiation strategies in emerging electricity markets (2004) Lecture Notes in Artificial Intelligence, 3070, pp. 1099-1104. , ICAISC 2004Batten, D.F., (2000) Discovering Artificial Economics: How Agents Learn and Economies Evolve, , Westview Press, Boulder, ColoradoBower, J., Bunn, D.W., Experimental analysis of the efficiency of uniform-price versus discriminatory auctions in the England and Wales electricity market (2001) Journal of Economic Dynamics and Control, 25 (3-4), pp. 561-592. , MarBunn, D.W., Oliveira, F.S., Agent-based simulation - An application to the New Electricity Trading Arrangements of England and Wales (2001) IEEE Transactions on Evolutionary Computing, 5 (5), pp. 493-503. , OctF. S. Carvalho and C. D. N. Vinhal. Temporal difference methods applied to thermoelectric energy markets: A distributed multi-agents approach. In XV Congresso Brasileiro de AutomÃ¡tica (CBA 2004), Gramado, RS, Sept.21-24 2004(2000) FIPA 2000, , http://www.fipa.org/repository/fipa2000.html, Foundation for Intelligent Physical AgentsGenoud, C., Regulation as a game: The role of independent regulatory agencies in the regulatory process (2003) CARR Risk and Regulation Research Student Conference, , London, UK, Sept, London School of Economics and Political ScienceM. Griss and R. Letsinger. Games at work - Agent mediated e-commerce simulation. In Autonomous Agents 2000, Barcelona, Spain, June 2000. HP Laboratories Technical Report HPL-2000-52Harp, S.A., Brignone, S., Wollenberg, B.F., Samad, T., SEPIA: A simulator for electric power industry agents (2000) IEEE Control Systems Magazine, 20 (4), pp. 53-69. , AugLima, W., Freitas, E.N.A., A multi agent based simulator for Brazilian wholesale electricity energy market (2006) Lecture Notes in Computer Science, 4140, pp. 68-77. , X Ibero-American Artificial Intelligence Conference, XVIII Brazilian Artificial Intelligence Symposium, IBERAMIA '2006/SBIA '2006, of, RibeirÃ£o Preto, SP, Oct, SpringerMonclar, F.-R., Quatrain, R., Simulation of electricity markets: A multi-agent approach (2001) International Conference on Intelligent System Application to Power Systems, pp. 207-212. , Budapest, Hungary, June, IEEE Power Engineering SocietyPraÃ§a, I., Ramos, C., Vale, Z., Cordeiro, M., MASCEM: A multiagent system that simulates competitive electricity markets (2003) IEEE Intelligent Systems, 18 (6), pp. 54-60. , Nov-DecSimon, H.A., From substantive to procedural rationality (1979) Philosophy and Economic Theory, pp. 65-86. , F. Hahn and M. Hollis, editors, Oxford University PressSycara, K., Decker, K., Williamson, M., Matchmaking and brokering (1996) Proceedings of the Second International Conference on Multi-Agent SystemsTesfatsion, L., Agent-based computational economics: Growing economies from the bottom up (2002) Artificial Life, 8 (1), pp. 55-82Handbook of Computational Economics: Agent-Based Computational Economics (2006) Handbooks in Economics, 2. , L. Tesfatsion and K. L. Judd, editors, of, North HollandVeit, D., Matchmaking in electronic markets: An agent-based approach towards matchmaking in electronic negotiations (2003) Lecture Notes in Artificial Intelligence, 2882. , J. G. Carbonell and J. Siekmann, editors, SpringerWalter, I., (2006) Sistemas Multiagentes em Mercados de Energia ElÃ©trica, , PhD thesis, Faculdade de Engenharia ElÃ©trica e de ComputaÃ§Ã£o, Universidade Estadual de Campinas, DecWalter, I., Gomide, F., SimulaÃ§Ã£o de mercados de energia elÃ©trica: Abordagem multi-agentes (2005) VII SBAI SimpÃ³sio Brasileiro de AutomaÃ§Ã£o Inteligente, , O. R. Saavedra et al, editors, SÃ£o LuÃ­s, MA, SeptI. Walter and F. Gomide. Design of coordination strategies in multiagent systems via genetic fuzzy systems. Soft Computing, 10(10):903-915, Aug. 2006. Special Issue: New Trends in the Design of Fuzzy SystemsWalter, I., Gomide, F., Genetic fuzzy systems to evolve coordination strategies in multiagent systems (2007) International Journal of Intelligent Systems, 22 (9), pp. 971-991. , Special Issue on Genetic Fuzzy SystemsWellman, M.P., A market-oriented programming environment and its application to distributed multicommodity flow problems (1993) Journal of Artificial Intelligence Research, 1 (1), pp. 1-23. , AugWooldridge, M., Jennings, N.R., Intelligent agents: Theory and practice (1995) The Knowledge Engineering Review, 10 (2), pp. 115-15",e-commerce,227,unknown
,to_check,core,,2013-07-23 00:00:00,core,mobile code distributed systems a new development,,"Abstract: The paper presents an introduction in the Mobile Agents Systems and describes how this technology can be used in wireless applications. Also it is shown the possibility of securing wireless applications that use mobile agents and distributed computing. Wireless networks are a relatively new technology in the LAN market. With the weak encryption and security defined in the IEEE standards, wireless LANs, when improperly deployed or administered, can provide a significant risk to those economic sectors. These sectors include health-care, government, and banking in particular. Increasingly diverse heterogeneous wireless infrastructures in combination with more narrowly defined roles of parties participating in the delivery of applications to mobile users pose new challenges for support for delivering these applications. Key-Words: mobile agents, artificial intelligence, intelligent agents, wireless applications. 1. Mobile and intelligent agents Mobile software agents are a new concept used in distributed systems and this concept is based on human agents idea â real estate agent, travel agent. Figure 1 presents a new vision about intelligent agents",finance,228,not included
,to_check,core,,2018-05-23 00:00:00,core,matching de faces de diferentes domÃ­nios para sistemas de seguranÃ§a bancÃ¡rio,,"DissertaÃ§Ã£o (mestrado)âUniversidade de BrasÃ­lia, Faculdade de Tecnologia, Departamento de Engenharia ElÃ©trica, 2018.Um dos principais desafios enfrentados pelo sistema bancÃ¡rio Ã© garantir a seguranÃ§a das transaÃ§Ãµes financeiras. Devido Ã  conveniÃªncia e aceitaÃ§Ã£o, o uso de caracterÄ±Ìsticas faciais para autenticaÃ§Ã£o biomÃ©trica de usuÃ¡rios em sistemas bancÃ¡rios estÃ¡ se tornando uma tendÃªncia mundial. Essa abordagem de autenticaÃ§Ã£o de usuÃ¡rios estÃ¡ atraindo grandes investimentos de instituiÃ§Ãµes bancÃ¡rias e financeiras, especialmente em cenÃ¡rios de diferentes domÄ±Ìnios, nos quais imagens faciais tiradas de documentos de identificaÃ§Ã£o sÃ£o comparadas com autorretratos digitais (selfies) tiradas com cÃ¢meras de dispositivos mÃ³veis. Neste estudo, coletamos das bases de dados do maior banco pÃºblico brasileiro um grande dataset, chamado FaceBank, com 27.002 imagens de selfies e fotos de documentos de identificaÃ§Ã£o de 13.501 sujeitos. Em seguida, avaliamos os desempenhos de dois modelos de Redes Neurais Convolucionais bem referenciados (VGG-Face e OpenFace) para extraÃ§Ã£o de caracterÄ±Ìsticas profundas, bem como os desempenhos de quatro classificadores (SVM Linear, SVM Power Mean, Random Forest e Random Forest com o Ensemble Vote) para autenticaÃ§Ã£o robusta de face em diferentes domÄ±Ìnios. Com base nos resultados obtidos (precisÃµes superiores a 90%, em geral), Ã© possÄ±Ìvel concluir que a abordagem de matching de faces profundas avaliada neste estudo Ã© adequada para autenticaÃ§Ã£o de usuÃ¡rios em aplicaÃ§Ãµes bancÃ¡rias entre domÄ±Ìnios. AtÃ© onde sabemos, este Ã© o primeiro trabalho que usa um grande conjunto de dados composto por imagens bancÃ¡rias reais para avaliar a abordagem de autenticaÃ§Ã£o de face entre domÄ±Ìnios. AlÃ©m disso, este trabalho apresenta um estudo sobre as reais necessidades na implementaÃ§Ã£o futura de um sistema biomÃ©trico, propondo um sistema de nuvem para permitir a adoÃ§Ã£o de tecnologias biomÃ©tricas. Por fim, propÃµe tambÃ©m um modelo seguro e integrado de subsistema ABIS de transmissÃ£o de dados. Toda a anÃ¡lise e implementaÃ§Ã£o leva em conta a total aderÃªncia e compatibilidade com padrÃµes e especificaÃ§Ãµes propostos pelo governo brasileiro.Ensuring the security of transactions is currently one of the major challenges facing banking systems. The use of facial features for biometric authentication of users in banking systems is becoming a worldwide trend, due to the convenience and acceptability of this form of identification, and also because computers and mobile devices already have built-in cameras. This user authentication approach is attracting large investments from banking and financial institutions especially in cross-domain scenarios, in which facial images taken from ID documents are compared with digital self-portraits (selfies) taken with mobile device cameras. In this study, from the databases of the largest public Brazilian bank we collected a large dataset, called FaceBank, with 27,002 images of selfies and ID document photos from 13,501 subjects. Then, we assessed the performances of two well-referenced Convolutional Neural Networks models (VGG-Face and OpenFace) for deep face features extraction, as well as the performances of four effective classifiers (Linear SVM, Power Mean SVM, Random Forest and Random Forest with Ensemble Vote) for robust cross-domain face authentication. Based on the results obtained (authentication accuracies higher than 90%, in general), it is possible to conclude that the deep face matching approach assessed in this study is suitable for user authentication in cross-domain banking applications. To the best of our knowledge, this is the first study that uses a large dataset composed of real banking images to assess the cross-domain face authentication approach to be used in banking systems. As an additional, this work presents a study on the real needs in the future implementation of a biometric system proposing a cloud system to enable the adoption of biometrics technologies, creating a new model of service delivery. Besides that, proposes a secure and integrated ABIS Data Transmission subsystem model. All the analysis and implementation takes into account the total adherence and compatibility with the standards and specifications proposed by the Brazilian government, at the same time, establish mechanisms and controls to ensure the effective protection of data",finance,229,unknown
,to_check,core,,2007-11-22 00:00:00,core,(al) and an application program (known as an,,"The development and deployment of multi-agent systems in real world settings raises a number of important research issues and problems which must be overcome if Distributed AI (DAI) is to become a widespread solution technology. Work undertaken in the context of the ARCHON project has provided a number of important insights into these issues. By providing an in depth analysis of ARCHON&apos;s electricity transportation management application, this paper draws together many of the experiences obtained when building one of the world&apos;s first operational DAI systems.  Introduction  In many industrial applications a substantial amount of time, effort and finance has been devoted to developing complex and sophisticated software systems. These systems are often viewed in a piecemeal manner as isolated islands of automation, when, in reality, they should be seen as components of a much larger business function (Jennings, 1994a). The main benefit of taking a holistic perspective is that the partial ..",finance,230,not included
,to_check,core,,2012-10-25 00:00:00,core,desenvolvimento de um modelo de sistema multiagente para previsÃ£o de retorno sobre indices de aÃ§Ãµes,https://core.ac.uk/download/30375150.pdf,"DissertaÃ§Ã£o (mestrado) - Universidade Federal de Santa Catarina, Centro TecnolÃ³gico, Programa de PÃ³s-GraduaÃ§Ã£o em CiÃªncia da ComputaÃ§Ã£o, FlorianÃ³polis, 2010No mundo das finanÃ§as, a Teoria dos Mercados Eficientes (TME) afirma que a flutuaÃ§Ã£o do preÃ§o dos ativos financeiros Ã© aleatÃ³ria, sendo assim, nÃ£o existem maneiras de proteger o investidor, prevendo os futuros movimentos do mercado. Contudo, vÃ¡rias iniciativas empÃ­ricas tÃªm demonstrado que a afirmaÃ§Ã£o da TME nÃ£o Ã© totalmente correta. Entre as frentes de pesquisa que buscam prever os movimentos de ativos financeiros, pode-se destacar a Ã¡rea com um ponto de vista economÃ©trico, que tenta prever movimentos mediante mÃ©todos matemÃ¡ticos e estatÃ­sticos, como regressÃ£o linear e regressÃ£o nÃ£o linear, bem como as redes neurais. AlÃ©m disso, em outra Ã¡rea de pesquisa, hÃ¡ a Teoria Multiagente de Modelagem de Mercado (Theory of Multi-Agent Market Modeling) que foca sua atenÃ§Ã£o na microestrutura do mercado, partindo do princÃ­pio de que os movimentos de preÃ§os emergem da interaÃ§Ã£o de muitos agentes individuais do mercado. Contudo, esses modelos financeiros baseados em agentes tÃªm algumas limitaÃ§Ãµes. NÃ£o Ã© possÃ­vel adequar os agentes a dados reais do mercado para gerar previsÃµes futuras, pois na maioria dos modelos a tomada de decisÃ£o Ã© feita por meio de funÃ§Ãµes ad-hoc ou mecanismos que nÃ£o podem ser ajustados a dados externos. Para obter uma previsÃ£o real do modelo, Ã© preciso adaptar o mecanismo de decisÃ£o dos agentes com modelos economÃ©tricos, como as redes neurais, que podem ser ajustadas a sÃ©ries de dados reais. Dessa forma, por intermÃ©dio da interaÃ§Ã£o dos agentes, o modelo de mercado resultante pode capturar a dinÃ¢mica oculta do mercado e prever os movimentos futuros com uma eficÃ¡cia maior do que faria um sistema de regressÃ£o nÃ£o linear isoladamente. Este trabalho propÃµe um modelo computacional baseado na utilizaÃ§Ã£o do comportamento emergente de uma comunidade de agentes de software com mecanismos de decisÃ£o cognitiva, baseados em redes neurais, com o objetivo de realizar previsÃµes do comportamento do Ãndice da Bolsa de Valores de SÃ£o Paulo # Ãndice Bovespa. Os agentes da comunidade interagem com um mecanismo de coleta dos valores das 65 aÃ§Ãµes mais negociadas da Bovespa que compÃµem o Ãndice Bovespa, utilizados para computar as previsÃµes de evoluÃ§Ã£o do Ã­ndice. A fim de validar a hipÃ³tese da pesquisa, Ã© feita uma comparaÃ§Ã£o entre os resultados obtidos pelo modelo implementado, com uma abordagem tradicional de previsÃ£o baseada exclusivamente em redes neurais. Os resultados obtidos demonstram que as previsÃµes do modelo proposto sÃ£o mais performÃ¡ticas do que a previsÃ£o isolada do Ã­ndice baseada exclusivamente em redes neurais, jÃ¡ que o modelo proposto captura melhor a microestrutura do mercado, prevendo um passo a frente de maneira mais eficaz.In finance world , the efficient-market hypothesis (EMH) states that the financial assets have random price fluctuation, so, there are not ways to protect the investor forecasting the future market movements. However, several initiatives have shown that empirical assertion of EMH is not entirely correct. Among the research fronts that seek to predict the movements of financial assets, we can highlight the area with an econometric point of view, which attempts to predict movements through mathematical and statistical methods such as linear regression and nonlinear regression, as well as neural networks. Moreover, in another area of research, we have the Theory of Multi-Agent Market Modeling that focuses attention on the micro-structure of the market, assuming that the price movements emerge from the interaction of many actors in the market. However, these financial models based on agents have some limitations. It is not possible to match agents to actual market data to forecast a step ahead, as in most models of decision making is done through ad-hoc functions or mechanisms that can not be adjusted to external data. For a preview of the real model, you need to adapt the decision-making mechanism of the agents with econometric models such as neural networks, which can be adjusted to actual data sets. Thus, through the interaction of the agents, the market model results can capture the hidden dynamics of the market and predict future movements with greater effectiveness than would a system of non-linear regression alone. This paper proposes a computational model based on the use of emergent behavior of a community of software agents with cognitive decision-making mechanisms base on neural networks in order to make predictions of the behavior of the index of He Stock Exchange of SÃ£o Paulo # Bovespa Index. The agents of the community interact with one engine collects the values of the 65 most actively traded stocks that comprise the Bovespa Bovespa Index, used to compute the projected trend of the index. In order to validate the hypothesis of the study, a comparison is made between the results obtained by the model implemented with a traditional approach to forecasting based solely on neural networks. The results show that by the proposed model are more performing than the forecast index alone based solely on neural networks, since the proposed model better captures the microstructure of the market, and one step ahead more effectively",finance,231,unknown
,to_check,core,HAL CCSD,2003-09-01 00:00:00,core,two expert diagnosis systems for smes: from database- only technologies to the unavoidable addition of ai techniques,,"International audienceIn this application-oriented paper, we describe two expert diagnosis systems we have developed for SMEs. Both systems are fully implemented and operational, and both have been put to use on data from actual SMEs. Although both systems are packed with knowledge and expertise on SMEs, neither has been implemented with AI techniques. We explain why and how both systems relate to knowledge-based and expert systems. We identify aspects of both systems that will benefit from the addition of AI techniques in future developments. 1. Expertise for Small and Medium-sized Enterprises (SMEs) The work we describe here takes place within the context of the Research Institute for SMEs 1. The specific lab in which we have conducted the research projects we refer to in this paper is the LaRePE 2 (LAboratoire de REcherche sur la Performance des Entreprises). This lab is mainly concerned with the development of scientific expertise on the study and modeling of SMEs' performance, including a variety of interrelated subjects such as finance, management , information systems, production, technology, etc. The vast majority of research projects carried out at the LaRePE involves both theoretical and practical aspects, often necessitating in-field studies with SMEs. As a result, our research projects always attempt to provide practical solutions to real problems confronting SMEs. In this application-oriented paper we briefly describe two expert diagnosis systems we have developed for SMEs. Both can be considered as decision support systems (Turban and Aronson, 2001). The first is the PDG system: a benchmarking software that evaluates production and management activities, and the results of these activities in terms of productivity, profitability, vulnerability and efficiency. The second is the eRisC system: a software that helps identify, measure and manage the main risk factors that could compromise the success of SME development projects. Both systems are fully implemented and operational. Moreover, both have been put to use on data from actual SMEs. What is of particular interest here, especially from a knowledge-based systems perspective, is the fact that although both the PDG and the eRisC systems are packed with knowledge and 1 The core mission of the Institute (http://www.uqtr.ca/inrpme/anglais/index.html), which supports fundamental and applied research, is to foster the advancement of knowledge on SMEs to contribute to their development. ",finance,232,unknown
,to_check,core,DigitalCommons@USU,2021-08-09 00:00:00,core,an ai-based goal-oriented agent for advanced on-board automation,https://core.ac.uk/download/478906302.pdf,"In the context of fierce competition arising in the space economy, the number of satellites and constellations that will be placed in orbit is set to increase considerably in the upcoming years. In such a dynamic environment, raising the autonomy level of the next space missions is key to maintaining a competitive edge in terms of the scientific, technological, and commercial outcome.
We propose the adoption of an AI-based autonomous agent aiming to fully enable spacecraftâs goal-oriented autonomy. The implemented cognitive architecture collects input starting from the sensing of the surrounding operating environment and defines a low-level schedule of tasks that will be carried out throughout the specified horizon. Furthermore, the agent provides a planner module designed to find optimal solutions that maximize the outcome of the pursued objective goal. The autonomous loop is closed by comparing the expected outcome of these scheduled tasks against the real environment measurements.
The entire algorithmic pipeline was tested in a simulated operational environment, specifically developed for replicating inputs and resources relative to Earth Observation missions. The autonomous reasoning agent was evaluated against the classical, non-autonomous, mission control approach, considering both the quantity and the quality of collected observation data in addition to the quantity of the observation opportunities exploited throughout the simulation time. The preliminary simulation results point out that the adoption of our software agent enhances dramatically the effectiveness of the entire mission, increasing and optimizing in-orbit activities, on the one hand, reducing events\u27 response latency (opportunities, failures, malfunctioning, etc.) on the other.
In the presentation, we will cover the description of the high-level algorithmic structure of the proposed goal-oriented reasoning model, as well as a brief explanation of each internal moduleâs contribution to the overall agentâs architecture. Besides, an overview of the parameters processed as input and the expected algorithms\u27 output will be provided, to contextualize the placement of the proposed solution. Finally, an Earth Observation use case will be used as the benchmark to test the performances of the proposed approach against the classical one, highlighting promising conclusions regarding our autonomous agentâs adoption",space,233,unknown
,to_check,core,æ³æ¿å¤§å­¦å¤§å­¦é¢ãã¶ã¤ã³å·¥å­¦ç ç©¶ç§,2021-03-24 00:00:00,core,development of an object position and attitude sharing networked ar game system using omni directional laser scanner,https://core.ac.uk/download/426984051.pdf,"In recent years, the development environment for mobile applications using Augmented Reality (AR) technology, which superimposes virtual objects on real space, has improved dramatically, enabling the implementation of advanced applications by combining image processing using artificial intelligence (AI) and space sharing using network communication. In particular, the technology for simultaneous presentation of virtual objects to multiple users through space sharing is important for applications such as product demonstrations, design meetings, surgical support, or competitive games. However, such space-sharing technology alone can share virtual objects in the same space, but it cannot share remote objects as virtual objects with each other.In this research, as a proposal for the use of technology to mutually share remote objects as virtual objects, I will develop an AR game system that can communicate and compete with each other in the same way as real competitive games even in remote areas, by using a 360-degree omni-directional laser scanner to acquire the position and posture of a real radio-controlled car in each user\u27s space, and using network communication technology to mutually share this information and reproduce it as a virtual object in real time",space,234,unknown
,to_check,core,Heterogeneous CPU+GPU Stochastic Gradient Descent Algorithms,2020-04-19 00:00:00,core,http://arxiv.org/abs/2004.08771,,"The widely-adopted practice is to train deep learning models with specialized
hardware accelerators, e.g., GPUs or TPUs, due to their superior performance on
linear algebra operations. However, this strategy does not employ effectively
the extensive CPU and memory resources -- which are used only for
preprocessing, data transfer, and scheduling -- available by default on the
accelerated servers. In this paper, we study training algorithms for deep
learning on heterogeneous CPU+GPU architectures. Our two-fold objective --
maximize convergence rate and resource utilization simultaneously -- makes the
problem challenging. In order to allow for a principled exploration of the
design space, we first introduce a generic deep learning framework that
exploits the difference in computational power and memory hierarchy between CPU
and GPU through asynchronous message passing. Based on insights gained through
experimentation with the framework, we design two heterogeneous asynchronous
stochastic gradient descent (SGD) algorithms. The first algorithm -- CPU+GPU
Hogbatch -- combines small batches on CPU with large batches on GPU in order to
maximize the utilization of both resources. However, this generates an
unbalanced model update distribution which hinders the statistical convergence.
The second algorithm -- Adaptive Hogbatch -- assigns batches with continuously
evolving size based on the relative speed of CPU and GPU. This balances the
model updates ratio at the expense of a customizable decrease in utilization.
We show that the implementation of these algorithms in the proposed CPU+GPU
framework achieves both faster convergence and higher resource utilization than
TensorFlow on several real datasets and on two computing architectures -- an
on-premises server and a cloud instance",space,235,unknown
,to_check,core,HSSMATCH: um modelo hÃ­brido para semantic schema matching em arquiteturas orientadas a microsserviÃ§os,2020-02-06 00:00:00,core,https://core.ac.uk/download/389140546.pdf,Engenharia ElÃ©trica,"Os ecossistemas de software da atualidade possuem estilo arquitetural de

microsserviÃ§os e caracterÃ­sticas especÃ­ficas, sistemas e dados distribuÃ­dos em

diferentes fontes, o que dificulta o gerenciamento de dados. Como os modelos

conceituais do mundo real sÃ£o diferentes entre os sistemas, hÃ¡ problemas para

integrar os dados e realizar a comunicaÃ§Ã£o entre esses microsserviÃ§os, o que

implica na necessidade de matching entre os esquemas e mensagens. A

literatura evidencia problemas de matching como o tamanho do espaÃ§o de

busca, a heterogeneidade semÃ¢ntica dos dados, e as atualizaÃ§Ãµes pelas quais

os esquemas passam constantemente, e mostra como lacunas a inadequaÃ§Ã£o

das interfaces de usuÃ¡rios, a acomodaÃ§Ã£o de alteraÃ§Ãµes nas estruturas de

dados, e, ainda, a escassez de abordagens para uso prÃ¡tico. Assim, o presente

estudo teve por objetivo Ã© apresentar um modelo hÃ­brido de semantic schema

matching para microsserviÃ§os com capacidade de identificar similaridades entre

os elementos de dois esquemas em larga escala, que suporte a atualizaÃ§Ã£o dos

esquemas e seus dados e, considere os resultados da validaÃ§Ã£o humana para

reuso. Para tanto, foi apresentada a arquitetura do modelo HSSMatch e

implementado o protÃ³tipo do HSSMatch System que permite ao usuÃ¡rio, por

meio de uma interface grÃ¡fica Web, gerenciar o processo de schema matching

de microsserviÃ§os. A avaliaÃ§Ã£o desse protÃ³tipo, no que se refere Ã  sua

adequaÃ§Ã£o de design de interaÃ§Ã£o, foi feita por meio de experimentos e

questionÃ¡rios aplicados a usuÃ¡rios que atuam na Ã¡rea de integraÃ§Ã£o de dados e

comunicaÃ§Ã£o entre sistemas de software. A avaliaÃ§Ã£o tambÃ©m foi realizada em

experimentos com dois datasets, e mostrou aspectos que confirmam a hipÃ³tese

deste estudo, pois verificou-se melhoria na eficiÃªncia e eficÃ¡cia do processo de

schema matching utilizando a abordagem hÃ­brida que acomoda alteraÃ§Ãµes nos

dados, reduz o espaÃ§o de busca e combina matchers em nÃ­vel de esquema e de

instÃ¢ncias. Como trabalhos futuros, podem ser explorados mÃ©todos

supervisionados de aprendizado de mÃ¡quina para configuraÃ§Ã£o semiautomÃ¡tica

de estratÃ©gias de schema matching, e ainda outras tÃ©cnicas de particionamento

de esquemas, ontologias de domÃ­nio em prol da melhoria da qualidade do

resultado de schema matching para domÃ­nios especÃ­ficos.Today's software ecosystems have an architectural style of microservices and

specific characteristics, systems and data distributed across different sources,

making data management difficult. Because real-world conceptual models differ

across systems, there are problems integrating data and carrying out

communication between these microservices, which implies the need for

matching between schemas and messages. The literature highlights matching

problems such as search space size, semantic heterogeneity of data, and

updates that schemas constantly go through, and shows as gaps the inadequacy

of the user interfaces, the accommodation of changes in data structures, and the

scarcity of approaches for practical use. Thus, the present study aimed to present

a hybrid model of semantic schema matching for microservices with the ability to

identify similarities between the elements of two large-scale schemes that

supports the updating of the schemes and their data and considers the results of

human validation for reuse. For this, the architecture of the HSSMatch model was

presented, and the prototype of the HSSMatch System was implemented,

allowing the user, through a web graphical interface, to manage the

microservices schema matching process. The evaluation of this prototype,

regarding its suitability of interaction design, was made through experiments and

questionnaires applied to users working in the area of data integration and

communication between software systems. The evaluation was also performed

in experiments with two datasets, and showed aspects that confirm the

hypothesis of this study, since there was an improvement in the efficiency and

effectiveness of the schema matching process using the hybrid approach that

accommodates data changes, reduces search space and combines matchers at

schema and instance levels. As future work, supervised machine learning

methods for semiautomatic configuration of schema matching strategies, as well

as other schema partitioning techniques, domain ontologies for improving the

quality of schema matching results for specific domains can be explored.Instituto Presbiteriano Mackenzi",space,236,unknown
,to_check,core,"CorGAN: Correlation-Capturing Convolutional Generative Adversarial
  Networks for Generating Synthetic Healthcare Records",2020-03-04 00:00:00,core,http://arxiv.org/abs/2001.09346,,"Deep learning models have demonstrated high-quality performance in areas such
as image classification and speech processing. However, creating a deep
learning model using electronic health record (EHR) data, requires addressing
particular privacy challenges that are unique to researchers in this domain.
This matter focuses attention on generating realistic synthetic data while
ensuring privacy. In this paper, we propose a novel framework called
correlation-capturing Generative Adversarial Network (CorGAN), to generate
synthetic healthcare records. In CorGAN we utilize Convolutional Neural
Networks to capture the correlations between adjacent medical features in the
data representation space by combining Convolutional Generative Adversarial
Networks and Convolutional Autoencoders. To demonstrate the model fidelity, we
show that CorGAN generates synthetic data with performance similar to that of
real data in various Machine Learning settings such as classification and
prediction. We also give a privacy assessment and report on statistical
analysis regarding realistic characteristics of the synthetic data. The
software of this work is open-source and is available at:
https://github.com/astorfi/cor-gan.Comment: Accepted to be published in the 33rd International FLAIRS Conference,
  AI in Healthcare Informatic",space,237,unknown
,to_check,core,Generation of refactoring algorithms through grammatical evolution,2020-01-01 00:00:00,core,https://core.ac.uk/download/346536233.pdf,,"Orientadora: Silvia Regina VergilioCoorientador: Marouane KessentiniTese (doutorado) - Universidade Federal do ParanÃ¡, Setor de CiÃªncias Exatas, Programa de PÃ³s-GraduaÃ§Ã£o em InformÃ¡tica. Defesa : Curitiba, 24/04/2020Inclui referÃªncias: p. 61-66Ãrea de concentraÃ§Ã£o: CiÃªncia da ComputaÃ§Ã£oResumo: A atividade de refatoraÃ§Ã£o tem como principal objetivo aplicar um conjunto de transformaÃ§Ãµes em um artefato de software para melhorar sua estrutura sem alterar sua funcionalidade. Alguns estudos recentes, apresentam bons resultados ao gerarem modelos de prediÃ§Ã£o de refatoraÃ§Ãµes. AlÃ©m disso, os estudos mostram que refatoraÃ§Ãµes similares sÃ£o aplicadas em diferentes contextos e podem ser aprendidas. Neste sentido, a maioria dos trabalhos existentes utiliza tÃ©cnicas de aprendizado de mÃ¡quina para gerar modelos que predizem se um dado trecho de cÃ³digo deve ser refatorado. Entretanto, essas abordagens possuem limitaÃ§Ãµes. Elas buscam por refatoraÃ§Ãµes especÃ­ficas e exatamente como aplicadas por desenvolvedores, o que limita que outras refatoraÃ§Ãµes sejam encontradas. Dada a natureza subjetiva da atividade de refatoraÃ§Ã£o de software, a exploraÃ§Ã£o por refatoraÃ§Ãµes com base em outros critÃ©rios tambÃ©m Ã© vantajosa. Existem trabalhos na Ã¡rea conhecida como RefatoraÃ§Ã£o de Software Baseada em Busca (SBR) (do inglÃªs, Search Based Software Refactoring), em que algoritmos de busca sÃ£o utilizados para encontrar refatoraÃ§Ãµes em um grande espaÃ§o de busca e visando a melhorar diversos aspectos. Recentemente, trabalhos em SBR comeÃ§aram a utilizar exemplos de refatoraÃ§Ãµes jÃ¡ aplicadas por desenvolvedores para incorporar aprendizado na busca. Entretanto, essas abordagens sÃ£o limitadas em termos de generalizaÃ§Ã£o dos resultados, uma vez que nÃ£o geram um modelo que possa ser utilizado para diferentes programas. Desse modo, abordagens existentes de SBR devem ser configuradas e executadas a cada novo programa. Neste contexto, este trabalho visa a incorporar os benefÃ­cios encontrados na Ã¡rea de aprendizado de mÃ¡quina e na Ã¡rea de SBR, apresentando uma abordagem chamada Gorgeous (do inglÃªs, Generation of Refactoring Algorithms through Grammatical Evolution). Gorgeous tem como objetivo gerar algoritmos de refatoraÃ§Ã£o compostos por regras, que quando executados, determinam trechos de cÃ³digo que devem ser refatorados e refatoraÃ§Ãµes a serem aplicadas. Os algoritmos sÃ£o criados de forma que as refatoraÃ§Ãµes sugeridas sejam similares a refatoraÃ§Ãµes aplicadas na prÃ¡tica e que tambÃ©m melhorem a qualidade do software. Os algoritmos sÃ£o criados utilizando um processo de aprendizado que primeiro extrai padrÃµes de refatoraÃ§Ã£o de programas agrupando elementos que foram refatorados de maneira similar. ApÃ³s isso, uma evoluÃ§Ã£o gramatical Ã© executada para gerar algoritmos de refatoraÃ§Ã£o com base nos padrÃµes extraÃ­dos. Gorgeous Ã© avaliada utilizando dados de refatoraÃ§Ã£o extraÃ­dos de 40 programas Java do repositÃ³rio GitHub. Como resultado, os algoritmos gerados foram capazes de obter bons resultados para diferentes programas, melhorando em mÃ©dia 60% a qualidade do programa e obtendo 50% de similaridade com refatoraÃ§Ãµes aplicadas na prÃ¡tica. Palavras-chave: RefatoraÃ§Ã£o, Engenharia de Software Baseada em Busca,Agrupamento, EvoluÃ§Ã£o GramaticalAbstract: The refactoring activity addresses the application of a set of transformations in software artifacts to improve their structure while preserving their functionality. Recent studies present promising results generating prediction models for refactoring. Furthermore, they provide evidences that similar refactoring operations are applied in different contexts and they can be learned using Machine Learning (ML). Most works on ML based refactoring generate models to predict if a piece of code should be refactored. Despite the capability of prediction, existing works are limited to learn specific refactoring operations as applied by developers. However, to explore refactoring operations possibilities based on other criteria is also beneficial, mainly by the subjective context of refactoring. In this context, the Search-Based Software Refactoring (SBR) area addresses studies using search algorithms to find refactoring operations in a huge search space, aiming at improving several other aspects. However, existing SBR approaches do not support generalization of results since they do not generate a model as ML studies. In this way, a SBR approach needs to be configured and executed for each program in need of refactoring. In this context, this work introduces a SBR learning approach aiming at taking most advantage of both fields. Gorgeous (Generation of Refactoring Algorithms through Grammatical Evolution) generates refactoring algorithms composed by several rules determining pieces of code that should be refactored and the refactoring types to be used. A refactoring algorithm provides as solution a set of refactoring operations to be applied in a program. In this respect, the algorithm is generated with the goal of increasing similarity of the refactoring operations with the ones applied in practice, and also improving program quality. To do this, a learning process first extracts refactoring patterns from programs by grouping their elements that were refactored in similar ways. After that, a Grammatical Evolution (GE) is executed to generate the algorithms based on the extracted patterns. Gorgeous is evaluated using refactoring data from 40 Java programs of GitHub repository. The refactoring algorithms are capable of obtaining good results to different programs, obtaining around 60% of program quality improvement and 50% of similarity with real refactoring applications. Keywords: Refactoring, Search-Based Software Engineering, Clustering, Grammatical Evolutio",space,238,unknown
,to_check,core,"Improving the Performance of Fine-Grain Image Classifiers via Generative
  Data Augmentation",2020-08-12 00:00:00,core,http://arxiv.org/abs/2008.05381,,"Recent advances in machine learning (ML) and computer vision tools have
enabled applications in a wide variety of arenas such as financial analytics,
medical diagnostics, and even within the Department of Defense. However, their
widespread implementation in real-world use cases poses several challenges: (1)
many applications are highly specialized, and hence operate in a \emph{sparse
data} domain; (2) ML tools are sensitive to their training sets and typically
require cumbersome, labor-intensive data collection and data labelling
processes; and (3) ML tools can be extremely ""black box,"" offering users little
to no insight into the decision-making process or how new data might affect
prediction performance. To address these challenges, we have designed and
developed Data Augmentation from Proficient Pre-Training of Robust Generative
Adversarial Networks (DAPPER GAN), an ML analytics support tool that
automatically generates novel views of training images in order to improve
downstream classifier performance. DAPPER GAN leverages high-fidelity
embeddings generated by a StyleGAN2 model (trained on the LSUN cars dataset) to
create novel imagery for previously unseen classes. We experimentally evaluate
this technique on the Stanford Cars dataset, demonstrating improved vehicle
make and model classification accuracy and reduced requirements for real data
using our GAN based data augmentation framework. The method's validity was
supported through an analysis of classifier performance on both augmented and
non-augmented datasets, achieving comparable or better accuracy with up to 30\%
less real data across visually similar classes. To support this method, we
developed a novel augmentation method that can manipulate semantically
meaningful dimensions (e.g., orientation) of the target object in the embedding
space",space,239,unknown
,to_check,core,Global Robustness Verification Networks,2020-06-08 00:00:00,core,http://arxiv.org/abs/2006.04403,,"The wide deployment of deep neural networks, though achieving great success
in many domains, has severe safety and reliability concerns. Existing
adversarial attack generation and automatic verification techniques cannot
formally verify whether a network is globally robust, i.e., the absence or not
of adversarial examples in the input space. To address this problem, we develop
a global robustness verification framework with three components: 1) a novel
rule-based ``back-propagation'' finding which input region is responsible for
the class assignment by logic reasoning; 2) a new network architecture Sliding
Door Network (SDN) enabling feasible rule-based ``back-propagation''; 3) a
region-based global robustness verification (RGRV) approach. Moreover, we
demonstrate the effectiveness of our approach on both synthetic and real
datasets",space,240,not included
,to_check,core,"eScholarship, University of California",2020-01-01 00:00:00,core,machine learning in compiler optimization,,"The end of Moore's law is driving the search for new techniques to improve system performance as applications continue to evolve rapidly and computing power demands continue to rise. One promising technique is to build more intelligent compilers.Compilers map high-level programs to lower-level primitives that run on hardware. During this process, compilers perform many complex optimizations to boost the performance of the generated code. These optimizations often require solving NP-Hard problems and dealing with an enormous search space. To overcome these challenges, compilers currently use hand-engineered heuristics that can achieve good but often far-from-optimal performance. Alternatively, software engineers resort to manually writing the optimizations for every section in the code, a burdensome process that requires prior experience and significantly increases the development time.In this thesis, novel approaches for automatically handling complex compiler optimization tasks are explored. End-to-end solutions using deep reinforcement learning and other machine learning algorithms are proposed. These solutions dramatically reduce the search time while capturing the code structure, different instructions, dependencies, and data structures to enable learning a sophisticated model that can better predict the actual performance cost and determine superior compiler optimizations. The proposed techniques can outperform existing state-of-the-art solutions while requiring shorter search time. Furthermore, unlike existing solutions, the deep reinforcement learning solutions are shown to generalize well to real benchmarks",space,241,unknown
,to_check,core,'Pisa University Press',2020-02-16 00:00:00,core,fpga extension of a battery management system for fail-operational control of lithium-ion batteries in safety-critical applications,,"La crescente attenzione riguardo alle tematiche ambientali sta portando sempre piÃ¹ allâattenzione i problemi legati allâinquinamento, specialmente per quanto riguarda lâemissione dei gas serra. Una delle principali cause dellâaumento di gas serra Ã¨ senza dubbio lâutilizzo dei combustibili fossili, anche nel campo della mobilitÃ . Per questo motivo, negli ultimi decenni, sono state cercate alternative piÃ¹ ecologiche: la tendenza attuale Ã¨ senza dubbio quella di muoversi in direzione della trazione elettrica, e in special modo a guida autonoma. Il lavoro oggetto di tesi Ã¨ appunto inquadrato allâinterno del progetto europeo AutoDrive, il quale mira alla progettazione di componenti elettronici e architetture di tipo Fail-Operational, ovvero che continuano a eseguire un insieme definito delle loro funzioni anche in presenza di guasti, che permettano lâintroduzione della guida autonoma in autoveicoli di tutte le categorie, con lâintento di contribuire a una mobilitÃ  piÃ¹ efficiente e sicura. Infatti, dato il crescente numero di implementazioni software e meccatroniche allâinterno delle automobili, sono presenti sempre piÃ¹ rischi dovuti a loro possibili guasti, sia sistematici che aleatori, che devono quindi essere tenuti in considerazione al fine della sicurezza funzionale. PoichÃ© questâultima venga garantita Ã¨ necessario introdurre allâinterno del sistema, in modo controllato, un qualche tipo di ridondanza che permetta di mascherare o individuare i guasti che si possono verificare.
La possibilitÃ  di realizzare veicoli a trazione elettrica Ã¨ strettamente legata ai progressi conseguiti nel campo delle batterie, specialmente per quanto riguarda le tecnologie basate sugli ioni di Litio. Infatti, questâultime presentano una maggiore densitÃ  di energia e di potenza, una tensione di cella piÃ¹ elevata, la mancanza di effetto memoria e una minore corrente di auto scarica se confrontate con le altre chimiche esistenti. Grazie a questa serie di vantaggi, le tecnologie basate sugli ioni di Litio, rappresentano lâunico vero candidato per il futuro della mobilitÃ  elettrica. Questa tecnologia, tuttavia, presenta degli svantaggi in quanto Ã¨ necessario lâinserimento di un sistema elettronico che monitori la batteria, il Battery Management System (BMS), il quale ha, tra gli altri, il compito di garantirne il corretto funzionamento in termini di range operativi di tensione, temperatura e corrente. Difatti, la fuoriuscita di queste grandezze dalla loro Safe Operating Area (SOA), oltre a portare un degradamento delle prestazioni della batteria, puÃ² provocare lâinnescarsi di condizioni ben piÃ¹ gravi quali fughe termiche interne alle celle o persino lâesplosione delle celle stesse. Qualora il BMS identifichi una situazione critica per lâoperativitÃ  della batteria, questo sistema interviene attraverso un sistema di feedback sul circuito, andando ad esempio a distaccare il carico. Il BMS misura costantemente tutte le grandezze fisiche delle celle che compongono la batteria, quali corrente, tensione e temperatura. A partire da queste, oltre a verificare se i dati sono allâinterno della loro SOA, questo sistema esegue degli algoritmi di stima dello stato della batteria andando a ricavare dei parametri tipici quali lo stato di carica (SoC) e lo stato di salute (SoH) e, qualora fosse richiesto, si occupa di loggare e comunicare queste informazioni agli altri blocchi che compongono il sistema.
Nel caso in cui si voglia realizzare un intero sistema avente una batteria agli ioni litio e che presenti un comportamento di tipo Fail-Operational, come quello in esame nel progetto Autodrive, anche il BMS deve avere questa caratteristica. Per questo motivo, partendo dunque dalla struttura convenzionale del BMS, sono state aggiunte strutture ridondanti al fine di raggiungere lâobbiettivo preposto. La ridondanza puÃ² essere classificata in due grandi categorie, spaziale e temporale: la prima involve lâintroduzione allâinterno del sistema di componenti, o funzioni, che sarebbero inutili in ambienti privi di guasti, mentre la seconda Ã¨ basata sulla ripetizione delle operazioni eseguite e il confronto con il risultato precedente. Specialmente in applicazioni safety-critical, quali quella automotive, la ridondanza di tipo spaziale Ã¨ necessaria al fine di raggiungere i massimi livelli di sicurezza stabiliti dagli standard, quali ISO 26262.
Per questo motivo la struttura convenzionale del BMS Ã¨ stata replicata e unâestensione del BMS stesso, che ricopre un ruolo decisionale, Ã¨ stata sviluppata su una piattaforma FPGA. La scelta di utilizzare un FPGA invece di un microcontrollore, porta numerosi vantaggi, tra i quali un incremento delle capacitÃ  computazionali e una maggiore flessibilitÃ  e riconfigurabilitÃ  del sistema sviluppato. Nel dettaglio Ã¨ stato scelto di modificare la struttura del BMS andando a sviluppare un sistema in triplice ridondanza nel quale, come suggerisce il nome, si ha una triplicazione parallela del sistema. I tre flussi di dati ottenuti vengono sottoposti a un sistema di voting a maggioranza, dal quale si ottiene un unico flusso di uscita, permettendo dunque di mascherare un guasto su uno dei tre ingressi. A questo scopo sono state sviluppate delle periferiche hardware su FPGA per eseguire le operazioni di voting sui dati e per stimare la regione di lavoro della batteria. Ã stata inoltre sviluppata una interfaccia grafica su PC, la quale permette di configurare opportunamente le periferiche implementate e mostra in tempo reale tutte le informazioni riguardo alla batteria, tra le quali i valori di tensione delle celle, le temperature e lo stato del sistema. Il sistema Ã¨ stato infine testato andando a triplicare virtualmente il BMS convenzionale a nostra disposizione e, manipolando i dati su ciascuna linea in modo controllato, Ã¨ stato verificato il corretto funzionamento delle periferiche sviluppate e validata lâarchitettura proposta.
#english version#
The growing awareness about the environmental issues is bringing to the attention the pollution topics, especially the greenhouse gas emission. One of the main causes is fossil fuel consumption for the energy production, even in the automotive systems. For this reason, during the last decades greener alternatives have been sought, and the actual trend is the one that leads to the electrical traction, especially towards the autonomous driving system. The thesis work is part of the European project AutoDrive, which aims at designing Fail-Operational electronic components and system architectures, that enables the introduction of automated driving in all car categories to make future mobility more efficient and safer. It is said that a system presents a Fail-Operational behaviour if it continues to execute a defined set of its function even in presence of faults. Given the increasing number of software and mechatronic implementations within the automotive systems, there are increasing risks from systematic failures and random hardware failures that must be considered within the scope of functional safety. Since this last one must be guaranteed, the introduction within the system of some kind of redundancy is mandatory in order to detect or mask the possible faults.
The possibility of developing electrical traction vehicles is closely related to the progress made in the battery field, especially with regards to the Lithium-ion (Li-ion) based technologies. In fact, these types of cells present a higher energy and power density, an higher cell voltage, no memory effect and a lower auto discharge current compared to the other chemistries. Thanks to these advantages, Li-ion based technologies are the only real candidate for the future electrical mobility. However, this chemistry also brings some disadvantages since a battery monitoring system, the Battery Management System (BMS), is mandatory. This system has to ensure the maintenance of the all cells composing the battery pack within their operative ranges in terms of voltages, temperatures and current. In fact, the coming out of one or more of these measures from its Safe Operating Area (SOA) brings a degradation of the cellâs performance, and could also lead to hazardous conditions, such us thermal runaway within the cell itself or even explosions. If the BMS identifies a critical condition for the battery functionality, it acts on the circuit though a feedback system, for example disconnecting the load. The BMS also uses the acquired measures in order to estimate typical battery parameters, such as State of Charge (SoC) and State of Health (SoH), and, if required, it also provides logging functionality and communicates to the other blocks composing the system.
In the case of a system containing a Li-ion battery with Fail-Operational behaviour has to be developed, such as the project AutoDrive one, even the BMS must show this characteristic. For this reason, starting from a conventional BMS, redundant structures have been added to the system in order to reach the responsible goal.
Redundancy can be classified into two main categories: space redundancy and time redundancy. The former involves the introduction within the system of components, or functions, that would be useless in a fault-free environment, while the latter is based on the repetition of the tasks and the comparison of the results to a stored copy of the previous ones. Especially in safety-critical applications, such as automotive, space redundancy is mandatory in order to ensure the safety level required by standards, such as ISO 26262. Therefore, the conventional structure of BMS has been replicated and an extension of BMS itself, which acts as decisional unit, has been developed on a FPGA platform. The FPGA approach, compared to a microcontroller-based one, brings several advantages, such as an increased computational capability and a higher flexibility and reconfigurability of the developed system. More specifically, the conventional structure of the BMS has been modified by using a Triple Modular Redundancy (TMR) approach. As the name suggests, TMR involves the triplication of the components to perform the same computation in parallel. The three obtained data flows are then subjected to a majority voting unit, which provides a single data flow as output, allowing to mask a fault in one of the inputs. For this purpose, hardware peripherals within the FPGA fabric have been developed in order to execute voting and operating area estimation algorithms. Furthermore, a PC graphical user interface has been developed and it allows to configure the hardware peripherals and to show real-time information about the battery pack, such as cell voltages, temperatures and current. The system has been finally tested by virtually triplicating the conventional BMS at our disposal and, manipulating each data flow in a controlled manner, the proper functioning of the developed peripherals has been verified and the proposed architecture has been validated",space,242,unknown
,to_check,core,,2020-02-09 00:00:00,core,solving a multi-attribute vehicle routing problem in the freight delivery industry,,"Freight transportation industry is characterized by several decisional problems that operations managers have to cope with. Not only the routes planning must be realized before their execution, but also other types of decisions must be taken, in order to answer events that may dynamically occur during operations, as for instance road network congestion or vehicle

failures. Each decision can involve different aspects: for instance, the price negotiation of a just-in-time order should take into consideration the current routes status and planning. Off-the-shelf decision support software, although able to independently support the decision makers in each area, tend to keep tasks compartmentalized.

Trans-Cel, a small trucking company in Padova (Italy), has a Research and Development branch developing a cloud-based platform, called Chainment, able to host different decision support tools that can communicate through a data sharing system. These tools rely on an algorithmic engine that includes a routing optimization algorithm and artificial intelligence systems. In particular, the routing problem combines express couriers requirements, generally studied in urban contexts, with routes and vehicle features typical of medium- and long-haul trips, showing interesting characteristics that are worth of study in the Operation Research field.

In this thesis, we focus on the design of an optimization algorithm able to provide a solution to a Vehicle Routing Problem (VRP) inspired by the Trans-Cel scenario, that we name Express Pickup and Delivery in freight Trucking problem (EPDT).

The classical VRP definition includes a set of customers and a fleet of vehicles and aims to define a set of routes such that all customers are visited exactly once while minimizing the overall distance traveled. In the scientific literature, the basic definition of the problem has been generalized in order to consider additional attributes, often rising from real-world scenarios, as for instance capacity of vehicles, time windows and orders with both pickup and delivery operations. Often, in real-world cases, decision makers must simultaneously deal with a large number of attributes, thus defining a class of routing problems called Multi-Attribute VRP (MAVRP), which includes EPDT.

The thesis proposes a meta-heuristic algorithm for the solution of EPDT, with the aim of embedding it in the algorithmic engine of Chainment. In order to comply with the platform  requirements, the algorithm is designed so that a solution is returned within few seconds.

The solution method we propose consists of a two-level heuristic: at the first level, a Tabu Search algorithm hybridized with a Variable Neighborhood Descent explores the order-to-vehicle assignments, while, at the second level, it makes use of a Local Search to determine the sequence of customers visited and obtain an evaluation of routes.

The algorithm efficiency is enhanced by the use of a granular exploration, by procedures for fast evaluation of solutions in the neighborhoods, and parallel implementation of specific algorithmic components. These elements are adapted to the specific attributes of EPDT and represent some of the thesis contributions. The improvement in computational times have been validated by the experimental results, verifying the desired requirements for the platform integration.

The quality of the solutions obtained with the proposed meta-heuristic algorithm has been assessed both on the field, by comparison with Trans-Cel operations managers, and through bounds obtained with mathematical programming methods. To this purpose, the thesis proposes an Integer Linear Programming formulation for EPDT and a solution method for its continuous

relaxation based on Column Generation. In particular, the thesis presents new pricing procedures suitable for the specific EPDT attributes. The available bounds show optimality or near-optimality of solutions provided by the heuristic algorithm for real instances. Moreover, the algorithm has been tested on literature benchmarks related to the Pickup and Delivery

Problem with Time Windows (PDPTW), providing solutions that are competitive with the state-of-the-art.

The thesis also proposes a preliminary study of new approaches for vehicle routing problems in dynamic contexts. In particular, the thesis explores the possibility of taking advantage from the availability of historical data on orders by means of anticipatory strategies. The first strategy is based on clustering methods that are applied to the orders to define space-time

points that aggregate the information on future demand. A second strategy is based on the concept of accessibility, as defined in the discrete choice theory and urban logistic, to represent the route capability of intercepting future orders.

The heuristic algorithm proposed for EPDT has been integrated in the algorithmic engine of the Chainment platform at Trans-Cel. The thesis describes integration and the adaptation of the proposed optimization algorithms for a proper interaction with the different modules in the operational context handled by the platform, as, for instance, initial routes planning, reacting to dynamic events or order price negotiation",space,243,unknown
,to_check,core,,2020-03-27 00:00:00,core,Î¼Î¬Î¸Î·ÏÎ· Î¼Î­ÏÏ ÏÎ±ÏÎ±ÏÎ®ÏÎ·ÏÎ·Ï Î³Î¹Î± ÏÎ·Î½ ÎµÏÎ¯ÏÎµÏÎ¾Î· ÏÎ¿Î¼ÏÎ¿ÏÎ¹ÎºÏÎ½ Î´ÏÎ¬ÏÎµÏÎ½ ÏÎµÎ¹ÏÎ¹ÏÎ¼Î¿Ï,,"Î ÏÎ±ÏÎ¿ÏÏÎ± Î´Î¹Î´Î±ÎºÏÎ¿ÏÎ¹ÎºÎ® Î´Î¹Î±ÏÏÎ¹Î²Î® Î±ÏÎ¿ÏÎ¬ ÏÎ· Î¼ÎµÎ»Î­ÏÎ·, Î±Î½Î¬ÏÏÏÎ¾Î· ÎºÎ±Î¹ ÎµÏÎ±ÏÎ¼Î¿Î³Î®, Î¼ÎµÎ¸ÏÎ´ÏÎ½
ÎÎ·ÏÎ±Î½Î¹ÎºÎ®Ï ÎÎ¬Î¸Î·ÏÎ·Ï Î¼Î­ÏÏ Î Î±ÏÎ±ÏÎ®ÏÎ·ÏÎ·Ï (Learning from Demonstration) Î¼Îµ ÏÏÏÏÎ¿ ÏÎ·Î½
ÏÎ¿Î¼ÏÎ¿ÏÎ¹ÎºÎ® Î±Î½Î±ÏÎ±ÏÎ±Î³ÏÎ³Î® Î´ÏÎ¬ÏÎµÏÎ½ ÏÎµÎ¹ÏÎ¹ÏÎ¼Î¿Ï. Î Î¼ÎµÎ¸Î¿Î´Î¿Î»Î¿Î³Î¯Î± Î±ÏÏÎ® ÏÏÎ·ÏÎ¯Î¶ÎµÏÎ±Î¹ ÏÏÎ·Î½
Î´Î·Î¼Î¹Î¿ÏÏÎ³Î¯Î± Î¼Î¹Î±Ï Î±Î½ÏÎ¹ÏÏÎ¿Î¯ÏÎ¹ÏÎ·Ï (mapping) Î¼ÎµÏÎ±Î¾Ï ÏÎ·Ï ÎºÎ¹Î½Î·Î¼Î±ÏÎ¹ÎºÎ®Ï ÏÎ¿Ï Î±Î½Î¸ÏÏÏÎ¹Î½Î¿Ï ÏÎµÏÎ¹Î¿Ï ÎºÎ±Î¹
ÎµÎ½ÏÏ ÏÎ¿Î¼ÏÎ¿ÏÎ¹ÎºÎ¿Ï Î²ÏÎ±ÏÎ¯Î¿Î½Î±, Î® ÏÎ¹Î¿ ÏÏÎ³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î± Î¼ÎµÏÎ±Î¾Ï ÏÎ¿Ï ÏÎ¿Î»ÏÎ´Î¹Î¬ÏÏÎ±ÏÎ¿Ï ÏÏÏÎ¿Ï ÏÏÎ½
ÎºÎ¹Î½Î®ÏÎµÏÎ½ ÏÎ¿Ï Î±Î½Î¸ÏÏÏÎ¿Ï (human actor) Î¼Îµ ÏÎ¿Î½ ÎµÏÎ¯ÏÎ·Ï ÏÎ¿Î»ÏÎ´Î¹Î¬ÏÏÎ±ÏÎ¿ ÏÏÏÎ¿ Î´ÏÎ¬ÏÎ·Ï ÏÎ¿Ï
ÏÎ¿Î¼ÏÏÏ. Î ÏÏÏÏÎ­ÏÎ¹ÏÎ· ÏÏÎ½ Î±Î½Î¸ÏÏÏÎ¹Î½ÏÎ½ ÎµÎ½ÎµÏÎ³ÎµÎ¹ÏÎ½ Î¼Îµ Î±Î½ÏÎ¯ÏÏÎ¿Î¹ÏÎµÏ ÏÎ¿Î¼ÏÎ¿ÏÎ¹ÎºÎ­Ï, ÎµÏÎ¹ÏÏÎ³ÏÎ¬Î½ÎµÏÎ±Î¹
Î¼Î­ÏÏ Î¼Î¹Î±Ï Î¬Î´Î·Î»Î·Ï Î±Î½Î±ÏÎ±ÏÎ¬ÏÏÎ±ÏÎ·Ï, ÏÎ¿Ï Î¿Î½Î¿Î¼Î¬Î¶ÎµÏÎ±Î¹ Î»Î±Î½Î¸Î¬Î½Î¿ÏÏÎ± Î±ÏÎµÎ¹ÎºÏÎ½Î¹ÏÎ· ÏÏÏÎ¿Ï (latent
space). Î Î¹Î¿ ÏÏÎ³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î±, Î¼ÎµÎ»ÎµÏÎ¬Î¼Îµ ÏÎ·Î½ Î±Î¼Î¿Î¹Î²Î±Î¯Î± Î±Î»Î»Î·Î»ÎµÏÎ¯Î´ÏÎ±ÏÎ· ÏÎ·Ï Î±Î½ÏÎ¯Î»Î·ÏÎ·Ï ÎºÎ±Î¹ ÏÎ·Ï 
Î´ÏÎ¬ÏÎ·Ï, ÏÏÎ¿ÎºÎµÎ¹Î¼Î­Î½Î¿Ï Î½Î± Î´Î¹Î´Î¬Î¾Î¿ÏÎ¼Îµ ÏÎ± ÏÎ¿Î¼ÏÏÏ Î¼Î¹Î± ÏÎ¿Î¹ÎºÎ¹Î»Î¯Î± Î±ÏÏ Î½Î­ÎµÏ ÎºÎ¹Î½Î®ÏÎµÎ¹Ï ÏÎµÎ¹ÏÏÏ. Î©Ï ÎµÎº
ÏÎ¿ÏÏÎ¿Ï, ÏÎ»Î¿ÏÎ¿Î¹Î®Î¸Î·ÎºÎµ Î­Î½Î± Î¼ÎµÎ¸Î¿Î´Î¿Î»Î¿Î³Î¹ÎºÏ ÏÎ»Î±Î¯ÏÎ¹Î¿ Î¼Î¬Î¸Î·ÏÎ·Ï Î¼Î­ÏÏ ÏÎ±ÏÎ±ÏÎ®ÏÎ·ÏÎ·Ï, ÏÎ¿ Î¿ÏÎ¿Î¯Î¿
Î¿Î½Î¿Î¼Î¬Î¶ÎµÏÎ±Î¹ IMFO (Imitation Framework by Observation), ÏÎ¿Ï Î´Î¹ÎµÏÎºÎ¿Î»ÏÎ½ÎµÎ¹ ÏÎ·Î½ Î±Î½Î±ÏÎ±ÏÎ±Î³ÏÎ³Î®
Î¼Î±Î¸Î·Î¼Î­Î½ÏÎ½ ÎºÎ±Î¹ Î½Î­ÏÎ½ ÎºÎ¹Î½Î®ÏÎµÏÎ½ ÏÎµÎ¹ÏÎ¹ÏÎ¼Î¿Ï Î±ÏÏ Î­Î½Î± ÏÎ¿Î¼ÏÏÏ (manipulation tasks) ÎºÎ±Î¹,
ÏÎ±ÏÎ¬Î»Î»Î·Î»Î±, Î­ÏÎµÎ¹ ÎµÏÏÎµÎ¯Î± ÎµÏÎ±ÏÎ¼Î¿Î³Î® ÏÎµ ÏÎµÎ½Î¬ÏÎ¹Î± Î±Î»Î»Î·Î»ÎµÏÎ¯Î´ÏÎ±ÏÎ·Ï Î±Î½Î¸ÏÏÏÎ¿Ï-ÏÎ¿Î¼ÏÏÏ (HRI) ÏÎµ
ÎºÎ±Î¸Î·Î¼ÎµÏÎ¹Î½Î¬ ÏÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½ÏÎ±.
ÎÏÎ¹ÏÎ»Î­Î¿Î½, ÏÎµ Î±ÏÏÎ® ÏÎ· Î´Î¹Î±ÏÏÎ¹Î²Î®, ÎµÎ¾ÎµÏÎ¬Î¶Î¿ÏÎ¼Îµ ÏÎ¿ ÏÏÎ»Î¿ ÏÎ·Ï ÏÏÎ¿Î½Î¹ÎºÎ®Ï Î´Î¹Î¬ÏÎºÎµÎ¹Î±Ï ÎµÎºÏÎ­Î»ÎµÏÎ·Ï Î¼Î¹Î±Ï
ÎºÎ¯Î½Î·ÏÎ·Ï Î¼Î­ÏÎ± Î±ÏÏ ÏÎ· Î´Î¹Î±Î´Î¹ÎºÎ±ÏÎ¯Î± Î¼Î¬Î¸Î·ÏÎ·Ï Î±ÏÏ ÏÎ±ÏÎ±ÏÎ®ÏÎ·ÏÎ·, ÎµÎ½Î¹ÏÏÏÎ¿Î½ÏÎ±Ï ÏÎ¿ Î´Î¹Î±Î¼Î¿ÏÏÏÎ¼Î­Î½Î¿
ÏÎ»Î±Î¯ÏÎ¹Î¿ IMFO Î¼Îµ ÏÎ·Î½ Î´ÏÎ½Î±ÏÏÏÎ·ÏÎ± Î±Î½Î±ÏÎ±ÏÎ¬ÏÏÎ±ÏÎ·Ï ÎºÎ±Î¹ Î±Î½Î±ÏÎ±ÏÎ±Î³ÏÎ³Î®Ï ÏÏÏÎ¿ ÏÏÎ½ ÏÏÏÎ¹ÎºÏÎ½ ÏÏÎ¿
ÎºÎ±Î¹ ÏÏÎ½ ÏÏÎ¿Î½Î¹ÎºÏÎ½ ÏÎ±ÏÎ±ÎºÏÎ·ÏÎ¹ÏÏÎ¹ÎºÏÎ½ ÏÏÎ½ Î±Î½Î¸ÏÏÏÎ¹Î½ÏÎ½ ÎºÎ¹Î½Î®ÏÎµÏÎ½. Î£Îµ Î±Î½ÏÎ¯Î¸ÎµÏÎ· Î¼Îµ Î¬Î»Î»ÎµÏ
Î¼ÎµÎ¸ÏÎ´Î¿ÏÏ Î¼Î¬Î¸Î·ÏÎ·Ï Î¼Î­ÏÏ ÏÎ±ÏÎ±ÏÎ®ÏÎ·ÏÎ·Ï (LfD) ÏÎ¿Ï ÏÎµÏÎ¹Î³ÏÎ¬ÏÎ¿ÏÎ½ ÏÎ·Î½ ÎµÎºÏÎµÎ»Î¿ÏÎ¼ÎµÎ½Î· Î´ÏÎ¬ÏÎ· Î¼ÏÎ½Î¿
Î¼Îµ Î²Î¬ÏÎ· ÏÎ± ÏÏÏÎ¹ÎºÎ¬ ÏÎ±ÏÎ±ÎºÏÎ·ÏÎ¹ÏÏÎ¹ÎºÎ¬ ÏÎ·Ï, Î· ÏÏÎ¿ÏÎµÎ¹Î½ÏÎ¼ÎµÎ½Î· Î¼ÎµÎ¸Î¿Î´Î¿Î»Î¿Î³Î¯Î± ÎµÎ½Î¹ÏÏÏÎµÎ¹ ÏÎ·Î½
Î±Î½Î±ÏÎ±ÏÎ±Î³ÏÎ³Î® ÏÏÎ½ ÏÏÏÎ¿ÏÏÎ¿Î½Î¹ÎºÏÎ½ ÏÏÏÏÏÎ½ Î¼Î¹Î±Ï ÎºÎ¯Î½Î·ÏÎ·Ï ÎµÏÎ¹ÏÏÎ­ÏÎ¿Î½ÏÎ±Ï ÏÎ·Î½ Î±ÏÎ¿ÏÎµÎ»ÎµÏÎ¼Î±ÏÎ¹ÎºÎ®
ÎµÏÎ±ÏÎ¼Î¿Î³Î® ÏÎ·Ï ÏÎµ ÏÎ¹Î¿ ÏÏÎ½Î¸ÎµÏÎ± ÏÎµÎ½Î¬ÏÎ¹Î± HRI, ÏÏÎ¿Ï Î· ÏÏÎ¿Î½Î¹ÎºÎ® Î±Î»Î»Î·Î»Î¿ÏÏÎ¯Î± ÏÏÎ½ Î´ÏÎ¬ÏÎµÏÎ½ ÎµÎ¯Î½Î±Î¹
ÏÎ·Î¼Î±Î½ÏÎ¹ÎºÎ®. ÎÏÎ¹ÏÏÏÏÎ¸ÎµÏÎ±, ÎµÎ¹ÏÎ¬Î³ÎµÏÎ±Î¹ Î­Î½Î± ÏÏÎ½Î¿Î»Î¿ ÎºÎ±Î»Î¬ ÎºÎ±Î¸Î¿ÏÎ¹ÏÎ¼Î­Î½ÏÎ½ Î¼ÎµÏÏÎ¹ÎºÏÎ½ Î±Î¾Î¹Î¿Î»ÏÎ³Î·ÏÎ·Ï
(evaluation metrics) Î³Î¹Î± Î½Î± Î±ÏÎ¿ÏÎ¹Î¼Î·Î¸ÎµÎ¯ Î· ÎµÎ³ÎºÏÏÏÏÎ·ÏÎ± ÏÎ·Ï ÏÏÎ¿ÏÎµÎ¹Î½ÏÎ¼ÎµÎ½Î·Ï ÏÏÎ¿ÏÎ­Î³Î³Î¹ÏÎ·Ï
Î»Î±Î¼Î²Î¬Î½Î¿Î½ÏÎ±Ï ÏÏÏÏÎ· ÏÎ· ÏÏÎ¿Î½Î¹ÎºÎ® ÎºÎ±Î¹ ÏÏÏÎ¹ÎºÎ® ÏÏÎ½Î­ÏÎµÎ¹Î± ÏÏÎ½ Î±Î½Î±ÏÎ±ÏÎ±Î³ÏÎ¼ÎµÎ½ÏÎ½ ÏÏÎ¼ÏÎµÏÎ¹ÏÎ¿ÏÏÎ½.
ÎÎ¹Î± Î±Î¾Î¹Î¿ÏÎ·Î¼ÎµÎ¯ÏÏÎ· ÎµÏÎ­ÎºÏÎ±ÏÎ· ÏÎ¿Ï ÏÏÎ¿Î±Î½Î±ÏÎµÏÎ¸Î­Î½ÏÎ¿Ï ÏÎ»Î±Î¹ÏÎ¯Î¿Ï Î±Î½Î±ÏÎ­ÏÎµÏÎ±Î¹ ÏÏÎ·Î½ ÎµÎºÎ¼Î¬Î¸Î·ÏÎ·
ÏÎ·Ï Î´ÏÎ½Î±Î¼Î·Ï ÏÎ¿Ï ÎµÏÎ¹Î²Î¬Î»Î»ÎµÏÎ±Î¹ Î±ÏÏ ÏÎ¿Î½ ÏÏÎ®ÏÏÎ· Î³Î¹Î± ÏÎ·Î½ ÎµÏÎ¹ÏÏÏÎ·Î¼Î­Î½Î· ÎµÎºÏÎ­Î»ÎµÏÎ· Î»ÎµÏÏÏÎ½
ÏÎµÎ¹ÏÎ¹ÏÎ¼ÏÎ½. ÎÏÏÎ® Î· Î´Î¹Î±Î´Î¹ÎºÎ±ÏÎ¯Î± ÏÎ±ÏÎ¿ÏÏÎ¹Î¬Î¶ÎµÏÎ±Î¹ ÎµÏÎ¯ÏÎ·Ï ÏÏÎ·Î½ ÏÎ±ÏÎ¿ÏÏÎ± Î´Î¹Î±ÏÏÎ¹Î²Î® Î¼Î­ÏÏ ÎµÎ½ÏÏ
Î½Î­Î¿Ï ÏÎ»Î±Î¹ÏÎ¯Î¿Ï ÎµÏÎ¿ÏÏÎµÏÏÎ¼ÎµÎ½Î·Ï Î¼Î¬Î¸Î·ÏÎ·Ï, ÏÎ¿ Î¿ÏÎ¿Î¯Î¿ Î¿Î½Î¿Î¼Î¬Î¶ÎµÏÎ±Î¹ SLF (Supervised Learning
scheme for Force-based manipulation). Î¤Î¿ SLF Î´Î¹Î±ÏÏÏÏÎ½ÎµÏÎ±Î¹ ÏÏ Î¼Î¯Î± Î´Î¹Î±Î´Î¹ÎºÎ±ÏÎ¯Î± ÏÏÎ¹ÏÎ½
ÏÏÎ±Î´Î¯ÏÎ½: (Î±) ÎµÏÎ¹Î²Î»ÎµÏÏÎ¼ÎµÎ½Î· Î´Î¹Î±Î´Î¹ÎºÎ±ÏÎ¯Î± ÎµÎºÏÎ­Î»ÎµÏÎ·Ï ÎºÎ¹Î½Î®ÏÎµÏÎ½ ÏÎµÎ¹ÏÎ¹ÏÎ¼Î¿Ï ÏÎµ ÏÏÎ¿ÏÎ¿Î¼Î¿Î¯ÏÏÎ· Î³Î¹Î±
ÏÎ·Î½ Î±ÏÏÎºÏÎ·ÏÎ· ÎµÏÎ±ÏÎºÏÎ½ Î´ÎµÎ´Î¿Î¼Î­Î½ÏÎ½, (Î²) Î´Î¹Î±Î´Î¹ÎºÎ±ÏÎ¯Î± ÎµÎºÏÎ±Î¯Î´ÎµÏÏÎ·Ï (training) Î³Î¹Î± ÏÎ·
Î´Î¹ÎµÏÎºÏÎ»ÏÎ½ÏÎ· ÏÎ·Ï Î¼Î¬Î¸Î·ÏÎ·Ï ÎºÎ¹Î½Î®ÏÎµÏÎ½ ÏÎµÎ¹ÏÎ¹ÏÎ¼Î¿Ï Î¼Îµ ÏÎ·Î½ ÎºÎ±ÏÎ¬Î»Î»Î·Î»Î· ÏÏÎ¿ÏÎ±ÏÎ¼Î¿Î³Î® ÏÎ¿Ï ÎºÎ±ÏÏÎ¿Ï
ÎºÎ±Î¹ ÏÎ·Ï Î´ÏÎ½Î±Î¼Î· ÏÎ¹Î±ÏÎ¯Î¼Î±ÏÎ¿Ï ÎºÎ±Î¹ Î¼ÎµÏÎ±ÏÎ¿ÏÎ¬Ï ÎºÎ±Î¹ (Î³) ÎµÎºÏÎ­Î»ÎµÏÎ· ÏÎ·Ï ÎºÎ¯Î½Î·ÏÎ·Ï Î±ÏÏ ÏÎ¿Î¼ÏÎ¿ÏÎ¹ÎºÏ
Î²ÏÎ±ÏÎ¯Î¿Î½Î± ÏÎµ ÏÏÎ¿ÏÎ¿Î¼Î¿Î¯ÏÏÎ·. Î£ÏÎ· ÏÏÎ½Î­ÏÎµÎ¹Î±, Î¼Îµ ÏÎ· ÏÏÎ®ÏÎ· ÏÎ·Ï Î¼ÎµÎ¸ÏÎ´Î¿Ï sim-to-real transfer,
ÎµÏÎ¹ÏÏÎ³ÏÎ¬Î½ÎµÏÎ±Î¹ Î±Î½Î±ÏÎ±ÏÎ±Î³ÏÎ³Î® ÏÏÎ½ Î¼Î±Î¸Î·Î¼Î­Î½ÏÎ½ Î´ÏÎ¬ÏÎµÏÎ½ ÏÎµ ÏÏÎ±Î³Î¼Î±ÏÎ¹ÎºÎ¬ ÏÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½ÏÎ±
Î³ÎµÎ½Î¹ÎºÎµÏÎ¿Î½ÏÎ±Ï ÏÎ·Î½ ÎµÏÎ±ÏÎ¼Î¿Î³Î® ÏÎ¿Ï ÏÎ»Î±Î¹ÏÎ¯Î¿Ï Î¼Î¬Î¸Î·ÏÎ·Ï ÏÎµ ÎµÏÎ¹ÏÎ»Î­Î¿Î½ ÏÏÎ½Î¸Î®ÎºÎµÏ ÏÎµÎ¹ÏÎ¹ÏÎ¼Î¿Ï
ÎµÏÎ¸ÏÎ±ÏÏÏÏÎ½ Î±Î½ÏÎ¹ÎºÎµÎ¹Î¼Î­Î½ÏÎ½. Î¤Î± Î±ÏÎ¿ÏÎµÎ»Î­ÏÎ¼Î±ÏÎ± Î¼Îµ ÏÎ· ÏÏÎ®ÏÎ· ÏÎ¿Ï ÏÎ¿Î¼ÏÎ¿ÏÎ¹ÎºÎ¿Ï Î²ÏÎ±ÏÎ¯Î¿Î½Î± YuMi,
ÏÎµ ÏÎµÎ¹ÏÎ¬Î¼Î±ÏÎ± Î¼Îµ Î´Î¹Î±ÏÎ¿ÏÎµÏÎ¹ÎºÎ¬ Î±Î½ÏÎ¹ÎºÎµÎ¯Î¼ÎµÎ½Î± Î¼Îµ ÏÎ±ÏÏÎ¼Î¿Î¹Î¿ÏÏ ÏÏÎ½ÏÎµÎ»ÎµÏÏÎ­Ï ÏÏÎ¹Î²Î®Ï, ÎºÎ±Î¹
ÎµÎ½Î±Î»Î»Î±ÎºÏÎ¹ÎºÎ­Ï ÏÏÎ¶ÎµÏ ÏÎ¹Î±ÏÎ¯Î¼Î±ÏÎ¿Ï, Î±ÏÎ¿Î´ÎµÎ¹ÎºÎ½ÏÎ¿ÏÎ½ ÏÏÎ¹ ÏÎ¿ ÏÎ¿Î¼ÏÏÏ ÎµÎ¯Î½Î±Î¹ ÏÎµ Î¸Î­ÏÎ· Î½Î± Î±Î½Î±ÏÎ±ÏÎ¬Î³ÎµÎ¹
Î±ÏÎ¿ÏÎµÎ»ÎµÏÎ¼Î±ÏÎ¹ÎºÎ¬ Î±ÏÎ±Î¹ÏÎ·ÏÎ¹ÎºÎ­Ï ÎºÎ¹Î½Î®ÏÎµÎ¹Ï Î¼ÎµÏÎ±ÏÎ¿ÏÎ¬Ï ÎºÎ±Î¹ ÏÎµÎ¹ÏÎ¹ÏÎ¼Î¿Ï Î¼ÎµÏÎ¬ ÏÎ·Î½ Î¿Î»Î¿ÎºÎ»Î®ÏÏÏÎ· ÏÎ·Ï
Î´Î¹Î±Î´Î¹ÎºÎ±ÏÎ¯Î±Ï Î¼Î¬Î¸Î·ÏÎ·Ï.
Î£ÏÎ½Î¿ÏÏÎ¹ÎºÎ¬, Î· ÏÎ±ÏÎ¿ÏÏÎ± Î´Î¹Î±ÏÏÎ¹Î²Î® Î¼ÎµÎ»ÎµÏÎ¬ ÏÎ·Î½ Î´Î¹Î±Î´Î¹ÎºÎ±ÏÎ¯Î± Î¼Î¬Î¸Î·ÏÎ·Ï Î¼Î­ÏÏ ÏÎ±ÏÎ±ÏÎ®ÏÎ·ÏÎ·Ï
ÏÏÎ½ÎµÎ¹ÏÏÎ­ÏÎ¿Î½ÏÎ±Ï Î¼Îµ Î¼Î¹Î± Î½Î­Î± ÏÏÎ¿ÏÎ­Î³Î³Î¹ÏÎ· ÏÎ¿Ï ÎµÎ¹ÏÎ¬Î³ÎµÎ¹ ÏÎ·Î½ Î¼ÎµÎ»Î­ÏÎ· Î´ÏÎ¬ÏÎµÏÎ½ ÏÎµÎ¹ÏÎ¹ÏÎ¼Î¿Ï
Î±Î½ÏÎ¹ÎºÎµÎ¹Î¼Î­Î½ÏÎ½ Î¼Î­ÏÎ± Î±ÏÏ Î­Î½Î±Î½ ÏÏÏÎ¿ Î¼ÎµÎ¹ÏÎ¼Î­Î½ÏÎ½ Î´Î¹Î±ÏÏÎ¬ÏÎµÏÎ½, Î³Î¹Î± ÏÎ·Î½ ÎµÏÎºÎ¿Î»Î· ÎºÎ±Î¹ ÏÏÎ¼ÏÎ±Î³Î®
ÎºÏÎ´Î¹ÎºÎ¿ÏÎ¿Î¯Î·ÏÎ· ÏÏÎ½ ÎµÏÎ¹Î¼Î­ÏÎ¿ÏÏ ÏÎ±ÏÎ±ÎºÏÎ·ÏÎ¹ÏÏÎ¹ÎºÏÎ½ ÏÏÎ½ Î´ÏÎ¬ÏÎµÏÎ½. Î¤Î±ÏÏÏÏÏÎ¿Î½Î± Î¼ÎµÎ»ÎµÏÏÎ½ÏÎ±Î¹ ÏÎ±
ÏÏÎ¿Î½Î¹ÎºÎ¬ ÏÎ±ÏÎ±ÎºÏÎ·ÏÎ¹ÏÏÎ¹ÎºÎ¬ ÏÏÎ½ ÎºÎ¹Î½Î®ÏÎµÏÎ½ ÏÏÏÎµ Î½Î± ÎµÎ½Î¹ÏÏÏÎ¸ÎµÎ¯ Î· ÎµÏÎ±ÏÎ¼Î¿Î³Î® ÏÎ·Ï Î¼ÎµÎ¸ÏÎ´Î¿Ï ÏÎµ
ÏÏÎ½Î¸ÎµÏÎµÏ, ÏÏÎ±Î³Î¼Î±ÏÎ¹ÎºÎ­Ï ÏÏÎ½Î¸Î®ÎºÎµÏ ÏÎ¿Ï Î±ÏÎ±Î¹ÏÎ¿ÏÎ½ ÏÏÎ¿Î½Î¹ÎºÎ® Î±ÎºÏÎ¯Î²ÎµÎ¹Î± Î±Î½Î±ÏÎ±ÏÎ±Î³ÏÎ³Î®Ï. Î¤Î­Î»Î¿Ï, Î·
Î´Î¹Î±Î¼ÏÏÏÏÏÎ· Î¼Î¹Î±Ï Î³ÎµÎ½Î¹ÎºÎµÏÎ¼Î­Î½Î·Ï Î´Î¹Î±Î´Î¹ÎºÎ±ÏÎ¯Î±Ï ÎµÏÎ¿ÏÏÎµÏÏÎ¼ÎµÎ½Î·Ï Î¼Î¬Î¸Î·ÏÎ·Ï Î³Î¹Î± ÏÎ¿Î½ ÏÎµÎ¹ÏÎ¹ÏÎ¼Ï
ÎµÏÎ¸ÏÎ±ÏÏÏÏÎ½ Î±Î½ÏÎ¹ÎºÎµÎ¯Î¼ÎµÎ½ÏÎ½ Î±Î½Î±Î²Î±Î¸Î¼Î¯Î¶ÎµÎ¹ ÏÎµÏÎ±Î¹ÏÎ­ÏÏ ÏÎ¿ Î±ÏÏÎ¹ÎºÏ ÏÎ»Î±Î¯ÏÎ¹Î¿ Î¼Î¬Î¸Î·ÏÎ·Ï.The current PhD thesis addresses the formulation and implementation of a methodological
framework for robot Learning from Demonstration (LfD). The latter refers to methodologies
that develop behavioral policies from example state-to-action mappings. To this
end, we study the reciprocal interaction of perception and action, in order to teach robots
a repertoire of novel action behaviors. Based on that, we design, develop and implement
a robust imitation framework, termed IMFO (IMitation Framework by Observation), that
facilitates imitation learning and relevant applications in human-robot interaction (HRI)
tasks. IMFO can cope with the reproduction of learned (i.e. previously observed) actions,
aswell as novel ones. Mapping of human actions to the respective robotic ones is achieved
via an indeterminate depiction, termed latent space representation. The latter accomplishes
a compact, yet precise abstraction of action trajectories, effectively representing
high dimensional raw actions in a low dimensional space.
Moreover, throughout this thesis, we examine the role of time in LfD by enhancing
the aforementioned framework with the notion of learning both the spatial and temporal
characteristics of human motions. Accordingly, learned actions can be subsequently reproduced
in the context of more complex time-informed HRI scenarios. Unlike previous
LfD methods that cope only with the spatial traits of an action, the formulated scheme
effectively encompasses spatial and temporal aspects. Extensive experimentation with a
variety of real robotic platforms demonstrates the robustness and applicability of the introduced
integrated LfD scheme.
Learned actions are reproduced under the high level control of a time-informed task
planner. During the implementation of the studied scenarios, temporal and physical constraints
may impose speed adaptations in the performed actions. The employed latent
space representation readily supports such variations, giving rise to novel actions in the
temporal domain. Experimental results demonstrate the effectiveness of the proposed
enhanced imitation scheme in the implementation of HRI scenarios. Additionally, a set
of well defined evaluation metrics are introduced to assess the validity of the proposed
approach considering the temporal and spatial consistency of the reproduced behaviors.
A noteworthy extension of the above regards force-based object grasping for executing
sensitive manipulation tasks. This is also treated in the current thesis via a novel supervised
learning scheme, termed SLF (Supervised Learning for Force-based manipulation).
SLF is formulated as a three-stage process: (a) supervised trial-execution in simulation
to acquire sufficient training data; (b) training to facilitate grasp learning with suitable
robot-arm pose and lifting force; (c) grasp execution in simulation. Subsequently, following
sim-to-real transfer, operation in real environments is achieved in addition to simulated
ones, generalizing also for objects not included in the trial sessions. The proposed
learning scheme is demonstrated in object lifting tasks where the applied force varies for
different objects with similar contact friction coefficients, and likewise the grasping pose.
Experimental results on the manipulator YuMi show that the robot is able to effectively
reproduce demanding lifting and manipulation tasks after learning is accomplished.
In summary, our thesis has studied LfD and has contributed with a novel approach that
introduced latent space representations to encode the action characteristics. A framework
implementation (IMFO) of our approach allowed extensive experimentation and also conduction
of HRI scenarios. The inclusion of temporal aspects in our approach enhanced it
to cope with complex, real-life interactions. Finally, the extension of IMFO with forcebased
grasping facilitated manipulation tasks with sensitive objects",space,244,unknown
,to_check,core,,2018-06-30 00:00:00,core,software and system health management for autonomous robotics missions,10.1184/r1/6710654.v1,"Advanced autonomous robotics space missions rely heavily on the flawless interaction of complex hardware, multiple sensors, and a mission-critical software system.  This software system consists of an operating system, device drivers, controllers, and executives; recently highly complex AI-based autonomy software have also been introduced. Prior to launch, this software has to undergo rigorous verification and validation (V&V).  Nevertheless, dormant software bugs, failing sensors, unexpected hardware-software interactions, and unanticipated environmental conditionsâlikely on a space exploration missionâcan cause major software faults that can endanger the entire mission.

Our Integrated Software Health Management (ISWHM) system continuously monitors the hardware sensors and the software in real-time. The ISWHM uses Bayesian networks, compiled to arithmetic circuits, to model software and hardware interactions. Advanced reasoning algorithms using arithmetic circuits not only enable the ISWHM to handle large, hierarchical models that are necessary in the realm of complex autonomous systems, but also enable efficient execution on small embedded processors. The latter capability is of extreme importance for small (mobile) autonomous units with limited computational power and low telemetry bandwidth.  In this paper, we discuss the requirements of ISWHM.  As our initial demonstration platform, we use a primitive Lego rover. A Lego 
Mindstorms microcontroller is used to implement a highly simplified autonomous rover driving system, running on the OSEK real-time operating system. We demonstrate that our ISWHM, running on this small embedded microcontroller, can perform fault detection as well as on-board reasoning for advanced diagnosis and root-cause detection in real time",space,246,unknown
,to_check,core,'Institute of Electrical and Electronics Engineers (IEEE)',2018-06-29 00:00:00,core,"navigating the landscape for real-time localisation and mapping for robotics, virtual and augmented reality",10.1109/JPROC.2018.2856739,"Visual understanding of 3-D environments in real time, at low power, is a huge computational challenge. Often referred to as simultaneous localization and mapping (SLAM), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, and virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are: 1) tools and methodology for systematic quantitative evaluation of SLAM algorithms; 2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives; 3) end-to-end simulation tools to enable optimization of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches; and 4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context",space,247,unknown
,to_check,core,'Frontiers Media SA',2018-01-01 00:00:00,core,oncilla robot: a versatile open-source quadruped research robot with compliant pantograph legs,10.3389/frobt.2018.00067,"Sprowitz AT, Tuleu A, Ajallooeian M, et al. Oncilla Robot: A Versatile Open-Source Quadruped Research Robot With Compliant Pantograph Legs. FRONTIERS IN ROBOTICS AND AI. 2018;5: 18.We present Oncilla robot, a novel mobile, quadruped legged locomotion machine. This large-cat sized, 5.1 kg robot is one of a kind of a recent, bioinspired legged robot class designed with the capability of model-free locomotion control. Animal legged locomotion in rough terrain is clearly shaped by sensor feedback systems. Results with Oncilla robot show that agile and versatile locomotion is possible without sensory signals to some extend, and tracking becomes robust when feedback control is added (Ajallooeian, 2015). By incorporating mechanical and control blueprints inspired from animals, and by observing the resulting robot locomotion characteristics, we aim to understand the contribution of individual components. Legged robots have a wide mechanical and control design parameter space, and a unique potential as research tools to investigate principles of biomechanics and legged locomotion control. But the hardware and controller design can be a steep initial hurdle for academic research. To facilitate the easy start and development of legged robots, Oncilla-robot's blueprints are available through open-source. The robot's locomotion capabilities are shown in several scenarios. Specifically, its spring-loaded pantographic leg design compensates for overdetermined body and leg postures, i.e., during turning maneuvers, locomotion outdoors, or while going up and down slopes. The robot's active degree of freedom allow tight and swift direction changes, and turns on the spot. Presented hardware experiments are conducted in an open-loop manner, with little control and computational effort. For more versatile locomotion control, Oncilla-robot can sense leg joint rotations, and leg-trunk forces. Additional sensors can be included for feedback control with an open communication protocol interface. The robot's customized actuators are designed for robust actuation, and efficient locomotion. It trots with a cost of transport of 3.2 J/(Nm),at a speed of 0.63 m s(-1) (Froude number 0.25). The robot trots inclined slopes up to 10 degrees, at 0.25 m s(-1). The multi-body Webots model of Oncilla robot, and Oncilla robot's extensive software architecture enables users to design and test scenarios in simulation. Controllers can directly be transferred to the real robot. Oncilla robot's blueprints are open-source published (hardware GLP v3, software LGPL v3)",space,248,not included
,to_check,core,Proceedings of Machine Learning Research,2018-01-01 00:00:00,core,learning deployable navigation policies at kilometer scale from a single traversal,,"Model-free reinforcement learning has recently been shown to be effective at learning navigation policies from complex image input. However, these algorithms tend to require large amounts of interaction with the environment, which can be prohibitively costly to obtain on robots in the real world. We present an approach for efficiently learning goal-directed navigation policies on a mobile robot, from only a single coverage traversal of recorded data. The navigation agent learns an effective policy over a diverse action space in a large heterogeneous environment consisting of more than 2km of travel, through buildings and outdoor regions that collectively exhibit large variations in visual appearance, self-similarity, and connectivity. We compare pretrained visual encoders that enable precomputation of visual embeddings to achieve a throughput of tens of thousands of transitions per second at training time on a commodity desktop computer, allowing agents to learn from millions of trajectories of experience in a matter of hours. We propose multi- ple forms of computationally efficient stochastic augmentation to enable the learned policy to generalise beyond these precomputed embeddings, and demonstrate successful deployment of the learned policy on the real robot without fine tuning, despite environmental appearance differences at test time. The dataset and code required to reproduce these results and apply the technique to other datasets and robots is made publicly available at rl-navigation.github.io/deployable ",space,249,unknown
,to_check,core,'American Astronomical Society',2018-05-01 00:00:00,core,machine-learning-based brokers for real-time classification of the lsst alert stream,10.3847/1538-4365/aab781,"The unprecedented volume and rate of transient events that will be discovered by the Large Synoptic Survey Telescope (LSST) demand that the astronomical community update its follow-up paradigm. Alert-brokers-automated software system to sift through, characterize, annotate, and prioritize events for follow-up-will be critical tools for managing alert streams in the LSST era. The Arizona-NOAO Temporal Analysis and Response to Events System (ANTARES) is one such broker. In this work, we develop a machine learning pipeline to characterize and classify variable and transient sources only using the available multiband optical photometry. We describe three illustrative stages of the pipeline, serving the three goals of early, intermediate, and retrospective classification of alerts. The first takes the form of variable versus transient categorization, the second a multiclass typing of the combined variable and transient data set, and the third a purity-driven subtyping of a transient class. Although several similar algorithms have proven themselves in simulations, we validate their performance on real observations for the first time. We quantitatively evaluate our pipeline on sparse, unevenly sampled, heteroskedastic data from various existing observational campaigns, and demonstrate very competitive classification performance. We describe our progress toward adapting the pipeline developed in this work into a real-time broker working on live alert streams from time-domain surveys.Lasker Fellowship at the Space Telescope Science Institute; SKA SA; NRF; AIMS; NSF INSPIRE grant [CISE AST-1344204]; NOAO Community Science Data Center (CSDC)This item from the UA Faculty Publications collection is made available by the University of Arizona with support from the University of Arizona Libraries. If you have questions, please contact us at repository@u.library.arizona.edu",space,250,unknown
,to_check,core,'EDP Sciences',2018-10-16 00:00:00,core,a new method for unveiling open clusters in,10.1051/0004-6361/201833390,"Context. The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in astronomy. It includes precise astrometric data (positions, proper motions, and parallaxes) for more than 1.3 billion sources, mostly stars. To analyse such a vast amount of new data, the use of data-mining techniques and machine-learning algorithms is mandatory.
				Aims. A great example of the application of such techniques and algorithms is the search for open clusters (OCs), groups of stars that were born and move together, located in the disc. Our aim is to develop a method to automatically explore the data space, requiring minimal manual intervention.
				Methods. We explore the performance of a density-based clustering algorithm, DBSCAN, to find clusters in the data together with a supervised learning method such as an artificial neural network (ANN) to automatically distinguish between real OCs and statistical clusters.
				Results. The development and implementation of this method in a five-dimensional space (l, b, Ï, Î¼Î±*, Î¼Î´) with the Tycho-Gaia Astrometric Solution (TGAS) data, and a posterior validation using Gaia DR2 data, lead to the proposal of a set of new nearby OCs. Conclusions. We have developed a method to find OCs in astrometric data, designed to be applied to the full Gaia DR2 archive",space,251,unknown
,to_check,core,'Khmelnytskyi National University',2019-01-01 00:00:00,core,Ð±ÑÐ±Ð»ÑÐ¾Ð¼ÐµÑÑÐ¸ÑÐ½Ð¸Ð¹ Ð°Ð½Ð°Ð»ÑÐ· Ð´Ð¾ÑÐ»ÑÐ´Ð¶ÐµÐ½Ñ ÐºÑÐ±ÐµÑÐ·Ð»Ð¾ÑÐ¸Ð½Ð½Ð¾ÑÑÑ Ð² ÑÐ¼Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑÐ¾Ð²ÑÐ·Ð°ÑÑÑ ÑÑÐ½Ð°Ð½ÑÐ¾Ð²Ð¾Ð³Ð¾ ÑÐµÐºÑÐ¾ÑÑ ÐµÐºÐ¾Ð½Ð¾Ð¼ÑÐºÐ¸ Ð´ÐµÑÐ¶Ð°Ð²Ð¸,10.31891/2307-5740-2019-276-6(2)-253-259.,"Ð Ð¾Ð·Ð²Ð¸ÑÐ¾Ðº ÑÐ½ÑÐ¾ÑÐ¼Ð°ÑÑÐ¹Ð½Ð¸Ñ ÑÐµÑÐ½Ð¾Ð»Ð¾Ð³ÑÐ¹ Ð·Ð¼ÑÐ½ÑÑ Ð²ÑÑ ÑÑÑÐ°Ð»ÐµÐ½Ñ Ð¿ÑÐ¾ÑÐµÑÐ¸ ÑÑÐ½ÐºÑÑÐ¾Ð½ÑÐ²Ð°Ð½Ð½Ñ ÑÐº ÐµÐºÐ¾Ð½Ð¾Ð¼ÑÐºÐ¸ Ð² ÑÑÐ»Ð¾Ð¼Ñ, ÑÐ°Ðº Ñ ÑÑÐ½Ð°Ð½ÑÐ¾Ð²Ð¾Ð³Ð¾ ÑÐµÐºÑÐ¾ÑÑ Ð·Ð¾ÐºÑÐµÐ¼Ð°. Ð£ ÑÐ¾Ð¹ Ð¶Ðµ ÑÐ°Ñ, ÑÐ¸ÑÑÐ¾Ð²ÑÐ·Ð°ÑÑÑ ÑÑÐ½Ð°Ð½ÑÐ¾Ð²Ð¾Ð³Ð¾ ÑÐµÐºÑÐ¾ÑÐ¸ ÑÑÐ²Ð¾ÑÐ¸Ð»Ð° ÑÐ¿ÑÐ¸ÑÑÐ»Ð¸Ð²Ñ ÑÐ¼Ð¾Ð²Ð¸ Ð´Ð»Ñ ÑÐ¾Ð·Ð²Ð¸ÑÐºÑ ÐºÑÐ±ÐµÑÐ·Ð»Ð¾ÑÐ¸Ð½Ð½Ð¾ÑÑÑ, ÑÐ°Ðº Ð·Ð´ÑÐ¹ÑÐ½ÐµÐ½Ð½Ñ ÑÑÐ½Ð°Ð½ÑÐ¾Ð²Ð¸Ñ Ð·Ð»Ð¾ÑÐ¸Ð½ÑÐ² Ð¿ÐµÑÐµÐ¹ÑÐ»Ð¾ Ð· ÑÐµÐ°Ð»ÑÐ½Ð¾Ð³Ð¾ Ñ Ð²ÑÑÑÑÐ°Ð»ÑÐ½Ð¸Ð¹ Ð¿ÑÐ¾ÑÑÑÑ ÑÐ° Ð·Ð¾ÑÐµÑÐµÐ´Ð¸Ð»Ð¾ÑÑ Ð½Ð° Ð²Ð¸ÐºÑÐ°Ð´Ð°Ð½Ð½Ñ ÑÐ½ÑÐ¾ÑÐ¼Ð°ÑÑÑ Ð¿ÑÐ¾ Ð±Ð°Ð½ÐºÑÐ²ÑÑÐºÑ ÑÐ°ÑÑÐ½ÐºÐ¸ Ð¹ Ð±Ð°Ð½ÐºÑÐ²ÑÑÐºÑ ÐºÐ°ÑÑÐ¸, Ð·Ð»Ð¾Ð¼Ð¸ Ð¿Ð°ÑÐ¾Ð»ÑÐ², ÑÐ°ÑÑÐ°Ð¹ÑÑÐ²Ð¾ Ð· Ð±Ð°Ð½ÐºÐ¾Ð¼Ð°ÑÐ°Ð¼Ð¸ ÑÐ° ÑÐ½ÑÐµ. Ð£ ÑÑÐ°ÑÑÑ Ð¿ÑÐ¾Ð²ÐµÐ´ÐµÐ½Ð¾ Ð±ÑÐ±Ð»ÑÐ¾Ð¼ÐµÑÑÐ¸ÑÐ½Ð¸Ð¹ Ð°Ð½Ð°Ð»ÑÐ· Ð½Ð°ÑÐºÐ¾Ð²Ð¸Ñ Ð¿ÑÐ±Ð»ÑÐºÐ°ÑÑÐ¹, Ð¿ÑÐ¸ÑÐ²ÑÑÐµÐ½Ð¸Ñ Ð¿ÑÐ¾Ð±Ð»ÐµÐ¼Ñ ÐºÑÐ±ÐµÑÐ·Ð»Ð¾ÑÐ¸Ð½Ð½Ð¾ÑÑÑ Ð² ÑÐ¼Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑÐ¾Ð²ÑÐ·Ð°ÑÑÑ ÑÑÐ½Ð°Ð½ÑÐ¾Ð²Ð¾Ð³Ð¾ ÑÐµÐºÑÐ¾ÑÑ ÐµÐºÐ¾Ð½Ð¾Ð¼ÑÐºÐ¸ Ð£ÐºÑÐ°ÑÐ½Ð¸. Ð ÑÐµÐ·ÑÐ»ÑÑÐ°ÑÑ Ð¿Ð¾Ð±ÑÐ´Ð¾Ð²Ð¸ Ð³ÑÐ°ÑÑÐºÑÐ² Ð´Ð¸Ð½Ð°Ð¼ÑÐºÐ¸ Ð¿ÑÐ±Ð»ÑÐºÐ°ÑÑÐ¹ Ð±Ð°Ð·Ð¸ Ð´Ð°Ð½Ð¸Ñ Scopus Ð²ÑÑÐ°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾ Ð·ÑÐ¾ÑÑÐ°ÑÑÐ¸ ÑÐµÐ½Ð´ÐµÐ½ÑÑÑ Ð¿ÑÐ±Ð»ÑÐºÐ°ÑÐ¸Ð²Ð½Ð¾Ñ Ð°ÐºÑÐ¸Ð²Ð½Ð¾ÑÑÑ Ð½Ð°ÑÐºÐ¾Ð²ÑÑÐ² Ð² ÑÑÐµÑÑ ÐºÑÐ±ÐµÑÐ·Ð»Ð¾ÑÐ¸Ð½Ð½Ð¾ÑÑÑ. ÐÐ°Ð¹Ð²Ð¸ÑÑ Ð°ÐºÑÐ¸Ð²Ð½ÑÑÑÑ Ñ Ð²Ð¸ÑÑÑÐµÐ½Ð½Ñ Ð¿ÑÐ¾Ð±Ð»ÐµÐ¼ Ð¿Ð¾Ð²âÑÐ·Ð°Ð½Ð¸Ñ Ð· ÐºÑÐ±ÐµÑÐ·Ð»Ð¾ÑÐ¸Ð½Ð°Ð¼Ð¸ Ð¿ÑÐ¾ÑÐ²Ð»ÑÑÑÑ Ð½Ð°ÑÐºÐ¾Ð²ÑÑ Ð¡Ð¨Ð ÑÐ° ÐÐµÐ»Ð¸ÐºÐ¾Ð±ÑÐ¸ÑÐ°Ð½ÑÑ, ÑÐº Ð¿ÐµÑÐµÐ´Ð¾Ð²Ð¸Ñ ÐºÑÐ°ÑÐ½ Ð² ÑÑÐµÑÑ ÐµÐ»ÐµÐºÑÑÐ¾Ð½Ð½Ð¾Ñ ÐºÐ¾Ð¼ÐµÑÑÑÑ.Ð Ð°Ð·Ð²Ð¸ÑÐ¸Ðµ Ð¸Ð½ÑÐ¾ÑÐ¼Ð°ÑÐ¸Ð¾Ð½Ð½ÑÑ ÑÐµÑÐ½Ð¾Ð»Ð¾Ð³Ð¸Ð¹ Ð¼ÐµÐ½ÑÐµÑ Ð²ÑÐµ ÑÑÑÐ¾ÑÐ²ÑÐ¸ÐµÑÑ Ð¿ÑÐ¾ÑÐµÑÑÑ ÑÑÐ½ÐºÑÐ¸Ð¾Ð½Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ ÐºÐ°Ðº ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÐºÐ¸ Ð² ÑÐµÐ»Ð¾Ð¼, ÑÐ°Ðº Ð¸ ÑÐ¸Ð½Ð°Ð½ÑÐ¾Ð²Ð¾Ð³Ð¾ ÑÐµÐºÑÐ¾ÑÐ° Ð² ÑÐ°ÑÑÐ½Ð¾ÑÑÐ¸. Ð ÑÐ¾ Ð¶Ðµ Ð²ÑÐµÐ¼Ñ, ÑÐ¸ÑÑÐ¾Ð²Ð¸Ð·Ð°ÑÐ¸Ñ ÑÐ¸Ð½Ð°Ð½ÑÐ¾Ð²Ð¾Ð³Ð¾ ÑÐµÐºÑÐ¾ÑÐ° ÑÐ¾Ð·Ð´Ð°Ð»Ð° Ð±Ð»Ð°Ð³Ð¾Ð¿ÑÐ¸ÑÑÐ½ÑÐµ ÑÑÐ»Ð¾Ð²Ð¸Ñ Ð´Ð»Ñ ÑÐ°Ð·Ð²Ð¸ÑÐ¸Ñ ÐºÐ¸Ð±ÐµÑÐ¿ÑÐµÑÑÑÐ¿Ð½Ð¾ÑÑÐ¸, ÑÐ°Ðº Ð¾ÑÑÑÐµÑÑÐ²Ð»ÐµÐ½Ð¸Ðµ ÑÐ¸Ð½Ð°Ð½ÑÐ¾Ð²ÑÑ Ð¿ÑÐµÑÑÑÐ¿Ð»ÐµÐ½Ð¸Ð¹ Ð¿ÐµÑÐµÑÐ»Ð¾ Ð¸Ð· ÑÐµÐ°Ð»ÑÐ½Ð¾Ð¹ Ð² Ð²Ð¸ÑÑÑÐ°Ð»ÑÐ½Ð¾Ðµ Ð¿ÑÐ¾ÑÑÑÐ°Ð½ÑÑÐ²Ð¾ Ð¸ ÑÐ¾ÑÑÐµÐ´Ð¾ÑÐ¾ÑÐ¸Ð»Ð¾ÑÑ Ð½Ð° Ð¿Ð¾ÑÐ¸ÑÐµÐ½Ð¸Ð¸ Ð¸Ð½ÑÐ¾ÑÐ¼Ð°ÑÐ¸Ð¸ Ð¾ Ð±Ð°Ð½ÐºÐ¾Ð²ÑÐºÐ¸Ñ ÑÑÐµÑÐ°Ñ Ð¸ ââÐ±Ð°Ð½ÐºÐ¾Ð²ÑÐºÐ¸Ðµ ÐºÐ°ÑÑÑ, Ð²Ð·Ð»Ð¾Ð¼Ñ Ð¿Ð°ÑÐ¾Ð»ÐµÐ¹, Ð¼Ð¾ÑÐµÐ½Ð½Ð¸ÑÐµÑÑÐ²Ð¾ Ñ Ð±Ð°Ð½ÐºÐ¾Ð¼Ð°ÑÐ°Ð¼Ð¸ Ð¸ Ð¿ÑÐ¾ÑÐµÐµ. Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐ¾Ð²ÐµÐ´ÐµÐ½ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ð¼ÐµÑÑÐ¸ÑÐµÑÐºÐ¸Ðµ Ð°Ð½Ð°Ð»Ð¸Ð· Ð½Ð°ÑÑÐ½ÑÑ Ð¿ÑÐ±Ð»Ð¸ÐºÐ°ÑÐ¸Ð¹, Ð¿Ð¾ÑÐ²ÑÑÐµÐ½Ð½ÑÑ Ð¿ÑÐ¾Ð±Ð»ÐµÐ¼Ðµ ÐºÐ¸Ð±ÐµÑÐ¿ÑÐµÑÑÑÐ¿Ð½Ð¾ÑÑÐ¸ Ð² ÑÑÐ»Ð¾Ð²Ð¸ÑÑ ÑÐ¸ÑÑÐ¾Ð²Ð¸Ð·Ð°ÑÐ¸Ð¸ ÑÐ¸Ð½Ð°Ð½ÑÐ¾Ð²Ð¾Ð³Ð¾ ÑÐµÐºÑÐ¾ÑÐ° ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÐºÐ¸ Ð£ÐºÑÐ°Ð¸Ð½Ñ. Ð ÑÐµÐ·ÑÐ»ÑÑÐ°ÑÐµ Ð¿Ð¾ÑÑÑÐ¾ÐµÐ½Ð¸Ñ Ð³ÑÐ°ÑÐ¸ÐºÐ¾Ð² Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸ Ð¿ÑÐ±Ð»Ð¸ÐºÐ°ÑÐ¸Ð¹ Ð±Ð°Ð·Ñ Ð´Ð°Ð½Ð½ÑÑ Scopus ÑÑÑÐ°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾ Ð²ÑÑÐ°ÑÑÐ°Ñ ÑÐµÐ½Ð´ÐµÐ½ÑÐ¸Ð¸ Ð¿ÑÐ±Ð»Ð¸ÐºÐ°ÑÐ¸Ð²Ð½Ð¾Ð¸ Ð°ÐºÑÐ¸Ð²Ð½Ð¾ÑÑÐ¸ ÑÑÐµÐ½ÑÑ Ð² ÑÑÐµÑÐµ ÐºÐ¸Ð±ÐµÑÐ¿ÑÐµÑÑÑÐ¿Ð½Ð¾ÑÑÐ¸. ÐÐ°Ð¸Ð²ÑÑÑÑÑ Ð°ÐºÑÐ¸Ð²Ð½Ð¾ÑÑÑ Ð² ÑÐµÑÐµÐ½Ð¸Ð¸ Ð¿ÑÐ¾Ð±Ð»ÐµÐ¼ ÑÐ²ÑÐ·Ð°Ð½Ð½ÑÑ Ñ ÐºÐ¸Ð±ÐµÑÐ¿ÑÐµÑÑÑÐ¿Ð½Ð¾ÑÑÑÑ Ð¿ÑÐ¾ÑÐ²Ð»ÑÑÑ ÑÑÐµÐ½ÑÐµ Ð¡Ð¨Ð Ð¸ ÐÐµÐ»Ð¸ÐºÐ¾Ð±ÑÐ¸ÑÐ°Ð½Ð¸Ð¸, ÐºÐ°Ðº Ð¿ÐµÑÐµÐ´Ð¾Ð²ÑÑ ÑÑÑÐ°Ð½ Ð² ÑÑÐµÑÐµ ÑÐ»ÐµÐºÑÑÐ¾Ð½Ð½Ð¾Ð¹ ÐºÐ¾Ð¼Ð¼ÐµÑÑÐ¸Ð¸.The development of information technology changes the long-standing processes of functioning of both the economy in general and the financial sector in particular. Automation of business processes of financial institutions and digitalization of products of financial intermediaries create new conditions for the implementation of economic activities of all economic agents. Meanwhile, the digitalization of the financial sector has created favorable conditions for the development of cybercrime. Thus, financial crimes moved from the real to the virtual space and focused on the theft of information about bank accounts and bank cards, password cracking, ATM fraud, etc. The purpose of this article is to conduct a bibliometric analysis of scientific publications that deal with the issue of cybercrime in the context of digitalization of the financial sector of the Ukrainian economy. The research was carried out based on publications indexed in the Scopus database during the last 7 years. The plotting of the dynamics of publications in the Scopus database showed an increase in the general trend of publication activity in the field of cybercrime and the field of its interaction with the financial sector. The histogram of the geography of studies dealing with the issue of cybercrime in the context of digitalization of the financial sector of the economy showed the highest activity in solving the problem by scientists from the USA and Great Britain, which are leaders in the field of e-commerce. The bibliometric map of keywords of publications, built using the VOSviewer software, allowed identifying 7 clusters representing the areas of research: computer security, analysis of large databases, digital data storage, artificial intelligence, e-commerce, digital and mobile forensics. The results obtained can be used to identify the most potential areas for the spread of cybercrimes in the digital financial space, as well as tools to counter these types of crimes in Ukraine",space,252,not included
,to_check,core,Digital Commons @ University of South Florida,2019-11-08 00:00:00,core,algorithms for multi-objective mixed integer programming problems,,"This thesis presents a total of 3 groups of contributions related to multi-objective optimization.  The first group includes the development of a new algorithm and an open-source user-friendly package for optimization over the efficient set for bi-objective mixed integer linear programs.  The second group includes an application of a special case of optimization over the efficient on conservation planning problems modeled with modern portfolio theory. Finally, the third group presents a machine learning framework to enhance criterion space search algorithms for multi-objective binary linear programming.
In the first group of contributions, this thesis presents the first (criterion space search) algorithm for optimizing a linear function over the set of efficient solutions of bi-objective mixed integer linear programs. The proposed algorithm is developed based on the triangle splitting method (Boland et al.), which can find a full representation of the nondominated frontier of any bi-objective mixed integer linear program. The proposed algorithm is easy to understand and implement, and converges quickly to an optimal solution. An extensive computational study shows the efficacy of the algorithm. Is numerically shown in this thesis that the proposed algorithm can be used to quickly generate a provably high-quality approximate solution because it maintains a lower and an upper bound on the optimal value of the linear function at any point in time. Additionally, this thesis presents OOESAlgorithm.jl, a comprehensive julia package based on the proposed algorithm. The proposed package ex- tends the first implementation of the algorithm by adding two main features: (a) in addition to CPLEX, the package allows employing any single-objective solver supported by Math- ProgBase.jl, for example, GLPK, CPLEX, and SCIP; (b) the package supports execution
on multiple processors and is compatible with the JuMP modeling language. An extensive computational study shows the efficacy of the package and its features.
In the  second group of contributions, this thesis presents a Nash bargaining solu- tion approach for spatial conservation planning problems modeled with modern portfolio theory.  The proposed modern portfolio optimization formulation corresponds to a spatial conservation planning problem involving two conflicting objectives: maximizing return and minimizing risk. A Nash bargaining solution approach is presented in this thesis to directly compute a desirable Pareto-optimal (nondominated) solution for the proposed bi-objective optimization formulation in natural resource management problems. Numerical examples in this thesis show that to directly compute a Nash bargaining solution, a Binary Quadratically Constrained Quadratic Program (BQCQP) can be solved. This thesis also shows that the proposed approach (implementable with commercial  solvers such as CPLEX) can effectively solve the proposed BQCQP for much larger problems than previous approaches published in the ecological literature.  The new approach expands considerably the applicability of such optimization methods to address real spatial conservation planning problems.
In the third group of contributions, this thesis investigates the possibility of improving the performance of multi-objective optimization solution approaches using machine learning techniques. Specifically, this thesis focus on multi-objective binary linear programs and employs one of the most effective and recently developed criterion space search algorithms, the so-called KSA, during our study.  This algorithm computes all nondominated points of a problem with p objectives by searching on a projected criterion space, i.e., a (p â 1)- dimensional criterion space.  This thesis presents an effective and fast learning approach to identify on which projected space the KSA should work, and also presents several generic features that can be used in machine learning techniques for identifying the best-projected space. Finally, a bi-objective optimization-based heuristic for selecting the best subset of the features to overcome the issue of overfitting in learning is presented. Through an extensive computational study, the performance of the proposed learning approach is tested",space,253,not included
,to_check,core,'Copernicus GmbH',2019-01-01 00:00:00,core,a neural network radiative transfer model approach applied to the tropospheric monitoring instrument aerosol height algorithm,10.5194/amt-12-6619-2019,"To retrieve aerosol properties from satellite measurements of the oxygen A-band in the near-infrared, a line-by-line radiative transfer model implementation requires a large number of calculations. These calculations severely restrict a retrieval algorithm's operational capability as it can take several minutes to retrieve the aerosol layer height for a single ground pixel. This paper proposes a forward modelling approach using artificial neural networks to speed up the retrieval algorithm. The forward model outputs are trained into a set of neural network models to completely replace line-by-line calculations in the operational processor. Results comparing the forward model to the neural network alternative show an encouraging outcome with good agreement between the two when they are applied to retrieval scenarios using both synthetic and real measured spectra from TROPOMI (TROPOspheric Monitoring Instrument) on board the European Space Agency (ESA) Sentinel-5 Precursor mission. With an enhancement of the computational speed by 3 orders of magnitude, TROPOMI's operational aerosol layer height processor is now able to retrieve aerosol layer heights well within operational capacity.Atmospheric Remote Sensin",space,254,unknown
,to_check,core,"Data_Sheet_1_A Neuro-Inspired System for Online Learning and Recognition of Parallel Spike Trains, Based on Spike Latency, and Heterosynaptic STDP.PDF",2018-10-31 00:00:00,core,10.3389/fnins.2018.00780.s001,,"<p>Humans perform remarkably well in many cognitive tasks including pattern recognition. However, the neuronal mechanisms underlying this process are not well understood. Nevertheless, artificial neural networks, inspired in brain circuits, have been designed and used to tackle spatio-temporal pattern recognition tasks. In this paper we present a multi-neuronal spike pattern detection structure able to autonomously implement online learning and recognition of parallel spike sequences (i.e., sequences of pulses belonging to different neurons/neural ensembles). The operating principle of this structure is based on two spiking/synaptic neurocomputational characteristics: spike latency, which enables neurons to fire spikes with a certain delay and heterosynaptic plasticity, which allows the own regulation of synaptic weights. From the perspective of the information representation, the structure allows mapping a spatio-temporal stimulus into a multi-dimensional, temporal, feature space. In this space, the parameter coordinate and the time at which a neuron fires represent one specific feature. In this sense, each feature can be considered to span a single temporal axis. We applied our proposed scheme to experimental data obtained from a motor-inhibitory cognitive task. The results show that out method exhibits similar performance compared with other classification methods, indicating the effectiveness of our approach. In addition, its simplicity and low computational cost suggest a large scale implementation for real time recognition applications in several areas, such as brain computer interface, personal biometrics authentication, or early detection of diseases.</p",space,255,not included
,to_check,core,New trends on soft computing models in industrial and environmental applications,2013-01-01 00:00:00,core,https://core.ac.uk/download/211485892.pdf,'Elsevier BV',"The twelve papers included in this special issue represent a selection of extended contributions presented at the Sixth International Conference on Soft Computing Models in Industrial and Environmental Applications, held in Salamanca, Spain, 6â8th April, 2011. Papers were selected on the basis of fundamental ideas and concepts rather than the direct usage of well-established techniques. This special issue is then aimed at practitioners, researchers and post-graduate students, who are engaged in developing and applying advanced Soft Computing Models to solving real-world problems in the Industrial and Environmental fields. The papers are organized as follows. In the first contribution, GraÃ±a and Gonzalez-AcuÃ±a develop a formulation of dendritic classifiers based on lattice kernels and train them using a direct Monte Carlo approach and a Sparse Bayesian Learning. The results of both kinds of training are compared with the Relevance Vector Machines on a collection of benchmark datasets. In the second contribution by Irigoyen and MiÃ±ano, the authors present the results of the identification of the relationship in time, between the required exercise (machine resistance) and the heart rate of the patient in medical effort tests, using a NARX neural network model. In the experimental stage, test data have been obtained by exercising with a cyclo-ergometer in two different tests: Power Step Response and Conconi. Carneiro et al. in the third contribution present a biologically inspired method to deal with the problem in which genetic algorithms are used to create possible solutions for a given dispute. The approach presented is able to generate a broad number of diverse solutions that cover virtually the whole search space for a given problem. The results of this work are being applied in a negotiation tool that is part of the UMCourt conflict resolution platform. In the fourth contribution by Donate et al., they propose a novel Evolutionary Artificial Neural Networks (EANN) approach, where a weighted n-fold validation fitness scheme is used to build an ensemble of neural networks, under four different combination methods: mean, median, softmax and rank-based combinations. Several experiments were held, using six real-world time series with different characteristics and from distinct domains. Overall, the proposed approach achieved competitive results when compared with non weighted n-fold EANN ensembles, the simpler 0-fold EANN and also the popular HoltâWinters statistical method. Dan Burdescu et al. in the fifth contribution, present a system used in the medical domain for three distinct tasks: image annotation, semantic based image retrieval and content based image retrieval. An original image segmentation algorithm based on a hexagonal structure was used to perform the segmentation of medical images. Image's regions are described using a vocabulary of blobs generated from image features using the K-means clustering algorithm. The annotation and semantic based retrieval task is evaluated for two annotation models: Cross Media Relevance Model and Continuous-space Relevance Model. Semantic based image retrieval is performed using the methods provided by the annotation models. The ontology used by the annotation process was created in an original manner starting from the information content provided by the Medical Subject Headings (MeSH). The experiments were made using a database containing colour images retrieved from medical domain using an endoscope and related to digestive diseases. In sixth paper by Pedraza et al., they develop a face recognition system based on soft computing techniques, which complies with privacy-by-design rules and defines a set of principles that context-aware applications (including biometric sensors) should contain to conform to European and US law. This research deals with the necessity to consider legal issues concerning privacy or human rights in the development of biometric identification in ambient intelligence systems. Clearly, context-based services and ambient intelligence (and the most promising research area in Europe, namely ambient assisted living, ALL) call for a major research effort on new identification procedures. The aim of the research by Redel-MacÃ­as et al. in paper seven is to develop a novel model which can be used in pass-by noise test in vehicles based on ensembles of hybrid Evolutionary Product Unit or Radial Basis function Neural Networks (EPUNN or ERBFNNs) at high frequencies. Statistical models and ensembles of hybrid EPUNN and ERBFNN approaches have been used to develop different noise identification models. The results obtained using different ensembles of hybrid EPUNNs and ERBFNNs show that the functional model and the hybrid algorithms proposed provide a very accurate identification compared to other statistical methodologies used to solve this regression problem. In the eighth paper, Wu et al. analyse the existence criterion of loop strategies, and then present some corollaries and theorems, by which the loop strategies and chain strategies can be found, also superfluous strategies and inconsistent strategies. It presents a ranking model that indicates the weak node in strategy set and it also introduces a probability-based model which is the basis of evaluation of strategy. Additionally, this research proposes a method to generate offensive strategy, and the statistic results of simulation game prove the validity of the method. Pop et al. in the ninth paper present an efficient hybrid heuristic algorithm obtained by combining a genetic algorithm (GA) with a localâglobal approach to the generalized vehicle routing problem (GVRP) and a powerful local search procedure. The computational experiments on several benchmarks instances show that the hybrid algorithm is competitive to all of the known heuristics published to date. In the tenth paper Kramer et al. illustrate how methods from neural computation can serve as forecasting, and monitoring techniques, contributing to a successful integration of wind into sustainable, and smart energy grids. The study is based on the application of kernel methods like support vector regression and kernel density estimation as prediction methods. Furthermore, dimension reduction techniques like self-organizing maps for monitoring of high-dimensional wind time series are applied. The methods are briefly introduced, related work is presented, and experimental case studies are exemplarily described. The experimental parts are based on real wind energy time series data from the NREL western wind resource dataset. Vera et al. in the eleventh contribution present a novel soft computing procedure based on the application of artificial neural networks, genetic algorithms and identification systems, which makes it possible to optimise the implementation conditions in the manufacturing process of high precision parts, including finishing precision, while saving both time and financial costs and/or energy. The novel proposed approach was tested under real dental milling processes using a high precision machining centre with five axes, requiring high finishing precision of measures in micrometres with a large number of process factors to analyse. The results of the experiment, which validate the performance of the proposed approach, are presented in this study. The final contribution, by Sakalauskas and Kriksciuniene, presents a research about financial market efficiency and to recognize major reversal points of long-term trend of stock market index, which could indicate forthcoming crisis or market raise periods. The study suggests a computational model of financial time series analysis, which combines several approaches of soft computing, including information efficiency evaluation methods (Shannon's entropy, Hurst exponent), neural networks and sensitivity analysis. The model aims to derive the aggregated measure for evaluating efficiency of the financial market and to find its interrelationships with the reversal of long-term trend. The radial basis function neural network was designed for forecasting moments of cardinal changes in stock market behaviour, expressed by its entropy values derived from the symbolized time series of stock market index. The performance of neural network model is explored by applying sensitivity analysis and resulted in selecting smoothing parameters of the input variables. The experimental research investigates behaviour of the long-term trend of the three emerging financial markets within NASDAQ OMX Baltic stock exchange. Introduction of information efficiency measures improve ability of the model to recognize the approaching reversal of long-term trend from temporary market ânervousnessâ and can be useful for calibrating stock trading strategy. First, we would like to thank all the authors for their valuable contributions, which made this special issue possible. We also like to thank our peer-reviewers for their timely diligent work and efficient efforts. We are also grateful to the Editor-in-Chief of Neurocomputing Journal, Prof. Tom Heskes, for his continued support for the SOCO series of conferences and for this Special Issue on this prestigious journal. Finally, we hope the reader will share our joy and find this special issue very useful",space,256,unknown
,to_check,core,A new tracking approach for multipath mitigation based on antenna array,2011-01-01 00:00:00,core,https://core.ac.uk/download/12042846.pdf,,"In Global Navigation Satellites Systems (GNSS), multipaths (MP) are still one of the major error sources. The additional signal replica due to reflection will introduce a bias in conventional Delay Lock Loops (DLL) which will finally cause a strong positioning error. Several techniques, based on Maximum Likelihood estimation (ML), have been developed for multipaths mitigation/estimation such as the Narrow correlator spacing [1] or the Multipath Estimating Delay-Lock-Loop  (MEDLL) [2] algorithm. These techniques try to discriminate the MP from the Line Of Sight Signal (LOSS) on the time and frequency domains and thus, short delay multipaths (<0.1Chips) can not be completely mitigated. Antenna array perform a spatial sampling of the wave front what makes possible the discrimination of the sources on the space domain (azimuth and elevation). As the time-delay domain and space domain can be assumed independent, we can expect to mitigate/estimate very short delay MP by using an antenna array. However, we don't want to increase too much the size, the complexity and the cost of the receivers and thus, we focus our study on small arrays with a small number of antennas: typically a square 2x2 array. Consequently, conventional beamforming (space Fast Fourier Transform) is not directive enough to assure the mitigation of the multipaths, and then this first class of solutions was rejected. In order to improve the resolution, adaptive beamformers have also been tested. However, the LOSS and the MP signal are strongly correlated and thus, classical adaptive algorithms [3] are not able to discriminate the sources. These preliminary studies have shown that the mitigation/estimation of multipaths based on the space domain will exhibit limited performances in presence of close sources. Then, in order to propose robust algorithms, we decided to investigate a space-time-frequency estimation of the sources. Space Alternating Generalized Expectation maximisation (SAGE) algorithm [4], which is a low-complexity generalization of the Expectation Maximisation (EM) algorithm, has been considered. The basic concept of the SAGE algorithm is the hidden data space [4]. Instead of estimating the parameters of all impinging waves in parallel in one iteration step as done by the EM algorithm, the SAGE algorithm estimates the parameters of each signal sequentially. Moreover, SAGE algorithm breaks down the multi-dimensional optimization problem into several smaller problems. In [5], it can be seen that SAGE algorithm is efficient for any multipaths configurations (small relative delays, close DOAs) and space-time-frequency approach is clearly outperforming classical time-frequency approaches. Notwithstanding, SAGE algorithm is a post processing algorithm. Thus, it's necessary to memorise in the receiver the incoming signal in order to apply SAGE estimation. For example, if we want to process 10ms of signal with a 10MHz sampling rate, we need to store a matrix of m*105 with m the number of antennas. In such condition, we can understand than SAGE algorithm is hardly implemented in real time. The challenge is then to find a new type of algorithms that reach the efficiency of the SAGE algorithms, but with a reduced complexity in order to enable real time processing.

Furthermore, the implementation should be compatible with conventional GNSS tracking loops (DLL and PLL). To cope with these two constraints, we propose to apply the SAGE algorithm on the post-correlated signal. Indeed, the correlation step can be seen as a compression step and thus, the size of the studied signal is strongly reduced. In such a way, SAGE algorithm is able to provide estimates of the relative delay and Doppler of the received signals with respect to the local replicas. Thus, a post correlation implementation of SAGE can be seen as a discriminator for both the DLL and the PLL",space,257,not included
,to_check,core,Installation of a very broad band borehole seismic station in Ferrara (Emilia),2012-11-21 00:00:00,core,https://core.ac.uk/download/pdf/41152694.pdf,OGS,"The Istituto Nazionale di Geofisica e Vulcanologia (INGV) is the Italian agency devoted to monitor in real time the seismicity on the Italian territory.  The seismicity in Italy is of course variable in time and space, being also very much dependant on local noise conditions. Specifically, monitoring seismicity in an alluvial basin like the Po one is a challenge, due to consistent site effects induced by soft alluvial deposits and bad coupling with the deep bedrock (Steidl et al., 1996). This problem was tackled by INGV first with the Cavola experiment (Bordoni et al., 2007), where a landslide was seismically characterized using a seismic array and also down-hole logging of P- and S-wave travel times at a borehole drilled within the array; later, with an ad hoc project in 2000-2001, with the first installation of a broad band seismic station nearby Ferrara in a borehole of 135 meters depth. Comparison of recordings with a surface seismic station indicated a noise reduction of 2 decades in power spectral density at frequencies larger than 1.0 Hz (Cocco et al., 2001). The instrumentation in Ferrara has been working for several months but after that the seismic station was discontinued due to lack of maintenance manpower.
The Centro di Ricerche Sismologiche (CRS, Seismological Research Center) of the Istituto Nazionale di Oceanografia e di Geofisica Sperimentale (OGS, Italian National Institute for Oceanography and Experimental Geophysics) in Udine (Italy) after the strong earthquake of magnitude M=6.4 occurred in 1976 in the Italian Friuli-Venezia Giulia region, started to operate the Northeastern Italy (NI) Seismic Network: it currently consists of 15 very sensitive broad band and 21 simpler short period seismic stations, all telemetered to and acquired in real time at the OGS-CRS data center in Udine (Fig. 1).
Real time data exchange agreements in place with other Italian, Slovenian, Austrian and Swiss seismological institutes lead to a total number of about 100 seismic stations acquired in real time, which makes the OGS the reference institute for seismic monitoring of Northeastern Italy. Since 2002 OGS-CRS is using the Antelope software suite on several workstations plus a SUN cluster as the main tool for collecting, analyzing, archiving and exchanging seismic data, initially in the framework of the EU Interreg IIIA project âTrans-national seismological networks in the South-Eastern Alpsâ. SeisComP is also used as a real time data exchange server tool (Bragato et al., 2011).
Among the various Italian institution with which OGS is cooperating for real time monitoring of local seismicity there is the Regione Veneto (Barnaba et al., 2012). The Southern part of the Veneto Region stands on the Po alluvial basin: earthquake localization and characterization is here again affected in this area by the presence of soft alluvial deposits. OGS ha already experience in running a local seismic network in difficult noise conditions making use of borehole installations (Priolo et al., 2012) in the case of the monitoring of a local storage site for the Italian national electricity company ENEL. Following the ML=5.9 earthquake that struck the Emilia region around Ferrara in Northern Italy on May 20, 2012 at 02:03:53 UTC, a cooperation of INGV, OGS, the Comune di Ferrara and the University of Ferrara lead to the reinstallation of the very broad band borehole seismic station in Ferrara. The aim of the OGS intervention was on one hand to extend its real time seismic monitoring capabilities toward South-East (Fig. 1), including Ferrara and its surroundings, and on the other hand to evaluate the seismic  response at the site.
As concerns the superficial geology of the area where the borehole seismic station  has been installed, the outcropping materials are represented by alluvial deposits of different environments, like channel and proximal levee, inter-fluvial, meander and swamps deposits. As  a consequence, the outcropping deposits are everywhere Holocene in age substantially loose or poorly compacted in the first meters-decameters and granulometrically could vary from clay to coarse sand.
Two preliminary reports prepared by the Italian Department of Civil Defense (Dipartimento Nazionale di Protezione Civile) in collaboration with other institutions describe the data  recorded by the national accelerometric network and complemented by additional data recorded by a number of temporary stations (Dolce et al., 2012a; Dolce et al., 2012b). These reports bear witness of strong ground motion values with an acceleration peak of about 0.9 g in the vertical component recorded during the ML=5.8 earthquake of May 29, 2012 by the Mirandola station, located at about 2 km from the epicentre. The analysis of the seismic noise recorded at some stations shows a quite pronounced peak of the horizontal-to-vertical spectral ratio (H/V) in the frequency range of 0.6 â 0.9 Hz common to all stations. Finally, strong evidence of liquefaction phenomena are reported at several sites (e.g.: S. Carlo, S. Agostino and Mirabello), most of which have been attributed to the occurrence of saturated sandy layer(s) at shallow depth deposited along an abandoned reach of the Reno River (Papathanassiou et al., 2012).
Details of the station configuration and installation will be outlined, with first results",space,258,not included
,to_check,core,Analysis of interplanetary solar sail trajectories with attitude dynamics,2012-03-01 00:00:00,core,https://core.ac.uk/download/19609981.pdf,Univelt Inc,"We present a new approach to the problem of optimal control of solar sails for low-thrust trajectory optimization. The objective was to find the required control torque magnitudes in order to steer a solar sail in interplanetary space. A new steering strategy, controlling the solar sail with generic torques applied about the spacecraft body axes, is integrated into the existing low-thrust trajectory optimization software InTrance. This software combines artificial neural networks and evolutionary algorithms to find steering strategies close to the global optimum without an initial guess. Furthermore, we implement a three rotational degree-of-freedom rigid-body attitude dynamics model to represent the solar sail in space. Two interplanetary transfers to Mars and Neptune are chosen to represent typical future solar sail mission scenarios. The results found with the new steering strategy are compared to the existing reference trajectories without attitude dynamics. The resulting control torques required to accomplish the missions are investigated, as they pose the primary requirements to a real on-board attitude control system",space,259,unknown
,to_check,core,"Click fraud : how to spot it, how to stop it?",2011-08-01 00:00:00,core,https://core.ac.uk/download/143830668.pdf,ThinkIR: The University of Louisville\u27s Institutional Repository,"Online search advertising is currently the greatest source of revenue for many Internet giants such as Googleâ¢, Yahoo!â¢, and Bingâ¢. The increased number of specialized websites and modern profiling techniques have all contributed to an explosion of the income of ad brokers from online advertising. The single biggest threat to this growth is however click fraud. Trained botnets and even individuals are hired by click-fraud specialists in order to maximize the revenue of certain users from the ads they publish on  their websites, or to launch an attack between competing businesses. Most academics and consultants who study online advertising estimate that 15% to 35% of ads in pay per click (PPC) online advertising systems are not authentic. In the first two quarters of 2010, US marketers alone spent $5.7 billion on PPC ads, where PPC ads are between 45 and 50 percent of all online ad spending. On average about $1.5 billion is wasted due to click-fraud. These fraudulent clicks are believed to be initiated by users in poor countries, or botnets, who are trained to click on specific ads. For example, according to a 2010 study from Information Warfare Monitor, the operators of Koobface, a program that installed malicious software to participate in click fraud, made over $2 million in just over a year. The process of making such illegitimate clicks to generate revenue is called click-fraud. Search engines claim they filter out most questionable clicks and either not charge for them or reimburse advertisers that have been wrongly billed. However this is a hard task, despite the claims that brokers\u27 efforts are satisfactory. In the simplest scenario, a publisher continuously clicks on the ads displayed on his own website in order to make revenue. In a more complicated scenario. a travel agent may hire a large, globally distributed, botnet to click on its competitor\u27s ads, hence depleting their daily budget. We analyzed those different types of click fraud methods and proposed new methodologies to detect and prevent them real time. While traditional commercial approaches detect only some specific types of click fraud, Collaborative Click Fraud Detection and Prevention (CCFDP) system, an architecture that we have implemented based on the proposed methodologies, can detect and prevents all major types of click fraud. The proposed solution analyzes the detailed user activities on both, the server side and client side collaboratively to better describe the intention of the click. Data fusion techniques are developed to combine evidences from several data mining models and to obtain a better estimation of the quality of the click traffic. Our ideas are experimented through the development of the Collaborative Click Fraud Detection and Prevention (CCFDP) system. Experimental results show that the CCFDP system is better than the existing commercial click fraud solution in three major aspects: 1) detecting more click fraud especially clicks generated by software; 2) providing prevention ability; 3) proposing the concept of click quality score for click quality estimation. In the CCFDP initial version, we analyzed the performances of the click fraud detection and prediction model by using a rule base algorithm, which is similar to most of the existing systems. We have assigned a quality score for each click instead of classifying the click as fraud or genuine, because it is hard to get solid evidence of click fraud just based on the data collected, and it is difficult to determine the real intention of users who make the clicks. Results from initial version revealed that the diversity of CF attack Results from initial version revealed that the diversity of CF attack types makes it hard for a single counter measure to prevent click fraud. Therefore, it is important to be able to combine multiple measures capable of effective protection from click fraud. Therefore, in the CCFDP improved version, we provide the traffic quality score as a combination of evidence from several data mining algorithms. We have tested the system with a data from an actual ad campaign in 2007 and 2008.  We have compared the results with Google Adwords reports for the same campaign. Results show that a higher percentage of click fraud present even with the most popular search engine. The multiple model based CCFDP always estimated less valid traffic compare to Google. Sometimes the difference is as high as 53%. Detection of duplicates, fast and efficient, is one of the most important requirement in any click fraud solution. Usually duplicate detection algorithms run in real time. In order to provide real time results, solution providers should utilize data structures that can be updated in real time. In addition, space requirement to hold data should be minimum. In this dissertation, we also addressed the problem of detecting duplicate clicks in pay-per-click streams. We proposed a simple data structure, Temporal Stateful Bloom Filter (TSBF), an extension to the regular Bloom Filter and Counting Bloom Filter. The bit vector in the Bloom Filter was replaced with a status vector. Duplicate detection results of TSBF method is compared with Buffering, FPBuffering, and CBF methods. False positive rate of TSBF is less than 1% and it does not have false negatives. Space requirement of TSBF is minimal among other solutions. Even though Buffering does not have either false positives or false negatives its space requirement increases exponentially with the size of the stream data size. When the false positive rate of the FPBuffering is set to 1% its false negative rate jumps to around 5%, which will not be tolerated by most of the streaming data applications. We also compared the TSBF results with CBF. TSBF uses only half the space or less than standard CBF with the same false positive probability. One of the biggest successes with CCFDP is the discovery of new mercantile click bot, the Smart ClickBot. We presented a Bayesian approach for detecting the Smart ClickBot type clicks. The system combines evidence extracted from web server sessions to determine the final class of each click. Some of these evidences can be used alone, while some can be used in combination with other features for the click bot detection. During training and testing we also addressed the class imbalance problem. Our best classifier shows recall of 94%. and precision of 89%, with F1 measure calculated as 92%. The high accuracy of our system proves the effectiveness of the proposed methodology. Since the Smart ClickBot is a sophisticated click bot that manipulate every possible parameters to go undetected, the techniques that we discussed here can lead to detection of other types of software bots too. Despite the enormous capabilities of modern machine learning and data mining techniques in modeling complicated problems, most of the available click fraud detection systems are rule-based. Click fraud solution providers keep the rules as a secret weapon and bargain with others to prove their superiority. We proposed validation framework to acquire another model of the clicks data that is not rule dependent, a model that learns the inherent statistical regularities of the data. Then the output of both models is compared. Due to the uniqueness of the CCFDP system architecture, it is better than current commercial solution and search engine/ISP solution. The system protects Pay-Per-Click advertisers from click fraud and improves their Return on Investment (ROI). The system can also provide an arbitration system for advertiser and PPC publisher whenever the click fraud argument arises. Advertisers can gain their confidence on PPC advertisement by having a channel to argue the traffic quality with big search engine publishers. The results of this system will booster the internet economy by eliminating the shortcoming of PPC business model. General consumer will gain their confidence on internet business model by reducing fraudulent activities which are numerous in current virtual internet world",space,260,unknown
,to_check,core,cytonGrasp: Cyton Alpha Controller via GraspIt! Simulation,2011-12-01 00:00:00,core,https://core.ac.uk/download/268809713.pdf,TRACE: Tennessee Research and Creative Exchange,"This thesis addresses an expansion of the control programs for the Cyton Alpha 7D 1G arm. The original control system made use of configurable software which exploited the armâs seven degrees of freedom and kinematic redundancy to control the arm based on desired behaviors that were configured off-line. The inclusions of the GraspIt! grasp planning simulator and toolkit enables the Cyton Alpha to be used in more proactive on-line grasping problems, as well as, presenting many additional tools for on-line learning applications. In short, GraspIt! expands what is possible with the Cyton Alpha to include many machine learning tools and opportunities for future research. Noteworthy features of GraspIt!:
â¢ A 3D user interface allowing the user to see and interact virtual objects, obstacles, and robots, in addition to a 3D representation of the Cyton Alpha
â¢ A collision detection and contact determination system within simulation â¢ On-line grasp analysis routines
â¢ Visualization methods for determining the weak points within a grasp, as well as, creating projections of grasp quality and ability to resist dynamic forces.
â¢ Computation of numerical grasp quality metrics and visualization methods for proposed grasps
â¢ Dynamics engine
â¢ Support for lower-dimensional hand posture subspaces
â¢ Interaction with sensors (Flock of Birds tracker) and hardware (Pioneer robot) within simulation
â¢ GraspIt! can generate huge databases of labeled grasp data, which can be used for data-driven grasp-planning algorithms and has built in support for the Columbia Grasp Database.
By making use of the GraspIt! simulator, it is possible to test algorithms for grasp manipulation, grasp planning, or grasp synthesis more quickly and with greater repeatability than would be possible on the real robot. Contributions of this system include:
1. A joint based 3D rendering of the Cyton Alpha 7D 1G arm
2. Simulated bodies for several objects in the DI Lab
3. Support for multiple representations of joint data within three-dimensional space
â¢ Euler Angles
â¢ Quaternions
â¢ Denavit-Hartenberg Parameters
4. Framework for future work in grasp-planning, grasp synthesis, cooperative grasping tasks, and transfer learning applications with the Cyton Alpha arm",space,261,unknown
,to_check,core,Software Analyzes Complex Systems in Real Time,2008-01-01 00:00:00,core,https://core.ac.uk/download/pdf/10546711.pdf,,"Expert system software programs, also known as knowledge-based systems, are computer programs that emulate the knowledge and analytical skills of one or more human experts, related to a specific subject. SHINE (Spacecraft Health Inference Engine) is one such program, a software inference engine (expert system) designed by NASA for the purpose of monitoring, analyzing, and diagnosing both real-time and non-real-time systems. It was developed to meet many of the Agency s demanding and rigorous artificial intelligence goals for current and future needs. NASA developed the sophisticated and reusable software based on the experience and requirements of its Jet Propulsion Laboratory s (JPL) Artificial Intelligence Research Group in developing expert systems for space flight operations specifically, the diagnosis of spacecraft health. It was designed to be efficient enough to operate in demanding real time and in limited hardware environments, and to be utilized by non-expert systems applications written in conventional programming languages. The technology is currently used in several ongoing NASA applications, including the Mars Exploration Rovers and the Spacecraft Health Automatic Reasoning Pilot (SHARP) program for the diagnosis of telecommunication anomalies during the Neptune Voyager Encounter. It is also finding applications outside of the Space Agency",space,262,unknown
,to_check,core,Development of a Computer Architecture to Support the Optical Plume Anomaly Detection (OPAD) System,1996-10-01 00:00:00,core,https://core.ac.uk/download/pdf/10473436.pdf,,"The NASA OPAD spectrometer system relies heavily on extensive software which repetitively extracts spectral information from the engine plume and reports the amounts of metals which are present in the plume. The development of this software is at a sufficiently advanced stage where it can be used in actual engine tests to provide valuable data on engine operation and health. This activity will continue and, in addition, the OPAD system is planned to be used in flight aboard space vehicles. The two implementations, test-stand and in-flight, may have some differing requirements. For example, the data stored during a test-stand experiment are much more extensive than in the in-flight case. In both cases though, the majority of the requirements are similar. New data from the spectrograph is generated at a rate of once every 0.5 sec or faster. All processing must be completed within this period of time to maintain real-time performance. Every 0.5 sec, the OPAD system must report the amounts of specific metals within the engine plume, given the spectral data. At present, the software in the OPAD system performs this function by solving the inverse problem. It uses powerful physics-based computational models (the SPECTRA code), which receive amounts of metals as inputs to produce the spectral data that would have been observed, had the same metal amounts been present in the engine plume. During the experiment, for every spectrum that is observed, an initial approximation is performed using neural networks to establish an initial metal composition which approximates as accurately as possible the real one. Then, using optimization techniques, the SPECTRA code is repetitively used to produce a fit to the data, by adjusting the metal input amounts until the produced spectrum matches the observed one to within a given level of tolerance. This iterative solution to the original problem of determining the metal composition in the plume requires a relatively long period of time to execute the software in a modern single-processor workstation, and therefore real-time operation is currently not possible. A different number of iterations may be required to perform spectral data fitting per spectral sample. Yet, the OPAD system must be designed to maintain real-time performance in all cases. Although faster single-processor workstations are available for execution of the fitting and SPECTRA software, this option is unattractive due to the excessive cost associated with very fast workstations and also due to the fact that such hardware is not easily expandable to accommodate future versions of the software which may require more processing power. Initial research has already demonstrated that the OPAD software can take advantage of a parallel computer architecture to achieve the necessary speedup. Current work has improved the software by converting it into a form which is easily parallelizable. Timing experiments have been performed to establish the computational complexity and execution speed of major components of the software. This work provides the foundation of future work which will create a fully parallel version of the software executing in a shared-memory multiprocessor system",space,263,unknown
,to_check,core,Parallelization of Rocket Engine Simulator Software (PRESS),1998-01-01 00:00:00,core,https://core.ac.uk/download/pdf/42769102.pdf,,"We have outlined our work in the last half of the funding period. We have shown how a demo package for RESSAP using MPI can be done. However, we also mentioned the difficulties with the UNIX platform. We have reiterated some of the suggestions made during the presentation of the progress of the at Fourth Annual HBCU Conference. Although we have discussed, in some detail, how TURBDES/PUMPDES software can be run in parallel using MPI, at present, we are unable to experiment any further with either MPI or PVM. Due to X windows not being implemented, we are also not able to experiment further with XPVM, which it will be recalled, has a nice GUI interface. There are also some concerns, on our part, about MPI being an appropriate tool. The best thing about MPr is that it is public domain. Although and plenty of documentation exists for the intricacies of using MPI, little information is available on its actual implementations. Other than very typical, somewhat contrived examples, such as Jacobi algorithm for solving Laplace's equation, there are few examples which can readily be applied to real situations, such as in our case. In effect, the review of literature on both MPI and PVM, and there is a lot, indicate something similar to the enormous effort which was spent on LISP and LISP-like languages as tools for artificial intelligence research. During the development of a book on programming languages [12], when we searched the literature for very simple examples like taking averages, reading and writing records, multiplying matrices, etc., we could hardly find a any! Yet, so much was said and done on that topic in academic circles. It appears that we faced the same problem with MPI, where despite significant documentation, we could not find even a simple example which supports course-grain parallelism involving only a few processes. From the foregoing, it appears that a new direction may be required for more productive research during the extension period (10/19/98 - 10/18/99). At the least, the research would need to be done on Windows 95/Windows NT based platforms. Moreover, with the acquisition of Lahey Fortran package for PC platform, and the existing Borland C + + 5. 0, we can do work on C + + wrapper issues. We have carefully studied the blueprint for Space Transportation Propulsion Integrated Design Environment for the next 25 years [13] and found the inclusion of HBCUs in that effort encouraging. Especially in the long period for which a map is provided, there is no doubt that HBCUs will grow and become better equipped to do meaningful research. In the shorter period, as was suggested in our presentation at the HBCU conference, some key decisions regarding the aging Fortran based software for rocket propellants will need to be made. One important issue is whether or not object oriented languages such as C + + or Java should be used for distributed computing. Whether or not ""distributed computing"" is necessary for the existing software is yet another, larger, question to be tackled with",space,264,not included
,to_check,core,Thunderstorm Hypothesis Reasoner,1994-01-01 00:00:00,core,https://core.ac.uk/download/pdf/42787396.pdf,,"THOR is a knowledge-based system which incorporates techniques from signal processing, pattern recognition, and artificial intelligence (AI) in order to determine the boundary of small thunderstorms which develop and dissipate over the area encompassed by KSC and the Cape Canaveral Air Force Station. THOR interprets electric field mill data (derived from a network of electric field mills) by using heuristics and algorithms about thunderstorms that have been obtained from several domain specialists. THOR generates two forms of output: contour plots which visually describe the electric field activity over the network and a verbal interpretation of the activity. THOR uses signal processing and pattern recognition to detect signatures associated with noise or thunderstorm behavior in a near real time fashion from over 31 electrical field mills. THOR's AI component generates hypotheses identifying areas which are under a threat from storm activity, such as lightning. THOR runs on a VAX/VMS at the Kennedy Space Center. Its software is a coupling of C and FORTRAN programs, several signal processing packages, and an expert system development shell",space,265,unknown
,to_check,core,Neural networks: Application to medical imaging,1994-01-01 00:00:00,core,https://core.ac.uk/download/pdf/42781573.pdf,,"The research mission is the development of computer assisted diagnostic (CAD) methods for improved diagnosis of medical images including digital x-ray sensors and tomographic imaging modalities. The CAD algorithms include advanced methods for adaptive nonlinear filters for image noise suppression, hybrid wavelet methods for feature segmentation and enhancement, and high convergence neural networks for feature detection and VLSI implementation of neural networks for real time analysis. Other missions include (1) implementation of CAD methods on hospital based picture archiving computer systems (PACS) and information networks for central and remote diagnosis and (2) collaboration with defense and medical industry, NASA, and federal laboratories in the area of dual use technology conversion from defense or aerospace to medicine",space,266,unknown
,to_check,core,Artificial intelligence and the space station software support environment,1986-01-01 00:00:00,core,https://core.ac.uk/download/pdf/42829617.pdf,,"In a software system the size of the Space Station Software Support Environment (SSE), no one software development or implementation methodology is presently powerful enough to provide safe, reliable, maintainable, cost effective real time or near real time software. In an environment that must survive one of the most harsh and long life times, software must be produced that will perform as predicted, from the first time it is executed to the last. Many of the software challenges that will be faced will require strategies borrowed from Artificial Intelligence (AI). AI is the only development area mentioned as an example of a legitimate reason for a waiver from the overall requirement to use the Ada programming language for software development. The limits are defined of the applicability of the Ada language Ada Programming Support Environment (of which the SSE is a special case), and software engineering to AI solutions by describing a scenario that involves many facets of AI methodologies",space,267,included
,to_check,core,"Monterey, California.  Naval Postgraduate School",2017-03-01 00:00:00,core,data consolidation of disparate procurement data sources for correlated performance-based acquisition decision support,https://core.ac.uk/download/343432594.pdf,"Frank Kendall, then Under Secretary of Defense for Acquisition, Technology and Logistics, released the first defense acquisition system performance report in June 2013. This report focused primarily on performance related to the collective outcomes of Major Defense Acquisition Programs (MDAPs), but additionally explored various descriptive dimensions and acquisition approaches of the same (Kendall, 2013). Each annual report builds on the work previously conducted, and focuses on data-driven analysis relying on statistical techniques to identify trends that improve the defense acquisition communityï¾s insights into how contract incentives are motivating better contractor/vendor performance (Kendal, 2016). Nevertheless, large amounts of data (in modern jargon, ï¾Big Dataï¾) are now available for research in the area of defense acquisition. Over the past several years, changes in electronic commerce have increased the amounts of both structured and unstructured data availableï¾both in runtime and archived environments. This electronic data, from a variety of different acquisition agencies, can be obtained by a variety of means and used for a multitude of purposes (Snider et al., 2014). Traditional statistical and trend analysis methods thus far have been primarily relied upon to explore trends and test metrics in the sets of acquisition data at hand. Sometimes, spreadsheets of linear regression correlation are employed, or, in some more modern applications, multivariate structural equation models via scientific applications such as SPSS and AMOS are leveraged for their ability to evaluate complex variable relationships, such as nested or recursive if-then patterns (Byrne, 2016). However, not only are todayï¾s modern datasets large in magnitude, they are also large in variety and complexity (Gartner, 2013). Furthermore, to address this state of data, new statistical modeling techniques, more powerful than before, have had to be created. This is due to the older methods finding difficulty with some of the size problems Big Data represents, such as privacy and security concerns (Parms, 2017). Thankfully, computer power necessary to employ the modern techniques is less expensive today, the software near free, and the storage capacities available now yield bewildering capacities at a fingertip, and with amazingly fast access speed. In fact, these performance parameters appear to continue along a Mooreï¾s trend line against critical opposition (Magee, Basnet, Funk, & Benson, 2015). Presently, one of the more interesting of the new statistical modeling techniques is neural network algorithm machine learning. Neural network modeling involves utilizing a ï¾powerful computational data model that is able to capture and represent input/output relationships.ï¾ This model was developed out of the desire to create artificial intelligence systems capable of completing functions that were previously executed solely by the human brain. One benefit of using neural network modeling lies with its capacity to display and comprehend both linear and non-linear relationships from the data to which it is supplied (NeuroSolutions, 2015). Research Question Because ï¾Big Dataï¾ is present in the Defense Acquisition Business space, and, because the demand to critically understand real cause-and-effect relationships between variables within that data is persistent from the Acquisition community, this paperï¾s research question is, Can a neural network modeling technique be confidently relied upon to meaningfully explore variable relationships within acquisition business datasets? Because, if it is, then any question may be reasonably asked by anyone of such a dataset; and, via the neural network-enabled tool, the answers they receive will come with scientific statistical confidence as to whether they can be trusted as interesting or useful answers.1 In order to explore this research question, the study opted to use business data on contractor performance and attempted to isolate predictive variables from past performance information predictive of good performance.Naval Postgraduate School Acquisition Research Progra",e-commerce,268,unknown
,to_check,core,,2004-07-06 00:00:00,core,"autonomous agents in bargaining games: an evolutionary investigation of fundamentals, strategies, and business applications",,"Bargaining is becoming increasingly important due to developments within the field of electronic commerce, especially the development of autonomous software agents. Software agents are programs which, given instructions from a user, are capable of autonomously and intelligently realise a given task. By means of such agents, the bargaining process can be automated, allowing products and services together with related conditions, such as warranty and delivery time, to be flexible and tuned to the individual preferences of the people concerned. In this theses we concentrate on both fundamental aspects of bargaining as well as business-related applications of automated bargaining using software agents. The fundamental part investigates bargaining outcomes within a stylised world, and the factors that influence these outcomes. This can provide insights for the production of software agents, strategies, and setting up bargaining rules for practical situations. We study these aspects using computational simulations of bargaining agents. Hereby we consider adaptive systems, i.e., where agents learn to adjust their bargaining strategy given past experience. This learning behaviour is simulated using evolutionary algorithms. These algorithms originate from the field of artificial intelligence, and are inspired by the biological theory of evolution. Originally, evolutionary algorithms were designed for solving optimisation problems, but they are now increasingly being used within economics for modelling human learning behaviour. Besides computational simulations, we also consider mathematical solutions from game theory for relatively simple cases. Game theory is mainly concerned with the ârational manâ, that is, with optimal outcomes within an stylised setting (or game) where people act rationally. We use the game-theoretic outcomes to validate the computational experiments. The advantage of computer simulations is that less strict assumptions are necessary, and that more complex interactions that are closer to real-world settings can be investigated. First of all, we study a bargaining setting where two players exchange offers and counter offers, the so-called alternating-offers game. This game is frequently used for modelling bargaining about for instance the price of a product or service. It is also important, however, to allow other product- and service-related aspects to be negotiated, such as quality, delivery time, and warranty. This enables compromises by conceding on less important issues and demanding a higher value for relatively important aspects. This way, bargaining is less competitive and the resulting outcome can be mutually beneficial. Therefore, we investigate using computational simulations an extended version of the alternating-offers game, where multiple aspects are negotiated concurrently. Moreover, we apply game theory to validate the results of the computational experiments. The simulation shows that learning agents are capable of quickly finding optimal compromises, also called Pareto-efficient outcomes. In addition, we study the effects of time pressure that arise if negotiations are broken off with a small probability, for example due to external eventualities. In absence of time pressure and a maximum number of negotiation rounds, outcomes are very unbalanced: the player that has the opportunity to make a final offer proposes a take-it-or-leave-it offer in the last round, which leaves the other player with a deal that is only slightly better than no deal at all. With relatively high time pressure, on the other hand, the first offer is most important and almost all agreements are reached in the first round. Another interesting result is that the simulation outcomes after a long period of learning in general coincide with the results from game theory, in spite of the fact that the learning agents are not ârationalâ. In reality, not only the final outcome is important, but also other factors play a role, such as the fairness of an offer. Using the simulation we study the influence of such fairness norms on the bargaining outcomes. The fairness norms result in much more balanced outcomes, even with no time pressure, and seem to be closer outcomes in the real world. Negotiations are rarely isolated, but can also be influenced by external factors such as additional bargaining opportunities. We therefore also consider bargaining within a market-like setting, where both buyers and sellers can bargain with several opponents before reaching an agreement. The negotiations are executed consecutively until an agreement is reached or no more opportunities are available. Each bargaining game is reduced to a single round, where player 1 makes an offer and player 2 can only respond by rejecting or accepting this offer. Using an evolutionary simulation we study several properties of this market game. It appears that the outcomes depend on the information that is available to the players. If players are informed about the bargaining opportunities of their opponents, the first player in turn has the advantage and always proposes a take-it-or-leave-it deal that leaves the other player with a relatively poor outcome. This outcome is consistent with a game-theoretic analysis which we also present in this thesis. If this information is not available, a theoretical analysis is very hard. The evolutionary simulation, however, shows that in this case the responder obtains a better deal. This occurs because the first player can no longer anticipate the response of the other player, and therefore bids lower to avoid a disagreement. In this thesis, we additionally consider other factors that influence the outcomes of the market game, such as negotiation over multiple issues simultaneously, search costs, and break off probabilities. Besides fundamental issues, this thesis presents a number of business-related applications of automated bargaining, as well as generic bargaining strategies for agents that can be employed in related areas. As a first application, we introduce a framework where negotiation is used for recommending shops to customers, for example on a web page of an electronic shopping mall. Through a market-driven auction a relevant selection of shops is determined in a distributed fashion. This is achieved by selling a limited number of banner spaces in an electronic auction. For each arriving customer on the web page, shops can automatically place bids for this âcustomer attention spaceâ through their shop agents. These software agents bid based on a customer profile, containing personal data of the customer, such as age, interests, and/or keywords in a search query. The shop agents are adaptive and learn, given feedback from the customers, which profiles to target and how much to bid in the auction. The highest bidders are then selected and displayed to the customer. The feasibility of this distributed approach for matching shops to customers is demonstrated using an evolutionary simulation. Several customer models and auction mechanisms are studied, and we show that the market-based approach results in a proper selection of shops for the customers. Bargaining can be especially beneficial if not only the price, but other aspects are considered as well. This allows for example to customise products and services to the personal preferences of a user. We developed a system makes use of these properties for selling and personalising so-called information goods, such as news articles, software, and music. Using the alternating-offers protocol, a seller agent negotiates with several buyers simultaneously about a fixed price, a per-item price, and the quality of a bundle of information goods. The system is capable of taking into account important business-related conditions such as the fairness of the negotiation. The agents combine a search strategy and a concession strategy to generate offers in the negotiations. The concession strategy determines the amount the agent will concede each round, whereas the search strategy takes care of the personalisation of the offer. We introduce two search strategies in this thesis, and show through computer experiments that the use of these strategies by a buyer and seller agent, result in personalised outcomes, also when combined with various concession strategies. The search strategies presented here can be easily applied to other domains where personalisation is important. In addition, we also developed concession strategies for the seller agent that can be used in settings where a single seller agent bargains with several buyer agents simultaneously. Even if bargaining itself is bilateral (i.e., between two parties), a seller agent can actually benefit from the fact that several such negotiations occur concurrently. The developed strategies are focussed on domains where supply is flexible and can be adjusted to meet demand, like for information goods. We study fixed strategies, time-dependent strategies and introduce several auction-inspired strategies. Auctions are often used when one party negotiates with several opponents simultaneously. Although the latter strategies benefit from the advantages of auctions, the actual negotiation remains bilateral and consists of exchanging offers and counter offers. We developed an evolutionary simulation environment to evaluate the seller agentâs strategies. We especially consider the case where buyers are time-impatient and under pressure to reach agreements early. The simulations show that the auction-inspired strategies are able to obtain almost maximum profits from the negotiations, given sufficient time pressure of the buyers",e-commerce,270,unknown
,to_check,core,,2014-01-22 00:00:00,core,finding additive biclusters with random background,,"Abstract. The biclustering problem has been extensively studied in many areas including e-commerce, data mining, machine learning, pattern recognition, statistics, and more recently in computational biology. Given an n m matrix A (n ï¿½ m), the main goal of biclustering is to identify a subset of rows (called objects) and a subset of columns (called properties) such that some objective function that specifies the quality of the found bicluster (formed by the subsets of rows and of columns of A) is optimized. The problem has been proved or conjectured to be NP-hard under various mathematical models. In this paper, we study a probabilistic model of the implanted additive bicluster problem, where each element in the n m background matrix is a random number from [0 ï¿½ L 1], and a k k implanted additive bicluster is obtained from an error-free additive bicluster by randomly changing each element to a number in [0 ï¿½ L 1] with probability ï¿½. We propose an O(n2m) time voting algorithm to solve the 2 1 problem. We show that for any constant Ã such that (1 Ã)(1 ï¿½)  ï¿½ 0, L when k ï¿½ max 8 n log n ï¿½ 8logn log(2L),wherecisaconstant number, the c voting algorithm can correctly find the implanted bicluster with probability at 9 least 1 n2. We also implement our algorithm as a software tool for finding novel biclusters in microarray gene expression data, called VOTE. The implementation incorporates several nontrivial ideas for estimating the size of an implanted bicluster, adjusting the threshold in voting, dealing with small biclusters, and dealing with multiple (and overlapping) implanted biclusters. Our experimental results on both simulated and real datasets show that VOTE can find biclusters with a high accuracy and speed",e-commerce,271,unknown
,to_check,core,"Mary Ann Liebert, Inc.",,core,an efficient voting algorithm for finding additive biclusters with random background,,"The biclustering problem has been extensively studied in many areas, including e-commerce, data mining, machine learning, pattern recognition, statistics, and, more recently, computational biology. Given an n Ã m matrix A (n â¥ m), the main goal of biclustering is to identify a subset of rows (called objects) and a subset of columns (called properties) such that some objective function that specifies the quality of the found bicluster (formed by the subsets of rows and of columns of A) is optimized. The problem has been proved or conjectured to be NP-hard for various objective functions. In this article, we study a probabilistic model for the implanted additive bicluster problem, where each element in the n Ã m background matrix is a random integer from [0, L â 1] for some integer L, and a k Ã k implanted additive bicluster is obtained from an error-free additive bicluster by randomly changing each element to a number in [0, L â 1] with probability Î¸. We propose an O (n2m) time algorithm based on voting to solve the problem. We show that when \documentclass{aastex}\usepackage{amsbsy}\usepackage{amsfonts}\usepackage{amssymb}\usepackage{bm}\usepackage{mathrsfs}\usepackage{pifont}\usepackage{stmaryrd}\usepackage{textcomp}\usepackage{portland, xspace}\usepackage{amsmath, amsxtra}\pagestyle{empty}\DeclareMathSizes{10}{9}{7}{6}\begin{document}$$k \geq \Omega (\sqrt{n \log n})$$\end{document}, the voting algorithm can correctly find the implanted bicluster with probability at least \documentclass{aastex}\usepackage{amsbsy}\usepackage{amsfonts}\usepackage{amssymb}\usepackage{bm}\usepackage{mathrsfs}\usepackage{pifont}\usepackage{stmaryrd}\usepackage{textcomp}\usepackage{portland, xspace}\usepackage{amsmath, amsxtra}\pagestyle{empty}\DeclareMathSizes{10}{9}{7}{6}\begin{document}$$1 - {\frac {9} {n^ {2}}}$$\end{document}. We also implement our algorithm as a C++ program named VOTE. The implementation incorporates several ideas for estimating the size of an implanted bicluster, adjusting the threshold in voting, dealing with small biclusters, and dealing with overlapping implanted biclusters. Our experimental results on both simulated and real datasets show that VOTE can find biclusters with a high accuracy and speed",e-commerce,272,unknown
48c6c90f708013906e4fb9e82635a94f1db386db,to_check,semantic_scholar,2021 IEEE Intelligent Vehicles Symposium (IV),2021-01-01 00:00:00,semantic_scholar,digimobot: digital twin for human-robot collaboration in indoor environments,https://www.semanticscholar.org/paper/48c6c90f708013906e4fb9e82635a94f1db386db,"Human-robot collaboration and cooperation are critical for Autonomous Mobile Robots (AMRs) in order to use them in indoor environments, such as offices, hospitals, libraries, schools, factories, and warehouses. Since a long transition period might be required to fully automate such facilities, we have to deploy AMRs while improving safety in the mixed environments of human and mobile robots. In addition, human behaviors in such environments might be difficult to predict. In this paper, we present a Digital Twin for Autonomous Mobile Robots system named DigiMobot to support, manage, monitor, and validate AMRs in indoor environments. First, DigiMobot can simulate human behaviors and robot movements to verify and validate AMRs to improve safety in a virtual world. Secondly, DigiMobot can monitor and manage AMRs in the physical world by collecting sensor data from each robot in real-time. Since DigiMobot enables us to test the robot systems in the virtual world, we can deploy and implement AMRs in each facility without any modifications. To show the feasibility of DigiMobot, we develop a software framework and two different types of autonomous mobile robots. Finally, we conduct real-world experiments in a warehouse located in Saitama, Japan, in which more than 400, 000 items are stored for commercial purposes.",oceanology,273,unknown
334c96fd3595cf14477eaa52455f14b17d2b8ffc,to_check,semantic_scholar,J. Intell. Robotic Syst.,2020-01-01 00:00:00,semantic_scholar,pie: a tool for data-driven autonomous uav flight testing,https://www.semanticscholar.org/paper/334c96fd3595cf14477eaa52455f14b17d2b8ffc,"In this paper, a novel technique is presented to test the flight of an unmanned aerial vehicle autonomously in a real-world scenario using a data-driven technique without intervening with its onboard software. With the growing applications of such vehicles, testing of autonomous flight is a very important task for rapid deployment. There are different tools for modeling and simulating unmanned vehicles in virtual worlds such as Gazebo, MATLAB, Simulink, and Webots to name a few. None of these simulation tools are able to model all possible physical parameters of a real-world environment. Hence, the flight controller or mission planning software has to be tested in the physical world in the presence of an expert before deployment for a specific task. A Perception Inference Engine evaluation tool is presented that can infer internal states of the autonomous system from external observations only. The Gazebo simulation platform is used to collect data to develop the perception model. For real-time data collection, a VICON motion capture system is used to observe the autonomous flight of a small unmanned aerial vehicle. A state-of-the-art decision tree algorithm is used to implement the data-driven approach. The technique was tested using simulation data and verified with real-time data from Intel Aero Ready to Fly and Parrot AR. 2.0 drones. Moreover, we analyzed the robustness of the proposed system by introducing noise in sensor measurement and ambiguity in the testing scenario. We compared the performance of the decision tree classifier with NaÃ¯ve bayes and support vector machine classifiers. It is shown that the developed system can be used for the performance evaluation of a UAV operating in the physical world by significantly reducing uncertainty in mission failure due to environmental parameters.",oceanology,274,unknown
fab176450e9142a31f47c0dbe1990f1e647cb66d,to_check,semantic_scholar,International Journal of Parallel Programming,2019-01-01 00:00:00,semantic_scholar,guest editorial: special issue on emerging technology for software define network enabled internet of things,https://www.semanticscholar.org/paper/fab176450e9142a31f47c0dbe1990f1e647cb66d,"The Internet of Things (IoT) has been considered as a technology, which takes the first step towards a smarter world bridging the physical world with the cyber world. Even though IoT notion has gained much attention during the last few decades, real-world implementation of large-scale IoT network is still evolving in its infancy. Since deployment lags far behind the theoretic notion, comprehensive research efforts on innovative applications, architecture, and a network of IoT are required to promote the implementation of large-scale, high-quality, efficient, and secure IoT scenarios. Software Defined Networks (SDN) facilitates a variety of opportunities for network evolution. The key feature of SDN is to decouple data and control planes, which removes control plane from network hardware. As a result, it offers a remarkable resilience in programming, while providing a broad range of opportunities to optimize the utilization of network resources. Owing to the characteristics of SDN, experts in both industry and academia claims SDN as one of the ideal technologies to bridge the gaps and to overcome drawbacks of IoT deployment. Exploiting the benefits of scalable and adaptable network devices, SDN is considered as highly promising in empowering smart, powerful, and open IoT services and communication functionalities. In fact, SDN is capable of addressing numerous challenges in IoT varying from innumerable service requests and responses, the enormous data flow of IoT sensors, devices, and appli-",oceanology,275,unknown
10.1109/scc.2017.68,to_check,2017 IEEE International Conference on Services Computing (SCC),IEEE,2017-06-30 00:00:00,ieeexplore,synadapt: automated synthesis of adaptive agents,https://ieeexplore.ieee.org/document/8035021/,"Distributed autonomous multi-agent reasoning and classification systems have been thought of to be the basis of intelligence and have wide applications in the space of operational intelligence in closing the loop between sensing, analytics, and actions. This paper targets multi-agent systems that employ rulebased logics (i.e., rules that determine the output/response of an agent depending on the range of the input values) with pre-defined rules to accurately perceive the environment, and provide associated reactions. Such rule-based systems do not perform well in scenarios, where human generated rules cannot adapt to dynamic variations in the data distribution arising due to dynamic changes in the environment, especially if data dimensionality is very high. Examples of such scenarios exist wherever the sensed data arrives from the physical world - such as weather data, physical sensor data, human behaviour controlled data, etc. Clearly, to meet the adaptivity requirements of such scenarios we require the agents to possess adaptive reasoning capability such that they can adapt the underlying rules with respect to the changing environment. Developing such adaptive agents requires the developer to additionally possess considerable expertise of state-of-the-art machine learning techniques, apart from possessing knowledge of the agent's target domain. To address the above issues, we automate the process of development and deployment of adaptive agents. We present the fundamental design concepts behind the development of SynAdapt: a new adaptive meta-learning based multi-agent synthesis framework, that automates the synthesis of adaptive multi-agent systems from high-level user specifications. SynAdapt provides the following key features: a) Automated synthesis and deployment of adaptive agents from high-level user specification, b) Agents synthesised by SynAdapt can select a learning strategy that is particularly suited for given user specifications and input dataset, and c) Agents synthesised by SynAdapt can leverage adaptive ensemble learning techniques to deal with concept drift.",space,276,unknown
