id,updated,published,title,summary,database
http://arxiv.org/abs/2203.07967v1,2022-03-15T14:52:52Z,2022-03-15T14:52:52Z,Intrinsic Neural Fields: Learning Functions on Manifolds,"Neural fields have gained significant attention in the computer vision
community due to their excellent performance in novel view synthesis, geometry
reconstruction, and generative modeling. Some of their advantages are a sound
theoretic foundation and an easy implementation in current deep learning
frameworks. While neural fields have been applied to signals on manifolds,
e.g., for texture reconstruction, their representation has been limited to
extrinsically embedding the shape into Euclidean space. The extrinsic embedding
ignores known intrinsic manifold properties and is inflexible wrt. transfer of
the learned function. To overcome these limitations, this work introduces
intrinsic neural fields, a novel and versatile representation for neural fields
on manifolds. Intrinsic neural fields combine the advantages of neural fields
with the spectral properties of the Laplace-Beltrami operator. We show
theoretically that intrinsic neural fields inherit many desirable properties of
the extrinsic neural field framework but exhibit additional intrinsic
qualities, like isometry invariance. In experiments, we show intrinsic neural
fields can reconstruct high-fidelity textures from images with state-of-the-art
quality and are robust to the discretization of the underlying manifold. We
demonstrate the versatility of intrinsic neural fields by tackling various
applications: texture transfer between deformed shapes & different shapes,
texture reconstruction from real-world images with view dependence, and
discretization-agnostic learning on meshes and point clouds.",arxiv
http://arxiv.org/abs/2203.07390v1,2022-03-14T18:00:03Z,2022-03-14T18:00:03Z,"There's no difference: Convolutional Neural Networks for transient
  detection without template subtraction","We present a Convolutional Neural Network (CNN) model for the separation of
astrophysical transients from image artifacts, a task known as ""real-bogus""
classification, that does not rely on Difference Image Analysis (DIA) which is
a computationally expensive process involving image matching on small spatial
scales in large volumes of data. We explore the use of CNNs to (1) automate the
""real-bogus"" classification, (2) reduce the computational costs of transient
discovery. We compare the efficiency of two CNNs with similar architectures,
one that uses ""image triplets"" (templates, search, and the corresponding
difference image) and one that adopts a similar architecture but takes as input
the template and search only. Without substantially changing the model
architecture or retuning the hyperparameters to the new input, we observe only
a small decrease in model efficiency (97% to 92% accuracy). We further
investigate how the model that does not receive the difference image learns the
required information from the template and search by exploring the saliency
maps. Our work demonstrates that (1) CNNs are excellent models for ""real-bogus""
classification that rely exclusively on the imaging data and require no feature
engineering task; (2) high-accuracy models can be built without the need to
construct difference images. Since once trained, neural networks can generate
predictions at minimal computational costs, we argue that future
implementations of this methodology could dramatically reduce the computational
costs in the detection of genuine transients in synoptic surveys like Rubin
Observatory's Legacy Survey of Space and Time by bypassing the DIA step
entirely.",arxiv
http://arxiv.org/abs/2203.05334v1,2022-03-10T12:30:50Z,2022-03-10T12:30:50Z,"Iterative Corresponding Geometry: Fusing Region and Depth for Highly
  Efficient 3D Tracking of Textureless Objects","Tracking objects in 3D space and predicting their 6DoF pose is an essential
task in computer vision. State-of-the-art approaches often rely on object
texture to tackle this problem. However, while they achieve impressive results,
many objects do not contain sufficient texture, violating the main underlying
assumption. In the following, we thus propose ICG, a novel probabilistic
tracker that fuses region and depth information and only requires the object
geometry. Our method deploys correspondence lines and points to iteratively
refine the pose. We also implement robust occlusion handling to improve
performance in real-world settings. Experiments on the YCB-Video, OPT, and Choi
datasets demonstrate that, even for textured objects, our approach outperforms
the current state of the art with respect to accuracy and robustness. At the
same time, ICG shows fast convergence and outstanding efficiency, requiring
only 1.3 ms per frame on a single CPU core. Finally, we analyze the influence
of individual components and discuss our performance compared to deep
learning-based methods. The source code of our tracker is publicly available.",arxiv
http://arxiv.org/abs/2203.02662v1,2022-03-05T05:33:55Z,2022-03-05T05:33:55Z,"A Survey on Metaverse: Fundamentals, Security, and Privacy","Metaverse, as an evolving paradigm of the next-generation Internet, aims to
build a fully immersive, hyper spatiotemporal, and self-sustaining virtual
shared space for humans to play, work, and socialize. Driven by recent advances
in emerging technologies such as extended reality, artificial intelligence, and
blockchain, metaverse is stepping from the science fiction to an upcoming
reality. However, severe privacy invasions and security breaches (inherited
from underlying technologies or emerged in the new digital ecology) of
metaverse can impede its wide deployment. At the same time, a series of
fundamental challenges (e.g., scalability and interoperability) can arise in
metaverse security provisioning owing to the intrinsic characteristics of
metaverse, such as immersive realism, hyper spatiotemporality, sustainability,
and heterogeneity. In this paper, we present a comprehensive survey of the
fundamentals, security, and privacy of metaverse. Specifically, we first
investigate a novel distributed metaverse architecture and its key
characteristics with ternary-world interactions. Then, we discuss the security
and privacy threats, present the critical challenges of metaverse systems, and
review the state-of-the-art countermeasures. Finally, we draw open research
directions for building future metaverse systems.",arxiv
http://arxiv.org/abs/2203.00927v1,2022-03-02T08:14:06Z,2022-03-02T08:14:06Z,"TransDARC: Transformer-based Driver Activity Recognition with Latent
  Space Feature Calibration","Traditional video-based human activity recognition has experienced remarkable
progress linked to the rise of deep learning, but this effect was slower as it
comes to the downstream task of driver behavior understanding. Understanding
the situation inside the vehicle cabin is essential for Advanced Driving
Assistant System (ADAS) as it enables identifying distraction, predicting
driver's intent and leads to more convenient human-vehicle interaction. At the
same time, driver observation systems face substantial obstacles as they need
to capture different granularities of driver states, while the complexity of
such secondary activities grows with the rising automation and increased driver
freedom. Furthermore, a model is rarely deployed under conditions identical to
the ones in the training set, as sensor placements and types vary from vehicle
to vehicle, constituting a substantial obstacle for real-life deployment of
data-driven models. In this work, we present a novel vision-based framework for
recognizing secondary driver behaviours based on visual transformers and an
additional augmented feature distribution calibration module. This module
operates in the latent feature-space enriching and diversifying the training
set at feature-level in order to improve generalization to novel data
appearances, (e.g., sensor changes) and general feature quality. Our framework
consistently leads to better recognition rates, surpassing previous
state-of-the-art results of the public Drive&Act benchmark on all granularity
levels. Our code will be made publicly available at
https://github.com/KPeng9510/TransDARC.",arxiv
http://arxiv.org/abs/2203.00112v1,2022-02-28T22:00:02Z,2022-02-28T22:00:02Z,GraphWorld: Fake Graphs Bring Real Insights for GNNs,"Despite advances in the field of Graph Neural Networks (GNNs), only a small
number (~5) of datasets are currently used to evaluate new models. This
continued reliance on a handful of datasets provides minimal insight into the
performance differences between models, and is especially challenging for
industrial practitioners who are likely to have datasets which look very
different from those used as academic benchmarks. In the course of our work on
GNN infrastructure and open-source software at Google, we have sought to
develop improved benchmarks that are robust, tunable, scalable,and
generalizable. In this work we introduce GraphWorld, a novel methodology and
system for benchmarking GNN models on an arbitrarily-large population of
synthetic graphs for any conceivable GNN task. GraphWorld allows a user to
efficiently generate a world with millions of statistically diverse datasets.
It is accessible, scalable, and easy to use. GraphWorld can be run on a single
machine without specialized hardware, or it can be easily scaled up to run on
arbitrary clusters or cloud frameworks. Using GraphWorld, a user has
fine-grained control over graph generator parameters, and can benchmark
arbitrary GNN models with built-in hyperparameter tuning. We present insights
from GraphWorld experiments regarding the performance characteristics of tens
of thousands of GNN models over millions of benchmark datasets. We further show
that GraphWorld efficiently explores regions of benchmark dataset space
uncovered by standard benchmarks, revealing comparisons between models that
have not been historically obtainable. Using GraphWorld, we also are able to
study in-detail the relationship between graph properties and task performance
metrics, which is nearly impossible with the classic collection of real-world
benchmarks.",arxiv
http://arxiv.org/abs/2202.12622v1,2022-02-25T11:19:05Z,2022-02-25T11:19:05Z,Towards neoRL networks; the emergence of purposive graphs,"The neoRL framework for purposive AI implements latent learning by emulated
cognitive maps, with general value functions (GVF) expressing operant desires
toward separate states. The agent's expectancy of reward, expressed as learned
projections in the considered space, allows the neoRL agent to extract
purposive behavior from the learned map according to the reward hypothesis. We
explore this allegory further, considering neoRL modules as nodes in a network
with desire as input and state-action Q-value as output; we see that action
sets with Euclidean significance imply an interpretation of state-action
vectors as Euclidean projections of desire. Autonomous desire from neoRL nodes
within the agent allows for deeper neoRL behavioral graphs. Experiments confirm
the effect of neoRL networks governed by autonomous desire, verifying the four
principles for purposive networks. A neoRL agent governed by purposive networks
can navigate Euclidean spaces in real-time while learning, exemplifying how
modern AI still can profit from inspiration from early psychology.",arxiv
http://arxiv.org/abs/2202.10707v1,2022-02-22T07:38:23Z,2022-02-22T07:38:23Z,"Targeting occupant feedback using digital twins: Adaptive
  spatial-temporal thermal preference sampling to optimize personal comfort
  models","Collecting intensive longitudinal thermal preference data from building
occupants is emerging as an innovative means of characterizing the performance
of buildings and the people who use them. These techniques have occupants
giving subjective feedback using smartphones or smartwatches frequently over
the course of days or weeks. The intention is that the data will be collected
with high spatial and temporal diversity to best characterize a building and
the occupant's preferences. But in reality, leaving the occupant to respond in
an ad-hoc or fixed interval way creates unneeded survey fatigue and redundant
data. This paper outlines a scenario-based (virtual experiment) method for
optimizing data sampling using a smartwatch to achieve comparable accuracy in a
personal thermal preference model with less data. This method uses
BIM-extracted spatial data, and Graph Neural Network (GNN) based modeling to
find regions of similar comfort preference to identify the best scenarios for
triggering the occupant to give feedback. This method is compared to two
baseline scenarios based on the spatial context of specific spaces and 4 x 4 m
grid squares in the building using a theoretical implementation on two
field-collected data sets. The results show that the proposed Build2Vec method
is 18-23% more in the overall sampling quality than the spaces-based and the
square-grid-based sampling methods. The Build2Vec method also has similar
performance to the baselines when removing redundant occupant feedback points
but with better scalability potential.",arxiv
http://arxiv.org/abs/2202.10574v1,2022-02-21T23:36:40Z,2022-02-21T23:36:40Z,"A Multi-Agent Reinforcement Learning Framework for Off-Policy Evaluation
  in Two-sided Markets","The two-sided markets such as ride-sharing companies often involve a group of
subjects who are making sequential decisions across time and/or location. With
the rapid development of smart phones and internet of things, they have
substantially transformed the transportation landscape of human beings. In this
paper we consider large-scale fleet management in ride-sharing companies that
involve multiple units in different areas receiving sequences of products (or
treatments) over time. Major technical challenges, such as policy evaluation,
arise in those studies because (i) spatial and temporal proximities induce
interference between locations and times; and (ii) the large number of
locations results in the curse of dimensionality. To address both challenges
simultaneously, we introduce a multi-agent reinforcement learning (MARL)
framework for carrying policy evaluation in these studies. We propose novel
estimators for mean outcomes under different products that are consistent
despite the high-dimensionality of state-action space. The proposed estimator
works favorably in simulation experiments. We further illustrate our method
using a real dataset obtained from a two-sided marketplace company to evaluate
the effects of applying different subsidizing policies. A Python implementation
of the proposed method is available at
https://github.com/RunzheStat/CausalMARL.",arxiv
http://arxiv.org/abs/2202.08004v1,2022-02-16T11:40:36Z,2022-02-16T11:40:36Z,Deep Koopman Operator with Control for Nonlinear Systems,"Recently Koopman operator has become a promising data-driven tool to
facilitate real-time control for unknown nonlinear systems. It maps nonlinear
systems into equivalent linear systems in embedding space, ready for real-time
linear control methods. However, designing an appropriate Koopman embedding
function remains a challenging task. Furthermore, most Koopman-based algorithms
only consider nonlinear systems with linear control input, resulting in lousy
prediction and control performance when the system is fully nonlinear with the
control input. In this work, we propose an end-to-end deep learning framework
to learn the Koopman embedding function and Koopman Operator together to
alleviate such difficulties. We first parameterize the embedding function and
Koopman Operator with the neural network and train them end-to-end with the
K-steps loss function. We then design an auxiliary control network to encode
the nonlinear state-dependent control term to model the nonlinearity in control
input. For linear control, this encoded term is considered the new control
variable instead, ensuring the linearity of the embedding space. Then we deploy
Linear Quadratic Regulator (LQR) on the linear embedding space to derive the
optimal control policy and decode the actual control input from the control
net. Experimental results demonstrate that our approach outperforms other
existing methods, reducing the prediction error by order-of-magnitude and
achieving superior control performance in several nonlinear dynamic systems
like damping pendulum, CartPole, and 7 Dof robotic manipulator.",arxiv
http://arxiv.org/abs/2202.06836v1,2022-02-14T16:19:40Z,2022-02-14T16:19:40Z,"A Machine Learning Framework for Event Identification via Modal Analysis
  of PMU Data","Power systems are prone to a variety of events (e.g. line trips and
generation loss) and real-time identification of such events is crucial in
terms of situational awareness, reliability, and security. Using measurements
from multiple synchrophasors, i.e., phasor measurement units (PMUs), we propose
to identify events by extracting features based on modal dynamics. We combine
such traditional physics-based feature extraction methods with machine learning
to distinguish different event types. Including all measurement channels at
each PMU allows exploiting diverse features but also requires learning
classification models over a high-dimensional space. To address this issue,
various feature selection methods are implemented to choose the best subset of
features. Using the obtained subset of features, we investigate the performance
of two well-known classification models, namely, logistic regression (LR) and
support vector machines (SVM) to identify generation loss and line trip events
in two datasets. The first dataset is obtained from simulated generation loss
and line trip events in the Texas 2000-bus synthetic grid. The second is a
proprietary dataset with labeled events obtained from a large utility in the
USA involving measurements from nearly 500 PMUs. Our results indicate that the
proposed framework is promising for identifying the two types of events.",arxiv
http://arxiv.org/abs/2202.06639v1,2022-02-14T11:47:26Z,2022-02-14T11:47:26Z,"On the Complexity of Object Detection on Real-world Public
  Transportation Images for Social Distancing Measurement","Social distancing in public spaces has become an essential aspect in helping
to reduce the impact of the COVID-19 pandemic. Exploiting recent advances in
machine learning, there have been many studies in the literature implementing
social distancing via object detection through the use of surveillance cameras
in public spaces. However, to date, there has been no study of social distance
measurement on public transport. The public transport setting has some unique
challenges, including some low-resolution images and camera locations that can
lead to the partial occlusion of passengers, which make it challenging to
perform accurate detection. Thus, in this paper, we investigate the challenges
of performing accurate social distance measurement on public transportation. We
benchmark several state-of-the-art object detection algorithms using real-world
footage taken from the London Underground and bus network. The work highlights
the complexity of performing social distancing measurement on images from
current public transportation onboard cameras. Further, exploiting domain
knowledge of expected passenger behaviour, we attempt to improve the quality of
the detections using various strategies and show improvement over using vanilla
object detection alone.",arxiv
http://arxiv.org/abs/2202.06558v2,2022-02-16T10:53:59Z,2022-02-14T08:57:01Z,"SAUTE RL: Almost Surely Safe Reinforcement Learning Using State
  Augmentation","Satisfying safety constraints almost surely (or with probability one) can be
critical for deployment of Reinforcement Learning (RL) in real-life
applications. For example, plane landing and take-off should ideally occur with
probability one. We address the problem by introducing Safety Augmented (Saute)
Markov Decision Processes (MDPs), where the safety constraints are eliminated
by augmenting them into the state-space and reshaping the objective. We show
that Saute MDP satisfies the Bellman equation and moves us closer to solving
Safe RL with constraints satisfied almost surely. We argue that Saute MDP
allows to view Safe RL problem from a different perspective enabling new
features. For instance, our approach has a plug-and-play nature, i.e., any RL
algorithm can be ""sauteed"". Additionally, state augmentation allows for policy
generalization across safety constraints. We finally show that Saute RL
algorithms can outperform their state-of-the-art counterparts when constraint
satisfaction is of high importance.",arxiv
http://arxiv.org/abs/2202.05953v1,2022-02-12T02:13:55Z,2022-02-12T02:13:55Z,Open-set Adversarial Defense with Clean-Adversarial Mutual Learning,"Open-set recognition and adversarial defense study two key aspects of deep
learning that are vital for real-world deployment. The objective of open-set
recognition is to identify samples from open-set classes during testing, while
adversarial defense aims to robustify the network against images perturbed by
imperceptible adversarial noise. This paper demonstrates that open-set
recognition systems are vulnerable to adversarial samples. Furthermore, this
paper shows that adversarial defense mechanisms trained on known classes are
unable to generalize well to open-set samples. Motivated by these observations,
we emphasize the necessity of an Open-Set Adversarial Defense (OSAD) mechanism.
This paper proposes an Open-Set Defense Network with Clean-Adversarial Mutual
Learning (OSDN-CAML) as a solution to the OSAD problem. The proposed network
designs an encoder with dual-attentive feature-denoising layers coupled with a
classifier to learn a noise-free latent feature representation, which
adaptively removes adversarial noise guided by channel and spatial-wise
attentive filters. Several techniques are exploited to learn a noise-free and
informative latent feature space with the aim of improving the performance of
adversarial defense and open-set recognition. First, we incorporate a decoder
to ensure that clean images can be well reconstructed from the obtained latent
features. Then, self-supervision is used to ensure that the latent features are
informative enough to carry out an auxiliary task. Finally, to exploit more
complementary knowledge from clean image classification to facilitate feature
denoising and search for a more generalized local minimum for open-set
recognition, we further propose clean-adversarial mutual learning, where a peer
network (classifying clean images) is further introduced to mutually learn with
the classifier (classifying adversarial images).",arxiv
http://arxiv.org/abs/2202.05741v1,2022-02-11T16:27:14Z,2022-02-11T16:27:14Z,"Neural-Network Decoders for Quantum Error Correction using Surface
  Codes:A Space Exploration of the Hardware Cost-Performance Trade-Offs","Quantum Error Correction (QEC) is required in quantum computers to mitigate
the effect of errors on physical qubits. When adopting a QEC scheme based on
surface codes, error decoding is the most computationally expensive task in the
classical electronic back-end. Decoders employing neural networks (NN) are
well-suited for this task but their hardware implementation has not been
presented yet. This work presents a space exploration of fully-connected
feed-forward NN decoders for small distance surface codes. The goal is to
optimize the neural network for high decoding performance, while keeping a
minimalistic hardware implementation. This is needed to meet the tight delay
constraints of real-time surface code decoding. We demonstrate that hardware
based NN-decoders can achieve high decoding performance comparable to other
state-of-the-art decoding algorithms whilst being well below the tight delay
requirements $(\approx 440\ \mathrm{ns})$ of current solid-state qubit
technologies for both ASIC designs $(<30\ \mathrm{ns})$ and FPGA
implementations $(<90\ \mathrm{ns})$. These results designates NN-decoders as
fitting candidates for an integrated hardware implementation in future
large-scale quantum computers.",arxiv
http://arxiv.org/abs/2202.03087v2,2022-02-08T14:28:18Z,2022-02-07T11:55:23Z,Unsupervised Long-Term Person Re-Identification with Clothes Change,"We investigate unsupervised person re-identification (Re-ID) with clothes
change, a new challenging problem with more practical usability and scalability
to real-world deployment. Most existing re-id methods artificially assume the
clothes of every single person to be stationary across space and time. This
condition is mostly valid for short-term re-id scenarios since an average
person would often change the clothes even within a single day. To alleviate
this assumption, several recent works have introduced the clothes change facet
to re-id, with a focus on supervised learning person identity discriminative
representation with invariance to clothes changes. Taking a step further
towards this long-term re-id direction, we further eliminate the requirement of
person identity labels, as they are significantly more expensive and more
tedious to annotate in comparison to short-term person re-id datasets. Compared
to conventional unsupervised short-term re-id, this new problem is drastically
more challenging as different people may have similar clothes whilst the same
person can wear multiple suites of clothes over different locations and times
with very distinct appearance. To overcome such obstacles, we introduce a novel
Curriculum Person Clustering (CPC) method that can adaptively regulate the
unsupervised clustering criterion according to the clustering confidence.
Experiments on three long-term person re-id datasets show that our CPC
outperforms SOTA unsupervised re-id methods and even closely matches the
supervised re-id models.",arxiv
http://arxiv.org/abs/2202.01197v3,2022-02-04T17:41:29Z,2022-02-02T18:43:01Z,VOS: Learning What You Don't Know by Virtual Outlier Synthesis,"Out-of-distribution (OOD) detection has received much attention lately due to
its importance in the safe deployment of neural networks. One of the key
challenges is that models lack supervision signals from unknown data, and as a
result, can produce overconfident predictions on OOD data. Previous approaches
rely on real outlier datasets for model regularization, which can be costly and
sometimes infeasible to obtain in practice. In this paper, we present VOS, a
novel framework for OOD detection by adaptively synthesizing virtual outliers
that can meaningfully regularize the model's decision boundary during training.
Specifically, VOS samples virtual outliers from the low-likelihood region of
the class-conditional distribution estimated in the feature space. Alongside,
we introduce a novel unknown-aware training objective, which contrastively
shapes the uncertainty space between the ID data and synthesized outlier data.
VOS achieves state-of-the-art performance on both object detection and image
classification models, reducing the FPR95 by up to 7.87% compared to the
previous best method. Code is available at
https://github.com/deeplearning-wisc/vos.",arxiv
http://arxiv.org/abs/2201.12705v1,2022-01-30T02:10:01Z,2022-01-30T02:10:01Z,"A Robust Framework for Deep Learning Approaches to Facial Emotion
  Recognition and Evaluation","Facial emotion recognition is a vast and complex problem space within the
domain of computer vision and thus requires a universally accepted baseline
method with which to evaluate proposed models. While test datasets have served
this purpose in the academic sphere real world application and testing of such
models lacks any real comparison. Therefore we propose a framework in which
models developed for FER can be compared and contrasted against one another in
a constant standardized fashion. A lightweight convolutional neural network is
trained on the AffectNet dataset a large variable dataset for facial emotion
recognition and a web application is developed and deployed with our proposed
framework as a proof of concept. The CNN is embedded into our application and
is capable of instant real time facial emotion recognition. When tested on the
AffectNet test set this model achieves high accuracy for emotion classification
of eight different emotions. Using our framework the validity of this model and
others can be properly tested by evaluating a model efficacy not only based on
its accuracy on a sample test dataset, but also on in the wild experiments.
Additionally, our application is built with the ability to save and store any
image captured or uploaded to it for emotion recognition, allowing for the
curation of more quality and diverse facial emotion recognition datasets.",arxiv
http://arxiv.org/abs/2201.05858v1,2022-01-15T14:15:46Z,2022-01-15T14:15:46Z,"Smart Parking Space Detection under Hazy conditions using Convolutional
  Neural Networks: A Novel Approach","Limited urban parking space combined with urbanization has necessitated the
development of smart parking systems that can communicate the availability of
parking slots to the end users. Towards this, various deep learning based
solutions using convolutional neural networks have been proposed for parking
space occupation detection. Though these approaches are robust to partial
obstructions and lighting conditions, their performance is found to degrade in
the presence of haze conditions. Looking in this direction, this paper
investigates the use of dehazing networks that improves the performance of
parking space occupancy classifier under hazy conditions. Additionally,
training procedures are proposed for dehazing networks to maximize the
performance of the system on both hazy and non-hazy conditions. The proposed
system is deployable as part of existing smart parking systems where limited
number of cameras are used to monitor hundreds of parking spaces. To validate
our approach, we have developed a custom hazy parking system dataset from
real-world task-driven test set of RESIDE-\b{eta} dataset. The proposed
approach is tested against existing state-of-the-art parking space detectors on
CNRPark-EXT and hazy parking system datasets. Experimental results indicate
that there is a significant accuracy improvement of the proposed approach on
the hazy parking system dataset.",arxiv
http://arxiv.org/abs/2201.05158v1,2022-01-13T16:35:45Z,2022-01-13T16:35:45Z,Decompositional Quantum Graph Neural Network,"Quantum machine learning is a fast emerging field that aims to tackle machine
learning using quantum algorithms and quantum computing. Due to the lack of
physical qubits and an effective means to map real-world data from Euclidean
space to Hilbert space, most of these methods focus on quantum analogies or
process simulations rather than devising concrete architectures based on
qubits. In this paper, we propose a novel hybrid quantum-classical algorithm
for graph-structured data, which we refer to as the Decompositional Quantum
Graph Neural Network (DQGNN). DQGNN implements the GNN theoretical framework
using the tensor product and unity matrices representation, which greatly
reduces the number of model parameters required. When controlled by a classical
computer, DQGNN can accommodate arbitrarily sized graphs by processing
substructures from the input graph using a modestly-sized quantum device. The
architecture is based on a novel mapping from real-world data to Hilbert space.
This mapping maintains the distance relations present in the data and reduces
information loss. Experimental results show that the proposed method
outperforms competitive state-of-the-art models with only 1.68\% parameters
compared to those models.",arxiv
http://arxiv.org/abs/2112.13389v1,2021-12-26T14:38:17Z,2021-12-26T14:38:17Z,"Attributed Graph Neural Networks for Recommendation Systems on
  Large-Scale and Sparse Graph","Link prediction in structured-data is an important problem for many
applications, especially for recommendation systems. Existing methods focus on
how to learn the node representation based on graph-based structure.
High-dimensional sparse edge features are not fully exploited. Because
balancing precision and computation efficiency is significant for
recommendation systems in real world, multiple-level feature representation in
large-scale sparse graph still lacks effective and efficient solution. In this
paper, we propose a practical solution about graph neural networks called
Attributed Graph Convolutional Networks(AGCN) to incorporate edge attributes
when apply graph neural networks in large-scale sparse networks. We formulate
the link prediction problem as a subgraph classification problem. We firstly
propose an efficient two-level projection to decompose topological structures
to node-edge pairs and project them into the same interaction feature space.
Then we apply multi-layer GCN to combine the projected node-edge pairs to
capture the topological structures. Finally, the pooling representation of two
units is treated as the input of classifier to predict the probability. We
conduct offline experiments on two industrial datasets and one public dataset
and demonstrate that AGCN outperforms other excellent baselines. Moreover, we
also deploy AGCN method to important scenarios on Xianyu and AliExpress. In
online systems, AGCN achieves over 5% improvement on online metrics.",arxiv
http://arxiv.org/abs/2112.10513v1,2021-12-20T13:13:05Z,2021-12-20T13:13:05Z,"Learning Robust Policy against Disturbance in Transition Dynamics via
  State-Conservative Policy Optimization","Deep reinforcement learning algorithms can perform poorly in real-world tasks
due to the discrepancy between source and target environments. This discrepancy
is commonly viewed as the disturbance in transition dynamics. Many existing
algorithms learn robust policies by modeling the disturbance and applying it to
source environments during training, which usually requires prior knowledge
about the disturbance and control of simulators. However, these algorithms can
fail in scenarios where the disturbance from target environments is unknown or
is intractable to model in simulators. To tackle this problem, we propose a
novel model-free actor-critic algorithm -- namely, state-conservative policy
optimization (SCPO) -- to learn robust policies without modeling the
disturbance in advance. Specifically, SCPO reduces the disturbance in
transition dynamics to that in state space and then approximates it by a simple
gradient-based regularizer. The appealing features of SCPO include that it is
simple to implement and does not require additional knowledge about the
disturbance or specially designed simulators. Experiments in several robot
control tasks demonstrate that SCPO learns robust policies against the
disturbance in transition dynamics.",arxiv
http://arxiv.org/abs/2112.12061v1,2021-12-20T05:10:48Z,2021-12-20T05:10:48Z,"Adaptive model reduction and state estimation of agro-hydrological
  systems","Closed-loop irrigation can deliver a promising solution for precision
irrigation. The accurate soil moisture (state) estimation is critical in
implementing the closed-loop irrigation of agrohydrological systems. In
general, the agricultural fields are high dimensional systems. Due to the high
dimensionality for a large field, it is very challenging to solve an
optimizationbased advanced state estimator like moving horizon estimation
(MHE). This work addresses the aforementioned challenge and proposes a
systematic approach for state estimation of large agricultural fields. We use a
non-linear state-space model based on discretization of the cylindrical
coordinate version of Richards equation to describe the agro-hydrological
systems equipped with a central pivot irrigation system. We propose a
structure-preserving adaptive model reduction method using trajectory-based
unsupervised machine learning techniques. Furthermore, the adaptive MHE
algorithm is developed based on an adaptive reduced model. The proposed
algorithms are applied to a small simulated field to compare the performance of
adaptive MHE over original MHE. Finally, the proposed approach is applied to a
large-scale real agricultural field to test the effectiveness and superiority
to address the current challenges. Extensive simulations are carried out to
show the efficiency of the proposed approach.",arxiv
http://arxiv.org/abs/2112.09159v1,2021-12-16T19:11:29Z,2021-12-16T19:11:29Z,"Implementation of a Binary Neural Network on a Passive Array of Magnetic
  Tunnel Junctions","The increasing scale of neural networks and their growing application space
have produced demand for more energy- and memory-efficient
artificial-intelligence-specific hardware. Avenues to mitigate the main issue,
the von Neumann bottleneck, include in-memory and near-memory architectures, as
well as algorithmic approaches. Here we leverage the low-power and the
inherently binary operation of magnetic tunnel junctions (MTJs) to demonstrate
neural network hardware inference based on passive arrays of MTJs. In general,
transferring a trained network model to hardware for inference is confronted by
degradation in performance due to device-to-device variations, write errors,
parasitic resistance, and nonidealities in the substrate. To quantify the
effect of these hardware realities, we benchmark 300 unique weight matrix
solutions of a 2-layer perceptron to classify the Wine dataset for both
classification accuracy and write fidelity. Despite device imperfections, we
achieve software-equivalent accuracy of up to 95.3 % with proper tuning of
network parameters in 15 x 15 MTJ arrays having a range of device sizes. The
success of this tuning process shows that new metrics are needed to
characterize the performance and quality of networks reproduced in mixed signal
hardware.",arxiv
http://arxiv.org/abs/2112.08211v1,2021-12-15T15:36:57Z,2021-12-15T15:36:57Z,"TrialGraph: Machine Intelligence Enabled Insight from Graph Modelling of
  Clinical Trials","A major impediment to successful drug development is the complexity, cost,
and scale of clinical trials. The detailed internal structure of clinical trial
data can make conventional optimization difficult to achieve. Recent advances
in machine learning, specifically graph-structured data analysis, have the
potential to enable significant progress in improving the clinical trial
design. TrialGraph seeks to apply these methodologies to produce a
proof-of-concept framework for developing models which can aid drug development
and benefit patients. In this work, we first introduce a curated clinical trial
data set compiled from the CT.gov, AACT and TrialTrove databases (n=1191
trials; representing one million patients) and describe the conversion of this
data to graph-structured formats. We then detail the mathematical basis and
implementation of a selection of graph machine learning algorithms, which
typically use standard machine classifiers on graph data embedded in a
low-dimensional feature space. We trained these models to predict side effect
information for a clinical trial given information on the disease, existing
medical conditions, and treatment. The MetaPath2Vec algorithm performed
exceptionally well, with standard Logistic Regression, Decision Tree, Random
Forest, Support Vector, and Neural Network classifiers exhibiting typical
ROC-AUC scores of 0.85, 0.68, 0.86, 0.80, and 0.77, respectively. Remarkably,
the best performing classifiers could only produce typical ROC-AUC scores of
0.70 when trained on equivalent array-structured data. Our work demonstrates
that graph modelling can significantly improve prediction accuracy on
appropriate datasets. Successive versions of the project that refine modelling
assumptions and incorporate more data types can produce excellent predictors
with real-world applications in drug development.",arxiv
http://arxiv.org/abs/2112.07508v2,2022-03-10T11:10:15Z,2021-12-14T16:12:30Z,"Anti-Money Laundering Alert Optimization Using Machine Learning with
  Graphs","Money laundering is a global problem that concerns legitimizing proceeds from
serious felonies (1.7-4 trillion euros annually) such as drug dealing, human
trafficking, or corruption. The anti-money laundering systems deployed by
financial institutions typically comprise rules aligned with regulatory
frameworks. Human investigators review the alerts and report suspicious cases.
Such systems suffer from high false-positive rates, undermining their
effectiveness and resulting in high operational costs. We propose a machine
learning triage model, which complements the rule-based system and learns to
predict the risk of an alert accurately. Our model uses both entity-centric
engineered features and attributes characterizing inter-entity relations in the
form of graph-based features. We leverage time windows to construct the dynamic
graph, optimizing for time and space efficiency. We validate our model on a
real-world banking dataset and show how the triage model can reduce the number
of false positives by 80% while detecting over 90% of true positives. In this
way, our model can significantly improve anti-money laundering operations.",arxiv
http://arxiv.org/abs/2112.05393v1,2021-12-10T08:56:55Z,2021-12-10T08:56:55Z,A Self-supervised Mixed-curvature Graph Neural Network,"Graph representation learning received increasing attentions in recent years.
Most of existing methods ignore the complexity of the graph structures and
restrict graphs in a single constant-curvature representation space, which is
only suitable to particular kinds of graph structure indeed. Additionally,
these methods follow the supervised or semi-supervised learning paradigm, and
thereby notably limit their deployment on the unlabeled graphs in real
applications. To address these aforementioned limitations, we take the first
attempt to study the self-supervised graph representation learning in the
mixed-curvature spaces. In this paper, we present a novel Self-supervised
Mixed-curvature Graph Neural Network (SelfMGNN). Instead of working on one
single constant-curvature space, we construct a mixed-curvature space via the
Cartesian product of multiple Riemannian component spaces and design
hierarchical attention mechanisms for learning and fusing the representations
across these component spaces. To enable the self-supervisd learning, we
propose a novel dual contrastive approach. The mixed-curvature Riemannian space
actually provides multiple Riemannian views for the contrastive learning. We
introduce a Riemannian projector to reveal these views, and utilize a
well-designed Riemannian discriminator for the single-view and cross-view
contrastive learning within and across the Riemannian views. Finally, extensive
experiments show that SelfMGNN captures the complicated graph structures in
reality and outperforms state-of-the-art baselines.",arxiv
http://arxiv.org/abs/2112.05792v1,2021-12-08T15:55:57Z,2021-12-08T15:55:57Z,"Digital Twin of Electrical Tomography for Quantitative Multiphase Flow
  Imaging","We report a digital twin (DT) framework of electrical tomography (ET) to
address the challenge of real-time quantitative multiphase flow imaging based
on non-invasive and non-radioactive technologies. Multiphase flow is ubiquitous
in nature, industry, and research. Accurate flow imaging is the key to
understanding this complex phenomenon. Existing non-radioactive multiphase flow
imaging methods based on electrical tomography are limited to providing
qualitative images. The proposed DT framework, building upon a synergistic
integration of 3D field coupling simulation, model-based deep learning, and
edge computing, allows ET to dynamically learn the flow features in the virtual
space and implement the model in the physical system, thus providing
unprecedented resolution and accuracy. The DT framework is demonstrated on
gas-liquid two-phase flow and electrical capacitance tomography (ECT). It can
be readily extended to various tomography modalities, scenarios, and scales in
biomedical, energy, and aerospace applications as an effective alternative to
radioactive solutions for precise flow visualization and characterization.",arxiv
http://arxiv.org/abs/2112.04263v1,2021-12-08T12:50:33Z,2021-12-08T12:50:33Z,"Artificial Intelligence Powered Mobile Networks: From Cognition to
  Decision","Mobile networks (MN) are anticipated to provide unprecedented opportunities
to enable a new world of connected experiences and radically shift the way
people interact with everything. MN are becoming more and more complex, driven
by ever-increasingly complicated configuration issues and blossoming new
service requirements. This complexity poses significant challenges in
deployment, management, operation, optimization, and maintenance, since they
require a complete understanding and cognition of MN. Artificial intelligence
(AI), which deals with the simulation of intelligent behavior in computers, has
demonstrated enormous success in many application domains, suggesting its
potential in cognizing the state of MN and making intelligent decisions. In
this paper, we first propose an AI-powered mobile network architecture and
discuss challenges in terms of cognition complexity, decisions with
high-dimensional action space, and self-adaption to system dynamics. Then,
potential solutions that are associated with AI are discussed. Finally, we
propose a deep learning approach that directly maps the state of MN to
perceived QoS, integrating cognition with the decision. Our proposed approach
helps operators in making more intelligent decisions to guarantee QoS.
Meanwhile, the effectiveness and advantages of our proposed approach are
demonstrated on a real-world dataset, involving $31261$ users over $77$
stations within $5$ days.",arxiv
http://arxiv.org/abs/2112.03765v1,2021-12-07T15:19:41Z,2021-12-07T15:19:41Z,In-flight Novelty Detection with Convolutional Neural Networks,"Gas turbine engines are complex machines that typically generate a vast
amount of data, and require careful monitoring to allow for cost-effective
preventative maintenance. In aerospace applications, returning all measured
data to ground is prohibitively expensive, often causing useful, high value,
data to be discarded. The ability to detect, prioritise, and return useful data
in real-time is therefore vital. This paper proposes that system output
measurements, described by a convolutional neural network model of normality,
are prioritised in real-time for the attention of preventative maintenance
decision makers.
  Due to the complexity of gas turbine engine time-varying behaviours, deriving
accurate physical models is difficult, and often leads to models with low
prediction accuracy and incompatibility with real-time execution. Data-driven
modelling is a desirable alternative producing high accuracy, asset specific
models without the need for derivation from first principles.
  We present a data-driven system for online detection and prioritisation of
anomalous data. Biased data assessment deriving from novel operating conditions
is avoided by uncertainty management integrated into the deep neural predictive
model. Testing is performed on real and synthetic data, showing sensitivity to
both real and synthetic faults. The system is capable of running in real-time
on low-power embedded hardware and is currently in deployment on the
Rolls-Royce Pearl 15 engine flight trials.",arxiv
http://arxiv.org/abs/2112.03732v1,2021-12-07T14:41:28Z,2021-12-07T14:41:28Z,A coarse space acceleration of deep-DDM,"The use of deep learning methods for solving PDEs is a field in full
expansion. In particular, Physical Informed Neural Networks, that implement a
sampling of the physical domain and use a loss function that penalizes the
violation of the partial differential equation, have shown their great
potential. Yet, to address large scale problems encountered in real
applications and compete with existing numerical methods for PDEs, it is
important to design parallel algorithms with good scalability properties. In
the vein of traditional domain decomposition methods (DDM), we consider the
recently proposed deep-ddm approach. We present an extension of this method
that relies on the use of a coarse space correction, similarly to what is done
in traditional DDM solvers. Our investigations shows that the coarse correction
is able to alleviate the deterioration of the convergence of the solver when
the number of subdomains is increased thanks to an instantaneous information
exchange between subdomains at each iteration. Experimental results demonstrate
that our approach induces a remarkable acceleration of the original deep-ddm
method, at a reduced additional computational cost.",arxiv
http://arxiv.org/abs/2112.02424v2,2022-01-28T17:22:50Z,2021-12-04T20:27:31Z,Variational Wasserstein gradient flow,"Wasserstein gradient flow has emerged as a promising approach to solve
optimization problems over the space of probability distributions. A recent
trend is to use the well-known JKO scheme in combination with input convex
neural networks to numerically implement the proximal step. The most
challenging step, in this setup, is to evaluate functions involving density
explicitly, such as entropy, in terms of samples. This paper builds on the
recent works with a slight but crucial difference: we propose to utilize a
variational formulation of the objective function formulated as maximization
over a parametric class of functions. Theoretically, the proposed variational
formulation allows the construction of gradient flows directly for empirical
distributions with a well-defined and meaningful objective function.
Computationally, this approach replaces the computationally expensive step in
existing methods, to handle objective functions involving density, with inner
loop updates that only require a small batch of samples and scale well with the
dimension. The performance and scalability of the proposed method are
illustrated with the aid of several numerical experiments involving
high-dimensional synthetic and real datasets.",arxiv
http://arxiv.org/abs/2112.01650v1,2021-12-03T00:11:36Z,2021-12-03T00:11:36Z,"The impact of varying electrical stimulation parameters on neuromuscular
  response","High density neurostimulation systems are coming to market to help spinal
cord injury patients by stimulating and recording neuromuscular function.
However, the parameter space that these systems have to explore is exceedingly
large, and would need an artificial intelligence (AI) system to optimize. We
need a platform that will allow us to determine the optimal parameter space for
these systems. Our project aims to build a platform for mapping and controlling
neuromuscular activity, as a high-throughput testbed for implementing and
testing closed-loop neuromuscular activity. This abstract presents the first
phase (the mapping phase) of building that testbed by combining multi-electrode
stimulation/recording with visual motion-tracking. A 3D-printed rectangular
raceway was used with 4 pairs of differential recording electrodes, and two
stimulation electrodes embedded in the raceway bed. Non-anesthetized earthworms
were placed on the raceway with their head section on the stimulating
electrodes. Bipolar sinusoidal stimulation pulses of a range of voltages (2 to
6Vp-p), pulse durations (2 ms to 6.7 ms), and a burst rate of 1 pulse per
second were applied, and action potentials and physical motion were recorded
and analyzed. Action potentials were found to correlate with
expansion/contraction displacements of worm segments, and voltage increases
were shown to increase action potential propagation amplitude. Using the
multiple electrode recording allowed us to capture the wave propagation of
action potential pulse over the length of the worm. Feasibility of a platform
to simultaneously monitor action potentials and motion of earthworms with
real-time mapping was demonstrated.",arxiv
http://arxiv.org/abs/2112.01031v2,2022-02-18T00:12:23Z,2021-12-02T07:49:24Z,"Improving sensitivity of the ARIANNA detector by rejecting thermal noise
  with deep learning","The ARIANNA experiment is an Askaryan detector designed to record radio
signals induced by neutrino interactions in the Antarctic ice. Because of the
low neutrino flux at high energies ($E > 10^{16} $), the physics output is
limited by statistics. Hence, an increase in sensitivity significantly improves
the interpretation of data and offers the ability to probe new parameter
spaces. The amplitudes of the trigger threshold are limited by the rate of
triggering on unavoidable thermal noise fluctuations. We present a real-time
thermal noise rejection algorithm that enables the trigger thresholds to be
lowered, which increases the sensitivity to neutrinos by up to a factor of two
(depending on energy) compared to the current ARIANNA capabilities. A deep
learning discriminator, based on a Convolutional Neural Network (CNN), is
implemented to identify and remove thermal events in real time. We describe a
CNN trained on MC data that runs on the current ARIANNA microcomputer and
retains 95 percent of the neutrino signal at a thermal noise rejection factor
of $10^5$, compared to a template matching procedure which reaches only $10^2$
for the same signal efficiency. Then the results are verified in a lab
measurement by feeding in generated neutrino-like signal pulses and thermal
noise directly into the ARIANNA data acquisition system. Lastly, the same CNN
is used to classify cosmic-rays events to make sure they are not rejected. The
network classified 102 out of 104 cosmic-ray events as signal.",arxiv
http://arxiv.org/abs/2111.15646v2,2022-02-01T21:29:24Z,2021-11-30T18:28:19Z,The Exponentially Tilted Gaussian Prior for Variational Autoencoders,"An important property for deep neural networks is the ability to perform
robust out-of-distribution detection on previously unseen data. This property
is essential for safety purposes when deploying models for real world
applications. Recent studies show that probabilistic generative models can
perform poorly on this task, which is surprising given that they seek to
estimate the likelihood of training data. To alleviate this issue, we propose
the exponentially tilted Gaussian prior distribution for the Variational
Autoencoder (VAE) which pulls points onto the surface of a hyper-sphere in
latent space. This achieves state-of-the art results on the area under the
curve-receiver operator characteristics metric using just the negative
log-likelihood that the VAE naturally assigns. Because this prior is a simple
modification of the traditional VAE prior, it is faster and easier to implement
than competitive methods.",arxiv
http://arxiv.org/abs/2111.14826v1,2021-11-29T18:59:55Z,2021-11-29T18:59:55Z,"Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via
  Generalized Straight-Through Estimation","The nonuniform quantization strategy for compressing neural networks usually
achieves better performance than its counterpart, i.e., uniform strategy, due
to its superior representational capacity. However, many nonuniform
quantization methods overlook the complicated projection process in
implementing the nonuniformly quantized weights/activations, which incurs
non-negligible time and space overhead in hardware deployment. In this study,
we propose Nonuniform-to-Uniform Quantization (N2UQ), a method that can
maintain the strong representation ability of nonuniform methods while being
hardware-friendly and efficient as the uniform quantization for model
inference. We achieve this through learning the flexible in-equidistant input
thresholds to better fit the underlying distribution while quantizing these
real-valued inputs into equidistant output levels. To train the quantized
network with learnable input thresholds, we introduce a generalized
straight-through estimator (G-STE) for intractable backward derivative
calculation w.r.t. threshold parameters. Additionally, we consider entropy
preserving regularization to further reduce information loss in weight
quantization. Even under this adverse constraint of imposing uniformly
quantized weights and activations, our N2UQ outperforms state-of-the-art
nonuniform quantization methods by 0.7~1.8% on ImageNet, demonstrating the
contribution of N2UQ design. Code will be made publicly available.",arxiv
http://arxiv.org/abs/2111.13330v2,2021-12-11T19:25:47Z,2021-11-26T06:35:15Z,"ArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural
  Networks","Over the past few years, deep neural networks (DNNs) have achieved tremendous
success and have been continuously applied in many application domains.
However, during the practical deployment in the industrial tasks, DNNs are
found to be erroneous-prone due to various reasons such as overfitting, lacking
robustness to real-world corruptions during practical usage. To address these
challenges, many recent attempts have been made to repair DNNs for version
updates under practical operational contexts by updating weights (i.e., network
parameters) through retraining, fine-tuning, or direct weight fixing at a
neural level. In this work, as the first attempt, we initiate to repair DNNs by
jointly optimizing the architecture and weights at a higher (i.e., block)
level.
  We first perform empirical studies to investigate the limitation of whole
network-level and layer-level repairing, which motivates us to explore a novel
repairing direction for DNN repair at the block level. To this end, we first
propose adversarial-aware spectrum analysis for vulnerable block localization
that considers the neurons' status and weights' gradients in blocks during the
forward and backward processes, which enables more accurate candidate block
localization for repairing even under a few examples. Then, we further propose
the architecture-oriented search-based repairing that relaxes the targeted
block to a continuous repairing search space at higher deep feature levels. By
jointly optimizing the architecture and weights in that space, we can identify
a much better block architecture. We implement our proposed repairing
techniques as a tool, named ArchRepair, and conduct extensive experiments to
validate the proposed method. The results show that our method can not only
repair but also enhance accuracy & robustness, outperforming the
state-of-the-art DNN repair techniques.",arxiv
http://arxiv.org/abs/2111.12184v1,2021-11-23T22:45:04Z,2021-11-23T22:45:04Z,Style-Guided Web Application Exploration,"A wide range of analysis and testing techniques targeting modern web apps
rely on the automated exploration of their state space by firing events that
mimic user interactions. However, finding out which elements are actionable in
web apps is not a trivial task. To improve the efficacy of exploring the event
space of web apps, we propose a browser-independent, instrumentation-free
approach based on structural and visual stylistic cues. Our approach,
implemented in a tool called StyleX, employs machine learning models, trained
on 700,000 web elements from 1,000 real-world websites, to predict actionable
elements on a webpage a priori. In addition, our approach uses stylistic cues
for ranking these actionable elements while exploring the app. Our actionable
predictor models achieve 90.14\% precision and 87.76\% recall when considering
the click event listener, and on average, 75.42\% precision and 77.76\% recall
when considering the five most-frequent event types. Our evaluations show that
StyleX can improve the JavaScript code coverage achieved by a general-purpose
crawler by up to 23\%.",arxiv
http://arxiv.org/abs/2111.11872v1,2021-11-23T13:40:22Z,2021-11-23T13:40:22Z,"Real-time intelligent big data processing: technology, platform, and
  applications","Human beings keep exploring the physical space using information means. Only
recently, with the rapid development of information technologies and the
increasing accumulation of data, human beings can learn more about the unknown
world with data-driven methods. Given data timeliness, there is a growing
awareness of the importance of real-time data. There are two categories of
technologies accounting for data processing: batching big data and streaming
processing, which have not been integrated well. Thus, we propose an innovative
incremental processing technology named after Stream Cube to process both big
data and stream data. Also, we implement a real-time intelligent data
processing system, which is based on real-time acquisition, real-time
processing, real-time analysis, and real-time decision-making. The real-time
intelligent data processing technology system is equipped with a batching big
data platform, data analysis tools, and machine learning models. Based on our
applications and analysis, the real-time intelligent data processing system is
a crucial solution to the problems of the national society and economy.",arxiv
http://arxiv.org/abs/2111.09999v1,2021-11-19T01:35:10Z,2021-11-19T01:35:10Z,"TnT Attacks! Universal Naturalistic Adversarial Patches Against Deep
  Neural Network Systems","Deep neural networks are vulnerable to attacks from adversarial inputs and,
more recently, Trojans to misguide or hijack the decision of the model. We
expose the existence of an intriguing class of bounded adversarial examples --
Universal NaTuralistic adversarial paTches -- we call TnTs, by exploring the
superset of the bounded adversarial example space and the natural input space
within generative adversarial networks. Now, an adversary can arm themselves
with a patch that is naturalistic, less malicious-looking, physically
realizable, highly effective -- achieving high attack success rates, and
universal. A TnT is universal because any input image captured with a TnT in
the scene will: i) misguide a network (untargeted attack); or ii) force the
network to make a malicious decision (targeted attack). Interestingly, now, an
adversarial patch attacker has the potential to exert a greater level of
control -- the ability to choose a location independent, natural-looking patch
as a trigger in contrast to being constrained to noisy perturbations -- an
ability is thus far shown to be only possible with Trojan attack methods
needing to interfere with the model building processes to embed a backdoor at
the risk discovery; but, still realize a patch deployable in the physical
world. Through extensive experiments on the large-scale visual classification
task, ImageNet with evaluations across its entire validation set of 50,000
images, we demonstrate the realistic threat from TnTs and the robustness of the
attack. We show a generalization of the attack to create patches achieving
higher attack success rates than existing state-of-the-art methods. Our results
show the generalizability of the attack to different visual classification
tasks (CIFAR-10, GTSRB, PubFig) and multiple state-of-the-art deep neural
networks such as WideResnet50, Inception-V3 and VGG-16.",arxiv
http://arxiv.org/abs/2111.09805v1,2021-11-18T17:11:43Z,2021-11-18T17:11:43Z,On the Effectiveness of Sparsification for Detecting the Deep Unknowns,"Detecting out-of-distribution (OOD) inputs is a central challenge for safely
deploying machine learning models in the real world. Previous methods commonly
rely on an OOD score derived from the overparameterized weight space, while
largely overlooking the role of sparsification. In this paper, we reveal
important insights that reliance on unimportant weights and units can directly
attribute to the brittleness of OOD detection. To mitigate the issue, we
propose a sparsification-based OOD detection framework termed DICE. Our key
idea is to rank weights based on a measure of contribution, and selectively use
the most salient weights to derive the output for OOD detection. We provide
both empirical and theoretical insights, characterizing and explaining the
mechanism by which DICE improves OOD detection. By pruning away noisy signals,
DICE provably reduces the output variance for OOD data, resulting in a sharper
output distribution and stronger separability from ID data. DICE establishes
superior performance, reducing the FPR95 by up to 24.69% compared to the
previous best method.",arxiv
http://arxiv.org/abs/2111.08892v2,2021-11-26T16:13:57Z,2021-11-17T03:57:11Z,"SAPNet: Segmentation-Aware Progressive Network for Perceptual
  Contrastive Deraining","Deep learning algorithms have recently achieved promising deraining
performances on both the natural and synthetic rainy datasets. As an essential
low-level pre-processing stage, a deraining network should clear the rain
streaks and preserve the fine semantic details. However, most existing methods
only consider low-level image restoration. That limits their performances at
high-level tasks requiring precise semantic information. To address this issue,
in this paper, we present a segmentation-aware progressive network (SAPNet)
based upon contrastive learning for single image deraining. We start our method
with a lightweight derain network formed with progressive dilated units (PDU).
The PDU can significantly expand the receptive field and characterize
multi-scale rain streaks without the heavy computation on multi-scale images. A
fundamental aspect of this work is an unsupervised background segmentation
(UBS) network initialized with ImageNet and Gaussian weights. The UBS can
faithfully preserve an image's semantic information and improve the
generalization ability to unseen photos. Furthermore, we introduce a perceptual
contrastive loss (PCL) and a learned perceptual image similarity loss (LPISL)
to regulate model learning. By exploiting the rainy image and groundtruth as
the negative and the positive sample in the VGG-16 latent space, we bridge the
fine semantic details between the derained image and the groundtruth in a fully
constrained manner. Comprehensive experiments on synthetic and real-world rainy
images show our model surpasses top-performing methods and aids object
detection and semantic segmentation with considerable efficacy. A Pytorch
Implementation is available at
https://github.com/ShenZheng2000/SAPNet-for-image-deraining.",arxiv
http://arxiv.org/abs/2111.08885v1,2021-11-17T03:29:59Z,2021-11-17T03:29:59Z,Jump Interval-Learning for Individualized Decision Making,"An individualized decision rule (IDR) is a decision function that assigns
each individual a given treatment based on his/her observed characteristics.
Most of the existing works in the literature consider settings with binary or
finitely many treatment options. In this paper, we focus on the continuous
treatment setting and propose a jump interval-learning to develop an
individualized interval-valued decision rule (I2DR) that maximizes the expected
outcome. Unlike IDRs that recommend a single treatment, the proposed I2DR
yields an interval of treatment options for each individual, making it more
flexible to implement in practice. To derive an optimal I2DR, our jump
interval-learning method estimates the conditional mean of the outcome given
the treatment and the covariates via jump penalized regression, and derives the
corresponding optimal I2DR based on the estimated outcome regression function.
The regressor is allowed to be either linear for clear interpretation or deep
neural network to model complex treatment-covariates interactions. To implement
jump interval-learning, we develop a searching algorithm based on dynamic
programming that efficiently computes the outcome regression function.
Statistical properties of the resulting I2DR are established when the outcome
regression function is either a piecewise or continuous function over the
treatment space. We further develop a procedure to infer the mean outcome under
the (estimated) optimal policy. Extensive simulations and a real data
application to a warfarin study are conducted to demonstrate the empirical
validity of the proposed I2DR.",arxiv
http://arxiv.org/abs/2111.07171v1,2021-11-13T18:48:28Z,2021-11-13T18:48:28Z,"Deep Reinforcement Learning with Shallow Controllers: An Experimental
  Application to PID Tuning","Deep reinforcement learning (RL) is an optimization-driven framework for
producing control strategies for general dynamical systems without explicit
reliance on process models. Good results have been reported in simulation. Here
we demonstrate the challenges in implementing a state of the art deep RL
algorithm on a real physical system. Aspects include the interplay between
software and existing hardware; experiment design and sample efficiency;
training subject to input constraints; and interpretability of the algorithm
and control law. At the core of our approach is the use of a PID controller as
the trainable RL policy. In addition to its simplicity, this approach has
several appealing features: No additional hardware needs to be added to the
control system, since a PID controller can easily be implemented through a
standard programmable logic controller; the control law can easily be
initialized in a ""safe'' region of the parameter space; and the final product
-- a well-tuned PID controller -- has a form that practitioners can reason
about and deploy with confidence.",arxiv
http://arxiv.org/abs/2111.02149v1,2021-11-03T11:37:11Z,2021-11-03T11:37:11Z,"Deployment Optimization for Shared e-Mobility Systems with Multi-agent
  Deep Neural Search","Shared e-mobility services have been widely tested and piloted in cities
across the globe, and already woven into the fabric of modern urban planning.
This paper studies a practical yet important problem in those systems: how to
deploy and manage their infrastructure across space and time, so that the
services are ubiquitous to the users while sustainable in profitability.
However, in real-world systems evaluating the performance of different
deployment strategies and then finding the optimal plan is prohibitively
expensive, as it is often infeasible to conduct many iterations of
trial-and-error. We tackle this by designing a high-fidelity simulation
environment, which abstracts the key operation details of the shared e-mobility
systems at fine-granularity, and is calibrated using data collected from the
real-world. This allows us to try out arbitrary deployment plans to learn the
optimal given specific context, before actually implementing any in the
real-world systems. In particular, we propose a novel multi-agent neural search
approach, in which we design a hierarchical controller to produce tentative
deployment plans. The generated deployment plans are then tested using a
multi-simulation paradigm, i.e., evaluated in parallel, where the results are
used to train the controller with deep reinforcement learning. With this closed
loop, the controller can be steered to have higher probability of generating
better deployment plans in future iterations. The proposed approach has been
evaluated extensively in our simulation environment, and experimental results
show that it outperforms baselines e.g., human knowledge, and state-of-the-art
heuristic-based optimization approaches in both service coverage and net
revenue.",arxiv
http://arxiv.org/abs/2111.01365v1,2021-11-02T04:32:18Z,2021-11-02T04:32:18Z,"Koopman Q-learning: Offline Reinforcement Learning via Symmetries of
  Dynamics","Offline reinforcement learning leverages large datasets to train policies
without interactions with the environment. The learned policies may then be
deployed in real-world settings where interactions are costly or dangerous.
Current algorithms over-fit to the training dataset and as a consequence
perform poorly when deployed to out-of-distribution generalizations of the
environment. We aim to address these limitations by learning a Koopman latent
representation which allows us to infer symmetries of the system's underlying
dynamic. The latter is then utilized to extend the otherwise static offline
dataset during training; this constitutes a novel data augmentation framework
which reflects the system's dynamic and is thus to be interpreted as an
exploration of the environments phase space. To obtain the symmetries we employ
Koopman theory in which nonlinear dynamics are represented in terms of a linear
operator acting on the space of measurement functions of the system and thus
symmetries of the dynamics may be inferred directly. We provide novel
theoretical results on the existence and nature of symmetries relevant for
control systems such as reinforcement learning settings. Moreover, we
empirically evaluate our method on several benchmark offline reinforcement
learning tasks and datasets including D4RL, Metaworld and Robosuite and find
that by using our framework we consistently improve the state-of-the-art for
Q-learning methods.",arxiv
http://arxiv.org/abs/2110.14007v1,2021-10-26T20:25:55Z,2021-10-26T20:25:55Z,TOD: Tensor-based Outlier Detection,"To scale outlier detection (OD) to large-scale, high-dimensional datasets, we
propose TOD, a novel system that abstracts OD algorithms into basic tensor
operations for efficient GPU acceleration. To make TOD highly efficient in both
time and space, we leverage recent advances in deep learning infrastructure in
both hardware and software. To deploy large OD applications on GPUs with
limited memory, we introduce two key techniques. First, provable quantization
accelerates OD computation and reduces the memory requirement by performing
specific OD computations in lower precision while provably guaranteeing no
accuracy loss. Second, to exploit the aggregated compute resources and memory
capacity of multiple GPUs, we introduce automatic batching, which decomposes OD
computations into small batches that can be executed on multiple GPUs in
parallel.
  TOD supports a comprehensive set of OD algorithms and utility functions.
Extensive evaluation on both real and synthetic OD datasets shows that TOD is
on average 11.9X faster than the state-of-the-art comprehensive OD system PyOD,
and takes less than an hour to detect outliers within a million samples. TOD
enables straightforward integration for additional OD algorithms and provides a
unified framework for combining classical OD algorithms with deep learning
methods. These combinations result in an infinite number of OD methods, many of
which are novel and can be easily prototyped in TOD.",arxiv
http://arxiv.org/abs/2111.00345v2,2021-11-08T17:49:28Z,2021-10-26T00:21:15Z,Multi-Agent Advisor Q-Learning,"In the last decade, there have been significant advances in multi-agent
reinforcement learning (MARL) but there are still numerous challenges, such as
high sample complexity and slow convergence to stable policies, that need to be
overcome before wide-spread deployment is possible. However, many real-world
environments already, in practice, deploy sub-optimal or heuristic approaches
for generating policies. An interesting question which arises is how to best
use such approaches as advisors to help improve reinforcement learning in
multi-agent domains. In this paper, we provide a principled framework for
incorporating action recommendations from online sub-optimal advisors in
multi-agent settings. We describe the problem of ADvising Multiple Intelligent
Reinforcement Agents (ADMIRAL) in nonrestrictive general-sum stochastic game
environments and present two novel Q-learning based algorithms: ADMIRAL -
Decision Making (ADMIRAL-DM) and ADMIRAL - Advisor Evaluation (ADMIRAL-AE),
which allow us to improve learning by appropriately incorporating advice from
an advisor (ADMIRAL-DM), and evaluate the effectiveness of an advisor
(ADMIRAL-AE). We analyze the algorithms theoretically and provide fixed-point
guarantees regarding their learning in general-sum stochastic games.
Furthermore, extensive experiments illustrate that these algorithms: can be
used in a variety of environments, have performances that compare favourably to
other related baselines, can scale to large state-action spaces, and are robust
to poor advice from advisors.",arxiv
http://arxiv.org/abs/2110.13650v1,2021-10-25T15:09:10Z,2021-10-25T15:09:10Z,GANash -- A GAN approach to steganography,"Data security is of the utmost concern of a communication system. Since the
early days, many developments have been made to improve the performance of the
system. PSNR of the received signal, secure transmission channel, quality of
encoding used, etc. are some of the key attributes of a good system. To ensure
security, the most commonly used technique is cryptography in which the message
is altered with respect to a key and using the same, the encoded message is
decoded at the receiver side. A complementary technique that is popularly used
to insure security is steganography. The advancements in Artificial
Intelligence(AI) have paved way for performing steganography in an intelligent,
tamper-proof manner. The recent discovery by researchers in the field of Deep
Learning(DL), an unsupervised learning network known as the Generative
Adversarial Networks(GAN) has improved the performance of this technique
exponentially. It has been demonstrated that deep neural networks are highly
sensitive to tiny perturbations of input data, giving rise to adversarial
examples. Though this property is usually considered a weakness of learned
models, it could be beneficial if used appropriately. The work that has been
accomplished by MIT for this purpose, a deep-neural model by the name of
SteganoGAN, has shown obligation for using this technique for steganography. In
this work, we have proposed a novel approach to improve the performance of the
existing system using latent space compression on the encoded data. This
theoretically would improve the performance exponentially. Thus, the algorithms
used to improve the system's performance and the results obtained have been
enunciated in this work. The results indicate the level of dominance this
system could achieve to be able to diminish the difficulties in solving
real-time problems in terms of security, deployment and database management.",arxiv
http://arxiv.org/abs/2110.10921v1,2021-10-21T06:26:31Z,2021-10-21T06:26:31Z,CATRO: Channel Pruning via Class-Aware Trace Ratio Optimization,"Deep convolutional neural networks are shown to be overkill with high
parametric and computational redundancy in many application scenarios, and an
increasing number of works have explored model pruning to obtain lightweight
and efficient networks. However, most existing pruning approaches are driven by
empirical heuristics and rarely consider the joint impact of channels, leading
to unguaranteed and suboptimal performance. In this paper, we propose a novel
channel pruning method via class-aware trace ratio optimization (CATRO) to
reduce the computational burden and accelerate the model inference. Utilizing
class information from a few samples, CATRO measures the joint impact of
multiple channels by feature space discriminations and consolidates the
layer-wise impact of preserved channels. By formulating channel pruning as a
submodular set function maximization problem, CATRO solves it efficiently via a
two-stage greedy iterative optimization procedure. More importantly, we present
theoretical justifications on convergence and performance of CATRO.
Experimental results demonstrate that CATRO achieves higher accuracy with
similar computation cost or lower computation cost with similar accuracy than
other state-of-the-art channel pruning algorithms. In addition, because of its
class-aware property, CATRO is suitable to prune efficient networks adaptively
for various classification subtasks, enhancing handy deployment and usage of
deep networks in real-world applications.",arxiv
http://arxiv.org/abs/2110.09236v1,2021-10-18T12:32:35Z,2021-10-18T12:32:35Z,"Model-Based Reinforcement Learning Framework of Online Network Resource
  Allocation","Online Network Resource Allocation (ONRA) for service provisioning is a
fundamental problem in communication networks. As a sequential decision-making
under uncertainty problem, it is promising to approach ONRA via Reinforcement
Learning (RL). But, RL solutions suffer from the sample complexity issue; i.e.,
a large number of interactions with the environment needed to find an efficient
policy. This is a barrier to utilize RL for ONRA as on one hand, it is not
practical to train the RL agent offline due to lack of information about future
requests, and on the other hand, online training in the real network leads to
significant performance loss because of the sub-optimal policy during the
prolonged learning time. This performance degradation is even higher in
non-stationary ONRA where the agent should continually adapt the policy with
the changes in service requests. To deal with this issue, we develop a general
resource allocation framework, named RADAR, using model-based RL for a class of
ONRA problems with the known immediate reward of each action. RADAR improves
sample efficiency via exploring the state space in the background and
exploiting the policy in the decision-time using synthetic samples by the model
of the environment, which is trained by real interactions. Applying RADAR on
the multi-domain service federation problem, to maximize profit via selecting
proper domains for service requests deployment, shows its continual learning
capability and up to 44% performance improvement w.r.t. the standard model-free
RL solution.",arxiv
http://arxiv.org/abs/2110.08732v1,2021-10-17T06:12:02Z,2021-10-17T06:12:02Z,A Deep Learning-based Approach for Real-time Facemask Detection,"The COVID-19 pandemic is causing a global health crisis. Public spaces need
to be safeguarded from the adverse effects of this pandemic. Wearing a facemask
becomes one of the effective protection solutions adopted by many governments.
Manual real-time monitoring of facemask wearing for a large group of people is
becoming a difficult task. The goal of this paper is to use deep learning (DL),
which has shown excellent results in many real-life applications, to ensure
efficient real-time facemask detection. The proposed approach is based on two
steps. An off-line step aiming to create a DL model that is able to detect and
locate facemasks and whether they are appropriately worn. An online step that
deploys the DL model at edge computing in order to detect masks in real-time.
In this study, we propose to use MobileNetV2 to detect facemask in real-time.
Several experiments are conducted and show good performances of the proposed
approach (99% for training and testing accuracy). In addition, several
comparisons with many state-of-the-art models namely ResNet50, DenseNet, and
VGG16 show good performance of the MobileNetV2 in terms of training time and
accuracy.",arxiv
http://arxiv.org/abs/2110.08307v1,2021-10-15T18:29:46Z,2021-10-15T18:29:46Z,GrowSpace: Learning How to Shape Plants,"Plants are dynamic systems that are integral to our existence and survival.
Plants face environment changes and adapt over time to their surrounding
conditions. We argue that plant responses to an environmental stimulus are a
good example of a real-world problem that can be approached within a
reinforcement learning (RL)framework. With the objective of controlling a plant
by moving the light source, we propose GrowSpace, as a new RL benchmark. The
back-end of the simulator is implemented using the Space Colonisation
Algorithm, a plant growing model based on competition for space. Compared to
video game RL environments, this simulator addresses a real-world problem and
serves as a test bed to visualize plant growth and movement in a faster way
than physical experiments. GrowSpace is composed of a suite of challenges that
tackle several problems such as control, multi-stage learning,fairness and
multi-objective learning. We provide agent baselines alongside case studies to
demonstrate the difficulty of the proposed benchmark.",arxiv
http://arxiv.org/abs/2111.09410v1,2021-10-14T14:06:57Z,2021-10-14T14:06:57Z,"EdgeML: Towards Network-Accelerated Federated Learning over Wireless
  Edge","Federated learning (FL) is a distributed machine learning technology for
next-generation AI systems that allows a number of workers, i.e., edge devices,
collaboratively learn a shared global model while keeping their data locally to
prevent privacy leakage. Enabling FL over wireless multi-hop networks can
democratize AI and make it accessible in a cost-effective manner. However, the
noisy bandwidth-limited multi-hop wireless connections can lead to delayed and
nomadic model updates, which significantly slows down the FL convergence speed.
To address such challenges, this paper aims to accelerate FL convergence over
wireless edge by optimizing the multi-hop federated networking performance. In
particular, the FL convergence optimization problem is formulated as a Markov
decision process (MDP). To solve such MDP, multi-agent reinforcement learning
(MA-RL) algorithms along with domain-specific action space refining schemes are
developed, which online learn the delay-minimum forwarding paths to minimize
the model exchange latency between the edge devices (i.e., workers) and the
remote server. To validate the proposed solutions, FedEdge is developed and
implemented, which is the first experimental framework in the literature for FL
over multi-hop wireless edge computing networks. FedEdge allows us to fast
prototype, deploy, and evaluate novel FL algorithms along with RL-based system
optimization methods in real wireless devices. Moreover, a physical
experimental testbed is implemented by customizing the widely adopted Linux
wireless routers and ML computing nodes.Finally, our experimentation results on
the testbed show that the proposed network-accelerated FL system can
practically and significantly improve FL convergence speed, compared to the FL
system empowered by the production-grade commercially available wireless
networking protocol, BATMAN-Adv.",arxiv
http://arxiv.org/abs/2110.06196v1,2021-10-12T17:49:46Z,2021-10-12T17:49:46Z,GraPE: fast and scalable Graph Processing and Embedding,"Graph Representation Learning methods have enabled a wide range of learning
problems to be addressed for data that can be represented in graph form.
Nevertheless, several real world problems in economy, biology, medicine and
other fields raised relevant scaling problems with existing methods and their
software implementation, due to the size of real world graphs characterized by
millions of nodes and billions of edges. We present GraPE, a software resource
for graph processing and random walk based embedding, that can scale with large
and high-degree graphs and significantly speed up-computation. GraPE comprises
specialized data structures, algorithms, and a fast parallel implementation
that displays everal orders of magnitude improvement in empirical space and
time complexity compared to state of the art software resources, with a
corresponding boost in the performance of machine learning methods for edge and
node label prediction and for the unsupervised analysis of graphs.GraPE is
designed to run on laptop and desktop computers, as well as on high performance
computing clusters",arxiv
http://arxiv.org/abs/2110.05606v2,2022-02-25T07:14:05Z,2021-10-11T20:57:01Z,"Nearest Subspace Search in The Signed Cumulative Distribution Transform
  Space for 1D Signal Classification","This paper presents a new method to classify 1D signals using the signed
cumulative distribution transform (SCDT). The proposed method exploits certain
linearization properties of the SCDT to render the problem easier to solve in
the SCDT space. The method uses the nearest subspace search technique in the
SCDT domain to provide a non-iterative, effective, and simple to implement
classification algorithm. Experiments show that the proposed technique
outperforms the state-of-the-art neural networks using a very low number of
training samples and is also robust to out-of-distribution examples on
simulated data. We also demonstrate the efficacy of the proposed technique in
real-world applications by applying it to an ECG classification problem. The
python code implementing the proposed classifier can be found in PyTransKit
(https://github.com/rohdelab/PyTransKit).",arxiv
http://arxiv.org/abs/2110.05098v1,2021-10-11T09:10:19Z,2021-10-11T09:10:19Z,SurroundNet: Towards Effective Low-Light Image Enhancement,"Although Convolution Neural Networks (CNNs) has made substantial progress in
the low-light image enhancement task, one critical problem of CNNs is the
paradox of model complexity and performance. This paper presents a novel
SurroundNet which only involves less than 150$K$ parameters (about 80-98
percent size reduction compared to SOTAs) and achieves very competitive
performance. The proposed network comprises several Adaptive Retinex Blocks
(ARBlock), which can be viewed as a novel extension of Single Scale Retinex in
feature space. The core of our ARBlock is an efficient illumination estimation
function called Adaptive Surround Function (ASF). It can be regarded as a
general form of surround functions and be implemented by convolution layers. In
addition, we also introduce a Low-Exposure Denoiser (LED) to smooth the
low-light image before the enhancement. We evaluate the proposed method on the
real-world low-light dataset. Experimental results demonstrate that the
superiority of our submitted SurroundNet in both performance and network
parameters against State-of-the-Art low-light image enhancement methods. Code
is available at https: github.com/ouc-ocean-group/SurroundNet.",arxiv
http://arxiv.org/abs/2110.04953v1,2021-10-11T01:23:07Z,2021-10-11T01:23:07Z,"Compact CNN Models for On-device Ocular-based User Recognition in Mobile
  Devices","A number of studies have demonstrated the efficacy of deep learning
convolutional neural network (CNN) models for ocular-based user recognition in
mobile devices. However, these high-performing networks have enormous space and
computational complexity due to the millions of parameters and computations
involved. These requirements make the deployment of deep learning models to
resource-constrained mobile devices challenging. To this end, only a handful of
studies based on knowledge distillation and patch-based models have been
proposed to obtain compact size CNN models for ocular recognition in the mobile
environment. In order to further advance the state-of-the-art, this study for
the first time evaluates five neural network pruning methods and compares them
with the knowledge distillation method for on-device CNN inference and mobile
user verification using ocular images. Subject-independent analysis on VISOB
and UPFR-Periocular datasets suggest the efficacy of layerwise magnitude-based
pruning at a compression rate of 8 for mobile ocular-based authentication using
ResNet50 as the base model. Further, comparison with the knowledge distillation
suggests the efficacy of knowledge distillation over pruning methods in terms
of verification accuracy and the real-time inference measured as deep feature
extraction time on five mobile devices, namely, iPhone 6, iPhone X, iPhone XR,
iPad Air 2 and iPad 7th Generation.",arxiv
http://arxiv.org/abs/2110.04003v2,2021-12-06T15:10:15Z,2021-10-08T09:59:12Z,Learning to Centralize Dual-Arm Assembly,"Robotic manipulators are widely used in modern manufacturing processes.
However, their deployment in unstructured environments remains an open problem.
To deal with the variety, complexity, and uncertainty of real-world
manipulation tasks, it is essential to develop a flexible framework with
reduced assumptions on the environment characteristics. In recent years,
reinforcement learning (RL) has shown great results for single-arm robotic
manipulation. However, research focusing on dual-arm manipulation is still
rare. From a classical control perspective, solving such tasks often involves
complex modeling of interactions between two manipulators and the objects
encountered in the tasks, as well as the two robots coupling at a control
level. Instead, in this work, we explore the applicability of model-free RL to
dual-arm assembly. As we aim to contribute towards an approach that is not
limited to dual-arm assembly, but dual-arm manipulation in general, we keep
modeling efforts at a minimum. Hence, to avoid modeling the interaction between
the two robots and the used assembly tools, we present a modular approach with
two decentralized single-arm controllers which are coupled using a single
centralized learned policy. We reduce modeling effort to a minimum by using
sparse rewards only. Our architecture enables successful assembly and simple
transfer from simulation to the real world. We demonstrate the effectiveness of
the framework on dual-arm peg-in-hole and analyze sample efficiency and success
rates for different action spaces. Moreover, we compare results on different
clearances and showcase disturbance recovery and robustness, when dealing with
position uncertainties. Finally we zero-shot transfer policies trained in
simulation to the real world and evaluate their performance.",arxiv
http://arxiv.org/abs/2110.03979v2,2021-12-23T14:39:28Z,2021-10-08T08:58:36Z,"MilliTRACE-IR: Contact Tracing and Temperature Screening via mm-Wave and
  Infrared Sensing","Social distancing and temperature screening have been widely employed to
counteract the COVID-19 pandemic, sparking great interest from academia,
industry and public administrations worldwide. While most solutions have dealt
with these aspects separately, their combination would greatly benefit the
continuous monitoring of public spaces and help trigger effective
countermeasures. This work presents milliTRACE-IR, a joint mmWave radar and
infrared imaging sensing system performing unobtrusive and privacy preserving
human body temperature screening and contact tracing in indoor spaces.
milliTRACE-IR combines, via a robust sensor fusion approach, mmWave radars and
infrared thermal cameras. It achieves fully automated measurement of distancing
and body temperature, by jointly tracking the subjects's faces in the thermal
camera image plane and the human motion in the radar reference system.
Moreover, milliTRACE-IR performs contact tracing: a person with high body
temperature is reliably detected by the thermal camera sensor and subsequently
traced across a large indoor area in a non-invasive way by the radars. When
entering a new room, a subject is re-identified among several other individuals
by computing gait-related features from the radar reflections through a deep
neural network and using a weighted extreme learning machine as the final
re-identification tool. Experimental results, obtained from a real
implementation of milliTRACE-IR, demonstrate decimeter-level accuracy in
distance/trajectory estimation, inter-personal distance estimation (effective
for subjects getting as close as 0.2 m), and accurate temperature monitoring
(max. errors of 0.5{\deg}C). Furthermore, milliTRACE-IR provides contact
tracing through highly accurate (95%) person re-identification, in less than 20
seconds.",arxiv
http://arxiv.org/abs/2110.05242v1,2021-10-08T06:35:20Z,2021-10-08T06:35:20Z,"Accelerating Multi-Objective Neural Architecture Search by Random-Weight
  Evaluation","For the goal of automated design of high-performance deep convolutional
neural networks (CNNs), Neural Architecture Search (NAS) methodology is
becoming increasingly important for both academia and industries.Due to the
costly stochastic gradient descent (SGD) training of CNNs for performance
evaluation, most existing NAS methods are computationally expensive for
real-world deployments. To address this issue, we first introduce a new
performance estimation metric, named Random-Weight Evaluation (RWE) to quantify
the quality of CNNs in a cost-efficient manner. Instead of fully training the
entire CNN, the RWE only trains its last layer and leaves the remainders with
randomly initialized weights, which results in a single network evaluation in
seconds.Second, a complexity metric is adopted for multi-objective NAS to
balance the model size and performance. Overall, our proposed method obtains a
set of efficient models with state-of-the-art performance in two real-world
search spaces. Then the results obtained on the CIFAR-10 dataset are
transferred to the ImageNet dataset to validate the practicality of the
proposed algorithm. Moreover, ablation studies on NAS-Bench-301 datasets reveal
the effectiveness of the proposed RWE in estimating the performance compared
with existing methods.",arxiv
http://arxiv.org/abs/2110.02718v1,2021-10-06T13:05:45Z,2021-10-06T13:05:45Z,Generalizing Neural Networks by Reflecting Deviating Data in Production,"Trained with a sufficiently large training and testing dataset, Deep Neural
Networks (DNNs) are expected to generalize. However, inputs may deviate from
the training dataset distribution in real deployments. This is a fundamental
issue with using a finite dataset. Even worse, real inputs may change over time
from the expected distribution. Taken together, these issues may lead deployed
DNNs to mis-predict in production.
  In this work, we present a runtime approach that mitigates DNN
mis-predictions caused by the unexpected runtime inputs to the DNN. In contrast
to previous work that considers the structure and parameters of the DNN itself,
our approach treats the DNN as a blackbox and focuses on the inputs to the DNN.
Our approach has two steps. First, it recognizes and distinguishes ""unseen""
semantically-preserving inputs. For this we use a distribution analyzer based
on the distance metric learned by a Siamese network. Second, our approach
transforms those unexpected inputs into inputs from the training set that are
identified as having similar semantics. We call this process input reflection
and formulate it as a search problem over the embedding space on the training
set. This embedding space is learned by a Quadruplet network as an auxiliary
model for the subject model to improve the generalization.
  We implemented a tool called InputReflector based on the above two-step
approach and evaluated it with experiments on three DNN models trained on
CIFAR-10, MNIST, and FMINST image datasets. The results show that
InputReflector can effectively distinguish inputs that retain semantics of the
distribution (e.g., blurred, brightened, contrasted, and zoomed images) and
out-of-distribution inputs from normal inputs.",arxiv
http://arxiv.org/abs/2110.05187v1,2021-10-06T06:19:30Z,2021-10-06T06:19:30Z,Clustering Plotted Data by Image Segmentation,"Clustering algorithms are one of the main analytical methods to detect
patterns in unlabeled data. Existing clustering methods typically treat samples
in a dataset as points in a metric space and compute distances to group
together similar points. In this paper, we present a wholly different way of
clustering points in 2-dimensional space, inspired by how humans cluster data:
by training neural networks to perform instance segmentation on plotted data.
Our approach, Visual Clustering, has several advantages over traditional
clustering algorithms: it is much faster than most existing clustering
algorithms (making it suitable for very large datasets), it agrees strongly
with human intuition for clusters, and it is by default hyperparameter free
(although additional steps with hyperparameters can be introduced for more
control of the algorithm). We describe the method and compare it to ten other
clustering methods on synthetic data to illustrate its advantages and
disadvantages. We then demonstrate how our approach can be extended to higher
dimensional data and illustrate its performance on real-world data. The
implementation of Visual Clustering is publicly available and can be applied to
any dataset in a few lines of code.",arxiv
http://arxiv.org/abs/2110.00840v1,2021-10-02T16:52:28Z,2021-10-02T16:52:28Z,"Induction, Popper, and machine learning","Francis Bacon popularized the idea that science is based on a process of
induction by which repeated observations are, in some unspecified way,
generalized to theories based on the assumption that the future resembles the
past. This idea was criticized by Hume and others as untenable leading to the
famous problem of induction. It wasn't until the work of Karl Popper that this
problem was solved, by demonstrating that induction is not the basis for
science and that the development of scientific knowledge is instead based on
the same principles as biological evolution. Today, machine learning is also
taught as being rooted in induction from big data. Solomonoff induction
implemented in an idealized Bayesian agent (Hutter's AIXI) is widely discussed
and touted as a framework for understanding AI algorithms, even though
real-world attempts to implement something like AIXI immediately encounter
fatal problems. In this paper, we contrast frameworks based on induction with
Donald T. Campbell's universal Darwinism. We show that most AI algorithms in
use today can be understood as using an evolutionary trial and error process
searching over a solution space. In this work we argue that a universal
Darwinian framework provides a better foundation for understanding AI systems.
Moreover, at a more meta level the process of development of all AI algorithms
can be understood under the framework of universal Darwinism.",arxiv
http://arxiv.org/abs/2110.00330v2,2022-01-26T11:45:33Z,2021-10-01T11:47:56Z,"Discovering Boundary Values of Feature-based Machine Learning
  Classifiers through Exploratory Datamorphic Testing","Testing has been widely recognised as difficult for AI applications. This
paper proposes a set of testing strategies for testing machine learning
applications in the framework of the datamorphism testing methodology. In these
strategies, testing aims at exploring the data space of a classification or
clustering application to discover the boundaries between classes that the
machine learning application defines. This enables the tester to understand
precisely the behaviour and function of the software under test. In the paper,
three variants of exploratory strategies are presented with the algorithms
implemented in the automated datamorphic testing tool Morphy. The correctness
of these algorithms are formally proved. Their capability and cost of
discovering borders between classes are evaluated via a set of controlled
experiments with manually designed subjects and a set of case studies with real
machine learning models.",arxiv
http://arxiv.org/abs/2110.00218v2,2021-10-09T21:44:45Z,2021-10-01T05:19:32Z,"On the Importance of Gradients for Detecting Distributional Shifts in
  the Wild","Detecting out-of-distribution (OOD) data has become a critical component in
ensuring the safe deployment of machine learning models in the real world.
Existing OOD detection approaches primarily rely on the output or feature space
for deriving OOD scores, while largely overlooking information from the
gradient space. In this paper, we present GradNorm, a simple and effective
approach for detecting OOD inputs by utilizing information extracted from the
gradient space. GradNorm directly employs the vector norm of gradients,
backpropagated from the KL divergence between the softmax output and a uniform
probability distribution. Our key idea is that the magnitude of gradients is
higher for in-distribution (ID) data than that for OOD data, making it
informative for OOD detection. GradNorm demonstrates superior performance,
reducing the average FPR95 by up to 16.33% compared to the previous best
method.",arxiv
http://arxiv.org/abs/2109.13570v2,2022-03-03T13:22:21Z,2021-09-28T09:00:55Z,"Adaptive Informative Path Planning Using Deep Reinforcement Learning for
  UAV-based Active Sensing","Aerial robots are increasingly being utilized for environmental monitoring
and exploration. However, a key challenge is efficiently planning paths to
maximize the information value of acquired data as an initially unknown
environment is explored. To address this, we propose a new approach for
informative path planning based on deep reinforcement learning (RL). Combining
recent advances in RL and robotic applications, our method combines tree search
with an offline-learned neural network predicting informative sensing actions.
We introduce several components making our approach applicable for robotic
tasks with high-dimensional state and large action spaces. By deploying the
trained network during a mission, our method enables sample-efficient online
replanning on platforms with limited computational resources. Simulations show
that our approach performs on par with existing methods while reducing runtime
by 8-10x. We validate its performance using real-world surface temperature
data.",arxiv
http://arxiv.org/abs/2109.13521v2,2021-11-23T05:34:54Z,2021-09-28T06:49:40Z,"A multi-stage semi-supervised improved deep embedded clustering method
  for bearing fault diagnosis under the situation of insufficient labeled
  samples","Although data-driven fault diagnosis methods have been widely applied,
massive labeled data are required for model training. However, a difficulty of
implementing this in real industries hinders the application of these methods.
Hence, an effective diagnostic approach that can work well in such situation is
urgently needed.In this study, a multi-stage semi-supervised improved deep
embedded clustering (MS-SSIDEC) method, which combines semi-supervised learning
with improved deep embedded clustering (IDEC), is proposed to jointly explore
scarce labeled data and massive unlabeled data. In the first stage, a
skip-connection-based convolutional auto-encoder (SCCAE) that can automatically
map the unlabeled data into a low-dimensional feature space is proposed and
pre-trained to be a fault feature extractor. In the second stage, a
semi-supervised improved deep embedded clustering (SSIDEC) network is proposed
for clustering. It is first initialized with available labeled data and then
used to simultaneously optimize the clustering label assignment and make the
feature space to be more clustering-friendly. To tackle the phenomenon of
overfitting, virtual adversarial training (VAT) is introduced as a
regularization term in this stage. In the third stage, pseudo labels are
obtained by the high-quality results of SSIDEC. The labeled dataset can be
augmented by these pseudo-labeled data and then leveraged to train a bearing
fault diagnosis model. Two public datasets of vibration data from rolling
bearings are used to evaluate the performance of the proposed method.
Experimental results indicate that the proposed method achieves a promising
performance in both semi-supervised and unsupervised fault diagnosis tasks.
This method provides a new approach for fault diagnosis under the situation of
limited labeled samples by effectively exploring unsupervised data.",arxiv
http://arxiv.org/abs/2109.10719v2,2021-09-27T10:56:50Z,2021-09-22T13:22:40Z,Autonomous Blimp Control using Deep Reinforcement Learning,"Aerial robot solutions are becoming ubiquitous for an increasing number of
tasks. Among the various types of aerial robots, blimps are very well suited to
perform long-duration tasks while being energy efficient, relatively silent and
safe. To address the blimp navigation and control task, in our recent work, we
have developed a software-in-the-loop simulation and a PID-based controller for
large blimps in the presence of wind disturbance. However, blimps have a
deformable structure and their dynamics are inherently non-linear and
time-delayed, often resulting in large trajectory tracking errors. Moreover,
the buoyancy of a blimp is constantly changing due to changes in the ambient
temperature and pressure. In the present paper, we explore a deep reinforcement
learning (DRL) approach to address these issues. We train only in simulation,
while keeping conditions as close as possible to the real-world scenario. We
derive a compact state representation to reduce the training time and a
discrete action space to enforce control smoothness. Our initial results in
simulation show a significant potential of DRL in solving the blimp control
task and robustness against moderate wind and parameter uncertainty. Extensive
experiments are presented to study the robustness of our approach. We also
openly provide the source code of our approach.",arxiv
http://arxiv.org/abs/2109.08642v1,2021-09-17T16:52:03Z,2021-09-17T16:52:03Z,Efficient State Representation Learning for Dynamic Robotic Scenarios,"While the rapid progress of deep learning fuels end-to-end reinforcement
learning (RL), direct application, especially in high-dimensional space like
robotic scenarios still suffers from high sample efficiency. Therefore State
Representation Learning (SRL) is proposed to specifically learn to encode
task-relevant features from complex sensory data into low-dimensional states.
However, the pervasive implementation of SRL is usually conducted by a
decoupling strategy in which the observation-state mapping is learned
separately, which is prone to over-fit. To handle such problem, we present a
new algorithm called Policy Optimization via Abstract Representation which
integrates SRL into the original RL scale. Firstly, We engage RL loss to assist
in updating SRL model so that the states can evolve to meet the demand of
reinforcement learning and maintain a good physical interpretation. Secondly,
we introduce a dynamic parameter adjustment mechanism so that both models can
efficiently adapt to each other. Thirdly, we introduce a new prior called
domain resemblance to leverage expert demonstration to train the SRL model.
Finally, we provide a real-time access by state graph to monitor the course of
learning. Results show that our algorithm outperforms the PPO baselines and
decoupling strategies in terms of sample efficiency and final rewards. Thus our
model can efficiently deal with tasks in high dimensions and facilitate
training real-life robots directly from scratch.",arxiv
http://arxiv.org/abs/2109.08267v2,2021-12-22T13:33:39Z,2021-09-17T01:02:27Z,"CompilerGym: Robust, Performant Compiler Optimization Environments for
  AI Research","Interest in applying Artificial Intelligence (AI) techniques to compiler
optimizations is increasing rapidly, but compiler research has a high entry
barrier. Unlike in other domains, compiler and AI researchers do not have
access to the datasets and frameworks that enable fast iteration and
development of ideas, and getting started requires a significant engineering
investment. What is needed is an easy, reusable experimental infrastructure for
real world compiler optimization tasks that can serve as a common benchmark for
comparing techniques, and as a platform to accelerate progress in the field.
  We introduce CompilerGym, a set of environments for real world compiler
optimization tasks, and a toolkit for exposing new optimization tasks to
compiler researchers. CompilerGym enables anyone to experiment on production
compiler optimization problems through an easy-to-use package, regardless of
their experience with compilers. We build upon the popular OpenAI Gym interface
enabling researchers to interact with compilers using Python and a familiar
API.
  We describe the CompilerGym architecture and implementation, characterize the
optimization spaces and computational efficiencies of three included compiler
environments, and provide extensive empirical evaluations. Compared to prior
works, CompilerGym offers larger datasets and optimization spaces, is 27x more
computationally efficient, is fault-tolerant, and capable of detecting
reproducibility bugs in the underlying compilers.
  In making it easy for anyone to experiment with compilers - irrespective of
their background - we aim to accelerate progress in the AI and compiler
research domains.",arxiv
http://arxiv.org/abs/2109.07827v1,2021-09-16T09:36:53Z,2021-09-16T09:36:53Z,"Enabling risk-aware Reinforcement Learning for medical interventions
  through uncertainty decomposition","Reinforcement Learning (RL) is emerging as tool for tackling complex control
and decision-making problems. However, in high-risk environments such as
healthcare, manufacturing, automotive or aerospace, it is often challenging to
bridge the gap between an apparently optimal policy learnt by an agent and its
real-world deployment, due to the uncertainties and risk associated with it.
Broadly speaking RL agents face two kinds of uncertainty, 1. aleatoric
uncertainty, which reflects randomness or noise in the dynamics of the world,
and 2. epistemic uncertainty, which reflects the bounded knowledge of the agent
due to model limitations and finite amount of information/data the agent has
acquired about the world. These two types of uncertainty carry fundamentally
different implications for the evaluation of performance and the level of risk
or trust. Yet these aleatoric and epistemic uncertainties are generally
confounded as standard and even distributional RL is agnostic to this
difference. Here we propose how a distributional approach (UA-DQN) can be
recast to render uncertainties by decomposing the net effects of each
uncertainty. We demonstrate the operation of this method in grid world examples
to build intuition and then show a proof of concept application for an RL agent
operating as a clinical decision support system in critical care",arxiv
http://arxiv.org/abs/2108.13680v2,2021-09-07T01:18:16Z,2021-08-31T08:37:58Z,Learning Practically Feasible Policies for Online 3D Bin Packing,"We tackle the Online 3D Bin Packing Problem, a challenging yet practically
useful variant of the classical Bin Packing Problem. In this problem, the items
are delivered to the agent without informing the full sequence information.
Agent must directly pack these items into the target bin stably without
changing their arrival order, and no further adjustment is permitted. Online
3D-BPP can be naturally formulated as Markov Decision Process (MDP). We adopt
deep reinforcement learning, in particular, the on-policy actor-critic
framework, to solve this MDP with constrained action space. To learn a
practically feasible packing policy, we propose three critical designs. First,
we propose an online analysis of packing stability based on a novel stacking
tree. It attains a high analysis accuracy while reducing the computational
complexity from $O(N^2)$ to $O(N \log N)$, making it especially suited for RL
training. Second, we propose a decoupled packing policy learning for different
dimensions of placement which enables high-resolution spatial discretization
and hence high packing precision. Third, we introduce a reward function that
dictates the robot to place items in a far-to-near order and therefore
simplifies the collision avoidance in movement planning of the robotic arm.
Furthermore, we provide a comprehensive discussion on several key implemental
issues. The extensive evaluation demonstrates that our learned policy
outperforms the state-of-the-art methods significantly and is practically
usable for real-world applications.",arxiv
http://arxiv.org/abs/2108.12704v1,2021-08-28T20:39:54Z,2021-08-28T20:39:54Z,"Compact representations of convolutional neural networks via weight
  pruning and quantization","The state-of-the-art performance for several real-world problems is currently
reached by convolutional neural networks (CNN). Such learning models exploit
recent results in the field of deep learning, typically leading to highly
performing, yet very large neural networks with (at least) millions of
parameters. As a result, the deployment of such models is not possible when
only small amounts of RAM are available, or in general within resource-limited
platforms, and strategies to compress CNNs became thus of paramount importance.
In this paper we propose a novel lossless storage format for CNNs based on
source coding and leveraging both weight pruning and quantization. We
theoretically derive the space upper bounds for the proposed structures,
showing their relationship with both sparsity and quantization levels of the
weight matrices. Both compression rates and excution times have been tested
against reference methods for matrix compression, and an empirical evaluation
of state-of-the-art quantization schemes based on weight sharing is also
discussed, to assess their impact on the performance when applied to both
convolutional and fully connected layers. On four benchmarks for classification
and regression problems and comparing to the baseline pre-trained uncompressed
network, we achieved a reduction of space occupancy up to 0.6% on fully
connected layers and 5.44% on the whole network, while performing at least as
competitive as the baseline.",arxiv
http://arxiv.org/abs/2108.08631v2,2021-08-22T04:44:25Z,2021-08-19T11:51:36Z,"Determinant-free fermionic wave function using feed-forward neural
  networks","We propose a general framework for finding the ground state of many-body
fermionic systems by using feed-forward neural networks. The anticommutation
relation for fermions is usually implemented to a variational wave function by
the Slater determinant (or Pfaffian), which is a computational bottleneck
because of the numerical cost of $O(N^3)$ for $N$ particles. We bypass this
bottleneck by explicitly calculating the sign changes associated with particle
exchanges in real space and using fully connected neural networks for
optimizing the rest parts of the wave function. This reduces the computational
cost to $O(N^2)$ or less. We show that the accuracy of the approximation can be
improved by optimizing the ""variance"" of the energy simultaneously with the
energy itself. We also find that a reweighting method in Monte Carlo sampling
can stabilize the calculation. These improvements can be applied to other
approaches based on variational Monte Carlo methods. Moreover, we show that the
accuracy can be further improved by using the symmetry of the system, the
representative states, and an additional neural network implementing a
generalized Gutzwiller-Jastrow factor. We demonstrate the efficiency of the
method by applying it to a two-dimensional Hubbard model.",arxiv
http://arxiv.org/abs/2108.13475v1,2021-08-18T13:39:50Z,2021-08-18T13:39:50Z,"An Analysis Of Entire Space Multi-Task Models For Post-Click Conversion
  Prediction","Industrial recommender systems are frequently tasked with approximating
probabilities for multiple, often closely related, user actions. For example,
predicting if a user will click on an advertisement and if they will then
purchase the advertised product. The conceptual similarity between these tasks
has promoted the use of multi-task learning: a class of algorithms that aim to
bring positive inductive transfer from related tasks. Here, we empirically
evaluate multi-task learning approaches with neural networks for an online
advertising task. Specifically, we consider approximating the probability of
post-click conversion events (installs) (CVR) for mobile app advertising on a
large-scale advertising platform, using the related click events (CTR) as an
auxiliary task. We use an ablation approach to systematically study recent
approaches that incorporate both multitask learning and ""entire space modeling""
which train the CVR on all logged examples rather than learning a conditional
likelihood of conversion given clicked. Based on these results we show that
several different approaches result in similar levels of positive transfer from
the data-abundant CTR task to the CVR task and offer some insight into how the
multi-task design choices address the two primary problems affecting the CVR
task: data sparsity and data bias. Our findings add to the growing body of
evidence suggesting that standard multi-task learning is a sensible approach to
modelling related events in real-world large-scale applications and suggest the
specific multitask approach can be guided by ease of implementation in an
existing system.",arxiv
http://arxiv.org/abs/2109.00889v1,2021-08-17T00:35:43Z,2021-08-17T00:35:43Z,"Dealing with Distribution Mismatch in Semi-supervised Deep Learning for
  Covid-19 Detection Using Chest X-ray Images: A Novel Approach Using Feature
  Densities","In the context of the global coronavirus pandemic, different deep learning
solutions for infected subject detection using chest X-ray images have been
proposed. However, deep learning models usually need large labelled datasets to
be effective. Semi-supervised deep learning is an attractive alternative, where
unlabelled data is leveraged to improve the overall model's accuracy. However,
in real-world usage settings, an unlabelled dataset might present a different
distribution than the labelled dataset (i.e. the labelled dataset was sampled
from a target clinic and the unlabelled dataset from a source clinic). This
results in a distribution mismatch between the unlabelled and labelled
datasets. In this work, we assess the impact of the distribution mismatch
between the labelled and the unlabelled datasets, for a semi-supervised model
trained with chest X-ray images, for COVID-19 detection. Under strong
distribution mismatch conditions, we found an accuracy hit of almost 30\%,
suggesting that the unlabelled dataset distribution has a strong influence in
the behaviour of the model. Therefore, we propose a straightforward approach to
diminish the impact of such distribution mismatch. Our proposed method uses a
density approximation of the feature space. It is built upon the target dataset
to filter out the observations in the source unlabelled dataset that might harm
the accuracy of the semi-supervised model. It assumes that a small labelled
source dataset is available together with a larger source unlabelled dataset.
Our proposed method does not require any model training, it is simple and
computationally cheap. We compare our proposed method against two popular state
of the art out-of-distribution data detectors, which are also cheap and simple
to implement. In our tests, our method yielded accuracy gains of up to 32\%,
when compared to the previous state of the art methods.",arxiv
http://arxiv.org/abs/2108.06771v2,2021-08-29T06:51:53Z,2021-08-15T16:00:06Z,"NPBDREG: A Non-parametric Bayesian Deep-Learning Based Approach for
  Diffeomorphic Brain MRI Registration","Quantification of uncertainty in deep-neural-networks (DNN) based image
registration algorithms plays an important role in the safe deployment of
real-world medical applications and research-oriented processing pipelines, and
in improving generalization capabilities. Currently available approaches for
uncertainty estimation, including the variational encoder-decoder architecture
and the inference-time dropout approach, require specific network architectures
and assume parametric distribution of the latent space which may result in
sub-optimal characterization of the posterior distribution for the predicted
deformation-fields. We introduce the NPBDREG, a fully non-parametric Bayesian
framework for unsupervised DNN-based deformable image registration by combining
an \texttt{Adam} optimizer with stochastic gradient Langevin dynamics (SGLD) to
characterize the true posterior distribution through posterior sampling. The
NPBDREG provides a principled non-parametric way to characterize the true
posterior distribution, thus providing improved uncertainty estimates and
confidence measures in a theoretically well-founded and computationally
efficient way. We demonstrated the added-value of NPBDREG, compared to the
baseline probabilistic \texttt{VoxelMorph} unsupervised model (PrVXM), on brain
MRI images registration using $390$ image pairs from four publicly available
databases: MGH10, CMUC12, ISBR18 and LPBA40. The NPBDREG shows a slight
improvement in the registration accuracy compared to PrVXM (Dice score of
$0.73$ vs. $0.68$, $p \ll 0.01$), a better generalization capability for data
corrupted by a mixed structure noise (e.g Dice score of $0.729$ vs. $0.686$ for
$\alpha=0.2$) and last but foremost, a significantly better correlation of the
predicted uncertainty with out-of-distribution data ($r>0.95$ vs. $r<0.5$).",arxiv
http://arxiv.org/abs/2108.05713v1,2021-08-08T11:29:16Z,2021-08-08T11:29:16Z,Towards real-world navigation with deep differentiable planners,"We train embodied neural networks to plan and navigate unseen complex 3D
environments, emphasising real-world deployment. Rather than requiring prior
knowledge of the agent or environment, the planner learns to model the state
transitions and rewards. To avoid the potentially hazardous trial-and-error of
reinforcement learning, we focus on differentiable planners such as Value
Iteration Networks (VIN), which are trained offline from safe expert
demonstrations. Although they work well in small simulations, we address two
major limitations that hinder their deployment. First, we observed that current
differentiable planners struggle to plan long-term in environments with a high
branching complexity. While they should ideally learn to assign low rewards to
obstacles to avoid collisions, we posit that the constraints imposed on the
network are not strong enough to guarantee the network to learn sufficiently
large penalties for every possible collision. We thus impose a structural
constraint on the value iteration, which explicitly learns to model any
impossible actions. Secondly, we extend the model to work with a limited
perspective camera under translation and rotation, which is crucial for real
robot deployment. Many VIN-like planners assume a 360 degrees or overhead view
without rotation. In contrast, our method uses a memory-efficient lattice map
to aggregate CNN embeddings of partial observations, and models the rotational
dynamics explicitly using a 3D state-space grid (translation and rotation). Our
proposals significantly improve semantic navigation and exploration on several
2D and 3D environments, succeeding in settings that are otherwise challenging
for this class of methods. As far as we know, we are the first to successfully
perform differentiable planning on the difficult Active Vision Dataset,
consisting of real images captured from a robot.",arxiv
http://arxiv.org/abs/2108.03173v1,2021-08-04T09:03:53Z,2021-08-04T09:03:53Z,"Incremental learning of LSTM framework for sensor fusion in attitude
  estimation","This paper presents a novel method for attitude estimation of an object in 3D
space by incremental learning of the Long-Short Term Memory (LSTM) network.
Gyroscope, accelerometer, and magnetometer are few widely used sensors in
attitude estimation applications. Traditionally, multi-sensor fusion methods
such as the Extended Kalman Filter and Complementary Filter are employed to
fuse the measurements from these sensors. However, these methods exhibit
limitations in accounting for the uncertainty, unpredictability, and dynamic
nature of the motion in real-world situations. In this paper, the inertial
sensors data are fed to the LSTM network which are then updated incrementally
to incorporate the dynamic changes in motion occurring in the run time. The
robustness and efficiency of the proposed framework is demonstrated on the
dataset collected from a commercially available inertial measurement unit. The
proposed framework offers a significant improvement in the results compared to
the traditional method, even in the case of a highly dynamic environment. The
LSTM framework-based attitude estimation approach can be deployed on a standard
AI-supported processing module for real-time applications.",arxiv
http://arxiv.org/abs/2108.01846v2,2022-03-11T04:38:21Z,2021-08-04T04:59:05Z,"Learning Barrier Certificates: Towards Safe Reinforcement Learning with
  Zero Training-time Violations","Training-time safety violations have been a major concern when we deploy
reinforcement learning algorithms in the real world. This paper explores the
possibility of safe RL algorithms with zero training-time safety violations in
the challenging setting where we are only given a safe but trivial-reward
initial policy without any prior knowledge of the dynamics model and additional
offline data. We propose an algorithm, Co-trained Barrier Certificate for Safe
RL (CRABS), which iteratively learns barrier certificates, dynamics models, and
policies. The barrier certificates, learned via adversarial training, ensure
the policy's safety assuming calibrated learned dynamics model. We also add a
regularization term to encourage larger certified regions to enable better
exploration. Empirical simulations show that zero safety violations are already
challenging for a suite of simple environments with only 2-4 dimensional state
space, especially if high-reward policies have to visit regions near the safety
boundary. Prior methods require hundreds of violations to achieve decent
rewards on these tasks, whereas our proposed algorithms incur zero violations.",arxiv
http://arxiv.org/abs/2107.14796v1,2021-07-30T17:54:44Z,2021-07-30T17:54:44Z,Data-driven modeling of time-domain induced polarization,"We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.",arxiv
http://arxiv.org/abs/2107.11750v2,2021-07-30T08:42:18Z,2021-07-25T07:52:53Z,"Improving Variational Autoencoder based Out-of-Distribution Detection
  for Embedded Real-time Applications","Uncertainties in machine learning are a significant roadblock for its
application in safety-critical cyber-physical systems (CPS). One source of
uncertainty arises from distribution shifts in the input data between training
and test scenarios. Detecting such distribution shifts in real-time is an
emerging approach to address the challenge. The high dimensional input space in
CPS applications involving imaging adds extra difficulty to the task.
Generative learning models are widely adopted for the task, namely
out-of-distribution (OoD) detection. To improve the state-of-the-art, we
studied existing proposals from both machine learning and CPS fields. In the
latter, safety monitoring in real-time for autonomous driving agents has been a
focus. Exploiting the spatiotemporal correlation of motion in videos, we can
robustly detect hazardous motion around autonomous driving agents. Inspired by
the latest advances in the Variational Autoencoder (VAE) theory and practice,
we tapped into the prior knowledge in data to further boost OoD detection's
robustness. Comparison studies over nuScenes and Synthia data sets show our
methods significantly improve detection capabilities of OoD factors unique to
driving scenarios, 42% better than state-of-the-art approaches. Our model also
generalized near-perfectly, 97% better than the state-of-the-art across the
real-world and simulation driving data sets experimented. Finally, we
customized one proposed method into a twin-encoder model that can be deployed
to resource limited embedded devices for real-time OoD detection. Its execution
time was reduced over four times in low-precision 8-bit integer inference,
while detection capability is comparable to its corresponding floating-point
model.",arxiv
http://arxiv.org/abs/2107.11397v1,2021-07-23T18:00:15Z,2021-07-23T18:00:15Z,"Artificial Neural Networks for Galaxy Clustering. Learning from the
  two-point correlation function of BOSS galaxies","The increasingly large amount of cosmological data coming from ground-based
and space-borne telescopes requires highly efficient and fast enough data
analysis techniques to maximise the scientific exploitation. In this work, we
explore the capabilities of supervised machine learning algorithms to learn the
properties of the large-scale structure of the Universe, aiming at constraining
the matter density parameter, Omega m. We implement a new Artificial Neural
Network for a regression data analysis, and train it on a large set of galaxy
two-point correlation functions in standard cosmologies with different values
of Omega m. The training set is constructed from log-normal mock catalogues
which reproduce the clustering of the Baryon Oscillation Spectroscopic Survey
(BOSS) galaxies. The presented statistical method requires no specific
analytical model to construct the likelihood function, and runs with negligible
computational cost, after training. We test this new Artificial Neural Network
on real BOSS data, finding Omega m=0.309p/m0.008, which is remarkably
consistent with standard analysis results.",arxiv
http://arxiv.org/abs/2107.11003v1,2021-07-23T02:41:51Z,2021-07-23T02:41:51Z,"Model Selection for Offline Reinforcement Learning: Practical
  Considerations for Healthcare Settings","Reinforcement learning (RL) can be used to learn treatment policies and aid
decision making in healthcare. However, given the need for generalization over
complex state/action spaces, the incorporation of function approximators (e.g.,
deep neural networks) requires model selection to reduce overfitting and
improve policy performance at deployment. Yet a standard validation pipeline
for model selection requires running a learned policy in the actual
environment, which is often infeasible in a healthcare setting. In this work,
we investigate a model selection pipeline for offline RL that relies on
off-policy evaluation (OPE) as a proxy for validation performance. We present
an in-depth analysis of popular OPE methods, highlighting the additional
hyperparameters and computational requirements (fitting/inference of auxiliary
models) when used to rank a set of candidate policies. We compare the utility
of different OPE methods as part of the model selection pipeline in the context
of learning to treat patients with sepsis. Among all the OPE methods we
considered, fitted Q evaluation (FQE) consistently leads to the best validation
ranking, but at a high computational cost. To balance this trade-off between
accuracy of ranking and computational efficiency, we propose a simple two-stage
approach to accelerate model selection by avoiding potentially unnecessary
computation. Our work serves as a practical guide for offline RL model
selection and can help RL practitioners select policies using real-world
datasets. To facilitate reproducibility and future extensions, the code
accompanying this paper is available online at
https://github.com/MLD3/OfflineRL_ModelSelection.",arxiv
http://arxiv.org/abs/2107.08706v1,2021-07-19T09:30:52Z,2021-07-19T09:30:52Z,"Uncertainty-aware Cardinality Estimation by Neural Network Gaussian
  Process","Deep Learning (DL) has achieved great success in many real applications.
Despite its success, there are some main problems when deploying advanced DL
models in database systems, such as hyper-parameters tuning, the risk of
overfitting, and lack of prediction uncertainty. In this paper, we study
cardinality estimation for SQL queries with a focus on uncertainty, which we
believe is important in database systems when dealing with a large number of
user queries on various applications. With uncertainty ensured, instead of
trusting an estimator learned as it is, a query optimizer can explore other
options when the estimator learned has a large variance, and it also becomes
possible to update the estimator to improve its prediction in areas with high
uncertainty. The approach we explore is different from the direction of
deploying sophisticated DL models in database systems to build cardinality
estimators. We employ Bayesian deep learning (BDL), which serves as a bridge
between Bayesian inference and deep learning.The prediction distribution by BDL
provides principled uncertainty calibration for the prediction. In addition,
when the network width of a BDL model goes to infinity, the model performs
equivalent to Gaussian Process (GP). This special class of BDL, known as Neural
Network Gaussian Process (NNGP), inherits the advantages of Bayesian approach
while keeping universal approximation of neural network, and can utilize a much
larger model space to model distribution-free data as a nonparametric model. We
show that our uncertainty-aware NNGP estimator achieves high accuracy, can be
built very fast, and is robust to query workload shift, in our extensive
performance studies by comparing with the existing approaches.",arxiv
http://arxiv.org/abs/2107.08114v2,2021-07-26T17:00:47Z,2021-07-16T20:49:30Z,"Decentralized Multi-Agent Reinforcement Learning for Task Offloading
  Under Uncertainty","Multi-Agent Reinforcement Learning (MARL) is a challenging subarea of
Reinforcement Learning due to the non-stationarity of the environments and the
large dimensionality of the combined action space. Deep MARL algorithms have
been applied to solve different task offloading problems. However, in
real-world applications, information required by the agents (i.e. rewards and
states) are subject to noise and alterations. The stability and the robustness
of deep MARL to practical challenges is still an open research problem. In this
work, we apply state-of-the art MARL algorithms to solve task offloading with
reward uncertainty. We show that perturbations in the reward signal can induce
decrease in the performance compared to learning with perfect rewards. We
expect this paper to stimulate more research in studying and addressing the
practical challenges of deploying deep MARL solutions in wireless
communications systems.",arxiv
http://arxiv.org/abs/2107.07502v2,2021-11-10T07:31:56Z,2021-07-15T17:54:36Z,MultiBench: Multiscale Benchmarks for Multimodal Representation Learning,"Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. It is a challenging yet crucial area
with numerous real-world applications in multimedia, affective computing,
robotics, finance, human-computer interaction, and healthcare. Unfortunately,
multimodal research has seen limited resources to study (1) generalization
across domains and modalities, (2) complexity during training and inference,
and (3) robustness to noisy and missing modalities. In order to accelerate
progress towards understudied modalities and tasks while ensuring real-world
robustness, we release MultiBench, a systematic and unified large-scale
benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6
research areas. MultiBench provides an automated end-to-end machine learning
pipeline that simplifies and standardizes data loading, experimental setup, and
model evaluation. To enable holistic evaluation, MultiBench offers a
comprehensive methodology to assess (1) generalization, (2) time and space
complexity, and (3) modality robustness. MultiBench introduces impactful
challenges for future research, including scalability to large-scale multimodal
datasets and robustness to realistic imperfections. To accompany this
benchmark, we also provide a standardized implementation of 20 core approaches
in multimodal learning. Simply applying methods proposed in different research
areas can improve the state-of-the-art performance on 9/15 datasets. Therefore,
MultiBench presents a milestone in unifying disjoint efforts in multimodal
research and paves the way towards a better understanding of the capabilities
and limitations of multimodal models, all the while ensuring ease of use,
accessibility, and reproducibility. MultiBench, our standardized code, and
leaderboards are publicly available, will be regularly updated, and welcomes
inputs from the community.",arxiv
http://arxiv.org/abs/2107.04457v2,2021-11-16T07:44:54Z,2021-07-09T14:23:01Z,"Aligning an optical interferometer with beam divergence control and
  continuous action space","Reinforcement learning is finding its way to real-world problem application,
transferring from simulated environments to physical setups. In this work, we
implement vision-based alignment of an optical Mach-Zehnder interferometer with
a confocal telescope in one arm, which controls the diameter and divergence of
the corresponding beam. We use a continuous action space; exponential scaling
enables us to handle actions within a range of over two orders of magnitude.
Our agent trains only in a simulated environment with domain randomizations. In
an experimental evaluation, the agent significantly outperforms an existing
solution and a human expert.",arxiv
http://arxiv.org/abs/2107.01830v1,2021-07-05T07:37:24Z,2021-07-05T07:37:24Z,ARM-Net: Adaptive Relation Modeling Network for Structured Data,"Relational databases are the de facto standard for storing and querying
structured data, and extracting insights from structured data requires advanced
analytics. Deep neural networks (DNNs) have achieved super-human prediction
performance in particular data types, e.g., images. However, existing DNNs may
not produce meaningful results when applied to structured data. The reason is
that there are correlations and dependencies across combinations of attribute
values in a table, and these do not follow simple additive patterns that can be
easily mimicked by a DNN. The number of possible such cross features is
combinatorial, making them computationally prohibitive to model. Furthermore,
the deployment of learning models in real-world applications has also
highlighted the need for interpretability, especially for high-stakes
applications, which remains another issue of concern to DNNs.
  In this paper, we present ARM-Net, an adaptive relation modeling network
tailored for structured data, and a lightweight framework ARMOR based on
ARM-Net for relational data analytics. The key idea is to model feature
interactions with cross features selectively and dynamically, by first
transforming the input features into exponential space, and then determining
the interaction order and interaction weights adaptively for each cross
feature. We propose a novel sparse attention mechanism to dynamically generate
the interaction weights given the input tuple, so that we can explicitly model
cross features of arbitrary orders with noisy features filtered selectively.
Then during model inference, ARM-Net can specify the cross features being used
for each prediction for higher accuracy and better interpretability. Our
extensive experiments on real-world datasets demonstrate that ARM-Net
consistently outperforms existing models and provides more interpretable
predictions for data-driven decision making.",arxiv
http://arxiv.org/abs/2107.01427v1,2021-07-03T13:03:55Z,2021-07-03T13:03:55Z,Multi-Objective Congestion Control,"Decades of research on Internet congestion control (CC) has produced a
plethora of algorithms that optimize for different performance objectives.
Applications face the challenge of choosing the most suitable algorithm based
on their needs, and it takes tremendous efforts and expertise to customize CC
algorithms when new demands emerge. In this paper, we explore a basic question:
can we design a single CC algorithm to satisfy different objectives? We propose
MOCC, the first multi-objective congestion control algorithm that attempts to
address this challenge. The core of MOCC is a novel multi-objective
reinforcement learning framework for CC that can automatically learn the
correlations between different application requirements and the corresponding
optimal control policies. Under this framework, MOCC further applies transfer
learning to transfer the knowledge from past experience to new applications,
quickly adapting itself to a new objective even if it is unforeseen. We provide
both user-space and kernel-space implementation of MOCC. Real-world experiments
and extensive simulations show that MOCC well supports multi-objective,
competing or outperforming the best existing CC algorithms on individual
objectives, and quickly adapting to new applications (e.g., 14.2x faster than
prior work) without compromising old ones.",arxiv
http://arxiv.org/abs/2106.14739v1,2021-06-28T14:11:48Z,2021-06-28T14:11:48Z,"Real-Time Human Pose Estimation on a Smart Walker using Convolutional
  Neural Networks","Rehabilitation is important to improve quality of life for mobility-impaired
patients. Smart walkers are a commonly used solution that should embed
automatic and objective tools for data-driven human-in-the-loop control and
monitoring. However, present solutions focus on extracting few specific metrics
from dedicated sensors with no unified full-body approach. We investigate a
general, real-time, full-body pose estimation framework based on two RGB+D
camera streams with non-overlapping views mounted on a smart walker equipment
used in rehabilitation. Human keypoint estimation is performed using a
two-stage neural network framework. The 2D-Stage implements a detection module
that locates body keypoints in the 2D image frames. The 3D-Stage implements a
regression module that lifts and relates the detected keypoints in both cameras
to the 3D space relative to the walker. Model predictions are low-pass filtered
to improve temporal consistency. A custom acquisition method was used to obtain
a dataset, with 14 healthy subjects, used for training and evaluating the
proposed framework offline, which was then deployed on the real walker
equipment. An overall keypoint detection error of 3.73 pixels for the 2D-Stage
and 44.05mm for the 3D-Stage were reported, with an inference time of 26.6ms
when deployed on the constrained hardware of the walker. We present a novel
approach to patient monitoring and data-driven human-in-the-loop control in the
context of smart walkers. It is able to extract a complete and compact body
representation in real-time and from inexpensive sensors, serving as a common
base for downstream metrics extraction solutions, and Human-Robot interaction
applications. Despite promising results, more data should be collected on users
with impairments, to assess its performance as a rehabilitation tool in
real-world scenarios.",arxiv
http://arxiv.org/abs/2106.13842v2,2021-06-29T01:27:49Z,2021-06-25T18:43:23Z,"EARLIN: Early Out-of-Distribution Detection for Resource-efficient
  Collaborative Inference","Collaborative inference enables resource-constrained edge devices to make
inferences by uploading inputs (e.g., images) to a server (i.e., cloud) where
the heavy deep learning models run. While this setup works cost-effectively for
successful inferences, it severely underperforms when the model faces input
samples on which the model was not trained (known as Out-of-Distribution (OOD)
samples). If the edge devices could, at least, detect that an input sample is
an OOD, that could potentially save communication and computation resources by
not uploading those inputs to the server for inference workload. In this paper,
we propose a novel lightweight OOD detection approach that mines important
features from the shallow layers of a pretrained CNN model and detects an input
sample as ID (In-Distribution) or OOD based on a distance function defined on
the reduced feature space. Our technique (a) works on pretrained models without
any retraining of those models, and (b) does not expose itself to any OOD
dataset (all detection parameters are obtained from the ID training dataset).
To this end, we develop EARLIN (EARLy OOD detection for Collaborative
INference) that takes a pretrained model and partitions the model at the OOD
detection layer and deploys the considerably small OOD part on an edge device
and the rest on the cloud. By experimenting using real datasets and a prototype
implementation, we show that our technique achieves better results than other
approaches in terms of overall accuracy and cost when tested against popular
OOD datasets on top of popular deep learning models pretrained on benchmark
datasets.",arxiv
http://arxiv.org/abs/2106.10352v1,2021-06-18T20:54:18Z,2021-06-18T20:54:18Z,"Semi-supervised Optimal Transport with Self-paced Ensemble for
  Cross-hospital Sepsis Early Detection","The utilization of computer technology to solve problems in medical scenarios
has attracted considerable attention in recent years, which still has great
potential and space for exploration. Among them, machine learning has been
widely used in the prediction, diagnosis and even treatment of Sepsis. However,
state-of-the-art methods require large amounts of labeled medical data for
supervised learning. In real-world applications, the lack of labeled data will
cause enormous obstacles if one hospital wants to deploy a new Sepsis detection
system. Different from the supervised learning setting, we need to use known
information (e.g., from another hospital with rich labeled data) to help build
a model with acceptable performance, i.e., transfer learning. In this paper, we
propose a semi-supervised optimal transport with self-paced ensemble framework
for Sepsis early detection, called SPSSOT, to transfer knowledge from the other
that has rich labeled data. In SPSSOT, we first extract the same clinical
indicators from the source domain (e.g., hospital with rich labeled data) and
the target domain (e.g., hospital with little labeled data), then we combine
the semi-supervised domain adaptation based on optimal transport theory with
self-paced under-sampling to avoid a negative transfer possibly caused by
covariate shift and class imbalance. On the whole, SPSSOT is an end-to-end
transfer learning method for Sepsis early detection which can automatically
select suitable samples from two domains respectively according to the number
of iterations and align feature space of two domains. Extensive experiments on
two open clinical datasets demonstrate that comparing with other methods, our
proposed SPSSOT, can significantly improve the AUC values with only 1% labeled
data in the target domain in two transfer learning scenarios, MIMIC
$rightarrow$ Challenge and Challenge $rightarrow$ MIMIC.",arxiv
http://arxiv.org/abs/2106.09357v1,2021-06-17T10:20:45Z,2021-06-17T10:20:45Z,"Cat-like Jumping and Landing of Legged Robots in Low-gravity Using Deep
  Reinforcement Learning","In this article, we show that learned policies can be applied to solve legged
locomotion control tasks with extensive flight phases, such as those
encountered in space exploration. Using an off-the-shelf deep reinforcement
learning algorithm, we trained a neural network to control a jumping quadruped
robot while solely using its limbs for attitude control. We present tasks of
increasing complexity leading to a combination of three-dimensional
(re-)orientation and landing locomotion behaviors of a quadruped robot
traversing simulated low-gravity celestial bodies. We show that our approach
easily generalizes across these tasks and successfully trains policies for each
case. Using sim-to-real transfer, we deploy trained policies in the real world
on the SpaceBok robot placed on an experimental testbed designed for
two-dimensional micro-gravity experiments. The experimental results demonstrate
that repetitive, controlled jumping and landing with natural agility is
possible.",arxiv
http://arxiv.org/abs/2106.08437v1,2021-06-15T21:09:19Z,2021-06-15T21:09:19Z,Deep reinforcement learning on a multi-asset environment for trading,"Financial trading has been widely analyzed for decades with market
participants and academics always looking for advanced methods to improve
trading performance. Deep reinforcement learning (DRL), a recently
reinvigorated method with significant success in multiple domains, still has to
show its benefit in the financial markets. We use a deep Q-network (DQN) to
design long-short trading strategies for futures contracts. The state space
consists of volatility-normalized daily returns, with buying or selling being
the reinforcement learning action and the total reward defined as the
cumulative profits from our actions. Our trading strategy is trained and tested
both on real and simulated price series and we compare the results with an
index benchmark. We analyze how training based on a combination of artificial
data and actual price series can be successfully deployed in real markets. The
trained reinforcement learning agent is applied to trading the E-mini S&P 500
continuous futures contract. Our results in this study are preliminary and need
further improvement.",arxiv
http://arxiv.org/abs/2106.07178v4,2021-10-11T10:02:11Z,2021-06-14T06:04:57Z,A Comprehensive Survey on Graph Anomaly Detection with Deep Learning,"Anomalies represent rare observations (e.g., data records or events) that
deviate significantly from others. Over several decades, research on anomaly
mining has received increasing interests due to the implications of these
occurrences in a wide range of disciplines. Anomaly detection, which aims to
identify rare observations, is among the most vital tasks in the world, and has
shown its power in preventing detrimental events, such as financial fraud,
network intrusion, and social spam. The detection task is typically solved by
identifying outlying data points in the feature space and inherently overlooks
the relational information in real-world data. Graphs have been prevalently
used to represent the structural information, which raises the graph anomaly
detection problem - identifying anomalous graph objects (i.e., nodes, edges and
sub-graphs) in a single graph, or anomalous graphs in a database/set of graphs.
However, conventional anomaly detection techniques cannot tackle this problem
well because of the complexity of graph data. For the advent of deep learning,
graph anomaly detection with deep learning has received a growing attention
recently. In this survey, we aim to provide a systematic and comprehensive
review of the contemporary deep learning techniques for graph anomaly
detection. We compile open-sourced implementations, public datasets, and
commonly-used evaluation metrics to provide affluent resources for future
studies. More importantly, we highlight twelve extensive future research
directions according to our survey results covering unsolved and emerging
research problems and real-world applications. With this survey, our goal is to
create a ""one-stop-shop"" that provides a unified understanding of the problem
categories and existing approaches, publicly available hands-on resources, and
high-impact open challenges for graph anomaly detection using deep learning.",arxiv
http://arxiv.org/abs/2106.04941v1,2021-06-09T09:33:33Z,2021-06-09T09:33:33Z,Symmetric Spaces for Graph Embeddings: A Finsler-Riemannian Approach,"Learning faithful graph representations as sets of vertex embeddings has
become a fundamental intermediary step in a wide range of machine learning
applications. We propose the systematic use of symmetric spaces in
representation learning, a class encompassing many of the previously used
embedding targets. This enables us to introduce a new method, the use of
Finsler metrics integrated in a Riemannian optimization scheme, that better
adapts to dissimilar structures in the graph. We develop a tool to analyze the
embeddings and infer structural properties of the data sets. For
implementation, we choose Siegel spaces, a versatile family of symmetric
spaces. Our approach outperforms competitive baselines for graph reconstruction
tasks on various synthetic and real-world datasets. We further demonstrate its
applicability on two downstream tasks, recommender systems and node
classification.",arxiv
http://arxiv.org/abs/2106.04569v1,2021-06-08T17:58:10Z,2021-06-08T17:58:10Z,Simulated Adversarial Testing of Face Recognition Models,"Most machine learning models are validated and tested on fixed datasets. This
can give an incomplete picture of the capabilities and weaknesses of the model.
Such weaknesses can be revealed at test time in the real world. The risks
involved in such failures can be loss of profits, loss of time or even loss of
life in certain critical applications. In order to alleviate this issue,
simulators can be controlled in a fine-grained manner using interpretable
parameters to explore the semantic image manifold. In this work, we propose a
framework for learning how to test machine learning algorithms using simulators
in an adversarial manner in order to find weaknesses in the model before
deploying it in critical scenarios. We apply this model in a face recognition
scenario. We are the first to show that weaknesses of models trained on real
data can be discovered using simulated samples. Using our proposed method, we
can find adversarial synthetic faces that fool contemporary face recognition
models. This demonstrates the fact that these models have weaknesses that are
not measured by commonly used validation datasets. We hypothesize that this
type of adversarial examples are not isolated, but usually lie in connected
components in the latent space of the simulator. We present a method to find
these adversarial regions as opposed to the typical adversarial points found in
the adversarial example literature.",arxiv
http://arxiv.org/abs/2106.15301v1,2021-06-05T19:28:16Z,2021-06-05T19:28:16Z,"VolterraNet: A higher order convolutional network with group
  equivariance for homogeneous manifolds","Convolutional neural networks have been highly successful in image-based
learning tasks due to their translation equivariance property. Recent work has
generalized the traditional convolutional layer of a convolutional neural
network to non-Euclidean spaces and shown group equivariance of the generalized
convolution operation. In this paper, we present a novel higher order Volterra
convolutional neural network (VolterraNet) for data defined as samples of
functions on Riemannian homogeneous spaces. Analagous to the result for
traditional convolutions, we prove that the Volterra functional convolutions
are equivariant to the action of the isometry group admitted by the Riemannian
homogeneous spaces, and under some restrictions, any non-linear equivariant
function can be expressed as our homogeneous space Volterra convolution,
generalizing the non-linear shift equivariant characterization of Volterra
expansions in Euclidean space. We also prove that second order functional
convolution operations can be represented as cascaded convolutions which leads
to an efficient implementation. Beyond this, we also propose a dilated
VolterraNet model. These advances lead to large parameter reductions relative
to baseline non-Euclidean CNNs. To demonstrate the efficacy of the VolterraNet
performance, we present several real data experiments involving classification
tasks on spherical-MNIST, atomic energy, Shrec17 data sets, and group testing
on diffusion MRI data. Performance comparisons to the state-of-the-art are also
presented.",arxiv
http://arxiv.org/abs/2106.01503v1,2021-06-02T23:17:29Z,2021-06-02T23:17:29Z,Towards an Explanation Space to Align Humans and Explainable-AI Teamwork,"Providing meaningful and actionable explanations to end-users is a
fundamental prerequisite for implementing explainable intelligent systems in
the real world. Explainability is a situated interaction between a user and the
AI system rather than being static design principles. The content of
explanations is context-dependent and must be defined by evidence about the
user and its context. This paper seeks to operationalize this concept by
proposing a formative architecture that defines the explanation space from a
user-inspired perspective. The architecture comprises five intertwined
components to outline explanation requirements for a task: (1) the end-users
mental models, (2) the end-users cognitive process, (3) the user interface, (4)
the human-explainer agent, and the (5) agent process. We first define each
component of the architecture. Then we present the Abstracted Explanation
Space, a modeling tool that aggregates the architecture's components to support
designers in systematically aligning explanations with the end-users work
practices, needs, and goals. It guides the specifications of what needs to be
explained (content - end-users mental model), why this explanation is necessary
(context - end-users cognitive process), to delimit how to explain it (format -
human-explainer agent and user interface), and when should the explanations be
given. We then exemplify the tool's use in an ongoing case study in the
aircraft maintenance domain. Finally, we discuss possible contributions of the
tool, known limitations/areas for improvement, and future work to be done.",arxiv
http://arxiv.org/abs/2107.01002v2,2021-09-27T13:42:44Z,2021-05-31T12:09:46Z,"WiCluster: Passive Indoor 2D/3D Positioning using WiFi without Precise
  Labels","We introduce WiCluster, a new machine learning (ML) approach for passive
indoor positioning using radio frequency (RF) channel state information (CSI).
WiCluster can predict both a zone-level position and a precise 2D or 3D
position, without using any precise position labels during training. Prior
CSI-based indoor positioning work has relied on non-parametric approaches using
digital signal-processing (DSP) and, more recently, parametric approaches
(e.g., fully supervised ML methods). However these do not handle the complexity
of real-world environments well and do not meet requirements for large-scale
commercial deployments: the accuracy of DSP-based method deteriorates
significantly in non-line-of-sight conditions, while supervised ML methods need
large amounts of hard-to-acquire centimeter accuracy position labels. In
contrast, WiCluster is precise, requires weaker label-information that can be
easily collected, and works well in non-line-of-sight conditions. Our first
contribution is a novel dimensionality reduction method for charting. It
combines a triplet-loss with a multi-scale clustering-loss to map the
high-dimensional CSI representation to a 2D/3D latent space. Our second
contribution is two weakly supervised losses that map this latent space into a
Cartesian map, resulting in meter-accuracy position results. These losses only
require simple to acquire priors: a sketch of the floorplan, approximate
access-point locations and a few CSI packets that are labelled with the
corresponding zone in the floorplan. Thirdly, we report results and a
robustness study for 2D positioning in two single-floor office buildings and 3D
positioning in a two-story home.",arxiv
http://arxiv.org/abs/2105.14280v1,2021-05-29T12:04:27Z,2021-05-29T12:04:27Z,Hashing-Accelerated Graph Neural Networks for Link Prediction,"Networks are ubiquitous in the real world. Link prediction, as one of the key
problems for network-structured data, aims to predict whether there exists a
link between two nodes. The traditional approaches are based on the explicit
similarity computation between the compact node representation by embedding
each node into a low-dimensional space. In order to efficiently handle the
intensive similarity computation in link prediction, the hashing technique has
been successfully used to produce the node representation in the Hamming space.
However, the hashing-based link prediction algorithms face accuracy loss from
the randomized hashing techniques or inefficiency from the learning to hash
techniques in the embedding process. Currently, the Graph Neural Network (GNN)
framework has been widely applied to the graph-related tasks in an end-to-end
manner, but it commonly requires substantial computational resources and memory
costs due to massive parameter learning, which makes the GNN-based algorithms
impractical without the help of a powerful workhorse. In this paper, we propose
a simple and effective model called #GNN, which balances the trade-off between
accuracy and efficiency. #GNN is able to efficiently acquire node
representation in the Hamming space for link prediction by exploiting the
randomized hashing technique to implement message passing and capture
high-order proximity in the GNN framework. Furthermore, we characterize the
discriminative power of #GNN in probability. The extensive experimental results
demonstrate that the proposed #GNN algorithm achieves accuracy comparable to
the learning-based algorithms and outperforms the randomized algorithm, while
running significantly faster than the learning-based algorithms. Also, the
proposed algorithm shows excellent scalability on a large-scale network with
the limited resources.",arxiv
http://arxiv.org/abs/2105.13429v1,2021-05-27T20:05:41Z,2021-05-27T20:05:41Z,"Flow based features and validation metric for machine learning
  reconstruction of PIV data","Reconstruction of flow field from real sparse data by a physics-oriented
approach is a current challenge for fluid scientists in the AI community. The
problem includes feature recognition and implementation of AI algorithms that
link data to a physical feature space in order to produce reconstructed data.
The present article applies machine learning approach to study contribution of
different flow-based features with practical fluid mechanics applications for
reconstruction of the missing data of turbomachinery PIV measurements. Support
vector regression (SVR) and multi-layer perceptron (MLP) are selected as two
robust regressors capable of modelling non-linear fluid flow phenomena. The
proposed flow-based features are optimally scaled and filtered to extract the
best configuration. In addition to conventional data-based validation of the
regressors, a metric is proposed that reflects mass conservation law as an
important requirement for a physical flow reproduction. For a velocity field
including 25% of clustered missing data, the reconstruction accuracy achieved
by SVR in terms of R2-score is as high as 0.993 for the in-plane velocity
vectors in comparison with that obtained by MLP which is up to 0.981. In terms
of mass conservation metric, the SVR model by R2-score up to 0.96 is
considerably more accurate than the MLP estimator. For extremely sparse data
with a gappiness of 75%, vector and contour plots from SVR and MLP were
consistent with those of the original field.",arxiv
http://arxiv.org/abs/2105.13191v1,2021-05-26T14:04:04Z,2021-05-26T14:04:04Z,"Deep Learning Techniques for Compressive Sensing-Based Reconstruction
  and Inference -- A Ubiquitous Systems Perspective","Compressive sensing (CS) is a mathematically elegant tool for reducing the
sampling rate, potentially bringing context-awareness to a wider range of
devices. Nevertheless, practical issues with the sampling and reconstruction
algorithms prevent further proliferation of CS in real world domains,
especially among heterogeneous ubiquitous devices. Deep learning (DL) naturally
complements CS for adapting the sampling matrix, reconstructing the signal, and
learning form the compressed samples. While the CS-DL integration has received
substantial research interest recently, it has not yet been thoroughly
surveyed, nor has the light been shed on practical issues towards bringing the
CS-DL to real world implementations in the ubicomp domain. In this paper we
identify main possible ways in which CS and DL can interplay, extract key ideas
for making CS-DL efficient, identify major trends in CS-DL research space, and
derive guidelines for future evolution of CS-DL within the ubicomp domain.",arxiv
http://arxiv.org/abs/2105.12123v1,2021-05-25T09:45:59Z,2021-05-25T09:45:59Z,Photonic extreme learning machine by free-space optical propagation,"Photonic brain-inspired platforms are emerging as novel analog computing
devices, enabling fast and energy-efficient operations for machine learning.
These artificial neural networks generally require tailored optical elements,
such as integrated photonic circuits, engineered diffractive layers,
nanophotonic materials, or time-delay schemes, which are challenging to train
or stabilize. Here we present a neuromorphic photonic scheme - photonic extreme
learning machines - that can be implemented simply by using an optical encoder
and coherent wave propagation in free space. We realize the concept through
spatial light modulation of a laser beam, with the far field that acts as
feature mapping space. We experimentally demonstrated learning from data on
various classification and regression tasks, achieving accuracies comparable to
digital extreme learning machines. Our findings point out an optical machine
learning device that is easy-to-train, energetically efficient, scalable and
fabrication-constraint free. The scheme can be generalized to a plethora of
photonic systems, opening the route to real-time neuromorphic processing of
optical data.",arxiv
http://arxiv.org/abs/2105.11328v1,2021-05-24T15:05:58Z,2021-05-24T15:05:58Z,Room Clearance with Feudal Hierarchical Reinforcement Learning,"Reinforcement learning (RL) is a general framework that allows systems to
learn autonomously through trial-and-error interaction with their environment.
In recent years combining RL with expressive, high-capacity neural network
models has led to impressive performance in a diverse range of domains.
However, dealing with the large state and action spaces often required for
problems in the real world still remains a significant challenge. In this paper
we introduce a new simulation environment, ""Gambit"", designed as a tool to
build scenarios that can drive RL research in a direction useful for military
analysis. Using this environment we focus on an abstracted and simplified room
clearance scenario, where a team of blue agents have to make their way through
a building and ensure that all rooms are cleared of (and remain clear) of enemy
red agents. We implement a multi-agent version of feudal hierarchical RL that
introduces a command hierarchy where a commander at the higher level sends
orders to multiple agents at the lower level who simply have to learn to follow
these orders. We find that breaking the task down in this way allows us to
solve a number of non-trivial floorplans that require the coordination of
multiple agents much more efficiently than the standard baseline RL algorithms
we compare with. We then go on to explore how qualitatively different behaviour
can emerge depending on what we prioritise in the agent's reward function (e.g.
clearing the building quickly vs. prioritising rescuing civilians).",arxiv
http://arxiv.org/abs/2105.10907v1,2021-05-23T10:34:48Z,2021-05-23T10:34:48Z,"An Efficient Application of Neuroevolution for Competitive Multiagent
  Learning","Multiagent systems provide an ideal environment for the evaluation and
analysis of real-world problems using reinforcement learning algorithms. Most
traditional approaches to multiagent learning are affected by long training
periods as well as high computational complexity. NEAT (NeuroEvolution of
Augmenting Topologies) is a popular evolutionary strategy used to obtain the
best performing neural network architecture often used to tackle optimization
problems in the field of artificial intelligence. This paper utilizes the NEAT
algorithm to achieve competitive multiagent learning on a modified pong game
environment in an efficient manner. The competing agents abide by different
rules while having similar observation space parameters. The proposed algorithm
utilizes this property of the environment to define a singular
neuroevolutionary procedure that obtains the optimal policy for all the agents.
The compiled results indicate that the proposed implementation achieves ideal
behaviour in a very short training period when compared to existing multiagent
reinforcement learning models.",arxiv
http://arxiv.org/abs/2105.07615v2,2021-08-16T13:01:14Z,2021-05-17T05:30:41Z,Differentially Private Federated Knowledge Graphs Embedding,"Knowledge graph embedding plays an important role in knowledge
representation, reasoning, and data mining applications. However, for multiple
cross-domain knowledge graphs, state-of-the-art embedding models cannot make
full use of the data from different knowledge domains while preserving the
privacy of exchanged data. In addition, the centralized embedding model may not
scale to the extensive real-world knowledge graphs. Therefore, we propose a
novel decentralized scalable learning framework, \emph{Federated Knowledge
Graphs Embedding} (FKGE), where embeddings from different knowledge graphs can
be learnt in an asynchronous and peer-to-peer manner while being
privacy-preserving. FKGE exploits adversarial generation between pairs of
knowledge graphs to translate identical entities and relations of different
domains into near embedding spaces. In order to protect the privacy of the
training data, FKGE further implements a privacy-preserving neural network
structure to guarantee no raw data leakage. We conduct extensive experiments to
evaluate FKGE on 11 knowledge graphs, demonstrating a significant and
consistent improvement in model quality with at most 17.85\% and 7.90\%
increases in performance on triple classification and link prediction tasks.",arxiv
http://arxiv.org/abs/2105.06631v4,2021-09-15T14:46:28Z,2021-05-14T03:49:59Z,Ordering-Based Causal Discovery with Reinforcement Learning,"It is a long-standing question to discover causal relations among a set of
variables in many empirical sciences. Recently, Reinforcement Learning (RL) has
achieved promising results in causal discovery from observational data.
However, searching the space of directed graphs and enforcing acyclicity by
implicit penalties tend to be inefficient and restrict the existing RL-based
method to small scale problems. In this work, we propose a novel RL-based
approach for causal discovery, by incorporating RL into the ordering-based
paradigm. Specifically, we formulate the ordering search problem as a
multi-step Markov decision process, implement the ordering generating process
with an encoder-decoder architecture, and finally use RL to optimize the
proposed model based on the reward mechanisms designed for~each ordering. A
generated ordering would then be processed using variable selection to obtain
the final causal graph. We analyze the consistency and computational complexity
of the proposed method, and empirically show that a pretrained model can be
exploited to accelerate training. Experimental results on both synthetic and
real data sets shows that the proposed method achieves a much improved
performance over existing RL-based method.",arxiv
http://arxiv.org/abs/2105.01879v1,2021-05-05T05:58:29Z,2021-05-05T05:58:29Z,"MOS: Towards Scaling Out-of-distribution Detection for Large Semantic
  Space","Detecting out-of-distribution (OOD) inputs is a central challenge for safely
deploying machine learning models in the real world. Existing solutions are
mainly driven by small datasets, with low resolution and very few class labels
(e.g., CIFAR). As a result, OOD detection for large-scale image classification
tasks remains largely unexplored. In this paper, we bridge this critical gap by
proposing a group-based OOD detection framework, along with a novel OOD scoring
function termed MOS. Our key idea is to decompose the large semantic space into
smaller groups with similar concepts, which allows simplifying the decision
boundaries between in- vs. out-of-distribution data for effective OOD
detection. Our method scales substantially better for high-dimensional class
space than previous approaches. We evaluate models trained on ImageNet against
four carefully curated OOD datasets, spanning diverse semantics. MOS
establishes state-of-the-art performance, reducing the average FPR95 by 14.33%
while achieving 6x speedup in inference compared to the previous best method.",arxiv
http://arxiv.org/abs/2105.01571v1,2021-05-03T14:13:42Z,2021-05-03T14:13:42Z,"Effective Sparsification of Neural Networks with Global Sparsity
  Constraint","Weight pruning is an effective technique to reduce the model size and
inference time for deep neural networks in real-world deployments. However,
since magnitudes and relative importance of weights are very different for
different layers of a neural network, existing methods rely on either manual
tuning or handcrafted heuristic rules to find appropriate pruning rates
individually for each layer. This approach generally leads to suboptimal
performance. In this paper, by directly working on the probability space, we
propose an effective network sparsification method called {\it probabilistic
masking} (ProbMask), which solves a natural sparsification formulation under
global sparsity constraint. The key idea is to use probability as a global
criterion for all layers to measure the weight importance. An appealing feature
of ProbMask is that the amounts of weight redundancy can be learned
automatically via our constraint and thus we avoid the problem of tuning
pruning rates individually for different layers in a network. Extensive
experimental results on CIFAR-10/100 and ImageNet demonstrate that our method
is highly effective, and can outperform previous state-of-the-art methods by a
significant margin, especially in the high pruning rate situation. Notably, the
gap of Top-1 accuracy between our ProbMask and existing methods can be up to
10\%. As a by-product, we show ProbMask is also highly effective in identifying
supermasks, which are subnetworks with high performance in a randomly weighted
dense neural network.",arxiv
http://arxiv.org/abs/2104.14870v1,2021-04-30T09:53:28Z,2021-04-30T09:53:28Z,"Action in Mind: A Neural Network Approach to Action Recognition and
  Segmentation","Recognizing and categorizing human actions is an important task with
applications in various fields such as human-robot interaction, video analysis,
surveillance, video retrieval, health care system and entertainment industry.
This thesis presents a novel computational approach for human action
recognition through different implementations of multi-layer architectures
based on artificial neural networks. Each system level development is designed
to solve different aspects of the action recognition problem including online
real-time processing, action segmentation and the involvement of objects. The
analysis of the experimental results are illustrated and described in six
articles. The proposed action recognition architecture of this thesis is
composed of several processing layers including a preprocessing layer, an
ordered vector representation layer and three layers of neural networks. It
utilizes self-organizing neural networks such as Kohonen feature maps and
growing grids as the main neural network layers. Thus the architecture presents
a biological plausible approach with certain features such as topographic
organization of the neurons, lateral interactions, semi-supervised learning and
the ability to represent high dimensional input space in lower dimensional
maps. For each level of development the system is trained with the input data
consisting of consecutive 3D body postures and tested with generalized input
data that the system has never met before. The experimental results of
different system level developments show that the system performs well with
quite high accuracy for recognizing human actions.",arxiv
http://arxiv.org/abs/2104.12125v1,2021-04-25T10:33:35Z,2021-04-25T10:33:35Z,"Development of a Soft Actor Critic Deep Reinforcement Learning Approach
  for Harnessing Energy Flexibility in a Large Office Building","This research is concerned with the novel application and investigation of
`Soft Actor Critic' (SAC) based Deep Reinforcement Learning (DRL) to control
the cooling setpoint (and hence cooling loads) of a large commercial building
to harness energy flexibility. The research is motivated by the challenge
associated with the development and application of conventional model-based
control approaches at scale to the wider building stock. SAC is a model-free
DRL technique that is able to handle continuous action spaces and which has
seen limited application to real-life or high-fidelity simulation
implementations in the context of automated and intelligent control of building
energy systems. Such control techniques are seen as one possible solution to
supporting the operation of a smart, sustainable and future electrical grid.
This research tests the suitability of the SAC DRL technique through training
and deployment of the agent on an EnergyPlus based environment of the office
building. The SAC DRL was found to learn an optimal control policy that was
able to minimise energy costs by 9.7% compared to the default rule-based
control (RBC) scheme and was able to improve or maintain thermal comfort limits
over a test period of one week. The algorithm was shown to be robust to the
different hyperparameters and this optimal control policy was learnt through
the use of a minimal state space consisting of readily available variables. The
robustness of the algorithm was tested through investigation of the speed of
learning and ability to deploy to different seasons and climates. It was found
that the SAC DRL requires minimal training sample points and outperforms the
RBC after three months of operation and also without disruption to thermal
comfort during this period. The agent is transferable to other climates and
seasons although further retraining or hyperparameter tuning is recommended.",arxiv
http://arxiv.org/abs/2104.11918v1,2021-04-24T10:04:14Z,2021-04-24T10:04:14Z,"Constraint-Guided Reinforcement Learning: Augmenting the
  Agent-Environment-Interaction","Reinforcement Learning (RL) agents have great successes in solving tasks with
large observation and action spaces from limited feedback. Still, training the
agents is data-intensive and there are no guarantees that the learned behavior
is safe and does not violate rules of the environment, which has limitations
for the practical deployment in real-world scenarios. This paper discusses the
engineering of reliable agents via the integration of deep RL with
constraint-based augmentation models to guide the RL agent towards safe
behavior. Within the constraints set, the RL agent is free to adapt and
explore, such that its effectiveness to solve the given problem is not
hindered. However, once the RL agent leaves the space defined by the
constraints, the outside models can provide guidance to still work reliably. We
discuss integration points for constraint guidance within the RL process and
perform experiments on two case studies: a strictly constrained card game and a
grid world environment with additional combinatorial subgoals. Our results show
that constraint-guidance does both provide reliability improvements and safer
behavior, as well as accelerated training.",arxiv
http://arxiv.org/abs/2104.11146v1,2021-04-22T15:59:56Z,2021-04-22T15:59:56Z,"An Efficient One-Class SVM for Anomaly Detection in the Internet of
  Things","Insecure Internet of things (IoT) devices pose significant threats to
critical infrastructure and the Internet at large; detecting anomalous behavior
from these devices remains of critical importance, but fast, efficient,
accurate anomaly detection (also called ""novelty detection"") for these classes
of devices remains elusive. One-Class Support Vector Machines (OCSVM) are one
of the state-of-the-art approaches for novelty detection (or anomaly detection)
in machine learning, due to their flexibility in fitting complex nonlinear
boundaries between {normal} and {novel} data. IoT devices in smart homes and
cities and connected building infrastructure present a compelling use case for
novelty detection with OCSVM due to the variety of devices, traffic patterns,
and types of anomalies that can manifest in such environments. Much previous
research has thus applied OCSVM to novelty detection for IoT. Unfortunately,
conventional OCSVMs introduce significant memory requirements and are
computationally expensive at prediction time as the size of the train set
grows, requiring space and time that scales with the number of training points.
These memory and computational constraints can be prohibitive in practical,
real-world deployments, where large training sets are typically needed to
develop accurate models when fitting complex decision boundaries. In this work,
we extend so-called Nystr\""om and (Gaussian) Sketching approaches to OCSVM, by
combining these methods with clustering and Gaussian mixture models to achieve
significant speedups in prediction time and space in various IoT settings,
without sacrificing detection accuracy.",arxiv
http://arxiv.org/abs/2104.10398v1,2021-04-21T08:09:57Z,2021-04-21T08:09:57Z,Learning future terrorist targets through temporal meta-graphs,"In the last 20 years, terrorism has led to hundreds of thousands of deaths
and massive economic, political, and humanitarian crises in several regions of
the world. Using real-world data on attacks occurred in Afghanistan and Iraq
from 2001 to 2018, we propose the use of temporal meta-graphs and deep learning
to forecast future terrorist targets. Focusing on three event dimensions, i.e.,
employed weapons, deployed tactics and chosen targets, meta-graphs map the
connections among temporally close attacks, capturing their operational
similarities and dependencies. From these temporal meta-graphs, we derive
2-day-based time series that measure the centrality of each feature within each
dimension over time. Formulating the problem in the context of the strategic
behavior of terrorist actors, these multivariate temporal sequences are then
utilized to learn what target types are at the highest risk of being chosen.
The paper makes two contributions. First, it demonstrates that engineering the
feature space via temporal meta-graphs produces richer knowledge than shallow
time-series that only rely on frequency of feature occurrences. Second, the
performed experiments reveal that bi-directional LSTM networks achieve superior
forecasting performance compared to other algorithms, calling for future
research aiming at fully discovering the potential of artificial intelligence
to counter terrorist violence.",arxiv
http://arxiv.org/abs/2104.10159v1,2021-04-20T17:58:22Z,2021-04-20T17:58:22Z,MBRL-Lib: A Modular Library for Model-based Reinforcement Learning,"Model-based reinforcement learning is a compelling framework for
data-efficient learning of agents that interact with the world. This family of
algorithms has many subcomponents that need to be carefully selected and tuned.
As a result the entry-bar for researchers to approach the field and to deploy
it in real-world tasks can be daunting. In this paper, we present MBRL-Lib -- a
machine learning library for model-based reinforcement learning in continuous
state-action spaces based on PyTorch. MBRL-Lib is designed as a platform for
both researchers, to easily develop, debug and compare new algorithms, and
non-expert user, to lower the entry-bar of deploying state-of-the-art
algorithms. MBRL-Lib is open-source at
https://github.com/facebookresearch/mbrl-lib.",arxiv
http://arxiv.org/abs/2104.09684v2,2022-03-15T15:04:18Z,2021-04-19T23:28:32Z,Suppressing simulation bias using multi-modal data,"Many problems in science and engineering require making predictions based on
few observations. To build a robust predictive model, these sparse data may
need to be augmented with simulated data, especially when the design space is
multi-dimensional. Simulations, however, often suffer from an inherent bias.
Estimation of this bias may be poorly constrained not only because of data
sparsity, but also because traditional predictive models fit only one type of
observed outputs, such as scalars or images, instead of all available output
data modalities, which might have been acquired and simulated at great cost. To
break this limitation and open up the path for multi-modal calibration, we
propose to combine a novel, transfer learning technique for suppressing the
bias with recent developments in deep learning, which allow building predictive
models with multi-modal outputs. First, we train an initial neural network
model on simulated data to learn important correlations between different
output modalities and between simulation inputs and outputs. Then, the model is
partially retrained, or transfer learned, to fit the experiments; a method that
has never been implemented in this type of architecture. Using fewer than 10
inertial confinement fusion experiments for training, transfer learning
systematically improves the simulation predictions while a simple output
calibration, which we design as a baseline, makes the predictions worse. We
also offer extensive cross-validation with real and carefully designed
synthetic data. The method described in this paper can be applied to a wide
range of problems that require transferring knowledge from simulations to the
domain of experiments.",arxiv
http://arxiv.org/abs/2104.10067v1,2021-04-17T09:17:53Z,2021-04-17T09:17:53Z,Spherical Multi-Modal Place Recognition for Heterogeneous Sensor Systems,"In this paper, we propose a robust end-to-end multi-modal pipeline for place
recognition where the sensor systems can differ from the map building to the
query. Our approach operates directly on images and LiDAR scans without
requiring any local feature extraction modules. By projecting the sensor data
onto the unit sphere, we learn a multi-modal descriptor of partially
overlapping scenes using a spherical convolutional neural network. The employed
spherical projection model enables the support of arbitrary LiDAR and camera
systems readily without losing information. Loop closure candidates are found
using a nearest-neighbor lookup in the embedding space. We tackle the problem
of correctly identifying the closest place by correlating the candidates' power
spectra, obtaining a confidence value per prospect. Our estimate for the
correct place corresponds then to the candidate with the highest confidence. We
evaluate our proposal w.r.t. state-of-the-art approaches in place recognition
using real-world data acquired using different sensors. Our approach can
achieve a recall that is up to 10% and 5% higher than for a LiDAR- and
vision-based system, respectively, when the sensor setup differs between model
training and deployment. Additionally, our place selection can correctly
identify up to 95% matches from the candidate set.",arxiv
http://arxiv.org/abs/2104.07282v1,2021-04-15T07:40:27Z,2021-04-15T07:40:27Z,"Rule-Based Reinforcement Learning for Efficient Robot Navigation with
  Space Reduction","For real-world deployments, it is critical to allow robots to navigate in
complex environments autonomously. Traditional methods usually maintain an
internal map of the environment, and then design several simple rules, in
conjunction with a localization and planning approach, to navigate through the
internal map. These approaches often involve a variety of assumptions and prior
knowledge. In contrast, recent reinforcement learning (RL) methods can provide
a model-free, self-learning mechanism as the robot interacts with an initially
unknown environment, but are expensive to deploy in real-world scenarios due to
inefficient exploration. In this paper, we focus on efficient navigation with
the RL technique and combine the advantages of these two kinds of methods into
a rule-based RL (RuRL) algorithm for reducing the sample complexity and cost of
time. First, we use the rule of wall-following to generate a closed-loop
trajectory. Second, we employ a reduction rule to shrink the trajectory, which
in turn effectively reduces the redundant exploration space. Besides, we give
the detailed theoretical guarantee that the optimal navigation path is still in
the reduced space. Third, in the reduced space, we utilize the Pledge rule to
guide the exploration strategy for accelerating the RL process at the early
stage. Experiments conducted on real robot navigation problems in hex-grid
environments demonstrate that RuRL can achieve improved navigation performance.",arxiv
http://arxiv.org/abs/2104.06890v2,2021-11-17T11:57:35Z,2021-04-14T14:31:51Z,An Introduction of mini-AlphaStar,"StarCraft II (SC2) is a real-time strategy game in which players produce and
control multiple units to fight against opponent's units. Due to its
difficulties, such as huge state space, various action space, a long time
horizon, and imperfect information, SC2 has been a research hotspot in
reinforcement learning. Recently, an agent called AlphaStar (AS) has been
proposed, which shows good performance, obtaining a high win rate of 99.8%
against human players. We implemented a mini-scaled version of it called
mini-AlphaStar (mAS) based on AS's paper and pseudocode. The difference between
AS and mAS is that we substituted the hyper-parameters of AS with smaller ones
for mini-scale training. Codes of mAS are all open-sourced
(https://github.com/liuruoze/mini-AlphaStar) for future research.",arxiv
http://arxiv.org/abs/2104.05819v1,2021-04-12T21:07:53Z,2021-04-12T21:07:53Z,Learning from Executions for Semantic Parsing,"Semantic parsing aims at translating natural language (NL) utterances onto
machine-interpretable programs, which can be executed against a real-world
environment. The expensive annotation of utterance-program pairs has long been
acknowledged as a major bottleneck for the deployment of contemporary neural
models to real-life applications. In this work, we focus on the task of
semi-supervised learning where a limited amount of annotated data is available
together with many unlabeled NL utterances. Based on the observation that
programs which correspond to NL utterances must be always executable, we
propose to encourage a parser to generate executable programs for unlabeled
utterances. Due to the large search space of executable programs, conventional
methods that use approximations based on beam-search such as self-training and
top-k marginal likelihood training, do not perform as well. Instead, we view
the problem of learning from executions from the perspective of posterior
regularization and propose a set of new training objectives. Experimental
results on Overnight and GeoQuery show that our new objectives outperform
conventional methods, bridging the gap between semi-supervised and supervised
learning.",arxiv
http://arxiv.org/abs/2104.03780v1,2021-04-08T14:05:15Z,2021-04-08T14:05:15Z,"Enabling Cross-Domain Communication: How to Bridge the Gap between AI
  and HW Engineers","A key issue in system design is the lack of communication between hardware,
software and domain expert. Recent research work shows progress in automatic
HW/SW co-design flows of neural accelerators that seems to make this kind of
communication obsolete. Most real-world systems, however, are a composition of
multiple processing units, communication networks and memories. A HW/SW
co-design process of (reconfigurable) neural accelerators, therefore, is an
important sub-problem towards a common co-design methodology. The ultimate
challenge is to define the constraints for the design space exploration on
system level - a task which requires deep knowledge and understanding of
hardware architectures, mapping of workloads onto hardware and the application
domain, e.g. artificial intelligence.
  For most projects, these skills are distributed among several people or even
different teams which is one of the major reasons why there is no established
end-to-end development methodology for digital systems. This position paper
discusses possibilities how to establish such a methodology for systems that
include (reconfigurable) dedicated accelerators and outlines the central role
that languages and tools play in the process.",arxiv
http://arxiv.org/abs/2104.03693v1,2021-04-08T11:29:11Z,2021-04-08T11:29:11Z,Learning specialized activation functions with the Piecewise Linear Unit,"The choice of activation functions is crucial for modern deep neural
networks. Popular hand-designed activation functions like Rectified Linear
Unit(ReLU) and its variants show promising performance in various tasks and
models. Swish, the automatically discovered activation function, has been
proposed and outperforms ReLU on many challenging datasets. However, it has two
main drawbacks. First, the tree-based search space is highly discrete and
restricted, which is difficult for searching. Second, the sample-based
searching method is inefficient, making it infeasible to find specialized
activation functions for each dataset or neural architecture. To tackle these
drawbacks, we propose a new activation function called Piecewise Linear
Unit(PWLU), which incorporates a carefully designed formulation and learning
method. It can learn specialized activation functions and achieves SOTA
performance on large-scale datasets like ImageNet and COCO. For example, on
ImageNet classification dataset, PWLU improves 0.9%/0.53%/1.0%/1.7%/1.0% top-1
accuracy over Swish for
ResNet-18/ResNet-50/MobileNet-V2/MobileNet-V3/EfficientNet-B0. PWLU is also
easy to implement and efficient at inference, which can be widely applied in
real-world applications.",arxiv
http://arxiv.org/abs/2104.02459v2,2021-04-07T07:09:30Z,2021-04-06T12:35:23Z,Contrastive Explanations for Explaining Model Adaptations,"Many decision making systems deployed in the real world are not static - a
phenomenon known as model adaptation takes place over time. The need for
transparency and interpretability of AI-based decision models is widely
accepted and thus have been worked on extensively. Usually, explanation methods
assume a static system that has to be explained. Explaining non-static systems
is still an open research question, which poses the challenge how to explain
model adaptations. In this contribution, we propose and (empirically) evaluate
a framework for explaining model adaptations by contrastive explanations. We
also propose a method for automatically finding regions in data space that are
affected by a given model adaptation and thus should be explained.",arxiv
http://arxiv.org/abs/2104.02306v1,2021-04-06T06:04:57Z,2021-04-06T06:04:57Z,Binary Neural Network for Speaker Verification,"Although deep neural networks are successful for many tasks in the speech
domain, the high computational and memory costs of deep neural networks make it
difficult to directly deploy highperformance Neural Network systems on
low-resource embedded devices. There are several mechanisms to reduce the size
of the neural networks i.e. parameter pruning, parameter quantization, etc.
This paper focuses on how to apply binary neural networks to the task of
speaker verification. The proposed binarization of training parameters can
largely maintain the performance while significantly reducing storage space
requirements and computational costs. Experiment results show that, after
binarizing the Convolutional Neural Network, the ResNet34-based network
achieves an EER of around 5% on the Voxceleb1 testing dataset and even
outperforms the traditional real number network on the text-dependent dataset:
Xiaole while having a 32x memory saving.",arxiv
http://arxiv.org/abs/2104.01747v5,2021-10-02T04:56:18Z,2021-04-05T02:59:45Z,Fast Design Space Exploration of Nonlinear Systems: Part I,"System design tools are often only available as input-output blackboxes: for
a given design as input they compute an output representing system behavior.
Blackboxes are intended to be run in the forward direction. This paper presents
a new method of solving the inverse design problem namely, given requirements
or constraints on output, find an input that also optimizes an objective
function. This problem is challenging for several reasons. First, blackboxes
are not designed to be run in reverse. Second, inputs and outputs can be
discrete and continuous. Third, finding designs concurrently satisfying a set
of requirements is hard because designs satisfying individual requirements may
conflict with each other. Fourth, blackbox evaluations can be expensive.
Finally, blackboxes can sometimes fail to produce an output. This paper
presents CNMA, a new method of solving the inverse problem that overcomes these
challenges. CNMA tries to sample only the part of the design space relevant to
solving the problem, leveraging the power of neural networks, Mixed Integer
Linear Programs, and a new learning-from-failure feedback loop. The paper also
presents a parallel version of CNMA that improves the efficiency and quality of
solutions over the sequential version, and tries to steer it away from local
optima. CNMA's performance is evaluated against conventional optimization
methods for seven nonlinear design problems of 8 (two problems), 10, 15, 36 and
60 real-valued dimensions and one with 186 binary dimensions. Conventional
methods evaluated are off-the-shelf implementations of Bayesian Optimization
with Gaussian Processes, Nelder Mead and Random Search. The first two do not
solve problems that are high-dimensional, have discrete and continuous
variables or whose blackboxes can fail to return values. CNMA solves all
problems, and surpasses the performance of conventional methods by 1%-87%.",arxiv
http://arxiv.org/abs/2104.01716v2,2021-04-11T07:30:29Z,2021-04-05T00:02:36Z,"Quaternion Factorization Machines: A Lightweight Solution to Intricate
  Feature Interaction Modelling","As a well-established approach, factorization machine (FM) is capable of
automatically learning high-order interactions among features to make
predictions without the need for manual feature engineering. With the prominent
development of deep neural networks (DNNs), there is a recent and ongoing trend
of enhancing the expressiveness of FM-based models with DNNs. However, though
better results are obtained with DNN-based FM variants, such performance gain
is paid off by an enormous amount (usually millions) of excessive model
parameters on top of the plain FM. Consequently, the heavy parameterization
impedes the real-life practicality of those deep models, especially efficient
deployment on resource-constrained IoT and edge devices. In this paper, we move
beyond the traditional real space where most deep FM-based models are defined,
and seek solutions from quaternion representations within the hypercomplex
space. Specifically, we propose the quaternion factorization machine (QFM) and
quaternion neural factorization machine (QNFM), which are two novel lightweight
and memory-efficient quaternion-valued models for sparse predictive analytics.
By introducing a brand new take on FM-based models with the notion of
quaternion algebra, our models not only enable expressive inter-component
feature interactions, but also significantly reduce the parameter size due to
lower degrees of freedom in the hypercomplex Hamilton product compared with
real-valued matrix multiplication. Extensive experimental results on three
large-scale datasets demonstrate that QFM achieves 4.36% performance
improvement over the plain FM without introducing any extra parameters, while
QNFM outperforms all baselines with up to two magnitudes' parameter size
reduction in comparison to state-of-the-art peer methods.",arxiv
http://arxiv.org/abs/2104.00941v1,2021-04-02T08:41:51Z,2021-04-02T08:41:51Z,Multi-Class Data Description for Out-of-distribution Detection,"The capability of reliably detecting out-of-distribution samples is one of
the key factors in deploying a good classifier, as the test distribution always
does not match with the training distribution in most real-world applications.
In this work, we present a deep multi-class data description, termed as
Deep-MCDD, which is effective to detect out-of-distribution (OOD) samples as
well as classify in-distribution (ID) samples. Unlike the softmax classifier
that only focuses on the linear decision boundary partitioning its latent space
into multiple regions, our Deep-MCDD aims to find a spherical decision boundary
for each class which determines whether a test sample belongs to the class or
not. By integrating the concept of Gaussian discriminant analysis into deep
neural networks, we propose a deep learning objective to learn
class-conditional distributions that are explicitly modeled as separable
Gaussian distributions. Thereby, we can define the confidence score by the
distance of a test sample from each class-conditional distribution, and utilize
it for identifying OOD samples. Our empirical evaluation on multi-class tabular
and image datasets demonstrates that Deep-MCDD achieves the best performances
in distinguishing OOD samples while showing the classification accuracy as high
as the other competitors.",arxiv
http://arxiv.org/abs/2104.00165v2,2022-03-08T00:16:31Z,2021-03-31T23:58:34Z,"Encoding Event-Based Data With a Hybrid SNN Guided Variational
  Auto-encoder in Neuromorphic Hardware","Neuromorphic hardware equipped with learning capabilities can adapt to new,
real-time data. While models of Spiking Neural Networks (SNNs) can now be
trained using gradient descent to reach an accuracy comparable to equivalent
conventional neural networks, such learning often relies on external labels.
However, real-world data is unlabeled which can make supervised methods
inapplicable. To solve this problem, we propose a Hybrid Guided Variational
Autoencoder (VAE) which encodes event based data sensed by a Dynamic Vision
Sensor (DVS) into a latent space representation using an SNN. These
representations can be used as an embedding to measure data similarity and
predict labels in real-world data. We show that the Hybrid Guided-VAE achieves
87% classification accuracy on the DVSGesture dataset and it can encode the
sparse, noisy inputs into an interpretable latent space representation,
visualized through T-SNE plots. We also implement the encoder component of the
model on neuromorphic hardware and discuss the potential for our algorithm to
enable real-time learning from real-world event data.",arxiv
http://arxiv.org/abs/2103.16584v1,2021-03-30T18:01:06Z,2021-03-30T18:01:06Z,"Parameterized Hypercomplex Graph Neural Networks for Graph
  Classification","Despite recent advances in representation learning in hypercomplex (HC)
space, this subject is still vastly unexplored in the context of graphs.
Motivated by the complex and quaternion algebras, which have been found in
several contexts to enable effective representation learning that inherently
incorporates a weight-sharing mechanism, we develop graph neural networks that
leverage the properties of hypercomplex feature transformation. In particular,
in our proposed class of models, the multiplication rule specifying the algebra
itself is inferred from the data during training. Given a fixed model
architecture, we present empirical evidence that our proposed model
incorporates a regularization effect, alleviating the risk of overfitting. We
also show that for fixed model capacity, our proposed method outperforms its
corresponding real-formulated GNN, providing additional confirmation for the
enhanced expressivity of HC embeddings. Finally, we test our proposed
hypercomplex GNN on several open graph benchmark datasets and show that our
models reach state-of-the-art performance while consuming a much lower memory
footprint with 70& fewer parameters. Our implementations are available at
https://github.com/bayer-science-for-a-better-life/phc-gnn.",arxiv
http://arxiv.org/abs/2103.16323v2,2021-04-08T09:28:14Z,2021-03-30T13:15:48Z,"Thermal Neural Networks: Lumped-Parameter Thermal Modeling With
  State-Space Machine Learning","With electric power systems becoming more compact and increasingly powerful,
the relevance of thermal stress especially during overload operation is
expected to increase ceaselessly. Whenever critical temperatures cannot be
measured economically on a sensor base, a thermal model lends itself to
estimate those unknown quantities. Thermal models for electric power systems
are usually required to be both, real-time capable and of high estimation
accuracy. Moreover, ease of implementation and time to production play an
increasingly important role. In this work, the thermal neural network (TNN) is
introduced, which unifies both, consolidated knowledge in the form of
heat-transfer-based lumped-parameter models, and data-driven nonlinear function
approximation with supervised machine learning. A quasi-linear
parameter-varying system is identified solely from empirical data, where
relationships between scheduling variables and system matrices are inferred
statistically and automatically. At the same time, a TNN has physically
interpretable states through its state-space representation, is end-to-end
trainable -- similar to deep learning models -- with automatic differentiation,
and requires no material, geometry, nor expert knowledge for its design.
Experiments on an electric motor data set show that a TNN achieves higher
temperature estimation accuracies than previous white-/grey- or black-box
models with a mean squared error of $3.18~\text{K}^2$ and a worst-case error of
$5.84~\text{K}$ at 64 model parameters.",arxiv
http://arxiv.org/abs/2103.16010v1,2021-03-30T00:49:40Z,2021-03-30T00:49:40Z,"Theory-Guided Machine Learning for Process Simulation of Advanced
  Composites","Science-based simulation tools such as Finite Element (FE) models are
routinely used in scientific and engineering applications. While their success
is strongly dependent on our understanding of underlying governing physical
laws, they suffer inherent limitations including trade-off between
fidelity/accuracy and speed. The recent rise of Machine Learning (ML) proposes
a theory-agnostic paradigm. In complex multi-physics problems, however,
creating large enough datasets for successful training of ML models has proven
to be challenging. One promising strategy to bridge the divide between these
approaches and take advantage of their respective strengths is Theory-Guided
Machine Learning (TGML) which aims to integrate physical laws into ML
algorithms. In this paper, three case studies on thermal management during
processing of advanced composites are presented and studied using FE, ML and
TGML. A structured approach to incrementally adding increasingly complex
physics to training of TGML model is presented. The benefits of TGML over ML
models are seen in more accurate predictions, particularly outside the training
region, and ability to train with small datasets. One benefit of TGML over FE
is significant speed improvement to potentially develop real-time feedback
systems. A recent successful implementation of a TGML model to assess
producibility of aerospace composite parts is presented.",arxiv
http://arxiv.org/abs/2103.14251v1,2021-03-26T04:16:20Z,2021-03-26T04:16:20Z,"Embedding Power Flow into Machine Learning for Parameter and State
  Estimation","Modern state and parameter estimations in power systems consist of two
stages: the outer problem of minimizing the mismatch between network
observation and prediction over the network parameters, and the inner problem
of predicting the system state for given values of the parameters. The standard
solution of the combined problem is iterative: (a) set the parameters, e.g. to
priors on the power line characteristics, (b) map input observation to
prediction of the output, (c) compute the mismatch between predicted and
observed output, (d) make a gradient descent step in the space of parameters to
minimize the mismatch, and loop back to (a). We show how modern Machine
Learning (ML), and specifically training guided by automatic differentiation,
allows to resolve the iterative loop more efficiently. Moreover, we extend the
scheme to the case of incomplete observations, where Phasor Measurement Units
(reporting real and reactive powers, voltage and phase) are available only at
the generators (PV buses), while loads (PQ buses) report (via SCADA controls)
only active and reactive powers. Considering it from the implementation
perspective, our methodology of resolving the parameter and state estimation
problem can be viewed as embedding of the Power Flow (PF) solver into the
training loop of the Machine Learning framework (PyTorch, in this study). We
argue that this embedding can help to resolve high-level optimization problems
in power system operations and planning.",arxiv
http://arxiv.org/abs/2103.12344v2,2021-03-30T03:47:32Z,2021-03-23T06:39:29Z,"Joint Distribution across Representation Space for Out-of-Distribution
  Detection","Deep neural networks (DNNs) have become a key part of many modern software
applications. After training and validating, the DNN is deployed as an
irrevocable component and applied in real-world scenarios. Although most DNNs
are built meticulously with huge volumes of training data, data in the real
world still remain unknown to the DNN model, which leads to the crucial
requirement of runtime out-of-distribution (OOD) detection. However, many
existing approaches 1) need OOD data for classifier training or parameter
tuning, or 2) simply combine the scores of each hidden layer as an ensemble of
features for OOD detection. In this paper, we present a novel outlook on
in-distribution data in a generative manner, which takes their latent features
generated from each hidden layer as a joint distribution across representation
spaces. Since only the in-distribution latent features are comprehensively
understood in representation space, the internal difference between
in-distribution and OOD data can be naturally revealed without the intervention
of any OOD data. Specifically, We construct a generative model, called Latent
Sequential Gaussian Mixture (LSGM), to depict how the in-distribution latent
features are generated in terms of the trace of DNN inference across
representation spaces. We first construct the Gaussian Mixture Model (GMM)
based on in-distribution latent features for each hidden layer, and then
connect GMMs via the transition probabilities of the inference traces.
Experimental evaluations on popular benchmark OOD datasets and models validate
the superiority of the proposed method over the state-of-the-art methods in OOD
detection.",arxiv
http://arxiv.org/abs/2103.10562v1,2021-03-18T23:15:35Z,2021-03-18T23:15:35Z,Dynamic Grasping with Reachability and Motion Awareness,"Grasping in dynamic environments presents a unique set of challenges. A
stable and reachable grasp can become unreachable and unstable as the target
object moves, motion planning needs to be adaptive and in real time, the delay
in computation makes prediction necessary. In this paper, we present a dynamic
grasping framework that is reachability-aware and motion-aware. Specifically,
we model the reachability space of the robot using a signed distance field
which enables us to quickly screen unreachable grasps. Also, we train a neural
network to predict the grasp quality conditioned on the current motion of the
target. Using these as ranking functions, we quickly filter a large grasp
database to a few grasps in real time. In addition, we present a seeding
approach for arm motion generation that utilizes solution from previous time
step. This quickly generates a new arm trajectory that is close to the previous
plan and prevents fluctuation. We implement a recurrent neural network (RNN)
for modelling and predicting the object motion. Our extensive experiments
demonstrate the importance of each of these components and we validate our
pipeline on a real robot.",arxiv
http://arxiv.org/abs/2103.07541v1,2021-03-12T21:35:26Z,2021-03-12T21:35:26Z,"Machine Learning aided k-t SENSE for fast reconstruction of highly
  accelerated PCMR data","Purpose: We implemented the Machine Learning (ML) aided k-t SENSE
reconstruction to enable high resolution quantitative real-time phase contrast
MR (PCMR). Methods: A residual U-net and our U-net M were used to generate the
high resolution x-f space estimate for k-t SENSE regularisation prior. The
networks were judged on their ability to generalise to real undersampled data.
The in-vivo validation was done on 20 real-time 18x prospectively undersmapled
GASperturbed PCMR data. The ML aided k-t SENSE reconstruction results were
compared against the free-breathing Cartesian retrospectively gated sequence
and the compressed sensing (CS) reconstruction of the same data. Results: In
general, the ML aided k-t SENSE generated flow curves that were visually
sharper than those produced using CS. In two exceptional cases, U-net M
predictions exhibited blurring which propagated to the extracted velocity
curves. However, there were no statistical differences in the measured peak
velocities and stroke volumes between the tested methods. The ML aided k-t
SENSE was estimated to be ~3.6x faster in processing than CS. Conclusion: The
ML aided k-t SENSE reconstruction enables artefact suppression on a par with CS
with no significant differences in quantitative measures. The timing results
suggest the on-line implementation could deliver a substantial increase in
clinical throughput.",arxiv
http://arxiv.org/abs/2103.07138v2,2021-04-13T04:21:44Z,2021-03-12T08:23:21Z,UIEC^2-Net: CNN-based Underwater Image Enhancement Using Two Color Space,"Underwater image enhancement has attracted much attention due to the rise of
marine resource development in recent years. Benefit from the powerful
representation capabilities of Convolution Neural Networks(CNNs), multiple
underwater image enhancement algorithms based on CNNs have been proposed in the
last few years. However, almost all of these algorithms employ RGB color space
setting, which is insensitive to image properties such as luminance and
saturation. To address this problem, we proposed Underwater Image Enhancement
Convolution Neural Network using 2 Color Space (UICE^2-Net) that efficiently
and effectively integrate both RGB Color Space and HSV Color Space in one
single CNN. To our best knowledge, this method is the first to use HSV color
space for underwater image enhancement based on deep learning. UIEC^2-Net is an
end-to-end trainable network, consisting of three blocks as follow: a RGB
pixel-level block implements fundamental operations such as denoising and
removing color cast, a HSV global-adjust block for globally adjusting
underwater image luminance, color and saturation by adopting a novel neural
curve layer, and an attention map block for combining the advantages of RGB and
HSV block output images by distributing weight to each pixel. Experimental
results on synthetic and real-world underwater images show the good performance
of our proposed method in both subjective comparisons and objective metrics.
The code are available at https://github.com/BIGWangYuDong/UWEnhancement.",arxiv
http://arxiv.org/abs/2103.07085v1,2021-03-12T04:56:45Z,2021-03-12T04:56:45Z,Wireframe-Based UI Design Search Through Image Autoencoder,"UI design is an integral part of software development. For many developers
who do not have much UI design experience, exposing them to a large database of
real-application UI designs can help them quickly build up a realistic
understanding of the design space for a software feature and get design
inspirations from existing applications. However, existing keyword-based,
image-similarity-based, and component-matching-based methods cannot reliably
find relevant high-fidelity UI designs in a large database alike to the UI
wireframe that the developers sketch, in face of the great variations in UI
designs. In this article, we propose a deep-learning-based UI design search
engine to fill in the gap. The key innovation of our search engine is to train
a wireframe image autoencoder using a large database of real-application UI
designs, without the need for labeling relevant UI designs. We implement our
approach for Android UI design search, and conduct extensive experiments with
artificially created relevant UI designs and human evaluation of UI design
search results. Our experiments confirm the superior performance of our search
engine over existing image-similarity or component-matching-based methods and
demonstrate the usefulness of our search engine in real-world UI design tasks.",arxiv
http://arxiv.org/abs/2103.06326v2,2021-07-05T03:46:03Z,2021-03-10T20:13:21Z,"S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement
  Learning","Offline reinforcement learning proposes to learn policies from large
collected datasets without interacting with the physical environment. These
algorithms have made it possible to learn useful skills from data that can then
be deployed in the environment in real-world settings where interactions may be
costly or dangerous, such as autonomous driving or factories. However, current
algorithms overfit to the dataset they are trained on and exhibit poor
out-of-distribution generalization to the environment when deployed. In this
paper, we study the effectiveness of performing data augmentations on the state
space, and study 7 different augmentation schemes and how they behave with
existing offline RL algorithms. We then combine the best data performing
augmentation scheme with a state-of-the-art Q-learning technique, and improve
the function approximation of the Q-networks by smoothening out the learned
state-action space. We experimentally show that using this Surprisingly Simple
Self-Supervision technique in RL (S4RL), we significantly improve over the
current state-of-the-art algorithms on offline robot learning environments such
as MetaWorld [1] and RoboSuite [2,3], and benchmark datasets such as D4RL [4].",arxiv
http://arxiv.org/abs/2103.04263v2,2021-03-11T01:08:38Z,2021-03-07T04:40:15Z,Deepfake Videos in the Wild: Analysis and Detection,"AI-manipulated videos, commonly known as deepfakes, are an emerging problem.
Recently, researchers in academia and industry have contributed several
(self-created) benchmark deepfake datasets, and deepfake detection algorithms.
However, little effort has gone towards understanding deepfake videos in the
wild, leading to a limited understanding of the real-world applicability of
research contributions in this space. Even if detection schemes are shown to
perform well on existing datasets, it is unclear how well the methods
generalize to real-world deepfakes. To bridge this gap in knowledge, we make
the following contributions: First, we collect and present the largest dataset
of deepfake videos in the wild, containing 1,869 videos from YouTube and
Bilibili, and extract over 4.8M frames of content. Second, we present a
comprehensive analysis of the growth patterns, popularity, creators,
manipulation strategies, and production methods of deepfake content in the
real-world. Third, we systematically evaluate existing defenses using our new
dataset, and observe that they are not ready for deployment in the real-world.
Fourth, we explore the potential for transfer learning schemes and
competition-winning techniques to improve defenses.",arxiv
http://arxiv.org/abs/2103.03745v1,2021-03-05T15:17:13Z,2021-03-05T15:17:13Z,"Can You Fix My Neural Network? Real-Time Adaptive Waveform Synthesis for
  Resilient Wireless Signal Classification","Thanks to its capability of classifying complex phenomena without explicit
modeling, deep learning (DL) has been demonstrated to be a key enabler of
Wireless Signal Classification (WSC). Although DL can achieve a very high
accuracy under certain conditions, recent research has unveiled that the
wireless channel can disrupt the features learned by the DL model during
training, thus drastically reducing the classification performance in
real-world live settings. Since retraining classifiers is cumbersome after
deployment, existing work has leveraged the usage of carefully-tailored Finite
Impulse Response (FIR) filters that, when applied at the transmitter's side,
can restore the features that are lost because of the the channel actions,
i.e., waveform synthesis. However, these approaches compute FIRs using offline
optimization strategies, which limits their efficacy in highly-dynamic channel
settings. In this paper, we improve the state of the art by proposing Chares, a
Deep Reinforcement Learning (DRL)-based framework for channel-resilient
adaptive waveform synthesis. Chares adapts to new and unseen channel conditions
by optimally computing through DRL the FIRs in real-time. Chares is a DRL agent
whose architecture is-based upon the Twin Delayed Deep Deterministic Policy
Gradients (TD3), which requires minimal feedback from the receiver and explores
a continuous action space. Chares has been extensively evaluated on two
well-known datasets. We have also evaluated the real-time latency of Chares
with an implementation on field-programmable gate array (FPGA). Results show
that Chares increases the accuracy up to 4.1x when no waveform synthesis is
performed, by 1.9x with respect to existing work, and can compute new actions
within 41us.",arxiv
http://arxiv.org/abs/2103.00858v3,2022-01-21T13:05:37Z,2021-03-01T09:20:53Z,"CARMI: A Cache-Aware Learned Index with a Cost-based Construction
  Algorithm","Learned indexes, which use machine learning models to replace traditional
index structures, have shown promising results in recent studies. Existing
learned indexes use heuristic rules to construct index structures, which are
often suboptimal and sensitive to data distribution. In this paper, we argue
that upper-level RMI nodes should focus on data partitioning instead of model
fitting, and show that it leads to much better results in real-world datasets.
We introduce entropy as a metric to quantify and characterize the models in
learned indexes, which provides a new theoretical basis for subsequent works.
Moreover, we propose a new memory layout design with a fixed node size
throughout the tree structure, which allows the type of each node to be
flexibly chosen at runtime.
  We propose CARMI, a new efficient and updatable cache-aware RMI framework. To
reduce reliance on the expertise of database administrators, CARMI uses a
hybrid construction algorithm to automatically construct the index structures
under various datasets and workloads without any manual tuning. Our
experimental study shows that CARMI performs better and is more robust compared
to baselines, achieving an average of 2.37x/1.98x speedup compared to B+
Tree/ALEX, while using only about 0.70x memory space of B+ Tree. In the SOSD
platform, CARMI outperforms all the baselines over the real-world datasets,
with an average speedup of 1.21x over the nearest competitor. We believe that
our theoretical analysis and proposed framework can help learned indexes to get
closer to practical deployment.",arxiv
http://arxiv.org/abs/2102.12567v1,2021-02-24T21:34:40Z,2021-02-24T21:34:40Z,"Sketching Curvature for Efficient Out-of-Distribution Detection for Deep
  Neural Networks","In order to safely deploy Deep Neural Networks (DNNs) within the perception
pipelines of real-time decision making systems, there is a need for safeguards
that can detect out-of-training-distribution (OoD) inputs both efficiently and
accurately. Building on recent work leveraging the local curvature of DNNs to
reason about epistemic uncertainty, we propose Sketching Curvature of OoD
Detection (SCOD), an architecture-agnostic framework for equipping any trained
DNN with a task-relevant epistemic uncertainty estimate. Offline, given a
trained model and its training data, SCOD employs tools from matrix sketching
to tractably compute a low-rank approximation of the Fisher information matrix,
which characterizes which directions in the weight space are most influential
on the predictions over the training data. Online, we estimate uncertainty by
measuring how much perturbations orthogonal to these directions can alter
predictions at a new test input. We apply SCOD to pre-trained networks of
varying architectures on several tasks, ranging from regression to
classification. We demonstrate that SCOD achieves comparable or better OoD
detection performance with lower computational burden relative to existing
baselines.",arxiv
http://arxiv.org/abs/2102.11026v1,2021-02-19T02:30:14Z,2021-02-19T02:30:14Z,High-order Differentiable Autoencoder for Nonlinear Model Reduction,"This paper provides a new avenue for exploiting deep neural networks to
improve physics-based simulation. Specifically, we integrate the classic
Lagrangian mechanics with a deep autoencoder to accelerate elastic simulation
of deformable solids. Due to the inertia effect, the dynamic equilibrium cannot
be established without evaluating the second-order derivatives of the deep
autoencoder network. This is beyond the capability of off-the-shelf automatic
differentiation packages and algorithms, which mainly focus on the gradient
evaluation. Solving the nonlinear force equilibrium is even more challenging if
the standard Newton's method is to be used. This is because we need to compute
a third-order derivative of the network to obtain the variational Hessian. We
attack those difficulties by exploiting complex-step finite difference, coupled
with reverse automatic differentiation. This strategy allows us to enjoy the
convenience and accuracy of complex-step finite difference and in the meantime,
to deploy complex-value perturbations as collectively as possible to save
excessive network passes. With a GPU-based implementation, we are able to wield
deep autoencoders (e.g., $10+$ layers) with a relatively high-dimension latent
space in real-time. Along this pipeline, we also design a sampling network and
a weighting network to enable \emph{weight-varying} Cubature integration in
order to incorporate nonlinearity in the model reduction. We believe this work
will inspire and benefit future research efforts in nonlinearly reduced
physical simulation problems.",arxiv
http://arxiv.org/abs/2102.06336v1,2021-02-12T03:07:06Z,2021-02-12T03:07:06Z,"Dancing along Battery: Enabling Transformer with Run-time
  Reconfigurability on Mobile Devices","A pruning-based AutoML framework for run-time reconfigurability, namely RT3,
is proposed in this work. This enables Transformer-based large Natural Language
Processing (NLP) models to be efficiently executed on resource-constrained
mobile devices and reconfigured (i.e., switching models for dynamic hardware
conditions) at run-time. Such reconfigurability is the key to save energy for
battery-powered mobile devices, which widely use dynamic voltage and frequency
scaling (DVFS) technique for hardware reconfiguration to prolong battery life.
In this work, we creatively explore a hybrid block-structured pruning (BP) and
pattern pruning (PP) for Transformer-based models and first attempt to combine
hardware and software reconfiguration to maximally save energy for
battery-powered mobile devices. Specifically, RT3 integrates two-level
optimizations: First, it utilizes an efficient BP as the first-step compression
for resource-constrained mobile devices; then, RT3 heuristically generates a
shrunken search space based on the first level optimization and searches
multiple pattern sets with diverse sparsity for PP via reinforcement learning
to support lightweight software reconfiguration, which corresponds to available
frequency levels of DVFS (i.e., hardware reconfiguration). At run-time, RT3 can
switch the lightweight pattern sets within 45ms to guarantee the required
real-time constraint at different frequency levels. Results further show that
RT3 can prolong battery life over 4x improvement with less than 1% accuracy
loss for Transformer and 1.5% score decrease for DistilBERT.",arxiv
http://arxiv.org/abs/2102.05334v2,2021-09-02T07:50:28Z,2021-02-10T09:16:09Z,"Enhancing Real-World Adversarial Patches through 3D Modeling of Complex
  Target Scenes","Adversarial examples have proven to be a concerning threat to deep learning
models, particularly in the image domain. However, while many studies have
examined adversarial examples in the real world, most of them relied on 2D
photos of the attack scene. As a result, the attacks proposed may have limited
effectiveness when implemented in realistic environments with 3D objects or
varied conditions. There are few studies on adversarial learning that use 3D
objects, and in many cases, other researchers are unable to replicate the
real-world evaluation process. In this study, we present a framework that uses
3D modeling to craft adversarial patches for an existing real-world scene. Our
approach uses a 3D digital approximation of the scene as a simulation of the
real world. With the ability to add and manipulate any element in the digital
scene, our framework enables the attacker to improve the adversarial patch's
impact in real-world settings. We use the framework to create a patch for an
everyday scene and evaluate its performance using a novel evaluation process
that ensures that our results are reproducible in both the digital space and
the real world. Our evaluation results show that the framework can generate
adversarial patches that are robust to different settings in the real world.",arxiv
http://arxiv.org/abs/2101.10955v2,2021-11-14T17:09:25Z,2021-01-26T17:23:46Z,"RAPIQUE: Rapid and Accurate Video Quality Prediction of User Generated
  Content","Blind or no-reference video quality assessment of user-generated content
(UGC) has become a trending, challenging, heretofore unsolved problem. Accurate
and efficient video quality predictors suitable for this content are thus in
great demand to achieve more intelligent analysis and processing of UGC videos.
Previous studies have shown that natural scene statistics and deep learning
features are both sufficient to capture spatial distortions, which contribute
to a significant aspect of UGC video quality issues. However, these models are
either incapable or inefficient for predicting the quality of complex and
diverse UGC videos in practical applications. Here we introduce an effective
and efficient video quality model for UGC content, which we dub the Rapid and
Accurate Video Quality Evaluator (RAPIQUE), which we show performs comparably
to state-of-the-art (SOTA) models but with orders-of-magnitude faster runtime.
RAPIQUE combines and leverages the advantages of both quality-aware scene
statistics features and semantics-aware deep convolutional features, allowing
us to design the first general and efficient spatial and temporal (space-time)
bandpass statistics model for video quality modeling. Our experimental results
on recent large-scale UGC video quality databases show that RAPIQUE delivers
top performances on all the datasets at a considerably lower computational
expense. We hope this work promotes and inspires further efforts towards
practical modeling of video quality problems for potential real-time and
low-latency applications. To promote public usage, an implementation of RAPIQUE
has been made freely available online: \url{https://github.com/vztu/RAPIQUE}.",arxiv
http://arxiv.org/abs/2101.09577v1,2021-01-23T20:23:31Z,2021-01-23T20:23:31Z,"ReliefE: Feature Ranking in High-dimensional Spaces via Manifold
  Embeddings","Feature ranking has been widely adopted in machine learning applications such
as high-throughput biology and social sciences. The approaches of the popular
Relief family of algorithms assign importances to features by iteratively
accounting for nearest relevant and irrelevant instances. Despite their high
utility, these algorithms can be computationally expensive and not-well suited
for high-dimensional sparse input spaces. In contrast, recent embedding-based
methods learn compact, low-dimensional representations, potentially
facilitating down-stream learning capabilities of conventional learners. This
paper explores how the Relief branch of algorithms can be adapted to benefit
from (Riemannian) manifold-based embeddings of instance and target spaces,
where a given embedding's dimensionality is intrinsic to the dimensionality of
the considered data set. The developed ReliefE algorithm is faster and can
result in better feature rankings, as shown by our evaluation on 20 real-life
data sets for multi-class and multi-label classification tasks. The utility of
ReliefE for high-dimensional data sets is ensured by its implementation that
utilizes sparse matrix algebraic operations. Finally, the relation of ReliefE
to other ranking algorithms is studied via the Fuzzy Jaccard Index.",arxiv
http://arxiv.org/abs/2101.09336v1,2021-01-22T21:13:46Z,2021-01-22T21:13:46Z,A Comprehensive Survey on Hardware-Aware Neural Architecture Search,"Neural Architecture Search (NAS) methods have been growing in popularity.
These techniques have been fundamental to automate and speed up the time
consuming and error-prone process of synthesizing novel Deep Learning (DL)
architectures. NAS has been extensively studied in the past few years. Arguably
their most significant impact has been in image classification and object
detection tasks where the state of the art results have been obtained. Despite
the significant success achieved to date, applying NAS to real-world problems
still poses significant challenges and is not widely practical. In general, the
synthesized Convolution Neural Network (CNN) architectures are too complex to
be deployed in resource-limited platforms, such as IoT, mobile, and embedded
systems. One solution growing in popularity is to use multi-objective
optimization algorithms in the NAS search strategy by taking into account
execution latency, energy consumption, memory footprint, etc. This kind of NAS,
called hardware-aware NAS (HW-NAS), makes searching the most efficient
architecture more complicated and opens several questions.
  In this survey, we provide a detailed review of existing HW-NAS research and
categorize them according to four key dimensions: the search space, the search
strategy, the acceleration technique, and the hardware cost estimation
strategies. We further discuss the challenges and limitations of existing
approaches and potential future directions. This is the first survey paper
focusing on hardware-aware NAS. We hope it serves as a valuable reference for
the various techniques and algorithms discussed and paves the road for future
research towards hardware-aware NAS.",arxiv
http://arxiv.org/abs/2101.06582v1,2021-01-17T03:45:25Z,2021-01-17T03:45:25Z,"Tailored Learning-Based Scheduling for Kubernetes-Oriented Edge-Cloud
  System","Kubernetes (k8s) has the potential to merge the distributed edge and the
cloud but lacks a scheduling framework specifically for edge-cloud systems.
Besides, the hierarchical distribution of heterogeneous resources and the
complex dependencies among requests and resources make the modeling and
scheduling of k8s-oriented edge-cloud systems particularly sophisticated. In
this paper, we introduce KaiS, a learning-based scheduling framework for such
edge-cloud systems to improve the long-term throughput rate of request
processing. First, we design a coordinated multi-agent actor-critic algorithm
to cater to decentralized request dispatch and dynamic dispatch spaces within
the edge cluster. Second, for diverse system scales and structures, we use
graph neural networks to embed system state information, and combine the
embedding results with multiple policy networks to reduce the orchestration
dimensionality by stepwise scheduling. Finally, we adopt a two-time-scale
scheduling mechanism to harmonize request dispatch and service orchestration,
and present the implementation design of deploying the above algorithms
compatible with native k8s components. Experiments using real workload traces
show that KaiS can successfully learn appropriate scheduling policies,
irrespective of request arrival patterns and system scales. Moreover, KaiS can
enhance the average system throughput rate by 14.3% while reducing scheduling
cost by 34.7% compared to baselines.",arxiv
http://arxiv.org/abs/2101.03989v2,2021-11-29T17:41:07Z,2021-01-11T15:54:48Z,Technology Readiness Levels for Machine Learning Systems,"The development and deployment of machine learning (ML) systems can be
executed easily with modern tools, but the process is typically rushed and
means-to-an-end. The lack of diligence can lead to technical debt, scope creep
and misaligned objectives, model misuse and failures, and expensive
consequences. Engineering systems, on the other hand, follow well-defined
processes and testing standards to streamline development for high-quality,
reliable results. The extreme is spacecraft systems, where mission critical
measures and robustness are ingrained in the development process. Drawing on
experience in both spacecraft engineering and ML (from research through product
across domain areas), we have developed a proven systems engineering approach
for machine learning development and deployment. Our ""Machine Learning
Technology Readiness Levels"" (MLTRL) framework defines a principled process to
ensure robust, reliable, and responsible systems while being streamlined for ML
workflows, including key distinctions from traditional software engineering.
Even more, MLTRL defines a lingua franca for people across teams and
organizations to work collaboratively on artificial intelligence and machine
learning technologies. Here we describe the framework and elucidate it with
several real world use-cases of developing ML methods from basic research
through productization and deployment, in areas such as medical diagnostics,
consumer computer vision, satellite imagery, and particle physics.",arxiv
http://arxiv.org/abs/2101.02780v1,2021-01-07T22:01:30Z,2021-01-07T22:01:30Z,"SHARKS: Smart Hacking Approaches for RisK Scanning in Internet-of-Things
  and Cyber-Physical Systems based on Machine Learning","Cyber-physical systems (CPS) and Internet-of-Things (IoT) devices are
increasingly being deployed across multiple functionalities, ranging from
healthcare devices and wearables to critical infrastructures, e.g., nuclear
power plants, autonomous vehicles, smart cities, and smart homes. These devices
are inherently not secure across their comprehensive software, hardware, and
network stacks, thus presenting a large attack surface that can be exploited by
hackers. In this article, we present an innovative technique for detecting
unknown system vulnerabilities, managing these vulnerabilities, and improving
incident response when such vulnerabilities are exploited. The novelty of this
approach lies in extracting intelligence from known real-world CPS/IoT attacks,
representing them in the form of regular expressions, and employing machine
learning (ML) techniques on this ensemble of regular expressions to generate
new attack vectors and security vulnerabilities. Our results show that 10 new
attack vectors and 122 new vulnerability exploits can be successfully generated
that have the potential to exploit a CPS or an IoT ecosystem. The ML
methodology achieves an accuracy of 97.4% and enables us to predict these
attacks efficiently with an 87.2% reduction in the search space. We demonstrate
the application of our method to the hacking of the in-vehicle network of a
connected car. To defend against the known attacks and possible novel exploits,
we discuss a defense-in-depth mechanism for various classes of attacks and the
classification of data targeted by such attacks. This defense mechanism
optimizes the cost of security measures based on the sensitivity of the
protected resource, thus incentivizing its adoption in real-world CPS/IoT by
cybersecurity practitioners.",arxiv
http://arxiv.org/abs/2012.15823v2,2021-03-29T23:48:56Z,2020-12-31T18:48:58Z,Binary Graph Neural Networks,"Graph Neural Networks (GNNs) have emerged as a powerful and flexible
framework for representation learning on irregular data. As they generalize the
operations of classical CNNs on grids to arbitrary topologies, GNNs also bring
much of the implementation challenges of their Euclidean counterparts. Model
size, memory footprint, and energy consumption are common concerns for many
real-world applications. Network binarization allocates a single bit to
parameters and activations, thus dramatically reducing the memory requirements
(up to 32x compared to single-precision floating-point numbers) and maximizing
the benefits of fast SIMD instructions on modern hardware for measurable
speedups. However, in spite of the large body of work on binarization for
classical CNNs, this area remains largely unexplored in geometric deep
learning. In this paper, we present and evaluate different strategies for the
binarization of graph neural networks. We show that through careful design of
the models, and control of the training process, binary graph neural networks
can be trained at only a moderate cost in accuracy on challenging benchmarks.
In particular, we present the first dynamic graph neural network in Hamming
space, able to leverage efficient k-NN search on binary vectors to speed-up the
construction of the dynamic graph. We further verify that the binary models
offer significant savings on embedded devices. Our code is publicly available
on Github.",arxiv
http://arxiv.org/abs/2012.15728v1,2020-12-27T06:13:29Z,2020-12-27T06:13:29Z,"Multi-Channel Sequential Behavior Networks for User Modeling in Online
  Advertising","Multiple content providers rely on native advertisement for revenue by
placing ads within the organic content of their pages. We refer to this setting
as ``queryless'' to differentiate from search advertisement where a user
submits a search query and gets back related ads. Understanding user intent is
critical because relevant ads improve user experience and increase the
likelihood of delivering clicks that have value to our advertisers.
  This paper presents Multi-Channel Sequential Behavior Network (MC-SBN), a
deep learning approach for embedding users and ads in a semantic space in which
relevance can be evaluated. Our proposed user encoder architecture summarizes
user activities from multiple input channels--such as previous search queries,
visited pages, or clicked ads--into a user vector. It uses multiple RNNs to
encode sequences of event sessions from the different channels and then applies
an attention mechanism to create the user representation. A key property of our
approach is that user vectors can be maintained and updated incrementally,
which makes it feasible to be deployed for large-scale serving. We conduct
extensive experiments on real-world datasets. The results demonstrate that
MC-SBN can improve the ranking of relevant ads and boost the performance of
both click prediction and conversion prediction in the queryless native
advertising setting.",arxiv
http://arxiv.org/abs/2012.12142v1,2020-12-22T16:25:12Z,2020-12-22T16:25:12Z,High-Speed Robot Navigation using Predicted Occupancy Maps,"Safe and high-speed navigation is a key enabling capability for real world
deployment of robotic systems. A significant limitation of existing approaches
is the computational bottleneck associated with explicit mapping and the
limited field of view (FOV) of existing sensor technologies. In this paper, we
study algorithmic approaches that allow the robot to predict spaces extending
beyond the sensor horizon for robust planning at high speeds. We accomplish
this using a generative neural network trained from real-world data without
requiring human annotated labels. Further, we extend our existing control
algorithms to support leveraging the predicted spaces to improve collision-free
planning and navigation at high speeds. Our experiments are conducted on a
physical robot based on the MIT race car using an RGBD sensor where were able
to demonstrate improved performance at 4 m/s compared to a controller not
operating on predicted regions of the map.",arxiv
http://arxiv.org/abs/2101.03889v2,2021-02-16T13:44:56Z,2020-12-21T10:25:27Z,A Comprehensive Survey of 6G Wireless Communications,"While fifth-generation (5G) communications are being rolled out worldwide,
sixth-generation (6G) communications have attracted much attention from both
the industry and the academia. Compared with 5G, 6G will have a wider frequency
band, higher transmission rate, spectrum efficiency, greater connection
capacity, shorter delay, broader coverage, and more robust anti-interference
capability to satisfy various network requirements. This survey presents an
insightful understanding of 6G wireless communications by introducing
requirements, features, critical technologies, challenges, and applications.
First, we give an overview of 6G from perspectives of technologies, security
and privacy, and applications. Subsequently, we introduce various 6G
technologies and their existing challenges in detail, e.g., artificial
intelligence (AI), intelligent surfaces, THz, space-air-ground-sea integrated
network, cell-free massive MIMO, etc. Because of these technologies, 6G is
expected to outperform existing wireless communication systems regarding the
transmission rate, latency, global coverage, etc. Next, we discuss security and
privacy techniques that can be applied to protect data in 6G. Since edge
devices are expected to gain popularity soon, the vast amount of generated data
and frequent data exchange make the leakage of data easily. Finally, we predict
real-world applications built on the technologies and features of 6G; for
example, smart healthcare, smart city, and smart manufacturing will be
implemented by taking advantage of AI.",arxiv
http://arxiv.org/abs/2012.10943v2,2021-10-31T11:28:09Z,2020-12-20T14:52:57Z,"Trace-class Gaussian priors for Bayesian learning of neural networks
  with MCMC","This paper introduces a new neural network based prior for real valued
functions on $\mathbb R^d$ which, by construction, is more easily and cheaply
scaled up in the domain dimension $d$ compared to the usual Karhunen-Lo\`eve
function space prior. The new prior is a Gaussian neural network prior, where
each weight and bias has an independent Gaussian prior, but with the key
difference that the variances decrease in the width of the network in such a
way that the resulting function is almost surely well defined in the limit of
an infinite width network. We show that in a Bayesian treatment of inferring
unknown functions, the induced posterior over functions is amenable to Monte
Carlo sampling using Hilbert space Markov chain Monte Carlo (MCMC) methods.
This type of MCMC is popular, e.g. in the Bayesian Inverse Problems literature,
because it is stable under mesh refinement, i.e. the acceptance probability
does not shrink to $0$ as more parameters of the function's prior are
introduced, even ad infinitum. In numerical examples we demonstrate these
stated competitive advantages over other function space priors. We also
implement examples in Bayesian Reinforcement Learning to automate tasks from
data and demonstrate, for the first time, stability of MCMC to mesh refinement
for these type of problems.",arxiv
http://arxiv.org/abs/2012.10610v3,2021-02-16T17:31:15Z,2020-12-19T07:00:09Z,"SpaceML: Distributed Open-source Research with Citizen Scientists for
  the Advancement of Space Technology for NASA","Traditionally, academic labs conduct open-ended research with the primary
focus on discoveries with long-term value, rather than direct products that can
be deployed in the real world. On the other hand, research in the industry is
driven by its expected commercial return on investment, and hence focuses on a
real world product with short-term timelines. In both cases, opportunity is
selective, often available to researchers with advanced educational
backgrounds. Research often happens behind closed doors and may be kept
confidential until either its publication or product release, exacerbating the
problem of AI reproducibility and slowing down future research by others in the
field. As many research organizations tend to exclusively focus on specific
areas, opportunities for interdisciplinary research reduce. Undertaking
long-term bold research in unexplored fields with non-commercial yet great
public value is hard due to factors including the high upfront risk, budgetary
constraints, and a lack of availability of data and experts in niche fields.
Only a few companies or well-funded research labs can afford to do such
long-term research. With research organizations focused on an exploding array
of fields and resources spread thin, opportunities for the maturation of
interdisciplinary research reduce. Apart from these exigencies, there is also a
need to engage citizen scientists through open-source contributors to play an
active part in the research dialogue. We present a short case study of SpaceML,
an extension of the Frontier Development Lab, an AI accelerator for NASA.
SpaceML distributes open-source research and invites volunteer citizen
scientists to partake in development and deployment of high social value
products at the intersection of space and AI.",arxiv
http://arxiv.org/abs/2012.10389v1,2020-12-18T17:53:39Z,2020-12-18T17:53:39Z,"Reinforcement Learning for Unified Allocation and Patrolling in
  Signaling Games with Uncertainty","Green Security Games (GSGs) have been successfully used in the protection of
valuable resources such as fisheries, forests and wildlife. While real-world
deployment involves both resource allocation and subsequent coordinated
patrolling with communication and real-time, uncertain information, previous
game models do not fully address both of these stages simultaneously.
Furthermore, adopting existing solution strategies is difficult since they do
not scale well for larger, more complex variants of the game models.
  We therefore first propose a novel GSG model that combines defender
allocation, patrolling, real-time drone notification to human patrollers, and
drones sending warning signals to attackers. The model further incorporates
uncertainty for real-time decision-making within a team of drones and human
patrollers. Second, we present CombSGPO, a novel and scalable algorithm based
on reinforcement learning, to compute a defender strategy for this game model.
CombSGPO performs policy search over a multi-dimensional, discrete action space
to compute an allocation strategy that is best suited to a best-response
patrolling strategy for the defender, learnt by training a multi-agent Deep
Q-Network. We show via experiments that CombSGPO converges to better strategies
and is more scalable than comparable approaches. Third, we provide a detailed
analysis of the coordination and signaling behavior learnt by CombSGPO, showing
group formation between defender resources and patrolling formations based on
signaling and notifications between resources. Importantly, we find that
strategic signaling emerges in the final learnt strategy. Finally, we perform
experiments to evaluate these strategies under different levels of uncertainty.",arxiv
http://arxiv.org/abs/2012.08015v2,2021-08-26T16:29:07Z,2020-12-15T00:09:37Z,Active Learning for Deep Gaussian Process Surrogates,"Deep Gaussian processes (DGPs) are increasingly popular as predictive models
in machine learning (ML) for their non-stationary flexibility and ability to
cope with abrupt regime changes in training data. Here we explore DGPs as
surrogates for computer simulation experiments whose response surfaces exhibit
similar characteristics. In particular, we transport a DGP's automatic warping
of the input space and full uncertainty quantification (UQ), via a novel
elliptical slice sampling (ESS) Bayesian posterior inferential scheme, through
to active learning (AL) strategies that distribute runs non-uniformly in the
input space -- something an ordinary (stationary) GP could not do. Building up
the design sequentially in this way allows smaller training sets, limiting both
expensive evaluation of the simulator code and mitigating cubic costs of DGP
inference. When training data sizes are kept small through careful acquisition,
and with parsimonious layout of latent layers, the framework can be both
effective and computationally tractable. Our methods are illustrated on
simulation data and two real computer experiments of varying input
dimensionality. We provide an open source implementation in the ""deepgp""
package on CRAN.",arxiv
http://arxiv.org/abs/2012.07145v1,2020-12-13T20:40:08Z,2020-12-13T20:40:08Z,Learning to Schedule Halide Pipelines for the GPU,"We present a new algorithm to automatically generate high-performance GPU
implementations of complex imaging and machine learning pipelines, directly
from high-level Halide algorithm code. It is fully automatic, requiring no
schedule templates or hand-optimized kernels, and it targets a diverse range of
computations which is significantly broader than existing autoschedulers. We
address the scalability challenge of extending previous approaches to schedule
large real world programs, while enabling a broad set of program rewrites that
take into account the nested parallelism and memory hierarchy introduced by GPU
architectures. We achieve this using a hierarchical sampling strategy that
groups programs into buckets based on their structural similarity, then samples
representatives to be evaluated, allowing us to explore a large space by only
considering a subset of the space, and a pre-pass that 'freezes' decisions for
the lowest cost sections of a program, allowing more time to be spent on the
important stages. We then apply an efficient cost model combining machine
learning, program analysis, and GPU architecture knowledge. Our method scales
combinatorially better with respect to the deeper nested parallelism required
by GPUs compared to previous work. We evaluate its performance on a diverse
suite of real-world imaging and machine learning pipelines. We demonstrate
results that are on average 1.66X faster than existing automatic solutions (up
to 5X), and competitive with what the best human experts were able to achieve
in an active effort to beat our automatic results.",arxiv
http://arxiv.org/abs/2012.08364v1,2020-12-13T17:05:06Z,2020-12-13T17:05:06Z,GAP-net for Snapshot Compressive Imaging,"Snapshot compressive imaging (SCI) systems aim to capture high-dimensional
($\ge3$D) images in a single shot using 2D detectors. SCI devices include two
main parts: a hardware encoder and a software decoder. The hardware encoder
typically consists of an (optical) imaging system designed to capture
{compressed measurements}. The software decoder on the other hand refers to a
reconstruction algorithm that retrieves the desired high-dimensional signal
from those measurements. In this paper, using deep unfolding ideas, we propose
an SCI recovery algorithm, namely GAP-net, which unfolds the generalized
alternating projection (GAP) algorithm. At each stage, GAP-net passes its
current estimate of the desired signal through a trained convolutional neural
network (CNN). The CNN operates as a denoiser that projects the estimate back
to the desired signal space. For the GAP-net that employs trained
auto-encoder-based denoisers, we prove a probabilistic global convergence
result. Finally, we investigate the performance of GAP-net in solving video SCI
and spectral SCI problems. In both cases, GAP-net demonstrates competitive
performance on both synthetic and real data. In addition to having high
accuracy and high speed, we show that GAP-net is flexible with respect to
signal modulation implying that a trained GAP-net decoder can be applied in
different systems. Our code is at https://github.com/mengziyi64/ADMM-net.",arxiv
http://arxiv.org/abs/2012.05214v2,2020-12-10T12:26:59Z,2020-12-09T18:23:21Z,E3D: Event-Based 3D Shape Reconstruction,"3D shape reconstruction is a primary component of augmented/virtual reality.
Despite being highly advanced, existing solutions based on RGB, RGB-D and Lidar
sensors are power and data intensive, which introduces challenges for
deployment in edge devices. We approach 3D reconstruction with an event camera,
a sensor with significantly lower power, latency and data expense while
enabling high dynamic range. While previous event-based 3D reconstruction
methods are primarily based on stereo vision, we cast the problem as multi-view
shape from silhouette using a monocular event camera. The output from a moving
event camera is a sparse point set of space-time gradients, largely sketching
scene/object edges and contours. We first introduce an event-to-silhouette
(E2S) neural network module to transform a stack of event frames to the
corresponding silhouettes, with additional neural branches for camera pose
regression. Second, we introduce E3D, which employs a 3D differentiable
renderer (PyTorch3D) to enforce cross-view 3D mesh consistency and fine-tune
the E2S and pose network. Lastly, we introduce a 3D-to-events simulation
pipeline and apply it to publicly available object datasets and generate
synthetic event/silhouette training pairs for supervised learning.",arxiv
http://arxiv.org/abs/2012.04746v2,2021-04-02T22:28:33Z,2020-12-08T21:20:54Z,"Robust Neural Routing Through Space Partitions for Camera Relocalization
  in Dynamic Indoor Environments","Localizing the camera in a known indoor environment is a key building block
for scene mapping, robot navigation, AR, etc. Recent advances estimate the
camera pose via optimization over the 2D/3D-3D correspondences established
between the coordinates in 2D/3D camera space and 3D world space. Such a
mapping is estimated with either a convolution neural network or a decision
tree using only the static input image sequence, which makes these approaches
vulnerable to dynamic indoor environments that are quite common yet challenging
in the real world. To address the aforementioned issues, in this paper, we
propose a novel outlier-aware neural tree which bridges the two worlds, deep
learning and decision tree approaches. It builds on three important blocks: (a)
a hierarchical space partition over the indoor scene to construct the decision
tree; (b) a neural routing function, implemented as a deep classification
network, employed for better 3D scene understanding; and (c) an outlier
rejection module used to filter out dynamic points during the hierarchical
routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark
developed for camera relocalization in dynamic indoor environments. It achieves
robust neural routing through space partitions and outperforms the
state-of-the-art approaches by around 30% on camera pose accuracy, while
running comparably fast for evaluation.",arxiv
http://arxiv.org/abs/2011.14654v2,2020-12-16T19:56:16Z,2020-11-30T09:47:20Z,Feature Space Singularity for Out-of-Distribution Detection,"Out-of-Distribution (OoD) detection is important for building safe artificial
intelligence systems. However, current OoD detection methods still cannot meet
the performance requirements for practical deployment. In this paper, we
propose a simple yet effective algorithm based on a novel observation: in a
trained neural network, OoD samples with bounded norms well concentrate in the
feature space. We call the center of OoD features the Feature Space Singularity
(FSS), and denote the distance of a sample feature to FSS as FSSD. Then, OoD
samples can be identified by taking a threshold on the FSSD. Our analysis of
the phenomenon reveals why our algorithm works. We demonstrate that our
algorithm achieves state-of-the-art performance on various OoD detection
benchmarks. Besides, FSSD also enjoys robustness to slight corruption in test
data and can be further enhanced by ensembling. These make FSSD a promising
algorithm to be employed in real world. We release our code at
\url{https://github.com/megvii-research/FSSD_OoD_Detection}.",arxiv
http://arxiv.org/abs/2011.14925v1,2020-11-26T02:37:39Z,2020-11-26T02:37:39Z,"Autonomous Graph Mining Algorithm Search with Best Speed/Accuracy
  Trade-off","Graph data is ubiquitous in academia and industry, from social networks to
bioinformatics. The pervasiveness of graphs today has raised the demand for
algorithms that can answer various questions: Which products would a user like
to purchase given her order list? Which users are buying fake followers to
increase their public reputation? Myriads of new graph mining algorithms are
proposed every year to answer such questions - each with a distinct problem
formulation, computational time, and memory footprint. This lack of unity makes
it difficult for a practitioner to compare different algorithms and pick the
most suitable one for a specific application. These challenges - even more
severe for non-experts - create a gap in which state-of-the-art techniques
developed in academic settings fail to be optimally deployed in real-world
applications. To bridge this gap, we propose AUTOGM, an automated system for
graph mining algorithm development. We first define a unified framework
UNIFIEDGM that integrates various message-passing based graph algorithms,
ranging from conventional algorithms like PageRank to graph neural networks.
Then UNIFIEDGM defines a search space in which five parameters are required to
determine a graph algorithm. Under this search space, AUTOGM explicitly
optimizes for the optimal parameter set of UNIFIEDGM using Bayesian
Optimization. AUTOGM defines a novel budget-aware objective function for the
optimization to incorporate a practical issue - finding the best speed-accuracy
trade-off under a computation budget - into the graph algorithm generation
problem. Experiments on real-world benchmark datasets demonstrate that AUTOGM
generates novel graph mining algorithms with the best speed/accuracy trade-off
compared to existing models with heuristic parameters.",arxiv
http://arxiv.org/abs/2011.09747v2,2021-09-29T15:26:04Z,2020-11-19T09:53:27Z,"Energy Aware Deep Reinforcement Learning Scheduling for Sensors
  Correlated in Time and Space","Millions of battery-powered sensors deployed for monitoring purposes in a
multitude of scenarios, e.g., agriculture, smart cities, industry, etc.,
require energy-efficient solutions to prolong their lifetime. When these
sensors observe a phenomenon distributed in space and evolving in time, it is
expected that collected observations will be correlated in time and space. In
this paper, we propose a Deep Reinforcement Learning (DRL) based scheduling
mechanism capable of taking advantage of correlated information. We design our
solution using the Deep Deterministic Policy Gradient (DDPG) algorithm. The
proposed mechanism is capable of determining the frequency with which sensors
should transmit their updates, to ensure accurate collection of observations,
while simultaneously considering the energy available. To evaluate our
scheduling mechanism, we use multiple datasets containing environmental
observations obtained in multiple real deployments. The real observations
enable us to model the environment with which the mechanism interacts as
realistically as possible. We show that our solution can significantly extend
the sensors' lifetime. We compare our mechanism to an idealized, all-knowing
scheduler to demonstrate that its performance is near-optimal. Additionally, we
highlight the unique feature of our design, energy-awareness, by displaying the
impact of sensors' energy levels on the frequency of updates.",arxiv
http://arxiv.org/abs/2011.09643v2,2021-03-08T10:48:14Z,2020-11-19T04:18:01Z,Node Similarity Preserving Graph Convolutional Networks,"Graph Neural Networks (GNNs) have achieved tremendous success in various
real-world applications due to their strong ability in graph representation
learning. GNNs explore the graph structure and node features by aggregating and
transforming information within node neighborhoods. However, through
theoretical and empirical analysis, we reveal that the aggregation process of
GNNs tends to destroy node similarity in the original feature space. There are
many scenarios where node similarity plays a crucial role. Thus, it has
motivated the proposed framework SimP-GCN that can effectively and efficiently
preserve node similarity while exploiting graph structure. Specifically, to
balance information from graph structure and node features, we propose a
feature similarity preserving aggregation which adaptively integrates graph
structure and node features. Furthermore, we employ self-supervised learning to
explicitly capture the complex feature similarity and dissimilarity relations
between nodes. We validate the effectiveness of SimP-GCN on seven benchmark
datasets including three assortative and four disassorative graphs. The results
demonstrate that SimP-GCN outperforms representative baselines. Further probe
shows various advantages of the proposed framework. The implementation of
SimP-GCN is available at \url{https://github.com/ChandlerBang/SimP-GCN}.",arxiv
http://arxiv.org/abs/2011.09902v1,2020-11-17T04:11:31Z,2020-11-17T04:11:31Z,"Low-latency Federated Learning and Blockchain for Edge Association in
  Digital Twin empowered 6G Networks","Emerging technologies such as digital twins and 6th Generation mobile
networks (6G) have accelerated the realization of edge intelligence in
Industrial Internet of Things (IIoT). The integration of digital twin and 6G
bridges the physical system with digital space and enables robust instant
wireless connectivity. With increasing concerns on data privacy, federated
learning has been regarded as a promising solution for deploying distributed
data processing and learning in wireless networks. However, unreliable
communication channels, limited resources, and lack of trust among users,
hinder the effective application of federated learning in IIoT. In this paper,
we introduce the Digital Twin Wireless Networks (DTWN) by incorporating digital
twins into wireless networks, to migrate real-time data processing and
computation to the edge plane. Then, we propose a blockchain empowered
federated learning framework running in the DTWN for collaborative computing,
which improves the reliability and security of the system, and enhances data
privacy. Moreover, to balance the learning accuracy and time cost of the
proposed scheme, we formulate an optimization problem for edge association by
jointly considering digital twin association, training data batch size, and
bandwidth allocation. We exploit multi-agent reinforcement learning to find an
optimal solution to the problem. Numerical results on real-world dataset show
that the proposed scheme yields improved efficiency and reduced cost compared
to benchmark learning method.",arxiv
http://arxiv.org/abs/2011.06501v2,2020-12-18T15:00:07Z,2020-11-12T17:11:50Z,VARCLUST: clustering variables using dimensionality reduction,"VARCLUST algorithm is proposed for clustering variables under the assumption
that variables in a given cluster are linear combinations of a small number of
hidden latent variables, corrupted by the random noise. The entire clustering
task is viewed as the problem of selection of the statistical model, which is
defined by the number of clusters, the partition of variables into these
clusters and the 'cluster dimensions', i.e. the vector of dimensions of linear
subspaces spanning each of the clusters. The optimal model is selected using
the approximate Bayesian criterion based on the Laplace approximations and
using a non-informative uniform prior on the number of clusters. To solve the
problem of the search over a huge space of possible models we propose an
extension of the ClustOfVar algorithm which was dedicated to subspaces of
dimension only 1, and which is similar in structure to the $K$-centroid
algorithm. We provide a complete methodology with theoretical guarantees,
extensive numerical experimentations, complete data analyses and
implementation. Our algorithm assigns variables to appropriate clusterse based
on the consistent Bayesian Information Criterion (BIC), and estimates the
dimensionality of each cluster by the PEnalized SEmi-integrated Likelihood
Criterion (PESEL), whose consistency we prove. Additionally, we prove that each
iteration of our algorithm leads to an increase of the Laplace approximation to
the model posterior probability and provide the criterion for the estimation of
the number of clusters. Numerical comparisons with other algorithms show that
VARCLUST may outperform some popular machine learning tools for sparse subspace
clustering. We also report the results of real data analysis including TCGA
breast cancer data and meteorological data. The proposed method is implemented
in the publicly available R package varclust.",arxiv
http://arxiv.org/abs/2011.05128v1,2020-11-10T14:51:25Z,2020-11-10T14:51:25Z,"Laplacian Eigenmaps with variational circuits: a quantum embedding of
  graph data","With the development of quantum algorithms, high-cost computations are being
scrutinized in the hope of a quantum advantage. While graphs offer a convenient
framework for multiple real-world problems, their analytics still comes with
high computation and space. By mapping the graph data into a low dimensional
space, in which graph structural information is preserved, the eigenvectors of
the Laplacian matrix constitute a powerful node embedding, called Laplacian
Eigenmaps. Computing these embeddings is on its own an expensive task knowing
that using specific sparse methods, the eigendecomposition of a Laplacian
matrix has a cost of O($rn^2$), $r$ being the ratio of nonzero elements.
  We propose a method to compute a Laplacian Eigenmap using a quantum
variational circuit. The idea of our algorithm is to reach the eigenstates of
the laplacian matrix, which can be considered as a hamiltonian operator, by
adapting the variational quantum eigensolver algorithm. By estimating the $d$
first eigenvectors of the Laplacian at the same time, our algorithm directly
generates a $d$ dimension quantum embedding of the graph. We demonstrate that
it is possible to use the embedding for graph machine learning tasks by
implementing a quantum classifier on the top of it. The overall circuit
consists in a full quantum node classification algorithm. Tests on 32 nodes
graph with a quantum simulator shows that we can achieve similar performances
as the classical laplacian eigenmap algorithm. Although mathematical properties
of this approximate approach are not fully understood, this algorithm opens
perspectives for graph pre-processing using noisy quantum computers.",arxiv
http://arxiv.org/abs/2011.04813v2,2021-05-24T01:05:25Z,2020-11-09T22:49:19Z,"Bimanual Regrasping for Suture Needles using Reinforcement Learning for
  Rapid Motion Planning","Regrasping a suture needle is an important yet time-consuming process in
suturing. To bring efficiency into regrasping, prior work either designs a
task-specific mechanism or guides the gripper toward some specific pick-up
point for proper grasping of a needle. Yet, these methods are usually not
deployable when the working space is changed. Therefore, in this work, we
present rapid trajectory generation for bimanual needle regrasping via
reinforcement learning (RL). Demonstrations from a sampling-based motion
planning algorithm is incorporated to speed up the learning. In addition, we
propose the ego-centric state and action spaces for this bimanual planning
problem, where the reference frames are on the end-effectors instead of some
fixed frame. Thus, the learned policy can be directly applied to any feasible
robot configuration. Our experiments in simulation show that the success rate
of a single pass is 97%, and the planning time is 0.0212s on average, which
outperforms other widely used motion planning algorithms. For the real-world
experiments, the success rate is 73.3% if the needle pose is reconstructed from
an RGB image, with a planning time of 0.0846s and a run time of 5.1454s. If the
needle pose is known beforehand, the success rate becomes 90.5%, with a
planning time of 0.0807s and a run time of 2.8801s.",arxiv
http://arxiv.org/abs/2010.15689v1,2020-10-29T15:32:00Z,2020-10-29T15:32:00Z,"Learning Deep Interleaved Networks with Asymmetric Co-Attention for
  Image Restoration","Recently, convolutional neural network (CNN) has demonstrated significant
success for image restoration (IR) tasks (e.g., image super-resolution, image
deblurring, rain streak removal, and dehazing). However, existing CNN based
models are commonly implemented as a single-path stream to enrich feature
representations from low-quality (LQ) input space for final predictions, which
fail to fully incorporate preceding low-level contexts into later high-level
features within networks, thereby producing inferior results. In this paper, we
present a deep interleaved network (DIN) that learns how information at
different states should be combined for high-quality (HQ) images
reconstruction. The proposed DIN follows a multi-path and multi-branch pattern
allowing multiple interconnected branches to interleave and fuse at different
states. In this way, the shallow information can guide deep representative
features prediction to enhance the feature expression ability. Furthermore, we
propose asymmetric co-attention (AsyCA) which is attached at each interleaved
node to model the feature dependencies. Such AsyCA can not only adaptively
emphasize the informative features from different states, but also improves the
discriminative ability of networks. Our presented DIN can be trained end-to-end
and applied to various IR tasks. Comprehensive evaluations on public benchmarks
and real-world datasets demonstrate that the proposed DIN perform favorably
against the state-of-the-art methods quantitatively and qualitatively.",arxiv
http://arxiv.org/abs/2010.14605v3,2021-06-07T10:06:48Z,2020-10-27T20:56:49Z,"Traffic Refinery: Cost-Aware Data Representation for Machine Learning on
  Network Traffic","Network management often relies on machine learning to make predictions about
performance and security from network traffic. Often, the representation of the
traffic is as important as the choice of the model. The features that the model
relies on, and the representation of those features, ultimately determine model
accuracy, as well as where and whether the model can be deployed in practice.
Thus, the design and evaluation of these models ultimately requires
understanding not only model accuracy but also the systems costs associated
with deploying the model in an operational network. Towards this goal, this
paper develops a new framework and system that enables a joint evaluation of
both the conventional notions of machine learning performance (e.g., model
accuracy) and the systems-level costs of different representations of network
traffic. We highlight these two dimensions for two practical network management
tasks, video streaming quality inference and malware detection, to demonstrate
the importance of exploring different representations to find the appropriate
operating point. We demonstrate the benefit of exploring a range of
representations of network traffic and present Traffic Refinery, a
proof-of-concept implementation that both monitors network traffic at 10 Gbps
and transforms traffic in real time to produce a variety of feature
representations for machine learning. Traffic Refinery both highlights this
design space and makes it possible to explore different representations for
learning, balancing systems costs related to feature extraction and model
training against model accuracy.",arxiv
http://arxiv.org/abs/2010.14000v2,2020-12-08T18:04:58Z,2020-10-27T02:19:40Z,"Graph-based Reinforcement Learning for Active Learning in Real Time: An
  Application in Modeling River Networks","Effective training of advanced ML models requires large amounts of labeled
data, which is often scarce in scientific problems given the substantial human
labor and material cost to collect labeled data. This poses a challenge on
determining when and where we should deploy measuring instruments (e.g.,
in-situ sensors) to collect labeled data efficiently. This problem differs from
traditional pool-based active learning settings in that the labeling decisions
have to be made immediately after we observe the input data that come in a time
series. In this paper, we develop a real-time active learning method that uses
the spatial and temporal contextual information to select representative query
samples in a reinforcement learning framework. To reduce the need for large
training data, we further propose to transfer the policy learned from
simulation data which is generated by existing physics-based models. We
demonstrate the effectiveness of the proposed method by predicting streamflow
and water temperature in the Delaware River Basin given a limited budget for
collecting labeled data. We further study the spatial and temporal distribution
of selected samples to verify the ability of this method in selecting
informative samples over space and time.",arxiv
http://arxiv.org/abs/2010.12996v3,2021-04-20T18:22:35Z,2020-10-24T21:42:04Z,"Assessing the Potential of Deep Learning for Emulating Cloud
  Superparameterization in Climate Models with Real-Geography Boundary
  Conditions","We explore the potential of feed-forward deep neural networks (DNNs) for
emulating cloud superparameterization in realistic geography, using offline
fits to data from the Super Parameterized Community Atmospheric Model. To
identify the network architecture of greatest skill, we formally optimize
hyperparameters using ~250 trials. Our DNN explains over 70 percent of the
temporal variance at the 15-minute sampling scale throughout the mid-to-upper
troposphere. Autocorrelation timescale analysis compared against DNN skill
suggests the less good fit in the tropical, marine boundary layer is driven by
neural network difficulty emulating fast, stochastic signals in convection.
However, spectral analysis in the temporal domain indicates skillful emulation
of signals on diurnal to synoptic scales. A close look at the diurnal cycle
reveals correct emulation of land-sea contrasts and vertical structure in the
heating and moistening fields, but some distortion of precipitation.
Sensitivity tests targeting precipitation skill reveal complementary effects of
adding positive constraints vs. hyperparameter tuning, motivating the use of
both in the future. A first attempt to force an offline land model with DNN
emulated atmospheric fields produces reassuring results further supporting
neural network emulation viability in real-geography settings. Overall, the fit
skill is competitive with recent attempts by sophisticated Residual and
Convolutional Neural Network architectures trained on added information,
including memory of past states. Our results confirm the parameterizability of
superparameterized convection with continents through machine learning and we
highlight advantages of casting this problem locally in space and time for
accurate emulation and hopefully quick implementation of hybrid climate models.",arxiv
http://arxiv.org/abs/2010.12751v2,2021-11-30T20:08:49Z,2020-10-24T03:09:37Z,"Model Extraction Attacks on Graph Neural Networks: Taxonomy and
  Realization","Machine learning models are shown to face a severe threat from Model
Extraction Attacks, where a well-trained private model owned by a service
provider can be stolen by an attacker pretending as a client. Unfortunately,
prior works focus on the models trained over the Euclidean space, e.g., images
and texts, while how to extract a GNN model that contains a graph structure and
node features is yet to be explored. In this paper, for the first time, we
comprehensively investigate and develop model extraction attacks against GNN
models. We first systematically formalise the threat modelling in the context
of GNN model extraction and classify the adversarial threats into seven
categories by considering different background knowledge of the attacker, e.g.,
attributes and/or neighbour connections of the nodes obtained by the attacker.
Then we present detailed methods which utilise the accessible knowledge in each
threat to implement the attacks. By evaluating over three real-world datasets,
our attacks are shown to extract duplicated models effectively, i.e., 84% - 89%
of the inputs in the target domain have the same output predictions as the
victim model.",arxiv
http://arxiv.org/abs/2010.10969v2,2021-01-06T10:07:56Z,2020-10-21T13:00:05Z,"Incorporating Interpretable Output Constraints in Bayesian Neural
  Networks","Domains where supervised models are deployed often come with task-specific
constraints, such as prior expert knowledge on the ground-truth function, or
desiderata like safety and fairness. We introduce a novel probabilistic
framework for reasoning with such constraints and formulate a prior that
enables us to effectively incorporate them into Bayesian neural networks
(BNNs), including a variant that can be amortized over tasks. The resulting
Output-Constrained BNN (OC-BNN) is fully consistent with the Bayesian framework
for uncertainty quantification and is amenable to black-box inference. Unlike
typical BNN inference in uninterpretable parameter space, OC-BNNs widen the
range of functional knowledge that can be incorporated, especially for model
users without expertise in machine learning. We demonstrate the efficacy of
OC-BNNs on real-world datasets, spanning multiple domains such as healthcare,
criminal justice, and credit scoring.",arxiv
http://arxiv.org/abs/2010.09635v1,2020-10-19T16:20:45Z,2020-10-19T16:20:45Z,"Deep Reinforcement Learning with Population-Coded Spiking Neural Network
  for Continuous Control","The energy-efficient control of mobile robots is crucial as the complexity of
their real-world applications increasingly involves high-dimensional
observation and action spaces, which cannot be offset by limited on-board
resources. An emerging non-Von Neumann model of intelligence, where spiking
neural networks (SNNs) are run on neuromorphic processors, is regarded as an
energy-efficient and robust alternative to the state-of-the-art real-time
robotic controllers for low dimensional control tasks. The challenge now for
this new computing paradigm is to scale so that it can keep up with real-world
tasks. To do so, SNNs need to overcome the inherent limitations of their
training, namely the limited ability of their spiking neurons to represent
information and the lack of effective learning algorithms. Here, we propose a
population-coded spiking actor network (PopSAN) trained in conjunction with a
deep critic network using deep reinforcement learning (DRL). The population
coding scheme dramatically increased the representation capacity of the network
and the hybrid learning combined the training advantages of deep networks with
the energy-efficient inference of spiking networks. To show the general
applicability of our approach, we integrated it with a spectrum of both
on-policy and off-policy DRL algorithms. We deployed the trained PopSAN on
Intel's Loihi neuromorphic chip and benchmarked our method against the
mainstream DRL algorithms for continuous control. To allow for a fair
comparison among all methods, we validated them on OpenAI gym tasks. Our
Loihi-run PopSAN consumed 140 times less energy per inference when compared
against the deep actor network on Jetson TX2, and had the same level of
performance. Our results support the efficiency of neuromorphic controllers and
suggest our hybrid RL as an alternative to deep learning, when both
energy-efficiency and robustness are important.",arxiv
http://arxiv.org/abs/2010.10246v4,2021-03-16T12:54:40Z,2020-10-17T13:34:48Z,"MLCask: Efficient Management of Component Evolution in Collaborative
  Data Analytics Pipelines","With the ever-increasing adoption of machine learning for data analytics,
maintaining a machine learning pipeline is becoming more complex as both the
datasets and trained models evolve with time. In a collaborative environment,
the changes and updates due to pipeline evolution often cause cumbersome
coordination and maintenance work, raising the costs and making it hard to use.
Existing solutions, unfortunately, do not address the version evolution
problem, especially in a collaborative environment where non-linear version
control semantics are necessary to isolate operations made by different user
roles. The lack of version control semantics also incurs unnecessary storage
consumption and lowers efficiency due to data duplication and repeated data
pre-processing, which are avoidable. In this paper, we identify two main
challenges that arise during the deployment of machine learning pipelines, and
address them with the design of versioning for an end-to-end analytics system
MLCask. The system supports multiple user roles with the ability to perform
Git-like branching and merging operations in the context of the machine
learning pipelines. We define and accelerate the metric-driven merge operation
by pruning the pipeline search tree using reusable history records and pipeline
compatibility information. Further, we design and implement the prioritized
pipeline search, which gives preference to the pipelines that probably yield
better performance. The effectiveness of MLCask is evaluated through an
extensive study over several real-world deployment cases. The performance
evaluation shows that the proposed merge operation is up to 7.8x faster and
saves up to 11.9x storage space than the baseline method that does not utilize
history records.",arxiv
http://arxiv.org/abs/2010.08600v2,2020-11-16T06:26:16Z,2020-10-16T19:40:08Z,"Robot Navigation in Constrained Pedestrian Environments using
  Reinforcement Learning","Navigating fluently around pedestrians is a necessary capability for mobile
robots deployed in human environments, such as buildings and homes. While
research on social navigation has focused mainly on the scalability with the
number of pedestrians in open spaces, typical indoor environments present the
additional challenge of constrained spaces such as corridors and doorways that
limit maneuverability and influence patterns of pedestrian interaction. We
present an approach based on reinforcement learning (RL) to learn policies
capable of dynamic adaptation to the presence of moving pedestrians while
navigating between desired locations in constrained environments. The policy
network receives guidance from a motion planner that provides waypoints to
follow a globally planned trajectory, whereas RL handles the local
interactions. We explore a compositional principle for multi-layout training
and find that policies trained in a small set of geometrically simple layouts
successfully generalize to more complex unseen layouts that exhibit composition
of the structural elements available during training. Going beyond walls-world
like domains, we show transfer of the learned policy to unseen 3D
reconstructions of two real environments. These results support the
applicability of the compositional principle to navigation in real-world
buildings and indicate promising usage of multi-agent simulation within
reconstructed environments for tasks that involve interaction.",arxiv
http://arxiv.org/abs/2010.08098v2,2021-03-07T17:28:22Z,2020-10-16T02:04:45Z,"Agile Robot Navigation through Hallucinated Learning and Sober
  Deployment","Learning from Hallucination (LfH) is a recent machine learning paradigm for
autonomous navigation, which uses training data collected in completely safe
environments and adds numerous imaginary obstacles to make the environment
densely constrained, to learn navigation planners that produce feasible
navigation even in highly constrained (more dangerous) spaces. However, LfH
requires hallucinating the robot perception during deployment to match with the
hallucinated training data, which creates a need for sometimes-infeasible prior
knowledge and tends to generate very conservative planning. In this work, we
propose a new LfH paradigm that does not require runtime hallucination -- a
feature we call ""sober deployment"" -- and can therefore adapt to more realistic
navigation scenarios. This novel Hallucinated Learning and Sober Deployment
(HLSD) paradigm is tested in a benchmark testbed of 300 simulated navigation
environments with a wide range of difficulty levels, and in the real-world. In
most cases, HLSD outperforms both the original LfH method and a classical
navigation planner.",arxiv
http://arxiv.org/abs/2010.05878v2,2022-01-18T19:43:10Z,2020-10-12T17:27:43Z,PECOS: Prediction for Enormous and Correlated Output Spaces,"Many large-scale applications amount to finding relevant results from an
enormous output space of potential candidates. For example, finding the best
matching product from a large catalog or suggesting related search phrases on a
search engine. The size of the output space for these problems can range from
millions to billions, and can even be infinite in some applications. Moreover,
training data is often limited for the long-tail items in the output space.
Fortunately, items in the output space are often correlated thereby presenting
an opportunity to alleviate the data sparsity issue. In this paper, we propose
the Prediction for Enormous and Correlated Output Spaces (PECOS) framework, a
versatile and modular machine learning framework for solving prediction
problems for very large output spaces, and apply it to the eXtreme Multilabel
Ranking (XMR) problem: given an input instance, find and rank the most relevant
items from an enormous but fixed and finite output space. We propose a three
phase framework for PECOS: (i) in the first phase, PECOS organizes the output
space using a semantic indexing scheme, (ii) in the second phase, PECOS uses
the indexing to narrow down the output space by orders of magnitude using a
machine learned matching scheme, and (iii) in the third phase, PECOS ranks the
matched items using a final ranking scheme. The versatility and modularity of
PECOS allows for easy plug-and-play of various choices for the indexing,
matching, and ranking phases. We also develop very fast inference procedures
which allow us to perform XMR predictions in real time; for example, inference
takes less than 1 millisecond per input on the dataset with 2.8 million labels.
The PECOS software is available at https://libpecos.org.",arxiv
http://arxiv.org/abs/2010.03665v2,2020-10-22T16:37:39Z,2020-10-07T21:35:16Z,A Bandit-Based Algorithm for Fairness-Aware Hyperparameter Optimization,"Considerable research effort has been guided towards algorithmic fairness but
there is still no major breakthrough. In practice, an exhaustive search over
all possible techniques and hyperparameters is needed to find optimal
fairness-accuracy trade-offs. Hence, coupled with the lack of tools for ML
practitioners, real-world adoption of bias reduction methods is still scarce.
To address this, we present Fairband, a bandit-based fairness-aware
hyperparameter optimization (HO) algorithm. Fairband is conceptually simple,
resource-efficient, easy to implement, and agnostic to both the objective
metrics, model types and the hyperparameter space being explored. Moreover, by
introducing fairness notions into HO, we enable seamless and efficient
integration of fairness objectives into real-world ML pipelines. We compare
Fairband with popular HO methods on four real-world decision-making datasets.
We show that Fairband can efficiently navigate the fairness-accuracy trade-off
through hyperparameter optimization. Furthermore, without extra training cost,
it consistently finds configurations attaining substantially improved fairness
at a comparatively small decrease in predictive accuracy.",arxiv
http://arxiv.org/abs/2010.04012v2,2021-06-30T15:32:57Z,2020-10-07T14:22:51Z,Invertible Manifold Learning for Dimension Reduction,"Dimension reduction (DR) aims to learn low-dimensional representations of
high-dimensional data with the preservation of essential information. In the
context of manifold learning, we define that the representation after
information-lossless DR preserves the topological and geometric properties of
data manifolds formally, and propose a novel two-stage DR method, called
invertible manifold learning (inv-ML) to bridge the gap between theoretical
information-lossless and practical DR. The first stage includes a homeomorphic
sparse coordinate transformation to learn low-dimensional representations
without destroying topology and a local isometry constraint to preserve local
geometry. In the second stage, a linear compression is implemented for the
trade-off between the target dimension and the incurred information loss in
excessive DR scenarios. Experiments are conducted on seven datasets with a
neural network implementation of inv-ML, called i-ML-Enc. Empirically, i-ML-Enc
achieves invertible DR in comparison with typical existing methods as well as
reveals the characteristics of the learned manifolds. Through latent space
interpolation on real-world datasets, we find that the reliability of tangent
space approximated by the local neighborhood is the key to the success of
manifold-based DR algorithms.",arxiv
http://arxiv.org/abs/2010.01397v1,2020-10-03T17:35:44Z,2020-10-03T17:35:44Z,Automated Performance Tuning for Highly-Configurable Software Systems,"Performance is an important non-functional aspect of the software
requirement. Modern software systems are highly-configurable and
misconfigurations may easily cause performance issues. A software system that
suffers performance issues may exhibit low program throughput and long response
time. However, the sheer size of the configuration space makes it challenging
for administrators to manually select and adjust the configuration options to
achieve better performance. In this paper, we propose ConfRL, an approach to
tune software performance automatically. The key idea of ConfRL is to use
reinforcement learning to explore the configuration space by a trial-and-error
approach and to use the feedback received from the environment to tune
configuration option values to achieve better performance. To reduce the cost
of reinforcement learning, ConfRL employs sampling, clustering, and dynamic
state reduction techniques to keep states in a large configuration space
manageable. Our evaluation of four real-world highly-configurable server
programs shows that ConfRL can efficiently and effectively guide software
systems to achieve higher long-term performance.",arxiv
http://arxiv.org/abs/2010.00432v1,2020-10-01T14:27:28Z,2020-10-01T14:27:28Z,"The RFML Ecosystem: A Look at the Unique Challenges of Applying Deep
  Learning to Radio Frequency Applications","While deep machine learning technologies are now pervasive in
state-of-the-art image recognition and natural language processing
applications, only in recent years have these technologies started to
sufficiently mature in applications related to wireless communications. In
particular, recent research has shown deep machine learning to be an enabling
technology for cognitive radio applications as well as a useful tool for
supplementing expertly defined algorithms for spectrum sensing applications
such as signal detection, estimation, and classification (termed here as Radio
Frequency Machine Learning, or RFML). A major driver for the usage of deep
machine learning in the context of wireless communications is that little, to
no, a priori knowledge of the intended spectral environment is required, given
that there is an abundance of representative data to facilitate training and
evaluation. However, in addition to this fundamental need for sufficient data,
there are other key considerations, such as trust, security, and
hardware/software issues, that must be taken into account before deploying deep
machine learning systems in real-world wireless communication applications.
This paper provides an overview and survey of prior work related to these major
research considerations. In particular, we present their unique considerations
in the RFML application space, which are not generally present in the image,
audio, and/or text application spaces.",arxiv
http://arxiv.org/abs/2010.07029v2,2021-02-27T19:06:48Z,2020-09-29T21:32:28Z,"Basic principles and concept design of a real-time clinical decision
  support system for managing medical emergencies on missions to Mars","Space agencies and private companies prepare the beginning of human space
exploration for the 2030s with missions to put the first human on the Mars
surface. The absence of gravity and radiation, along with distance, isolation
and hostile environments, are expected to increase medical events where
previously unseen manifestations may arise. The current healthcare strategy
based on telemedicine and the possibility to stabilize and transport the
injured crewmember to a terrestrial definitive medical facility is not
applicable in exploration class missions. Therefore, the need for deploying the
full autonomous capability to solve medical emergencies may guide the design of
future onboard healthcare systems. We present ten basic principles and concept
design of a software suite to bring onboard decision support to help the crew
dealing with medical emergencies taking into consideration physiological
disturbances in space and spaceflight restrictions. 1) give real-time support
for emergency medical decision making, 2) give patient-specific advice for
executive problem-solving, 3) take into account available information from life
support and monitoring of crewmembers, 4) be fully autonomous from remote
facilities, 5) continuously adapt predictions to physiological disturbance and
changing conditions, 6) optimize emergency medical decision making in terms of
mission fundamental priorities, 7) take into account medical supplies and
equipment on board, 8) apply health standards for the level of care V, 9)
implement ethics responsibilities for spaceflights, and 10) apply ethical
standards for artificial intelligence. Based on these principles, we propose an
autonomous clinical decision support system (CDSS) to provide real-time advice
for emergency medical interventions on board of space exploration missions.",arxiv
http://arxiv.org/abs/2009.12095v1,2020-09-25T09:11:52Z,2020-09-25T09:11:52Z,Fully reconfigurable coherent optical vector-matrix multiplication,"Optics is a promising platform in which to help realise the next generation
of fast, parallel and energy-efficient computation. We demonstrate a
reconfigurable free-space optical multiplier that is capable of over 3000
computations in parallel, using spatial light modulators with a pixel
resolution of only 340x340. This enables vector-matrix multiplication and
parallel vector-vector multiplication with vector size of up to 56. Our design
is the first to simultaneously support optical implementation of
reconfigurable, large-size and real-valued linear algebraic operations. Such an
optical multiplier can serve as a building block of special-purpose optical
processors such as optical neural networks and optical Ising machines.",arxiv
http://arxiv.org/abs/2009.11380v2,2021-03-24T18:29:36Z,2020-09-23T21:19:55Z,"Combining Weighted Total Variation and Deep Image Prior for natural and
  medical image restoration via ADMM","In the last decades, unsupervised deep learning based methods have caught
researchers attention, since in many real applications, such as medical
imaging, collecting a great amount of training examples is not always feasible.
Moreover, the construction of a good training set is time consuming and hard
because the selected data have to be enough representative for the task. In
this paper, we focus on the Deep Image Prior (DIP) framework and we propose to
combine it with a space-variant Total Variation regularizer with an automatic
estimation of the local regularization parameters. Differently from other
existing approaches, we solve the arising minimization problem via the flexible
Alternating Direction Method of Multipliers (ADMM). Furthermore, we provide a
specific implementation also for the standard isotropic Total Variation. The
promising performances of the proposed approach, in terms of PSNR and SSIM
values, are addressed through several experiments on simulated as well as real
natural and medical corrupted images.",arxiv
http://arxiv.org/abs/2009.09231v1,2020-09-19T13:47:33Z,2020-09-19T13:47:33Z,Adversarial Exposure Attack on Diabetic Retinopathy Imagery,"Diabetic retinopathy (DR) is a leading cause of vision loss in the world and
numerous cutting-edge works have built powerful deep neural networks (DNNs) to
automatically classify the DR cases via the retinal fundus images (RFIs).
However, RFIs are usually affected by the widely existing camera exposure while
the robustness of DNNs to the exposure is rarely explored. In this paper, we
study this problem from the viewpoint of adversarial attack and identify a
totally new task, i.e., adversarial exposure attack generating adversarial
images by tuning image exposure to mislead the DNNs with significantly high
transferability. To this end, we first implement a straightforward method,
i.e., multiplicative-perturbation-based exposure attack, and reveal the big
challenges of this new task. Then, to make the adversarial image naturalness,
we propose the adversarial bracketed exposure fusion that regards the exposure
attack as an element-wise bracketed exposure fusion problem in the
Laplacian-pyramid space. Moreover, to realize high transferability, we further
propose the convolutional bracketed exposure fusion where the element-wise
multiplicative operation is extended to the convolution. We validate our method
on the real public DR dataset with the advanced DNNs, e.g., ResNet50,
MobileNet, and EfficientNet, showing our method can achieve high image quality
and success rate of the transfer attack. Our method reveals the potential
threats to the DNN-based DR automated diagnosis and can definitely benefit the
development of exposure-robust automated DR diagnosis method in the future.",arxiv
http://arxiv.org/abs/2009.05780v1,2020-09-12T12:38:47Z,2020-09-12T12:38:47Z,"EdgeLoc: An Edge-IoT Framework for Robust Indoor Localization Using
  Capsule Networks","With the unprecedented demand for location-based services in indoor
scenarios, wireless indoor localization has become essential for mobile users.
While GPS is not available at indoor spaces, WiFi RSS fingerprinting has become
popular with its ubiquitous accessibility. However, it is challenging to
achieve robust and efficient indoor localization with two major challenges.
First, the localization accuracy can be degraded by the random signal
fluctuations, which would influence conventional localization algorithms that
simply learn handcrafted features from raw fingerprint data. Second, mobile
users are sensitive to the localization delay, but conventional indoor
localization algorithms are computation-intensive and time-consuming. In this
paper, we propose EdgeLoc, an edge-IoT framework for efficient and robust
indoor localization using capsule networks. We develop a deep learning model
with the CapsNet to efficiently extract hierarchical information from WiFi
fingerprint data, thereby significantly improving the localization accuracy.
Moreover, we implement an edge-computing prototype system to achieve a nearly
real-time localization process, by enabling mobile users with the deep-learning
model that has been well-trained by the edge server. We conduct a real-world
field experimental study with over 33,600 data points and an extensive
synthetic experiment with the open dataset, and the experimental results
validate the effectiveness of EdgeLoc. The best trade-off of the EdgeLoc system
achieves 98.5% localization accuracy within an average positioning time of only
2.31 ms in the field experiment.",arxiv
http://arxiv.org/abs/2009.00871v1,2020-09-02T07:45:03Z,2020-09-02T07:45:03Z,"HL-Pow: A Learning-Based Power Modeling Framework for High-Level
  Synthesis","High-level synthesis (HLS) enables designers to customize hardware designs
efficiently. However, it is still challenging to foresee the correlation
between power consumption and HLS-based applications at an early design stage.
To overcome this problem, we introduce HL-Pow, a power modeling framework for
FPGA HLS based on state-of-the-art machine learning techniques. HL-Pow
incorporates an automated feature construction flow to efficiently identify and
extract features that exert a major influence on power consumption, simply
based upon HLS results, and a modeling flow that can build an accurate and
generic power model applicable to a variety of designs with HLS. By using
HL-Pow, the power evaluation process for FPGA designs can be significantly
expedited because the power inference of HL-Pow is established on HLS instead
of the time-consuming register-transfer level (RTL) implementation flow.
Experimental results demonstrate that HL-Pow can achieve accurate power
modeling that is only 4.67% (24.02 mW) away from onboard power measurement. To
further facilitate power-oriented optimizations, we describe a novel design
space exploration (DSE) algorithm built on top of HL-Pow to trade off between
latency and power consumption. This algorithm can reach a close approximation
of the real Pareto frontier while only requiring running HLS flow for 20% of
design points in the entire design space.",arxiv
http://arxiv.org/abs/2009.00814v1,2020-09-02T04:35:33Z,2020-09-02T04:35:33Z,Open-set Adversarial Defense,"Open-set recognition and adversarial defense study two key aspects of deep
learning that are vital for real-world deployment. The objective of open-set
recognition is to identify samples from open-set classes during testing, while
adversarial defense aims to defend the network against images with
imperceptible adversarial perturbations. In this paper, we show that open-set
recognition systems are vulnerable to adversarial attacks. Furthermore, we show
that adversarial defense mechanisms trained on known classes do not generalize
well to open-set samples. Motivated by this observation, we emphasize the need
of an Open-Set Adversarial Defense (OSAD) mechanism. This paper proposes an
Open-Set Defense Network (OSDN) as a solution to the OSAD problem. The proposed
network uses an encoder with feature-denoising layers coupled with a classifier
to learn a noise-free latent feature representation. Two techniques are
employed to obtain an informative latent feature space with the objective of
improving open-set performance. First, a decoder is used to ensure that clean
images can be reconstructed from the obtained latent features. Then,
self-supervision is used to ensure that the latent features are informative
enough to carry out an auxiliary task. We introduce a testing protocol to
evaluate OSAD performance and show the effectiveness of the proposed method in
multiple object classification datasets. The implementation code of the
proposed method is available at: https://github.com/rshaojimmy/ECCV2020-OSAD.",arxiv
http://arxiv.org/abs/2008.13223v2,2020-10-25T17:47:42Z,2020-08-30T17:29:43Z,"Deep Reinforcement Learning for Contact-Rich Skills Using Compliant
  Movement Primitives","In recent years, industrial robots have been installed in various industries
to handle advanced manufacturing and high precision tasks. However, further
integration of industrial robots is hampered by their limited flexibility,
adaptability and decision making skills compared to human operators. Assembly
tasks are especially challenging for robots since they are contact-rich and
sensitive to even small uncertainties. While reinforcement learning (RL) offers
a promising framework to learn contact-rich control policies from scratch, its
applicability to high-dimensional continuous state-action spaces remains rather
limited due to high brittleness and sample complexity. To address those issues,
we propose different pruning methods that facilitate convergence and
generalization. In particular, we divide the task into free and contact-rich
sub-tasks, perform the control in Cartesian rather than joint space, and
parameterize the control policy. Those pruning methods are naturally
implemented within the framework of dynamic movement primitives (DMP). To
handle contact-rich tasks, we extend the DMP framework by introducing a
coupling term that acts like the human wrist and provides active compliance
under contact with the environment. We demonstrate that the proposed method can
learn insertion skills that are invariant to space, size, shape, and closely
related scenarios, while handling large uncertainties. Finally we demonstrate
that the learned policy can be easily transferred from simulations to real
world and achieve similar performance on UR5e robot.",arxiv
http://arxiv.org/abs/2008.13585v1,2020-08-26T14:03:25Z,2020-08-26T14:03:25Z,At Your Service: Coffee Beans Recommendation From a Robot Assistant,"With advances in the field of machine learning, precisely algorithms for
recommendation systems, robot assistants are envisioned to become more present
in the hospitality industry. Additionally, the COVID-19 pandemic has also
highlighted the need to have more service robots in our everyday lives, to
minimise the risk of human to-human transmission. One such example would be
coffee shops, which have become intrinsic to our everyday lives. However,
serving an excellent cup of coffee is not a trivial feat as a coffee blend
typically comprises rich aromas, indulgent and unique flavours and a lingering
aftertaste. Our work addresses this by proposing a computational model which
recommends optimal coffee beans resulting from the user's preferences.
Specifically, given a set of coffee bean properties (objective features), we
apply different supervised learning techniques to predict coffee qualities
(subjective features). We then consider an unsupervised learning method to
analyse the relationship between coffee beans in the subjective feature space.
Evaluated on a real coffee beans dataset based on digitised reviews, our
results illustrate that the proposed computational model gives up to 92.7
percent recommendation accuracy for coffee beans prediction. From this, we
propose how this computational model can be deployed on a service robot to
reliably predict customers' coffee bean preferences, starting from the user
inputting their coffee preferences to the robot recommending the coffee beans
that best meet the user's likings.",arxiv
http://arxiv.org/abs/2008.08563v2,2020-08-30T11:05:13Z,2020-08-19T17:41:37Z,"Physically-Constrained Transfer Learning through Shared Abundance Space
  for Hyperspectral Image Classification","Hyperspectral image (HSI) classification is one of the most active research
topics and has achieved promising results boosted by the recent development of
deep learning. However, most state-of-the-art approaches tend to perform poorly
when the training and testing images are on different domains, e.g., source
domain and target domain, respectively, due to the spectral variability caused
by different acquisition conditions. Transfer learning-based methods address
this problem by pre-training in the source domain and fine-tuning on the target
domain. Nonetheless, a considerable amount of data on the target domain has to
be labeled and non-negligible computational resources are required to retrain
the whole network. In this paper, we propose a new transfer learning scheme to
bridge the gap between the source and target domains by projecting the HSI data
from the source and target domains into a shared abundance space based on their
own physical characteristics. In this way, the domain discrepancy would be
largely reduced such that the model trained on the source domain could be
applied on the target domain without extra efforts for data labeling or network
retraining. The proposed method is referred to as physically-constrained
transfer learning through shared abundance space (PCTL-SAS). Extensive
experimental results demonstrate the superiority of the proposed method as
compared to the state-of-the-art. The success of this endeavor would largely
facilitate the deployment of HSI classification for real-world sensing
scenarios.",arxiv
http://arxiv.org/abs/2008.05381v1,2020-08-12T15:29:11Z,2020-08-12T15:29:11Z,"Improving the Performance of Fine-Grain Image Classifiers via Generative
  Data Augmentation","Recent advances in machine learning (ML) and computer vision tools have
enabled applications in a wide variety of arenas such as financial analytics,
medical diagnostics, and even within the Department of Defense. However, their
widespread implementation in real-world use cases poses several challenges: (1)
many applications are highly specialized, and hence operate in a \emph{sparse
data} domain; (2) ML tools are sensitive to their training sets and typically
require cumbersome, labor-intensive data collection and data labelling
processes; and (3) ML tools can be extremely ""black box,"" offering users little
to no insight into the decision-making process or how new data might affect
prediction performance. To address these challenges, we have designed and
developed Data Augmentation from Proficient Pre-Training of Robust Generative
Adversarial Networks (DAPPER GAN), an ML analytics support tool that
automatically generates novel views of training images in order to improve
downstream classifier performance. DAPPER GAN leverages high-fidelity
embeddings generated by a StyleGAN2 model (trained on the LSUN cars dataset) to
create novel imagery for previously unseen classes. We experimentally evaluate
this technique on the Stanford Cars dataset, demonstrating improved vehicle
make and model classification accuracy and reduced requirements for real data
using our GAN based data augmentation framework. The method's validity was
supported through an analysis of classifier performance on both augmented and
non-augmented datasets, achieving comparable or better accuracy with up to 30\%
less real data across visually similar classes. To support this method, we
developed a novel augmentation method that can manipulate semantically
meaningful dimensions (e.g., orientation) of the target object in the embedding
space.",arxiv
http://arxiv.org/abs/2008.01567v3,2021-02-20T01:06:06Z,2020-08-01T02:32:43Z,"Multi-Slice Fusion for Sparse-View and Limited-Angle 4D CT
  Reconstruction","Inverse problems spanning four or more dimensions such as space, time and
other independent parameters have become increasingly important.
State-of-the-art 4D reconstruction methods use model based iterative
reconstruction (MBIR), but depend critically on the quality of the prior
modeling. Recently, plug-and-play (PnP) methods have been shown to be an
effective way to incorporate advanced prior models using state-of-the-art
denoising algorithms. However, state-of-the-art denoisers such as BM4D and deep
convolutional neural networks (CNNs) are primarily available for 2D or 3D
images and extending them to higher dimensions is difficult due to algorithmic
complexity and the increased difficulty of effective training.
  In this paper, we present multi-slice fusion, a novel algorithm for 4D
reconstruction, based on the fusion of multiple low-dimensional denoisers. Our
approach uses multi-agent consensus equilibrium (MACE), an extension of
plug-and-play, as a framework for integrating the multiple lower-dimensional
models. We apply our method to 4D cone-beam X-ray CT reconstruction for non
destructive evaluation (NDE) of samples that are dynamically moving during
acquisition. We implement multi-slice fusion on distributed, heterogeneous
clusters in order to reconstruct large 4D volumes in reasonable time and
demonstrate the inherent parallelizable nature of the algorithm. We present
simulated and real experimental results on sparse-view and limited-angle CT
data to demonstrate that multi-slice fusion can substantially improve the
quality of reconstructions relative to traditional methods, while also being
practical to implement and train.",arxiv
http://arxiv.org/abs/2007.14573v2,2021-06-02T03:00:12Z,2020-07-29T03:33:18Z,FIVES: Feature Interaction Via Edge Search for Large-Scale Tabular Data,"High-order interactive features capture the correlation between different
columns and thus are promising to enhance various learning tasks on ubiquitous
tabular data. To automate the generation of interactive features, existing
works either explicitly traverse the feature space or implicitly express the
interactions via intermediate activations of some designed models. These two
kinds of methods show that there is essentially a trade-off between feature
interpretability and search efficiency. To possess both of their merits, we
propose a novel method named Feature Interaction Via Edge Search (FIVES), which
formulates the task of interactive feature generation as searching for edges on
the defined feature graph. Specifically, we first present our theoretical
evidence that motivates us to search for useful interactive features with
increasing order. Then we instantiate this search strategy by optimizing both a
dedicated graph neural network (GNN) and the adjacency tensor associated with
the defined feature graph. In this way, the proposed FIVES method simplifies
the time-consuming traversal as a typical training course of GNN and enables
explicit feature generation according to the learned adjacency tensor.
Experimental results on both benchmark and real-world datasets show the
advantages of FIVES over several state-of-the-art methods. Moreover, the
interactive features identified by FIVES are deployed on the recommender system
of Taobao, a worldwide leading e-commerce platform. Results of an online A/B
testing further verify the effectiveness of the proposed method FIVES, and we
further provide FIVES as AI utilities for the customers of Alibaba Cloud.",arxiv
http://arxiv.org/abs/2007.14390v5,2022-03-05T20:30:32Z,2020-07-28T17:59:07Z,Flower: A Friendly Federated Learning Research Framework,"Federated Learning (FL) has emerged as a promising technique for edge devices
to collaboratively learn a shared prediction model, while keeping their
training data on the device, thereby decoupling the ability to do machine
learning from the need to store the data in the cloud. However, FL is difficult
to implement realistically, both in terms of scale and systems heterogeneity.
Although there are a number of research frameworks available to simulate FL
algorithms, they do not support the study of scalable FL workloads on
heterogeneous edge devices.
  In this paper, we present Flower -- a comprehensive FL framework that
distinguishes itself from existing platforms by offering new facilities to
execute large-scale FL experiments and consider richly heterogeneous FL device
scenarios. Our experiments show Flower can perform FL experiments up to 15M in
client size using only a pair of high-end GPUs. Researchers can then seamlessly
migrate experiments to real devices to examine other parts of the design space.
We believe Flower provides the community with a critical new tool for FL study
and development.",arxiv
http://arxiv.org/abs/2007.12640v1,2020-07-24T16:50:41Z,2020-07-24T16:50:41Z,"Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning
  on Graphs","We consider an autonomous exploration problem in which a range-sensing mobile
robot is tasked with accurately mapping the landmarks in an a priori unknown
environment efficiently in real-time; it must choose sensing actions that both
curb localization uncertainty and achieve information gain. For this problem,
belief space planning methods that forward-simulate robot sensing and
estimation may often fail in real-time implementation, scaling poorly with
increasing size of the state, belief and action spaces. We propose a novel
approach that uses graph neural networks (GNNs) in conjunction with deep
reinforcement learning (DRL), enabling decision-making over graphs containing
exploration information to predict a robot's optimal sensing action in belief
space. The policy, which is trained in different random environments without
human intervention, offers a real-time, scalable decision-making process whose
high-performance exploratory sensing actions yield accurate maps and high rates
of information gain.",arxiv
http://arxiv.org/abs/2007.12327v2,2021-06-26T00:31:19Z,2020-07-24T03:09:26Z,"Stochastic Dynamic Information Flow Tracking Game using Supervised
  Learning for Detecting Advanced Persistent Threats","Advanced persistent threats (APTs) are organized prolonged cyberattacks by
sophisticated attackers. Although APT activities are stealthy, they interact
with the system components and these interactions lead to information flows.
Dynamic Information Flow Tracking (DIFT) has been proposed as one of the
effective ways to detect APTs using the information flows. However, wide range
security analysis using DIFT results in a significant increase in performance
overhead and high rates of false-positives and false-negatives generated by
DIFT. In this paper, we model the strategic interaction between APT and DIFT as
a non-cooperative stochastic game. The game unfolds on a state space
constructed from an information flow graph (IFG) that is extracted from the
system log. The objective of the APT in the game is to choose transitions in
the IFG to find an optimal path in the IFG from an entry point of the attack to
an attack target. On the other hand, the objective of DIFT is to dynamically
select nodes in the IFG to perform security analysis for detecting APT. Our
game model has imperfect information as the players do not have information
about the actions of the opponent. We consider two scenarios of the game (i)
when the false-positive and false-negative rates are known to both players and
(ii) when the false-positive and false-negative rates are unknown to both
players. Case (i) translates to a game model with complete information and we
propose a value iteration-based algorithm and prove the convergence. Case (ii)
translates to a game with unknown transition probabilities. In this case, we
propose Hierarchical Supervised Learning (HSL) algorithm that integrates a
neural network, to predict the value vector of the game, with a policy
iteration algorithm to compute an approximate equilibrium. We implemented our
algorithms on real attack datasets and validated the performance of our
approach.",arxiv
http://arxiv.org/abs/2007.10243v1,2020-07-20T16:32:27Z,2020-07-20T16:32:27Z,Inter-Homines: Distance-Based Risk Estimation for Human Safety,"In this document, we report our proposal for modeling the risk of possible
contagiousity in a given area monitored by RGB cameras where people freely move
and interact. Our system, called Inter-Homines, evaluates in real-time the
contagion risk in a monitored area by analyzing video streams: it is able to
locate people in 3D space, calculate interpersonal distances and predict risk
levels by building dynamic maps of the monitored area. Inter-Homines works both
indoor and outdoor, in public and private crowded areas. The software is
applicable to already installed cameras or low-cost cameras on industrial PCs,
equipped with an additional embedded edge-AI system for temporary measurements.
From the AI-side, we exploit a robust pipeline for real-time people detection
and localization in the ground plane by homographic transformation based on
state-of-the-art computer vision algorithms; it is a combination of a people
detector and a pose estimator. From the risk modeling side, we propose a
parametric model for a spatio-temporal dynamic risk estimation, that, validated
by epidemiologists, could be useful for safety monitoring the acceptance of
social distancing prevention measures by predicting the risk level of the
scene.",arxiv
http://arxiv.org/abs/2007.10227v2,2020-08-29T19:38:40Z,2020-07-20T16:17:27Z,"Nengo and low-power AI hardware for robust, embedded neurorobotics","In this paper we demonstrate how the Nengo neural modeling and simulation
libraries enable users to quickly develop robotic perception and action neural
networks for simulation on neuromorphic hardware using familiar tools, such as
Keras and Python. We identify four primary challenges in building robust,
embedded neurorobotic systems: 1) developing infrastructure for interfacing
with the environment and sensors; 2) processing task specific sensory signals;
3) generating robust, explainable control signals; and 4) compiling neural
networks to run on target hardware. Nengo helps to address these challenges by:
1) providing the NengoInterfaces library, which defines a simple but powerful
API for users to interact with simulations and hardware; 2) providing the
NengoDL library, which lets users use the Keras and TensorFlow API to develop
Nengo models; 3) implementing the Neural Engineering Framework, which provides
white-box methods for implementing known functions and circuits; and 4)
providing multiple backend libraries, such as NengoLoihi, that enable users to
compile the same model to different hardware. We present two examples using
Nengo to develop neural networks that run on CPUs, GPUs, and Intel's
neuromorphic chip, Loihi, to demonstrate this workflow. The first example is an
end-to-end spiking neural network that controls a rover simulated in Mujoco.
The network integrates a deep convolutional network that processes visual input
from mounted cameras to track a target, and a control system implementing
steering and drive functions to guide the rover to the target. The second
example augments a force-based operational space controller with neural
adaptive control to improve performance during a reaching task using a
real-world Kinova Jaco2 robotic arm. Code and details are provided with the
intent of enabling other researchers to build their own neurorobotic systems.",arxiv
http://arxiv.org/abs/2007.09055v1,2020-07-17T15:30:38Z,2020-07-17T15:30:38Z,Hyperparameter Selection for Offline Reinforcement Learning,"Offline reinforcement learning (RL purely from logged data) is an important
avenue for deploying RL techniques in real-world scenarios. However, existing
hyperparameter selection methods for offline RL break the offline assumption by
evaluating policies corresponding to each hyperparameter setting in the
environment. This online execution is often infeasible and hence undermines the
main aim of offline RL. Therefore, in this work, we focus on \textit{offline
hyperparameter selection}, i.e. methods for choosing the best policy from a set
of many policies trained using different hyperparameters, given only logged
data. Through large-scale empirical evaluation we show that: 1) offline RL
algorithms are not robust to hyperparameter choices, 2) factors such as the
offline RL algorithm and method for estimating Q values can have a big impact
on hyperparameter selection, and 3) when we control those factors carefully, we
can reliably rank policies across hyperparameter choices, and therefore choose
policies which are close to the best policy in the set. Overall, our results
present an optimistic view that offline hyperparameter selection is within
reach, even in challenging tasks with pixel observations, high dimensional
action spaces, and long horizon.",arxiv
http://arxiv.org/abs/2007.10784v2,2021-07-10T17:50:27Z,2020-07-16T21:14:45Z,Fast Neural Models for Symbolic Regression at Scale,"Deep learning owes much of its success to the astonishing expressiveness of
neural networks. However, this comes at the cost of complex, black-boxed models
that extrapolate poorly beyond the domain of the training dataset, conflicting
with goals of finding analytic expressions to describe science, engineering and
real world data. Under the hypothesis that the hierarchical modularity of such
laws can be captured by training a neural network, we introduce OccamNet, a
neural network model that finds interpretable, compact, and sparse solutions
for fitting data, \`{a} la Occam's razor. Our model defines a probability
distribution over a non-differentiable function space. We introduce a two-step
optimization method that samples functions and updates the weights with
backpropagation based on cross-entropy matching in an evolutionary strategy: we
train by biasing the probability mass toward better fitting solutions. OccamNet
is able to fit a variety of symbolic laws including simple analytic functions,
recursive programs, implicit functions, simple image classification, and can
outperform noticeably state-of-the-art symbolic regression methods on real
world regression datasets. Our method requires minimal memory footprint, does
not require AI accelerators for efficient training, fits complicated functions
in minutes of training on a single CPU, and demonstrates significant
performance gains when scaled on a GPU. Our implementation, demonstrations and
instructions for reproducing the experiments are available at
https://github.com/druidowm/OccamNet_Public.",arxiv
http://arxiv.org/abs/2007.09072v1,2020-07-15T09:40:13Z,2020-07-15T09:40:13Z,"Joint Multi-User DNN Partitioning and Computational Resource Allocation
  for Collaborative Edge Intelligence","Mobile Edge Computing (MEC) has emerged as a promising supporting
architecture providing a variety of resources to the network edge, thus acting
as an enabler for edge intelligence services empowering massive mobile and
Internet of Things (IoT) devices with AI capability. With the assistance of
edge servers, user equipments (UEs) are able to run deep neural network (DNN)
based AI applications, which are generally resource-hungry and
compute-intensive, such that an individual UE can hardly afford by itself in
real time. However the resources in each individual edge server are typically
limited. Therefore, any resource optimization involving edge servers is by
nature a resource-constrained optimization problem and needs to be tackled in
such realistic context. Motivated by this observation, we investigate the
optimization problem of DNN partitioning (an emerging DNN offloading scheme) in
a realistic multi-user resource-constrained condition that rarely considered in
previous works. Despite the extremely large solution space, we reveal several
properties of this specific optimization problem of joint multi-UE DNN
partitioning and computational resource allocation. We propose an algorithm
called Iterative Alternating Optimization (IAO) that can achieve the optimal
solution in polynomial time. In addition, we present rigorous theoretic
analysis of our algorithm in terms of time complexity and performance under
realistic estimation error. Moreover, we build a prototype that implements our
framework and conduct extensive experiments using realistic DNN models, whose
results demonstrate its effectiveness and efficiency.",arxiv
http://arxiv.org/abs/2007.05889v5,2021-04-12T17:23:01Z,2020-07-12T02:21:19Z,"Deep Learning Techniques to make Gravitational Wave Detections from Weak
  Time-Series Data","Gravitational waves are ripples in the space time fabric when high energy
events such as black hole mergers or neutron star collisions take place. The
first Gravitational Wave (GW) detection (GW150914) was made by the Laser
Interferometer Gravitational-wave Observatory (LIGO) and Virgo Collaboration on
September 14, 2015. Furthermore, the proof of the existence of GWs had
countless implications from Stellar Evolution to General Relativity.
Gravitational waves detection requires multiple filters and the filtered data
has to be studied intensively to come to conclusions on whether the data is a
just a glitch or an actual gravitational wave detection. However, with the use
of Deep Learning the process is simplified heavily, as it reduces the level of
filtering greatly, and the output is more definitive, even though the model
produces a probabilistic result. Our technique, Deep Learning, utilizes a
different implementation of a one-dimensional convolutional neural network
(CNN). The model is trained by a composite of real LIGO noise, and injections
of GW waveform templates. The CNN effectively uses classification to
differentiate weak GW time series from non-gaussian noise from glitches in the
LIGO data stream. In addition, we are the first study to utilize fine-tuning as
a means to train the model with a second pass of data, while maintaining all
the learned features from the initial training iteration. This enables our
model to have a sensitivity of 100%, higher than all prior studies in this
field, when making real-time detections of GWs at an extremely low
Signal-to-noise ratios (SNR), while still being less computationally expensive.
This sensitivity, in part, is also achieved through the use of deep signal
manifolds from both the Hanford and Livingston detectors, which enable the
neural network to be responsive to false positives.",arxiv
http://arxiv.org/abs/2007.04862v2,2020-07-12T15:32:16Z,2020-07-09T15:04:26Z,Attention or memory? Neurointerpretable agents in space and time,"In neuroscience, attention has been shown to bidirectionally interact with
reinforcement learning (RL) processes. This interaction is thought to support
dimensionality reduction of task representations, restricting computations to
relevant features. However, it remains unclear whether these properties can
translate into real algorithmic advantages for artificial agents, especially in
dynamic environments. We design a model incorporating a self-attention
mechanism that implements task-state representations in semantic feature-space,
and test it on a battery of Atari games. To evaluate the agent's selective
properties, we add a large volume of task-irrelevant features to observations.
In line with neuroscience predictions, self-attention leads to increased
robustness to noise compared to benchmark models. Strikingly, this
self-attention mechanism is general enough, such that it can be naturally
extended to implement a transient working-memory, able to solve a partially
observable maze task. Lastly, we highlight the predictive quality of attended
stimuli. Because we use semantic observations, we can uncover not only which
features the agent elects to base decisions on, but also how it chooses to
compile more complex, relational features from simpler ones. These results
formally illustrate the benefits of attention in deep RL and provide evidence
for the interpretability of self-attention mechanisms.",arxiv
http://arxiv.org/abs/2008.03226v1,2020-06-28T20:59:03Z,2020-06-28T20:59:03Z,"The Photoswitch Dataset: A Molecular Machine Learning Benchmark for the
  Advancement of Synthetic Chemistry","The space of synthesizable molecules is greater than $10^{60}$, meaning only
a vanishingly small fraction of these molecules have ever been realized in the
lab. In order to prioritize which regions of this space to explore next,
synthetic chemists need access to accurate molecular property predictions.
While great advances in molecular machine learning have been made, there is a
dearth of benchmarks featuring properties that are useful for the synthetic
chemist. Focussing directly on the needs of the synthetic chemist, we introduce
the Photoswitch Dataset, a new benchmark for molecular machine learning where
improvements in model performance can be immediately observed in the throughput
of promising molecules synthesized in the lab. Photoswitches are a versatile
class of molecule for medical and renewable energy applications where a
molecule's efficacy is governed by its electronic transition wavelengths. We
demonstrate superior performance in predicting these wavelengths compared to
both time-dependent density functional theory (TD-DFT), the incumbent first
principles quantum mechanical approach, as well as a panel of human experts.
Our baseline models are currently being deployed in the lab as part of the
decision process for candidate synthesis. It is our hope that this benchmark
can drive real discoveries in photoswitch chemistry and that future benchmarks
can be introduced to pivot learning algorithm development to benefit more
expansive areas of synthetic chemistry.",arxiv
http://arxiv.org/abs/2006.15350v1,2020-06-27T12:13:22Z,2020-06-27T12:13:22Z,"MiniNet: An extremely lightweight convolutional neural network for
  real-time unsupervised monocular depth estimation","Predicting depth from a single image is an attractive research topic since it
provides one more dimension of information to enable machines to better
perceive the world. Recently, deep learning has emerged as an effective
approach to monocular depth estimation. As obtaining labeled data is costly,
there is a recent trend to move from supervised learning to unsupervised
learning to obtain monocular depth. However, most unsupervised learning methods
capable of achieving high depth prediction accuracy will require a deep network
architecture which will be too heavy and complex to run on embedded devices
with limited storage and memory spaces. To address this issue, we propose a new
powerful network with a recurrent module to achieve the capability of a deep
network while at the same time maintaining an extremely lightweight size for
real-time high performance unsupervised monocular depth prediction from video
sequences. Besides, a novel efficient upsample block is proposed to fuse the
features from the associated encoder layer and recover the spatial size of
features with the small number of model parameters. We validate the
effectiveness of our approach via extensive experiments on the KITTI dataset.
Our new model can run at a speed of about 110 frames per second (fps) on a
single GPU, 37 fps on a single CPU, and 2 fps on a Raspberry Pi 3. Moreover, it
achieves higher depth accuracy with nearly 33 times fewer model parameters than
state-of-the-art models. To the best of our knowledge, this work is the first
extremely lightweight neural network trained on monocular video sequences for
real-time unsupervised monocular depth estimation, which opens up the
possibility of implementing deep learning-based real-time unsupervised
monocular depth prediction on low-cost embedded devices.",arxiv
http://arxiv.org/abs/2006.13379v1,2020-06-23T23:15:54Z,2020-06-23T23:15:54Z,Deep Generative Model-based Quality Control for Cardiac MRI Segmentation,"In recent years, convolutional neural networks have demonstrated promising
performance in a variety of medical image segmentation tasks. However, when a
trained segmentation model is deployed into the real clinical world, the model
may not perform optimally. A major challenge is the potential poor-quality
segmentations generated due to degraded image quality or domain shift issues.
There is a timely need to develop an automated quality control method that can
detect poor segmentations and feedback to clinicians. Here we propose a novel
deep generative model-based framework for quality control of cardiac MRI
segmentation. It first learns a manifold of good-quality image-segmentation
pairs using a generative model. The quality of a given test segmentation is
then assessed by evaluating the difference from its projection onto the
good-quality manifold. In particular, the projection is refined through
iterative search in the latent space. The proposed method achieves high
prediction accuracy on two publicly available cardiac MRI datasets. Moreover,
it shows better generalisation ability than traditional regression-based
methods. Our approach provides a real-time and model-agnostic quality control
for cardiac MRI segmentation, which has the potential to be integrated into
clinical image analysis workflows.",arxiv
http://arxiv.org/abs/2006.13018v3,2021-01-20T22:37:14Z,2020-06-21T07:04:16Z,The classification for High-dimension low-sample size data,"Huge amount of applications in various fields, such as gene expression
analysis or computer vision, undergo data sets with high-dimensional
low-sample-size (HDLSS), which has putted forward great challenges for standard
statistical and modern machine learning methods. In this paper, we propose a
novel classification criterion on HDLSS, tolerance similarity, which emphasizes
the maximization of within-class variance on the premise of class separability.
According to this criterion, a novel linear binary classifier is designed,
denoted by No-separated Data Maximum Dispersion classifier (NPDMD). The
objective of NPDMD is to find a projecting direction w in which all of training
samples scatter in as large an interval as possible. NPDMD has several
characteristics compared to the state-of-the-art classification methods. First,
it works well on HDLSS. Second, it combines the sample statistical information
and local structural information (supporting vectors) into the objective
function to find the solution of projecting direction in the whole feature
spaces. Third, it solves the inverse of high dimensional matrix in low
dimensional space. Fourth, it is relatively simple to be implemented based on
Quadratic Programming. Fifth, it is robust to the model specification for
various real applications. The theoretical properties of NPDMD are deduced. We
conduct a series of evaluations on one simulated and six real-world benchmark
data sets, including face classification and mRNA classification. NPDMD
outperforms those widely used approaches in most cases, or at least obtains
comparable results.",arxiv
http://arxiv.org/abs/2006.09191v2,2020-10-25T23:11:44Z,2020-06-16T14:34:40Z,"Sample-Efficient Optimization in the Latent Space of Deep Generative
  Models via Weighted Retraining","Many important problems in science and engineering, such as drug design,
involve optimizing an expensive black-box objective function over a complex,
high-dimensional, and structured input space. Although machine learning
techniques have shown promise in solving such problems, existing approaches
substantially lack sample efficiency. We introduce an improved method for
efficient black-box optimization, which performs the optimization in the
low-dimensional, continuous latent manifold learned by a deep generative model.
In contrast to previous approaches, we actively steer the generative model to
maintain a latent manifold that is highly useful for efficiently optimizing the
objective. We achieve this by periodically retraining the generative model on
the data points queried along the optimization trajectory, as well as weighting
those data points according to their objective function value. This weighted
retraining can be easily implemented on top of existing methods, and is
empirically shown to significantly improve their efficiency and performance on
synthetic and real-world optimization problems.",arxiv
http://arxiv.org/abs/2006.08852v1,2020-06-16T01:04:26Z,2020-06-16T01:04:26Z,Counterexample-Guided Learning of Monotonic Neural Networks,"The widespread adoption of deep learning is often attributed to its automatic
feature construction with minimal inductive bias. However, in many real-world
tasks, the learned function is intended to satisfy domain-specific constraints.
We focus on monotonicity constraints, which are common and require that the
function's output increases with increasing values of specific input features.
We develop a counterexample-guided technique to provably enforce monotonicity
constraints at prediction time. Additionally, we propose a technique to use
monotonicity as an inductive bias for deep learning. It works by iteratively
incorporating monotonicity counterexamples in the learning process. Contrary to
prior work in monotonic learning, we target general ReLU neural networks and do
not further restrict the hypothesis space. We have implemented these techniques
in a tool called COMET. Experiments on real-world datasets demonstrate that our
approach achieves state-of-the-art results compared to existing monotonic
learners, and can improve the model quality compared to those that were trained
without taking monotonicity constraints into account.",arxiv
http://arxiv.org/abs/2006.04924v1,2020-06-08T20:42:39Z,2020-06-08T20:42:39Z,A Self-supervised Approach for Adversarial Robustness,"Adversarial examples can cause catastrophic mistakes in Deep Neural Network
(DNNs) based vision systems e.g., for classification, segmentation and object
detection. The vulnerability of DNNs against such attacks can prove a major
roadblock towards their real-world deployment. Transferability of adversarial
examples demand generalizable defenses that can provide cross-task protection.
Adversarial training that enhances robustness by modifying target model's
parameters lacks such generalizability. On the other hand, different input
processing based defenses fall short in the face of continuously evolving
attacks. In this paper, we take the first step to combine the benefits of both
approaches and propose a self-supervised adversarial training mechanism in the
input space. By design, our defense is a generalizable approach and provides
significant robustness against the \textbf{unseen} adversarial attacks (\eg by
reducing the success rate of translation-invariant \textbf{ensemble} attack
from 82.6\% to 31.9\% in comparison to previous state-of-the-art). It can be
deployed as a plug-and-play solution to protect a variety of vision systems, as
we demonstrate for the case of classification, segmentation and detection. Code
is available at: {\small\url{https://github.com/Muzammal-Naseer/NRP}}.",arxiv
http://arxiv.org/abs/2006.04403v1,2020-06-08T08:09:20Z,2020-06-08T08:09:20Z,Global Robustness Verification Networks,"The wide deployment of deep neural networks, though achieving great success
in many domains, has severe safety and reliability concerns. Existing
adversarial attack generation and automatic verification techniques cannot
formally verify whether a network is globally robust, i.e., the absence or not
of adversarial examples in the input space. To address this problem, we develop
a global robustness verification framework with three components: 1) a novel
rule-based ``back-propagation'' finding which input region is responsible for
the class assignment by logic reasoning; 2) a new network architecture Sliding
Door Network (SDN) enabling feasible rule-based ``back-propagation''; 3) a
region-based global robustness verification (RGRV) approach. Moreover, we
demonstrate the effectiveness of our approach on both synthetic and real
datasets.",arxiv
http://arxiv.org/abs/2005.14302v1,2020-05-28T21:25:21Z,2020-05-28T21:25:21Z,Monocular Depth Estimators: Vulnerabilities and Attacks,"Recent advancements of neural networks lead to reliable monocular depth
estimation. Monocular depth estimated techniques have the upper hand over
traditional depth estimation techniques as it only needs one image during
inference. Depth estimation is one of the essential tasks in robotics, and
monocular depth estimation has a wide variety of safety-critical applications
like in self-driving cars and surgical devices. Thus, the robustness of such
techniques is very crucial. It has been shown in recent works that these deep
neural networks are highly vulnerable to adversarial samples for tasks like
classification, detection and segmentation. These adversarial samples can
completely ruin the output of the system, making their credibility in real-time
deployment questionable. In this paper, we investigate the robustness of the
most state-of-the-art monocular depth estimation networks against adversarial
attacks. Our experiments show that tiny perturbations on an image that are
invisible to the naked eye (perturbation attack) and corruption less than about
1% of an image (patch attack) can affect the depth estimation drastically. We
introduce a novel deep feature annihilation loss that corrupts the hidden
feature space representation forcing the decoder of the network to output poor
depth maps. The white-box and black-box test compliments the effectiveness of
the proposed attack. We also perform adversarial example transferability tests,
mainly cross-data transferability.",arxiv
http://arxiv.org/abs/2005.13506v4,2020-09-30T09:58:49Z,2020-05-27T17:32:51Z,"Non-Intrusive Reduced-Order Modeling Using Uncertainty-Aware Deep Neural
  Networks and Proper Orthogonal Decomposition: Application to Flood Modeling","Deep Learning research is advancing at a fantastic rate, and there is much to
gain from transferring this knowledge to older fields like Computational Fluid
Dynamics in practical engineering contexts. This work compares state-of-the-art
methods that address uncertainty quantification in Deep Neural Networks,
pushing forward the reduced-order modeling approach of Proper Orthogonal
Decomposition-Neural Networks (POD-NN) with Deep Ensembles and Variational
Inference-based Bayesian Neural Networks on two-dimensional problems in space.
These are first tested on benchmark problems, and then applied to a real-life
application: flooding predictions in the Mille \^Iles river in the Montreal,
Quebec, Canada metropolitan area. Our setup involves a set of input parameters,
with a potentially noisy distribution, and accumulates the simulation data
resulting from these parameters. The goal is to build a non-intrusive surrogate
model that is able to know when it does not know, which is still an open
research area in Neural Networks (and in AI in general). With the help of this
model, probabilistic flooding maps are generated, aware of the model
uncertainty. These insights on the unknown are also utilized for an uncertainty
propagation task, allowing for flooded area predictions that are broader and
safer than those made with a regular uncertainty-uninformed surrogate model.
Our study of the time-dependent and highly nonlinear case of a dam break is
also presented. Both the ensembles and the Bayesian approach lead to reliable
results for multiple smooth physical solutions, providing the correct warning
when going out-of-distribution. However, the former, referred to as POD-EnsNN,
proved much easier to implement and showed greater flexibility than the latter
in the case of discontinuities, where standard algorithms may oscillate or fail
to converge.",arxiv
http://arxiv.org/abs/2005.13373v1,2020-05-27T14:08:46Z,2020-05-27T14:08:46Z,Deep Learning Assisted Data Inspection for Radio Astronomy,"Modern radio telescopes combine thousands of receivers, long-distance
networks, large-scale compute hardware, and intricate software. Due to this
complexity, failures occur relatively frequently. In this work we propose novel
use of unsupervised deep learning to diagnose system health for modern radio
telescopes. The model is a convolutional Variational Autoencoder (VAE) that
enables the projection of the high dimensional time-frequency data to a
low-dimensional prescriptive space. Using this projection, telescope operators
are able to visually inspect failures thereby maintaining system health. We
have trained and evaluated the performance of the VAE quantitatively in
controlled experiments on simulated data from HERA. Moreover, we present a
qualitative assessment of the the model trained and tested on real LOFAR data.
Through the use of a naive SVM classifier on the projected synthesised data, we
show that there is a trade-off between the dimensionality of the projection and
the number of compounded features in a given spectrogram. The VAE and SVM
combination scores between 65% and 90% accuracy depending on the number of
features in a given input. Finally, we show the prototype
system-health-diagnostic web framework that integrates the evaluated model. The
system is currently undergoing testing at the ASTRON observatory.",arxiv
http://arxiv.org/abs/2005.12392v2,2020-09-11T04:22:34Z,2020-05-25T20:39:59Z,MTFuzz: Fuzzing with a Multi-Task Neural Network,"Fuzzing is a widely used technique for detecting software bugs and
vulnerabilities. Most popular fuzzers generate new inputs using an evolutionary
search to maximize code coverage. Essentially, these fuzzers start with a set
of seed inputs, mutate them to generate new inputs, and identify the promising
inputs using an evolutionary fitness function for further mutation. Despite
their success, evolutionary fuzzers tend to get stuck in long sequences of
unproductive mutations. In recent years, machine learning (ML) based mutation
strategies have reported promising results. However, the existing ML-based
fuzzers are limited by the lack of quality and diversity of the training data.
As the input space of the target programs is high dimensional and sparse, it is
prohibitively expensive to collect many diverse samples demonstrating
successful and unsuccessful mutations to train the model. In this paper, we
address these issues by using a Multi-Task Neural Network that can learn a
compact embedding of the input space based on diverse training samples for
multiple related tasks (i.e., predicting for different types of coverage). The
compact embedding can guide the mutation process by focusing most of the
mutations on the parts of the embedding where the gradient is high. \tool
uncovers $11$ previously unseen bugs and achieves an average of $2\times$ more
edge coverage compared with 5 state-of-the-art fuzzer on 10 real-world
programs.",arxiv
http://arxiv.org/abs/2005.10224v2,2021-06-05T22:47:16Z,2020-05-20T17:41:40Z,The Random Feature Model for Input-Output Maps between Banach Spaces,"Well known to the machine learning community, the random feature model is a
parametric approximation to kernel interpolation or regression methods. It is
typically used to approximate functions mapping a finite-dimensional input
space to the real line. In this paper, we instead propose a methodology for use
of the random feature model as a data-driven surrogate for operators that map
an input Banach space to an output Banach space. Although the methodology is
quite general, we consider operators defined by partial differential equations
(PDEs); here, the inputs and outputs are themselves functions, with the input
parameters being functions required to specify the problem, such as initial
data or coefficients, and the outputs being solutions of the problem. Upon
discretization, the model inherits several desirable attributes from this
infinite-dimensional viewpoint, including mesh-invariant approximation error
with respect to the true PDE solution map and the capability to be trained at
one mesh resolution and then deployed at different mesh resolutions. We view
the random feature model as a non-intrusive data-driven emulator, provide a
mathematical framework for its interpretation, and demonstrate its ability to
efficiently and accurately approximate the nonlinear parameter-to-solution maps
of two prototypical PDEs arising in physical science and engineering
applications: viscous Burgers' equation and a variable coefficient elliptic
equation.",arxiv
http://arxiv.org/abs/2005.08859v2,2021-08-13T17:58:10Z,2020-05-18T16:34:11Z,"PDE constraints on smooth hierarchical functions computed by neural
  networks","Neural networks are versatile tools for computation, having the ability to
approximate a broad range of functions. An important problem in the theory of
deep neural networks is expressivity; that is, we want to understand the
functions that are computable by a given network. We study real infinitely
differentiable (smooth) hierarchical functions implemented by feedforward
neural networks via composing simpler functions in two cases:
  1) each constituent function of the composition has fewer inputs than the
resulting function;
  2) constituent functions are in the more specific yet prevalent form of a
non-linear univariate function (e.g. tanh) applied to a linear multivariate
function.
  We establish that in each of these regimes there exist non-trivial algebraic
partial differential equations (PDEs), which are satisfied by the computed
functions. These PDEs are purely in terms of the partial derivatives and are
dependent only on the topology of the network. For compositions of polynomial
functions, the algebraic PDEs yield non-trivial equations (of degrees dependent
only on the architecture) in the ambient polynomial space that are satisfied on
the associated functional varieties. Conversely, we conjecture that such PDE
constraints, once accompanied by appropriate non-singularity conditions and
perhaps certain inequalities involving partial derivatives, guarantee that the
smooth function under consideration can be represented by the network. The
conjecture is verified in numerous examples including the case of tree
architectures which are of neuroscientific interest. Our approach is a step
toward formulating an algebraic description of functional spaces associated
with specific neural networks, and may provide new, useful tools for
constructing neural networks.",arxiv
http://arxiv.org/abs/2005.07460v1,2020-05-15T10:34:51Z,2020-05-15T10:34:51Z,"Collective Risk Minimization via a Bayesian Model for Statistical
  Software Testing","In the last four years, the number of distinct autonomous vehicles platforms
deployed in the streets of California increased 6-fold, while the reported
accidents increased 12-fold. This can become a trend with no signs of subsiding
as it is fueled by a constant stream of innovations in hardware sensors and
machine learning software. Meanwhile, if we expect the public and regulators to
trust the autonomous vehicle platforms, we need to find better ways to solve
the problem of adding technological complexity without increasing the risk of
accidents. We studied this problem from the perspective of reliability
engineering in which a given risk of an accident has severity and probability
of occurring. Timely information on accidents is important for engineers to
anticipate and reuse previous failures to approximate the risk of accidents in
a new city. However, this is challenging in the context of autonomous vehicles
because of the sparse nature of data on the operational scenarios (driving
trajectories in a new city). Our approach was to mitigate data sparsity by
reducing the state space through monitoring of multiple-vehicles operations. We
then minimized the risk of accidents by determining proper allocation of tests
for each equivalence class. Our contributions comprise (1) a set of strategies
to monitor the operational data of multiple autonomous vehicles, (2) a Bayesian
model that estimates changes in the risk of accidents, and (3) a feedback
control-loop that minimizes these risks by reallocating test effort. Our
results are promising in the sense that we were able to measure and control
risk for a diversity of changes in the operational scenarios. We evaluated our
models with data from two real cities with distinct traffic patterns and made
the data available for the community.",arxiv
http://arxiv.org/abs/2005.04849v2,2020-10-16T03:59:24Z,2020-05-11T03:45:22Z,Revealing hidden dynamics from time-series data by ODENet,"To derive the hidden dynamics from observed data is one of the fundamental
but also challenging problems in many different fields. In this study, we
propose a new type of interpretable network called the ordinary differential
equation network (ODENet), in which the numerical integration of explicit
ordinary differential equations (ODEs) are embedded into the machine learning
scheme to build a general framework for revealing the hidden dynamics buried in
massive time-series data efficiently and reliably. ODENet takes full advantage
of both machine learning algorithms and ODE modeling. On one hand, the
embedding of ODEs makes the framework more interpretable benefiting from the
mature theories of ODEs. On the other hand, the schemes of machine learning
enable data handling, paralleling, and optimization to be easily and
efficiently implemented. From classical Lotka-Volterra equations to chaotic
Lorenz equations, the ODENet exhibits its remarkable capability in handling
time-series data even in the presence of large noise. We further apply the
ODENet to real actin aggregation data, which shows an impressive performance as
well. These results demonstrate the superiority of ODENet in dealing with noisy
data, data with either non-equal spacing or large sampling time steps over
other traditional machine learning algorithms.",arxiv
http://arxiv.org/abs/2005.02979v3,2021-10-14T16:40:00Z,2020-05-06T17:31:51Z,"A Survey of Algorithms for Black-Box Safety Validation of Cyber-Physical
  Systems","Autonomous cyber-physical systems (CPS) can improve safety and efficiency for
safety-critical applications, but require rigorous testing before deployment.
The complexity of these systems often precludes the use of formal verification
and real-world testing can be too dangerous during development. Therefore,
simulation-based techniques have been developed that treat the system under
test as a black box operating in a simulated environment. Safety validation
tasks include finding disturbances in the environment that cause the system to
fail (falsification), finding the most-likely failure, and estimating the
probability that the system fails. Motivated by the prevalence of
safety-critical artificial intelligence, this work provides a survey of
state-of-the-art safety validation techniques for CPS with a focus on applied
algorithms and their modifications for the safety validation problem. We
present and discuss algorithms in the domains of optimization, path planning,
reinforcement learning, and importance sampling. Problem decomposition
techniques are presented to help scale algorithms to large state spaces, which
are common for CPS. A brief overview of safety-critical applications is given,
including autonomous vehicles and aircraft collision avoidance systems.
Finally, we present a survey of existing academic and commercially available
safety validation tools.",arxiv
http://arxiv.org/abs/2005.00828v1,2020-05-02T13:16:16Z,2020-05-02T13:16:16Z,DroTrack: High-speed Drone-based Object Tracking Under Uncertainty,"We present DroTrack, a high-speed visual single-object tracking framework for
drone-captured video sequences. Most of the existing object tracking methods
are designed to tackle well-known challenges, such as occlusion and cluttered
backgrounds. The complex motion of drones, i.e., multiple degrees of freedom in
three-dimensional space, causes high uncertainty. The uncertainty problem leads
to inaccurate location predictions and fuzziness in scale estimations. DroTrack
solves such issues by discovering the dependency between object representation
and motion geometry. We implement an effective object segmentation based on
Fuzzy C Means (FCM). We incorporate the spatial information into the membership
function to cluster the most discriminative segments. We then enhance the
object segmentation by using a pre-trained Convolution Neural Network (CNN)
model. DroTrack also leverages the geometrical angular motion to estimate a
reliable object scale. We discuss the experimental results and performance
evaluation using two datasets of 51,462 drone-captured frames. The combination
of the FCM segmentation and the angular scaling increased DroTrack precision by
up to $9\%$ and decreased the centre location error by $162$ pixels on average.
DroTrack outperforms all the high-speed trackers and achieves comparable
results in comparison to deep learning trackers. DroTrack offers high frame
rates up to 1000 frame per second (fps) with the best location precision, more
than a set of state-of-the-art real-time trackers.",arxiv
http://arxiv.org/abs/2004.14690v4,2021-03-10T06:24:57Z,2020-04-30T11:08:49Z,AIBench Training: Balanced Industry-Standard AI Training Benchmarking,"Earlier-stage evaluations of a new AI architecture/system need affordable
benchmarks. Only using a few AI component benchmarks like MLPerfalone in the
other stages may lead to misleading conclusions. Moreover, the learning
dynamics are not well understood, and the benchmarks' shelf-life is short. This
paper proposes a balanced benchmarking methodology. We use real-world
benchmarks to cover the factors space that impacts the learning dynamics to the
most considerable extent. After performing an exhaustive survey on Internet
service AI domains, we identify and implement nineteen representative AI tasks
with state-of-the-art models. For repeatable performance ranking (RPR subset)
and workload characterization (WC subset), we keep two subsets to a minimum for
affordability. We contribute by far the most comprehensive AI training
benchmark suite. The evaluations show: (1) AIBench Training (v1.1) outperforms
MLPerfTraining (v0.7) in terms of diversity and representativeness of model
complexity, computational cost, convergent rate, computation, and memory access
patterns, and hotspot functions; (2) Against the AIBench full benchmarks, its
RPR subset shortens the benchmarking cost by 64%, while maintaining the primary
workload characteristics; (3) The performance ranking shows the single-purpose
AI accelerator like TPU with the optimized TensorFlowframework performs better
than that of GPUs while losing the latter's general support for various AI
models. The specification, source code, and performance numbers are available
from the AIBench homepage
https://www.benchcouncil.org/aibench-training/index.html.",arxiv
http://arxiv.org/abs/2004.11003v1,2020-04-23T07:03:20Z,2020-04-23T07:03:20Z,Adaptive Techniques in Practical Quantum Key Distribution,"Quantum Key Distribution (QKD) can provide information-theoretically secure
communications and is a strong candidate for the next generation of
cryptography. However, in practice, the performance of QKD is limited by
""practical imperfections"" in realistic sources, channels, and detectors (such
as multi-photon components or imperfect encoding from the sources, losses and
misalignment in the channels, or dark counts in detectors). Addressing such
practical imperfections is a crucial part of implementing QKD protocols with
good performance in reality. There are two highly important future directions
for QKD: (1) QKD over free space, which can allow secure communications between
mobile platforms such as handheld systems, drones, planes, and even satellites,
and (2) fibre-based QKD networks, which can simultaneously provide QKD service
to numerous users at arbitrary locations. These directions are both highly
promising, but so far they are limited by practical imperfections in the
channels and devices, which pose huge challenges and limit their performance.
In this thesis, we develop adaptive techniques with innovative protocol and
algorithm design, as well as novel techniques such as machine learning, to
address some of these key challenges, including (a) atmospheric turbulence in
channels for free-space QKD, (b) asymmetric losses in channels for QKD network,
and (c) efficient parameter optimization in real time, which is important for
both free-space QKD and QKD networks. We believe that this work will pave the
way to important implementations of free-space QKD and fibre-based QKD networks
in the future.",arxiv
http://arxiv.org/abs/2004.09608v3,2022-02-02T14:15:46Z,2020-04-20T20:14:00Z,"Flow-based Algorithms for Improving Clusters: A Unifying Framework,
  Software, and Performance","Clustering points in a vector space or nodes in a graph is a ubiquitous
primitive in statistical data analysis, and it is commonly used for exploratory
data analysis. In practice, it is often of interest to ""refine"" or ""improve"" a
given cluster that has been obtained by some other method. In this survey, we
focus on principled algorithms for this cluster improvement problem. Many such
cluster improvement algorithms are flow-based methods, by which we mean that
operationally they require the solution of a sequence of maximum flow problems
on a (typically implicitly) modified data graph. These cluster improvement
algorithms are powerful, both in theory and in practice, but they have not been
widely adopted for problems such as community detection, local graph
clustering, semi-supervised learning, etc. Possible reasons for this are: the
steep learning curve for these algorithms; the lack of efficient and easy to
use software; and the lack of detailed numerical experiments on real-world data
that demonstrate their usefulness. Our objective here is to address these
issues. To do so, we guide the reader through the whole process of
understanding how to implement and apply these powerful algorithms. We present
a unifying fractional programming optimization framework that permits us to
distill, in a simple way, the crucial components of all these algorithms. It
also makes apparent similarities and differences between related methods.
Viewing these cluster improvement algorithms via a fractional programming
framework suggests directions for future algorithm development. Finally, we
develop efficient implementations of these algorithms in our
LocalGraphClustering Python package, and we perform extensive numerical
experiments to demonstrate the performance of these methods on social networks
and image-based data graphs.",arxiv
http://arxiv.org/abs/2004.08771v1,2020-04-19T05:21:20Z,2020-04-19T05:21:20Z,Heterogeneous CPU+GPU Stochastic Gradient Descent Algorithms,"The widely-adopted practice is to train deep learning models with specialized
hardware accelerators, e.g., GPUs or TPUs, due to their superior performance on
linear algebra operations. However, this strategy does not employ effectively
the extensive CPU and memory resources -- which are used only for
preprocessing, data transfer, and scheduling -- available by default on the
accelerated servers. In this paper, we study training algorithms for deep
learning on heterogeneous CPU+GPU architectures. Our two-fold objective --
maximize convergence rate and resource utilization simultaneously -- makes the
problem challenging. In order to allow for a principled exploration of the
design space, we first introduce a generic deep learning framework that
exploits the difference in computational power and memory hierarchy between CPU
and GPU through asynchronous message passing. Based on insights gained through
experimentation with the framework, we design two heterogeneous asynchronous
stochastic gradient descent (SGD) algorithms. The first algorithm -- CPU+GPU
Hogbatch -- combines small batches on CPU with large batches on GPU in order to
maximize the utilization of both resources. However, this generates an
unbalanced model update distribution which hinders the statistical convergence.
The second algorithm -- Adaptive Hogbatch -- assigns batches with continuously
evolving size based on the relative speed of CPU and GPU. This balances the
model updates ratio at the expense of a customizable decrease in utilization.
We show that the implementation of these algorithms in the proposed CPU+GPU
framework achieves both faster convergence and higher resource utilization than
TensorFlow on several real datasets and on two computing architectures -- an
on-premises server and a cloud instance.",arxiv
http://arxiv.org/abs/2004.00622v1,2020-04-01T17:59:59Z,2020-04-01T17:59:59Z,Evading Deepfake-Image Detectors with White- and Black-Box Attacks,"It is now possible to synthesize highly realistic images of people who don't
exist. Such content has, for example, been implicated in the creation of
fraudulent social-media profiles responsible for dis-information campaigns.
Significant efforts are, therefore, being deployed to detect
synthetically-generated content. One popular forensic approach trains a neural
network to distinguish real from synthetic content.
  We show that such forensic classifiers are vulnerable to a range of attacks
that reduce the classifier to near-0% accuracy. We develop five attack case
studies on a state-of-the-art classifier that achieves an area under the ROC
curve (AUC) of 0.95 on almost all existing image generators, when only trained
on one generator. With full access to the classifier, we can flip the lowest
bit of each pixel in an image to reduce the classifier's AUC to 0.0005; perturb
1% of the image area to reduce the classifier's AUC to 0.08; or add a single
noise pattern in the synthesizer's latent space to reduce the classifier's AUC
to 0.17. We also develop a black-box attack that, with no access to the target
classifier, reduces the AUC to 0.22. These attacks reveal significant
vulnerabilities of certain image-forensic classifiers.",arxiv
http://arxiv.org/abs/2003.13361v1,2020-03-30T11:43:49Z,2020-03-30T11:43:49Z,Efficient attention guided 5G power amplifier digital predistortion,"We investigate neural network (NN) assisted techniques for compensating the
non-linear behaviour and the memory effect of a 5G PA through digital
predistortion (DPD). Traditionally, the most prevalent compensation technique
computes the compensation element using a Memory Polynomial Model (MPM).
Various neural network proposals have been shown to improve on this
performance. However, thus far they mostly come with prohibitive training or
inference costs for real world implementations. In this paper, we propose a DPD
architecture that builds upon the practical MPM formulation governed by neural
attention. Our approach enables a set of MPM DPD components to individually
learn to target different regions of the data space, combining their outputs
for a superior overall compensation. Our method produces similar performance to
that of higher capacity NN models with minimal complexity. Finally, we view our
approach as a framework that can be extended to a wide variety of local
compensator types.",arxiv
http://arxiv.org/abs/2003.09052v2,2020-06-02T15:31:08Z,2020-03-20T00:20:36Z,Design and operation of the ATLAS Transient Science Server,"The Asteroid Terrestrial impact Last Alert System (ATLAS) system consists of
two 0.5m Schmidt telescopes with cameras covering 29 square degrees at plate
scale of 1.86 arcsec per pixel. Working in tandem, the telescopes routinely
survey the whole sky visible from Hawaii (above $\delta > -50^{\circ}$) every
two nights, exposing four times per night, typically reaching $o < 19$
magnitude per exposure when the moon is illuminated and $c < 19.5$ per exposure
in dark skies. Construction is underway of two further units to be sited in
Chile and South Africa which will result in an all-sky daily cadence from 2021.
Initially designed for detecting potentially hazardous near earth objects, the
ATLAS data enable a range of astrophysical time domain science. To extract
transients from the data stream requires a computing system to process the
data, assimilate detections in time and space and associate them with known
astrophysical sources. Here we describe the hardware and software
infrastructure to produce a stream of clean, real, astrophysical transients in
real time. This involves machine learning and boosted decision tree algorithms
to identify extragalactic and Galactic transients. Typically we detect 10-15
supernova candidates per night which we immediately announce publicly. The
ATLAS discoveries not only enable rapid follow-up of interesting sources but
will provide complete statistical samples within the local volume of 100 Mpc. A
simple comparison of the detected supernova rate within 100 Mpc, with no
corrections for completeness, is already significantly higher (factor 1.5 to 2)
than the current accepted rates.",arxiv
http://arxiv.org/abs/2003.08360v1,2020-03-14T13:21:07Z,2020-03-14T13:21:07Z,Compatible Learning for Deep Photonic Neural Network,"Realization of deep learning with coherent optical field has attracted
remarkably attentions presently, which benefits on the fact that optical matrix
manipulation can be executed at speed of light with inherent parallel
computation as well as low latency. Photonic neural network has a significant
potential for prediction-oriented tasks. Yet, real-value Backpropagation
behaves somewhat intractably for coherent photonic intelligent training. We
develop a compatible learning protocol in complex space, of which nonlinear
activation could be selected efficiently depending on the unveiled compatible
condition. Compatibility indicates that matrix representation in complex space
covers its real counterpart, which could enable a single channel mingled
training in real and complex space as a unified model. The phase logical XOR
gate with Mach-Zehnder interferometers and diffractive neural network with
optical modulation mechanism, implementing intelligent weight learned from
compatible learning, are presented to prove the availability. Compatible
learning opens an envisaged window for deep photonic neural network.",arxiv
http://arxiv.org/abs/2003.04956v1,2020-03-10T20:26:26Z,2020-03-10T20:26:26Z,"SQUIRL: Robust and Efficient Learning from Video Demonstration of
  Long-Horizon Robotic Manipulation Tasks","Recent advances in deep reinforcement learning (RL) have demonstrated its
potential to learn complex robotic manipulation tasks. However, RL still
requires the robot to collect a large amount of real-world experience. To
address this problem, recent works have proposed learning from expert
demonstrations (LfD), particularly via inverse reinforcement learning (IRL),
given its ability to achieve robust performance with only a small number of
expert demonstrations. Nevertheless, deploying IRL on real robots is still
challenging due to the large number of robot experiences it requires. This
paper aims to address this scalability challenge with a robust,
sample-efficient, and general meta-IRL algorithm, SQUIRL, that performs a new
but related long-horizon task robustly given only a single video demonstration.
First, this algorithm bootstraps the learning of a task encoder and a
task-conditioned policy using behavioral cloning (BC). It then collects
real-robot experiences and bypasses reward learning by directly recovering a
Q-function from the combined robot and expert trajectories. Next, this
algorithm uses the Q-function to re-evaluate all cumulative experiences
collected by the robot to improve the policy quickly. In the end, the policy
performs more robustly (90%+ success) than BC on new tasks while requiring no
trial-and-errors at test time. Finally, our real-robot and simulated
experiments demonstrate our algorithm's generality across different state
spaces, action spaces, and vision-based manipulation tasks, e.g.,
pick-pour-place and pick-carry-drop.",arxiv
http://arxiv.org/abs/2003.04273v1,2020-03-09T17:29:39Z,2020-03-09T17:29:39Z,"Finding Input Characterizations for Output Properties in ReLU Neural
  Networks","Deep Neural Networks (DNNs) have emerged as a powerful mechanism and are
being increasingly deployed in real-world safety-critical domains. Despite the
widespread success, their complex architecture makes proving any formal
guarantees about them difficult. Identifying how logical notions of high-level
correctness relate to the complex low-level network architecture is a
significant challenge. In this project, we extend the ideas presented in and
introduce a way to bridge the gap between the architecture and the high-level
specifications. Our key insight is that instead of directly proving the safety
properties that are required, we first prove properties that relate closely to
the structure of the neural net and use them to reason about the safety
properties. We build theoretical foundations for our approach, and empirically
evaluate the performance through various experiments, achieving promising
results than the existing approach by identifying a larger region of input
space that guarantees a certain property on the output.",arxiv
http://arxiv.org/abs/2003.03396v1,2020-03-06T19:09:42Z,2020-03-06T19:09:42Z,"Scalable Uncertainty for Computer Vision with Functional Variational
  Inference","As Deep Learning continues to yield successful applications in Computer
Vision, the ability to quantify all forms of uncertainty is a paramount
requirement for its safe and reliable deployment in the real-world. In this
work, we leverage the formulation of variational inference in function space,
where we associate Gaussian Processes (GPs) to both Bayesian CNN priors and
variational family. Since GPs are fully determined by their mean and covariance
functions, we are able to obtain predictive uncertainty estimates at the cost
of a single forward pass through any chosen CNN architecture and for any
supervised learning task. By leveraging the structure of the induced covariance
matrices, we propose numerically efficient algorithms which enable fast
training in the context of high-dimensional tasks such as depth estimation and
semantic segmentation. Additionally, we provide sufficient conditions for
constructing regression loss functions whose probabilistic counterparts are
compatible with aleatoric uncertainty quantification.",arxiv
http://arxiv.org/abs/2003.00671v2,2020-03-04T19:48:50Z,2020-03-02T05:35:32Z,"AutoPhase: Juggling HLS Phase Orderings in Random Forests with Deep
  Reinforcement Learning","The performance of the code a compiler generates depends on the order in
which it applies the optimization passes. Choosing a good order--often referred
to as the phase-ordering problem, is an NP-hard problem. As a result, existing
solutions rely on a variety of heuristics. In this paper, we evaluate a new
technique to address the phase-ordering problem: deep reinforcement learning.
To this end, we implement AutoPhase: a framework that takes a program and uses
deep reinforcement learning to find a sequence of compilation passes that
minimizes its execution time. Without loss of generality, we construct this
framework in the context of the LLVM compiler toolchain and target high-level
synthesis programs. We use random forests to quantify the correlation between
the effectiveness of a given pass and the program's features. This helps us
reduce the search space by avoiding phase orderings that are unlikely to
improve the performance of a given program. We compare the performance of
AutoPhase to state-of-the-art algorithms that address the phase-ordering
problem. In our evaluation, we show that AutoPhase improves circuit performance
by 28% when compared to using the -O3 compiler flag, and achieves competitive
results compared to the state-of-the-art solutions, while requiring fewer
samples. Furthermore, unlike existing state-of-the-art solutions, our deep
reinforcement learning solution shows promising result in generalizing to real
benchmarks and 12,874 different randomly generated programs, after training on
a hundred randomly generated programs.",arxiv
http://arxiv.org/abs/2003.00628v3,2020-07-20T02:39:28Z,2020-03-02T01:58:03Z,"Learning Force Control for Contact-rich Manipulation Tasks with Rigid
  Position-controlled Robots","Reinforcement Learning (RL) methods have been proven successful in solving
manipulation tasks autonomously. However, RL is still not widely adopted on
real robotic systems because working with real hardware entails additional
challenges, especially when using rigid position-controlled manipulators. These
challenges include the need for a robust controller to avoid undesired
behavior, that risk damaging the robot and its environment, and constant
supervision from a human operator. The main contributions of this work are,
first, we proposed a learning-based force control framework combining RL
techniques with traditional force control. Within said control scheme, we
implemented two different conventional approaches to achieve force control with
position-controlled robots; one is a modified parallel position/force control,
and the other is an admittance control. Secondly, we empirically study both
control schemes when used as the action space of the RL agent. Thirdly, we
developed a fail-safe mechanism for safely training an RL agent on manipulation
tasks using a real rigid robot manipulator. The proposed methods are validated
on simulation and a real robot, an UR3 e-series robotic arm.",arxiv
http://arxiv.org/abs/2003.00097v2,2020-06-19T00:22:54Z,2020-02-28T22:32:05Z,Jointly Learning to Recommend and Advertise,"Online recommendation and advertising are two major income channels for
online recommendation platforms (e.g. e-commerce and news feed site). However,
most platforms optimize recommending and advertising strategies by different
teams separately via different techniques, which may lead to suboptimal overall
performances. To this end, in this paper, we propose a novel two-level
reinforcement learning framework to jointly optimize the recommending and
advertising strategies, where the first level generates a list of
recommendations to optimize user experience in the long run; then the second
level inserts ads into the recommendation list that can balance the immediate
advertising revenue from advertisers and the negative influence of ads on
long-term user experience. To be specific, the first level tackles high
combinatorial action space problem that selects a subset items from the large
item space; while the second level determines three internally related tasks,
i.e., (i) whether to insert an ad, and if yes, (ii) the optimal ad and (iii)
the optimal location to insert. The experimental results based on real-world
data demonstrate the effectiveness of the proposed framework. We have released
the implementation code to ease reproductivity.",arxiv
http://arxiv.org/abs/2003.04816v1,2020-02-21T07:29:15Z,2020-02-21T07:29:15Z,"Data Freshness and Energy-Efficient UAV Navigation Optimization: A Deep
  Reinforcement Learning Approach","In this paper, we design a navigation policy for multiple unmanned aerial
vehicles (UAVs) where mobile base stations (BSs) are deployed to improve the
data freshness and connectivity to the Internet of Things (IoT) devices. First,
we formulate an energy-efficient trajectory optimization problem in which the
objective is to maximize the energy efficiency by optimizing the UAV-BS
trajectory policy. We also incorporate different contextual information such as
energy and age of information (AoI) constraints to ensure the data freshness at
the ground BS. Second, we propose an agile deep reinforcement learning with
experience replay model to solve the formulated problem concerning the
contextual constraints for the UAV-BS navigation. Moreover, the proposed
approach is well-suited for solving the problem, since the state space of the
problem is extremely large and finding the best trajectory policy with useful
contextual features is too complex for the UAV-BSs. By applying the proposed
trained model, an effective real-time trajectory policy for the UAV-BSs
captures the observable network states over time. Finally, the simulation
results illustrate the proposed approach is 3.6% and 3.13% more energy
efficient than those of the greedy and baseline deep Q Network (DQN)
approaches.",arxiv
http://arxiv.org/abs/2002.09063v1,2020-02-20T23:37:43Z,2020-02-20T23:37:43Z,"Real-Time Optimal Guidance and Control for Interplanetary Transfers
  Using Deep Networks","We consider the Earth-Venus mass-optimal interplanetary transfer of a
low-thrust spacecraft and show how the optimal guidance can be represented by
deep networks in a large portion of the state space and to a high degree of
accuracy. Imitation (supervised) learning of optimal examples is used as a
network training paradigm. The resulting models are suitable for an on-board,
real-time, implementation of the optimal guidance and control system of the
spacecraft and are called G&CNETs. A new general methodology called Backward
Generation of Optimal Examples is introduced and shown to be able to
efficiently create all the optimal state action pairs necessary to train
G&CNETs without solving optimal control problems. With respect to previous
works, we are able to produce datasets containing a few orders of magnitude
more optimal trajectories and obtain network performances compatible with real
missions requirements. Several schemes able to train representations of either
the optimal policy (thrust profile) or the value function (optimal mass) are
proposed and tested. We find that both policy learning and value function
learning successfully and accurately learn the optimal thrust and that a
spacecraft employing the learned thrust is able to reach the target conditions
orbit spending only 2 permil more propellant than in the corresponding
mathematically optimal transfer. Moreover, the optimal propellant mass can be
predicted (in case of value function learning) within an error well within 1%.
All G&CNETs produced are tested during simulations of interplanetary transfers
with respect to their ability to reach the target conditions optimally starting
from nominal and off-nominal conditions.",arxiv
http://arxiv.org/abs/2002.08541v2,2020-10-22T11:28:41Z,2020-02-20T02:41:02Z,Simple and Scalable Sparse k-means Clustering via Feature Ranking,"Clustering, a fundamental activity in unsupervised learning, is notoriously
difficult when the feature space is high-dimensional. Fortunately, in many
realistic scenarios, only a handful of features are relevant in distinguishing
clusters. This has motivated the development of sparse clustering techniques
that typically rely on k-means within outer algorithms of high computational
complexity. Current techniques also require careful tuning of shrinkage
parameters, further limiting their scalability. In this paper, we propose a
novel framework for sparse k-means clustering that is intuitive, simple to
implement, and competitive with state-of-the-art algorithms. We show that our
algorithm enjoys consistency and convergence guarantees. Our core method
readily generalizes to several task-specific algorithms such as clustering on
subsets of attributes and in partially observed data settings. We showcase
these contributions thoroughly via simulated experiments and real data
benchmarks, including a case study on protein expression in trisomic mice.",arxiv
http://arxiv.org/abs/2002.07684v3,2020-04-17T11:37:42Z,2020-02-18T16:13:24Z,"A Lagrangian Approach to Information Propagation in Graph Neural
  Networks","In many real world applications, data are characterized by a complex
structure, that can be naturally encoded as a graph. In the last years, the
popularity of deep learning techniques has renewed the interest in neural
models able to process complex patterns. In particular, inspired by the Graph
Neural Network (GNN) model, different architectures have been proposed to
extend the original GNN scheme. GNNs exploit a set of state variables, each
assigned to a graph node, and a diffusion mechanism of the states among
neighbor nodes, to implement an iterative procedure to compute the fixed point
of the (learnable) state transition function. In this paper, we propose a novel
approach to the state computation and the learning algorithm for GNNs, based on
a constraint optimisation task solved in the Lagrangian framework. The state
convergence procedure is implicitly expressed by the constraint satisfaction
mechanism and does not require a separate iterative phase for each epoch of the
learning procedure. In fact, the computational structure is based on the search
for saddle points of the Lagrangian in the adjoint space composed of weights,
neural outputs (node states), and Lagrange multipliers. The proposed approach
is compared experimentally with other popular models for processing graphs.",arxiv
http://arxiv.org/abs/2002.06637v1,2020-02-16T18:18:19Z,2020-02-16T18:18:19Z,Real-time binaural speech separation with preserved spatial cues,"Deep learning speech separation algorithms have achieved great success in
improving the quality and intelligibility of separated speech from mixed audio.
Most previous methods focused on generating a single-channel output for each of
the target speakers, hence discarding the spatial cues needed for the
localization of sound sources in space. However, preserving the spatial
information is important in many applications that aim to accurately render the
acoustic scene such as in hearing aids and augmented reality (AR). Here, we
propose a speech separation algorithm that preserves the interaural cues of
separated sound sources and can be implemented with low latency and high
fidelity, therefore enabling a real-time modification of the acoustic scene.
Based on the time-domain audio separation network (TasNet), a single-channel
time-domain speech separation system that can be implemented in real-time, we
propose a multi-input-multi-output (MIMO) end-to-end extension of TasNet that
takes binaural mixed audio as input and simultaneously separates target
speakers in both channels. Experimental results show that the proposed
end-to-end MIMO system is able to significantly improve the separation
performance and keep the perceived location of the modified sources intact in
various acoustic scenes.",arxiv
http://arxiv.org/abs/2002.05287v2,2020-02-14T01:47:35Z,2020-02-13T00:03:09Z,Geom-GCN: Geometric Graph Convolutional Networks,"Message-passing neural networks (MPNNs) have been successfully applied to
representation learning on graphs in a variety of real-world applications.
However, two fundamental weaknesses of MPNNs' aggregators limit their ability
to represent graph-structured data: losing the structural information of nodes
in neighborhoods and lacking the ability to capture long-range dependencies in
disassortative graphs. Few studies have noticed the weaknesses from different
perspectives. From the observations on classical neural network and network
geometry, we propose a novel geometric aggregation scheme for graph neural
networks to overcome the two weaknesses. The behind basic idea is the
aggregation on a graph can benefit from a continuous space underlying the
graph. The proposed aggregation scheme is permutation-invariant and consists of
three modules, node embedding, structural neighborhood, and bi-level
aggregation. We also present an implementation of the scheme in graph
convolutional networks, termed Geom-GCN (Geometric Graph Convolutional
Networks), to perform transductive learning on graphs. Experimental results
show the proposed Geom-GCN achieved state-of-the-art performance on a wide
range of open datasets of graphs. Code is available at
https://github.com/graphdml-uiuc-jlu/geom-gcn.",arxiv
http://arxiv.org/abs/2002.04788v3,2021-06-30T21:05:16Z,2020-02-12T04:05:31Z,"To Split or Not to Split: The Impact of Disparate Treatment in
  Classification","Disparate treatment occurs when a machine learning model yields different
decisions for individuals based on a sensitive attribute (e.g., age, sex). In
domains where prediction accuracy is paramount, it could potentially be
acceptable to fit a model which exhibits disparate treatment. To evaluate the
effect of disparate treatment, we compare the performance of split classifiers
(i.e., classifiers trained and deployed separately on each group) with
group-blind classifiers (i.e., classifiers which do not use a sensitive
attribute). We introduce the benefit-of-splitting for quantifying the
performance improvement by splitting classifiers. Computing the
benefit-of-splitting directly from its definition could be intractable since it
involves solving optimization problems over an infinite-dimensional functional
space. Under different performance measures, we (i) prove an equivalent
expression for the benefit-of-splitting which can be efficiently computed by
solving small-scale convex programs; (ii) provide sharp upper and lower bounds
for the benefit-of-splitting which reveal precise conditions where a
group-blind classifier will always suffer from a non-trivial performance gap
from the split classifiers. In the finite sample regime, splitting is not
necessarily beneficial and we provide data-dependent bounds to understand this
effect. Finally, we validate our theoretical results through numerical
experiments on both synthetic and real-world datasets.",arxiv
http://arxiv.org/abs/2002.02363v3,2020-10-14T14:05:51Z,2020-02-06T17:11:11Z,"Topological quantum phase transitions retrieved through unsupervised
  machine learning","The discovery of topological features of quantum states plays an important
role in modern condensed matter physics and various artificial systems. Due to
the absence of local order parameters, the detection of topological quantum
phase transitions remains a challenge. Machine learning may provide effective
methods for identifying topological features. In this work, we show that the
unsupervised manifold learning can successfully retrieve topological quantum
phase transitions in momentum and real space. Our results show that the
Chebyshev distance between two data points sharpens the characteristic features
of topological quantum phase transitions in momentum space, while the widely
used Euclidean distance is in general suboptimal. Then a diffusion map or
isometric map can be applied to implement the dimensionality reduction, and to
learn about topological quantum phase transitions in an unsupervised manner. We
demonstrate this method on the prototypical Su-Schrieffer-Heeger (SSH) model,
the Qi-Wu-Zhang (QWZ) model, and the quenched SSH model in momentum space, and
further provide implications and demonstrations for learning in real space,
where the topological invariants could be unknown or hard to compute. The
interpretable good performance of our approach shows the capability of manifold
learning, when equipped with a suitable distance metric, in exploring
topological quantum phase transitions.",arxiv
http://arxiv.org/abs/2002.00831v1,2020-02-03T15:39:56Z,2020-02-03T15:39:56Z,An Actor-Critic-Based UAV-BSs Deployment Method for Dynamic Environments,"In this paper, the real-time deployment of unmanned aerial vehicles (UAVs) as
flying base stations (BSs) for optimizing the throughput of mobile users is
investigated for UAV networks. This problem is formulated as a time-varying
mixed-integer non-convex programming (MINP) problem, which is challenging to
find an optimal solution in a short time with conventional optimization
techniques. Hence, we propose an actor-critic-based (AC-based) deep
reinforcement learning (DRL) method to find near-optimal UAV positions at every
moment. In the proposed method, the process searching for the solution
iteratively at a particular moment is modeled as a Markov decision process
(MDP). To handle infinite state and action spaces and improve the robustness of
the decision process, two powerful neural networks (NNs) are configured to
evaluate the UAV position adjustments and make decisions, respectively.
Compared with the heuristic algorithm, sequential least-squares programming and
fixed UAVs methods, simulation results have shown that the proposed method
outperforms these three benchmarks in terms of the throughput at every moment
in UAV networks.",arxiv
http://arxiv.org/abs/2001.09346v2,2020-03-04T19:22:37Z,2020-01-25T18:43:47Z,"CorGAN: Correlation-Capturing Convolutional Generative Adversarial
  Networks for Generating Synthetic Healthcare Records","Deep learning models have demonstrated high-quality performance in areas such
as image classification and speech processing. However, creating a deep
learning model using electronic health record (EHR) data, requires addressing
particular privacy challenges that are unique to researchers in this domain.
This matter focuses attention on generating realistic synthetic data while
ensuring privacy. In this paper, we propose a novel framework called
correlation-capturing Generative Adversarial Network (CorGAN), to generate
synthetic healthcare records. In CorGAN we utilize Convolutional Neural
Networks to capture the correlations between adjacent medical features in the
data representation space by combining Convolutional Generative Adversarial
Networks and Convolutional Autoencoders. To demonstrate the model fidelity, we
show that CorGAN generates synthetic data with performance similar to that of
real data in various Machine Learning settings such as classification and
prediction. We also give a privacy assessment and report on statistical
analysis regarding realistic characteristics of the synthetic data. The
software of this work is open-source and is available at:
https://github.com/astorfi/cor-gan.",arxiv
http://arxiv.org/abs/2001.03864v1,2020-01-12T06:06:03Z,2020-01-12T06:06:03Z,"Learning to drive via Apprenticeship Learning and Deep Reinforcement
  Learning","With the implementation of reinforcement learning (RL) algorithms, current
state-of-art autonomous vehicle technology have the potential to get closer to
full automation. However, most of the applications have been limited to game
domains or discrete action space which are far from the real world driving.
Moreover, it is very tough to tune the parameters of reward mechanism since the
driving styles vary a lot among the different users. For instance, an
aggressive driver may prefer driving with high acceleration whereas some
conservative drivers prefer a safer driving style. Therefore, we propose an
apprenticeship learning in combination with deep reinforcement learning
approach that allows the agent to learn the driving and stopping behaviors with
continuous actions. We use gradient inverse reinforcement learning (GIRL)
algorithm to recover the unknown reward function and employ REINFORCE as well
as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal
policy. The performance of our method is evaluated in simulation-based scenario
and the results demonstrate that the agent performs human like driving and even
better in some aspects after training.",arxiv
http://arxiv.org/abs/2001.03855v1,2020-01-12T05:25:02Z,2020-01-12T05:25:02Z,"Hyperparameters optimization for Deep Learning based emotion prediction
  for Human Robot Interaction","To enable humanoid robots to share our social space we need to develop
technology for easy interaction with the robots using multiple modes such as
speech, gestures and share our emotions with them. We have targeted this
research towards addressing the core issue of emotion recognition problem which
would require less computation resources and much lesser number of network
hyperparameters which will be more adaptive to be computed on low resourced
social robots for real time communication. More specifically, here we have
proposed an Inception module based Convolutional Neural Network Architecture
which has achieved improved accuracy of upto 6% improvement over the existing
network architecture for emotion classification when combinedly tested over
multiple datasets when tried over humanoid robots in real - time. Our proposed
model is reducing the trainable Hyperparameters to an extent of 94% as compared
to vanilla CNN model which clearly indicates that it can be used in real time
based application such as human robot interaction. Rigorous experiments have
been performed to validate our methodology which is sufficiently robust and
could achieve high level of accuracy. Finally, the model is implemented in a
humanoid robot, NAO in real time and robustness of the model is evaluated.",arxiv
http://arxiv.org/abs/2001.03360v4,2020-08-04T00:56:49Z,2020-01-10T09:26:04Z,NWPU-Crowd: A Large-Scale Benchmark for Crowd Counting and Localization,"In the last decade, crowd counting and localization attract much attention of
researchers due to its wide-spread applications, including crowd monitoring,
public safety, space design, etc. Many Convolutional Neural Networks (CNN) are
designed for tackling this task. However, currently released datasets are so
small-scale that they can not meet the needs of the supervised CNN-based
algorithms. To remedy this problem, we construct a large-scale congested crowd
counting and localization dataset, NWPU-Crowd, consisting of 5,109 images, in a
total of 2,133,375 annotated heads with points and boxes. Compared with other
real-world datasets, it contains various illumination scenes and has the
largest density range (0~20,033). Besides, a benchmark website is developed for
impartially evaluating the different methods, which allows researchers to
submit the results of the test set. Based on the proposed dataset, we further
describe the data characteristics, evaluate the performance of some mainstream
state-of-the-art (SOTA) methods, and analyze the new problems that arise on the
new data. What's more, the benchmark is deployed at
\url{https://www.crowdbenchmark.com/}, and the dataset/code/models/results are
available at \url{https://gjy3035.github.io/NWPU-Crowd-Sample-Code/}.",arxiv
http://arxiv.org/abs/2001.03535v4,2020-06-10T23:50:57Z,2020-01-06T05:32:15Z,"AutoDNNchip: An Automated DNN Chip Predictor and Builder for Both FPGAs
  and ASICs","Recent breakthroughs in Deep Neural Networks (DNNs) have fueled a growing
demand for DNN chips. However, designing DNN chips is non-trivial because: (1)
mainstream DNNs have millions of parameters and operations; (2) the large
design space due to the numerous design choices of dataflows, processing
elements, memory hierarchy, etc.; and (3) an algorithm/hardware co-design is
needed to allow the same DNN functionality to have a different decomposition,
which would require different hardware IPs to meet the application
specifications. Therefore, DNN chips take a long time to design and require
cross-disciplinary experts. To enable fast and effective DNN chip design, we
propose AutoDNNchip - a DNN chip generator that can automatically generate both
FPGA- and ASIC-based DNN chip implementation given DNNs from machine learning
frameworks (e.g., PyTorch) for a designated application and dataset.
Specifically, AutoDNNchip consists of two integrated enablers: (1) a Chip
Predictor, built on top of a graph-based accelerator representation, which can
accurately and efficiently predict a DNN accelerator's energy, throughput, and
area based on the DNN model parameters, hardware configuration,
technology-based IPs, and platform constraints; and (2) a Chip Builder, which
can automatically explore the design space of DNN chips (including IP
selection, block configuration, resource balancing, etc.), optimize chip design
via the Chip Predictor, and then generate optimized synthesizable RTL to
achieve the target design metrics. Experimental results show that our Chip
Predictor's predicted performance differs from real-measured ones by < 10% when
validated using 15 DNN models and 4 platforms (edge-FPGA/TPU/GPU and ASIC).
Furthermore, accelerators generated by our AutoDNNchip can achieve better (up
to 3.86X improvement) performance than that of expert-crafted state-of-the-art
accelerators.",arxiv
http://arxiv.org/abs/1912.08202v2,2020-10-01T06:12:27Z,2019-12-17T23:33:58Z,"Extrinsic Kernel Ridge Regression Classifier for Planar Kendall Shape
  Space","Kernel methods have had great success in Statistics and Machine Learning.
Despite their growing popularity, however, less effort has been drawn towards
developing kernel based classification methods on Riemannian manifolds due to
difficulty in dealing with non-Euclidean geometry. In this paper, motivated by
the extrinsic framework of manifold-valued data analysis, we propose a new
positive definite kernel on planar Kendall shape space $\Sigma_2^k$, called
extrinsic Veronese Whitney Gaussian kernel. We show that our approach can be
extended to develop Gaussian kernels on any embedded manifold. Furthermore,
kernel ridge regression classifier (KRRC) is implemented to address the shape
classification problem on $\Sigma_2^k$, and their promising performances are
illustrated through the real data analysis.",arxiv
http://arxiv.org/abs/1912.06321v2,2020-08-17T03:26:55Z,2019-12-13T04:29:38Z,"Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World
  Performance?","Does progress in simulation translate to progress on robots? If one method
outperforms another in simulation, how likely is that trend to hold in reality
on a robot? We examine this question for embodied PointGoal navigation,
developing engineering tools and a research paradigm for evaluating a simulator
by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy),
a library for seamless execution of identical code on simulated agents and
robots, transferring simulation-trained agents to a LoCoBot platform with a
one-line code change. Second, we investigate the sim2real predictivity of
Habitat-Sim for PointGoal navigation. We 3D-scan a physical lab space to create
a virtualized replica, and run parallel tests of 9 different models in reality
and simulation. We present a new metric called Sim-vs-Real Correlation
Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as
used for the CVPR19 challenge is low (0.18 for the success metric), suggesting
that performance differences in this simulator-based challenge do not persist
after physical deployment. This gap is largely due to AI agents learning to
exploit simulator imperfections, abusing collision dynamics to 'slide' along
walls, leading to shortcuts through otherwise non-navigable space. Naturally,
such exploits do not work in the real world. Our experiments show that it is
possible to tune simulation parameters to improve sim2real predictivity (e.g.
improving $SRCC_{Succ}$ from 0.18 to 0.844), increasing confidence that
in-simulation comparisons will translate to deployed systems in reality.",arxiv
http://arxiv.org/abs/1912.03647v2,2020-08-11T03:42:21Z,2019-12-08T09:51:08Z,Compressing 3DCNNs Based on Tensor Train Decomposition,"Three dimensional convolutional neural networks (3DCNNs) have been applied in
many tasks, e.g., video and 3D point cloud recognition. However, due to the
higher dimension of convolutional kernels, the space complexity of 3DCNNs is
generally larger than that of traditional two dimensional convolutional neural
networks (2DCNNs). To miniaturize 3DCNNs for the deployment in confining
environments such as embedded devices, neural network compression is a
promising approach. In this work, we adopt the tensor train (TT) decomposition,
a straightforward and simple in situ training compression method, to shrink the
3DCNN models. Through proposing tensorizing 3D convolutional kernels in TT
format, we investigate how to select appropriate TT ranks for achieving higher
compression ratio. We have also discussed the redundancy of 3D convolutional
kernels for compression, core significance and future directions of this work,
as well as the theoretical computation complexity versus practical executing
time of convolution in TT. In the light of multiple contrast experiments based
on VIVA challenge, UCF11, and UCF101 datasets, we conclude that TT
decomposition can compress 3DCNNs by around one hundred times without
significant accuracy loss, which will enable its applications in extensive real
world scenarios.",arxiv
http://arxiv.org/abs/1912.01100v2,2020-03-04T09:50:32Z,2019-12-02T22:16:32Z,Latent Replay for Real-Time Continual Learning,"Training deep neural networks at the edge on light computational devices,
embedded systems and robotic platforms is nowadays very challenging. Continual
learning techniques, where complex models are incrementally trained on small
batches of new data, can make the learning problem tractable even for CPU-only
embedded devices enabling remarkable levels of adaptiveness and autonomy.
However, a number of practical problems need to be solved: catastrophic
forgetting before anything else. In this paper we introduce an original
technique named ""Latent Replay"" where, instead of storing a portion of past
data in the input space, we store activations volumes at some intermediate
layer. This can significantly reduce the computation and storage required by
native rehearsal. To keep the representation stable and the stored activations
valid we propose to slow-down learning at all the layers below the latent
replay one, leaving the layers above free to learn at full pace. In our
experiments we show that Latent Replay, combined with existing continual
learning techniques, achieves state-of-the-art performance on complex video
benchmarks such as CORe50 NICv2 (with nearly 400 small and highly non-i.i.d.
batches) and OpenLORIS. Finally, we demonstrate the feasibility of nearly
real-time continual learning on the edge through the deployment of the proposed
technique on a smartphone device.",arxiv
http://arxiv.org/abs/1911.12672v1,2019-11-28T12:35:55Z,2019-11-28T12:35:55Z,"Improved cross-validation for classifiers that make algorithmic choices
  to minimise runtime without compromising output correctness","Our topic is the use of machine learning to improve software by making
choices which do not compromise the correctness of the output, but do affect
the time taken to produce such output. We are particularly concerned with
computer algebra systems (CASs), and in particular, our experiments are for
selecting the variable ordering to use when performing a cylindrical algebraic
decomposition of $n$-dimensional real space with respect to the signs of a set
of polynomials.
  In our prior work we explored the different ML models that could be used, and
how to identify suitable features of the input polynomials. In the present
paper we both repeat our prior experiments on problems which have more
variables (and thus exponentially more possible orderings), and examine the
metric which our ML classifiers targets. The natural metric is computational
runtime, with classifiers trained to pick the ordering which minimises this.
However, this leads to the situation were models do not distinguish between any
of the non-optimal orderings, whose runtimes may still vary dramatically. In
this paper we investigate a modification to the cross-validation algorithms of
the classifiers so that they do distinguish these cases, leading to improved
results.",arxiv
http://arxiv.org/abs/1911.10751v1,2019-11-25T08:02:17Z,2019-11-25T08:02:17Z,"Deep Image-to-Video Adaptation and Fusion Networks for Action
  Recognition","Existing deep learning methods for action recognition in videos require a
large number of labeled videos for training, which is labor-intensive and
time-consuming. For the same action, the knowledge learned from different media
types, e.g., videos and images, may be related and complementary. However, due
to the domain shifts and heterogeneous feature representations between videos
and images, the performance of classifiers trained on images may be
dramatically degraded when directly deployed to videos. In this paper, we
propose a novel method, named Deep Image-to-Video Adaptation and Fusion
Networks (DIVAFN), to enhance action recognition in videos by transferring
knowledge from images using video keyframes as a bridge. The DIVAFN is a
unified deep learning model, which integrates domain-invariant representations
learning and cross-modal feature fusion into a unified optimization framework.
Specifically, we design an efficient cross-modal similarities metric to reduce
the modality shift among images, keyframes and videos. Then, we adopt an
autoencoder architecture, whose hidden layer is constrained to be the semantic
representations of the action class names. In this way, when the autoencoder is
adopted to project the learned features from different domains to the same
space, more compact, informative and discriminative representations can be
obtained. Finally, the concatenation of the learned semantic feature
representations from these three autoencoders are used to train the classifier
for action recognition in videos. Comprehensive experiments on four real-world
datasets show that our method outperforms some state-of-the-art domain
adaptation and action recognition methods.",arxiv
http://arxiv.org/abs/1911.09592v1,2019-11-21T16:37:18Z,2019-11-21T16:37:18Z,"mm-Pose: Real-Time Human Skeletal Posture Estimation using mmWave Radars
  and CNNs","In this paper, mm-Pose, a novel approach to detect and track human skeletons
in real-time using an mmWave radar, is proposed. To the best of the authors'
knowledge, this is the first method to detect >15 distinct skeletal joints
using mmWave radar reflection signals. The proposed method would find several
applications in traffic monitoring systems, autonomous vehicles, patient
monitoring systems and defense forces to detect and track human skeleton for
effective and preventive decision making in real-time. The use of radar makes
the system operationally robust to scene lighting and adverse weather
conditions. The reflected radar point cloud in range, azimuth and elevation are
first resolved and projected in Range-Azimuth and Range-Elevation planes. A
novel low-size high-resolution radar-to-image representation is also presented,
that overcomes the sparsity in traditional point cloud data and offers
significant reduction in the subsequent machine learning architecture. The RGB
channels were assigned with the normalized values of range, elevation/azimuth
and the power level of the reflection signals for each of the points. A forked
CNN architecture was used to predict the real-world position of the skeletal
joints in 3-D space, using the radar-to-image representation. The proposed
method was tested for a single human scenario for four primary motions, (i)
Walking, (ii) Swinging left arm, (iii) Swinging right arm, and (iv) Swinging
both arms to validate accurate predictions for motion in range, azimuth and
elevation. The detailed methodology, implementation, challenges, and validation
results are presented.",arxiv
http://arxiv.org/abs/1911.02142v2,2020-03-16T20:05:29Z,2019-11-05T23:39:55Z,Intriguing Properties of Adversarial ML Attacks in the Problem Space,"Recent research efforts on adversarial ML have investigated problem-space
attacks, focusing on the generation of real evasive objects in domains where,
unlike images, there is no clear inverse mapping to the feature space (e.g.,
software). However, the design, comparison, and real-world implications of
problem-space attacks remain underexplored. This paper makes two major
contributions. First, we propose a novel formalization for adversarial ML
evasion attacks in the problem-space, which includes the definition of a
comprehensive set of constraints on available transformations, preserved
semantics, robustness to preprocessing, and plausibility. We shed light on the
relationship between feature space and problem space, and we introduce the
concept of side-effect features as the byproduct of the inverse feature-mapping
problem. This enables us to define and prove necessary and sufficient
conditions for the existence of problem-space attacks. We further demonstrate
the expressive power of our formalization by using it to describe several
attacks from related literature across different domains. Second, building on
our formalization, we propose a novel problem-space attack on Android malware
that overcomes past limitations. Experiments on a dataset with 170K Android
apps from 2017 and 2018 show the practical feasibility of evading a
state-of-the-art malware classifier along with its hardened version. Our
results demonstrate that ""adversarial-malware as a service"" is a realistic
threat, as we automatically generate thousands of realistic and inconspicuous
adversarial applications at scale, where on average it takes only a few minutes
to generate an adversarial app. Our formalization of problem-space attacks
paves the way to more principled research in this domain.",arxiv
http://arxiv.org/abs/1910.13676v1,2019-10-30T05:13:33Z,2019-10-30T05:13:33Z,Multi Modal Semantic Segmentation using Synthetic Data,"Semantic understanding of scenes in three-dimensional space (3D) is a
quintessential part of robotics oriented applications such as autonomous
driving as it provides geometric cues such as size, orientation and true
distance of separation to objects which are crucial for taking mission critical
decisions. As a first step, in this work we investigate the possibility of
semantically classifying different parts of a given scene in 3D by learning the
underlying geometric context in addition to the texture cues BUT in the absence
of labelled real-world datasets. To this end we generate a large number of
synthetic scenes, their pixel-wise labels and corresponding 3D representations
using CARLA software framework. We then build a deep neural network that learns
underlying category specific 3D representation and texture cues from color
information of the rendered synthetic scenes. Further on we apply the learned
model on different real world datasets to evaluate its performance. Our
preliminary investigation of results show that the neural network is able to
learn the geometric context from synthetic scenes and effectively apply this
knowledge to classify each point of a 3D representation of a scene in
real-world.",arxiv
http://arxiv.org/abs/1910.13580v1,2019-10-29T23:43:01Z,2019-10-29T23:43:01Z,Domain Generalization via Model-Agnostic Learning of Semantic Features,"Generalization capability to unseen domains is crucial for machine learning
models when deploying to real-world conditions. We investigate the challenging
problem of domain generalization, i.e., training a model on multi-domain source
data such that it can directly generalize to target domains with unknown
statistics. We adopt a model-agnostic learning paradigm with gradient-based
meta-train and meta-test procedures to expose the optimization to domain shift.
Further, we introduce two complementary losses which explicitly regularize the
semantic structure of the feature space. Globally, we align a derived soft
confusion matrix to preserve general knowledge about inter-class relationships.
Locally, we promote domain-independent class-specific cohesion and separation
of sample features with a metric-learning component. The effectiveness of our
method is demonstrated with new state-of-the-art results on two common object
recognition benchmarks. Our method also shows consistent improvement on a
medical image segmentation task.",arxiv
http://arxiv.org/abs/1910.12980v1,2019-10-28T21:43:22Z,2019-10-28T21:43:22Z,Learning Transferable Graph Exploration,"This paper considers the problem of efficient exploration of unseen
environments, a key challenge in AI. We propose a `learning to explore'
framework where we learn a policy from a distribution of environments. At test
time, presented with an unseen environment from the same distribution, the
policy aims to generalize the exploration strategy to visit the maximum number
of unique states in a limited number of steps. We particularly focus on
environments with graph-structured state-spaces that are encountered in many
important real-world applications like software testing and map building. We
formulate this task as a reinforcement learning problem where the `exploration'
agent is rewarded for transitioning to previously unseen environment states and
employ a graph-structured memory to encode the agent's past trajectory.
Experimental results demonstrate that our approach is extremely effective for
exploration of spatial maps; and when applied on the challenging problems of
coverage-guided software-testing of domain-specific programs and real-world
mobile applications, it outperforms methods that have been hand-engineered by
human experts.",arxiv
http://arxiv.org/abs/2001.09938v1,2019-10-22T15:57:20Z,2019-10-22T15:57:20Z,"Autonomous discovery of battery electrolytes with robotic
  experimentation and machine-learning","Innovations in batteries take years to formulate and commercialize, requiring
extensive experimentation during the design and optimization phases. We
approached the design and selection of a battery electrolyte through a
black-box optimization algorithm directly integrated into a robotic test-stand.
We report here the discovery of a novel battery electrolyte by this experiment
completely guided by the machine-learning software without human intervention.
Motivated by the recent trend toward super-concentrated aqueous electrolytes
for high-performance batteries, we utilize Dragonfly - a Bayesian
machine-learning software package - to search mixtures of commonly used lithium
and sodium salts for super-concentrated aqueous electrolytes with wide
electrochemical stability windows. Dragonfly autonomously managed the robotic
test-stand, recommending electrolyte designs to test and receiving experimental
feedback in real time. In 40 hours of continuous experimentation over a
four-dimensional design space with millions of potential candidates, Dragonfly
discovered a novel, mixed-anion aqueous sodium electrolyte with a wider
electrochemical stability window than state-of-the-art sodium electrolyte. A
human-guided design process may have missed this optimal electrolyte. This
result demonstrates the possibility of integrating robotics with
machine-learning to rapidly and autonomously discover novel battery materials.",arxiv
http://arxiv.org/abs/1910.09430v2,2020-02-06T16:28:34Z,2019-10-21T15:06:03Z,Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video,"Key challenges for the deployment of reinforcement learning (RL) agents in
the real world are the discovery, representation and reuse of skills in the
absence of a reward function. To this end, we propose a novel approach to learn
a task-agnostic skill embedding space from unlabeled multi-view videos. Our
method learns a general skill embedding independently from the task context by
using an adversarial loss. We combine a metric learning loss, which utilizes
temporal video coherence to learn a state representation, with an entropy
regularized adversarial skill-transfer loss. The metric learning loss learns a
disentangled representation by attracting simultaneous viewpoints of the same
observations and repelling visually similar frames from temporal neighbors. The
adversarial skill-transfer loss enhances re-usability of learned skill
embeddings over multiple task domains. We show that the learned embedding
enables training of continuous control policies to solve novel tasks that
require the interpolation of previously seen skills. Our extensive evaluation
with both simulation and real world data demonstrates the effectiveness of our
method in learning transferable skills from unlabeled interaction videos and
composing them for new tasks. Code, pretrained models and dataset are available
at http://robotskills.cs.uni-freiburg.de",arxiv
http://arxiv.org/abs/1910.06893v1,2019-10-15T16:07:34Z,2019-10-15T16:07:34Z,"Extracting robust and accurate features via a robust information
  bottleneck","We propose a novel strategy for extracting features in supervised learning
that can be used to construct a classifier which is more robust to small
perturbations in the input space. Our method builds upon the idea of the
information bottleneck by introducing an additional penalty term that
encourages the Fisher information of the extracted features to be small, when
parametrized by the inputs. By tuning the regularization parameter, we can
explicitly trade off the opposing desiderata of robustness and accuracy when
constructing a classifier. We derive the optimal solution to the robust
information bottleneck when the inputs and outputs are jointly Gaussian,
proving that the optimally robust features are also jointly Gaussian in that
setting. Furthermore, we propose a method for optimizing a variational bound on
the robust information bottleneck objective in general settings using
stochastic gradient descent, which may be implemented efficiently in neural
networks. Our experimental results for synthetic and real data sets show that
the proposed feature extraction method indeed produces classifiers with
increased robustness to perturbations.",arxiv
http://arxiv.org/abs/1910.06316v3,2021-03-25T03:03:02Z,2019-10-14T17:58:23Z,NeurVPS: Neural Vanishing Point Scanning via Conic Convolution,"We present a simple yet effective end-to-end trainable deep network with
geometry-inspired convolutional operators for detecting vanishing points in
images. Traditional convolutional neural networks rely on aggregating edge
features and do not have mechanisms to directly exploit the geometric
properties of vanishing points as the intersections of parallel lines. In this
work, we identify a canonical conic space in which the neural network can
effectively compute the global geometric information of vanishing points
locally, and we propose a novel operator named conic convolution that can be
implemented as regular convolutions in this space. This new operator explicitly
enforces feature extractions and aggregations along the structural lines and
yet has the same number of parameters as the regular 2D convolution. Our
extensive experiments on both synthetic and real-world datasets show that the
proposed operator significantly improves the performance of vanishing point
detection over traditional methods. The code and dataset have been made
publicly available at https://github.com/zhou13/neurvps.",arxiv
http://arxiv.org/abs/1910.05639v2,2019-11-06T19:40:41Z,2019-10-12T19:57:55Z,"Disentangling Interpretable Generative Parameters of Random and
  Real-World Graphs","While a wide range of interpretable generative procedures for graphs exist,
matching observed graph topologies with such procedures and choices for its
parameters remains an open problem. Devising generative models that closely
reproduce real-world graphs requires domain knowledge and time-consuming
simulation. While existing deep learning approaches rely on less manual
modelling, they offer little interpretability. This work approaches graph
generation (decoding) as the inverse of graph compression (encoding). We show
that in a disentanglement-focused deep autoencoding framework, specifically
Beta-Variational Autoencoders (Beta-VAE), choices of generative procedures and
their parameters arise naturally in the latent space. Our model is capable of
learning disentangled, interpretable latent variables that represent the
generative parameters of procedurally generated random graphs and real-world
graphs. The degree of disentanglement is quantitatively measured using the
Mutual Information Gap (MIG). When training our Beta-VAE model on ER random
graphs, its latent variables have a near one-to-one mapping to the ER random
graph parameters n and p. We deploy the model to analyse the correlation
between graph topology and node attributes measuring their mutual dependence
without handpicking topological properties.",arxiv
http://arxiv.org/abs/1910.00024v3,2020-04-28T15:16:17Z,2019-09-30T18:00:05Z,Neural Canonical Transformation with Symplectic Flows,"Canonical transformation plays a fundamental role in simplifying and solving
classical Hamiltonian systems. We construct flexible and powerful canonical
transformations as generative models using symplectic neural networks. The
model transforms physical variables towards a latent representation with an
independent harmonic oscillator Hamiltonian. Correspondingly, the phase space
density of the physical system flows towards a factorized Gaussian distribution
in the latent space. Since the canonical transformation preserves the
Hamiltonian evolution, the model captures nonlinear collective modes in the
learned latent representation. We present an efficient implementation of
symplectic neural coordinate transformations and two ways to train the model.
The variational free energy calculation is based on the analytical form of
physical Hamiltonian. While the phase space density estimation only requires
samples in the coordinate space for separable Hamiltonians. We demonstrate
appealing features of neural canonical transformation using toy problems
including two-dimensional ring potential and harmonic chain. Finally, we apply
the approach to real-world problems such as identifying slow collective modes
in alanine dipeptide and conceptual compression of the MNIST dataset.",arxiv
http://arxiv.org/abs/1909.11799v4,2021-08-07T16:30:21Z,2019-09-25T22:28:47Z,"Manifold Oblique Random Forests: Towards Closing the Gap on
  Convolutional Deep Networks","Decision forests (Forests), in particular random forests and gradient
boosting trees, have demonstrated state-of-the-art accuracy compared to other
methods in many supervised learning scenarios. In particular, Forests dominate
other methods in tabular data, that is, when the feature space is unstructured,
so that the signal is invariant to a permutation of the feature indices.
However, in structured data lying on a manifold (such as images, text, and
speech) deep networks (Networks), specifically convolutional deep networks
(ConvNets), tend to outperform Forests. We conjecture that at least part of the
reason for this is that the input to Networks is not simply the feature
magnitudes, but also their indices. In contrast, naive Forest implementations
fail to explicitly consider feature indices. A recently proposed Forest
approach demonstrates that Forests, for each node, implicitly sample a random
matrix from some specific distribution. These Forests, like some classes of
Networks, learn by partitioning the feature space into convex polytopes
corresponding to linear functions. We build on that approach and show that one
can choose distributions in a manifold-aware fashion to incorporate feature
locality. We demonstrate the empirical performance on data whose features live
on three different manifolds: a torus, images, and time-series. Moreover, we
demonstrate its strength in multivariate simulated settings and also show
superiority in predicting surgical outcome in epilepsy patients and predicting
movement direction from raw stereotactic EEG data from non-motor brain regions.
In all simulations and real data, Manifold Oblique Random Forest (MORF)
algorithm outperforms approaches that ignore feature space structure and
challenges the performance of ConvNets. Moreover, MORF runs fast and maintains
interpretability and theoretical justification.",arxiv
http://arxiv.org/abs/1909.10707v6,2020-07-05T03:19:06Z,2019-09-24T04:34:58Z,"Invariant Transform Experience Replay: Data Augmentation for Deep
  Reinforcement Learning","Deep Reinforcement Learning (RL) is a promising approach for adaptive robot
control, but its current application to robotics is currently hindered by high
sample requirements. To alleviate this issue, we propose to exploit the
symmetries present in robotic tasks. Intuitively, symmetries from observed
trajectories define transformations that leave the space of feasible RL
trajectories invariant and can be used to generate new feasible trajectories,
which could be used for training. Based on this data augmentation idea, we
formulate a general framework, called Invariant Transform Experience Replay
that we present with two techniques: (i) Kaleidoscope Experience Replay
exploits reflectional symmetries and (ii) Goal-augmented Experience Replay
which takes advantage of lax goal definitions. In the Fetch tasks from OpenAI
Gym, our experimental results show significant increases in learning rates and
success rates. Particularly, we attain a 13, 3, and 5 times speedup in the
pushing, sliding, and pick-and-place tasks respectively in the multi-goal
setting. Performance gains are also observed in similar tasks with obstacles
and we successfully deployed a trained policy on a real Baxter robot. Our work
demonstrates that invariant transformations on RL trajectories are a promising
methodology to speed up learning in deep RL.",arxiv
http://arxiv.org/abs/1909.08703v1,2019-09-18T20:57:35Z,2019-09-18T20:57:35Z,"Deep Complex Networks for Protocol-Agnostic Radio Frequency Device
  Fingerprinting in the Wild","Researchers have demonstrated various techniques for fingerprinting and
identifying devices. Previous approaches have identified devices from their
network traffic or transmitted signals while relying on software or operating
system specific artifacts (e.g., predictability of protocol header fields) or
characteristics of the underlying protocol (e.g.,frequency offset). As these
constraints can be a hindrance in real-world settings, we introduce a
practical, generalizable approach that offers significant operational value for
a variety of scenarios, including as an additional factor of authentication for
preventing impersonation attacks. Our goal is to identify artifacts in
transmitted signals that are caused by a device's unique hardware
""imperfections"" without any knowledge about the nature of the signal. We
develop RF-DCN, a novel Deep Complex-valued Neural Network (DCN) that operates
on raw RF signals and is completely agnostic of the underlying applications and
protocols. We present two DCN variations: (i) Convolutional DCN (CDCN) for
modeling full signals, and (ii) Recurrent DCN (RDCN) for modeling time series.
Our system handles raw I/Q data from open air captures within a given spectrum
window, without knowledge of the modulation scheme or even the carrier
frequencies. While our experiments demonstrate the effectiveness of our system,
especially under challenging conditions where other neural network
architectures break down, we identify additional challenges in signal-based
fingerprinting and provide guidelines for future explorations. Our work lays
the foundation for more research within this vast and challenging space by
establishing fundamental directions for using raw RF I/Q data in novel
complex-valued networks.",arxiv
http://arxiv.org/abs/1909.08030v2,2020-04-01T14:57:13Z,2019-09-17T18:59:15Z,Auto-tuning of double dot devices in situ with machine learning,"The current practice of manually tuning quantum dots (QDs) for qubit
operation is a relatively time-consuming procedure that is inherently
impractical for scaling up and applications. In this work, we report on the
{\it in situ} implementation of a recently proposed autotuning protocol that
combines machine learning (ML) with an optimization routine to navigate the
parameter space. In particular, we show that a ML algorithm trained using
exclusively simulated data to quantitatively classify the state of a double-QD
device can be used to replace human heuristics in the tuning of gate voltages
in real devices. We demonstrate active feedback of a functional double-dot
device operated at millikelvin temperatures and discuss success rates as a
function of the initial conditions and the device performance. Modifications to
the training network, fitness function, and optimizer are discussed as a path
toward further improvement in the success rate when starting both near and far
detuned from the target double-dot range.",arxiv
http://arxiv.org/abs/1909.12906v1,2019-09-16T11:59:40Z,2019-09-16T11:59:40Z,Meta Reinforcement Learning for Sim-to-real Domain Adaptation,"Modern reinforcement learning methods suffer from low sample efficiency and
unsafe exploration, making it infeasible to train robotic policies entirely on
real hardware. In this work, we propose to address the problem of sim-to-real
domain transfer by using meta learning to train a policy that can adapt to a
variety of dynamic conditions, and using a task-specific trajectory generation
model to provide an action space that facilitates quick exploration. We
evaluate the method by performing domain adaptation in simulation and analyzing
the structure of the latent space during adaptation. We then deploy this policy
on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a
hockey puck to a target. Our method shows more consistent and stable domain
adaptation than the baseline, resulting in better overall performance.",arxiv
http://arxiv.org/abs/1909.06993v2,2020-03-08T13:22:41Z,2019-09-16T05:23:14Z,"Learning Visuomotor Policies for Aerial Navigation Using Cross-Modal
  Representations","Machines are a long way from robustly solving open-world perception-control
tasks, such as first-person view (FPV) aerial navigation. While recent advances
in end-to-end Machine Learning, especially Imitation and Reinforcement Learning
appear promising, they are constrained by the need of large amounts of
difficult-to-collect labeled real-world data. Simulated data, on the other
hand, is easy to generate, but generally does not render safe behaviors in
diverse real-life scenarios. In this work we propose a novel method for
learning robust visuomotor policies for real-world deployment which can be
trained purely with simulated data. We develop rich state representations that
combine supervised and unsupervised environment data. Our approach takes a
cross-modal perspective, where separate modalities correspond to the raw camera
data and the system states relevant to the task, such as the relative pose of
gates to the drone in the case of drone racing. We feed both data modalities
into a novel factored architecture, which learns a joint low-dimensional
embedding via Variational Auto Encoders. This compact representation is then
fed into a control policy, which we trained using imitation learning with
expert trajectories in a simulator. We analyze the rich latent spaces learned
with our proposed representations, and show that the use of our cross-modal
architecture significantly improves control policy performance as compared to
end-to-end learning or purely unsupervised feature extractors. We also present
real-world results for drone navigation through gates in different track
configurations and environmental conditions. Our proposed method, which runs
fully onboard, can successfully generalize the learned representations and
policies across simulation and reality, significantly outperforming baseline
approaches.
  Supplementary video: https://youtu.be/VKc3A5HlUU8",arxiv
http://arxiv.org/abs/1909.06769v1,2019-09-15T09:42:33Z,2019-09-15T09:42:33Z,VILD: Variational Imitation Learning with Diverse-quality Demonstrations,"The goal of imitation learning (IL) is to learn a good policy from
high-quality demonstrations. However, the quality of demonstrations in reality
can be diverse, since it is easier and cheaper to collect demonstrations from a
mix of experts and amateurs. IL in such situations can be challenging,
especially when the level of demonstrators' expertise is unknown. We propose a
new IL method called \underline{v}ariational \underline{i}mitation
\underline{l}earning with \underline{d}iverse-quality demonstrations (VILD),
where we explicitly model the level of demonstrators' expertise with a
probabilistic graphical model and estimate it along with a reward function. We
show that a naive approach to estimation is not suitable to large state and
action spaces, and fix its issues by using a variational approach which can be
easily implemented using existing reinforcement learning methods. Experiments
on continuous-control benchmarks demonstrate that VILD outperforms
state-of-the-art methods. Our work enables scalable and data-efficient IL under
more realistic settings than before.",arxiv
http://arxiv.org/abs/1909.02583v2,2019-11-19T03:36:57Z,2019-09-05T18:04:04Z,"Spatiotemporally Constrained Action Space Attacks on Deep Reinforcement
  Learning Agents","Robustness of Deep Reinforcement Learning (DRL) algorithms towards
adversarial attacks in real world applications such as those deployed in
cyber-physical systems (CPS) are of increasing concern. Numerous studies have
investigated the mechanisms of attacks on the RL agent's state space.
Nonetheless, attacks on the RL agent's action space (AS) (corresponding to
actuators in engineering systems) are equally perverse; such attacks are
relatively less studied in the ML literature. In this work, we first frame the
problem as an optimization problem of minimizing the cumulative reward of an RL
agent with decoupled constraints as the budget of attack. We propose a
white-box Myopic Action Space (MAS) attack algorithm that distributes the
attacks across the action space dimensions. Next, we reformulate the
optimization problem above with the same objective function, but with a
temporally coupled constraint on the attack budget to take into account the
approximated dynamics of the agent. This leads to the white-box Look-ahead
Action Space (LAS) attack algorithm that distributes the attacks across the
action and temporal dimensions. Our results shows that using the same amount of
resources, the LAS attack deteriorates the agent's performance significantly
more than the MAS attack. This reveals the possibility that with limited
resource, an adversary can utilize the agent's dynamics to malevolently craft
attacks that causes the agent to fail. Additionally, we leverage these attack
strategies as a possible tool to gain insights on the potential vulnerabilities
of DRL agents.",arxiv
http://arxiv.org/abs/1908.05376v1,2019-08-15T00:06:23Z,2019-08-15T00:06:23Z,"Maximum Relevance and Minimum Redundancy Feature Selection Methods for a
  Marketing Machine Learning Platform","In machine learning applications for online product offerings and marketing
strategies, there are often hundreds or thousands of features available to
build such models. Feature selection is one essential method in such
applications for multiple objectives: improving the prediction accuracy by
eliminating irrelevant features, accelerating the model training and prediction
speed, reducing the monitoring and maintenance workload for feature data
pipeline, and providing better model interpretation and diagnosis capability.
However, selecting an optimal feature subset from a large feature space is
considered as an NP-complete problem. The mRMR (Minimum Redundancy and Maximum
Relevance) feature selection framework solves this problem by selecting the
relevant features while controlling for the redundancy within the selected
features. This paper describes the approach to extend, evaluate, and implement
the mRMR feature selection methods for classification problem in a marketing
machine learning platform at Uber that automates creation and deployment of
targeting and personalization models at scale. This study first extends the
existing mRMR methods by introducing a non-linear feature redundancy measure
and a model-based feature relevance measure. Then an extensive empirical
evaluation is performed for eight different feature selection methods, using
one synthetic dataset and three real-world marketing datasets at Uber to cover
different use cases. Based on the empirical results, the selected mRMR method
is implemented in production for the marketing machine learning platform. A
description of the production implementation is provided and an online
experiment deployed through the platform is discussed.",arxiv
http://arxiv.org/abs/1908.04909v1,2019-08-14T01:31:45Z,2019-08-14T01:31:45Z,Constrained Multi-Objective Optimization for Automated Machine Learning,"Automated machine learning has gained a lot of attention recently. Building
and selecting the right machine learning models is often a multi-objective
optimization problem. General purpose machine learning software that
simultaneously supports multiple objectives and constraints is scant, though
the potential benefits are great. In this work, we present a framework called
Autotune that effectively handles multiple objectives and constraints that
arise in machine learning problems. Autotune is built on a suite of
derivative-free optimization methods, and utilizes multi-level parallelism in a
distributed computing environment for automatically training, scoring, and
selecting good models. Incorporation of multiple objectives and constraints in
the model exploration and selection process provides the flexibility needed to
satisfy trade-offs necessary in practical machine learning applications.
Experimental results from standard multi-objective optimization benchmark
problems show that Autotune is very efficient in capturing Pareto fronts. These
benchmark results also show how adding constraints can guide the search to more
promising regions of the solution space, ultimately producing more desirable
Pareto fronts. Results from two real-world case studies demonstrate the
effectiveness of the constrained multi-objective optimization capability
offered by Autotune.",arxiv
http://arxiv.org/abs/1908.04355v4,2020-07-02T13:47:36Z,2019-08-12T19:33:58Z,Adversarial Neural Pruning with Latent Vulnerability Suppression,"Despite the remarkable performance of deep neural networks on various
computer vision tasks, they are known to be susceptible to adversarial
perturbations, which makes it challenging to deploy them in real-world
safety-critical applications. In this paper, we conjecture that the leading
cause of adversarial vulnerability is the distortion in the latent feature
space, and provide methods to suppress them effectively. Explicitly, we define
\emph{vulnerability} for each latent feature and then propose a new loss for
adversarial learning, \emph{Vulnerability Suppression (VS)} loss, that aims to
minimize the feature-level vulnerability during training. We further propose a
Bayesian framework to prune features with high vulnerability to reduce both
vulnerability and loss on adversarial samples. We validate our
\emph{Adversarial Neural Pruning with Vulnerability Suppression (ANP-VS)}
method on multiple benchmark datasets, on which it not only obtains
state-of-the-art adversarial robustness but also improves the performance on
clean examples, using only a fraction of the parameters used by the full
network. Further qualitative analysis suggests that the improvements come from
the suppression of feature-level vulnerability.",arxiv
http://arxiv.org/abs/1908.02947v1,2019-08-08T06:45:05Z,2019-08-08T06:45:05Z,Graph Node Embeddings using Domain-Aware Biased Random Walks,"The recent proliferation of publicly available graph-structured data has
sparked an interest in machine learning algorithms for graph data. Since most
traditional machine learning algorithms assume data to be tabular, embedding
algorithms for mapping graph data to real-valued vector spaces has become an
active area of research. Existing graph embedding approaches are based purely
on structural information and ignore any semantic information from the
underlying domain. In this paper, we demonstrate that semantic information can
play a useful role in computing graph embeddings. Specifically, we present a
framework for devising embedding strategies aware of domain-specific
interpretations of graph nodes and edges, and use knowledge of downstream
machine learning tasks to identify relevant graph substructures. Using two
real-life domains, we show that our framework yields embeddings that are simple
to implement and yet achieve equal or greater accuracy in machine learning
tasks compared to domain independent approaches.",arxiv
http://arxiv.org/abs/1908.00754v1,2019-08-02T08:31:36Z,2019-08-02T08:31:36Z,"A Visual Technique to Analyze Flow of Information in a Machine Learning
  System","Machine learning (ML) algorithms and machine learning based software systems
implicitly or explicitly involve complex flow of information between various
entities such as training data, feature space, validation set and results.
Understanding the statistical distribution of such information and how they
flow from one entity to another influence the operation and correctness of such
systems, especially in large-scale applications that perform classification or
prediction in real time. In this paper, we propose a visual approach to
understand and analyze flow of information during model training and serving
phases. We build the visualizations using a technique called Sankey Diagram -
conventionally used to understand data flow among sets - to address various use
cases of in a machine learning system. We demonstrate how the proposed
technique, tweaked and twisted to suit a classification problem, can play a
critical role in better understanding of the training data, the features, and
the classifier performance. We also discuss how this technique enables
diagnostic analysis of model predictions and comparative analysis of
predictions from multiple classifiers. The proposed concept is illustrated with
the example of categorization of millions of products in the e-commerce domain
- a multi-class hierarchical classification problem.",arxiv
http://arxiv.org/abs/1907.13525v1,2019-07-31T14:28:55Z,2019-07-31T14:28:55Z,"Local Interpretation Methods to Machine Learning Using the Domain of the
  Feature Space","As machine learning becomes an important part of many real world applications
affecting human lives, new requirements, besides high predictive accuracy,
become important. One important requirement is transparency, which has been
associated with model interpretability. Many machine learning algorithms induce
models difficult to interpret, named black box. Moreover, people have
difficulty to trust models that cannot be explained. In particular for machine
learning, many groups are investigating new methods able to explain black box
models. These methods usually look inside the black models to explain their
inner work. By doing so, they allow the interpretation of the decision making
process used by black box models. Among the recently proposed model
interpretation methods, there is a group, named local estimators, which are
designed to explain how the label of particular instance is predicted. For
such, they induce interpretable models on the neighborhood of the instance to
be explained. Local estimators have been successfully used to explain specific
predictions. Although they provide some degree of model interpretability, it is
still not clear what is the best way to implement and apply them. Open
questions include: how to best define the neighborhood of an instance? How to
control the trade-off between the accuracy of the interpretation method and its
interpretability? How to make the obtained solution robust to small variations
on the instance to be explained? To answer to these questions, we propose and
investigate two strategies: (i) using data instance properties to provide
improved explanations, and (ii) making sure that the neighborhood of an
instance is properly defined by taking the geometry of the domain of the
feature space into account. We evaluate these strategies in a regression task
and present experimental results that show that they can improve local
explanations.",arxiv
http://arxiv.org/abs/1907.09209v1,2019-07-22T10:04:22Z,2019-07-22T10:04:22Z,"Automatic Calibration of Artificial Neural Networks for Zebrafish
  Collective Behaviours using a Quality Diversity Algorithm","During the last two decades, various models have been proposed for fish
collective motion. These models are mainly developed to decipher the biological
mechanisms of social interaction between animals. They consider very simple
homogeneous unbounded environments and it is not clear that they can simulate
accurately the collective trajectories. Moreover when the models are more
accurate, the question of their scalability to either larger groups or more
elaborate environments remains open. This study deals with learning how to
simulate realistic collective motion of collective of zebrafish, using
real-world tracking data. The objective is to devise an agent-based model that
can be implemented on an artificial robotic fish that can blend into a
collective of real fish. We present a novel approach that uses Quality
Diversity algorithms, a class of algorithms that emphasise exploration over
pure optimisation. In particular, we use CVT-MAP-Elites, a variant of the
state-of-the-art MAP-Elites algorithm for high dimensional search space.
Results show that Quality Diversity algorithms not only outperform classic
evolutionary reinforcement learning methods at the macroscopic level (i.e.
group behaviour), but are also able to generate more realistic biomimetic
behaviours at the microscopic level (i.e. individual behaviour).",arxiv
http://arxiv.org/abs/1907.07958v1,2019-07-18T09:58:27Z,2019-07-18T09:58:27Z,Transfer Learning Across Simulated Robots With Different Sensors,"For a robot to learn a good policy, it often requires expensive equipment
(such as sophisticated sensors) and a prepared training environment conducive
to learning. However, it is seldom possible to perfectly equip robots for
economic reasons, nor to guarantee ideal learning conditions, when deployed in
real-life environments. A solution would be to prepare the robot in the lab
environment, when all necessary material is available to learn a good policy.
After training in the lab, the robot should be able to get by without the
expensive equipment that used to be available to it, and yet still be
guaranteed to perform well on the field. The transition between the lab
(source) and the real-world environment (target) is related to transfer
learning, where the state-space between the source and target tasks differ. We
tackle a simulated task with continuous states and discrete actions presenting
this challenge, using Bootstrapped Dual Policy Iteration, a model-free
actor-critic reinforcement learning algorithm, and Policy Shaping.
Specifically, we train a BDPI agent, embodied by a virtual robot performing a
task in the V-Rep simulator, sensing its environment through several proximity
sensors. The resulting policy is then used by a second agent learning the same
task in the same environment, but with camera images as input. The goal is to
obtain a policy able to perform the task relying on merely camera images.",arxiv
http://arxiv.org/abs/1907.02526v1,2019-07-03T21:25:21Z,2019-07-03T21:25:21Z,"Convolutional Neural Network-based Speech Enhancement for Cochlear
  Implant Recipients","Attempts to develop speech enhancement algorithms with improved speech
intelligibility for cochlear implant (CI) users have met with limited success.
To improve speech enhancement methods for CI users, we propose to perform
speech enhancement in a cochlear filter-bank feature space, a feature-set
specifically designed for CI users based on CI auditory stimuli. We leverage a
convolutional neural network (CNN) to extract both stationary and
non-stationary components of environmental acoustics and speech. We propose
three CNN architectures: (1) vanilla CNN that directly generates the enhanced
signal; (2) spectral-subtraction-style CNN (SS-CNN) that first predicts noise
and then generates the enhanced signal by subtracting noise from the noisy
signal; (3) Wiener-style CNN (Wiener-CNN) that generates an optimal mask for
suppressing noise. An important problem of the proposed networks is that they
introduce considerable delays, which limits their real-time application for CI
users. To address this, this study also considers causal variations of these
networks. Our experiments show that the proposed networks (both causal and
non-causal forms) achieve significant improvement over existing baseline
systems. We also found that causal Wiener-CNN outperforms other networks, and
leads to the best overall envelope coefficient measure (ECM). The proposed
algorithms represent a viable option for implementation on the CCi-MOBILE
research platform as a pre-processor for CI users in naturalistic environments.",arxiv
http://arxiv.org/abs/1907.00498v4,2020-07-08T22:48:11Z,2019-06-30T23:46:30Z,"Proof of Witness Presence: Blockchain Consensus for Augmented Democracy
  in Smart Cities","Smart Cities evolve into complex and pervasive urban environments with a
citizens' mandate to meet sustainable development goals. Repositioning
democratic values of citizens' choices in these complex ecosystems has turned
out to be imperative in an era of social media filter bubbles, fake news and
opportunities for manipulating electoral results with such means. This paper
introduces a new paradigm of augmented democracy that promises actively
engaging citizens in a more informed decision-making augmented into public
urban space. The proposed concept is inspired by a digital revive of the
Ancient Agora of Athens, an arena of public discourse, a Polis where citizens
assemble to actively deliberate and collectively decide about public matters.
The core contribution of the proposed paradigm is the concept of proving
witness presence: making decision-making subject of providing secure evidence
and testifying for choices made in the physical space. This paper shows how the
challenge of proving witness presence can be tackled with blockchain consensus
to empower citizens' trust and overcome security vulnerabilities of GPS
localization. Moreover, a novel platform for collective decision-making and
crowd-sensing in urban space is introduced: Smart Agora. It is shown how
real-time collective measurements over citizens' choices can be made in a fully
decentralized and privacy-preserving way. Witness presence is tested by
deploying a decentralized system for crowd-sensing the sustainable use of
transport means. Furthermore, witness presence of cycling risk is validated
using official accident data from public authorities, which are compared
against wisdom of the crowd. The paramount role of dynamic consensus,
self-governance and ethically aligned artificial intelligence in the augmented
democracy paradigm is outlined.",arxiv
http://arxiv.org/abs/1907.00456v2,2019-07-08T17:21:46Z,2019-06-30T20:53:19Z,"Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human
  Preferences in Dialog","Most deep reinforcement learning (RL) systems are not able to learn
effectively from off-policy data, especially if they cannot explore online in
the environment. These are critical shortcomings for applying RL to real-world
problems where collecting data is expensive, and models must be tested offline
before being deployed to interact with the environment -- e.g. systems that
learn from human interaction. Thus, we develop a novel class of off-policy
batch RL algorithms, which are able to effectively learn offline, without
exploring, from a fixed batch of human interaction data. We leverage models
pre-trained on data as a strong prior, and use KL-control to penalize
divergence from this prior during RL training. We also use dropout-based
uncertainty estimates to lower bound the target Q-values as a more efficient
alternative to Double Q-Learning. The algorithms are tested on the problem of
open-domain dialog generation -- a challenging reinforcement learning problem
with a 20,000-dimensional action space. Using our Way Off-Policy algorithm, we
can extract multiple different reward functions post-hoc from collected human
interaction data, and learn effectively from all of these. We test the
real-world generalization of these systems by deploying them live to converse
with humans in an open-domain setting, and demonstrate that our algorithm
achieves significant improvements over prior methods in off-policy batch RL.",arxiv
http://arxiv.org/abs/1907.00036v1,2019-06-26T01:16:22Z,2019-06-26T01:16:22Z,"Novel Suboptimal approaches for Hyperparameter Tuning of Deep Neural
  Network [under the shelf of Optical Communication]","Hyperparameter tuning is the main challenge of machine learning (ML)
algorithms. Grid search is a popular method in hyperparameter tuning of simple
ML algorithms; however, high computational complexity in complex ML algorithms
such as Deep Neural Networks (DNN) is the main barrier towards its practical
implementation. In this paper, two novel suboptimal grid search methods are
presented, which search the grid marginally and alternating. In order to
examine these methods, hyperparameter tuning is applied on two different DNN
based Optical Communication (OC) systems (Fiber OC, and Free Space Optical
(FSO) communication). The hyperparameter tuning of ML algorithms, despite its
importance is ignored in ML for OC investigations. In addition, this is the
first consideration of both FSO and Fiber OC systems in an ML for OC
investigation. Results indicate that despite greatly reducing computation load,
favorable performance could be achieved by the proposed methods. In addition,
it is shown that the alternating search method has better performance than
marginal grid search method. In sum, the proposed structures are
cost-effective, and appropriate for real-time applications.",arxiv
http://arxiv.org/abs/1906.04023v1,2019-06-10T14:35:07Z,2019-06-10T14:35:07Z,Project Thyia: A Forever Gameplayer,"The space of Artificial Intelligence entities is dominated by conversational
bots. Some of them fit in our pockets and we take them everywhere we go, or
allow them to be a part of human homes. Siri, Alexa, they are recognised as
present in our world. But a lot of games research is restricted to existing in
the separate realm of software. We enter different worlds when playing games,
but those worlds cease to exist once we quit. Similarly, AI game-players are
run once on a game (or maybe for longer periods of time, in the case of
learning algorithms which need some, still limited, period for training), and
they cease to exist once the game ends. But what if they didn't? What if there
existed artificial game-players that continuously played games, learned from
their experiences and kept getting better? What if they interacted with the
real world and us, humans: live-streaming games, chatting with viewers,
accepting suggestions for strategies or games to play, forming opinions on
popular game titles? In this paper, we introduce the vision behind a new
project called Thyia, which focuses around creating a present, continuous,
`always-on', interactive game-player.",arxiv
http://arxiv.org/abs/1905.13118v1,2019-05-30T15:50:14Z,2019-05-30T15:50:14Z,"Standing on the Shoulders of Giants: AI-driven Calibration of
  Localisation Technologies","High accuracy localisation technologies exist but are prohibitively expensive
to deploy for large indoor spaces such as warehouses, factories, and
supermarkets to track assets and people. However, these technologies can be
used to lend their highly accurate localisation capabilities to low-cost,
commodity, and less-accurate technologies. In this paper, we bridge this link
by proposing a technology-agnostic calibration framework based on artificial
intelligence to assist such low-cost technologies through highly accurate
localisation systems. A single-layer neural network is used to calibrate less
accurate technology using more accurate one such as BLE using UWB and UWB using
a professional motion tracking system. On a real indoor testbed, we demonstrate
an increase in accuracy of approximately 70% for BLE and 50% for UWB. Not only
the proposed approach requires a very short measurement campaign, the low
complexity of the single-layer neural network also makes it ideal for
deployment on constrained devices typically for localisation purposes.",arxiv
http://arxiv.org/abs/1905.09673v2,2019-05-29T16:35:23Z,2019-05-23T14:15:51Z,"Deep Q-Learning with Q-Matrix Transfer Learning for Novel Fire
  Evacuation Environment","We focus on the important problem of emergency evacuation, which clearly
could benefit from reinforcement learning that has been largely unaddressed.
Emergency evacuation is a complex task which is difficult to solve with
reinforcement learning, since an emergency situation is highly dynamic, with a
lot of changing variables and complex constraints that makes it difficult to
train on. In this paper, we propose the first fire evacuation environment to
train reinforcement learning agents for evacuation planning. The environment is
modelled as a graph capturing the building structure. It consists of realistic
features like fire spread, uncertainty and bottlenecks. We have implemented the
environment in the OpenAI gym format, to facilitate future research. We also
propose a new reinforcement learning approach that entails pretraining the
network weights of a DQN based agents to incorporate information on the
shortest path to the exit. We achieved this by using tabular Q-learning to
learn the shortest path on the building model's graph. This information is
transferred to the network by deliberately overfitting it on the Q-matrix.
Then, the pretrained DQN model is trained on the fire evacuation environment to
generate the optimal evacuation path under time varying conditions. We perform
comparisons of the proposed approach with state-of-the-art reinforcement
learning algorithms like PPO, VPG, SARSA, A2C and ACKTR. The results show that
our method is able to outperform state-of-the-art models by a huge margin
including the original DQN based models. Finally, we test our model on a large
and complex real building consisting of 91 rooms, with the possibility to move
to any other room, hence giving 8281 actions. We use an attention based
mechanism to deal with large action spaces. Our model achieves near optimal
performance on the real world emergency environment.",arxiv
http://arxiv.org/abs/1905.07082v6,2021-06-26T12:14:13Z,2019-05-17T01:35:26Z,"The Audio Auditor: User-Level Membership Inference in Internet of Things
  Voice Services","With the rapid development of deep learning techniques, the popularity of
voice services implemented on various Internet of Things (IoT) devices is ever
increasing. In this paper, we examine user-level membership inference in the
problem space of voice services, by designing an audio auditor to verify
whether a specific user had unwillingly contributed audio used to train an
automatic speech recognition (ASR) model under strict black-box access. With
user representation of the input audio data and their corresponding translated
text, our trained auditor is effective in user-level audit. We also observe
that the auditor trained on specific data can be generalized well regardless of
the ASR model architecture. We validate the auditor on ASR models trained with
LSTM, RNNs, and GRU algorithms on two state-of-the-art pipelines, the hybrid
ASR system and the end-to-end ASR system. Finally, we conduct a real-world
trial of our auditor on iPhone Siri, achieving an overall accuracy exceeding
80\%. We hope the methodology developed in this paper and findings can inform
privacy advocates to overhaul IoT privacy.",arxiv
http://arxiv.org/abs/1905.06809v1,2019-05-16T14:50:28Z,2019-05-16T14:50:28Z,Occupancy Estimation Using Low-Cost Wi-Fi Sniffers,"Real-time measurements on the occupancy status of indoor and outdoor spaces
can be exploited in many scenarios (HVAC and lighting system control, building
energy optimization, allocation and reservation of spaces, etc.). Traditional
systems for occupancy estimation rely on environmental sensors (CO2,
temperature, humidity) or video cameras. In this paper, we depart from such
traditional approaches and propose a novel occupancy estimation system which is
based on the capture of Wi-Fi management packets from users' devices. The
system, implemented on a low-cost ESP8266 microcontroller, leverages a
supervised learning model to adapt to different spaces and transmits occupancy
information through the MQTT protocol to a web-based dashboard. Experimental
results demonstrate the validity of the proposed solution in four different
indoor university spaces.",arxiv
http://arxiv.org/abs/1905.03813v4,2019-10-15T06:11:03Z,2019-05-09T18:47:38Z,When Deep Learning Met Code Search,"There have been multiple recent proposals on using deep neural networks for
code search using natural language. Common across these proposals is the idea
of $\mathit{embedding}$ code and natural language queries, into real vectors
and then using vector distance to approximate semantic correlation between code
and the query. Multiple approaches exist for learning these embeddings,
including $\mathit{unsupervised}$ techniques, which rely only on a corpus of
code examples, and $\mathit{supervised}$ techniques, which use an
$\mathit{aligned}$ corpus of paired code and natural language descriptions. The
goal of this supervision is to produce embeddings that are more similar for a
query and the corresponding desired code snippet. Clearly, there are choices in
whether to use supervised techniques at all, and if one does, what sort of
network and training to use for supervision. This paper is the first to
evaluate these choices systematically. To this end, we assembled
implementations of state-of-the-art techniques to run on a common platform,
training and evaluation corpora. To explore the design space in network
complexity, we also introduced a new design point that is a $\mathit{minimal}$
supervision extension to an existing unsupervised technique. Our evaluation
shows that: 1. adding supervision to an existing unsupervised technique can
improve performance, though not necessarily by much; 2. simple networks for
supervision can be more effective that more sophisticated sequence-based
networks for code search; 3. while it is common to use docstrings to carry out
supervision, there is a sizeable gap between the effectiveness of docstrings
and a more query-appropriate supervision corpus.
  The evaluation dataset is now available at arXiv:1908.09804",arxiv
http://arxiv.org/abs/1904.13001v1,2019-04-30T00:24:06Z,2019-04-30T00:24:06Z,"Encoding Categorical Variables with Conjugate Bayesian Models for WeWork
  Lead Scoring Engine","Applied Data Scientists throughout various industries are commonly faced with
the challenging task of encoding high-cardinality categorical features into
digestible inputs for machine learning algorithms. This paper describes a
Bayesian encoding technique developed for WeWork's lead scoring engine which
outputs the probability of a person touring one of our office spaces based on
interaction, enrichment, and geospatial data. We present a paradigm for
ensemble modeling which mitigates the need to build complicated preprocessing
and encoding schemes for categorical variables. In particular, domain-specific
conjugate Bayesian models are employed as base learners for features in a
stacked ensemble model. For each column of a categorical feature matrix we fit
a problem-specific prior distribution, for example, the Beta distribution for a
binary classification problem. In order to analytically derive the moments of
the posterior distribution, we update the prior with the conjugate likelihood
of the corresponding target variable for each unique value of the given
categorical feature. This function of column and value encodes the categorical
feature matrix so that the final learner in the ensemble model ingests
low-dimensional numerical input. Experimental results on both curated and real
world datasets demonstrate impressive accuracy and computational efficiency on
a variety of problem archetypes. Particularly, for the lead scoring engine at
WeWork -- where some categorical features have as many as 300,000 levels -- we
have seen an AUC improvement from 0.87 to 0.97 through implementing conjugate
Bayesian model encoding.",arxiv
http://arxiv.org/abs/1904.10698v1,2019-04-24T08:43:18Z,2019-04-24T08:43:18Z,Multi-scale deep neural networks for real image super-resolution,"Single image super-resolution (SR) is extremely difficult if the upscaling
factors of image pairs are unknown and different from each other, which is
common in real image SR. To tackle the difficulty, we develop two multi-scale
deep neural networks (MsDNN) in this work. Firstly, due to the high computation
complexity in high-resolution spaces, we process an input image mainly in two
different downscaling spaces, which could greatly lower the usage of GPU
memory. Then, to reconstruct the details of an image, we design a multi-scale
residual network (MsRN) in the downscaling spaces based on the residual blocks.
Besides, we propose a multi-scale dense network based on the dense blocks to
compare with MsRN. Finally, our empirical experiments show the robustness of
MsDNN for image SR when the upscaling factor is unknown. According to the
preliminary results of NTIRE 2019 image SR challenge, our team
(ZXHresearch@fudan) ranks 21-st among all participants. The implementation of
MsDNN is released https://github.com/shangqigao/gsq-image-SR",arxiv
http://arxiv.org/abs/1904.08489v2,2019-08-15T19:56:17Z,2019-04-17T20:39:17Z,"Semantic Adversarial Attacks: Parametric Transformations That Fool Deep
  Classifiers","Deep neural networks have been shown to exhibit an intriguing vulnerability
to adversarial input images corrupted with imperceptible perturbations.
However, the majority of adversarial attacks assume global, fine-grained
control over the image pixel space. In this paper, we consider a different
setting: what happens if the adversary could only alter specific attributes of
the input image? These would generate inputs that might be perceptibly
different, but still natural-looking and enough to fool a classifier. We
propose a novel approach to generate such `semantic' adversarial examples by
optimizing a particular adversarial loss over the range-space of a parametric
conditional generative model. We demonstrate implementations of our attacks on
binary classifiers trained on face images, and show that such natural-looking
semantic adversarial examples exist. We evaluate the effectiveness of our
attack on synthetic and real data, and present detailed comparisons with
existing attack methods. We supplement our empirical results with theoretical
bounds that demonstrate the existence of such parametric adversarial examples.",arxiv
http://arxiv.org/abs/1904.07623v1,2019-04-16T12:33:56Z,2019-04-16T12:33:56Z,"DeepRadioID: Real-Time Channel-Resilient Optimization of Deep
  Learning-based Radio Fingerprinting Algorithms","Radio fingerprinting provides a reliable and energy-efficient IoT
authentication strategy. By mapping inputs onto a very large feature space,
deep learning algorithms can be trained to fingerprint large populations of
devices operating under any wireless standard. One of the most crucial
challenges in radio fingerprinting is to counteract the action of the wireless
channel, which decreases fingerprinting accuracy significantly by disrupting
hardware impairments. On the other hand, due to their sheer size, deep learning
algorithms are hardly re-trainable in real-time. Another aspect that is yet to
be investigated is whether an adversary can successfully impersonate another
device fingerprint. To address these key issues, this paper proposes
DeepRadioID, a system to optimize the accuracy of deep-learning-based radio
fingerprinting algorithms without retraining the underlying deep learning
model. We extensively evaluate DeepRadioID on a experimental testbed of 20
nominally-identical software-defined radios, as well as on two datasets made up
by 500 ADS-B devices and by 500 WiFi devices provided by the DARPA RFMLS
program. Experimental results show that DeepRadioID (i) increases
fingerprinting accuracy by about 35%, 50% and 58% on the three scenarios
considered; (ii) decreases an adversary's accuracy by about 54% when trying to
imitate other device fingerprints by using their filters; (iii) achieves 27%
improvement over the state of the art on a 100-device dataset.",arxiv
http://arxiv.org/abs/1904.05343v2,2020-03-26T02:51:10Z,2019-04-10T17:53:38Z,StegaStamp: Invisible Hyperlinks in Physical Photographs,"Printed and digitally displayed photos have the ability to hide imperceptible
digital data that can be accessed through internet-connected imaging systems.
Another way to think about this is physical photographs that have unique QR
codes invisibly embedded within them. This paper presents an architecture,
algorithms, and a prototype implementation addressing this vision. Our key
technical contribution is StegaStamp, a learned steganographic algorithm to
enable robust encoding and decoding of arbitrary hyperlink bitstrings into
photos in a manner that approaches perceptual invisibility. StegaStamp
comprises a deep neural network that learns an encoding/decoding algorithm
robust to image perturbations approximating the space of distortions resulting
from real printing and photography. We demonstrates real-time decoding of
hyperlinks in photos from in-the-wild videos that contain variation in
lighting, shadows, perspective, occlusion and viewing distance. Our prototype
system robustly retrieves 56 bit hyperlinks after error correction - sufficient
to embed a unique code within every photo on the internet.",arxiv
http://arxiv.org/abs/1904.04433v1,2019-04-09T02:45:35Z,2019-04-09T02:45:35Z,"Efficient Decision-based Black-box Adversarial Attacks on Face
  Recognition","Face recognition has obtained remarkable progress in recent years due to the
great improvement of deep convolutional neural networks (CNNs). However, deep
CNNs are vulnerable to adversarial examples, which can cause fateful
consequences in real-world face recognition applications with
security-sensitive purposes. Adversarial attacks are widely studied as they can
identify the vulnerability of the models before they are deployed. In this
paper, we evaluate the robustness of state-of-the-art face recognition models
in the decision-based black-box attack setting, where the attackers have no
access to the model parameters and gradients, but can only acquire hard-label
predictions by sending queries to the target model. This attack setting is more
practical in real-world face recognition systems. To improve the efficiency of
previous methods, we propose an evolutionary attack algorithm, which can model
the local geometries of the search directions and reduce the dimension of the
search space. Extensive experiments demonstrate the effectiveness of the
proposed method that induces a minimum perturbation to an input face image with
fewer queries. We also apply the proposed method to attack a real-world face
recognition system successfully.",arxiv
http://arxiv.org/abs/1904.00923v1,2019-04-01T15:51:12Z,2019-04-01T15:51:12Z,Robustness of 3D Deep Learning in an Adversarial Setting,"Understanding the spatial arrangement and nature of real-world objects is of
paramount importance to many complex engineering tasks, including autonomous
navigation. Deep learning has revolutionized state-of-the-art performance for
tasks in 3D environments; however, relatively little is known about the
robustness of these approaches in an adversarial setting. The lack of
comprehensive analysis makes it difficult to justify deployment of 3D deep
learning models in real-world, safety-critical applications. In this work, we
develop an algorithm for analysis of pointwise robustness of neural networks
that operate on 3D data. We show that current approaches presented for
understanding the resilience of state-of-the-art models vastly overestimate
their robustness. We then use our algorithm to evaluate an array of
state-of-the-art models in order to demonstrate their vulnerability to
occlusion attacks. We show that, in the worst case, these networks can be
reduced to 0% classification accuracy after the occlusion of at most 6.5% of
the occupied input space.",arxiv
http://arxiv.org/abs/1904.00035v1,2019-03-29T18:15:24Z,2019-03-29T18:15:24Z,Autonomous Highway Driving using Deep Reinforcement Learning,"The operational space of an autonomous vehicle (AV) can be diverse and vary
significantly. This may lead to a scenario that was not postulated in the
design phase. Due to this, formulating a rule based decision maker for
selecting maneuvers may not be ideal. Similarly, it may not be effective to
design an a-priori cost function and then solve the optimal control problem in
real-time. In order to address these issues and to avoid peculiar behaviors
when encountering unforeseen scenario, we propose a reinforcement learning (RL)
based method, where the ego car, i.e., an autonomous vehicle, learns to make
decisions by directly interacting with simulated traffic. The decision maker
for AV is implemented as a deep neural network providing an action choice for a
given system state. In a critical application such as driving, an RL agent
without explicit notion of safety may not converge or it may need extremely
large number of samples before finding a reliable policy. To best address the
issue, this paper incorporates reinforcement learning with an additional short
horizon safety check (SC). In a critical scenario, the safety check will also
provide an alternate safe action to the agent provided if it exists. This leads
to two novel contributions. First, it generalizes the states that could lead to
undesirable ""near-misses"" or ""collisions "". Second, inclusion of safety check
can provide a safe and stable training environment. This significantly enhances
learning efficiency without inhibiting meaningful exploration to ensure safe
and optimal learned behavior. We demonstrate the performance of the developed
algorithm in highway driving scenario where the trained AV encounters varying
traffic density in a highway setting.",arxiv
http://arxiv.org/abs/1903.12493v2,2019-06-01T02:05:24Z,2019-03-29T13:00:03Z,Asymmetric Deep Semantic Quantization for Image Retrieval,"Due to its fast retrieval and storage efficiency capabilities, hashing has
been widely used in nearest neighbor retrieval tasks. By using deep learning
based techniques, hashing can outperform non-learning based hashing technique
in many applications. However, we argue that the current deep learning based
hashing methods ignore some critical problems (e.g., the learned hash codes are
not discriminative due to the hashing methods being unable to discover rich
semantic information and the training strategy having difficulty optimizing the
discrete binary codes). In this paper, we propose a novel image hashing method,
termed as \textbf{\underline{A}}symmetric \textbf{\underline{D}}eep
\textbf{\underline{S}}emantic \textbf{\underline{Q}}uantization
(\textbf{ADSQ}). \textbf{ADSQ} is implemented using three stream frameworks,
which consist of one \emph{LabelNet} and two \emph{ImgNets}. The
\emph{LabelNet} leverages the power of three fully-connected layers, which are
used to capture rich semantic information between image pairs. For the two
\emph{ImgNets}, they each adopt the same convolutional neural network
structure, but with different weights (i.e., asymmetric convolutional neural
networks). The two \emph{ImgNets} are used to generate discriminative compact
hash codes. Specifically, the function of the \emph{LabelNet} is to capture
rich semantic information that is used to guide the two \emph{ImgNets} in
minimizing the gap between the real-continuous features and the discrete binary
codes. Furthermore, \textbf{ADSQ} can utilize the most critical semantic
information to guide the feature learning process and consider the consistency
of the common semantic space and Hamming space. Experimental results on three
benchmarks (i.e., CIFAR-10, NUS-WIDE, and ImageNet) demonstrate that the
proposed \textbf{ADSQ} can outperforms current state-of-the-art methods.",arxiv
http://arxiv.org/abs/1903.04824v1,2019-03-12T10:28:36Z,2019-03-12T10:28:36Z,"Proceedings of the Fifth International Conference on Cloud and Robotics
  (ICCR2018)","The 5th edition of the International Conference on Cloud and Robotics (ICCR
2018 - http://cloudrobotics.info) will be held on November 12-14 2018 in Paris
and Saint-Quentin, France. The conference is a co-event with GDR ALROB and the
industry exposition Robonumerique (http://www.robonumerique.fr).
  The domain of cloud robotics aims to converge robots with computation,
storage and communication resources provided by the cloud. The cloud may
complement robotic resources in several ways, including crowd-sourcing
knowledge databases, context information, computational offloading or
data-intensive information processing for artificial intelligence. Today, the
paradigms of cloud/fog/edge computing propose software architecture solutions
for robots to share computations or offload them to ambiant and networked
resources. Yet, combining distant computations with the real time constraints
of robotics is very challenging. As the challenges in this domain are
multi-disciplinary and similar in other research areas, Cloud Robotics aims at
building bridges among experts from academia and industry working in different
fields, such as robotics, cyber-physical systems, automotive, aerospace,
machine learning, artificial intelligence, software architecture, big data
analytics, Internet-of-Things, networked control and distributed cloud systems.",arxiv
http://arxiv.org/abs/1903.04507v1,2019-03-11T18:00:18Z,2019-03-11T18:00:18Z,"Realistic On-The-Fly Outcomes of Planetary Collisions: Machine Learning
  Applied to Simulations of Giant Impacts","Planet formation simulations are capable of directly integrating the
evolution of hundreds to thousands of planetary embryos and planetesimals, as
they accrete pairwise to become planets. In principle such investigations allow
us to better understand the final configuration and geochemistry of the
terrestrial planets, as well as to place our solar system in the context of
other exosolar systems. These simulations, however, classically prescribe
collisions to result in perfect mergers, but computational advances have begun
to allow for more complex outcomes to be implemented. Here we apply machine
learning to a large but sparse database of giant impact studies, streamlining
simulations into a classifier of collision outcomes and a regressor of
accretion efficiency. The classifier maps a 4-Dimensional parameter space
(target mass, projectile-to-target mass ratio, impact velocity, impact angle)
into the four major collision types: merger, ""graze-and-merge"", ""hit-and-run"",
and disruption. The definition of the four regimes and their boundary is fully
data-driven; the results do not suffer from any model assumption in the
fitting. The classifier maps the structure of the parameter space and provides
insights about the outcome regimes. The regressor is a neural network which is
trained to closely mimic the functional relationship between the 4-D space of
collision parameters, and a real-variable outcome, the mass of the largest
remnant. This work is a prototype of a more complete surrogate model, based on
extended sets of simulations (""big data""), that will quickly and reliably
predict specific collision outcomes for use in realistic N-body dynamical
studies of planetary formation.",arxiv
http://arxiv.org/abs/1903.03777v2,2019-04-05T06:54:27Z,2019-03-09T10:41:41Z,"Partial Order Pruning: for Best Speed/Accuracy Trade-off in Neural
  Architecture Search","Achieving good speed and accuracy trade-off on a target platform is very
important in deploying deep neural networks in real world scenarios. However,
most existing automatic architecture search approaches only concentrate on high
performance. In this work, we propose an algorithm that can offer better
speed/accuracy trade-off of searched networks, which is termed ""Partial Order
Pruning"". It prunes the architecture search space with a partial order
assumption to automatically search for the architectures with the best speed
and accuracy trade-off. Our algorithm explicitly takes profile information
about the inference speed on the target platform into consideration. With the
proposed algorithm, we present several Dongfeng (DF) networks that provide high
accuracy and fast inference speed on various application GPU platforms. By
further searching decoder architectures, our DF-Seg real-time segmentation
networks yield state-of-the-art speed/accuracy trade-off on both the target
embedded device and the high-end GPU.",arxiv
http://arxiv.org/abs/1903.03404v2,2019-03-28T08:25:41Z,2019-03-08T13:03:11Z,"Accelerating Generalized Linear Models with MLWeaving: A
  One-Size-Fits-All System for Any-precision Learning (Technical Report)","Learning from the data stored in a database is an important function
increasingly available in relational engines. Methods using lower precision
input data are of special interest given their overall higher efficiency but,
in databases, these methods have a hidden cost: the quantization of the real
value into a smaller number is an expensive step. To address the issue, in this
paper we present MLWeaving, a data structure and hardware acceleration
technique intended to speed up learning of generalized linear models in
databases. ML-Weaving provides a compact, in-memory representation enabling the
retrieval of data at any level of precision. MLWeaving also takes advantage of
the increasing availability of FPGA-based accelerators to provide a highly
efficient implementation of stochastic gradient descent. The solution adopted
in MLWeaving is more efficient than existing designs in terms of space (since
it can process any resolution on the same design) and resources (via the use of
bit-serial multipliers). MLWeaving also enables the runtime tuning of
precision, instead of a fixed precision level during the training. We
illustrate this using a simple, dynamic precision schedule. Experimental
results show MLWeaving achieves up to16 performance improvement over
low-precision CPU implementations of first-order methods.",arxiv
http://arxiv.org/abs/1903.02219v1,2019-03-06T07:39:11Z,2019-03-06T07:39:11Z,Training in Task Space to Speed Up and Guide Reinforcement Learning,"Recent breakthroughs in the reinforcement learning (RL) community have made
significant advances towards learning and deploying policies on real world
robotic systems. However, even with the current state-of-the-art algorithms and
computational resources, these algorithms are still plagued with high sample
complexity, and thus long training times, especially for high degree of freedom
(DOF) systems. There are also concerns arising from lack of perceived stability
or robustness guarantees from emerging policies. This paper aims at mitigating
these drawbacks by: (1) modeling a complex, high DOF system with a
representative simple one, (2) making explicit use of forward and inverse
kinematics without forcing the RL algorithm to ""learn"" them on its own, and (3)
learning locomotion policies in Cartesian space instead of joint space. In this
paper these methods are applied to JPL's Robosimian, but can be readily used on
any system with a base and end effector(s). These locomotion policies can be
produced in just a few minutes, trained on a single laptop. We compare the
robustness of the resulting learned policies to those of other control methods.
An accompanying video for this paper can be found at
https://youtu.be/xDxxSw5ahnc .",arxiv
http://arxiv.org/abs/1902.08730v1,2019-02-23T03:45:31Z,2019-02-23T03:45:31Z,AliGraph: A Comprehensive Graph Neural Network Platform,"An increasing number of machine learning tasks require dealing with large
graph datasets, which capture rich and complex relationship among potentially
billions of elements. Graph Neural Network (GNN) becomes an effective way to
address the graph learning problem by converting the graph data into a low
dimensional space while keeping both the structural and property information to
the maximum extent and constructing a neural network for training and
referencing. However, it is challenging to provide an efficient graph storage
and computation capabilities to facilitate GNN training and enable development
of new GNN algorithms. In this paper, we present a comprehensive graph neural
network system, namely AliGraph, which consists of distributed graph storage,
optimized sampling operators and runtime to efficiently support not only
existing popular GNNs but also a series of in-house developed ones for
different scenarios. The system is currently deployed at Alibaba to support a
variety of business scenarios, including product recommendation and
personalized search at Alibaba's E-Commerce platform. By conducting extensive
experiments on a real-world dataset with 492.90 million vertices, 6.82 billion
edges and rich attributes, AliGraph performs an order of magnitude faster in
terms of graph building (5 minutes vs hours reported from the state-of-the-art
PowerGraph platform). At training, AliGraph runs 40%-50% faster with the novel
caching strategy and demonstrates around 12 times speed up with the improved
runtime. In addition, our in-house developed GNN models all showcase their
statistically significant superiorities in terms of both effectiveness and
efficiency (e.g., 4.12%-17.19% lift by F1 scores).",arxiv
http://arxiv.org/abs/1902.06824v2,2019-06-13T19:14:27Z,2019-02-18T22:31:09Z,"Autonomous Airline Revenue Management: A Deep Reinforcement Learning
  Approach to Seat Inventory Control and Overbooking","Revenue management can enable airline corporations to maximize the revenue
generated from each scheduled flight departing in their transportation network
by means of finding the optimal policies for differential pricing, seat
inventory control and overbooking. As different demand segments in the market
have different Willingness-To-Pay (WTP), airlines use differential pricing,
booking restrictions, and service amenities to determine different fare classes
or products targeted at each of these demand segments. Because seats are
limited for each flight, airlines also need to allocate seats for each of these
fare classes to prevent lower fare class passengers from displacing higher fare
class ones and set overbooking limits in anticipation of cancellations and
no-shows such that revenue is maximized. Previous work addresses these problems
using optimization techniques or classical Reinforcement Learning methods. This
paper focuses on the latter problem - the seat inventory control problem -
casting it as a Markov Decision Process to be able to find the optimal policy.
Multiple fare classes, concurrent continuous arrival of passengers of different
fare classes, overbooking and random cancellations that are independent of
class have been considered in the model. We have addressed this problem using
Deep Q-Learning with the goal of maximizing the reward for each flight
departure. The implementation of this technique allows us to employ large
continuous state space but also presents the potential opportunity to test on
real time airline data. To generate data and train the agent, a basic
air-travel market simulator was developed. The performance of the agent in
different simulated market scenarios was compared against theoretically optimal
solutions and was found to be nearly close to the expected optimal revenue.",arxiv
http://arxiv.org/abs/1902.05577v2,2020-03-15T13:25:50Z,2019-02-14T19:43:10Z,"A Scalable Platform for Distributed Object Tracking across a Many-camera
  Network","Advances in deep neural networks (DNN) and computer vision (CV) algorithms
have made it feasible to extract meaningful insights from large-scale
deployments of urban cameras. Tracking an object of interest across the camera
network in near real-time is a canonical problem. However, current tracking
platforms have two key limitations: 1) They are monolithic, proprietary and
lack the ability to rapidly incorporate sophisticated tracking models; and 2)
They are less responsive to dynamism across wide-area computing resources that
include edge, fog and cloud abstractions. We address these gaps using Anveshak,
a runtime platform for composing and coordinating distributed tracking
applications. It provides a domain-specific dataflow programming model to
intuitively compose a tracking application, supporting contemporary CV advances
like query fusion and re-identification, and enabling dynamic scoping of the
camera network's search space to avoid wasted computation. We also offer
tunable batching and data-dropping strategies for dataflow blocks deployed on
distributed resources to respond to network and compute variability. These
balance the tracking accuracy, its real-time performance and the active
camera-set size. We illustrate the concise expressiveness of the programming
model for $4$ tracking applications. Our detailed experiments for a network of
1000 camera-feeds on modest resources exhibit the tunable scalability,
performance and quality trade-offs enabled by our dynamic tracking, batching
and dropping strategies.",arxiv
http://arxiv.org/abs/1902.05009v1,2019-02-13T17:03:33Z,2019-02-13T17:03:33Z,"ATMSeer: Increasing Transparency and Controllability in Automated
  Machine Learning","To relieve the pain of manually selecting machine learning algorithms and
tuning hyperparameters, automated machine learning (AutoML) methods have been
developed to automatically search for good models. Due to the huge model search
space, it is impossible to try all models. Users tend to distrust automatic
results and increase the search budget as much as they can, thereby undermining
the efficiency of AutoML. To address these issues, we design and implement
ATMSeer, an interactive visualization tool that supports users in refining the
search space of AutoML and analyzing the results. To guide the design of
ATMSeer, we derive a workflow of using AutoML based on interviews with machine
learning experts. A multi-granularity visualization is proposed to enable users
to monitor the AutoML process, analyze the searched models, and refine the
search space in real time. We demonstrate the utility and usability of ATMSeer
through two case studies, expert interviews, and a user study with 13 end
users.",arxiv
http://arxiv.org/abs/1902.02236v1,2019-02-06T15:38:06Z,2019-02-06T15:38:06Z,Dynamic Pricing for Airline Ancillaries with Customer Context,"Ancillaries have become a major source of revenue and profitability in the
travel industry. Yet, conventional pricing strategies are based on business
rules that are poorly optimized and do not respond to changing market
conditions. This paper describes the dynamic pricing model developed by Deepair
solutions, an AI technology provider for travel suppliers. We present a pricing
model that provides dynamic pricing recommendations specific to each customer
interaction and optimizes expected revenue per customer. The unique nature of
personalized pricing provides the opportunity to search over the market space
to find the optimal price-point of each ancillary for each customer, without
violating customer privacy. In this paper, we present and compare three
approaches for dynamic pricing of ancillaries, with increasing levels of
sophistication: (1) a two-stage forecasting and optimization model using a
logistic mapping function; (2) a two-stage model that uses a deep neural
network for forecasting, coupled with a revenue maximization technique using
discrete exhaustive search; (3) a single-stage end-to-end deep neural network
that recommends the optimal price. We describe the performance of these models
based on both offline and online evaluations. We also measure the real-world
business impact of these approaches by deploying them in an A/B test on an
airline's internet booking website. We show that traditional machine learning
techniques outperform human rule-based approaches in an online setting by
improving conversion by 36% and revenue per offer by 10%. We also provide
results for our offline experiments which show that deep learning algorithms
outperform traditional machine learning techniques for this problem. Our
end-to-end deep learning model is currently being deployed by the airline in
their booking system.",arxiv
http://arxiv.org/abs/1902.01084v2,2021-01-15T20:47:41Z,2019-02-04T08:51:50Z,Paracosm: A Language and Tool for Testing Autonomous Driving Systems,"Systematic testing of autonomous vehicles operating in complex real-world
scenarios is a difficult and expensive problem. We present Paracosm, a reactive
language for writing test scenarios for autonomous driving systems. Paracosm
allows users to programmatically describe complex driving situations with
specific visual features, e.g., road layout in an urban environment, as well as
reactive temporal behaviors of cars and pedestrians. Paracosm programs are
executed on top of a game engine that provides realistic physics simulation and
visual rendering. The infrastructure allows systematic exploration of the state
space, both for visual features (lighting, shadows, fog) and for reactive
interactions with the environment (pedestrians, other traffic). We define a
notion of test coverage for Paracosm configurations based on combinatorial
testing and low dispersion sequences. Paracosm comes with an automatic test
case generator that uses random sampling for discrete parameters and
deterministic quasi-Monte Carlo generation for continuous parameters. Through
an empirical evaluation, we demonstrate the modeling and testing capabilities
of Paracosm on a suite of autonomous driving systems implemented using deep
neural networks developed in research and education. We show how Paracosm can
expose incorrect behaviors or degraded performance.",arxiv
http://arxiv.org/abs/1901.11422v1,2019-01-31T15:20:43Z,2019-01-31T15:20:43Z,"High-dimensional Metric Combining for Non-coherent Molecular Signal
  Detection","In emerging Internet-of-Nano-Thing (IoNT), information will be embedded and
conveyed in the form of molecules through complex and diffusive medias. One
main challenge lies in the long-tail nature of the channel response causing
inter-symbol-interference (ISI), which deteriorates the detection performance.
If the channel is unknown, we cannot easily achieve traditional coherent
channel estimation and cancellation, and the impact of ISI will be more severe.
In this paper, we develop a novel high-dimensional non-coherent scheme for
blind detection of molecular signals. We achieve this in a higher-dimensional
metric space by combining different non-coherent metrics that exploit the
transient features of the signals. By deducing the theoretical bit error rate
(BER) for any constructed high-dimensional non-coherent metric, we prove that,
higher dimensionality always achieves a lower BER in the same sample space.
Then, we design a generalised blind detection algorithm that utilizes the
Parzen approximation and its probabilistic neural network (Parzen-PNN) to
detect information bits. Taking advantages of its fast convergence and parallel
implementation, our proposed scheme can meet the needs of detection accuracy
and real-time computing. Numerical simulations demonstrate that our proposed
scheme can gain 10dB BER compared with other state of the art methods.",arxiv
http://arxiv.org/abs/1901.10281v1,2019-01-29T13:43:57Z,2019-01-29T13:43:57Z,Structural Material Property Tailoring Using Deep Neural Networks,"Advances in robotics, artificial intelligence, and machine learning are
ushering in a new age of automation, as machines match or outperform human
performance. Machine intelligence can enable businesses to improve performance
by reducing errors, improving sensitivity, quality and speed, and in some cases
achieving outcomes that go beyond current resource capabilities. Relevant
applications include new product architecture design, rapid material
characterization, and life-cycle management tied with a digital strategy that
will enable efficient development of products from cradle to grave. In
addition, there are also challenges to overcome that must be addressed through
a major, sustained research effort that is based solidly on both inferential
and computational principles applied to design tailoring of functionally
optimized structures. Current applications of structural materials in the
aerospace industry demand the highest quality control of material
microstructure, especially for advanced rotational turbomachinery in aircraft
engines in order to have the best tailored material property. In this paper,
deep convolutional neural networks were developed to accurately predict
processing-structure-property relations from materials microstructures images,
surpassing current best practices and modeling efforts. The models
automatically learn critical features, without the need for manual
specification and/or subjective and expensive image analysis. Further, in
combination with generative deep learning models, a framework is proposed to
enable rapid material design space exploration and property identification and
optimization. The implementation must take account of real-time decision cycles
and the trade-offs between speed and accuracy.",arxiv
http://arxiv.org/abs/1901.09049v2,2019-02-21T17:15:18Z,2019-01-25T19:07:36Z,"Biologically inspired alternatives to backpropagation through time for
  learning in recurrent neural nets","The way how recurrently connected networks of spiking neurons in the brain
acquire powerful information processing capabilities through learning has
remained a mystery. This lack of understanding is linked to a lack of learning
algorithms for recurrent networks of spiking neurons (RSNNs) that are both
functionally powerful and can be implemented by known biological mechanisms.
Since RSNNs are simultaneously a primary target for implementations of
brain-inspired circuits in neuromorphic hardware, this lack of algorithmic
insight also hinders technological progress in that area. The gold standard for
learning in recurrent neural networks in machine learning is back-propagation
through time (BPTT), which implements stochastic gradient descent with regard
to a given loss function. But BPTT is unrealistic from a biological
perspective, since it requires a transmission of error signals backwards in
time and in space, i.e., from post- to presynaptic neurons. We show that an
online merging of locally available information during a computation with
suitable top-down learning signals in real-time provides highly capable
approximations to BPTT. For tasks where information on errors arises only late
during a network computation, we enrich locally available information through
feedforward eligibility traces of synapses that can easily be computed in an
online manner. The resulting new generation of learning algorithms for
recurrent neural networks provides a new understanding of network learning in
the brain that can be tested experimentally. In addition, these algorithms
provide efficient methods for on-chip training of RSNNs in neuromorphic
hardware.",arxiv
http://arxiv.org/abs/1901.08954v1,2019-01-25T16:18:22Z,2019-01-25T16:18:22Z,"Skip-GANomaly: Skip Connected and Adversarially Trained Encoder-Decoder
  Anomaly Detection","Despite inherent ill-definition, anomaly detection is a research endeavor of
great interest within machine learning and visual scene understanding alike.
Most commonly, anomaly detection is considered as the detection of outliers
within a given data distribution based on some measure of normality. The most
significant challenge in real-world anomaly detection problems is that
available data is highly imbalanced towards normality (i.e. non-anomalous) and
contains a most a subset of all possible anomalous samples - hence limiting the
use of well-established supervised learning methods. By contrast, we introduce
an unsupervised anomaly detection model, trained only on the normal
(non-anomalous, plentiful) samples in order to learn the normality distribution
of the domain and hence detect abnormality based on deviation from this model.
Our proposed approach employs an encoder-decoder convolutional neural network
with skip connections to thoroughly capture the multi-scale distribution of the
normal data distribution in high-dimensional image space. Furthermore,
utilizing an adversarial training scheme for this chosen architecture provides
superior reconstruction both within high-dimensional image space and a
lower-dimensional latent vector space encoding. Minimizing the reconstruction
error metric within both the image and hidden vector spaces during training
aids the model to learn the distribution of normality as required. Higher
reconstruction metrics during subsequent test and deployment are thus
indicative of a deviation from this normal distribution, hence indicative of an
anomaly. Experimentation over established anomaly detection benchmarks and
challenging real-world datasets, within the context of X-ray security
screening, shows the unique promise of such a proposed approach.",arxiv
http://arxiv.org/abs/1901.08548v1,2019-01-20T18:51:30Z,2019-01-20T18:51:30Z,A tensorized logic programming language for large-scale data,"We introduce a new logic programming language T-PRISM based on tensor
embeddings. Our embedding scheme is a modification of the distribution
semantics in PRISM, one of the state-of-the-art probabilistic logic programming
languages, by replacing distribution functions with multidimensional arrays,
i.e., tensors. T-PRISM consists of two parts: logic programming part and
numerical computation part. The former provides flexible and interpretable
modeling at the level of first order logic, and the latter part provides
scalable computation utilizing parallelization and hardware acceleration with
GPUs. Combing these two parts provides a remarkably wide range of high-level
declarative modeling from symbolic reasoning to deep learning. To embody this
programming language, we also introduce a new semantics, termed tensorized
semantics, which combines the traditional least model semantics in logic
programming with the embeddings of tensors. In T-PRISM, we first derive a set
of equations related to tensors from a given program using logical inference,
i.e., Prolog execution in a symbolic space and then solve the derived equations
in a continuous space by TensorFlow. Using our preliminary implementation of
T-PRISM, we have successfully dealt with a wide range of modeling. We have
succeeded in dealing with real large-scale data in the declarative modeling.
This paper presents a DistMult model for knowledge graphs using the FB15k and
WN18 datasets.",arxiv
http://arxiv.org/abs/1901.05356v1,2019-01-16T15:56:19Z,2019-01-16T15:56:19Z,"How to Host a Data Competition: Statistical Advice for Design and
  Analysis of a Data Competition","Data competitions rely on real-time leaderboards to rank competitor entries
and stimulate algorithm improvement. While such competitions have become quite
popular and prevalent, particularly in supervised learning formats, their
implementations by the host are highly variable. Without careful planning, a
supervised learning competition is vulnerable to overfitting, where the winning
solutions are so closely tuned to the particular set of provided data that they
cannot generalize to the underlying problem of interest to the host. This paper
outlines some important considerations for strategically designing relevant and
informative data sets to maximize the learning outcome from hosting a
competition based on our experience. It also describes a post-competition
analysis that enables robust and efficient assessment of the strengths and
weaknesses of solutions from different competitors, as well as greater
understanding of the regions of the input space that are well-solved. The
post-competition analysis, which complements the leaderboard, uses exploratory
data analysis and generalized linear models (GLMs). The GLMs not only expand
the range of results we can explore, they also provide more detailed analysis
of individual sub-questions including similarities and differences between
algorithms across different types of scenarios, universally easy or hard
regions of the input space, and different learning objectives. When coupled
with a strategically planned data generation approach, the methods provide
richer and more informative summaries to enhance the interpretation of results
beyond just the rankings on the leaderboard. The methods are illustrated with a
recently completed competition to evaluate algorithms capable of detecting,
identifying, and locating radioactive materials in an urban environment.",arxiv
http://arxiv.org/abs/1901.01632v1,2019-01-07T00:31:25Z,2019-01-07T00:31:25Z,ATP: a Datacenter Approximate Transmission Protocol,"Many datacenter applications such as machine learning and streaming systems
do not need the complete set of data to perform their computation. Current
approximate applications in datacenters run on a reliable network layer like
TCP. To improve performance, they either let sender select a subset of data and
transmit them to the receiver or transmit all the data and let receiver drop
some of them. These approaches are network oblivious and unnecessarily transmit
more data, affecting both application runtime and network bandwidth usage. On
the other hand, running approximate application on a lossy network with UDP
cannot guarantee the accuracy of application computation. We propose to run
approximate applications on a lossy network and to allow packet loss in a
controlled manner. Specifically, we designed a new network protocol called
Approximate Transmission Protocol, or ATP, for datacenter approximate
applications. ATP opportunistically exploits available network bandwidth as
much as possible, while performing a loss-based rate control algorithm to avoid
bandwidth waste and re-transmission. It also ensures bandwidth fair sharing
across flows and improves accurate applications' performance by leaving more
switch buffer space to accurate flows. We evaluated ATP with both simulation
and real implementation using two macro-benchmarks and two real applications,
Apache Kafka and Flink. Our evaluation results show that ATP reduces
application runtime by 13.9% to 74.6% compared to a TCP-based solution that
drops packets at sender, and it improves accuracy by up to 94.0% compared to
UDP.",arxiv
http://arxiv.org/abs/1812.07221v1,2018-12-18T08:11:00Z,2018-12-18T08:11:00Z,"Continuous Trajectory Planning Based on Learning Optimization in High
  Dimensional Input Space for Serial Manipulators","To continuously generate trajectories for serial manipulators with high
dimensional degrees of freedom (DOF) in the dynamic environment, a real-time
optimal trajectory generation method based on machine learning aiming at high
dimensional inputs is presented in this paper. First, a learning optimization
(LO) framework is established, and implementations with different sub-methods
are discussed. Additionally, multiple criteria are defined to evaluate the
performance of LO models. Furthermore, aiming at high dimensional inputs, a
database generation method based on input space dimension-reducing mapping is
proposed. At last, this method is validated on motion planning for haptic
feedback manipulators (HFM) in virtual reality systems. Results show that the
input space dimension-reducing method can significantly elevate the efficiency
and quality of database generation and consequently improve the performance of
the LO. Moreover, using this LO method, real-time trajectory generation with
high dimensional inputs can be achieved, which lays a foundation for continuous
trajectory planning for high-DOF-robots in complex environments.",arxiv
http://arxiv.org/abs/1812.01885v1,2018-12-05T10:02:59Z,2018-12-05T10:02:59Z,"Improving Medical Short Text Classification with Semantic Expansion
  Using Word-Cluster Embedding","Automatic text classification (TC) research can be used for real-world
problems such as the classification of in-patient discharge summaries and
medical text reports, which is beneficial to make medical documents more
understandable to doctors. However, in electronic medical records (EMR), the
texts containing sentences are shorter than that in general domain, which leads
to the lack of semantic features and the ambiguity of semantic. To tackle this
challenge, we propose to add word-cluster embedding to deep neural network for
improving short text classification. Concretely, we first use hierarchical
agglomerative clustering to cluster the word vectors in the semantic space.
Then we calculate the cluster center vector which represents the implicit topic
information of words in the cluster. Finally, we expand word vector with
cluster center vector, and implement classifiers using CNN and LSTM
respectively. To evaluate the performance of our proposed method, we conduct
experiments on public data sets TREC and the medical short sentences data sets
which is constructed and released by us. The experimental results demonstrate
that our proposed method outperforms state-of-the-art baselines in short
sentence classification on both medical domain and general domain.",arxiv
http://arxiv.org/abs/1811.12692v1,2018-11-30T09:59:20Z,2018-11-30T09:59:20Z,On the generation of probabilistic forecasts from deterministic models,"Most of the methods that produce space weather forecasts are based on
deterministic models. In order to generate a probabilistic forecast, a model
needs to be run several times sampling the input parameter space, in order to
generate an ensemble from which the distribution of outputs can be inferred.
However, ensemble simulations are costly and often preclude the possibility of
real-time forecasting. We introduce a simple and robust method to generate
uncertainties from deterministic models, that does not require ensemble
simulations. The method is based on the simple consideration that a
probabilistic forecast needs to be both accurate and well-calibrated
(reliable). We argue that these two requirements are equally important, and we
introduce the Accuracy-Reliability cost function that quantitatively measures
the trade-off between accuracy and reliability. We then define the optimal
uncertainties as the standard deviation of the Gaussian distribution that
minimizes the cost function. We demonstrate that this simple strategy,
implemented here by means of a regularized deep neural network, produces
accurate and well-calibrated forecasts, showing examples both on synthetic and
real-world space weather data.",arxiv
http://arxiv.org/abs/1811.12425v1,2018-11-29T19:00:08Z,2018-11-29T19:00:08Z,Classifying Snapshots of the Doped Hubbard Model with Machine Learning,"Quantum gas microscopes for ultracold atoms can provide high-resolution
real-space snapshots of complex many-body systems. We implement machine
learning to analyze and classify such snapshots of ultracold atoms.
Specifically, we compare the data from an experimental realization of the
two-dimensional Fermi-Hubbard model to two theoretical approaches: a doped
quantum spin liquid state of resonating valence bond type, and the geometric
string theory, describing a state with hidden spin order. This approach
considers all available information without a potential bias towards one
particular theory by the choice of an observable and can therefore select the
theory which is more predictive in general. Up to intermediate doping values,
our algorithm tends to classify experimental snapshots as
geometric-string-like, as compared to the doped spin liquid. Our results
demonstrate the potential for machine learning in processing the wealth of data
obtained through quantum gas microscopy for new physical insights.",arxiv
http://arxiv.org/abs/1811.11233v1,2018-11-27T20:08:28Z,2018-11-27T20:08:28Z,"Distributed traffic light control at uncoupled intersections with
  real-world topology by deep reinforcement learning","This work examines the implications of uncoupled intersections with local
real-world topology and sensor setup on traffic light control approaches.
Control approaches are evaluated with respect to: Traffic flow, fuel
consumption and noise emission at intersections.
  The real-world road network of Friedrichshafen is depicted, preprocessed and
the present traffic light controlled intersections are modeled with respect to
state space and action space.
  Different strategies, containing fixed-time, gap-based and time-based control
approaches as well as our deep reinforcement learning based control approach,
are implemented and assessed. Our novel DRL approach allows for modeling the
TLC action space, with respect to phase selection as well as selection of
transition timings. It was found that real-world topologies, and thus
irregularly arranged intersections have an influence on the performance of
traffic light control approaches. This is even to be observed within the same
intersection types (n-arm, m-phases). Moreover we could show, that these
influences can be efficiently dealt with by our deep reinforcement learning
based control approach.",arxiv
http://arxiv.org/abs/1811.08955v1,2018-11-21T21:20:24Z,2018-11-21T21:20:24Z,"Integrating Task-Motion Planning with Reinforcement Learning for Robust
  Decision Making in Mobile Robots","Task-motion planning (TMP) addresses the problem of efficiently generating
executable and low-cost task plans in a discrete space such that the (initially
unknown) action costs are determined by motion plans in a corresponding
continuous space. However, a task-motion plan can be sensitive to unexpected
domain uncertainty and changes, leading to suboptimal behaviors or execution
failures. In this paper, we propose a novel framework, TMP-RL, which is an
integration of TMP and reinforcement learning (RL) from the execution
experience, to solve the problem of robust task-motion planning in dynamic and
uncertain domains. TMP-RL features two nested planning-learning loops. In the
inner TMP loop, the robot generates a low-cost, feasible task-motion plan by
iteratively planning in the discrete space and updating relevant action costs
evaluated by the motion planner in continuous space. In the outer loop, the
plan is executed, and the robot learns from the execution experience via
model-free RL, to further improve its task-motion plans. RL in the outer loop
is more accurate to the current domain but also more expensive, and using less
costly task and motion planning leads to a jump-start for learning in the real
world. Our approach is evaluated on a mobile service robot conducting
navigation tasks in an office area. Results show that TMP-RL approach
significantly improves adaptability and robustness (in comparison to TMP
methods) and leads to rapid convergence (in comparison to task planning (TP)-RL
methods). We also show that TMP-RL can reuse learned values to smoothly adapt
to new scenarios during long-term deployments.",arxiv
http://arxiv.org/abs/1811.08716v1,2018-11-21T13:19:28Z,2018-11-21T13:19:28Z,Autonomous Dual-Arm Manipulation of Familiar Objects,"Autonomous dual-arm manipulation is an essential skill to deploy robots in
unstructured scenarios. However, this is a challenging undertaking,
particularly in terms of perception and planning. Unstructured scenarios are
full of objects with different shapes and appearances that have to be grasped
in a very specific manner so they can be functionally used. In this paper we
present an integrated approach to perform dual-arm pick tasks autonomously. Our
method consists of semantic segmentation, object pose estimation, deformable
model registration, grasp planning and arm trajectory optimization. The entire
pipeline can be executed on-board and is suitable for on-line grasping
scenarios. For this, our approach makes use of accumulated knowledge expressed
as convolutional neural network models and low-dimensional latent shape spaces.
For manipulating objects, we propose a stochastic trajectory optimization that
includes a kinematic chain closure constraint. Evaluation in simulation and on
the real robot corroborates the feasibility and applicability of the proposed
methods on a task of picking up unknown watering cans and drills using both
arms.",arxiv
http://arxiv.org/abs/1811.07315v1,2018-11-18T11:28:24Z,2018-11-18T11:28:24Z,"Learning to infer: RL-based search for DNN primitive selection on
  Heterogeneous Embedded Systems","Deep Learning is increasingly being adopted by industry for computer vision
applications running on embedded devices. While Convolutional Neural Networks'
accuracy has achieved a mature and remarkable state, inference latency and
throughput are a major concern especially when targeting low-cost and low-power
embedded platforms. CNNs' inference latency may become a bottleneck for Deep
Learning adoption by industry, as it is a crucial specification for many
real-time processes. Furthermore, deployment of CNNs across heterogeneous
platforms presents major compatibility issues due to vendor-specific technology
and acceleration libraries. In this work, we present QS-DNN, a fully automatic
search based on Reinforcement Learning which, combined with an inference engine
optimizer, efficiently explores through the design space and empirically finds
the optimal combinations of libraries and primitives to speed up the inference
of CNNs on heterogeneous embedded devices. We show that, an optimized
combination can achieve 45x speedup in inference latency on CPU compared to a
dependency-free baseline and 2x on average on GPGPU compared to the best vendor
library. Further, we demonstrate that, the quality of results and time
""to-solution"" is much better than with Random Search and achieves up to 15x
better results for a short-time search.",arxiv
http://arxiv.org/abs/1811.07234v1,2018-11-17T22:21:12Z,2018-11-17T22:21:12Z,"Improving Automatic Source Code Summarization via Deep Reinforcement
  Learning","Code summarization provides a high level natural language description of the
function performed by code, as it can benefit the software maintenance, code
categorization and retrieval. To the best of our knowledge, most
state-of-the-art approaches follow an encoder-decoder framework which encodes
the code into a hidden space and then decode it into natural language space,
suffering from two major drawbacks: a) Their encoders only consider the
sequential content of code, ignoring the tree structure which is also critical
for the task of code summarization, b) Their decoders are typically trained to
predict the next word by maximizing the likelihood of next ground-truth word
with previous ground-truth word given. However, it is expected to generate the
entire sequence from scratch at test time. This discrepancy can cause an
\textit{exposure bias} issue, making the learnt decoder suboptimal. In this
paper, we incorporate an abstract syntax tree structure as well as sequential
content of code snippets into a deep reinforcement learning framework (i.e.,
actor-critic network). The actor network provides the confidence of predicting
the next word according to current state. On the other hand, the critic network
evaluates the reward value of all possible extensions of the current state and
can provide global guidance for explorations. We employ an advantage reward
composed of BLEU metric to train both networks. Comprehensive experiments on a
real-world dataset show the effectiveness of our proposed model when compared
with some state-of-the-art methods.",arxiv
http://arxiv.org/abs/1811.06885v2,2019-03-25T07:05:04Z,2018-11-16T16:07:23Z,"A Generalized Meta-loss function for regression and classification using
  privileged information","Learning using privileged information (LUPI) is a powerful heterogenous
feature space machine learning framework that allows a machine learning model
to learn from highly informative or privileged features which are available
during training only to generate test predictions using input space features
which are available both during training and testing. LUPI can significantly
improve prediction performance in a variety of machine learning problems.
However, existing large margin and neural network implementations of learning
using privileged information are mostly designed for classification tasks. In
this work, we have proposed a simple yet effective formulation that allows us
to perform regression using privileged information through a custom loss
function. Apart from regression, our formulation allows general application of
LUPI to classification and other related problems as well. We have verified the
correctness, applicability and effectiveness of our method on regression and
classification problems over different synthetic and real-world problems. To
test the usefulness of the proposed model in real-world problems, we have
evaluated our method on the problem of protein binding affinity prediction. The
proposed LUPI regression-based model has shown to outperform the current
state-of-the-art predictor.",arxiv
http://arxiv.org/abs/1811.03934v1,2018-11-09T14:46:40Z,2018-11-09T14:46:40Z,"RadIoT: Radio Communications Intrusion Detection for IoT - A Protocol
  Independent Approach","Internet-of-Things (IoT) devices are nowadays massively integrated in daily
life: homes, factories, or public places. This technology offers attractive
services to improve the quality of life as well as new economic markets through
the exploitation of the collected data. However, these connected objects have
also become attractive targets for attackers because their current security
design is often weak or flawed, as illustrated by several vulnerabilities such
as Mirai, Blueborne, etc. This paper presents a novel approach for detecting
intrusions in smart spaces such as smarthomes, or smartfactories, that is based
on the monitoring and profiling of radio communications at the physical layer
using machine learning techniques. The approach is designed to be independent
of the large and heterogeneous set of wireless communication protocols
typically implemented by connected objects such as WiFi, Bluetooth, Zigbee,
Bluetooth-Low-Energy (BLE) or proprietary communication protocols. The main
concepts of the proposed approach are presented together with an experimental
case study illustrating its feasibility based on data collected during the
deployment of the intrusion detection approach in a smart home under real-life
conditions.",arxiv
http://arxiv.org/abs/1811.03909v2,2018-12-15T19:39:04Z,2018-11-09T14:10:18Z,"Evidence Transfer for Improving Clustering Tasks Using External
  Categorical Evidence","In this paper we introduce evidence transfer for clustering, a deep learning
method that can incrementally manipulate the latent representations of an
autoencoder, according to external categorical evidence, in order to improve a
clustering outcome. By evidence transfer we define the process by which the
categorical outcome of an external, auxiliary task is exploited to improve a
primary task, in this case representation learning for clustering. Our proposed
method makes no assumptions regarding the categorical evidence presented, nor
the structure of the latent space. We compare our method, against the baseline
solution by performing k-means clustering before and after its deployment.
Experiments with three different kinds of evidence show that our method
effectively manipulates the latent representations when introduced with real
corresponding evidence, while remaining robust when presented with low quality
evidence.",arxiv
http://arxiv.org/abs/1811.02872v2,2018-11-12T19:23:15Z,2018-11-07T13:30:40Z,Baselines for Reinforcement Learning in Text Games,"The ability to learn optimal control policies in systems where action space
is defined by sentences in natural language would allow many interesting
real-world applications such as automatic optimisation of dialogue systems.
Text-based games with multiple endings and rewards are a promising platform for
this task, since their feedback allows us to employ reinforcement learning
techniques to jointly learn text representations and control policies. We argue
that the key property of AI agents, especially in the text-games context, is
their ability to generalise to previously unseen games. We present a
minimalistic text-game playing agent, testing its generalisation and transfer
learning performance and showing its ability to play multiple games at once. We
also present pyfiction, an open-source library for universal access to
different text games that could, together with our agent that implements its
interface, serve as a baseline for future research.",arxiv
http://arxiv.org/abs/1810.10335v1,2018-10-24T12:27:33Z,2018-10-24T12:27:33Z,Emulating quantum computation with artificial neural networks,"We demonstrate, that artificial neural networks (ANN) can be trained to
emulate single or multiple basic quantum operations. In order to realize a
quantum state, we implement a novel ""quantumness gate"" that maps an arbitrary
matrix to the real representation of a positive hermitean normalized density
matrix. We train the CNOT gate, the Hadamard gate and a rotation in Hilbert
space as basic building blocks for processing the quantum density matrices of
two entangled qubits. During the training process the neural networks learn to
represent the complex structure, the hermiticity, the normalization and the
positivity of the output matrix. The requirement of successful training allows
us to find a critical bottleneck dimension which reflects the relevant quantum
information. Chains of individually trained neural quantum gates can be
constructed to realize any unitary transformation. For scaling to larger
quantum systems, we propose to use correlations of stochastic macroscopic
two-level observables or classical bits. This novel concept provides a path for
a classical implementation of computationally relevant quantum information
processing on classical neural networks, in particular on neuromorphic
computing machines featuring stochastic operations.",arxiv
http://arxiv.org/abs/1810.06637v2,2019-05-02T15:03:20Z,2018-10-15T19:46:00Z,"Nonlinear System Identification of Soft Robot Dynamics Using Koopman
  Operator Theory","Soft robots are challenging to model due in large part to the nonlinear
properties of soft materials. Fortunately, this softness makes it possible to
safely observe their behavior under random control inputs, making them amenable
to large-scale data collection and system identification. This paper implements
and evaluates a system identification method based on Koopman operator theory
in which models of nonlinear dynamical systems are constructed via linear
regression of observed data by exploiting the fact that every nonlinear system
has a linear representation in the infinite-dimensional space of real-valued
functions called observables. The approach does not suffer from some of the
shortcomings of other nonlinear system identification methods, which typically
require the manual tuning of training parameters and have limited convergence
guarantees. A dynamic model of a pneumatic soft robot arm is constructed via
this method, and used to predict the behavior of the real system. The total
normalized-root-mean-square error (NRMSE) of its predictions is lower than that
of several other identified models including a neural network, NLARX, nonlinear
Hammerstein-Wiener, and linear state space model.",arxiv
http://arxiv.org/abs/1810.05236v3,2019-07-24T22:33:56Z,2018-10-11T20:23:57Z,Practical Design Space Exploration,"Multi-objective optimization is a crucial matter in computer systems design
space exploration because real-world applications often rely on a trade-off
between several objectives. Derivatives are usually not available or
impractical to compute and the feasibility of an experiment can not always be
determined in advance. These problems are particularly difficult when the
feasible region is relatively small, and it may be prohibitive to even find a
feasible experiment, let alone an optimal one.
  We introduce a new methodology and corresponding software framework,
HyperMapper 2.0, which handles multi-objective optimization, unknown
feasibility constraints, and categorical/ordinal variables. This new
methodology also supports injection of the user prior knowledge in the search
when available. All of these features are common requirements in computer
systems but rarely exposed in existing design space exploration systems. The
proposed methodology follows a white-box model which is simple to understand
and interpret (unlike, for example, neural networks) and can be used by the
user to better understand the results of the automatic search.
  We apply and evaluate the new methodology to the automatic static tuning of
hardware accelerators within the recently introduced Spatial programming
language, with minimization of design run-time and compute logic under the
constraint of the design fitting in a target field-programmable gate array
chip. Our results show that HyperMapper 2.0 provides better Pareto fronts
compared to state-of-the-art baselines, with better or competitive hypervolume
indicator and with 8x improvement in sampling budget for most of the benchmarks
explored.",arxiv
http://arxiv.org/abs/1810.04793v3,2018-10-25T13:38:34Z,2018-10-10T16:41:05Z,"Patient2Vec: A Personalized Interpretable Deep Representation of the
  Longitudinal Electronic Health Record","The wide implementation of electronic health record (EHR) systems facilitates
the collection of large-scale health data from real clinical settings. Despite
the significant increase in adoption of EHR systems, this data remains largely
unexplored, but presents a rich data source for knowledge discovery from
patient health histories in tasks such as understanding disease correlations
and predicting health outcomes. However, the heterogeneity, sparsity, noise,
and bias in this data present many complex challenges. This complexity makes it
difficult to translate potentially relevant information into machine learning
algorithms. In this paper, we propose a computational framework, Patient2Vec,
to learn an interpretable deep representation of longitudinal EHR data which is
personalized for each patient. To evaluate this approach, we apply it to the
prediction of future hospitalizations using real EHR data and compare its
predictive performance with baseline methods. Patient2Vec produces a vector
space with meaningful structure and it achieves an AUC around 0.799
outperforming baseline methods. In the end, the learned feature importance can
be visualized and interpreted at both the individual and population levels to
bring clinical insights.",arxiv
http://arxiv.org/abs/1810.02648v3,2019-01-25T15:37:49Z,2018-10-05T12:39:37Z,LiveCap: Real-time Human Performance Capture from Monocular Video,"We present the first real-time human performance capture approach that
reconstructs dense, space-time coherent deforming geometry of entire humans in
general everyday clothing from just a single RGB video. We propose a novel
two-stage analysis-by-synthesis optimization whose formulation and
implementation are designed for high performance. In the first stage, a skinned
template model is jointly fitted to background subtracted input video, 2D and
3D skeleton joint positions found using a deep neural network, and a set of
sparse facial landmark detections. In the second stage, dense non-rigid 3D
deformations of skin and even loose apparel are captured based on a novel
real-time capable algorithm for non-rigid tracking using dense photometric and
silhouette constraints. Our novel energy formulation leverages automatically
identified material regions on the template to model the differing non-rigid
deformation behavior of skin and apparel. The two resulting non-linear
optimization problems per-frame are solved with specially-tailored
data-parallel Gauss-Newton solvers. In order to achieve real-time performance
of over 25Hz, we design a pipelined parallel architecture using the CPU and two
commodity GPUs. Our method is the first real-time monocular approach for
full-body performance capture. Our method yields comparable accuracy with
off-line performance capture techniques, while being orders of magnitude
faster.",arxiv
http://arxiv.org/abs/1809.03548v2,2018-09-14T11:39:07Z,2018-09-10T18:55:19Z,VPE: Variational Policy Embedding for Transfer Reinforcement Learning,"Reinforcement Learning methods are capable of solving complex problems, but
resulting policies might perform poorly in environments that are even slightly
different. In robotics especially, training and deployment conditions often
vary and data collection is expensive, making retraining undesirable.
Simulation training allows for feasible training times, but on the other hand
suffers from a reality-gap when applied in real-world settings. This raises the
need of efficient adaptation of policies acting in new environments. We
consider this as a problem of transferring knowledge within a family of similar
Markov decision processes.
  For this purpose we assume that Q-functions are generated by some
low-dimensional latent variable. Given such a Q-function, we can find a master
policy that can adapt given different values of this latent variable. Our
method learns both the generative mapping and an approximate posterior of the
latent variables, enabling identification of policies for new tasks by
searching only in the latent space, rather than the space of all policies. The
low-dimensional space, and master policy found by our method enables policies
to quickly adapt to new environments. We demonstrate the method on both a
pendulum swing-up task in simulation, and for simulation-to-real transfer on a
pushing task.",arxiv
http://arxiv.org/abs/1809.02797v2,2018-09-15T08:31:50Z,2018-09-08T13:08:26Z,Fast Gradient Attack on Network Embedding,"Network embedding maps a network into a low-dimensional Euclidean space, and
thus facilitate many network analysis tasks, such as node classification, link
prediction and community detection etc, by utilizing machine learning methods.
In social networks, we may pay special attention to user privacy, and would
like to prevent some target nodes from being identified by such network
analysis methods in certain cases. Inspired by successful adversarial attack on
deep learning models, we propose a framework to generate adversarial networks
based on the gradient information in Graph Convolutional Network (GCN). In
particular, we extract the gradient of pairwise nodes based on the adversarial
network, and select the pair of nodes with maximum absolute gradient to realize
the Fast Gradient Attack (FGA) and update the adversarial network. This process
is implemented iteratively and terminated until certain condition is satisfied,
i.e., the number of modified links reaches certain predefined value.
Comprehensive attacks, including unlimited attack, direct attack and indirect
attack, are performed on six well-known network embedding methods. The
experiments on real-world networks suggest that our proposed FGA behaves better
than some baseline methods, i.e., the network embedding can be easily disturbed
using FGA by only rewiring few links, achieving state-of-the-art attack
performance.",arxiv
http://arxiv.org/abs/1809.05193v1,2018-08-31T20:52:10Z,2018-08-31T20:52:10Z,"Context2Name: A Deep Learning-Based Approach to Infer Natural Variable
  Names from Usage Contexts","Most of the JavaScript code deployed in the wild has been minified, a process
in which identifier names are replaced with short, arbitrary and meaningless
names. Minified code occupies less space, but also makes the code extremely
difficult to manually inspect and understand. This paper presents Context2Name,
a deep learningbased technique that partially reverses the effect of
minification by predicting natural identifier names for minified names. The
core idea is to predict from the usage context of a variable a name that
captures the meaning of the variable. The approach combines a lightweight,
token-based static analysis with an auto-encoder neural network that summarizes
usage contexts and a recurrent neural network that predict natural names for a
given usage context. We evaluate Context2Name with a large corpus of real-world
JavaScript code and show that it successfully predicts 47.5% of all minified
identifiers while taking only 2.9 milliseconds on average to predict a name. A
comparison with the state-of-the-art tools JSNice and JSNaughty shows that our
approach performs comparably in terms of accuracy while improving in terms of
efficiency. Moreover, Context2Name complements the state-of-the-art by
predicting 5.3% additional identifiers that are missed by both existing tools.",arxiv
http://arxiv.org/abs/1808.07840v1,2018-08-23T16:55:53Z,2018-08-23T16:55:53Z,Learning to Importance Sample in Primary Sample Space,"Importance sampling is one of the most widely used variance reduction
strategies in Monte Carlo rendering. In this paper, we propose a novel
importance sampling technique that uses a neural network to learn how to sample
from a desired density represented by a set of samples. Our approach considers
an existing Monte Carlo rendering algorithm as a black box. During a
scene-dependent training phase, we learn to generate samples with a desired
density in the primary sample space of the rendering algorithm using maximum
likelihood estimation. We leverage a recent neural network architecture that
was designed to represent real-valued non-volume preserving ('Real NVP')
transformations in high dimensional spaces. We use Real NVP to non-linearly
warp primary sample space and obtain desired densities. In addition, Real NVP
efficiently computes the determinant of the Jacobian of the warp, which is
required to implement the change of integration variables implied by the warp.
A main advantage of our approach is that it is agnostic of underlying light
transport effects, and can be combined with many existing rendering techniques
by treating them as a black box. We show that our approach leads to effective
variance reduction in several practical scenarios.",arxiv
http://arxiv.org/abs/1808.06352v1,2018-08-20T09:06:21Z,2018-08-20T09:06:21Z,"Navigating the Landscape for Real-time Localisation and Mapping for
  Robotics and Virtual and Augmented Reality","Visual understanding of 3D environments in real-time, at low power, is a huge
computational challenge. Often referred to as SLAM (Simultaneous Localisation
and Mapping), it is central to applications spanning domestic and industrial
robotics, autonomous vehicles, virtual and augmented reality. This paper
describes the results of a major research effort to assemble the algorithms,
architectures, tools, and systems software needed to enable delivery of SLAM,
by supporting applications specialists in selecting and configuring the
appropriate algorithm and the appropriate hardware, and compilation pathway, to
meet their performance, accuracy, and energy consumption goals. The major
contributions we present are (1) tools and methodology for systematic
quantitative evaluation of SLAM algorithms, (2) automated,
machine-learning-guided exploration of the algorithmic and implementation
design space with respect to multiple objectives, (3) end-to-end simulation
tools to enable optimisation of heterogeneous, accelerated architectures for
the specific algorithmic requirements of the various SLAM algorithmic
approaches, and (4) tools for delivering, where appropriate, accelerated,
adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",arxiv
http://arxiv.org/abs/1808.06277v1,2018-08-20T00:54:29Z,2018-08-20T00:54:29Z,An Efficient Approach for Geo-Multimedia Cross-Modal Retrieval,"Due to the rapid development of mobile Internet techniques, cloud computation
and popularity of online social networking and location-based services, massive
amount of multimedia data with geographical information is generated and
uploaded to the Internet. In this paper, we propose a novel type of cross-modal
multimedia retrieval called geo-multimedia cross-modal retrieval which aims to
search out a set of geo-multimedia objects based on geographical distance
proximity and semantic similarity between different modalities. Previous
studies for cross-modal retrieval and spatial keyword search cannot address
this problem effectively because they do not consider multimedia data with
geo-tags and do not focus on this type of query. In order to address this
problem efficiently, we present the definition of $k$NN geo-multimedia
cross-modal query at the first time and introduce relevant conceptions such as
cross-modal semantic representation space. To bridge the semantic gap between
different modalities, we propose a method named cross-modal semantic matching
which contains two important component, i.e., CorrProj and LogsTran, which aims
to construct a common semantic representation space for cross-modal semantic
similarity measurement. Besides, we designed a framework based on deep learning
techniques to implement common semantic representation space construction. In
addition, a novel hybrid indexing structure named GMR-Tree combining
geo-multimedia data and R-Tree is presented and a efficient $k$NN search
algorithm called $k$GMCMS is designed. Comprehensive experimental evaluation on
real and synthetic dataset clearly demonstrates that our solution outperforms
the-state-of-the-art methods.",arxiv
http://arxiv.org/abs/1808.03941v2,2019-05-05T03:42:19Z,2018-08-12T13:30:27Z,"Denoising of 3-D Magnetic Resonance Images Using a Residual
  Encoder-Decoder Wasserstein Generative Adversarial Network","Structure-preserved denoising of 3D magnetic resonance imaging (MRI) images
is a critical step in medical image analysis. Over the past few years, many
algorithms with impressive performances have been proposed. In this paper,
inspired by the idea of deep learning, we introduce an MRI denoising method
based on the residual encoder-decoder Wasserstein generative adversarial
network (RED-WGAN). Specifically, to explore the structure similarity between
neighboring slices, a 3D configuration is utilized as the basic processing
unit. Residual autoencoders combined with deconvolution operations are
introduced into the generator network. Furthermore, to alleviate the
oversmoothing shortcoming of the traditional mean squared error (MSE) loss
function, the perceptual similarity, which is implemented by calculating the
distances in the feature space extracted by a pretrained VGG-19 network, is
incorporated with the MSE and adversarial losses to form the new loss function.
Extensive experiments are implemented to assess the performance of the proposed
method. The experimental results show that the proposed RED-WGAN achieves
performance superior to several state-of-the-art methods in both simulated and
real clinical data. In particular, our method demonstrates powerful abilities
in both noise suppression and structure preservation.",arxiv
http://arxiv.org/abs/1807.05211v1,2018-07-11T11:05:12Z,2018-07-11T11:05:12Z,"Learning Deployable Navigation Policies at Kilometer Scale from a Single
  Traversal","Model-free reinforcement learning has recently been shown to be effective at
learning navigation policies from complex image input. However, these
algorithms tend to require large amounts of interaction with the environment,
which can be prohibitively costly to obtain on robots in the real world. We
present an approach for efficiently learning goal-directed navigation policies
on a mobile robot, from only a single coverage traversal of recorded data. The
navigation agent learns an effective policy over a diverse action space in a
large heterogeneous environment consisting of more than 2km of travel, through
buildings and outdoor regions that collectively exhibit large variations in
visual appearance, self-similarity, and connectivity. We compare pretrained
visual encoders that enable precomputation of visual embeddings to achieve a
throughput of tens of thousands of transitions per second at training time on a
commodity desktop computer, allowing agents to learn from millions of
trajectories of experience in a matter of hours. We propose multiple forms of
computationally efficient stochastic augmentation to enable the learned policy
to generalise beyond these precomputed embeddings, and demonstrate successful
deployment of the learned policy on the real robot without fine tuning, despite
environmental appearance differences at test time. The dataset and code
required to reproduce these results and apply the technique to other datasets
and robots is made publicly available at rl-navigation.github.io/deployable.",arxiv
http://arxiv.org/abs/1807.03873v2,2018-07-13T00:19:43Z,2018-07-10T21:36:23Z,Automatic Gradient Boosting,"Automatic machine learning performs predictive modeling with high performing
machine learning tools without human interference. This is achieved by making
machine learning applications parameter-free, i.e. only a dataset is provided
while the complete model selection and model building process is handled
internally through (often meta) optimization. Projects like Auto-WEKA and
auto-sklearn aim to solve the Combined Algorithm Selection and Hyperparameter
optimization (CASH) problem resulting in huge configuration spaces. However,
for most real-world applications, the optimization over only a few different
key learning algorithms can not only be sufficient, but also potentially
beneficial. The latter becomes apparent when one considers that models have to
be validated, explained, deployed and maintained. Here, less complex model are
often preferred, for validation or efficiency reasons, or even a strict
requirement. Automatic gradient boosting simplifies this idea one step further,
using only gradient boosting as a single learning algorithm in combination with
model-based hyperparameter tuning, threshold optimization and encoding of
categorical features. We introduce this general framework as well as a concrete
implementation called autoxgboost. It is compared to current AutoML projects on
16 datasets and despite its simplicity is able to achieve comparable results on
about half of the datasets as well as performing best on two.",arxiv
http://arxiv.org/abs/1806.10409v1,2018-06-27T10:56:55Z,2018-06-27T10:56:55Z,"A neuro-inspired system for online learning and recognition of parallel
  spike trains, based on spike latency and heterosynaptic STDP","Humans perform remarkably well in many cognitive tasks including pattern
recognition. However, the neuronal mechanisms underlying this process are not
well understood. Nevertheless, artificial neural networks, inspired in brain
circuits, have been designed and used to tackle spatio-temporal pattern
recognition tasks. In this paper we present a multineuronal spike pattern
detection structure able to autonomously implement online learning and
recognition of parallel spike sequences (i.e., sequences of pulses belonging to
different neurons/neural ensembles). The operating principle of this structure
is based on two spiking/synaptic neurocomputational characteristics: spike
latency, that enables neurons to fire spikes with a certain delay and
heterosynaptic plasticity, that allows the own regulation of synaptic weights.
From the perspective of the information representation, the structure allows
mapping a spatio-temporal stimulus into a multidimensional, temporal, feature
space. In this space, the parameter coordinate and the time at which a neuron
fires represent one specific feature. In this sense, each feature can be
considered to span a single temporal axis. We applied our proposed scheme to
experimental data obtained from a motor inhibitory cognitive task. The test
exhibits good classification performance, indicating the adequateness of our
approach. In addition to its effectiveness, its simplicity and low
computational cost suggest a large scale implementation for real time
recognition applications in several areas, such as brain computer interface,
personal biometrics authentication or early detection of diseases.",arxiv
http://arxiv.org/abs/1806.07851v2,2018-10-08T01:32:02Z,2018-06-20T17:22:12Z,Sim-to-Real Reinforcement Learning for Deformable Object Manipulation,"We have seen much recent progress in rigid object manipulation, but
interaction with deformable objects has notably lagged behind. Due to the large
configuration space of deformable objects, solutions using traditional
modelling approaches require significant engineering work. Perhaps then,
bypassing the need for explicit modelling and instead learning the control in
an end-to-end manner serves as a better approach? Despite the growing interest
in the use of end-to-end robot learning approaches, only a small amount of work
has focused on their applicability to deformable object manipulation. Moreover,
due to the large amount of data needed to learn these end-to-end solutions, an
emerging trend is to learn control policies in simulation and then transfer
them over to the real world. To-date, no work has explored whether it is
possible to learn and transfer deformable object policies. We believe that if
sim-to-real methods are to be employed further, then it should be possible to
learn to interact with a wide variety of objects, and not only rigid objects.
In this work, we use a combination of state-of-the-art deep reinforcement
learning algorithms to solve the problem of manipulating deformable objects
(specifically cloth). We evaluate our approach on three tasks --- folding a
towel up to a mark, folding a face towel diagonally, and draping a piece of
cloth over a hanger. Our agents are fully trained in simulation with domain
randomisation, and then successfully deployed in the real world without having
seen any real deformable objects.",arxiv
http://arxiv.org/abs/1806.06876v3,2019-03-24T05:20:22Z,2018-06-18T18:24:16Z,"Diving Deep onto Discriminative Ensemble of Histological Hashing &
  Class-Specific Manifold Learning for Multi-class Breast Carcinoma Taxonomy","Histopathological images (HI) encrypt resolution dependent heterogeneous
textures & diverse color distribution variability, manifesting in
micro-structural surface tissue convolutions. Also, inherently high coherency
of cancerous cells poses significant challenges to breast cancer (BC)
multi-classification. As such, multi-class stratification is sparsely explored
& prior work mainly focus on benign & malignant tissue characterization only,
which forestalls further quantitative analysis of subordinate classes like
adenosis, mucinous carcinoma & fibroadenoma etc, for diagnostic competence. In
this work, a fully-automated, near-real-time & computationally inexpensive
robust multi-classification deep framework from HI is presented.
  The proposed scheme employs deep neural network (DNN) aided discriminative
ensemble of holistic class-specific manifold learning (CSML) for underlying HI
sub-space embedding & HI hashing based local shallow signatures. The model
achieves 95.8% accuracy pertinent to multi-classification & 2.8% overall
performance improvement & 38.2% enhancement for Lobular carcinoma (LC)
sub-class recognition rate as compared to the existing state-of-the-art on well
known BreakHis dataset is achieved. Also, 99.3% recognition rate at 200X & a
sensitivity of 100% for binary grading at all magnification validates its
suitability for clinical deployment in hand-held smart devices.",arxiv
http://arxiv.org/abs/1806.02350v1,2018-06-06T18:00:02Z,2018-06-06T18:00:02Z,Learning New Physics from a Machine,"We propose using neural networks to detect data departures from a given
reference model, with no prior bias on the nature of the new physics
responsible for the discrepancy. The virtues of neural networks as unbiased
function approximants make them particularly suited for this task. An algorithm
that implements this idea is constructed, as a straightforward application of
the likelihood-ratio hypothesis test. The algorithm compares observations with
an auxiliary set of reference-distributed events, possibly obtained with a
Monte Carlo event generator. It returns a p-value, which measures the
compatibility of the reference model with the data. It also identifies the most
discrepant phase-space region of the data set, to be selected for further
investigation. The most interesting potential applications are
model-independent new physics searches, although our approach could also be
used to compare the theoretical predictions of different Monte Carlo event
generators, or for data validation algorithms. In this work we study the
performance of our algorithm on a few simple examples. The results confirm the
model-independence of the approach, namely that it displays good sensitivity to
a variety of putative signals. Furthermore, we show that the reach does not
depend much on whether a favorable signal region is selected based on prior
expectations. We identify directions for improvement towards applications to
real experimental data sets.",arxiv
http://arxiv.org/abs/1806.00260v1,2018-06-01T09:59:09Z,2018-06-01T09:59:09Z,"The Proximal Alternating Minimization Algorithm for two-block separable
  convex optimization problems with linear constraints","The Alternating Minimization Algorithm (AMA) has been proposed by Tseng to
solve convex programming problems with two-block separable linear constraints
and objectives, whereby (at least) one of the components of the latter is
assumed to be strongly convex. The fact that one of the subproblems to be
solved within the iteration process of AMA does not usually correspond to the
calculation of a proximal operator through a closed formula, affects the
implementability of the algorithm. In this paper we allow in each block of the
objective a further smooth convex function and propose a proximal version of
AMA, called Proximal AMA, which is achieved by equipping the algorithm with
proximal terms induced by variable metrics. For suitable choices of the latter,
the solving of the two subproblems in the iterative scheme can be reduced to
the computation of proximal operators. We investigate the convergence of the
proposed algorithm in a real Hilbert space setting and illustrate its numerical
performances on two applications in image processing and machine learning.",arxiv
http://arxiv.org/abs/1805.03045v2,2018-06-12T08:10:11Z,2018-05-08T14:15:46Z,"A new method for unveiling Open Clusters in Gaia: new nearby Open
  Clusters confirmed by DR2","The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in
Astronomy. It includes precise astrometric data (positions, proper motions and
parallaxes) for more than $1.3$ billion sources, mostly stars. To analyse such
a vast amount of new data, the use of data mining techniques and machine
learning algorithms are mandatory. The search for Open Clusters, groups of
stars that were born and move together, located in the disk, is a great example
for the application of these techniques. Our aim is to develop a method to
automatically explore the data space, requiring minimal manual intervention. We
explore the performance of a density based clustering algorithm, DBSCAN, to
find clusters in the data together with a supervised learning method such as an
Artificial Neural Network (ANN) to automatically distinguish between real Open
Clusters and statistical clusters. The development and implementation of this
method to a $5$-Dimensional space ($l$, $b$, $\varpi$, $\mu_{\alpha^*}$,
$\mu_\delta$) to the Tycho-Gaia Astrometric Solution (TGAS) data, and a
posterior validation using Gaia DR2 data, lead to the proposal of a set of new
nearby Open Clusters. We have developed a method to find OCs in astrometric
data, designed to be applied to the full Gaia DR2 archive.",arxiv
http://arxiv.org/abs/1804.10332v2,2018-05-16T20:35:34Z,2018-04-27T03:42:55Z,Sim-to-Real: Learning Agile Locomotion For Quadruped Robots,"Designing agile locomotion for quadruped robots often requires extensive
expertise and tedious manual tuning. In this paper, we present a system to
automate this process by leveraging deep reinforcement learning techniques. Our
system can learn quadruped locomotion from scratch using simple reward signals.
In addition, users can provide an open loop reference to guide the learning
process when more control over the learned gait is needed. The control policies
are learned in a physics simulator and then deployed on real robots. In
robotics, policies trained in simulation often do not transfer to the real
world. We narrow this reality gap by improving the physics simulator and
learning robust policies. We improve the simulation using system
identification, developing an accurate actuator model and simulating latency.
We learn robust controllers by randomizing the physical environments, adding
perturbations and designing a compact observation space. We evaluate our system
on two agile locomotion gaits: trotting and galloping. After learning in
simulation, a quadruped robot can successfully perform both gaits in the real
world.",arxiv
http://arxiv.org/abs/1805.00330v1,2018-04-24T22:02:10Z,2018-04-24T22:02:10Z,"Real-Time Human Detection as an Edge Service Enabled by a Lightweight
  CNN","Edge computing allows more computing tasks to take place on the decentralized
nodes at the edge of networks. Today many delay sensitive, mission-critical
applications can leverage these edge devices to reduce the time delay or even
to enable real time, online decision making thanks to their onsite presence.
Human objects detection, behavior recognition and prediction in smart
surveillance fall into that category, where a transition of a huge volume of
video streaming data can take valuable time and place heavy pressure on
communication networks. It is widely recognized that video processing and
object detection are computing intensive and too expensive to be handled by
resource limited edge devices. Inspired by the depthwise separable convolution
and Single Shot Multi-Box Detector (SSD), a lightweight Convolutional Neural
Network (LCNN) is introduced in this paper. By narrowing down the classifier's
searching space to focus on human objects in surveillance video frames, the
proposed LCNN algorithm is able to detect pedestrians with an affordable
computation workload to an edge device. A prototype has been implemented on an
edge node (Raspberry PI 3) using openCV libraries, and satisfactory performance
is achieved using real world surveillance video streams. The experimental study
has validated the design of LCNN and shown it is a promising approach to
computing intensive applications at the edge.",arxiv
http://arxiv.org/abs/1804.06207v1,2018-04-17T12:51:52Z,2018-04-17T12:51:52Z,MetaBags: Bagged Meta-Decision Trees for Regression,"Ensembles are popular methods for solving practical supervised learning
problems. They reduce the risk of having underperforming models in
production-grade software. Although critical, methods for learning
heterogeneous regression ensembles have not been proposed at large scale,
whereas in classical ML literature, stacking, cascading and voting are mostly
restricted to classification problems. Regression poses distinct learning
challenges that may result in poor performance, even when using well
established homogeneous ensemble schemas such as bagging or boosting.
  In this paper, we introduce MetaBags, a novel, practically useful stacking
framework for regression. MetaBags is a meta-learning algorithm that learns a
set of meta-decision trees designed to select one base model (i.e. expert) for
each query, and focuses on inductive bias reduction. A set of meta-decision
trees are learned using different types of meta-features, specially created for
this purpose - to then be bagged at meta-level. This procedure is designed to
learn a model with a fair bias-variance trade-off, and its improvement over
base model performance is correlated with the prediction diversity of different
experts on specific input space subregions. The proposed method and
meta-features are designed in such a way that they enable good predictive
performance even in subregions of space which are not adequately represented in
the available training data.
  An exhaustive empirical testing of the method was performed, evaluating both
generalization error and scalability of the approach on synthetic, open and
real-world application datasets. The obtained results show that our method
significantly outperforms existing state-of-the-art approaches.",arxiv
http://arxiv.org/abs/1804.04756v1,2018-04-13T00:39:45Z,2018-04-13T00:39:45Z,Machine Learning Peeling and Loss Modelling of Time-Domain Reflectometry,"A fundamental pursuit of microwave metrology is the determination of the
characteristic impedance profile of microwave systems. Among other methods,
this can be practically achieved by means of time-domain reflectometry (TDR)
that measures the reflections from a device due to an applied stimulus.
Conventional TDR allows for the measurement of systems comprising a single
impedance. However, real systems typically feature impedance variations that
obscure the determination of all impedances subsequent to the first one. This
problem has been studied previously and is generally known as scattering
inversion or, in the context of microwave metrology, time-domain ""peeling"". In
this article, we demonstrate the implementation of a space-time efficient
peeling algorithm that corrects for the effect of prior impedance mismatch in a
nonuniform lossless transmission line, regardless of the nature of the
stimulus. We generalize TDR measurement analysis by introducing two tools: A
stochastic machine learning clustering tool and an arbitrary lossy transmission
line modeling tool. The former mitigates many of the imperfections typically
plaguing TDR measurements (except for dispersion) and allows for an efficient
processing of large datasets; the latter allows for a complete transmission
line characterization including both conductor and dielectric loss.",arxiv
http://arxiv.org/abs/1804.03289v1,2018-04-10T00:44:29Z,2018-04-10T00:44:29Z,"Planning Multi-Fingered Grasps as Probabilistic Inference in a Learned
  Deep Network","We propose a novel approach to multi-fingered grasp planning leveraging
learned deep neural network models. We train a convolutional neural network to
predict grasp success as a function of both visual information of an object and
grasp configuration. We can then formulate grasp planning as inferring the
grasp configuration which maximizes the probability of grasp success. We
efficiently perform this inference using a gradient-ascent optimization inside
the neural network using the backpropagation algorithm. Our work is the first
to directly plan high quality multifingered grasps in configuration space using
a deep neural network without the need of an external planner. We validate our
inference method performing both multifinger and two-finger grasps on real
robots. Our experimental results show that our planning method outperforms
existing planning methods for neural networks; while offering several other
benefits including being data-efficient in learning and fast enough to be
deployed in real robotic applications.",arxiv
http://arxiv.org/abs/1804.02395v2,2018-06-12T14:52:29Z,2018-04-06T15:25:14Z,"Structured Evolution with Compact Architectures for Scalable Policy
  Optimization","We present a new method of blackbox optimization via gradient approximation
with the use of structured random orthogonal matrices, providing more accurate
estimators than baselines and with provable theoretical guarantees. We show
that this algorithm can be successfully applied to learn better quality compact
policies than those using standard gradient estimation techniques. The compact
policies we learn have several advantages over unstructured ones, including
faster training algorithms and faster inference. These benefits are important
when the policy is deployed on real hardware with limited resources. Further,
compact policies provide more scalable architectures for derivative-free
optimization (DFO) in high-dimensional spaces. We show that most robotics tasks
from the OpenAI Gym can be solved using neural networks with less than 300
parameters, with almost linear time complexity of the inference phase, with up
to 13x fewer parameters relative to the Evolution Strategies (ES) algorithm
introduced by Salimans et al. (2017). We do not need heuristics such as fitness
shaping to learn good quality policies, resulting in a simple and theoretically
motivated training mechanism.",arxiv
http://arxiv.org/abs/1804.00064v1,2018-03-30T21:56:38Z,2018-03-30T21:56:38Z,"Learning Beyond Human Expertise with Generative Models for Dental
  Restorations","Computer vision has advanced significantly that many discriminative
approaches such as object recognition are now widely used in real applications.
We present another exciting development that utilizes generative models for the
mass customization of medical products such as dental crowns. In the dental
industry, it takes a technician years of training to design synthetic crowns
that restore the function and integrity of missing teeth. Each crown must be
customized to individual patients, and it requires human expertise in a
time-consuming and labor-intensive process, even with computer-assisted design
software. We develop a fully automatic approach that learns not only from human
designs of dental crowns, but also from natural spatial profiles between
opposing teeth. The latter is hard to account for by technicians but important
for proper biting and chewing functions. Built upon a Generative Adversar-ial
Network architecture (GAN), our deep learning model predicts the customized
crown-filled depth scan from the crown-missing depth scan and opposing depth
scan. We propose to incorporate additional space constraints and statistical
compatibility into learning. Our automatic designs exceed human technicians'
standards for good morphology and functionality, and our algorithm is being
tested for production use.",arxiv
http://arxiv.org/abs/1803.09786v1,2018-03-26T18:47:55Z,2018-03-26T18:47:55Z,"Transferable Joint Attribute-Identity Deep Learning for Unsupervised
  Person Re-Identification","Most existing person re-identification (re-id) methods require supervised
model learning from a separate large set of pairwise labelled training data for
every single camera pair. This significantly limits their scalability and
usability in real-world large scale deployments with the need for performing
re-id across many camera views. To address this scalability problem, we develop
a novel deep learning method for transferring the labelled information of an
existing dataset to a new unseen (unlabelled) target domain for person re-id
without any supervised learning in the target domain. Specifically, we
introduce an Transferable Joint Attribute-Identity Deep Learning (TJ-AIDL) for
simultaneously learning an attribute-semantic and identitydiscriminative
feature representation space transferrable to any new (unseen) target domain
for re-id tasks without the need for collecting new labelled training data from
the target domain (i.e. unsupervised learning in the target domain). Extensive
comparative evaluations validate the superiority of this new TJ-AIDL model for
unsupervised person re-id over a wide range of state-of-the-art methods on four
challenging benchmarks including VIPeR, PRID, Market-1501, and DukeMTMC-ReID.",arxiv
http://arxiv.org/abs/1803.08501v1,2018-03-22T14:59:16Z,2018-03-22T14:59:16Z,DOP: Deep Optimistic Planning with Approximate Value Function Evaluation,"Research on reinforcement learning has demonstrated promising results in
manifold applications and domains. Still, efficiently learning effective robot
behaviors is very difficult, due to unstructured scenarios, high uncertainties,
and large state dimensionality (e.g. multi-agent systems or hyper-redundant
robots). To alleviate this problem, we present DOP, a deep model-based
reinforcement learning algorithm, which exploits action values to both (1)
guide the exploration of the state space and (2) plan effective policies.
Specifically, we exploit deep neural networks to learn Q-functions that are
used to attack the curse of dimensionality during a Monte-Carlo tree search.
Our algorithm, in fact, constructs upper confidence bounds on the learned value
function to select actions optimistically. We implement and evaluate DOP on
different scenarios: (1) a cooperative navigation problem, (2) a fetching task
for a 7-DOF KUKA robot, and (3) a human-robot handover with a humanoid robot
(both in simulation and real). The obtained results show the effectiveness of
DOP in the chosen applications, where action values drive the exploration and
reduce the computational demand of the planning process while achieving good
performance.",arxiv
http://arxiv.org/abs/1803.07870v3,2020-06-07T20:02:21Z,2018-03-21T11:54:57Z,"Reservoir computing approaches for representation and classification of
  multivariate time series","Classification of multivariate time series (MTS) has been tackled with a
large variety of methodologies and applied to a wide range of scenarios.
Reservoir Computing (RC) provides efficient tools to generate a vectorial,
fixed-size representation of the MTS that can be further processed by standard
classifiers. Despite their unrivaled training speed, MTS classifiers based on a
standard RC architecture fail to achieve the same accuracy of fully trainable
neural networks. In this paper we introduce the reservoir model space, an
unsupervised approach based on RC to learn vectorial representations of MTS.
Each MTS is encoded within the parameters of a linear model trained to predict
a low-dimensional embedding of the reservoir dynamics. Compared to other RC
methods, our model space yields better representations and attains comparable
computational performance, thanks to an intermediate dimensionality reduction
procedure. As a second contribution we propose a modular RC framework for MTS
classification, with an associated open-source Python library. The framework
provides different modules to seamlessly implement advanced RC architectures.
The architectures are compared to other MTS classifiers, including deep
learning models and time series kernels. Results obtained on benchmark and
real-world MTS datasets show that RC classifiers are dramatically faster and,
when implemented using our proposed representation, also achieve superior
classification accuracy.",arxiv
http://arxiv.org/abs/1803.07634v1,2018-03-20T20:13:09Z,2018-03-20T20:13:09Z,Domain Adaptation with Randomized Expectation Maximization,"Domain adaptation (DA) is the task of classifying an unlabeled dataset
(target) using a labeled dataset (source) from a related domain. The majority
of successful DA methods try to directly match the distributions of the source
and target data by transforming the feature space. Despite their success, state
of the art methods based on this approach are either involved or unable to
directly scale to data with many features. This article shows that domain
adaptation can be successfully performed by using a very simple randomized
expectation maximization (EM) method. We consider two instances of the method,
which involve logistic regression and support vector machine, respectively. The
underlying assumption of the proposed method is the existence of a good single
linear classifier for both source and target domain. The potential limitations
of this assumption are alleviated by the flexibility of the method, which can
directly incorporate deep features extracted from a pre-trained deep neural
network. The resulting algorithm is strikingly easy to implement and apply. We
test its performance on 36 real-life adaptation tasks over text and image data
with diverse characteristics. The method achieves state-of-the-art results,
competitive with those of involved end-to-end deep transfer-learning methods.",arxiv
http://arxiv.org/abs/1803.04311v3,2019-01-30T12:54:32Z,2018-03-12T15:30:04Z,Deep Learning in Mobile and Wireless Networking: A Survey,"The rapid uptake of mobile devices and the rising popularity of mobile
applications and services pose unprecedented demands on mobile and wireless
networking infrastructure. Upcoming 5G systems are evolving to support
exploding mobile traffic volumes, agile management of network resource to
maximize user experience, and extraction of fine-grained real-time analytics.
Fulfilling these tasks is challenging, as mobile environments are increasingly
complex, heterogeneous, and evolving. One potential solution is to resort to
advanced machine learning techniques to help managing the rise in data volumes
and algorithm-driven applications. The recent success of deep learning
underpins new and powerful tools that tackle problems in this space.
  In this paper we bridge the gap between deep learning and mobile and wireless
networking research, by presenting a comprehensive survey of the crossovers
between the two areas. We first briefly introduce essential background and
state-of-the-art in deep learning techniques with potential applications to
networking. We then discuss several techniques and platforms that facilitate
the efficient deployment of deep learning onto mobile systems. Subsequently, we
provide an encyclopedic review of mobile and wireless networking research based
on deep learning, which we categorize by different domains. Drawing from our
experience, we discuss how to tailor deep learning to mobile environments. We
complete this survey by pinpointing current challenges and open future
directions for research.",arxiv
http://arxiv.org/abs/1802.08678v2,2018-02-26T11:03:21Z,2018-02-23T18:53:44Z,"Verifying Controllers Against Adversarial Examples with Bayesian
  Optimization","Recent successes in reinforcement learning have lead to the development of
complex controllers for real-world robots. As these robots are deployed in
safety-critical applications and interact with humans, it becomes critical to
ensure safety in order to avoid causing harm. A first step in this direction is
to test the controllers in simulation. To be able to do this, we need to
capture what we mean by safety and then efficiently search the space of all
behaviors to see if they are safe. In this paper, we present an active-testing
framework based on Bayesian Optimization. We specify safety constraints using
logic and exploit structure in the problem in order to test the system for
adversarial counter examples that violate the safety specifications. These
specifications are defined as complex boolean combinations of smooth functions
on the trajectories and, unlike reward functions in reinforcement learning, are
expressive and impose hard constraints on the system. In our framework, we
exploit regularity assumptions on individual functions in form of a Gaussian
Process (GP) prior. We combine these into a coherent optimization framework
using problem structure. The resulting algorithm is able to provably verify
complex safety specifications or alternatively find counter examples.
Experimental results show that the proposed method is able to find adversarial
examples quickly.",arxiv
http://arxiv.org/abs/1802.06958v1,2018-02-20T04:06:08Z,2018-02-20T04:06:08Z,"Deep Reinforcement Learning for Dynamic Multichannel Access in Wireless
  Networks","We consider a dynamic multichannel access problem, where multiple correlated
channels follow an unknown joint Markov model. A user at each time slot selects
a channel to transmit data and receives a reward based on the success or
failure of the transmission. The objective is to find a policy that maximizes
the expected long-term reward. The problem is formulated as a partially
observable Markov decision process (POMDP) with unknown system dynamics. To
overcome the challenges of unknown system dynamics as well as prohibitive
computation, we apply the concept of reinforcement learning and implement a
Deep Q-Network (DQN) that can deal with large state space without any prior
knowledge of the system dynamics. We provide an analytical study on the optimal
policy for fixed-pattern channel switching with known system dynamics and show
through simulations that DQN can achieve the same optimal performance without
knowing the system statistics. We compare the performance of DQN with a Myopic
policy and a Whittle Index-based heuristic through both simulations as well as
real-data trace and show that DQN achieves near-optimal performance in more
complex situations. Finally, we propose an adaptive DQN approach with the
capability to adapt its learning in time-varying, dynamic scenarios.",arxiv
http://arxiv.org/abs/1802.03938v1,2018-02-12T08:51:57Z,2018-02-12T08:51:57Z,"Revisiting the Vector Space Model: Sparse Weighted Nearest-Neighbor
  Method for Extreme Multi-Label Classification","Machine learning has played an important role in information retrieval (IR)
in recent times. In search engines, for example, query keywords are accepted
and documents are returned in order of relevance to the given query; this can
be cast as a multi-label ranking problem in machine learning. Generally, the
number of candidate documents is extremely large (from several thousand to
several million); thus, the classifier must handle many labels. This problem is
referred to as extreme multi-label classification (XMLC). In this paper, we
propose a novel approach to XMLC termed the Sparse Weighted Nearest-Neighbor
Method. This technique can be derived as a fast implementation of
state-of-the-art (SOTA) one-versus-rest linear classifiers for very sparse
datasets. In addition, we show that the classifier can be written as a sparse
generalization of a representer theorem with a linear kernel. Furthermore, our
method can be viewed as the vector space model used in IR. Finally, we show
that the Sparse Weighted Nearest-Neighbor Method can process data points in
real time on XMLC datasets with equivalent performance to SOTA models, with a
single thread and smaller storage footprint. In particular, our method exhibits
superior performance to the SOTA models on a dataset with 3 million labels.",arxiv
http://arxiv.org/abs/1802.02395v1,2018-02-07T12:01:13Z,2018-02-07T12:01:13Z,Evaluation of Deep Reinforcement Learning Methods for Modular Robots,"We propose a novel framework for Deep Reinforcement Learning (DRL) in modular
robotics using traditional robotic tools that extend state-of-the-art DRL
implementations and provide an end-to-end approach which trains a robot
directly from joint states. Moreover, we present a novel technique to transfer
these DLR methods into the real robot, aiming to close the simulation-reality
gap. We demonstrate the robustness of the performance of state-of-the-art DRL
methods for continuous action spaces in modular robots, with an empirical study
both in simulation and in the real robot where we also evaluate how
accelerating the simulation time affects the robot's performance. Our results
show that extending the modular robot from 3 degrees-of-freedom (DoF), to 4
DoF, does not affect the robot's learning. This paves the way towards training
modular robots using DRL techniques.",arxiv
http://arxiv.org/abs/1802.00264v1,2018-02-01T12:41:25Z,2018-02-01T12:41:25Z,Automatic Safety Helmet Wearing Detection,"Surveillance is essential for the safety of power substation. The detection
of whether wearing safety helmets or not for perambulatory workers is the key
component of overall intelligent surveillance system in power substation. In
this paper, a novel and practical safety helmet detection framework based on
computer vision, machine learning and image processing is proposed. In order to
ascertain motion objects in power substation, the ViBe background modelling
algorithm is employed. Moreover, based on the result of motion objects
segmentation, real-time human classification framework C4 is applied to locate
pedestrian in power substation accurately and quickly. Finally, according to
the result of pedestrian detection, the safety helmet wearing detection is
implemented using the head location, the color space transformation and the
color feature discrimination. Extensive compelling experimental results in
power substation illustrate the efficiency and effectiveness of the proposed
framework.",arxiv
http://arxiv.org/abs/1801.09684v2,2018-06-16T16:00:22Z,2018-01-29T19:00:01Z,Latent Space Purification via Neural Density Operators,"Machine learning is actively being explored for its potential to design,
validate, and even hybridize with near-term quantum devices. A central question
is whether neural networks can provide a tractable representation of a given
quantum state of interest. When true, stochastic neural networks can be
employed for many unsupervised tasks, including generative modeling and state
tomography. However, to be applicable for real experiments such methods must be
able to encode quantum mixed states. Here, we parametrize a density matrix
based on a restricted Boltzmann machine that is capable of purifying a mixed
state through auxiliary degrees of freedom embedded in the latent space of its
hidden units. We implement the algorithm numerically and use it to perform
tomography on some typical states of entangled photons, achieving fidelities
competitive with standard techniques.",arxiv
http://arxiv.org/abs/1801.09271v1,2018-01-28T19:29:50Z,2018-01-28T19:29:50Z,"Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical
  Registry Data","This paper presents the first deep reinforcement learning (DRL) framework to
estimate the optimal Dynamic Treatment Regimes from observational medical data.
This framework is more flexible and adaptive for high dimensional action and
state spaces than existing reinforcement learning methods to model real-life
complexity in heterogeneous disease progression and treatment choices, with the
goal of providing doctor and patients the data-driven personalized decision
recommendations. The proposed DRL framework comprises (i) a supervised learning
step to predict the most possible expert actions, and (ii) a deep reinforcement
learning step to estimate the long-term value function of Dynamic Treatment
Regimes. Both steps depend on deep neural networks.
  As a key motivational example, we have implemented the proposed framework on
a data set from the Center for International Bone Marrow Transplant Research
(CIBMTR) registry database, focusing on the sequence of prevention and
treatments for acute and chronic graft versus host disease after
transplantation. In the experimental results, we have demonstrated promising
accuracy in predicting human experts' decisions, as well as the high expected
reward function in the DRL-based dynamic treatment regimes.",arxiv
http://arxiv.org/abs/1801.05643v1,2018-01-17T12:51:01Z,2018-01-17T12:51:01Z,"The Case for Automatic Database Administration using Deep Reinforcement
  Learning","Like any large software system, a full-fledged DBMS offers an overwhelming
amount of configuration knobs. These range from static initialisation
parameters like buffer sizes, degree of concurrency, or level of replication to
complex runtime decisions like creating a secondary index on a particular
column or reorganising the physical layout of the store. To simplify the
configuration, industry grade DBMSs are usually shipped with various advisory
tools, that provide recommendations for given workloads and machines. However,
reality shows that the actual configuration, tuning, and maintenance is usually
still done by a human administrator, relying on intuition and experience.
Recent work on deep reinforcement learning has shown very promising results in
solving problems, that require such a sense of intuition. For instance, it has
been applied very successfully in learning how to play complicated games with
enormous search spaces. Motivated by these achievements, in this work we
explore how deep reinforcement learning can be used to administer a DBMS.
First, we will describe how deep reinforcement learning can be used to
automatically tune an arbitrary software system like a DBMS by defining a
problem environment. Second, we showcase our concept of NoDBA at the concrete
example of index selection and evaluate how well it recommends indexes for
given workloads.",arxiv
http://arxiv.org/abs/1801.02854v3,2018-07-25T07:54:52Z,2018-01-09T09:44:21Z,Riemannian Motion Policies,"We introduce the Riemannian Motion Policy (RMP), a new mathematical object
for modular motion generation. An RMP is a second-order dynamical system
(acceleration field or motion policy) coupled with a corresponding Riemannian
metric. The motion policy maps positions and velocities to accelerations, while
the metric captures the directions in the space important to the policy. We
show that RMPs provide a straightforward and convenient method for combining
multiple motion policies and transforming such policies from one space (such as
the task space) to another (such as the configuration space) in geometrically
consistent ways. The operators we derive for these combinations and
transformations are provably optimal, have linearity properties making them
agnostic to the order of application, and are strongly analogous to the
covariant transformations of natural gradients popular in the machine learning
literature. The RMP framework enables the fusion of motion policies from
different motion generation paradigms, such as dynamical systems, dynamic
movement primitives (DMPs), optimal control, operational space control,
nonlinear reactive controllers, motion optimization, and model predictive
control (MPC), thus unifying these disparate techniques from the literature.
RMPs are easy to implement and manipulate, facilitate controller design,
simplify handling of joint limits, and clarify a number of open questions
regarding the proper fusion of motion generation methods (such as incorporating
local reactive policies into long-horizon optimizers). We demonstrate the
effectiveness of RMPs on both simulation and real robots, including their
ability to naturally and efficiently solve complicated collision avoidance
problems previously handled by more complex planners.",arxiv
http://arxiv.org/abs/1801.02108v2,2018-06-07T14:44:00Z,2018-01-07T01:03:25Z,SBNet: Sparse Blocks Network for Fast Inference,"Conventional deep convolutional neural networks (CNNs) apply convolution
operators uniformly in space across all feature maps for hundreds of layers -
this incurs a high computational cost for real-time applications. For many
problems such as object detection and semantic segmentation, we are able to
obtain a low-cost computation mask, either from a priori problem knowledge, or
from a low-resolution segmentation network. We show that such computation masks
can be used to reduce computation in the high-resolution main network. Variants
of sparse activation CNNs have previously been explored on small-scale tasks
and showed no degradation in terms of object classification accuracy, but often
measured gains in terms of theoretical FLOPs without realizing a practical
speed-up when compared to highly optimized dense convolution implementations.
In this work, we leverage the sparsity structure of computation masks and
propose a novel tiling-based sparse convolution algorithm. We verified the
effectiveness of our sparse CNN on LiDAR-based 3D object detection, and we
report significant wall-clock speed-ups compared to dense convolution without
noticeable loss of accuracy.",arxiv
http://arxiv.org/abs/1801.01999v1,2018-01-06T10:38:51Z,2018-01-06T10:38:51Z,Using reinforcement learning to learn how to play text-based games,"The ability to learn optimal control policies in systems where action space
is defined by sentences in natural language would allow many interesting
real-world applications such as automatic optimisation of dialogue systems.
Text-based games with multiple endings and rewards are a promising platform for
this task, since their feedback allows us to employ reinforcement learning
techniques to jointly learn text representations and control policies. We
present a general text game playing agent, testing its generalisation and
transfer learning performance and showing its ability to play multiple games at
once. We also present pyfiction, an open-source library for universal access to
different text games that could, together with our agent that implements its
interface, serve as a baseline for future research.",arxiv
http://arxiv.org/abs/1801.00864v3,2020-07-16T17:07:07Z,2018-01-02T23:49:13Z,"FNS: an event-driven spiking neural network simulator based on the LIFL
  neuron model","Limitations in processing capabilities and memory of today's computers make
spiking neuron-based (human) whole-brain simulations inevitably characterized
by a compromise between bio-plausibility and computational cost. It translates
into brain models composed of a reduced number of neurons and a simplified
neuron's mathematical model, leading to the search for new simulation
strategies. Taking advantage of the sparse character of brain-like computation,
the event-driven technique could represent a way to carry out efficient
simulation of large-scale Spiking Neural Networks (SNN). The recent Leaky
Integrate-and-Fire with Latency (LIFL) spiking neuron model is event-driven
compatible and exhibits some realistic neuronal features, opening new avenues
for brain modelling. In this paper we introduce FNS, the first LIFL-based
spiking neural network framework, which combines spiking/synaptic neural
modelling with the event-driven approach, allowing us to define heterogeneous
neuron modules and multi-scale connectivity with delayed connections and
plastic synapses. In order to allow multi-thread implementations a novel
parallelization strategy is also introduced. This paper presents mathematical
models, software implementation and simulation routines on which FNS is based.
Finally, a brain subnetwork is modeled on the basis of real brain structural
data, and the resulting simulated activity is compared with associated brain
functional (source-space MEG) data, demonstrating a good matching between the
activity of the model and that of the experimetal data. This work aims to lay
the groundwork for future event-driven based personalised brain models.",arxiv
http://arxiv.org/abs/1712.06107v1,2017-12-17T13:00:25Z,2017-12-17T13:00:25Z,Railway Track Specific Traffic Signal Selection Using Deep Learning,"With the railway transportation Industry moving actively towards automation,
accurate location and inventory of wayside track assets like traffic signals,
crossings, switches, mileposts, etc. is of extreme importance. With the new
Positive Train Control (PTC) regulation coming into effect, many railway safety
rules will be tied directly to location of assets like mileposts and signals.
Newer speed regulations will be enforced based on location of the Train with
respect to a wayside asset. Hence it is essential for the railroads to have an
accurate database of the types and locations of these assets. This paper talks
about a real-world use-case of detecting railway signals from a camera mounted
on a moving locomotive and tracking their locations. The camera is engineered
to withstand the environment factors on a moving train and provide a consistent
steady image at around 30 frames per second. Using advanced image analysis and
deep learning techniques, signals are detected in these camera images and a
database of their locations is created. Railway signals differ a lot from road
signals in terms of shapes and rules for placement with respect to track. Due
to space constraint and traffic densities in urban areas signals are not placed
on the same side of the track and multiple lines can run in parallel. Hence
there is need to associate signal detected with the track on which the train
runs. We present a method to associate the signals to the specific track they
belong to using a video feed from the front facing camera mounted on the lead
locomotive. A pipeline of track detection, region of interest selection, signal
detection has been implemented which gives an overall accuracy of 94.7% on a
route covering 150km with 247 signals.",arxiv
http://arxiv.org/abs/1712.02294v4,2018-07-12T14:11:40Z,2017-12-06T17:20:21Z,Joint 3D Proposal Generation and Object Detection from View Aggregation,"We present AVOD, an Aggregate View Object Detection network for autonomous
driving scenarios. The proposed neural network architecture uses LIDAR point
clouds and RGB images to generate features that are shared by two subnetworks:
a region proposal network (RPN) and a second stage detector network. The
proposed RPN uses a novel architecture capable of performing multimodal feature
fusion on high resolution feature maps to generate reliable 3D object proposals
for multiple object classes in road scenes. Using these proposals, the second
stage detection network performs accurate oriented 3D bounding box regression
and category classification to predict the extents, orientation, and
classification of objects in 3D space. Our proposed architecture is shown to
produce state of the art results on the KITTI 3D object detection benchmark
while running in real time with a low memory footprint, making it a suitable
candidate for deployment on autonomous vehicles. Code is at:
https://github.com/kujason/avod",arxiv
http://arxiv.org/abs/1712.01785v3,2017-12-16T00:30:53Z,2017-12-05T17:49:18Z,"Towards Practical Verification of Machine Learning: The Case of Computer
  Vision Systems","Due to the increasing usage of machine learning (ML) techniques in security-
and safety-critical domains, such as autonomous systems and medical diagnosis,
ensuring correct behavior of ML systems, especially for different corner cases,
is of growing importance. In this paper, we propose a generic framework for
evaluating security and robustness of ML systems using different real-world
safety properties. We further design, implement and evaluate VeriVis, a
scalable methodology that can verify a diverse set of safety properties for
state-of-the-art computer vision systems with only blackbox access. VeriVis
leverage different input space reduction techniques for efficient verification
of different safety properties. VeriVis is able to find thousands of safety
violations in fifteen state-of-the-art computer vision systems including ten
Deep Neural Networks (DNNs) such as Inception-v3 and Nvidia's Dave self-driving
system with thousands of neurons as well as five commercial third-party vision
APIs including Google vision and Clarifai for twelve different safety
properties. Furthermore, VeriVis can successfully verify local safety
properties, on average, for around 31.7% of the test images. VeriVis finds up
to 64.8x more violations than existing gradient-based methods that, unlike
VeriVis, cannot ensure non-existence of any violations. Finally, we show that
retraining using the safety violations detected by VeriVis can reduce the
average number of violations up to 60.2%.",arxiv
http://arxiv.org/abs/1712.03073v3,2021-01-13T00:55:32Z,2017-12-01T16:58:32Z,DeepWear: Adaptive Local Offloading for On-Wearable Deep Learning,"Due to their on-body and ubiquitous nature, wearables can generate a wide
range of unique sensor data creating countless opportunities for deep learning
tasks. We propose DeepWear, a deep learning (DL) framework for wearable devices
to improve the performance and reduce the energy footprint. DeepWear
strategically offloads DL tasks from a wearable device to its paired handheld
device through local network. Compared to the remote-cloud-based offloading,
DeepWear requires no Internet connectivity, consumes less energy, and is robust
to privacy breach. DeepWear provides various novel techniques such as
context-aware offloading, strategic model partition, and pipelining support to
efficiently utilize the processing capacity from nearby paired handhelds.
Deployed as a user-space library, DeepWear offers developer-friendly APIs that
are as simple as those in traditional DL libraries such as TensorFlow. We have
implemented DeepWear on the Android OS and evaluated it on COTS smartphones and
smartwatches with real DL models. DeepWear brings up to 5.08X and 23.0X
execution speedup, as well as 53.5% and 85.5% energy saving compared to
wearable-only and handheld-only strategies, respectively.",arxiv
http://arxiv.org/abs/1711.07910v3,2021-01-06T23:07:06Z,2017-11-21T16:59:43Z,Domain Generalization by Marginal Transfer Learning,"In the problem of domain generalization (DG), there are labeled training data
sets from several related prediction problems, and the goal is to make accurate
predictions on future unlabeled data sets that are not known to the learner.
This problem arises in several applications where data distributions fluctuate
because of environmental, technical, or other sources of variation. We
introduce a formal framework for DG, and argue that it can be viewed as a kind
of supervised learning problem by augmenting the original feature space with
the marginal distribution of feature vectors. While our framework has several
connections to conventional analysis of supervised learning algorithms, several
unique aspects of DG require new methods of analysis.
  This work lays the learning theoretic foundations of domain generalization,
building on our earlier conference paper where the problem of DG was introduced
(Blanchard et al., 2011). We present two formal models of data generation,
corresponding notions of risk, and distribution-free generalization error
analysis. By focusing our attention on kernel methods, we also provide more
quantitative results and a universally consistent algorithm. An efficient
implementation is provided for this algorithm, which is experimentally compared
to a pooling strategy on one synthetic and three real-world data sets.",arxiv
http://arxiv.org/abs/1708.05208v1,2017-08-17T11:31:22Z,2017-08-17T11:31:22Z,"Automatic HVAC Control with Real-time Occupancy Recognition and
  Simulation-guided Model Predictive Control in Low-cost Embedded System","Intelligent building automation systems can reduce the energy consumption of
heating, ventilation and air-conditioning (HVAC) units by sensing the comfort
requirements automatically and scheduling the HVAC operations dynamically.
Traditional building automation systems rely on fairly inaccurate occupancy
sensors and basic predictive control using oversimplified building thermal
response models, all of which prevent such systems from reaching their full
potential. Such limitations can now be avoided due to the recent developments
in embedded system technologies, which provide viable low-cost computing
platforms with powerful processors and sizeable memory storage in a small
footprint. As a result, building automation systems can now efficiently execute
highly-sophisticated computational tasks, such as real-time video processing
and accurate thermal-response simulations. With this in mind, we designed and
implemented an occupancy-predictive HVAC control system in a low-cost yet
powerful embedded system (using Raspberry Pi 3) to demonstrate the following
key features for building automation: (1) real-time occupancy recognition using
video-processing and machine-learning techniques, (2) dynamic analysis and
prediction of occupancy patterns, and (3) model predictive control for HVAC
operations guided by real-time building thermal response simulations (using an
on-board EnergyPlus simulator). We deployed and evaluated our system for
providing automatic HVAC control in the large public indoor space of a mosque,
thereby achieving significant energy savings.",arxiv
http://arxiv.org/abs/1708.02190v2,2020-07-24T10:12:16Z,2017-08-07T16:32:39Z,"Intrinsically Motivated Goal Exploration Processes with Automatic
  Curriculum Learning","Intrinsically motivated spontaneous exploration is a key enabler of
autonomous lifelong learning in human children. It enables the discovery and
acquisition of large repertoires of skills through self-generation,
self-selection, self-ordering and self-experimentation of learning goals. We
present an algorithmic approach called Intrinsically Motivated Goal Exploration
Processes (IMGEP) to enable similar properties of autonomous or self-supervised
learning in machines. The IMGEP algorithmic architecture relies on several
principles: 1) self-generation of goals, generalized as fitness functions; 2)
selection of goals based on intrinsic rewards; 3) exploration with incremental
goal-parameterized policy search and exploitation of the gathered data with a
batch learning algorithm; 4) systematic reuse of information acquired when
targeting a goal for improving towards other goals. We present a particularly
efficient form of IMGEP, called Modular Population-Based IMGEP, that uses a
population-based policy and an object-centered modularity in goals and
mutations. We provide several implementations of this architecture and
demonstrate their ability to automatically generate a learning curriculum
within several experimental setups including a real humanoid robot that can
explore multiple spaces of goals with several hundred continuous dimensions.
While no particular target goal is provided to the system, this curriculum
allows the discovery of skills that act as stepping stone for learning more
complex skills, e.g. nested tool use. We show that learning diverse spaces of
goals with intrinsic motivations is more efficient for learning complex skills
than only trying to directly learn these complex skills.",arxiv
http://arxiv.org/abs/1707.02880v2,2017-08-22T19:26:08Z,2017-07-10T14:34:06Z,Deep Bilateral Learning for Real-Time Image Enhancement,"Performance is a critical challenge in mobile image processing. Given a
reference imaging pipeline, or even human-adjusted pairs of images, we seek to
reproduce the enhancements and enable real-time evaluation. For this, we
introduce a new neural network architecture inspired by bilateral grid
processing and local affine color transforms. Using pairs of input/output
images, we train a convolutional neural network to predict the coefficients of
a locally-affine model in bilateral space. Our architecture learns to make
local, global, and content-dependent decisions to approximate the desired image
transformation. At runtime, the neural network consumes a low-resolution
version of the input image, produces a set of affine transformations in
bilateral space, upsamples those transformations in an edge-preserving fashion
using a new slicing node, and then applies those upsampled transformations to
the full-resolution image. Our algorithm processes high-resolution images on a
smartphone in milliseconds, provides a real-time viewfinder at 1080p
resolution, and matches the quality of state-of-the-art approximation
techniques on a large class of image operators. Unlike previous work, our model
is trained off-line from data and therefore does not require access to the
original operator at runtime. This allows our model to learn complex,
scene-dependent transformations for which no reference implementation is
available, such as the photographic edits of a human retoucher.",arxiv
http://arxiv.org/abs/1706.06695v1,2017-06-20T22:52:10Z,2017-06-20T22:52:10Z,"Toward Real-Time Decentralized Reinforcement Learning using Finite
  Support Basis Functions","This paper addresses the design and implementation of complex Reinforcement
Learning (RL) behaviors where multi-dimensional action spaces are involved, as
well as the need to execute the behaviors in real-time using robotic platforms
with limited computational resources and training times. For this purpose, we
propose the use of decentralized RL, in combination with finite support basis
functions as alternatives to Gaussian RBF, in order to alleviate the effects of
the curse of dimensionality on the action and state spaces respectively, and to
reduce the computation time. As testbed, a RL based controller for the in-walk
kick in NAO robots, a challenging and critical problem for soccer robotics, is
used. The reported experiments show empirically that our solution saves up to
99.94% of execution time and 98.82% of memory consumption during execution,
without diminishing performance compared to classical approaches.",arxiv
http://arxiv.org/abs/1705.06599v1,2017-05-17T03:04:43Z,2017-05-17T03:04:43Z,Localized LRR on Grassmann Manifolds: An Extrinsic View,"Subspace data representation has recently become a common practice in many
computer vision tasks. It demands generalizing classical machine learning
algorithms for subspace data. Low-Rank Representation (LRR) is one of the most
successful models for clustering vectorial data according to their subspace
structures. This paper explores the possibility of extending LRR for subspace
data on Grassmann manifolds. Rather than directly embedding the Grassmann
manifolds into the symmetric matrix space, an extrinsic view is taken to build
the LRR self-representation in the local area of the tangent space at each
Grassmannian point, resulting in a localized LRR method on Grassmann manifolds.
A novel algorithm for solving the proposed model is investigated and
implemented. The performance of the new clustering algorithm is assessed
through experiments on several real-world datasets including MNIST handwritten
digits, ballet video clips, SKIG action clips, DynTex++ dataset and highway
traffic video clips. The experimental results show the new method outperforms a
number of state-of-the-art clustering methods",arxiv
http://arxiv.org/abs/1705.02436v9,2019-11-30T19:03:11Z,2017-05-06T03:13:21Z,Nonlinear Information Bottleneck,"Information bottleneck (IB) is a technique for extracting information in one
random variable $X$ that is relevant for predicting another random variable
$Y$. IB works by encoding $X$ in a compressed ""bottleneck"" random variable $M$
from which $Y$ can be accurately decoded. However, finding the optimal
bottleneck variable involves a difficult optimization problem, which until
recently has been considered for only two limited cases: discrete $X$ and $Y$
with small state spaces, and continuous $X$ and $Y$ with a Gaussian joint
distribution (in which case optimal encoding and decoding maps are linear). We
propose a method for performing IB on arbitrarily-distributed discrete and/or
continuous $X$ and $Y$, while allowing for nonlinear encoding and decoding
maps. Our approach relies on a novel non-parametric upper bound for mutual
information. We describe how to implement our method using neural networks. We
then show that it achieves better performance than the recently-proposed
""variational IB"" method on several real-world datasets.",arxiv
http://arxiv.org/abs/1704.07854v4,2019-02-20T13:27:28Z,2017-04-25T18:21:42Z,Generating Liquid Simulations with Deformation-aware Neural Networks,"We propose a novel approach for deformation-aware neural networks that learn
the weighting and synthesis of dense volumetric deformation fields. Our method
specifically targets the space-time representation of physical surfaces from
liquid simulations. Liquids exhibit highly complex, non-linear behavior under
changing simulation conditions such as different initial conditions. Our
algorithm captures these complex phenomena in two stages: a first neural
network computes a weighting function for a set of pre-computed deformations,
while a second network directly generates a deformation field for refining the
surface. Key for successful training runs in this setting is a suitable loss
function that encodes the effect of the deformations, and a robust calculation
of the corresponding gradients. To demonstrate the effectiveness of our
approach, we showcase our method with several complex examples of flowing
liquids with topology changes. Our representation makes it possible to rapidly
generate the desired implicit surfaces. We have implemented a mobile
application to demonstrate that real-time interactions with complex liquid
effects are possible with our approach.",arxiv
http://arxiv.org/abs/1704.06735v3,2017-06-12T19:47:02Z,2017-04-22T02:22:19Z,Asynchronous Distributed Variational Gaussian Processes for Regression,"Gaussian processes (GPs) are powerful non-parametric function estimators.
However, their applications are largely limited by the expensive computational
cost of the inference procedures. Existing stochastic or distributed
synchronous variational inferences, although have alleviated this issue by
scaling up GPs to millions of samples, are still far from satisfactory for
real-world large applications, where the data sizes are often orders of
magnitudes larger, say, billions. To solve this problem, we propose ADVGP, the
first Asynchronous Distributed Variational Gaussian Process inference for
regression, on the recent large-scale machine learning platform,
PARAMETERSERVER. ADVGP uses a novel, flexible variational framework based on a
weight space augmentation, and implements the highly efficient, asynchronous
proximal gradient optimization. While maintaining comparable or better
predictive performance, ADVGP greatly improves upon the efficiency of the
existing variational methods. With ADVGP, we effortlessly scale up GP
regression to a real-world application with billions of samples and demonstrate
an excellent, superior prediction accuracy to the popular linear models.",arxiv
http://arxiv.org/abs/1702.06329v1,2017-02-21T11:07:27Z,2017-02-21T11:07:27Z,"Towards a Common Implementation of Reinforcement Learning for Multiple
  Robotic Tasks","Mobile robots are increasingly being employed for performing complex tasks in
dynamic environments. Reinforcement learning (RL) methods are recognized to be
promising for specifying such tasks in a relatively simple manner. However, the
strong dependency between the learning method and the task to learn is a
well-known problem that restricts practical implementations of RL in robotics,
often requiring major modifications of parameters and adding other techniques
for each particular task. In this paper we present a practical core
implementation of RL which enables the learning process for multiple robotic
tasks with minimal per-task tuning or none. Based on value iteration methods,
this implementation includes a novel approach for action selection, called
Q-biased softmax regression (QBIASSR), which avoids poor performance of the
learning process when the robot reaches new unexplored states. Our approach
takes advantage of the structure of the state space by attending the physical
variables involved (e.g., distances to obstacles, X,Y,{\theta} pose, etc.),
thus experienced sets of states may favor the decision-making process of
unexplored or rarely-explored states. This improvement has a relevant role in
reducing the tuning of the algorithm for particular tasks. Experiments with
real and simulated robots, performed with the software framework also
introduced here, show that our implementation is effectively able to learn
different robotic tasks without tuning the learning method. Results also
suggest that the combination of true online SARSA({\lambda}) with QBIASSR can
outperform the existing RL core algorithms in low-dimensional robotic tasks.",arxiv
http://arxiv.org/abs/1702.05596v1,2017-02-18T10:47:16Z,2017-02-18T10:47:16Z,Brain Inspired Cognitive Model with Attention for Self-Driving Cars,"Perception-driven approach and end-to-end system are two major vision-based
frameworks for self-driving cars. However, it is difficult to introduce
attention and historical information of autonomous driving process, which are
the essential factors for achieving human-like driving into these two methods.
In this paper, we propose a novel model for self-driving cars named
brain-inspired cognitive model with attention (CMA). This model consists of
three parts: a convolutional neural network for simulating human visual cortex,
a cognitive map built to describe relationships between objects in complex
traffic scene and a recurrent neural network that combines with the real-time
updated cognitive map to implement attention mechanism and long-short term
memory. The benefit of our model is that can accurately solve three tasks
simultaneously:1) detection of the free space and boundaries of the current and
adjacent lanes. 2)estimation of obstacle distance and vehicle attitude, and 3)
learning of driving behavior and decision making from human driver. More
significantly, the proposed model could accept external navigating instructions
during an end-to-end driving process. For evaluation, we build a large-scale
road-vehicle dataset which contains more than forty thousand labeled road
images captured by three cameras on our self-driving car. Moreover, human
driving activities and vehicle states are recorded in the meanwhile.",arxiv
http://arxiv.org/abs/1702.02978v1,2017-02-09T20:53:57Z,2017-02-09T20:53:57Z,"Elastic Resource Management with Adaptive State Space Partitioning of
  Markov Decision Processes","Modern large-scale computing deployments consist of complex applications
running over machine clusters. An important issue in these is the offering of
elasticity, i.e., the dynamic allocation of resources to applications to meet
fluctuating workload demands. Threshold based approaches are typically
employed, yet they are difficult to configure and optimize. Approaches based on
reinforcement learning have been proposed, but they require a large number of
states in order to model complex application behavior. Methods that adaptively
partition the state space have been proposed, but their partitioning criteria
and strategies are sub-optimal. In this work we present MDP_DT, a novel
full-model based reinforcement learning algorithm for elastic resource
management that employs adaptive state space partitioning. We propose two novel
statistical criteria and three strategies and we experimentally prove that they
correctly decide both where and when to partition, outperforming existing
approaches. We experimentally evaluate MDP_DT in a real large scale cluster
over variable not-encountered workloads and we show that it takes more informed
decisions compared to static and model-free approaches, while requiring a
minimal amount of training data.",arxiv
http://arxiv.org/abs/1702.02676v1,2017-02-09T02:02:27Z,2017-02-09T02:02:27Z,Energy Saving Additive Neural Network,"In recent years, machine learning techniques based on neural networks for
mobile computing become increasingly popular. Classical multi-layer neural
networks require matrix multiplications at each stage. Multiplication operation
is not an energy efficient operation and consequently it drains the battery of
the mobile device. In this paper, we propose a new energy efficient neural
network with the universal approximation property over space of Lebesgue
integrable functions. This network, called, additive neural network, is very
suitable for mobile computing. The neural structure is based on a novel vector
product definition, called ef-operator, that permits a multiplier-free
implementation. In ef-operation, the ""product"" of two real numbers is defined
as the sum of their absolute values, with the sign determined by the sign of
the product of the numbers. This ""product"" is used to construct a vector
product in $R^N$. The vector product induces the $l_1$ norm. The proposed
additive neural network successfully solves the XOR problem. The experiments on
MNIST dataset show that the classification performances of the proposed
additive neural networks are very similar to the corresponding multi-layer
perceptron and convolutional neural networks (LeNet).",arxiv
http://arxiv.org/abs/1701.08878v1,2017-01-31T00:16:15Z,2017-01-31T00:16:15Z,"Deep Reinforcement Learning for Robotic Manipulation-The state of the
  art","The focus of this work is to enumerate the various approaches and algorithms
that center around application of reinforcement learning in robotic ma-
]]nipulation tasks. Earlier methods utilized specialized policy representations
and human demonstrations to constrict the policy. Such methods worked well with
continuous state and policy space of robots but failed to come up with
generalized policies. Subsequently, high dimensional non-linear function
approximators like neural networks have been used to learn policies from
scratch. Several novel and recent approaches have also embedded control policy
with efficient perceptual representation using deep learning. This has led to
the emergence of a new branch of dynamic robot control system called deep r
inforcement learning(DRL). This work embodies a survey of the most recent
algorithms, architectures and their implementations in simulations and real
world robotic platforms. The gamut of DRL architectures are partitioned into
two different branches namely, discrete action space algorithms(DAS) and
continuous action space algorithms(CAS). Further, the CAS algorithms are
divided into stochastic continuous action space(SCAS) and deterministic
continuous action space(DCAS) algorithms. Along with elucidating an organ-
isation of the DRL algorithms this work also manifests some of the state of the
art applications of these approaches in robotic manipulation tasks.",arxiv
http://arxiv.org/abs/1701.08748v3,2017-03-20T11:03:51Z,2017-01-30T18:39:38Z,"On the realistic validation of photometric redshifts, or why Teddy will
  never be Happy","Two of the main problems encountered in the development and accurate
validation of photometric redshift (photo-z) techniques are the lack of
spectroscopic coverage in feature space (e.g. colours and magnitudes) and the
mismatch between photometric error distributions associated with the
spectroscopic and photometric samples. Although these issues are well known,
there is currently no standard benchmark allowing a quantitative analysis of
their impact on the final photo-z estimation. In this work, we present two
galaxy catalogues, Teddy and Happy, built to enable a more demanding and
realistic test of photo-z methods. Using photometry from the Sloan Digital Sky
Survey and spectroscopy from a collection of sources, we constructed datasets
which mimic the biases between the underlying probability distribution of the
real spectroscopic and photometric sample. We demonstrate the potential of
these catalogues by submitting them to the scrutiny of different photo-z
methods, including machine learning (ML) and template fitting approaches.
Beyond the expected bad results from most ML algorithms for cases with missing
coverage in feature space, we were able to recognize the superiority of global
models in the same situation and the general failure across all types of
methods when incomplete coverage is convoluted with the presence of photometric
errors - a data situation which photo-z methods were not trained to deal with
up to now and which must be addressed by future large scale surveys. Our
catalogues represent the first controlled environment allowing a
straightforward implementation of such tests. The data are publicly available
within the COINtoolbox (https://github.com/COINtoolbox/photoz_catalogues).",arxiv
http://arxiv.org/abs/1611.08930v2,2017-03-28T03:15:07Z,2016-11-27T22:47:23Z,Deep attractor network for single-microphone speaker separation,"Despite the overwhelming success of deep learning in various speech
processing tasks, the problem of separating simultaneous speakers in a mixture
remains challenging. Two major difficulties in such systems are the arbitrary
source permutation and unknown number of sources in the mixture. We propose a
novel deep learning framework for single channel speech separation by creating
attractor points in high dimensional embedding space of the acoustic signals
which pull together the time-frequency bins corresponding to each source.
Attractor points in this study are created by finding the centroids of the
sources in the embedding space, which are subsequently used to determine the
similarity of each bin in the mixture to each source. The network is then
trained to minimize the reconstruction error of each source by optimizing the
embeddings. The proposed model is different from prior works in that it
implements an end-to-end training, and it does not depend on the number of
sources in the mixture. Two strategies are explored in the test time, K-means
and fixed attractor points, where the latter requires no post-processing and
can be implemented in real-time. We evaluated our system on Wall Street Journal
dataset and show 5.49\% improvement over the previous state-of-the-art methods.",arxiv
http://arxiv.org/abs/1610.09027v1,2016-10-27T22:38:05Z,2016-10-27T22:38:05Z,Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes,"Neural networks augmented with external memory have the ability to learn
algorithmic solutions to complex tasks. These models appear promising for
applications such as language modeling and machine translation. However, they
scale poorly in both space and time as the amount of memory grows --- limiting
their applicability to real-world domains. Here, we present an end-to-end
differentiable memory access scheme, which we call Sparse Access Memory (SAM),
that retains the representational power of the original approaches whilst
training efficiently with very large memories. We show that SAM achieves
asymptotic lower bounds in space and time complexity, and find that an
implementation runs $1,\!000\times$ faster and with $3,\!000\times$ less
physical memory than non-sparse models. SAM learns with comparable data
efficiency to existing models on a range of synthetic tasks and one-shot
Omniglot character recognition, and can scale to tasks requiring $100,\!000$s
of time steps and memories. As well, we show how our approach can be adapted
for models that maintain temporal associations between memories, as with the
recently introduced Differentiable Neural Computer.",arxiv
http://arxiv.org/abs/1610.07862v2,2016-10-26T02:32:30Z,2016-10-24T02:15:46Z,Intelligence in Artificial Intelligence,"The elusive quest for intelligence in artificial intelligence prompts us to
consider that instituting human-level intelligence in systems may be (still) in
the realm of utopia. In about a quarter century, we have witnessed the winter
of AI (1990) being transformed and transported to the zenith of tabloid fodder
about AI (2015). The discussion at hand is about the elements that constitute
the canonical idea of intelligence. The delivery of intelligence as a
pay-per-use-service, popping out of an app or from a shrink-wrapped software
defined point solution, is in contrast to the bio-inspired view of intelligence
as an outcome, perhaps formed from a tapestry of events, cross-pollinated by
instances, each with its own microcosm of experiences and learning, which may
not be discrete all-or-none functions but continuous, over space and time. The
enterprise world may not require, aspire or desire such an engaged solution to
improve its services for enabling digital transformation through the deployment
of digital twins, for example. One might ask whether the ""work-flow on
steroids"" version of decision support may suffice for intelligence? Are we
harking back to the era of rule based expert systems? The image conjured by the
publicity machines offers deep solutions with human-level AI and preposterous
claims about capturing the ""brain in a box"" by 2020. Even emulating insects may
be difficult in terms of real progress. Perhaps we can try to focus on worms
(Caenorhabditis elegans) which may be better suited for what business needs to
quench its thirst for so-called intelligence in AI.",arxiv
http://arxiv.org/abs/1610.04872v1,2016-10-16T15:14:36Z,2016-10-16T15:14:36Z,"Fault Detection Engine in Intelligent Predictive Analytics Platform for
  DCIM","With the advancement of huge data generation and data handling capability,
Machine Learning and Probabilistic modelling enables an immense opportunity to
employ predictive analytics platform in high security critical industries
namely data centers, electricity grids, utilities, airport etc. where downtime
minimization is one of the primary objectives. This paper proposes a novel,
complete architecture of an intelligent predictive analytics platform, Fault
Engine, for huge device network connected with electrical/information flow.
Three unique modules, here proposed, seamlessly integrate with available
technology stack of data handling and connect with middleware to produce online
intelligent prediction in critical failure scenarios. The Markov Failure module
predicts the severity of a failure along with survival probability of a device
at any given instances. The Root Cause Analysis model indicates probable
devices as potential root cause employing Bayesian probability assignment and
topological sort. Finally, a community detection algorithm produces correlated
clusters of device in terms of failure probability which will further narrow
down the search space of finding route cause. The whole Engine has been tested
with different size of network with simulated failure environments and shows
its potential to be scalable in real-time implementation.",arxiv
http://arxiv.org/abs/1609.08018v1,2016-09-26T15:15:09Z,2016-09-26T15:15:09Z,"Small near-Earth asteroids in the Palomar Transient Factory survey: A
  real-time streak-detection system","Near-Earth asteroids (NEAs) in the 1-100 meter size range are estimated to be
$\sim$1,000 times more numerous than the $\sim$15,000 currently-catalogued
NEAs, most of which are in the 0.5-10 kilometer size range. Impacts from 10-100
meter size NEAs are not statistically life-threatening but may cause
significant regional damage, while 1-10 meter size NEAs with low velocities
relative to Earth are compelling targets for space missions. We describe the
implementation and initial results of a real-time NEA-discovery system
specialized for the detection of small, high angular rate (visually-streaked)
NEAs in Palomar Transient Factory (PTF) images. PTF is a 1.2-m aperture,
7.3-deg$^2$ field-of-view optical survey designed primarily for the discovery
of extragalactic transients (e.g., supernovae) in 60-second exposures reaching
$\sim$20.5 visual magnitude. Our real-time NEA discovery pipeline uses a
machine-learned classifier to filter a large number of false-positive streak
detections, permitting a human scanner to efficiently and remotely identify
real asteroid streaks during the night. Upon recognition of a streaked NEA
detection (typically within an hour of the discovery exposure), the scanner
triggers follow-up with the same telescope and posts the observations to the
Minor Planet Center for worldwide confirmation. We describe our ten initial
confirmed discoveries, all small NEAs that passed 0.3-15 lunar distances from
Earth. Lastly, we derive useful scaling laws for comparing
streaked-NEA-detection capabilities of different surveys as a function of their
hardware and survey-pattern characteristics. This work most directly informs
estimates of the streak-detection capabilities of the Zwicky Transient Facility
(ZTF, planned to succeed PTF in 2017), which will apply PTF's current
resolution and sensitivity over a 47-deg$^2$ field-of-view.",arxiv
http://arxiv.org/abs/1609.01926v1,2016-09-07T10:44:28Z,2016-09-07T10:44:28Z,"A modular architecture for transparent computation in Recurrent Neural
  Networks","Computation is classically studied in terms of automata, formal languages and
algorithms; yet, the relation between neural dynamics and symbolic
representations and operations is still unclear in traditional eliminative
connectionism. Therefore, we suggest a unique perspective on this central
issue, to which we would like to refer as to transparent connectionism, by
proposing accounts of how symbolic computation can be implemented in neural
substrates. In this study we first introduce a new model of dynamics on a
symbolic space, the versatile shift, showing that it supports the real-time
simulation of a range of automata. We then show that the Goedelization of
versatile shifts defines nonlinear dynamical automata, dynamical systems
evolving on a vectorial space. Finally, we present a mapping between nonlinear
dynamical automata and recurrent artificial neural networks. The mapping
defines an architecture characterized by its granular modularity, where data,
symbolic operations and their control are not only distinguishable in
activation space, but also spatially localizable in the network itself, while
maintaining a distributed encoding of symbolic representations. The resulting
networks simulate automata in real-time and are programmed directly, in absence
of network training. To discuss the unique characteristics of the architecture
and their consequences, we present two examples: i) the design of a Central
Pattern Generator from a finite-state locomotive controller, and ii) the
creation of a network simulating a system of interactive automata that supports
the parsing of garden-path sentences as investigated in psycholinguistics
experiments.",arxiv
http://arxiv.org/abs/1606.03966v2,2017-05-09T14:41:15Z,2016-06-13T14:17:00Z,Making Contextual Decisions with Low Technical Debt,"Applications and systems are constantly faced with decisions that require
picking from a set of actions based on contextual information.
Reinforcement-based learning algorithms such as contextual bandits can be very
effective in these settings, but applying them in practice is fraught with
technical debt, and no general system exists that supports them completely. We
address this and create the first general system for contextual learning,
called the Decision Service.
  Existing systems often suffer from technical debt that arises from issues
like incorrect data collection and weak debuggability, issues we systematically
address through our ML methodology and system abstractions. The Decision
Service enables all aspects of contextual bandit learning using four system
abstractions which connect together in a loop: explore (the decision space),
log, learn, and deploy. Notably, our new explore and log abstractions ensure
the system produces correct, unbiased data, which our learner uses for online
learning and to enable real-time safeguards, all in a fully reproducible
manner.
  The Decision Service has a simple user interface and works with a variety of
applications: we present two live production deployments for content
recommendation that achieved click-through improvements of 25-30%, another with
18% revenue lift in the landing page, and ongoing applications in tech support
and machine failure handling. The service makes real-time decisions and learns
continuously and scalably, while significantly lowering technical debt.",arxiv
http://arxiv.org/abs/1604.05091v2,2016-04-19T14:09:26Z,2016-04-18T11:15:56Z,"End-to-End Tracking and Semantic Segmentation Using Recurrent Neural
  Networks","In this work we present a novel end-to-end framework for tracking and
classifying a robot's surroundings in complex, dynamic and only partially
observable real-world environments. The approach deploys a recurrent neural
network to filter an input stream of raw laser measurements in order to
directly infer object locations, along with their identity in both visible and
occluded areas. To achieve this we first train the network using unsupervised
Deep Tracking, a recently proposed theoretical framework for end-to-end space
occupancy prediction. We show that by learning to track on a large amount of
unsupervised data, the network creates a rich internal representation of its
environment which we in turn exploit through the principle of inductive
transfer of knowledge to perform the task of it's semantic classification. As a
result, we show that only a small amount of labelled data suffices to steer the
network towards mastering this additional task. Furthermore we propose a novel
recurrent neural network architecture specifically tailored to tracking and
semantic classification in real-world robotics applications. We demonstrate the
tracking and classification performance of the method on real-world data
collected at a busy road junction. Our evaluation shows that the proposed
end-to-end framework compares favourably to a state-of-the-art, model-free
tracking solution and that it outperforms a conventional one-shot training
scheme for semantic classification.",arxiv
http://arxiv.org/abs/1602.07188v2,2016-03-13T21:13:57Z,2016-02-23T15:17:55Z,Exploring the Neural Algorithm of Artistic Style,"We explore the method of style transfer presented in the article ""A Neural
Algorithm of Artistic Style"" by Leon A. Gatys, Alexander S. Ecker and Matthias
Bethge (arXiv:1508.06576).
  We first demonstrate the power of the suggested style space on a few
examples. We then vary different hyper-parameters and program properties that
were not discussed in the original paper, among which are the recognition
network used, starting point of the gradient descent and different ways to
partition style and content layers. We also give a brief comparison of some of
the existing algorithm implementations and deep learning frameworks used.
  To study the style space further we attempt to generate synthetic images by
maximizing a single entry in one of the Gram matrices $\mathcal{G}_l$ and some
interesting results are observed. Next, we try to mimic the sparsity and
intensity distribution of Gram matrices obtained from a real painting and
generate more complex textures.
  Finally, we propose two new style representations built on top of network's
features and discuss how one could be used to achieve local and potentially
content-aware style transfer.",arxiv
http://arxiv.org/abs/1601.04385v1,2016-01-18T02:06:45Z,2016-01-18T02:06:45Z,Real-Time Data Mining of Massive Data Streams from Synoptic Sky Surveys,"The nature of scientific and technological data collection is evolving
rapidly: data volumes and rates grow exponentially, with increasing complexity
and information content, and there has been a transition from static data sets
to data streams that must be analyzed in real time. Interesting or anomalous
phenomena must be quickly characterized and followed up with additional
measurements via optimal deployment of limited assets. Modern astronomy
presents a variety of such phenomena in the form of transient events in digital
synoptic sky surveys, including cosmic explosions (supernovae, gamma ray
bursts), relativistic phenomena (black hole formation, jets), potentially
hazardous asteroids, etc. We have been developing a set of machine learning
tools to detect, classify and plan a response to transient events for astronomy
applications, using the Catalina Real-time Transient Survey (CRTS) as a
scientific and methodological testbed. The ability to respond rapidly to the
potentially most interesting events is a key bottleneck that limits the
scientific returns from the current and anticipated synoptic sky surveys.
Similar challenge arise in other contexts, from environmental monitoring using
sensor networks to autonomous spacecraft systems. Given the exponential growth
of data rates, and the time-critical response, we need a fully automated and
robust approach. We describe the results obtained to date, and the possible
future developments.",arxiv
http://arxiv.org/abs/1512.01192v2,2018-04-25T13:40:35Z,2015-12-03T19:06:16Z,Prototypical Priors: From Improving Classification to Zero-Shot Learning,"Recent works on zero-shot learning make use of side information such as
visual attributes or natural language semantics to define the relations between
output visual classes and then use these relationships to draw inference on new
unseen classes at test time. In a novel extension to this idea, we propose the
use of visual prototypical concepts as side information. For most real-world
visual object categories, it may be difficult to establish a unique prototype.
However, in cases such as traffic signs, brand logos, flags, and even natural
language characters, these prototypical templates are available and can be
leveraged for an improved recognition performance. The present work proposes a
way to incorporate this prototypical information in a deep learning framework.
Using prototypes as prior information, the deepnet pipeline learns the input
image projections into the prototypical embedding space subject to minimization
of the final classification loss. Based on our experiments with two different
datasets of traffic signs and brand logos, prototypical embeddings incorporated
in a conventional convolutional neural network improve the recognition
performance. Recognition accuracy on the Belga logo dataset is especially
noteworthy and establishes a new state-of-the-art. In zero-shot learning
scenarios, the same system can be directly deployed to draw inference on unseen
classes by simply adding the prototypical information for these new classes at
test time. Thus, unlike earlier approaches, testing on seen and unseen classes
is handled using the same pipeline, and the system can be tuned for a trade-off
of seen and unseen class performance as per task requirement. Comparison with
one of the latest works in the zero-shot learning domain yields top results on
the two datasets mentioned above.",arxiv
http://arxiv.org/abs/1511.06201v1,2015-11-19T15:14:02Z,2015-11-19T15:14:02Z,Adjustable Bounded Rectifiers: Towards Deep Binary Representations,"Binary representation is desirable for its memory efficiency, computation
speed and robustness. In this paper, we propose adjustable bounded rectifiers
to learn binary representations for deep neural networks. While hard
constraining representations across layers to be binary makes training
unreasonably difficult, we softly encourage activations to diverge from real
values to binary by approximating step functions. Our final representation is
completely binary. We test our approach on MNIST, CIFAR10, and ILSVRC2012
dataset, and systematically study the training dynamics of the binarization
process. Our approach can binarize the last layer representation without loss
of performance and binarize all the layers with reasonably small degradations.
The memory space that it saves may allow more sophisticated models to be
deployed, thus compensating the loss. To the best of our knowledge, this is the
first work to report results on current deep network architectures using
complete binary middle representations. Given the learned representations, we
find that the firing or inhibition of a binary neuron is usually associated
with a meaningful interpretation across different classes. This suggests that
the semantic structure of a neural network may be manifested through a guided
binarization process.",arxiv
http://arxiv.org/abs/1510.02055v1,2015-10-07T18:34:36Z,2015-10-07T18:34:36Z,"Diverse Large-Scale ITS Dataset Created from Continuous Learning for
  Real-Time Vehicle Detection","In traffic engineering, vehicle detectors are trained on limited datasets
resulting in poor accuracy when deployed in real world applications. Annotating
large-scale high quality datasets is challenging. Typically, these datasets
have limited diversity; they do not reflect the real-world operating
environment. There is a need for a large-scale, cloud based positive and
negative mining (PNM) process and a large-scale learning and evaluation system
for the application of traffic event detection. The proposed positive and
negative mining process addresses the quality of crowd sourced ground truth
data through machine learning review and human feedback mechanisms. The
proposed learning and evaluation system uses a distributed cloud computing
framework to handle data-scaling issues associated with large numbers of
samples and a high-dimensional feature space. The system is trained using
AdaBoost on $1,000,000$ Haar-like features extracted from $70,000$ annotated
video frames. The trained real-time vehicle detector achieves an accuracy of at
least $95\%$ for $1/2$ and about $78\%$ for $19/20$ of the time when tested on
approximately $7,500,000$ video frames. At the end of 2015, the dataset is
expect to have over one billion annotated video frames.",arxiv
http://arxiv.org/abs/1508.00317v1,2015-08-03T05:58:52Z,2015-08-03T05:58:52Z,"Time-series modeling with undecimated fully convolutional neural
  networks","We present a new convolutional neural network-based time-series model.
Typical convolutional neural network (CNN) architectures rely on the use of
max-pooling operators in between layers, which leads to reduced resolution at
the top layers. Instead, in this work we consider a fully convolutional network
(FCN) architecture that uses causal filtering operations, and allows for the
rate of the output signal to be the same as that of the input signal. We
furthermore propose an undecimated version of the FCN, which we refer to as the
undecimated fully convolutional neural network (UFCNN), and is motivated by the
undecimated wavelet transform. Our experimental results verify that using the
undecimated version of the FCN is necessary in order to allow for effective
time-series modeling. The UFCNN has several advantages compared to other
time-series models such as the recurrent neural network (RNN) and long
short-term memory (LSTM), since it does not suffer from either the vanishing or
exploding gradients problems, and is therefore easier to train. Convolution
operations can also be implemented more efficiently compared to the recursion
that is involved in RNN-based models. We evaluate the performance of our model
in a synthetic target tracking task using bearing only measurements generated
from a state-space model, a probabilistic modeling of polyphonic music
sequences problem, and a high frequency trading task using a time-series of
ask/bid quotes and their corresponding volumes. Our experimental results using
synthetic and real datasets verify the significant advantages of the UFCNN
compared to the RNN and LSTM baselines.",arxiv
http://arxiv.org/abs/1502.07402v2,2015-06-15T13:55:15Z,2015-02-25T23:40:19Z,"Real-time capable first principle based modelling of tokamak turbulent
  transport","A real-time capable core turbulence tokamak transport model is developed.
This model is constructed from the regularized nonlinear regression of
quasilinear gyrokinetic transport code output. The regression is performed with
a multilayer perceptron neural network. The transport code input for the neural
network training set consists of five dimensions, and is limited to adiabatic
electrons. The neural network model successfully reproduces transport fluxes
predicted by the original quasilinear model, while gaining five orders of
magnitude in computation time. The model is implemented in a real-time capable
tokamak simulator, and simulates a 300s ITER discharge in 10s. This
proof-of-principle for regression based transport models anticipates a
significant widening of input space dimensionality and physics realism for
future training sets. This aims to provide unprecedented computational speed
coupled with first-principle based physics for real-time control and integrated
modelling applications.",arxiv
http://arxiv.org/abs/1410.5792v1,2014-10-21T19:17:19Z,2014-10-21T19:17:19Z,"Generalized Compression Dictionary Distance as Universal Similarity
  Measure","We present a new similarity measure based on information theoretic measures
which is superior than Normalized Compression Distance for clustering problems
and inherits the useful properties of conditional Kolmogorov complexity. We
show that Normalized Compression Dictionary Size and Normalized Compression
Dictionary Entropy are computationally more efficient, as the need to perform
the compression itself is eliminated. Also they scale linearly with exponential
vector size growth and are content independent. We show that normalized
compression dictionary distance is compressor independent, if limited to
lossless compressors, which gives space for optimizations and implementation
speed improvement for real-time and big data applications. The introduced
measure is applicable for machine learning tasks of parameter-free unsupervised
clustering, supervised learning such as classification and regression, feature
selection, and is applicable for big data problems with order of magnitude
speed increase.",arxiv
http://arxiv.org/abs/1409.0470v1,2014-09-01T16:20:41Z,2014-09-01T16:20:41Z,"Neural coordination can be enhanced by occasional interruption of normal
  firing patterns: A self-optimizing spiking neural network model","The state space of a conventional Hopfield network typically exhibits many
different attractors of which only a small subset satisfy constraints between
neurons in a globally optimal fashion. It has recently been demonstrated that
combining Hebbian learning with occasional alterations of normal neural states
avoids this problem by means of self-organized enlargement of the best basins
of attraction. However, so far it is not clear to what extent this process of
self-optimization is also operative in real brains. Here we demonstrate that it
can be transferred to more biologically plausible neural networks by
implementing a self-optimizing spiking neural network model. In addition, by
using this spiking neural network to emulate a Hopfield network with Hebbian
learning, we attempt to make a connection between rate-based and temporal
coding based neural systems. Although further work is required to make this
model more realistic, it already suggests that the efficacy of the
self-optimizing process is independent from the simplifying assumptions of a
conventional Hopfield network. We also discuss natural and cultural processes
that could be responsible for occasional alteration of neural firing patterns
in actual brains",arxiv
http://arxiv.org/abs/1407.5949v2,2014-12-18T17:04:23Z,2014-07-22T17:25:50Z,Deep Recurrent Neural Networks for Time Series Prediction,"Ability of deep networks to extract high level features and of recurrent
networks to perform time-series inference have been studied. In view of
universality of one hidden layer network at approximating functions under weak
constraints, the benefit of multiple layers is to enlarge the space of
dynamical systems approximated or, given the space, reduce the number of units
required for a certain error. Traditionally shallow networks with manually
engineered features are used, back-propagation extent is limited to one and
attempt to choose a large number of hidden units to satisfy the Markov
condition is made. In case of Markov models, it has been shown that many
systems need to be modeled as higher order. In the present work, we present
deep recurrent networks with longer backpropagation through time extent as a
solution to modeling systems that are high order and to predicting ahead. We
study epileptic seizure suppression electro-stimulator. Extraction of manually
engineered complex features and prediction employing them has not allowed small
low-power implementations as, to avoid possibility of surgery, extraction of
any features that may be required has to be included. In this solution, a
recurrent neural network performs both feature extraction and prediction. We
prove analytically that adding hidden layers or increasing backpropagation
extent increases the rate of decrease of approximation error. A Dynamic
Programming (DP) training procedure employing matrix operations is derived. DP
and use of matrix operations makes the procedure efficient particularly when
using data-parallel computing. The simulation studies show the geometry of the
parameter space, that the network learns the temporal structure, that
parameters converge while model output displays same dynamic behavior as the
system and greater than .99 Average Detection Rate on all real seizure data
tried.",arxiv
http://arxiv.org/abs/1407.3502v1,2014-07-13T19:09:30Z,2014-07-13T19:09:30Z,"Automated Real-Time Classification and Decision Making in Massive Data
  Streams from Synoptic Sky Surveys","The nature of scientific and technological data collection is evolving
rapidly: data volumes and rates grow exponentially, with increasing complexity
and information content, and there has been a transition from static data sets
to data streams that must be analyzed in real time. Interesting or anomalous
phenomena must be quickly characterized and followed up with additional
measurements via optimal deployment of limited assets. Modern astronomy
presents a variety of such phenomena in the form of transient events in digital
synoptic sky surveys, including cosmic explosions (supernovae, gamma ray
bursts), relativistic phenomena (black hole formation, jets), potentially
hazardous asteroids, etc. We have been developing a set of machine learning
tools to detect, classify and plan a response to transient events for astronomy
applications, using the Catalina Real-time Transient Survey (CRTS) as a
scientific and methodological testbed. The ability to respond rapidly to the
potentially most interesting events is a key bottleneck that limits the
scientific returns from the current and anticipated synoptic sky surveys.
Similar challenge arise in other contexts, from environmental monitoring using
sensor networks to autonomous spacecraft systems. Given the exponential growth
of data rates, and the time-critical response, we need a fully automated and
robust approach. We describe the results obtained to date, and the possible
future developments.",arxiv
http://arxiv.org/abs/1308.3015v1,2013-08-14T02:30:40Z,2013-08-14T02:30:40Z,"On Generalized Bayesian Data Fusion with Complex Models in Large Scale
  Networks","Recent advances in communications, mobile computing, and artificial
intelligence have greatly expanded the application space of intelligent
distributed sensor networks. This in turn motivates the development of
generalized Bayesian decentralized data fusion (DDF) algorithms for robust and
efficient information sharing among autonomous agents using probabilistic
belief models. However, DDF is significantly challenging to implement for
general real-world applications requiring the use of dynamic/ad hoc network
topologies and complex belief models, such as Gaussian mixtures or hybrid
Bayesian networks. To tackle these issues, we first discuss some new key
mathematical insights about exact DDF and conservative approximations to DDF.
These insights are then used to develop novel generalized DDF algorithms for
complex beliefs based on mixture pdfs and conditional factors. Numerical
examples motivated by multi-robot target search demonstrate that our methods
lead to significantly better fusion results, and thus have great potential to
enhance distributed intelligent reasoning in sensor networks.",arxiv
http://arxiv.org/abs/1212.4174v1,2012-12-17T21:43:31Z,2012-12-17T21:43:31Z,Feature Clustering for Accelerating Parallel Coordinate Descent,"Large-scale L1-regularized loss minimization problems arise in
high-dimensional applications such as compressed sensing and high-dimensional
supervised learning, including classification and regression problems.
High-performance algorithms and implementations are critical to efficiently
solving these problems. Building upon previous work on coordinate descent
algorithms for L1-regularized problems, we introduce a novel family of
algorithms called block-greedy coordinate descent that includes, as special
cases, several existing algorithms such as SCD, Greedy CD, Shotgun, and
Thread-Greedy. We give a unified convergence analysis for the family of
block-greedy algorithms. The analysis suggests that block-greedy coordinate
descent can better exploit parallelism if features are clustered so that the
maximum inner product between features in different blocks is small. Our
theoretical convergence analysis is supported with experimental re- sults using
data from diverse real-world applications. We hope that algorithmic approaches
and convergence analysis we provide will not only advance the field, but will
also encourage researchers to systematically explore the design space of
algorithms for solving large-scale L1-regularization problems.",arxiv
http://arxiv.org/abs/1208.2112v2,2013-01-21T08:12:56Z,2012-08-10T08:36:49Z,Inverse Reinforcement Learning with Gaussian Process,"We present new algorithms for inverse reinforcement learning (IRL, or inverse
optimal control) in convex optimization settings. We argue that finite-space
IRL can be posed as a convex quadratic program under a Bayesian inference
framework with the objective of maximum a posterior estimation. To deal with
problems in large or even infinite state space, we propose a Gaussian process
model and use preference graphs to represent observations of decision
trajectories. Our method is distinguished from other approaches to IRL in that
it makes no assumptions about the form of the reward function and yet it
retains the promise of computationally manageable implementations for potential
real-world applications. In comparison with an establish algorithm on
small-scale numerical problems, our method demonstrated better accuracy in
apprenticeship learning and a more robust dependence on the number of
observations.",arxiv
http://arxiv.org/abs/1201.6626v1,2012-01-31T17:26:17Z,2012-01-31T17:26:17Z,Learning RoboCup-Keepaway with Kernels,"We apply kernel-based methods to solve the difficult reinforcement learning
problem of 3vs2 keepaway in RoboCup simulated soccer. Key challenges in
keepaway are the high-dimensionality of the state space (rendering conventional
discretization-based function approximation like tilecoding infeasible), the
stochasticity due to noise and multiple learning agents needing to cooperate
(meaning that the exact dynamics of the environment are unknown) and real-time
learning (meaning that an efficient online implementation is required). We
employ the general framework of approximate policy iteration with
least-squares-based policy evaluation. As underlying function approximator we
consider the family of regularization networks with subset of regressors
approximation. The core of our proposed solution is an efficient recursive
implementation with automatic supervised selection of relevant basis functions.
Simulation results indicate that the behavior learned through our approach
clearly outperforms the best results obtained earlier with tilecoding by Stone
et al. (2005).",arxiv
http://arxiv.org/abs/0908.4564v1,2009-08-31T15:51:56Z,2009-08-31T15:51:56Z,Theory and modeling of the magnetic field measurement in LISA PathFinder,"The magnetic diagnostics subsystem of the LISA Technology Package (LTP) on
board the LISA PathFinder (LPF) spacecraft includes a set of four tri-axial
fluxgate magnetometers, intended to measure with high precision the magnetic
field at their respective positions. However, their readouts do not provide a
direct measurement of the magnetic field at the positions of the test masses,
and hence an interpolation method must be designed and implemented to obtain
the values of the magnetic field at these positions. However, such
interpolation process faces serious difficulties. Indeed, the size of the
interpolation region is excessive for a linear interpolation to be reliable
while, on the other hand, the number of magnetometer channels does not provide
sufficient data to go beyond the linear approximation. We describe an
alternative method to address this issue, by means of neural network
algorithms. The key point in this approach is the ability of neural networks to
learn from suitable training data representing the behavior of the magnetic
field. Despite the relatively large distance between the test masses and the
magnetometers, and the insufficient number of data channels, we find that our
artificial neural network algorithm is able to reduce the estimation errors of
the field and gradient down to levels below 10%, a quite satisfactory result.
Learning efficiency can be best improved by making use of data obtained in
on-ground measurements prior to mission launch in all relevant satellite
locations and in real operation conditions. Reliable information on that
appears to be essential for a meaningful assessment of magnetic noise in the
LTP.",arxiv
http://arxiv.org/abs/0908.3934v3,2010-06-22T04:05:08Z,2009-08-27T16:33:52Z,"A framework for simulating and estimating the state and functional
  topology of complex dynamic geometric networks","We present a framework for simulating signal propagation in geometric
networks (i.e. networks that can be mapped to geometric graphs in some space)
and for developing algorithms that estimate (i.e. map) the state and functional
topology of complex dynamic geometric net- works. Within the framework we
define the key features typically present in such networks and of particular
relevance to biological cellular neural networks: Dynamics, signaling,
observation, and control. The framework is particularly well-suited for
estimating functional connectivity in cellular neural networks from
experimentally observable data, and has been implemented using graphics
processing unit (GPU) high performance computing. Computationally, the
framework can simulate cellular network signaling close to or faster than real
time. We further propose a standard test set of networks to measure performance
and compare different mapping algorithms.",arxiv
http://arxiv.org/abs/math/0401157v1,2004-01-14T16:56:55Z,2004-01-14T16:56:55Z,Generalized PSK in Space Time Coding,"A wireless communication system using multiple antennas promises reliable
transmission under Rayleigh flat fading assumptions. Design criteria and
practical schemes have been presented for both coherent and non-coherent
communication channels. In this paper we generalize one dimensional phase shift
keying (PSK) signals and introduce space time constellations from generalized
phase shift keying (GPSK) signals based on the complex and real orthogonal
designs. The resulting space time constellations reallocate the energy for each
transmitting antenna and feature good diversity products, consequently their
performances are better than some of the existing comparable codes. Moreover
since the maximum likelihood (ML) decoding of our proposed codes can be
decomposed to one dimensional PSK signal demodulation, the ML decoding of our
codes can be implemented in a very efficient way.",arxiv
