id,type,publication,publisher,publication_date,database,title,url,abstract,domain
10.1016/j.egyr.2022.01.224,Journal,Energy Reports,scopus,2022-07-01,sciencedirect,Research on anomaly detection of wireless data acquisition in power system based on spark,https://api.elsevier.com/content/abstract/scopus_id/85125473240,"In the era of big data, the network data of power system is more and more complex. Due to the limitation of data storage and processing capacity, the abnormal data detection of power grid terminal information system has the problems of low accuracy and high false alarm rate. The original machine learning algorithm with good detection effect is limited by the processing capacity and storage space of the traditional platform, and the detection effect and efficiency are significantly reduced. This paper takes improving the detection accuracy of abnormal data as the main research target, and designs an abnormal data behavior analysis program based on the Internet of Things under the Spark framework combined with improved Support Vector Machine (SVM) and random forest algorithm. The parallel SA_SVM_RF anomaly data behavior detection model based on Spark is mainly studied and applied to real-time detection. Combined with the respective advantages of Internet of Things technology and machine learning in anomaly data detection, the detection capability and rate of power grid anomaly data detection model are further improved. Experimental tests show that the proposed program is superior to traditional methods in data anomaly detection efficiency and quality, and has certain research significance in the field of power grid security.",space
10.1016/j.future.2022.02.008,Journal,Future Generation Computer Systems,scopus,2022-07-01,sciencedirect,Hatch: Self-distributing systems for data centers,https://api.elsevier.com/content/abstract/scopus_id/85125223395,"Designing and maintaining distributed systems remains highly challenging: there is a high-dimensional design space of potential ways to distribute a system’s sub-components over a large-scale infrastructure; and the deployment environment for a system tends to change in unforeseen ways over time. For engineers, this is a complex prediction problem to gauge which distributed design may best suit a given environment. We present the concept of self-distributing systems, in which any local system built using our framework can learn, at runtime, the most appropriate distributed design given its perceived operating conditions. Our concept abstracts distribution of a system’s sub-components to a list of simple actions in a reward matrix of distributed design alternatives to be used by reinforcement learning algorithms. By doing this, we enable software to experiment, in a live production environment, with different ways in which to distribute its software modules by placing them in different hosts throughout the system’s infrastructure. We implement this concept in a framework we call Hatch, which has three major elements: (i) a transparent and generalized RPC layer that supports seamless relocation of any local component to a remote host during execution; (ii) a set of primitives, including relocation, replication and sharding, from which to create an action/reward matrix of possible distributed designs of a system; and (iii) a decentralized reinforcement learning approach to converge towards more optimal designs in real time. Using an example of a self-distributing web-serving infrastructure, Hatch is able to autonomously select the most suitable distributed design from among 
                        ≈
                     700,000 alternatives in about 5 min.",space
10.1016/j.jbusres.2022.02.039,Journal,Journal of Business Research,scopus,2022-05-01,sciencedirect,Multi-target CNN-LSTM regressor for predicting urban distribution of short-term food delivery demand,https://api.elsevier.com/content/abstract/scopus_id/85124904640,"The food delivery market has increased rapidly in the last few years, becoming a well-established reality in the business world and a common feature of urban life. Food delivery platforms provide the end-to-end services that connect restaurants with consumers, including the delivery service to those people ordering food through an online portal. A key component of these platforms is logistics, specifically the logistics of drivers. Ideally, the number of drivers operating in an urban area should be just the right number to serve the demand in that area. Since the demand is extremely dynamic in space and time, the spatial–temporal distribution of drivers remains a challenging problem, partially solved by means of variable incentives in different city areas at different times. In this context, a precise demand prediction would avoid a local lack of drivers in some areas, and an inefficient concentration of drivers in some other areas. For this reason, we propose a deep neural network-based methodology to forecast short-term food delivery demand distribution over urban areas. The study, carried out on a real-world dataset from a food delivery company, focuses on hourly demands and frequent prediction updates. The sequential modeling approach, designed to catch rapid changes and sudden variations beyond the general demand trend, is based on a multi-target CNN-LSTM regressor trained on location-specific time series. The methodology uses a single model for all service areas simultaneously, and a single one-step volume inference for every area at each time update. The results disclose a better performance over baselines (historical estimates for the same time-area) and more traditional statistical approaches (moving averages and univariate time-series forecasting), demonstrating a promising implementation potential within an online delivery platform framework.",space
10.1016/j.jss.2022.111231,Journal,Journal of Systems and Software,scopus,2022-05-01,sciencedirect,Discovering boundary values of feature-based machine learning classifiers through exploratory datamorphic testing,https://api.elsevier.com/content/abstract/scopus_id/85124473834,"Testing has been widely recognised as difficult for AI applications. This paper proposes a set of testing strategies for testing machine learning applications in the framework of the datamorphism testing methodology. In these strategies, testing aims at exploring the data space of a classification or clustering application to discover the boundaries between classes that the machine learning application defines. This enables the tester to understand precisely the behaviour and function of the software under test. In the paper, three variants of exploratory strategies are presented with the algorithms implemented in the automated datamorphic testing tool Morphy. The correctness of these algorithms are formally proved. Their capability and cost of discovering borders between classes are evaluated via a set of controlled experiments with manually designed subjects and a set of case studies with real machine learning models.",space
10.1016/j.ijheatmasstransfer.2021.122444,Journal,International Journal of Heat and Mass Transfer,scopus,2022-05-01,sciencedirect,A versatile inversion approach for space/temperature/time-related thermal conductivity via deep learning,https://api.elsevier.com/content/abstract/scopus_id/85122239809,"Identifying the thermophysical properties of unknown material through the measurement of temperature is of great significance in computational heat transfer. Existing numerical algorithms are generally computationally cumbersome and resource demanding. Rapid advances in deep learning (DL) offer an alternative pathway to speed up the inversion process by fully utilizing the parallel computing ability of Graphics Processing Units (GPUs). In this paper, a DL framework is proposed to reconstruct the thermal conductivity related to space, temperature or time. The whole framework consists of a forward data generation module, a denoising module and an inversion module. It is noteworthy that the physics informed neural network (PINN) is employed in the process of generating training data, which avoids the use of commercial software based on traditional methods. In order to simulate the measurement error in practical scenarios, a certain intensity of Gaussian noise is added to the generated data. After denoising by the U-net, the measured temperature is fed to the nonlinear mapping module (NMM) for the inversion of the unknown thermal conductivity. As a result, a well-trained framework can realize high precision real-time inversion even with intensive environmental noise, offering great potential for applications pertaining to the reconstruction of thermophysical properties.",space
10.1016/j.eswa.2021.116323,Journal,Expert Systems with Applications,scopus,2022-04-15,sciencedirect,Deep multi-agent reinforcement learning for multi-level preventive maintenance in manufacturing systems[Formula presented],https://api.elsevier.com/content/abstract/scopus_id/85121584320,"Designing preventive maintenance (PM) policies that ensure smooth and efficient production for large-scale manufacturing systems is non-trivial. Recent model-free reinforcement learning (RL) methods shed lights on how to cope with the non-linearity and stochasticity in such complex systems. However, the action space explosion impedes RL-based PM policies to be generalized to real applications. In order to obtain cost efficient PM policies for a serial production line that has multiple levels of PM actions, a novel multi-agent modeling is adopted to support adaptive learning by modeling each machine as cooperative agent. The evaluation of system-level production loss is leveraged to construct the reward function. An adaptive learning framework based on value-decomposition multi-agent actor–critic algorithm is utilized to obtain PM policies. In simulation study, the proposed framework demonstrates its effectiveness by leading other baselines on a comprehensive set of metrics whereas the centralized RL-based methods struggles to converge to stable policies. Our analysis further demonstrates that our multi-agent reinforcement learning based method learns effective PM policies without any knowledge about the environment and maintenance strategies.",space
10.1016/j.jobe.2021.103778,Journal,Journal of Building Engineering,scopus,2022-04-15,sciencedirect,A coupled deep learning-based internal heat gains detection and prediction method for energy-efficient office building operation,https://api.elsevier.com/content/abstract/scopus_id/85121269022,"Occupants' behaviour and the use of electrical equipment can significantly impact the building energy demand. Accurate occupancy and equipment usage information are key to improving the performance of demand-driven control, which can automatically adjust the heating, cooling and ventilation system operation. Employing static schedules is commonly used for the operation of heating, ventilation and air-conditioning systems, while it cannot satisfy the actual requirements due to the dynamic variations within the conditioned spaces. This study introduces a coupled real-time occupancy and equipment usage detection and recognition approach using deep learning and computer vision techniques for efficient building energy controls. The experimental results presented an overall equipment detection and occupancy activity detection accuracy of 78.39% and 93.60%. To investigate the influence of the implementation of the approach on building energy demand, a case study office building was selected to conduct experimental tests and modeled using a building energy simulation tool. Four scenarios with different occupancy and equipment profiles were defined and evaluated. The simulation results showed that heat gains, when employing static profiles were larger than the heat gains predicted when using the deep learning influenced profiles. Up to 53.95% lower heat gains were estimated when using both occupancy and equipment detection approaches than static schedules solely. The results highlighted the importance of monitoring real-time occupancy and electrical equipment usage and the advantages of using deep learning detection techniques to provide data for demand-driven controls, optimising building energy efficiency while maintaining a comfortable indoor environment.",space
10.1016/j.conengprac.2021.105046,Journal,Control Engineering Practice,scopus,2022-04-01,sciencedirect,Deep reinforcement learning with shallow controllers: An experimental application to PID tuning,https://api.elsevier.com/content/abstract/scopus_id/85122624409,"Deep reinforcement learning (RL) is an optimization-driven framework for producing control strategies for general dynamical systems without explicit reliance on process models. Good results have been reported in simulation. Here we demonstrate the challenges in implementing a state of the art deep RL algorithm on a real physical system. Aspects include the interplay between software and existing hardware; experiment design and sample efficiency; training subject to input constraints; and interpretability of the algorithm and control law. At the core of our approach is the use of a PID controller as the trainable RL policy. In addition to its simplicity, this approach has several appealing features: No additional hardware needs to be added to the control system, since a PID controller can easily be implemented through a standard programmable logic controller; the control law can easily be initialized in a “safe” region of the parameter space; and the final product—a well-tuned PID controller—has a form that practitioners can reason about and deploy with confidence.",space
10.1016/j.buildenv.2022.108786,Journal,Building and Environment,scopus,2022-03-15,sciencedirect,Decision Support System for technology selection based on multi-criteria ranking: Application to NZEB refurbishment,https://api.elsevier.com/content/abstract/scopus_id/85123838318,"Refurbishing existing building into Near Zero Energy Building (NZEB) is a key objective for the European Union. In order to achieve high rate of conversion, new refurbishment process must allow Decision Makers (DMs) (architects or designers) to sort through an ever increasing list of new technologies while taking into account uncertain preferences from multiple stakeholders.
                  A Decision Support System (DSS) based on Multi-Criteria Decision-Making (MCDM) approaches is proposed. The DSS enables the DMs to browse the solutions space by selecting the relevant criteria, order them by preferences and specify the granularity in the assessment of the technologies regarding each criteria.
                  This DSS is based on a ranking algorithm that operates on multiple types of quantitative (continuous, discrete, or binary) and qualitative (nominative or ordinal) variables from technological and human sources. An online user interface allows the real-time exploration of the solution space. A sensitivity analysis of the algorithm is conducted to expose the influence of the ranking algorithm parameters and to demonstrate the robustness of this algorithm. The proposed DSS is eventually implemented and validated through a use case concerning the choice of insulating materials considering heterogeneous criteria that model sustainable constraints.",space
10.1016/j.jbi.2022.103996,Journal,Journal of Biomedical Informatics,scopus,2022-03-01,sciencedirect,Evaluating pointwise reliability of machine learning prediction,https://api.elsevier.com/content/abstract/scopus_id/85123373748,"Interest in Machine Learning applications to tackle clinical and biological problems is increasing. This is driven by promising results reported in many research papers, the increasing number of AI-based software products, and by the general interest in Artificial Intelligence to solve complex problems. It is therefore of importance to improve the quality of machine learning output and add safeguards to support their adoption. In addition to regulatory and logistical strategies, a crucial aspect is to detect when a Machine Learning model is not able to generalize to new unseen instances, which may originate from a population distant to that of the training population or from an under-represented subpopulation. As a result, the prediction of the machine learning model for these instances may be often wrong, given that the model is applied outside its “reliable” space of work, leading to a decreasing trust of the final users, such as clinicians. For this reason, when a model is deployed in practice, it would be important to advise users when the model’s predictions may be unreliable, especially in high-stakes applications, including those in healthcare. Yet, reliability assessment of each machine learning prediction is still poorly addressed.
                  Here, we review approaches that can support the identification of unreliable predictions, we harmonize the notation and terminology of relevant concepts, and we highlight and extend possible interrelationships and overlap among concepts. We then demonstrate, on simulated and real data for ICU in-hospital death prediction, a possible integrative framework for the identification of reliable and unreliable predictions. To do so, our proposed approach implements two complementary principles, namely the density principle and the local fit principle. The density principle verifies that the instance we want to evaluate is similar to the training set. The local fit principle verifies that the trained model performs well on training subsets that are more similar to the instance under evaluation. Our work can contribute to consolidating work in machine learning especially in medicine.",space
10.1016/j.ins.2021.11.011,Journal,Information Sciences,scopus,2022-03-01,sciencedirect,Survival functions versus conditional aggregation-based survival functions on discrete space,https://api.elsevier.com/content/abstract/scopus_id/85122830123,In this paper we deal with conditional aggregation-based survival functions recently introduced by Boczek et al. (2020). The concept is worth to study because of its possible implementation in real-life situations and mathematical theory as well. The aim of this paper is the comparison of this new notion with the standard survival function. We state sufficient and necessary conditions under which the generalized and the standard survival function equal. The main result is the characterization of the family of conditional aggregation operators (on discrete space) for which these functions coincide.,space
10.1016/j.adhoc.2021.102757,Journal,Ad Hoc Networks,scopus,2022-03-01,sciencedirect,Deep embedded median clustering for routing misbehaviour and attacks detection in ad-hoc networks,https://api.elsevier.com/content/abstract/scopus_id/85120863584,"Due to the properties of ad-hoc networks, it appears that designing sophisticated defence schemes with more computing capital is impossible in most situations. Recently, an inconsistency in the ad-hoc design of intrusion detection in the network has gotten a lot of coverage, with these intrusion detection techniques operating in either cluster-based or host-based configurations. The host and cluster-based systems have advantages and disadvantages, such as the network preserve security in case of delay in replacing a cluster head. Many detection systems in these networks use a supervised learning method to learn from shared routing knowledge. Deep learning is the trending supervised learning method which is been suggested for many applications, due to its deep feature extraction and classification capability. The deep learning method is best suitable to resolve the problems of the ad hoc network. But due to its limitation of supervised learning nature, more research finds are needed before implementation. These intelligence methods need a massive labeled dataset to self-train and take a decision in real-time. Also, these methods will be vulnerable to new attacks. To address the issues posed, the deep learning approach requires a technique of incorporating unsupervised learning behaviours. This paper proposes and highlights the methodology - Deep Embedded Median Clustering (DEMC), which performs two-phase operations (1) Organization of latent feature space (2) K-median clustering to cluster the Z with Kullback–Leibler divergence as the objective function. Many researchers suggested various methodologies for better anomaly detection in the network, but the knowledge gap and the possibilities for a better solution still exist. This study explores the new possibility and potential of an unsupervised learning technique that works with the nature of deep learning for analyzing and detecting anomalies and intrusion in ad hoc networks. The test to check the DEMC ability has been organized, and the findings are tabulated for analysis.",space
10.1016/j.apenergy.2021.118336,Journal,Applied Energy,scopus,2022-02-15,sciencedirect,Real-time monitoring of occupancy activities and window opening within buildings using an integrated deep learning-based approach for reducing energy demand,https://api.elsevier.com/content/abstract/scopus_id/85122427364,"Occupancy behaviour in buildings can impact the energy performance and operation of heating, ventilation and air-conditioning (HVAC) systems. HVAC, which uses conventional control strategies or “fixed” setpoint schedules, could not adjust to the conditioned spaces' actual requirements, resulting in building spaces being over or under-conditioned. While the unintended opening of windows can lead to substantial heat loss and consequently raises energy consumption. To optimise building operations, it is necessary to employ solutions such as demand-driven controls, which can monitor the utilisation of indoor spaces and provide the actual thermal comfort requirements of occupants. This study presents a novel vision-based deep learning framework for occupancy activity detection and recognition including the manual window operations in buildings. A region-based Convolutional Neural Network (R-CNN) model was trained and deployed to a camera for real-time detection and recognition. Based on the field experiments conducted within a case study University building, overall accuracy of 85.63% was achieved for occupancy activity detection and 92.20% for window operation detection. Building energy simulation and various scenario-based cases were used to assess the impact of such an approach on the building energy demand and provide insights into how the proposed detection method can enable HVAC systems to respond to dynamic changes within indoor spaces. Results showed that the proposed approach could reduce the over-or under-estimation of occupancy heat gains compared with the use of “fixed” or static profiles. In addition, the approach can help alert building users or managers about windows left open unintentionally, which can reduce unnecessary ventilation heat losses. Furthermore, the approach can also predict the room CO2 concentration and advise occupants about a suitable natural ventilation strategy. The study highlighted the potential of the multi-purpose detection approach, but further development is necessary, including optimisation of the deep learning model, full integration with HVAC controls and further model training and field testing.",space
10.1016/j.comnet.2021.108616,Journal,Computer Networks,scopus,2022-02-11,sciencedirect,LightLog: A lightweight temporal convolutional network for log anomaly detection on the edge,https://api.elsevier.com/content/abstract/scopus_id/85119961419,"Log anomaly detection on edge devices is the key to enhance edge security when deploying IoT systems. Despite the success of many newly proposed deep learning based log anomaly detection methods, handling large-scale logs on edge devices is still a bottleneck due to the limited computational power on these devices to fulfil the real-time processing requirement for accurate anomaly detection. In this work, we propose a novel lightweight log anomaly detection algorithm, named LightLog, to tackle this research gap. In specific, we achieve real-time processing speed on the task via two aspects: (i) creation of a low-dimensional semantic vector space based on word2vec and post-processing algorithms (PPA); and (ii) design of a lightweight temporal convolutional network (TCN) for the detection. These two components significantly reduce the number of parameters and computations of a standard TCN while improving the detection performance. Experimental results show that our LightLog outperforms several benchmarking methods, namely DeepLog, LogAnomaly and RobustLog, by achieving 97.0 F1 score on HDFS Dataset and 97.2 F1 score on BGL with smallest model size. This effective yet efficient method paves the way to the deployment of log anomaly detection on the edge. Our source code and datasets are freely available on https://github.com/Aquariuaa/LightLog.",space
10.1016/j.vrih.2022.01.004,Journal,Virtual Reality and Intelligent Hardware,scopus,2022-02-01,sciencedirect,Virtual-reality-based digital twin of office spaces with social distance measurement feature,https://api.elsevier.com/content/abstract/scopus_id/85124517698,"Background
                  Social distancing is an effective way to reduce the spread of the SARS-CoV-2 virus. Many students and researchers have already attempted to use computer vision technology to automatically detect human beings in the field of view of a camera and help enforce social distancing. However, because of the present lockdown measures in several countries, the validation of computer vision systems using large-scale datasets is a challenge.
               
                  Methods
                  In this paper, a new method is proposed for generating customized datasets and validating deep-learning-based computer vision models using virtual reality (VR) technology. Using VR, we modeled a digital twin (DT) of an existing office space and used it to create a dataset of individuals in different postures, dresses, and locations. To test the proposed solution, we implemented a convolutional neural network (CNN) model for detecting people in a limited-sized dataset of real humans and a simulated dataset of humanoid figures.
               
                  Results
                  We detected the number of persons in both the real and synthetic datasets with more than 90% accuracy, and the actual and measured distances were significantly correlated (r=0.99). Finally, we used intermittent-layer- and heatmap-based data visualization techniques to explain the failure modes of a CNN.
               
                  Conclusions
                  A new application of DTs is proposed to enhance workplace safety by measuring the social distance between individuals. The use of our proposed pipeline along with a DT of the shared space for visualizing both environmental and human behavior aspects preserves the privacy of individuals and improves the latency of such monitoring systems because only the extracted information is streamed.",space
10.1016/j.rse.2021.112809,Journal,Remote Sensing of Environment,scopus,2022-02-01,sciencedirect,MethaNet – An AI-driven approach to quantifying methane point-source emission from high-resolution 2-D plume imagery,https://api.elsevier.com/content/abstract/scopus_id/85120521703,"Methane is one of the most important anthropogenic greenhouse gases with a significant impact on the Earth's radiation budget and tropospheric background ozone. Despite a well-constrained global budget, quantification of local and regional methane emissions has proven challenging. Recent advancements in airborne remote sensing instruments such as from the next-generation Airborne Visible/Infrared Imaging Spectrometer (AVIRIS-NG) provide 2-D observations of CH4 plume column enhancements at an unprecedented resolution of 1–5 m over large geographic areas. Quantifying an emission rate from observed plumes is a critical step for understanding local emission distributions and prioritizing mitigation efforts. However, there exists no method that can predict emission rates from detected plumes in real-time without ancillary data reliably. In order to predict methane point-source emissions directly from high resolution 2-D plume images without relying on other local measurements such as background wind speeds, we trained a convolutional neural network model called MethaNet. The training data was derived from large eddy simulations of methane plumes and realistic measurement noise over agricultural, desert and urban environments. Our model has a mean absolute percentage error for predicting unseen plumes under 17%, a significant improvement from previous methods that require wind information. Using MethaNet, a validation against a natural gas controlled-release experiment agrees to within the precision error estimate. Our results support the basis for the applicability of using deep learning techniques to quantify CH4 point sources in an automated manner over large geographical areas, not only for present and future airborne field campaigns but also for upcoming space-based observations in this decade.",space
10.1016/j.cose.2021.102547,Journal,Computers and Security,scopus,2022-02-01,sciencedirect,Jadeite: A novel image-behavior-based approach for Java malware detection using deep learning,https://api.elsevier.com/content/abstract/scopus_id/85120335865,"Java malware exploiting language vulnerabilities has become increasingly prevalent in the recent past. Since Java is a platform-independent language, these security threats open up the opportunity for multi-platform exploitation. Although security researchers continuously develop different approaches for protecting against Java malware programs, the presence of complicated Java malware properties, such as code obfuscation, makes these malware programs fly under the radar. These challenges present the need to develop new approaches that are resilient to such properties. This article presents Jadeite, a novel approach for detecting Java bytecode malware programs using static analysis and recent advancements in the image-based, deep-learning classification space. In particular, Jadeite extracts the Interprocedural Control Flow Graph (ICFG) from a given Java bytecode file and then prunes the ICFG and converts it into an adjacency matrix. Finally, Jadeite constructs a grayscale image from this matrix. We leverage an object detection algorithm in a deep Convolutional Neural Network (CNN) classifier to determine maliciousness. Also, Jadeite extracts an additional set of features from the Java malware program to improve the accuracy of malware classification. These features are consolidated with the extracted images and used as inputs to the CNN classifier. Experimental results demonstrate that Jadeite achieves high accuracy (98.4%) compared to other Java malware detection approaches and is capable of detecting both known and previously-unseen real-world malicious Java programs.",space
10.1016/j.scs.2021.103559,Journal,Sustainable Cities and Society,scopus,2022-02-01,sciencedirect,Assessment of sustainable development objectives in Smart Labs: technology and sustainability at the service of society,https://api.elsevier.com/content/abstract/scopus_id/85120052266,"Sustainable development is the working basis of engineering research and cities are becoming increasingly flexible, inclusive and intelligent. In this context, there is a need for environments that emulate real-life spaces in which cutting-edge technologies can be implemented for subsequent deployment in society. Smart Labs or Living Labs are spaces for innovation, research and experimentation that integrate systems, devices and methodologies focused on people and their environments. The technologies studied and developed in such labs can then be deployed in human spaces to provide intelligence, comfort, health and sustainability. Health and wellness, energy and environment, artificial intelligence, big data and digital rights are some of the disciplines being studied. At the same time, the UN 2030 Agenda provides a comprehensive framework to promote human well-being through the Sustainable Development Goals. In this work, an evaluation model of its indicators in smart environments is performed through a mixed review methodology. The objective of this work is the analysis and implementation of the SDGs in Smart Labs through a literature review and a case study of UJAmI, the smart laboratory of the University of Jaén. The results provide quantitative and qualitative data on the present and future of the smart devices implemented in the UJAmI lab, providing a roadmap for future developments.",space
10.1016/j.inffus.2021.09.004,Journal,Information Fusion,scopus,2022-02-01,sciencedirect,Multimodal Earth observation data fusion: Graph-based approach in shared latent space,https://api.elsevier.com/content/abstract/scopus_id/85115401406,"Multiple and heterogenous Earth observation (EO) platforms are broadly used for a wide array of applications, and the integration of these diverse modalities facilitates better extraction of information than using them individually. The detection capability of the multispectral unmanned aerial vehicle (UAV) and satellite imagery can be significantly improved by fusing with ground hyperspectral data. However, variability in spatial and spectral resolution can affect the efficiency of such dataset's fusion. In this study, to address the modality bias, the input data was projected to a shared latent space using cross-modal generative approaches or guided unsupervised transformation. The proposed adversarial networks and variational encoder-based strategies used bi-directional transformations to model the cross-domain correlation without using cross-domain correspondence. It may be noted that an interpolation-based convolution was adopted instead of the normal convolution for learning the features of the point spectral data (ground spectra). The proposed generative adversarial network-based approach employed dynamic time wrapping based layers along with a cyclic consistency constraint to use the minimal number of unlabeled samples, having cross-domain correlation, to compute a cross-modal generative latent space. The proposed variational encoder-based transformation also addressed the cross-modal resolution differences and limited availability of cross-domain samples by using a mixture of expert-based strategy, cross-domain constraints, and adversarial learning. In addition, the latent space was modelled to be composed of modality independent and modality dependent spaces, thereby further reducing the requirement of training samples and addressing the cross-modality biases. An unsupervised covariance guided transformation was also proposed to transform the labelled samples without using cross-domain correlation prior. The proposed latent space transformation approaches resolved the requirement of cross-domain samples which has been a critical issue with the fusion of multi-modal Earth observation data. This study also proposed a latent graph generation and graph convolutional approach to predict the labels resolving the domain discrepancy and cross-modality biases. Based on the experiments over different standard benchmark airborne datasets and real-world UAV datasets, the developed approaches outperformed the prominent hyperspectral panchromatic sharpening, image fusion, and domain adaptation approaches. By using specific constraints and regularizations, the network developed was less sensitive to network parameters, unlike in similar implementations. The proposed approach illustrated improved generalizability in comparison with the prominent existing approaches. In addition to the fusion-based classification of the multispectral and hyperspectral datasets, the proposed approach was extended to the classification of hyperspectral airborne datasets where the latent graph generation and convolution were employed to resolve the domain bias with a small number of training samples. Overall, the developed transformations and architectures will be useful for the semantic interpretation and analysis of multimodal data and are applicable to signal processing, manifold learning, video analysis, data mining, and time series analysis, to name a few.",space
10.1016/j.comcom.2021.10.037,Journal,Computer Communications,scopus,2022-01-15,sciencedirect,SAAS parallel task scheduling based on cloud service flow load algorithm,https://api.elsevier.com/content/abstract/scopus_id/85120359168,"In cloud platform applications, the user’s goal is to obtain high-quality application services, while the service provider’s goal is to obtain revenue by performing the tasks submitted by the user. The platform built by the service provider’s application resources needs to improve the mapping between service requests and resources to achieve higher value. Through the current situation of resource management in the cloud environment, it is found that many task scheduling and resource allocation algorithms are still affected by factors such as the diversity, dynamics, and multiple constraints of resources and tasks. This paper focuses on Software as a Service (SaaS) applications’ task scheduling and resource configuration in a dynamic and uncertain cloud environment. It is a challenging online scheduling problem to automatically and intelligently allocate user task requests that continually reach SaaS applications to appropriate resources for execution. To this end, a real-time task scheduling method based on deep reinforcement learning is proposed, which automatically and intelligently allocates user task requests that continually reach SaaS applications to appropriate resources for execution. In this way, the limited virtual machine resources rented by SaaS providers can be used in a balanced and efficient manner. In the experiment, by comparing with other five task scheduling algorithms, it is proved that the algorithm proposed in this paper not only improves the execution efficiency of better deploying workflow in IaaS public cloud, but also makes the resources provided by SaaS are used in a balanced and efficient manner.",space
10.1016/j.apacoust.2021.108439,Journal,Applied Acoustics,scopus,2022-01-15,sciencedirect,"CAMNet: A controllable acoustic model for efficient, expressive, high-quality text-to-speech",https://api.elsevier.com/content/abstract/scopus_id/85116891718,"Spoken language is becoming one of the key components of human–machine interaction, both to send information to the machine – e.g. voice control – and to receive from it – e.g. virtual assistants. In this scenario, text-to-speech (TTS) models have become an essential artificial intelligence capacity. Even though this interaction can be based on neutral style speech, generating speech with different styles, pitches and speaking rates may improve user experience. With this in view, this paper presents CAMNet, a controllable acoustic model for efficient, expressive, high-quality TTS. CAMNet is based on deep convolutional TTS (DCTTS), a state-of-art acoustic model which is efficient and produces neutral speech. DCTTS was first adapted to generate Bark cepstrum acoustic features in order to integrate well with the LPCNet (linear prediction coefficient) neural vocoder and to remove the reduction factor which demanded the presence of an upsampling network before the vocoder – i.e. the CAMNet output can be directly fed into LPCNet. Next, style transfer functionality was added by means of a novel characterisation of the prosodic information from the Bark cepstrum acoustic features and a new approach to inject this information into the convolutional layers. Finally, controllability is provided via a variational auto-encoder module which creates a smoothed disentangled latent space which allows interpolation and extrapolation of reference styles as well as independent and simultaneous control of two generative factors: pitch and speaking rate. Moreover, this controllability is implemented using a simple offset-based approach. To sum up, CAMNet is an efficient acoustic model which provides a simple but consistent controllability on coarse-grained expression, pitch and speaking rate while still providing high-quality synthesised speech.",space
10.1016/j.neucom.2021.10.004,Journal,Neurocomputing,scopus,2022-01-11,sciencedirect,A CNN-based policy for optimizing continuous action control by learning state sequences,https://api.elsevier.com/content/abstract/scopus_id/85118138176,"Continuous action control is widespread in real-world applications. It controls an agent to take action in continuous space for transiting from one state to another until achieving the desired goal. The optimization of continuous action control is an important issue, which aims to find the optimal policy for the agent to achieve the desired goal with the lowest consumption in continuous action space. A useful tool for this issue is reinforcement learning where an optimal policy is learned for the agent by maximizing the cumulative reward of the state transitions. When updating the policy at each state, most existing reinforcement learning methods consider only the one-step transition of this state. However, for each state in continuous action control, the recognizable information is usually hidden in the sequence of its previous states, thus these methods cannot learn the policy effectively enough for continuous action control. In this paper, we propose a new policy, called convolutional deterministic policy, to solve this problem. Enlightened from the convolutional neural networks used in natural language processing, our convolutional deterministic policy uses convolutional neural networks to learn the recognizable information in the state sequences. Then for each collected state, we update the convolutional deterministic policy by not only the recognizable information in the one-step transition of this state but also the recognizable information in the sequence of its previous states. As a result, our convolutional deterministic policy can make the agent take better action. Based on an effective reinforcement learning method, TD3, the implementation of our convolutional deterministic policy is in CTD3. The theoretical analysis and the experiment illustrate that our CTD3 can learn the policy not only better than but also faster than the existing RL methods for continuous action control. The source code can be downloaded from https://github.com/grcai.",space
10.1016/j.knosys.2021.107624,Journal,Knowledge-Based Systems,scopus,2022-01-10,sciencedirect,Manifold-based aggregation clustering for unsupervised vehicle re-identification,https://api.elsevier.com/content/abstract/scopus_id/85118363031,"Most vehicle re-identification (V-reID) approaches are based on supervised learning methods which require a considerable amount of tedious and impractical annotations. In this paper, we propose a novel unsupervised V-reID approach based on Manifold-based Aggregation Clustering (MAC) with the unknown number of clusters. The proposed MAC is implemented by alternatively conducting two modules, i.e., deep feature learning module and aggregation clustering module. Specifically, deep feature learning module is responsible for training a convolutional neural network to encourage deep features to be close to the centroids of corresponding clusters which are yielded by an aggregation clustering mechanism based on manifold distance in the feature space. Moreover, the classification-agglomeration loss and manifold-based seeds searching criterion are proposed to improve the discriminative power of the learned features and deal with the problem of varied visual appearance respectively. Note that both annotations and even the certain number of vehicle identities are unknown for the proposed method, which is totally consistent with the real-world unsupervised V-reID condition. Extensive experiments on VehicleID and Veri-776 benchmark datasets show that the proposed method outperforms the state-of-the-art unsupervised V-reID approaches.",space
10.1016/j.engappai.2021.104514,Journal,Engineering Applications of Artificial Intelligence,scopus,2022-01-01,sciencedirect,Instance-based defense against adversarial attacks in Deep Reinforcement Learning,https://api.elsevier.com/content/abstract/scopus_id/85118104144,"Deep Reinforcement Learning systems are now a hot topic in Machine Learning for their effectiveness in many complex tasks, but their application in safety-critical domains (e.g., robot control or self-autonomous driving) remains dangerous without mechanism to detect and prevent risk situations. In Deep RL, such risk is mostly in the form of adversarial attacks, which introduce small perturbations to sensor inputs with the aim of changing the network-based decisions and thus cause catastrophic situations. In the light of these dangers, a promising line of research is that of providing these Deep RL algorithms with suitable defenses, especially when deploying in real environments. This paper suggests that this line of research could be greatly improved by the concepts from the existing research field of Safe Reinforcement Learning, which has been postulated as a family of RL algorithms capable of providing defenses against many forms of risks. However, the connections between Safe RL and the design of defenses against adversarial attacks in Deep RL remain largely unexplored. This paper seeks to explore precisely some of these connections. In particular, this paper proposes to reuse some of the concepts from existing Safe RL algorithms to create a novel and effective instance-based defense for the deployment stage of Deep RL policies. The proposed algorithm uses a risk function based on how far a state is from the state space known by the agent, that allows identifying and preventing adversarial situations. The success of the proposed defense has been evaluated in 4 Atari games.",space
10.1016/j.bspc.2021.103245,Journal,Biomedical Signal Processing and Control,scopus,2022-01-01,sciencedirect,Prototype design for bidirectional control of stepper motor using features of brain signals and soft computing tools,https://api.elsevier.com/content/abstract/scopus_id/85117841558,"The brain-computer interface (BCI) plays a significant role in supporting specially-abled people to control devices with the brain signals or electroencephalogram (EEG). The proper functioning of BCI systems requires classification algorithms to distinguish between different tasks based on the features extracted from EEG. Motivated by the requirement of designing a reliable BCI with reduced memory and computational cost, this work proposes an EEG controlled stepper motor drive-based aiding device. The drive system is executed in real-time based on the processing and classification of EEG. The scheme initiates with the extraction of discriminatory features from raw time-domain EEG signals using higher-order statistics (HOS) and phase locking value (PLV).
                  Further, with extracted feature vector, the present study contemplates the operation of backpropagation neural network (BPNN), k-Nearest neighbours (k-NN), and support vector machine (SVM). Signal classification by SVM in conjugation with nonlinear principal component analysis (NLPCA) is implemented to reduce the feature vector dimension. Average classification accuracy of 82.70% and 80.46% is achieved using NLPCA with SVM for PLV and HOS features. The classifier output is utilized to spin the stepper motor clockwise and anticlockwise as needed. The control of stepper motor uses the classifier output and hence the brain signals are implemented on a laboratory-developed digital test bench comprising of TI TMS320F28379D launchpad DSP board and MITSUMI stepper motor. The validation of the proposed scheme for different signals reflects its effectiveness in combining the software and hardware aspects required for realizing an actual BCI system for real-time settings.",space
10.1016/j.energy.2021.121873,Journal,Energy,scopus,2022-01-01,sciencedirect,Real-time optimal energy management of microgrid with uncertainties based on deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85115035192,"Microgrid (MG) is an effective way to integrate renewable energy into power system at the consumer side. In the MG, the energy management system (EMS) is necessary to be deployed to realize efficient utilization and stable operation. To help the EMS make optimal schedule decisions, we proposed a real-time dynamic optimal energy management (OEM) based on deep reinforcement learning (DRL) algorithm. Traditionally, the OEM problem is solved by mathematical programming (MP) or heuristic algorithms, which may lead to low computation accuracy or efficiency. While for the proposed DRL algorithm, the MG-OEM is formulated as a Markov decision process (MDP) considering environment uncertainties, and then solved by the PPO algorithm. The PPO is a novel policy-based DRL algorithm with continuous state and action spaces, which includes two phases: offline training and online operation. In the training process, the PPO can learn from historical data to capture the uncertainty characteristic of renewable energy generation and load consumption. Finally, the case study demonstrates the effectiveness and the computation efficiency of the proposed method.",space
10.1016/j.comnet.2021.108560,Journal,Computer Networks,scopus,2021-12-24,sciencedirect,Distributed scheduling method for multiple workflows with parallelism prediction and DAG prioritizing for time constrained cloud applications,https://api.elsevier.com/content/abstract/scopus_id/85118887015,"Fog computing is an emerging popular paradigm that extends the availability of resources to the network's edge in order to improve the quality metrics of existing Cloud-based applications. However, scheduling workflow applications with time-constraints are complex regarding the count of resources, physical topology of clusters, and the structure of the task graph of the workflows. Adding Fog resources to the intricate problem space of Cloud-based scheduling needs even more time-consuming and complicated algorithms. In this paper, a multi-criteria Mamdani fuzzy algorithm is proposed to analyze the workflow graphs with the assistance of a Long-Short Term Memory neural network parallelism prediction module. The group-based priority assignment schema performed by the fuzzy inference system assigns a priority value to workflows to indicate the relative precedence of requests. Distributed schedulers then send the workflows to target sites according to their current workloads. The whole process is performed in a decentralized manner to prevent any bottlenecks. We have used an extensive software simulation study to compare the proposed algorithm in real workloads with two recent and notable algorithms. The simulation results confirm the proposed algorithm's superiority in fulfilling time-constraints, resource utilization, and overall application scheduling success rate.",space
10.1016/j.softx.2021.100854,Journal,SoftwareX,scopus,2021-12-01,sciencedirect,MicroVIP: Microscopy image simulation on the Virtual Imaging Platform,https://api.elsevier.com/content/abstract/scopus_id/85119052033,"MicroVIP is an open source software that assembles, in a unified web-application running on distributed computing resources, simulators of the main fluorescent microscopy imaging modalities (with existing codes or newly developed). MicroVIP provides realistic simulated images including several sources of noise (microfluidic blur effect, diffraction, Poisson noise, camera read out noise). MicroVIP also includes a module which simulates single cells with fluorescent markers and a module to analyze the simulated images with textural and pointillist feature spaces. MicroVIP is shown to be of value for supervised machine learning. It allow to automatically generate large sets of training images and virtual instrumentation to optimize the optical parameters before realizing real experiments.",space
10.1016/j.compeleceng.2021.107567,Journal,Computers and Electrical Engineering,scopus,2021-12-01,sciencedirect,Predicting activities of daily living via temporal point processes: Approaches and experimental results,https://api.elsevier.com/content/abstract/scopus_id/85118746684,"Activity Prediction is foreseeing the following activity people are going to execute. This is a crucial task in smart home environments, i.e., in order to facilitate the daily routines of elderly people with or without special needs. In this paper, we focused on Activity Daily Living prediction and we proposed a novel activity prediction technique based on the combination of Marked Temporal Point Processes and Neural Networks. Experiments on real and synthetic smart space datasets have shown that our approach is able to conveniently represent and predict daily living activities in an unsupervised way. We evaluated its performance and compared its results with state-of-the-art methods providing freely available implementations. Noticeably, the proposed approach outperforms the best concurrent algorithm by obtaining an improvement of F1-score of 60% (on average of the considered datasets).",space
10.1016/j.pss.2021.105371,Journal,Planetary and Space Science,scopus,2021-12-01,sciencedirect,Terrain classification-based rover traverse planner with kinematic constraints for Mars exploration,https://api.elsevier.com/content/abstract/scopus_id/85118490703,"In Mars exploration mission, the rover path planning is essential to ensure the safety and efficiency of the rover traverse. At present, traverse planning deployed Mars rovers requires experienced geologists and planetary scientists to spend a lot of time on surface-topography analysis. To a large extent, this is a costly and time-consuming manual process. Meanwhile, traditional optimal search algorithms (A∗ and Dijkstra) can search for a globally optimal path, but the path found does not satisfy the nonholonomic constraint of the vehicle, thus not directly drivable. The Terrain Classification-based Rover Traverse Planner (TCRTP), introduced in this article, is an automated algorithm that uses deep learning-based terrain classification as the guide to generate the optimal traverses with kinematic constraints. TCRTP combines the terrain classification with nonholonomic constraints, and it can quickly and accurately obtain the global optimal solution under the heuristic function. We also propose modifications of the cost function for path optimization to improve the smoothness of the solution. This algorithm plans forward and reverse movement, and penalizes reverse driving and changing the direction of movement. The TCRTP algorithm is easy to be extended, and the optimization traversal under multiple missions can be realized by adding traffic constraints, such as terrain roughness, elevation changes and so on. The TCRTP-based rover allows for real-time and frequent planning, and the addition of terrain constraints ensures the safety of the planned route. Our planner, combined with computer vision, shows the potential to reduce operator workloads and explore future planetary space.",space
10.1016/j.eswa.2021.115498,Journal,Expert Systems with Applications,scopus,2021-12-01,sciencedirect,Real-time human pose estimation on a smart walker using convolutional neural networks,https://api.elsevier.com/content/abstract/scopus_id/85109217957,"Rehabilitation is important to improve quality of life for mobility-impaired patients. Smart walkers are a commonly used solution that should embed automatic and objective tools for data-driven human-in-the-loop control and monitoring. However, present solutions focus on extracting few specific metrics from dedicated sensors with no unified full-body approach. We investigate a general, real-time, full-body pose estimation framework based on two RGB+D camera streams with non-overlapping views mounted on a smart walker equipment used in rehabilitation. Human keypoint estimation is performed using a two-stage neural network framework. The 2D-Stage implements a detection module that locates body keypoints in the 2D image frames. The 3D-Stage implements a regression module that lifts and relates the detected keypoints in both cameras to the 3D space relative to the walker. Model predictions are low-pass filtered to improve temporal consistency. A custom acquisition method was used to obtain a dataset, with 14 healthy subjects, used for training and evaluating the proposed framework offline, which was then deployed on the real walker equipment. An overall keypoint detection error of 3.73 pixels for the 2D-Stage and 44.05 mm for the 3D-Stage were reported, with an inference time of 26.6 ms when deployed on the constrained hardware of the walker. We present a novel approach to patient monitoring and data-driven human-in-the-loop control in the context of smart walkers. It is able to extract a complete and compact body representation in real-time and from inexpensive sensors, serving as a common base for downstream metrics extraction solutions, and Human-Robot interaction applications. Despite promising results, more data should be collected on users with impairments, to assess its performance as a rehabilitation tool in real-world scenarios.",space
10.1016/j.cie.2021.107621,Journal,Computers and Industrial Engineering,scopus,2021-11-01,sciencedirect,Deep deterministic policy gradient algorithm for crowd-evacuation path planning,https://api.elsevier.com/content/abstract/scopus_id/85113275288,"In existing evacuation methods, the large number of pedestrians and the complex environment will affect the efficiency of evacuation. Therefore, we propose a hierarchical evacuation method based on multi-agent deep reinforcement learning (MADRL) to solve the above problem. First, we use a two-level evacuation mechanism to guide evacuations, the crowd is divided into leaders and followers. Second, in the upper level, leaders perform path planning to guide the evacuation. To obtain the best evacuation path, we propose the efficient multi-agent deep deterministic policy gradient (E-MADDPG) algorithm for crowd-evacuation path planning. E-MADDPG algorithm combines learning curves to improve the fixed experience pool of MADDPG algorithm and uses high-priority experience playback strategy to improve the sampling strategy. The improvement increases the learning efficiency of the algorithm. Meanwhile we extract pedestrian motion trajectories from real motion videos to reduce the state space of algorithm. Third, in the bottom layer, followers use the relative velocity obstacle (RVO) algorithm to avoid collisions and follow leaders to evacuate. Finally, experimental results illustrate that the E-MADDPG algorithm can improve path planning efficiency, while the proposed method can improve the efficiency of crowd evacuation.",space
10.1016/j.actaastro.2021.08.002,Journal,Acta Astronautica,scopus,2021-11-01,sciencedirect,Improved orbit predictions using two-line elements through error pattern mining and transferring,https://api.elsevier.com/content/abstract/scopus_id/85112129954,"As the sole orbit data source of space objects for public access, the NORAD catalog provides the basic orbital information in the form of two-line element (TLE) for various space tasks. But the accuracy of orbit prediction (OP) using TLE with the analytical Simplified General Perturbations-4 (SGP4) propagator drops quickly with time, especially for low orbits, significantly limiting the capability of TLEs for advanced space situational awareness (SSA) applications. To enhance the TLE performance over long-duration OP, this paper proposes a data-driven method for improved TLE-based orbit predictions through mining and transferring the orbit error patterns. Two state-of-the-art learning methods, the gradient boosting decision tree (GBDT) and convolutional neural networks (CNN), are applied to model the underlying error patterns, and then the learned models are used as an error corrector to modify the future orbit predictions. Experimental results demonstrate that the time-varying orbit error patterns in the past can be captured by the developed learning framework. As a result, the accuracy of orbit predictions over the future 14 days can be improved by more than 75% in the along-track direction and 90% in the cross-track and radial directions, when the model-predicted errors are used. It also shows that the error pattern learning and application process is computationally efficient, and it could be implemented for near real-time applications. This study demonstrates the promising potential of machine learning (ML)/deep learning (DL) for enhanced SSA capability with the publicly available TLEs.",space
10.1016/j.apenergy.2021.117504,Journal,Applied Energy,scopus,2021-11-01,sciencedirect,Deep reinforcement learning control of electric vehicle charging in the presence of photovoltaic generation,https://api.elsevier.com/content/abstract/scopus_id/85111920114,"In recent years, the importance of electric mobility has increased in response to climate change. The fast-growing deployment of electric vehicles (EVs) worldwide is expected to decrease transportation-related 
                        
                           C
                           
                              
                                 O
                              
                              
                                 2
                              
                           
                        
                      emissions, facilitate the integration of renewables, and support the grid through demand–response services. Simultaneously, inadequate EV charging patterns can lead to undesirable effects in grid operation, such as high peak-loads or low self-consumption of solar electricity, thus calling for novel methods of control. This work focuses on applying deep reinforcement learning (RL) to the EV charging control problem with the objectives to increase photovoltaic self-consumption and EV state of charge at departure. Particularly, we propose mathematical formulations of environments with discrete, continuous, and parametrized action spaces and respective deep RL algorithms to resolve them. The benchmarking of the deep RL control against naive, rule-based, deterministic optimization, and model-predictive control demonstrates that the suggested methodology can produce consistent and employable EV charging strategies, while its performance holds a great promise for real-time implementations.",space
10.1016/j.actaastro.2021.07.012,Journal,Acta Astronautica,scopus,2021-11-01,sciencedirect,"A review of space surgery - What have we achieved, current challenges, and future prospects",https://api.elsevier.com/content/abstract/scopus_id/85110745640,"Major surgical events/incidents onboard are rare but can be catastrophic to any mission. National Aeronautics and Space Administration (NASA) uses the Integrated Medical Model (IMM) to develop an integrated, quantified, evidence-based decision support tool useful for crew health and mission planners to assess risk and design medical systems. In 2017, the IMM of the NASA Human Research Program included a list of 100 medical conditions that could be anticipated during space flight. Of those conditions, 27 are expected to need surgical treatment. Consequently, there has been a continuing interest in surgical capabilities for exploration space flight. The surgical system capabilities aboard all space stations and analogue flights have been designed and implemented with an emphasis on stabilisation, medical evacuation, and ATLS capabilities. However, with future missions to the Moon and Mars, evacuation is not a possibility and astronauts will need to troubleshoot, adapt, and self-administer complex surgical care autonomously.
                  This narrative review aims to examine the published work on surgical care in space, discuss the inherent challenges, and identify scope for future studies. The review evaluates and analyses results from several landmark experiments covering important technical aspects such as basic surgical skills, laparoscopic surgery, robotic surgery, and tele surgery. Relevant studies for the review were identified from the MEDLINE, PubMed, and EMBASE databases. Eligible studies were published between 1960 and June 2021 and were identified using the terms “space surgery”, “microgravity”, “zero gravity”, “weightlessness”, “parabolic flight”, “neutral buoyancy”, and “spaceflight”. Only articles in English were selected and references cited in the selected publications were followed up and included where appropriate. Documents available in the public domain and/or archives of National Space agencies were also included. The search yielded a total of 86 hits including review articles, commentaries, studies, meeting summaries and technical reports submitted to National Space agencies. Results were then filtered for eligible papers relevant to this narrative review. Challenges on a long-duration mission will be unique, unlike anything we have faced so far in the last 60 years of space travel. Despite the progress in space surgery in the last 40 years, there are several challenges to achieving a fully functional surgical care system on any mission outside Low Earth Orbit. The microgravity environment presents unique challenges related to altered physiology as well as mechanics and techniques pertinent to surgical care. Some of the challenges include but are not limited to crew selection, role of prophylactic surgery, adaptation to zero gravity, lack of ground support, training and maintenance of surgical skills and limitation of weight and volume for hardware. Ultrasound imaging, 3D printing and AI-based surgical assistance coupled with robotic surgery have shown promise, but their real efficacy and functionality remains to be tested.",space
10.1016/j.renene.2021.05.155,Journal,Renewable Energy,scopus,2021-11-01,sciencedirect,A deep learning approach towards the detection and recognition of opening of windows for effective management of building ventilation heat losses and reducing space heating demand,https://api.elsevier.com/content/abstract/scopus_id/85107941088,"Building ventilation accounts for up to 30% of the heat loss in commercial buildings and 25% in industrial buildings. To effectively aid the reduction of energy consumption in the building sector, the development of demand-driven control systems for heating ventilation and air-conditioning (HVAC) is necessary. In countries with temperate climates such as the UK, many buildings depend on natural ventilation strategies such as openable windows, which are useful for reducing overheating prevalence during the summer. The manual opening and adjustment of windows by occupants, particularly during the heating season, can lead to substantial heat loss and consequent energy consumption. This could also result in the unnecessary or over ventilation of the space, or the fresh air is more than what is required to ensure adequate air quality. Furthermore, energy losses build up when windows are left open for extended periods. Hence, it is important to develop control strategies that can detect and recognise the period and amount of window opening in real-time and at the same time adjust the HVAC systems to minimise energy wastage and maintain indoor environment quality and thermal comfort. This paper presents a vision-based deep learning framework for the detection and recognition of manual window operation in buildings. A trained deep learning model is deployed into an artificial intelligence-powered camera. To assess the proposed strategy's capabilities, building energy simulation was used with various operation profiles of the opening of the windows based on various scenarios. Initial experimental tests were conducted within a university lecture room with a south-facing window. Deep learning influenced profile (DLIP) was generated via the framework, which uses real-time window detection and recognition data. The generated DLIP were compared with the actual observations, and the initial detection results showed that the method was capable of identifying windows that were opened and had an average accuracy of 97.29%. The results for the three scenarios showed that the proposed strategy could potentially be used to help adjust the HVAC setpoint or alert the occupants or building managers to prevent unnecessary heating demand. Further developments include enhancing the framework ability to detect multiple window opening types and sizes and the detection accuracy by optimising the model.",space
10.1016/j.ymssp.2021.107915,Journal,Mechanical Systems and Signal Processing,scopus,2021-11-01,sciencedirect,Machine learning based frequency modelling,https://api.elsevier.com/content/abstract/scopus_id/85103975336,"Detection of cracks in structures has always been an important research topic in the industrial domain closely associated with aerospace, mechanical, marine and civil engineering. The presence of the cracks alters the dynamic response properties. Hence, it becomes crucial to locate these cracks in the structures to avoid any catastrophic failures and maintain structural integrity and performance. The study's objective is to propose two distinct statistical procedures for conducting the machine learning experiment for modelling the frequency and show the effect of experiment design on the results. In the study, the predictive performance of machine learning models and their ensembles is compared within each experiment design and between two experimental designs for the task of prediction of first six natural frequencies of a fixed ended cracked beam. The study highlights the significance of more than one experimental design to reduce the confirmation bias in the research and discusses the proposed methods' generalizability over the different modelling constraints and modelling parameters. The study also discusses a real-world implementation of the learned machine learning models from the perspective of Bayesian optimization.",space
10.1016/j.probengmech.2021.103173,Journal,Probabilistic Engineering Mechanics,scopus,2021-10-01,sciencedirect,Machine learning based digital twin for stochastic nonlinear multi-degree of freedom dynamical system,https://api.elsevier.com/content/abstract/scopus_id/85117922944,"The potential of digital twin technology is immense, specifically in the infrastructure, aerospace, and automotive sector. However, practical implementation of this technology is not at an expected speed, specifically because of lack of application-specific details. In this paper, we propose a novel digital twin framework for stochastic nonlinear multi-degree of freedom (MDOF) dynamical systems. The proposed digital twin has four modules — (a) a physics-based nominal model, (b) a data collection module, (c) algorithm for real-time update of the digital twin and (d) module for predicting future state. The modules for real-time update and prediction are based on the so-called gray-box modeling approach, and utilizes both physics based and data driven frameworks; this enables the proposed digital twin to generalize and predict future responses. The gray box modeling framework used within the digital twin is developed by coupling Bayesian filtering and machine learning algorithm. Although, the proposed digital twin can be used with any machine learning regression algorithm, we have used Gaussian process in this study. Performance of the proposed approach is illustrated using two examples. Results obtained indicate the applicability and excellent performance of the proposed digital twin framework.",space
10.1016/j.ins.2021.06.008,Journal,Information Sciences,scopus,2021-10-01,sciencedirect,Discriminative group-sparsity constrained broad learning system for visual recognition,https://api.elsevier.com/content/abstract/scopus_id/85113805302,"Broad Learning System (BLS) is an emerging network paradigm that has received considerable attention in the regression and classification fields. However, there are two deficiencies which seriously hinder its deployment in real applications. The first one is the internal correlations among samples are not fully considered in the modeling process. Second, the strict binary label matrix utilized in BLS provides little freedom for classification. In this paper, to address the above issues, we propose to impose group-sparsity constraints on the class-specific transformed features and label error terms, respectively. The effect is not only the more appropriate margins between data can be preserved, but also the learnt label space can be flexible for recognition. As a result, the obtained projection matrix can show more vital discriminative ability. Further, we employ the alternating direction method of multipliers to solve the resulting optimization problem. Extensive experiments and analysis on diverse benchmark databases are carried out to confirm our proposed model’s superiority in comparison with other competing classification methods.",space
10.1016/j.jappgeo.2021.104434,Journal,Journal of Applied Geophysics,scopus,2021-10-01,sciencedirect,A convolutional neural network approach to electrical resistivity tomography,https://api.elsevier.com/content/abstract/scopus_id/85112776120,"Electrical resistivity tomography (ERT) is an ill-posed and non-linear inverse problem commonly solved through deterministic gradient-based methods. These algorithms guarantee fast convergence toward the final solution but hinder accurate uncertainty assessments. On the contrary, numerical Markov Chain Monte Carlo algorithms provide accurate uncertainty appraisals but at the expense of a considerable computational effort. In this work, we develop a novel approach to ERT that guarantees an extremely fast inversion process and reliable uncertainty appraisals. The implemented method combines a Discrete Cosine Transform (DCT) reparameterization of data and model spaces with a Convolutional Neural Network. The CNN is employed to learn the inverse non-linear mapping between the DCT-compressed data and the DCT-compressed 2-D resistivity model. The DCT is an orthogonal transformation that here acts as an additional feature extraction technique that reduces the dimensionality of the input and output of the network. The DCT also acts as a regularization operator in the model space that significantly reduces the number of unknown parameters and the ill-conditioning of the inversion procedure, thereby preserving the spatial continuity of the resistivity values in the recovered solution. The estimation of model uncertainties is a key step of geophysical inverse problems and hence we implement a Monte Carlo simulation framework that propagates onto the estimated model the uncertainties related to both noise contamination and network approximation (the so-called modeling error). We first apply the approach to synthetic data to investigate its robustness in case of erroneous assumptions on the noise and model statistics used to generate the training set. Then, we demonstrate the applicability of the method through inverting real data measured along a river embankment. We also demonstrate that transfer learning avoids retraining the network from scratch when the statistical properties of training and target sets are different. Our tests confirm the suitability of the proposed approach, opening the possibility to estimate the subsurface resistivity values and the associated uncertainties in near real-time.",space
10.1016/j.oceaneng.2021.109680,Journal,Ocean Engineering,scopus,2021-10-01,sciencedirect,Linear reduced order method for design-space dimensionality reduction and flow-field learning in hull form optimization,https://api.elsevier.com/content/abstract/scopus_id/85112742342,"In the earlier stage of hull form optimization design, a series of design variables is usually needed to control the hull shape, so as to find optimal hull forms with better performance. In the surrogate-based hydrodynamic performance optimization for ships, with the increase of the dimensionality of design space, the number of new sample hulls to construct surrogate model needs to be larger, which will bring a large amount of calculation. Through reduced order method, the dimensionality of the optimization design space can be reduced while keeping the deformation range of the original design space to a great extent, for instance, using the linear combination of a smaller number of bases to represent the deformation range. In addition, in the later stage of hull form optimization design, flow field results of the new sample hulls can be fully utilized to do the dimensionality reduction multi-physics field learning. In this paper, the principle of the Proper Orthogonal Decomposition method is used and briefly introduced, the steps of dimensionality reduction of the design space are shown then, and some important problems for the design-space dimensionality reduction in the specific field of hull form optimization, such as retainability of fixed control points, irrelevance of the relative order of data to dimensionality reduction results, and decision of the new design space range after dimensionality reduction, are deep analyzed. Furthermore, taking the resistance optimization of the modified Wigley ship as an example, the specific application and error analysis of the dimensionality reduction method for design-space dimensionality reduction in the earlier stage of hull form optimization and the multi-physics field learning in the later stage of hull form optimization are given, and the applicability and reliability of the method are demonstrated by analyzing the influence of mode order and sample number on reconstruction effect of the hull shape or flow field, and the prediction effect of flow field for not-in-the-database new hull form in detail. Results show that the linear dimensionality reduction method can reduce samples needed for optimization, thus reduce the amount of calculation for the surrogate-based hull form optimization, and be used for quick prediction of multi-physics fields of any new form in the design space. Furthermore, it can not only be applied to the sensitivity analysis or a Pareto frontier selection in comprehensive performance optimization of hull form based on CFD, but also be implemented in the real-time forecast of the flow field and influence analysis of the ship performance when adjusting the hull form (or hull appendages).",space
10.1016/j.asoc.2021.107720,Journal,Applied Soft Computing,scopus,2021-10-01,sciencedirect,TDMatcher: A topic-based approach to task-developer matching with predictive intelligence for recommendation,https://api.elsevier.com/content/abstract/scopus_id/85111305283,"Artificial Intelligence is currently gripping the business world, which is the next step on the journey from Big Data to full automation. As crowdsourcing has been widely adopted by more enterprises and developers, the software crowdsourcing platform is able to collect enough data. Therefore, we introduce predictive intelligence to solve complex problems. This provides a bridge between software developers and enterprises: developers look for suitable tasks, whose aim is to gain revenues with respect to their interests and abilities; enterprises look for developers that are able to complete crowdsourcing tasks and/or solve hard problems. One main problem is the prediction challenge, i.e., how to perfectly predict the developers for the software crowdsourcing tasks and make appropriate recommendations. To solve the problem, this paper introduces predictive intelligence and proposes TDMatcher, which can effectively perform task-developer pairs prediction and recommendations for software crowdsourcing. First, we builds a unified model for tasks and developers such that they can be matched in the same domain space. Second, we quantitatively measures the matching degree between tasks and developers. Third, we randomly generates potential matchings between developers and crowdsourcing tasks and then employs an MCMC sampling approach to optimize the whole process. Highly matched task-developer pairs can be achieved in the sampling process. In order to solve the cold-start problem, we constructs a social network for each new developer, which indicates that the developer’s interests/abilities to be modeled We implemented TDMatcher and evaluated it against the state-of-the-art approaches on the real-world dataset. The experimental results clearly demonstrate the superiority of TDMatcher. We measured our proposed TDMatcher through the accuracy, diversity and Harmonic Mean of TDMatcher, and found that: (1) TDMatcher outperforms the state-of-the-arts by 15+% in the prediction accuracy and 30% in diversity; and (2) TDMatcher achieves a balance between accuracy and diversity. We believe that TDMatcher provides crowdsourcing platforms with much more capabilities in finding appropriate developers to complete crowdsourcing tasks or vice versa.",space
10.1016/j.jobe.2021.102799,Journal,Journal of Building Engineering,scopus,2021-10-01,sciencedirect,A step-by-step numerical method for optimization of mechanical ventilation in deep underground enclosed parking lots: A case-design study,https://api.elsevier.com/content/abstract/scopus_id/85110290956,"To reduce polluted air, mechanical ventilation (MV) is essential for enclosed parking spaces. The traditional prescriptive design method, the index-based design, cannot guarantee ventilation performance of each fan in the enclosed parking lot. To solve this, the performance-based design approach is the best alternative that specifically addresses performance-related criteria of MV system. In this study of practice-based learning in a real construction project, we proposed a unique design optimization methodology for improving the performance of MV systems using iterative, step-by-step computational fluid dynamics (CFD) simulation. Five numerical simulation levels on seven engineering steps and a techno-economic analysis were utilized. Ultimately, fan selection was based on calculating the airflow and pressure requirements of a MV system and finding a fan of the right design to hedge against the risk of system effect and surging phenomenon. Results showed that a stable fan selection with an error of 5% of the design air flow rate could be implemented by repeating numerical analysis for the performance optimization of the MV system. When the MV design optimization was applied to the reference parking lot, the number of fans could be reduced by 30%, and energy demand of the MV system by at least 16%. Consequently, the annual energy savings was projected to recover the increase in initial investment cost in about 5.8 years. The key contribution of this research is that it overcame the limitations of the traditional index-based design for selecting the optimal fans of MV systems.",space
10.1016/j.rcim.2021.102176,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2021-10-01,sciencedirect,Robotic grasping: from wrench space heuristics to deep learning policies,https://api.elsevier.com/content/abstract/scopus_id/85104603575,"The robotic grasping task persists as a modern industry problem that seeks autonomous, fast implementation, and efficient techniques. Domestic robots are also a reality demanding a delicate and accurate human–machine interaction, with precise robotic grasping and handling. From decades ago, with analytical heuristics, to recent days, with the new deep learning policies, grasping in complex scenarios is still the aim of several works’ that propose distinctive approaches. In this context, this paper aims to cover recent methodologies’ development and discuss them, showing state-of-the-art challenges and the gap to industrial applications deployment. Given the complexity of the related issue associated with the elaborated proposed methods, this paper formulates some fair and transparent definitions for results’ assessment to provide researchers with a clear and standardised idea of the comparison between the new proposals.",space
10.1016/j.knosys.2021.107302,Journal,Knowledge-Based Systems,scopus,2021-09-27,sciencedirect,A novel deep quantile matrix completion model for top-N recommendation[Formula presented],https://api.elsevier.com/content/abstract/scopus_id/85111890353,"Matrix completion models have been receiving keen attention due to their wide applications in science and engineering. However, the majority of these models assumes a symmetric noise distribution in their completion processes and uses conditional mean to characterize data distribution in a data set, the assumption of which incurs noticeable bias toward outliers. Recognizing the fact that noise distribution tends to be asymmetric in the real-world, this paper proposes a novel Deep Quantile Matrix Completion model, abbreviated as DQMC, which aims to accurately capture noise distribution in a data set by modeling conditional quantile of the data set instead of its conditional mean as traditionally handled by many state-of-the-art methods. Implemented via a deep computing paradigm, the newly proposed model maps a data set from its input space to the latent spaces through a two-branched deep autoencoder network. Such a mapping can effectively capture complex information latent in the data set. The proposed model is empowered by two key designed elements, including: (1) its two-branched deep autoencoder network that provides a flexible computing pathway to attain completion results with a high quality; (2) the introduction of a quantile loss function in combination with the proposed deep network, leading to a new unsupervised learning algorithm for tackling the matrix completion tasks with a superior capability. Comparative experimental results consistently demonstrate the superiority of the proposed DQMC model in conducting the top-N recommendation tasks involving both explicit and implicit rating data sets with respect to a series of state-of-the-art recommendation algorithms.",space
10.1016/j.neucom.2021.04.091,Journal,Neurocomputing,scopus,2021-09-24,sciencedirect,Quantum maximum mean discrepancy GAN,https://api.elsevier.com/content/abstract/scopus_id/85107052048,"Generative adversarial network (GAN) has shown profound power in machine learning. It inspires many researchers from other fields to create powerful tools for various tasks, including quantum state preparation, quantum circuit translation, and so on. It is known as classical techniques cannot efficiently simulate the quantum system, and the existing works haven’t investigated the quantum version of maximum mean discrepancy as the metric in learning models and applied it to quantum data. In this paper, we propose a metric named quantum maximum mean discrepancy (qMMD), which can be used to measure the distance between quantum data in Hilbert space. Based on the qMMD, we then design a quantum generative adversarial model, named qMMD-GAN, under the hybrid quantum–classical methods. We also provide the construction of qMMD-GAN that can be easily implemented on a quantum device. We demonstrate the power of our qMMD-GAN by applying it to a crucial real-world application that is generating an unknown quantum state. Our numerical experiments show that qMMD-GAN has a competitive performance compared to existing results. We believe that the hybrid-based models will not only be applied to physics research but provide a new direction for improving classical data processing tasks.",space
10.1016/j.anucene.2021.108355,Journal,Annals of Nuclear Energy,scopus,2021-09-15,sciencedirect,Large-scale design optimisation of boiling water reactor bundles with neuroevolution,https://api.elsevier.com/content/abstract/scopus_id/85105888765,"We combine advances in deep reinforcement learning (RL) with evolutionary computation to perform large-scale optimisation of boiling water reactor (BWR) bundles using CASMO4/SIMULATE3 codes; capturing fine details, radial/axial fuel heterogeneity, and real-world constraints. RL constructs neural networks that learn how to assign fuel and poison enrichment by narrowing the search space into the areas where human/physics knowledge demonstrate merit. Evolution strategies diversify the search in these areas, through obtaining guidance from RL candidates. With very efficient/parallel implementation, our optimisation approach is able to solve a coupled multi-zone BWR bundle optimisation with 
                        
                           ~
                        
                     40 constraints. The methodology is applied to a GE14-10×10 bundle, showing the ability of neuroevolution to find 
                        
                           ~
                        
                     100 feasible designs. The optimal bundle has 7 axial zones with non-uniform enrichment radially and axially. The results of this work also demonstrate that our neuroevolution methodology is sufficiently generic to adapt to other assembly and reactor designs with minor adjustments.",space
10.1016/j.pmcj.2021.101459,Journal,Pervasive and Mobile Computing,scopus,2021-09-01,sciencedirect,REAM: A Framework for Resource Efficient Adaptive Monitoring of Community Spaces,https://api.elsevier.com/content/abstract/scopus_id/85113273806,"Nowadays, many Internet-of-Things (IoT) devices with rich sensors and actuators are being deployed to monitor community spaces. The data generated by these devices are analyzed and turned into actionable information by analytics operators. In this article, we present a Resource Efficient Adaptive Monitoring (REAM) framework at the edge that adaptively selects workflows of devices and analytics to maintain an adequate quality of information for the applications at hand while judiciously consuming the limited resources available on edge servers. Since community spaces are complex and in a state of continuous flux, developing a one-size-fits-all model that works for all spaces is infeasible. The REAM framework utilizes reinforcement learning agents that learn by interacting with each community space and make decisions based on the state of the environment in each space and other contextual information. We demonstrate the resource-efficient monitoring capabilities of REAM on two real-world testbeds in Orange County, USA and NTHU, Taiwan, where we show that community spaces using REAM can achieve 
                        
                           >
                           90
                           %
                        
                      monitoring accuracy while incurring 
                        
                           ∼
                           50
                           %
                        
                      less resource consumption costs compared to existing static monitoring approaches. We also show REAM’s awareness of network link quality in its decision-making, resulting in a 42% improvement in accuracy over network agnostic approaches.",space
10.1016/j.egyai.2021.100101,Journal,Energy and AI,scopus,2021-09-01,sciencedirect,Development of a Soft Actor Critic deep reinforcement learning approach for harnessing energy flexibility in a Large Office building,https://api.elsevier.com/content/abstract/scopus_id/85109095539,"This research is concerned with the novel application and investigation of ‘Soft Actor Critic’ based deep reinforcement learning to control the cooling setpoint (and hence cooling loads) of a large commercial building to harness energy flexibility. The research is motivated by the challenge associated with the development and application of conventional model-based control approaches at scale to the wider building stock. Soft Actor Critic is a model-free deep reinforcement learning technique that is able to handle continuous action spaces and which has seen limited application to real-life or high-fidelity simulation implementations in the context of automated and intelligent control of building energy systems. Such control techniques are seen as one possible solution to supporting the operation of a smart, sustainable and future electrical grid. This research tests the suitability of the technique through training and deployment of the agent on an EnergyPlus based environment of the office building. The agent was found to learn an optimal control policy that was able to minimise energy costs by 9.7% compared to the default rule-based control scheme and was able to improve or maintain thermal comfort limits over a test period of one week. The algorithm was shown to be robust to the different hyperparameters and this optimal control policy was learnt through the use of a minimal state space consisting of readily available variables. The robustness of the algorithm was tested through investigation of the speed of learning and ability to deploy to different seasons and climates. It was found that the agent requires minimal training sample points and outperforms the baseline after three months of operation and also without disruption to thermal comfort during this period. The agent is transferable to other climates and seasons although further retraining or hyperparameter tuning is recommended.",space
10.1016/j.engappai.2021.104316,Journal,Engineering Applications of Artificial Intelligence,scopus,2021-09-01,sciencedirect,Deep replacement: Reinforcement learning based constellation management and autonomous replacement,https://api.elsevier.com/content/abstract/scopus_id/85108677220,"The Deep Reinforcement Learning (DRL) algorithm, Proximal Policy Optimization (PPO2), is deployed on a custom spacecraft (S/C) build and loss model to determine if an Artificial Intelligence (AI) can learn to monitor satellite constellation health and determine an optimal replacement strategy. A custom environment is created to simulate how S/C are built, launched, generate revenue, and finally decay. The reinforcement learning agent successfully learned an optimal policy for two models: a Simplified Model where the financial cost of actions is ignored; and an Advanced Model where the financial cost of actions is a major element. In both models the AI monitors the constellations and takes multiple strategic and tactical actions to replace satellites to maintain constellation performance. The Simplified Model showed that the PPO2 algorithm was able to converge on an optimal solution after 
                        ∼
                     200,000 simulations. The Advanced Model was much more difficult for the AI to learn, and thus, the performance drops during the early episodes, but eventually converges to an optimal policy at 
                        ∼
                     25,000,000 simulations. With the Advanced Model, the AI is taking actions that are successfully providing strategies for constellation management and satellite replacements which include these actions’ financial implications. Thus, the methods in this paper provide initial research developments towards a real-world tool and an AI application that can aid various Aerospace businesses in managing Low Earth Orbit (LEO) constellations. This type of AI application may become imperative for deploying and maintaining small satellite mega-constellations.",space
10.1016/j.adhoc.2021.102562,Journal,Ad Hoc Networks,scopus,2021-09-01,sciencedirect,Developing novel low complexity models using received in-phase and quadrature-phase samples for interference detection and classification in Wireless Sensor Network and GPS edge devices,https://api.elsevier.com/content/abstract/scopus_id/85107952090,"Despite Wireless Sensor Networks (WSNs) significantly developing over the past decade, these networks, like most wireless networks, remain susceptible to malicious interference and spectrum coexistence. Other vulnerabilities arise as WSN applications adopt open standards and typically resource and energy-constrained commercial-off-the-shelf equipment. Deployments include safety-critical applications such as the internet of things, medical, aerospace and space and deep-sea exploration. To manage safety and privacy requirements across such a diverse wireless landscape, security on wireless edge devices needs improvement while maintaining low complexity. This paper improves wireless edge device security by developing a novel intelligent interference diagnostic framework. Received in-phase (I) and quadrature-phase (Q) samples are exclusively utilized to detect modern, subtle and traditional crude jamming attacks. This I/Q sample utilization inherently enables decentralized decision-making, where the low-order features were extracted in a previous study focused on classifying typical 2.4–2.5 GHz wireless signals. The associated optimal intelligent models are leveraged as the foundation for this paper’s work. Initially, Matlab Monte Carlo simulations investigate the ideal case, which incorporates no hardware limitations, identifies the required data type of signal interactions and motivates a hardware investigation. Software-defined radios (SDRs) collect the required live over-the-air I/Q data and transmit matched signal (ZigBee) and continuous-wave interference in developed ZigBee wireless testbeds. Low complexity supervised machine learning models are developed based exclusively on the low-order features and achieve an average accuracy among the developed models above 98%. The designed methodology involves examining ZigBee over-the-air data for artificial jamming and SDR jamming of ZigBee signals transmitted from SDR and commercial (XBee) sources. This approach expands to a legitimate node classification technique and an overall algorithm for wireless edge device interference diagnostic tools. The investigation includes developing Support Vector Machine, XGBoost and Deep Neural Network (DNN) models, where XGBoost is optimal. Adapting the optimized models to global positioning system signals establishes the transferability of the designed methodology. Implementing the designed approaches on a Raspberry Pi embedded device examines a relatively resource-constrained deployment. The primary contribution is the real experimentally validated interference diagnostic framework that enables independent device operation, as no channel assumptions, network-level information or spectral images are required. Developed models exclusively use I/Q data low-order features and achieve high accuracy and generalization to unseen data.",space
10.1016/j.scs.2021.103071,Journal,Sustainable Cities and Society,scopus,2021-09-01,sciencedirect,A stochastic machine learning based approach for observability enhancement of automated smart grids,https://api.elsevier.com/content/abstract/scopus_id/85107608455,"This paper develops a machine learning aggregated integer linear programming approach for the full observability of the automated smart grids by positioning of micro-synchrophasor units, taking into account the reconfigurable structure of the distribution systems. The proposed stochastic approach presents a strategy occurring in several stages to micro-synchrophasor unit positioning based on the load level and demand in the system and based on the pre-determined sectionalizing and tie switches. Such a technique can also deploy the zero-injection limitations of the model and reduce the search space of the problem. Moreover, a novel method based on whale optimization method (WOM) is introduced to simultaneously enhance the reliability indices in order to specify the optimum topology for each phase and reduce the costs of power losses and customer interruptions. Although the problem of micro-synchrophasor placement is formulated in an integer linear programming framework, the restructuring technique is resolved on the basis of the WOM heuristic approach. Considering the uncertainty due to the metering devices or forecast errors, a stochastic framework based on point estimation is deployed to handle the uncertainty effects. The simulation and numerical results on a real system verify that the proposed method assures visibility of the distribution network pre and post reconfiguration in the time horizon of the planning. Furthermore, the results show that the system observability can be guaranteed at different load levels even though the system experiences different reconfiguration and topologies.",space
10.1016/j.sysarc.2021.102183,Journal,Journal of Systems Architecture,scopus,2021-09-01,sciencedirect,Memory-efficient deep learning inference with incremental weight loading and data layout reorganization on edge systems,https://api.elsevier.com/content/abstract/scopus_id/85107073021,"Pattern recognition applications such as face recognition and agricultural product detection have drawn a rapid interest on Cyber–Physical–Social-Systems (CPSS). These CPSS applications rely on the deep neural networks (DNN) to conduct the image classification. However, traditional DNN inference models in the cloud could suffer from network delay fluctuations and privacy leakage problems. In this regard, current real-time CPSS applications are preferred to be deployed on edge-end embedded devices. Constrained by the computing power and memory limitations of edge devices, improving the memory management efficacy is the key to improving the quality of service for model inference. First, this study explored the incremental loading strategy of model weights for the model inference. Second, the memory space at runtime is optimized through data layout reorganization from the spatial dimension. In particular, the proposed schemes are orthogonal to existing models. Experimental results demonstrate that the proposed approach reduced the memory consumption by 61.05% without additional inference time overhead.",space
10.1016/j.asoc.2021.107418,Journal,Applied Soft Computing,scopus,2021-09-01,sciencedirect,Development and experimental realization of an adaptive neural-based discrete model predictive direct torque and flux controller for induction motor drive,https://api.elsevier.com/content/abstract/scopus_id/85104714129,"This paper develops a neural network-based discrete predictive direct torque and flux control (NNPDTFC) for induction motor drive with the space vector modulation (SVM) technique. Moreover, this SVM technique with NNPDTFC is implemented to activate the inverter in the two-level operation and the performance is compared with the conventional PI direct torque and flux control (PIDTFC) technique. The PSO based model predictive control incorporated with the neural network is developed here in the NNPDTFC and is analyzed using MATLAB software. Disturbance reduction, simple control, and real-time implementation are the major features of NNPDTFC and it also enhances the transient performance of the motor drive by reducing settling time and peak overshoot. In addition, the flux and the torque ripples are significantly improved using the proposed NNPDTFC technique which is extensively used for the fast dynamic response of the induction motor drives as compared to PIDTFC. In order to show the potentiality of the proposed controller, a prototype controller is developed and validated with the laboratory setup and the control signals are generated for both NNPDTFC and PIDTFC using a low-cost Digital signal processor (DSP) controller which is fed to the induction motor of 3.7 kW capacity in the real-time platform. It is observed that the results with NNPDTFC are not only found to be extremely satisfactory even with the system and parameter uncertainties and external load perturbations but also, it produces enhanced dynamic as well as steady-state performance along with the reduced ripples in the signal flux, torque, and current compared to that of PIDTFC.",space
10.1016/j.jprocont.2021.06.004,Journal,Journal of Process Control,scopus,2021-08-01,sciencedirect,Online reinforcement learning for a continuous space system with experimental validation,https://api.elsevier.com/content/abstract/scopus_id/85111075597,"Reinforcement learning (RL) for continuous state/action space systems has remained a challenge for nonlinear multivariate dynamical systems even at a simulation level. Implementing such schemes for real-time control is still of a difficulty and remains largely unanswered. In this study, several critical strategies for practical implementation of RL are developed, and a multivariable, multi-modal, hybrid three-tank (HTT) physical process is utilized to illustrate the proposed strategies. A successful real-time implementation of RL is reported. The first step is a meta-heuristic first principles model parameter optimization, where a custom pseudo random binary signal (PRBS) is used to obtain open-loop experimental data. This is followed by in silico asynchronous advantage actor–critic (A3C/A-A2C) based policy learning. In the second step, three different approaches (namely proximal learning, single trajectory learning, and multiple trajectory learning) are utilized to explore the state/action space. In the final step, online learning (A2C) using the best in silico policy on the real process using a socket connection is established. The extent of exploration (EoE, a measure of exploration) is proposed as a parameter for quantifying exploration of the state/action space. While the online sample efficiency of RL application is enhanced, a soft constraint based constrained learning is proposed and validated. With considerations of the proposed strategies, this work demonstrates the possibility of applying RL to solve practical control problems.",space
10.1016/j.isprsjprs.2021.05.019,Journal,ISPRS Journal of Photogrammetry and Remote Sensing,scopus,2021-08-01,sciencedirect,Rapid and large-scale mapping of flood inundation via integrating spaceborne synthetic aperture radar imagery with unsupervised deep learning,https://api.elsevier.com/content/abstract/scopus_id/85108716493,"Synthetic aperture radar (SAR) has great potential for timely monitoring of flood information as it penetrates the clouds during flood events. Moreover, the proliferation of SAR satellites with high spatial and temporal resolution provides a tremendous opportunity to understand the flood risk and its quick response. However, traditional algorithms to extract flood inundation using SAR often require manual parameter tuning or data annotation, which presents a challenge for the rapid automated mapping of large and complex flooded scenarios. To address this issue, we proposed a segmentation algorithm for automatic flood mapping in near-real-time over vast areas and for all-weather conditions by integrating Sentinel-1 SAR imagery with an unsupervised machine learning approach named Felz-CNN. The algorithm consists of three phases: (i) super-pixel generation; (ii) convolutional neural network-based featurization; (iii) super-pixel aggregation. We evaluated the Felz-CNN algorithm by mapping flood inundation during the Yangtze River flood in 2020, covering a total study area of 1,140,300 km2. When validated on fine-resolution Planet satellite imagery, the algorithm accurately identified flood extent with producer and user accuracy of 93% and 94%, respectively. The results are indicative of the usefulness of our unsupervised approach for the application of flood mapping. Meanwhile, we overlapped the post-disaster inundation map with a 10-m resolution global land cover map (FROM-GLC10) to assess the damages to different land cover types. Of these types, cropland and residential settlements were most severely affected, with inundation areas of 9,430.36 km2 and 1,397.50 km2, respectively, results that are in agreement with statistics from relevant agencies. Compared with traditional supervised classification algorithms that require time-consuming data annotation, our unsupervised algorithm can be deployed directly to high-performance computing platforms such as Google Earth Engine and PIE-Engine to generate a large-spatial map of flood-affected areas within minutes, without time-consuming data downloading and processing. Importantly, this efficiency enables the fast and effective monitoring of flood conditions to aid in disaster governance and mitigation globally.",space
10.1016/j.coastaleng.2021.103919,Journal,Coastal Engineering,scopus,2021-08-01,sciencedirect,Satellite optical imagery in Coastal Engineering,https://api.elsevier.com/content/abstract/scopus_id/85106393729,"This Short Communication provides a Coastal Engineering perspective on present and emerging capabilities of satellite optical imagery, including real-world applications that can now be realistically implemented from the desktop. Significantly, at the vast majority of locations worldwide, satellite remote sensing is currently the only source of information to complement much more limited in-situ instrumentation for land and sea mapping, monitoring and measurement. Less well recognised is that publicly available, routinely sampled and now easily accessible optical imagery covering virtually every position along the world's coastlines already spans multiple decades. In the past five years the common obstacles of (1) limited access to high-performance computing and (2) specialist remote sensing technical expertise, have been largely removed. The emergence of several internet-accessible application programming interfaces (APIs) now enable applied users to access petabytes of satellite imagery along with the necessary tools and processing power to extract, manipulate and analyse information of practical interest. Following a brief overview and timeline of civilian Earth observations from space, satellite-derived shorelines (SDS) and satellite-derived bathymetry (SDB) are used to introduce and demonstrate some of the present real-world capabilities of satellite optical imagery most relevant to coastal professionals and researchers. These practical examples illustrate the use of satellite imagery to monitor and quantify both engineered and storm-induced coastline changes, as well as the emerging potential to obtain seamless topo/bathy surveys along coastal regions. Significantly, timescales of satellite-derived changes at the coast can range from decades to days, with spatial scales of interest extending from individual project sites up to unprecedented regional and global studies. While we foresee the uptake and routine use of satellite-derived information becoming quickly ubiquitous within the Coastal Engineering profession, on-ground observations are – and in our view will remain - fundamentally important. Compared to precision in-situ instrumentation, present intrinsic limitations of satellites are their relatively low rates of revisit and decimetre spatial accuracy. New satellite advances including ‘video from space’ and the potential to combine Earth observation with numerical and data-driven coastal models through assimilation and artificial intelligence are advances that we foresee will have future major impact in Coastal Engineering.",space
10.1016/j.buildenv.2021.107929,Journal,Building and Environment,scopus,2021-08-01,sciencedirect,MOOSAS – A systematic solution for multiple objective building performance optimization in the early design stage,https://api.elsevier.com/content/abstract/scopus_id/85105785192,"There is great potential for building performance optimization (BPO) in the early design stage, but there is still a lack of methods, algorithms, and tools to support the BPO process in this stage. Through a comprehensive review, this study identified three critical issues that affect the implementation of the BPO process in the early design stage: model integration, real-time performance analysis, and interactive optimization design. This study provides a systematic solution to these three critical issues. In terms of model integration, a feature-based and graph-based 3D building space recognition algorithm is proposed to automatically convert the computer-aided design (CAD model) into a computer-aided engineering model (CAE model). In terms of real-time performance analysis, a simplified physical method, an HPC-accelerated method, and an AI-based method are explored, and a real-time energy modeling module and a real-time daylighting analysis module are developed. In terms of the interactive optimization design, a preference-based multi-objective BPO design algorithm that can consider user preferences is proposed to make full use of the decision-making ability of humans and the computing power of machines and significantly improve the optimization efficiency and result satisfaction. Based on the systematic solution, a multi-objective BPO design software, MOOSAS, is developed. MOOSAS allows real-time performance feedback, dynamic parameter analysis, and interactive optimization, supporting the BPO process in the early design stage. The innovations of this study are as follows: first, this study proposes a systematic solution to the three critical issues of the BPO process, i.e., model integration, real-time performance analysis, and interactive optimization design; second, this study develops a multi-objective BPO design software (MOOSAS) for the early design stage.",space
10.1016/j.jmsy.2021.04.005,Journal,Journal of Manufacturing Systems,scopus,2021-07-01,sciencedirect,LearningADD: Machine learning based acoustic defect detection in factory automation,https://api.elsevier.com/content/abstract/scopus_id/85106283308,"Defect inspection of glass bottles in the beverage industrial is of significance to prevent unexpected losses caused by the damage of bottles during manufacturing and transporting. The commonly used manual methods suffer from inefficiency, excessive space consumption, and beverage wastes after filling. To replace the manual operations in the pre-filling detection with improved efficiency and reduced costs, this paper proposes a machine learning based Acoustic Defect Detection (LearningADD) system. Moreover, to realize scalable deployment on edge and cloud computing platforms, deployment strategies especially partitioning and allocation of functionalities need to be compared and optimized under realistic constraints such as latency, complexity, and capacity of the platforms. In particular, to distinguish the defects in glass bottles efficiently, the improved Hilbert-Huang transform (HHT) is employed to extend the extracted feature sets, and then Shuffled Frog Leaping Algorithm (SFLA) based feature selection is applied to optimize the feature sets. Five deployment strategies are quantitatively compared to optimize real-time performances based on the constraints measured from a real edge and cloud environment. The LearningADD algorithms are validated by the datasets from a real-life beverage factory, and the F-measure of the system reaches 98.48 %. The proposed deployment strategies are verified by experiments on private cloud platforms, which shows that the Distributed Heavy Edge deployment outperforms other strategies, benefited from the parallel computing and edge computing, where the Defect Detection Time for one bottle is less than 2.061 s in 99 % probability.",space
10.1016/j.petrol.2021.108488,Journal,Journal of Petroleum Science and Engineering,scopus,2021-07-01,sciencedirect,Data-driven machine learning for accurate prediction and statistical quantification of two phase flow regimes,https://api.elsevier.com/content/abstract/scopus_id/85102536455,Two different two-phase flow regimes including slug and dispersed flows are examined through the implementation of system identification methods to attain reduced-order models. The obtained models accurately capture the flow dynamics of the studied regimes. The models also provide state-space frequency by defining the transfer functions. The system identification results are compared with those of the bidirectional neural network to predict the phase fraction of the considered two-phase flows. The result of long short-term memory shows correlations of 91% between the real and predicted phase fractions.,space
10.1016/j.ymssp.2020.107510,Journal,Mechanical Systems and Signal Processing,scopus,2021-06-16,sciencedirect,Metric-based meta-learning model for few-shot fault diagnosis under multiple limited data conditions,https://api.elsevier.com/content/abstract/scopus_id/85100211264,"The real-world large industry has gradually become a data-rich environment with the development of information and sensor technology, making the technology of data-driven fault diagnosis acquire a thriving development and application. The success of these advanced methods depends on the assumption that enough labeled samples for each fault type are available. However, in some practical situations, it is extremely difficult to collect enough data, e.g., when the sudden catastrophic failure happens, only a few samples can be acquired before the system shuts down. This phenomenon leads to the few-shot fault diagnosis aiming at distinguishing the failure attribution accurately under very limited data conditions. In this paper, we propose a new approach, called Feature Space Metric-based Meta-learning Model (FSM3), to overcome the challenge of the few-shot fault diagnosis under multiple limited data conditions. Our method is a mixture of general supervised learning and episodic metric meta-learning, which will exploit both the attribute information from individual samples and the similarity information from sample groups. The experiment results demonstrate that our method outperforms a series of baseline methods on the 1-shot and 5-shot learning tasks of bearing and gearbox fault diagnosis across various limited data conditions. The time complexity and implementation difficulty have been analyzed to show that our method has relatively high feasibility. The feature embedding is visualized by t-SNE to investigate the effectiveness of our proposed model.",space
10.1016/j.compeleceng.2021.107121,Journal,Computers and Electrical Engineering,scopus,2021-06-01,sciencedirect,Efficient neural networks for edge devices,https://api.elsevier.com/content/abstract/scopus_id/85103242184,"Due to limited computation and storage resources of industrial internet of things (IoT) edge devices, many emerging intelligent industrial IoT applications based on deep neural networks (DNNs) heavily depend on cloud computing for computation and storage. However, cloud computing faces technical issues in long latency, poor reliability, and weak privacy, resulting in the need for on-device computation and storage. On-device computation is essential for many time-critical industrial IoT applications, which require real-time data processing. In this paper, we review three major research areas for on-device computation, specifically quantization, pruning, and network architecture design. The three techniques could enable a DNN model to be deployed on edge devices for real-time computation and storage, mainly due to the reduction of computation and space complexity. More importantly, these techniques could make DNNs applicable to industrial IoT devices.",space
10.1016/j.micpro.2021.103988,Journal,Microprocessors and Microsystems,scopus,2021-06-01,sciencedirect,Computer simulation of urban garden landscape design based on FPGA and neural network,https://api.elsevier.com/content/abstract/scopus_id/85099498199,"Digital Landscape is a combination of the system and the computer software and hardware system of a high simulation model. The author analyzes the application of computer simulation in landscape design and value analysis of a city garden. In the computer-aided design, the importance of digitizing information in the landscape design process, mainly human and the interaction of computer, is reflected in the digital model's creation and multimedia performance, becoming more and more evident. To form a two-dimensional or three-dimensional spatial data, to realize real-time, statistical Analysis, using the human living environment, multi-dimensional, efficient, and humane, and environmental landscape plan to more rational and practical, used the computer simulation techniques. Effective use of urban rainwater, to reduce the flooding of urban areas, it is possible to alleviate the water crisis, the organic combination of rainwater can be used in the course of the construction of the urban landscape as well as make-up landscape, visual beautification has optimized the ecosystem, and from many rainwater utilization functions; These functions in landscape design, rainwater garden, It can be realized the rooftop garden, and the city's green. The construction and sustainable economy and the promotion of the ecological park's social development will positively sign. Suitable for rainwater regulation, water (recovery) is stored—Computer-Aided Design (CAD) green space. Technical measures save of suggestions for practical application of the square: innovation and new of space design, new artificial wetland system, and garden rainwater in the application of the regulation (population) storage system design of the water-saving of these to the sustainable development of such new square of rainwater adjustment (group) storage system design and urban landscape environment. It is useful for the application of technology.",space
10.1016/j.cja.2020.09.011,Journal,Chinese Journal of Aeronautics,scopus,2021-06-01,sciencedirect,Framework and development of data-driven physics based model with application in dimensional accuracy prediction in pocket milling,https://api.elsevier.com/content/abstract/scopus_id/85097765922,"In the manufacturing of thin wall components for aerospace industry, apart from the side wall contour error, the Remaining Bottom Thickness Error (RBTE) for the thin-wall pocket component (e.g. rocket shell) is of the same importance but overlooked in current research. If the RBTE reduces by 30%, the weight reduction of the entire component will reach up to tens of kilograms while improving the dynamic balance performance of the large component. Current RBTE control requires the off-process measurement of limited discrete points on the component bottom to provide the reference value for compensation. This leads to incompleteness in the remaining bottom thickness control and redundant measurement in manufacturing. In this paper, the framework of data-driven physics based model is proposed and developed for the real-time prediction of critical quality for large components, which enables accurate prediction and compensation of RBTE value for the thin wall components. The physics based model considers the primary root cause, in terms of tool deflection and clamping stiffness induced Axial Material Removal Thickness (AMRT) variation, for the RBTE formation. And to incorporate the dynamic and inherent coupling of the complicated manufacturing system, the multi-feature fusion and machine learning algorithm, i.e. kernel Principal Component Analysis (kPCA) and kernel Support Vector Regression (kSVR), are incorporated with the physics based model. Therefore, the proposed data-driven physics based model combines both process mechanism and the system disturbance to achieve better prediction accuracy. The final verification experiment is implemented to validate the effectiveness of the proposed method for dimensional accuracy prediction in pocket milling, and the prediction accuracy of AMRT achieves 0.014 mm and 0.019 mm for straight and corner milling, respectively.",space
10.1016/j.evopsy.2021.03.006,Journal,Evolution Psychiatrique,scopus,2021-05-01,sciencedirect,"From Digital Identity to Connected Personality, From Augmented Diagnostician to Virtual Caregiver: What Are the Challenges for the Psychology and the Psychiatry of the Future?",https://api.elsevier.com/content/abstract/scopus_id/85104125089,"Objectifs
                  Qui sommes-nous devenus, citoyens, patients, praticiens ? En quoi les moyens de communications et l’informatisation de notre société modifient-ils, intègrent-ils nos identités ? L’intelligence artificielle comprendrait-elle bientôt plus justement l’être humain dont elle s’émanciperait ?
               
                  Matériel et méthodes
                  Cheminons à partir de la lexicologie pour tenter de saisir, via le point de vue de la philosophie, l’identité contemporaine vers la notion d’« identité numérique » dont les incidents psychologiques normaux ou pathologiques entraînent ce que nous définissons « la personnalité numérique ». Puis, posant les bases d’une psychologie de l’identité contemporaine, nous envisageons comment « la psychologie » et « la psychiatrie » actuelles considèrent « la personnalité » du patient et, en retour, comment elles se définissent du point du vue du « praticien en ligne » ou du « chercheur connecté ».
               
                  Résultats
                  En échange de son utilisation « gratuite », l’action de l’internaute sur le Web 2.0 produit du contenu et alimente des bases de données, déclaratives ou non. En perte d’intimité au fur et à mesure que « ses » données ne lui appartiennent plus, l’identité du citoyen se décompose en fonctions des supports digitaux : site de rencontre amical, plateforme de liens amoureux, blog concernant un loisir ou un voyage, etc. Par le même mouvement, l’identité numérique se compose en autre-soi possédant une part d’intelligence artificielle pourvoyeuse de capacité d’existence propre. Plutôt que deux entités parallèlement différentiables, réelle ou augmentée, naît une identité hybride « réalistiquo-virtuelle ». Quelles conséquences normales ou pathologiques chez l’être humain ? Les tendances sociétales post-modernes issues du digital ou y trouvant expression peuvent entraîner, chez un individu donné, une exacerbation des traits de personnalité préalablement existants, voire des symptômes. Parallèlement, il arrive que les moyens de communication moderne deviennent une aide pour expérimenter le monde, majorer l’estime de soi, rêver favorablement ses phantasmes, se confier plus facilement à des « inconnu(e)s », etc. Mais dans tous les cas, chez le sujet souffrant, ou ne souffrant pas, préalablement à sa surexposition, de maladie neuropsychiatrique ou de trouble psychopathologique, il s’avère aujourd’hui scientifiquement documenté que la confrontation numérique accrue induit des atteintes neuropsychiques massives (affaiblissement de la mémoire de travail, des capacités d’attention et de concentration, des aptitudes à construire des opérations cognitives élaborées, etc.). Sur le plan psychopathologique, plutôt que la terminologie de « trouble de l’identité » ou une notion de « co-identités », le terme d’« identité trouble » nous paraît le mieux rendre compte de cette mutation du « moi » où la frontière entre réalité et virtualités s’amenuise : la dissociation prévaut. L’homme post-moderne et ses objets connectés ne font plus qu’un, mais cet « uniforme » apparaît constitué d’un patchwork de confettis identificatoires plus ou moins accolés, sans réelle harmonisation d’ensemble. La personnalité commune se marque d’hyperexpressivité et d’hyperémotivité, au détriment de la possibilité de contrôle des affects et du développement des capacités d’introspection. Contre le risque du vide, tend à se développer une contra-phobie par l’ordiphone, par l’objet lui-même, par la possibilité de contacter en permanence ses proches si nécessaire, et en retour rester toujours « disponible », ce qui alimente une forme d’égocentrisme addictogène. Résulte de ses évolutions, globalement dans la société, un affaiblissement des capacités langagières, et ainsi de réflexion, y compris pour l’espace clinique et scientifique.
               
                  Discussion
                  Pour les domaines de la psychologie et de la psychiatrie, s’associent actuellement deux évolutions : une velléité d’« objectivité-scientificité » et une numérisation de la relation patient–soignant. Du côté de la « science », la médecine objective « factuelle » s’intéresse de plus en plus à la pathologie aux dépens du sujet en souffrance, confondant signe et symptôme, glissant jusqu’à un niveau moléculaire, très en-deçà du patient, vers une psychiatrie ou une psychologie « post-clinique ». Qu’on veuille la promouvoir ou l’anéantir, du côté du clinicien ou du chercheur, la « subjectivité » est devenue un signifiant à la mode pour le domaine de la santé psychique. Ce retour actuel du « subjectif » prospère sur une sorte de peur de la subjectivité depuis la fin de la seconde guerre mondiale qui avait entraîné la nosographie américaine vers les « objectifs » des DSM (Manuel Diagnostique et Statistique des Troubles Psychiques publié par l’American Psychiatric Association depuis 1952). Mais plutôt qu’une connaissance validable, et/ou invariable concernant tel ou tel trouble psychique, le changement, la relativité des entités nosographiques d’une version à l’autre du manuel traduit, en miroir, la subjectivité d’une époque, ce que nous appelons « subjectivité sociétale ». Autant qu’elle témoigne de notre temps, la révolution bio-numérique s’imposera probablement dans une future édition de la nosographie : la validité diagnostique devrait se majorer par la définition précise de marqueurs biologiques et/ou neuroradiologiques, si ceux-ci participent à construire une théorie étiopathogénique des phénomènes psychiques observés. Cette orientation reste toutefois balbutiante : outre l’infime nombre de biomarqueurs identifiés, et surtout utilisables en pratique quotidienne, leurs liens de causalité ou de conséquentialité avec les symptômes ou le processus morbide restent le plus souvent incertains autant qu’ils sont fort divers et interreliés. Le chercheur en neurosciences vise à mesurer et analyser une multitude de données, intégrant en particulier les mimiques et les émotions authentifiables par caméra thermique, les mouvements des segments des corps et dynamiques des regards enregistrables par des capteurs, la standardisation des voix et des discours pour analyse par logiciel informatique de la prosodie, des signifiants employés, de la syntaxe… le tout s’intégrant dans un phénotypage digital de la souffrance. Pourra-t-on bientôt parler, en remplacement du psychologue ou du psychiatre, de « diagnosticien augmenté » ?
               
                  Conclusion
                  Apparaît-il actuellement hasardeux de faire confiance à un thérapeute entièrement virtuel… expérience déjà lancée il y a plus de 50 ans ! L’être humain est un « être de sens », or, selon le modèle de la clinique traumatique, le surgissement du tout-numérique peut entraîner un « effondrement du sens » générateur d’une tendance à la dissociation de la personnalité. Accordant le rétablissement des liens entre émotions, affects, comportements et cognitions, le langage parlé atténue puis fait disparaître la dissociation. Guidée par le praticien, cette parole thérapeutique est parfois qualifiée de « maïeutique », du nom de la science de l’accouchement : elle construit synchroniquement à son essence la pensée, et une prise de conscience de celle-ci, plutôt qu’elle n’en rendrait compte secondairement. Il s’agit d’une réinterprétation causale d’un sens compris ou plutôt « attribué » singulièrement par le sujet, après-coup, le passé revisité dans l’instant noue une synthèse, le hasard est transformé en destin. Le sujet qui parle réélabore son histoire vers une reconstruction sémantique, une densification de ses réseaux de signification. Reconquérant son être par la création d’un discours, de méandres véridiques comme fictionnels, la narration, voire la poétisation, offre l’illusion ponctuelle d’une meilleure cohérence, toujours relative, illusoire La parole thérapeutique et le discours sur celle-ci restent en devenir, inachevés, incertains autant que vivants, caractérisant une « post-psychothérapie », c’est-à-dire une psychothérapie et non pas une technique rééducative qui se trouverait figée dans des objectifs connus à l’avance. Les notions de faits et de réalité sont ici secondaires, non pas au sens de l’objectif, ni même du subjectif, mais du second degré, puis d’autres degrés successifs ou imbriqués portant l’effort intellectuel. Vers l’apaisement, si nous voulions amener la réflexion à son paroxysme, nous pourrions avancer qu’il suffirait de donner « n’importe quel sens », d’en choisir un quel qu’il soit, du côté du patient ou du praticien, sans qu’il ne soit nécessairement le même, témoignage d’une construction intersubjective formellement invalide.
               
                  Objectives
                  Who have we become, as citizens, patients, practitioners? How do the means of communication and the computerization of our society, its digitization, modify and integrate our identities? Can we assume that artificial intelligence will soon have a more accurate understanding of the human being from whom it will have emancipated itself?
               
                  Materials and methods
                  We move from lexicology to try to grasp, from the point of view of philosophy, a contemporary identity that is moving towards the notion of a “digital identity” whose normal or pathological psychological incidents lead to what we define as “the digital personality.” Then, laying the foundations for a contemporary psychology of identity, we consider how current “psychology” and “psychiatry” view the patient's “personality” and, in turn, how they define themselves from the point of view of “the patient,” or, inversely, from the point of view of the “online practitioner” or “connected researcher.”
               
                  Results
                  In exchange for its “free” use, the Internet user's action on Web 2.0 produces content and feeds databases, whether this is declared or not. Users’ privacy is lost, as “their” data no longer belongs to them; and citizens’ identity is broken down into digital media functions: a site for meeting friends, a dating platform, a blog about hobbies or travel, etc. At the same time, digital identity is made up of an other-self, including a part of artificial intelligence that provides capacity for its own existence. Rather than two parallel, differentiable entities, real or augmented, a “realistic-virtual” hybrid identity is born. What are the normal or pathological consequences for humans? Postmodern societal trends emerging from or finding expression in the digital can lead to an exacerbation of previously existing personality traits, or even symptoms, in a given individual. At the same time, it happens that the modern means of communication become an aid to experience the world, to increase self-esteem, to dream favorably about one's fantasies, to confide more easily in “strangers,” etc. But in all cases, in the subject suffering, or not suffering, prior to his overexposure, from a neuropsychiatric disease or a psychopathological disorder, it now turns out to be scientifically documented that the increased numerical confrontation induces massive neuropsychic damage (weakening working memory, attention and concentration skills, skills in constructing sophisticated cognitive operations, etc.). On the psychopathological level, rather than the terminology of “identity disorder” or a notion of “co-identities,” the term “identity elusive"" seems to us to best account for this mutation of the “me” where the border between reality and virtualities is shrinking: dissociation prevails. The postmodern human and its connected objects become one, but this “uniformity” appears to be made up of a patchwork of identifying confetti more or less joined together, without a real overall harmonization. The common personality is marked by hyperexpressiveness and hyperemotivity, to the detriment of the possibility of controlling affects and the development of introspective capacities. Against the risk of a vacuum, a contra-phobia tends to develop through the smartphone, by the object itself, by the possibility of constantly contacting relatives if necessary, and in return always remaining “available,” which fuels a form of addicting self-centeredness. The result of these developments, for society in general, is a weakening of language skills, and thus of reflection, including in the clinical and scientific space.
               
                  Discussion
                  For the areas of psychology and psychiatry, two developments are currently associated: a desire for “objectivity-scientificity” and a digitization of the patient–caregiver relationship. On the side of “science,” objective “factual” medicine is increasingly interested in pathology at the expense of the suffering subject, confusing sign and symptom, sliding down to a molecular level, far below the patient, towards psychiatry or postclinical psychology. Whether we want to promote it or destroy it, on the side of the clinician or the researcher, “subjectivity” has become a fashionable signifier in the field of mental health. This current return of the “subjective” thrives on a kind of fear of subjectivity present since the end of World War II, which had led American nosography towards the “objectives” of the DSM (Diagnostic and Statistical Manual of Mental Disorders, published by the American Psychiatric Association since 1952). But rather than a verifiable and/or invariable knowledge concerning a particular psychic disorder, the changes and the relativity of nosographic entities from one version of the manual to another provides us with a mirror image of the subjectivity of an era, which we propose to call “societal subjectivity.” As much as it is a product of our time, the bio-digital revolution will probably impose itself in a future edition of nosography: the diagnostic validity should be increased by the precise definition of biological and/or neuroradiological markers, if these participate in building an etiopathogenic theory of observed psychic phenomena. This orientation remains in its infancy, however: in addition to the tiny number of identified biomarkers, and above all, those that are usable in daily practice, their causal or consequential links with symptoms or with the morbid process remain most often uncertain, inasmuch as they are diverse and interrelated. The neuroscience researcher aims to measure and analyze a multitude of data, integrating, in particular, mimicry and emotions authenticated by thermal camera; movements of body segments and gaze dynamics recorded by sensors; the standardization of voices and speeches for computer software analysis of prosody, used signifiers, syntax… all of which is integrated into a digital phenotyping of suffering. Will we soon be able to speak, replacing the psychologist or the psychiatrist, of an “augmented diagnostician?”.
               
                  Conclusion
                  Does it currently appear risky to trust an entirely virtual therapist… an experiment already launched more than 50 years ago! The human being is a “being of meaning,” yet, according to the model of trauma, the emergence of the all-digital can lead to a “collapse of meaning,” generating a tendency to personality dissociation. Granting the reestablishment of the links between emotions, affects, behaviors, and cognitions, spoken language attenuates dissociation, then makes it disappear. Guided by the practitioner, this therapeutic word is sometimes qualified as “maieutics,” from the name of the science of childbirth: it builds thought synchronously to its essence, and an awareness of it, rather than nondisclosure, would account for it secondarily. It is a causal reinterpretation of a meaning understood or rather “attributed” singularly by the subject, after the fact: the past revisited in the present moment creates a synthesis, and chance is transformed into fate. The speaking subject re-elaborates her/his story towards a semantic reconstruction, a densification of her/his networks of signification. Reclaiming one's being by the creation of a discourse, of veridical as well as fictional meanders, narration, even poetization, offers the punctual illusion of a better coherence, always relative, illusory… Therapeutic speech and discourse about such speech–these are still being made, unfinished, uncertain, and alive. These are the characteristics of what we could a “post-psychotherapy,” that is, a psychotherapy and not a re-educational technique whose objectives would be fixed and known in advance. The notions of facts and reality are secondary here, not in the sense of the objective, nor even of the subjective, but of the second degree, then of other successive or overlapping degrees that require intellectual effort. Moving towards appeasement, if we wanted to bring the reflection to its paroxysm, we could advance that it would be enough to give “any meaning,” whatever it may be. This would apply both to the patient and to the practitioner, without each party's meaning necessarily being the same: a testimony to a formally invalid intersubjective construction.",space
10.1016/j.ssci.2021.105190,Journal,Safety Science,scopus,2021-05-01,sciencedirect,Beirut explosion 2020: A case study for a large-scale urban blast simulation,https://api.elsevier.com/content/abstract/scopus_id/85100545374,"In the face of continued global urbanization, cities are challenged to satisfy increasing standards in terms of quality of life, environmental conditions, safety, security, health, economic growth and mobility. The concept of “smart cities” aims at utilising advanced technologies, artificial intelligence and high computational capacity to increase their resilience and improve the services provided to the citizens. Computation-based numerical simulations have been essentially used to estimate the effects of explosion events in urban environments in terms of both structural damage and human casualties. These provide urban planners and decision makers with valuable information for vulnerability assessment and aid developing prevention or mitigation solutions. In this article, we present a framework to generate a 3D large-scale urbanistic finite element model, where the desired geospatial data are extracted from the open-source world map OpenStreetMap. The model is used to simulate blast wave propagation effects in a wide urban area taking into account the reflections at building surfaces via a sophisticated Fluid-Structure interaction technique integrated in the EUROPLEXUS explicit finite element method software. The explosion in the Port of Beirut in Lebanon, which took place on the 4th of August 2020, was remarkable for the large amount of explosive material causing considerable damage to surrounding structures and a high number of deaths and injured. Such characteristics make the event suitable for assessing the performance of the proposed computational approach in a widely exposed (by the blast wave) urban zone.",space
10.1016/j.eswa.2020.114139,Journal,Expert Systems with Applications,scopus,2021-04-15,sciencedirect,A dynamic framework for tuning SVM hyper parameters based on Moth-Flame Optimization and knowledge-based-search,https://api.elsevier.com/content/abstract/scopus_id/85099517458,"In the real world, most of the collections of data are dynamic in nature, i.e. their size may grow with time. This dynamic nature of the data not only reduces the performance of the classifiers but also demands more optimized models for retaining the performance. Due to this, machine learning models developed in a static environment cannot be deployed efficiently to solve the real-world problems. Nowadays, maximum existing works consider only the static behaviour of the data for the training of machine learning models where the size of the collection of training data does not change over time. This paperwork imposes Support Vector Machine (SVM) in a dynamic environment. It has been identified that shifting of the optimum values of two hyper-parameters C (Penalty Parameter) and γ (Kernel Parameter) in the search space is one of the primary reasons for the performance degradation of SVM in dynamic environment. This paper proposes a novel framework that uses a new optimization module Knowledge-Based-Search (KBS) along with Moth –Flame Optimization (MFO) to optimize 
                        
                           C
                        
                      and 
                        
                           γ
                        
                      in a dynamic environment to train SVM efficiently. KBS uses knowledge gathered at various instances of time, which are the bi-products of MFO. MFO in our framework is the base optimization algorithm which works underneath KBS. The experiments have shown that KBS helps in controlling the exponential growth of the time complexity of the optimization process where only MFO is used to optimize 
                        
                           C
                        
                      and
                        
                           γ
                        
                     . Integration of KBS with MFO brings down the time complexity to a large extent. To validate the proposed framework we have used a simulated dynamic environment for profit/loss classification problem for organizations. The experiments have also shown that KBS's integration with MFO outperforms integration of KBS with other modern optimization techniques such as Particle Swarm Optimization (PSO), Multi-Verse Optimization (MVO), Grey-Wolf Optimization (GWO), Cuckoo Search (CS), Whale Optimization Algorithm (WOA), Genetic Algorithm (GA), Fire-Fly Algorithm (FFA) and Salp Swarm Algorithm (SSA).",space
10.1016/j.eswa.2020.114402,Journal,Expert Systems with Applications,scopus,2021-04-15,sciencedirect,Unsupervised feature selection for attributed graphs,https://api.elsevier.com/content/abstract/scopus_id/85097572982,"Many real-world applications generate attributed graphs that contain both link structures and content information associated with nodes. Content information in real networks always contains high dimensional feature space. In recent years, unsupervised feature selection has been widely used in handling high dimensional data without label information. Most existing unsupervised feature selection methods assume that instances in datasets are independent and identically distributed. However, instances in attributed graphs are intrinsically correlated. Considering the wide applications of feature selection in attributed graphs, we propose a new unsupervised feature selection method based on regularized sparse learning. We use pseudo class labels to learn the interdependency from both link and content information, and embed the obtained information into a sparse learning based feature selection framework. In particular, a new regularization term is designed to learn link information, which capture group behavior among the connected instances utilizing latent social dimensions. To solve the proposed feature selection model, we consider both convex and nonconvex cases and design the corresponding algorithms based on the Alternating Direction Method of Multipliers (ADMM) combined with ConCave Convex Procedure (CCCP). Numerical studies are implemented on real-world datasets to validate the advantage of our new method.",space
10.1016/j.jnca.2021.102995,Journal,Journal of Network and Computer Applications,scopus,2021-04-01,sciencedirect,Dew computing-inspired health-meteorological factor analysis for early prediction of bronchial asthma,https://api.elsevier.com/content/abstract/scopus_id/85100605135,"Bronchial asthma is one of the most common chronic diseases of childhood and considered as a major health problem globally. The irregularity in meteorological factors has become a primary cause of health severity for the individuals suffering from asthma. In the presented research, a dew-cloud assisted cyber-physical system (CPS) is proposed to analyze the correlation between the meteorological and health parameters of the individuals. The work is primarily focused on determining the health adversity caused by the irregular scale of meteorological factors in real-time. IoT-assisted smart sensors are utilized to capture ubiquitous information from indoor environment that make a vital impact on the health of the individual directly or indirectly. The data is analyzed over the cyber-space to quantify the probable irregular health events by utilizing the data classification efficiency of Weighted-Naïve Bayes modeling technique. Moreover, the relationship between meteorological and health parameters is estimated by utilizing the Adaptive Neuro-Fuzzy Inference System (ANFIS) and calculate a unifying factor over the temporal scale. To validate the monitoring performance, the proposed model is implemented in the four schools of Jalandhar, India. The experimental evaluation of the proposed model acknowledges the performance efficiency through several statistical approaches. Furthermore, the comparative analysis is evaluated with state-of-the-art decision-making algorithms that demonstrate the effectiveness of the proposed solution for the targeted application.",space
10.1016/j.petrol.2020.108296,Journal,Journal of Petroleum Science and Engineering,scopus,2021-04-01,sciencedirect,Geological structure-guided hybrid MCMC and Bayesian linearized inversion methodology,https://api.elsevier.com/content/abstract/scopus_id/85100209414,"Seismic inversion is a common method for hydrocarbon reservoir characterization, as it consists of a proven and effective approach to derive elastic properties from reflectivity seismic data. Markov Chain Monte Carlo (MCMC) based seismic inversion approach is a suitable choice to numerically evaluate the posterior uncertainties associated with the inverse solution without assuming linear forward operators, Gaussian, or generalized Gaussian prior models. However, the existing MCMC based seismic inversion approaches are mostly performed trace-by-trace, which means that the spatial coupling of model parameters is not considered. When the results of trace-by-trace based inversion are combined to generate a 2D profile, the final results will be laterally discontinuous. Moreover, the large dimension of the model space causes low convergence efficiency of MCMC-based seismic inversion. To overcome these issues, a geological structure-guided hybrid MCMC and Bayesian linearized inversion (BLI) methodology for seismic inversion is implemented. The geological structure information obtained using plane wave destruction (PWD) is incorporated to the MCMC based inversion algorithm in the form of dips yields more geologically meaningful results. The hybrid MCMC and BLI strategy, which takes advantage of BLI's high efficiency to provide initial configuration for MCMC, is used to improve the convergence of MCMC-based inversion. Additionally, the block coordinate descent (BCD) algorithm is introduced to replace the large-scale matrix solution in geological structure-guided, and consequently reduce memory consumption and time cost. This methodology is validated on a synthetic seismic dataset, as well as on a real case. It has proven to be a reliable approach to obtain acoustic impedance (AI) from post-stack seismic data in an efficient way. It also addresses the uncertainty related with the ill-posed characteristics of the inversion methodology itself.",space
10.1016/j.actaastro.2021.01.048,Journal,Acta Astronautica,scopus,2021-04-01,sciencedirect,A transfer learning approach to space debris classification using observational light curve data,https://api.elsevier.com/content/abstract/scopus_id/85100001105,"This paper presents a data driven approach to space object characterisation through the application of machine learning techniques to observational light curve data. One-dimensional convolutional neural networks are shown to be effective at classifying the shape of objects from both simulated and real light curve data. To the best of the authors’ knowledge this is the first generalised attempt to classify the shape of space objects using real observational light curve data.
                  It is also demonstrated that transfer learning is successful in improving the overall classification accuracy on real light curve datasets. The authors develop a simulated light curve dataset using a high fidelity three-dimensional ray-tracing software. The simulator takes in a textured geometric model of a Resident Space Object as well as its ephemeris and uses ray-tracing software to generate photo-realistic images of the object that are then processed to extract the light curve. Models that are pre-trained on the simulated dataset and then fine-tuned on the real datasets are shown to outperform models purely trained on the real datasets. This result indicates that transfer learning will allow organisations to effectively utilise deep learning techniques without the requirement to build up large real light curve datasets for training.",space
10.1016/j.cma.2020.113609,Journal,Computer Methods in Applied Mechanics and Engineering,scopus,2021-04-01,sciencedirect,The Arithmetic Optimization Algorithm,https://api.elsevier.com/content/abstract/scopus_id/85099194941,"This work proposes a new meta-heuristic method called Arithmetic Optimization Algorithm (AOA) that utilizes the distribution behavior of the main arithmetic operators in mathematics including (Multiplication (
                        M
                     ), Division (
                        D
                     ), Subtraction (
                        S
                     ), and Addition (
                        A
                     )). AOA is mathematically modeled and implemented to perform the optimization processes in a wide range of search spaces. The performance of AOA is checked on twenty-nine benchmark functions and several real-world engineering design problems to showcase its applicability. The analysis of performance, convergence behaviors, and the computational complexity of the proposed AOA have been evaluated by different scenarios. Experimental results show that the AOA provides very promising results in solving challenging optimization problems compared with eleven other well-known optimization algorithms. Source codes of AOA are publicly available at and .",space
10.1016/j.medidd.2021.100081,Journal,Medicine in Drug Discovery,scopus,2021-03-01,sciencedirect,Peptides in chemical space,https://api.elsevier.com/content/abstract/scopus_id/85104918401,"Peptides, defined as sequences of amino acids up to approximately 50 residues in length, represent an extremely large reservoir of potentially bioactive compounds, referred to here as the peptide chemical space. Recent advances in computer hardware and software have led to a wide application of computational methods to explore this chemical space. Here, we review different in silico approaches including structure-based design, genetic algorithms, and machine learning. We also review the use of molecular fingerprints to sample virtual libraries and to visualize the peptide chemical space. Finally, we present an overview of the known peptide chemical space in form of an interactive map representing 40,531 peptides collected from eleven open-access peptide and peptide-containing databases, accessible at https://tm.gdb.tools/map4/peptide_databases_tmap/. These peptides are displayed as TMAP (Tree-Map) according to their molecular fingerprint similarity computed using MAP4, a MinHashed atom pair fingerprint well suited to analyze large molecules.",space
10.1016/j.rser.2020.110519,Journal,Renewable and Sustainable Energy Reviews,scopus,2021-03-01,sciencedirect,Development of a constraint non-causal wave energy control algorithm based on artificial intelligence,https://api.elsevier.com/content/abstract/scopus_id/85095602765,"The real-time implementation of wave energy control leads to non-causality as the wave load that comes in the next few seconds is used to optimize the control command. The present work tackles non-causality through online forecasting of future wave force using artificial intelligence technique. The past free surface elevation is used to forecast the incoming wave load. A feedforward artificial neural network is developed for the forecasting, which learns to establish the intrinsic link between past free surface elevation and future wave force through machine learning algorithm. With the implementation of the developed online wave force prediction algorithm, a real-time discrete control algorithm taking constraint on response amplitude into account is developed and implemented to a bi-oscillator wave energy converter in the present research. The dynamic response and the wave power extraction are simulated using a state-space hydrodynamic model. It is shown that the developed real-time control algorithm enhances the power capture substantially whereas the motion of the system is hardly increased. The prediction error effect on power extraction is investigated. The reduction of power extraction is mainly caused by phase error, whilst the amplitude error has minimal influence. A link between the power capture efficiency and the constraint on control is also identified.",space
10.1016/j.future.2020.10.011,Journal,Future Generation Computer Systems,scopus,2021-03-01,sciencedirect,Human action identification by a quality-guided fusion of multi-model feature,https://api.elsevier.com/content/abstract/scopus_id/85094321431,"Human motion recognition has become an active research area in the field of computer vision due to its wide range of implementations in domains of video monitoring, virtual reality, human–machine interaction. Dealing with the problem that the RGB images cannot provide enough depth information, a multi-modal depth neural network based on joint cost function is proposed for human motion recognition. In the architecture, the features of the RGB video frames are extracted by the 3D CNN architecture while the characteristics of human motion recognition in the SSDDI graphics utilizing depth map are extracted by the LSTM. Moreover, the model utilizes joint cost function including the cross-entropy loss and the distance constraint between the feature space of training samples and their center values within each category. The experimental results on the MSR Action 3D datasets suggest that the proposed model demonstrates a higher accuracy rate than do the other competing models.",space
10.1016/j.quaint.2020.08.018,Journal,Quaternary International,scopus,2021-02-20,sciencedirect,Characterization of geomorphological features of lunar surface using Chandrayaan-1 Mini-SAR and LRO Mini-RF data,https://api.elsevier.com/content/abstract/scopus_id/85090021453,"The lunar surface comprises complex geomorphological features, which have been formed by the conjunction of processes namely impact cratering and volcanism. Geological features on the Lunar surface can be bifurcated into two main areas named Maria region and the Highland region. Taurus-Littrow valley, which was the Apollo-17 mission landing site, consisting of unique geomorphological characteristics by having a sample size of both Lunar Maria and Highland regions. The dielectric constant is a parameter that gives an approximate distribution of the constituent material of the target area. It is a complex quantity, which indicates a periodic variation of the electric field. The real part of dielectric constant indicates stored energy and the imaginary part indicates dielectric loss factor or the loss of the electric field in the medium due to continuous varying electric field. Planetary surfaces for which determining dielectric constant is an important analysis for most of the space missions, ground measurement is not feasible. This work includes the machine learning-based modeling of dielectric constant for the Apollo 17 landing site the Taurus-Littrow valley. Based on the surface roughness of the study area, two models Gaussian and Exponential have been implemented and compared for the modeled output of the dielectric constant values.The modeling approaches for dielectric characterization of the lunar surface were implemented on NASA's LRO Mini-RF SAR data and Mini-SAR hybrid-pol data of ISRO's Chandrayaan-1 mission. The coefficient of determination (r2) and the root mean square error (RMSE) of the theoretical Gaussian model was 0.995, 0.042 and the Exponential model was 0.948, 0.1349 respectively. When compared with the already calculated values of dielectric constant from Apollo 17 return samples and literature survey, the Gaussian model gives a better variation. Gaussian model was further applied to the Lunar north pole crater namely Hermite-A crater, whose distinctive geomorphological characteristics and location being lunar north pole region, makes it one of the coldest places in the Solar System and a prominent location of water ice deposits.",space
10.1016/j.neuron.2020.11.021,Journal,Neuron,scopus,2021-02-17,sciencedirect,Using deep reinforcement learning to reveal how the brain encodes abstract state-space representations in high-dimensional environments,https://api.elsevier.com/content/abstract/scopus_id/85099151634,"Humans possess an exceptional aptitude to efficiently make decisions from high-dimensional sensory observations. However, it is unknown how the brain compactly represents the current state of the environment to guide this process. The deep Q-network (DQN) achieves this by capturing highly nonlinear mappings from multivariate inputs to the values of potential actions. We deployed DQN as a model of brain activity and behavior in participants playing three Atari video games during fMRI. Hidden layers of DQN exhibited a striking resemblance to voxel activity in a distributed sensorimotor network, extending throughout the dorsal visual pathway into posterior parietal cortex. Neural state-space representations emerged from nonlinear transformations of the pixel space bridging perception to action and reward. These transformations reshape axes to reflect relevant high-level features and strip away information about task-irrelevant sensory features. Our findings shed light on the neural encoding of task representations for decision-making in real-world situations.",space
10.1016/j.oceaneng.2020.108530,Journal,Ocean Engineering,scopus,2021-02-01,sciencedirect,Prediction and optimisation of fuel consumption for inland ships considering real-time status and environmental factors,https://api.elsevier.com/content/abstract/scopus_id/85098555033,"The information about ships’ fuel consumption is critical for condition monitoring, navigation planning, energy management and intelligent decision-making. Detailed analysis, modelling and optimisation of fuel consumption can provide great support for maritime management and operation and are of significance to water transportation. In this study, the real-time status monitoring data and hydrological data of inland ships are collected by multiple sensors, and a multi-source data processing method and a calculation method for real-time fuel consumption are proposed. Considering the influence of navigational status and environmental factors, including water depth, water speed, wind speed and wind angle, the Long Short-Term Memory (LSTM) neural network is then tailored and implemented to build models for prediction of real-time fuel consumption rate. The validation experiment shows the developed model performs better than some regression models and conventional Recurrent Neural Networks (RNNs). Finally, based on the fuel consumption rate model and the speed over ground model constructed by LSTM, the Reduced Space Searching Algorithm (RSSA) is successfully used to optimise the fuel consumption and the total cost of a whole voyage.",space
10.1016/j.micpro.2020.103579,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Development of cultural tourism platform based on FPGA and convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85097346419,"Data mining can be described as a typical analysis of large datasets to investigate early unknown types, styles, and interpersonal relationships to generate the right decision information. It improves their markets and today to maintain control over whether these companies are forced into the data mining tools and technologies they use to develop and manage tourism products and services in the market. It is falling out of the favorable situation of the travel and tourism industry. Objective work is to provide and display its application in data mining and tourism. Advances in mobile technology provide an opportunity to obtain real-time information of travelers, such as time and space behavior, at the destination they visit. This study analyzed a large-scale mobile phone data set to capture the mobile phone traces of international tourists who visited South Korea. We adopt the trajectory data mining method to understand tourism activities’ spatial structure in three different destinations. The research reveals tourist destinations and multiple “hot spots” (or popular areas) that interact spatially in these places through spatial cluster analysis and sequential pattern mining. Therefore, this article provides the planning of spatial model destinations to integrate important tourism influences, which is based on tourism design. The proposed system is modelled in Field Programmable Gate Array (FPGA) using Xilinx software.",space
10.1016/j.micpro.2020.103343,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Ecological evolution path of smart education platform based on deep learning and image detection,https://api.elsevier.com/content/abstract/scopus_id/85095585894,"Smart environments are becoming a reality in our society, and the number of smart devices integrated into these spaces is overgrowing. End users are being provided a simplified way to handle complex smart features, as the combination of smart elements opens up a wide range of new opportunities to facilitate. This article explores the significant challenges to be overcome in designing an intelligent educational environment for the main characteristics and the personalized support of ecology. To integrate intelligent learning environments into learning ecology and educational environments, innovative applications, and new teaching methods should be implemented to coordinate formal and informal learning. However, despite the increased use of smart learning environments in higher education, at the same time, there is an excellent network that does not define a set of demand models for the development and evaluation of smart learning environment education that considers teaching, evaluation, and design. Deep learning is one of the modern methods that can be used to automate the process of effective intellectual education based on image detection. The deep learning process is based on image discovery. It provides an overview of ecological evaluation based smart education level analysis used image detection. The system that has been proposed here is an intelligent education system that has been customized to provide the resources of the evolution of the ecosystem to the learner to suit their perceptions and education center to start the platform.",space
10.1016/j.micpro.2020.103318,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Enterprise financial cost management platform based on FPGA and neural network,https://api.elsevier.com/content/abstract/scopus_id/85094858518,"At present, the domestic costs of most construction companies are relatively scattered with the cost data of various business agents. Unless it is controlled by an experienced manager, decision-makers cannot have the real-time dynamic cost of a project. In the information age, it is of vital importance to use the information to control the cost of construction projects dynamically. Cost management and the establishment of an information platform are ways to control the platform integrated cost data, operators, computer software and hardware, and corresponding method information, and its core is cost data information. The place for financial cost analysis and decision making is a conceptually rich field where information is a commercial product which is complicated, extensive, and invaluable. In this model, first,a set of extracts from the macro-credit feature space is designed and then, FPGA and neural network (FPGA, NN) models for credit evaluation is built based on these indicators, eventually it is applied scientifically, and reasonably, practically. Several state credit metrics are randomly selected. Our model shows applications that are both practical and competent. Using this model, authorities can analyze local credit conditions, allowing investors to make wise decisions to invest while saving on operating and credit costs. Most importantly, this model can help impulsive local government leaders, businesses, and even everyone to enhance competitiveness and capacities of attractive regions, thereby foster a good atmosphere for a credit culture.",space
10.1016/j.jneumeth.2020.109006,Journal,Journal of Neuroscience Methods,scopus,2021-01-15,sciencedirect,Real-Time Point Process Filter for Multidimensional Decoding Problems Using Mixture Models,https://api.elsevier.com/content/abstract/scopus_id/85097710119,"There is an increasing demand for a computationally efficient and accurate point process filter solution for real-time decoding of population spiking activity in multidimensional spaces. Real-time tools for neural data analysis, specifically real-time neural decoding solutions open doors for developing experiments in a closed-loop setting and more versatile brain-machine interfaces. Over the past decade, the point process filter has been successfully applied in the decoding of behavioral and biological signals using spiking activity of an ensemble of cells; however, the filter solution is computationally expensive in multi-dimensional filtering problems. Here, we propose an approximate filter solution for a general point-process filter problem when the conditional intensity of a cell’s spiking activity is characterized using a Mixture of Gaussians. We propose the filter solution for a broader class of point process observation called marked point-process, which encompasses both clustered – mainly, called sorted – and clusterless – generally called unsorted or raw– spiking activity. We assume that the posterior distribution on each filtering time-step can be approximated using a Gaussian Mixture Model and propose a computationally efficient algorithm to estimate the optimal number of mixture components and their corresponding weights, mean, and covariance estimates. This algorithm provides a real-time solution for multi-dimensional point-process filter problem and attains accuracy comparable to the exact solution. Our solution takes advantage of mixture dropping and merging algorithms, which collectively control the growth of mixture components on each filtering time-step. We apply this methodology in decoding a rat’s position in both 1-D and 2-D spaces using clusterless spiking data of an ensemble of rat hippocampus place cells. The approximate solution in 1-D and 2-D decoding is more than 20 and 4,000 times faster than the exact solution, while their accuracy in decoding a rat position only drops by less than 9% and 4% in RMSE and 95% highest probability coverage area (HPD) performance metrics. Though the marked-point filter solution is better suited for real-time decoding problems, we discuss how the filter solution can be applied to sorted spike data to better reflect the proposed methodology versatility.",space
10.1016/j.knosys.2020.106580,Journal,Knowledge-Based Systems,scopus,2021-01-09,sciencedirect,Hybrid artificial neural network and cooperation search algorithm for nonlinear river flow time series forecasting in humid and semi-humid regions,https://api.elsevier.com/content/abstract/scopus_id/85095736783,"Accurate river flow forecasting is of great importance for the scientific management of water resources system. With the advantages of easy implementation and high flexibility, artificial neural network (ANN) has been widely employed to address the complex hydrological forecasting problem. However, the conventional ANN method often suffers from some defects in practice, like slow convergence and local minimum. In order to enhance the ANN performance, this study proposes a hybrid river flow forecasting method by integrating the novel cooperation search algorithm (CSA) into the learning process of ANN. In other words, the computational parameters of the ANN network (like threshold and linking weights) are iteratively optimized by the CSA method in the feasible state space. The proposed method is applied to the river flow data collected from two real-world hydrological stations in China. Several Quantitative indexes are chosen to compare the performance of the developed models, while the comprehensive analysis between the simulated and observed flow data are conducted. The experimental results show that in different scenarios, the hybrid method based on ANN and CSA always outperforms the control models and yields superior forecasting results during both training and testing phases. In Three Gorges Project, the presented method makes 11.10% and 5.42% improvements in the Nash–Sutcliffe efficiency and Coefficient correlation values of the standard ANN method in the testing phase. Thus, this interesting finding shows that the performance of the artificial intelligence models in river flow time series forecasting can be effectively improved by metaheuristic algorithm with outstanding global search ability.",space
10.1016/j.ifacol.2021.10.504,Conference Proceeding,IFAC-PapersOnLine,scopus,2021-01-01,sciencedirect,Advanced state fuzzy cognitive maps applied on nearly zero energy building model,https://api.elsevier.com/content/abstract/scopus_id/85120711263,"Fuzzy Cognitive Maps method combines the advantages of Fuzzy Logic, such as their human reasoning and linguistic features, with the advantages of Neural Networks, such as their low mathematical calculation requirements, in order to model complex dynamic systems on a wide variety of applications. The system variables and their interconnections are described using a graph and a weight matrix. Application of experts’ knowledge leads towards more realistic system models. In addition, the implementation of state-space theory in combination with learning algorithms, lead to a new generation of Fuzzy Cognitive Maps, the Advanced State Fuzzy Cognitive Maps. All the above are implemented on a nearly Zero Energy Building model, using real weather data and presenting its annual energy response.",space
10.1016/j.patrec.2021.10.014,Journal,Pattern Recognition Letters,scopus,2021-01-01,sciencedirect,Adversarial learning and decomposition-based domain generalization for face anti-spoofing,https://api.elsevier.com/content/abstract/scopus_id/85119451640,"Face anti-spoofing (FAS) plays a critical role in the face recognition community for securing the face presentation attacks. Many works have been proposed to regard FAS as a domain generalization problem for robust deployment in real-world scenarios. However, existing methods focus on extracting intrinsic spoofing cues to improve the generalization ability, yet neglect to train a robust classifier. In this paper, we propose a framework to improve the generalization ability of face anti-spoofing in two folds:) a generalized feature space is obtained via aggregation of all live faces while dispersing each domain’s spoof faces; and) a domain agnostic classifier is trained through low-rank decomposition. Specifically, a Common Specific Decomposition for Specific (CSD-S) layer is deployed in the last layer of the network to select common features while discarding domain-specific ones among multiple source domains. The above-mentioned two components are integrated into an end-to-end framework, ensuring the generalization ability to unseen scenarios. The extensive experiments demonstrate that the proposed method achieves state-of-the-art results on four public datasets, including CASIA-MFSD, MSU-MFSD, Replay-Attack, and OULU-NPU.",space
10.1016/j.mex.2021.101572,Journal,MethodsX,scopus,2021-01-01,sciencedirect,Methodology for using a Bayesian nonparametric model to uncover universal patterns in color naming,https://api.elsevier.com/content/abstract/scopus_id/85118571924,"Language is an integral part of society which enables communication among its members. To shed light on how words gain their meaning and how their meaning evolves over time, color naming is often used as a case study. The color domain can be defined by a physical space, making it a useful concept for studying denotation of meaning. Though humans can distinguish millions of colors, language provides us with a small, manageable set of terms for categorizing the space. Partitions of the color space vary across different language groups and evolve over time (e.g. new color terms may enter a language). Investigating universal patterns in color naming provides insight into the mechanisms that give rise to the observed data. Recently, computational techniques have been utilized to study this phenomenon. Here, we develop a methodology for transforming a color naming data set—namely, the World Color Survey—which is based on constraints imposed by the stimulus space. This transformed data is used to initialize a nonparametric Bayesian machine learning model in order to implement a culture and theory-independent study of universal color naming patterns across different language groups. All of the methods described are executed by our Python software package called ColorBBDP.
                  • Data from the World Color Survey is transformed from its original format into binary features vectors which can be given as input to the Beta-Bernoulli Dirichlet Process Mixture Model.
                  • This paper provides a specific application of Variational Inference on the Beta-Bernoulli Dirichlet Process Mixture Model towards a color naming data set.
                  • New mathematical measures for performing post-cluster analyses are also detailed in this paper.",space
10.1016/j.procs.2021.09.233,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,A note on the applications of artificial intelligence in the hospitality industry: Preliminary results of a survey,https://api.elsevier.com/content/abstract/scopus_id/85116885410,"Intelligent technologies are widely implemented in different areas of modern society but specific approaches should be applied in services. Basic relationships refer to supporting customers and people responsible for services offering for these customers. The aim of the paper is to analyze and evaluate the state-of-the art of artificial intelligence (AI) applications in the hospitality industry. Our findings show that the major deployments concern in-person customer services, chatbots and messaging tools, business intelligence tools powered by machine learning, and virtual reality & augmented reality. Moreover, we performed a survey (n = 178), asking respondents about their perceptions and attitudes toward AI, including its implementation within a hotel space. The paper attempts to discuss how the hotel industry can be motivated by potential customers to apply selected AI solutions. In our opinion, these results provide useful insights for understanding the phenomenon under investigation. Nevertheless, since the results are not conclusive, more research is still needed on this topic. Future studies may concern both qualitative and quantitative methods, devoted to developing models that: a) quantify the potential benefits and risks of AI implementations, b) determine and evaluate the factors affecting the AI adoption by the customers, and c) measure the user (guest) experience of the hotel services, fueled by AI-based technologies.",space
10.1016/j.isatra.2021.06.010,Journal,ISA Transactions,scopus,2021-01-01,sciencedirect,A real-world application of Markov chain Monte Carlo method for Bayesian trajectory control of a robotic manipulator,https://api.elsevier.com/content/abstract/scopus_id/85108508566,"Reinforcement learning methods are being applied to control problems in robotics domain. These algorithms are well suited for dealing with the continuous large scale state spaces in robotics field. Even though policy search methods related to stochastic gradient optimization algorithms have become a successful candidate for coping with challenging robotics and control problems in recent years, they may become unstable when abrupt variations occur in gradient computations. Moreover, they may end up with a locally optimal solution. To avoid these disadvantages, a Markov chain Monte Carlo (MCMC) algorithm for policy learning under the RL configuration is proposed. The policy space is explored in a non-contiguous manner such that higher reward regions have a higher probability of being visited. The proposed algorithm is applied in a risk-sensitive setting where the reward structure is multiplicative. Our method has the advantages of being model-free and gradient-free, as well as being suitable for real-world implementation. The merits of the proposed algorithm are shown with experimental evaluations on a 2-Degree of Freedom robot arm. The experiments demonstrate that it can perform a thorough policy space search while maintaining adequate control performance and can learn a complex trajectory control task within a small finite number of iteration steps.",space
10.1016/j.ejor.2020.12.024,Journal,European Journal of Operational Research,scopus,2021-01-01,sciencedirect,Robust low-rank multiple kernel learning with compound regularization,https://api.elsevier.com/content/abstract/scopus_id/85099124915,"Kernel learning is widely used in nonlinear models during the implementation of forecasting tasks in analytics. However, existing forecasting models lack robustness and accuracy. Therefore, in this study, a novel supervised forecasting approach based on robust low-rank multiple kernel learning with compound regularization is investigated. The proposed method extracts the benefits from robust regression, multiple kernel learning with low-rank approximation, and sparse learning systems. Unlike existing hybrid forecasting methods, which frequently combine different models in parallel, we embed a Huber or quantile loss function and a compound regularization composed of smoothly clipped absolute deviation and ridge regularizations in a support vector machine with predefined number of kernels. To select the optimal kernels, 
                        
                           L
                           1
                        
                      penalization with positive constraint is also considered. The proposed model exhibits robustness, forecasting accuracy, and sparsity in the reproducing kernel Hilbert space. For computation, a simple algorithm is designed based on local quadratic approximation to implement the proposed method. Theoretically, the forecasting and estimation error bounds of the proposed estimators are derived under a null consistency assumption. Real data experiments using datasets from various scientific research fields demonstrate the superior performances of the proposed approach compared with other state-of-the-art competitors.",space
10.1016/j.cogsys.2020.10.005,Journal,Cognitive Systems Research,scopus,2021-01-01,sciencedirect,Cogmic space for narrative-based world representation,https://api.elsevier.com/content/abstract/scopus_id/85096684889,"Representing a world or a physical/social environment in an agent’s cognitive system is essential for creating human-like artificial intelligence. This study takes a story-centered approach to this issue. In this context, a story refers to an internal representation involving a narrative structure, which is assumed to be a common form of organizing past, present, future, and fictional events and situations. In the artificial intelligence field, a story or narrative is traditionally treated as a symbolic representation. However, a symbolic story representation is limited in its representational power to construct a rich world. For example, a symbolic story representation is unfit to handle the sensory/bodily dimension of a world. In search of a computational theory for narrative-based world representation, this study proposes the conceptual framework of a Cogmic Space for a comic strip-like representation of a world. In the proposed framework, a story is positioned as a mid-level representation, in which the conceptual and sensory/bodily dimensions of a world are unified. The events and their background situations that constitute a story are unified into a sequence of panels. Based on this structure, a representation (i.e., a story) and the represented environment are connected via an isomorphism of their temporal, spatial, and relational structures. Furthermore, the framework of a Cogmic Space is associated with the generative aspect of representations, which is conceptualized in terms of unconscious- and conscious-level processes/representations. Finally, a proof-of-concept implementation is presented to provide a concrete account of the proposed framework.",space
10.1016/j.ajo.2020.07.020,Journal,American Journal of Ophthalmology,scopus,2021-01-01,sciencedirect,Lightweight Learning-Based Automatic Segmentation of Subretinal Blebs on Microscope-Integrated Optical Coherence Tomography Images,https://api.elsevier.com/content/abstract/scopus_id/85092610123,"Purpose
                  Subretinal injections of therapeutics are commonly used to treat ocular diseases. Accurate dosing of therapeutics at target locations is crucial but difficult to achieve using subretinal injections due to leakage, and there is no method available to measure the volume of therapeutics successfully administered to the subretinal location during surgery. Here, we introduce the first automatic method for quantifying the volume of subretinal blebs, using porcine eyes injected with Ringer's lactate solution as samples.
               
                  Design
                  Ex vivo animal study.
               
                  Methods
                  Microscope-integrated optical coherence tomography was used to obtain 3D visualization of subretinal blebs in porcine eyes at Duke Eye Center. Two different injection phases were imaged and analyzed in 15 eyes (30 volumes), selected from a total of 37 eyes. The inclusion/exclusion criteria were set independently from the algorithm-development and testing team. A novel lightweight, deep learning–based algorithm was designed to segment subretinal bleb boundaries. A cross-validation method was used to avoid selection bias. An ensemble-classifier strategy was applied to generate final results for the test dataset.
               
                  Results
                  The algorithm performs notably better than 4 other state-of-the-art deep learning–based segmentation methods, achieving an F1 score of 93.86 ± 1.17% and 96.90 ± 0.59% on the independent test data for entry and full blebs, respectively.
               
                  Conclusion
                  The proposed algorithm accurately segmented the volumetric boundaries of Ringer's lactate solution delivered into the subretinal space of porcine eyes with robust performance and real-time speed. This is the first step for future applications in computer-guided delivery of therapeutics into the subretinal space in human subjects.",space
10.1016/j.jobe.2020.101854,Journal,Journal of Building Engineering,scopus,2021-01-01,sciencedirect,Modeling for indoor temperature prediction based on time-delay and Elman neural network in air conditioning system,https://api.elsevier.com/content/abstract/scopus_id/85092148207,"An effective indoor temperature model would assist in improving energy efficiency and indoor thermal comfort of air conditioning system. However, it is difficult to build an accurate model due to lag response characteristic in the regulation process of indoor temperature. To solve this problem, the modeling and prediction methods for indoor temperature lag response characteristic based on time-delay neural network (TDNN) and Elman network neural (ENN) are presented. Then, taking variable air volume (VAV) air conditioning system as the study object, the effectiveness and practicability of proposed methods are validated using simulation sampling data and real-time operating data. Results indicate that ENN could be considered as a better modeling method for indoor temperature prediction for its simpler network structure, smaller storing space and better prediction accuracy. The contribution of this study is to provide an applicable online ANN modeling method for indoor temperature lag characteristic, and detailed training and validation for online implementation are presented, which will benefit for engineers and technicians to use in practical engineering. Meanwhile, this study provides the reference for online application of advanced intelligent algorithms in the building engineering.",space
10.1016/j.ymssp.2020.107061,Journal,Mechanical Systems and Signal Processing,scopus,2021-01-01,sciencedirect,Recovering compressed images for automatic crack segmentation using generative models,https://api.elsevier.com/content/abstract/scopus_id/85086994715,"In a structural health monitoring (SHM) system that uses digital cameras to monitor cracks of structural surfaces, techniques for reliable and effective data compression are essential to ensure a stable and energy-efficient crack images transmission in wireless devices, e.g., drones and robots with high definition cameras installed. Compressive sensing (CS) is a signal processing technique that allows accurate recovery of a signal from a sampling rate much smaller than the limitation of the Nyquist sampling theorem. Different from the popular approach of simultaneously training encoder and decoder using neural network models, the CS theory ensures a high probability of accurate signal reconstruction based on random measurements that is shorter than the length of the original signal under a sparsity constraint. Such method is particularly useful when measurements are expensive, such as wireless sensing of civil structures, because its hardware implementation allows down sampling of signals during the sensing process. Hence, CS methods can achieve significant energy saving for the sensing devices. However, the strong assumption of the signals being highly sparse in an invertible space is relatively hard to guarantee for many real images, such as image of cracks. In this paper, we present a new approach of CS that replaces the sparsity regularization with a generative model that is able to effectively capture a low dimension representation of targeted images. We develop a recovery framework for automatic crack segmentation of compressed crack images based on this new CS method. We demonstrate the remarkable performance of our method that takes advantage of the strong capability of generative models to capture the necessary features required in the crack segmentation task even the backgrounds of the generated images are not well reconstructed. The superior performance of our recovery framework is illustrated by comparisons to three existing CS algorithms. Furthermore, we show that our framework is potentially extensible to other common problems in automatic crack segmentation, such as defect recovery from motion blurring and occlusion.",space
10.1016/j.neucom.2019.12.006,Journal,Neurocomputing,scopus,2020-12-22,sciencedirect,Learning salient features to prevent model drift for correlation tracking,https://api.elsevier.com/content/abstract/scopus_id/85089801054,"Correlation Filter (CF) based algorithms play an important role in the field of Visual Object Tracking (VOT) due to their high accuracy and low computational complexity. While existing CF tracking algorithms suffer performance degradation due to inaccurate object modeling. In this paper, we improve the object modeling accuracy in both CF training stage and target detection procedure to preventing the drift problem. Specifically, we propose a multi-model structure for CF trackers to capture the target appearance changes, where different appearance models are trained with specific samples to catch the salient features of the target and reduce the computational cost. Furthermore, a space filter for detection features is designed to suppress the boundary effect under Gaussian motion prior, which contributes to improving the accuracy of position estimation. We deploy our method to three hand-crafted features based CF trackers to perform real-time visual tracking on popular benchmarks. The experimental results demonstrate the efficacy of our proposed scheme and the efficiency of our trackers. In addition, we provide a comprehensive analysis of the proposed method to facilitate application.",space
10.1016/j.iot.2020.100301,Journal,Internet of Things (Netherlands),scopus,2020-12-01,sciencedirect,Predicting parking occupancy via machine learning in the web of things,https://api.elsevier.com/content/abstract/scopus_id/85098729769,"The Web of Things (WoT) enables information gathered by sensors deployed in urban environments to be easily shared utilizing open Web standards and semantic technologies, creating easier integration with other Web-based information, towards advanced knowledge. Besides WoT, an essential aspect of understanding dynamic urban systems is artificial intelligence (AI). Via AI, data produced by WoT-enabled sensory observations can be analyzed and transformed into meaningful information, which describes and predicts current and future situations in time and space. This paper examines the impact of WoT and AI in smart cities, considering a real-world problem, the one of predicting parking availability. Traffic cameras are used as WoT sensors, together with weather forecasting Web services. Machine learning (ML) is employed for AI analysis, using predictive models based on neural networks and random forests. The performance of the ML models for the prediction of parking occupancy is better than the state of the art work in the problem under study, scoring an MSE of 7.18 at a time horizon of 60 minutes.",space
10.1016/j.jpdc.2020.08.008,Journal,Journal of Parallel and Distributed Computing,scopus,2020-12-01,sciencedirect,Towards cost-effective service migration in mobile edge: A Q-learning approach,https://api.elsevier.com/content/abstract/scopus_id/85090113588,"Service migration in mobile edge computing is a promising approach to improving the quality of service (QoS) for mobile users and reducing the network operational cost for service providers as well. However, these benefits are not free, coming at costs of bulk-data transfer, and likely service disruption, which could consequently increase the overall service costs. To gain the benefits of service migration while minimizing its cost across the edge nodes, in this paper, we leverage reinforcement learning (RL) method to design a cost-effective framework, called Mig-RL, for the service migration with a reduction of total service costs as a goal in a mobile edge environment. The Mig-RL leverages the infrastructure of edge network and deploys a migration agent through Q-learning to learn the optimal policy with respect to the service migration status. We distinguish the Mig-RL from other existing works in several major aspects. First, we fully exploit the nature of this problem in a modest migration space, which allows us to constrain the number of service replicas whereby a defined state–action space could be effectively handled, as opposed to those methods that need to always approximate a huge state–action space for policy optimality. Second, we advocate a migration policy-base as a cache to save the learning process by retrieving the most effective policy whenever a similar migration pattern is encountered as time goes on. Finally, by exploiting the idea of software defined network, we also investigate the efficient implementation of Mig-RL in mobile edge network. Experimental results based on some real and synthesized access sequences show that Mig-RL, compared with the selected existing algorithms, can substantially minimize the service costs, and in the meantime, efficiently improve the QoS by adapting to the changes of mobile access patterns.",space
10.1016/j.inffus.2020.07.003,Journal,Information Fusion,scopus,2020-12-01,sciencedirect,"Data fusion strategies for energy efficiency in buildings: Overview, challenges and novel orientations",https://api.elsevier.com/content/abstract/scopus_id/85087624082,"Recently, tremendous interest has been devoted to develop data fusion strategies for energy efficiency in buildings, where various kinds of information can be processed. However, applying the appropriate data fusion strategy to design an efficient energy efficiency system is not straightforward; it requires a priori knowledge of existing fusion strategies, their applications and their properties. To this regard, seeking to provide the energy research community with a better understanding of data fusion strategies in building energy saving systems, their principles, advantages, and potential applications, this paper proposes an extensive survey of existing data fusion mechanisms deployed to reduce excessive consumption and promote sustainability. We investigate their conceptualizations, advantages, challenges and drawbacks, as well as performing a taxonomy of existing data fusion strategies and other contributing factors. Following, a comprehensive comparison of the state-of-the-art data fusion based energy efficiency frameworks is conducted using various parameters, including data fusion level, data fusion techniques, behavioral change influencer, behavioral change incentive, recorded data, platform architecture, IoT technology and application scenario. Moreover, a novel method for electrical appliance identification is proposed based on the fusion of 2D local texture descriptors, where 1D power signals are transformed into 2D space and treated as images. The empirical evaluation, conducted on three real datasets, shows promising performance, in which up to 99.68% accuracy and 99.52% F1 score have been attained. In addition, various open research challenges and future orientations to improve data fusion based energy efficiency ecosystems are explored.",space
10.1016/j.neunet.2020.08.012,Journal,Neural Networks,scopus,2020-12-01,sciencedirect,Latent Dirichlet allocation based generative adversarial networks,https://api.elsevier.com/content/abstract/scopus_id/85083895792,"Generative adversarial networks have been extensively studied in recent years and powered a wide range of applications, ranging from image generation, image-to-image translation, to text-to-image generation, and visual recognition. These methods typically model the mapping from latent space to image with single or multiple generators. However, they have obvious drawbacks: (i) ignoring the multi-modal structure of images, and (ii) lacking model interpretability. Importantly, the existing methods mostly assume one or more generators can cover all image modes even if we do not know the structure of data. Thus, mode dropping and collapse often take place along with GANs training. Despite the importance of exploring the data structure in generation, it has been almost unexplored. In this work, aiming at generating multi-modal images and interpreting model explicitly, we explore the theory on how to integrate GANs with data structure prior, and propose latent Dirichlet allocation based generative adversarial networks (LDAGAN). This framework is extended to combine with a variety of state-of-the-art single-generator GANs and achieves improved performance. Extensive experiments on synthetic and real datasets demonstrate the efficacy of LDAGAN for multi-modal image generation. An implementation of LDAGAN is available at https://github.com/Sumching/LDAGAN.",space
10.1016/j.knosys.2020.106457,Journal,Knowledge-Based Systems,scopus,2020-11-15,sciencedirect,Mining high utility itemsets using extended chain structure and utility machine,https://api.elsevier.com/content/abstract/scopus_id/85091059855,"High utility itemsets are sets of items that have a high utility (e.g. a high profit or a high importance) in a transaction database. Discovering high utility itemsets has many important applications in real-life such as market basket analysis. Nonetheless, mining these patterns is a time-consuming process due to the huge search space and the high cost of utility computation. Most of previous work is devoted to search space pruning but pay little attention to utility computation. Factually, not only search space pruning but also high utility itemset identification have to resort to the computation of various utilities. This paper proposes a novel algorithm named REX (Rapid itEmset eXtraction), which extends the classic d
                        
                           
                           
                              2
                           
                        
                     HUP algorithm with an improved structure, a 
                        k
                     -item utility machine, and an efficient switch strategy. The structure can significantly reduce the time complexity of utility computation compared with the original structure used in d
                        
                           
                           
                              2
                           
                        
                     HUP. The machine can quickly merge identical transactions and applies an efficient procedure for computing the utilities of extensions of a given itemset. The strategy derived from trial and error drastically gives rise to performance improvement on some databases and is also competitive with the switch strategy used in d
                        
                           
                           
                              2
                           
                        
                     HUP on other databases. Experimental results show that REX achieves a speedup of from fifty percent to three orders of magnitude over d
                        
                           
                           
                              2
                           
                        
                     HUP even though they use identical pruning techniques and that REX considerably outperforms state-of-the-art algorithms on real-life and synthetic databases.",space
10.1016/j.comnet.2020.107436,Journal,Computer Networks,scopus,2020-11-09,sciencedirect,Arena: A 64-antenna SDR-based ceiling grid testing platform for sub-6 GHz 5G-and-Beyond radio spectrum research,https://api.elsevier.com/content/abstract/scopus_id/85090236272,"Arena is an open-access wireless testing platform based on a grid of antennas mounted on the ceiling of a large office-space environment. Each antenna is connected to programmable software-defined radios (SDR) enabling sub-6 GHz 5G-and-beyond spectrum research. With 12 computational servers, 24 SDRs synchronized at the symbol level, and a total of 64 antennas, Arena provides the computational power and the scale to foster new technology development in some of the most crowded spectrum bands. Arena is based on a three-tier design, where the servers and the SDRs are housed in a double rack in a dedicated room, while the antennas are hung off the ceiling of a 2240 square feet office space and cabled to the radios through 100 ft-long cables. This ensures a reconfigurable, scalable, and repeatable real-time experimental evaluation in a real wireless indoor environment.
                  In this paper, we introduce the architecture, capabilities, and system design choices of Arena, and provides details of the software and hardware implementation of various testbed components. Furthermore, we describe key capabilities by providing examples of published work that employed Arena for applications as diverse as synchronized MIMO transmission schemes, multi-hop ad hoc networking, multi-cell 5G networks, AI-powered Radio-Frequency fingerprinting, secure wireless communications, and spectrum sensing for cognitive radio.",space
10.1016/j.neunet.2020.07.028,Journal,Neural Networks,scopus,2020-11-01,sciencedirect,Compressing 3DCNNs based on tensor train decomposition,https://api.elsevier.com/content/abstract/scopus_id/85089391288,"Three-dimensional convolutional neural networks (3DCNNs) have been applied in many tasks, e.g., video and 3D point cloud recognition. However, due to the higher dimension of convolutional kernels, the space complexity of 3DCNNs is generally larger than that of traditional two-dimensional convolutional neural networks (2DCNNs). To miniaturize 3DCNNs for the deployment in confining environments such as embedded devices, neural network compression is a promising approach. In this work, we adopt the tensor train (TT) decomposition, a straightforward and simple in situ training compression method, to shrink the 3DCNN models. Through proposing tensorizing 3D convolutional kernels in TT format, we investigate how to select appropriate TT ranks for achieving higher compression ratio. We have also discussed the redundancy of 3D convolutional kernels for compression, core significance and future directions of this work, as well as the theoretical computation complexity versus practical executing time of convolution in TT. In the light of multiple contrast experiments based on VIVA challenge, UCF11, UCF101, and ModelNet40 datasets, we conclude that TT decomposition can compress 3DCNNs by around one hundred times without significant accuracy loss, which will enable its applications in extensive real world scenarios.",space
10.1016/j.sigpro.2020.107717,Journal,Signal Processing,scopus,2020-11-01,sciencedirect,Fast and efficient implementation of image filtering using a side window convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85088128229,"Convolutional neural networks (CNNs) designed for object recognition have been successfully applied to low-level tasks such as image filtering. However, these networks are usually very large which occupy large memory space and demand very high computational capacity. This makes them unsuitable for real time low-level applications on smart and portable devices with limited memory and computational capacities. In this paper, we specifically design a novel CNN, side window convolutional neural network (SW-CNN), for the fast and efficient implementation of image filtering. In SW-CNN, a new convolutional strategy, called side kernel convolution (SKC) is proposed which aligns the side or corner of the convolutional window with the pixels under processing to preserve edges during convolution. By combining SKC and the representational power of CNNs, SW-CNN can learn various image-filtering tasks very effectively. Compared to the state-of-the-art networks, the superiority of SW-CNN includes three aspects. First, the number of learnable parameters is reduced by 96%. Second, the memory consumption is reduced to 50%. Third, the running time is decreased to 50%. Results of extensive experiments demonstrate that SW-CNN not only has good performance on implementing various edge-preserving filters, but also has the adaptability and flexibility on other low-level image processing applications.",space
10.1016/j.physd.2020.132615,Journal,Physica D: Nonlinear Phenomena,scopus,2020-11-01,sciencedirect,A Reduced Order Deep Data Assimilation model,https://api.elsevier.com/content/abstract/scopus_id/85087917230,"A new Reduced Order Deep Data Assimilation (RODDA) model combining Reduced order models (ROM), Data Assimilation (DA) and Machine Learning is proposed in this paper. The RODDA model aims to improve the accuracy of Computational Fluid Dynamics (CFD) simulations. The DA model ingests information from observed data in the simulation provided by the CFD model. The results of the DA are used to train a neural network learning a function which predicts the misfit between the results of the CFD model and the DA model. Thus, the trained function is combined with the original CFD model in order to generate forecasts with implicit DA given by neural network. Due to the time complexity of the numerical models used to implement DA and the neural network, and due to the scale of the forecasting area considered for forecasting problems in real case scenarios, the implementation of RODDA mandated the introduction of opportune reduced spaces. Here, RODDA is applied to a CFD simulation for air pollution, using the CFD software Fluidity, in South London (UK). We show that, using this framework, the data forecasted by the coupled model CFD+RODDA are closer to the observations with a gain in terms of execution time with respect to the classic prediction–correction cycle given by coupling CFD with a standard DA. Additionally, RODDA predicts future observations, if not available, since these are embedded in the data assimilated state in which the network is trained on. The RODDA framework is not exclusive to air pollution, Fluidity, or the study area in South London, and therefore the workflow could be applied to different physical models if enough temporal data are available.",space
10.1016/j.cpc.2020.107365,Journal,Computer Physics Communications,scopus,2020-11-01,sciencedirect,CONUNDrum: A program for orbital-free density functional theory calculations,https://api.elsevier.com/content/abstract/scopus_id/85086471143,"We present a new code for energy minimization, structure relaxation and evaluation of bulk parameters in the framework of orbital-free density functional theory (OF-DFT). The implementation is based on solving the Euler–Lagrange equation on an equidistant real space grid on which density dependent variables and derivatives are computed. Some potential components are computed in Fourier space. The code is able to use semilocal and non-local kinetic energy functionals (KEF) as well as neural network based KEFs thus facilitating testing and development of emerging machine-learned KEFs. For semi-local and machine-learned KEFs the kinetic energy potentials are evaluated with real-space differentiation of the components, which are partial derivatives of the KE with respect to the electron density, its gradient and Laplacian.
               
                  Program summary
                  
                     Program title: CONUNDrum.
                  
                     CPC Library link to program files: 
                     http://dx.doi.org/10.17632/phnz2gg8mz.1
                  
                  
                     Licensing provision: GNU GPL v3
                  
                     Programming language: C++
                  
                     External routines: Fastest Fourier Transform in the West (FFTW) library (http://www.fftw.org/)
                  
                     Nature of problem: Calculation of the electronic and structural properties of molecules and extended systems in the framework of the orbital-free density functional theory. Evaluation of the bulk parameters of solid compounds.
                  
                     Solution method: High-order central finite-difference method and fast Fourier transform are used for calculation of different total energy components. Density optimization is performed with the steepest descent or the Polak–Ribière variant of the non-linear conjugate-gradient method with a line search procedure based on the Armijo condition. A numerical approach is used for structural optimization — the total energies with respect to small variations in lattice geometries are computed directly, with subsequent evaluation of the force components via a high-order central-finite difference method. The same numerical procedure is used for evaluation of bulk properties.
                  
                     Restrictions: Local pseudopotentials.",space
10.1016/j.cels.2020.08.016,Journal,Cell Systems,scopus,2020-10-21,sciencedirect,Fast and Flexible Protein Design Using Deep Graph Neural Networks,https://api.elsevier.com/content/abstract/scopus_id/85092248944,"Protein structure and function is determined by the arrangement of the linear sequence of amino acids in 3D space. We show that a deep graph neural network, ProteinSolver, can precisely design sequences that fold into a predetermined shape by phrasing this challenge as a constraint satisfaction problem (CSP), akin to Sudoku puzzles. We trained ProteinSolver on over 70,000,000 real protein sequences corresponding to over 80,000 structures. We show that our method rapidly designs new protein sequences and benchmark them in silico using energy-based scores, molecular dynamics, and structure prediction methods. As a proof-of-principle validation, we use ProteinSolver to generate sequences that match the structure of serum albumin, then synthesize the top-scoring design and validate it in vitro using circular dichroism. ProteinSolver is freely available at http://design.proteinsolver.org and https://gitlab.com/ostrokach/proteinsolver. A record of this paper’s transparent peer review process is included in the Supplemental Information.",space
10.1016/j.ijleo.2020.165205,Journal,Optik,scopus,2020-10-01,sciencedirect,Pattern recognition based on pulse scanning imaging and convolutional neural network for vibrational events in Φ-OTDR,https://api.elsevier.com/content/abstract/scopus_id/85087790249,"Feature extraction method of a phase-sensitive optical time-domain reflectometer distributed optical fiber vibration detection system requires a priori knowledge. A lack of feature evaluation methods leads to a low pattern recognition accuracy. Traditional pattern recognition methods cannot be widely applied. This paper presents the implementation of a deep learning-based method to identify vibration signal categories. First, the vibration signal was reconstructed in the time and space domain, which is regarded as a pulse scanning image. Secondly, moving average was used to reduce noise, and seeking the signal envelope surface as an image sample. Finally, the image sample was inputted into the trained convolutional neural network (CNN) to obtain recognition results. Experiments showed that the phase-sensitive optical time-domain reflectometer pulse scanning imaging pattern recognition method based on deep learning proposed in this paper improved recognition accuracy while ensuring recognition efficiency. The algorithm is easy to implement and apply and satisfies the requirements of real-time online monitoring.",space
10.1016/j.ast.2020.105953,Journal,Aerospace Science and Technology,scopus,2020-10-01,sciencedirect,A reliable data-driven model for Ablative Pulsed Plasma Thruster,https://api.elsevier.com/content/abstract/scopus_id/85087284442,"Ablative Pulsed Plasma Thrusters (APPTs) are high specific impulse electric space propulsion system, but a reliable model equivalent of the experimental model is still unavailable. In this paper, a reliable model is developed based on APPT experimental data by using Machine Learning (ML) ecosystem. The goals of this study are to justify the accuracy and reliability of the newly built APPT model with the existing experimental and simulation model. For four sets of operating conditions, 600 experimental and simulation test operations are done. The experimental voltages and currents are measured with a high-voltage probe and a Rogowski coil, respectively. The simulation voltages and currents are gathered by running the respective simulation program. Comparison results show that the newly built APPT model has better accuracy and reliability than the simulated APPT model as compared to real APPT used in the experiment. This data-driven approach provides a novel way of designing a reliable alternative model of physical APPTs.",space
10.1016/j.rser.2020.109920,Journal,Renewable and Sustainable Energy Reviews,scopus,2020-09-01,sciencedirect,Virtual testbed for model predictive control development in district cooling systems,https://api.elsevier.com/content/abstract/scopus_id/85086021217,"Recently, with increasing cooling demands, district cooling has assumed an important role as it is more efficient than stand-alone cooling systems. District cooling reduces the environmental impact and promotes the use of renewable sources. Earlier studies to optimise the production plants of district cooling systems were focused primarily on plants with compressor chillers and thermal energy storage devices. Although absorption chillers are crucial for integrating renewable sources into these systems, very few studies have considered them from the cooling perspective. In this regard, this paper presents the progress and results of the implementation of a virtual testbed based on a digital twin of a district cooling production plant with both compressor and absorption chillers. The aim of this study, carried out within the framework of INDIGO, a European Union-funded project, was (i) to develop a reliable model that can be used in a model predictive controller and (ii) to simulate the plant using this controller. The production plant components, which included absorption and compressor chillers, as well as cooling towers, were built using the equation-based Modelica programming language, and were calibrated using information from the manufacturer, together with real operation data. The remainder of the plant was modelled in Python. To integrate the Modelica models into the Python environment, a combination of machine learning techniques and state-space representation models was used. With these techniques, models with a high computational speed were obtained, which were suitable for real-time applications. These models were then used to build a model predictive control for the production plant to minimise the primary energy usage. The improvements in the control and the resultant energy savings achieved were compared with a baseline case working on a standard cascade control. Energy savings up to 50% were obtained in the simulation-based experiments.",space
10.1016/j.neucom.2020.02.104,Journal,Neurocomputing,scopus,2020-08-18,sciencedirect,An overview of recent multi-view clustering,https://api.elsevier.com/content/abstract/scopus_id/85083344834,"With the widespread deployment of sensors and the Internet-of-Things, multi-view data has become more common and publicly available. Compared to traditional data that describes objects from single perspective, multi-view data is semantically richer, more useful, however more complex. Since traditional clustering algorithms cannot handle such data, multi-view clustering has become a research hotspot. In this paper, we review some of the latest multi-view clustering algorithms, which are reasonably divided into three categories. To evaluate their performance, we perform extensive experiments on seven real-world data sets. Three mainstream metrics are used, including clustering accuracy, normalized mutual information and purity. Based on the experimental results and a large number of literature reading, we also discuss existing problems in current multi-view clustering and point out possible research directions in the future. This research provides some insights for researchers in related fields and may further promote the development of multi-view clustering algorithms.",space
10.1016/j.patter.2020.100074,Journal,Patterns,scopus,2020-08-14,sciencedirect,Machine-Learning Approaches in COVID-19 Survival Analysis and Discharge-Time Likelihood Prediction Using Clinical Data,https://api.elsevier.com/content/abstract/scopus_id/85092796371,"As a highly contagious respiratory disease, COVID-19 has yielded high mortality rates since its emergence in December 2019. As the number of COVID-19 cases soars in epicenters, health officials are warning about the possibility of the designated treatment centers being overwhelmed by coronavirus patients. In this study, several computational techniques are implemented to analyze the survival characteristics of 1,182 patients. The computational results agree with the outcome reported in early clinical reports released for a group of patients from China that confirmed a higher mortality rate in men compared with women and in older age groups. The discharge-time prediction of COVID-19 patients was also evaluated using different machine-learning and statistical analysis methods. The results indicate that the Gradient Boosting survival model outperforms other models for patient survival prediction in this study. This research study is aimed to help health officials make more educated decisions during the outbreak.",space
10.1016/j.trc.2020.102649,Journal,Transportation Research Part C: Emerging Technologies,scopus,2020-08-01,sciencedirect,Differential variable speed limits control for freeway recurrent bottlenecks via deep actor-critic algorithm,https://api.elsevier.com/content/abstract/scopus_id/85086802562,"Variable speed limit (VSL) control is a flexible way to improve traffic conditions, increase safety, and reduce emissions. There is an emerging trend of using reinforcement learning methods for VSL control. Currently, deep learning is enabling reinforcement learning to develop autonomous control agents for problems that were previously intractable. In this paper, a more effective deep reinforcement learning (DRL) model is developed for differential variable speed limit (DVSL) control, in which dynamic and distinct speed limits among lanes can be imposed. The proposed DRL model uses a novel actor-critic architecture to learn a large number of discrete speed limits in a continuous action space. Different reward signals, such as total travel time, bottleneck speed, emergency braking, and vehicular emissions are used to train the DVSL controller, and a comparison between these reward signals is conducted. The proposed DRL-based DVSL controllers are tested on a freeway with a simulated recurrent bottleneck. The simulation results show that the DRL based DVSL control strategy is able to improve the safety, efficiency and environment-friendliness of the freeway. In order to verify whether the controller generalizes to real world implementation, we also evaluate the generalization of the controllers on environments with different driving behavior attributes. and the robustness of the DRL agent is observed from the results.",space
10.1016/j.ast.2020.105902,Journal,Aerospace Science and Technology,scopus,2020-08-01,sciencedirect,Hybrid MultiGene Genetic Programming - Artificial neural networks approach for dynamic performance prediction of an aeroengine,https://api.elsevier.com/content/abstract/scopus_id/85086503677,"Dynamic aeroengine models have an important role in the design of real-time control systems. Modelling of aeroengines using dynamic performance simulations is a key step in the design process in order to reduce costs and the development period. A dynamic model can provide a numerical counterpart for the development of control systems and for the study of the engine behaviour in both steady and unsteady scenarios. The latter situation is particularly felt in the military field. The Viper 632-43 engine analysed in this work is a military turbojet, so it was necessary to develop a model that would replicate its behaviour as realistically as possible. The model was built using the Gas turbine Simulation Program (GSP) software and validated both in steady and transient conditions.
                  Once the engine model was validated, different machine learning techniques were used to estimate (data mining) and predict an engine parameter; the Exhaust Gas Temperature (EGT) has been chosen as the key parameter. A MultiGene Genetic Programming (MGGP) technique has been used to derive simple mathematical relationships between different input parameters and the EGT. These, then, can be used to calculate the EGT value of a real Viper 632-43 engine knowing a priori the input parameters and in any operating condition.
                  Finally, the EGT estimated by this algorithm has been added to the dataset used for the one-step-ahead EGT prediction by Artificial Neural Network (ANN). A time-series ANN was used for the EGT prediction, i.e. the Nonlinear AutoRegressive with eXogenous inputs (NARX) neural network. This network recognizes the input data as a real time series and is therefore able to predict the output in the next time step. It was chosen to use, as forecasting method, the one-step-ahead technique which allows to predict the EGT in the immediately next time step.",space
10.1016/j.automatica.2020.109032,Journal,Automatica,scopus,2020-08-01,sciencedirect,Efficient spatio-temporal Gaussian regression via Kalman filtering,https://api.elsevier.com/content/abstract/scopus_id/85084595084,"We study the non-parametric reconstruction of spatio-temporal dynamical processes via Gaussian Processes (GPs) regression from sparse and noisy data. GPs have been mainly applied to spatial regression where they represent one of the most powerful estimation approaches also thanks to their universal representing properties. Their extension to dynamical processes has been instead elusive so far since classical implementations lead to unscalable algorithms or require some sort of approximation. We propose a novel procedure to address this problem by coupling GPs regression and Kalman filtering. In particular, assuming space/time separability of the covariance (kernel) of the process and rational time spectrum, we build a finite-dimensional discrete-time state-space process representation amenable to Kalman filtering. With sampling over a finite set of fixed spatial locations, our major finding is that the current Kalman filter state represents a sufficient statistic to compute the minimum variance estimate of the process at any future time over the entire spatial domain. In machine learning, a representer theorem states that an important class of infinite-dimensional variational problems admits a computable and finite-dimensional exact solution. In view of this, our result can be interpreted as a novel Dynamic Representer Theorem for GPs. We then extend the study to situations where the spatial input locations set varies over time. The proposed algorithms are tested on both synthetic and real field data, providing comparisons with standard GP and truncated GP regression techniques.",space
10.1016/j.neucom.2020.02.035,Journal,Neurocomputing,scopus,2020-07-20,sciencedirect,Sparse low rank factorization for deep neural network compression,https://api.elsevier.com/content/abstract/scopus_id/85081399638,"Storing and processing millions of parameters in deep neural networks is highly challenging during the deployment of model in real-time application on resource constrained devices. Popular low-rank approximation approach singular value decomposition (SVD) is generally applied to the weights of fully connected layers where compact storage is achieved by keeping only the most prominent components of the decomposed matrices. Years of research on pruning-based neural network model compression revealed that the relative importance or contribution of each neuron in a layer highly vary among each other. Recently, synapses pruning has also demonstrated that having sparse matrices in network architecture achieve lower space and faster computation during inference time. We extend these arguments by proposing that the low-rank decomposition of weight matrices should also consider significance of both input as well as output neurons of a layer. Combining the ideas of sparsity and existence of unequal contributions of neurons towards achieving the target, we propose sparse low rank (SLR) method which sparsifies SVD matrices to achieve better compression rate by keeping lower rank for unimportant neurons. We demonstrate the effectiveness of our method in compressing famous convolutional neural networks based image recognition frameworks which are trained on popular datasets. Experimental results show that the proposed approach SLR outperforms vanilla truncated SVD and a pruning baseline, achieving better compression rates with minimal or no loss in the accuracy. Code of the proposed approach is avaialble at https://github.com/sridarah/slr.",space
10.1016/j.pmcj.2020.101210,Journal,Pervasive and Mobile Computing,scopus,2020-07-01,sciencedirect,A Deep Learning approach for Path Prediction in a Location-based IoT system,https://api.elsevier.com/content/abstract/scopus_id/85086567816,"Knowing in real-time the position of objects and people, both in indoor and outdoor spaces, allows companies and organizations to improve their processes and offer new kind of services. Nowadays Location-based Services (LBS) generate a significant amount of data thank to the widespread of the Internet of Things; since they have been quickly perceived as a potential source of profit, several companies have started to design and develop a wide range of such services. One of the most challenging research tasks is undoubtedly represented by the analysis of LBS data through Machine Learning algorithms and methodologies in order to infer new knowledge and build-up even more customized services. Cultural Heritage is a domain that can benefit from such studies since it is characterized by a strong interaction between people, cultural items and spaces. Data gathered in a museum on visitor movements and behaviours can constitute the knowledge base to realize an advanced monitoring system able to offer museum stakeholders a complete and real-time snapshot of the museum locations occupancy. Furthermore, exploiting such data through Deep Learning methodologies can lead to the development of a predictive monitoring system able to suggest stakeholders the museum locations occupancy not only in real-time but also in the next future, opening new scenarios in the management of a museum. In this paper, we present and discuss a Deep Learning methodology applied to data coming from a non-invasive Bluetooth IoT monitoring system deployed inside a cultural space. Through the analysis of visitors’ paths, the main goal is to predict the occupancy of the available rooms. Experimental results on real data demonstrate the feasibility of the proposed approach; it can represent a useful instrument, in the hands of the museum management, to enhance the quality-of-service within this kind of spaces.",space
10.1016/j.buildenv.2020.106854,Journal,Building and Environment,scopus,2020-06-15,sciencedirect,A daylight-linked shading strategy for automated blinds based on model-based control and Radial Basis Function (RBF) optimization,https://api.elsevier.com/content/abstract/scopus_id/85084048092,"Addressing both daylight maximization and glare control over the entire workplace is always challenging for developing the automated shading control system. For the sake of cost and space usage, it is impractical to mount multiple sensors or cameras for real-time daylight environment monitoring to guarantee the control precision. Cut-off control is popular while it cannot attenuate the glare caused by excessive diffuse daylight. This paper introduces a model-based shading control for predetermining shading positions at each time step. A Useful Daylight Illuminance paradigm modality called rUDI is proposed as a variable criterion added to assist the cut-off strategy for further eliminating glare. The controller could be developed through real-time daylight simulations and an optimizer based on the surrogate model. This method was implemented in a full-scale office in Harbin, China. The surrogate model grounded on the Radial Basis Function Neural Network (RBF) was trained, validated and test with the experimental data sets. The control strategy was further incorporated with an adaptive light-switch model. The comparative simulations were conducted, and their corresponding results were generated for evaluating their performance in visual comfort, daylighting and electrical energy savings, demonstrating the advantages of the proposed control approach in terms of its adequate performance.",space
10.1016/j.compbiomed.2020.103800,Journal,Computers in Biology and Medicine,scopus,2020-06-01,sciencedirect,ECG signal classification with binarized convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85084584293,"Arrhythmias are a group of common conditions associated with irregular heart rhythms. Some of these conditions, for instance, atrial fibrillation (AF), might develop into serious syndromes if not treated in time. Therefore, for high-risk patients, early detection of arrhythmias is crucial. In this study, we propose employing deep convolutional neural network (CNN)-based algorithms for real-time arrhythmia detection. We first build a full-precision deep convolutional network model. With our proposed construction, we are able to achieve state-of-the-art level performance on the PhysioNet/CinC AF Classification Challenge 2017 dataset with our full-precision model. It is desirable to employ models with low computing resource requirements. It has been shown that a binarized model requires much less computing power and memory space than a full-precision model. We proceed to verify the feasibility of binarization in our neural network model. Network binarization can cause significant model performance degradation. Therefore, we propose employing a full-precision model as the teacher to regularize the training of the binarized model through knowledge distillation. With our proposed approach, we observe that network binarization only causes a small performance loss (the F1 score decreases from 0.88 to 0.87 for the validation set). Given that binarized convolutional networks can achieve favorable model performance while dramatically reducing computing cost, they are ideal for deployment on long-term cardiac condition monitoring devices. (Source code is available at https://github.com/yangfansun/bnn-ecg).",space
10.1016/j.actaastro.2020.02.036,Journal,Acta Astronautica,scopus,2020-06-01,sciencedirect,Terminal adaptive guidance via reinforcement meta-learning: Applications to autonomous asteroid close-proximity operations,https://api.elsevier.com/content/abstract/scopus_id/85080064409,"Current practice for asteroid close proximity maneuvers requires extremely accurate characterization of the environmental dynamics and precise spacecraft positioning prior to the maneuver. This creates a delay of several months between the spacecraft's arrival and the ability to safely complete close proximity maneuvers. In this work we develop an adaptive integrated guidance, navigation, and control system that can complete these maneuvers in environments with unknown dynamics, with initial conditions spanning a large deployment region, and without a shape model of the asteroid. The system is implemented as a policy optimized using reinforcement meta-learning. The lander is equipped with an optical seeker that locks to either a terrain feature, reflected light from a targeting laser, or an active beacon, and the policy maps observations consisting of seeker angles and LIDAR range readings directly to engine thrust commands. The policy implements a recurrent network layer that allows the deployed policy to adapt real time to both environmental forces acting on the agent and internal disturbances such as actuator failure and center of mass variation. We validate the guidance system through simulated landing maneuvers in a six degrees-of-freedom simulator. The simulator randomizes the asteroid's characteristics such as solar radiation pressure, density, spin rate, and nutation angle, requiring the guidance and control system to adapt to the environment. We also demonstrate robustness to actuator failure, sensor bias, and changes in the lander's center of mass and inertia tensor. Finally, we suggest a concept of operations for asteroid close proximity maneuvers that is compatible with the guidance system.",space
10.1016/j.renene.2020.01.092,Journal,Renewable Energy,scopus,2020-06-01,sciencedirect,Solar irradiance forecasting models without on-site training measurements,https://api.elsevier.com/content/abstract/scopus_id/85078400450,"Much effort has been made to increase the integration of solar photovoltaic (PV) systems to reduce the environmental impacts of fossil fuels. An essential process in PV systems is the forecasting of solar irradiance to avoid safety and stability problems due to its intermittent nature. Most of the research has been focused on improving the prediction accuracy based on the assumption that enough on-site training data are available. However, in many situations, it is required for the implementation of PV systems in locations where not enough solar irradiance measurements have been collected. Our hypothesis is that measurements from other sites can be used to train accurate forecasting models, given an appropriate definition of site similarity. We propose a methodology that takes information from exogenous variables that are correlated to on-site solar irradiance and constructs a multidimensional space equipped with a metric. Each site is a point in this space, and the learned metric is used to select those sites that can provide measurements to train an accurate forecasting model on an unobserved site. We show through experiments with real data that using the learned metric provides better predictions than using the measurements collected from the whole set of available sites.",space
10.1016/j.knosys.2020.105645,Journal,Knowledge-Based Systems,scopus,2020-05-11,sciencedirect,Bayesian optimisation in unknown bounded search domains,https://api.elsevier.com/content/abstract/scopus_id/85079889517,"Bayesian optimisation (BO) is one of the most sample efficient methods for determining the optima of expensive, noisy black-box functions. Despite its tremendous success in scientific discovery and hyperparameter tuning, it still requires a bounded search space. The search spaces boundaries are, however, often chosen heuristically with an educated guess. If the boundaries are misspecified, then the search space may either be unnecessarily large and hence more expensive to optimise, or it may simply not contain the global optimum. In this paper, we introduce a method for dynamically determining the bound directly from the data. This is done using a distribution of the bound derived in a Bayesian setting. The prior is chosen by the user and the likelihood is derived with Thompson sampling. This results in a bound that is both cheap to optimise and has a high probability of containing the global optimum. We compare the performance of our method with the alternative methods on a range of synthetic and real-world problems and demonstrate that our method achieves consistently superior results.",space
10.1016/j.ress.2020.106821,Journal,Reliability Engineering and System Safety,scopus,2020-05-01,sciencedirect,Towards Efficient Robust Optimization using Data based Optimal Segmentation of Uncertain Space,https://api.elsevier.com/content/abstract/scopus_id/85078707908,"Performing multi-objective optimization under uncertainty is a common requirement in industries and academia. Robust optimization (RO) is considered as an efficient and tractable approach provided one has access to behavioral data for the uncertain parameters. However, solutions of RO may be far from the real solution and less reliable due to inability to map the uncertain space accurately, especially when the data appears discontinuous and scattered in the uncertain domain. Amalgamating machine learning algorithms with RO, this paper proposes a data-driven methodology, where a novel fuzzy clustering mechanism is implemented along-with boundary construction, to transcript the uncertain space such that the specific regions of uncertainty are identified. Subsequently, using intelligent Sobol sampling, samples are generated in the mapped uncertain regions. Results of two test cases are presented along with a comprehensive comparison study. Considered case-studies include highly nonlinear model for continuous casting process from steelmaking industries, where a multi-objective optimization problem under uncertainty is solved to balance the conflict between productivity and energy consumption. The Pareto-optimal solutions of the resulting RO problem are obtained through Non-Dominated Sorting Genetic Algorithm – II, and ~23–29% improvement is observed in the uncertain objective function. Further, the spread and diversity metrics are enhanced by ~10–95% as compared to those obtained using other standard uncertainty sets.",space
10.1016/j.jpowsour.2020.227964,Journal,Journal of Power Sources,scopus,2020-04-15,sciencedirect,Data-driven reinforcement-learning-based hierarchical energy management strategy for fuel cell/battery/ultracapacitor hybrid electric vehicles,https://api.elsevier.com/content/abstract/scopus_id/85080125591,"A reinforcement-learning-based energy management strategy is proposed in this paper for managing energy system of Fuel Cell Hybrid Electric Vehicles (FCHEV) equipped with three power sources. A hierarchical power splitting structure is employed to shrink large state-action space based on an adaptive fuzzy filter. Then, the reinforcement-learning-based algorithm using Equivalent Consumption Minimization Strategy (ECMS) is proposed for tackling high-dimensional state-action space, and finding a trade-off between global learning and real-time implementation. The power splitting policy based on experimental data is obtained by using reinforcement learning algorithm, which allows for many different driving cycles and traffic conditions. The proposed energy management strategy can achieve low computation cost, optimal fuel cell efficiency and energy consumption economy. Simulation results confirm that, compared with existing learning algorithms and optimization methods, the proposed reinforcement-learning-based energy management strategy using ECMS can achieve high computation efficiency, lower power fluctuation of fuel cell and optimal fuel economy of FCHEV.",space
10.1016/j.ins.2019.11.018,Journal,Information Sciences,scopus,2020-04-01,sciencedirect,EHAUSM: An efficient algorithm for high average utility sequence mining,https://api.elsevier.com/content/abstract/scopus_id/85076715468,"Identifying high utility sequences in a quantitative sequence database is an important data mining task. However, a key problem of current approaches is that extensions of a high utility sequence often have a high utility. Hence, traditional techniques are often biased toward finding long patterns. To circumvent this problem, this paper proposes techniques for the problem of high average-utility sequence (HAUS) mining (
                        
                           H
                           A
                           U
                           S
                           M
                        
                     ). HAUSs are more meaningful than high utility sequences because the former are found using the average-utility measure, which consider the length of patterns in utility calculations. 
                        
                           H
                           A
                           U
                           S
                           M
                        
                      is more general than high average-utility itemset mining but it is also a more difficult problem because the downward-closure property used for search space reduction does not hold for the average-utility. To overcome that challenge, this paper introduces two upper bounds and a weak upper bound on the average-utility measure, and four width pruning, depth pruning, reducing, and tightening strategies. These strategies are designed to eliminate candidate HAUSs to speed up HAUS mining. Based on these theoretical results, a novel algorithm named EHAUSM is proposed for 
                        
                           H
                           A
                           U
                           S
                           M
                        
                     . Experiments on both real-life and synthetic quantitative sequence databases have confirmed its efficiency in terms of memory consumption and runtime.",space
10.1016/j.cmpb.2019.105099,Journal,Computer Methods and Programs in Biomedicine,scopus,2020-04-01,sciencedirect,Augmented reality navigation for liver resection with a stereoscopic laparoscope,https://api.elsevier.com/content/abstract/scopus_id/85073003483,"Objective
                  Understanding the three-dimensional (3D) spatial position and orientation of vessels and tumor(s) is vital in laparoscopic liver resection procedures. Augmented reality (AR) techniques can help surgeons see the patient's internal anatomy in conjunction with laparoscopic video images.
               
                  Method
                  In this paper, we present an AR-assisted navigation system for liver resection based on a rigid stereoscopic laparoscope. The stereo image pairs from the laparoscope are used by an unsupervised convolutional network (CNN) framework to estimate depth and generate an intraoperative 3D liver surface. Meanwhile, 3D models of the patient's surgical field are segmented from preoperative CT images using V-Net architecture for volumetric image data in an end-to-end predictive style. A globally optimal iterative closest point (Go-ICP) algorithm is adopted to register the pre- and intraoperative models into a unified coordinate space; then, the preoperative 3D models are superimposed on the live laparoscopic images to provide the surgeon with detailed information about the subsurface of the patient's anatomy, including tumors, their resection margins and vessels.
               
                  Results
                  The proposed navigation system is tested on four laboratory ex vivo porcine livers and five operating theatre in vivo porcine experiments to validate its accuracy. The ex vivo and in vivo reprojection errors (RPE) are 6.04 ± 1.85 mm and 8.73 ± 2.43 mm, respectively.
               
                  Conclusion and Significance
                  Both the qualitative and quantitative results indicate that our AR-assisted navigation system shows promise and has the potential to be highly useful in clinical practice.",space
10.1016/j.ejor.2019.02.046,Journal,European Journal of Operational Research,scopus,2020-03-16,sciencedirect,Merging anomalous data usage in wireless mobile telecommunications: Business analytics with a strategy-focused data-driven approach for sustainability,https://api.elsevier.com/content/abstract/scopus_id/85063225278,"Mobile internet usage has exploded with the mass popularity of smartphones that offer more convenient and efficient ways of doing anything from watching movies, playing games, and streaming music. Understanding the patterns of data usage is thus essential for strategy-focused data-driven business analytics. However, data usage has several unique stylized facts (such as high dimensionality, heteroscedasticity, and sparsity) due to a great variety of user behaviour. To manage these facts, we propose a novel density-based subspace clustering approach (i.e., a three-stage iterative optimization procedure) for intelligent segmentation of consumer data usage/demand. We discuss the characteristics of the proposed method and illustrate its performance in both simulation with synthetic data and business analytics with real data. In a field experiment of wireless mobile telecommunications for data-driven strategic design and managerial implementation, we show that our method is adequate for business analytics and plausible for sustainability in search of business value.",space
10.1016/j.ast.2019.105657,Journal,Aerospace Science and Technology,scopus,2020-03-01,sciencedirect,Reinforcement learning in dual-arm trajectory planning for a free-floating space robot,https://api.elsevier.com/content/abstract/scopus_id/85077502803,"A free-floating space robot exhibits strong dynamic coupling between the arm and the base, and the resulting position of the end of the arm depends not only on the joint angles but also on the state of the base. Dynamic modeling is complicated for multiple degree of freedom (DOF) manipulators, especially for a space robot with two arms. Therefore, the trajectories are typically planned offline and tracked online. However, this approach is not suitable if the target has relative motion with respect to the servicing space robot. To handle this issue, a model-free reinforcement learning strategy is proposed for training a policy for online trajectory planning without establishing the dynamic and kinematic models of the space robot. The model-free learning algorithm learns a policy that maps states to actions via trial and error in a simulation environment. With the learned policy, which is represented by a feedforward neural network with 2 hidden layers, the space robot can schedule and perform actions quickly and can be implemented for real-time applications. The feasibility of the trained policy is demonstrated for both fixed and moving targets.",space
10.1016/j.engappai.2019.103427,Journal,Engineering Applications of Artificial Intelligence,scopus,2020-03-01,sciencedirect,Stochastic parallel extreme artificial hydrocarbon networks: An implementation for fast and robust supervised machine learning in high-dimensional data,https://api.elsevier.com/content/abstract/scopus_id/85076620125,"Artificial hydrocarbon networks (AHN) – a supervised learning method inspired on organic chemical structures and mechanisms – have shown improvements in predictive power and interpretability in comparison with other well-known machine learning models. However, AHN are very time-consuming that are not able to deal with large data until now. In this paper, we introduce the stochastic parallel extreme artificial hydrocarbon networks (SPE-AHN), an algorithm for fast and robust training of supervised AHN models in high-dimensional data. This training method comprises a population-based meta-heuristic optimization with defined individual encoding and objective function related to the AHN-model, an implementation in parallel-computing, and a stochastic learning approach for consuming large data. We conducted three experiments with synthetic and real data sets to validate the training execution time and performance of the proposed algorithm. Experimental results demonstrated that the proposed SPE-AHN outperforms the original-AHN method, increasing the speed of training more than 
                        
                           10
                           ,
                           000
                           x
                        
                      times in the worst case scenario. Additionally, we present two case studies in real data sets for solar-panel deployment prediction (regression problem), and human falls and daily activities classification in healthcare monitoring systems (classification problem). These case studies showed that SPE-AHN improves the state-of-the-art machine learning models in both engineering problems. We anticipate our new training algorithm to be useful in many applications of AHN like robotics, finance, medical engineering, aerospace, and others, in which large amounts of data (e.g. big data) is essential.",space
10.1016/j.actaastro.2019.11.037,Journal,Acta Astronautica,scopus,2020-03-01,sciencedirect,Pattern recognition in time series for space missions: A rosetta magnetic field case study,https://api.elsevier.com/content/abstract/scopus_id/85076239823,"Time series analysis is a technique widely employed in space science. In unpredictable environments like space, scientific analysis relies on large data sets to enable interpretation of observations. Artificial signal interferences caused by the spacecraft itself further impede this process. The most time consuming part of these studies is the efficient identification of recurrent pattern in observations, both of artificial and natural origin, often forcing researchers to limit their analysis to a reduced set of observations. While pattern recognition techniques for time series are well known, their application is discussed and evaluated primarily on purpose built or heavily preprocessed data sets. The aim of this paper is to evaluate the performance of state of the art pattern recognition techniques in terms of computational efficiency and validity on a real-life testcase. For this purpose the most suitable techniques for different types of pattern are discussed and subsequently evaluated on various hardware in comparison to manual identification. Using magnetic field observations of the ESA Rosetta mission as a representative example, both disturbances and natural patterns are identified. Compared to manual selection a speed-up of a factor up to 100 is achieved, with values for recall and precision above 80%. Moreover, the detection process is fully automated and reproducible. Using the presented method it was possible to detect and correct artificial interference. Finally, the feasibility of onboard deployment is briefly discussed.",space
10.1016/j.cmpb.2019.105132,Journal,Computer Methods and Programs in Biomedicine,scopus,2020-03-01,sciencedirect,Virtual reality-based measurement of ocular deviation in strabismus,https://api.elsevier.com/content/abstract/scopus_id/85073938186,"Background and objective
                  Strabismus is an eye movement disorder in which shows the abnormal ocular deviation. Cover tests have mainly been used in the clinical diagnosis of strabismus for treatment. However, the whole process depends on the doctor's level of experience, which could be subjected to several factors. In this study, an automated technique for measurement of ocular deviation using a virtual reality (VR) device is developed.
               
                  Methods
                  A VR display system in which the screens that have the fixation target are changed alternately between on and off stages is used to simulate the normal strabismus diagnosis steps. Patients watch special-designed 3D scenes, and their eye motions are recorded by two infrared (IR) cameras. An image-processing-based pupil tracking technique is then applied to track their eye movement. After recording eye motion, two strategies for strabismus angle estimation are implemented: direct measurement and stepwise approximation. The direct measurement converts the eye movement to a strabismus angle after considering the eyeball diameter, while the stepwise approximation measures the ocular deviation through the feedback calibration process.
               
                  Results
                  Experiments are carried out with various strabismus patients. The results are compared to those of their doctors’ measurement, which shows good agreement.
               
                  Conclusions
                  The results clearly indicate that these techniques could identify ocular deviation with high accuracy and efficiency. The proposed system can be applied in small space and has high tolerance for the unexpected head movements compared with other camera-based system.",space
10.1016/j.apenergy.2019.114232,Journal,Applied Energy,scopus,2020-02-15,sciencedirect,Greedy search based data-driven algorithm of centralized thermoelectric generation system under non-uniform temperature distribution,https://api.elsevier.com/content/abstract/scopus_id/85076006670,"The generation efficiency of thermoelectric generation system is relatively low, thus how maximize its power production is of great importance. This paper designs a novel greedy search based data-driven method for centralized thermoelectric generation system to achieve maximum power point tracking under non-uniform temperature distribution. In order to effectively distinguish the local maximum power points and the global maximum power point under non-uniform temperature distribution, greedy search based data-driven employs a two-layer feed-forward neural network to accurately fit the curve between the power output and the controllable variable based on the real-time updated operation data. Based on the approximation curve, a greedy search is designed to efficiently approach the global maximum power point from a shrinking search space. Cases studies such as start-up test, step variation of temperature, stochastic temperature change, and analyse of sensitivity, are implemented to prove the effectiveness and superiority of the proposed algorithm. Simulation results verify that the proposed method can generate the highest energy under non-uniform temperature distribution condition, e.g., 391.34%, 115.71%, 110.92%, and 109.43% to that of perturb and observe, particle swarm optimization, whale optimization algorithm, and grey wolf optimizer in the stochastic temperature change. Lastly, the implementation feasibility of the proposed method is demonstrated by the hardware-in-the-loop experiment based on dSpace platform.",space
10.1016/j.physa.2019.123151,Journal,Physica A: Statistical Mechanics and its Applications,scopus,2020-02-15,sciencedirect,Early warning system: From face recognition by surveillance cameras to social media analysis to detecting suspicious people,https://api.elsevier.com/content/abstract/scopus_id/85074532417,"Surveillance security cameras are increasingly deployed in almost every location for monitoring purposes, including watching people and their actions for security purposes. For criminology, images collected from these cameras are usually used after an incident occurs to analyze who could be the people involved. While this usage of the cameras is important for a post crime action, there exists the need for real time monitoring to act as an early warning to prevent or avoid an incident before it occurs. In this paper, we describe the development and implementation of an early warning system that recognizes people automatically in a surveillance camera environment and then use data from various sources to identify these people and build their profile and network. The current literature is still missing a complete workflow from identifying people/criminals from a video surveillance to building a criminal information extraction framework and identifying those people and their interactions with others We train a feature extraction model for face recognition using convolutional neural networks to get a good recognition rate on the Chokepoint dataset collected using surveillance cameras. The system also provides the function to record people appearance in a location, such that unknown people passing through a scene excessive number of times (above a threshold decided by a security expert) will then be further analyzed to collect information about them. We implemented a queue based system to record people entrance. We try to avoid missing relevant individuals passing through as in some cases it is not possible to add every passing person to the queue which is maintained using some cache handling techniques. We collect and analyze information about unknown people by comparing their images from the cameras to a list of social media profiles collected from Facebook and intelligent services archives. After locating the profile of a person, traditional news and other social media platforms are crawled to collect and analyze more information about the identified person. The analyzed information is then presented to the analyst where a list of keywords and verb phrases are shown. We also construct the person’s network from individuals mentioned with him/her in the text. Further analysis will allow security experts to mark this person as a suspect or safe. This work shows that building a complete early warning system is feasible to tackle and identify criminals so that authorities can take the required actions on the spot.",space
10.1016/j.ifacol.2020.12.1251,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,A Framework for Ethics in Cyber-Physical-Human Systems,https://api.elsevier.com/content/abstract/scopus_id/85113481995,"This paper proposes a conceptual framework for consideration of ethical issues in the emerging category of smart cyber-physical systems. Cyber-physical systems (CPS) that bring together controls, communications, computing, and physical systems are being developed in a wide variety of application domains ranging from transportation, energy, and manufacturing, to biomedical and agriculture. Smart CPS are already being and will increasingly be deployed to work with humans, in workplaces, homes, or public spaces, resulting in the creation of cyber-physical human systems (CPHS). Ethical issues in smart CPS and CPHS can be examined within the larger frameworks of ethics of technology and ethics of artificial intelligence. We begin with a description of trends and visions for the future development of smart CPS. We next outline fundamental theories of ethics that offer foundations for thinking about ethical issues in smart CPHS. We argue that it is necessary to fight the tendency toward technological determinism. We argue that in analyzing ethics of smart CPHS, we need to anticipate increasing capabilities and the future deployment of such systems. Ultimately, if these systems are widely deployed in society, they will have a very significant impact, including possible negative consequences, on individuals, communities, nations, and the world. Our framework has two main dimensions: (i) stage of development of CPHS domain from early stage research to mature technologies; and (ii) locus of decision making: individual, corporate, and government settings. We illustrate the framework with some specific examples.",space
10.1016/j.procs.2020.03.027,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Strategic zoning approach for urban areas: Towards a shared transportation system,https://api.elsevier.com/content/abstract/scopus_id/85085571988,"Investigating downstream freight demand is a prerequisite to accomplishing the overall strategic implementation of transportation systems. Machine learning has recently become widely applied in order to support decision-making in several logistic operational levels: travel/arrival time prediction, occupancy forecasting of logistic spaces, route optimization and so on. Nevertheless, strategic decision-making often overlooks flow tendencies forecasting. Targeting this perspective, the present paper aims at proposing an urban zoning approach based on time series forecasting of supply chain demand through clustering customers. To conduct our approach, we have selected a set of machine learning algorithms that are believed to be robust according to the literature and the achieved accuracy benchmarks. Considering real-life data-based computational results, a number of analytical insights are illustrated.",space
10.1016/j.promfg.2020.04.037,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Implementing AR/MR - Learning factories as protected learning space to rise the acceptance for mixed and augmented reality devices in production,https://api.elsevier.com/content/abstract/scopus_id/85085498037,"When talking about digitization, changes in the way of working are inevitable: The implementation of intelligent machines or dealing with real-time data lead to new tasks supported by new technology. Also digital technologies such as Augmented and Mixed Reality (AR/MR) are pushing the market and setting new standards in collaboration, prototyping or maintenance. The correct handling of AR/MR devices requires a change in the employees’ behavior; changing working routines are followed by a new skill set and a change in the culture. The acceptance of employees can therefore be regarded as a critical success factor for the implementation of such technologies. Thus, the present paper answers the research question ‘what factors influence the employee’s acceptance of AR and MR data glasses in industry’. On the basis of a comprehensive literature analysis, an implementation workshop was developed and validated in cooperation with an industrial partner. The results were transformed into a workshop within the learning and research factory ‘Smart Production Lab’ to give employees and students the opportunity to train the handling of data glasses in a protected learning space in order to increase the acceptance for the technology.",space
10.1016/j.softx.2020.100419,Journal,SoftwareX,scopus,2020-01-01,sciencedirect,TWINKLE: A digital-twin-building kernel for real-time computer-aided engineering,https://api.elsevier.com/content/abstract/scopus_id/85079158568,"TWINKLE is a library for building families of solvers to perform Canonical Polyadic Decomposition (CPD) of tensors. The common characteristic of these solvers is that the data structure supporting the tuneable solution strategy is based on a Galerkin projection of the phase space. This allows processing and recovering tensors described by highly sparse and unstructured data. For achieving high performance, TWINKLE is written in C++ and uses the Armadillo open source library for linear algebra and scientific computing, based on LAPACK (Linear Algebra PACKage) and BLAS (Basic Linear Algebra Subprograms) routines. The library has been implemented keeping in mind its future extensibility and adaptability to fulfil the different users’ needs in academia and industry regarding Reduced Order Modelling (ROM) and data analysis by means of tensor decomposition. It is especially focused on post-processing data from Computer-Aided-Engineering (CAE) simulation tools.",space
10.1016/bs.adcom.2019.09.008,Book Series,Advances in Computers,scopus,2020-01-01,sciencedirect,Stepping into the digitally instrumented and interconnected era,https://api.elsevier.com/content/abstract/scopus_id/85078358345,"This chapter is to tell all about the digitization-inspired possibilities and opportunities and how software-defined cloud centers are the best fit for hosting and running digital applications. Also, how the next-generation data analytics can be smartly accomplished through cloud platforms and infrastructures is also explained in detail. We are to describe some of the impactful developments and technological advancements brewing in the IT space, how the tremendous amount of data getting produced and processed through cloud systems is to impact the IT and business domains, and how next-generation IT infrastructures are accordingly getting refactored, remedied and readied for the impending big data-induced challenges, how likely the move of the data analytics discipline toward fulfilling the digital universe requirements of extracting and extrapolating actionable insights for the knowledge-parched is, and finally for the establishment and sustenance of the dreamt smarter planet. In short, the uninhibited explosion of digitized systems and connected devices pour out a tremendous amount of multi-structured data and the impending challenge is to make sense out of the data heaps. Data analytics is the way to go and in the recent past, the overwhelming trend is to empower our everyday systems with machine and deep learning algorithms to automatically learn out of data heaps and streams in order to be distinctively intelligent in their actions and reactions. This chapter is specially prepared to put a stimulating foundation for explaining the nitty-gritty of the Digital Twin paradigm.",space
10.1016/j.cogsys.2019.09.015,Journal,Cognitive Systems Research,scopus,2020-01-01,sciencedirect,Multi-Agent neurocognitive models of semantics of spatial localization of events,https://api.elsevier.com/content/abstract/scopus_id/85072851037,"The purpose of the study is to develop a learning system for internal representation of the events localization space to realize orientation and navigation of autonomous mobile systems. The task of the research is the development of simulation models of the semantics of the event localization space based on multi-agent neurocognitive architectures. The paper proves that the multi-agent neurocognitive architecture is an effective formalism for describing the semantics of the spatial localization of events. Main theoretical foundations have been developed for the simulation of spatial relations using the so-called multi-agent facts, consisting of software agents-concepts, reflecting semantic categories corresponding to parts of speech. It is shown that locative software agents that describe the spatial location of objects and events, forming homogeneous connections, compose the so-called field locations. The latter describes a holistic view of the intellectual agent about the environment. The paper defines conceptual foundations of multi-agent modeling of the semantics of subjective reflexive mapping of the interaction between real objects, space and time.",space
10.1016/j.comcom.2019.09.014,Journal,Computer Communications,scopus,2019-12-15,sciencedirect,VARMAN: Multi-plane security framework for software defined networks,https://api.elsevier.com/content/abstract/scopus_id/85072873993,"In the context of future networking technologies, Software-Defined paradigm offers compelling solutions and advantages for traffic orchestration and shaping, flexible and dynamic routing, programmable control and smart application-driven resource management. But the SDN operation has to confront critical issues and technical vulnerabilities, security problems and threats in the enabling technical architecture itself. To address the critical security problems in SDN enabled data centers, we propose a collaborative “Network Security and Intrusion Detection System(NIDS)” scheme called ‘
                        VARMAN
                     : adVanced multi-plAne secuRity fraMework for softwAre defined Networks’. The SDN security scheme comprises of coarse-grained flow monitoring algorithms on the dataplane for rapid anomaly detection and prediction of network-centric DDoS/botnet attacks. In addition, this is combined with a fine-grained hybrid deep-learning based classifier pipeline on the control plane. It is observed that existing ML-based classifiers improve the accuracy of NIDS, however, at the cost of higher processing power and memory requirement, thus unrealistic for real-time solutions. To address these problems and still achieve accuracy and speed, we designed a hybrid model, combining both deep and shallow learning techniques, that are implemented in an improved SDN stack. The data plane deploys attack prediction and behavioral trigger mechanisms, efficient data filtering, feature selection, and data reduction techniques. To demonstrate the practical feasibility of our security scheme in real modern datacenters, we utilized the popular NSL-KDD dataset, most recent CICIDS2017 dataset, and refined it to a balanced dataset containing a comparable number of normal traffic and malware samples. We further augmented the training by organically generating datasets from lab-simulated and public-network hosted hackathon websites. The results show that VARMAN framework is capable of detecting attacks in real-time with accuracy more than 98% under attack intensities up to 50k packets/second. In a multi-controller interconnected SDN domain, the flow setup time improves by 70% on an average, and controller response time reduces by 40%, without incurring additional latency due to security intelligence processing overhead in SDN stack. The comparisons of VARMAN under similar attack scenarios and test environment, with related recent works that utilized ML-based NIDS, demonstrate that our scheme offers higher accuracy, less than 5% false positive rate for various attack intensities and significant training space/time reduction.",space
10.1016/j.eswa.2019.06.066,Journal,Expert Systems with Applications,scopus,2019-12-15,sciencedirect,Double Q-PID algorithm for mobile robot control,https://api.elsevier.com/content/abstract/scopus_id/85068505390,"Many expert systems have been developed for self-adaptive PID controllers of mobile robots. However, the high computational requirements of the expert systems layers, developed for the tuning of the PID controllers, still require previous expert knowledge and high efficiency in algorithmic and software execution for real-time applications. To address these problems, in this paper we propose an expert agent-based system, based on a reinforcement learning agent, for self-adapting multiple low-level PID controllers in mobile robots. For the formulation of the artificial expert agent, we develop an incremental model-free algorithm version of the double Q-Learning algorithm for fast on-line adaptation of multiple low-level PID controllers. Fast learning and high on-line adaptability of the artificial expert agent is achieved by means of a proposed incremental active-learning exploration-exploitation procedure, for a non-uniform state space exploration, along with an experience replay mechanism for multiple value functions updates in the double Q-learning algorithm. A comprehensive comparative simulation study and experiments in a real mobile robot demonstrate the high performance of the proposed algorithm for a real-time simultaneous tuning of multiple adaptive low-level PID controllers of mobile robots in real world conditions.",space
10.1016/j.ins.2019.07.098,Journal,Information Sciences,scopus,2019-12-01,sciencedirect,Counting the frequency of time-constrained serial episodes in a streaming sequence,https://api.elsevier.com/content/abstract/scopus_id/85069902935,"As a representative sequential pattern mining problem, counting the frequency of serial episodes from a streaming sequence has drawn continuous attention in academia due to its wide application in practice, e.g., telecommunication alarms, stock market, transaction logs, bioinformatics, etc. Although a number of serial episodes mining algorithms have been developed recently, most of them are neither stream-oriented, as they require processing the whole dataset multiple times, nor time-aware, as they fail to take into account the time constraint of serial episodes. In this paper, we propose two novel one-pass algorithms, ONCE and ONCE+, each of which can respectively compute two popular frequencies of given episodes satisfying predefined time-constraint as signals in a stream arrives one-after-another. ONCE is only used for non-overlapped frequency where the occurrences of a serial episode in sequence are not intersected. ONCE+ is designed for the distinct frequency where the occurrences of a serial episode do not share any event. Theoretical study proves that our algorithm can correctly mine the frequency of target time constraint serial episodes in a given stream. Experimental study over both real-world and synthetic datasets demonstrates that the proposed algorithm can work, with little time and space, in signal-intensive streams where millions of signals arrive within a single second. Moreover, the algorithm has been applied in a real stream processing system, where the efficacy and efficiency of this work are tested in practical applications.",space
10.1016/j.neucom.2019.08.031,Journal,Neurocomputing,scopus,2019-11-20,sciencedirect,Deep network for human action recognition using Weber motion,https://api.elsevier.com/content/abstract/scopus_id/85070678569,"Effective motion estimation is one of the prime steps for any human action recognition (HAR) algorithm. Optical flow (OF) and motion history image (MHI) are two well-known methods for motion estimation in videos. OF has several advantages over MHI. But the major drawback with OF is that it is computationally very expensive as compared to the MHI. Therefore, in this paper, a new motion estimation technique named as Weber Motion History Image (WMHI) is proposed. Here, an extremely fast algorithm is proposed for HAR using WMHI, pose information, and convolutional neural network (CNN). In spite of being fast and less space consuming, the algorithm outperforms the existing pose based CNN results on five benchmark datasets namely JHMDB [1], sub-JHMDB [1], MPII [2] and HMDB51 [3] and UCF101 [4]. The work mainly focuses on a new efficient algorithm which can be implemented for real-time HAR in videos. For real-time implementation, the two basic criteria on which an algorithm can be analyzed are space and time complexity. The proposed algorithm is faster as compared to the existing OF based HAR systems. In terms of space complexity, the feature size of the proposed algorithm is almost 50% of the existing OF based algorithm. The recognition results still outperform the existing result by a significant margin.",space
10.1016/j.geoderma.2019.07.004,Journal,Geoderma,scopus,2019-11-15,sciencedirect,Optimisation in machine learning: An application to topsoil organic stocks prediction in a dry forest ecosystem,https://api.elsevier.com/content/abstract/scopus_id/85069648816,"Soil organic carbon (SOC) sequestration plays a key role in reducing the atmospheric greenhouse gas concentration. However, dry forest ecosystems in Ecuador are endangered to become a source of carbon emissions because of deforestation. Often spatial information, necessary to quantify potential carbon loss to the atmosphere, is missing. This particularly applies to remote areas of limited accessibility. This study aims to regionalise the SOC stocks of a small and poorly accessible dry forest ecosystem in southwestern Ecuador by using boosted regression tree (BRT) models. Resampling in a nested repeated k-fold cross validation approach was applied to develop robust models for a dataset of 118 samples with limited predictor information. To select an optimal set of model parameters, optimisation by differential evolution (DE) was applied for parameter tuning. Predictor selection was implemented using the same optimisation algorithm. This study demonstrates how the predictive performance of BRT models can be improved by applying an optimisation approach for parameter tuning and predictor selection. Model performance was improved by approximately 40% concerning the R2. Still, the results also demonstrated the difficulties of machine learning applications in small and highly heterogeneous natural areas. Very variable or even random factors were assumed to distort the relationship between predictor and response variables. We assume that the presented approach is particularly successful in the case of a real-valued multivariate space of tuning parameters. However, this requires testing in further machine learning applications and algorithms.",space
10.1016/j.cose.2019.101590,Journal,Computers and Security,scopus,2019-11-01,sciencedirect,Volatile memory analysis using the MinHash method for efficient and secured detection of malware in private cloud,https://api.elsevier.com/content/abstract/scopus_id/85070387186,"Today, most organizations employ cloud computing environments for both computational reasons and for storing their critical files and data. Virtual servers are an example of widely used virtual resources provided by cloud computing architecture. Therefore, virtual servers are considered an attractive target for cyber-attackers, who launch their attacks by malware such as the well-known remote access trojans (RATs) and more modern malware such as ransomware and cryptojacking. Existing security solutions implemented on virtual servers fail to detect these newly created malware (zero-day attacks). In fact, by the time the security solution is updated, the organization has likely already been attacked. In this study, we present a designated framework aimed at trusted and secured detection of newly created and unknown instances of malware on virtual machines in an organization's private cloud. We took volatile memory dumps from a virtual machine (VM) in a secured and trusted manner, and analyzed all of the data within the memory dumps using the MinHash method; MinHash is well suited for the accurate detection of malware in VMs based on efficient volatile memory dump comparisons. The proposed framework is evaluated in a comprehensive set of experiments of increasing difficulty in which we also measured the detection performance of different classifiers (both similarity and machine learning-based classifiers, using collections of real-world, professional, notorious malware and legitimate applications. The evaluation results show that our framework can detect the anomalous state of a virtual server, as well as known, new, and unknown malware, with very high TPRs (100% for ransomware and RATs) and very low FPRs (1.8% for ransomware and no FPR for RATs). We also show how the methodology's performance can be improved, in terms of required time and storage space, saving more than 86% of these resources. Finally, we demonstrate the generalization capabilities and practicality of our methodology by using transfer learning and learning from just one virtual server in order to detect unknown malware on a different virtual server.",space
10.1016/j.ins.2019.07.019,Journal,Information Sciences,scopus,2019-11-01,sciencedirect,Labeled graph sketches: Keeping up with real-time graph streams,https://api.elsevier.com/content/abstract/scopus_id/85068588796,"Currently, graphs serve as fundamental data structures for many applications, such as road networks, social and communication networks, and web requests. In many applications, graph edges stream in and users are only interested in the recent data. In data exploration, the storage and processing of such massive amounts of graph stream data has become a significant problem. As the categorical attributes of vertices and edges are often referred to as labels, we propose a labeled graph sketch that stores real-time graph structural information using only sublinear space and that supports graph queries of diverse types. This sketch also works for sliding-window queries. We conduct extensive experiments on real-world datasets in six different domains and compare the results with a state-of-the-art method to show the accuracy, efficiency, and practicability of our proposed approach.",space
10.1016/j.knosys.2019.06.003,Journal,Knowledge-Based Systems,scopus,2019-10-15,sciencedirect,Efficient processing of top k group skyline queries,https://api.elsevier.com/content/abstract/scopus_id/85067023283,"For a given multi-dimensional data set, a group skyline query returns the optimal groups not dominated by any other group of equal size. The group skyline query is a powerful tool in many applications that call for optimal groups. However, it is common to return a large number of results which make users overwhelmed since it prevents them from making quick and rational decisions. To address this problem, we first identify and formulate a top 
                        k
                      group skyline (T
                        k
                     GSky) query which returns 
                        k
                      optimal groups dominating the highest number of points in the given data set. Next, new pruning strategies are presented to reduce the search space. Then, we propose efficient algorithms by exploiting novel techniques including a grouping strategy, a hybrid strategy, and a point-based replacement strategy, respectively. Finally, we also develop an approximate algorithm to further improve the T
                        k
                     GSky query performance. The performance of the proposed algorithms is studied by extensive experiments over synthetic and real datasets.",space
10.1016/j.cie.2019.07.007,Journal,Computers and Industrial Engineering,scopus,2019-10-01,sciencedirect,Artificial search agents with cognitive intelligence for binary optimization problems,https://api.elsevier.com/content/abstract/scopus_id/85068529900,"Artificial intelligence techniques bring about new opportunities in problem solving. The notion such techniques have in common is learning mechanisms that are mostly problem and environment dependent. Although optimality is not guaranteed by these techniques, they draw attention due to being able to solve challenging optimization problems efficiently. Accordingly, the present study introduces a swarm-based optimization algorithm that is comprised of artificial search agents each with individual cognitive intelligence. In this technique, each agent is allowed to learn from problem space individually. Therefore, each of the search agents exhibits a different search characteristic. Nevertheless, they occasionally share information of the promising regions with each other. Thus, central swarm intelligence is also allowed to lead those independent search agents. Moreover, information-sharing techniques in the developed algorithm are designed as adaptive procedures so that search agents learn throughout generations by avoiding premature convergence and local optima problems as much as possible. The performance of the proposed algorithm is tested on a set of binary optimization problems including the set-union knapsack problem and the uncapacitated facility location problem, which have numerous real-life applications. All reported benchmarking problems are solved by the developed algorithm. As demonstrated by the comprehensive computational study and statistical tests, the proposed swarm-based algorithm significantly improves most of the published results.",space
10.1016/j.jngse.2019.102933,Journal,Journal of Natural Gas Science and Engineering,scopus,2019-09-01,sciencedirect,Machine learning for surveillance of fluid leakage from reservoir using only injection rates and bottomhole pressures,https://api.elsevier.com/content/abstract/scopus_id/85068973220,"Carbon-neutral economies would require preventing the release of industrial-scale CO2 into the atmosphere by injecting into geologic formations. Large-scale injection of CO2 into deep reservoirs carries a potential for its undesired leakage into above zones, which can act as an obstacle to its large-scale implementation. Current methods for surveillance of CO2 leaks are costly and not very robust, especially the methods that simulate expected pressure behavior based on an assumed reservoir model.
                  This study proposes a machine learning method for surveillance of fluid leakage using deconvolution response function (a non-linear function of time varying bottomhole pressure and injection rates) from injection and monitoring wells as a measure of leakage that is simulated via multivariate linear regression of all the wells present in the reservoir. Leakage is detected by comparing “expected” (baseline without leaks) deconvolution response of all monitoring wells with their “observed” deconvolution response. Three key advantages of the proposed method are that it i) uses only injection rates and bottomhole pressure data (with no reservoir or geological model), ii) is independent of physical process parameterization uncertainties, and iii) applicable to both conventional and unconventional (e.g. fractured tight formations) reservoirs with any fluid (e.g. compressible, incompressible). The proposed method is first trained to learn well history with no leakage, followed by its validation after which it can be used to detect leakage by tracking a meaningful deviation error (at least twenty times the error of no leakage base scenario over same time period) between expected well response and observed well response at all monitoring wells. The well history required for the proposed method comes directly from measurements made at wells in a real field, but in absence of field data the proposed method is illustrated through well history simulated by reservoir simulations; no such numerical simulations are required for application of this method in a real world scenario with well measurements.",space
10.1016/j.patrec.2018.02.013,Journal,Pattern Recognition Letters,scopus,2019-09-01,sciencedirect,Biometric surveillance using visual question answering,https://api.elsevier.com/content/abstract/scopus_id/85042474147,"Surveillance of individuals using visual data requires human-level capabilities for understanding the characteristics that differentiate one person from another. However, because the influx of both video and imagery is increasing at a greater rate than humans can cope with, biometric-based surveillance systems are required to assist with the triage of information based on human-generated queries. Unfortunately, current systems are not robust enough to tackle new tasks, as they involve specialized models that do not leverage existing, pre-trained components. To mitigate these issues, we propose a novel system for biometric-based surveillance that utilizes models that are relevance-aware to triage images and videos based on interaction with single or multiple users. As the system is initially focused on detection of people via their appearance and clothing, we have named the system Context and Collaborative (C2) Visual Question Answering (VQA) for Biometric Object-Attribute Relevance and Surveillance (C2VQA-BOARS). To validate the usefulness of C2VQA-BOARS in real-world scenarios, we provide an implementation of two novel components (Relevance and Triage) and apply them in tasks against two datasets created for biometric surveillance. Our results outperform baseline approaches, proving that a system with a minimal amount of fine-tuned components can robustly handle new datasets and problems as needed.",space
10.1016/j.neucom.2018.06.095,Journal,Neurocomputing,scopus,2019-08-18,sciencedirect,Speeding up k-Nearest Neighbors classifier for large-scale multi-label learning on GPUs,https://api.elsevier.com/content/abstract/scopus_id/85065140025,"Multi-label classification is one of the most dynamically growing fields of machine learning, due to its numerous real-life applications in solving problems that can be described by multiple labels at the same time. While most of works in this field focus on proposing novel and accurate classification algorithms, the issue of the computational complexity on growing dataset sizes is somehow marginalized. Owning to the ever-increasing capabilities of data capturing, we are faced with the problem of large-scale data mining that forces learners to be not only highly accurate, but also fast and scalable on high-dimensional spaces of instances, features, and labels. In this paper, we propose a highly efficient parallel approach for computing the multi-label k-Nearest Neighbor classifier on GPUs. While this method is highly effective due to its accuracy and simplicity, its computational complexity makes it prohibitive for large-scale data. We propose a four-step implementation that takes an advantage of the GPU architecture, allowing for an efficient execution of the multi-label k-Nearest Neighbors classifier without any loss of accuracy. Experiments carried out on a number of real and artificial benchmarks show that we are able to achieve speedups up to 200 times when compared to a sequential CPU execution, while efficiently scaling up to varying number of instances and features.",space
10.1016/j.ufug.2019.126365,Journal,Urban Forestry and Urban Greening,scopus,2019-07-01,sciencedirect,Exploring the effect of urban features and immediate environment on body responses,https://api.elsevier.com/content/abstract/scopus_id/85067818650,"This study investigates the relationship between urban features (sky exposure, green spaces, visual complexity, and built-up area), immediate environmental factors (air temperature, relative humidity, Heat Stress Index, Wet Bulb Globe Temperature, wind speed, and noise), personal characteristics (perceived restorativeness) and body reactions (body skin temperature and skin conductance responses). The proposed framework is based on multi-sensor data fusion from wearable physiological sensors, wireless environmental sensors, smartphones, images, geographic information systems datasets, and questionnaires. An experimental setup in a real-world setting is conducted and machine learning algorithms for regression problems and feature selection for variable importance are implemented. The results suggest a significant association between immediate environmental factors and body reactions; however, urban features are found to be weak explanatory variables. A deeper analysis of the identified stress hotspots revealed that locations with more dense green spaces, greater sky exposure, and smaller built-up area tended to report lower levels of stress reaction.",space
10.1016/j.media.2019.05.001,Journal,Medical Image Analysis,scopus,2019-07-01,sciencedirect,Denoising of 3D magnetic resonance images using a residual encoder–decoder Wasserstein generative adversarial network,https://api.elsevier.com/content/abstract/scopus_id/85065426790,"Structure-preserved denoising of 3D magnetic resonance imaging (MRI) images is a critical step in medical image analysis. Over the past few years, many algorithms with impressive performances have been proposed. In this paper, inspired by the idea of deep learning, we introduce an MRI denoising method based on the residual encoder–decoder Wasserstein generative adversarial network (RED-WGAN). Specifically, to explore the structure similarity between neighboring slices, a 3D configuration is utilized as the basic processing unit. Residual autoencoders combined with deconvolution operations are introduced into the generator network. Furthermore, to alleviate the oversmoothing shortcoming of the traditional mean squared error (MSE) loss function, the perceptual similarity, which is implemented by calculating the distances in the feature space extracted by a pretrained VGG-19 network, is incorporated with the MSE and adversarial losses to form the new loss function. Extensive experiments are implemented to assess the performance of the proposed method. The experimental results show that the proposed RED-WGAN achieves performance superior to several state-of-the-art methods in both simulated and real clinical data. In particular, our method demonstrates powerful abilities in both noise suppression and structure preservation.",space
10.1016/j.scs.2019.101523,Journal,Sustainable Cities and Society,scopus,2019-07-01,sciencedirect,Cost efficient resource allocation for real-time tasks in embedded systems,https://api.elsevier.com/content/abstract/scopus_id/85065049277,"Various application classes are being deployed to the cloud these days making use of a pay-as-you-go policy. However, existing cloud technologies are still at an early stage of maturity for applications with real-time constraints. With the emergence of Internet of Things (IoT) deployments and embedded systems in smart infrastructure, requirements for off-loading computation to cloud are increasing. In real-time systems, the resource allocation problem is NP-hard, especially when these systems are deployed in cloud computing environments where task execution involves deadline constraints. As a solution, hybrid approaches provide the opportunities to investigate efficient resource allocation for task scheduling problems. We propose a hybridized form of cuckoo search and genetic algorithms known as HGCS (hybrid genetic and cuckoo search) by embedding genetic operators that optimize makespan and cost of real-time tasks scheduled on cloud virtual machines. The inclusion of genetic operators in the cuckoo search algorithm leads to a rigorous search of the solution space, finding the best feasible schedule that can execute tasks in the lowest time, which in turn reduces the total resources usage cost. The performance of the proposed algorithm is tested by using real-time tasks that need data files for successful completion. The HGCS algorithm is evaluated by comparing the results with genetic and cuckoo search algorithms individually. The experimental results favor HGCS over the other two counterparts in providing a schedule respecting the time constraints of the system with reduced makespan and execution cost.",space
10.1016/j.measurement.2019.03.032,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2019-07-01,sciencedirect,Experimental characterisation of eye-tracking sensors for adaptive human-machine systems,https://api.elsevier.com/content/abstract/scopus_id/85064266296,"Adaptive Human-Machine Interfaces and Interactions (HMI2) are closed-loop cyber-physical systems comprising a network of sensors measuring human, environmental and mission parameters, in conjunction with suitable software for adapting the HMI2 (command, control and display functions) in response to these real-time measurements. Cognitive HMI2 are a particular subclass of these systems, which support dynamic HMI2 adaptations based on the user’s cognitive state. These states are estimated in real-time using various neuro-physiological parameters from gaze, cardiorespiratory and brain signals, which are processed by an Adaptive Neuro-Fuzzy Inference System (ANFIS). However, the accuracy and precision of neuro-physiological measurements are affected by a variety of environmental factors and therefore need to be accurately characterised prior to operational use. This paper describes the characterisation activities performed on two types of eye tracking devices used in the Aerospace Intelligent and Autonomous Systems (AIAS) laboratory of RMIT University to support the development of cognitive human-machine systems. The uncertainty associated with the ANFIS outputs is quantified by propagating the uncertainties in the input data (determined experimentally) through the inference engine. This process is of growing relevance because similar machine learning techniques are now being developed for an increasing number of applications including aerospace, transport, biomedical and defence cyber-physical systems.",space
10.1016/j.cogsys.2019.01.003,Journal,Cognitive Systems Research,scopus,2019-06-01,sciencedirect,The CORTEX cognitive robotics architecture: Use cases,https://api.elsevier.com/content/abstract/scopus_id/85060622773,"CORTEX is a cognitive robotics architecture inspired by three key ideas: modularity, internal modelling and graph representations. CORTEX is also a computational framework designed to support early forms of intelligence in real world, human interacting robots, by selecting an a priori functional decomposition of the capabilities of the robot. This set of abilities was then translated to computational modules or agents, each one built as a network of software interconnected components. The nature of these agents can range from pure reactive modules connected to sensors and/or actuators, to pure deliberative ones, but they can only communicate with each other through a graph structure called Deep State Representation (DSR). DSR is a short-term dynamic representation of the space surrounding the robot, the objects and the humans in it, and the robot itself. All these entities are perceived and transformed into different levels of abstraction, ranging from geometric data to high-level symbolic relations such as “the person is talking and gazing at me”. The combination of symbolic and geometric information endows the architecture with the potential to simulate and anticipate the outcome of the actions executed by the robot. In this paper we present recent advances in the CORTEX architecture and several real-world human-robot interaction scenarios in which they have been tested. We describe our interpretation of the ideas inspiring the architecture and the reasons why this specific computational framework is a promising architecture for the social robots of tomorrow.",space
10.1016/j.datak.2019.05.003,Journal,Data and Knowledge Engineering,scopus,2019-05-01,sciencedirect,SubspaceDB: In-database subspace clustering for analytical query processing,https://api.elsevier.com/content/abstract/scopus_id/85066093010,"High dimensional data analysis within relational database management systems (RDBMS) is challenging because of inadequate support from SQL. Currently, subspace clustering of high dimensional data is implemented either outside DBMS using wrapper code or inside DBMS using SQL User Defined Functions/Aggregates(UDFs/UDAs). However, both these approaches have potential disadvantages from performance, resource usage, and security perspective for voluminous and frequently updated data. Hence, we propose an efficient querying system, named SubspaceDB, that implements subspace clustering directly within an RDBMS. SubspaceDB provides a novel set of query operators, each with an optimization objective, to facilitate interactive analysis for subspace clustering. The query operators focus on retrieving optimal answers to four key query types : (a) Medoid queries, (b) Neighbourhood queries, (c) Partial similarity queries, and (d) Prominence queries, that aid the formation of subspace clusters. Experimental studies on real and synthetic databases of size 
                        15
                        M
                      tuples and 104 attributes show that our proposed approach SubspaceDB can be over 10 times faster as compared to a conventional wrapper-based or SQL UDF approach. The proposed approach is also efficient in retrieving at least 50% data with performance improvement of at least 25%.",space
10.1016/j.enconman.2019.02.086,Journal,Energy Conversion and Management,scopus,2019-05-01,sciencedirect,"Multi-step short-term wind speed forecasting approach based on multi-scale dominant ingredient chaotic analysis, improved hybrid GWO-SCA optimization and ELM",https://api.elsevier.com/content/abstract/scopus_id/85063231962,"Accurate wind speed prediction possesses a significant impact on reasonable scheduling and safe operation of power system. For this purpose, a novel hybrid approach based on multi-scale dominant ingredient chaotic analysis, improved hybrid GWO-SCA (IHGWOSCA) algorithm and extreme learning machine (ELM) is proposed for multi-step short-term wind speed prediction, in which the multi-scale dominant ingredient chaotic analysis combines the proposed optimal variational mode decomposition (OVMD), singular spectrum analysis (SSA) and phase space reconstruction (PSR). To begin with, the mode number and updating step of VMD are pre-determined by center frequency observation method and the proposed least-squares error index (LSEI), thus decomposing the non-stationary wind speed series into a set of intrinsic mode functions (IMFs). Later, the extraction of the dominant ingredient and residuary ingredient for each sub-series is implemented by SSA for the construction of forecasting components. Subsequently, the proposed IHGWOSCA algorithm coded with discrete integers and real-valued are investigated to search optimal parameters in PSR and ELM successively. Lastly, the ultimate forecasting results of the original wind speed are calculated by accumulating results of all the predicted components. Furthermore, seven data sets from Sotavento Galicia and Inner Mongolia have been employed to evaluate the proposed approach. The results illustrate that: (1) the proposed OVMD-based models obtained better RMSE, MAE and MAPE indexes comparing with the benchmark models through weakening the non-stationary of the original signal; (2) the proposed dominant ingredient chaotic analysis combining SSA and PSR enhanced the multi-steps prediction performance effectively; (3) the proposed IHGWOSCA optimization algorithm possessed good capability for optimal parameters searching and fast convergence.",space
10.1016/j.engappai.2019.03.001,Journal,Engineering Applications of Artificial Intelligence,scopus,2019-05-01,sciencedirect,Affective analytics of demonstration sites,https://api.elsevier.com/content/abstract/scopus_id/85062991999,"Multiple-criteria decision-making (MCDM) typically assumes that crowds make completely rational decisions. In MCDM, a crowd as a whole, or its individual members, generally make decisions free from any influence of valence, arousal, emotional state or environment. In contrast, various theories dealing with crowd psychology (Gustave Le Bon, Freudian, Deindividuation, Convergence, Emergent norm, Social identity) analyze, in one form or another, the emotions of the crowd. According to above theories, crowd is influenced by a range of behavioral factors, such as physical, social, psychological, culture, norms, and emotions. It can be argued that the emotional state, valence and arousal of crowds affect their decision making to a considerable degree and multiple criteria crowd behavior modeling must, therefore, consider this impact as well. In this light, the integration of crowd simulation and biometric methods, behavioral operations research and emotions in decision making has taken a prominent place as it leads to a better understanding of crowd emotions and crowd decision making. In this context, the authors developed the Affective Analytics of Demonstration Sites (ANDES) that added to this body of research in four ways. The crowd analysis and simulations conducted with ANDES used a neuro decision matrix. The matrix contains a detailed description of demonstration sites (public spaces) in question and the emotions, valence, arousal and physiological parameters of people present there. With ANDES’s Remote Sensor Network, emotional (emotions, valence, arousal) and physiological (average crowd facial temperature, crowd composition by gender and age group, etc.) parameters of people present at demonstration sites can be mapped. ANDES can assist experts in more effective implementations of public spaces planning and a participation process by attendees by collecting and examining various layers of data on the emotional and physiological parameters of visitors based on a visitors-centric public spaces planning approach. ANDES can determine the public space and real estate values.",space
10.1016/j.robot.2018.11.017,Journal,Robotics and Autonomous Systems,scopus,2019-05-01,sciencedirect,A real-time framework for kinodynamic planning in dynamic environments with application to quadrotor obstacle avoidance,https://api.elsevier.com/content/abstract/scopus_id/85062619374,"The objective of this paper is to present a full-stack, real-time motion planning framework for kinodynamic robots and then show how it is applied and demonstrated on a physical quadrotor system operating in a laboratory environment. The proposed framework utilizes an offline–online computation paradigm, neighborhood classification through machine learning, sampling-based motion planning with an optimal cost distance metric, and trajectory smoothing to achieve real-time planning for aerial vehicles. This framework accounts for dynamic obstacles with an event-based replanning structure and a locally reactive control layer that minimizes replanning events. The approach is demonstrated on a quadrotor navigating moving obstacles in an indoor space and stands as, arguably, one of the first demonstrations of full-online kinodynamic motion planning, with execution cycles of 3 Hz to 5 Hz. For the quadrotor, a simplified dynamics model is used during the planning phase to accelerate online computation. A trajectory smoothing phase, which leverages the differentially flat nature of quadrotor dynamics, is then implemented to guarantee a dynamically feasible trajectory.",space
10.1016/j.cels.2019.03.003,Journal,Cell Systems,scopus,2019-04-24,sciencedirect,DoubletFinder: Doublet Detection in Single-Cell RNA Sequencing Data Using Artificial Nearest Neighbors,https://api.elsevier.com/content/abstract/scopus_id/85064396876,"Single-cell RNA sequencing (scRNA-seq) data are commonly affected by technical artifacts known as “doublets,” which limit cell throughput and lead to spurious biological conclusions. Here, we present a computational doublet detection tool—DoubletFinder—that identifies doublets using only gene expression data. DoubletFinder predicts doublets according to each real cell’s proximity in gene expression space to artificial doublets created by averaging the transcriptional profile of randomly chosen cell pairs. We first use scRNA-seq datasets where the identity of doublets is known to show that DoubletFinder identifies doublets formed from transcriptionally distinct cells. When these doublets are removed, the identification of differentially expressed genes is enhanced. Second, we provide a method for estimating DoubletFinder input parameters, allowing its application across scRNA-seq datasets with diverse distributions of cell types. Lastly, we present “best practices” for DoubletFinder applications and illustrate that DoubletFinder is insensitive to an experimentally validated kidney cell type with “hybrid” expression features.",space
10.1016/j.mfglet.2019.05.003,Journal,Manufacturing Letters,scopus,2019-04-01,sciencedirect,A blockchain enabled Cyber-Physical System architecture for Industry 4.0 manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/85066168835,"Cyber-Physical Production Systems (CPPSs) are complex manufacturing systems which aim to integrate and synchronize machine world and manufacturing facility to the cyber computational space. However, having intensive interconnectivity and a computational platform is crucial for real-world implementation of CPPSs. In this paper, the potential impacts of blockchain technology in development and realization of real-world CPPSs are discussed. A unified three-level blockchain architecture is proposed as a guideline for researchers and industries to clearly identify the potentials of blockchain and adapt, develop, and incorporate this technology with their manufacturing developments towards Industry 4.0.",space
10.1016/j.neucom.2019.01.019,Journal,Neurocomputing,scopus,2019-03-28,sciencedirect,Multi-view transfer learning with privileged learning framework,https://api.elsevier.com/content/abstract/scopus_id/85060576231,"In this paper, we present a multi-view transfer learning model named Multi-view Transfer Discriminative Model (MTDM) for both image and text classification tasks. Transfer learning, which aims to learn a robust classifier for the target domain using data from a different distribution, has been proved to be effective in many real-world applications. However, most of the existing transfer learning methods map across domain data into a high-dimension space which the distance between domains is closed. This strategy always fails in the multi-view scenario. On the contrary, the multi-view learning methods are also difficult to extend in the transfer learning settings. One of our goals in this paper is to develop a model which can perform better in both multi-view and transfer learning settings. On the one hand, the problem of multi-view is implemented by the paradigm of learning using privileged information (LUPI), which could guarantee the principle of complementary and consensus. On the other hand, the model adequately utilizes the source domain data to build a robust classifier for the target domain. We evaluate our model on both image and text classification tasks and show the effectiveness compared with other baseline approaches.",space
10.1016/j.dss.2019.01.003,Journal,Decision Support Systems,scopus,2019-03-01,sciencedirect,Deep learning based personalized recommendation with multi-view information integration,https://api.elsevier.com/content/abstract/scopus_id/85060338648,"With the rapid proliferation of images on e-commerce platforms today, embracing and integrating versatile information sources have become increasingly important in recommender systems. Owing to the heterogeneity in information sources and consumers, it is necessary and meaningful to consider the potential synergy between visual and textual content as well as consumers' different cognitive styles. This paper proposes a multi-view model, namely, Deep Multi-view Information iNtEgration (Deep-MINE), to take multiple sources of content (i.e., product images, descriptions and review texts) into account and design an end-to-end recommendation model. In doing so, stacked auto-encoder networks are deployed to map multi-view information into a unified latent space, a cognition layer is added to depict consumers' heterogeneous cognition styles and an integration module is introduced to reflect the interaction of multi-view latent representations. Extensive experiments on real world data demonstrate that Deep-MINE achieves high accuracy in product ranking, especially in the cold-start case. In addition, Deep-MINE is able to boost overall model performance compared with models taking a single view, further verifying the proposed model's effectiveness on information integration.",space
10.1016/j.solener.2019.01.027,Journal,Solar Energy,scopus,2019-03-01,sciencedirect,ANN based automatic slat angle control of venetian blind for minimized total load in an office building,https://api.elsevier.com/content/abstract/scopus_id/85059868380,"Windows are the only part of a building that can directly penetrate the solar radiation into the occupied space and thus the shading devices are needed to control the solar penetration. A variety of research have been conducted to develop the optimized slat angle control in the existing literature, but the research incorporating artificial intelligence technique with slat angle control is limited thus far. Therefore, in this study, the ANN (Artificial Neural Network) model was applied to minimize the combined total load consisting of lighting, cooling, and heating loads through automatic slat angle control of venetian blinds. A three-story rectangular office building was simulated using EnergyPlus, and dimming control was applied to control the lighting. The interlocked simulation between Matlab and EnergyPlus was conducted through BCVTB. As a result of comparing automatic blind control via the ANN to fixed blind slat angle, the automatic blind control via the ANN showed 9.1% lower total load than the blind angle fixed at 50°. It was confirmed that the cooling and heating load could be significantly reduced by real-time automatic control via the ANN under various operating conditions, rather than fixing the blinds at one angle.",space
10.1016/j.ress.2018.07.024,Journal,Reliability Engineering and System Safety,scopus,2019-03-01,sciencedirect,A new approach for estimating the parameters of Weibull distribution via particle swarm optimization: An application to the strengths of glass fibre data,https://api.elsevier.com/content/abstract/scopus_id/85056745362,"Three-parameter Weibull is one of the most popular and most widely-used distribution in many fields of science. Therefore, many studies have been conducted concerning the statistical inferences of the parameters of Weibull distribution. In general, the maximum likelihood (ML) methodology is used in the estimation process of unknown parameters. In this study, the ML estimation of the parameters of Weibull distribution is considered using particle swarm optimization (PSO). As in other heuristic optimization methods, the performance of PSO is affected by initial conditions. The novelty of this study comes from the fact that we propose a new adaptive search space based on confidence intervals in PSO. The modified maximum likelihood (MML) estimators are utilized for constructing the confidence intervals. MML based confidence intervals allow a narrower search space for the parameters of Weibull distribution than the search space used in the literature. Therefore, the performance of PSO increases, since the search space is wisely narrowed. In order to show the performance of the proposed approach, an extensive Monte-Carlo simulation study is conducted. Simulation results show that the proposed approach works well. In addition, real world data is analyzed to show implementation of the proposed method.",space
10.1016/j.jhydrol.2018.11.069,Journal,Journal of Hydrology,scopus,2019-02-01,sciencedirect,"An enhanced extreme learning machine model for river flow forecasting: State-of-the-art, practical applications in water resource engineering area and future research direction",https://api.elsevier.com/content/abstract/scopus_id/85059162436,"Despite the massive diversity in the modeling requirements for practical hydrological applications, there remains a need to develop more reliable and intelligent expert systems used for real-time prediction purposes. The challenge in meeting the standards of an expert system is primarily due to the influence and behavior of hydrological processes that is driven by natural fluctuations over the physical scale, and the resulting variance in the underlying model input datasets. River flow forecasting is an imperative task for water resources operation and management, water demand assessments, irrigation and agriculture, early flood warning and hydropower generations. This paper aims to investigate the viability of the enhanced version of extreme learning machine (EELM) model in river flow forecasting applied in a tropical environment. Herein, we apply the complete orthogonal decomposition (COD) learning tool to tune the output-hidden layer of the ELM model’s internal neuronal system, instead of the conventional multi-resolution tool (e.g., singular value decomposition). To demonstrate the application of EELM model, the Kelantan River, located in the Malaysian peninsular, selected as a case study. For a comparison of the EELM model, and further model evaluation, two distinct data-intelligent models are developed (i.e., the classical ELM and the support vector regression, SVR model). An exhaustive list of diagnostic indicators are used to evaluate the EELM model in respect to the benchmark algorithms, namely, SVR and ELM. The model performance indicators exhibit superior results for the EELM model relative to ELM and SVR models. In addition, the EELM model is presented as a more accurate, alternative predictive tool for modelling the tropical river flow patterns and its underlying characteristic perturbations in the physical space. Several statistical metrics defined as the coefficient of determination (r), Nash-Sutcliffe efficiency (Ens
                     ), Willmott’s Index (WI), root-mean-square error (RMSE) and mean absolute error (MAE) are computed to assess the model’s effectiveness. In quantitative terms, superiority of EELM over ELM and SVR models was exhibited by Ens
                      = 0.7995, 0.7434 and 0.665, r = 0.894, 0.869 and 0.818 and WI = 0.9380, 0.9180 and 0.8921, respectively. Whereas, EELM model attained lower (RMSE and MAE) values by approximately (11.61–22.53%) and (8.26–8.72%) relative to ELM and SVR models, respectively. The obtained results reveal that the EELM model is a robust expert model and can be embraced practically in real-life water resources management and river sustainability decisions. As a complementary component of this paper, we also review state-of-art research works where scholars have embraced extensive implementation of the ELM model in water resource engineering problems. A comprehensive evaluation is carried out to recognize the current limitations, and also to propose potential opportunities of applying improved variants of the ELM model presented as a future research direction.",space
10.1016/j.gaitpost.2018.11.029,Journal,Gait and Posture,scopus,2019-02-01,sciencedirect,"Three-dimensional cameras and skeleton pose tracking for physical function assessment: A review of uses, validity, current developments and Kinect alternatives",https://api.elsevier.com/content/abstract/scopus_id/85057183966,"Background
                  Three-dimensional camera systems that integrate depth assessment with traditional two-dimensional images, such as the Microsoft Kinect, Intel Realsense, StereoLabs Zed and Orbecc, hold great promise as physical function assessment tools. When combined with point cloud and skeleton pose tracking software they can be used to assess many different aspects of physical function and anatomy. These assessments have received great interest over the past decade, and will likely receive further study as the integration of depth sensing and augmented reality smartphone cameras occurs more in everyday life.
               
                  Research Question
                  The aim of this review is to discuss how these devices work, what options are available, the best methods for performing assessments and how they can be used in the future.
               
                  Methods
                  Firstly, a review of the Microsoft Kinect devices and associated artificial intelligence, automated skeleton tracking algorithms is provided. This includes a narrative critique of the validity and clinical utility of these devices for assessing different aspects of physical function including spatiotemporal, kinematic and inverse dynamics data derived from gait and balance trials, and anatomical assessments performed using the depth sensor information. Methods for improving the accuracy of data are examined, including multiple-camera systems and sensor fusion with inertial monitoring units, model fitting, and marker tracking. Secondly, alternative hardware, including other structured light and time of flight methods, stereoscopic cameras and augmented reality leveraging smartphone and tablet cameras to perform measurements in three-dimensional space are summarised. Software options related to depth sensing cameras are then discussed, focussing on recent advances such as OpenPose and web-based methods such as PoseNet.
               
                  Results and Significance
                  The clinical and non-laboratory utility of these devices holds great promise for physical function assessment, and recent developments could strengthen their ability to provide important and impactful health-related data.",space
10.1016/j.robot.2018.10.015,Journal,Robotics and Autonomous Systems,scopus,2019-02-01,sciencedirect,Time-dependent genetic algorithm and its application to quadruped's locomotion,https://api.elsevier.com/content/abstract/scopus_id/85056925953,"Genetic algorithms (GAs) are widely used in machine learning and optimization. This paper proposes a time-dependent genetic algorithm (TDGA) based on real-coded genetic algorithm (RCGA) to improve the convergence performance of functions over time such as a foot trajectory. TDGA has several distinguishing features when compared with traditional RCGA. First, individuals are arranged over time, and then the individuals are optimized in sequence. Second, search spaces of design variables are newly comprised of processes of reductions for search spaces. Third, the search space for crossover operations is expanded to avoid local minima traps that can occur in new search spaces up to the previous search space before performing any reduction of search space, and boundary mutation operation is performed to the new search spaces. Computer simulations are implemented to verify the convergence performance of the robot locomotion optimized by TDGA. Then, TDGA optimizes the desired feet trajectories of quadruped robots that climb up a slope and the impedance parameters of admittance control so that quadruped robots can trot stably over irregular terrains. Simulation results clearly represent that the convergence performance is improved by TDGA, which also shows that TDGA could be broadly used in robot locomotion research.",space
10.1016/j.comnet.2018.11.013,Journal,Computer Networks,scopus,2019-01-15,sciencedirect,Towards automatic fingerprinting of IoT devices in the cyberspace,https://api.elsevier.com/content/abstract/scopus_id/85056904979,"Nowadays, the cyberspace consists of an increasing number of IoT devices, such as net-printers, webcams, and routers. Illuminating the nature of online devices would provide insights into detecting potentially vulnerable devices on the Internet. However, there is a lack of device discovery in large-scale due to the massive number of device models (i.e., types, vendors, and products). In this paper, we propose an efficient approach to generate fingerprints of IoT devices. We observe that device manufacturers have different network system implementations on their products. We explore features spaces of IoT devices in three network layers, including the network-layer, transport-layer, and application-layer. Utilizing the feature of network protocols, we generate IoT device fingerprints based on neural network algorithms. Furthermore, we implement the prototype system and conduct real experiments to validate the performance of device fingerprints. Results show that our classification can generate device class labels with a 94% precision and 95% recall. We use those device fingerprints to discover 15.3 million network-connected devices and analyze their distribution characteristics in cyberspace.",space
10.1016/j.procs.2019.09.069,Conference Proceeding,Procedia Computer Science,scopus,2019-01-01,sciencedirect,An Innovative Technology: Augmented Reality Based Information Systems,https://api.elsevier.com/content/abstract/scopus_id/85076255225,"In our generation the information systems evolve with new technologies: augmented reality (AR), IoT, artificial intelligence, blockchain etc. Anymore they perform information exchange by sensors. It is estimated that the systems will be in a state of extreme interaction and reach 50 billion devices connected in Internet in 2020. We know that everything around us will be in interaction and they will do everything without any need of human interference. For example, when our dishwasher is full, it will start to wash automatically, or when the run out of the gasoline, our car will drive to the nearest station, or even when a burglar is entered to our house, it will automatically be detected and be announced to the police office. In business life, the processes will be automatical in maximum level and this technology will increase productivity and efficiency. Next to mobile technology, it is thought that these new generation information systems (IS) will take the biggest place in our lives. AR also will be integrated to these systems to augment the information in real world. Humanity will augment its habitat in an innovative way thanks to these AR based IS. This paper surveys the current state-of-the-art AR systems related with aerospace & defense, industry, education, medical and gaming sectors. The connection of AR based IS and innovation is explained with a technological insight. In addition to international use cases HAVELSAN’s use cases are also given that are performed from the aspect of applied open innovation strategy. This strategy is addressed specific to the implemented activities of AR based IS.",space
10.1016/j.asoc.2018.10.010,Journal,Applied Soft Computing Journal,scopus,2019-01-01,sciencedirect,A hybridization of extended Kalman filter and Ant Colony Optimization for state estimation of nonlinear systems,https://api.elsevier.com/content/abstract/scopus_id/85056208326,"In this paper, a new nonlinear heuristic filter based on the hybridization of an extended Kalman filter and an ant colony estimator is proposed to estimate the states of a nonlinear system. In this filter, a group of virtual ants searches the state space stochastically and dynamically to find and track the best state estimation while the position of each ant is updated at the measurement time using the extended Kalman filter. The performance of the proposed filter is compared with well-known heuristic filters using a nonlinear benchmark problem. The statistical results show that this algorithm is able to provide promising and competitive results. Then, the new filter is tested on a nonlinear engineering problem with more than one state. The problem is to estimate simultaneously the states of an unmanned aerial vehicle as well as the wind disturbances, applied to the system. In this case, a processor-in-the-loop experiment is also performed to verify the implementation capability of the proposed approach. This paper also investigates the real-time implementation capability of the proposed filter in the attitude estimation of a three degrees of freedom experimental setup of a quadrotor to further investigate its effectiveness in practice.",space
10.1016/j.euromechsol.2018.10.011,Journal,"European Journal of Mechanics, A/Solids",scopus,2019-01-01,sciencedirect,A dual interpolation boundary face method for elasticity problems,https://api.elsevier.com/content/abstract/scopus_id/85055917199,"A dual interpolation boundary face method (DiBFM) is proposed to unify the conforming and nonconforming elements in boundary element method (BEM) implementation. In the DiBFM, the nodes of a conventional conforming element are sorted into two groups: the nodes on the boundary (called virtual nodes) and the internal nodes (called source nodes). Without virtual nodes, the conforming element turns to be a conventional nonconforming element of a lower order. Physical variables are interpolated using the conforming elements, the same way as conforming BEM. Boundary integral equations are collocated at source nodes, the same way as nonconforming BEM. To make the final system of linear equations solvable, additional constraint equations are required to condense the degrees of freedom for all the virtual nodes. These constraints are constructed using the moving least-squares (MLS). Besides, both boundary integration and MLS are performed in the parametric spaces of curves, namely, the geometric data, such as coordinates, out normals and Jacobians, are calculated directly from curves rather than from elements. Thus, no geometric errors are introduced no matter how coarse the discretization is. The method has been implemented successfully for solving two-dimensional elasticity problems. A number of numerical examples with real engineering background have demonstrated the accuracy and efficiency of the new method.",space
10.1016/j.infsof.2018.08.003,Journal,Information and Software Technology,scopus,2019-01-01,sciencedirect,An extensible framework for software configuration optimization on heterogeneous computing systems: Time and energy case study,https://api.elsevier.com/content/abstract/scopus_id/85051630181,"Context: Application of component based software engineering methods to heterogeneous computing (HC) enables different software configurations to realize the same function with different non–functional properties (NFP). Finding the best software configuration with respect to multiple NFPs is a non–trivial task.
                  
                     Objective: We propose a Software Component Allocation Framework (SCAF) with the goal to acquire a (sub–) optimal software configuration with respect to multiple NFPs, thus providing performance prediction of a software configuration in its early design phase. We focus on the software configuration optimization for the average energy consumption and average execution time.
                  
                     Method: We validated SCAF through its instantiation on a real–world demonstrator and a simulation. Firstly, we verified the correctness of our model through comparing the performance prediction of six software configurations to the actual performance, obtained through extensive measurements with a confidence interval of 95%. Secondly, to demonstrate how SCAF scales up, we performed software configuration optimization on 55 generated use–cases (with solution spaces ranging from 1030 to 3070) and benchmark the results against best performing random configurations.
                  
                     Results: The performance of a configuration as predicted by our framework matched the configuration implemented and measured on a real–world platform. Furthermore, by applying the genetic algorithm and simulated annealing to the weight function given in SCAF, we obtain sub–optimal software configurations differing in performance at most 7% and 13% from the optimal configuration (respectfully).
                  
                     Conclusion: SCAF is capable of correctly describing a HC platform and reliably predict the performance of software configuration in the early design phase. Automated in the form of an Eclipse plugin, SCAF allows software architects to model architectural constraints and preferences, acting as a multi–criterion software architecture decision support system. In addition to said, we also point out several interesting research directions, to further investigate and improve our approach.",space
10.1016/j.jnca.2018.09.023,Journal,Journal of Network and Computer Applications,scopus,2018-12-15,sciencedirect,Dynamic workload patterns prediction for proactive auto-scaling of web applications,https://api.elsevier.com/content/abstract/scopus_id/85054442625,"Proactive auto-scaling methods dynamically manage the resources for an application according to the current and future load predictions to preserve the desired performance at a reduced cost. However, auto-scaling web applications remain challenging mainly due to dynamic workload intensity and characteristics which are difficult to predict. Most existing methods mainly predict the request arrival rate which only partially captures the workload characteristics and the changing system dynamics that influence the resource needs. This may lead to inappropriate resource provisioning decisions. In this paper, we address these challenges by proposing a framework for prediction of dynamic workload patterns as follows. First, we use an unsupervised learning method to analyze the web application access logs to discover URI (Uniform Resource Identifier) space partitions based on the response time and the document size features. Then for each application URI, we compute its distribution across these partitions based on historical access logs to accurately capture the workload characteristics compared to just representing the workload using the request arrival rate. These URI distributions are then used to compute the Probabilistic Workload Pattern (PWP), which is a probability vector describing the overall distribution of incoming requests across URI partitions. Finally, the identified workload patterns for a specific number of last time intervals are used to predict the workload pattern of the next interval. The latter is used for future resource demand prediction and proactive auto-scaling to dynamically control the provisioning of resources. The framework is implemented and experimentally evaluated using historical access logs of three real web applications, each with increasing, decreasing, periodic, and randomly varying arrival rate behaviors. Results show that the proposed solution yields significantly more accurate predictions of workload patterns and resource demands of web applications compared to existing approaches.",space
10.1016/j.neuroimage.2018.08.031,Journal,NeuroImage,scopus,2018-12-01,sciencedirect,Phase shift invariant imaging of coherent sources (PSIICOS) from MEG data,https://api.elsevier.com/content/abstract/scopus_id/85054308167,"Increasing evidence suggests that neuronal communication is a defining property of functionally specialized brain networks and that it is implemented through synchronization between population activities of distinct brain areas. The detection of long-range coupling in electroencephalography (EEG) and magnetoencephalography (MEG) data using conventional metrics (such as coherence or phase-locking value) is by definition contaminated by spatial leakage. Methods such as imaginary coherence, phase-lag index or orthogonalized amplitude correlations tackle spatial leakage by ignoring zero-phase interactions. Although useful, these metrics will by construction lead to false negatives in cases where true zero-phase coupling exists in the data and will underestimate interactions with phase lags in the vicinity of zero. Yet, empirically observed neuronal synchrony in invasive recordings indicates that it is not uncommon to find zero or close-to-zero phase lag between the activity profiles of coupled neuronal assemblies.
                  Here, we introduce a novel method that allows us to mitigate the undesired spatial leakage effects and detect zero and near zero phase interactions. To this end, we propose a projection operation that operates on sensor-space cross-spectrum and suppresses the spatial leakage contribution but retains the true zero-phase interaction component. We then solve the network estimation task as a source estimation problem defined in the product space of interacting source topographies. We show how this framework provides reliable interaction detection for all phase-lag values and we thus refer to the method as Phase Shift Invariant Imaging of Coherent Sources (PSIICOS).
                  Realistic simulations demonstrate that PSIICOS has better detector characteristics than existing interaction metrics. Finally, we illustrate the performance of PSIICOS by applying it to real MEG dataset recorded during a standard mental rotation task. Taken together, using analytical derivations, data simulations and real brain data, this study presents a novel source-space MEG/EEG connectivity method that overcomes previous limitations and for the first time allows for the estimation of true zero-phase coupling via non-invasive electrophysiological recordings.",space
10.1016/j.neucom.2018.06.045,Journal,Neurocomputing,scopus,2018-11-03,sciencedirect,Adaptive Neighborhood MinMax Projections,https://api.elsevier.com/content/abstract/scopus_id/85049433041,"Dimensionality reduction as one of most attractive topics in machine learning research area has aroused extensive attentions in recent years. In order to preserve the local structure of data, most of dimensionality reduction methods consider constructing the relationships among each sample and its k nearest neighbors, and they find the neighbors in original space by using Euclidean distance. Since the data in original space contain some noises and redundant features, finding the neighbors in original space is incorrect and may degrade the subsequent performance. Therefore, how to find the optimal k nearest neighbors for each sample is the key point to improve the robustness of model. In this paper, we propose a novel dimensionality reduction method, named Adaptive Neighborhood MinMax Projections (ANMMP) which finds the neighbors in the optimal subspace by solving Trace Ratio problem in which the noises and redundant features have been removed already. Meanwhile, the samples within same class are pulled together while the samples between different classes are pushed far away in such learned subspace. Besides, proposed model is a general approach which can be implemented easily and applied on other methods to improve the robustness. Extensive experiments conducted on several synthetic data and real-world data sets and achieve some encouraging performance with comparison to metric learning and feature extraction methods, which demonstrates the efficiency of our method.",space
10.1016/j.ijepes.2018.03.031,Journal,International Journal of Electrical Power and Energy Systems,scopus,2018-10-01,sciencedirect,Clustering-based novelty detection for identification of non-technical losses,https://api.elsevier.com/content/abstract/scopus_id/85044607485,"The reduction of non-technical losses is a significant part of the total potential benefits resulting from implementations of the smart grid concept. This paper proposes a data-based method to detect sources of theft and other commercial losses. Prototypes of typical consumption behavior are extracted through clustering of data collected from smart meters. A distance-based novelty detection framework classifies new data samples as malign if their distance to the typical consumption prototypes is significant. The proposed method works on the space of four different indicators of irregular consumption, enabling the easy interpretation of results. A use case based on real data is presented to evaluate the method. The threat model considers sixteen different possible types of changes in consumption pattern that result from non-technical losses, including attacks and defects present since the first day of metering. The proposed clustering-based novelty detection method for identification of non-technical losses, using the Gustafson-Kessel fuzzy clustering algorithm, achieves a true positive rate of 63.6% and false positive rate of 24.3%, outperforming other state-of-the-art unsupervised learning methods.",space
10.1016/j.ins.2018.04.070,Journal,Information Sciences,scopus,2018-08-01,sciencedirect,Continuously maintaining approximate quantile summaries over large uncertain datasets,https://api.elsevier.com/content/abstract/scopus_id/85047087656,"Quantile summarization is a useful tool for management of massive datasets in the rapidly growing number of applications, and its importance is further enhanced with uncertainty in the data being explored. In this paper, we focus on the problem of computing approximate quantile summaries over large uncertain datasets. On the basis of GK [14] algorithm, we propose a novel online algorithm namely uGK. Using only little space, the proposed uGK algorithm maintains a small set of tuples, each of which contains a point value and the “count” of uncertain elements that are not larger than this value, and supports any quantile query within a given error. Experimental evaluation on both synthetic and real-life datasets illustrates the effectiveness of our uGK algorithm.",space
10.1016/j.apacoust.2018.03.012,Journal,Applied Acoustics,scopus,2018-08-01,sciencedirect,Chaotic fractal walk trainer for sonar data set classification using multi-layer perceptron neural network and its hardware implementation,https://api.elsevier.com/content/abstract/scopus_id/85044150028,"First, this study proposes the use of the newly developed Stochastic Fractal Search (SFS) algorithm for training MLP NNs to design the evolutionary classifier. Evolutionary classifiers, often experience problems of slow convergence speed, trapping in local minima, and non-real-time classification. This paper also use four chaotic maps to improve the performance of the SFS. This modified version of SFS has been called Chaotic Fractal Walk Trainer (CFWT). To assess the performance of the proposed classifiers, these networks will be evaluated using the two benchmark datasets and a high-dimensional practical sonar dataset. For endorsement, the results are compared to four popular meta-heuristics trainers. The results show that new classifiers suggest better performance than the other benchmark algorithms, in terms of entrapment in local minima, classification accuracy, and convergence speed. This paper also implements the designed classifier on the Filed Programmable Field Array (FPGA) substrate for testing the real-time processing ability of the proposed method. The results of the real application prove that the designed classifiers are applicable to high-dimension challenging problems with unknown search spaces.",space
10.1016/j.media.2018.04.004,Journal,Medical Image Analysis,scopus,2018-07-01,sciencedirect,VP-Nets: Efficient automatic localization of key brain structures in 3D fetal neurosonography,https://api.elsevier.com/content/abstract/scopus_id/85046367108,"Three-dimensional (3D) fetal neurosonography is used clinically to detect cerebral abnormalities and to assess growth in the developing brain. However, manual identification of key brain structures in 3D ultrasound images requires expertise to perform and even then is tedious. Inspired by how sonographers view and interact with volumes during real-time clinical scanning, we propose an efficient automatic method to simultaneously localize multiple brain structures in 3D fetal neurosonography. The proposed View-based Projection Networks (VP-Nets), uses three view-based Convolutional Neural Networks (CNNs), to simplify 3D localizations by directly predicting 2D projections of the key structures onto three anatomical views.
                  While designed for efficient use of data and GPU memory, the proposed VP-Nets allows for full-resolution 3D prediction. We investigated parameters that influence the performance of VP-Nets, e.g. depth and number of feature channels. Moreover, we demonstrate that the model can pinpoint the structure in 3D space by visualizing the trained VP-Nets, despite only 2D supervision being provided for a single stream during training. For comparison, we implemented two other baseline solutions based on Random Forest and 3D U-Nets. In the reported experiments, VP-Nets consistently outperformed other methods on localization. To test the importance of loss function, two identical models are trained with binary corss-entropy and dice coefficient loss respectively. Our best VP-Net model achieved prediction center deviation: 1.8 ± 1.4 mm, size difference: 1.9 ± 1.5 mm, and 3D Intersection Over Union (IOU): 63.2 ± 14.7% when compared to the ground truth. To make the whole pipeline intervention free, we also implement a skull-stripping tool using 3D CNN, which achieves high segmentation accuracy. As a result, the proposed processing pipeline takes a raw ultrasound brain image as input, and output a skull-stripped image with five detected key brain structures.",space
10.1016/j.eswa.2017.11.011,Journal,Expert Systems with Applications,scopus,2018-06-15,sciencedirect,Towards a common implementation of reinforcement learning for multiple robotic tasks,https://api.elsevier.com/content/abstract/scopus_id/85035079318,"Mobile robots are increasingly being employed for performing complex tasks in dynamic environments. Those tasks can be either explicitly programmed by an engineer or learned by means of some automatic learning method, which improves the adaptability of the robot and reduces the effort of setting it up. In this sense, reinforcement learning (RL) methods are recognized as a promising tool for a machine to learn autonomously how to do tasks that are specified in a relatively simple manner. However, the dependency between these methods and the particular task to learn is a well-known problem that has strongly restricted practical implementations in robotics so far. Breaking this barrier would have a significant impact on these and other intelligent systems; in particular, having a core method that requires little tuning effort for being applicable to diverse tasks would boost their autonomy in learning and self-adaptation capabilities. In this paper we present such a practical core implementation of RL, which enables the learning process for multiple robotic tasks with minimal per-task tuning or none. Based on value iteration methods, we introduce a novel approach for action selection, called Q-biased softmax regression (QBIASSR), that takes advantage of the structure of the state space by attending the physical variables involved (e.g., distances to obstacles, robot pose, etc.), thus experienced sets of states accelerate the decision-making process of unexplored or rarely-explored states. Intensive experiments with both real and simulated robots, carried out with the software framework also introduced here, show that our implementation is able to learn different robotic tasks without tuning the learning method. They also suggest that the combination of true online SARSA(λ) (TOSL) with QBIASSR can outperform the existing RL core algorithms in low-dimensional robotic tasks. All of these are promising results towards the possibility of learning much more complex tasks autonomously by a robotic agent.",space
10.1016/j.compind.2018.03.014,Journal,Computers in Industry,scopus,2018-06-01,sciencedirect,Real-time object detection in agricultural/remote environments using the multiple-expert colour feature extreme learning machine (MEC-ELM),https://api.elsevier.com/content/abstract/scopus_id/85044151304,"It is necessary for autonomous robotics in agriculture to provide real time feedback, but due to a diverse array of objects and lack of landscape uniformity this objective is inherently complex. The current study presents two implementations of the multiple-expert colour feature extreme learning machine (MEC-ELM). The MEC-ELM is a cascading algorithm that has been implemented along side a summed area table (SAT) for fast feature extraction and object classification, for a fully functioning object detection algorithm. The MEC-ELM is an implementation of the colour feature extreme learning machine (CF-ELM), which is an extreme learning machine (ELM) with a partially connected hidden layer; taking three colour bands as inputs. The colour implementation used with the SAT enable the MEC-ELM to find and classify objects quickly, with 84% precision and 91% recall in weed detection in the Y’UV colour space and in 0.5 s per frame. The colour implementation is however limited to low resolution images and for this reason a colour level co-occurrence matrix (CLCM) variant of the MEC-ELM is proposed. This variant uses the SAT to produce a CLCM and texture analyses, with texture values processed as an input to the MEC-ELM. This enabled the MEC-ELM to achieve 78–85% precision and 81–93% recall in cattle, weed and quad bike detection and in times between 1 and 2 s per frame. Both implementations were benchmarked on a standard i7 mobile processor. Thus the results presented in this paper demonstrated that the MEC-ELM with SAT grid and CLCM makes an ideal candidate for fast object detection in complex and/or agricultural landscapes.",space
10.1016/j.knosys.2018.03.005,Journal,Knowledge-Based Systems,scopus,2018-06-01,sciencedirect,Unsupervised geographically discriminative feature learning for landmark tagging,https://api.elsevier.com/content/abstract/scopus_id/85043325506,"Recently, a large number of geo-tagged landmark images have been uploaded through various social media services. Usually, these geo-tagged images are annotated by users with GPS and tags related to the landmarks where they are taken. Landmark tagging aims to automatically annotate an image with the tags to describe the landmark where the image is taken. It has been observed that the images and tags show strong correlation with the geographical locations. The widely used assumption by many existing tagging methods is that images are independently and identically distributed is not effective to capture the geographical correlation. In this paper, we study the novel problem of utilizing the geographical correlation among images and landmarks for better tagging landmark images. In particular, we propose an unsupervised feature learning approach to learn the geographically discriminative features across geographical locations, by integrating latent space learning and geographically structural analysis (LSGSA) into a joint model. A latent space learning model is proposed to effectively fuse the heterogeneous features of visual content and tags. Meanwhile, the geographical structure analysis and group sparsity are applied to learn the geographically discriminative features. Then, a geo-guided sparse reconstruction method is proposed to tag images by utilizing the discriminative information of features, in which the landmark-specific tags are boosted by a weighting method. Experiments on the real-world datasets demonstrate the superiority of our approach.",space
10.1016/j.measurement.2018.02.060,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2018-06-01,sciencedirect,Development of a novel machine vision procedure for rapid and non-contact measurement of soil moisture content,https://api.elsevier.com/content/abstract/scopus_id/85042874145,"Soil moisture measurement is one of the essential management components to decrease water consumption and prevent water stresses in plants. In this study, a fast and non-contact method using machine vision and artificial intelligence was developed so as to make operators capable of having an estimate of soil moisture by taking only one image. Three soil textures along with three levels of added organic matter were applied. Mean comparison and the subsequent stepwise multiple regression were applied to find superior features from different color spaces. ANFIS and stepwise multiple regression were used to predict the soil moisture. Results indicated that the general model could predict the soil moisture with mean absolute error of less than 1.1%. This value reached to 0.3% for some sub-models belonging to the texture–organic matter group. Application of the present method is highly recommended for soil moisture measurement because of simple implementation and potential for online measurements.",space
10.1016/j.patcog.2018.01.012,Journal,Pattern Recognition,scopus,2018-06-01,sciencedirect,Multi-view manifold learning with locality alignment,https://api.elsevier.com/content/abstract/scopus_id/85042389370,"Manifold learning aims to discover the low dimensional space where the input high dimensional data are embedded by preserving the geometric structure. Unfortunately, almost all the existing manifold learning methods were proposed under single view scenario, and they cannot be straightforwardly applied to multiple feature sets. Although concatenating multiple views into a single feature provides a plausible solution, it remains a question on how to better explore the independence and interdependence of different views while conducting manifold learning. In this paper, we propose a multi-view manifold learning with locality alignment (MVML-LA) framework to learn a common yet discriminative low-dimensional latent space that contain sufficient information of original inputs. Both supervised algorithm (S-MVML-LA) and unsupervised algorithm (U-MVML-LA) are developed. Experiments on benchmark real-world datasets demonstrate the superiority of our proposed S-MVML-LA and U-MVML-LA over existing state-of-the-art methods.",space
10.1016/j.apenergy.2018.02.156,Journal,Applied Energy,scopus,2018-05-15,sciencedirect,Approximate model predictive building control via machine learning,https://api.elsevier.com/content/abstract/scopus_id/85043463406,"Many studies have proven that the building sector can significantly benefit from replacing the current practice rule-based controllers (RBC) by more advanced control strategies like model predictive control (MPC). However, the optimization-based control algorithms, like MPC, impose increasing hardware and software requirements, together with more complicated error handling capabilities required from the commissioning staff. In recent years, several studies introduced promising remedy for these problems by using machine learning algorithms. The idea is based on devising simplified control laws learned from MPC. The main advantage of the proposed methods stems from their easy implementation even on low-level hardware. However, most of the reported studies were dealing only with problems with a limited complexity of the parametric space, and devising laws only for a single control variable, which inevitably limits their applicability to more complex building control problems. In this paper, we introduce a versatile framework for synthesis of simple, yet well-performing control strategies that mimic the behavior of optimization-based controllers, also for large scale multiple-input-multiple-output (MIMO) control problems which are common in the building sector. The approach employs multivariate regression and dimensionality reduction algorithms. Particularly, deep time delay neural networks (TDNN) and regression trees (RT) are used to derive the dependency of multiple real-valued control inputs on parameters. The complexity of the problem, as well as implementation cost, are further reduced by selecting the most significant features from the set of parameters. This reduction is based on straightforward manual selection, principal component analysis (PCA) and dynamic analysis of the building model. The approach is demonstrated on a case study employing temperature control in a six-zone building, described by a linear model with 286 states and 42 disturbances, resulting in an MPC problem with more than thousand of parameters. The results show that simplified control laws retain most of the performance of the complex MPC, while significantly decreasing the complexity and implementation cost.",space
10.1016/j.neunet.2018.01.014,Journal,Neural Networks,scopus,2018-04-01,sciencedirect,Accelerated low-rank representation for subspace clustering and semi-supervised classification on large-scale data,https://api.elsevier.com/content/abstract/scopus_id/85042324490,"The scalability of low-rank representation (LRR) to large-scale data is still a major research issue, because it is extremely time-consuming to solve singular value decomposition (SVD) in each optimization iteration especially for large matrices. Several methods were proposed to speed up LRR, but they are still computationally heavy, and the overall representation results were also found degenerated. In this paper, a novel method, called accelerated LRR (ALRR) is proposed for large-scale data. The proposed accelerated method integrates matrix factorization with nuclear-norm minimization to find a low-rank representation. In our proposed method, the large square matrix of representation coefficients is transformed into a significantly smaller square matrix, on which SVD can be efficiently implemented. The size of the transformed matrix is not related to the number of data points and the optimization of ALRR is linear with the number of data points. The proposed ALRR is convex, accurate, robust, and efficient for large-scale data. In this paper, ALRR is compared with state-of-the-art in subspace clustering and semi-supervised classification on real image datasets. The obtained results verify the effectiveness and superiority of the proposed ALRR method.",space
10.1016/j.ins.2016.08.009,Journal,Information Sciences,scopus,2018-04-01,sciencedirect,Decentralized Clustering by Finding Loose and Distributed Density Cores,https://api.elsevier.com/content/abstract/scopus_id/84981714778,"Centroid-based clustering approaches fail to recognize extremely complex patterns that are non-isotropic. We analyze the underlying causes and find some inherent flaws in these approaches, including Shape Loss, False Distances and False Peaks, which typically cause centroid-based approaches to fail when applied to complex patterns. As an alternative to current methods, we propose a hybrid decentralized approach named DCore, which is based on finding density cores instead of centroids, to overcome these flaws. The underlying idea is that we consider each cluster to have a shrunken density core that roughly retains the shape of the cluster. Each such core consists of a set of loosely connected local density peaks of higher density than their surroundings. Borders, edges and outliers are distributed around the outsides of these cores in a hierarchical structure. Experiments demonstrate that the promise of DCore lies in its power to recognize extremely complex patterns and its high performance in real applications, for example, image segmentation and face clustering, regardless of the dimensionality of the space in which the data are embedded.",space
10.1016/j.artint.2017.12.001,Journal,Artificial Intelligence,scopus,2018-03-01,sciencedirect,Decentralized Reinforcement Learning of robot behaviors,https://api.elsevier.com/content/abstract/scopus_id/85038868982,"A multi-agent methodology is proposed for Decentralized Reinforcement Learning (DRL) of individual behaviors in problems where multi-dimensional action spaces are involved. When using this methodology, sub-tasks are learned in parallel by individual agents working toward a common goal. In addition to proposing this methodology, three specific multi agent DRL approaches are considered: DRL-Independent, DRL Cooperative-Adaptive (CA), and DRL-Lenient. These approaches are validated and analyzed with an extensive empirical study using four different problems: 3D Mountain Car, SCARA Real-Time Trajectory Generation, Ball-Dribbling in humanoid soccer robotics, and Ball-Pushing using differential drive robots. The experimental validation provides evidence that DRL implementations show better performances and faster learning times than their centralized counterparts, while using less computational resources. DRL-Lenient and DRL-CA algorithms achieve the best final performances for the four tested problems, outperforming their DRL-Independent counterparts. Furthermore, the benefits of the DRL-Lenient and DRL-CA are more noticeable when the problem complexity increases and the centralized scheme becomes intractable given the available computational resources and training time.",space
10.1016/j.artint.2017.11.006,Journal,Artificial Intelligence,scopus,2018-03-01,sciencedirect,Strong temporal planning with uncontrollable durations,https://api.elsevier.com/content/abstract/scopus_id/85036477809,"Planning in real world domains often involves modeling and reasoning about the duration of actions. Temporal planning allows such modeling and reasoning by looking for plans that specify start and end time points for each action. In many practical cases, however, the duration of actions may be uncertain and not under the full control of the executor. For example, a navigation task may take more or less time, depending on external conditions such as terrain or weather.
                  In this paper, we tackle the problem of strong temporal planning with uncontrollable action durations (STPUD). For actions with uncontrollable durations, the planner is only allowed to choose the start of the actions, while the end is chosen, within known bounds, by the environment. A solution plan must be robust with respect to all uncontrollable action durations, and must achieve the goal on all executions, despite the choices of the environment.
                  We propose two complementary techniques. First, we discuss a dedicated planning method, that generalizes the state-space temporal planning framework, leveraging SMT-based techniques for temporal networks under uncertainty. Second, we present a compilation-based method, that reduces any STPUD problem to an ordinary temporal planning problem. Moreover, we investigate a set of sufficient conditions to simplify domains by removing some of the uncontrollability.
                  We implemented both our approaches, and we experimentally evaluated our techniques on a large number of instances. Our results demonstrate the practical applicability of the two techniques, which show complementary behavior.",space
10.1016/j.ins.2017.11.012,Journal,Information Sciences,scopus,2018-03-01,sciencedirect,Finding the hottest item in data streams,https://api.elsevier.com/content/abstract/scopus_id/85035804309,"We study a problem of finding the hottest item interval in a data stream, where the hotness of an item over an interval is determined by its average frequency. Finding the hottest item interval is particularly helpful in business promotions, such as monitoring the peak sales records, finding the hottest period in an online game, digging the highest click rate of an online music, etc. Existing work focus on finding the most frequent item over a fixed length interval. However, these solutions cannot return the hottest interval since the best length (i.e., maximizing the average frequency) is unknown in advance. To discover the hottest item interval, a straightforward solution is to calculate the average frequencies of items for every possible interval length, which is too costly for stream applications. To efficiently compute the hottest item interval, we propose an algorithm that employs the arrival timestamps of items and reduce the search space by three pruning strategies. Extensive experiments show that the proposed algorithms can efficiently discover the hottest item interval on both real and synthetic datasets.",space
10.1016/j.knosys.2017.09.036,Journal,Knowledge-Based Systems,scopus,2018-03-01,sciencedirect,A temporal consistency method for online review ranking,https://api.elsevier.com/content/abstract/scopus_id/85031315796,"Providing appropriate online review ranking consistently with the entire review set is deemed important for e-commerce services to facilitate consumers decision making. Unlike the existing efforts that often treat online reviews statically, this paper takes the temporal dynamics of online reviews into account, and designs an effective method for time-aware review ranking. In doing so, first of all, a time-aware review consistency ranking (TRCR) problem is formulated, based on a newly defined metric, which aims to derive a compact review list with maximized expected consistency degree to the original review set. Furthermore, this problem is proven to be NP-hard, which leads us to developing an effective approximation by heuristically restricting the search space (i.e., TRCRea). This proposed method with related improvements show strengths on two aspects: one is that the informational decay of the reviews is well addressed at both macro and micro levels; and the other is that the compact review list provided to the consumers is obtained from a combined perspective of consistency and time-awareness in light of product features and sentiment orientations. Finally, the experiments on real-world data demonstrate the effectiveness and efficiency of the proposed method over baseline methods.",space
10.1016/j.jfranklin.2017.07.037,Journal,Journal of the Franklin Institute,scopus,2018-03-01,sciencedirect,System-on-a-chip (SoC)-based hardware acceleration for foreground and background identification,https://api.elsevier.com/content/abstract/scopus_id/85027498536,"The rapid growth of embedded vision applications and accessibility in recent years has instigated a philosophical shift in algorithm and implementation design for artificial intelligence. With the popularization of high-definition video, the amount of data available to be processed has also increased substantially, posing massive computational and communication demands. Hardware acceleration through specialization has received renewed interest in recent years; such acceleration has generally been implemented using two chips, with the image signal processing (ISP) part being performed by a DSP, a GPU or an FPGA and the video content analytics (VCA) part being executed by a processor. GPUs consume a substantial amount of power; thus, it is challenging to deploy them in embedded environments. However, the new generation of SoC-FPGAs that are fabricated with both the microprocessor and FPGA on a single chip consumes less power and can be built into small systems, thereby offering an attractive platform for embedded applications. This study presents the hardware acceleration of a real-time adaptive background and foreground identification algorithm in a SoC, including the capture, processing and display stages. The algorithm can be performed in either 2D or 3D space. The proposed platform uses photometric invariant color, depth data and local binary patterns (LBPs) to distinguish background from foreground. The system uses minimal cell resources, an elastically pipelined architecture is used to absorb variations in processing time, and each pipeline stage is optimized to use the available FPGA primitives. Additionally, the communication-centric architecture used in this work simplifies the implementation of embedded vision algorithms.",space
10.1016/j.inffus.2017.05.003,Journal,Information Fusion,scopus,2018-03-01,sciencedirect,A Social-aware online short-text feature selection technique for social media,https://api.elsevier.com/content/abstract/scopus_id/85019594702,"Large-scale text categorisation in social environments, characterised by the high dimensionality of feature spaces, is one of the most relevant problems in machine learning and data mining nowadays. Short-texts, which are posted at unprecedented rates, accentuate both the importance of learning tasks and the challenges posed by such large feature space. A collection of social media short-texts does not only provide textual information but also topological information given by the relationships between posts and their authors. The linked nature of social data causes new complementary data dimensions to be added to the feature space, which, at the same time, becomes sparser. Additionally, in the context of social media, posts usually arrive simultaneously in streams, which hinders the deployment of efficient traditional feature selection techniques that assume a feature space fully known in advance. Hence, efficient and scalable online feature selection becomes an important requirement in numerous large-scale social applications. This work presents an online feature selection technique for high-dimensional data based on the integration of two information sources, social and content-based, for the real-time classification of short-text streams coming from social media. It focuses on discovering implicit relations amongst new posts, already known ones and their corresponding authors to identify groups of socially related posts. Then, each discovered group is represented by a set of non-redundant and relevant textual features. Finally, such features are used to train different learning models for classifying newly arriving posts. Extensive experiments conducted on real-world short-texts demonstrate that the proposed approach helps to improve classification results when compared to state-of-the-art and traditional online feature selection techniques.",space
10.1016/j.neucom.2017.01.115,Journal,Neurocomputing,scopus,2018-02-14,sciencedirect,Extreme Learning Machine for Joint Embedding and Clustering,https://api.elsevier.com/content/abstract/scopus_id/85029666112,"Clustering generic data, i.e., data not specific to a particular field, is a challenging problem due to their diverse complex structures in the original feature space. Traditional approaches address this problem by complementing clustering with feature learning methods, which either capture the intrinsic structure of the data or represent the data such that clusters are better revealed. In this paper, we propose an approach referred to as Extreme Learning Machine for Joint Embedding and Clustering (ELM-JEC), which incorporates desirable properties of both types of feature learning methods at the same time, specifically by (1) preserving the manifold structure of the data in the original space; (2) maximizing the class separability of the data in the embedded space. Since either type of method has improved clustering performance in some cases, our motivation is to integrate the two desirable properties to further improve the accuracy and robustness of clustering. Additional notable features of ELM-JEC are that it provides nonlinear feature mappings and achieves feature learning and clustering in the same formulation. The proposed approach can be implemented using alternating optimization, and its clustering performance compares favorably with several state-of-the-art methods on the real-world benchmark datasets.",space
10.1016/j.neucom.2017.08.036,Journal,Neurocomputing,scopus,2018-01-31,sciencedirect,Data-driven model-free slip control of anti-lock braking systems using reinforcement Q-learning,https://api.elsevier.com/content/abstract/scopus_id/85029168035,"This paper proposes the design and implementation of a model-free tire slip control for a fast and highly nonlinear Anti-lock Braking System (ABS). A reinforcement Q-learning optimal control approach is inserted in a batch neural fitted scheme using two neural networks to approximate the value function and the controller, respectively. The transition samples required for learning high performance control can be collected by interacting with the process either by online exploiting the current iteration controller (or policy) under an ε-greedy exploration strategy, or by using data collected under any other controller that is capable of ensuring efficient exploration of the action-state space. Both approaches are highlighted in the paper. Fortunately, the ABS process fits this type of learning-by-interaction because it does not need an initial stabilizing controller. The validation case studies conducted on a real laboratory setup reveal that high control system performance can be achieved using the proposed approaches. Insightful comments on the observed control behavior are offered along with performance comparisons with several types of model-based and model-free controllers including relay, model-based optimal PI, an original model-free neural network state-feedback VRFT controller and a model-free neural network adaptive actor-critic one. With the ability to improve control performance starting from different supervisory controllers or to learn high performance controllers from scratch, the proposed Q-learning optimal control approach proves its performance in a wide operating range and is therefore recommended to its industrial application on ABS.",space
10.1016/j.patrec.2017.12.010,Journal,Pattern Recognition Letters,scopus,2018-01-15,sciencedirect,Face alignment with cascaded semi-parametric deep greedy neural forests,https://api.elsevier.com/content/abstract/scopus_id/85038375628,"Face alignment is an active topic in computer vision, consisting in aligning a shape model on the face. To this end, most modern approaches refine the shape in a cascaded manner, starting from an initial guess. Those shape updates can either be applied in the feature point space (i.e. explicit updates) or in a low-dimensional, parametric space. In this paper, we propose a semi-parametric cascade that first aligns a parametric shape, then captures more fine-grained deformations of an explicit shape. For the purpose of learning shape updates at each cascade stage, we introduce a deep greedy neural forest (GNF) model, which is an improved version of deep neural forest (NF). GNF appears as an ideal regressor for face alignment, as it combines differentiability, high expressivity and fast evaluation runtime. The proposed framework is very fast and achieves high accuracies on multiple challenging benchmarks, including small, medium and large pose experiments.",space
10.1016/j.neucom.2017.01.110,Journal,Neurocomputing,scopus,2018-01-03,sciencedirect,Making physical proofs of concept of reinforcement learning control in single robot hose transport task complete,https://api.elsevier.com/content/abstract/scopus_id/85023642304,"This paper deals with the realization of physical proof of concept experiments in the paradigm of Linked Multi-Component Robotic Systems (LMCRS). The main objective is to demonstrate that the controllers learned through Reinforcement Learning (RL) algorithms with different state space formalizations and different spatial discretizations in a simulator are reliable in a real world configuration of the task of transporting a hose by a single robot. This one is a prototypical example of LMCRS task (extendable to much more complex tasks). We describe how the complete system has been designed and implemented. Two different previously learned RL controllers have been tested solving two different LMCRS control problems, using different state space modeling and discretization step in each case. The physical realizations validate previously published simulation based results, giving a strong argument in favor of the suitability of RL techniques to deal with LMCRS systems.",space
10.1016/j.procs.2018.10.493,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Paraphrasing Arabic Metaphor with Neural Machine Translation,https://api.elsevier.com/content/abstract/scopus_id/85065721331,"The task of recognizing and generating paraphrases is an essential component in many Arabic natural language processing (NLP) applications. A well-established machine translation approach for automatically extracting paraphrases, leverages bilingual corpora to find the equivalent meaning of phrases in a single language, is performed by ""pivoting"" over a shared translation in another language. Neural machine translation has recently become a viable alternative approach to the more widely-used statistical machine translation. In this paper, we revisit bilingual pivoting in the context of neural machine translation and present a paraphrasing model based mainly on neural networks. Our model describes paraphrases in a continuous space and generates candidate paraphrases for an Arabic source input. Experimental results across datasets confirm that neural paraphrases significantly outperform those obtained with statistical machine translation, in particular the Google translator, and indicate high similarity correlation between our model and human translation, making our model attractive for real-world deployment.",space
10.1016/j.procs.2018.05.113,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Real Time High Performance of Sliding Mode Controlled Induction Motor Drives,https://api.elsevier.com/content/abstract/scopus_id/85049099142,"Several industrial applications demand high performance speed functioning and require new control techniques so as to ensure a fast dynamic response. The present work investigates real time implementation and experimental sliding mode controlled (SMC) induction motor drives (IM). The strategy of sliding mode control is a powerful tool to ensure robustness. Nevertheless, the chattering phenomenon is a major disadvantage for non linear systems. For this purpose, two different types of analysis such as layer boundary methods are implemented in dSPACE 1104 controller board and compared between them in order to obtain the best method to reduce or eliminate chattering phenomenon. An experimental results using dSPACE 1104 based on TMS320F240 DSP are described in this work.",space
10.1016/j.procs.2018.05.105,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Geo Map Visualization for Frequent Purchaser in Online Shopping Database Using an Algorithm LP-Growth for Mining Closed Frequent Itemsets,https://api.elsevier.com/content/abstract/scopus_id/85049073263,"Numerous frequent itemsets are explored using frequent itemset mining algorithm which contains redundant information. Fortunately, this issue is decreased to the mining of closed frequent itemsets. However, these approaches still have some performance bottlenecks like processing time and storage space. Moreover, new algorithms of closed frequent itemsets are presented. In this paper, the proposed closed frequent itemset (LP-Closed-tree) using Linear prefix growth method is introduced which is a powerful approach for mining frequent itemsets. It builds basic tree and mines frequent itemsets. The proposed LP-Closed-tree is implemented on real and dense database like online shopping database and chess and is evaluated with other existing algorithms. While using online shopping dataset, the frequent purchaser of the dataset is visualized using google map in geographical method. After comprehensive empirical appraisal it is found that the proposed LP-Closed-tree algorithms are faster in many cases. Moreover, these algorithms are known for relatively lesser consumption of time and memory in cases of large and dense database.",space
10.1016/j.procs.2018.05.168,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Detection of A Shadow of Animated Video Frames in RGB Color Space,https://api.elsevier.com/content/abstract/scopus_id/85049060245,"The study is made on the detection of a shadow of animated video frames in RGB color space. The present study is based on color invariant shadow detection accurately as far as practicable within the limitation of the Matlab software. The grayscale histogram is obtained to extract segments of the shadows. The main aim of the study is to accurately visualize the shadows of the animated objects in consecutive animated video frames. Such type of algorithm can be applied to detect the shadows of real-time images. The advantages of shadow detection are to find out the timing of the day which is caused by Sun, to detect the moving object shadow can be used, just to deceive the enemies from the target object shadows can be used.",space
10.1016/j.artmed.2017.12.004,Journal,Artificial Intelligence in Medicine,scopus,2018-01-01,sciencedirect,Random ensemble learning for EEG classification,https://api.elsevier.com/content/abstract/scopus_id/85039865976,"Real-time detection of seizure activity in epilepsy patients is critical in averting seizure activity and improving patients’ quality of life. Accurate evaluation, presurgical assessment, seizure prevention, and emergency alerts all depend on the rapid detection of seizure onset. A new method of feature selection and classification for rapid and precise seizure detection is discussed wherein informative components of electroencephalogram (EEG)-derived data are extracted and an automatic method is presented using infinite independent component analysis (I-ICA) to select independent features. The feature space is divided into subspaces via random selection and multichannel support vector machines (SVMs) are used to classify these subspaces. The result of each classifier is then combined by majority voting to establish the final output. In addition, a random subspace ensemble using a combination of SVM, multilayer perceptron (MLP) neural network and an extended k-nearest neighbors (k-NN), called extended nearest neighbor (ENN), is developed for the EEG and electrocorticography (ECoG) big data problem. To evaluate the solution, a benchmark ECoG of eight patients with temporal and extratemporal epilepsy was implemented in a distributed computing framework as a multitier cloud-computing architecture. Using leave-one-out cross-validation, the accuracy, sensitivity, specificity, and both false positive and false negative ratios of the proposed method were found to be 0.97, 0.98, 0.96, 0.04, and 0.02, respectively. Application of the solution to cases under investigation with ECoG has also been effected to demonstrate its utility.",space
10.1016/j.enbuild.2017.07.077,Journal,Energy and Buildings,scopus,2017-11-01,sciencedirect,Automatic HVAC control with real-time occupancy recognition and simulation-guided model predictive control in low-cost embedded system,https://api.elsevier.com/content/abstract/scopus_id/85028732660,"Intelligent building automation systems can reduce the energy consumption of heating, ventilation and air-conditioning (HVAC) units by sensing the comfort requirements automatically and scheduling the HVAC operations dynamically. Traditional building automation systems rely on fairly inaccurate occupancy sensors and basic predictive control using oversimplified building thermal response models, all of which prevent such systems from reaching their full potential. Such limitations can now be avoided due to the recent developments in embedded system technologies, which provide viable low-cost computing platforms with powerful processors and sizeable memory storage in a small footprint. As a result, building automation systems can now efficiently execute highly sophisticated computational tasks, such as real-time video processing and accurate thermal-response simulations. With this in mind, we designed and implemented an occupancy-predictive HVAC control system in a low-cost yet powerful embedded system (using Raspberry Pi 3) to demonstrate the following key features for building automation: (1) real-time occupancy recognition using video-processing and machine-learning techniques, (2) dynamic analysis and prediction of occupancy patterns, and (3) model predictive control for HVAC operations guided by real-time building thermal response simulations (using an on-board EnergyPlus simulator). We deployed and evaluated our system for providing automatic HVAC control in the large public indoor space of a mosque, thereby achieving significant energy savings.",space
10.1016/j.actaastro.2017.07.038,Journal,Acta Astronautica,scopus,2017-11-01,sciencedirect,Self-supervised learning as an enabling technology for future space exploration robots: ISS experiments on monocular distance learning,https://api.elsevier.com/content/abstract/scopus_id/85026883328,"Although machine learning holds an enormous promise for autonomous space robots, it is currently not employed because of the inherent uncertain outcome of learning processes. In this article we investigate a learning mechanism, Self-Supervised Learning (SSL), which is very reliable and hence an important candidate for real-world deployment even on safety-critical systems such as space robots. To demonstrate this reliability, we introduce a novel SSL setup that allows a stereo vision equipped robot to cope with the failure of one of its cameras. The setup learns to estimate average depth using a monocular image, by using the stereo vision depths from the past as trusted ground truth. We present preliminary results from an experiment on the International Space Station (ISS) performed with the MIT/NASA SPHERES VERTIGO satellite. The presented experiments were performed on October 8th, 2015 on board the ISS. The main goals were (1) data gathering, and (2) navigation based on stereo vision. First the astronaut Kimiya Yui moved the satellite around the Japanese Experiment Module to gather stereo vision data for learning. Subsequently, the satellite freely explored the space in the module based on its (trusted) stereo vision system and a pre-programmed exploration behavior, while simultaneously performing the self-supervised learning of monocular depth estimation on board. The two main goals were successfully achieved, representing the first online learning robotic experiments in space. These results lay the groundwork for a follow-up experiment in which the satellite will use the learned single-camera depth estimation for autonomous exploration in the ISS, and are an advancement towards future space robots that continuously improve their navigation capabilities over time, even in harsh and completely unknown space environments.",space
10.1016/j.imavis.2016.11.020,Journal,Image and Vision Computing,scopus,2017-09-01,sciencedirect,Strength modelling for real-worldautomatic continuous affect recognition from audiovisual signals,https://api.elsevier.com/content/abstract/scopus_id/85008150025,"Automatic continuous affect recognition from audiovisual cues is arguably one of the most active research areas in machine learning. In addressing this regression problem, the advantages of the models, such as the global-optimisation capability of Support Vector Machine for Regression and the context-sensitive capability of memory-enhanced neural networks, have been frequently explored, but in an isolated way. Motivated to leverage the individual advantages of these techniques, this paper proposes and explores a novel framework, Strength Modelling, where two models are concatenated in a hierarchical framework. In doing this, the strength information of the first model, as represented by its predictions, is joined with the original features, and this expanded feature space is then utilised as the input by the successive model. A major advantage of Strength Modelling, besides its ability to hierarchically explore the strength of different machine learning algorithms, is that it can work together with the conventional feature- and decision-level fusion strategies for multimodal affect recognition. To highlight the effectiveness and robustness of the proposed approach, extensive experiments have been carried out on two time- and value-continuous spontaneous emotion databases (RECOLA and SEMAINE) using audio and video signals. The experimental results indicate that employing Strength Modelling can deliver a significant performance improvement for both arousal and valence in the unimodal and bimodal settings. The results further show that the proposed systems is competitive or outperform the other state-of-the-art approaches, but being with a simple implementation.",space
10.1016/j.compmedimag.2016.11.006,Journal,Computerized Medical Imaging and Graphics,scopus,2017-09-01,sciencedirect,Quantitative PET image reconstruction employing nested expectation-maximization deconvolution for motion compensation,https://api.elsevier.com/content/abstract/scopus_id/85007011515,"Bulk body motion may randomly occur during PET acquisitions introducing blurring, attenuation-emission mismatches and, in dynamic PET, discontinuities in the measured time activity curves between consecutive frames. Meanwhile, dynamic PET scans are longer, thus increasing the probability of bulk motion. In this study, we propose a streamlined 3D PET motion-compensated image reconstruction (3D-MCIR) framework, capable of robustly deconvolving intra-frame motion from a static or dynamic 3D sinogram. The presented 3D-MCIR methods need not partition the data into multiple gates, such as 4D MCIR algorithms, or access list-mode (LM) data, such as LM MCIR methods, both associated with increased computation or memory resources. The proposed algorithms can support compensation for any periodic and non-periodic motion, such as cardio-respiratory or bulk motion, the latter including rolling, twisting or drifting. Inspired from the widely adopted point-spread function (PSF) deconvolution 3D PET reconstruction techniques, here we introduce an image-based 3D generalized motion deconvolution method within the standard 3D maximum-likelihood expectation-maximization (ML-EM) reconstruction framework. In particular, we initially integrate a motion blurring kernel, accounting for every tracked motion within a frame, as an additional MLEM modeling component in the image space (integrated 3D-MCIR). Subsequently, we replaced the integrated model component with a nested iterative Richardson-Lucy (RL) image-based deconvolution method to accelerate the MLEM algorithm convergence rate (RL-3D-MCIR). The final method was evaluated with realistic simulations of whole-body dynamic PET data employing the XCAT phantom and real human bulk motion profiles, the latter estimated from volunteer dynamic MRI scans. In addition, metabolic uptake rate Ki
                      parametric images were generated with the standard Patlak method. Our results demonstrate significant improvement in contrast-to-noise ratio (CNR) and noise-bias performance in both dynamic and parametric images. The proposed nested RL-3D-MCIR method is implemented on the Software for Tomographic Image Reconstruction (STIR) open-source platform and is scheduled for public release.",space
10.1016/j.patrec.2016.10.006,Journal,Pattern Recognition Letters,scopus,2017-07-01,sciencedirect,Redundancy-driven modified Tomek-link based undersampling: A solution to class imbalance,https://api.elsevier.com/content/abstract/scopus_id/85002707897,"Class imbalance can be defined as a span among data mining, machine learning and pattern recognition domains that provides to learn from a data-space having unequal class distribution. Common classifiers when trained by imbalanced data tend to bias towards the class possessing bulk instances causing misclassification of upcoming patterns/instances. The study reveals that presence of redundant borderline instances and outliers in the data-space severely catalyzes the effect of class imbalance. The Condensed Nearest Neighbor and Tomek-link undersampling techniques are used as the baseline systems for the present study, and an improved undersampling algorithm is proposed to be employed in the pre-processing stage by amalgamating aspects of outlier and redundancy detection to the baseline system. The proposed scheme imparts to detect outlier, redundant and noisy instances having least contribution in estimating accurate class labels. Thus, a data-level solution has been offered to the concerned problem with novelty in effective elimination of majority instances without losing valuable information. The proposed scheme is implemented and validated with Back Propagation Neural Network (BPNN), K-Nearest-Neighbor (K-NN), Support Vector Machine (SVM) and Naive Bayes classifiers for 10 real-life datasets. The experimental results obtained clearly manifest the superiority of the proposed scheme over the baseline schemes.",space
10.1016/j.comcom.2016.12.015,Journal,Computer Communications,scopus,2017-06-01,sciencedirect,Imola: A decentralised learning-driven protocol for multi-hop White-Fi,https://api.elsevier.com/content/abstract/scopus_id/85028254628,"In this paper we tackle the digital exclusion problem in developing and remote locations by proposing Imola, an inexpensive learning-driven access mechanism for multi-hop wireless networks that operate across TV white-spaces (TVWS). Stations running Imola only rely on passively acquired neighbourhood information to achieve scheduled-like operation in a decentralised way, without explicit synchronisation. Our design overcomes pathological circumstances such as hidden and exposed terminals that arise due to carrier sensing and are exceptionally problematic in low frequency bands. We present a prototype implementation of our proposal and conduct experiments in a real test bed, which confirms the practical feasibility of deploying our solution in mesh networks that build upon the IEEE 802.11af standard. Finally, the extensive system level simulations we perform demonstrate that Imola achieves up to 4× more throughput than the channel access protocol defined by the standard and reduces frame loss rate by up to 100%.",space
10.1016/j.engappai.2016.08.019,Journal,Engineering Applications of Artificial Intelligence,scopus,2017-06-01,sciencedirect,GPU-based parallel optimization of immune convolutional neural network and embedded system,https://api.elsevier.com/content/abstract/scopus_id/84995489085,"Up to now, the image recognition system has been utilized more and more widely in the security monitoring, the industrial intelligent monitoring, the unmanned vehicle, and even the space exploration. In designing the image recognition system, the traditional convolutional neural network has some defects such as long training time, easy over-fitting and high misclassification rate. In order to overcome these defects, we firstly used the immune mechanism to improve the convolutional neural network and put forward a novel immune convolutional neural network algorithm, after we analyzed the network structure and parameters of the convolutional neural network. Our algorithm not only integrated the location data of the network nodes and the adjustable parameters, but also dynamically adjusted the smoothing factor of the basis function. In addition, we utilized the NVIDIA GPU (Graphics Processing Unit) to accelerate the new immune convolutional neural network (ICNN) in parallel computing and built a real-time embedded image recognition system for this ICNN. The immune convolutional neural network algorithm was improved with CUDA programming and was tested with the sample data in the GPU-based environment. The GPU-based implementation of the novel immune convolutional neural network algorithm was made with the cuDNN, which was designed by NVIDIA for GPU-based accelerating of DNNs in machine learning. Experimental results show that our new immune convolutional neural network has higher recognition rate, more stable performance and faster computing speed than the traditional convolutional neural network.",space
10.1016/j.artmed.2017.06.002,Journal,Artificial Intelligence in Medicine,scopus,2017-05-01,sciencedirect,Automatic detection of surgical haemorrhage using computer vision,https://api.elsevier.com/content/abstract/scopus_id/85020769333,"Background and objectives
                  On occasions, a surgical intervention can be associated with serious, potentially life-threatening complications. One of these complications is a haemorrhage during the operation, an unsolved issue that could delay the intervention or even cause the patient's death. On laparoscopic surgery this complication is even more dangerous, due to the limited vision and mobility imposed by the minimally invasive techniques.
               
                  Methods
                  In this paper it is described a computer vision algorithm designed to analyse the images captured by a laparoscopic camera, classifying the pixels of each frame in blood pixels and background pixels and finally detecting a massive haemorrhage. The pixel classification is carried out by comparing the parameter B/R and G/R of the RGB space colour of each pixel with a threshold obtained using the global average of the whole frame of these parameters. The detection of and starting haemorrhage is achieved by analysing the variation of the previous parameters and the amount of pixel blood classified.
               
                  Results
                  When classifying in vitro images, the proposed algorithm obtains accuracy over 96%, but during the analysis of an in vivo images obtained from real operations, the results worsen slightly due to poor illumination, visual interferences or sudden moves of the camera, obtaining accuracy over 88%. The detection of haemorrhages directly depends of the correct classification of blood pixels, so the analysis achieves an accuracy of 78%.
               
                  Conclusions
                  The proposed algorithm turns out to be a good starting point for an automatic detection of blood and bleeding in the surgical environment which can be applied to enhance the surgeon vision, for example showing the last frame previous to a massive haemorrhage where the incision could be seen using augmented reality capabilities.",space
10.1016/j.cmpb.2017.02.016,Journal,Computer Methods and Programs in Biomedicine,scopus,2017-05-01,sciencedirect,A study of EMR-based medical knowledge network and its applications,https://api.elsevier.com/content/abstract/scopus_id/85014111750,"Background and Objective
                  Electronic medical records (EMRs) contain an amount of medical knowledge which can be used for clinical decision support. We attempt to integrate this medical knowledge into a complex network, and then implement a diagnosis model based on this network.
               
                  Methods
                  The dataset of our study contains 992 records which are uniformly sampled from different departments of the hospital. In order to integrate the knowledge of these records, an EMR-based medical knowledge network (EMKN) is constructed. This network takes medical entities as nodes, and co-occurrence relationships between the two entities as edges. Selected properties of this network are analyzed. To make use of this network, a basic diagnosis model is implemented. Seven hundred records are randomly selected to re-construct the network, and the remaining 292 records are used as test records. The vector space model is applied to illustrate the relationships between diseases and symptoms. Because there may exist more than one actual disease in a record, the recall rate of the first ten results, and the average precision are adopted as evaluation measures.
               
                  Results
                  Compared with a random network of the same size, this network has a similar average length but a much higher clustering coefficient. Additionally, it can be observed that there are direct correlations between the community structure and the real department classes in the hospital. For the diagnosis model, the vector space model using disease as a base obtains the best result. At least one accurate disease can be obtained in 73.27% of the records in the first ten results.
               
                  Conclusion
                  We constructed an EMR-based medical knowledge network by extracting the medical entities. This network has the small-world and scale-free properties. Moreover, the community structure showed that entities in the same department have a tendency to be self-aggregated. Based on this network, a diagnosis model was proposed. This model uses only the symptoms as inputs and is not restricted to a specific disease. The experiments conducted demonstrated that EMKN is a simple and universal technique to integrate different medical knowledge from EMRs, and can be used for clinical decision support.",space
10.1016/j.imavis.2016.08.006,Journal,Image and Vision Computing,scopus,2017-04-01,sciencedirect,Random Multi-Graphs: A semi-supervised learning framework for classification of high dimensional data,https://api.elsevier.com/content/abstract/scopus_id/84994719659,"Currently, high dimensional data processing confronts two main difficulties: inefficient similarity measure and high computational complexity in both time and memory space. Common methods to deal with these two difficulties are based on dimensionality reduction and feature selection. In this paper, we present a different way to solve high dimensional data problems by combining the ideas of Random Forests and Anchor Graph semi-supervised learning. We randomly select a subset of features and use the Anchor Graph method to construct a graph. This process is repeated many times to obtain multiple graphs, a process which can be implemented in parallel to ensure runtime efficiency. Then the multiple graphs vote to determine the labels for the unlabeled data. We argue that the randomness can be viewed as a kind of regularization. We evaluate the proposed method on eight real-world data sets by comparing it with two traditional graph-based methods and one state-of-the-art semi-supervised learning method based on Anchor Graph to show its effectiveness. We also apply the proposed method to the subject of face recognition.",space
10.1016/j.neucom.2016.03.107,Journal,Neurocomputing,scopus,2017-03-29,sciencedirect,Influencing over people with a social emotional model,https://api.elsevier.com/content/abstract/scopus_id/85005873049,"This paper presents an approach of a social emotional model, which allows to extract the social emotion of a group of intelligent entities. The emotional model PAD allows to represent the emotion of an intelligent entity in 3-D space, allowing the representation of different emotional states. The social emotional model presented in this paper uses individual emotions of each one of the entities, which are represented in the emotional space PAD. Using a social emotional model within intelligent entities allows the creation of more real simulations, in which emotional states can influence decision-making. The result of this social emotional mode is represented by a series of examples, which are intended to represent a number of situations in which the emotions of each individual modify the emotion of the group. Moreover, the paper introduces an example which employs the proposed model in order to learn and predict future actions trying to influence in the social emotion of a group of people.",space
10.1016/j.isprsjprs.2017.01.002,Journal,ISPRS Journal of Photogrammetry and Remote Sensing,scopus,2017-03-01,sciencedirect,Appearance learning for 3D pose detection of a satellite at close-range,https://api.elsevier.com/content/abstract/scopus_id/85009110439,"In this paper we present a learning-based 3D detection of a highly challenging specular object exposed to a direct sunlight at very close-range. An object detection is one of the most important areas of image processing, and can also be used for initialization of local visual tracking methods. While the object detection in 3D space is generally a difficult problem, it poses more difficulties when the object is specular and exposed to the direct sunlight as in a space environment. Our solution to a such problem relies on an appearance learning of a real satellite mock-up based on a vector quantization and the vocabulary tree. Our method, implemented on a standard computer (CPU), exploits a full perspective projection model and provides near real-time 3D pose detection of a satellite for close-range approach and manipulation. The time consuming part of the training (feature description, building the vocabulary tree and indexing, depth buffering and back-projection) are performed offline, while a fast image retrieval and 3D-2D registration are performed on-line. In contrast, the state of the art image-based 3D pose detection methods are slower on CPU or assume a weak perspective camera projection model. In our case the dimension of the satellite is larger than the distance to the camera, hence the assumption of the weak perspective model does not hold. To evaluate the proposed method, the appearance of a full scale mock-up of the rear part of the TerraSAR-X satellite is trained under various illumination and camera views. The training images are captured with a camera mounted on six degrees of freedom robot, which enables to position the camera in a desired view, sampled over a sphere. The views that are not within the workspace of the robot are interpolated using image-based rendering. Moreover, we generate ground truth poses to verify the accuracy of the detection algorithm. The achieved results are robust and accurate even under noise due to specular reflection, and able to initialize a local tracking method.",space
10.1016/j.ijar.2016.12.003,Journal,International Journal of Approximate Reasoning,scopus,2017-03-01,sciencedirect,Scalable learning and inference in Markov logic networks,https://api.elsevier.com/content/abstract/scopus_id/85006307364,"Markov logic networks (MLNs) have emerged as a powerful representation that incorporates first-order logic and probabilistic graphical models. They have shown very good results in many problem domains. However, current implementations of MLNs do not scale well due to the large search space and the intractable clause groundings, which is preventing their widespread adoption. In this paper, we propose a general framework named Ground Network Sampling (GNS) for scaling up MLN learning and inference. GNS offers a new instantiation perspective by encoding ground substitutions as simple paths in the Herbrand universe, which uses the interactions existing among the objects to constrain the search space. To further make this search tractable for large scale problems, GNS integrates random walks and subgraph pattern mining, gradually building up a representative subset of simple paths. When inference is concerned, a template network is introduced to quickly locate promising paths that can ground given logical statements. The resulting sampled paths are then transformed into ground clauses, which can be used for clause creation and probabilistic inference. The experiments on several real-world datasets demonstrate that our approach offers better scalability while maintaining comparable or better predictive performance compared to state-of-the-art MLN techniques.",space
10.1016/j.procs.2017.06.036,Conference Proceeding,Procedia Computer Science,scopus,2017-01-01,sciencedirect,Domestic water consumption monitoring and behaviour intervention by employing the internet of things technologies,https://api.elsevier.com/content/abstract/scopus_id/85029365048,"As the water resource is becoming scarce, conservation of water has a high priority around the globe, study on water management and conservation becomes an important research problem. People are increasingly becoming more individual households, which tend to be less efficient, requiring more resources per capita than larger households. In order to address these challenges, this paper presents the achievements of monitoring domestic water consumption at the appliance level and intervening people’s water usage behavior which have been made in ISS-EWATUS (http://www.issewatus.eu), an European Commission funded FP7 project. The water amount consumed by every household appliance is wirelessly recorded with the exact consumption time and stored in a central database. People’s water consumption behavior is likely affected by the real-time water consumption awareness, instant practical advices regarding water-saving activities and classification of water consumption behavior for individuals, all of which are provided by a decision support system deployed as a mobile application in a tablet or any other mobile devices. Only the enhanced water consumption awareness is presented in this paper due to the space limitation. The integrated monitoring and decision support system has been deployed and in use in Sosnowiec in Poland and Skiathos in Greece since March 2015. The domestic water consumption monitoring system at appliance level and the local DSS for affecting people’s water consumption behavior are innovative and have little seen before according to the knowledge of the authors.",space
10.1016/j.procs.2017.03.056,Conference Proceeding,Procedia Computer Science,scopus,2017-01-01,sciencedirect,The Current Status and Future Perspectives of Virtual Maintenance,https://api.elsevier.com/content/abstract/scopus_id/85029176726,"Virtual maintenance, which is widely used in aerospace, automobile, military equipment, etc., has been given abroad attention among equipment life-cycle including concept definition, system design, component production, daily operation, troubleshooting, and so on. Virtual maintenance has been given many different definitions and lot of technologies for implementation, but there is no clear systematic conclusion on the both. Based on the review of the current achievements, the elements of virtual maintenance are extracted, the technologies are systematically explored, and the applications are developed. Meanwhile, the future perspectives of virtual maintenance are discussed associated with virtual reality and augmented reality, multi-person collaboration, remote assistance, as well as artificial intelligence.",space
10.1016/j.energy.2017.07.005,Journal,Energy,scopus,2017-01-01,sciencedirect,Mathematical modeling and evolutionary generation of rule sets for energy-efficient flexible job shops,https://api.elsevier.com/content/abstract/scopus_id/85024852274,"As environmental awareness grows, sustainable scheduling is attracting increasing attention. The purposes of this paper are obtain the lower bound of energy-efficient flexible job shops with machine selection, job sequencing, and machine on-off decision making via a new mathematical model and to discover more energy-efficient rules with easy implementation in real practice via an efficient Gene Expression Programming (eGEP) algorithm. This paper first formulates a novel mixed-integer linear mathematical model to achieve effective machine selection, job sequencing, and machine off-on decision making. Then for the purpose of avoiding the empirical combination, five attributes exerting direct influence on the total energy consumption are extracted and consequently involved in the evolutionary process of eGEP. Furthermore, diversified rule mining operations with multi-gene representation and self-study are designed to enhance the search space and solutions quality. And, unsupervised learning is utilized in which global best and current worst are set to guide evolution direction since the learning progress has no prior knowledge. Experimental results show that machine off-on decisions efficiently reduce the total energy consumption; and, the discovered rules reach the lower bound calculated by GAMS/CPLEX in small problems and have significant superiority over other dispatching rules in energy saving.",space
10.1016/j.neunet.2016.09.001,Journal,Neural Networks,scopus,2017-01-01,sciencedirect,A modular architecture for transparent computation in recurrent neural networks,https://api.elsevier.com/content/abstract/scopus_id/84994319086,"Computation is classically studied in terms of automata, formal languages and algorithms; yet, the relation between neural dynamics and symbolic representations and operations is still unclear in traditional eliminative connectionism. Therefore, we suggest a unique perspective on this central issue, to which we would like to refer as transparent connectionism, by proposing accounts of how symbolic computation can be implemented in neural substrates. In this study we first introduce a new model of dynamics on a symbolic space, the versatile shift, showing that it supports the real-time simulation of a range of automata. We then show that the Gödelization of versatile shifts defines nonlinear dynamical automata, dynamical systems evolving on a vectorial space. Finally, we present a mapping between nonlinear dynamical automata and recurrent artificial neural networks. The mapping defines an architecture characterized by its granular modularity, where data, symbolic operations and their control are not only distinguishable in activation space, but also spatially localizable in the network itself, while maintaining a distributed encoding of symbolic representations. The resulting networks simulate automata in real-time and are programmed directly, in the absence of network training. To discuss the unique characteristics of the architecture and their consequences, we present two examples: (i) the design of a Central Pattern Generator from a finite-state locomotive controller, and (ii) the creation of a network simulating a system of interactive automata that supports the parsing of garden-path sentences as investigated in psycholinguistics experiments.",space
10.1016/j.compbiomed.2016.10.006,Journal,Computers in Biology and Medicine,scopus,2016-12-01,sciencedirect,CAFÉ-Map: Context Aware Feature Mapping for mining high dimensional biomedical data,https://api.elsevier.com/content/abstract/scopus_id/84995610155,"Feature selection and ranking is of great importance in the analysis of biomedical data. In addition to reducing the number of features used in classification or other machine learning tasks, it allows us to extract meaningful biological and medical information from a machine learning model. Most existing approaches in this domain do not directly model the fact that the relative importance of features can be different in different regions of the feature space. In this work, we present a context aware feature ranking algorithm called CAFÉ-Map. CAFÉ-Map is a locally linear feature ranking framework that allows recognition of important features in any given region of the feature space or for any individual example. This allows for simultaneous classification and feature ranking in an interpretable manner. We have benchmarked CAFÉ-Map on a number of toy and real world biomedical data sets. Our comparative study with a number of published methods shows that CAFÉ-Map achieves better accuracies on these data sets. The top ranking features obtained through CAFÉ-Map in a gene profiling study correlate very well with the importance of different genes reported in the literature. Furthermore, CAFÉ-Map provides a more in-depth analysis of feature ranking at the level of individual examples.
                  
                     Availability: CAFÉ-Map Python code is available at:
                  
                     http://faculty.pieas.edu.pk/fayyaz/software.html#cafemap .
                  The CAFÉ-Map package supports parallelization and sparse data and provides example scripts for classification. This code can be used to reconstruct the results given in this paper.",space
10.1016/j.trc.2016.10.019,Journal,Transportation Research Part C: Emerging Technologies,scopus,2016-12-01,sciencedirect,Short-term speed predictions exploiting big data on large urban road networks,https://api.elsevier.com/content/abstract/scopus_id/84994322993,"Big data from floating cars supply a frequent, ubiquitous sampling of traffic conditions on the road network and provide great opportunities for enhanced short-term traffic predictions based on real-time information on the whole network. Two network-based machine learning models, a Bayesian network and a neural network, are formulated with a double star framework that reflects time and space correlation among traffic variables and because of its modular structure is suitable for an automatic implementation on large road networks. Among different mono-dimensional time-series models, a seasonal autoregressive moving average model (SARMA) is selected for comparison. The time-series model is also used in a hybrid modeling framework to provide the Bayesian network with an a priori estimation of the predicted speed, which is then corrected exploiting the information collected on other links. A large floating car data set on a sub-area of the road network of Rome is used for validation. To account for the variable accuracy of the speed estimated from floating car data, a new error indicator is introduced that relates accuracy of prediction to accuracy of measure. Validation results highlighted that the spatial architecture of the Bayesian network is advantageous in standard conditions, where a priori knowledge is more significant, while mono-dimensional time series revealed to be more valuable in the few cases of non-recurrent congestion conditions observed in the data set. The results obtained suggested introducing a supervisor framework that selects the most suitable prediction depending on the detected traffic regimes.",space
10.1016/j.ins.2016.08.050,Journal,Information Sciences,scopus,2016-12-01,sciencedirect,Pedestrian detection by learning a mixture mask model and its implementation,https://api.elsevier.com/content/abstract/scopus_id/84989928835,"Pedestrian detection from videos is a useful technique in intelligent transportation systems. Some key challenges of accurate pedestrian detection are the large variations in pedestrian appearance as the pedestrians assume different poses and the different camera views that are involved. This makes the generic visual descriptors unreliable for real-world pedestrian detection. In this paper, we propose a high-level human-specific descriptor for detecting pedestrians in multiple videos. More specifically, by obtaining the feature matrix from a sliding window, we use multiple mapping vectors to project the original feature matrix into different mask spaces. Inspired by the part-based model [12], it is natural to formulate the pedestrian detection into a multiple-instance learning (MIL) framework. Afterward, we adopt an MI-SVM [9] to solve it. To evaluate the proposed detection algorithm, we implement the pedestrian detection algorithm in FPGA, which can process over 30 fps. Moreover, our method outperforms many existing object detection algorithms in terms of accuracy.",space
10.1016/j.cie.2016.07.031,Journal,Computers and Industrial Engineering,scopus,2016-11-01,sciencedirect,TIMSPAT – Reachability graph search-based optimization tool for colored Petri net-based scheduling,https://api.elsevier.com/content/abstract/scopus_id/84991205081,"The combination of Petri net (PN) modeling with AI-based heuristic search (HS) algorithms (PNHS) has been successfully applied as an integrated approach to deal with scheduling problems that can be transformed into a search problem in the reachability graph. While several efficient HS algorithms have been proposed albeit using timed PN, the practical application of these algorithms requires an appropriate tool to facilitate its development and analysis. However, there is a lack of tool support for the optimization of timed colored PN (TCPN) models based on the PNHS approach for schedule generation. Because of its complex data structure, TCPN-based scheduling has often been limited to simulation-based performance analysis only. Also, it is quite difficult to evaluate the strength and tractability of algorithms for different scheduling scenarios due to the different computing platforms, programming languages and data structures employed. In this light, this paper presents a new tool called TIMSPAT, developed to overcome the shortcomings of existing tools. Some features that distinguish this tool are the collection of several HS algorithms, XML-based model integration, the event-driven exploration of the timed state space including its condensed variant, localized enabling of transitions, the introduction of static place, and the easy-to-use syntax statements. The tool is easily extensible and can be integrated as a component into existing PN simulators and software environments. A comparative study is performed on a real-world eyeglass production system to demonstrate the application of the tool for scheduling purposes.",space
10.1016/j.forsciint.2016.09.010,Journal,Forensic Science International,scopus,2016-11-01,sciencedirect,Pornography classification: The hidden clues in video space–time,https://api.elsevier.com/content/abstract/scopus_id/84989311576,"As web technologies and social networks become part of the general public's life, the problem of automatically detecting pornography is into every parent's mind — nobody feels completely safe when their children go online. In this paper, we focus on video-pornography classification, a hard problem in which traditional methods often employ still-image techniques — labeling frames individually prior to a global decision. Frame-based approaches, however, ignore significant cogent information brought by motion. Here, we introduce a space-temporal interest point detector and descriptor called Temporal Robust Features (TRoF). TRoF was custom-tailored for efficient (low processing time and memory footprint) and effective (high classification accuracy and low false negative rate) motion description, particularly suited to the task at hand. We aggregate local information extracted by TRoF into a mid-level representation using Fisher Vectors, the state-of-the-art model of Bags of Visual Words (BoVW). We evaluate our original strategy, contrasting it both to commercial pornography detection solutions, and to BoVW solutions based upon other space-temporal features from the scientific literature. The performance is assessed using the Pornography-2k dataset, a new challenging pornographic benchmark, comprising 2000 web videos and 140h of video footage. The dataset is also a contribution of this work and is very assorted, including both professional and amateur content, and it depicts several genres of pornography, from cartoon to live action, with diverse behavior and ethnicity. The best approach, based on a dense application of TRoF, yields a classification error reduction of almost 79% when compared to the best commercial classifier. A sparse description relying on TRoF detector is also noteworthy, for yielding a classification error reduction of over 69%, with 19× less memory footprint than the dense solution, and yet can also be implemented to meet real-time requirements.",space
10.1016/j.scico.2016.03.009,Journal,Science of Computer Programming,scopus,2016-11-01,sciencedirect,Model-driven processes and tools to design robot-based generative learning objects for computer science education,https://api.elsevier.com/content/abstract/scopus_id/84971228847,"In this paper, we introduce a methodology to design robot-oriented generative learning objects (GLOs) that are, in fact, heterogeneous meta-programs to teach computer science (CS) topics such as programming. The methodology includes CS learning variability modelling using the feature-based approaches borrowed from the SW engineering domain. Firstly, we define the CS learning domain using the known educational framework TPACK (Technology, Pedagogy And Content Knowledge). By learning variability we mean the attributes of the framework extracted and represented as feature models with multiple values. Therefore, the CS learning variability represents the problem domain. Meta-programming is considered as a solution domain. Both are represented by feature models. The GLO design task is formulated as mapping the problem domain model on the solution domain model. Next, we present the design framework to design GLOs manually or semi-automatically. The multi-level separation of concepts, model representation and transformation forms the conceptual background. Its theoretical background includes: (a) a formal definition of feature-based models; (b) a graph-based and set-based definition of meta-programming concepts; (c) transformation rules to support the model mapping; (d) a computational Abstract State Machine model to define the processes and design tool for developing GLOs. We present the architecture and some characteristics of the tool. The tool enables to improve the GLO design process significantly (in terms of time and quality) and to achieve a higher quality and functionality of GLOs themselves (in terms of the parameter space enlargement for reuse and adaptation). We demonstrate the appropriateness of the methodology in the real teaching setting. In this paper, we present the case study that analyses three robot-oriented GLOs as the higher-level specifications. Then, using the meta-language processor, we are able to produce, from the specifications, the concrete robot control programs on demand automatically and to demonstrate teaching algorithms visually by robot's actions. We evaluate the approach from technological and pedagogical perspectives using the known structural metrics. Also, we indicate the merits and demerits of the approach. The main contribution and originality of the paper is the seamless integration of two known technologies (feature modelling and meta-programming) in designing robot-oriented GLOs and their supporting tools.",space
10.1016/j.eswa.2016.04.026,Journal,Expert Systems with Applications,scopus,2016-10-30,sciencedirect,Using dynamical systems tools to detect concept drift in data streams,https://api.elsevier.com/content/abstract/scopus_id/84965132319,"Real-world data streams may change their behaviors along time, what is referred to as concept drift. By detecting those changes, researchers obtain relevant information about the phenomena that produced such streams (e.g. temperatures in a region, bacteria population, disease occurrence, etc.). Many concept drift detection algorithms consider supervised or semi-supervised approaches which tend to be unfeasible when data is collected at high frequencies, due to the difficulties involved in labeling. Complementarily, current studies usually assume data as statistically independent and identically distributed, disregarding any temporal relationship among observations and, consequently, risking the quality of data modeling. In order to tackle both aspects, we employ dynamical system modeling to represent the temporal relationships among data observations and how they modify along time in attempt to detect concept drift. This approach considers Taken’s immersion theorem to unfold consecutive windows of data observations into the phase space in attempt to represent and compare time dependencies. From this perspective, we proposed four new concept drift detection algorithms based on the unsupervised machine learning paradigm. The first algorithm builds dendrograms of consecutive phase spaces (every phase space represents the time relationships for the observations contained in a particular data window) and compare them out by using the Gromov–Hausdorff distance, providing enough guarantees to detect concept drifts. The second algorithm employs the Cross Recurrence Plot and the Recurrence Quantification Analysis to detect relevant changes in consecutive phase spaces and warn about relevant data modifications. We also preprocess data windows by considering the Empirical Mode Decomposition method and Mutual Information in attempt to take only the deterministic stream behavior into account. All algorithms were implemented as plugins for the Massive Online Analysis (MOA) software and then compared to well-known algorithms from literature. Results confirm the proposed algorithms were capable of detecting most of the behavior changes, creating few false alarms.",space
10.1016/j.ijleo.2016.06.126,Journal,Optik,scopus,2016-10-01,sciencedirect,Development of a calibrating algorithm for Delta Robot's visual positioning based on artificial neural network,https://api.elsevier.com/content/abstract/scopus_id/84978140749,"Delta robot with vision system can automatically control the end-actuator to accurately grasp moving objects on the conveyor belt. Establishment of the mapping relationship between the image feature space and the robot working space form a closed-loop chain for transformational link between the robot coordinate, camera coordinate and conveyor belt coordinate. The vision system calibration is a basic problem of robot vision research and implementation. The artificial neural networks (ANN) which has learning ability, adaptive ability and nonlinear function approximation ability can establish the nonlinear relationship between space points and pixel points to complete accurate calibration of the vision system. The convergence speed of calibration algorithm affects the real-time visual servo system. The calibration precision, generalization ability and calibration space of algorithm influence the robot grasping accuracy. Therefore, a new calibration technique for delta robot’s vision system was presented in this paper. The algorithm combines ANN with Faugeras vision system calibration technology. The setting of the initial value, network structure and the choice of the activation function is based on the model of Faugeras vision system calibration algorithm, which makes the actual output of the network closer to the target output. Experiments proved that this algorithm has higher calibration accuracy and generalization ability compared with the conventional calibration algorithm, as well as faster convergence speed compared with the conventional artificial neural network structure in the case of high calibration accuracy.",space
10.1016/j.adhoc.2016.06.011,Journal,Ad Hoc Networks,scopus,2016-10-01,sciencedirect,Feature selection for performance characterization in multi-hop wireless sensor networks,https://api.elsevier.com/content/abstract/scopus_id/84977657790,"Current trends in Wireless Sensor Networks are faced with the challenge of shifting from testbeds in controlled environments to real-life deployments, characterized by unattended and long-term operation. The network performance in such settings depends on various factors, ranging from the operational space, the behavior of the protocol stack, the intra-network dynamics, and the status of each individual node. As such, characterizing the network’s high-level performance based exclusively on link-quality estimation, can yield episodic snapshots on the performance of specific, point-to-point links. The objective of this work is to provide an integrated framework for the unsupervised selection of the dominant features that have crucial impact on the performance of end-to-end links, established over a multi-hop topology. Our focus is on compressing the original feature vector of network parameters, by eliminating redundant network attributes with predictable behavior. The proposed approach is implemented alongside different cases of protocol stacks and evaluated on data collected from real-life deployments in rural and industrial environments. Discussions on the efficacy of the proposed scheme, and the dominant network characteristics per deployment are offered.",space
10.1016/j.jnca.2016.06.010,Journal,Journal of Network and Computer Applications,scopus,2016-09-01,sciencedirect,Traffic and mobility aware resource prediction using cognitive agent in mobile ad hoc networks,https://api.elsevier.com/content/abstract/scopus_id/84989815364,"Mobile Ad hoc NETwork (MANET) characteristics such as limited resources, shared channel, unpredictable mobility, improper load balancing, and variation in signal strength affect the routing of real-time multimedia data that requires Quality of Service (QoS) provisioning. Accurate prediction of the resource availability assists efficient resource allocation before the routing of such data. Most of the published work on resource prediction in MANET focuses on either bandwidth or energy without considering mobility effects. Adoption of intelligent software agent such as Cognitive Agent (CA) for the accurate resource prediction has a significant potential to solve the challenges of resource prediction in MANET. The intelligence provided in CA is similar to the logical thinking like a human for decision-making. The predominant CA architecture is the Belief-Desire-Intention (BDI) model, which performs the various tasks on behalf of the human user as an assistant.
                  In this paper, we propose a CA-based Resource Prediction mechanism considering Mobility (CA-RPM) that predicts the resources using agents through the resource prediction agency consisting of one static agent, one cognitive agent and two mobile agents. Agents predict the traffic, mobility, buffer space, energy, and bandwidth effectively that is necessary for efficient resource allocation to support real-time and multimedia communications. The mobile agents collect and distribute network traffic statistics over MANET whereas a static agent collects the local statistics. CA creates static/mobile agent during the process of resource prediction. Initially, the designed time-series Wavelet Neural Networks (WNNs) predict traffic and mobility. Buffer space, energy, and bandwidth prediction use the predicted mobility and traffic. Simulation results show that the predicted resources closely match with the real values at the cost of little overheads due to the usage of agents. Simulation analysis of predicted traffic and mobility also shows the improvement compared to recurrent WNN in terms of mean square error, covariance, memory overhead, agent overhead and computation overhead. We plan to use these predicted resources for its efficient utilization in QoS routing is our future work.",space
10.1016/j.ijpe.2016.06.005,Journal,International Journal of Production Economics,scopus,2016-09-01,sciencedirect,Hybrid flow shop batching and scheduling with a bi-criteria objective,https://api.elsevier.com/content/abstract/scopus_id/84975859979,"This paper addresses the hybrid flow shop batching and scheduling problem where sequence-dependent family setup times are present and the objective is to simultaneously minimize the weighted sum of the total weighted completion time and total weighted tardiness. In particular, it disregards the group technology assumptions by allowing for the possibility of splitting pre-determined groups of jobs into inconsistent batches in order to improve the operational efficiency. A benchmark of small size problems is considered to show the benefits of batching on group scheduling. Since the problem is strongly NP-hard, several algorithms based upon tabu search are developed at three levels, which move back and forth between batching and scheduling phases. Two algorithms incorporate tabu search into the framework of path-relinking to exploit the information on good solutions. These tabu search/path-relinking algorithms comprise several distinguishing features including two relinking procedures to effectively construct paths and the stage-based improvement procedure to consider the move interdependency. The best tabu search algorithm as a local search algorithm is compared to a population-based algorithm, and the superiority of the former over the latter is shown using a statistical experiment. The initial solution finding mechanism is implemented to trigger the search into the solution space. The efficiency and effectiveness of the best algorithm is verified with the help of the results found by CPLEX. The results show that the best algorithm, based on tabu search/path relinking and the stage-based improvement procedure, could find solutions at least as good as CPLEX, but in drastically shorter computational time. In order to reflect the real industry requirements, dynamic machine availability times, dynamic job release times, machine eligibility and machine capability for processing jobs, desired lower bounds on batch sizes, and job skipping are considered.",space
10.1016/j.eswa.2016.03.012,Journal,Expert Systems with Applications,scopus,2016-09-01,sciencedirect,Optimization of neural networks through grammatical evolution and a genetic algorithm,https://api.elsevier.com/content/abstract/scopus_id/84962738944,"This paper proposes a hybrid neuro-evolutive algorithm (NEA) that uses a compact indirect encoding scheme (IES) for representing its genotypes (a set of ten production rules of a Lindenmayer System with memory), moreover has the ability to reuse the genotypes and automatically build modular, hierarchical and recurrent neural networks. A genetic algorithm (GA) evolves a Lindenmayer System (L-System) that is used to design the neural network’s architecture. This basic neural codification confers scalability and search space reduction in relation to other methods. Furthermore, the system uses a parallel genome scan engine that increases both the implicit parallelism and convergence of the GA. The fitness function of the NEA rewards economical artificial neural networks (ANNs) that are easily implemented. The NEA was tested on five real-world classification datasets and three well-known datasets for time series forecasting (TSF). The results are statistically compared against established state-of-the-art algorithms and various forecasting methods (ADANN, ARIMA, UCM, and Forecast Pro). In most cases, our NEA outperformed the other methods, delivering the most accurate classification and time series forecasting with the least computational effort. These superior results are attributed to the improved effectiveness and efficiency of NEA in the decision-making process. The result is an optimized neural network architecture for solving classification problems and simulating dynamical systems.",space
10.1016/j.bica.2016.07.001,Journal,Biologically Inspired Cognitive Architectures,scopus,2016-07-01,sciencedirect,A dual-process Qualia Modeling Framework (QMF),https://api.elsevier.com/content/abstract/scopus_id/84979681532,"We present the software implementation of our Qualia Modeling Framework (QMF), a computational cognitive model based on the dual-process theory, which theorizes that reasoning and decision making rely on integrated experiences from two interactive minds: the autonomous mind, without the agent’s conscious awareness, and the reflective mind, of which the agent is consciously aware. In the QMF, artificial qualia are the vocabulary of the conscious mind, required to reason over conceptual memory, and generate cognitive inferences. The autonomous mind employs pattern-matching, for fast reasoning over episodic memories. An ACT-R model with conventional declarative memory represents the autonomous mind. A second ACT-R model, with an unconventional implementation of declarative memory utilizing a hypernetwork theory based model of qualia space, represents the reflective mind. Using real-world, non-trivial, data sets, our cognitive model achieved classification accuracy comparable to, or greater than, analogous machine learning classifiers kNN and DT, while providing improvements in flexibility by allowing the Target Attribute to be identified or changed any time during training and testing. We advance the BICA challenge by providing a generalizable, efficient, algorithm which models the phenomenal structure of consciousness as proposed by a contemporary theory, and provides an effective decision aid in complex environments where data are too broad or diverse for a human to evaluate without computational assistance.",space
10.1016/j.biosystems.2016.05.007,Journal,BioSystems,scopus,2016-07-01,sciencedirect,Robotic action acquisition with cognitive biases in coarse-grained state space,https://api.elsevier.com/content/abstract/scopus_id/84973440991,"Some of the authors have previously proposed a cognitively inspired reinforcement learning architecture (LS-Q) that mimics cognitive biases in humans. LS-Q adaptively learns under uniform, coarse-grained state division and performs well without parameter tuning in a giant-swing robot task. However, these results were shown only in simulations. In this study, we test the validity of the LS-Q implemented in a robot in a real environment. In addition, we analyze the learning process to elucidate the mechanism by which the LS-Q adaptively learns under the partially observable environment. We argue that the LS-Q may be a versatile reinforcement learning architecture, which is, despite its simplicity, easily applicable and does not require well-prepared settings.",space
10.1016/j.cmpb.2016.04.005,Journal,Computer Methods and Programs in Biomedicine,scopus,2016-07-01,sciencedirect,A MapReduce approach to diminish imbalance parameters for big deoxyribonucleic acid dataset,https://api.elsevier.com/content/abstract/scopus_id/84964534094,"Background
                  In the age of information superhighway, big data play a significant role in information processing, extractions, retrieving and management. In computational biology, the continuous challenge is to manage the biological data. Data mining techniques are sometimes imperfect for new space and time requirements. Thus, it is critical to process massive amounts of data to retrieve knowledge. The existing software and automated tools to handle big data sets are not sufficient. As a result, an expandable mining technique that enfolds the large storage and processing capability of distributed or parallel processing platforms is essential.
               
                  Method
                  In this analysis, a contemporary distributed clustering methodology for imbalance data reduction using k-nearest neighbor (K-NN) classification approach has been introduced. The pivotal objective of this work is to illustrate real training data sets with reduced amount of elements or instances. These reduced amounts of data sets will ensure faster data classification and standard storage management with less sensitivity. However, general data reduction methods cannot manage very big data sets. To minimize these difficulties, a MapReduce-oriented framework is designed using various clusters of automated contents, comprising multiple algorithmic approaches.
               
                  Results
                  To test the proposed approach, a real DNA (deoxyribonucleic acid) dataset that consists of 90 million pairs has been used. The proposed model reduces the imbalance data sets from large-scale data sets without loss of its accuracy.
               
                  Conclusions
                  The obtained results depict that MapReduce based K-NN classifier provided accurate results for big data of DNA.",space
10.1016/j.ins.2016.01.097,Journal,Information Sciences,scopus,2016-06-20,sciencedirect,Generalized quasi-metric on strings,https://api.elsevier.com/content/abstract/scopus_id/84959352366,"In this paper, we propose a generalized quasi-metric in spaces of strings, which is based on edit operations (insertion and deletions) and taking values as pairs of non-negative integers. We show that with such a generalization is possible to carry more information about similarity between strings than in the usual case where the distance between the strings is a simple real number. An algorithm for the calculation of this quasi-metric is presented and as well as an illustrative example of the application of this quasi-metric in handwritten digit classification. We also show some relations of this quasi-metric with the concept of subsequence of a string.",space
10.1016/j.neucom.2016.02.014,Journal,Neurocomputing,scopus,2016-06-12,sciencedirect,Model reference fractional order control using type-2 fuzzy neural networks structure: Implementation on a 2-DOF helicopter,https://api.elsevier.com/content/abstract/scopus_id/84959905863,"In this paper, an adaptive learning algorithm is proposed for an interval type-2 fuzzy fractional order controller. The use of fractional order controller adds more degrees of freedom which makes it possible to obtain superior performance in comparison with ordinary differential controllers. A fractional order reference model is used to define the desired trajectory of the nonlinear dynamic system. The structure of the system is based on the feedback error learning method. The stability of the adaptation laws is proved using Lyapunov theory. In order to test the efficiency and efficacy of the proposed learning and the control algorithm, the trajectory tracking problem of a magnetic rigid spacecraft is studied. The simulation results show that the proposed control algorithm outperforms the case when ordinary differential fuzzy controller is used. Furthermore, it is shown that it is possible to define a master chaotic system as a reference model and obtain synchronization between the two chaotic systems using the proposed approach. In the simulation part the synchronization between two Duffing–Holmes system is also achieved. In order to show the implementability of the proposed method, it is used to control a real time laboratory setup 2-DOF helicopter. It is shown that the proposed fractional order controller can be implemented in a low cost embedded system and can successfully control a highly nonlinear dynamic system.",space
10.1016/j.future.2015.10.013,Journal,Future Generation Computer Systems,scopus,2016-06-01,sciencedirect,Real-time data mining of massive data streams from synoptic sky surveys,https://api.elsevier.com/content/abstract/scopus_id/84961285522,"The nature of scientific and technological data collection is evolving rapidly: data volumes and rates grow exponentially, with increasing complexity and information content, and there has been a transition from static data sets to data streams that must be analyzed in real time. Interesting or anomalous phenomena must be quickly characterized and followed up with additional measurements via optimal deployment of limited assets. Modern astronomy presents a variety of such phenomena in the form of transient events in digital synoptic sky surveys, including cosmic explosions (supernovae, gamma ray bursts), relativistic phenomena (black hole formation, jets), potentially hazardous asteroids, etc. We have been developing a set of machine learning tools to detect, classify and plan a response to transient events for astronomy applications, using the Catalina Real-time Transient Survey (CRTS) as a scientific and methodological testbed. The ability to respond rapidly to the potentially most interesting events is a key bottleneck that limits the scientific returns from the current and anticipated synoptic sky surveys. Similar challenge arises in other contexts, from environmental monitoring using sensor networks to autonomous spacecraft systems. Given the exponential growth of data rates, and the time-critical response, we need a fully automated and robust approach. We describe the results obtained to date, and the possible future developments.",space
10.1016/j.ijepes.2015.11.077,Journal,International Journal of Electrical Power and Energy Systems,scopus,2016-06-01,sciencedirect,Power distribution network reconfiguration for power loss minimization using novel dynamic fuzzy c-means (dFCM) clustering based ANN approach,https://api.elsevier.com/content/abstract/scopus_id/84949995877,"In this study, a three-layer artificial neural network (ANN) is proposed to reconfigure power distribution networks to obtain the optimal configuration in which the active power loss is minimal. Then, the proposed ANN is reduced in size by transforming the input space with kernels using a proposed modified dynamic fuzzy c-means (dFCM) clustering algorithm to obtain a novel framework. The proposed framework and ANN both are implemented on the two IEEE 33-bus and IEEE 69-bus power distribution networks. The ANN and framework both are trained using the training set consisting of only 64 training samples. The simulated results are compared to the results obtained by performing a selected traditional method which is the switching algorithm. The comparative results explicitly verify that using the proposed framework for distribution networks reconfiguration has some benefits such as a very short process time that is far shorter than the others, a very simple structure including only a minimal number of neurons and higher accuracy compared to the others. These features show that the proposed framework can be effectively used for real-time reconfiguration of power distribution networks.",space
10.1016/j.tics.2015.11.007,Journal,Trends in Cognitive Sciences,scopus,2016-03-01,sciencedirect,Conceptual Alignment: How Brains Achieve Mutual Understanding,https://api.elsevier.com/content/abstract/scopus_id/84958161462,"We share our thoughts with other minds, but we do not understand how. Having a common language certainly helps, but infants’ and tourists’ communicative success clearly illustrates that sharing thoughts does not require signals with a pre-assigned meaning. In fact, human communicators jointly build a fleeting conceptual space in which signals are a means to seek and provide evidence for mutual understanding. Recent work has started to capture the neural mechanisms supporting those fleeting conceptual alignments. The evidence suggests that communicators and addressees achieve mutual understanding by using the same computational procedures, implemented in the same neuronal substrate, and operating over temporal scales independent from the signals’ occurrences.",space
10.1016/j.neucom.2015.12.013,Journal,Neurocomputing,scopus,2016-02-29,sciencedirect,A distributed spatial-temporal weighted model on MapReduce for short-term traffic flow forecasting,https://api.elsevier.com/content/abstract/scopus_id/84955646724,"Accurate and timely traffic flow prediction is crucial to proactive traffic management and control in data-driven intelligent transportation systems (D2ITS), which has attracted great research interest in the last few years. In this paper, we propose a Spatial–Temporal Weighted K-Nearest Neighbor model, named STW-KNN, in a general MapReduce framework of distributed modeling on a Hadoop platform, to enhance the accuracy and efficiency of short-term traffic flow forecasting. More specifically, STW-KNN considers the spatial–temporal correlation and weight of traffic flow with trend adjustment features, to optimize the search mechanisms containing state vector, proximity measure, prediction function, and 
                        K
                      selection. Furthermore, STW-KNN is implemented on a widely adopted Hadoop distributed computing platform with the MapReduce parallel processing paradigm, for parallel prediction of traffic flow in real time. Finally, with extensive experiments on real-world big taxi trajectory data, STW-KNN is compared with the state-of-the-art prediction models including conventional K-Nearest Neighbor (KNN), Artificial Neural Networks (ANNs), Naïve Bayes (NB), Random Forest (RF), and C4.5. The results demonstrate that the proposed model is superior to existing models on accuracy by decreasing the mean absolute percentage error (MAPE) value more than 11.59% only in time domain and even achieves 89.71% accuracy improvement with the MAPEs of between 3.34% and 6.00% in both space and time domains, and also significantly improves the efficiency and scalability of short-term traffic flow forecasting over existing approaches.",space
10.1016/j.ins.2015.09.051,Journal,Information Sciences,scopus,2016-02-01,sciencedirect,Across neighborhood search for numerical optimization,https://api.elsevier.com/content/abstract/scopus_id/84949671034,"Population-based search algorithms (PBSAs), including swarm intelligence algorithms (SIAs) and evolutionary algorithms (EAs), are competitive alternatives for solving complex optimization problems and they have been widely applied to real-world optimization problems in different fields. In this study, a novel population-based across neighborhood search (ANS) is proposed for numerical optimization. ANS is motivated by two straightforward assumptions and three important issues raised in improving and designing efficient PBSAs. In ANS, a group of individuals collaboratively search the solution space for an optimal solution of the optimization problem considered. A collection of superior solutions found by individuals so far is maintained and updated dynamically. At each generation, an individual directly searches across the neighborhoods of multiple superior solutions with the guidance of a Gaussian distribution. This search manner is referred to as across neighborhood search. The characteristics of ANS are discussed and the concept comparisons with other PBSAs are given. The principle behind ANS is simple. Moreover, ANS is easy for implementation and application with three parameters being required to tune. Extensive experiments on 18 benchmark optimization functions of different types show that ANS has well balanced exploration and exploitation capabilities and performs competitively compared with many efficient PBSAs (Related Matlab codes used in the experiments are available from http://guohuawunudt.gotoip2.com/publications.html).",space
10.1016/j.ins.2015.08.055,Journal,Information Sciences,scopus,2016-02-01,sciencedirect,Using tensor products to detect unconditional label dependence in multilabel classifications,https://api.elsevier.com/content/abstract/scopus_id/84949658865,"Multilabel (ML) classification tasks consist of assigning a set of labels to each input. It is well known that detecting label dependencies is crucial in order to improve the performance in ML problems. In this paper, we study a new kernel approach to take into account unconditional label dependence between labels. The aim is to improve the performance measured by a micro-averaged loss function. The core idea is to transform a ML task into a binary classification problem whose inputs are drawn from a tensor space of the original input space and a representation of the labels. In this joint feature space we define a kernel to explicitly involve both labels and object descriptions. In addition to the theoretical contributions, the experimental results of this study provide an interesting conclusion: the performance in terms of Hamming Loss can be improved when unconditional label dependence is considered, as our method does. We report a thoroughly experimentation carried out with real world domains and several synthetic datasets devised to analyze the effect of exploiting label dependence in scenarios with different degrees of dependency.",space
10.1016/j.neucom.2015.02.096,Journal,Neurocomputing,scopus,2016-01-22,sciencedirect,Building feature space of extreme learning machine with sparse denoising stacked-autoencoder,https://api.elsevier.com/content/abstract/scopus_id/84940061662,"The random-hidden-node extreme learning machine (ELM) is a much more generalized cluster of single-hidden-layer feed-forward neural networks (SLFNs) which has three parts: random projection, non-linear transformation, and ridge regression (RR) model. Networks with deep architectures have demonstrated state-of-the-art performance in a variety of settings, especially with computer vision tasks. Deep learning algorithms such as stacked autoencoder (SAE) and deep belief network (DBN) are built on learning several levels of representation of the input. Beyond simply learning features by stacking autoencoders (AE), there is a need for increasing its robustness to noise and reinforcing the sparsity of weights to make it easier to discover interesting and prominent features. The sparse AE and denoising AE was hence developed for this purpose. This paper proposes an approach: SSDAE-RR (stacked sparse denoising autoencoder – ridge regression) that effectively integrates the advantages in SAE, sparse AE, denoising AE, and the RR implementation in ELM algorithm. We conducted experimental study on real-world classification (binary and multiclass) and regression problems with different scales among several relevant approaches: SSDAE-RR, ELM, DBN, neural network (NN), and SAE. The performance analysis shows that the SSDAE-RR tends to achieve a better generalization ability on relatively large datasets (large sample size and high dimension) that were not pre-processed for feature abstraction. For 16 out of 18 tested datasets, the performance of SSDAE-RR is more stable than other tested approaches. We also note that the sparsity regularization and denoising mechanism seem to be mandatory for constructing interpretable feature representations. The fact that a SSDAE-RR approach often has a comparable training time to ELM makes it useful in some real applications.",space
10.1016/j.procs.2016.08.136,Conference Proceeding,Procedia Computer Science,scopus,2016-01-01,sciencedirect,Formal Description and Automatic Generation of Learning Spaces Based on Ontologies,https://api.elsevier.com/content/abstract/scopus_id/84988908359,"A good virtual Learning Space (LS) should convey pertinent learning information to the visitors at the most adequate time and locations to favor their knowledge acquisition.
                  Considering the consolidation of the internet and the improvement of the interaction, searching, and learning mechanisms, we propose a generic architecture, called CaVa, to create virtual Learning Spaces building up on cultural institution documents. More precisely, our proposal is to automatically create ontology-based virtual learning environments.
                  Thus, to impart relevant learning materials to the virtual LS, we propose the use of ontologies to represent the key concepts and semantic relations in an user- and machine-understandable format. These concepts together with the data (extracted from the real documents) stored in a digital storage format (XML datasets, relational databases, etc.) are displayed in an ontology-based learning space that enables the visitors to use the available features and tools to learn about a specific domain.
                  According to the approach here discussed, each desired virtual LS must be specified rigorously through a domain specific language (DSL) that was designed and implemented.
                  To validate the proposed architecture, three case studies will be used as instances of CaVa architecture.",space
10.1016/j.procs.2016.03.044,Conference Proceeding,Procedia Computer Science,scopus,2016-01-01,sciencedirect,A Comparative Analysis of Particle Swarm Optimization and Support Vector Machines for Devnagri Character Recognition: An Android Application,https://api.elsevier.com/content/abstract/scopus_id/84964863522,"Devanagari script is widely used in the Indian subcontinent in several major languages such as Hindi, Sanskrit, Marathi and Nepali. Recognition of unconstrained (Handwritten) Devanagari character is more complex due to shape of constituent strokes. Hence character recognition (CR) has been an active area of research till now and it continues to be a challenging research topic due to its diverse applicable environment. As the size of the vocabulary increases, the complexity of algorithms also increases linearly due to the need for a larger search space. Devnagari script recognition systems using Zernike moments, fuzzy rule and quadratic classifier provide less accuracy and less efficiency. Classification methods based on learning from examples have been widely applied to character recognition from the 1990s and have brought forth significant improvements of recognition accuracies. In this paper techniques like particle swarm optimization and support vector machines are implemented and compared. An android phone is used for taking input character and MATLAB software for showing the recognized Devnagari character. For the connection between android device and MATLAB we are using PHP language. The particle swarm optimization technique provides accuracy up to 90%.",space
10.1016/j.ins.2015.06.027,Journal,Information Sciences,scopus,2015-12-01,sciencedirect,The responsibility weighted Mahalanobis kernel for semi-supervised training of support vector machines for classification,https://api.elsevier.com/content/abstract/scopus_id/84940920656,"Kernel functions in support vector machines (SVM) are needed to assess the similarity of input samples in order to classify these samples, for instance. Besides standard kernels such as Gaussian (i.e., radial basis function, RBF) or polynomial kernels, there are also specific kernels tailored to consider structure in the data for similarity assessment. In this paper, we will capture structure in data by means of probabilistic mixture density models, for example Gaussian mixtures in the case of real-valued input spaces. From the distance measures that are inherently contained in these models, e.g., Mahalanobis distances in the case of Gaussian mixtures, we derive a new kernel, the responsibility weighted Mahalanobis (RWM) kernel. Basically, this kernel emphasizes the influence of model components from which any two samples that are compared are assumed to originate (that is, the “responsible” model components). We will see that this kernel outperforms the RBF kernel and other kernels capturing structure in data (such as the LAP kernel in Laplacian SVM) in many applications where partially labeled data are available, i.e., for semi-supervised training of SVM. Other key advantages are that the RWM kernel can easily be used with standard SVM implementations and training algorithms such as sequential minimal optimization, and heuristics known for the parametrization of RBF kernels in a C-SVM can easily be transferred to this new kernel. Properties of the RWM kernel are demonstrated with 20 benchmark data sets and an increasing percentage of labeled samples in the training data.",space
10.1016/j.amc.2015.04.123,Journal,Applied Mathematics and Computation,scopus,2015-09-15,sciencedirect,An efficient initialization mechanism of neurons for Winner Takes All Neural Network implemented in the CMOS technology,https://api.elsevier.com/content/abstract/scopus_id/84942982088,"The paper presents a new initialization mechanism based on a Convex Combination Method (CCM) for Kohonen self-organizing Neural Networks (NNs) realized in the CMOS technology. A proper selection of initial values of the neuron weights exhibits a strong impact on the quality of the overall learning process. Unfortunately, in case of real input data, e.g. biomedical data, proper initialization is not easy to perform, as an exact data distribution is usually unknown. Bad initialization causes that even 70%–80% of neurons remain inactive, which increases the quantization error and thus limits the classification abilities of the NN. The proposed initialization algorithm has a couple of important advantages. Firstly, it does not require a knowledge of data distribution in the input data space. Secondly, there is no necessity for an initial polarization of the neuron weights before starting the learning process. This feature is very convenient in case of transistor level realizations. In this case the programming lines, which in other approaches occupy a large chip area, are not required. We proposed a modification of the original CCM algorithm. A new parameter which in the proposed analog CMOS realization is represented by an external current, allows to fit the behavior of the mechanism to NNs containing different numbers of neurons. The investigations show that the modified CCM operates properly for the NN containing even 250 neurons. A single CCM block realized in the CMOS 180nm technology occupies an area of 300
                     
                        
                           μ
                        
                     m2 and dissipates an average power of 20
                     
                        
                           μ
                        
                     W and at data rate of up to 20MHz.",space
10.1016/j.patcog.2015.03.017,Journal,Pattern Recognition,scopus,2015-09-01,sciencedirect,IODA: An input/output deep architecture for image labeling,https://api.elsevier.com/content/abstract/scopus_id/84929516076,"In this paper, we propose a deep neural network (DNN) architecture called Input Output Deep Architecture (IODA) for solving the problem of image labeling. IODA directly links a whole image to a whole label map, assigning a label to each pixel using a single neural network forward step. Instead of designing a handcrafted a priori model on labels (such as an atlas in the medical domain), we propose to automatically learn the dependencies between labels. The originality of IODA is to transpose DNN input pre-training trick to the output space, in order to learn a high level representation of labels. It allows a fast image labeling inside a fully neural network framework, without the need of any preprocessing such as feature designing or output coding.
                  In this paper, IODA is applied on both a toy texture problem and a real-world medical image dataset, showing promising results. We provide an open source implementation of IODA.
                        1
                     
                     
                        1
                        
                           http://mloss.org/software/view/562/
                        
                     
                     ,
                     
                        2
                     
                     
                        2
                        
                           https://github.com/jlerouge/crino",space
10.1016/j.echo.2015.02.020,Journal,Journal of the American Society of Echocardiography,scopus,2015-08-01,sciencedirect,Accuracy and Test-Retest Reproducibility of Two-Dimensional Knowledge-Based Volumetric Reconstruction of the Right Ventricle in Pulmonary Hypertension,https://api.elsevier.com/content/abstract/scopus_id/84938546753,"Background
                  Right heart function is the key determinant of symptoms and prognosis in pulmonary hypertension (PH), but the right ventricle has a complex geometry that is challenging to quantify by two-dimensional (2D) echocardiography. A novel 2D echocardiographic technique for right ventricular (RV) quantitation involves knowledge-based reconstruction (KBR), a hybrid of 2D echocardiography–acquired coordinates localized in three-dimensional space and connected by reference to a disease-specific RV shape library. The aim of this study was to determine the accuracy of 2D KBR against cardiac magnetic resonance imaging in PH and the test-retest reproducibility of both conventional 2D echocardiographic RV fractional area change (FAC) and 2D KBR.
               
                  Methods
                  Twenty-eight patients with PH underwent same-day echocardiography and cardiac magnetic resonance imaging. Two operators performed serial RV FAC and 2D KBR acquisition and postprocessing to assess inter- and intraobserver test-retest reproducibility.
               
                  Results
                  Bland-Altman analysis (mean bias ± 95% limits of agreement) showed good agreement for end-diastolic volume (3.5 ± 25.0 mL), end-systolic volume (0.9 ± 19.9 mL), stroke volume (2.6 ± 23.1 mL), and ejection fraction (0.4 ± 10.2%) measured by 2D KBR and cardiac magnetic resonance imaging. There were no significant interobserver or intraobserver test-retest differences for 2D KBR RV metrics, with acceptable limits of agreement (interobserver end-diastolic volume, −0.9 ± 21.8 mL; end-systolic volume, −1.3 ± 25.8 mL; stroke volume, −0.2 ± 24.2 mL; ejection fraction, 0.7 ± 14.4%). Significant test-retest variability was observed for 2D echocardiographic RV areas and FAC.
               
                  Conclusions
                  Two-dimensional KBR is an accurate, novel technique for RV volumetric quantification in PH, with superior test-retest reproducibility compared with conventional 2D echocardiographic RV FAC.",space
10.1016/j.scico.2015.04.002,Journal,Science of Computer Programming,scopus,2015-08-01,sciencedirect,DREMS ML: A wide spectrum architecture design language for distributed computing platforms,https://api.elsevier.com/content/abstract/scopus_id/84930180339,"Complex sensing, processing and control applications running on distributed platforms are difficult to design, develop, analyze, integrate, deploy and operate, especially if resource constraints, fault tolerance and security issues are to be addressed. While technology exists today for engineering distributed, real-time component-based applications, many problems remain unsolved by existing tools. Model-driven development techniques are powerful, but there are very few existing and complete tool chains that offer an end-to-end solution to developers, from design to deployment. There is a need for an integrated model-driven development environment that addresses all phases of application lifecycle including design, development, verification, analysis, integration, deployment, operation and maintenance, with supporting automation in every phase. Arguably, a centerpiece of such a model-driven environment is the modeling language. To that end, this paper presents a wide-spectrum architecture design language called DREMS ML that itself is an integrated collection of individual domain-specific sub-languages. We claim that the language promotes “correct-by-construction” software development and integration by supporting each individual phase of the application lifecycle. Using a case study, we demonstrate how the design of DREMS ML impacts the development of embedded systems.",space
10.1016/j.trb.2015.02.008,Journal,Transportation Research Part B: Methodological,scopus,2015-06-01,sciencedirect,Nonlinear multivariate time-space threshold vector error correction model for short term traffic state prediction,https://api.elsevier.com/content/abstract/scopus_id/84925046498,We propose Time–Space Threshold Vector Error Correction (TS-TVEC) model for short term (hourly) traffic state prediction. The theory and method of cointegration with error correction mechanism is employed in the general design of the new statistical model TS-TVEC. An inherent connection between mathematical form of error correction model and traffic flow theory is revealed through the transformation of the well-known Fundamental Traffic Diagrams. A threshold regime switching framework is implemented to overcome any unknown structural changes in traffic time series. Spatial cross correlated information is incorporated with a piecewise linear vector error correction model. A Neural Network model is also constructed in parallel to comparatively test the effectiveness and robustness of the new statistical model. Our empirical study shows that the TS-TVEC model is an effective tool that is capable of modeling the complexity of stochastic traffic flow processes and potentially applicable to real time traffic state prediction.,space
10.1016/j.knosys.2014.12.010,Journal,Knowledge-Based Systems,scopus,2015-03-01,sciencedirect,Top-k high utility pattern mining with effective threshold raising strategies,https://api.elsevier.com/content/abstract/scopus_id/84923103642,"In pattern mining, users generally set a minimum threshold to find useful patterns from databases. As a result, patterns with higher values than the user-given threshold are discovered. However, it is hard for the users to determine an appropriate minimum threshold. The reason for this is that they cannot predict the exact number of patterns mined by the threshold and control the mining result precisely, which can lead to performance degradation. To address this issue, top-k mining has been proposed for discovering patterns from ones with the highest value to ones with the kth highest value with setting the desired number of patterns, k. Top-k utility mining has emerged to consider characteristics of real-world databases such as relative importance of items and item quantities with the advantages of top-k mining. Although a relevant algorithm has been suggested in recent years, it generates a huge number of candidate patterns, which results in an enormous amount of execution time. In this paper, we propose an efficient algorithm for mining top-k high utility patterns with highly decreased candidates. For this purpose, we develop three strategies that can reduce the search space by raising a minimum threshold effectively in the construction of a global tree, where they utilize exact and pre-evaluated utilities of itemsets. Moreover, we suggest a strategy to identify actual top-k high utility patterns from candidates with the exact and pre-calculated utilities. Comprehensive experimental results on both real and synthetic datasets show that our algorithm with the strategies outperforms state-of-the-art methods.",space
10.1016/j.jbi.2014.10.009,Journal,Journal of Biomedical Informatics,scopus,2015-02-01,sciencedirect,Quantifying the determinants of outbreak detection performance through simulation and machine learning,https://api.elsevier.com/content/abstract/scopus_id/84924493147,"Objective
                  To develop a probabilistic model for discovering and quantifying determinants of outbreak detection and to use the model to predict detection performance for new outbreaks.
               
                  Materials and methods
                  We used an existing software platform to simulate waterborne disease outbreaks of varying duration and magnitude. The simulated data were overlaid on real data from visits to emergency department in Montreal for gastroenteritis. We analyzed the combined data using biosurveillance algorithms, varying their parameters over a wide range. We then applied structure and parameter learning algorithms to the resulting data set to build a Bayesian network model for predicting detection performance as a function of outbreak characteristics and surveillance system parameters. We evaluated the predictions of this model through 5-fold cross-validation.
               
                  Results
                  The model predicted performance metrics of commonly used outbreak detection methods with an accuracy greater than 0.80. The model also quantified the influence of different outbreak characteristics and parameters of biosurveillance algorithms on detection performance in practically relevant surveillance scenarios. In addition to identifying characteristics expected a priori to have a strong influence on detection performance, such as the alerting threshold and the peak size of the outbreak, the model suggested an important role for other algorithm features, such as adjustment for weekly patterns.
               
                  Conclusion
                  We developed a model that accurately predicts how characteristics of disease outbreaks and detection methods will influence on detection. This model can be used to compare the performance of detection methods under different surveillance scenarios, to gain insight into which characteristics of outbreaks and biosurveillance algorithms drive detection performance, and to guide the configuration of surveillance systems.",space
10.1016/j.procs.2015.08.504,Conference Proceeding,Procedia Computer Science,scopus,2015-01-01,sciencedirect,Fitness based position update in spider monkey optimization algorithm,https://api.elsevier.com/content/abstract/scopus_id/84962597795,Spider Monkey Optimization (SMO) technique is most recent member in the family of swarm optimization algorithms.SMO algorithm fall in class of Nature Inspired Algorithm (NIA). SMO algorithm is good in exploration and exploitation of local search space and it is well balanced algorithm most of the times. This paper presents a new strategy to update position of solution during local leader phase using fitness of individuals. The proposed algorithm is named as Fitness based Position Update in SMO (FPSMO) algorithm as it updates position of individuals based on their fitness. The anticipated strategy enhances the rate of convergence. The planned FPSMO approach tested over nineteen benchmark functions and for one real world problem so as to establish superiority of it over basic SMO algorithm.,space
10.1016/j.procs.2015.05.364,Conference Proceeding,Procedia Computer Science,scopus,2015-01-01,sciencedirect,Multiobjective design optimization in the Lightweight Dataflow for DDDAS Environment (LiD4E),https://api.elsevier.com/content/abstract/scopus_id/84939212317,"In this paper, we introduce new methods for multiobjective, system-level optimization that have been incorporated into the Lightweight Dataflow for Dynamic Data Driven Application Systems (DDDAS) Environment (LiD4E). LiD4E is a design tool for optimized implementation of dynamic, data-driven stream mining systems using high-level dataflow models of computation. More specifically, we develop in this paper new methods for integrated modeling and optimization of real-time stream mining constraints, multidimensional stream mining performance (precision and recall), and energy efficiency. Using a design methodology centered on data-driven control of and coordination between alternative dataflow subsystems for stream mining (classification modes), we develop systematic methods for exploring complex, multidimensional design spaces associated with dynamic stream mining systems, and deriving sets of Pareto-optimal system configurations that can be switched among based on data characteristics and operating constraints.",space
10.1016/j.patcog.2014.07.024,Journal,Pattern Recognition,scopus,2015-01-01,sciencedirect,Inexact and incremental bilinear Lanczos components algorithms for high dimensionality reduction and image reconstruction,https://api.elsevier.com/content/abstract/scopus_id/84908031277,"Recently, matrix-based methods have gained wide attentions in pattern recognition and machine learning communities. The generalized low rank approximations of matrices (GLRAM) and the bilinear Lanczos components (BLC) algorithm are two popular algorithms that treat data as the native two-dimensional matrix patterns. However, these two algorithms often require heavy computation time and memory space in practice, especially for large scale problems. In this paper, we propose inexact and incremental bilinear Lanczos components algorithms for high dimensionality reduction and image reconstruction. We first introduce the thick-restarting strategy to the BLC algorithm, and present a thick-restarted Lanczos components (TRBLC) algorithm. In this algorithm, we use the Ritz vectors as approximations to dominant eigenvectors instead of the Lanczos vectors. In our implementation, the iterative matrices are neither formed nor stored explicitly, thanks to the characteristic of the Lanczos procedure. Then, we explore the relationship between the reconstruction error and the accuracy of the Ritz vectors, so that the computational complexities of eigenpairs can be reduced significantly. As a result, we propose an inexact thick-restarted Lanczos components (Inex-TRBLC) algorithm. Moreover, we investigate the problem of incremental generalized low rank approximations of matrices, and propose an incremental and inexact TRBLC (Incr-TRBLC) algorithm. Numerical experiments illustrate the superiority of the new algorithms over the GLRAM algorithm and its variations, as well as the BLC algorithm for some real-world image reconstruction and face recognition problems.",space
10.1016/j.neuron.2014.08.042,Journal,Neuron,scopus,2014-10-22,sciencedirect,Engagement of Neural Circuits Underlying 2D Spatial Navigation in a Rodent Virtual Reality System,https://api.elsevier.com/content/abstract/scopus_id/84908234689,"Virtual reality (VR) enables precise control of an animal’s environment and otherwise impossible experimental manipulations. Neural activity in rodents has been studied on virtual 1D tracks. However, 2D navigation imposes additional requirements, such as the processing of head direction and environment boundaries, and it is unknown whether the neural circuits underlying 2D representations can be sufficiently engaged in VR. We implemented a VR setup for rats, including software and large-scale electrophysiology, that supports 2D navigation by allowing rotation and walking in any direction. The entorhinal-hippocampal circuit, including place, head direction, and grid cells, showed 2D activity patterns similar to those in the real world. Furthermore, border cells were observed, and hippocampal remapping was driven by environment shape, suggesting functional processing of virtual boundaries. These results illustrate that 2D spatial representations can be engaged by visual and rotational vestibular stimuli alone and suggest a novel VR tool for studying rat navigation.",space
10.1016/j.ins.2014.02.024,Journal,Information Sciences,scopus,2014-08-10,sciencedirect,Oppositional extension of reinforcement learning techniques,https://api.elsevier.com/content/abstract/scopus_id/84900824395,"In this paper, we present different opposition schemes for four reinforcement learning methods: Q-learning, Q(
                        
                           λ
                        
                     ), Sarsa, and Sarsa(
                        
                           λ
                        
                     ) under assumptions that are reasonable for many real-world problems where type-II opposites generally better reflect the nature of the problem at hand. It appears that the aggregation of opposition-based schemes with regular learning methods can significantly speed up the learning process, especially where the number of observations is small or the state space is large. We verify the performance of the proposed methods using two different applications: a grid-world problem and a single water reservoir management problem.",space
10.1016/j.neucom.2014.01.012,Journal,Neurocomputing,scopus,2014-07-20,sciencedirect,Text style analysis using trace ratio criterion patch alignment embedding,https://api.elsevier.com/content/abstract/scopus_id/84897963546,"An effective algorithm for extracting cues of text styles is proposed in this paper. When processing document collections, the documents are first converted to a high dimensional data set with the assistant of a group of style markers. We also employ the Trace Ratio Criterion Patch Alignment Embedding (TR-PAE) to obtain lower dimensional representation in a textual space. The TR-PAE has some advantages that the inter-class separability and intra-class compactness are well characterized by the special designed intrinsic graph and penalty graph, which are based on discriminative patch alignment strategy. Another advantage is that the proposed method is based on trace ratio criterion, which directly represents the average between-class distance and average within-class distance in the low-dimensional space. To evaluate our proposed algorithm, three corpuses are designed and collected using existing popular corpuses and real-life data covering diverse topics and genres. Extensive simulations are conducted to illustrate the feasibility and effectiveness of our implementation. Our simulations demonstrate that the proposed method is able to extract the deeply hidden information of styles of given documents, and efficiently conduct reliable text analysis results on text styles can be provided.",space
10.1016/j.ins.2014.01.008,Journal,Information Sciences,scopus,2014-06-10,sciencedirect,Embedded local feature selection within mixture of experts,https://api.elsevier.com/content/abstract/scopus_id/84897053423,"A useful strategy to deal with complex classification scenarios is the “divide and conquer” approach. The mixture of experts (MoE) technique makes use of this strategy by jointly training a set of classifiers, or experts, that are specialized in different regions of the input space. A global model, or gate function, complements the experts by learning a function that weighs their relevance in different parts of the input space. Local feature selection appears as an attractive alternative to improve the specialization of experts and gate function, particularly, in the case of high dimensional data. In general, subsets of dimensions, or subspaces, are usually more appropriate to classify instances located in different regions of the input space. Accordingly, this work contributes with a regularized variant of MoE that incorporates an embedded process for local feature selection using 
                        
                           
                              
                                 L
                              
                              
                                 1
                              
                           
                        
                      regularization. Experiments using artificial and real-world datasets provide evidence that the proposed method improves the classical MoE technique, in terms of accuracy and sparseness of the solution. Furthermore, our results indicate that the advantages of the proposed technique increase with the dimensionality of the data.",space
10.1016/j.neucom.2013.03.060,Journal,Neurocomputing,scopus,2014-05-20,sciencedirect,Autonomous UAV based search operations using constrained sampling evolutionary algorithms,https://api.elsevier.com/content/abstract/scopus_id/84896707806,"This paper introduces and studies the application of Constrained Sampling Evolutionary Algorithms in the framework of an UAV based search and rescue scenario. These algorithms have been developed as a way to harness the power of Evolutionary Algorithms (EA) when operating in complex, noisy, multimodal optimization problems and transfer the advantages of their approach to real time real world problems that can be transformed into search and optimization challenges. These types of problems are denoted as Constrained Sampling problems and are characterized by the fact that the physical limitations of reality do not allow for an instantaneous determination of the fitness of the points present in the population that must be evolved. A general approach to address these problems is presented and a particular implementation using Differential Evolution as an example of CS-EA is created and evaluated using teams of UAVs in search and rescue missions. The results are compared to those of a Swarm Intelligence based strategy in the same type of problem as this approach has been widely used within the UAV path planning field in different variants by many authors.",space
10.1016/j.ins.2014.01.010,Journal,Information Sciences,scopus,2014-05-10,sciencedirect,Traffic sign recognition using group sparse coding,https://api.elsevier.com/content/abstract/scopus_id/84894099092,"Recognizing traffic signs is a challenging problem; and it has captured the attention of the computer vision community for several decades. Essentially, traffic sign recognition is a multi-class classification problem that has become a real challenge for computer vision and machine learning techniques. Although many machine learning approaches are used for traffic sign recognition, they are primarily used for classification, not feature design. Identifying rich features using modern machine learning methods has recently attracted attention and has achieved success in many benchmarks. However these approaches have not been fully implemented in the traffic sign recognition problem. In this paper, we propose a new approach to tackle the traffic sign recognition problem. First, we introduce a new feature learning approach using group sparse coding. The primary goal is to exploit the intrinsic structure of the pre-learned visual codebook. This new coding strategy preserves locality and encourages similar descriptors to share similar sparse representation patterns. Second, we use a non-uniform quantization approach based on log-polar mapping. Using the log-polar mapping of the traffic sign image, rotated and scaled patterns are converted into shifted patterns in the new space. We extract the local descriptors from these patterns to learn the features. Finally, by evaluating the proposed approach using the German Traffic Sign Recognition Benchmark dataset, we show that the proposed coding strategy outperforms existing coding methods and the obtained results are comparable to the state-of-the-art.",space
10.1016/j.tcs.2013.09.027,Journal,Theoretical Computer Science,scopus,2014-01-30,sciencedirect,Domain adaptation and sample bias correction theory and algorithm for regression,https://api.elsevier.com/content/abstract/scopus_id/84892371351,"We present a series of new theoretical, algorithmic, and empirical results for domain adaptation and sample bias correction in regression. We prove that the discrepancy is a distance for the squared loss when the hypothesis set is the reproducing kernel Hilbert space induced by a universal kernel such as the Gaussian kernel. We give new pointwise loss guarantees based on the discrepancy of the empirical source and target distributions for the general class of kernel-based regularization algorithms. These bounds have a simpler form than previous results and hold for a broader class of convex loss functions not necessarily differentiable, including 
                        
                           
                              L
                           
                           
                              q
                           
                        
                      losses and the hinge loss. We also give finer bounds based on the discrepancy and a weighted feature discrepancy parameter. We extend the discrepancy minimization adaptation algorithm to the more significant case where kernels are used and show that the problem can be cast as an SDP similar to the one in the feature space. We also show that techniques from smooth optimization can be used to derive an efficient algorithm for solving such SDPs even for very high-dimensional feature spaces and large samples. We have implemented this algorithm and report the results of experiments both with artificial and real-world data sets demonstrating its benefits both for general scenario of adaptation and the more specific scenario of sample bias correction. Our results show that it can scale to large data sets of tens of thousands or more points and demonstrate its performance improvement benefits.",space
10.1016/j.neunet.2014.07.001,Journal,Neural Networks,scopus,2014-01-01,sciencedirect,Ordinal regression neural networks based on concentric hyperspheres,https://api.elsevier.com/content/abstract/scopus_id/84904878701,"Threshold models are one of the most common approaches for ordinal regression, based on projecting patterns to the real line and dividing this real line in consecutive intervals, one interval for each class. However, finding such one-dimensional projection can be too harsh an imposition for some datasets. This paper proposes a multidimensional latent space representation with the purpose of relaxing this projection, where the different classes are arranged based on concentric hyperspheres, each class containing the previous classes in the ordinal scale. The proposal is implemented through a neural network model, each dimension being a linear combination of a common set of basis functions. The model is compared to a nominal neural network, a neural network based on the proportional odds model and to other state-of-the-art ordinal regression methods for a total of 12 datasets. The proposed latent space shows an improvement on the two performance metrics considered, and the model based on the three-dimensional latent space obtains competitive performance when compared to the other methods.",space
10.1016/j.engappai.2014.01.007,Journal,Engineering Applications of Artificial Intelligence,scopus,2014-01-01,sciencedirect,Adaptive multi-objective reinforcement learning with hybrid exploration for traffic signal control based on cooperative multi-agent framework,https://api.elsevier.com/content/abstract/scopus_id/84894106219,"In this paper, we focus on computing a consistent traffic signal configuration at each junction that optimizes multiple performance indices, i.e., multi-objective traffic signal control. The multi-objective function includes minimizing trip waiting time, total trip time, and junction waiting time. Moreover, the multi-objective function includes maximizing flow rate, satisfying green waves for platoons traveling in main roads, avoiding accidents especially in residential areas, and forcing vehicles to move within moderate speed range of minimum fuel consumption. In particular, we formulate our multi-objective traffic signal control as a multi-agent system (MAS). Traffic signal controllers have a distributed nature in which each traffic signal agent acts individually and possibly cooperatively in a MAS. In addition, agents act autonomously according to the current traffic situation without any human intervention. Thus, we develop a multi-agent multi-objective reinforcement learning (RL) traffic signal control framework that simulates the driver's behavior (acceleration/deceleration) continuously in space and time dimensions. The proposed framework is based on a multi-objective sequential decision making process whose parameters are estimated based on the Bayesian interpretation of probability. Using this interpretation together with a novel adaptive cooperative exploration technique, the proposed traffic signal controller can make real-time adaptation in the sense that it responds effectively to the changing road dynamics. These road dynamics are simulated by the Green Light District (GLD) vehicle traffic simulator that is the testbed of our traffic signal control. We have implemented the Intelligent Driver Model (IDM) acceleration model in the GLD traffic simulator. The change in road conditions is modeled by varying the traffic demand probability distribution and adapting the IDM parameters to the adverse weather conditions. Under the congested and free traffic situations, the proposed multi-objective controller significantly outperforms the underlying single objective controller which only minimizes the trip waiting time (i.e., the total waiting time in the whole vehicle trip rather than at a specific junction). For instance, the average trip and waiting times are ≃8 and 6 times lower respectively when using the multi-objective controller.",space
10.1016/j.chemolab.2013.08.009,Journal,Chemometrics and Intelligent Laboratory Systems,scopus,2013-10-15,sciencedirect,Genetic algorithm search space splicing particle swarm optimization as general-purpose optimizer,https://api.elsevier.com/content/abstract/scopus_id/84884383367,"A heuristic search space splicing scheme has been implemented to aid the convergence of the particle swarm optimization (PSO) algorithm to the global optimum. Genetic algorithm (GA) was used to splice the search space into smaller subspaces, thereby reducing the number of local minima. PSO algorithm was subsequently used to locate the global optima in the subspaces. A set of 11 well-known test functions had been used for the assessment of this novel GA search space splicing PSO (GA-SSS-PSO) architecture. Of the methods tested in this study, the GA-SSS-PSO approach was the only one that could optimize all functions to a desirable level. To demonstrate the algorithm's applicability, three optimization tasks of different categories commonly faced in the field of chemometrics were subjected to optimization by GA-SSS-PSO and results indicated that the novel hybrid algorithm provided robust performance for both theoretical and real life problems and may be suited as general-purpose optimizer for medium-sized optimization tasks.",space
10.1016/j.bica.2013.07.009,Journal,Biologically Inspired Cognitive Architectures,scopus,2013-10-01,sciencedirect,Emotional biologically inspired cognitive architecture,https://api.elsevier.com/content/abstract/scopus_id/84883206490,"Human-like artificial emotional intelligence is vital for integration of future robots into the human society. This work introduces a general framework for representation and processing of emotional contents in a cognitive architecture, called “emotional biologically inspired cognitive architecture” (eBICA). Unlike in previous attempts, in this framework emotional elements are added virtually to all cognitive representations and processes by modifying the main building blocks of the prototype architectures. The key elements are appraisals associated as attributes with schemas and mental states, moral schemas that control patterns of appraisals and represent social emotions, and semantic spaces that give values to appraisals. Proposed principles are tested in an experiment involving human subjects and virtual agents, based on a simple paradigm in imaginary virtual world. It is shown that with moral schemas, but probably not without them, eBICA can account for human behavior in the selected paradigm. The model sheds light on clustering of social emotions and allows for their elegant mathematical description. The new framework will be suitable for implementation of believable emotional intelligence in artifacts, necessary for emotionally informed behavior, collaboration of virtual partners with humans, and self-regulated learning of virtual agents.",space
10.1016/j.neunet.2013.05.010,Journal,Neural Networks,scopus,2013-10-01,sciencedirect,Complexity-reduced implementations of complete and null-space-based linear discriminant analysis,https://api.elsevier.com/content/abstract/scopus_id/84879563722,"Dimensionality reduction has become an important data preprocessing step in a lot of applications. Linear discriminant analysis (LDA) is one of the most well-known dimensionality reduction methods. However, the classical LDA cannot be used directly in the small sample size (SSS) problem where the within-class scatter matrix is singular. In the past, many generalized LDA methods has been reported to address the SSS problem. Among these methods, complete linear discriminant analysis (CLDA) and null-space-based LDA (NLDA) provide good performances. The existing implementations of CLDA are computationally expensive. In this paper, we propose a new and fast implementation of CLDA. Our proposed implementation of CLDA, which is the most efficient one, is equivalent to the existing implementations of CLDA in theory. Since CLDA is an extension of null-space-based LDA (NLDA), our implementation of CLDA also provides a fast implementation of NLDA. Experiments on some real-world data sets demonstrate the effectiveness of our proposed new CLDA and NLDA algorithms.",space
10.1016/j.asoc.2013.07.009,Journal,Applied Soft Computing Journal,scopus,2013-08-27,sciencedirect,Synergizing fitness learning with proximity-based food source selection in artificial bee colony algorithm for numerical optimization,https://api.elsevier.com/content/abstract/scopus_id/84886723307,"Evolutionary computation (EC) paradigm has undergone extensions in the recent years diverging from the natural process of genetic evolution to the simulation of natural life processes exhibited by the living organisms. Bee colonies exemplify a high level of intrinsic interdependence and co-ordination among its members, and algorithms inspired from the bee colonies have gained recent prominence in the field of swarm based metaheuristics. The artificial bee colony (ABC) algorithm was recently developed, by simulating the minimalistic foraging model of honeybees in search of food sources, for solving real-parameter, non-convex, and non-smooth optimization problems. The single parameter perturbation in classical ABC resulted in fairly commendable performance for simple problems without epistasis of variables (separable). However, it suffered from narrow search zone and slow convergence which eventually led to poor exploitation tendency. Even with the increase in dimensionality, a significant deterioration was observed in the ability of ABC to locate the optimum in a huge search volume. Some of the probable shortcomings in the basic ABC approach, as observed, are the single parameter perturbation instead of a multiple one, ignoring the fitness to reward ratio while selecting food sites, and most importantly the absence of environmental factors in the algorithm design. Research has shown that spatial environmental factors play a crucial role in insect locomotion and foragers seem to learn the direction to be undertaken based on the relative analysis of its proximal surroundings. Most importantly, the mapping of the forager locomotion from three dimensional search spaces to a multidimensional solution space calls forth the implementation of multiple modification schemes. Based on the fundamental observation pertaining to the dynamics of ABC, this article proposes an improved variant of ABC aimed at improving the optimizing ability of the algorithm over an extended set of problems. The hybridization of the proposed fitness learning mechanism with a weighted selection scheme and proximity based stimuli helps to achieve a fine blending of explorative and exploitative behaviour by enhancing both local and global searching ability of the algorithm. This enhances the ability of the swarm agents to detect optimal regions in the unexplored fitness basins. With respect to its immediate surroundings, a proximity based component is added to the normal positional modification of the onlookers and is enacted through an improved probability selection scheme that takes the T/E (total reward to distance) ratio metric into account. The biologically-motivated, hybridized variant of ABC achieves a statistically superior performance on majority of the tested benchmark instances, as compared to some of the most prominent state-of-the-art algorithms, as is demonstrated through a detailed experimental evaluation and verified statistically.",space
10.1016/j.eswa.2012.12.080,Journal,Expert Systems with Applications,scopus,2013-08-01,sciencedirect,A decision support system for optimal deployment of sonobuoy networks based on sea current forecasts and multi-objective evolutionary optimization,https://api.elsevier.com/content/abstract/scopus_id/84875365598,"A decision support system for the optimal deployment of drifting acoustic sensor networks for cooperative track detection in underwater surveillance applications is proposed and tested on a simulated scenario. The system integrates sea water current forecasts, sensor range models and simple drifting buoy kinematic models to predict sensor positions and temporal network performance. A multi-objective genetic optimization algorithm is used for searching a set of Pareto optimal deployment solutions (i.e. the initial position of drifting sonobuoys of the network) by simultaneously optimizing two quality of service metrics: the temporal mean of the network area coverage and the tracking coverage. The solutions found after optimization, which represent different efficient tradeoffs between the two metrics, can be conveniently evaluated by the mission planner in order to choose the solution with the desired compromise between the two conflicting objectives. Sensitivity analysis through the Unscented Transform is also performed in order to test the robustness of the solutions with respect to network parameters and environmental uncertainty. Results on a simulated scenario making use of real probabilistic sea water current forecasts are provided showing the effectiveness of the proposed approach. Future work is envisioned to make the tool fully operational and ready to use in real scenarios.",space
10.1016/j.sigpro.2012.10.020,Journal,Signal Processing,scopus,2013-06-01,sciencedirect,3D CBIR with sparse coding for image-guided neurosurgery,https://api.elsevier.com/content/abstract/scopus_id/84875248258,"This research takes an application-specific approach to investigate, extend and implement the state of the art in the fields of both visual information retrieval and machine learning, bridging the gap between theoretical models and real world applications. During an image-guided neurosurgery, path planning remains the foremost and hence the most important step to perform an operation and ensures the maximum resection of an intended target and minimum sacrifice of health tissues. In this investigation, the technique of content-based image retrieval (CBIR) coupled with machine learning algorithms are exploited in designing a computer aided path planning system (CAP) to assist junior doctors in planning surgical paths while sustaining the highest precision. Specifically, after evaluation of approaches of sparse coding and K-means in constructing a codebook, the model of sparse codes of 3D SIFT has been furthered and thereafter employed for retrieving, The novelty of this work lies in the fact that not only the existing algorithms for 2D images have been successfully extended into 3D space, leading to promising results, but also the application of CBIR that is mainly in a research realm, to a clinical sector can be achieved by the integration with machine learning techniques. Comparison with the other four popular existing methods is also conducted, which demonstrates that with the implementation of sparse coding, all methods give better retrieval results than without while constituting the codebook, implying the significant contribution of machine learning techniques.",space
10.1016/j.jhydrol.2012.10.019,Journal,Journal of Hydrology,scopus,2013-01-07,sciencedirect,A comparison of methods to avoid overfitting in neural networks training in the case of catchment runoff modelling,https://api.elsevier.com/content/abstract/scopus_id/84871010255,"Artificial neural networks (ANNs) becomes very popular tool in hydrology, especially in rainfall–runoff modelling. However, a number of issues should be addressed to apply this technique to a particular problem in an efficient way, including selection of network type, its architecture, proper optimization algorithm and a method to deal with overfitting of the data. The present paper addresses the last, rarely considered issue, namely comparison of methods to prevent multi-layer perceptron neural networks from overfitting of the training data in the case of daily catchment runoff modelling. Among a number of methods to avoid overfitting the early stopping, the noise injection and the weight decay have been known for about two decades, however only the first one is frequently applied in practice. Recently a new methodology called optimized approximation algorithm has been proposed in the literature.
                  Overfitting of the training data leads to deterioration of generalization properties of the model and results in its untrustworthy performance when applied to novel measurements. Hence the purpose of the methods to avoid overfitting is somehow contradictory to the goal of optimization algorithms, which aims at finding the best possible solution in parameter space according to pre-defined objective function and available data. Moreover, different optimization algorithms may perform better for simpler or larger ANN architectures. This suggest the importance of proper coupling of different optimization algorithms, ANN architectures and methods to avoid overfitting of real-world data – an issue that is also studied in details in the present paper.
                  The study is performed for Annapolis River catchment, characterized by significant seasonal changes in runoff, rapid floods during winter and spring, moderately dry summers, severe winters with snowfall, snow melting, frequent freeze and thaw, and presence of river ice. The present paper shows that the elaborated noise injection method may prevent overfitting slightly better than the most popular early stopping approach. However, the implementation of noise injection to real-world problems is difficult and the final model performance depends significantly on a number of very technical details, what somehow limits its practical applicability. It is shown that optimized approximation algorithm does not improve the results obtained by older methods, possibly due to over-simplified criterion of stopping the algorithm. Extensive calculations reveal that Evolutionary Computation-based algorithm performs better for simpler ANN architectures, whereas classical gradient-based Levenberg–Marquardt algorithm is able to benefit from additional input variables, representing precipitation and snow cover from one more previous day, and from more complicated ANN architectures. This confirms that the curse of dimensionality has severe impact on the performance of Evolutionary Computing methods.",space
10.1016/j.procs.2013.05.187,Conference Proceeding,Procedia Computer Science,scopus,2013-01-01,sciencedirect,Comparing support vector machines and artificial neural networks in the recognition of steering angle for driving of mobile robots through paths in plantations,https://api.elsevier.com/content/abstract/scopus_id/84896966222,"The use of mobile robots turns out to be interesting in activities where the action of human specialist is difficult or dangerous. Mobile robots are often used for the exploration in areas of difficult access, such as rescue operations and space missions, to avoid human experts exposition to risky situations. Mobile robots are also used in agriculture for planting tasks as well as for keeping the application of pesticides within minimal amounts to mitigate environmental pollution. In this paper we present the development of a system to control the navigation of an autonomous mobile robot through tracks in plantations. Track images are used to control robot direction by pre-processing them to extract image features. Such features are then submitted to a support vector machine and an artificial neural network in order to find out the most appropriate route. A comparison of the two approaches was performed to ascertain the one presenting the best outcome. The overall goal of the project to which this work is connected is to develop a real time robot control system to be embedded into a hardware platform. In this paper we report the software implementation of a support vector machine and of an artificial neural network, which so far presented respectively around 93% and 90% accuracy in predicting the appropriate route.",space
10.3182/20130902-3-CN-3020.00124,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2013-01-01,sciencedirect,Adaptive dynamic programming for solving non-zero-sum differential games,https://api.elsevier.com/content/abstract/scopus_id/84896325180,"In this paper, a novel adaptive dynamic programming algorithm based on policy iteration is developed to solve online multi-player non-zero-sum differential game for continuous-time nonlinear systems. This algorithm is mathematically equivalent to the quasi-Newton's iteration in a Banach space. The implementation using neural networks is given, where a critic neural network is used to learn its value function, and an action neural network sharing the same parameters with the corresponding critic neural network is used to learn its optimal control policy for each player. All the critic and action neural networks are updated online in real-time and continuously. A simulation example is presented to demonstrate the effectiveness of the developed scheme.",space
10.1016/j.pmcj.2012.11.003,Journal,Pervasive and Mobile Computing,scopus,2013-01-01,sciencedirect,Enabling real-time city sensing with kernel stream oracles and MapReduce,https://api.elsevier.com/content/abstract/scopus_id/84883463862,"An algorithmic architecture for kernel-based modelling of data streams from city sensing infrastructures is introduced. It is both applicable for pre-installed, moving and extemporaneous sensors, including the “citizen-as-a-sensor” view on user-generated data. The approach is centred around a kernel dictionary implementing a general hypothesis space which is updated incrementally, accounting for memory and processing capacity limitations. It is general for both kernel-based classification and regression. An extension to area-to-point modelling is introduced to account for the data aggregated over a spatial region. A distributed implementation realised under the Map-Reduce framework is presented to train an ensemble of sequential kernel learners.",space
10.1016/j.patcog.2012.07.018,Journal,Pattern Recognition,scopus,2013-01-01,sciencedirect,Self-taught dimensionality reduction on the high-dimensional small-sized data,https://api.elsevier.com/content/abstract/scopus_id/84866033003,"To build an effective dimensionality reduction model usually requires sufficient data. Otherwise, traditional dimensionality reduction methods might be less effective. However, sufficient data cannot always be guaranteed in real applications. In this paper we focus on performing unsupervised dimensionality reduction on the high-dimensional and small-sized data, in which the dimensionality of target data is high and the number of target data is small. To handle the problem, we propose a novel Self-taught Dimensionality Reduction (STDR) approach, which is able to transfer external knowledge (or information) from freely available external (or auxiliary) data to the high-dimensional and small-sized target data. The proposed STDR consists of three steps: First, the bases are learnt from sufficient external data, which might come from the same “type” or “modality” of target data. The bases are the common part between external data and target data, i.e., the external knowledge (or information). Second, target data are reconstructed by the learnt bases by proposing a novel joint graph sparse coding model, which not only provides robust reconstruction ability but also preserves the local structures amongst target data in the original space. This process transfers the external knowledge (i.e., the learnt bases) to target data. Moreover, the proposed solver to the proposed model is theoretically guaranteed that the objective function of the proposed model converges to the global optimum. After this, target data are mapped into the learnt basis space, and are sparsely represented by the bases, i.e., represented by parts of the bases. Third, the sparse features (that is, the rows with zero (or small) values) of the new representations of target data are deleted for achieving the effectiveness and the efficiency. That is, this step performs feature selection on the new representations of target data. Finally, experimental results at various types of datasets show the proposed STDR outperforms the state-of-the-art algorithms in terms of k-means clustering performance.",space
10.1016/j.camwa.2012.01.079,Journal,Computers and Mathematics with Applications,scopus,2012-12-01,sciencedirect,Machine learning in agent-based stochastic simulation: Inferential theory and evaluation in transportation logistics,https://api.elsevier.com/content/abstract/scopus_id/84870236735,"Multiagent-based simulation is an approach to realize stochastic simulation where both the behavior of the modeled multiagent system and dynamic aspects of its environment are implemented with autonomous agents. Such simulation provides an ideal environment for intelligent agents to learn to perform their tasks before being deployed in a real-world environment. The presented research investigates theoretical and practical aspects of learning by autonomous agents within stochastic agent-based simulation. The theoretical work is based on the Inferential Theory of Learning, which describes learning processes from the perspective of a learner’s goal as a search through knowledge space. The theory is extended for approximate and probabilistic learning to account for the situations encountered when learning in stochastic environments. Practical aspects are exemplified by two use cases in autonomous logistics: learning predictive models for environment conditions in the future, and learning in the context of evolutionary plan optimization.",space
10.1016/j.asoc.2012.01.014,Journal,Applied Soft Computing Journal,scopus,2012-12-01,sciencedirect,Autonomous real-time landing site selection for Venus and Titan using Evolutionary Fuzzy Cognitive Maps,https://api.elsevier.com/content/abstract/scopus_id/84869086510,"Future science-driven landing missions, conceived to collect in situ data on regions of planetary bodies that have the highest potential to yield important scientific discoveries, will require a higher degree of autonomy. The latter includes the ability of the spacecraft to autonomously select the landing site using real-time data acquired during the descent phase. This paper presents the development of an Evolutionary Fuzzy Cognitive Map (E-FCM) model that implements an artificial intelligence system capable of autonomously selecting a landing site with the highest potential for scientific discoveries constrained by the requirement of soft landing in a region with safe terrains. The proposed E-FCM evolves its internal states and interconnections as a function of real-time data collected during the descent phase, therefore improving the decision process as more accurate information becomes available. The E-FCM is constructed using knowledge accumulated by planetary experts and it is tested on scenarios that simulate the decision process during the descent phase toward the Hyndla Regio on Venus. The E-FCM is shown to quickly reach conclusions that are consistent with what would be the choice of a planetary expert if the scientist were presented with the same information. The proposed methodology is fast and efficient and may be suitable for on-board spacecraft implementation and real-time decision making during the course of robotic exploration of the Solar System.",space
10.1016/j.actaastro.2012.06.003,Journal,Acta Astronautica,scopus,2012-11-01,sciencedirect,"Robotic Mission to Mars: Hands-on, minds-on, web-based learning",https://api.elsevier.com/content/abstract/scopus_id/84865224510,"Problem-based learning has been demonstrated as an effective methodology for developing analytical skills and critical thinking. The use of scenario-based learning incorporates problem-based learning whilst encouraging students to collaborate with their colleagues and dynamically adapt to their environment. This increased interaction stimulates a deeper understanding and the generation of new knowledge. The Victorian Space Science Education Centre (VSSEC) uses scenario-based learning in its Mission to Mars, Mission to the Orbiting Space Laboratory and Primary Expedition to the M.A.R.S. Base programs. These programs utilize methodologies such as hands-on applications, immersive-learning, integrated technologies, critical thinking and mentoring to engage students in Science, Technology, Engineering and Mathematics (STEM) and highlight potential career paths in science and engineering. The immersive nature of the programs demands specialist environments such as a simulated Mars environment, Mission Control and Space Laboratory, thus restricting these programs to a physical location and limiting student access to the programs. To move beyond these limitations, VSSEC worked with its university partners to develop a web-based mission that delivered the benefits of scenario-based learning within a school environment. The Robotic Mission to Mars allows students to remotely control a real rover, developed by the Australian Centre for Field Robotics (ACFR), on the VSSEC Mars surface. After completing a pre-mission training program and site selection activity, students take on the roles of scientists and engineers in Mission Control to complete a mission and collect data for further analysis. Mission Control is established using software developed by the ACRI Games Technology Lab at La Trobe University using the principles of serious gaming. The software allows students to control the rover, monitor its systems and collect scientific data for analysis. This program encourages students to work scientifically and explores the interaction between scientists and engineers. This paper presents the development of the program, including the involvement of university students in the development of the rover, the software, and the collation of the scientific data. It also presents the results of the trial phase of this program including the impact on student engagement and learning outcomes.",space
10.1016/j.eswa.2012.04.074,Journal,Expert Systems with Applications,scopus,2012-10-15,sciencedirect,Shared decision support system on dental restoration,https://api.elsevier.com/content/abstract/scopus_id/84861811784,"Shared decision making (SDM) is an approach in which doctor–patient communication regarding available evidence and patient preferences is facilitated to enable the patient to participate in treatment decisions. SDM affords not only the inclusion of the ethical diversities involved in patient-centered care, but also the quality improvements in decision-making process. Though SDM has been studied extensively, there have been few practical implementations in real clinical environments. In this paper, we propose a shared decision-making system with its focus on dental restorative treatment planning. In our system, restorative treatment alternatives for SDM were generated by employing an ontology that had captured the clinical knowledge required for treatments. We considered patient preferences for treatment as an important support for mutual agreements between the patient and the doctor on healthcare decisions. We constructed a consistent and robust hierarchy of preferences using the analytic hierarchy process (AHP) method, to help determine treatment priorities. On the basis of our proposed system, we developed a Web-based application for the visualization of evidence-based treatment recommendations with preference-based weights. We tested our system using a scenario to illustrate how doctors and patients can make shared decisions. The application is of high value in supporting SDM between doctors and patients, and expedites effective treatments and enhanced patient satisfaction.",space
10.1016/j.ijepes.2012.02.008,Journal,International Journal of Electrical Power and Energy Systems,scopus,2012-09-01,sciencedirect,Assessing the relevance of load profiling information in electrical load forecasting based on neural network models,https://api.elsevier.com/content/abstract/scopus_id/84860742606,"The article is focused on evaluating the relevance of load profiling information in electrical load forecasting, using neural networks as the forecasting methodology. Different models, with and without load profiling information, were tested and compared, and, the importance of the different inputs was investigated, using the concept of partial derivatives to understand the relevance of including this type of data in the input space. The paper presents a model for the day ahead load profile prediction for an area with many consumers. The results were analyzed with a simulated load diagram (to illustrate a distribution feeder) and also with a specific output of a 60/15kV real distribution substation that feeds a small town. The adopted methodology was successfully implemented and resulted in reducing the mean absolute percentage error between 0.5% and 16%, depending on the nature of the concurrent methodology used and the forecasted day, with a major benefit regarding the treatment of special days (holidays). The results illustrate an interesting potential for the use of the load profiling information in forecasting.",space
10.1016/j.optcom.2012.05.037,Journal,Optics Communications,scopus,2012-08-15,sciencedirect,Semi-supervised kernel learning based optical image recognition,https://api.elsevier.com/content/abstract/scopus_id/84863471802,"This paper is to propose semi-supervised kernel learning based optical image recognition, called Semi-supervised Graph-based Global and Local Preserving Projection (SGGLPP) through integrating graph construction with the specific DR process into one unified framework. SGGLPP preserves not only the positive and negative constraints but also the local and global structure of the data in the low dimensional space. In SGGLPP, the intrinsic and cost graphs are constructed using the positive and negative constraints from side-information and k nearest neighbor criterion from unlabeled samples. Moreover, kernel trick is applied to extend SGGLPP called KSGGLPP by on the performance of nonlinear feature extraction. Experiments are implemented on UCI database and two real image databases to testify the feasibility and performance of the proposed algorithm.",space
10.1016/S1000-9361(11)60423-8,Journal,Chinese Journal of Aeronautics,scopus,2012-08-01,sciencedirect,Approximate maximum likelihood algorithm for moving source localization using TDOA and FDOA measurements,https://api.elsevier.com/content/abstract/scopus_id/84865610506,"A closed-form approximate maximum likelihood (AML) algorithm for estimating the position and velocity of a moving source is proposed by utilizing the time difference of arrival (TDOA) and frequency difference of arrival (FDOA) measurements of a signal received at a number of receivers. The maximum likelihood (ML) technique is a powerful tool to solve this problem. But a direct approach that uses the ML estimator to solve the localization problem is exhaustive search in the solution space, and it is very computationally expensive, and prohibits real-time processing. On the basis of ML function, a closed-form approximate solution to the ML equations can be obtained, which can allow real-time implementation as well as global convergence. Simulation results show that the proposed estimator achieves better performance than the two-step weighted least squares (WLS) approach, which makes it possible to attain the Cramér-Rao lower bound (CRLB) at a sufficiently high noise level before the threshold effect occurs.",space
10.1016/j.neunet.2012.02.032,Journal,Neural Networks,scopus,2012-08-01,sciencedirect,Autonomous Growing Neural Gas for applications with time constraint: Optimal parameter estimation,https://api.elsevier.com/content/abstract/scopus_id/84861761321,"This paper aims to address the ability of self-organizing neural network models to manage real-time applications. Specifically, we introduce fAGNG (fast Autonomous Growing Neural Gas), a modified learning algorithm for the incremental model Growing Neural Gas (GNG) network. The Growing Neural Gas network with its attributes of growth, flexibility, rapid adaptation, and excellent quality of representation of the input space makes it a suitable model for real time applications. However, under time constraints GNG fails to produce the optimal topological map for any input data set. In contrast to existing algorithms, the proposed fAGNG algorithm introduces multiple neurons per iteration. The number of neurons inserted and input data generated is controlled autonomous and dynamically based on a priory or online learnt model. A detailed study of the topological preservation and quality of representation depending on the neural network parameter selection has been developed to find the best alternatives to represent different linear and non-linear input spaces under time restrictions or specific quality of representation requirements.",space
10.1016/j.patrec.2012.01.015,Journal,Pattern Recognition Letters,scopus,2012-07-01,sciencedirect,A cluster-assumption based batch mode active learning technique,https://api.elsevier.com/content/abstract/scopus_id/84859302340,"In this paper, we propose an active learning technique for solving multiclass problems with support vector machine (SVM) classifiers. The technique is based on both uncertainty and diversity criteria. The uncertainty criterion is implemented by analyzing the one-dimensional output space of the SVM classifier. A simple histogram thresholding algorithm is used to find out the low density region in the SVM output space to identify the most uncertain samples. Then the diversity criterion exploits the kernel k-means clustering algorithm to select uncorrelated informative samples among the selected uncertain samples. To assess the effectiveness of the proposed method we compared it with other batch mode active learning techniques presented in the literature using one toy data set and three real data sets. Experimental results confirmed that the proposed technique provided a very good tradeoff among robustness to biased initial training samples, classification accuracy, computational complexity, and number of new labeled samples necessary to reach the convergence.",space
10.1016/j.jtice.2011.11.006,Journal,Journal of the Taiwan Institute of Chemical Engineers,scopus,2012-05-01,sciencedirect,"Mathematical modeling, optimal design and control of an SCR reactor for NOx removal",https://api.elsevier.com/content/abstract/scopus_id/84860316497,"The elimination of nitrogen oxides (NOx) is an important issue for global environment. This paper deals with the model development, optimal design and feedback control of an SCR (selective catalytic reduction) reactor for NOx removal. A 3D dynamic simulation model for use to investigate the reaction behavior and the transport phenomena in the catalytic filter of the SCR reactor is proposed. To estimate the model's kinetic parameters from experimental data, an optimization technique that integrates Taguchi method, a real-coded genetic algorithm and a neural network auxiliary model is developed. With the proposed dynamic model, we investigated the effects of the key parameters, such as the gas hourly space velocity, operating temperature and the amount of ammonium used, on the NOx conversion and NH3 slip phenomena. To improve the NOx abatement performance, the proposed optimization technique is then applied to search for a set of best operation conditions for the SCR reactor. The obtained results indicate that the optimized SCR can achieve the NOx reduction rate up to 99.93%, which is over 9% better in performance than the previously reported one in the literature. Besides, the optimal operating temperature is considerably lower and the emission of ammonium from the reactor is insignificant. Compared with conventional designs, the proposed design is much better in energy savings and is environment-friendly. To attenuate the negative effects of environmental disturbances on reactor's performance, we implemented a direct adaptive control strategy to the SCR reactor. The stability of the control system is theoretically guaranteed with a Lyapunov-based approach. Extensive simulation results show that the learning-type, nonlinear control strategy presents significantly much better NOx reduction performance than a conventional IMC-PI controller, especially when facing process uncertainties and disturbances.",space
10.1016/j.websem.2011.11.003,Journal,Journal of Web Semantics,scopus,2012-03-01,sciencedirect,Induction of robust classifiers for web ontologies through kernel machines,https://api.elsevier.com/content/abstract/scopus_id/84857784576,"The paper focuses on the task of approximate classification of semantically annotated individual resources in ontological knowledge bases. The method is based on classification models built through kernel methods, a well-known class of effective statistical learning algorithms. Kernel functions encode a notion of similarity among elements of some input space. The definition of a family of parametric language-independent kernel functions for individuals occurring in an ontology allows the application of these statistical learning methods on Semantic Web knowledge bases. The classification models induced by kernel methods offer an alternative way to classify individuals with respect to the typical exact and approximate deductive reasoning procedures. The proposed statistical setting enables further inductive approaches to a variety of other tasks that can better cope with the inherent incompleteness of the knowledge bases in the Semantic Web and with their potential incoherence due to their distributed nature. The effectiveness of the proposed method is empirically proved through experiments on the task of approximate classification with real ontologies collected from standard repositories.",space
10.1016/j.patcog.2011.08.005,Journal,Pattern Recognition,scopus,2012-03-01,sciencedirect,Accurate real-time neural disparity MAP estimation with FPGA,https://api.elsevier.com/content/abstract/scopus_id/80055025069,"We propose in this paper a new method for real-time dense disparity map computing using a stereo pair of rectified images. Based on the neural network and Disparity Space Image (DSI) data structure, the disparity map computing consists of two main steps: initial disparity map estimation by combining the neuronal network and the DSI structure, and its refinement. Four improvements are introduced so that an accurate and fast result will be reached. The first one concerns the proposition of a new strategy in order to optimize the computation time of the initial disparity map. In the second one, a specific treatment is proposed in order to obtain more accurate disparity for the neighboring pixels to boundaries. The third one, it concerns the pixel similarity measure for matching score computation and it consists of using in addition to the traditional pixel intensities, the magnitude and orientation of the gradients providing more accuracy. Finally, the processing time of the method has been decreased consequently to our implementation of some critical steps on FPGAs. Experimental results on real datasets are conducted and a comparative evaluation of the obtained results relative to the state-of-art methods is presented.",space
10.1016/j.neuroimage.2011.03.052,Journal,NeuroImage,scopus,2012-01-02,sciencedirect,Effects of training strategies implemented in a complex videogame on functional connectivity of attentional networks,https://api.elsevier.com/content/abstract/scopus_id/80054109529,"We used the Space Fortress videogame, originally developed by cognitive psychologists to study skill acquisition, as a platform to examine learning-induced plasticity of interacting brain networks. Novice videogame players learned Space Fortress using one of two training strategies: (a) focus on all aspects of the game during learning (fixed priority), or (b) focus on improving separate game components in the context of the whole game (variable priority). Participants were scanned during game play using functional magnetic resonance imaging (fMRI), both before and after 20h of training. As expected, variable priority training enhanced learning, particularly for individuals who initially performed poorly. Functional connectivity analysis revealed changes in brain network interaction reflective of more flexible skill learning and retrieval with variable priority training, compared to procedural learning and skill implementation with fixed priority training. These results provide the first evidence for differences in the interaction of large-scale brain networks when learning with different training strategies. Our approach and findings also provide a foundation for exploring the brain plasticity involved in transfer of trained abilities to novel real-world tasks such as driving, sport, or neurorehabilitation.",space
10.3182/20120403-3-DE-3010.00010,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,RodosVisor - An object-oriented and customizable hypervisor: The CPU virtualization,https://api.elsevier.com/content/abstract/scopus_id/84866091325,"RodosVisor is an object-oriented and bare-metal virtual machine monitor (VMM) or hypervisor designed for the aerospace industry, mainly to provide time and spatial separation to the NetworkCentric core avionics machine, Montenegro and Dittrich (2009). The NetworkCentric core avionics machine consists of several harmonized components working together to implement dependable computing in a simple way, with computing units managed by the local real-time operating system RODOS. To support partitioned software architectures such as AIR, Rufino et al. (2009), and MILS, DeLong, R. (2007), RodosVisor adapted the Popek and Goldberg's fidelity, efficiency and resource control virtualization requirements, Popek and Goldberg (1974), to the space application domain by extending them with extra ones, like timing determinism, reactivity and improved dependability. Another distinctive RodosVisor feature is the customized design based on generative programming techniques, such as aspect oriented programming and template meta-programming.",space
10.1016/B978-0-444-59520-1.50104-4,Book Series,Computer Aided Chemical Engineering,scopus,2012-01-01,sciencedirect,Intelligent Automation Platform for Bioprocess Development,https://api.elsevier.com/content/abstract/scopus_id/84862870640,"High throughput technology has been increasingly adapted for drug screen and bioprocess development, due to the small amount of processing materials and reagents required and parallel experiment execution. It allows a wide design space to be explored in order to discover novel bioprocess solutions. Currently, the high throughput experiments for bioprocess development are implemented in a sequential fashion in which liquid handling system will perform the web lab experiment to prepare the samples; standalone analysis devices detect the data such as protein concentration; and specific software is used to realise the data analysis for process design or further experimentation.
                  The aim of this paper is to show how the efficiency of the high throughput bioprocess development approach can be enhanced by creating an intelligent automation platform that systematically drives liquid handling system, analysis devices and data analysis to perform a closed-loop learning. The first generation prototype has been established which consists of three parts: automated devices, design algorithms and database. In order to prove the concept of prototype, both simulation and real experiments studies have been established. In this case study, the platform is used to investigate the solubility of lysozyme at various ion strengths and pH values. Tecan liquid handling system for experimentation as well as buffer preparation and a plate reader for uv absorption measurement to determine protein concentration were used as the automated devices. The simplex search algorithm and artificial neural network modelling were utilised as design algorithm to iteratively select the experiments to execute and determine the optimal design solution. An entity-relationship database with Tecan system configuration information and experimental data was established. The results demonstrate this integrated approach can implement experiments and data analysis automatically to provide specific bioprocess design solutions in a closed loop strategy at first time. It is a promising approach that may significant increase the level of lab automation to release the engineer from the labour intensive R&D activities and provides the base for sophisticated artificial intelligent learning in the future.",space
10.1016/j.autcon.2011.05.018,Journal,Automation in Construction,scopus,2012-01-01,sciencedirect,Simulation and analytical techniques for construction resource planning and scheduling,https://api.elsevier.com/content/abstract/scopus_id/81355138778,"To date, few construction methods or models in the literature have discussed about helping the project managers decide the near-optimum distributions of manpower, material, equipment and space according to their project objectives and project constraints. Thus, the traditional scheduling methods or models often result in a “seat-of-the-pants” style of management, rather than decision making based on an analysis of real data. This paper presents an intelligent scheduling system (ISS) that can help the project managers to find the near-optimum schedule plan according to their project objectives and project constraints. ISS uses simulation techniques to distribute resources and assign different levels of priorities to different activities in each simulation cycle to find the near-optimal solution. ISS considers and integrates most of the important construction factors (schedule, cost, space, manpower, equipment and material) simultaneously in a unified environment, which makes the resulting schedule that will be closer to optimal. Furthermore, ISS allows for what-if analyses of possible scenarios, and schedule adjustments based on unforeseen conditions (change orders, late material delivery, etc.). Finally, two sample applications and one real-world construction project are utilized to illustrate and compare the effectiveness of ISS with two widely used software packages, Primavera Project Planner and Microsoft Project.",space
10.1016/j.actaastro.2011.05.027,Journal,Acta Astronautica,scopus,2011-09-01,sciencedirect,Dynamic fiber Bragg gratings based health monitoring system of composite aerospace structures,https://api.elsevier.com/content/abstract/scopus_id/79960979180,"The main purpose of the current work is to develop a new system for structural health monitoring of composite aerospace structures based on real-time dynamic measurements, in order to identify the structural state condition. Long-gauge Fibre Bragg Grating (FBG) optical sensors were used for monitoring the dynamic response of the composite structure. The algorithm that was developed for structural damage detection utilizes the collected dynamic response data, analyzes them in various ways and through an artificial neural network identifies the damage state and its location. Damage was simulated by slightly varying locally the mass of the structure (by adding a known mass) at different zones of the structure. Lumped masses in different locations upon the structure alter the eigen-frequencies in a way similar to actual damage. The structural dynamic behaviour has been numerically simulated and experimentally verified by means of modal testing on two different composite aerospace structures.
                  Advanced digital signal processing techniques, e.g. the wavelet transform (WT), were used for the analysis of the dynamic response for feature extraction. WT's capability of separating the different frequency components in the time domain without loosing frequency information makes it a versatile tool for demanding signal processing applications. The use of WT is also suggested by the no-stationary nature of dynamic response signals and the opportunity of evaluating the temporal evolution of their frequency contents. Feature extraction is the first step of the procedure. The extracted features are effective indices of damage size and location. The classification step comprises of a feed-forward back propagation network, whose output determines the simulated damage location. Finally, dedicated training and validation activities were carried out by means of numerical simulations and experimental procedures.
                  Experimental validation was performed initially on a flat stiffened panel, representing a section of a typical aeronautical structure, manufactured and tested in the lab and, as a second step, on a scaled up space oriented structure, which is a composite honeycomb plate, used as a deployment base for antenna arrays. An integrated FBG sensor network, based on the advantage of multiplexing, was mounted on both structures and different excitation positions and boundary conditions were used. The analysis of operational dynamic responses was employed to identify both the damage and its position. The system that was designed and tested initially on the thin composite panel, was successfully validated on the larger honeycomb structure. Numerical simulation of both structures was used as a support tool at all the steps of the work providing among others the location of the optical sensors used. The proposed work will be the base for the whole system qualification and validation on an antenna reflector in future work.",space
10.1016/j.asoc.2011.01.045,Journal,Applied Soft Computing Journal,scopus,2011-09-01,sciencedirect,Knowledge of opposite actions for reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/79956159518,"Abstract
                  Reinforcement learning (RL) is one of the machine intelligence techniques with several characteristics that make it suitable for solving real-world problems. However, RL agents generally face a very large state space in many applications. They must take actions in every state many times to find the optimal policy. In this work, a special type of knowledge about actions is employed to improve the performance of the off-policy, incremental, and model-free reinforcement learning with discrete state and action space. One of the components of RL agent is the action. For each action, its associate opposite action is defined. The actions and opposite actions are implemented in the framework of reinforcement learning to update the value function resulting in a faster convergence. The effects of opposite action on some of the reinforcement learning algorithms are investigated.",space
10.1016/j.jvlc.2011.03.004,Journal,Journal of Visual Languages and Computing,scopus,2011-08-01,sciencedirect,An alternative map of the United States based on an n-dimensional model of geographic space,https://api.elsevier.com/content/abstract/scopus_id/79960409782,"Geographic features have traditionally been visualized with fairly high amount of geometric detail, while relationships among these features in attribute space have been represented at a much coarser resolution. This limits our ability to understand complex high-dimensional relationships and structures existing in attribute space. In this paper, we present an alternative approach aimed at creating a high-resolution representation of geographic features with the help of a self-organizing map (SOM) consisting of a large number of neurons. In a proof-of-concept implementation, we spatialize 200,000+ U.S. Census block groups using a SOM consisting of 250,000 neurons. The geographic attributes considered in this study reflect a more holistic representation of geographic reality than in previous studies. The study includes 69 attributes regarding population statistics, land use/land cover, climate, geology, topography, and soils. This diversity of attributes is informed by our desire to build a comprehensive two-dimensional base map of n-dimensional geographic space. The paper discusses how standard GIS methods and neural network processing are combined towards the creation of an alternative map of the United States.",space
10.1016/j.conengprac.2011.03.002,Journal,Control Engineering Practice,scopus,2011-07-01,sciencedirect,Survey and application of sensor fault detection and isolation schemes,https://api.elsevier.com/content/abstract/scopus_id/79957944993,"Model-based sensor fault detection, isolation and accommodation (SFDIA) is a direction of development in particular with UAVs where sensor redundancy may not be an option due to weight, cost and space implications. SFDIA via neural networks (NNs) have been proposed over the years due to their nonlinear structures and online learning capabilities. The majority of papers tend to consider single sensor faults. While useful, this assumption can limit application to real systems where sensor faults can occur simultaneously or consecutively. In this paper we consider the latter scenario, where it is assumed that a 1s time gap is present between consecutive faults. Furthermore few applications have considered fixed-wing UAVs where full autonomy is most needed. In this paper an EMRAN RBF NN is chosen for modelling purposes due to its ability to adapt well to nonlinear environments while maintaining high computational speeds. A nonlinear UAV model is used for demonstration, where decoupled longitudinal motion is considered. System and measurement noise is also included in the UAV model as wind gust disturbances on the angle of attack and sensor noise, respectively. The UAV is assumed to operate at an initial trimmed condition of speed, 32m/s and altitude, 1000m. After 30 separate SFDIA tests implemented on a 1.6GHz Pentium processor, the NN-SFDIA scheme detected all but 2 faults and the NN processing time was 97% lower than the flight data sampling time.",space
10.1016/j.knosys.2011.01.007,Journal,Knowledge-Based Systems,scopus,2011-05-01,sciencedirect,A knowledge-based problem solving method in GIS application,https://api.elsevier.com/content/abstract/scopus_id/79952451251,"Model design for theme analysis is one of the biggest challenges in GIS. Many real applications in GIS require functioning not only in data management and visualization, but also in analysis and decision-making. Confronted with an application of planning a new metro line in a city, a typical GIS is unable to accomplish the task in the absence of human experts or artificial intelligence technologies. Apart from being models for analyzing in different themes, some applications are also instances of problem solving in AI. Therefore, in order to strengthen its ability in automatic analysis, many theories and technologies from AI can be embedded in the GIS. In this paper, a state space is defined to formalize the metro line planning problem. By utilizing the defined state evaluation function, knowledge-based rules and strategies, a heuristic searching method is developed to optimize the solutions iteratively. Experiments are implemented to illuminate the validity of this AI-enhanced automatic analysis model of GIS.",space
10.1016/j.neucom.2010.11.023,Journal,Neurocomputing,scopus,2011-04-01,sciencedirect,Manifold Mapping Machine,https://api.elsevier.com/content/abstract/scopus_id/79953057596,"Nonlinear classification has been a non-trivial task in machine learning for the past decades. In recent years, kernel machines have successfully generalized the inner-product based linear classifiers to nonlinear ones by transforming data into some high or infinite dimensional feature space. However, due to their implicit space transformation and unobservable latent feature space, it is hard to have an intuitive understanding of their working mechanism. In this paper, we propose a comprehensible framework for nonlinear classifier design, called Manifold Mapping Machine (M3). M3 can generalize any linear classifier to nonlinear by transforming data into some low-dimensional feature space explicitly. To demonstrate the effectiveness of M3 framework, we further present an algorithmic implementation of M3 named Supervised Spectral Space Classifier (S3C). Compared with the kernel classifiers, S3C can achieve similar or even better data separation by mapping data into the low-dimensional spectral space, allowing both of its mapped data and new feature space to be examined directly. Moreover, with the discriminative information integrated into the spectral space transformation, the classification performance of S3C is more robust than that of the kernel classifiers. Experimental results show that S3C is superior to other state-of-the-art nonlinear classifiers on both synthetic and real-world data sets.",space
10.1016/j.asoc.2010.09.007,Journal,Applied Soft Computing Journal,scopus,2011-03-01,sciencedirect,Forecasting stock markets using wavelet transforms and recurrent neural networks: An integrated system based on artificial bee colony algorithm,https://api.elsevier.com/content/abstract/scopus_id/78751613501,"This study presents an integrated system where wavelet transforms and recurrent neural network (RNN) based on artificial bee colony (abc) algorithm (called ABC-RNN) are combined for stock price forecasting. The system comprises three stages. First, the wavelet transform using the Haar wavelet is applied to decompose the stock price time series and thus eliminate noise. Second, the RNN, which has a simple architecture and uses numerous fundamental and technical indicators, is applied to construct the input features chosen via Stepwise Regression-Correlation Selection (SRCS). Third, the Artificial Bee Colony algorithm (ABC) is utilized to optimize the RNN weights and biases under a parameter space design. For illustration and evaluation purposes, this study refers to the simulation results of several international stock markets, including the Dow Jones Industrial Average Index (DJIA), London FTSE-100 Index (FTSE), Tokyo Nikkei-225 Index (Nikkei), and Taiwan Stock Exchange Capitalization Weighted Stock Index (TAIEX). As these simulation results demonstrate, the proposed system is highly promising and can be implemented in a real-time trading system for forecasting stock prices and maximizing profits.",space
10.1016/j.microc.2010.09.008,Journal,Microchemical Journal,scopus,2011-03-01,sciencedirect,Full-range optical pH sensor array based on neural networks,https://api.elsevier.com/content/abstract/scopus_id/78649931852,"A neural network multivariate calibration is used to predict the pH of a solution in the full-range (0–14) from hue (H) values coming from imaging an optical pH sensor array based on 11 sensing elements with immobilized pH indicators. Different colorimetric acid-base indicators were tested for membrane preparation fulfilling the following conditions: 1) no leaching; 2) change in tonal coordinate by reaction and 3) covering the full pH range with overlapping between their pH responses. The sensor array was imaged after equilibration with a solution using a scanner working in transmission mode. Using software developed by us, the H coordinate of the colour space HSV was calculated from the RGB coordinates of each element.
                  The neural network was trained with the calibration data set using the Levenberg–Marquardt training method. The network structure has 11 input neurons (each one matching the hue of a single element in the sensor array), 1 output (the pH approximation value) and 1 hidden layer with 10 hidden neurons. The network provides an MSE=0.0098 in the training data, MSE=0.0183 in the validation data and MSE=0.0426 in the test data coming from a set of real water samples. The resulting correlation coefficient R obtained in the Pearson correlation test is R=0.999.",space
10.3182/20110828-6-IT-1002.03103,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2011-01-01,sciencedirect,UniBot Remote Laboratory: A scalable web-based set-up for education and experimental activities in robotics,https://api.elsevier.com/content/abstract/scopus_id/84866747278,"Abstract
                  The direct work on a real set-ups is an important experience for students in control theory and robotics. On the other hand, for several reasons (space, costs, complexity, etc.), it is not always possible to give students an individual access to laboratory set-ups, for their practical activities. Therefore, in recent years many tele-laboratories have been implemented by different universities, providing experimental set-ups to each student, while minimizing problems related to costs, spaces, and so on. The UniBot Remote Lab has been implemented to provide remote access via TCP connection, to assign to students different time-slots for their experiences, and to reduce the financial effort required by real set-ups. Moreover, the entire framework has been developed with high modularity both from the hardware and software point of view and, even if the basic set-up has been conceived for mobile robotics, different kind of robots or automatic machines can be easily added and be available for experimental activities.",space
10.1016/j.procs.2011.07.038,Conference Proceeding,Procedia Computer Science,scopus,2011-01-01,sciencedirect,From context to micro-context - Issues and challenges in sensorizing smart spaces for assistive living,https://api.elsevier.com/content/abstract/scopus_id/84863045088,"Most smart home based monitoring / assistive systems that attempt to recognize activities within a smart home are targeted towards living-alone elderly, and stop at providing instantaneous coarse grained information such as room-occupancy or provide specific programmed reminders for taking medication etc. In our work, we target multiple residents, while restricting the use of wearable devices / sensors. In addition we do away with video due to privacy concerns. In this paper we present the design challenges and issues in putting together a sensor network for obtaining micro-context information in multi-person smart spaces. In order to support greater levels of ambient intelligence we support fine grained spatio-temporal data and context acquisition. The architecture is being currently developed into a prototype in a modular fashion for deployment and testing in a variety of environments, and is being concurrently evaluated and tested in real conditions, prior to deployment in a facility for elderly residents with mild cognitive disorder.",space
10.1016/j.jas.2011.07.015,Journal,Journal of Archaeological Science,scopus,2011-01-01,sciencedirect,"Living the past: 3D models, virtual reality and game engines as tools for supporting archaeology and the reconstruction of cultural heritage - the case-study of the Roman villa of Casal de Freiria",https://api.elsevier.com/content/abstract/scopus_id/80054111601,"“Learn about the past to better understand the present and predict the future”.
                  Although used abundantly to justify our interest in ancient societies, this statement lacks practical meaning due to the high degree of uncertainty which cloaks archaeological studies and theories, and the fact that there is no real way to prove or validate them. That is why it is so important to approach this study from a multidisciplinary point of view, providing several inputs which complement each other and so maximize the amount of factual information drawn from the analysis. Even then, the study will never be truly complete because there will always be a missing document, a small trace of an object (Verhagen, 2008), that still needs to be analysed.
                  This paper aims to be a useful contribution to historical research, specifically to the study of architectural history. Its purpose is to create a series of methods and tools for testing and analysing theories and hypotheses for historical scenarios (Vasáros, 2008) through the use of 3D modelling tools and Virtual Reality (VR) engines.
                  The project was developed in two stages:
                  The first was the creation of several three-dimensional (3D) models, each representing a different theory or hypothesis. The models were based on accurate Computer Assisted Design (CAD) (Autodesk® AutoCAD) models for the reconstruction of the buildings, and Geographic Information Systems (GIS) (ESRI®
                     ) for the recreation of the terrain, thereby creating a realistic representation of what exists now, and a close approximation to what may have once existed.
                  In the second stage, a simplified version of the models was imported into a Virtual Reality (VR) game engine (Bethesda Softworks®
                     ) to create the ambience of the villa at the time, allowing full exploration of the space. It also includes fauna and flora, as well as Artificial Intelligence (AI)-driven avatars, as can be seen in the Video 1 provided in the electronic version of this manuscript.",space
10.1016/j.atmosenv.2010.07.024,Journal,Atmospheric Environment,scopus,2010-11-01,sciencedirect,Prediction of hourly O<inf>3</inf> concentrations using support vector regression algorithms,https://api.elsevier.com/content/abstract/scopus_id/77956886806,"In this paper we present an application of the Support Vector Regression algorithm (SVMr) to the prediction of hourly ozone values in Madrid urban area. In order to improve the training capacity of SVMrs, we have used a recently proposed approach, based on reductions of the SVMr hyper-parameters search space. Using the modified SVMr, we study different influences which may modify the ozone prediction, such as previous ozone measurements in a given station, measurements in neighbors stations, and the influence of meteorologic variables. We use statistical tests to verify the significance of incorporating different variables into the SVMr. A comparison with the results obtained using a neural network (multi-layer perceptron) is also carried out. This study has been carried out in 5 different stations of the air pollution monitoring network of Madrid, so the conclusions raised are backed by real data. The final result of the work is a robust and powerful software for tropospheric ozone prediction in Madrid. Also, the prediction tool based on SVMr is flexible enough to incorporate any other prediction variable, such as city models, or traffic patters, which may improve the prediction obtained with the SVMr.",space
10.1016/j.simpat.2010.04.010,Journal,Simulation Modelling Practice and Theory,scopus,2010-10-01,sciencedirect,Revisiting state space exploration of timed coloured petri net models to optimize manufacturing system's performance,https://api.elsevier.com/content/abstract/scopus_id/77955305732,"Due to constant fluctuations in market demands, nowadays scheduling of flexible manufacturing systems is taking great importance to improve competitiveness. Coloured Petri Nets (CPN) is a high level modelling formalism which have been widely used to model and verify systems, allowing representing not only the system’s dynamic behaviour but also the information flow. One approach that focuses in performance optimization of industrial systems is the one that uses the CPN formalism extended with time features (Timed Coloured Petri Nets) and explores all the possible states of the model (state space) looking for states of particular interest under industrial scope. Unfortunately, using the time extension, the state space becomes awkward for most industrial problems, reason why there is a recognized need of approaches that could tackle optimization problems such as the scheduling of manufacturing activities without simplifying any important aspect of the real system. In this paper a timed state space approach for properties verification and systems optimization is presented together with new algorithms in order to get better results when time is used as a cost function for optimizing the makespan of manufacturing systems. A benchmarking example of a job-shop is modelled in CPN formalism to illustrate the improvements that can be achieved with the proposed implementations.",space
10.1016/j.cor.2009.11.016,Journal,Computers and Operations Research,scopus,2010-10-01,sciencedirect,A tabu search with an oscillation strategy for the discriminant analysis problem,https://api.elsevier.com/content/abstract/scopus_id/77649180337,"This article proposes a tabu search approach to solve a mathematical programming formulation of the linear classification problem, which consists of determining an hyperplane that separates two groups of points as well as possible in ℜ
                        m
                     . The tabu search approach proposed is based on a non-standard formulation using linear system infeasibility. The search space is the set of bases defined on the matrix that describes the linear system. The moves are performed by pivoting on a specified row and column. On real machine learning databases, our approach compares favorably with implementations based on parametric programming and irreducible infeasible constraint sets. Additional computational results for randomly generated instances confirm that our method provides a suitable alternative to the mixed integer programming formulation that is solved by a commercial code when the number of attributes m increases.",space
10.1016/j.ejor.2010.01.026,Journal,European Journal of Operational Research,scopus,2010-09-16,sciencedirect,A travel demand management strategy: The downtown space reservation system,https://api.elsevier.com/content/abstract/scopus_id/77949310027,"In this paper, a Travel Demand Management strategy known as the Downtown Space Reservation System (DSRS) is introduced. The purpose of this system is to facilitate the mitigation of traffic congestion in a cordon-based downtown area by requiring people who want to drive into this area to make reservations in advance. An integer programming formulation is provided to obtain the optimal mix of vehicles and trips that are characterized by a series of factors such as vehicle occupancy, departure time, and trip length with an objective of maximizing total system throughput and revenue. Based upon the optimal solution, an “intelligent” module is built using artificial neural networks that enables the transportation authority to make decisions in real time on whether to accept an incoming request. An example is provided that demonstrates that the solution of the “intelligent” module resembles the optimal solution with an acceptable error rate. Finally, implementation issues of the DSRS are addressed.",space
10.1016/j.engappai.2010.02.007,Journal,Engineering Applications of Artificial Intelligence,scopus,2010-06-01,sciencedirect,A knowledge-based architecture for distributed fault analysis in power networks,https://api.elsevier.com/content/abstract/scopus_id/77950510844,"Power industry around the world is facing several changes since deregulation with constant pressure put on improving security, reliability and quality of the power supply. Computational fault analysis and diagnosis of power networks have been active research topics with several theories and algorithms proposed. This paper proposes a distributed diagnostic algorithm for fault analysis in power networks. Distributed architecture for power network fault analysis (DAPFA) is an intelligent, model-based diagnostic algorithm that incorporates a hierarchical power network representation and model. The architecture is based on the industry’s substation automation implementation standards. The structural and functional model is a multi-level representation with each level depicting a more complex grouping of components than its predecessor in the hierarchy. The distributed functional representation contains the behavioral knowledge related to the components of that level in the structural model.
                  The diagnostic algorithm of DAPFA is designed to perform fault analysis in pre-diagnostic and diagnostic levels. Pre-diagnostic phase provides real-time analysis while the diagnostic phase provides the final diagnostic analysis. The diagnostic algorithm incorporates knowledge-based and model-based reasoning mechanisms with one of the model levels represented as a network of neural nets. The relevant algorithms and techniques are discussed. The resulting system has been implemented on a New Zealand sub-system and the results are analyzed.",space
10.1016/j.tsf.2009.10.152,Journal,Thin Solid Films,scopus,2010-05-31,sciencedirect,Application of neural classification in ellipsometry for robust thin-film characterizations,https://api.elsevier.com/content/abstract/scopus_id/77953297755,"Nowadays, ellipsometry is a widely used technique for thin-film analyses among the existing methods. It offers a rapid, accurate and non-destructive control. The main task in this technique remains in the inverse problem which goal is to extract the interesting characteristics of the sample from the ellipsometric measurement. This is a purely mathematical step and the common algorithms used to achieve it are based on the gradient method. These latter proceed iteratively and hence require a well-chosen initial solution in order to converge towards a global minimum corresponding to the real physical solution. The main limitation of these algorithms is the risk to slip into a local minimum, leading to an erroneous solution. In this paper, we propose an original method based on neural classification in order to give a first estimation about the location of the solution in the multi-parameter space. This operation generally takes place before the characterization step itself. In this work, the method is implemented experimentally to estimate the thickness range of photoresist thin films deposited on a glass substrate.",space
10.1016/j.landurbplan.2010.03.002,Journal,Landscape and Urban Planning,scopus,2010-05-30,sciencedirect,Incorporating spatio-temporal knowledge in an Intelligent Agent Model for natural resource management,https://api.elsevier.com/content/abstract/scopus_id/77951254750,"Space and time are intrinsic components of the decision-making process in natural resource management. Decisions to extract resources from a specific location have consequences for all future decisions as they may lead to profitable opportunities or, conversely, towards unfavorable outcomes. As such, the spatio-temporal nature of decision-making should be acknowledged and incorporated into models developed to assist the management of natural resources. The objective of this research is to develop an Intelligent Agent Model that is able to learn through repetitive simulation how to make decisions regarding natural resource extraction. Specifically, an agent is guided by heuristic algorithms to search a natural landscape and learn which locations hold the highest profits and when it is best to extract the resource in order to improve the potential of future opportunities. The model is implemented using hypothetical and real data sets to emulate the process of harvesting trees in natural forests in order to maximize profits while respecting spatial constraints that are imposed in order to conserve various aspects of the forest. Simulation results reveal the ability of the Intelligent Agent Model to utilize spatio-temporal knowledge in order learn how to devise optimal solutions in a variety of scenarios. Furthermore, the model demonstrates how the timing of decisions is linked to the spatial constraints imposed on the operation. The findings from this research can be used to inform natural resource management about the importance of the relationship between the location and timing of resource-based activities.",space
10.1016/j.micpro.2010.06.001,Journal,Microprocessors and Microsystems,scopus,2010-01-01,sciencedirect,An FPGA implemented cellular automaton crowd evacuation model inspired by the electrostatic-induced potential fields,https://api.elsevier.com/content/abstract/scopus_id/79957783489,"This paper studies the on-chip realisation of a dynamic model proposed to simulate crowd behaviour, originated from electrostatic-induced potential fields. It is based on cellular automata (CA), thus taking advantage of their inherent ability to represent sufficiently phenomena of arbitrary complexity and, additionally, to be simulated precisely by digital computers. The model combines electrostatic-induced potential fields to incorporate flexibility in the movement of pedestrians. It primarily calculates distances in an obstacle filled space based on the Euclidean metric. Furthermore, it adopts a computationally fast and efficient method to overcome trouble-inducing obstacles by shifting the moving mechanism to a potential field method based on Manhattan-distance. The hardware implementation of the model is based on FPGA logic. Initialisation of the dedicated processor takes place in collaboration with a detecting and tracking algorithm supported by cameras. The instant response of the processor provides the location of pedestrians around exits. Hardware implementation exploits the prominent feature of parallelism that CA structures inherently possess in contrast to the serial computers, thus accelerating the response of the model. Furthermore, FPGA implementation of the model is advantageous in terms of low-cost, high-speed, compactness and portability features. Finally, the processor could be used as a part of an embedded, real-time, decision support system, aiming at the efficient guidance of crowd in cases of mass egress.",space
10.1016/j.comnet.2009.07.012,Journal,Computer Networks,scopus,2009-12-24,sciencedirect,An RL-based scheduling algorithm for video traffic in high-rate wireless personal area networks,https://api.elsevier.com/content/abstract/scopus_id/70449529532,"The emerging high-rate wireless personal area network (WPAN) technology is capable of supporting high-speed and high-quality real-time multimedia applications. In particular, video streams are deemed to be a dominant traffic type, and require quality of service (QoS) support. However, in the current IEEE 802.15.3 standard for MAC (media access control) of high-rate WPANs, the implementation details of some key issues such as scheduling and QoS provisioning have not been addressed. In this paper, we first propose a Markov decision process (MDP) model for optimal scheduling for video flows in high-rate WPANs. Using this model, we also propose a scheduler that incorporates compact state space representation, function approximation, and reinforcement learning (RL). Simulation results show that our proposed RL scheduler achieves nearly optimal performance and performs better than F-SRPT, EDD+SRPT, and PAP scheduling algorithms in terms of a lower decoding failure rate.",space
10.1016/j.jss.2008.06.043,Journal,Journal of Surgical Research,scopus,2009-08-01,sciencedirect,Endotoxin Alters Early Fetal Lung Morphogenesis,https://api.elsevier.com/content/abstract/scopus_id/67651115865,"Background
                  The effects of immaturity and hypoplasia of the premature lung can be affected by proinflammatory stimuli in late gestation or the postnatal period from acute lung injury secondary to intensive ventilatory management or the metabolic consequences of surgery. These stimuli alter alveolarization and contribute to bronchopulmonary dysplasia. While prior research has focused primarily on late gestational effects of inflammation on alveolar development, we sought to study whether early gestational exposure to endotoxin affects branching morphogenesis, during the critical pseudoglandular stage of lung development.
               
                  Method
                  Gestational day 15 (E15) fetal rat lung explants (term = 22 d) were treated with either 200 ng/mL or 2 μg/mL lipopolysaccharides (LPS) with controls and examined daily by phase microscopy. After 5 d, explants were fixed in 4% formaldehyde, paraffin embedded, and sectioned at 5 μm in the coronal plane. Immunohistochemical analysis was performed with platelet endothelial cell adhesion molecule (PECAM) to define endothelial cells, vascular endothelial growth factor (VEGF) to examine endothelial mitogenesis, and COX-2 antibodies as a marker for prostaglandin synthesis. Real-time PCR examined inducible nitric oxide synthase (iNOS), FGF9, FGF10, and FGFr2 gene expression. Air space fraction and airway epithelium were analyzed with Image J software.
               
                  Results
                  Phase contrast microscopy and hematoxylin-eosin histology revealed progressive, dose-related changes in air sac contraction and interstitial thickening. Compared with control E15 explants, day 5 explants incubated with high dose LPS demonstrated thickened and shrunken airway sacs with stunted branching and increased matrix deposition in interstitial areas. By immunohistochemical staining, COX-2 was quantitatively increased after high dose LPS exposure, while PECAM was reduced. VEGF expression was unaltered. LPS increased iNOS, but decreased FGF9, FGF10, and FGFr2 gene expression.
               
                  Conclusions
                  These data support evidence for an inflammatory effect of LPS on the early phase of lung development in the fetal rat, affecting branching morphogenesis during the pseudoglandular phase. Fetal endothelial cells are clearly affected, while COX-2 elevation suggests activation of an as yet undefined fetal pulmonary inflammatory cascade. We speculate that proinflammatory stimuli may ultimately lead to abnormal pulmonary development via fibroblastic growth factor (FGF)-directed mechanisms that affect epithelial-mesenchymal interaction and differentiation at a much earlier gestational age than was previously recognized.",space
10.1016/j.cmpb.2009.01.009,Journal,Computer Methods and Programs in Biomedicine,scopus,2009-08-01,sciencedirect,An event-driven distributed processing architecture for image-guided cardiac ablation therapy,https://api.elsevier.com/content/abstract/scopus_id/67349086267,"Medical imaging data is becoming increasing valuable in interventional medicine, not only for preoperative planning, but also for real-time guidance during clinical procedures. Three key components necessary for image-guided intervention are real-time tracking of the surgical instrument, aligning the real-world patient space with image-space, and creating a meaningful display that integrates the tracked instrument and patient data. Issues to consider when developing image-guided intervention systems include the communication scheme, the ability to distribute CPU intensive tasks, and flexibility to allow for new technologies. In this work, we have designed a communication architecture for use in image-guided catheter ablation therapy. Communication between the system components is through a database which contains an event queue and auxiliary data tables. The communication scheme is unique in that each system component is responsible for querying and responding to relevant events from the centralized database queue. An advantage of the architecture is the flexibility to add new system components without affecting existing software code. In addition, the architecture is intrinsically distributed, in that components can run on different CPU boxes, and even different operating systems. We refer to this Framework for Image-Guided Navigation using a Distributed Event-Driven Database in Real-Time as the FINDER architecture. This architecture has been implemented for the specific application of image-guided cardiac ablation therapy. We describe our prototype image-guidance system and demonstrate its functionality by emulating a cardiac ablation procedure with a patient-specific phantom. The proposed architecture, designed to be modular, flexible, and intuitive, is a key step towards our goal of developing a complete system for visualization and targeting in image-guided cardiac ablation procedures.",space
10.1016/j.neunet.2009.06.038,Journal,Neural Networks,scopus,2009-07-01,sciencedirect,Integrated feature and parameter optimization for an evolving spiking neural network: Exploring heterogeneous probabilistic models,https://api.elsevier.com/content/abstract/scopus_id/68149171764,"This study introduces a quantum-inspired spiking neural network (QiSNN) as an integrated connectionist system, in which the features and parameters of an evolving spiking neural network are optimized together with the use of a quantum-inspired evolutionary algorithm. We propose here a novel optimization method that uses different representations to explore the two search spaces: A binary representation for optimizing feature subsets and a continuous representation for evolving appropriate real-valued configurations of the spiking network. The properties and characteristics of the improved framework are studied on two different synthetic benchmark datasets. Results are compared to traditional methods, namely a multi-layer-perceptron and a naïve Bayesian classifier (NBC). A previously used real world ecological dataset on invasive species establishment prediction is revisited and new results are obtained and analyzed by an ecological expert. The proposed method results in a much faster convergence to an optimal solution (or a close to it), in a better accuracy, and in a more informative set of features selected.",space
10.1016/j.neunet.2009.06.043,Journal,Neural Networks,scopus,2009-07-01,sciencedirect,Neural networks with multiple general neuron models: A hybrid computational intelligence approach using Genetic Programming,https://api.elsevier.com/content/abstract/scopus_id/68149160210,"Classical neural networks are composed of neurons whose nature is determined by a certain function (the neuron model), usually pre-specified. In this paper, a type of neural network (NN-GP) is presented in which: (i) each neuron may have its own neuron model in the form of a general function, (ii) any layout (i.e network interconnection) is possible, and (iii) no bias nodes or weights are associated to the connections, neurons or layers. The general functions associated to a neuron are learned by searching a function space. They are not provided a priori, but are rather built as part of an Evolutionary Computation process based on Genetic Programming. The resulting network solutions are evaluated based on a fitness measure, which may, for example, be based on classification or regression errors. Two real-world examples are presented to illustrate the promising behaviour on classification problems via construction of a low-dimensional representation of a high-dimensional parameter space associated to the set of all network solutions.",space
10.1016/j.ins.2008.12.015,Journal,Information Sciences,scopus,2009-04-29,sciencedirect,"V-detector: An efficient negative selection algorithm with ""probably adequate"" detector coverage",https://api.elsevier.com/content/abstract/scopus_id/61449165664,"This paper describes an enhanced negative selection algorithm (NSA) called V-detector. Several key characteristics make this method a state-of-the-art advance in the decade-old NSA. First, individual-specific size (or matching threshold) of the detectors is utilized to maximize the anomaly coverage at little extra cost. Second, statistical estimation is integrated in the detector generation algorithm so the target coverage can be achieved with given probability. Furthermore, this algorithm is presented in a generic form based on the abstract concepts of data points and matching threshold. Hence it can be extended from the current real-valued implementation to other problem space with different distance measure, data/detector representation schemes, etc. By using one-shot process to generate the detector set, this algorithm is more efficient than strongly evolutionary approaches. It also includes the option to interpret the training data as a whole so the boundary between the self and nonself areas can be detected more distinctly. The discussion is focused on the features attributed to negative selection algorithms instead of combination with other strategies.",space
10.1016/j.advwatres.2009.01.001,Journal,Advances in Water Resources,scopus,2009-04-01,sciencedirect,Pumping optimization of coastal aquifers based on evolutionary algorithms and surrogate modular neural network models,https://api.elsevier.com/content/abstract/scopus_id/62349136438,"Pumping optimization of coastal aquifers involves complex numerical models. In problems with many decision variables, the computational burden for reaching the optimal solution can be excessive. Artificial Neural Networks (ANN) are flexible function approximators and have been used as surrogate models of complex numerical models in groundwater optimization. However, this approach is not practical in cases where the number of decision variables is large, because the required neural network structure can be very complex and difficult to train. The present study develops an optimization method based on modular neural networks, in which several small subnetwork modules, trained using a fast adaptive procedure, cooperate to solve a complex pumping optimization problem with many decision variables. The method utilizes the fact that salinity distribution in the aquifer, depends more on pumping from nearby wells rather than from distant ones. Each subnetwork predicts salinity in only one monitoring well, and is controlled by relatively few pumping wells falling within certain control distance from the monitoring well. While the initial control area is radial, its shape is adaptively improved using a Hermite interpolation procedure. The modular neural subnetworks are trained adaptively during optimization, and it is possible to retrain only the ones not performing well. As optimization progresses, the subnetworks are adapted to maximize performance near the current search space of the optimization algorithm. The modular neural subnetwork models are combined with an efficient optimization algorithm and are applied to a real coastal aquifer in the Greek island of Santorini. The numerical code SEAWAT was selected for solving the partial differential equations of flow and density dependent transport. The decision variables correspond to pumping rates from 34 wells. The modular subnetwork implementation resulted in significant reduction in CPU time and identified an even better solution than the original numerical model.",space
10.1016/j.commatsci.2008.04.030,Journal,Computational Materials Science,scopus,2009-03-01,sciencedirect,Hybrid intelligent approach for modeling and optimization of semiconductor devices and nanostructures,https://api.elsevier.com/content/abstract/scopus_id/59749102668,"In this work, we present a hybrid intelligent approach for parameter extraction and design optimization of semiconductor nanoscale devices and nanostructures. Based on evolutionary algorithms, numerical methods, neural network scheme and parallel computing technique, the optimization methodology is developed and successfully implemented. In the hybrid approach, an evolutionary algorithm, such as genetic algorithm or particle swarm optimization, firstly searches the entire problem space to get a set of roughly estimated solutions. The numerical method, such as Levenberg–Marquardt method, then performs a local optima search and sets the local optima as the suggested values for the genetic algorithm to perform further optimizations. Meanwhile, the neural network is applied to investigate the influence of parameters on the optimized functions which thus guides the evolutionary direction of genetic algorithm. For solving real world problems, all functional blocks are performed under a PC-based Linux cluster system with message-passing interface libraries. This hybrid intelligent approach has experimentally been implemented and validated for different applications in semiconductor nanodevices and nanostructures. For semiconductor nanodevice parameter extraction, this approach shows its capability to automatically extract a set of global parameters among sixteen 90nm complementary metal oxide semiconductor (CMOS) devices. Compared with the measured current–voltage (I–V) curves of fabricated CMOS samples, the optimized I–V results are within 3% of accuracy. The computational examinations including sensitivity, convergence property, and parallelization are discussed. For parameter extraction of organic light emitting diode (OLED), the approach also achieves good accuracy for red, green, blue OLEDs. For the third and fourth applications, optimal structure design of silicon photonic taper waveguide and photonic crystal are further advanced by integrating a simulation-based technique in the developed system. All of these experiments demonstrate interesting results and validate the optimization methodology. The concept of hybrid intelligent approach may benefit modeling and optimization in diverse science and engineering problems.",space
10.1016/j.eswa.2008.08.022,Journal,Expert Systems with Applications,scopus,2009-01-01,sciencedirect,Text feature selection using ant colony optimization,https://api.elsevier.com/content/abstract/scopus_id/58349085948,"Feature selection and feature extraction are the most important steps in classification systems. Feature selection is commonly used to reduce dimensionality of datasets with tens or hundreds of thousands of features which would be impossible to process further. One of the problems in which feature selection is essential is text categorization. A major problem of text categorization is the high dimensionality of the feature space; therefore, feature selection is the most important step in text categorization. At present there are many methods to deal with text feature selection. To improve the performance of text categorization, we present a novel feature selection algorithm that is based on ant colony optimization. Ant colony optimization algorithm is inspired by observation on real ants in their search for the shortest paths to food sources. Proposed algorithm is easily implemented and because of use of a simple classifier in that, its computational complexity is very low. The performance of proposed algorithm is compared to the performance of genetic algorithm, information gain and CHI on the task of feature selection in Reuters-21578 dataset. Simulation results on Reuters-21578 dataset show the superiority of the proposed algorithm.",space
10.1016/j.neucom.2008.02.008,Journal,Neurocomputing,scopus,2008-10-01,sciencedirect,Improve local tangent space alignment using various dimensional local coordinates,https://api.elsevier.com/content/abstract/scopus_id/56549124687,"In the past few years, the problem of nonlinear dimensionality reduction arises in many fields of information processing. The local tangent space alignment (LTSA) is one of the effective and efficient algorithms to perform nonlinear dimensionality reduction. It has a number of attractive features: simple geometric intuitions, straightforward implementation, and global optimization. However, LTSA may fail on the manifold with nonuniformly distributed noise or large curvatures. In this paper, LTSA is improved by introducing various dimensional local coordinates to represent the local geometry for each neighborhood. The modified LTSA (MLTSA) is much stable and theoretical analysis is given to show the improvement of MLTSA on noisy manifold. We also illustrate the effectiveness of our method on both synthetic and real-world data sets.",space
10.1016/j.micpro.2008.04.002,Journal,Microprocessors and Microsystems,scopus,2008-10-01,sciencedirect,A tunable high-performance architecture for enhancement of stream video captured under non-uniform lighting conditions,https://api.elsevier.com/content/abstract/scopus_id/54549122634,"A novel architecture for performing hue-saturation-value (HSV) domain enhancement of digital color images captured under non-uniform lighting conditions is proposed in this paper for video streaming applications. The approach promotes log-domain computation to eliminate all multiplications, divisions and exponentiations utilizing the compact high-speed logarithmic estimation modules. An optimized quadrant symmetric architecture is incorporated into the design of homomorphic filter for the enhancement of intensity value. Efficient modules are also presented for conversion between RGB and HSV color spaces with tunable H and S components in HSV for more flexible color rendering. The design is able to bring out details hidden in shadow regions of the image and preserve the bright parts with adjustable vividness and color shift for improvement of visual quality while maintaining its consistency. It is capable of producing 187.86 million outputs per second (MOPs) on Xilinx’s Virtex II XC2V2000-4ff896 field programmable gate array (FPGA) at a clock frequency of 187.86MHz. It can process over 179.1 (1024×1024) frames per second, which is very suitable for high definition videos, and consumes approximately 70.7% and 76.8% less hardware resource with 127% and 280% performance boost when compared to the designs with machine learning algorithm in [M.Z. Zhang, M.J. Seow, V.K. Asari, A high performance architecture for color image enhancement using a machine learning approach, International Journal of Computational Intelligence Research – Special Issue on Advances in Neural Networks 2(1) (2006) 40–47], and with separated dynamic and contrast enhancements in [H.T. Ngo, M.Z. Zhang, L. Tao, V.K. Asari, Design of a high performance architecture for real-time enhancement of video stream captured in extremely low lighting environment, International Journal of Embedded Systems: Special Issue on Media and Stream Processing, in press], respectively. This approach also provide 83.4 times performance gain with more consistent fidelity in the results compared to some DSP based implementations (256×256 frame size) [G.D. Hines, Z. Rahman, D.J. Jobson, G.A. Woodell, DSP implementation of the retinex image enhancement algorithm, visual information processing XIII, in: Proceedings of the SPIE, vol. 5438, 2004, pp. 13–24; G.D. Hines, Z. Rahman, D.J. Jobson, G.A. Woodell, Single-scale retinex using digital signal processors, in: Proceedings of the Global Signal Processing Conference, September 2004, pp. 1–6] under the reflectance-illuminance category of image enhancement models.",space
10.1016/j.neunet.2007.12.009,Journal,Neural Networks,scopus,2008-03-01,sciencedirect,Compact hardware liquid state machines on FPGA for real-time speech recognition,https://api.elsevier.com/content/abstract/scopus_id/40649092298,"Hardware implementations of Spiking Neural Networks are numerous because they are well suited for implementation in digital and analog hardware, and outperform classic neural networks. This work presents an application driven digital hardware exploration where we implement real-time, isolated digit speech recognition using a Liquid State Machine. The Liquid State Machine is a recurrent neural network of spiking neurons where only the output layer is trained. First we test two existing hardware architectures which we improve and extend, but that appears to be too fast and thus area consuming for this application. Next, we present a scalable, serialized architecture that allows a very compact implementation of spiking neural networks that is still fast enough for real-time processing. All architectures support leaky integrate-and-fire membranes with exponential synaptic models. This work shows that there is actually a large hardware design space of Spiking Neural Network hardware that can be explored. Existing architectures have only spanned part of it.",space
10.1016/j.apacoust.2007.05.007,Journal,Applied Acoustics,scopus,2008-02-01,sciencedirect,HRTF personalization based on artificial neural network in individual virtual auditory space,https://api.elsevier.com/content/abstract/scopus_id/39149101013,"The synthesis of individual virtual auditory space (VAS) is an important and challenging task in virtual reality. One of the key factors for individual VAS is to obtain a set of individual head related transfer functions (HRTFs). A customization method based on back-propagation (BP) artificial neural network (ANN) is proposed to obtain an individual HRTF without complex measurement. The inputs of the neural network are the anthropometric parameters chosen by correlation analysis and the outputs are the characteristic parameters of HRTFs together with the interaural time difference (ITD). Objective simulation experiments and subjective sound localization experiments are implemented to evaluate the performance of the proposed method. Experiments show that the estimated non-individual HRTF has small mean square error, and has similar perception effect to the corresponding one obtained from the database. Furthermore, the localization accuracy of personalized HRTF is increased compared to the non-individual HRTF.",space
10.1016/j.bspc.2007.09.002,Journal,Biomedical Signal Processing and Control,scopus,2008-01-01,sciencedirect,Multi-channel surface EMG classification using support vector machines and signal-based wavelet optimization,https://api.elsevier.com/content/abstract/scopus_id/41149127373,"The study proposes a method for supervised classification of multi-channel surface electromyographic signals with the aim of controlling myoelectric prostheses. The representation space is based on the discrete wavelet transform (DWT) of each recorded EMG signal using unconstrained parameterization of the mother wavelet. The classification is performed with a support vector machine (SVM) approach in a multi-channel representation space. The mother wavelet is optimized with the criterion of minimum classification error, as estimated from the learning signal set. The method was applied to the classification of six hand movements with recording of the surface EMG from eight locations over the forearm. Misclassification rate in six subjects using the eight channels was (mean±S.D.) 4.7±3.7% with the proposed approach while it was 11.1±10.0% without wavelet optimization (Daubechies wavelet). The DWT and SVM can be implemented with fast algorithms, thus, the method is suitable for real-time implementation.",space
10.1016/j.actaastro.2006.12.021,Journal,Acta Astronautica,scopus,2008-01-01,sciencedirect,Abort determination with non-adaptive neural networks for the Mars precision landers,https://api.elsevier.com/content/abstract/scopus_id/36049019599,"The 2009 Mars Science Laboratory (MSL) will attempt the first precision landing on Mars using a modified version of the Apollo Earth entry guidance program. The guidance routine, Entry Terminal Point Controller (ETPC), commands the deployment of a supersonic parachute while converging the range to the landing target. For very dispersed cases, ETPC is unlikely to converge the range to the target and command parachute deployment within Mach number and dynamic pressure constraints. A full-lift-up abort can save 85% of these failed trajectories while abandoning the precision landing objective. In order to implement an abort, a failed trajectory needs to be recognized in real time. The application of artificial neural networks (ANNs) as an abort determination technique was evaluated. An ANN was designed, trained and tested using Monte Carlo simulations of MSL descent for a severe dust storm scenario. When incorporated into ETPC, the ANN correctly classifies 87% of descent trajectories as abort or non-abort, reducing the probability of losing MSL in a severe dust storm from 18% to 3.5%. This research shows that ANNs are capable of recognizing failed descent trajectories and can significantly increase the survivability of MSL for very dispersed cases.",space
10.1016/j.jsv.2007.06.028,Journal,Journal of Sound and Vibration,scopus,2007-11-06,sciencedirect,MIMO adaptive vibration control of smart structures with quickly varying parameters: Neural networks vs classical control approach,https://api.elsevier.com/content/abstract/scopus_id/34548564073,"This paper presents experimental adaptive identification and control of smart structures using neural networks based on system classification technique. An inverted L-structure with surface-bonded piezoceramic sensors/actuators is used for analysis. The state space, as well as matrix fraction description presentation, from control input voltages to output sensor voltage, is established in multivariable form. It is observed that the computational time required for online parameter identification and controller design is generally quite high. For the system, whose parameters change abruptly with large amplitudes, classical adaptive control techniques give poor transient behavior and sometimes instability. Also, for obtaining the ideal closed-loop performance, linear quadratic regulator cannot be re-designed in real-time for changed parameters of the smart structures, even if these parameters are identified in real time. Closed-loop identification of system parameters and control gains using system classification-based neural networks is proposed and implemented. A preliminary experimental study is also done to see the effectiveness of the proposed technique over classical control methods.",space
10.1016/j.enganabound.2006.12.003,Journal,Engineering Analysis with Boundary Elements,scopus,2007-08-01,sciencedirect,Principal component analysis and artificial neural network approach to electrical impedance tomography problems approximated by multi-region boundary element method,https://api.elsevier.com/content/abstract/scopus_id/34347221886,"The idea of electrical impedance tomography (EIT) is to evaluate conductivity or permittivity distribution inside the examined object by measuring the voltages between electrodes placed on its surface. In this paper, EIT as a default 3D diagnostic method of the breast cancer is suggested. The breast was modelled as a hemisphere consists of two spatially homogenous areas with different conductivity. In order to determine the distribution of potential in the breast model, a multi-region boundary element method (BEM) was implemented. In this paper, a multi-region BEM with quadratic interpolation function for the flat, triangular surface elements was introduced. The inverse problem solution provided the identification of the size and the position of the anomalies in the breast tissue. For this purpose the efficient method based on principal component analysis (PCA) and the artificial neural network (ANN) was used. PCA applied to EIT data allows reducing dimensionality of measured data for 3D space and removing the unused part of information, usually corresponding to noise and interrelated variables. ANN method allows to obtain the results of inverse problem solution in real-time.",space
10.1016/j.measurement.2006.09.004,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2007-07-01,sciencedirect,Neuro-fuzzy state modeling of flexible robotic arm employing dynamically varying cognitive and social component based PSO,https://api.elsevier.com/content/abstract/scopus_id/34249704095,The present paper proposes the development of a neuro-fuzzy state-space model for flexible robotic arm on the basis of real sensor data acquired. The training problem of the neuro-fuzzy architecture has been configured as a highly multidimensional stochastic global optimization problem and improved variants of particle swarm optimization (PSO) techniques have been successfully implemented for it. The effects of dynamically varying the “cognitive” and the “social” components of the improved PSOs on the training performance have been studied in detail. The practical utility of such a model development procedure is aptly demonstrated by employing the best trained model to design a stable fuzzy state controller and implementing it in real life for the same flexible robotic arm.,space
10.3182/20070625-5-fr-2916.00025,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2007-01-01,sciencedirect,A smart approach to precision attitude maneuvers of spacecrafts,https://api.elsevier.com/content/abstract/scopus_id/79960998950,"A nonlinear adaptive approach is presented to achieve rest-to-rest attitude maneuvers for spacecrafts in the presence of parameter uncertainties and unknown disturbances. A nonlinear controller, designed on the principle of dynamic inversion achieves the goals for the nominal model but suffers performance degradation in the presence of off-nominal parameter values and unwanted inputs. To address this issue, a model-following neuro-adaptive control design is carried out by taking the help of neural networks. Due to the structured approach followed here, the adaptation is restricted to the momentum level equations. The adaptive technique presented is computationally nonintensive and hence can be implemented in real-time. Because of these features, this new approach is named as structured model-following adaptive real-time technique (SMART). From simulation studies, this SMART approach is found to be very effective in achieving precision attitude maneuvers in the presence of parameter uncertainties and unknown disturbances.",space
10.1016/j.buildenv.2005.07.008,Journal,Building and Environment,scopus,2006-12-01,sciencedirect,Quasi-adaptive fuzzy heating control of solar buildings,https://api.elsevier.com/content/abstract/scopus_id/33745977431,"Significant progress has been made on maximising passive solar heat gains to building spaces in winter. Control of the space heating in these applications is complicated due to the lagging influence of the useful solar heat gain coupled with the wide range of construction materials and heating system choices. Additionally, and in common with most building control applications, there is a need to develop control solutions that permit simple and transparent set-up and commissioning procedures. This paper addresses the development and testing of a quasi-adaptive fuzzy logic control method that addresses these issues. The controller is developed in two steps. A feed-forward neural network is used to predict the internal air temperature, in which a singular value decomposition (SVD) algorithm is used to remove the highly correlated data from the inputs of the neural network to reduce the network structure. The fuzzy controller is then designed to have two inputs: the first input being the error between the set-point temperature and the internal air temperature and the second the predicted future internal air temperature. The controller was implemented in real-time using a test cell with controlled ventilation and a modulating electric heating system. Results, compared with validated simulations of conventionally controlled heating, confirm that the proposed controller achieves superior tracking and reduced overheating when compared with the conventional method of control.",space
10.1016/j.patcog.2005.10.026,Journal,Pattern Recognition,scopus,2006-05-01,sciencedirect,RBF-based neurodynamic nearest neighbor classification in real pattern space,https://api.elsevier.com/content/abstract/scopus_id/33244467462,"Superposition of radial basis functions centered at given prototype patterns constitutes one of the most suitable energy forms for gradient systems that perform nearest neighbor classification with real-valued static prototypes. It is shown in this paper that a continuous-time dynamical neural network model, employing a radial basis function and a sigmoid multi-layer perceptron sub-networks, is capable of maximizing such an energy form locally, thus performing almost perfectly nearest neighbor classification, when initiated by a distorted pattern. The proposed design scheme allows for explicit representation of prototype patterns as network parameters, as well as augmenting additional or forgetting existing memory patterns. The dynamical classification scheme implemented by the network eliminates all comparisons, which are the vital steps of the conventional nearest neighbor classification process. The performance of the proposed network model is demonstrated on binary and gray-scale image reconstruction applications.",space
10.1016/j.neucom.2005.04.008,Journal,Neurocomputing,scopus,2006-03-01,sciencedirect,Cell assemblies for diagnostic problem-solving,https://api.elsevier.com/content/abstract/scopus_id/32544433164,"We describe a neuronal model for diagnostic problem-solving. This model which is inspired by cell assemblies gives some hints on how diagnostic problem-solving might actually be performed by the human brain. The diagnostic process is described by a deduction system that performs an abductive inference. The abductive inference itself is described by the verbal category theory. A mapping of a diagnostic problem into a diagnostic system represented by an associative memory with feedback connections is presented. The associative memory with feedback connections offers a self-contained architecture for the administration and representation of manifestations and disorders. This can be implemented efficiently on a serial computer, requiring low memory space and low computational costs. Because of these advantages, this model was chosen for the implementation of a real embedded diagnostic system for a wire bonder machine. The knowledge base of this system is composed of 350 rules, which are stored in 11 modules. These modules model the error behaviour of the microcontroller based units of the machine and are arranged in a taxonomy which corresponds to the hierarchical chains that describe the relationship between disorders and manifestations.",space
10.1016/j.asr.2005.12.004,Journal,Advances in Space Research,scopus,2006-01-01,sciencedirect,Analysis of Sileye-3/Alteino data with a neural network technique: Particle discrimination and energy reconstruction,https://api.elsevier.com/content/abstract/scopus_id/33744809796,"In this work, we present the data analysis of the Sileye-3/Alteino experiment with neural network technique. Sileye-3/Alteino is composed of two devices: the cosmic ray-advanced silicon telescope (an 8 plane, 32 strip silicon detector) and an electroencephalograph. It was placed on board the ISS on April the 27th 2002 to investigate on the Light Flash phenomenon and the radiation environment in space. We show the possibility of using neural networks as an useful tool for real-time data analysis. A feed-forward neural network (Multi-Layer Perceptron – MLP) has been implemented and trained (with Monte Carlo data) to perform on line particle identification for ions with Atomic Number (Z) ⩽8 and incident kinetic energy reconstruction for ions Z
                     >2. The result of the analysis of Sileye-3/Alteino real data with the neural network and the improvements over classical analysis techniques are discussed.",space
10.1016/j.agsy.2004.07.019,Journal,Agricultural Systems,scopus,2005-12-01,sciencedirect,Merging genomic control networks and soil-plant-atmosphere-continuum models,https://api.elsevier.com/content/abstract/scopus_id/27344439349,"Advances in genomic science make it desirable to include genomic controls in soil-plant-atmosphere-continuum (SPAC) models by methods proposed in this paper. Molecular genetic concepts suggest that a differential equation similar to ones used in neural networks can be used to model single-gene elements of larger systems. Natural modifications to the equation incorporate temperature dependency. Multi-gene components based on this element function as Boolean logic gates, linear arithmetic units, delays, differentiators, integrators, oscillators, coincidence detectors, and bi-stable devices. Related genetic circuitry from real organisms is shown. Genomic integration with SPAC models entails whole-plant modeling with realistic morphology. Plants are networks of parts, iterated in time and space under genetic control, that induce and modulate conservative SPAC mass/energy flows. Network developmental rules can be stated as Lindenmayer grammars whose symbols represent plant parts programmed as software objects. A structure is presented for simulators based on these concepts. The discussion argues that prior object-oriented plant modeling approaches (i) do not reflect how plants actually develop morphologically and (ii) may represent processes in tactically unwise ways at a time when genomics is advancing knowledge of process interactions. Finally, genomics and expanding computing power redefine concepts of model “simplicity” and “complexity” to favor increased realism.",space
10.1016/j.neunet.2005.06.029,Journal,Neural Networks,scopus,2005-07-01,sciencedirect,On-chip visual perception of motion: A bio-inspired connectionist model on FPGA,https://api.elsevier.com/content/abstract/scopus_id/27744451750,"Visual motion provides useful information to understand the dynamics of a scene to allow intelligent systems interact with their environment. Motion computation is usually restricted by real time requirements that need the design and implementation of specific hardware architectures. In this paper, the design of hardware architecture for a bio-inspired neural model for motion estimation is presented. The motion estimation is based on a strongly localized bio-inspired connectionist model with a particular adaptation of spatio-temporal Gabor-like filtering. The architecture is constituted by three main modules that perform spatial, temporal, and excitatory–inhibitory connectionist processing. The biomimetic architecture is modeled, simulated and validated in VHDL. The synthesis results on a Field Programmable Gate Array (FPGA) device show the potential achievement of real-time performance at an affordable silicon area.",space
10.1016/j.compind.2004.06.003,Journal,Computers in Industry,scopus,2005-02-01,sciencedirect,A solution to the unequal area facilities layout problem by genetic algorithm,https://api.elsevier.com/content/abstract/scopus_id/13444304124,"The majority of the issued facilities layout problems (FLPs) minimize the material handling cost and ignore other factors, such as area utilization, department shape and site shape size. These factors, however, might influence greatly the objective function and should give consideration. The research range of this paper is focus on the unequal areas department facilities layout problem, and implement analysis of variance (ANOVA) of statistics to find out the best site size of layout by genetic algorithm. The proposed module takes the minimum total layout cost (TLC) into account. TLC is an objective function combining material flow factor cost (MFFC), shape ratio factor (SRF) and area utilization factor (AUF). In addition, a rule-based of expert system is implemented to create space-filling curve for connecting each unequal area department to be continuously placed without disjoint (partition). In this manner, there is no gap between each unequal area department. The experimental results show that the proposed approach is more feasible in dealing with the facilities layout problems in the real world.",space
10.1016/j.acra.2004.09.012,Journal,Academic Radiology,scopus,2005-01-01,sciencedirect,Hidden Markov event sequence models: Toward unsupervised functional MRI brain mapping,https://api.elsevier.com/content/abstract/scopus_id/13244273665,"Rationale and objectives
                  Most methods used in functional MRI (fMRI) brain mapping require restrictive assumptions about the shape and timing of the fMRI signal in activated voxels. Consequently, fMRI data may be partially and misleadingly characterized, leading to suboptimal or invalid inference. To limit these assumptions and to capture the broad range of possible activation patterns, a novel statistical fMRI brain mapping method is proposed. It relies on hidden semi-Markov event sequence models (HSMESMs), a special class of hidden Markov models (HMMs) dedicated to the modeling and analysis of event-based random processes.
               
                  Materials and methods
                  Activation detection is formulated in terms of time coupling between (1) the observed sequence of hemodynamic response onset (HRO) events detected in the voxel’s fMRI signal and (2) the “hidden” sequence of task-induced neural activation onset (NAO) events underlying the HROs. Both event sequences are modeled within a single HSMESM. The resulting brain activation model is trained to automatically detect neural activity embedded in the input fMRI data set under analysis. The data sets considered in this article are threefold: synthetic epoch-related, real epoch-related (auditory lexical processing task), and real event-related (oddball detection task) fMRI data sets.
               
                  Results
                  
                     Synthetic data: Activation detection results demonstrate the superiority of the HSMESM mapping method with respect to a standard implementation of the statistical parametric mapping (SPM) approach. They are also very close, sometimes equivalent, to those obtained with an “ideal” implementation of SPM in which the activation patterns synthesized are reused for analysis. The HSMESM method appears clearly insensitive to timing variations of the hemodynamic response and exhibits low sensitivity to fluctuations of its shape (unsustained activation during task). Real epoch-related data: HSMESM activation detection results compete with those obtained with SPM, without requiring any prior definition of the expected activation patterns thanks to the unsupervised character of the HSMESM mapping approach. Along with activation maps, the method offers a wide range of additional fMRI analysis functionalities, including activation lag mapping, activation mode visualization, and hemodynamic response function analysis. Real event-related data: Activation detection results confirm and validate the overall strategy that consists in focusing the analysis on the transients, time-localized events that are the HROs.
               
                  Conclusion
                  All the experiments performed on synthetic and real fMRI data demonstrate the relevance of HSMESMs in fMRI brain mapping. In particular, the statistical character of these models, along with their learning and generalizing abilities are of particular interest when dealing with strong variabilities of the active fMRI signal across time, space, experiments, and subjects.",space
10.1016/j.envsoft.2003.10.003,Journal,Environmental Modelling and Software,scopus,2004-08-01,sciencedirect,Modelling SO<inf>2</inf> concentration at a point with statistical approaches,https://api.elsevier.com/content/abstract/scopus_id/3342982389,"In this paper, the results obtained by inter-comparing several statistical techniques for modelling SO2 concentration at a point such as neural networks, fuzzy logic, generalised additive techniques and other recently proposed statistical approaches are reported. The results of the inter-comparison are the fruits of collaboration between some of the partners of the APPETISE project funded under the Framework V Information Societies and Technologies (IST) programme. Two different cases for study were selected: the Siracusa industrial area, in Italy, where the pollution is dominated by industrial emissions and the Belfast urban area, in the UK, where domestic heating makes an important contribution. The different kinds of pollution (industrial/urban) and different locations of the areas considered make the results more general and interesting. In order to make the inter-comparison more objective, all the modellers considered the same datasets. Missing data in the original time series was filled by using appropriate techniques. The inter-comparison work was carried out on a rigorous basis according to the performance indices recommended by the European Topic Centre on Air and Climate Change (ETC/ACC). The targets for the implemented prediction models were defined according to the EC normative relating to limit values for sulphur dioxide. According to this normative, three different kinds of targets were considered namely daily mean values, daily maximum values and hourly mean values. The inter-compared models were tested on real cases of poor air quality. In the paper, the inter-compared techniques are ranked in terms of their capability to predict critical episodes. A ranking in terms of their predictability of the three different targets considered is also proposed. Several key issues are illustrated and discussed such as the role of input variable selection, the use of meteorological data, and the use of interpolated time series. Moreover, a novel approach referred to as the technique of balancing the training pattern set, which was successfully applied to improve the capability of ANN models to predict exceedences is introduced. The results show that there is no single modelling approach, which generates optimum results in terms of the full range of performance indices considered. In view of the implementation of a warning system for air quality control, approaches that are able to work better in the prediction of critical episodes must be preferred. Therefore, the artificial neural network prediction models can be recommended for this purpose. The best forecasts were achieved for daily averages of SO2 while daily maximum and hourly mean values are difficult to predict with acceptable accuracy.",space
10.1016/j.nima.2004.01.052,Journal,"Nuclear Instruments and Methods in Physics Research, Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",scopus,2004-05-21,sciencedirect,A neural network device for on-line particle identification in cosmic ray experiments,https://api.elsevier.com/content/abstract/scopus_id/2342492426,"On-line particle identification is one of the main goals of many experiments in space both for rare event studies and for optimizing measurements along the orbital trajectory. Neural networks can be a useful tool for signal processing and real time data analysis in such experiments. In this document we report on the performances of a programmable neural device which was developed in VLSI analog/digital technology. Neurons and synapses were accomplished by making use of Operational Transconductance Amplifier (OTA) structures. In this paper we report on the results of measurements performed in order to verify the agreement of the characteristic curves of each elementary cell with simulations and on the device performances obtained by implementing simple neural structures on the VLSI chip. A feed-forward neural network (Multi-Layer Perceptron, MLP) was implemented on the VLSI chip and trained to identify particles by processing the signals of two-dimensional position-sensitive Si detectors. The radiation monitoring device consisted of three double-sided silicon strip detectors. From the analysis of a set of simulated data it was found that the MLP implemented on the neural device gave results comparable with those obtained with the standard method of analysis confirming that the implemented neural network could be employed for real time particle identification.",space
10.1016/j.robot.2003.11.006,Journal,Robotics and Autonomous Systems,scopus,2004-02-29,sciencedirect,A reinforcement learning with evolutionary state recruitment strategy for autonomous mobile robots control,https://api.elsevier.com/content/abstract/scopus_id/0742289960,"In recent robotics fields, much attention has been focused on utilizing reinforcement learning (RL) for designing robot controllers, since environments where the robots will be situated in should be unpredictable for human designers in advance. However there exist some difficulties. One of them is well known as ‘curse of dimensionality problem’. Thus, in order to adopt RL for complicated systems, not only ‘adaptability’ but also ‘computational efficiencies’ should be taken into account. The paper proposes an adaptive state recruitment strategy for NGnet-based actor-critic RL. The strategy enables the learning system to rearrange/divide its state space gradually according to the task complexity and the progress of learning. Some simulation results and real robot implementations show the validity of the method.",space
10.1016/S0169-4332(03)00919-X,Journal,Applied Surface Science,scopus,2004-02-15,sciencedirect,Application of a genetic algorithm and a neural network for the discovery and optimization of new solid catalytic materials,https://api.elsevier.com/content/abstract/scopus_id/0346304811,In the process of discovering new catalytic compositions by combinatorial methods in heterogeneous catalysis usually various potential catalytic compounds have to be prepared and tested. To decrease the number of necessary experiments an optimization algorithm based on a genetic algorithm for deriving subsequent generations from the performance of the members of the preceding generation is described. This procedure is supplemented by using an artificial neural network for establishing relationships between catalyst compositions—or more general speaking—materials properties and their catalytic performance. By combining a trained neural network with the genetic algorithm software virtually computer experiments were done aiming at adjusting the control parameters of the optimization algorithm to the special requirement of catalyst development. The approach is illustrated by the search for new catalytic compositions for the oxidative dehydrogenation of propane.,space
10.1016/j.advengsoft.2004.04.002,Journal,Advances in Engineering Software,scopus,2004-01-01,sciencedirect,Intelligent flight support system (IFSS): A real-time intelligent decision support system for future manned spaceflight operations at Mission Control Center,https://api.elsevier.com/content/abstract/scopus_id/2942601776,"The Mission Control Center Systems (MCCS) is a functionally robust set of distributed systems primarily supporting the Space Shuttle Program (SSP) and the International Space Station (ISS) mission operations. Forged around the uniquely complex and demanding requirements of human spaceflight, the MCCS has evolved within the limits of the technological capabilities of the time. The dynamic environment in which MCCS functions has demanded that the systems architecture continue to evolve as well.
                  The MCCS provides the primary means of controlling crewed spacecraft operated by NASA. Flight controllers (FCs) monitor the spacecraft systems through telemetry sent from the spacecraft to the ground and from the ground to the vehicle. FCs utilize several application software to present telemetry data in a variety of output presentations. While most displays simply provide a densely packed screen of telemetry data, only a few provide graphical representations of the vehicle systems' status. New technological advances in user interface design have not penetrated into MCC especially since the SSP and ISS systems were developed when these technologies were not available. The Intelligent Flight Support System (IFSS) described in this paper promotes situational awareness at MCC with an interactive virtual model of the ISS and Space Shuttle combined with data and decision support displays. IFSS also incorporates an intelligent component to model various characteristics of space vehicle systems when predictable results of unknown scenarios are required. IFSS supports FCs in the planning, communications, command, and control operations of the ISS and Space Shuttle by providing knowledge and skills that are unavailable from internal representation.",space
10.1016/j.advengsoft.2004.04.001,Journal,Advances in Engineering Software,scopus,2004-01-01,sciencedirect,Evolutionary optimization of energy systems using population graphing and neural networks,https://api.elsevier.com/content/abstract/scopus_id/2942546504,"This paper examines the simultaneous use of graph based evolutionary algorithms (GBEAs) and a real-time estimate of the final fitness for evolutionary optimization of systems modeled using computational fluid dynamics (CFDs). GBEAs are used to control the rate at which information travels, enabling the diversity of the population to be tuned to match the solution space. During each fitness evaluation, the CFD solver iteratively solves the fluid flow and heat transfer characteristics of the proposed design. In this paper, an artificial neural network is used to develop a real-time estimate of the final fitness and error bounds at each iteration of the solver. Using these estimates, the evolutionary algorithm can determine when the fitness of the design is known with sufficient accuracy for the evolutionary process. This significantly reduces the overall compute time. These techniques are demonstrated by optimizing the spatial temperature profile of the cooking surface of a biomass cookstove. In this cookstove, hot gases from biomass combustion flow under the cooking surface. Within this flow area, a set of baffles direct the flow of hot gases and establish the spatial temperature profile of the stove's cooking surface. The location and size of a series of baffles within the hot gas flow area are determined by the optimization routine. In this design problem, it is found that the two techniques are compatible; both the number of fitness evaluations and the time required for each CFD fitness evaluation are reduced while utilizing GBEAs to preserve the diversity of the population.",space
10.1016/S0004-3702(03)00078-X,Journal,Artificial Intelligence,scopus,2003-12-01,sciencedirect,Enhancing disjunctive logic programming systems by SAT checkers,https://api.elsevier.com/content/abstract/scopus_id/0242365718,"Disjunctive logic programming (DLP) with stable model semantics is a powerful nonmonotonic formalism for knowledge representation and reasoning. Reasoning with DLP is harder than with normal (∨-free) logic programs, because stable model checking—deciding whether a given model is a stable model of a propositional DLP program—is co-NP-complete, while it is polynomial for normal logic programs.
                  This paper proposes a new transformation ΓM(P), which reduces stable model checking to UNSAT—i.e., to deciding whether a given CNF formula is unsatisfiable. The stability of a model M of a program 
                        P
                      thus can be verified by calling a Satisfiability Checker on the CNF formula ΓM(P). The transformation is parsimonious (i.e., no new symbol is added), and efficiently computable, as it runs in logarithmic space (and therefore in polynomial time). Moreover, the size of the generated CNF formula never exceeds the size of the input (and is usually much smaller). We complement this transformation with modular evaluation results, which allow for efficient handling of large real-world reasoning problems.
                  The proposed approach to stable model checking has been implemented in DLV—a state-of-the-art implementation of DLP. A number of experiments and benchmarks have been run using SATZ as Satisfiability checker. The results of the experiments are very positive and confirm the usefulness of our techniques.",space
10.1016/S0045-7906(01)00050-7,Journal,Computers and Electrical Engineering,scopus,2003-06-01,sciencedirect,Two-phase neural network based estimation of degree of insecurity of power system,https://api.elsevier.com/content/abstract/scopus_id/0037409373,"Increased loading and contingencies often lead to situations where the optimal power flow solution no longer remains within the secure region. In such situations there is a need of determining control actions to be taken quickly, as otherwise the system may become unstable. Hence it is important to quantify the degree of insecurity of the power system both in planning as well as at operational stages. The distance in parameter space between an insecure operating point and the closest point on feasible (secure) hyper-surface has been used as a measure of degree of insecurity. A method based on two-phase optimization neural network has been presented to compute the degree of insecurity and the voltages and angles at all the buses of the system corresponding to the closest secure point. Inclusion of security limits on power system variables assures a solution representing a secure system. When compared with conventional non-linear optimization techniques, the proposed neural network is superior, as it can be easily implemented using digital hardware and is highly suitable for real time implementation in energy management system.
                  The proposed method has been tested on IEEE 30-bus test system and a practical 75-bus Indian system. The results achieved are compared with results from a conventional method. Insecurity arising due to increase in load and contingencies has been considered in this work.",space
10.1016/S0378-7753(03)00309-4,Journal,Journal of Power Sources,scopus,2003-05-15,sciencedirect,Power supply quality improvement with a SOFC plant by neural-network-based control,https://api.elsevier.com/content/abstract/scopus_id/0038216724,"This paper demonstrates the potential of a solid-oxide fuel cell (SOFC) to perform functions other than the supply of real power to the grid. These additional functions however require the use of an inverter. The flux-vector control is used very effectively for the control of this inverter, where the space-vector pulsewidth modulation (SVM) is implemented by neural networks (NNs). The results presented in the paper show the effect of the fuel cell on the voltage at the sensitive load point. The performance of the fuel cell was found to be excellent.",space
10.1016/S0196-8904(02)00138-3,Journal,Energy Conversion and Management,scopus,2003-05-01,sciencedirect,A rotor position estimator for switched reluctance motors using CMAC,https://api.elsevier.com/content/abstract/scopus_id/0037400491,"This paper presents an approach to rotor position estimation in switched reluctance motors (SRMs) by using a cerebellum model articulation controller (CMAC). Previous research has shown that an artificial neural network (ANN) forms an efficient mapping structure through measurement of the flux linkages and currents for the phases. A CMAC is investigated in this paper in order to overcome the high computational power requirement problem that is encountered in a feedforward ANN based rotor position estimator. The CMAC structure does not contain neurons with activation functions, and all mathematical operations are performed without multiplication. These simplicities increase the throughput in real time implementation performed with conventional embedded controllers. However, the distributed memory structure of a CMAC requires more space. The issues involved in designing, training and implementing a CMAC are presented. In order to demonstrate the feasibility of the concept, a 20 kW, 6/4, three phase SRM is studied with training and evaluation data, which are obtained from a simulation program. A CMAC that is based on experimentally measured training and testing data for the same SRM is also used to demonstrate the promise of this approach.",space
10.1016/S0925-2312(02)00619-7,Journal,Neurocomputing,scopus,2003-04-01,sciencedirect,Pattern recognition using multilayer neural-genetic algorithm,https://api.elsevier.com/content/abstract/scopus_id/0037381137,"The genetic algorithm implemented with neural network to determine automatically the suitable network architecture and the set of parameters from a restricted region of space. The multilayer neural-genetic algorithm was applied in image processing for pattern recognition, and to determine the object orientation. The library to cover the views of object was build from real images of (10×10) pixels. Which is the smallest image size can be used in this algorithm to recognize the type of aircraft with its direction
                  The multilayer perecptron neural network integrated with the genetic algorithm, the result showed good optimization, by reducing the number of hidden nodes required to train the neural network (the number of epoch's reduced to less than 50%). One of the important results of the implemented algorithm is the reduction in the time required to train the neural network.",space
10.1016/S0893-6080(03)00015-7,Journal,Neural Networks,scopus,2003-01-01,sciencedirect,Improved system for object detection and star/galaxy classification via local subspace analysis,https://api.elsevier.com/content/abstract/scopus_id/0037380809,"The two traditional tasks of object detection and star/galaxy classification in astronomy can be automated by neural networks because the nature of the problems is that of pattern recognition. A typical existing system can be further improved by using one of the local Principal Component Analysis (PCA) models. Our analysis in the context of object detection and star/galaxy classification reveals that local PCA is not only superior to global PCA in feature extraction, but is also superior to gaussian mixture in clustering analysis. Unlike global PCA which performs PCA for the whole data set, local PCA applies PCA individually to each cluster of data. As a result, local PCA often outperforms global PCA for data of multi-modes. Moreover, since local PCA can effectively avoid the trouble of having to specify a large number of free elements of each covariance matrix of gaussian mixture, it can give a better description of local subspace structures of each cluster when applied on high dimensional data with small sample size. In this paper, the local PCA model proposed by Xu [IEEE Trans. Neural Networks 12 (2001) 822] under the general framework of Bayesian Ying Yang (BYY) normalization learning will be adopted. Endowed with the automatic model selection ability of BYY learning, the BYY normalization learning-based local PCA model can cope with those object detection and star/galaxy classification tasks with unknown model complexity. A detailed algorithm for implementation of the local PCA model will be proposed, and experimental results using both synthetic and real astronomical data will be demonstrated.",space
10.1016/S0925-2312(02)00450-2,Journal,Neurocomputing,scopus,2002-07-27,sciencedirect,A real-scale anatomical model of the dentate gyrus based on single cell reconstructions and 3D rendering of a brain atlas,https://api.elsevier.com/content/abstract/scopus_id/0036064039,"As a first step towards the creation of a cellular model of dentate gyrus (DG) anatomy, we distributed 1,000,000 digitized granule cells (gcs) in 3D in a virtual reality reconstruction of Swanson's brain atlas. DG coronal sections were assembled into 3D surfaces using implicit function generation. The resulting file included hilar, granular, and molecular boundaries. 20,000 replicas of each of 50 reconstructed gcs were added to the model by packing the somata in the appropriate layer and then radially orienting the dendritic tree axes. The model can be used to evaluate stereologic parameters such as dendritic overlap probability, space occupancy, and exposure to incoming fibers.",space
10.1016/S0165-0114(01)00034-3,Journal,Fuzzy Sets and Systems,scopus,2002-03-16,sciencedirect,A fast learning algorithm for parsimonious fuzzy neural systems,https://api.elsevier.com/content/abstract/scopus_id/0037117202,"In this paper, a novel learning algorithm for dynamic fuzzy neural networks based on extended radial basis function neural networks, which are functionally equivalent to Takagi–Sugeno–Kang fuzzy systems, is proposed. The algorithm comprises 4 parts: (1) criteria of rules generation; (2) allocation of premise parameters; (3) determination of consequent parameters and (4) pruning technology. The salient characteristics of the approach are: (1) a hierarchical on-line self-organizing learning paradigm is employed so that not only parameters can be adjusted, but also the determination of structure can be self-adaptive without partitioning the input space a priori; (2) fast learning speed can be achieved so that the system can be implemented in real time. Simulation studies and comprehensive comparisons with some other learning algorithms demonstrate that the proposed algorithm is superior in terms of simplicity of structure, learning efficiency and performance.",space
10.1016/S0950-7051(01)00151-4,Journal,Knowledge-Based Systems,scopus,2001-11-01,sciencedirect,Specifying fault tolerance in mission critical intelligent systems,https://api.elsevier.com/content/abstract/scopus_id/0035505448,"Real time intelligent systems are being increasingly used in mission critical applications in domains like military, aerospace, process control industry and medicine. Despite this vast potential, the major concern about deploying mission critical intelligent systems is their dependability. Dependability encompasses such notions as reliability, safety, security, maintainability and portability. A major concern about mission critical intelligent systems is their performance in the presence of failures. Intelligent systems are characterized by often non-existent, imprecise or rapidly changing specifications. This makes the task of characterizing an intelligent system's performance in the presence of failures much more difficult. In this paper, we characterize the failures that are likely in a mission critical intelligent system. We propose an extended I/O automata model to capture these failure specifications. We further demonstrate how these specifications can be realized in a real time expert system by structuring the knowledge base. This formalism can also be used to specify the fault tolerant properties of the underlying hardware and software over which the intelligent system resides. Thus we have an unified formalism to specify fault tolerance properties in hardware, system software and the intelligent system. This will enable us to reason about the performance of the entire system inclusive of all its components in an uniform manner.",space
10.1016/S0952-1976(01)00031-8,Journal,Engineering Applications of Artificial Intelligence,scopus,2001-10-01,sciencedirect,Reinforcement learning control of nonlinear multi-link system,https://api.elsevier.com/content/abstract/scopus_id/0035493967,"In this paper, the effects of basic parameters in reinforcement learning control such as eligibility, action and critic network constrained weights, system nonlinearities, gradient information, state-space partitioning, variance of exploration are studied in detail. It is attempted to increase feasibility for practical applications, implementation, learning efficiency, and enhance performance. Also, a novel adaptive grid algorithm is proposed to overcome the difficulty in partitioning the input space to achieve better performance. Reinforcement learning is applied for control of a nonlinear one and two-link robots. This problem dictates that the learning is performed on-line, based on a binary or real-valued reinforcement signal from a critic network, without knowing the system model or nonlinearity.",space
10.1016/S0921-8890(01)00113-0,Journal,Robotics and Autonomous Systems,scopus,2001-07-31,sciencedirect,Acquisition of stand-up behavior by a real robot using hierarchical reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/0035979437,"In this paper, we propose a hierarchical reinforcement learning architecture that realizes practical learning speed in real hardware control tasks. In order to enable learning in a practical number of trials, we introduce a low-dimensional representation of the state of the robot for higher-level planning. The upper level learns a discrete sequence of sub-goals in a low-dimensional state space for achieving the main goal of the task. The lower-level modules learn local trajectories in the original high-dimensional state space to achieve the sub-goal specified by the upper level.
                  We applied the hierarchical architecture to a three-link, two-joint robot for the task of learning to stand up by trial and error. The upper-level learning was implemented by Q-learning, while the lower-level learning was implemented by a continuous actor–critic method. The robot successfully learned to stand up within 750 trials in simulation and then in an additional 170 trials using real hardware. The effects of the setting of the search steps in the upper level and the use of a supplementary reward for achieving sub-goals are also tested in simulation.",space
10.1016/S0954-1810(01)00003-6,Journal,Artificial Intelligence in Engineering,scopus,2001-07-01,sciencedirect,A low-cost Internet-based telerobotic system for access to remote laboratories,https://api.elsevier.com/content/abstract/scopus_id/0035385318,"This paper presents an account of the design of a low-cost Internet-based teleoperation system implemented on China's Internet. Using a multimedia-rich human-computer interface, combining predictive displays and graphical overlays, a series of simple tasks were performed within a simulated space environment scenario. Internet clients anywhere can monitor the robotic workspace, talk with technicians, and control the Arm/Hand integrated system with 15DOF located in lab to perform tasks (such as grasping a vessel, pouring a liquid, and peg-in-hole assembling, etc.). Our main contributions are to establish a foundation for teleoperated science and engineering research, and we have addressed some issues involving the time-delay associated with the Internet. We also developed several key software adaptation technologies and products used for Internet-Based teleoperation, compatible with the BH-III dexterous hand, BH1 6-DOF mechanical arm and five-finger 11-DOF data glove, constructed in our laboratory. This system has been successfully tested and applied in remote robotic education (Virtual Laboratories) system via China's Internet using our Master/Slave architecture, which combines mixed modes of remote monitor/manipulate and local autonomous control.",space
10.1016/S0020-0255(01)00149-9,Journal,Information Sciences,scopus,2001-01-01,sciencedirect,Learning fuzzy classifier systems for multi-agent coordination,https://api.elsevier.com/content/abstract/scopus_id/0035426729,"We present ELF, a learning fuzzy classifier system (LFCS), and its application to the field of Learning Autonomous Agents. In particular, we will show how this kind of Reinforcement Learning systems can be successfully applied to learn both behaviors and their coordination for Autonomous Agents. We will discuss the importance of knowledge representation approach based on fuzzy sets to reduce the search space without losing the required precision. Moreover, we will show how we have applied ELF to learn the distributed coordination among agents which can exchange information with each other. The experimental validation has been done on software agents interacting in a real-time task.",space
10.1016/S0965-9978(00)00031-4,Journal,Advances in engineering software,scopus,2000-01-01,sciencedirect,CORBA extension for intelligent software environments,https://api.elsevier.com/content/abstract/scopus_id/0343714795,"We describe the implementation of a technology that achieves system-wide properties in large software systems by controlling and modifying inter-component communications. Traditional component-based applications intermix the code for component functionality with support for systematic properties. This produces non-reusable components and inflexible systems. The Object Infrastructure Framework (OIF) separates systematic properties from functional code and provides a mechanism for weaving them together with functional components. This allows a much richer variety of component reuse and system evolution. Key elements of this technology include intercepting inter-component communications with discrete, dynamically configurable “injectors”, annotating communications and processes with additional meta-information, and a high-level, declarative specification language for describing the mapping between desired system properties and services that achieve these properties. We have implemented these ideas in a CORBA/Java framework for distributed computing, and are currently applying them to a distributed system for the analysis of aerospace design (wind-tunnel and CFD) data.",space
10.1016/S0893-6080(99)00102-1,Journal,Neural Networks,scopus,2000-01-01,sciencedirect,Partially pre-calculated weights for the backpropagation learning regime and high accuracy function mapping using continuous input RAM-based sigma-pi nets,https://api.elsevier.com/content/abstract/scopus_id/0033980299,"In this article we present a methodology that partially pre-calculates the weight updates of the backpropagation learning regime and obtains high accuracy function mapping. The paper shows how to implement neural units in a digital formulation which enables the weights to be quantised to 8-bits and the activations to 9-bits. A novel methodology is introduced to enable the accuracy of sigma–pi units to be increased by expanding their internal state space. We, also, introduce a novel means of implementing bit-streams in ring memories instead of utilising shift registers. The investigation utilises digital “Higher Order” sigma–pi nodes and studies continuous input RAM-based sigma–pi units. The units are trained with the backpropagation learning regime to learn functions to a high accuracy. The neural model is the sigma–pi units which can be implemented in digital microelectronic technology.
                  The ability to perform tasks that require the input of real-valued information, is one of the central requirements of any cognitive system that utilises artificial neural network methodologies. In this article we present recent research which investigates a technique that can be used for mapping accurate real-valued functions to RAM-nets. One of our goals was to achieve accuracies of better than 1% for target output functions in the range Y∈[0,1], this is equivalent to an average Mean Square Error (MSE) over all training vectors of 0.0001 or an error modulus of 0.01. We present a development of the sigma–pi node which enables the provision of high accuracy outputs. The sigma–pi neural model was initially developed by Gurney (Learning in nets of structured hypercubes. PhD Thesis, Department of Electrical Engineering, Brunel University, Middlessex, UK, 1989; available as Technical Memo CN/R/144). Gurney's neuron models, the Time Integration Node (TIN), utilises an activation that was derived from a bit-stream. In this article we present a new methodology for storing sigma–pi node's activations as single values which are averages.
                  In the course of the article we state what we define as a real number; how we represent real numbers and input of continuous values in our neural system. We show how to utilise the bounded quantised site-values (weights) of sigma–pi nodes to make training of these neurocomputing systems simple, using pre-calculated look-up tables to train the nets. In order to meet our accuracy goal, we introduce a means of increasing the bandwidth capability of sigma–pi units by expanding their internal state-space. In our implementation we utilise bit-streams when we calculate the real-valued outputs of the net. To simplify the hardware implementation of bit-streams we present a method of mapping them to RAM-based hardware using ‘ring memories’. Finally, we study the sigma–pi units’ ability to generalise once they are trained to map real-valued, high accuracy, continuous functions. We use sigma–pi units as they have been shown to have shorter training times than their analogue counterparts and can also overcome some of the drawbacks of semi-linear units (Gurney, 1992. Neural Networks, 5, 289–303).",space
10.1016/S0030-4018(99)00359-4,Journal,Optics Communications,scopus,1999-09-15,sciencedirect,Vector-product Hopfield model,https://api.elsevier.com/content/abstract/scopus_id/0033344055,"For pattern recognition, we are frequently faced with the problem of recognition for a real-world three-dimensional object. The mathematical vector-product algorithm in three-dimensional space is introduced into the neural network domain, and a new type of neural network model — vector-product Hopfield model — is proposed. Computer simulations show that the vector-product Hopfield model can recall the entire stored three-dimensional vectors at a high recognition rate with the partial information of the stored vectors in one or two dimensions only. Such a performance cannot be realized with the Hopfield model by simply presenting only one-third or two-thirds of the stored vectors. Thus, the proposed model is highly interesting for further developments of neural network models and practical applications. Preliminary optical experimental implementation is also given.",space
10.1016/S0031-3203(98)00159-9,Journal,Pattern Recognition,scopus,1999-07-01,sciencedirect,Curvature scale-space-driven object recognition with an indexing scheme based on artificial neural networks,https://api.elsevier.com/content/abstract/scopus_id/0033167353,"This paper addresses the problem of recognizing real flat objects from two-dimensional images. In particular, a new object recognition technique which performs under occlusion and geometric transformations is presented. The method has mainly been designed to handle complex objects and incorporates two main ideas. First, matching operates hierarchically, guided by a curvature scale space segmentation scheme, and takes advantage of important object features, that is, features which distinguish an object from other objects. This is different from many classical approaches which employ a rather large number of very local features. Second, the model database is built by using artificial neural networks (ANNs). This is also different from traditional approaches where classical indexing schemes, such as hashing, are utilized to organize and search the model database. Important object features are obtained in two steps: first, by segmenting the object boundary at multiple scales using its resampled curvature scale space (RCSS) and second, by concentrating at each scale separately, searching for groups of segments which distinguish an object from other objects. These groups of segments are then used to build a model database which stores associations between segments and models. The model database is implemented using a set of ANNs which provide the essential mechanism not only for establishing correct associations between groups of segments and models but also for enabling efficient searching and robust retrieval. The method has been tested using both artificial and real data illustrating good performance.",space
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",space
10.1016/S0273-1177(99)00318-X,Journal,Advances in Space Research,scopus,1999-01-01,sciencedirect,Development of autonomous control in a closed microbial bioreactor,https://api.elsevier.com/content/abstract/scopus_id/0033374124,"Space-based life support systems which include ecological components will rely on sophisticated hardware and software to monitor and control key system parameters. Autonomous closed artificial ecosystems are useful for research in numerous fields. We are developing a bioreactor designed to study both microbe-environment interactions and autonomous control systems. Currently we are investigating N-cycling and N-mass balance in closed microbial systems. The design features of the system involve real-time monitoring of physical parameters (e.g. temperature, light), growth solution composition (e.g. pH, NOx, CO2), cell density and the status of important hardware components. Control of key system parameters is achieved by incorporation of artificial intelligence software tools that permit autonomous decision-making by the instrument. These developments provide a valuable research tool for terrestrial microbial ecology, as well as a testbed for implementation of artificial intelligence concepts. Autonomous instrumentation will be necessary for robust operation of space-based life support systems, and for use on robotic spacecraft. Sample data acquired from the system, important features of software components, and potential applications for terrestrial and space research will be presented.",space
10.1016/S0167-9236(99)00055-X,Journal,Decision Support Systems,scopus,1999-01-01,sciencedirect,Partitioning-based clustering for Web document categorization,https://api.elsevier.com/content/abstract/scopus_id/0033323923,"Clustering techniques have been used by many intelligent software agents in order to retrieve, filter, and categorize documents available on the World Wide Web. Clustering is also useful in extracting salient features of related Web documents to automatically formulate queries and search for other similar documents on the Web. Traditional clustering algorithms either use a priori knowledge of document structures to define a distance or similarity among these documents, or use probabilistic techniques such as Bayesian classification. Many of these traditional algorithms, however, falter when the dimensionality of the feature space becomes high relative to the size of the document space. In this paper, we introduce two new clustering algorithms that can effectively cluster documents, even in the presence of a very high dimensional feature space. These clustering techniques, which are based on generalizations of graph partitioning, do not require pre-specified ad hoc distance functions, and are capable of automatically discovering document similarities or associations. We conduct several experiments on real Web data using various feature selection heuristics, and compare our clustering schemes to standard distance-based techniques, such as hierarchical agglomeration clustering, and Bayesian classification methods, such as AutoClass.",space
10.1016/S0004-3702(99)00025-9,Journal,Artificial Intelligence,scopus,1999-01-01,sciencedirect,"Task decomposition, dynamic role assignment, and low-bandwidth communication for real-time strategic teamwork",https://api.elsevier.com/content/abstract/scopus_id/0033149662,"Multi-agent domains consisting of teams of agents that need to collaborate in an adversarial environment offer challenging research opportunities. In this article, we introduce periodic team synchronization (PTS) domains as time-critical environments in which agents act autonomously with low communication, but in which they can periodically synchronize in a full-communication setting. The two main contributions of this article are a flexible team agent structure and a method for inter-agent communication. First, the team agent structure allows agents to capture and reason about team agreements. We achieve collaboration between agents through the introduction of formations. A formation decomposes the task space defining a set of roles. Homogeneous agents can flexibly switch roles within formations, and agents can change formations dynamically, according to pre-defined triggers to be evaluated at run-time. This flexibility increases the performance of the overall team. Our teamwork structure further includes pre-planning for frequently occurring situations. Second, the communication method is designed for use during the low-communication periods in PTS domains. It overcomes the obstacles to inter-agent communication in multi-agent environments with unreliable, single-channel, high-cost, low-bandwidth communication. We fully implemented both the flexible teamwork structure and the communication method in the domain of simulated robotic soccer, and conducted controlled empirical experiments to verify their effectiveness. In addition, our simulator team made it to the semi-finals of the RoboCup-97 competition, in which 29 teams participated. It achieved a total score of 67–9 over six different games, and successfully demonstrated its flexible teamwork structure and inter-agent communication.",space
10.1016/S0020-0255(98)10064-6,Journal,Information Sciences,scopus,1999-01-01,sciencedirect,A real-time system-adapted anomaly detector,https://api.elsevier.com/content/abstract/scopus_id/0033117079,"We present techniques for detecting anomalies in the performance of computer systems. We take both “performance” and “computer systems” quite generally. We assume that samples of performance data are available both in real-time and also historically from past normal performance of the target system. In addition we assume that there is some vector space and each sample is a point in that vector space. Detections are made by comparing the most recent sample of data with the historical data, thus yielding detections in real-time which are adaptive to the particular target system. Our techniques are probabilistic but are distribution-free and otherwise make relatively mild assumptions. We present some supporting mathematics that shows how to select the false alarm rate. We address detection rate with a general theorem and also with a specific simulation experiment. The data used can be of wide variety, possibly finely grained, and can be collected from hardware, system software, software subsystems, applications, or distributed applications. Our techniques may be of value in various aspects of future computer systems and in other systems as well.",space
10.1016/S0927-5452(98)80009-9,Book Series,Advances in Parallel Computing,scopus,1998-01-01,sciencedirect,"Application of a multi-processor system for recognition of EEG-activities in amplitude, time and space in real-time",https://api.elsevier.com/content/abstract/scopus_id/85023117179,"The EEG system BrainScope consists of a special amplifier system for high quality signal detection in open field conditions during communicative situations. A high performance multi-processor system which is capable of processing the huge amounts of data produced by a multichannel EEG record to gain information in real-time has also been developed. Algorithms for recognition of events in single channels are implemented in the first level of the multi-processor system. We use high performance image processing algorithms in the second level, interpreting the sampled values of each channel as pixels of the image, 256 up to 2.000 times per second. This patented method describes the EEG activity as sequences of virtual sources in parameters of amplitude, time and space. Fuzzy logic and methods of AI are used to define and recognise sequences of virtual sources in real-time.
                  The network of two or more Personal Computers (PC’s) is co-ordinated through the multiprocessor system for presentation of EEG activity and controlling. Multi-media approaches to the application of psychological tests are possible through the user interface including tests in media of sound, words, pictures and moving pictures. These tests can be arranged and carried out in computer controlled sequences and modified by user interactions. Tools are also provided to allow the user to create his own tests. These methods are integrated into the powerful graphic user interface and uses a database system. Incorporated into this user interface are state of the art EEGSYS algorithms from the NIMH (Washington / USA) for mappings, FFT, etc.
                  The BrainScope demonstrates the impacts and applications of the new strategy for EEG investigation in communicative situations between:
                        
                           -
                           patient and physician for subjective evaluation,
                        
                        
                           -
                           patient and information technology for stimulation and acquisition of signals and reactions,
                        
                        
                           -
                           physician and information technology for quantitative analysis of signals and reactions. The major advantage of this new strategy is that the three processes can be carried out in realtime. It optimises the capacity of humans to interpret information with the capability of modern information technology to manipulate and process data. It therefore requires use with an experienced and trained physician who can make accurate observations during the process of an investigation. The physician can, for example, click on a significant EEG pattern (this makes it a further recognisable phenomenon through fuzzy logic) and correlate it with his own observations. The multi-processor system recognises this EEG activity, i.e. it interprets this as a possible description of the state of the brain, sets a defined stimulus and recognises and evaluates the Event Related Potential (ERP) immediately.",space
10.1016/S0928-4869(97)00003-7,Journal,Simulation Practice and Theory,scopus,1997-10-15,sciencedirect,A virtual testbed for analysis and design of sensorimotoric aspects of agent control,https://api.elsevier.com/content/abstract/scopus_id/0042544183,"In this paper 
                        XRaptor
                      is introduced, an object-oriented simulation tool. It provides a virtual multi-agent world which acts as testbed for agent control mechanisms. This environment encompasses a 3-dimensional space, in which the agents may move. Currently agents are realized modelling some abstract properties of flies and bats. 
                        XRaptor
                      provides different levels of information flow and world manipulation capabilities from the agents' point of view. A further purpose of 
                        XRaptor
                      is educational: Different teams of developers may design control units for agents which can then be subjected to a tournament.",space
10.1016/S0893-6080(97)00029-4,Journal,Neural Networks,scopus,1997-08-01,sciencedirect,Associative list memory,https://api.elsevier.com/content/abstract/scopus_id/0031214817,"This paper introduces an Associative List Memory (ALM) that has high recall fidelity with low memory and low processing requirements. This permits a simple implementation in software on a personal computer or space instrument microprocessor. Associative List Memory has a performance comparable with Sparse Distributed Memory (SDM) but differs from SDM in that convergence occurs during learning, rather than on recall, and in that the memory is in the form of a dynamic list rather than static randomly distributed locations. Associative List Memory is suitable for unsupervised finding of classes of phenomena in large databases. In particular, all of the class exemplars deduced can be easily accessed at any time to provide a summary of current database knowledge, being essentially the contents of the list. Examples are given where patterns of 1000 bits length with > 30% noise can be learned unsupervised to deduce the original pattern's noise free. A second pass through the data in recall mode can be used to assign to each input the appropriate original pattern, effectively removing all noise from the input data. At large input bit sizes the recall fidelity approaches closely to the maximum possible value. Associative List Memory compares well in recall fidelity with SDM and other associative memories. Its processing times on a personal computer are found to be practical for database applications. Implemented within a space instrument processor, ALM would greatly reduce downlink data transmission rates. © 1997 Elsevier Science Ltd.",space
10.1006/jvlc.1997.0042,Journal,Journal of Visual Languages and Computing,scopus,1997-01-01,sciencedirect,"A parallel, distributed and associative approach for searching image patterns with holographic dynamics",https://api.elsevier.com/content/abstract/scopus_id/0031167601,"This paper presents a new associative pattern-matching network based on a digital adaptation of optical holography. Unlike any existing neural network or associative memories, it can localize its search dynamically on any subset of the pattern space and at the same time generate a feedback on the quality of the match. Current associative memories based on neuro computing are unable to support such meta-interactions. The scheme involves adaptive ‘enfolding’ of the raw massive search space into a holograph and direct regeneration of the matched target pattern during a search. The search process is a constant-time operation compared to traditional algorithm approaches, inherently parallelizable, and is an excellent candidate for hardware or optical implementation. This new technique is expected to facilitate significantly applications that require direct pattern matching in massive image repositories in real time.Target recognition,visual query,content-based image retrieveandautomatic index-extractionare just a few of such applications.",space
10.1016/s0952-1976(96)00084-x,Journal,Engineering Applications of Artificial Intelligence,scopus,1997-01-01,sciencedirect,Collision-free motion planning for redundant robots using neural-network processing,https://api.elsevier.com/content/abstract/scopus_id/0031124335,"A computationally efficient obstacle-avoidance algorithm for redundant robots is presented in this paper. The algorithm incorporates Tank-Hopfield networks and the J function in the framework of resolved motion rate control, which is well suited for real-time implementation. Robot-arm kinematic control is carried out by the TH network. The connection weights of the network can be directly obtained from the known matrices at each sampling time, and joint velocity commands are generated from the outputs of the network. The obstacle-avoidance task can be achieved by formulating the performance criterion J > J
                     
                        min
                      (J
                     
                        min
                      represents the minimal distance between the redundant robot and obstacles), and by using a null space to ensure that this criterion is satisfied. Several simulation cases for a four-link planar manipulator are given, to demonstrate how the proposed collision free trajectory planning scheme works.",space
10.1016/S0921-8890(96)00026-7,Journal,Robotics and Autonomous Systems,scopus,1997-01-01,sciencedirect,Efficient parallel processing for depth calculation using stereo,https://api.elsevier.com/content/abstract/scopus_id/0031121096,"Stereo vision generates the depth map of a scene by fusing information in images taken from two or more views. Stereo is a computationally intensive task and an efficient real time stereo application needs either a dedicated hardware or a parallel computer. This paper proposes a parallel stereo algorithm developed at CAIR using both edges and regions as features for matching. The algorithm employs an intelligent stereo matching technique that reduces the search space for correspondence thereby reducing the computational load. A load balancing scheme is also proposed for further improving the efficiency of the algorithm, with the increase in the number of nodal processors. The algorithm is implemented on a PACE parallel processor developed at the DRDO laboratory ANURAG at Hyderabad, India, and its capability is demonstrated through an application example.",space
10.1016/S0968-090X(97)00007-7,Journal,Transportation Research Part C: Emerging Technologies,scopus,1997-01-01,sciencedirect,A parallel algorithm to extract information about the motion of road traffic using image analysis,https://api.elsevier.com/content/abstract/scopus_id/0031120013,"Road traffic movement is a very important source of information in traffic management. Although systems exist which can detect the presence of a vehicle and its speed under certain conditions, there is generally a lack of effective means to measure both the speed and direction of traffic movement. This is particularly true for road junctions, where conflicting traffic shares the same space and where some control strategy could be more effectively applied with the help of speed and direction estimates. The increasing use of closed circuit television (CCTV) systems has provided the opportunity to apply image processing techniques to extract such information. However, such techniques are computationally intensive in general, and the application of parallel processing methods is one of the best choices which could bring the desired acquisition of movement information into practical reality. This paper describes a parallel video-based image analysis system which is capable of extracting movement information, including direction and speed, of road vehicular traffic over any part of a road surface. The prototype has been implemented on an array of 36 transputers and an image grabber with a SUN SPARC IPC as the host machine. The software mainly consists of median filtering, feature extraction, spatio-temporal analysis, matching of image features in successive images by neural networks and aggregation of matched results. This algorithm has been tested using data for a signal-controlled junction aiming to capture an opposed turning traffic movement with promising results. It has also been shown that a real-time system based on the described algorithm is feasible.",space
10.1016/S0360-1323(96)00043-1,Journal,Building and Environment,scopus,1997-01-01,sciencedirect,An artificial intelligence approach to the prediction of natural lighting levels,https://api.elsevier.com/content/abstract/scopus_id/0031095364,"Controlling artificial lights within buildings to act solely as a supplement to available daylighting requires continuous knowledge of natural lighting levels within a space. Although this information is readily obtained by measurement whilst lights are extinguished, once illuminated the determination of the underlying natural light level is not so straightforward. This paper describes the use of a Genetic Algorithm (GA) at the heart of a self-commissioning, adaptive algorithm capable of the real-time prediction of natural light levels at chosen points within a room using external measurements of vertical plane illuminance. The algorithm is extremely compact and efficient and readily implemented on a microprocessor. As such, it is suggested that it could form the basis of a robust and practical lighting controller.",space
10.1016/s0004-3702(96)00031-8,Journal,Artificial Intelligence,scopus,1997-01-01,sciencedirect,Permissive planning: Extending classical planning to uncertain task domains,https://api.elsevier.com/content/abstract/scopus_id/0030712509,"Uncertainty, inherent in most real-world domains, can cause failure of apparently sound classical plans. On the other hand, reasoning with representations that explicitly reflect uncertainty can engender significant, even prohibitive, additional computational costs. This paper contributes a novel approach to planning in uncertain domains. The approach is an extension of classical planning. Machine learning is employed to adjust planner bias in response to execution failures. Thus, the classical planner is conditioned towards producing plans that tend to work when executed in the world.
                  The planner's representations are simple and crisp; uncertainty is represented and reasoned about only during learning. The user-supplied domain theory is left intact. The operator definitions and the planner's projection ability remain as the domain expert intended them. Some structuring of the planner's bias space is required. But with suitable structuring the approach scales well. The learning converges using no more than a polynomial number of examples. The system then probabilistically guarantees that either the plans produced will achieve their goal when executed or that adequate planning is not possible with the domain theory provided. An implemented robotic system is described.",space
10.1016/0165-0114(95)00259-6,Journal,Fuzzy Sets and Systems,scopus,1996-01-01,sciencedirect,A prolog-like inference system based on neural logic - An attempt towards fuzzy neural logic programming,https://api.elsevier.com/content/abstract/scopus_id/0030576822,"Research under the name of Neural Logic Networks is an attempt to integrate connectionist models and logic reasoning [8, 9]. With a Neural Logic Network, a simple neural network structure with suitable weight(s) can be used to represent a set of flexible operations, which offer increased possibilities in dealing with inference in real-world problem solving. They also possess useful properties in an extended logic system which is called Neural Logic. One of the important features of Neural Logic is that all its operations can be defined and realized by neural networks, which form Neural Logic Networks. As one part of the research on Neural Logic Networks, fuzzy neural logic programming has been proposed [6]. This paper introduces a Prolog-like inference system based on Neural Logic as an implementation of fuzzy neural logic programming. In this system, fuzzy reasoning is executed by the Neural Logic inference engine with incomplete or uncertain knowledge. The framework of the system and its inference mechanism are described.",space
10.1016/0376-0421(95)00011-9,Journal,Progress in Aerospace Sciences,scopus,1996-01-01,sciencedirect,Neural networks: Applications and opportunities in aeronautics,https://api.elsevier.com/content/abstract/scopus_id/0030263707,"Technologies based on neural networks are currently being developed which may assist in addressing a wide range of complex problems in aeronautics. The review indicates that the proper utilization of this technology offers a feasible approach to help meet current and future technological needs. This might include, but is not limited to, the following: the implementation of active control devices to harness or suppress unsteady aerodynamic effects such as dynamic stall on helicopter rotor blades; the parallel data processing of 10s to 1000s of sensors either for actuation and control or for system and component ‘health’ monitoring and fault detection; the development of simulators and control algorithms for severe, ‘unsteady’, six degree-of-freedom vehicle maneuvers where the linearized equations of motion do not adequately describe the vehicle dynamics; and the requirement to monitor these sensors, simulate the maneuvers and instigate control all on a time-scale which must be faster than the real-time phenomena. Clearly, neural networks alone will not solve these problems. However, neural networks provide a practical approach for determining solutions of complex nonlinear problems such as equations of motion, for the parallel processing of 1000s of sensors, and this can be achieved with the required computational speed.",space
10.1016/0094-5765(95)00058-8,Journal,Acta Astronautica,scopus,1995-01-01,sciencedirect,"Concepts for automated, intelligent control of advanced μg-Facilities",https://api.elsevier.com/content/abstract/scopus_id/58149210228,"Manned space flights experienced large complexity, long preparation time and high costs. For this reason they are reduced and the operations for micro-g research are changing from crew control toward telescience and automated control. Telescience, however requires nearly permanent real-time data transmission —a condition that cannot be fulfilled by many missions. Then “intelligent and autonomous control” is the only way out. Autonomous supervisor, Artificial intelligence, Real-time video elaboration and Expert system are notions of new techniques coming up in ground based application where they help to find the optimum operational conditions or are used for very fast decisions counter-acting critical phases in complex systems. Today it is natural to ask for an analysis on how these applications can be used in the control of experiments in micro-g research. In the presently running study, called SEMIR and supported by ESA, the possibilities, the chances, but also the constraints of the intelligent control system are investigated. Their implementation in the field of crystal growth, protein crystallization, critical point phenomena and fluid physics is analyzed.
                  In this paper a preliminary output of this study is given.",space
10.1016/0952-1976(95)00044-5,Journal,Engineering Applications of Artificial Intelligence,scopus,1995-01-01,sciencedirect,"Dependable, intelligent voting for real-time control software",https://api.elsevier.com/content/abstract/scopus_id/0029491715,"An intelligent and dependable voting mechanism for use in real-time control applications is presented. Strategies proposed by current safety standards advocate N-version software to minimize the effects of undetected software design faults (bugs). This requires diversity in design but presents a problem in that truly diverse code produces diverse results; that is, differences in output values, timeliness and reliability. Reaching a consensus requires an intelligent voter, especially when non-stop operation is demanded, e.g. in aerospace applications. This paper, therefore, firstly considers the applicable safety standards and the requirements for an intelligent voter service. The use of replicated voters to improve reliability is examined and a mechanism to ensure non-stop operation is presented. The formal mathematical analysis used to verify the crucial behavioural properties of the voting service design is detailed. Finally, the use of neural nets and genetic algorithms to create N- version redundant voters, is considered.",space
10.1016/0004-3702(94)00089-J,Journal,Artificial Intelligence,scopus,1995-01-01,sciencedirect,CABINS: a framework of knowledge acquisition and iterative revision for schedule improvement and reactive repair,https://api.elsevier.com/content/abstract/scopus_id/0029332288,"Practical scheduling problems generally require allocation of resources in the presence of a large, diverse and typically conflicting set of constraints and optimization criteria. The ill-structuredness of both the solution space and the desired objectives make scheduling problems difficult to formalize. This paper describes a case-based learning method for acquiring context-dependent user optimization preferences and tradeoffs and using them to incrementally improve schedule quality in predictive scheduling and reactive schedule management in response to unexpected execution events. The approach, implemented in the CABINS system, uses acquired user preferences to dynamically modify search control to guide schedule improvement. During iterative repair, cases are exploited for: (1) repair action selection, (2) evaluation of intermediate repair results and (3) recovery from revision failures. The method allows the system to dynamically switch between repair heuristic actions, each of which operates with respect to a particular local view of the problem and offers selective repair advantages. Application of a repair action tunes the search procedure to the characteristics of the local repair problem. This is achieved by dynamic modification of the search control bias. There is no a priori characterization of the amount of modification that may be required by repair actions. However, initial experimental results show that the approach is able to (a) capture and effectively utilize user scheduling preferences that were not present in the scheduling model, (b) produce schedules with high quality, without unduly sacrificing efficiency in predictive schedule generation and reactive response to unpredictable execution events along a variety of criteria that have been recognized as important in real operating environments.",space
10.1016/0098-1354(94)E0034-K,Journal,Computers and Chemical Engineering,scopus,1995-01-01,sciencedirect,Building a chemical process design system within soar-1. Design issues,https://api.elsevier.com/content/abstract/scopus_id/0029236841,"We explore the potential to include automatic learning in a design agent by implementing a simple distillation sequencing system, CPD-Soar, within Soar. Soar is an integrated software architecture with a build-in set of mechanisms for exhibiting intelligent behavior, including problem-solving, learning and interaction with the environment. Soar has a number of scientific uses: computer scientists build artificially intelligent agents with Soar as a foundation, and cognitive psychologists use Soar to model human cognition. CPD-Soar illustrates how design-related tasks can be cast within Soar's framework, hence demonstrating the functioning and potential of its problem- solving and learning mechanisms. This simple example system, which involves computations with real numbers, automatically learns things which are too specific, leading to the hypothesis that the generalization an agent infers from specific examples is strongly dependent upon the model the agent brings to the learning process.
                  We introduced the Soar architecture as a vehicle for developing design systems with capabilities seen to be important for chemical process domains but missing in most existing design systems. We reported upon CPD-Soar, a system developed within the Soar framework, and described in depth its tasks, problem-space structure, operation and performance.
                  The construction of CPD-Soar was a valuable exercise for two main reasons: one, it provided evidence that the mechanisms present in Soar can provide design systems with useful abilities, and two, the act of creating the system was useful in distinguishing between those aspects of the task domain that are well understood from those that are not. Selecting among competent evaluation functions and learning within numerically-intensive domains were two areas identified as not being well understood.
                  Our observation of CPD-Soar's learning behavior lead us to postulate an hypothesis about learning: namely, that the richer the model an agent has of its evaluation functions, the more general its learning will be.",space
10.1006/dspr.1994.1016,Journal,Digital Signal Processing,scopus,1994-01-01,sciencedirect,Minimal topology for a radial basis functions neural network for pattern classification,https://api.elsevier.com/content/abstract/scopus_id/33747746017,"In the context of pattern classification, the success of a classification scheme often depends on the geometrical properties of the pattern classes under consideration. As radial basis functions (RBF) neural networks have largely been applied in pattern classification problems, in this paper we present a brief overview of different trends in radial basis functions neural networks and their applications. The meanings of the weights and the processing units for a RBF network applied for pattern classification are given. A new learning algorithm for a RBF neural network is proposed in this paper. This algorithm gives a solution for classifying configurations of patterns in a feature space providing the minimum number of hidden units for the network implementation. The learning is based on the backpropagation algorithm. The performance of the proposed algorithm is assessed on different artificial and real applications. The algorithm is successfully applied for estimating a distribution, as well as for separating signals in a multiple access communication system and for recognizing static speech.",space
10.1016/0004-3702(94)90104-X,Journal,Artificial Intelligence,scopus,1994-01-01,sciencedirect,Exploiting the deep structure of constraint problems,https://api.elsevier.com/content/abstract/scopus_id/0028529156,"We introduce a technique for analyzing the behavior of sophisticated AI search programs working on realistic, large-scale problems. This approach allows us to predict where, in a space of problem instances, the hardest problems are to be found and where the fluctuations in difficulty are greatest. Our key insight is to shift emphasis from modelling sophisticated algorithms directly to modelling a search space that captures their principal effects. We compare our model's predictions with actual data on real problems obtained independently and show that the agreement is quite good. By systematically relaxing our underlying modelling assumptions we identify their relative contribution to the remaining error and then remedy it. We also discuss further applications of our model and suggest how this type of analysis can be generalized to other kinds of AI problems.",space
10.1016/0004-3702(94)90097-3,Journal,Artificial Intelligence,scopus,1994-01-01,sciencedirect,Investigating production system representations for non-combinatorial match,https://api.elsevier.com/content/abstract/scopus_id/0028464184,"Eliminating combinatorics from the match in production systems (or rule-based systems) is important for expert systems, real-time performance, machine learning (particularly with respect to the utility issue), parallel implementations and cognitive modeling. In [74], the unique-attribute representation was introduced to eliminate combinatorics from the match. However, in so doing, unique-attributes engender a sufficiently negative set of trade-offs, so that investigating whether there are alternative representations that yield better trade-offs becomes of critical importance.
                  This article identifies two promising spaces of such alternatives, and explores a number of the alternatives within these spaces. The first space is generated from local syntactic restrictions on working memory. Within this space, unique-attributes is shown to be the best alternative possible. The second space comes from restrictions on the search performed during the match of individual productions (match-search). In particular, this space is derived from the combination of a new, more relaxed, match formulation (instantiationless match) and a set of restrictions derived from the constraint-satisfaction literature. Within this space, new alternatives are found that outperform unique-attributes in some, but not yet all, domains.",space
10.1016/0952-1976(94)90056-6,Journal,Engineering Applications of Artificial Intelligence,scopus,1994-01-01,sciencedirect,Verification of real-time programs by a knowledge-based strategy,https://api.elsevier.com/content/abstract/scopus_id/0028449013,"A verification system for real-time programs must provide a means of showing that the programs are logically correct and of proving that all timing constraints are met. To prove a program correct, according to traditional methods, all possible execution sequences (i.e. traces) must be shown to satisfy the specifications. That, however, will lead to the exponential explosion of the traces. This paper adopts an Artificial Intelligence technique to verify real-time programs, and proposes a method by which the real-time programs are proved just in a so-called “Reasonable Trace Space” (RTS). The RTS is a subset of the set of all traces (in short, the ATS) and is determined by a knowledge base which is provided by the program designer or a software verification expert. If a real-time program is proved correct in the RTS (which is much smaller than the ATS), it can be concluded that it is error free in the sense of the knowledge base. A knowledge-based trace-generation algorithm which enumerates all reasonable traces and a trace-verification algorithm which decides whether a trace is correct or not, are presented. Their performances are also analyzed.",space
10.1016/0005-1098(94)90154-6,Journal,Automatica,scopus,1994-01-01,sciencedirect,FCMAC: A fuzzified cerebellar model articulation controller with self-organizing capacity,https://api.elsevier.com/content/abstract/scopus_id/0028413077,"The Albus's Cerebellar Model Articulation Controller (CMAC) network has been used in many practical areas with considerable success. This paper presents a fuzzified CMAC network (FCMAC) acting as a multivariable adaptive controller with the feature of self-organizing association cells and the further ability of self-learning the required teacher signals in real-time. In particular, the original CMAC has been reformulated within a framework of a simplified fuzzy control algorithm (SFCA) and the associated self-learning algorithms have been developed as a result of incorporating the schemes of competitive learning and iterative learning control into the system. By using a similarity-measure-based, instead of coding-algorithm-based, content-addressable scheme, FCMAC is capable of dealing with arbitrary-dimensional continuous input space in a simple manner without involving complicated discretizing, quantizing, coding, and hashing procedures used in the original CMAC. The learning control system described here can be thought of as either a completely unsupervised fuzzy-neural control strategy without relying on the process model or equivalently an automatic real-time knowledge acquisition scheme for the implementation of fuzzy controllers. The proposed approach has been applied to a multivariable blood pressure control problem which is characterized by strong interaction between variables and large time delays.",space
10.1006/jpdc.1993.1023,Journal,Journal of Parallel and Distributed Computing,scopus,1993-01-01,sciencedirect,An adaptive opto-electronic neural network for associative pattern retrieval,https://api.elsevier.com/content/abstract/scopus_id/38249005040,"A novel adaptive trinary neural network model is proposed for associative pattern retrieval from incomplete data. Systematic analysis of the convergence mechanism for the character recognition problem is provided to illustrate the derivation of this novel adaptive thresholding scheme. The adaptive scheme with trinary input representation outperforms other associative retrieval schemes in terms of convergence and storage capacity. The inherent parallelism of this neural network architecture is exploited for a parallel optical implementation. The tremendous speed and free-space interconnection capability of optics results in a very efficient real-time character recognition system. The adaptive threshold scheme developed here may have far reaching implications for other neural networks in enhancing their learning and retrieval, speed, and accuracy.",space
10.1016/0003-682X(93)90050-G,Journal,Applied Acoustics,scopus,1993-01-01,sciencedirect,Modelling of room acoustics and loudspeakers in JBL's complex array design program CADP2,https://api.elsevier.com/content/abstract/scopus_id/0027242485,"This paper presents a survey of the models and principles that are used in JBL's second generation program CADP2 to predict the performance of complex multi-loudspeaker sound reinforcement systems in real spaces. The paper explains how the image model for polyhydral spaces, statistical room acoustics and complex polar patterns for loudspeakers are combined into time and frequency domain sound field models. The models are applied to predict accurate 
                        
                           1
                           1
                        
                      or 
                        
                           1
                           3
                        
                      octave band SPLs in frequency and space from complex loudspeaker arrays. The models are also the basis for the prediction of speech intelligibility (ALcons, AI, RASTI and C50/80) from sound reinforcement systems taking propagation and electronic delays, discrete reflections, reverberation and noise into account.",space
10.1016/b978-0-08-041275-7.50077-6,Conference Proceeding,IFAC Symposia Series,scopus,1992-01-01,sciencedirect,Real-time fault diagnosis for propulsion systems,https://api.elsevier.com/content/abstract/scopus_id/0027099165,"Current research toward real-time fault diagnosis for propulsion systems at the National Aeronautics and Space Administration's Lewis Research Center is described. The research is being applied to both airbreathing and rocket propulsion systems. Topics include fault detection methods including neural networks, system modeling, and real-time implementations.",space
10.1016/0893-6080(91)90062-A,Journal,Neural Networks,scopus,1991-01-01,sciencedirect,DEFAnet-A deterministic neural network concept for function approximation,https://api.elsevier.com/content/abstract/scopus_id/0026375514,"A deterministic neural network concept for a “universal approximator” is proposed. The network has two hidden layers; only the synapses of the output layer are required to be plastic and only those depend on the function to be approximated. It is shown that a DEterministic Function Approximation Network (DEFAnet) allows to approximate an arbitrary continuous function from the finite-dimensional unit interval into the finite-dimensional real space with arbitrary accuracy; arbitrary Boolean functions may be implemented exactly in a simple subset of DEFAnets. In a supervised learning scheme, convergence to the desired function is guaranteed; back propagation of errors is not required. The concept is also open for reinforcement learning. In addition, when the topology of the network is determined according to the DEFAnet concept, it is possible to calculate all plastic synaptic weights in closed form, thus reducing the training considerably or replacing it altogether. Efficient algorithms for the calculation of synapse weights are given.",space
10.1016/0004-3702(91)90113-X,Journal,Artificial Intelligence,scopus,1991-01-01,sciencedirect,Dynamic across-time measurement interpretation,https://api.elsevier.com/content/abstract/scopus_id/0026243270,"Incrementally maintaining a qualitative understanding of physical system behavior based on observations is crucial to tasks such as real-time control, monitoring, and diagnosis. This paper describes the DATMI theory for interpretation tasks. The key idea of DATMI is to dynamically maintain a concise representation of the space of local and global interpretations across time that are consistent with the observations. This representation has two key advantages. First, a set of possible interpretations is more useful than a single (best) candidate for many tasks, such as conservative monitoring. Second, this representation simplifies switching to alternative interpretations when data are faulty or incomplete. Domain-specific knowledge about state and transition probabilities can be used to suggest the interpretation which is most likely. Domain-specific knowledge about durations of states and paths of states can also be used to further constrain the interpretation space. When no consistent interpretation exists, faulty-data hypotheses are generated and then tested by adjusting the interpretation space. The DATMI theory has been tested via implementation and we describe its performance on two examples.",space
10.1016/0922-338X(90)90030-Z,Journal,Journal of Fermentation and Bioengineering,scopus,1990-01-01,sciencedirect,An expert approach for control of fermentation processes as variable structure plants,https://api.elsevier.com/content/abstract/scopus_id/0025102057,"Structural variability is a fundamental property of biological systems. Fermentation processes, in particular, are often accompanied by physiological phenomena which have the characteristics of structural alterations. The conventional control approach does not have the competence to handle such processes, because it is based on the general assumption for a single-structure plant. In order to overcome this limitation, we consider under some conditions fermentation processes as variable structure plants and propose a two-level hierarchical scheme for their control. At the higher hierarchical level, which performs the organizing functions and operates in the structural space of the plant, the current plant structure is recognized as an element of a finite set of structures defined on the basis of ‘deep’ expert knowledge of the cell physiology and/or ‘shallow’ expert knowledge of the behavioral characteristics of the microbial population. The recognition is based on fuzzy decomposition of the structural space of the plant into several subspaces, mapped isomorphically into a respective set of rational control strategies. A set of expert rules is used for the actual synthesis of the recognition procedure, which is finally tuned by supervised learning. The adopted methodology at this hierarchical level provides the system with a kind of intelligence. To the lower hierarchical level, two distinct functions are assigned. The first one performs the global task of control of the plant structure and operates in its structural space. The second function has a local character and consists of control of the plant in a particular subspace of constant structure. It works in the state space of the plant. Under supervision of the higher level, the control strategy relevant to the current plant structure is picked out from a predefined pool and the corresponding control action is calculated. Extensive use of expert knowledge in the conceptual formulation of proper control strategies is assumed. Compared with the conventional single-structure approach, the variable structure concept results in rational and physiologically motivated control of fermentation processes. Instead of the development of a single but complicated process model and control structure, construction of several simple control structures, often in a ‘model-less’ form, is required. This method naturally incorporates and theoretizes the widely accepted policy of artificial induction of structural transformations in the plant, which is essential for enhanced productivity in many modern fermentation technologies. The proposed approach has been realized as a real-time software system, provided with enough flexibility to be used with different fermentation processes and equipment. This system is orientated to IBM personal computers and works under the QNX multi-tasking real-time operating system.",space
10.1016/S0736-5853(89)80029-2,Journal,Telematics and Informatics,scopus,1989-01-01,sciencedirect,"Expert system development methodology and the transition from prototyping to operations: Fiesta, a case study",https://api.elsevier.com/content/abstract/scopus_id/0024911679,"A major barrier in taking expert systems from prototype to operational status involves instilling end user confidence in the operational system. End users want assurances that their systems have been thoroughly tested, meet all their specifications and requirements, and are built based on designs which are reliable and maintainable. For most software systems, the waterfall life cycle model can provide those assurances. However, this model is inappropriate for expert system development, where an iterative refinement approach is commonly employed. This paper will look at different life cycle models and explore the advantages and disadvantages of each when applied to expert system development. The Fault Isolation Expert System for TDRSS Applications (FIESTA) is presented as a case study example of development of an expert system. FIESTA is planned for use in the Network Control Center (NCC) at Goddard Space Flight Center in Greenbelt, Maryland. The end user confidence necessary for operational use of this system is accentuated by the fact that it will handle real-time data in a secure environment, allowing little tolerance for errors. The paper discusses how FIESTA is dealing with transition problems as it moves from an off-line standalone prototype to an on-line real-time system.",space
10.1016/S0736-5853(89)80026-7,Journal,Telematics and Informatics,scopus,1989-01-01,sciencedirect,Synthetic organisms and self-designing systems,https://api.elsevier.com/content/abstract/scopus_id/0024911678,"This paper examines the need for complex, adaptive solutions to certain types of complex problems typified by the Strategic Defense System and NASA's Space Station and Mars Rover. Since natural systems have evolved with capabilities of intelligent behavior in complex, dynamic situations, it is proposed that biological principles be identified and abstracted for application to certain problems now facing industry, defense, and space exploration. Two classes of artificial neural networks are presented — a nonadaptive network used as a genetically determined “retina,” and a frequency-coded network used as an adaptive “brain.” The role of a specific environment coupled with a system of artificial neural networks having simulated sensors and effectors is seen as an ecosystem. Evolution of synthetic organisms within this ecosystem provides a powerful optimization methodology for creating intelligent systems able to function successfully in any desired environment. A complex software system involving a simulation of an environment and a program designed to cope with that environment are presented. Reliance on adaptive systems, as found in nature, is only part of the proposed answer, though an essential one. The second part of the proposed method makes use of an additional biological metaphor—that of natural selection—to solve the dynamic optimization problems every intelligent system eventually faces. A third area of concern in developing an adaptive, intelligent system is that of real-time computing. It is recognized that many of the problems now being explored in this area have their parallels in biological organisms, and many of the performance issues facing artificial neural networks may find resolution in the methodology of real-time computing.",space
10.1016/S0736-5853(89)80027-9,Journal,Telematics and Informatics,scopus,1989-01-01,sciencedirect,Integration of perception and reasoning in fast neural modules,https://api.elsevier.com/content/abstract/scopus_id/0024904803,"Artificial neural systems promise to integrate symbolic and subsymbolic processing to achieve real-time control of physical systems. Two potential alternatives exist. In one, neural nets can be used to front-end expert systems. The expert systems, in turn, are developed with varying degrees of parallelism, including their implementation in neural nets. In the other, rule-based reasoning and sensor data can be integrated within a single hybrid neural system. The hybrid system reacts as a unit to provide decisions (problem solutions) based on the simultaneous evaluation of data and rules. This paper discusses a model hybrid system based on the fuzzy cognitive map (FCM). The operation of the model is illustrated with the control of a hypothetical satellite that intelligently alters its attitude in space in response to an intersecting micrometeorite shower.",space
10.1016/0094-5765(89)90052-0,Journal,Acta Astronautica,scopus,1989-01-01,sciencedirect,"Telescience and microgravity. Impact on future facilities, ground segments and operations",https://api.elsevier.com/content/abstract/scopus_id/0024856605,"Scientific activities related to experimentation in long duration microgravity missions can only be accomplished by the implementation of the Telescience Concept.
                  Telescience is in fact the logical answer to the need of an intelligent interactive conduct of experiments, to the lack (or very little availability) of crew time on board of the Segments of the Columbus project and to the PIs demand for decentralized operations. Telescience could also be seen as the preparative phase for the ultimate, future exploitation of Microgravity by means of Expert Systems that will utilize AI and Robotics for routine operations (Data Factories, Space Productions and Commercial Enterprises).
                  The implications of Telescience on future Space Activities is reviewed with reference to the Principal Investigator Activities, Crew Members Roles and Facilities. The possibilities offered by newly designed Facilities to be operated in Telescience are pointed out with reference to the scientific objectives that would not be achieved otherwise.
                  Diagnostic facilities (mainly non invasive) that provide digital measurements to be inputted (in real time) into numerical codes for computation of field parameters are being considered. Ground Segment Structure, User Support Centers Organization and Test Bedding activities will be discussed as essential factors of the Telescience Scenario of the Multiuser, permanent platform Facilities for the Microgravity disciplines (Material, Fluid, Life and Engineering Science).",space
10.1016/S0006-3495(88)83038-8,Journal,Biophysical Journal,scopus,1988-01-01,sciencedirect,Principles of odor coding and a neural network for odor discrimination,https://api.elsevier.com/content/abstract/scopus_id/0024272690,"A concept of olfactory coding is proposed. It describes the stimulus responses of all receptor cells by the use of vector spaces. The morphological convergence pattern between receptor cells and glomeruli is given in the same vector space as the receptor cell activities. The overall input of a glomerulus follows as the scalar product of the receptor cell activity vector and the vector of the glomerulus' convergence pattern. The proposed coding concept shows how the network of the olfactory bulb succeeds in discriminating odors with high selectivity. It is concluded that sets of mitral cells coding similar odors work very much in the way of mutually inhibited matched filters. This solves one main problem both in olfaction as well as real-time odor detection by an artificial nose, i.e., how the fairly low degree of selectivity of receptor cells or sensors is overcome by the neural network following the receptor stage. The formal description of olfactory coding suggests that quality perception which is invariant under concentration shifts is accomplished by an associative memory in the olfactory bulb.",space
10.1016/0167-739X(87)90034-3,Journal,Future Generation Computer Systems,scopus,1987-01-01,sciencedirect,DLM - a powerful ai computer for embedded expert systems,https://api.elsevier.com/content/abstract/scopus_id/0023575081,"There is an increasing need to operate computers in a real-time environment whereby the solution path of the task cannot be pre-determined. Such problems (eg intelligent process control, vision understanding, data fusion etc.) can be solved with an expert system approach that may necessitate interaction with procedural processes. Since expert systems are in general more effectively implemented in declarative languages (eg Lisp, Prolog, Hope), the requirement exists for a powerful AI Computer capable of executing declarative languages and, if necessary, directing the operation of real-time equipment.
                  The Declarative Language Machine (DLM) is a dedicated operational computer designed and built by Bristish Aerospace for this purpose and incorporates the following key features: 
                        
                           1.
                           (1) Prolog execution rate of 620 KLIPS (the DLM is believed to be the fastest AI computer in the world).
                        
                        
                           2.
                           (2) Direct support for real-time operation and interaction with real-time equipment.
                        
                        
                           3.
                           (3) Capability for the efficient execution of Prolog, Parlog, Hope and Lisp.
                        
                        
                           4.
                           (4) Capable of being a stand-alone computer or embedded within host equipment.
                        
                        
                           5.
                           (5) Floating-point co-processor for arithmetic operations.
                        
                     
                  
                  This paper describes the architecture of the DLM",space
10.1016/0031-3203(87)90023-9,Journal,Pattern Recognition,scopus,1987-01-01,sciencedirect,VLSI architectures for string matching and pattern matching,https://api.elsevier.com/content/abstract/scopus_id/0023207478,"In this paper, we discuss string-matching and dynamic time-warp pattern-matching. The string-matching problem arises in a number of applications such as in artificial intelligence, pattern recognition and information retrieval. The method of dynamic time-warping is a well-established technique for time alignment and comparison of speech and image patterns. It has found extensive application in speech recognition and related areas of pattern-matching.
                  We propose a VLSI architecture based on the space-time domain expansion approach which can compute the string distance and also give the matching index-pairs which correspond to the edit sequence. The time complexity is O(max(m, n)) by using m× n processing elements array, where m is the length of the input string and n is the length of the reference string. With a uniprocessor the matching process will have the time complexity O(m × n). If there are p reference strings, using the proposed architecture the string-matching problem can be solved in time O(max(m, n, p)). With a uniprocessor the time complexity will be O(m × n × p). We also propose a VLSI architecture for dynamic time-warping based on the space-time expansion method which can obtain high throughput by using extensive pipelining and parallelism. It can measure the dissimilarity between two patterns in time O(max(m, n, N)). If using a uniprocessor the time complexity will be O(m × n × N), where m and n are the numbers of feature vectors of the unknown input pattern and the reference template respectively, and N is the number of elements of the feature vector. If there are p reference templates, the time complexity will be O(max(m, n, N × p)), and if using a uniprocessor the time complexity will be O(m × n × p × N). The algorithm partition problems are discussed. Verifications of the proposed VLSI architectures are also given. The backtracking procedures are discussed in much detail and their hardware implementations are also given. The proposed architectures can be applied to many areas such as pattern recognition, information retrieval, image processing, speech processing, remote sensing, robotics, computer vision, artificial intelligence and office automata. They are useful to real-time information processing.",space
10.1016/0734-189X(83)90095-6,Journal,"Computer Vision, Graphics and Image Processing",scopus,1983-01-01,sciencedirect,An implementation of a computational theory of visual surface interpolation,https://api.elsevier.com/content/abstract/scopus_id/0020642298,"Computational theories of structure-from-motion (Ullman, The Interpretation of Visual Motion, MIT Press, 1979) and stereo vision (Marr and Poggio, Proc. R. Soc. London Ser. B 
                        204, 1979, 301–328) only specify the computation of three-dimensional surface information at particular points in the image. Yet, the visual perception is clearly of complete surfaces. To account for this, a computational theory of the interpolation of surfaces from visual information was presented in Grimson. (From Images to Surfaces: A Computational Study of the Human Early Visual System, MIT Press, 1981; 
                        A Computational Theory of Visual Surface Interpolation, MIT Artificial Intelligence Lab Memo, No. 613, 1981; and 
                        Philos. Trans. R. Soc. London Ser. B 
                        298, 1982, 395–427). The problem is constrained by the fact that the surface must agree with the information from stereo or motion correspondence, and not vary radically between these points. Using the image irradiance equation (Horn, MIT Project MAC Tech. Rep. MACTR-79, 1970; 
                        The Psychology of Computer Vision, McGraw-Hill, 1975; and 
                        Artif. Intell. 
                        8, 1977, 201–231), an explicit form of this surface consistency constraint can be derived (Grimson, MIT Artificial Intelligence Lab Memo, No. 646, 1981). To determine which of two possible surfaces is more consistent with the surface consistency constraint, one must be able to compare the two surfaces. To do this, a functional from the space of possible functions to the real numbers is required. In this way, the surface most consistent with the visual information will be that which minimizes the functional. In Grimson, a set of conditions was derived which ensures that the functional has a unique minimal surface. Based on these conditions, a number of possible functionals were proposed. In Brady and Horn (MIT Artificial Intelligence Lab Memo, No. 654, 1981), it was shown that this set of possible functionals forms a vector space, spanned by the functional of quadratic variation and the functional of the square Laplacian. Analytic arguments were given in Grimson to support the choice of the quadratic variation as the functional whose minimal surface is the “best” interpolation of the known points. In this paper, algorithms for computing the minimal surface are derived. Using this implementation of the computational theory derived in Grimson the differences between minimal surfaces computed using quadratic variation and those computed using the square Laplacian are illustrated. These examples provide additional support for the choice of the quadratic variation. The performance of the algorithm in interpolating both random dot and natural stereograms which have been processed by the Marr-Poggio stereo algorithm (Grimson, Computing Shape Using a Theory of Human Stereo Vision, Ph.D. Thesis, MIT, Cambridge, 1980 and 
                        Philos. Trans. R. Soc. London Ser. B 
                        292, 1981, 217–253) is also illustrated.",space
10.1016/j.egyr.2022.01.224,Journal,Energy Reports,scopus,2022-07-01,sciencedirect,Research on anomaly detection of wireless data acquisition in power system based on spark,https://api.elsevier.com/content/abstract/scopus_id/85125473240,"In the era of big data, the network data of power system is more and more complex. Due to the limitation of data storage and processing capacity, the abnormal data detection of power grid terminal information system has the problems of low accuracy and high false alarm rate. The original machine learning algorithm with good detection effect is limited by the processing capacity and storage space of the traditional platform, and the detection effect and efficiency are significantly reduced. This paper takes improving the detection accuracy of abnormal data as the main research target, and designs an abnormal data behavior analysis program based on the Internet of Things under the Spark framework combined with improved Support Vector Machine (SVM) and random forest algorithm. The parallel SA_SVM_RF anomaly data behavior detection model based on Spark is mainly studied and applied to real-time detection. Combined with the respective advantages of Internet of Things technology and machine learning in anomaly data detection, the detection capability and rate of power grid anomaly data detection model are further improved. Experimental tests show that the proposed program is superior to traditional methods in data anomaly detection efficiency and quality, and has certain research significance in the field of power grid security.",space
10.1016/j.future.2022.02.008,Journal,Future Generation Computer Systems,scopus,2022-07-01,sciencedirect,Hatch: Self-distributing systems for data centers,https://api.elsevier.com/content/abstract/scopus_id/85125223395,"Designing and maintaining distributed systems remains highly challenging: there is a high-dimensional design space of potential ways to distribute a system’s sub-components over a large-scale infrastructure; and the deployment environment for a system tends to change in unforeseen ways over time. For engineers, this is a complex prediction problem to gauge which distributed design may best suit a given environment. We present the concept of self-distributing systems, in which any local system built using our framework can learn, at runtime, the most appropriate distributed design given its perceived operating conditions. Our concept abstracts distribution of a system’s sub-components to a list of simple actions in a reward matrix of distributed design alternatives to be used by reinforcement learning algorithms. By doing this, we enable software to experiment, in a live production environment, with different ways in which to distribute its software modules by placing them in different hosts throughout the system’s infrastructure. We implement this concept in a framework we call Hatch, which has three major elements: (i) a transparent and generalized RPC layer that supports seamless relocation of any local component to a remote host during execution; (ii) a set of primitives, including relocation, replication and sharding, from which to create an action/reward matrix of possible distributed designs of a system; and (iii) a decentralized reinforcement learning approach to converge towards more optimal designs in real time. Using an example of a self-distributing web-serving infrastructure, Hatch is able to autonomously select the most suitable distributed design from among 
                        ≈
                     700,000 alternatives in about 5 min.",space
10.1016/j.jbusres.2022.02.039,Journal,Journal of Business Research,scopus,2022-05-01,sciencedirect,Multi-target CNN-LSTM regressor for predicting urban distribution of short-term food delivery demand,https://api.elsevier.com/content/abstract/scopus_id/85124904640,"The food delivery market has increased rapidly in the last few years, becoming a well-established reality in the business world and a common feature of urban life. Food delivery platforms provide the end-to-end services that connect restaurants with consumers, including the delivery service to those people ordering food through an online portal. A key component of these platforms is logistics, specifically the logistics of drivers. Ideally, the number of drivers operating in an urban area should be just the right number to serve the demand in that area. Since the demand is extremely dynamic in space and time, the spatial–temporal distribution of drivers remains a challenging problem, partially solved by means of variable incentives in different city areas at different times. In this context, a precise demand prediction would avoid a local lack of drivers in some areas, and an inefficient concentration of drivers in some other areas. For this reason, we propose a deep neural network-based methodology to forecast short-term food delivery demand distribution over urban areas. The study, carried out on a real-world dataset from a food delivery company, focuses on hourly demands and frequent prediction updates. The sequential modeling approach, designed to catch rapid changes and sudden variations beyond the general demand trend, is based on a multi-target CNN-LSTM regressor trained on location-specific time series. The methodology uses a single model for all service areas simultaneously, and a single one-step volume inference for every area at each time update. The results disclose a better performance over baselines (historical estimates for the same time-area) and more traditional statistical approaches (moving averages and univariate time-series forecasting), demonstrating a promising implementation potential within an online delivery platform framework.",space
10.1016/j.jss.2022.111231,Journal,Journal of Systems and Software,scopus,2022-05-01,sciencedirect,Discovering boundary values of feature-based machine learning classifiers through exploratory datamorphic testing,https://api.elsevier.com/content/abstract/scopus_id/85124473834,"Testing has been widely recognised as difficult for AI applications. This paper proposes a set of testing strategies for testing machine learning applications in the framework of the datamorphism testing methodology. In these strategies, testing aims at exploring the data space of a classification or clustering application to discover the boundaries between classes that the machine learning application defines. This enables the tester to understand precisely the behaviour and function of the software under test. In the paper, three variants of exploratory strategies are presented with the algorithms implemented in the automated datamorphic testing tool Morphy. The correctness of these algorithms are formally proved. Their capability and cost of discovering borders between classes are evaluated via a set of controlled experiments with manually designed subjects and a set of case studies with real machine learning models.",space
10.1016/j.ijheatmasstransfer.2021.122444,Journal,International Journal of Heat and Mass Transfer,scopus,2022-05-01,sciencedirect,A versatile inversion approach for space/temperature/time-related thermal conductivity via deep learning,https://api.elsevier.com/content/abstract/scopus_id/85122239809,"Identifying the thermophysical properties of unknown material through the measurement of temperature is of great significance in computational heat transfer. Existing numerical algorithms are generally computationally cumbersome and resource demanding. Rapid advances in deep learning (DL) offer an alternative pathway to speed up the inversion process by fully utilizing the parallel computing ability of Graphics Processing Units (GPUs). In this paper, a DL framework is proposed to reconstruct the thermal conductivity related to space, temperature or time. The whole framework consists of a forward data generation module, a denoising module and an inversion module. It is noteworthy that the physics informed neural network (PINN) is employed in the process of generating training data, which avoids the use of commercial software based on traditional methods. In order to simulate the measurement error in practical scenarios, a certain intensity of Gaussian noise is added to the generated data. After denoising by the U-net, the measured temperature is fed to the nonlinear mapping module (NMM) for the inversion of the unknown thermal conductivity. As a result, a well-trained framework can realize high precision real-time inversion even with intensive environmental noise, offering great potential for applications pertaining to the reconstruction of thermophysical properties.",space
10.1016/j.eswa.2021.116323,Journal,Expert Systems with Applications,scopus,2022-04-15,sciencedirect,Deep multi-agent reinforcement learning for multi-level preventive maintenance in manufacturing systems[Formula presented],https://api.elsevier.com/content/abstract/scopus_id/85121584320,"Designing preventive maintenance (PM) policies that ensure smooth and efficient production for large-scale manufacturing systems is non-trivial. Recent model-free reinforcement learning (RL) methods shed lights on how to cope with the non-linearity and stochasticity in such complex systems. However, the action space explosion impedes RL-based PM policies to be generalized to real applications. In order to obtain cost efficient PM policies for a serial production line that has multiple levels of PM actions, a novel multi-agent modeling is adopted to support adaptive learning by modeling each machine as cooperative agent. The evaluation of system-level production loss is leveraged to construct the reward function. An adaptive learning framework based on value-decomposition multi-agent actor–critic algorithm is utilized to obtain PM policies. In simulation study, the proposed framework demonstrates its effectiveness by leading other baselines on a comprehensive set of metrics whereas the centralized RL-based methods struggles to converge to stable policies. Our analysis further demonstrates that our multi-agent reinforcement learning based method learns effective PM policies without any knowledge about the environment and maintenance strategies.",space
10.1016/j.jobe.2021.103778,Journal,Journal of Building Engineering,scopus,2022-04-15,sciencedirect,A coupled deep learning-based internal heat gains detection and prediction method for energy-efficient office building operation,https://api.elsevier.com/content/abstract/scopus_id/85121269022,"Occupants' behaviour and the use of electrical equipment can significantly impact the building energy demand. Accurate occupancy and equipment usage information are key to improving the performance of demand-driven control, which can automatically adjust the heating, cooling and ventilation system operation. Employing static schedules is commonly used for the operation of heating, ventilation and air-conditioning systems, while it cannot satisfy the actual requirements due to the dynamic variations within the conditioned spaces. This study introduces a coupled real-time occupancy and equipment usage detection and recognition approach using deep learning and computer vision techniques for efficient building energy controls. The experimental results presented an overall equipment detection and occupancy activity detection accuracy of 78.39% and 93.60%. To investigate the influence of the implementation of the approach on building energy demand, a case study office building was selected to conduct experimental tests and modeled using a building energy simulation tool. Four scenarios with different occupancy and equipment profiles were defined and evaluated. The simulation results showed that heat gains, when employing static profiles were larger than the heat gains predicted when using the deep learning influenced profiles. Up to 53.95% lower heat gains were estimated when using both occupancy and equipment detection approaches than static schedules solely. The results highlighted the importance of monitoring real-time occupancy and electrical equipment usage and the advantages of using deep learning detection techniques to provide data for demand-driven controls, optimising building energy efficiency while maintaining a comfortable indoor environment.",space
10.1016/j.conengprac.2021.105046,Journal,Control Engineering Practice,scopus,2022-04-01,sciencedirect,Deep reinforcement learning with shallow controllers: An experimental application to PID tuning,https://api.elsevier.com/content/abstract/scopus_id/85122624409,"Deep reinforcement learning (RL) is an optimization-driven framework for producing control strategies for general dynamical systems without explicit reliance on process models. Good results have been reported in simulation. Here we demonstrate the challenges in implementing a state of the art deep RL algorithm on a real physical system. Aspects include the interplay between software and existing hardware; experiment design and sample efficiency; training subject to input constraints; and interpretability of the algorithm and control law. At the core of our approach is the use of a PID controller as the trainable RL policy. In addition to its simplicity, this approach has several appealing features: No additional hardware needs to be added to the control system, since a PID controller can easily be implemented through a standard programmable logic controller; the control law can easily be initialized in a “safe” region of the parameter space; and the final product—a well-tuned PID controller—has a form that practitioners can reason about and deploy with confidence.",space
10.1016/j.buildenv.2022.108786,Journal,Building and Environment,scopus,2022-03-15,sciencedirect,Decision Support System for technology selection based on multi-criteria ranking: Application to NZEB refurbishment,https://api.elsevier.com/content/abstract/scopus_id/85123838318,"Refurbishing existing building into Near Zero Energy Building (NZEB) is a key objective for the European Union. In order to achieve high rate of conversion, new refurbishment process must allow Decision Makers (DMs) (architects or designers) to sort through an ever increasing list of new technologies while taking into account uncertain preferences from multiple stakeholders.
                  A Decision Support System (DSS) based on Multi-Criteria Decision-Making (MCDM) approaches is proposed. The DSS enables the DMs to browse the solutions space by selecting the relevant criteria, order them by preferences and specify the granularity in the assessment of the technologies regarding each criteria.
                  This DSS is based on a ranking algorithm that operates on multiple types of quantitative (continuous, discrete, or binary) and qualitative (nominative or ordinal) variables from technological and human sources. An online user interface allows the real-time exploration of the solution space. A sensitivity analysis of the algorithm is conducted to expose the influence of the ranking algorithm parameters and to demonstrate the robustness of this algorithm. The proposed DSS is eventually implemented and validated through a use case concerning the choice of insulating materials considering heterogeneous criteria that model sustainable constraints.",space
10.1016/j.jbi.2022.103996,Journal,Journal of Biomedical Informatics,scopus,2022-03-01,sciencedirect,Evaluating pointwise reliability of machine learning prediction,https://api.elsevier.com/content/abstract/scopus_id/85123373748,"Interest in Machine Learning applications to tackle clinical and biological problems is increasing. This is driven by promising results reported in many research papers, the increasing number of AI-based software products, and by the general interest in Artificial Intelligence to solve complex problems. It is therefore of importance to improve the quality of machine learning output and add safeguards to support their adoption. In addition to regulatory and logistical strategies, a crucial aspect is to detect when a Machine Learning model is not able to generalize to new unseen instances, which may originate from a population distant to that of the training population or from an under-represented subpopulation. As a result, the prediction of the machine learning model for these instances may be often wrong, given that the model is applied outside its “reliable” space of work, leading to a decreasing trust of the final users, such as clinicians. For this reason, when a model is deployed in practice, it would be important to advise users when the model’s predictions may be unreliable, especially in high-stakes applications, including those in healthcare. Yet, reliability assessment of each machine learning prediction is still poorly addressed.
                  Here, we review approaches that can support the identification of unreliable predictions, we harmonize the notation and terminology of relevant concepts, and we highlight and extend possible interrelationships and overlap among concepts. We then demonstrate, on simulated and real data for ICU in-hospital death prediction, a possible integrative framework for the identification of reliable and unreliable predictions. To do so, our proposed approach implements two complementary principles, namely the density principle and the local fit principle. The density principle verifies that the instance we want to evaluate is similar to the training set. The local fit principle verifies that the trained model performs well on training subsets that are more similar to the instance under evaluation. Our work can contribute to consolidating work in machine learning especially in medicine.",space
10.1016/j.ins.2021.11.011,Journal,Information Sciences,scopus,2022-03-01,sciencedirect,Survival functions versus conditional aggregation-based survival functions on discrete space,https://api.elsevier.com/content/abstract/scopus_id/85122830123,In this paper we deal with conditional aggregation-based survival functions recently introduced by Boczek et al. (2020). The concept is worth to study because of its possible implementation in real-life situations and mathematical theory as well. The aim of this paper is the comparison of this new notion with the standard survival function. We state sufficient and necessary conditions under which the generalized and the standard survival function equal. The main result is the characterization of the family of conditional aggregation operators (on discrete space) for which these functions coincide.,space
10.1016/j.adhoc.2021.102757,Journal,Ad Hoc Networks,scopus,2022-03-01,sciencedirect,Deep embedded median clustering for routing misbehaviour and attacks detection in ad-hoc networks,https://api.elsevier.com/content/abstract/scopus_id/85120863584,"Due to the properties of ad-hoc networks, it appears that designing sophisticated defence schemes with more computing capital is impossible in most situations. Recently, an inconsistency in the ad-hoc design of intrusion detection in the network has gotten a lot of coverage, with these intrusion detection techniques operating in either cluster-based or host-based configurations. The host and cluster-based systems have advantages and disadvantages, such as the network preserve security in case of delay in replacing a cluster head. Many detection systems in these networks use a supervised learning method to learn from shared routing knowledge. Deep learning is the trending supervised learning method which is been suggested for many applications, due to its deep feature extraction and classification capability. The deep learning method is best suitable to resolve the problems of the ad hoc network. But due to its limitation of supervised learning nature, more research finds are needed before implementation. These intelligence methods need a massive labeled dataset to self-train and take a decision in real-time. Also, these methods will be vulnerable to new attacks. To address the issues posed, the deep learning approach requires a technique of incorporating unsupervised learning behaviours. This paper proposes and highlights the methodology - Deep Embedded Median Clustering (DEMC), which performs two-phase operations (1) Organization of latent feature space (2) K-median clustering to cluster the Z with Kullback–Leibler divergence as the objective function. Many researchers suggested various methodologies for better anomaly detection in the network, but the knowledge gap and the possibilities for a better solution still exist. This study explores the new possibility and potential of an unsupervised learning technique that works with the nature of deep learning for analyzing and detecting anomalies and intrusion in ad hoc networks. The test to check the DEMC ability has been organized, and the findings are tabulated for analysis.",space
10.1016/j.apenergy.2021.118336,Journal,Applied Energy,scopus,2022-02-15,sciencedirect,Real-time monitoring of occupancy activities and window opening within buildings using an integrated deep learning-based approach for reducing energy demand,https://api.elsevier.com/content/abstract/scopus_id/85122427364,"Occupancy behaviour in buildings can impact the energy performance and operation of heating, ventilation and air-conditioning (HVAC) systems. HVAC, which uses conventional control strategies or “fixed” setpoint schedules, could not adjust to the conditioned spaces' actual requirements, resulting in building spaces being over or under-conditioned. While the unintended opening of windows can lead to substantial heat loss and consequently raises energy consumption. To optimise building operations, it is necessary to employ solutions such as demand-driven controls, which can monitor the utilisation of indoor spaces and provide the actual thermal comfort requirements of occupants. This study presents a novel vision-based deep learning framework for occupancy activity detection and recognition including the manual window operations in buildings. A region-based Convolutional Neural Network (R-CNN) model was trained and deployed to a camera for real-time detection and recognition. Based on the field experiments conducted within a case study University building, overall accuracy of 85.63% was achieved for occupancy activity detection and 92.20% for window operation detection. Building energy simulation and various scenario-based cases were used to assess the impact of such an approach on the building energy demand and provide insights into how the proposed detection method can enable HVAC systems to respond to dynamic changes within indoor spaces. Results showed that the proposed approach could reduce the over-or under-estimation of occupancy heat gains compared with the use of “fixed” or static profiles. In addition, the approach can help alert building users or managers about windows left open unintentionally, which can reduce unnecessary ventilation heat losses. Furthermore, the approach can also predict the room CO2 concentration and advise occupants about a suitable natural ventilation strategy. The study highlighted the potential of the multi-purpose detection approach, but further development is necessary, including optimisation of the deep learning model, full integration with HVAC controls and further model training and field testing.",space
10.1016/j.comnet.2021.108616,Journal,Computer Networks,scopus,2022-02-11,sciencedirect,LightLog: A lightweight temporal convolutional network for log anomaly detection on the edge,https://api.elsevier.com/content/abstract/scopus_id/85119961419,"Log anomaly detection on edge devices is the key to enhance edge security when deploying IoT systems. Despite the success of many newly proposed deep learning based log anomaly detection methods, handling large-scale logs on edge devices is still a bottleneck due to the limited computational power on these devices to fulfil the real-time processing requirement for accurate anomaly detection. In this work, we propose a novel lightweight log anomaly detection algorithm, named LightLog, to tackle this research gap. In specific, we achieve real-time processing speed on the task via two aspects: (i) creation of a low-dimensional semantic vector space based on word2vec and post-processing algorithms (PPA); and (ii) design of a lightweight temporal convolutional network (TCN) for the detection. These two components significantly reduce the number of parameters and computations of a standard TCN while improving the detection performance. Experimental results show that our LightLog outperforms several benchmarking methods, namely DeepLog, LogAnomaly and RobustLog, by achieving 97.0 F1 score on HDFS Dataset and 97.2 F1 score on BGL with smallest model size. This effective yet efficient method paves the way to the deployment of log anomaly detection on the edge. Our source code and datasets are freely available on https://github.com/Aquariuaa/LightLog.",space
10.1016/j.vrih.2022.01.004,Journal,Virtual Reality and Intelligent Hardware,scopus,2022-02-01,sciencedirect,Virtual-reality-based digital twin of office spaces with social distance measurement feature,https://api.elsevier.com/content/abstract/scopus_id/85124517698,"Background
                  Social distancing is an effective way to reduce the spread of the SARS-CoV-2 virus. Many students and researchers have already attempted to use computer vision technology to automatically detect human beings in the field of view of a camera and help enforce social distancing. However, because of the present lockdown measures in several countries, the validation of computer vision systems using large-scale datasets is a challenge.
               
                  Methods
                  In this paper, a new method is proposed for generating customized datasets and validating deep-learning-based computer vision models using virtual reality (VR) technology. Using VR, we modeled a digital twin (DT) of an existing office space and used it to create a dataset of individuals in different postures, dresses, and locations. To test the proposed solution, we implemented a convolutional neural network (CNN) model for detecting people in a limited-sized dataset of real humans and a simulated dataset of humanoid figures.
               
                  Results
                  We detected the number of persons in both the real and synthetic datasets with more than 90% accuracy, and the actual and measured distances were significantly correlated (r=0.99). Finally, we used intermittent-layer- and heatmap-based data visualization techniques to explain the failure modes of a CNN.
               
                  Conclusions
                  A new application of DTs is proposed to enhance workplace safety by measuring the social distance between individuals. The use of our proposed pipeline along with a DT of the shared space for visualizing both environmental and human behavior aspects preserves the privacy of individuals and improves the latency of such monitoring systems because only the extracted information is streamed.",space
10.1016/j.rse.2021.112809,Journal,Remote Sensing of Environment,scopus,2022-02-01,sciencedirect,MethaNet – An AI-driven approach to quantifying methane point-source emission from high-resolution 2-D plume imagery,https://api.elsevier.com/content/abstract/scopus_id/85120521703,"Methane is one of the most important anthropogenic greenhouse gases with a significant impact on the Earth's radiation budget and tropospheric background ozone. Despite a well-constrained global budget, quantification of local and regional methane emissions has proven challenging. Recent advancements in airborne remote sensing instruments such as from the next-generation Airborne Visible/Infrared Imaging Spectrometer (AVIRIS-NG) provide 2-D observations of CH4 plume column enhancements at an unprecedented resolution of 1–5 m over large geographic areas. Quantifying an emission rate from observed plumes is a critical step for understanding local emission distributions and prioritizing mitigation efforts. However, there exists no method that can predict emission rates from detected plumes in real-time without ancillary data reliably. In order to predict methane point-source emissions directly from high resolution 2-D plume images without relying on other local measurements such as background wind speeds, we trained a convolutional neural network model called MethaNet. The training data was derived from large eddy simulations of methane plumes and realistic measurement noise over agricultural, desert and urban environments. Our model has a mean absolute percentage error for predicting unseen plumes under 17%, a significant improvement from previous methods that require wind information. Using MethaNet, a validation against a natural gas controlled-release experiment agrees to within the precision error estimate. Our results support the basis for the applicability of using deep learning techniques to quantify CH4 point sources in an automated manner over large geographical areas, not only for present and future airborne field campaigns but also for upcoming space-based observations in this decade.",space
10.1016/j.cose.2021.102547,Journal,Computers and Security,scopus,2022-02-01,sciencedirect,Jadeite: A novel image-behavior-based approach for Java malware detection using deep learning,https://api.elsevier.com/content/abstract/scopus_id/85120335865,"Java malware exploiting language vulnerabilities has become increasingly prevalent in the recent past. Since Java is a platform-independent language, these security threats open up the opportunity for multi-platform exploitation. Although security researchers continuously develop different approaches for protecting against Java malware programs, the presence of complicated Java malware properties, such as code obfuscation, makes these malware programs fly under the radar. These challenges present the need to develop new approaches that are resilient to such properties. This article presents Jadeite, a novel approach for detecting Java bytecode malware programs using static analysis and recent advancements in the image-based, deep-learning classification space. In particular, Jadeite extracts the Interprocedural Control Flow Graph (ICFG) from a given Java bytecode file and then prunes the ICFG and converts it into an adjacency matrix. Finally, Jadeite constructs a grayscale image from this matrix. We leverage an object detection algorithm in a deep Convolutional Neural Network (CNN) classifier to determine maliciousness. Also, Jadeite extracts an additional set of features from the Java malware program to improve the accuracy of malware classification. These features are consolidated with the extracted images and used as inputs to the CNN classifier. Experimental results demonstrate that Jadeite achieves high accuracy (98.4%) compared to other Java malware detection approaches and is capable of detecting both known and previously-unseen real-world malicious Java programs.",space
10.1016/j.scs.2021.103559,Journal,Sustainable Cities and Society,scopus,2022-02-01,sciencedirect,Assessment of sustainable development objectives in Smart Labs: technology and sustainability at the service of society,https://api.elsevier.com/content/abstract/scopus_id/85120052266,"Sustainable development is the working basis of engineering research and cities are becoming increasingly flexible, inclusive and intelligent. In this context, there is a need for environments that emulate real-life spaces in which cutting-edge technologies can be implemented for subsequent deployment in society. Smart Labs or Living Labs are spaces for innovation, research and experimentation that integrate systems, devices and methodologies focused on people and their environments. The technologies studied and developed in such labs can then be deployed in human spaces to provide intelligence, comfort, health and sustainability. Health and wellness, energy and environment, artificial intelligence, big data and digital rights are some of the disciplines being studied. At the same time, the UN 2030 Agenda provides a comprehensive framework to promote human well-being through the Sustainable Development Goals. In this work, an evaluation model of its indicators in smart environments is performed through a mixed review methodology. The objective of this work is the analysis and implementation of the SDGs in Smart Labs through a literature review and a case study of UJAmI, the smart laboratory of the University of Jaén. The results provide quantitative and qualitative data on the present and future of the smart devices implemented in the UJAmI lab, providing a roadmap for future developments.",space
10.1016/j.inffus.2021.09.004,Journal,Information Fusion,scopus,2022-02-01,sciencedirect,Multimodal Earth observation data fusion: Graph-based approach in shared latent space,https://api.elsevier.com/content/abstract/scopus_id/85115401406,"Multiple and heterogenous Earth observation (EO) platforms are broadly used for a wide array of applications, and the integration of these diverse modalities facilitates better extraction of information than using them individually. The detection capability of the multispectral unmanned aerial vehicle (UAV) and satellite imagery can be significantly improved by fusing with ground hyperspectral data. However, variability in spatial and spectral resolution can affect the efficiency of such dataset's fusion. In this study, to address the modality bias, the input data was projected to a shared latent space using cross-modal generative approaches or guided unsupervised transformation. The proposed adversarial networks and variational encoder-based strategies used bi-directional transformations to model the cross-domain correlation without using cross-domain correspondence. It may be noted that an interpolation-based convolution was adopted instead of the normal convolution for learning the features of the point spectral data (ground spectra). The proposed generative adversarial network-based approach employed dynamic time wrapping based layers along with a cyclic consistency constraint to use the minimal number of unlabeled samples, having cross-domain correlation, to compute a cross-modal generative latent space. The proposed variational encoder-based transformation also addressed the cross-modal resolution differences and limited availability of cross-domain samples by using a mixture of expert-based strategy, cross-domain constraints, and adversarial learning. In addition, the latent space was modelled to be composed of modality independent and modality dependent spaces, thereby further reducing the requirement of training samples and addressing the cross-modality biases. An unsupervised covariance guided transformation was also proposed to transform the labelled samples without using cross-domain correlation prior. The proposed latent space transformation approaches resolved the requirement of cross-domain samples which has been a critical issue with the fusion of multi-modal Earth observation data. This study also proposed a latent graph generation and graph convolutional approach to predict the labels resolving the domain discrepancy and cross-modality biases. Based on the experiments over different standard benchmark airborne datasets and real-world UAV datasets, the developed approaches outperformed the prominent hyperspectral panchromatic sharpening, image fusion, and domain adaptation approaches. By using specific constraints and regularizations, the network developed was less sensitive to network parameters, unlike in similar implementations. The proposed approach illustrated improved generalizability in comparison with the prominent existing approaches. In addition to the fusion-based classification of the multispectral and hyperspectral datasets, the proposed approach was extended to the classification of hyperspectral airborne datasets where the latent graph generation and convolution were employed to resolve the domain bias with a small number of training samples. Overall, the developed transformations and architectures will be useful for the semantic interpretation and analysis of multimodal data and are applicable to signal processing, manifold learning, video analysis, data mining, and time series analysis, to name a few.",space
10.1016/j.comcom.2021.10.037,Journal,Computer Communications,scopus,2022-01-15,sciencedirect,SAAS parallel task scheduling based on cloud service flow load algorithm,https://api.elsevier.com/content/abstract/scopus_id/85120359168,"In cloud platform applications, the user’s goal is to obtain high-quality application services, while the service provider’s goal is to obtain revenue by performing the tasks submitted by the user. The platform built by the service provider’s application resources needs to improve the mapping between service requests and resources to achieve higher value. Through the current situation of resource management in the cloud environment, it is found that many task scheduling and resource allocation algorithms are still affected by factors such as the diversity, dynamics, and multiple constraints of resources and tasks. This paper focuses on Software as a Service (SaaS) applications’ task scheduling and resource configuration in a dynamic and uncertain cloud environment. It is a challenging online scheduling problem to automatically and intelligently allocate user task requests that continually reach SaaS applications to appropriate resources for execution. To this end, a real-time task scheduling method based on deep reinforcement learning is proposed, which automatically and intelligently allocates user task requests that continually reach SaaS applications to appropriate resources for execution. In this way, the limited virtual machine resources rented by SaaS providers can be used in a balanced and efficient manner. In the experiment, by comparing with other five task scheduling algorithms, it is proved that the algorithm proposed in this paper not only improves the execution efficiency of better deploying workflow in IaaS public cloud, but also makes the resources provided by SaaS are used in a balanced and efficient manner.",space
10.1016/j.apacoust.2021.108439,Journal,Applied Acoustics,scopus,2022-01-15,sciencedirect,"CAMNet: A controllable acoustic model for efficient, expressive, high-quality text-to-speech",https://api.elsevier.com/content/abstract/scopus_id/85116891718,"Spoken language is becoming one of the key components of human–machine interaction, both to send information to the machine – e.g. voice control – and to receive from it – e.g. virtual assistants. In this scenario, text-to-speech (TTS) models have become an essential artificial intelligence capacity. Even though this interaction can be based on neutral style speech, generating speech with different styles, pitches and speaking rates may improve user experience. With this in view, this paper presents CAMNet, a controllable acoustic model for efficient, expressive, high-quality TTS. CAMNet is based on deep convolutional TTS (DCTTS), a state-of-art acoustic model which is efficient and produces neutral speech. DCTTS was first adapted to generate Bark cepstrum acoustic features in order to integrate well with the LPCNet (linear prediction coefficient) neural vocoder and to remove the reduction factor which demanded the presence of an upsampling network before the vocoder – i.e. the CAMNet output can be directly fed into LPCNet. Next, style transfer functionality was added by means of a novel characterisation of the prosodic information from the Bark cepstrum acoustic features and a new approach to inject this information into the convolutional layers. Finally, controllability is provided via a variational auto-encoder module which creates a smoothed disentangled latent space which allows interpolation and extrapolation of reference styles as well as independent and simultaneous control of two generative factors: pitch and speaking rate. Moreover, this controllability is implemented using a simple offset-based approach. To sum up, CAMNet is an efficient acoustic model which provides a simple but consistent controllability on coarse-grained expression, pitch and speaking rate while still providing high-quality synthesised speech.",space
10.1016/j.neucom.2021.10.004,Journal,Neurocomputing,scopus,2022-01-11,sciencedirect,A CNN-based policy for optimizing continuous action control by learning state sequences,https://api.elsevier.com/content/abstract/scopus_id/85118138176,"Continuous action control is widespread in real-world applications. It controls an agent to take action in continuous space for transiting from one state to another until achieving the desired goal. The optimization of continuous action control is an important issue, which aims to find the optimal policy for the agent to achieve the desired goal with the lowest consumption in continuous action space. A useful tool for this issue is reinforcement learning where an optimal policy is learned for the agent by maximizing the cumulative reward of the state transitions. When updating the policy at each state, most existing reinforcement learning methods consider only the one-step transition of this state. However, for each state in continuous action control, the recognizable information is usually hidden in the sequence of its previous states, thus these methods cannot learn the policy effectively enough for continuous action control. In this paper, we propose a new policy, called convolutional deterministic policy, to solve this problem. Enlightened from the convolutional neural networks used in natural language processing, our convolutional deterministic policy uses convolutional neural networks to learn the recognizable information in the state sequences. Then for each collected state, we update the convolutional deterministic policy by not only the recognizable information in the one-step transition of this state but also the recognizable information in the sequence of its previous states. As a result, our convolutional deterministic policy can make the agent take better action. Based on an effective reinforcement learning method, TD3, the implementation of our convolutional deterministic policy is in CTD3. The theoretical analysis and the experiment illustrate that our CTD3 can learn the policy not only better than but also faster than the existing RL methods for continuous action control. The source code can be downloaded from https://github.com/grcai.",space
10.1016/j.knosys.2021.107624,Journal,Knowledge-Based Systems,scopus,2022-01-10,sciencedirect,Manifold-based aggregation clustering for unsupervised vehicle re-identification,https://api.elsevier.com/content/abstract/scopus_id/85118363031,"Most vehicle re-identification (V-reID) approaches are based on supervised learning methods which require a considerable amount of tedious and impractical annotations. In this paper, we propose a novel unsupervised V-reID approach based on Manifold-based Aggregation Clustering (MAC) with the unknown number of clusters. The proposed MAC is implemented by alternatively conducting two modules, i.e., deep feature learning module and aggregation clustering module. Specifically, deep feature learning module is responsible for training a convolutional neural network to encourage deep features to be close to the centroids of corresponding clusters which are yielded by an aggregation clustering mechanism based on manifold distance in the feature space. Moreover, the classification-agglomeration loss and manifold-based seeds searching criterion are proposed to improve the discriminative power of the learned features and deal with the problem of varied visual appearance respectively. Note that both annotations and even the certain number of vehicle identities are unknown for the proposed method, which is totally consistent with the real-world unsupervised V-reID condition. Extensive experiments on VehicleID and Veri-776 benchmark datasets show that the proposed method outperforms the state-of-the-art unsupervised V-reID approaches.",space
10.1016/j.engappai.2021.104514,Journal,Engineering Applications of Artificial Intelligence,scopus,2022-01-01,sciencedirect,Instance-based defense against adversarial attacks in Deep Reinforcement Learning,https://api.elsevier.com/content/abstract/scopus_id/85118104144,"Deep Reinforcement Learning systems are now a hot topic in Machine Learning for their effectiveness in many complex tasks, but their application in safety-critical domains (e.g., robot control or self-autonomous driving) remains dangerous without mechanism to detect and prevent risk situations. In Deep RL, such risk is mostly in the form of adversarial attacks, which introduce small perturbations to sensor inputs with the aim of changing the network-based decisions and thus cause catastrophic situations. In the light of these dangers, a promising line of research is that of providing these Deep RL algorithms with suitable defenses, especially when deploying in real environments. This paper suggests that this line of research could be greatly improved by the concepts from the existing research field of Safe Reinforcement Learning, which has been postulated as a family of RL algorithms capable of providing defenses against many forms of risks. However, the connections between Safe RL and the design of defenses against adversarial attacks in Deep RL remain largely unexplored. This paper seeks to explore precisely some of these connections. In particular, this paper proposes to reuse some of the concepts from existing Safe RL algorithms to create a novel and effective instance-based defense for the deployment stage of Deep RL policies. The proposed algorithm uses a risk function based on how far a state is from the state space known by the agent, that allows identifying and preventing adversarial situations. The success of the proposed defense has been evaluated in 4 Atari games.",space
10.1016/j.bspc.2021.103245,Journal,Biomedical Signal Processing and Control,scopus,2022-01-01,sciencedirect,Prototype design for bidirectional control of stepper motor using features of brain signals and soft computing tools,https://api.elsevier.com/content/abstract/scopus_id/85117841558,"The brain-computer interface (BCI) plays a significant role in supporting specially-abled people to control devices with the brain signals or electroencephalogram (EEG). The proper functioning of BCI systems requires classification algorithms to distinguish between different tasks based on the features extracted from EEG. Motivated by the requirement of designing a reliable BCI with reduced memory and computational cost, this work proposes an EEG controlled stepper motor drive-based aiding device. The drive system is executed in real-time based on the processing and classification of EEG. The scheme initiates with the extraction of discriminatory features from raw time-domain EEG signals using higher-order statistics (HOS) and phase locking value (PLV).
                  Further, with extracted feature vector, the present study contemplates the operation of backpropagation neural network (BPNN), k-Nearest neighbours (k-NN), and support vector machine (SVM). Signal classification by SVM in conjugation with nonlinear principal component analysis (NLPCA) is implemented to reduce the feature vector dimension. Average classification accuracy of 82.70% and 80.46% is achieved using NLPCA with SVM for PLV and HOS features. The classifier output is utilized to spin the stepper motor clockwise and anticlockwise as needed. The control of stepper motor uses the classifier output and hence the brain signals are implemented on a laboratory-developed digital test bench comprising of TI TMS320F28379D launchpad DSP board and MITSUMI stepper motor. The validation of the proposed scheme for different signals reflects its effectiveness in combining the software and hardware aspects required for realizing an actual BCI system for real-time settings.",space
10.1016/j.energy.2021.121873,Journal,Energy,scopus,2022-01-01,sciencedirect,Real-time optimal energy management of microgrid with uncertainties based on deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85115035192,"Microgrid (MG) is an effective way to integrate renewable energy into power system at the consumer side. In the MG, the energy management system (EMS) is necessary to be deployed to realize efficient utilization and stable operation. To help the EMS make optimal schedule decisions, we proposed a real-time dynamic optimal energy management (OEM) based on deep reinforcement learning (DRL) algorithm. Traditionally, the OEM problem is solved by mathematical programming (MP) or heuristic algorithms, which may lead to low computation accuracy or efficiency. While for the proposed DRL algorithm, the MG-OEM is formulated as a Markov decision process (MDP) considering environment uncertainties, and then solved by the PPO algorithm. The PPO is a novel policy-based DRL algorithm with continuous state and action spaces, which includes two phases: offline training and online operation. In the training process, the PPO can learn from historical data to capture the uncertainty characteristic of renewable energy generation and load consumption. Finally, the case study demonstrates the effectiveness and the computation efficiency of the proposed method.",space
10.1016/j.comnet.2021.108560,Journal,Computer Networks,scopus,2021-12-24,sciencedirect,Distributed scheduling method for multiple workflows with parallelism prediction and DAG prioritizing for time constrained cloud applications,https://api.elsevier.com/content/abstract/scopus_id/85118887015,"Fog computing is an emerging popular paradigm that extends the availability of resources to the network's edge in order to improve the quality metrics of existing Cloud-based applications. However, scheduling workflow applications with time-constraints are complex regarding the count of resources, physical topology of clusters, and the structure of the task graph of the workflows. Adding Fog resources to the intricate problem space of Cloud-based scheduling needs even more time-consuming and complicated algorithms. In this paper, a multi-criteria Mamdani fuzzy algorithm is proposed to analyze the workflow graphs with the assistance of a Long-Short Term Memory neural network parallelism prediction module. The group-based priority assignment schema performed by the fuzzy inference system assigns a priority value to workflows to indicate the relative precedence of requests. Distributed schedulers then send the workflows to target sites according to their current workloads. The whole process is performed in a decentralized manner to prevent any bottlenecks. We have used an extensive software simulation study to compare the proposed algorithm in real workloads with two recent and notable algorithms. The simulation results confirm the proposed algorithm's superiority in fulfilling time-constraints, resource utilization, and overall application scheduling success rate.",space
10.1016/j.softx.2021.100854,Journal,SoftwareX,scopus,2021-12-01,sciencedirect,MicroVIP: Microscopy image simulation on the Virtual Imaging Platform,https://api.elsevier.com/content/abstract/scopus_id/85119052033,"MicroVIP is an open source software that assembles, in a unified web-application running on distributed computing resources, simulators of the main fluorescent microscopy imaging modalities (with existing codes or newly developed). MicroVIP provides realistic simulated images including several sources of noise (microfluidic blur effect, diffraction, Poisson noise, camera read out noise). MicroVIP also includes a module which simulates single cells with fluorescent markers and a module to analyze the simulated images with textural and pointillist feature spaces. MicroVIP is shown to be of value for supervised machine learning. It allow to automatically generate large sets of training images and virtual instrumentation to optimize the optical parameters before realizing real experiments.",space
10.1016/j.compeleceng.2021.107567,Journal,Computers and Electrical Engineering,scopus,2021-12-01,sciencedirect,Predicting activities of daily living via temporal point processes: Approaches and experimental results,https://api.elsevier.com/content/abstract/scopus_id/85118746684,"Activity Prediction is foreseeing the following activity people are going to execute. This is a crucial task in smart home environments, i.e., in order to facilitate the daily routines of elderly people with or without special needs. In this paper, we focused on Activity Daily Living prediction and we proposed a novel activity prediction technique based on the combination of Marked Temporal Point Processes and Neural Networks. Experiments on real and synthetic smart space datasets have shown that our approach is able to conveniently represent and predict daily living activities in an unsupervised way. We evaluated its performance and compared its results with state-of-the-art methods providing freely available implementations. Noticeably, the proposed approach outperforms the best concurrent algorithm by obtaining an improvement of F1-score of 60% (on average of the considered datasets).",space
10.1016/j.pss.2021.105371,Journal,Planetary and Space Science,scopus,2021-12-01,sciencedirect,Terrain classification-based rover traverse planner with kinematic constraints for Mars exploration,https://api.elsevier.com/content/abstract/scopus_id/85118490703,"In Mars exploration mission, the rover path planning is essential to ensure the safety and efficiency of the rover traverse. At present, traverse planning deployed Mars rovers requires experienced geologists and planetary scientists to spend a lot of time on surface-topography analysis. To a large extent, this is a costly and time-consuming manual process. Meanwhile, traditional optimal search algorithms (A∗ and Dijkstra) can search for a globally optimal path, but the path found does not satisfy the nonholonomic constraint of the vehicle, thus not directly drivable. The Terrain Classification-based Rover Traverse Planner (TCRTP), introduced in this article, is an automated algorithm that uses deep learning-based terrain classification as the guide to generate the optimal traverses with kinematic constraints. TCRTP combines the terrain classification with nonholonomic constraints, and it can quickly and accurately obtain the global optimal solution under the heuristic function. We also propose modifications of the cost function for path optimization to improve the smoothness of the solution. This algorithm plans forward and reverse movement, and penalizes reverse driving and changing the direction of movement. The TCRTP algorithm is easy to be extended, and the optimization traversal under multiple missions can be realized by adding traffic constraints, such as terrain roughness, elevation changes and so on. The TCRTP-based rover allows for real-time and frequent planning, and the addition of terrain constraints ensures the safety of the planned route. Our planner, combined with computer vision, shows the potential to reduce operator workloads and explore future planetary space.",space
10.1016/j.eswa.2021.115498,Journal,Expert Systems with Applications,scopus,2021-12-01,sciencedirect,Real-time human pose estimation on a smart walker using convolutional neural networks,https://api.elsevier.com/content/abstract/scopus_id/85109217957,"Rehabilitation is important to improve quality of life for mobility-impaired patients. Smart walkers are a commonly used solution that should embed automatic and objective tools for data-driven human-in-the-loop control and monitoring. However, present solutions focus on extracting few specific metrics from dedicated sensors with no unified full-body approach. We investigate a general, real-time, full-body pose estimation framework based on two RGB+D camera streams with non-overlapping views mounted on a smart walker equipment used in rehabilitation. Human keypoint estimation is performed using a two-stage neural network framework. The 2D-Stage implements a detection module that locates body keypoints in the 2D image frames. The 3D-Stage implements a regression module that lifts and relates the detected keypoints in both cameras to the 3D space relative to the walker. Model predictions are low-pass filtered to improve temporal consistency. A custom acquisition method was used to obtain a dataset, with 14 healthy subjects, used for training and evaluating the proposed framework offline, which was then deployed on the real walker equipment. An overall keypoint detection error of 3.73 pixels for the 2D-Stage and 44.05 mm for the 3D-Stage were reported, with an inference time of 26.6 ms when deployed on the constrained hardware of the walker. We present a novel approach to patient monitoring and data-driven human-in-the-loop control in the context of smart walkers. It is able to extract a complete and compact body representation in real-time and from inexpensive sensors, serving as a common base for downstream metrics extraction solutions, and Human-Robot interaction applications. Despite promising results, more data should be collected on users with impairments, to assess its performance as a rehabilitation tool in real-world scenarios.",space
10.1016/j.cie.2021.107621,Journal,Computers and Industrial Engineering,scopus,2021-11-01,sciencedirect,Deep deterministic policy gradient algorithm for crowd-evacuation path planning,https://api.elsevier.com/content/abstract/scopus_id/85113275288,"In existing evacuation methods, the large number of pedestrians and the complex environment will affect the efficiency of evacuation. Therefore, we propose a hierarchical evacuation method based on multi-agent deep reinforcement learning (MADRL) to solve the above problem. First, we use a two-level evacuation mechanism to guide evacuations, the crowd is divided into leaders and followers. Second, in the upper level, leaders perform path planning to guide the evacuation. To obtain the best evacuation path, we propose the efficient multi-agent deep deterministic policy gradient (E-MADDPG) algorithm for crowd-evacuation path planning. E-MADDPG algorithm combines learning curves to improve the fixed experience pool of MADDPG algorithm and uses high-priority experience playback strategy to improve the sampling strategy. The improvement increases the learning efficiency of the algorithm. Meanwhile we extract pedestrian motion trajectories from real motion videos to reduce the state space of algorithm. Third, in the bottom layer, followers use the relative velocity obstacle (RVO) algorithm to avoid collisions and follow leaders to evacuate. Finally, experimental results illustrate that the E-MADDPG algorithm can improve path planning efficiency, while the proposed method can improve the efficiency of crowd evacuation.",space
10.1016/j.actaastro.2021.08.002,Journal,Acta Astronautica,scopus,2021-11-01,sciencedirect,Improved orbit predictions using two-line elements through error pattern mining and transferring,https://api.elsevier.com/content/abstract/scopus_id/85112129954,"As the sole orbit data source of space objects for public access, the NORAD catalog provides the basic orbital information in the form of two-line element (TLE) for various space tasks. But the accuracy of orbit prediction (OP) using TLE with the analytical Simplified General Perturbations-4 (SGP4) propagator drops quickly with time, especially for low orbits, significantly limiting the capability of TLEs for advanced space situational awareness (SSA) applications. To enhance the TLE performance over long-duration OP, this paper proposes a data-driven method for improved TLE-based orbit predictions through mining and transferring the orbit error patterns. Two state-of-the-art learning methods, the gradient boosting decision tree (GBDT) and convolutional neural networks (CNN), are applied to model the underlying error patterns, and then the learned models are used as an error corrector to modify the future orbit predictions. Experimental results demonstrate that the time-varying orbit error patterns in the past can be captured by the developed learning framework. As a result, the accuracy of orbit predictions over the future 14 days can be improved by more than 75% in the along-track direction and 90% in the cross-track and radial directions, when the model-predicted errors are used. It also shows that the error pattern learning and application process is computationally efficient, and it could be implemented for near real-time applications. This study demonstrates the promising potential of machine learning (ML)/deep learning (DL) for enhanced SSA capability with the publicly available TLEs.",space
10.1016/j.apenergy.2021.117504,Journal,Applied Energy,scopus,2021-11-01,sciencedirect,Deep reinforcement learning control of electric vehicle charging in the presence of photovoltaic generation,https://api.elsevier.com/content/abstract/scopus_id/85111920114,"In recent years, the importance of electric mobility has increased in response to climate change. The fast-growing deployment of electric vehicles (EVs) worldwide is expected to decrease transportation-related 
                        
                           C
                           
                              
                                 O
                              
                              
                                 2
                              
                           
                        
                      emissions, facilitate the integration of renewables, and support the grid through demand–response services. Simultaneously, inadequate EV charging patterns can lead to undesirable effects in grid operation, such as high peak-loads or low self-consumption of solar electricity, thus calling for novel methods of control. This work focuses on applying deep reinforcement learning (RL) to the EV charging control problem with the objectives to increase photovoltaic self-consumption and EV state of charge at departure. Particularly, we propose mathematical formulations of environments with discrete, continuous, and parametrized action spaces and respective deep RL algorithms to resolve them. The benchmarking of the deep RL control against naive, rule-based, deterministic optimization, and model-predictive control demonstrates that the suggested methodology can produce consistent and employable EV charging strategies, while its performance holds a great promise for real-time implementations.",space
10.1016/j.actaastro.2021.07.012,Journal,Acta Astronautica,scopus,2021-11-01,sciencedirect,"A review of space surgery - What have we achieved, current challenges, and future prospects",https://api.elsevier.com/content/abstract/scopus_id/85110745640,"Major surgical events/incidents onboard are rare but can be catastrophic to any mission. National Aeronautics and Space Administration (NASA) uses the Integrated Medical Model (IMM) to develop an integrated, quantified, evidence-based decision support tool useful for crew health and mission planners to assess risk and design medical systems. In 2017, the IMM of the NASA Human Research Program included a list of 100 medical conditions that could be anticipated during space flight. Of those conditions, 27 are expected to need surgical treatment. Consequently, there has been a continuing interest in surgical capabilities for exploration space flight. The surgical system capabilities aboard all space stations and analogue flights have been designed and implemented with an emphasis on stabilisation, medical evacuation, and ATLS capabilities. However, with future missions to the Moon and Mars, evacuation is not a possibility and astronauts will need to troubleshoot, adapt, and self-administer complex surgical care autonomously.
                  This narrative review aims to examine the published work on surgical care in space, discuss the inherent challenges, and identify scope for future studies. The review evaluates and analyses results from several landmark experiments covering important technical aspects such as basic surgical skills, laparoscopic surgery, robotic surgery, and tele surgery. Relevant studies for the review were identified from the MEDLINE, PubMed, and EMBASE databases. Eligible studies were published between 1960 and June 2021 and were identified using the terms “space surgery”, “microgravity”, “zero gravity”, “weightlessness”, “parabolic flight”, “neutral buoyancy”, and “spaceflight”. Only articles in English were selected and references cited in the selected publications were followed up and included where appropriate. Documents available in the public domain and/or archives of National Space agencies were also included. The search yielded a total of 86 hits including review articles, commentaries, studies, meeting summaries and technical reports submitted to National Space agencies. Results were then filtered for eligible papers relevant to this narrative review. Challenges on a long-duration mission will be unique, unlike anything we have faced so far in the last 60 years of space travel. Despite the progress in space surgery in the last 40 years, there are several challenges to achieving a fully functional surgical care system on any mission outside Low Earth Orbit. The microgravity environment presents unique challenges related to altered physiology as well as mechanics and techniques pertinent to surgical care. Some of the challenges include but are not limited to crew selection, role of prophylactic surgery, adaptation to zero gravity, lack of ground support, training and maintenance of surgical skills and limitation of weight and volume for hardware. Ultrasound imaging, 3D printing and AI-based surgical assistance coupled with robotic surgery have shown promise, but their real efficacy and functionality remains to be tested.",space
10.1016/j.renene.2021.05.155,Journal,Renewable Energy,scopus,2021-11-01,sciencedirect,A deep learning approach towards the detection and recognition of opening of windows for effective management of building ventilation heat losses and reducing space heating demand,https://api.elsevier.com/content/abstract/scopus_id/85107941088,"Building ventilation accounts for up to 30% of the heat loss in commercial buildings and 25% in industrial buildings. To effectively aid the reduction of energy consumption in the building sector, the development of demand-driven control systems for heating ventilation and air-conditioning (HVAC) is necessary. In countries with temperate climates such as the UK, many buildings depend on natural ventilation strategies such as openable windows, which are useful for reducing overheating prevalence during the summer. The manual opening and adjustment of windows by occupants, particularly during the heating season, can lead to substantial heat loss and consequent energy consumption. This could also result in the unnecessary or over ventilation of the space, or the fresh air is more than what is required to ensure adequate air quality. Furthermore, energy losses build up when windows are left open for extended periods. Hence, it is important to develop control strategies that can detect and recognise the period and amount of window opening in real-time and at the same time adjust the HVAC systems to minimise energy wastage and maintain indoor environment quality and thermal comfort. This paper presents a vision-based deep learning framework for the detection and recognition of manual window operation in buildings. A trained deep learning model is deployed into an artificial intelligence-powered camera. To assess the proposed strategy's capabilities, building energy simulation was used with various operation profiles of the opening of the windows based on various scenarios. Initial experimental tests were conducted within a university lecture room with a south-facing window. Deep learning influenced profile (DLIP) was generated via the framework, which uses real-time window detection and recognition data. The generated DLIP were compared with the actual observations, and the initial detection results showed that the method was capable of identifying windows that were opened and had an average accuracy of 97.29%. The results for the three scenarios showed that the proposed strategy could potentially be used to help adjust the HVAC setpoint or alert the occupants or building managers to prevent unnecessary heating demand. Further developments include enhancing the framework ability to detect multiple window opening types and sizes and the detection accuracy by optimising the model.",space
10.1016/j.ymssp.2021.107915,Journal,Mechanical Systems and Signal Processing,scopus,2021-11-01,sciencedirect,Machine learning based frequency modelling,https://api.elsevier.com/content/abstract/scopus_id/85103975336,"Detection of cracks in structures has always been an important research topic in the industrial domain closely associated with aerospace, mechanical, marine and civil engineering. The presence of the cracks alters the dynamic response properties. Hence, it becomes crucial to locate these cracks in the structures to avoid any catastrophic failures and maintain structural integrity and performance. The study's objective is to propose two distinct statistical procedures for conducting the machine learning experiment for modelling the frequency and show the effect of experiment design on the results. In the study, the predictive performance of machine learning models and their ensembles is compared within each experiment design and between two experimental designs for the task of prediction of first six natural frequencies of a fixed ended cracked beam. The study highlights the significance of more than one experimental design to reduce the confirmation bias in the research and discusses the proposed methods' generalizability over the different modelling constraints and modelling parameters. The study also discusses a real-world implementation of the learned machine learning models from the perspective of Bayesian optimization.",space
10.1016/j.probengmech.2021.103173,Journal,Probabilistic Engineering Mechanics,scopus,2021-10-01,sciencedirect,Machine learning based digital twin for stochastic nonlinear multi-degree of freedom dynamical system,https://api.elsevier.com/content/abstract/scopus_id/85117922944,"The potential of digital twin technology is immense, specifically in the infrastructure, aerospace, and automotive sector. However, practical implementation of this technology is not at an expected speed, specifically because of lack of application-specific details. In this paper, we propose a novel digital twin framework for stochastic nonlinear multi-degree of freedom (MDOF) dynamical systems. The proposed digital twin has four modules — (a) a physics-based nominal model, (b) a data collection module, (c) algorithm for real-time update of the digital twin and (d) module for predicting future state. The modules for real-time update and prediction are based on the so-called gray-box modeling approach, and utilizes both physics based and data driven frameworks; this enables the proposed digital twin to generalize and predict future responses. The gray box modeling framework used within the digital twin is developed by coupling Bayesian filtering and machine learning algorithm. Although, the proposed digital twin can be used with any machine learning regression algorithm, we have used Gaussian process in this study. Performance of the proposed approach is illustrated using two examples. Results obtained indicate the applicability and excellent performance of the proposed digital twin framework.",space
10.1016/j.ins.2021.06.008,Journal,Information Sciences,scopus,2021-10-01,sciencedirect,Discriminative group-sparsity constrained broad learning system for visual recognition,https://api.elsevier.com/content/abstract/scopus_id/85113805302,"Broad Learning System (BLS) is an emerging network paradigm that has received considerable attention in the regression and classification fields. However, there are two deficiencies which seriously hinder its deployment in real applications. The first one is the internal correlations among samples are not fully considered in the modeling process. Second, the strict binary label matrix utilized in BLS provides little freedom for classification. In this paper, to address the above issues, we propose to impose group-sparsity constraints on the class-specific transformed features and label error terms, respectively. The effect is not only the more appropriate margins between data can be preserved, but also the learnt label space can be flexible for recognition. As a result, the obtained projection matrix can show more vital discriminative ability. Further, we employ the alternating direction method of multipliers to solve the resulting optimization problem. Extensive experiments and analysis on diverse benchmark databases are carried out to confirm our proposed model’s superiority in comparison with other competing classification methods.",space
10.1016/j.jappgeo.2021.104434,Journal,Journal of Applied Geophysics,scopus,2021-10-01,sciencedirect,A convolutional neural network approach to electrical resistivity tomography,https://api.elsevier.com/content/abstract/scopus_id/85112776120,"Electrical resistivity tomography (ERT) is an ill-posed and non-linear inverse problem commonly solved through deterministic gradient-based methods. These algorithms guarantee fast convergence toward the final solution but hinder accurate uncertainty assessments. On the contrary, numerical Markov Chain Monte Carlo algorithms provide accurate uncertainty appraisals but at the expense of a considerable computational effort. In this work, we develop a novel approach to ERT that guarantees an extremely fast inversion process and reliable uncertainty appraisals. The implemented method combines a Discrete Cosine Transform (DCT) reparameterization of data and model spaces with a Convolutional Neural Network. The CNN is employed to learn the inverse non-linear mapping between the DCT-compressed data and the DCT-compressed 2-D resistivity model. The DCT is an orthogonal transformation that here acts as an additional feature extraction technique that reduces the dimensionality of the input and output of the network. The DCT also acts as a regularization operator in the model space that significantly reduces the number of unknown parameters and the ill-conditioning of the inversion procedure, thereby preserving the spatial continuity of the resistivity values in the recovered solution. The estimation of model uncertainties is a key step of geophysical inverse problems and hence we implement a Monte Carlo simulation framework that propagates onto the estimated model the uncertainties related to both noise contamination and network approximation (the so-called modeling error). We first apply the approach to synthetic data to investigate its robustness in case of erroneous assumptions on the noise and model statistics used to generate the training set. Then, we demonstrate the applicability of the method through inverting real data measured along a river embankment. We also demonstrate that transfer learning avoids retraining the network from scratch when the statistical properties of training and target sets are different. Our tests confirm the suitability of the proposed approach, opening the possibility to estimate the subsurface resistivity values and the associated uncertainties in near real-time.",space
10.1016/j.oceaneng.2021.109680,Journal,Ocean Engineering,scopus,2021-10-01,sciencedirect,Linear reduced order method for design-space dimensionality reduction and flow-field learning in hull form optimization,https://api.elsevier.com/content/abstract/scopus_id/85112742342,"In the earlier stage of hull form optimization design, a series of design variables is usually needed to control the hull shape, so as to find optimal hull forms with better performance. In the surrogate-based hydrodynamic performance optimization for ships, with the increase of the dimensionality of design space, the number of new sample hulls to construct surrogate model needs to be larger, which will bring a large amount of calculation. Through reduced order method, the dimensionality of the optimization design space can be reduced while keeping the deformation range of the original design space to a great extent, for instance, using the linear combination of a smaller number of bases to represent the deformation range. In addition, in the later stage of hull form optimization design, flow field results of the new sample hulls can be fully utilized to do the dimensionality reduction multi-physics field learning. In this paper, the principle of the Proper Orthogonal Decomposition method is used and briefly introduced, the steps of dimensionality reduction of the design space are shown then, and some important problems for the design-space dimensionality reduction in the specific field of hull form optimization, such as retainability of fixed control points, irrelevance of the relative order of data to dimensionality reduction results, and decision of the new design space range after dimensionality reduction, are deep analyzed. Furthermore, taking the resistance optimization of the modified Wigley ship as an example, the specific application and error analysis of the dimensionality reduction method for design-space dimensionality reduction in the earlier stage of hull form optimization and the multi-physics field learning in the later stage of hull form optimization are given, and the applicability and reliability of the method are demonstrated by analyzing the influence of mode order and sample number on reconstruction effect of the hull shape or flow field, and the prediction effect of flow field for not-in-the-database new hull form in detail. Results show that the linear dimensionality reduction method can reduce samples needed for optimization, thus reduce the amount of calculation for the surrogate-based hull form optimization, and be used for quick prediction of multi-physics fields of any new form in the design space. Furthermore, it can not only be applied to the sensitivity analysis or a Pareto frontier selection in comprehensive performance optimization of hull form based on CFD, but also be implemented in the real-time forecast of the flow field and influence analysis of the ship performance when adjusting the hull form (or hull appendages).",space
10.1016/j.asoc.2021.107720,Journal,Applied Soft Computing,scopus,2021-10-01,sciencedirect,TDMatcher: A topic-based approach to task-developer matching with predictive intelligence for recommendation,https://api.elsevier.com/content/abstract/scopus_id/85111305283,"Artificial Intelligence is currently gripping the business world, which is the next step on the journey from Big Data to full automation. As crowdsourcing has been widely adopted by more enterprises and developers, the software crowdsourcing platform is able to collect enough data. Therefore, we introduce predictive intelligence to solve complex problems. This provides a bridge between software developers and enterprises: developers look for suitable tasks, whose aim is to gain revenues with respect to their interests and abilities; enterprises look for developers that are able to complete crowdsourcing tasks and/or solve hard problems. One main problem is the prediction challenge, i.e., how to perfectly predict the developers for the software crowdsourcing tasks and make appropriate recommendations. To solve the problem, this paper introduces predictive intelligence and proposes TDMatcher, which can effectively perform task-developer pairs prediction and recommendations for software crowdsourcing. First, we builds a unified model for tasks and developers such that they can be matched in the same domain space. Second, we quantitatively measures the matching degree between tasks and developers. Third, we randomly generates potential matchings between developers and crowdsourcing tasks and then employs an MCMC sampling approach to optimize the whole process. Highly matched task-developer pairs can be achieved in the sampling process. In order to solve the cold-start problem, we constructs a social network for each new developer, which indicates that the developer’s interests/abilities to be modeled We implemented TDMatcher and evaluated it against the state-of-the-art approaches on the real-world dataset. The experimental results clearly demonstrate the superiority of TDMatcher. We measured our proposed TDMatcher through the accuracy, diversity and Harmonic Mean of TDMatcher, and found that: (1) TDMatcher outperforms the state-of-the-arts by 15+% in the prediction accuracy and 30% in diversity; and (2) TDMatcher achieves a balance between accuracy and diversity. We believe that TDMatcher provides crowdsourcing platforms with much more capabilities in finding appropriate developers to complete crowdsourcing tasks or vice versa.",space
10.1016/j.jobe.2021.102799,Journal,Journal of Building Engineering,scopus,2021-10-01,sciencedirect,A step-by-step numerical method for optimization of mechanical ventilation in deep underground enclosed parking lots: A case-design study,https://api.elsevier.com/content/abstract/scopus_id/85110290956,"To reduce polluted air, mechanical ventilation (MV) is essential for enclosed parking spaces. The traditional prescriptive design method, the index-based design, cannot guarantee ventilation performance of each fan in the enclosed parking lot. To solve this, the performance-based design approach is the best alternative that specifically addresses performance-related criteria of MV system. In this study of practice-based learning in a real construction project, we proposed a unique design optimization methodology for improving the performance of MV systems using iterative, step-by-step computational fluid dynamics (CFD) simulation. Five numerical simulation levels on seven engineering steps and a techno-economic analysis were utilized. Ultimately, fan selection was based on calculating the airflow and pressure requirements of a MV system and finding a fan of the right design to hedge against the risk of system effect and surging phenomenon. Results showed that a stable fan selection with an error of 5% of the design air flow rate could be implemented by repeating numerical analysis for the performance optimization of the MV system. When the MV design optimization was applied to the reference parking lot, the number of fans could be reduced by 30%, and energy demand of the MV system by at least 16%. Consequently, the annual energy savings was projected to recover the increase in initial investment cost in about 5.8 years. The key contribution of this research is that it overcame the limitations of the traditional index-based design for selecting the optimal fans of MV systems.",space
10.1016/j.rcim.2021.102176,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2021-10-01,sciencedirect,Robotic grasping: from wrench space heuristics to deep learning policies,https://api.elsevier.com/content/abstract/scopus_id/85104603575,"The robotic grasping task persists as a modern industry problem that seeks autonomous, fast implementation, and efficient techniques. Domestic robots are also a reality demanding a delicate and accurate human–machine interaction, with precise robotic grasping and handling. From decades ago, with analytical heuristics, to recent days, with the new deep learning policies, grasping in complex scenarios is still the aim of several works’ that propose distinctive approaches. In this context, this paper aims to cover recent methodologies’ development and discuss them, showing state-of-the-art challenges and the gap to industrial applications deployment. Given the complexity of the related issue associated with the elaborated proposed methods, this paper formulates some fair and transparent definitions for results’ assessment to provide researchers with a clear and standardised idea of the comparison between the new proposals.",space
10.1016/j.knosys.2021.107302,Journal,Knowledge-Based Systems,scopus,2021-09-27,sciencedirect,A novel deep quantile matrix completion model for top-N recommendation[Formula presented],https://api.elsevier.com/content/abstract/scopus_id/85111890353,"Matrix completion models have been receiving keen attention due to their wide applications in science and engineering. However, the majority of these models assumes a symmetric noise distribution in their completion processes and uses conditional mean to characterize data distribution in a data set, the assumption of which incurs noticeable bias toward outliers. Recognizing the fact that noise distribution tends to be asymmetric in the real-world, this paper proposes a novel Deep Quantile Matrix Completion model, abbreviated as DQMC, which aims to accurately capture noise distribution in a data set by modeling conditional quantile of the data set instead of its conditional mean as traditionally handled by many state-of-the-art methods. Implemented via a deep computing paradigm, the newly proposed model maps a data set from its input space to the latent spaces through a two-branched deep autoencoder network. Such a mapping can effectively capture complex information latent in the data set. The proposed model is empowered by two key designed elements, including: (1) its two-branched deep autoencoder network that provides a flexible computing pathway to attain completion results with a high quality; (2) the introduction of a quantile loss function in combination with the proposed deep network, leading to a new unsupervised learning algorithm for tackling the matrix completion tasks with a superior capability. Comparative experimental results consistently demonstrate the superiority of the proposed DQMC model in conducting the top-N recommendation tasks involving both explicit and implicit rating data sets with respect to a series of state-of-the-art recommendation algorithms.",space
10.1016/j.neucom.2021.04.091,Journal,Neurocomputing,scopus,2021-09-24,sciencedirect,Quantum maximum mean discrepancy GAN,https://api.elsevier.com/content/abstract/scopus_id/85107052048,"Generative adversarial network (GAN) has shown profound power in machine learning. It inspires many researchers from other fields to create powerful tools for various tasks, including quantum state preparation, quantum circuit translation, and so on. It is known as classical techniques cannot efficiently simulate the quantum system, and the existing works haven’t investigated the quantum version of maximum mean discrepancy as the metric in learning models and applied it to quantum data. In this paper, we propose a metric named quantum maximum mean discrepancy (qMMD), which can be used to measure the distance between quantum data in Hilbert space. Based on the qMMD, we then design a quantum generative adversarial model, named qMMD-GAN, under the hybrid quantum–classical methods. We also provide the construction of qMMD-GAN that can be easily implemented on a quantum device. We demonstrate the power of our qMMD-GAN by applying it to a crucial real-world application that is generating an unknown quantum state. Our numerical experiments show that qMMD-GAN has a competitive performance compared to existing results. We believe that the hybrid-based models will not only be applied to physics research but provide a new direction for improving classical data processing tasks.",space
10.1016/j.anucene.2021.108355,Journal,Annals of Nuclear Energy,scopus,2021-09-15,sciencedirect,Large-scale design optimisation of boiling water reactor bundles with neuroevolution,https://api.elsevier.com/content/abstract/scopus_id/85105888765,"We combine advances in deep reinforcement learning (RL) with evolutionary computation to perform large-scale optimisation of boiling water reactor (BWR) bundles using CASMO4/SIMULATE3 codes; capturing fine details, radial/axial fuel heterogeneity, and real-world constraints. RL constructs neural networks that learn how to assign fuel and poison enrichment by narrowing the search space into the areas where human/physics knowledge demonstrate merit. Evolution strategies diversify the search in these areas, through obtaining guidance from RL candidates. With very efficient/parallel implementation, our optimisation approach is able to solve a coupled multi-zone BWR bundle optimisation with 
                        
                           ~
                        
                     40 constraints. The methodology is applied to a GE14-10×10 bundle, showing the ability of neuroevolution to find 
                        
                           ~
                        
                     100 feasible designs. The optimal bundle has 7 axial zones with non-uniform enrichment radially and axially. The results of this work also demonstrate that our neuroevolution methodology is sufficiently generic to adapt to other assembly and reactor designs with minor adjustments.",space
10.1016/j.pmcj.2021.101459,Journal,Pervasive and Mobile Computing,scopus,2021-09-01,sciencedirect,REAM: A Framework for Resource Efficient Adaptive Monitoring of Community Spaces,https://api.elsevier.com/content/abstract/scopus_id/85113273806,"Nowadays, many Internet-of-Things (IoT) devices with rich sensors and actuators are being deployed to monitor community spaces. The data generated by these devices are analyzed and turned into actionable information by analytics operators. In this article, we present a Resource Efficient Adaptive Monitoring (REAM) framework at the edge that adaptively selects workflows of devices and analytics to maintain an adequate quality of information for the applications at hand while judiciously consuming the limited resources available on edge servers. Since community spaces are complex and in a state of continuous flux, developing a one-size-fits-all model that works for all spaces is infeasible. The REAM framework utilizes reinforcement learning agents that learn by interacting with each community space and make decisions based on the state of the environment in each space and other contextual information. We demonstrate the resource-efficient monitoring capabilities of REAM on two real-world testbeds in Orange County, USA and NTHU, Taiwan, where we show that community spaces using REAM can achieve 
                        
                           >
                           90
                           %
                        
                      monitoring accuracy while incurring 
                        
                           ∼
                           50
                           %
                        
                      less resource consumption costs compared to existing static monitoring approaches. We also show REAM’s awareness of network link quality in its decision-making, resulting in a 42% improvement in accuracy over network agnostic approaches.",space
10.1016/j.egyai.2021.100101,Journal,Energy and AI,scopus,2021-09-01,sciencedirect,Development of a Soft Actor Critic deep reinforcement learning approach for harnessing energy flexibility in a Large Office building,https://api.elsevier.com/content/abstract/scopus_id/85109095539,"This research is concerned with the novel application and investigation of ‘Soft Actor Critic’ based deep reinforcement learning to control the cooling setpoint (and hence cooling loads) of a large commercial building to harness energy flexibility. The research is motivated by the challenge associated with the development and application of conventional model-based control approaches at scale to the wider building stock. Soft Actor Critic is a model-free deep reinforcement learning technique that is able to handle continuous action spaces and which has seen limited application to real-life or high-fidelity simulation implementations in the context of automated and intelligent control of building energy systems. Such control techniques are seen as one possible solution to supporting the operation of a smart, sustainable and future electrical grid. This research tests the suitability of the technique through training and deployment of the agent on an EnergyPlus based environment of the office building. The agent was found to learn an optimal control policy that was able to minimise energy costs by 9.7% compared to the default rule-based control scheme and was able to improve or maintain thermal comfort limits over a test period of one week. The algorithm was shown to be robust to the different hyperparameters and this optimal control policy was learnt through the use of a minimal state space consisting of readily available variables. The robustness of the algorithm was tested through investigation of the speed of learning and ability to deploy to different seasons and climates. It was found that the agent requires minimal training sample points and outperforms the baseline after three months of operation and also without disruption to thermal comfort during this period. The agent is transferable to other climates and seasons although further retraining or hyperparameter tuning is recommended.",space
10.1016/j.engappai.2021.104316,Journal,Engineering Applications of Artificial Intelligence,scopus,2021-09-01,sciencedirect,Deep replacement: Reinforcement learning based constellation management and autonomous replacement,https://api.elsevier.com/content/abstract/scopus_id/85108677220,"The Deep Reinforcement Learning (DRL) algorithm, Proximal Policy Optimization (PPO2), is deployed on a custom spacecraft (S/C) build and loss model to determine if an Artificial Intelligence (AI) can learn to monitor satellite constellation health and determine an optimal replacement strategy. A custom environment is created to simulate how S/C are built, launched, generate revenue, and finally decay. The reinforcement learning agent successfully learned an optimal policy for two models: a Simplified Model where the financial cost of actions is ignored; and an Advanced Model where the financial cost of actions is a major element. In both models the AI monitors the constellations and takes multiple strategic and tactical actions to replace satellites to maintain constellation performance. The Simplified Model showed that the PPO2 algorithm was able to converge on an optimal solution after 
                        ∼
                     200,000 simulations. The Advanced Model was much more difficult for the AI to learn, and thus, the performance drops during the early episodes, but eventually converges to an optimal policy at 
                        ∼
                     25,000,000 simulations. With the Advanced Model, the AI is taking actions that are successfully providing strategies for constellation management and satellite replacements which include these actions’ financial implications. Thus, the methods in this paper provide initial research developments towards a real-world tool and an AI application that can aid various Aerospace businesses in managing Low Earth Orbit (LEO) constellations. This type of AI application may become imperative for deploying and maintaining small satellite mega-constellations.",space
10.1016/j.adhoc.2021.102562,Journal,Ad Hoc Networks,scopus,2021-09-01,sciencedirect,Developing novel low complexity models using received in-phase and quadrature-phase samples for interference detection and classification in Wireless Sensor Network and GPS edge devices,https://api.elsevier.com/content/abstract/scopus_id/85107952090,"Despite Wireless Sensor Networks (WSNs) significantly developing over the past decade, these networks, like most wireless networks, remain susceptible to malicious interference and spectrum coexistence. Other vulnerabilities arise as WSN applications adopt open standards and typically resource and energy-constrained commercial-off-the-shelf equipment. Deployments include safety-critical applications such as the internet of things, medical, aerospace and space and deep-sea exploration. To manage safety and privacy requirements across such a diverse wireless landscape, security on wireless edge devices needs improvement while maintaining low complexity. This paper improves wireless edge device security by developing a novel intelligent interference diagnostic framework. Received in-phase (I) and quadrature-phase (Q) samples are exclusively utilized to detect modern, subtle and traditional crude jamming attacks. This I/Q sample utilization inherently enables decentralized decision-making, where the low-order features were extracted in a previous study focused on classifying typical 2.4–2.5 GHz wireless signals. The associated optimal intelligent models are leveraged as the foundation for this paper’s work. Initially, Matlab Monte Carlo simulations investigate the ideal case, which incorporates no hardware limitations, identifies the required data type of signal interactions and motivates a hardware investigation. Software-defined radios (SDRs) collect the required live over-the-air I/Q data and transmit matched signal (ZigBee) and continuous-wave interference in developed ZigBee wireless testbeds. Low complexity supervised machine learning models are developed based exclusively on the low-order features and achieve an average accuracy among the developed models above 98%. The designed methodology involves examining ZigBee over-the-air data for artificial jamming and SDR jamming of ZigBee signals transmitted from SDR and commercial (XBee) sources. This approach expands to a legitimate node classification technique and an overall algorithm for wireless edge device interference diagnostic tools. The investigation includes developing Support Vector Machine, XGBoost and Deep Neural Network (DNN) models, where XGBoost is optimal. Adapting the optimized models to global positioning system signals establishes the transferability of the designed methodology. Implementing the designed approaches on a Raspberry Pi embedded device examines a relatively resource-constrained deployment. The primary contribution is the real experimentally validated interference diagnostic framework that enables independent device operation, as no channel assumptions, network-level information or spectral images are required. Developed models exclusively use I/Q data low-order features and achieve high accuracy and generalization to unseen data.",space
10.1016/j.scs.2021.103071,Journal,Sustainable Cities and Society,scopus,2021-09-01,sciencedirect,A stochastic machine learning based approach for observability enhancement of automated smart grids,https://api.elsevier.com/content/abstract/scopus_id/85107608455,"This paper develops a machine learning aggregated integer linear programming approach for the full observability of the automated smart grids by positioning of micro-synchrophasor units, taking into account the reconfigurable structure of the distribution systems. The proposed stochastic approach presents a strategy occurring in several stages to micro-synchrophasor unit positioning based on the load level and demand in the system and based on the pre-determined sectionalizing and tie switches. Such a technique can also deploy the zero-injection limitations of the model and reduce the search space of the problem. Moreover, a novel method based on whale optimization method (WOM) is introduced to simultaneously enhance the reliability indices in order to specify the optimum topology for each phase and reduce the costs of power losses and customer interruptions. Although the problem of micro-synchrophasor placement is formulated in an integer linear programming framework, the restructuring technique is resolved on the basis of the WOM heuristic approach. Considering the uncertainty due to the metering devices or forecast errors, a stochastic framework based on point estimation is deployed to handle the uncertainty effects. The simulation and numerical results on a real system verify that the proposed method assures visibility of the distribution network pre and post reconfiguration in the time horizon of the planning. Furthermore, the results show that the system observability can be guaranteed at different load levels even though the system experiences different reconfiguration and topologies.",space
10.1016/j.sysarc.2021.102183,Journal,Journal of Systems Architecture,scopus,2021-09-01,sciencedirect,Memory-efficient deep learning inference with incremental weight loading and data layout reorganization on edge systems,https://api.elsevier.com/content/abstract/scopus_id/85107073021,"Pattern recognition applications such as face recognition and agricultural product detection have drawn a rapid interest on Cyber–Physical–Social-Systems (CPSS). These CPSS applications rely on the deep neural networks (DNN) to conduct the image classification. However, traditional DNN inference models in the cloud could suffer from network delay fluctuations and privacy leakage problems. In this regard, current real-time CPSS applications are preferred to be deployed on edge-end embedded devices. Constrained by the computing power and memory limitations of edge devices, improving the memory management efficacy is the key to improving the quality of service for model inference. First, this study explored the incremental loading strategy of model weights for the model inference. Second, the memory space at runtime is optimized through data layout reorganization from the spatial dimension. In particular, the proposed schemes are orthogonal to existing models. Experimental results demonstrate that the proposed approach reduced the memory consumption by 61.05% without additional inference time overhead.",space
10.1016/j.asoc.2021.107418,Journal,Applied Soft Computing,scopus,2021-09-01,sciencedirect,Development and experimental realization of an adaptive neural-based discrete model predictive direct torque and flux controller for induction motor drive,https://api.elsevier.com/content/abstract/scopus_id/85104714129,"This paper develops a neural network-based discrete predictive direct torque and flux control (NNPDTFC) for induction motor drive with the space vector modulation (SVM) technique. Moreover, this SVM technique with NNPDTFC is implemented to activate the inverter in the two-level operation and the performance is compared with the conventional PI direct torque and flux control (PIDTFC) technique. The PSO based model predictive control incorporated with the neural network is developed here in the NNPDTFC and is analyzed using MATLAB software. Disturbance reduction, simple control, and real-time implementation are the major features of NNPDTFC and it also enhances the transient performance of the motor drive by reducing settling time and peak overshoot. In addition, the flux and the torque ripples are significantly improved using the proposed NNPDTFC technique which is extensively used for the fast dynamic response of the induction motor drives as compared to PIDTFC. In order to show the potentiality of the proposed controller, a prototype controller is developed and validated with the laboratory setup and the control signals are generated for both NNPDTFC and PIDTFC using a low-cost Digital signal processor (DSP) controller which is fed to the induction motor of 3.7 kW capacity in the real-time platform. It is observed that the results with NNPDTFC are not only found to be extremely satisfactory even with the system and parameter uncertainties and external load perturbations but also, it produces enhanced dynamic as well as steady-state performance along with the reduced ripples in the signal flux, torque, and current compared to that of PIDTFC.",space
10.1016/j.jprocont.2021.06.004,Journal,Journal of Process Control,scopus,2021-08-01,sciencedirect,Online reinforcement learning for a continuous space system with experimental validation,https://api.elsevier.com/content/abstract/scopus_id/85111075597,"Reinforcement learning (RL) for continuous state/action space systems has remained a challenge for nonlinear multivariate dynamical systems even at a simulation level. Implementing such schemes for real-time control is still of a difficulty and remains largely unanswered. In this study, several critical strategies for practical implementation of RL are developed, and a multivariable, multi-modal, hybrid three-tank (HTT) physical process is utilized to illustrate the proposed strategies. A successful real-time implementation of RL is reported. The first step is a meta-heuristic first principles model parameter optimization, where a custom pseudo random binary signal (PRBS) is used to obtain open-loop experimental data. This is followed by in silico asynchronous advantage actor–critic (A3C/A-A2C) based policy learning. In the second step, three different approaches (namely proximal learning, single trajectory learning, and multiple trajectory learning) are utilized to explore the state/action space. In the final step, online learning (A2C) using the best in silico policy on the real process using a socket connection is established. The extent of exploration (EoE, a measure of exploration) is proposed as a parameter for quantifying exploration of the state/action space. While the online sample efficiency of RL application is enhanced, a soft constraint based constrained learning is proposed and validated. With considerations of the proposed strategies, this work demonstrates the possibility of applying RL to solve practical control problems.",space
10.1016/j.isprsjprs.2021.05.019,Journal,ISPRS Journal of Photogrammetry and Remote Sensing,scopus,2021-08-01,sciencedirect,Rapid and large-scale mapping of flood inundation via integrating spaceborne synthetic aperture radar imagery with unsupervised deep learning,https://api.elsevier.com/content/abstract/scopus_id/85108716493,"Synthetic aperture radar (SAR) has great potential for timely monitoring of flood information as it penetrates the clouds during flood events. Moreover, the proliferation of SAR satellites with high spatial and temporal resolution provides a tremendous opportunity to understand the flood risk and its quick response. However, traditional algorithms to extract flood inundation using SAR often require manual parameter tuning or data annotation, which presents a challenge for the rapid automated mapping of large and complex flooded scenarios. To address this issue, we proposed a segmentation algorithm for automatic flood mapping in near-real-time over vast areas and for all-weather conditions by integrating Sentinel-1 SAR imagery with an unsupervised machine learning approach named Felz-CNN. The algorithm consists of three phases: (i) super-pixel generation; (ii) convolutional neural network-based featurization; (iii) super-pixel aggregation. We evaluated the Felz-CNN algorithm by mapping flood inundation during the Yangtze River flood in 2020, covering a total study area of 1,140,300 km2. When validated on fine-resolution Planet satellite imagery, the algorithm accurately identified flood extent with producer and user accuracy of 93% and 94%, respectively. The results are indicative of the usefulness of our unsupervised approach for the application of flood mapping. Meanwhile, we overlapped the post-disaster inundation map with a 10-m resolution global land cover map (FROM-GLC10) to assess the damages to different land cover types. Of these types, cropland and residential settlements were most severely affected, with inundation areas of 9,430.36 km2 and 1,397.50 km2, respectively, results that are in agreement with statistics from relevant agencies. Compared with traditional supervised classification algorithms that require time-consuming data annotation, our unsupervised algorithm can be deployed directly to high-performance computing platforms such as Google Earth Engine and PIE-Engine to generate a large-spatial map of flood-affected areas within minutes, without time-consuming data downloading and processing. Importantly, this efficiency enables the fast and effective monitoring of flood conditions to aid in disaster governance and mitigation globally.",space
10.1016/j.coastaleng.2021.103919,Journal,Coastal Engineering,scopus,2021-08-01,sciencedirect,Satellite optical imagery in Coastal Engineering,https://api.elsevier.com/content/abstract/scopus_id/85106393729,"This Short Communication provides a Coastal Engineering perspective on present and emerging capabilities of satellite optical imagery, including real-world applications that can now be realistically implemented from the desktop. Significantly, at the vast majority of locations worldwide, satellite remote sensing is currently the only source of information to complement much more limited in-situ instrumentation for land and sea mapping, monitoring and measurement. Less well recognised is that publicly available, routinely sampled and now easily accessible optical imagery covering virtually every position along the world's coastlines already spans multiple decades. In the past five years the common obstacles of (1) limited access to high-performance computing and (2) specialist remote sensing technical expertise, have been largely removed. The emergence of several internet-accessible application programming interfaces (APIs) now enable applied users to access petabytes of satellite imagery along with the necessary tools and processing power to extract, manipulate and analyse information of practical interest. Following a brief overview and timeline of civilian Earth observations from space, satellite-derived shorelines (SDS) and satellite-derived bathymetry (SDB) are used to introduce and demonstrate some of the present real-world capabilities of satellite optical imagery most relevant to coastal professionals and researchers. These practical examples illustrate the use of satellite imagery to monitor and quantify both engineered and storm-induced coastline changes, as well as the emerging potential to obtain seamless topo/bathy surveys along coastal regions. Significantly, timescales of satellite-derived changes at the coast can range from decades to days, with spatial scales of interest extending from individual project sites up to unprecedented regional and global studies. While we foresee the uptake and routine use of satellite-derived information becoming quickly ubiquitous within the Coastal Engineering profession, on-ground observations are – and in our view will remain - fundamentally important. Compared to precision in-situ instrumentation, present intrinsic limitations of satellites are their relatively low rates of revisit and decimetre spatial accuracy. New satellite advances including ‘video from space’ and the potential to combine Earth observation with numerical and data-driven coastal models through assimilation and artificial intelligence are advances that we foresee will have future major impact in Coastal Engineering.",space
10.1016/j.buildenv.2021.107929,Journal,Building and Environment,scopus,2021-08-01,sciencedirect,MOOSAS – A systematic solution for multiple objective building performance optimization in the early design stage,https://api.elsevier.com/content/abstract/scopus_id/85105785192,"There is great potential for building performance optimization (BPO) in the early design stage, but there is still a lack of methods, algorithms, and tools to support the BPO process in this stage. Through a comprehensive review, this study identified three critical issues that affect the implementation of the BPO process in the early design stage: model integration, real-time performance analysis, and interactive optimization design. This study provides a systematic solution to these three critical issues. In terms of model integration, a feature-based and graph-based 3D building space recognition algorithm is proposed to automatically convert the computer-aided design (CAD model) into a computer-aided engineering model (CAE model). In terms of real-time performance analysis, a simplified physical method, an HPC-accelerated method, and an AI-based method are explored, and a real-time energy modeling module and a real-time daylighting analysis module are developed. In terms of the interactive optimization design, a preference-based multi-objective BPO design algorithm that can consider user preferences is proposed to make full use of the decision-making ability of humans and the computing power of machines and significantly improve the optimization efficiency and result satisfaction. Based on the systematic solution, a multi-objective BPO design software, MOOSAS, is developed. MOOSAS allows real-time performance feedback, dynamic parameter analysis, and interactive optimization, supporting the BPO process in the early design stage. The innovations of this study are as follows: first, this study proposes a systematic solution to the three critical issues of the BPO process, i.e., model integration, real-time performance analysis, and interactive optimization design; second, this study develops a multi-objective BPO design software (MOOSAS) for the early design stage.",space
10.1016/j.jmsy.2021.04.005,Journal,Journal of Manufacturing Systems,scopus,2021-07-01,sciencedirect,LearningADD: Machine learning based acoustic defect detection in factory automation,https://api.elsevier.com/content/abstract/scopus_id/85106283308,"Defect inspection of glass bottles in the beverage industrial is of significance to prevent unexpected losses caused by the damage of bottles during manufacturing and transporting. The commonly used manual methods suffer from inefficiency, excessive space consumption, and beverage wastes after filling. To replace the manual operations in the pre-filling detection with improved efficiency and reduced costs, this paper proposes a machine learning based Acoustic Defect Detection (LearningADD) system. Moreover, to realize scalable deployment on edge and cloud computing platforms, deployment strategies especially partitioning and allocation of functionalities need to be compared and optimized under realistic constraints such as latency, complexity, and capacity of the platforms. In particular, to distinguish the defects in glass bottles efficiently, the improved Hilbert-Huang transform (HHT) is employed to extend the extracted feature sets, and then Shuffled Frog Leaping Algorithm (SFLA) based feature selection is applied to optimize the feature sets. Five deployment strategies are quantitatively compared to optimize real-time performances based on the constraints measured from a real edge and cloud environment. The LearningADD algorithms are validated by the datasets from a real-life beverage factory, and the F-measure of the system reaches 98.48 %. The proposed deployment strategies are verified by experiments on private cloud platforms, which shows that the Distributed Heavy Edge deployment outperforms other strategies, benefited from the parallel computing and edge computing, where the Defect Detection Time for one bottle is less than 2.061 s in 99 % probability.",space
10.1016/j.petrol.2021.108488,Journal,Journal of Petroleum Science and Engineering,scopus,2021-07-01,sciencedirect,Data-driven machine learning for accurate prediction and statistical quantification of two phase flow regimes,https://api.elsevier.com/content/abstract/scopus_id/85102536455,Two different two-phase flow regimes including slug and dispersed flows are examined through the implementation of system identification methods to attain reduced-order models. The obtained models accurately capture the flow dynamics of the studied regimes. The models also provide state-space frequency by defining the transfer functions. The system identification results are compared with those of the bidirectional neural network to predict the phase fraction of the considered two-phase flows. The result of long short-term memory shows correlations of 91% between the real and predicted phase fractions.,space
10.1016/j.ymssp.2020.107510,Journal,Mechanical Systems and Signal Processing,scopus,2021-06-16,sciencedirect,Metric-based meta-learning model for few-shot fault diagnosis under multiple limited data conditions,https://api.elsevier.com/content/abstract/scopus_id/85100211264,"The real-world large industry has gradually become a data-rich environment with the development of information and sensor technology, making the technology of data-driven fault diagnosis acquire a thriving development and application. The success of these advanced methods depends on the assumption that enough labeled samples for each fault type are available. However, in some practical situations, it is extremely difficult to collect enough data, e.g., when the sudden catastrophic failure happens, only a few samples can be acquired before the system shuts down. This phenomenon leads to the few-shot fault diagnosis aiming at distinguishing the failure attribution accurately under very limited data conditions. In this paper, we propose a new approach, called Feature Space Metric-based Meta-learning Model (FSM3), to overcome the challenge of the few-shot fault diagnosis under multiple limited data conditions. Our method is a mixture of general supervised learning and episodic metric meta-learning, which will exploit both the attribute information from individual samples and the similarity information from sample groups. The experiment results demonstrate that our method outperforms a series of baseline methods on the 1-shot and 5-shot learning tasks of bearing and gearbox fault diagnosis across various limited data conditions. The time complexity and implementation difficulty have been analyzed to show that our method has relatively high feasibility. The feature embedding is visualized by t-SNE to investigate the effectiveness of our proposed model.",space
10.1016/j.compeleceng.2021.107121,Journal,Computers and Electrical Engineering,scopus,2021-06-01,sciencedirect,Efficient neural networks for edge devices,https://api.elsevier.com/content/abstract/scopus_id/85103242184,"Due to limited computation and storage resources of industrial internet of things (IoT) edge devices, many emerging intelligent industrial IoT applications based on deep neural networks (DNNs) heavily depend on cloud computing for computation and storage. However, cloud computing faces technical issues in long latency, poor reliability, and weak privacy, resulting in the need for on-device computation and storage. On-device computation is essential for many time-critical industrial IoT applications, which require real-time data processing. In this paper, we review three major research areas for on-device computation, specifically quantization, pruning, and network architecture design. The three techniques could enable a DNN model to be deployed on edge devices for real-time computation and storage, mainly due to the reduction of computation and space complexity. More importantly, these techniques could make DNNs applicable to industrial IoT devices.",space
10.1016/j.micpro.2021.103988,Journal,Microprocessors and Microsystems,scopus,2021-06-01,sciencedirect,Computer simulation of urban garden landscape design based on FPGA and neural network,https://api.elsevier.com/content/abstract/scopus_id/85099498199,"Digital Landscape is a combination of the system and the computer software and hardware system of a high simulation model. The author analyzes the application of computer simulation in landscape design and value analysis of a city garden. In the computer-aided design, the importance of digitizing information in the landscape design process, mainly human and the interaction of computer, is reflected in the digital model's creation and multimedia performance, becoming more and more evident. To form a two-dimensional or three-dimensional spatial data, to realize real-time, statistical Analysis, using the human living environment, multi-dimensional, efficient, and humane, and environmental landscape plan to more rational and practical, used the computer simulation techniques. Effective use of urban rainwater, to reduce the flooding of urban areas, it is possible to alleviate the water crisis, the organic combination of rainwater can be used in the course of the construction of the urban landscape as well as make-up landscape, visual beautification has optimized the ecosystem, and from many rainwater utilization functions; These functions in landscape design, rainwater garden, It can be realized the rooftop garden, and the city's green. The construction and sustainable economy and the promotion of the ecological park's social development will positively sign. Suitable for rainwater regulation, water (recovery) is stored—Computer-Aided Design (CAD) green space. Technical measures save of suggestions for practical application of the square: innovation and new of space design, new artificial wetland system, and garden rainwater in the application of the regulation (population) storage system design of the water-saving of these to the sustainable development of such new square of rainwater adjustment (group) storage system design and urban landscape environment. It is useful for the application of technology.",space
10.1016/j.cja.2020.09.011,Journal,Chinese Journal of Aeronautics,scopus,2021-06-01,sciencedirect,Framework and development of data-driven physics based model with application in dimensional accuracy prediction in pocket milling,https://api.elsevier.com/content/abstract/scopus_id/85097765922,"In the manufacturing of thin wall components for aerospace industry, apart from the side wall contour error, the Remaining Bottom Thickness Error (RBTE) for the thin-wall pocket component (e.g. rocket shell) is of the same importance but overlooked in current research. If the RBTE reduces by 30%, the weight reduction of the entire component will reach up to tens of kilograms while improving the dynamic balance performance of the large component. Current RBTE control requires the off-process measurement of limited discrete points on the component bottom to provide the reference value for compensation. This leads to incompleteness in the remaining bottom thickness control and redundant measurement in manufacturing. In this paper, the framework of data-driven physics based model is proposed and developed for the real-time prediction of critical quality for large components, which enables accurate prediction and compensation of RBTE value for the thin wall components. The physics based model considers the primary root cause, in terms of tool deflection and clamping stiffness induced Axial Material Removal Thickness (AMRT) variation, for the RBTE formation. And to incorporate the dynamic and inherent coupling of the complicated manufacturing system, the multi-feature fusion and machine learning algorithm, i.e. kernel Principal Component Analysis (kPCA) and kernel Support Vector Regression (kSVR), are incorporated with the physics based model. Therefore, the proposed data-driven physics based model combines both process mechanism and the system disturbance to achieve better prediction accuracy. The final verification experiment is implemented to validate the effectiveness of the proposed method for dimensional accuracy prediction in pocket milling, and the prediction accuracy of AMRT achieves 0.014 mm and 0.019 mm for straight and corner milling, respectively.",space
10.1016/j.evopsy.2021.03.006,Journal,Evolution Psychiatrique,scopus,2021-05-01,sciencedirect,"From Digital Identity to Connected Personality, From Augmented Diagnostician to Virtual Caregiver: What Are the Challenges for the Psychology and the Psychiatry of the Future?",https://api.elsevier.com/content/abstract/scopus_id/85104125089,"Objectifs
                  Qui sommes-nous devenus, citoyens, patients, praticiens ? En quoi les moyens de communications et l’informatisation de notre société modifient-ils, intègrent-ils nos identités ? L’intelligence artificielle comprendrait-elle bientôt plus justement l’être humain dont elle s’émanciperait ?
               
                  Matériel et méthodes
                  Cheminons à partir de la lexicologie pour tenter de saisir, via le point de vue de la philosophie, l’identité contemporaine vers la notion d’« identité numérique » dont les incidents psychologiques normaux ou pathologiques entraînent ce que nous définissons « la personnalité numérique ». Puis, posant les bases d’une psychologie de l’identité contemporaine, nous envisageons comment « la psychologie » et « la psychiatrie » actuelles considèrent « la personnalité » du patient et, en retour, comment elles se définissent du point du vue du « praticien en ligne » ou du « chercheur connecté ».
               
                  Résultats
                  En échange de son utilisation « gratuite », l’action de l’internaute sur le Web 2.0 produit du contenu et alimente des bases de données, déclaratives ou non. En perte d’intimité au fur et à mesure que « ses » données ne lui appartiennent plus, l’identité du citoyen se décompose en fonctions des supports digitaux : site de rencontre amical, plateforme de liens amoureux, blog concernant un loisir ou un voyage, etc. Par le même mouvement, l’identité numérique se compose en autre-soi possédant une part d’intelligence artificielle pourvoyeuse de capacité d’existence propre. Plutôt que deux entités parallèlement différentiables, réelle ou augmentée, naît une identité hybride « réalistiquo-virtuelle ». Quelles conséquences normales ou pathologiques chez l’être humain ? Les tendances sociétales post-modernes issues du digital ou y trouvant expression peuvent entraîner, chez un individu donné, une exacerbation des traits de personnalité préalablement existants, voire des symptômes. Parallèlement, il arrive que les moyens de communication moderne deviennent une aide pour expérimenter le monde, majorer l’estime de soi, rêver favorablement ses phantasmes, se confier plus facilement à des « inconnu(e)s », etc. Mais dans tous les cas, chez le sujet souffrant, ou ne souffrant pas, préalablement à sa surexposition, de maladie neuropsychiatrique ou de trouble psychopathologique, il s’avère aujourd’hui scientifiquement documenté que la confrontation numérique accrue induit des atteintes neuropsychiques massives (affaiblissement de la mémoire de travail, des capacités d’attention et de concentration, des aptitudes à construire des opérations cognitives élaborées, etc.). Sur le plan psychopathologique, plutôt que la terminologie de « trouble de l’identité » ou une notion de « co-identités », le terme d’« identité trouble » nous paraît le mieux rendre compte de cette mutation du « moi » où la frontière entre réalité et virtualités s’amenuise : la dissociation prévaut. L’homme post-moderne et ses objets connectés ne font plus qu’un, mais cet « uniforme » apparaît constitué d’un patchwork de confettis identificatoires plus ou moins accolés, sans réelle harmonisation d’ensemble. La personnalité commune se marque d’hyperexpressivité et d’hyperémotivité, au détriment de la possibilité de contrôle des affects et du développement des capacités d’introspection. Contre le risque du vide, tend à se développer une contra-phobie par l’ordiphone, par l’objet lui-même, par la possibilité de contacter en permanence ses proches si nécessaire, et en retour rester toujours « disponible », ce qui alimente une forme d’égocentrisme addictogène. Résulte de ses évolutions, globalement dans la société, un affaiblissement des capacités langagières, et ainsi de réflexion, y compris pour l’espace clinique et scientifique.
               
                  Discussion
                  Pour les domaines de la psychologie et de la psychiatrie, s’associent actuellement deux évolutions : une velléité d’« objectivité-scientificité » et une numérisation de la relation patient–soignant. Du côté de la « science », la médecine objective « factuelle » s’intéresse de plus en plus à la pathologie aux dépens du sujet en souffrance, confondant signe et symptôme, glissant jusqu’à un niveau moléculaire, très en-deçà du patient, vers une psychiatrie ou une psychologie « post-clinique ». Qu’on veuille la promouvoir ou l’anéantir, du côté du clinicien ou du chercheur, la « subjectivité » est devenue un signifiant à la mode pour le domaine de la santé psychique. Ce retour actuel du « subjectif » prospère sur une sorte de peur de la subjectivité depuis la fin de la seconde guerre mondiale qui avait entraîné la nosographie américaine vers les « objectifs » des DSM (Manuel Diagnostique et Statistique des Troubles Psychiques publié par l’American Psychiatric Association depuis 1952). Mais plutôt qu’une connaissance validable, et/ou invariable concernant tel ou tel trouble psychique, le changement, la relativité des entités nosographiques d’une version à l’autre du manuel traduit, en miroir, la subjectivité d’une époque, ce que nous appelons « subjectivité sociétale ». Autant qu’elle témoigne de notre temps, la révolution bio-numérique s’imposera probablement dans une future édition de la nosographie : la validité diagnostique devrait se majorer par la définition précise de marqueurs biologiques et/ou neuroradiologiques, si ceux-ci participent à construire une théorie étiopathogénique des phénomènes psychiques observés. Cette orientation reste toutefois balbutiante : outre l’infime nombre de biomarqueurs identifiés, et surtout utilisables en pratique quotidienne, leurs liens de causalité ou de conséquentialité avec les symptômes ou le processus morbide restent le plus souvent incertains autant qu’ils sont fort divers et interreliés. Le chercheur en neurosciences vise à mesurer et analyser une multitude de données, intégrant en particulier les mimiques et les émotions authentifiables par caméra thermique, les mouvements des segments des corps et dynamiques des regards enregistrables par des capteurs, la standardisation des voix et des discours pour analyse par logiciel informatique de la prosodie, des signifiants employés, de la syntaxe… le tout s’intégrant dans un phénotypage digital de la souffrance. Pourra-t-on bientôt parler, en remplacement du psychologue ou du psychiatre, de « diagnosticien augmenté » ?
               
                  Conclusion
                  Apparaît-il actuellement hasardeux de faire confiance à un thérapeute entièrement virtuel… expérience déjà lancée il y a plus de 50 ans ! L’être humain est un « être de sens », or, selon le modèle de la clinique traumatique, le surgissement du tout-numérique peut entraîner un « effondrement du sens » générateur d’une tendance à la dissociation de la personnalité. Accordant le rétablissement des liens entre émotions, affects, comportements et cognitions, le langage parlé atténue puis fait disparaître la dissociation. Guidée par le praticien, cette parole thérapeutique est parfois qualifiée de « maïeutique », du nom de la science de l’accouchement : elle construit synchroniquement à son essence la pensée, et une prise de conscience de celle-ci, plutôt qu’elle n’en rendrait compte secondairement. Il s’agit d’une réinterprétation causale d’un sens compris ou plutôt « attribué » singulièrement par le sujet, après-coup, le passé revisité dans l’instant noue une synthèse, le hasard est transformé en destin. Le sujet qui parle réélabore son histoire vers une reconstruction sémantique, une densification de ses réseaux de signification. Reconquérant son être par la création d’un discours, de méandres véridiques comme fictionnels, la narration, voire la poétisation, offre l’illusion ponctuelle d’une meilleure cohérence, toujours relative, illusoire La parole thérapeutique et le discours sur celle-ci restent en devenir, inachevés, incertains autant que vivants, caractérisant une « post-psychothérapie », c’est-à-dire une psychothérapie et non pas une technique rééducative qui se trouverait figée dans des objectifs connus à l’avance. Les notions de faits et de réalité sont ici secondaires, non pas au sens de l’objectif, ni même du subjectif, mais du second degré, puis d’autres degrés successifs ou imbriqués portant l’effort intellectuel. Vers l’apaisement, si nous voulions amener la réflexion à son paroxysme, nous pourrions avancer qu’il suffirait de donner « n’importe quel sens », d’en choisir un quel qu’il soit, du côté du patient ou du praticien, sans qu’il ne soit nécessairement le même, témoignage d’une construction intersubjective formellement invalide.
               
                  Objectives
                  Who have we become, as citizens, patients, practitioners? How do the means of communication and the computerization of our society, its digitization, modify and integrate our identities? Can we assume that artificial intelligence will soon have a more accurate understanding of the human being from whom it will have emancipated itself?
               
                  Materials and methods
                  We move from lexicology to try to grasp, from the point of view of philosophy, a contemporary identity that is moving towards the notion of a “digital identity” whose normal or pathological psychological incidents lead to what we define as “the digital personality.” Then, laying the foundations for a contemporary psychology of identity, we consider how current “psychology” and “psychiatry” view the patient's “personality” and, in turn, how they define themselves from the point of view of “the patient,” or, inversely, from the point of view of the “online practitioner” or “connected researcher.”
               
                  Results
                  In exchange for its “free” use, the Internet user's action on Web 2.0 produces content and feeds databases, whether this is declared or not. Users’ privacy is lost, as “their” data no longer belongs to them; and citizens’ identity is broken down into digital media functions: a site for meeting friends, a dating platform, a blog about hobbies or travel, etc. At the same time, digital identity is made up of an other-self, including a part of artificial intelligence that provides capacity for its own existence. Rather than two parallel, differentiable entities, real or augmented, a “realistic-virtual” hybrid identity is born. What are the normal or pathological consequences for humans? Postmodern societal trends emerging from or finding expression in the digital can lead to an exacerbation of previously existing personality traits, or even symptoms, in a given individual. At the same time, it happens that the modern means of communication become an aid to experience the world, to increase self-esteem, to dream favorably about one's fantasies, to confide more easily in “strangers,” etc. But in all cases, in the subject suffering, or not suffering, prior to his overexposure, from a neuropsychiatric disease or a psychopathological disorder, it now turns out to be scientifically documented that the increased numerical confrontation induces massive neuropsychic damage (weakening working memory, attention and concentration skills, skills in constructing sophisticated cognitive operations, etc.). On the psychopathological level, rather than the terminology of “identity disorder” or a notion of “co-identities,” the term “identity elusive"" seems to us to best account for this mutation of the “me” where the border between reality and virtualities is shrinking: dissociation prevails. The postmodern human and its connected objects become one, but this “uniformity” appears to be made up of a patchwork of identifying confetti more or less joined together, without a real overall harmonization. The common personality is marked by hyperexpressiveness and hyperemotivity, to the detriment of the possibility of controlling affects and the development of introspective capacities. Against the risk of a vacuum, a contra-phobia tends to develop through the smartphone, by the object itself, by the possibility of constantly contacting relatives if necessary, and in return always remaining “available,” which fuels a form of addicting self-centeredness. The result of these developments, for society in general, is a weakening of language skills, and thus of reflection, including in the clinical and scientific space.
               
                  Discussion
                  For the areas of psychology and psychiatry, two developments are currently associated: a desire for “objectivity-scientificity” and a digitization of the patient–caregiver relationship. On the side of “science,” objective “factual” medicine is increasingly interested in pathology at the expense of the suffering subject, confusing sign and symptom, sliding down to a molecular level, far below the patient, towards psychiatry or postclinical psychology. Whether we want to promote it or destroy it, on the side of the clinician or the researcher, “subjectivity” has become a fashionable signifier in the field of mental health. This current return of the “subjective” thrives on a kind of fear of subjectivity present since the end of World War II, which had led American nosography towards the “objectives” of the DSM (Diagnostic and Statistical Manual of Mental Disorders, published by the American Psychiatric Association since 1952). But rather than a verifiable and/or invariable knowledge concerning a particular psychic disorder, the changes and the relativity of nosographic entities from one version of the manual to another provides us with a mirror image of the subjectivity of an era, which we propose to call “societal subjectivity.” As much as it is a product of our time, the bio-digital revolution will probably impose itself in a future edition of nosography: the diagnostic validity should be increased by the precise definition of biological and/or neuroradiological markers, if these participate in building an etiopathogenic theory of observed psychic phenomena. This orientation remains in its infancy, however: in addition to the tiny number of identified biomarkers, and above all, those that are usable in daily practice, their causal or consequential links with symptoms or with the morbid process remain most often uncertain, inasmuch as they are diverse and interrelated. The neuroscience researcher aims to measure and analyze a multitude of data, integrating, in particular, mimicry and emotions authenticated by thermal camera; movements of body segments and gaze dynamics recorded by sensors; the standardization of voices and speeches for computer software analysis of prosody, used signifiers, syntax… all of which is integrated into a digital phenotyping of suffering. Will we soon be able to speak, replacing the psychologist or the psychiatrist, of an “augmented diagnostician?”.
               
                  Conclusion
                  Does it currently appear risky to trust an entirely virtual therapist… an experiment already launched more than 50 years ago! The human being is a “being of meaning,” yet, according to the model of trauma, the emergence of the all-digital can lead to a “collapse of meaning,” generating a tendency to personality dissociation. Granting the reestablishment of the links between emotions, affects, behaviors, and cognitions, spoken language attenuates dissociation, then makes it disappear. Guided by the practitioner, this therapeutic word is sometimes qualified as “maieutics,” from the name of the science of childbirth: it builds thought synchronously to its essence, and an awareness of it, rather than nondisclosure, would account for it secondarily. It is a causal reinterpretation of a meaning understood or rather “attributed” singularly by the subject, after the fact: the past revisited in the present moment creates a synthesis, and chance is transformed into fate. The speaking subject re-elaborates her/his story towards a semantic reconstruction, a densification of her/his networks of signification. Reclaiming one's being by the creation of a discourse, of veridical as well as fictional meanders, narration, even poetization, offers the punctual illusion of a better coherence, always relative, illusory… Therapeutic speech and discourse about such speech–these are still being made, unfinished, uncertain, and alive. These are the characteristics of what we could a “post-psychotherapy,” that is, a psychotherapy and not a re-educational technique whose objectives would be fixed and known in advance. The notions of facts and reality are secondary here, not in the sense of the objective, nor even of the subjective, but of the second degree, then of other successive or overlapping degrees that require intellectual effort. Moving towards appeasement, if we wanted to bring the reflection to its paroxysm, we could advance that it would be enough to give “any meaning,” whatever it may be. This would apply both to the patient and to the practitioner, without each party's meaning necessarily being the same: a testimony to a formally invalid intersubjective construction.",space
10.1016/j.ssci.2021.105190,Journal,Safety Science,scopus,2021-05-01,sciencedirect,Beirut explosion 2020: A case study for a large-scale urban blast simulation,https://api.elsevier.com/content/abstract/scopus_id/85100545374,"In the face of continued global urbanization, cities are challenged to satisfy increasing standards in terms of quality of life, environmental conditions, safety, security, health, economic growth and mobility. The concept of “smart cities” aims at utilising advanced technologies, artificial intelligence and high computational capacity to increase their resilience and improve the services provided to the citizens. Computation-based numerical simulations have been essentially used to estimate the effects of explosion events in urban environments in terms of both structural damage and human casualties. These provide urban planners and decision makers with valuable information for vulnerability assessment and aid developing prevention or mitigation solutions. In this article, we present a framework to generate a 3D large-scale urbanistic finite element model, where the desired geospatial data are extracted from the open-source world map OpenStreetMap. The model is used to simulate blast wave propagation effects in a wide urban area taking into account the reflections at building surfaces via a sophisticated Fluid-Structure interaction technique integrated in the EUROPLEXUS explicit finite element method software. The explosion in the Port of Beirut in Lebanon, which took place on the 4th of August 2020, was remarkable for the large amount of explosive material causing considerable damage to surrounding structures and a high number of deaths and injured. Such characteristics make the event suitable for assessing the performance of the proposed computational approach in a widely exposed (by the blast wave) urban zone.",space
10.1016/j.eswa.2020.114139,Journal,Expert Systems with Applications,scopus,2021-04-15,sciencedirect,A dynamic framework for tuning SVM hyper parameters based on Moth-Flame Optimization and knowledge-based-search,https://api.elsevier.com/content/abstract/scopus_id/85099517458,"In the real world, most of the collections of data are dynamic in nature, i.e. their size may grow with time. This dynamic nature of the data not only reduces the performance of the classifiers but also demands more optimized models for retaining the performance. Due to this, machine learning models developed in a static environment cannot be deployed efficiently to solve the real-world problems. Nowadays, maximum existing works consider only the static behaviour of the data for the training of machine learning models where the size of the collection of training data does not change over time. This paperwork imposes Support Vector Machine (SVM) in a dynamic environment. It has been identified that shifting of the optimum values of two hyper-parameters C (Penalty Parameter) and γ (Kernel Parameter) in the search space is one of the primary reasons for the performance degradation of SVM in dynamic environment. This paper proposes a novel framework that uses a new optimization module Knowledge-Based-Search (KBS) along with Moth –Flame Optimization (MFO) to optimize 
                        
                           C
                        
                      and 
                        
                           γ
                        
                      in a dynamic environment to train SVM efficiently. KBS uses knowledge gathered at various instances of time, which are the bi-products of MFO. MFO in our framework is the base optimization algorithm which works underneath KBS. The experiments have shown that KBS helps in controlling the exponential growth of the time complexity of the optimization process where only MFO is used to optimize 
                        
                           C
                        
                      and
                        
                           γ
                        
                     . Integration of KBS with MFO brings down the time complexity to a large extent. To validate the proposed framework we have used a simulated dynamic environment for profit/loss classification problem for organizations. The experiments have also shown that KBS's integration with MFO outperforms integration of KBS with other modern optimization techniques such as Particle Swarm Optimization (PSO), Multi-Verse Optimization (MVO), Grey-Wolf Optimization (GWO), Cuckoo Search (CS), Whale Optimization Algorithm (WOA), Genetic Algorithm (GA), Fire-Fly Algorithm (FFA) and Salp Swarm Algorithm (SSA).",space
10.1016/j.eswa.2020.114402,Journal,Expert Systems with Applications,scopus,2021-04-15,sciencedirect,Unsupervised feature selection for attributed graphs,https://api.elsevier.com/content/abstract/scopus_id/85097572982,"Many real-world applications generate attributed graphs that contain both link structures and content information associated with nodes. Content information in real networks always contains high dimensional feature space. In recent years, unsupervised feature selection has been widely used in handling high dimensional data without label information. Most existing unsupervised feature selection methods assume that instances in datasets are independent and identically distributed. However, instances in attributed graphs are intrinsically correlated. Considering the wide applications of feature selection in attributed graphs, we propose a new unsupervised feature selection method based on regularized sparse learning. We use pseudo class labels to learn the interdependency from both link and content information, and embed the obtained information into a sparse learning based feature selection framework. In particular, a new regularization term is designed to learn link information, which capture group behavior among the connected instances utilizing latent social dimensions. To solve the proposed feature selection model, we consider both convex and nonconvex cases and design the corresponding algorithms based on the Alternating Direction Method of Multipliers (ADMM) combined with ConCave Convex Procedure (CCCP). Numerical studies are implemented on real-world datasets to validate the advantage of our new method.",space
10.1016/j.jnca.2021.102995,Journal,Journal of Network and Computer Applications,scopus,2021-04-01,sciencedirect,Dew computing-inspired health-meteorological factor analysis for early prediction of bronchial asthma,https://api.elsevier.com/content/abstract/scopus_id/85100605135,"Bronchial asthma is one of the most common chronic diseases of childhood and considered as a major health problem globally. The irregularity in meteorological factors has become a primary cause of health severity for the individuals suffering from asthma. In the presented research, a dew-cloud assisted cyber-physical system (CPS) is proposed to analyze the correlation between the meteorological and health parameters of the individuals. The work is primarily focused on determining the health adversity caused by the irregular scale of meteorological factors in real-time. IoT-assisted smart sensors are utilized to capture ubiquitous information from indoor environment that make a vital impact on the health of the individual directly or indirectly. The data is analyzed over the cyber-space to quantify the probable irregular health events by utilizing the data classification efficiency of Weighted-Naïve Bayes modeling technique. Moreover, the relationship between meteorological and health parameters is estimated by utilizing the Adaptive Neuro-Fuzzy Inference System (ANFIS) and calculate a unifying factor over the temporal scale. To validate the monitoring performance, the proposed model is implemented in the four schools of Jalandhar, India. The experimental evaluation of the proposed model acknowledges the performance efficiency through several statistical approaches. Furthermore, the comparative analysis is evaluated with state-of-the-art decision-making algorithms that demonstrate the effectiveness of the proposed solution for the targeted application.",space
10.1016/j.petrol.2020.108296,Journal,Journal of Petroleum Science and Engineering,scopus,2021-04-01,sciencedirect,Geological structure-guided hybrid MCMC and Bayesian linearized inversion methodology,https://api.elsevier.com/content/abstract/scopus_id/85100209414,"Seismic inversion is a common method for hydrocarbon reservoir characterization, as it consists of a proven and effective approach to derive elastic properties from reflectivity seismic data. Markov Chain Monte Carlo (MCMC) based seismic inversion approach is a suitable choice to numerically evaluate the posterior uncertainties associated with the inverse solution without assuming linear forward operators, Gaussian, or generalized Gaussian prior models. However, the existing MCMC based seismic inversion approaches are mostly performed trace-by-trace, which means that the spatial coupling of model parameters is not considered. When the results of trace-by-trace based inversion are combined to generate a 2D profile, the final results will be laterally discontinuous. Moreover, the large dimension of the model space causes low convergence efficiency of MCMC-based seismic inversion. To overcome these issues, a geological structure-guided hybrid MCMC and Bayesian linearized inversion (BLI) methodology for seismic inversion is implemented. The geological structure information obtained using plane wave destruction (PWD) is incorporated to the MCMC based inversion algorithm in the form of dips yields more geologically meaningful results. The hybrid MCMC and BLI strategy, which takes advantage of BLI's high efficiency to provide initial configuration for MCMC, is used to improve the convergence of MCMC-based inversion. Additionally, the block coordinate descent (BCD) algorithm is introduced to replace the large-scale matrix solution in geological structure-guided, and consequently reduce memory consumption and time cost. This methodology is validated on a synthetic seismic dataset, as well as on a real case. It has proven to be a reliable approach to obtain acoustic impedance (AI) from post-stack seismic data in an efficient way. It also addresses the uncertainty related with the ill-posed characteristics of the inversion methodology itself.",space
10.1016/j.actaastro.2021.01.048,Journal,Acta Astronautica,scopus,2021-04-01,sciencedirect,A transfer learning approach to space debris classification using observational light curve data,https://api.elsevier.com/content/abstract/scopus_id/85100001105,"This paper presents a data driven approach to space object characterisation through the application of machine learning techniques to observational light curve data. One-dimensional convolutional neural networks are shown to be effective at classifying the shape of objects from both simulated and real light curve data. To the best of the authors’ knowledge this is the first generalised attempt to classify the shape of space objects using real observational light curve data.
                  It is also demonstrated that transfer learning is successful in improving the overall classification accuracy on real light curve datasets. The authors develop a simulated light curve dataset using a high fidelity three-dimensional ray-tracing software. The simulator takes in a textured geometric model of a Resident Space Object as well as its ephemeris and uses ray-tracing software to generate photo-realistic images of the object that are then processed to extract the light curve. Models that are pre-trained on the simulated dataset and then fine-tuned on the real datasets are shown to outperform models purely trained on the real datasets. This result indicates that transfer learning will allow organisations to effectively utilise deep learning techniques without the requirement to build up large real light curve datasets for training.",space
10.1016/j.cma.2020.113609,Journal,Computer Methods in Applied Mechanics and Engineering,scopus,2021-04-01,sciencedirect,The Arithmetic Optimization Algorithm,https://api.elsevier.com/content/abstract/scopus_id/85099194941,"This work proposes a new meta-heuristic method called Arithmetic Optimization Algorithm (AOA) that utilizes the distribution behavior of the main arithmetic operators in mathematics including (Multiplication (
                        M
                     ), Division (
                        D
                     ), Subtraction (
                        S
                     ), and Addition (
                        A
                     )). AOA is mathematically modeled and implemented to perform the optimization processes in a wide range of search spaces. The performance of AOA is checked on twenty-nine benchmark functions and several real-world engineering design problems to showcase its applicability. The analysis of performance, convergence behaviors, and the computational complexity of the proposed AOA have been evaluated by different scenarios. Experimental results show that the AOA provides very promising results in solving challenging optimization problems compared with eleven other well-known optimization algorithms. Source codes of AOA are publicly available at and .",space
10.1016/j.medidd.2021.100081,Journal,Medicine in Drug Discovery,scopus,2021-03-01,sciencedirect,Peptides in chemical space,https://api.elsevier.com/content/abstract/scopus_id/85104918401,"Peptides, defined as sequences of amino acids up to approximately 50 residues in length, represent an extremely large reservoir of potentially bioactive compounds, referred to here as the peptide chemical space. Recent advances in computer hardware and software have led to a wide application of computational methods to explore this chemical space. Here, we review different in silico approaches including structure-based design, genetic algorithms, and machine learning. We also review the use of molecular fingerprints to sample virtual libraries and to visualize the peptide chemical space. Finally, we present an overview of the known peptide chemical space in form of an interactive map representing 40,531 peptides collected from eleven open-access peptide and peptide-containing databases, accessible at https://tm.gdb.tools/map4/peptide_databases_tmap/. These peptides are displayed as TMAP (Tree-Map) according to their molecular fingerprint similarity computed using MAP4, a MinHashed atom pair fingerprint well suited to analyze large molecules.",space
10.1016/j.rser.2020.110519,Journal,Renewable and Sustainable Energy Reviews,scopus,2021-03-01,sciencedirect,Development of a constraint non-causal wave energy control algorithm based on artificial intelligence,https://api.elsevier.com/content/abstract/scopus_id/85095602765,"The real-time implementation of wave energy control leads to non-causality as the wave load that comes in the next few seconds is used to optimize the control command. The present work tackles non-causality through online forecasting of future wave force using artificial intelligence technique. The past free surface elevation is used to forecast the incoming wave load. A feedforward artificial neural network is developed for the forecasting, which learns to establish the intrinsic link between past free surface elevation and future wave force through machine learning algorithm. With the implementation of the developed online wave force prediction algorithm, a real-time discrete control algorithm taking constraint on response amplitude into account is developed and implemented to a bi-oscillator wave energy converter in the present research. The dynamic response and the wave power extraction are simulated using a state-space hydrodynamic model. It is shown that the developed real-time control algorithm enhances the power capture substantially whereas the motion of the system is hardly increased. The prediction error effect on power extraction is investigated. The reduction of power extraction is mainly caused by phase error, whilst the amplitude error has minimal influence. A link between the power capture efficiency and the constraint on control is also identified.",space
10.1016/j.future.2020.10.011,Journal,Future Generation Computer Systems,scopus,2021-03-01,sciencedirect,Human action identification by a quality-guided fusion of multi-model feature,https://api.elsevier.com/content/abstract/scopus_id/85094321431,"Human motion recognition has become an active research area in the field of computer vision due to its wide range of implementations in domains of video monitoring, virtual reality, human–machine interaction. Dealing with the problem that the RGB images cannot provide enough depth information, a multi-modal depth neural network based on joint cost function is proposed for human motion recognition. In the architecture, the features of the RGB video frames are extracted by the 3D CNN architecture while the characteristics of human motion recognition in the SSDDI graphics utilizing depth map are extracted by the LSTM. Moreover, the model utilizes joint cost function including the cross-entropy loss and the distance constraint between the feature space of training samples and their center values within each category. The experimental results on the MSR Action 3D datasets suggest that the proposed model demonstrates a higher accuracy rate than do the other competing models.",space
10.1016/j.quaint.2020.08.018,Journal,Quaternary International,scopus,2021-02-20,sciencedirect,Characterization of geomorphological features of lunar surface using Chandrayaan-1 Mini-SAR and LRO Mini-RF data,https://api.elsevier.com/content/abstract/scopus_id/85090021453,"The lunar surface comprises complex geomorphological features, which have been formed by the conjunction of processes namely impact cratering and volcanism. Geological features on the Lunar surface can be bifurcated into two main areas named Maria region and the Highland region. Taurus-Littrow valley, which was the Apollo-17 mission landing site, consisting of unique geomorphological characteristics by having a sample size of both Lunar Maria and Highland regions. The dielectric constant is a parameter that gives an approximate distribution of the constituent material of the target area. It is a complex quantity, which indicates a periodic variation of the electric field. The real part of dielectric constant indicates stored energy and the imaginary part indicates dielectric loss factor or the loss of the electric field in the medium due to continuous varying electric field. Planetary surfaces for which determining dielectric constant is an important analysis for most of the space missions, ground measurement is not feasible. This work includes the machine learning-based modeling of dielectric constant for the Apollo 17 landing site the Taurus-Littrow valley. Based on the surface roughness of the study area, two models Gaussian and Exponential have been implemented and compared for the modeled output of the dielectric constant values.The modeling approaches for dielectric characterization of the lunar surface were implemented on NASA's LRO Mini-RF SAR data and Mini-SAR hybrid-pol data of ISRO's Chandrayaan-1 mission. The coefficient of determination (r2) and the root mean square error (RMSE) of the theoretical Gaussian model was 0.995, 0.042 and the Exponential model was 0.948, 0.1349 respectively. When compared with the already calculated values of dielectric constant from Apollo 17 return samples and literature survey, the Gaussian model gives a better variation. Gaussian model was further applied to the Lunar north pole crater namely Hermite-A crater, whose distinctive geomorphological characteristics and location being lunar north pole region, makes it one of the coldest places in the Solar System and a prominent location of water ice deposits.",space
10.1016/j.neuron.2020.11.021,Journal,Neuron,scopus,2021-02-17,sciencedirect,Using deep reinforcement learning to reveal how the brain encodes abstract state-space representations in high-dimensional environments,https://api.elsevier.com/content/abstract/scopus_id/85099151634,"Humans possess an exceptional aptitude to efficiently make decisions from high-dimensional sensory observations. However, it is unknown how the brain compactly represents the current state of the environment to guide this process. The deep Q-network (DQN) achieves this by capturing highly nonlinear mappings from multivariate inputs to the values of potential actions. We deployed DQN as a model of brain activity and behavior in participants playing three Atari video games during fMRI. Hidden layers of DQN exhibited a striking resemblance to voxel activity in a distributed sensorimotor network, extending throughout the dorsal visual pathway into posterior parietal cortex. Neural state-space representations emerged from nonlinear transformations of the pixel space bridging perception to action and reward. These transformations reshape axes to reflect relevant high-level features and strip away information about task-irrelevant sensory features. Our findings shed light on the neural encoding of task representations for decision-making in real-world situations.",space
10.1016/j.oceaneng.2020.108530,Journal,Ocean Engineering,scopus,2021-02-01,sciencedirect,Prediction and optimisation of fuel consumption for inland ships considering real-time status and environmental factors,https://api.elsevier.com/content/abstract/scopus_id/85098555033,"The information about ships’ fuel consumption is critical for condition monitoring, navigation planning, energy management and intelligent decision-making. Detailed analysis, modelling and optimisation of fuel consumption can provide great support for maritime management and operation and are of significance to water transportation. In this study, the real-time status monitoring data and hydrological data of inland ships are collected by multiple sensors, and a multi-source data processing method and a calculation method for real-time fuel consumption are proposed. Considering the influence of navigational status and environmental factors, including water depth, water speed, wind speed and wind angle, the Long Short-Term Memory (LSTM) neural network is then tailored and implemented to build models for prediction of real-time fuel consumption rate. The validation experiment shows the developed model performs better than some regression models and conventional Recurrent Neural Networks (RNNs). Finally, based on the fuel consumption rate model and the speed over ground model constructed by LSTM, the Reduced Space Searching Algorithm (RSSA) is successfully used to optimise the fuel consumption and the total cost of a whole voyage.",space
10.1016/j.micpro.2020.103579,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Development of cultural tourism platform based on FPGA and convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85097346419,"Data mining can be described as a typical analysis of large datasets to investigate early unknown types, styles, and interpersonal relationships to generate the right decision information. It improves their markets and today to maintain control over whether these companies are forced into the data mining tools and technologies they use to develop and manage tourism products and services in the market. It is falling out of the favorable situation of the travel and tourism industry. Objective work is to provide and display its application in data mining and tourism. Advances in mobile technology provide an opportunity to obtain real-time information of travelers, such as time and space behavior, at the destination they visit. This study analyzed a large-scale mobile phone data set to capture the mobile phone traces of international tourists who visited South Korea. We adopt the trajectory data mining method to understand tourism activities’ spatial structure in three different destinations. The research reveals tourist destinations and multiple “hot spots” (or popular areas) that interact spatially in these places through spatial cluster analysis and sequential pattern mining. Therefore, this article provides the planning of spatial model destinations to integrate important tourism influences, which is based on tourism design. The proposed system is modelled in Field Programmable Gate Array (FPGA) using Xilinx software.",space
10.1016/j.micpro.2020.103343,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Ecological evolution path of smart education platform based on deep learning and image detection,https://api.elsevier.com/content/abstract/scopus_id/85095585894,"Smart environments are becoming a reality in our society, and the number of smart devices integrated into these spaces is overgrowing. End users are being provided a simplified way to handle complex smart features, as the combination of smart elements opens up a wide range of new opportunities to facilitate. This article explores the significant challenges to be overcome in designing an intelligent educational environment for the main characteristics and the personalized support of ecology. To integrate intelligent learning environments into learning ecology and educational environments, innovative applications, and new teaching methods should be implemented to coordinate formal and informal learning. However, despite the increased use of smart learning environments in higher education, at the same time, there is an excellent network that does not define a set of demand models for the development and evaluation of smart learning environment education that considers teaching, evaluation, and design. Deep learning is one of the modern methods that can be used to automate the process of effective intellectual education based on image detection. The deep learning process is based on image discovery. It provides an overview of ecological evaluation based smart education level analysis used image detection. The system that has been proposed here is an intelligent education system that has been customized to provide the resources of the evolution of the ecosystem to the learner to suit their perceptions and education center to start the platform.",space
10.1016/j.micpro.2020.103318,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Enterprise financial cost management platform based on FPGA and neural network,https://api.elsevier.com/content/abstract/scopus_id/85094858518,"At present, the domestic costs of most construction companies are relatively scattered with the cost data of various business agents. Unless it is controlled by an experienced manager, decision-makers cannot have the real-time dynamic cost of a project. In the information age, it is of vital importance to use the information to control the cost of construction projects dynamically. Cost management and the establishment of an information platform are ways to control the platform integrated cost data, operators, computer software and hardware, and corresponding method information, and its core is cost data information. The place for financial cost analysis and decision making is a conceptually rich field where information is a commercial product which is complicated, extensive, and invaluable. In this model, first,a set of extracts from the macro-credit feature space is designed and then, FPGA and neural network (FPGA, NN) models for credit evaluation is built based on these indicators, eventually it is applied scientifically, and reasonably, practically. Several state credit metrics are randomly selected. Our model shows applications that are both practical and competent. Using this model, authorities can analyze local credit conditions, allowing investors to make wise decisions to invest while saving on operating and credit costs. Most importantly, this model can help impulsive local government leaders, businesses, and even everyone to enhance competitiveness and capacities of attractive regions, thereby foster a good atmosphere for a credit culture.",space
10.1016/j.jneumeth.2020.109006,Journal,Journal of Neuroscience Methods,scopus,2021-01-15,sciencedirect,Real-Time Point Process Filter for Multidimensional Decoding Problems Using Mixture Models,https://api.elsevier.com/content/abstract/scopus_id/85097710119,"There is an increasing demand for a computationally efficient and accurate point process filter solution for real-time decoding of population spiking activity in multidimensional spaces. Real-time tools for neural data analysis, specifically real-time neural decoding solutions open doors for developing experiments in a closed-loop setting and more versatile brain-machine interfaces. Over the past decade, the point process filter has been successfully applied in the decoding of behavioral and biological signals using spiking activity of an ensemble of cells; however, the filter solution is computationally expensive in multi-dimensional filtering problems. Here, we propose an approximate filter solution for a general point-process filter problem when the conditional intensity of a cell’s spiking activity is characterized using a Mixture of Gaussians. We propose the filter solution for a broader class of point process observation called marked point-process, which encompasses both clustered – mainly, called sorted – and clusterless – generally called unsorted or raw– spiking activity. We assume that the posterior distribution on each filtering time-step can be approximated using a Gaussian Mixture Model and propose a computationally efficient algorithm to estimate the optimal number of mixture components and their corresponding weights, mean, and covariance estimates. This algorithm provides a real-time solution for multi-dimensional point-process filter problem and attains accuracy comparable to the exact solution. Our solution takes advantage of mixture dropping and merging algorithms, which collectively control the growth of mixture components on each filtering time-step. We apply this methodology in decoding a rat’s position in both 1-D and 2-D spaces using clusterless spiking data of an ensemble of rat hippocampus place cells. The approximate solution in 1-D and 2-D decoding is more than 20 and 4,000 times faster than the exact solution, while their accuracy in decoding a rat position only drops by less than 9% and 4% in RMSE and 95% highest probability coverage area (HPD) performance metrics. Though the marked-point filter solution is better suited for real-time decoding problems, we discuss how the filter solution can be applied to sorted spike data to better reflect the proposed methodology versatility.",space
10.1016/j.knosys.2020.106580,Journal,Knowledge-Based Systems,scopus,2021-01-09,sciencedirect,Hybrid artificial neural network and cooperation search algorithm for nonlinear river flow time series forecasting in humid and semi-humid regions,https://api.elsevier.com/content/abstract/scopus_id/85095736783,"Accurate river flow forecasting is of great importance for the scientific management of water resources system. With the advantages of easy implementation and high flexibility, artificial neural network (ANN) has been widely employed to address the complex hydrological forecasting problem. However, the conventional ANN method often suffers from some defects in practice, like slow convergence and local minimum. In order to enhance the ANN performance, this study proposes a hybrid river flow forecasting method by integrating the novel cooperation search algorithm (CSA) into the learning process of ANN. In other words, the computational parameters of the ANN network (like threshold and linking weights) are iteratively optimized by the CSA method in the feasible state space. The proposed method is applied to the river flow data collected from two real-world hydrological stations in China. Several Quantitative indexes are chosen to compare the performance of the developed models, while the comprehensive analysis between the simulated and observed flow data are conducted. The experimental results show that in different scenarios, the hybrid method based on ANN and CSA always outperforms the control models and yields superior forecasting results during both training and testing phases. In Three Gorges Project, the presented method makes 11.10% and 5.42% improvements in the Nash–Sutcliffe efficiency and Coefficient correlation values of the standard ANN method in the testing phase. Thus, this interesting finding shows that the performance of the artificial intelligence models in river flow time series forecasting can be effectively improved by metaheuristic algorithm with outstanding global search ability.",space
10.1016/j.ifacol.2021.10.504,Conference Proceeding,IFAC-PapersOnLine,scopus,2021-01-01,sciencedirect,Advanced state fuzzy cognitive maps applied on nearly zero energy building model,https://api.elsevier.com/content/abstract/scopus_id/85120711263,"Fuzzy Cognitive Maps method combines the advantages of Fuzzy Logic, such as their human reasoning and linguistic features, with the advantages of Neural Networks, such as their low mathematical calculation requirements, in order to model complex dynamic systems on a wide variety of applications. The system variables and their interconnections are described using a graph and a weight matrix. Application of experts’ knowledge leads towards more realistic system models. In addition, the implementation of state-space theory in combination with learning algorithms, lead to a new generation of Fuzzy Cognitive Maps, the Advanced State Fuzzy Cognitive Maps. All the above are implemented on a nearly Zero Energy Building model, using real weather data and presenting its annual energy response.",space
10.1016/j.patrec.2021.10.014,Journal,Pattern Recognition Letters,scopus,2021-01-01,sciencedirect,Adversarial learning and decomposition-based domain generalization for face anti-spoofing,https://api.elsevier.com/content/abstract/scopus_id/85119451640,"Face anti-spoofing (FAS) plays a critical role in the face recognition community for securing the face presentation attacks. Many works have been proposed to regard FAS as a domain generalization problem for robust deployment in real-world scenarios. However, existing methods focus on extracting intrinsic spoofing cues to improve the generalization ability, yet neglect to train a robust classifier. In this paper, we propose a framework to improve the generalization ability of face anti-spoofing in two folds:) a generalized feature space is obtained via aggregation of all live faces while dispersing each domain’s spoof faces; and) a domain agnostic classifier is trained through low-rank decomposition. Specifically, a Common Specific Decomposition for Specific (CSD-S) layer is deployed in the last layer of the network to select common features while discarding domain-specific ones among multiple source domains. The above-mentioned two components are integrated into an end-to-end framework, ensuring the generalization ability to unseen scenarios. The extensive experiments demonstrate that the proposed method achieves state-of-the-art results on four public datasets, including CASIA-MFSD, MSU-MFSD, Replay-Attack, and OULU-NPU.",space
10.1016/j.mex.2021.101572,Journal,MethodsX,scopus,2021-01-01,sciencedirect,Methodology for using a Bayesian nonparametric model to uncover universal patterns in color naming,https://api.elsevier.com/content/abstract/scopus_id/85118571924,"Language is an integral part of society which enables communication among its members. To shed light on how words gain their meaning and how their meaning evolves over time, color naming is often used as a case study. The color domain can be defined by a physical space, making it a useful concept for studying denotation of meaning. Though humans can distinguish millions of colors, language provides us with a small, manageable set of terms for categorizing the space. Partitions of the color space vary across different language groups and evolve over time (e.g. new color terms may enter a language). Investigating universal patterns in color naming provides insight into the mechanisms that give rise to the observed data. Recently, computational techniques have been utilized to study this phenomenon. Here, we develop a methodology for transforming a color naming data set—namely, the World Color Survey—which is based on constraints imposed by the stimulus space. This transformed data is used to initialize a nonparametric Bayesian machine learning model in order to implement a culture and theory-independent study of universal color naming patterns across different language groups. All of the methods described are executed by our Python software package called ColorBBDP.
                  • Data from the World Color Survey is transformed from its original format into binary features vectors which can be given as input to the Beta-Bernoulli Dirichlet Process Mixture Model.
                  • This paper provides a specific application of Variational Inference on the Beta-Bernoulli Dirichlet Process Mixture Model towards a color naming data set.
                  • New mathematical measures for performing post-cluster analyses are also detailed in this paper.",space
10.1016/j.procs.2021.09.233,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,A note on the applications of artificial intelligence in the hospitality industry: Preliminary results of a survey,https://api.elsevier.com/content/abstract/scopus_id/85116885410,"Intelligent technologies are widely implemented in different areas of modern society but specific approaches should be applied in services. Basic relationships refer to supporting customers and people responsible for services offering for these customers. The aim of the paper is to analyze and evaluate the state-of-the art of artificial intelligence (AI) applications in the hospitality industry. Our findings show that the major deployments concern in-person customer services, chatbots and messaging tools, business intelligence tools powered by machine learning, and virtual reality & augmented reality. Moreover, we performed a survey (n = 178), asking respondents about their perceptions and attitudes toward AI, including its implementation within a hotel space. The paper attempts to discuss how the hotel industry can be motivated by potential customers to apply selected AI solutions. In our opinion, these results provide useful insights for understanding the phenomenon under investigation. Nevertheless, since the results are not conclusive, more research is still needed on this topic. Future studies may concern both qualitative and quantitative methods, devoted to developing models that: a) quantify the potential benefits and risks of AI implementations, b) determine and evaluate the factors affecting the AI adoption by the customers, and c) measure the user (guest) experience of the hotel services, fueled by AI-based technologies.",space
10.1016/j.isatra.2021.06.010,Journal,ISA Transactions,scopus,2021-01-01,sciencedirect,A real-world application of Markov chain Monte Carlo method for Bayesian trajectory control of a robotic manipulator,https://api.elsevier.com/content/abstract/scopus_id/85108508566,"Reinforcement learning methods are being applied to control problems in robotics domain. These algorithms are well suited for dealing with the continuous large scale state spaces in robotics field. Even though policy search methods related to stochastic gradient optimization algorithms have become a successful candidate for coping with challenging robotics and control problems in recent years, they may become unstable when abrupt variations occur in gradient computations. Moreover, they may end up with a locally optimal solution. To avoid these disadvantages, a Markov chain Monte Carlo (MCMC) algorithm for policy learning under the RL configuration is proposed. The policy space is explored in a non-contiguous manner such that higher reward regions have a higher probability of being visited. The proposed algorithm is applied in a risk-sensitive setting where the reward structure is multiplicative. Our method has the advantages of being model-free and gradient-free, as well as being suitable for real-world implementation. The merits of the proposed algorithm are shown with experimental evaluations on a 2-Degree of Freedom robot arm. The experiments demonstrate that it can perform a thorough policy space search while maintaining adequate control performance and can learn a complex trajectory control task within a small finite number of iteration steps.",space
10.1016/j.ejor.2020.12.024,Journal,European Journal of Operational Research,scopus,2021-01-01,sciencedirect,Robust low-rank multiple kernel learning with compound regularization,https://api.elsevier.com/content/abstract/scopus_id/85099124915,"Kernel learning is widely used in nonlinear models during the implementation of forecasting tasks in analytics. However, existing forecasting models lack robustness and accuracy. Therefore, in this study, a novel supervised forecasting approach based on robust low-rank multiple kernel learning with compound regularization is investigated. The proposed method extracts the benefits from robust regression, multiple kernel learning with low-rank approximation, and sparse learning systems. Unlike existing hybrid forecasting methods, which frequently combine different models in parallel, we embed a Huber or quantile loss function and a compound regularization composed of smoothly clipped absolute deviation and ridge regularizations in a support vector machine with predefined number of kernels. To select the optimal kernels, 
                        
                           L
                           1
                        
                      penalization with positive constraint is also considered. The proposed model exhibits robustness, forecasting accuracy, and sparsity in the reproducing kernel Hilbert space. For computation, a simple algorithm is designed based on local quadratic approximation to implement the proposed method. Theoretically, the forecasting and estimation error bounds of the proposed estimators are derived under a null consistency assumption. Real data experiments using datasets from various scientific research fields demonstrate the superior performances of the proposed approach compared with other state-of-the-art competitors.",space
10.1016/j.cogsys.2020.10.005,Journal,Cognitive Systems Research,scopus,2021-01-01,sciencedirect,Cogmic space for narrative-based world representation,https://api.elsevier.com/content/abstract/scopus_id/85096684889,"Representing a world or a physical/social environment in an agent’s cognitive system is essential for creating human-like artificial intelligence. This study takes a story-centered approach to this issue. In this context, a story refers to an internal representation involving a narrative structure, which is assumed to be a common form of organizing past, present, future, and fictional events and situations. In the artificial intelligence field, a story or narrative is traditionally treated as a symbolic representation. However, a symbolic story representation is limited in its representational power to construct a rich world. For example, a symbolic story representation is unfit to handle the sensory/bodily dimension of a world. In search of a computational theory for narrative-based world representation, this study proposes the conceptual framework of a Cogmic Space for a comic strip-like representation of a world. In the proposed framework, a story is positioned as a mid-level representation, in which the conceptual and sensory/bodily dimensions of a world are unified. The events and their background situations that constitute a story are unified into a sequence of panels. Based on this structure, a representation (i.e., a story) and the represented environment are connected via an isomorphism of their temporal, spatial, and relational structures. Furthermore, the framework of a Cogmic Space is associated with the generative aspect of representations, which is conceptualized in terms of unconscious- and conscious-level processes/representations. Finally, a proof-of-concept implementation is presented to provide a concrete account of the proposed framework.",space
10.1016/j.ajo.2020.07.020,Journal,American Journal of Ophthalmology,scopus,2021-01-01,sciencedirect,Lightweight Learning-Based Automatic Segmentation of Subretinal Blebs on Microscope-Integrated Optical Coherence Tomography Images,https://api.elsevier.com/content/abstract/scopus_id/85092610123,"Purpose
                  Subretinal injections of therapeutics are commonly used to treat ocular diseases. Accurate dosing of therapeutics at target locations is crucial but difficult to achieve using subretinal injections due to leakage, and there is no method available to measure the volume of therapeutics successfully administered to the subretinal location during surgery. Here, we introduce the first automatic method for quantifying the volume of subretinal blebs, using porcine eyes injected with Ringer's lactate solution as samples.
               
                  Design
                  Ex vivo animal study.
               
                  Methods
                  Microscope-integrated optical coherence tomography was used to obtain 3D visualization of subretinal blebs in porcine eyes at Duke Eye Center. Two different injection phases were imaged and analyzed in 15 eyes (30 volumes), selected from a total of 37 eyes. The inclusion/exclusion criteria were set independently from the algorithm-development and testing team. A novel lightweight, deep learning–based algorithm was designed to segment subretinal bleb boundaries. A cross-validation method was used to avoid selection bias. An ensemble-classifier strategy was applied to generate final results for the test dataset.
               
                  Results
                  The algorithm performs notably better than 4 other state-of-the-art deep learning–based segmentation methods, achieving an F1 score of 93.86 ± 1.17% and 96.90 ± 0.59% on the independent test data for entry and full blebs, respectively.
               
                  Conclusion
                  The proposed algorithm accurately segmented the volumetric boundaries of Ringer's lactate solution delivered into the subretinal space of porcine eyes with robust performance and real-time speed. This is the first step for future applications in computer-guided delivery of therapeutics into the subretinal space in human subjects.",space
10.1016/j.jobe.2020.101854,Journal,Journal of Building Engineering,scopus,2021-01-01,sciencedirect,Modeling for indoor temperature prediction based on time-delay and Elman neural network in air conditioning system,https://api.elsevier.com/content/abstract/scopus_id/85092148207,"An effective indoor temperature model would assist in improving energy efficiency and indoor thermal comfort of air conditioning system. However, it is difficult to build an accurate model due to lag response characteristic in the regulation process of indoor temperature. To solve this problem, the modeling and prediction methods for indoor temperature lag response characteristic based on time-delay neural network (TDNN) and Elman network neural (ENN) are presented. Then, taking variable air volume (VAV) air conditioning system as the study object, the effectiveness and practicability of proposed methods are validated using simulation sampling data and real-time operating data. Results indicate that ENN could be considered as a better modeling method for indoor temperature prediction for its simpler network structure, smaller storing space and better prediction accuracy. The contribution of this study is to provide an applicable online ANN modeling method for indoor temperature lag characteristic, and detailed training and validation for online implementation are presented, which will benefit for engineers and technicians to use in practical engineering. Meanwhile, this study provides the reference for online application of advanced intelligent algorithms in the building engineering.",space
10.1016/j.ymssp.2020.107061,Journal,Mechanical Systems and Signal Processing,scopus,2021-01-01,sciencedirect,Recovering compressed images for automatic crack segmentation using generative models,https://api.elsevier.com/content/abstract/scopus_id/85086994715,"In a structural health monitoring (SHM) system that uses digital cameras to monitor cracks of structural surfaces, techniques for reliable and effective data compression are essential to ensure a stable and energy-efficient crack images transmission in wireless devices, e.g., drones and robots with high definition cameras installed. Compressive sensing (CS) is a signal processing technique that allows accurate recovery of a signal from a sampling rate much smaller than the limitation of the Nyquist sampling theorem. Different from the popular approach of simultaneously training encoder and decoder using neural network models, the CS theory ensures a high probability of accurate signal reconstruction based on random measurements that is shorter than the length of the original signal under a sparsity constraint. Such method is particularly useful when measurements are expensive, such as wireless sensing of civil structures, because its hardware implementation allows down sampling of signals during the sensing process. Hence, CS methods can achieve significant energy saving for the sensing devices. However, the strong assumption of the signals being highly sparse in an invertible space is relatively hard to guarantee for many real images, such as image of cracks. In this paper, we present a new approach of CS that replaces the sparsity regularization with a generative model that is able to effectively capture a low dimension representation of targeted images. We develop a recovery framework for automatic crack segmentation of compressed crack images based on this new CS method. We demonstrate the remarkable performance of our method that takes advantage of the strong capability of generative models to capture the necessary features required in the crack segmentation task even the backgrounds of the generated images are not well reconstructed. The superior performance of our recovery framework is illustrated by comparisons to three existing CS algorithms. Furthermore, we show that our framework is potentially extensible to other common problems in automatic crack segmentation, such as defect recovery from motion blurring and occlusion.",space
10.1016/j.neucom.2019.12.006,Journal,Neurocomputing,scopus,2020-12-22,sciencedirect,Learning salient features to prevent model drift for correlation tracking,https://api.elsevier.com/content/abstract/scopus_id/85089801054,"Correlation Filter (CF) based algorithms play an important role in the field of Visual Object Tracking (VOT) due to their high accuracy and low computational complexity. While existing CF tracking algorithms suffer performance degradation due to inaccurate object modeling. In this paper, we improve the object modeling accuracy in both CF training stage and target detection procedure to preventing the drift problem. Specifically, we propose a multi-model structure for CF trackers to capture the target appearance changes, where different appearance models are trained with specific samples to catch the salient features of the target and reduce the computational cost. Furthermore, a space filter for detection features is designed to suppress the boundary effect under Gaussian motion prior, which contributes to improving the accuracy of position estimation. We deploy our method to three hand-crafted features based CF trackers to perform real-time visual tracking on popular benchmarks. The experimental results demonstrate the efficacy of our proposed scheme and the efficiency of our trackers. In addition, we provide a comprehensive analysis of the proposed method to facilitate application.",space
10.1016/j.iot.2020.100301,Journal,Internet of Things (Netherlands),scopus,2020-12-01,sciencedirect,Predicting parking occupancy via machine learning in the web of things,https://api.elsevier.com/content/abstract/scopus_id/85098729769,"The Web of Things (WoT) enables information gathered by sensors deployed in urban environments to be easily shared utilizing open Web standards and semantic technologies, creating easier integration with other Web-based information, towards advanced knowledge. Besides WoT, an essential aspect of understanding dynamic urban systems is artificial intelligence (AI). Via AI, data produced by WoT-enabled sensory observations can be analyzed and transformed into meaningful information, which describes and predicts current and future situations in time and space. This paper examines the impact of WoT and AI in smart cities, considering a real-world problem, the one of predicting parking availability. Traffic cameras are used as WoT sensors, together with weather forecasting Web services. Machine learning (ML) is employed for AI analysis, using predictive models based on neural networks and random forests. The performance of the ML models for the prediction of parking occupancy is better than the state of the art work in the problem under study, scoring an MSE of 7.18 at a time horizon of 60 minutes.",space
10.1016/j.jpdc.2020.08.008,Journal,Journal of Parallel and Distributed Computing,scopus,2020-12-01,sciencedirect,Towards cost-effective service migration in mobile edge: A Q-learning approach,https://api.elsevier.com/content/abstract/scopus_id/85090113588,"Service migration in mobile edge computing is a promising approach to improving the quality of service (QoS) for mobile users and reducing the network operational cost for service providers as well. However, these benefits are not free, coming at costs of bulk-data transfer, and likely service disruption, which could consequently increase the overall service costs. To gain the benefits of service migration while minimizing its cost across the edge nodes, in this paper, we leverage reinforcement learning (RL) method to design a cost-effective framework, called Mig-RL, for the service migration with a reduction of total service costs as a goal in a mobile edge environment. The Mig-RL leverages the infrastructure of edge network and deploys a migration agent through Q-learning to learn the optimal policy with respect to the service migration status. We distinguish the Mig-RL from other existing works in several major aspects. First, we fully exploit the nature of this problem in a modest migration space, which allows us to constrain the number of service replicas whereby a defined state–action space could be effectively handled, as opposed to those methods that need to always approximate a huge state–action space for policy optimality. Second, we advocate a migration policy-base as a cache to save the learning process by retrieving the most effective policy whenever a similar migration pattern is encountered as time goes on. Finally, by exploiting the idea of software defined network, we also investigate the efficient implementation of Mig-RL in mobile edge network. Experimental results based on some real and synthesized access sequences show that Mig-RL, compared with the selected existing algorithms, can substantially minimize the service costs, and in the meantime, efficiently improve the QoS by adapting to the changes of mobile access patterns.",space
10.1016/j.inffus.2020.07.003,Journal,Information Fusion,scopus,2020-12-01,sciencedirect,"Data fusion strategies for energy efficiency in buildings: Overview, challenges and novel orientations",https://api.elsevier.com/content/abstract/scopus_id/85087624082,"Recently, tremendous interest has been devoted to develop data fusion strategies for energy efficiency in buildings, where various kinds of information can be processed. However, applying the appropriate data fusion strategy to design an efficient energy efficiency system is not straightforward; it requires a priori knowledge of existing fusion strategies, their applications and their properties. To this regard, seeking to provide the energy research community with a better understanding of data fusion strategies in building energy saving systems, their principles, advantages, and potential applications, this paper proposes an extensive survey of existing data fusion mechanisms deployed to reduce excessive consumption and promote sustainability. We investigate their conceptualizations, advantages, challenges and drawbacks, as well as performing a taxonomy of existing data fusion strategies and other contributing factors. Following, a comprehensive comparison of the state-of-the-art data fusion based energy efficiency frameworks is conducted using various parameters, including data fusion level, data fusion techniques, behavioral change influencer, behavioral change incentive, recorded data, platform architecture, IoT technology and application scenario. Moreover, a novel method for electrical appliance identification is proposed based on the fusion of 2D local texture descriptors, where 1D power signals are transformed into 2D space and treated as images. The empirical evaluation, conducted on three real datasets, shows promising performance, in which up to 99.68% accuracy and 99.52% F1 score have been attained. In addition, various open research challenges and future orientations to improve data fusion based energy efficiency ecosystems are explored.",space
10.1016/j.neunet.2020.08.012,Journal,Neural Networks,scopus,2020-12-01,sciencedirect,Latent Dirichlet allocation based generative adversarial networks,https://api.elsevier.com/content/abstract/scopus_id/85083895792,"Generative adversarial networks have been extensively studied in recent years and powered a wide range of applications, ranging from image generation, image-to-image translation, to text-to-image generation, and visual recognition. These methods typically model the mapping from latent space to image with single or multiple generators. However, they have obvious drawbacks: (i) ignoring the multi-modal structure of images, and (ii) lacking model interpretability. Importantly, the existing methods mostly assume one or more generators can cover all image modes even if we do not know the structure of data. Thus, mode dropping and collapse often take place along with GANs training. Despite the importance of exploring the data structure in generation, it has been almost unexplored. In this work, aiming at generating multi-modal images and interpreting model explicitly, we explore the theory on how to integrate GANs with data structure prior, and propose latent Dirichlet allocation based generative adversarial networks (LDAGAN). This framework is extended to combine with a variety of state-of-the-art single-generator GANs and achieves improved performance. Extensive experiments on synthetic and real datasets demonstrate the efficacy of LDAGAN for multi-modal image generation. An implementation of LDAGAN is available at https://github.com/Sumching/LDAGAN.",space
10.1016/j.knosys.2020.106457,Journal,Knowledge-Based Systems,scopus,2020-11-15,sciencedirect,Mining high utility itemsets using extended chain structure and utility machine,https://api.elsevier.com/content/abstract/scopus_id/85091059855,"High utility itemsets are sets of items that have a high utility (e.g. a high profit or a high importance) in a transaction database. Discovering high utility itemsets has many important applications in real-life such as market basket analysis. Nonetheless, mining these patterns is a time-consuming process due to the huge search space and the high cost of utility computation. Most of previous work is devoted to search space pruning but pay little attention to utility computation. Factually, not only search space pruning but also high utility itemset identification have to resort to the computation of various utilities. This paper proposes a novel algorithm named REX (Rapid itEmset eXtraction), which extends the classic d
                        
                           
                           
                              2
                           
                        
                     HUP algorithm with an improved structure, a 
                        k
                     -item utility machine, and an efficient switch strategy. The structure can significantly reduce the time complexity of utility computation compared with the original structure used in d
                        
                           
                           
                              2
                           
                        
                     HUP. The machine can quickly merge identical transactions and applies an efficient procedure for computing the utilities of extensions of a given itemset. The strategy derived from trial and error drastically gives rise to performance improvement on some databases and is also competitive with the switch strategy used in d
                        
                           
                           
                              2
                           
                        
                     HUP on other databases. Experimental results show that REX achieves a speedup of from fifty percent to three orders of magnitude over d
                        
                           
                           
                              2
                           
                        
                     HUP even though they use identical pruning techniques and that REX considerably outperforms state-of-the-art algorithms on real-life and synthetic databases.",space
10.1016/j.comnet.2020.107436,Journal,Computer Networks,scopus,2020-11-09,sciencedirect,Arena: A 64-antenna SDR-based ceiling grid testing platform for sub-6 GHz 5G-and-Beyond radio spectrum research,https://api.elsevier.com/content/abstract/scopus_id/85090236272,"Arena is an open-access wireless testing platform based on a grid of antennas mounted on the ceiling of a large office-space environment. Each antenna is connected to programmable software-defined radios (SDR) enabling sub-6 GHz 5G-and-beyond spectrum research. With 12 computational servers, 24 SDRs synchronized at the symbol level, and a total of 64 antennas, Arena provides the computational power and the scale to foster new technology development in some of the most crowded spectrum bands. Arena is based on a three-tier design, where the servers and the SDRs are housed in a double rack in a dedicated room, while the antennas are hung off the ceiling of a 2240 square feet office space and cabled to the radios through 100 ft-long cables. This ensures a reconfigurable, scalable, and repeatable real-time experimental evaluation in a real wireless indoor environment.
                  In this paper, we introduce the architecture, capabilities, and system design choices of Arena, and provides details of the software and hardware implementation of various testbed components. Furthermore, we describe key capabilities by providing examples of published work that employed Arena for applications as diverse as synchronized MIMO transmission schemes, multi-hop ad hoc networking, multi-cell 5G networks, AI-powered Radio-Frequency fingerprinting, secure wireless communications, and spectrum sensing for cognitive radio.",space
10.1016/j.neunet.2020.07.028,Journal,Neural Networks,scopus,2020-11-01,sciencedirect,Compressing 3DCNNs based on tensor train decomposition,https://api.elsevier.com/content/abstract/scopus_id/85089391288,"Three-dimensional convolutional neural networks (3DCNNs) have been applied in many tasks, e.g., video and 3D point cloud recognition. However, due to the higher dimension of convolutional kernels, the space complexity of 3DCNNs is generally larger than that of traditional two-dimensional convolutional neural networks (2DCNNs). To miniaturize 3DCNNs for the deployment in confining environments such as embedded devices, neural network compression is a promising approach. In this work, we adopt the tensor train (TT) decomposition, a straightforward and simple in situ training compression method, to shrink the 3DCNN models. Through proposing tensorizing 3D convolutional kernels in TT format, we investigate how to select appropriate TT ranks for achieving higher compression ratio. We have also discussed the redundancy of 3D convolutional kernels for compression, core significance and future directions of this work, as well as the theoretical computation complexity versus practical executing time of convolution in TT. In the light of multiple contrast experiments based on VIVA challenge, UCF11, UCF101, and ModelNet40 datasets, we conclude that TT decomposition can compress 3DCNNs by around one hundred times without significant accuracy loss, which will enable its applications in extensive real world scenarios.",space
10.1016/j.sigpro.2020.107717,Journal,Signal Processing,scopus,2020-11-01,sciencedirect,Fast and efficient implementation of image filtering using a side window convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85088128229,"Convolutional neural networks (CNNs) designed for object recognition have been successfully applied to low-level tasks such as image filtering. However, these networks are usually very large which occupy large memory space and demand very high computational capacity. This makes them unsuitable for real time low-level applications on smart and portable devices with limited memory and computational capacities. In this paper, we specifically design a novel CNN, side window convolutional neural network (SW-CNN), for the fast and efficient implementation of image filtering. In SW-CNN, a new convolutional strategy, called side kernel convolution (SKC) is proposed which aligns the side or corner of the convolutional window with the pixels under processing to preserve edges during convolution. By combining SKC and the representational power of CNNs, SW-CNN can learn various image-filtering tasks very effectively. Compared to the state-of-the-art networks, the superiority of SW-CNN includes three aspects. First, the number of learnable parameters is reduced by 96%. Second, the memory consumption is reduced to 50%. Third, the running time is decreased to 50%. Results of extensive experiments demonstrate that SW-CNN not only has good performance on implementing various edge-preserving filters, but also has the adaptability and flexibility on other low-level image processing applications.",space
10.1016/j.physd.2020.132615,Journal,Physica D: Nonlinear Phenomena,scopus,2020-11-01,sciencedirect,A Reduced Order Deep Data Assimilation model,https://api.elsevier.com/content/abstract/scopus_id/85087917230,"A new Reduced Order Deep Data Assimilation (RODDA) model combining Reduced order models (ROM), Data Assimilation (DA) and Machine Learning is proposed in this paper. The RODDA model aims to improve the accuracy of Computational Fluid Dynamics (CFD) simulations. The DA model ingests information from observed data in the simulation provided by the CFD model. The results of the DA are used to train a neural network learning a function which predicts the misfit between the results of the CFD model and the DA model. Thus, the trained function is combined with the original CFD model in order to generate forecasts with implicit DA given by neural network. Due to the time complexity of the numerical models used to implement DA and the neural network, and due to the scale of the forecasting area considered for forecasting problems in real case scenarios, the implementation of RODDA mandated the introduction of opportune reduced spaces. Here, RODDA is applied to a CFD simulation for air pollution, using the CFD software Fluidity, in South London (UK). We show that, using this framework, the data forecasted by the coupled model CFD+RODDA are closer to the observations with a gain in terms of execution time with respect to the classic prediction–correction cycle given by coupling CFD with a standard DA. Additionally, RODDA predicts future observations, if not available, since these are embedded in the data assimilated state in which the network is trained on. The RODDA framework is not exclusive to air pollution, Fluidity, or the study area in South London, and therefore the workflow could be applied to different physical models if enough temporal data are available.",space
10.1016/j.cpc.2020.107365,Journal,Computer Physics Communications,scopus,2020-11-01,sciencedirect,CONUNDrum: A program for orbital-free density functional theory calculations,https://api.elsevier.com/content/abstract/scopus_id/85086471143,"We present a new code for energy minimization, structure relaxation and evaluation of bulk parameters in the framework of orbital-free density functional theory (OF-DFT). The implementation is based on solving the Euler–Lagrange equation on an equidistant real space grid on which density dependent variables and derivatives are computed. Some potential components are computed in Fourier space. The code is able to use semilocal and non-local kinetic energy functionals (KEF) as well as neural network based KEFs thus facilitating testing and development of emerging machine-learned KEFs. For semi-local and machine-learned KEFs the kinetic energy potentials are evaluated with real-space differentiation of the components, which are partial derivatives of the KE with respect to the electron density, its gradient and Laplacian.
               
                  Program summary
                  
                     Program title: CONUNDrum.
                  
                     CPC Library link to program files: 
                     http://dx.doi.org/10.17632/phnz2gg8mz.1
                  
                  
                     Licensing provision: GNU GPL v3
                  
                     Programming language: C++
                  
                     External routines: Fastest Fourier Transform in the West (FFTW) library (http://www.fftw.org/)
                  
                     Nature of problem: Calculation of the electronic and structural properties of molecules and extended systems in the framework of the orbital-free density functional theory. Evaluation of the bulk parameters of solid compounds.
                  
                     Solution method: High-order central finite-difference method and fast Fourier transform are used for calculation of different total energy components. Density optimization is performed with the steepest descent or the Polak–Ribière variant of the non-linear conjugate-gradient method with a line search procedure based on the Armijo condition. A numerical approach is used for structural optimization — the total energies with respect to small variations in lattice geometries are computed directly, with subsequent evaluation of the force components via a high-order central-finite difference method. The same numerical procedure is used for evaluation of bulk properties.
                  
                     Restrictions: Local pseudopotentials.",space
10.1016/j.cels.2020.08.016,Journal,Cell Systems,scopus,2020-10-21,sciencedirect,Fast and Flexible Protein Design Using Deep Graph Neural Networks,https://api.elsevier.com/content/abstract/scopus_id/85092248944,"Protein structure and function is determined by the arrangement of the linear sequence of amino acids in 3D space. We show that a deep graph neural network, ProteinSolver, can precisely design sequences that fold into a predetermined shape by phrasing this challenge as a constraint satisfaction problem (CSP), akin to Sudoku puzzles. We trained ProteinSolver on over 70,000,000 real protein sequences corresponding to over 80,000 structures. We show that our method rapidly designs new protein sequences and benchmark them in silico using energy-based scores, molecular dynamics, and structure prediction methods. As a proof-of-principle validation, we use ProteinSolver to generate sequences that match the structure of serum albumin, then synthesize the top-scoring design and validate it in vitro using circular dichroism. ProteinSolver is freely available at http://design.proteinsolver.org and https://gitlab.com/ostrokach/proteinsolver. A record of this paper’s transparent peer review process is included in the Supplemental Information.",space
10.1016/j.ijleo.2020.165205,Journal,Optik,scopus,2020-10-01,sciencedirect,Pattern recognition based on pulse scanning imaging and convolutional neural network for vibrational events in Φ-OTDR,https://api.elsevier.com/content/abstract/scopus_id/85087790249,"Feature extraction method of a phase-sensitive optical time-domain reflectometer distributed optical fiber vibration detection system requires a priori knowledge. A lack of feature evaluation methods leads to a low pattern recognition accuracy. Traditional pattern recognition methods cannot be widely applied. This paper presents the implementation of a deep learning-based method to identify vibration signal categories. First, the vibration signal was reconstructed in the time and space domain, which is regarded as a pulse scanning image. Secondly, moving average was used to reduce noise, and seeking the signal envelope surface as an image sample. Finally, the image sample was inputted into the trained convolutional neural network (CNN) to obtain recognition results. Experiments showed that the phase-sensitive optical time-domain reflectometer pulse scanning imaging pattern recognition method based on deep learning proposed in this paper improved recognition accuracy while ensuring recognition efficiency. The algorithm is easy to implement and apply and satisfies the requirements of real-time online monitoring.",space
10.1016/j.ast.2020.105953,Journal,Aerospace Science and Technology,scopus,2020-10-01,sciencedirect,A reliable data-driven model for Ablative Pulsed Plasma Thruster,https://api.elsevier.com/content/abstract/scopus_id/85087284442,"Ablative Pulsed Plasma Thrusters (APPTs) are high specific impulse electric space propulsion system, but a reliable model equivalent of the experimental model is still unavailable. In this paper, a reliable model is developed based on APPT experimental data by using Machine Learning (ML) ecosystem. The goals of this study are to justify the accuracy and reliability of the newly built APPT model with the existing experimental and simulation model. For four sets of operating conditions, 600 experimental and simulation test operations are done. The experimental voltages and currents are measured with a high-voltage probe and a Rogowski coil, respectively. The simulation voltages and currents are gathered by running the respective simulation program. Comparison results show that the newly built APPT model has better accuracy and reliability than the simulated APPT model as compared to real APPT used in the experiment. This data-driven approach provides a novel way of designing a reliable alternative model of physical APPTs.",space
10.1016/j.rser.2020.109920,Journal,Renewable and Sustainable Energy Reviews,scopus,2020-09-01,sciencedirect,Virtual testbed for model predictive control development in district cooling systems,https://api.elsevier.com/content/abstract/scopus_id/85086021217,"Recently, with increasing cooling demands, district cooling has assumed an important role as it is more efficient than stand-alone cooling systems. District cooling reduces the environmental impact and promotes the use of renewable sources. Earlier studies to optimise the production plants of district cooling systems were focused primarily on plants with compressor chillers and thermal energy storage devices. Although absorption chillers are crucial for integrating renewable sources into these systems, very few studies have considered them from the cooling perspective. In this regard, this paper presents the progress and results of the implementation of a virtual testbed based on a digital twin of a district cooling production plant with both compressor and absorption chillers. The aim of this study, carried out within the framework of INDIGO, a European Union-funded project, was (i) to develop a reliable model that can be used in a model predictive controller and (ii) to simulate the plant using this controller. The production plant components, which included absorption and compressor chillers, as well as cooling towers, were built using the equation-based Modelica programming language, and were calibrated using information from the manufacturer, together with real operation data. The remainder of the plant was modelled in Python. To integrate the Modelica models into the Python environment, a combination of machine learning techniques and state-space representation models was used. With these techniques, models with a high computational speed were obtained, which were suitable for real-time applications. These models were then used to build a model predictive control for the production plant to minimise the primary energy usage. The improvements in the control and the resultant energy savings achieved were compared with a baseline case working on a standard cascade control. Energy savings up to 50% were obtained in the simulation-based experiments.",space
10.1016/j.neucom.2020.02.104,Journal,Neurocomputing,scopus,2020-08-18,sciencedirect,An overview of recent multi-view clustering,https://api.elsevier.com/content/abstract/scopus_id/85083344834,"With the widespread deployment of sensors and the Internet-of-Things, multi-view data has become more common and publicly available. Compared to traditional data that describes objects from single perspective, multi-view data is semantically richer, more useful, however more complex. Since traditional clustering algorithms cannot handle such data, multi-view clustering has become a research hotspot. In this paper, we review some of the latest multi-view clustering algorithms, which are reasonably divided into three categories. To evaluate their performance, we perform extensive experiments on seven real-world data sets. Three mainstream metrics are used, including clustering accuracy, normalized mutual information and purity. Based on the experimental results and a large number of literature reading, we also discuss existing problems in current multi-view clustering and point out possible research directions in the future. This research provides some insights for researchers in related fields and may further promote the development of multi-view clustering algorithms.",space
10.1016/j.patter.2020.100074,Journal,Patterns,scopus,2020-08-14,sciencedirect,Machine-Learning Approaches in COVID-19 Survival Analysis and Discharge-Time Likelihood Prediction Using Clinical Data,https://api.elsevier.com/content/abstract/scopus_id/85092796371,"As a highly contagious respiratory disease, COVID-19 has yielded high mortality rates since its emergence in December 2019. As the number of COVID-19 cases soars in epicenters, health officials are warning about the possibility of the designated treatment centers being overwhelmed by coronavirus patients. In this study, several computational techniques are implemented to analyze the survival characteristics of 1,182 patients. The computational results agree with the outcome reported in early clinical reports released for a group of patients from China that confirmed a higher mortality rate in men compared with women and in older age groups. The discharge-time prediction of COVID-19 patients was also evaluated using different machine-learning and statistical analysis methods. The results indicate that the Gradient Boosting survival model outperforms other models for patient survival prediction in this study. This research study is aimed to help health officials make more educated decisions during the outbreak.",space
10.1016/j.trc.2020.102649,Journal,Transportation Research Part C: Emerging Technologies,scopus,2020-08-01,sciencedirect,Differential variable speed limits control for freeway recurrent bottlenecks via deep actor-critic algorithm,https://api.elsevier.com/content/abstract/scopus_id/85086802562,"Variable speed limit (VSL) control is a flexible way to improve traffic conditions, increase safety, and reduce emissions. There is an emerging trend of using reinforcement learning methods for VSL control. Currently, deep learning is enabling reinforcement learning to develop autonomous control agents for problems that were previously intractable. In this paper, a more effective deep reinforcement learning (DRL) model is developed for differential variable speed limit (DVSL) control, in which dynamic and distinct speed limits among lanes can be imposed. The proposed DRL model uses a novel actor-critic architecture to learn a large number of discrete speed limits in a continuous action space. Different reward signals, such as total travel time, bottleneck speed, emergency braking, and vehicular emissions are used to train the DVSL controller, and a comparison between these reward signals is conducted. The proposed DRL-based DVSL controllers are tested on a freeway with a simulated recurrent bottleneck. The simulation results show that the DRL based DVSL control strategy is able to improve the safety, efficiency and environment-friendliness of the freeway. In order to verify whether the controller generalizes to real world implementation, we also evaluate the generalization of the controllers on environments with different driving behavior attributes. and the robustness of the DRL agent is observed from the results.",space
10.1016/j.ast.2020.105902,Journal,Aerospace Science and Technology,scopus,2020-08-01,sciencedirect,Hybrid MultiGene Genetic Programming - Artificial neural networks approach for dynamic performance prediction of an aeroengine,https://api.elsevier.com/content/abstract/scopus_id/85086503677,"Dynamic aeroengine models have an important role in the design of real-time control systems. Modelling of aeroengines using dynamic performance simulations is a key step in the design process in order to reduce costs and the development period. A dynamic model can provide a numerical counterpart for the development of control systems and for the study of the engine behaviour in both steady and unsteady scenarios. The latter situation is particularly felt in the military field. The Viper 632-43 engine analysed in this work is a military turbojet, so it was necessary to develop a model that would replicate its behaviour as realistically as possible. The model was built using the Gas turbine Simulation Program (GSP) software and validated both in steady and transient conditions.
                  Once the engine model was validated, different machine learning techniques were used to estimate (data mining) and predict an engine parameter; the Exhaust Gas Temperature (EGT) has been chosen as the key parameter. A MultiGene Genetic Programming (MGGP) technique has been used to derive simple mathematical relationships between different input parameters and the EGT. These, then, can be used to calculate the EGT value of a real Viper 632-43 engine knowing a priori the input parameters and in any operating condition.
                  Finally, the EGT estimated by this algorithm has been added to the dataset used for the one-step-ahead EGT prediction by Artificial Neural Network (ANN). A time-series ANN was used for the EGT prediction, i.e. the Nonlinear AutoRegressive with eXogenous inputs (NARX) neural network. This network recognizes the input data as a real time series and is therefore able to predict the output in the next time step. It was chosen to use, as forecasting method, the one-step-ahead technique which allows to predict the EGT in the immediately next time step.",space
10.1016/j.automatica.2020.109032,Journal,Automatica,scopus,2020-08-01,sciencedirect,Efficient spatio-temporal Gaussian regression via Kalman filtering,https://api.elsevier.com/content/abstract/scopus_id/85084595084,"We study the non-parametric reconstruction of spatio-temporal dynamical processes via Gaussian Processes (GPs) regression from sparse and noisy data. GPs have been mainly applied to spatial regression where they represent one of the most powerful estimation approaches also thanks to their universal representing properties. Their extension to dynamical processes has been instead elusive so far since classical implementations lead to unscalable algorithms or require some sort of approximation. We propose a novel procedure to address this problem by coupling GPs regression and Kalman filtering. In particular, assuming space/time separability of the covariance (kernel) of the process and rational time spectrum, we build a finite-dimensional discrete-time state-space process representation amenable to Kalman filtering. With sampling over a finite set of fixed spatial locations, our major finding is that the current Kalman filter state represents a sufficient statistic to compute the minimum variance estimate of the process at any future time over the entire spatial domain. In machine learning, a representer theorem states that an important class of infinite-dimensional variational problems admits a computable and finite-dimensional exact solution. In view of this, our result can be interpreted as a novel Dynamic Representer Theorem for GPs. We then extend the study to situations where the spatial input locations set varies over time. The proposed algorithms are tested on both synthetic and real field data, providing comparisons with standard GP and truncated GP regression techniques.",space
10.1016/j.neucom.2020.02.035,Journal,Neurocomputing,scopus,2020-07-20,sciencedirect,Sparse low rank factorization for deep neural network compression,https://api.elsevier.com/content/abstract/scopus_id/85081399638,"Storing and processing millions of parameters in deep neural networks is highly challenging during the deployment of model in real-time application on resource constrained devices. Popular low-rank approximation approach singular value decomposition (SVD) is generally applied to the weights of fully connected layers where compact storage is achieved by keeping only the most prominent components of the decomposed matrices. Years of research on pruning-based neural network model compression revealed that the relative importance or contribution of each neuron in a layer highly vary among each other. Recently, synapses pruning has also demonstrated that having sparse matrices in network architecture achieve lower space and faster computation during inference time. We extend these arguments by proposing that the low-rank decomposition of weight matrices should also consider significance of both input as well as output neurons of a layer. Combining the ideas of sparsity and existence of unequal contributions of neurons towards achieving the target, we propose sparse low rank (SLR) method which sparsifies SVD matrices to achieve better compression rate by keeping lower rank for unimportant neurons. We demonstrate the effectiveness of our method in compressing famous convolutional neural networks based image recognition frameworks which are trained on popular datasets. Experimental results show that the proposed approach SLR outperforms vanilla truncated SVD and a pruning baseline, achieving better compression rates with minimal or no loss in the accuracy. Code of the proposed approach is avaialble at https://github.com/sridarah/slr.",space
10.1016/j.pmcj.2020.101210,Journal,Pervasive and Mobile Computing,scopus,2020-07-01,sciencedirect,A Deep Learning approach for Path Prediction in a Location-based IoT system,https://api.elsevier.com/content/abstract/scopus_id/85086567816,"Knowing in real-time the position of objects and people, both in indoor and outdoor spaces, allows companies and organizations to improve their processes and offer new kind of services. Nowadays Location-based Services (LBS) generate a significant amount of data thank to the widespread of the Internet of Things; since they have been quickly perceived as a potential source of profit, several companies have started to design and develop a wide range of such services. One of the most challenging research tasks is undoubtedly represented by the analysis of LBS data through Machine Learning algorithms and methodologies in order to infer new knowledge and build-up even more customized services. Cultural Heritage is a domain that can benefit from such studies since it is characterized by a strong interaction between people, cultural items and spaces. Data gathered in a museum on visitor movements and behaviours can constitute the knowledge base to realize an advanced monitoring system able to offer museum stakeholders a complete and real-time snapshot of the museum locations occupancy. Furthermore, exploiting such data through Deep Learning methodologies can lead to the development of a predictive monitoring system able to suggest stakeholders the museum locations occupancy not only in real-time but also in the next future, opening new scenarios in the management of a museum. In this paper, we present and discuss a Deep Learning methodology applied to data coming from a non-invasive Bluetooth IoT monitoring system deployed inside a cultural space. Through the analysis of visitors’ paths, the main goal is to predict the occupancy of the available rooms. Experimental results on real data demonstrate the feasibility of the proposed approach; it can represent a useful instrument, in the hands of the museum management, to enhance the quality-of-service within this kind of spaces.",space
10.1016/j.buildenv.2020.106854,Journal,Building and Environment,scopus,2020-06-15,sciencedirect,A daylight-linked shading strategy for automated blinds based on model-based control and Radial Basis Function (RBF) optimization,https://api.elsevier.com/content/abstract/scopus_id/85084048092,"Addressing both daylight maximization and glare control over the entire workplace is always challenging for developing the automated shading control system. For the sake of cost and space usage, it is impractical to mount multiple sensors or cameras for real-time daylight environment monitoring to guarantee the control precision. Cut-off control is popular while it cannot attenuate the glare caused by excessive diffuse daylight. This paper introduces a model-based shading control for predetermining shading positions at each time step. A Useful Daylight Illuminance paradigm modality called rUDI is proposed as a variable criterion added to assist the cut-off strategy for further eliminating glare. The controller could be developed through real-time daylight simulations and an optimizer based on the surrogate model. This method was implemented in a full-scale office in Harbin, China. The surrogate model grounded on the Radial Basis Function Neural Network (RBF) was trained, validated and test with the experimental data sets. The control strategy was further incorporated with an adaptive light-switch model. The comparative simulations were conducted, and their corresponding results were generated for evaluating their performance in visual comfort, daylighting and electrical energy savings, demonstrating the advantages of the proposed control approach in terms of its adequate performance.",space
10.1016/j.compbiomed.2020.103800,Journal,Computers in Biology and Medicine,scopus,2020-06-01,sciencedirect,ECG signal classification with binarized convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85084584293,"Arrhythmias are a group of common conditions associated with irregular heart rhythms. Some of these conditions, for instance, atrial fibrillation (AF), might develop into serious syndromes if not treated in time. Therefore, for high-risk patients, early detection of arrhythmias is crucial. In this study, we propose employing deep convolutional neural network (CNN)-based algorithms for real-time arrhythmia detection. We first build a full-precision deep convolutional network model. With our proposed construction, we are able to achieve state-of-the-art level performance on the PhysioNet/CinC AF Classification Challenge 2017 dataset with our full-precision model. It is desirable to employ models with low computing resource requirements. It has been shown that a binarized model requires much less computing power and memory space than a full-precision model. We proceed to verify the feasibility of binarization in our neural network model. Network binarization can cause significant model performance degradation. Therefore, we propose employing a full-precision model as the teacher to regularize the training of the binarized model through knowledge distillation. With our proposed approach, we observe that network binarization only causes a small performance loss (the F1 score decreases from 0.88 to 0.87 for the validation set). Given that binarized convolutional networks can achieve favorable model performance while dramatically reducing computing cost, they are ideal for deployment on long-term cardiac condition monitoring devices. (Source code is available at https://github.com/yangfansun/bnn-ecg).",space
10.1016/j.actaastro.2020.02.036,Journal,Acta Astronautica,scopus,2020-06-01,sciencedirect,Terminal adaptive guidance via reinforcement meta-learning: Applications to autonomous asteroid close-proximity operations,https://api.elsevier.com/content/abstract/scopus_id/85080064409,"Current practice for asteroid close proximity maneuvers requires extremely accurate characterization of the environmental dynamics and precise spacecraft positioning prior to the maneuver. This creates a delay of several months between the spacecraft's arrival and the ability to safely complete close proximity maneuvers. In this work we develop an adaptive integrated guidance, navigation, and control system that can complete these maneuvers in environments with unknown dynamics, with initial conditions spanning a large deployment region, and without a shape model of the asteroid. The system is implemented as a policy optimized using reinforcement meta-learning. The lander is equipped with an optical seeker that locks to either a terrain feature, reflected light from a targeting laser, or an active beacon, and the policy maps observations consisting of seeker angles and LIDAR range readings directly to engine thrust commands. The policy implements a recurrent network layer that allows the deployed policy to adapt real time to both environmental forces acting on the agent and internal disturbances such as actuator failure and center of mass variation. We validate the guidance system through simulated landing maneuvers in a six degrees-of-freedom simulator. The simulator randomizes the asteroid's characteristics such as solar radiation pressure, density, spin rate, and nutation angle, requiring the guidance and control system to adapt to the environment. We also demonstrate robustness to actuator failure, sensor bias, and changes in the lander's center of mass and inertia tensor. Finally, we suggest a concept of operations for asteroid close proximity maneuvers that is compatible with the guidance system.",space
10.1016/j.renene.2020.01.092,Journal,Renewable Energy,scopus,2020-06-01,sciencedirect,Solar irradiance forecasting models without on-site training measurements,https://api.elsevier.com/content/abstract/scopus_id/85078400450,"Much effort has been made to increase the integration of solar photovoltaic (PV) systems to reduce the environmental impacts of fossil fuels. An essential process in PV systems is the forecasting of solar irradiance to avoid safety and stability problems due to its intermittent nature. Most of the research has been focused on improving the prediction accuracy based on the assumption that enough on-site training data are available. However, in many situations, it is required for the implementation of PV systems in locations where not enough solar irradiance measurements have been collected. Our hypothesis is that measurements from other sites can be used to train accurate forecasting models, given an appropriate definition of site similarity. We propose a methodology that takes information from exogenous variables that are correlated to on-site solar irradiance and constructs a multidimensional space equipped with a metric. Each site is a point in this space, and the learned metric is used to select those sites that can provide measurements to train an accurate forecasting model on an unobserved site. We show through experiments with real data that using the learned metric provides better predictions than using the measurements collected from the whole set of available sites.",space
10.1016/j.knosys.2020.105645,Journal,Knowledge-Based Systems,scopus,2020-05-11,sciencedirect,Bayesian optimisation in unknown bounded search domains,https://api.elsevier.com/content/abstract/scopus_id/85079889517,"Bayesian optimisation (BO) is one of the most sample efficient methods for determining the optima of expensive, noisy black-box functions. Despite its tremendous success in scientific discovery and hyperparameter tuning, it still requires a bounded search space. The search spaces boundaries are, however, often chosen heuristically with an educated guess. If the boundaries are misspecified, then the search space may either be unnecessarily large and hence more expensive to optimise, or it may simply not contain the global optimum. In this paper, we introduce a method for dynamically determining the bound directly from the data. This is done using a distribution of the bound derived in a Bayesian setting. The prior is chosen by the user and the likelihood is derived with Thompson sampling. This results in a bound that is both cheap to optimise and has a high probability of containing the global optimum. We compare the performance of our method with the alternative methods on a range of synthetic and real-world problems and demonstrate that our method achieves consistently superior results.",space
10.1016/j.ress.2020.106821,Journal,Reliability Engineering and System Safety,scopus,2020-05-01,sciencedirect,Towards Efficient Robust Optimization using Data based Optimal Segmentation of Uncertain Space,https://api.elsevier.com/content/abstract/scopus_id/85078707908,"Performing multi-objective optimization under uncertainty is a common requirement in industries and academia. Robust optimization (RO) is considered as an efficient and tractable approach provided one has access to behavioral data for the uncertain parameters. However, solutions of RO may be far from the real solution and less reliable due to inability to map the uncertain space accurately, especially when the data appears discontinuous and scattered in the uncertain domain. Amalgamating machine learning algorithms with RO, this paper proposes a data-driven methodology, where a novel fuzzy clustering mechanism is implemented along-with boundary construction, to transcript the uncertain space such that the specific regions of uncertainty are identified. Subsequently, using intelligent Sobol sampling, samples are generated in the mapped uncertain regions. Results of two test cases are presented along with a comprehensive comparison study. Considered case-studies include highly nonlinear model for continuous casting process from steelmaking industries, where a multi-objective optimization problem under uncertainty is solved to balance the conflict between productivity and energy consumption. The Pareto-optimal solutions of the resulting RO problem are obtained through Non-Dominated Sorting Genetic Algorithm – II, and ~23–29% improvement is observed in the uncertain objective function. Further, the spread and diversity metrics are enhanced by ~10–95% as compared to those obtained using other standard uncertainty sets.",space
10.1016/j.jpowsour.2020.227964,Journal,Journal of Power Sources,scopus,2020-04-15,sciencedirect,Data-driven reinforcement-learning-based hierarchical energy management strategy for fuel cell/battery/ultracapacitor hybrid electric vehicles,https://api.elsevier.com/content/abstract/scopus_id/85080125591,"A reinforcement-learning-based energy management strategy is proposed in this paper for managing energy system of Fuel Cell Hybrid Electric Vehicles (FCHEV) equipped with three power sources. A hierarchical power splitting structure is employed to shrink large state-action space based on an adaptive fuzzy filter. Then, the reinforcement-learning-based algorithm using Equivalent Consumption Minimization Strategy (ECMS) is proposed for tackling high-dimensional state-action space, and finding a trade-off between global learning and real-time implementation. The power splitting policy based on experimental data is obtained by using reinforcement learning algorithm, which allows for many different driving cycles and traffic conditions. The proposed energy management strategy can achieve low computation cost, optimal fuel cell efficiency and energy consumption economy. Simulation results confirm that, compared with existing learning algorithms and optimization methods, the proposed reinforcement-learning-based energy management strategy using ECMS can achieve high computation efficiency, lower power fluctuation of fuel cell and optimal fuel economy of FCHEV.",space
10.1016/j.ins.2019.11.018,Journal,Information Sciences,scopus,2020-04-01,sciencedirect,EHAUSM: An efficient algorithm for high average utility sequence mining,https://api.elsevier.com/content/abstract/scopus_id/85076715468,"Identifying high utility sequences in a quantitative sequence database is an important data mining task. However, a key problem of current approaches is that extensions of a high utility sequence often have a high utility. Hence, traditional techniques are often biased toward finding long patterns. To circumvent this problem, this paper proposes techniques for the problem of high average-utility sequence (HAUS) mining (
                        
                           H
                           A
                           U
                           S
                           M
                        
                     ). HAUSs are more meaningful than high utility sequences because the former are found using the average-utility measure, which consider the length of patterns in utility calculations. 
                        
                           H
                           A
                           U
                           S
                           M
                        
                      is more general than high average-utility itemset mining but it is also a more difficult problem because the downward-closure property used for search space reduction does not hold for the average-utility. To overcome that challenge, this paper introduces two upper bounds and a weak upper bound on the average-utility measure, and four width pruning, depth pruning, reducing, and tightening strategies. These strategies are designed to eliminate candidate HAUSs to speed up HAUS mining. Based on these theoretical results, a novel algorithm named EHAUSM is proposed for 
                        
                           H
                           A
                           U
                           S
                           M
                        
                     . Experiments on both real-life and synthetic quantitative sequence databases have confirmed its efficiency in terms of memory consumption and runtime.",space
10.1016/j.cmpb.2019.105099,Journal,Computer Methods and Programs in Biomedicine,scopus,2020-04-01,sciencedirect,Augmented reality navigation for liver resection with a stereoscopic laparoscope,https://api.elsevier.com/content/abstract/scopus_id/85073003483,"Objective
                  Understanding the three-dimensional (3D) spatial position and orientation of vessels and tumor(s) is vital in laparoscopic liver resection procedures. Augmented reality (AR) techniques can help surgeons see the patient's internal anatomy in conjunction with laparoscopic video images.
               
                  Method
                  In this paper, we present an AR-assisted navigation system for liver resection based on a rigid stereoscopic laparoscope. The stereo image pairs from the laparoscope are used by an unsupervised convolutional network (CNN) framework to estimate depth and generate an intraoperative 3D liver surface. Meanwhile, 3D models of the patient's surgical field are segmented from preoperative CT images using V-Net architecture for volumetric image data in an end-to-end predictive style. A globally optimal iterative closest point (Go-ICP) algorithm is adopted to register the pre- and intraoperative models into a unified coordinate space; then, the preoperative 3D models are superimposed on the live laparoscopic images to provide the surgeon with detailed information about the subsurface of the patient's anatomy, including tumors, their resection margins and vessels.
               
                  Results
                  The proposed navigation system is tested on four laboratory ex vivo porcine livers and five operating theatre in vivo porcine experiments to validate its accuracy. The ex vivo and in vivo reprojection errors (RPE) are 6.04 ± 1.85 mm and 8.73 ± 2.43 mm, respectively.
               
                  Conclusion and Significance
                  Both the qualitative and quantitative results indicate that our AR-assisted navigation system shows promise and has the potential to be highly useful in clinical practice.",space
10.1016/j.ejor.2019.02.046,Journal,European Journal of Operational Research,scopus,2020-03-16,sciencedirect,Merging anomalous data usage in wireless mobile telecommunications: Business analytics with a strategy-focused data-driven approach for sustainability,https://api.elsevier.com/content/abstract/scopus_id/85063225278,"Mobile internet usage has exploded with the mass popularity of smartphones that offer more convenient and efficient ways of doing anything from watching movies, playing games, and streaming music. Understanding the patterns of data usage is thus essential for strategy-focused data-driven business analytics. However, data usage has several unique stylized facts (such as high dimensionality, heteroscedasticity, and sparsity) due to a great variety of user behaviour. To manage these facts, we propose a novel density-based subspace clustering approach (i.e., a three-stage iterative optimization procedure) for intelligent segmentation of consumer data usage/demand. We discuss the characteristics of the proposed method and illustrate its performance in both simulation with synthetic data and business analytics with real data. In a field experiment of wireless mobile telecommunications for data-driven strategic design and managerial implementation, we show that our method is adequate for business analytics and plausible for sustainability in search of business value.",space
10.1016/j.ast.2019.105657,Journal,Aerospace Science and Technology,scopus,2020-03-01,sciencedirect,Reinforcement learning in dual-arm trajectory planning for a free-floating space robot,https://api.elsevier.com/content/abstract/scopus_id/85077502803,"A free-floating space robot exhibits strong dynamic coupling between the arm and the base, and the resulting position of the end of the arm depends not only on the joint angles but also on the state of the base. Dynamic modeling is complicated for multiple degree of freedom (DOF) manipulators, especially for a space robot with two arms. Therefore, the trajectories are typically planned offline and tracked online. However, this approach is not suitable if the target has relative motion with respect to the servicing space robot. To handle this issue, a model-free reinforcement learning strategy is proposed for training a policy for online trajectory planning without establishing the dynamic and kinematic models of the space robot. The model-free learning algorithm learns a policy that maps states to actions via trial and error in a simulation environment. With the learned policy, which is represented by a feedforward neural network with 2 hidden layers, the space robot can schedule and perform actions quickly and can be implemented for real-time applications. The feasibility of the trained policy is demonstrated for both fixed and moving targets.",space
10.1016/j.engappai.2019.103427,Journal,Engineering Applications of Artificial Intelligence,scopus,2020-03-01,sciencedirect,Stochastic parallel extreme artificial hydrocarbon networks: An implementation for fast and robust supervised machine learning in high-dimensional data,https://api.elsevier.com/content/abstract/scopus_id/85076620125,"Artificial hydrocarbon networks (AHN) – a supervised learning method inspired on organic chemical structures and mechanisms – have shown improvements in predictive power and interpretability in comparison with other well-known machine learning models. However, AHN are very time-consuming that are not able to deal with large data until now. In this paper, we introduce the stochastic parallel extreme artificial hydrocarbon networks (SPE-AHN), an algorithm for fast and robust training of supervised AHN models in high-dimensional data. This training method comprises a population-based meta-heuristic optimization with defined individual encoding and objective function related to the AHN-model, an implementation in parallel-computing, and a stochastic learning approach for consuming large data. We conducted three experiments with synthetic and real data sets to validate the training execution time and performance of the proposed algorithm. Experimental results demonstrated that the proposed SPE-AHN outperforms the original-AHN method, increasing the speed of training more than 
                        
                           10
                           ,
                           000
                           x
                        
                      times in the worst case scenario. Additionally, we present two case studies in real data sets for solar-panel deployment prediction (regression problem), and human falls and daily activities classification in healthcare monitoring systems (classification problem). These case studies showed that SPE-AHN improves the state-of-the-art machine learning models in both engineering problems. We anticipate our new training algorithm to be useful in many applications of AHN like robotics, finance, medical engineering, aerospace, and others, in which large amounts of data (e.g. big data) is essential.",space
10.1016/j.actaastro.2019.11.037,Journal,Acta Astronautica,scopus,2020-03-01,sciencedirect,Pattern recognition in time series for space missions: A rosetta magnetic field case study,https://api.elsevier.com/content/abstract/scopus_id/85076239823,"Time series analysis is a technique widely employed in space science. In unpredictable environments like space, scientific analysis relies on large data sets to enable interpretation of observations. Artificial signal interferences caused by the spacecraft itself further impede this process. The most time consuming part of these studies is the efficient identification of recurrent pattern in observations, both of artificial and natural origin, often forcing researchers to limit their analysis to a reduced set of observations. While pattern recognition techniques for time series are well known, their application is discussed and evaluated primarily on purpose built or heavily preprocessed data sets. The aim of this paper is to evaluate the performance of state of the art pattern recognition techniques in terms of computational efficiency and validity on a real-life testcase. For this purpose the most suitable techniques for different types of pattern are discussed and subsequently evaluated on various hardware in comparison to manual identification. Using magnetic field observations of the ESA Rosetta mission as a representative example, both disturbances and natural patterns are identified. Compared to manual selection a speed-up of a factor up to 100 is achieved, with values for recall and precision above 80%. Moreover, the detection process is fully automated and reproducible. Using the presented method it was possible to detect and correct artificial interference. Finally, the feasibility of onboard deployment is briefly discussed.",space
10.1016/j.cmpb.2019.105132,Journal,Computer Methods and Programs in Biomedicine,scopus,2020-03-01,sciencedirect,Virtual reality-based measurement of ocular deviation in strabismus,https://api.elsevier.com/content/abstract/scopus_id/85073938186,"Background and objective
                  Strabismus is an eye movement disorder in which shows the abnormal ocular deviation. Cover tests have mainly been used in the clinical diagnosis of strabismus for treatment. However, the whole process depends on the doctor's level of experience, which could be subjected to several factors. In this study, an automated technique for measurement of ocular deviation using a virtual reality (VR) device is developed.
               
                  Methods
                  A VR display system in which the screens that have the fixation target are changed alternately between on and off stages is used to simulate the normal strabismus diagnosis steps. Patients watch special-designed 3D scenes, and their eye motions are recorded by two infrared (IR) cameras. An image-processing-based pupil tracking technique is then applied to track their eye movement. After recording eye motion, two strategies for strabismus angle estimation are implemented: direct measurement and stepwise approximation. The direct measurement converts the eye movement to a strabismus angle after considering the eyeball diameter, while the stepwise approximation measures the ocular deviation through the feedback calibration process.
               
                  Results
                  Experiments are carried out with various strabismus patients. The results are compared to those of their doctors’ measurement, which shows good agreement.
               
                  Conclusions
                  The results clearly indicate that these techniques could identify ocular deviation with high accuracy and efficiency. The proposed system can be applied in small space and has high tolerance for the unexpected head movements compared with other camera-based system.",space
10.1016/j.apenergy.2019.114232,Journal,Applied Energy,scopus,2020-02-15,sciencedirect,Greedy search based data-driven algorithm of centralized thermoelectric generation system under non-uniform temperature distribution,https://api.elsevier.com/content/abstract/scopus_id/85076006670,"The generation efficiency of thermoelectric generation system is relatively low, thus how maximize its power production is of great importance. This paper designs a novel greedy search based data-driven method for centralized thermoelectric generation system to achieve maximum power point tracking under non-uniform temperature distribution. In order to effectively distinguish the local maximum power points and the global maximum power point under non-uniform temperature distribution, greedy search based data-driven employs a two-layer feed-forward neural network to accurately fit the curve between the power output and the controllable variable based on the real-time updated operation data. Based on the approximation curve, a greedy search is designed to efficiently approach the global maximum power point from a shrinking search space. Cases studies such as start-up test, step variation of temperature, stochastic temperature change, and analyse of sensitivity, are implemented to prove the effectiveness and superiority of the proposed algorithm. Simulation results verify that the proposed method can generate the highest energy under non-uniform temperature distribution condition, e.g., 391.34%, 115.71%, 110.92%, and 109.43% to that of perturb and observe, particle swarm optimization, whale optimization algorithm, and grey wolf optimizer in the stochastic temperature change. Lastly, the implementation feasibility of the proposed method is demonstrated by the hardware-in-the-loop experiment based on dSpace platform.",space
10.1016/j.physa.2019.123151,Journal,Physica A: Statistical Mechanics and its Applications,scopus,2020-02-15,sciencedirect,Early warning system: From face recognition by surveillance cameras to social media analysis to detecting suspicious people,https://api.elsevier.com/content/abstract/scopus_id/85074532417,"Surveillance security cameras are increasingly deployed in almost every location for monitoring purposes, including watching people and their actions for security purposes. For criminology, images collected from these cameras are usually used after an incident occurs to analyze who could be the people involved. While this usage of the cameras is important for a post crime action, there exists the need for real time monitoring to act as an early warning to prevent or avoid an incident before it occurs. In this paper, we describe the development and implementation of an early warning system that recognizes people automatically in a surveillance camera environment and then use data from various sources to identify these people and build their profile and network. The current literature is still missing a complete workflow from identifying people/criminals from a video surveillance to building a criminal information extraction framework and identifying those people and their interactions with others We train a feature extraction model for face recognition using convolutional neural networks to get a good recognition rate on the Chokepoint dataset collected using surveillance cameras. The system also provides the function to record people appearance in a location, such that unknown people passing through a scene excessive number of times (above a threshold decided by a security expert) will then be further analyzed to collect information about them. We implemented a queue based system to record people entrance. We try to avoid missing relevant individuals passing through as in some cases it is not possible to add every passing person to the queue which is maintained using some cache handling techniques. We collect and analyze information about unknown people by comparing their images from the cameras to a list of social media profiles collected from Facebook and intelligent services archives. After locating the profile of a person, traditional news and other social media platforms are crawled to collect and analyze more information about the identified person. The analyzed information is then presented to the analyst where a list of keywords and verb phrases are shown. We also construct the person’s network from individuals mentioned with him/her in the text. Further analysis will allow security experts to mark this person as a suspect or safe. This work shows that building a complete early warning system is feasible to tackle and identify criminals so that authorities can take the required actions on the spot.",space
10.1016/j.ifacol.2020.12.1251,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,A Framework for Ethics in Cyber-Physical-Human Systems,https://api.elsevier.com/content/abstract/scopus_id/85113481995,"This paper proposes a conceptual framework for consideration of ethical issues in the emerging category of smart cyber-physical systems. Cyber-physical systems (CPS) that bring together controls, communications, computing, and physical systems are being developed in a wide variety of application domains ranging from transportation, energy, and manufacturing, to biomedical and agriculture. Smart CPS are already being and will increasingly be deployed to work with humans, in workplaces, homes, or public spaces, resulting in the creation of cyber-physical human systems (CPHS). Ethical issues in smart CPS and CPHS can be examined within the larger frameworks of ethics of technology and ethics of artificial intelligence. We begin with a description of trends and visions for the future development of smart CPS. We next outline fundamental theories of ethics that offer foundations for thinking about ethical issues in smart CPHS. We argue that it is necessary to fight the tendency toward technological determinism. We argue that in analyzing ethics of smart CPHS, we need to anticipate increasing capabilities and the future deployment of such systems. Ultimately, if these systems are widely deployed in society, they will have a very significant impact, including possible negative consequences, on individuals, communities, nations, and the world. Our framework has two main dimensions: (i) stage of development of CPHS domain from early stage research to mature technologies; and (ii) locus of decision making: individual, corporate, and government settings. We illustrate the framework with some specific examples.",space
10.1016/j.procs.2020.03.027,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Strategic zoning approach for urban areas: Towards a shared transportation system,https://api.elsevier.com/content/abstract/scopus_id/85085571988,"Investigating downstream freight demand is a prerequisite to accomplishing the overall strategic implementation of transportation systems. Machine learning has recently become widely applied in order to support decision-making in several logistic operational levels: travel/arrival time prediction, occupancy forecasting of logistic spaces, route optimization and so on. Nevertheless, strategic decision-making often overlooks flow tendencies forecasting. Targeting this perspective, the present paper aims at proposing an urban zoning approach based on time series forecasting of supply chain demand through clustering customers. To conduct our approach, we have selected a set of machine learning algorithms that are believed to be robust according to the literature and the achieved accuracy benchmarks. Considering real-life data-based computational results, a number of analytical insights are illustrated.",space
10.1016/j.promfg.2020.04.037,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Implementing AR/MR - Learning factories as protected learning space to rise the acceptance for mixed and augmented reality devices in production,https://api.elsevier.com/content/abstract/scopus_id/85085498037,"When talking about digitization, changes in the way of working are inevitable: The implementation of intelligent machines or dealing with real-time data lead to new tasks supported by new technology. Also digital technologies such as Augmented and Mixed Reality (AR/MR) are pushing the market and setting new standards in collaboration, prototyping or maintenance. The correct handling of AR/MR devices requires a change in the employees’ behavior; changing working routines are followed by a new skill set and a change in the culture. The acceptance of employees can therefore be regarded as a critical success factor for the implementation of such technologies. Thus, the present paper answers the research question ‘what factors influence the employee’s acceptance of AR and MR data glasses in industry’. On the basis of a comprehensive literature analysis, an implementation workshop was developed and validated in cooperation with an industrial partner. The results were transformed into a workshop within the learning and research factory ‘Smart Production Lab’ to give employees and students the opportunity to train the handling of data glasses in a protected learning space in order to increase the acceptance for the technology.",space
10.1016/j.softx.2020.100419,Journal,SoftwareX,scopus,2020-01-01,sciencedirect,TWINKLE: A digital-twin-building kernel for real-time computer-aided engineering,https://api.elsevier.com/content/abstract/scopus_id/85079158568,"TWINKLE is a library for building families of solvers to perform Canonical Polyadic Decomposition (CPD) of tensors. The common characteristic of these solvers is that the data structure supporting the tuneable solution strategy is based on a Galerkin projection of the phase space. This allows processing and recovering tensors described by highly sparse and unstructured data. For achieving high performance, TWINKLE is written in C++ and uses the Armadillo open source library for linear algebra and scientific computing, based on LAPACK (Linear Algebra PACKage) and BLAS (Basic Linear Algebra Subprograms) routines. The library has been implemented keeping in mind its future extensibility and adaptability to fulfil the different users’ needs in academia and industry regarding Reduced Order Modelling (ROM) and data analysis by means of tensor decomposition. It is especially focused on post-processing data from Computer-Aided-Engineering (CAE) simulation tools.",space
10.1016/bs.adcom.2019.09.008,Book Series,Advances in Computers,scopus,2020-01-01,sciencedirect,Stepping into the digitally instrumented and interconnected era,https://api.elsevier.com/content/abstract/scopus_id/85078358345,"This chapter is to tell all about the digitization-inspired possibilities and opportunities and how software-defined cloud centers are the best fit for hosting and running digital applications. Also, how the next-generation data analytics can be smartly accomplished through cloud platforms and infrastructures is also explained in detail. We are to describe some of the impactful developments and technological advancements brewing in the IT space, how the tremendous amount of data getting produced and processed through cloud systems is to impact the IT and business domains, and how next-generation IT infrastructures are accordingly getting refactored, remedied and readied for the impending big data-induced challenges, how likely the move of the data analytics discipline toward fulfilling the digital universe requirements of extracting and extrapolating actionable insights for the knowledge-parched is, and finally for the establishment and sustenance of the dreamt smarter planet. In short, the uninhibited explosion of digitized systems and connected devices pour out a tremendous amount of multi-structured data and the impending challenge is to make sense out of the data heaps. Data analytics is the way to go and in the recent past, the overwhelming trend is to empower our everyday systems with machine and deep learning algorithms to automatically learn out of data heaps and streams in order to be distinctively intelligent in their actions and reactions. This chapter is specially prepared to put a stimulating foundation for explaining the nitty-gritty of the Digital Twin paradigm.",space
10.1016/j.cogsys.2019.09.015,Journal,Cognitive Systems Research,scopus,2020-01-01,sciencedirect,Multi-Agent neurocognitive models of semantics of spatial localization of events,https://api.elsevier.com/content/abstract/scopus_id/85072851037,"The purpose of the study is to develop a learning system for internal representation of the events localization space to realize orientation and navigation of autonomous mobile systems. The task of the research is the development of simulation models of the semantics of the event localization space based on multi-agent neurocognitive architectures. The paper proves that the multi-agent neurocognitive architecture is an effective formalism for describing the semantics of the spatial localization of events. Main theoretical foundations have been developed for the simulation of spatial relations using the so-called multi-agent facts, consisting of software agents-concepts, reflecting semantic categories corresponding to parts of speech. It is shown that locative software agents that describe the spatial location of objects and events, forming homogeneous connections, compose the so-called field locations. The latter describes a holistic view of the intellectual agent about the environment. The paper defines conceptual foundations of multi-agent modeling of the semantics of subjective reflexive mapping of the interaction between real objects, space and time.",space
10.1016/j.comcom.2019.09.014,Journal,Computer Communications,scopus,2019-12-15,sciencedirect,VARMAN: Multi-plane security framework for software defined networks,https://api.elsevier.com/content/abstract/scopus_id/85072873993,"In the context of future networking technologies, Software-Defined paradigm offers compelling solutions and advantages for traffic orchestration and shaping, flexible and dynamic routing, programmable control and smart application-driven resource management. But the SDN operation has to confront critical issues and technical vulnerabilities, security problems and threats in the enabling technical architecture itself. To address the critical security problems in SDN enabled data centers, we propose a collaborative “Network Security and Intrusion Detection System(NIDS)” scheme called ‘
                        VARMAN
                     : adVanced multi-plAne secuRity fraMework for softwAre defined Networks’. The SDN security scheme comprises of coarse-grained flow monitoring algorithms on the dataplane for rapid anomaly detection and prediction of network-centric DDoS/botnet attacks. In addition, this is combined with a fine-grained hybrid deep-learning based classifier pipeline on the control plane. It is observed that existing ML-based classifiers improve the accuracy of NIDS, however, at the cost of higher processing power and memory requirement, thus unrealistic for real-time solutions. To address these problems and still achieve accuracy and speed, we designed a hybrid model, combining both deep and shallow learning techniques, that are implemented in an improved SDN stack. The data plane deploys attack prediction and behavioral trigger mechanisms, efficient data filtering, feature selection, and data reduction techniques. To demonstrate the practical feasibility of our security scheme in real modern datacenters, we utilized the popular NSL-KDD dataset, most recent CICIDS2017 dataset, and refined it to a balanced dataset containing a comparable number of normal traffic and malware samples. We further augmented the training by organically generating datasets from lab-simulated and public-network hosted hackathon websites. The results show that VARMAN framework is capable of detecting attacks in real-time with accuracy more than 98% under attack intensities up to 50k packets/second. In a multi-controller interconnected SDN domain, the flow setup time improves by 70% on an average, and controller response time reduces by 40%, without incurring additional latency due to security intelligence processing overhead in SDN stack. The comparisons of VARMAN under similar attack scenarios and test environment, with related recent works that utilized ML-based NIDS, demonstrate that our scheme offers higher accuracy, less than 5% false positive rate for various attack intensities and significant training space/time reduction.",space
10.1016/j.eswa.2019.06.066,Journal,Expert Systems with Applications,scopus,2019-12-15,sciencedirect,Double Q-PID algorithm for mobile robot control,https://api.elsevier.com/content/abstract/scopus_id/85068505390,"Many expert systems have been developed for self-adaptive PID controllers of mobile robots. However, the high computational requirements of the expert systems layers, developed for the tuning of the PID controllers, still require previous expert knowledge and high efficiency in algorithmic and software execution for real-time applications. To address these problems, in this paper we propose an expert agent-based system, based on a reinforcement learning agent, for self-adapting multiple low-level PID controllers in mobile robots. For the formulation of the artificial expert agent, we develop an incremental model-free algorithm version of the double Q-Learning algorithm for fast on-line adaptation of multiple low-level PID controllers. Fast learning and high on-line adaptability of the artificial expert agent is achieved by means of a proposed incremental active-learning exploration-exploitation procedure, for a non-uniform state space exploration, along with an experience replay mechanism for multiple value functions updates in the double Q-learning algorithm. A comprehensive comparative simulation study and experiments in a real mobile robot demonstrate the high performance of the proposed algorithm for a real-time simultaneous tuning of multiple adaptive low-level PID controllers of mobile robots in real world conditions.",space
10.1016/j.ins.2019.07.098,Journal,Information Sciences,scopus,2019-12-01,sciencedirect,Counting the frequency of time-constrained serial episodes in a streaming sequence,https://api.elsevier.com/content/abstract/scopus_id/85069902935,"As a representative sequential pattern mining problem, counting the frequency of serial episodes from a streaming sequence has drawn continuous attention in academia due to its wide application in practice, e.g., telecommunication alarms, stock market, transaction logs, bioinformatics, etc. Although a number of serial episodes mining algorithms have been developed recently, most of them are neither stream-oriented, as they require processing the whole dataset multiple times, nor time-aware, as they fail to take into account the time constraint of serial episodes. In this paper, we propose two novel one-pass algorithms, ONCE and ONCE+, each of which can respectively compute two popular frequencies of given episodes satisfying predefined time-constraint as signals in a stream arrives one-after-another. ONCE is only used for non-overlapped frequency where the occurrences of a serial episode in sequence are not intersected. ONCE+ is designed for the distinct frequency where the occurrences of a serial episode do not share any event. Theoretical study proves that our algorithm can correctly mine the frequency of target time constraint serial episodes in a given stream. Experimental study over both real-world and synthetic datasets demonstrates that the proposed algorithm can work, with little time and space, in signal-intensive streams where millions of signals arrive within a single second. Moreover, the algorithm has been applied in a real stream processing system, where the efficacy and efficiency of this work are tested in practical applications.",space
10.1016/j.neucom.2019.08.031,Journal,Neurocomputing,scopus,2019-11-20,sciencedirect,Deep network for human action recognition using Weber motion,https://api.elsevier.com/content/abstract/scopus_id/85070678569,"Effective motion estimation is one of the prime steps for any human action recognition (HAR) algorithm. Optical flow (OF) and motion history image (MHI) are two well-known methods for motion estimation in videos. OF has several advantages over MHI. But the major drawback with OF is that it is computationally very expensive as compared to the MHI. Therefore, in this paper, a new motion estimation technique named as Weber Motion History Image (WMHI) is proposed. Here, an extremely fast algorithm is proposed for HAR using WMHI, pose information, and convolutional neural network (CNN). In spite of being fast and less space consuming, the algorithm outperforms the existing pose based CNN results on five benchmark datasets namely JHMDB [1], sub-JHMDB [1], MPII [2] and HMDB51 [3] and UCF101 [4]. The work mainly focuses on a new efficient algorithm which can be implemented for real-time HAR in videos. For real-time implementation, the two basic criteria on which an algorithm can be analyzed are space and time complexity. The proposed algorithm is faster as compared to the existing OF based HAR systems. In terms of space complexity, the feature size of the proposed algorithm is almost 50% of the existing OF based algorithm. The recognition results still outperform the existing result by a significant margin.",space
10.1016/j.geoderma.2019.07.004,Journal,Geoderma,scopus,2019-11-15,sciencedirect,Optimisation in machine learning: An application to topsoil organic stocks prediction in a dry forest ecosystem,https://api.elsevier.com/content/abstract/scopus_id/85069648816,"Soil organic carbon (SOC) sequestration plays a key role in reducing the atmospheric greenhouse gas concentration. However, dry forest ecosystems in Ecuador are endangered to become a source of carbon emissions because of deforestation. Often spatial information, necessary to quantify potential carbon loss to the atmosphere, is missing. This particularly applies to remote areas of limited accessibility. This study aims to regionalise the SOC stocks of a small and poorly accessible dry forest ecosystem in southwestern Ecuador by using boosted regression tree (BRT) models. Resampling in a nested repeated k-fold cross validation approach was applied to develop robust models for a dataset of 118 samples with limited predictor information. To select an optimal set of model parameters, optimisation by differential evolution (DE) was applied for parameter tuning. Predictor selection was implemented using the same optimisation algorithm. This study demonstrates how the predictive performance of BRT models can be improved by applying an optimisation approach for parameter tuning and predictor selection. Model performance was improved by approximately 40% concerning the R2. Still, the results also demonstrated the difficulties of machine learning applications in small and highly heterogeneous natural areas. Very variable or even random factors were assumed to distort the relationship between predictor and response variables. We assume that the presented approach is particularly successful in the case of a real-valued multivariate space of tuning parameters. However, this requires testing in further machine learning applications and algorithms.",space
10.1016/j.cose.2019.101590,Journal,Computers and Security,scopus,2019-11-01,sciencedirect,Volatile memory analysis using the MinHash method for efficient and secured detection of malware in private cloud,https://api.elsevier.com/content/abstract/scopus_id/85070387186,"Today, most organizations employ cloud computing environments for both computational reasons and for storing their critical files and data. Virtual servers are an example of widely used virtual resources provided by cloud computing architecture. Therefore, virtual servers are considered an attractive target for cyber-attackers, who launch their attacks by malware such as the well-known remote access trojans (RATs) and more modern malware such as ransomware and cryptojacking. Existing security solutions implemented on virtual servers fail to detect these newly created malware (zero-day attacks). In fact, by the time the security solution is updated, the organization has likely already been attacked. In this study, we present a designated framework aimed at trusted and secured detection of newly created and unknown instances of malware on virtual machines in an organization's private cloud. We took volatile memory dumps from a virtual machine (VM) in a secured and trusted manner, and analyzed all of the data within the memory dumps using the MinHash method; MinHash is well suited for the accurate detection of malware in VMs based on efficient volatile memory dump comparisons. The proposed framework is evaluated in a comprehensive set of experiments of increasing difficulty in which we also measured the detection performance of different classifiers (both similarity and machine learning-based classifiers, using collections of real-world, professional, notorious malware and legitimate applications. The evaluation results show that our framework can detect the anomalous state of a virtual server, as well as known, new, and unknown malware, with very high TPRs (100% for ransomware and RATs) and very low FPRs (1.8% for ransomware and no FPR for RATs). We also show how the methodology's performance can be improved, in terms of required time and storage space, saving more than 86% of these resources. Finally, we demonstrate the generalization capabilities and practicality of our methodology by using transfer learning and learning from just one virtual server in order to detect unknown malware on a different virtual server.",space
10.1016/j.ins.2019.07.019,Journal,Information Sciences,scopus,2019-11-01,sciencedirect,Labeled graph sketches: Keeping up with real-time graph streams,https://api.elsevier.com/content/abstract/scopus_id/85068588796,"Currently, graphs serve as fundamental data structures for many applications, such as road networks, social and communication networks, and web requests. In many applications, graph edges stream in and users are only interested in the recent data. In data exploration, the storage and processing of such massive amounts of graph stream data has become a significant problem. As the categorical attributes of vertices and edges are often referred to as labels, we propose a labeled graph sketch that stores real-time graph structural information using only sublinear space and that supports graph queries of diverse types. This sketch also works for sliding-window queries. We conduct extensive experiments on real-world datasets in six different domains and compare the results with a state-of-the-art method to show the accuracy, efficiency, and practicability of our proposed approach.",space
10.1016/j.knosys.2019.06.003,Journal,Knowledge-Based Systems,scopus,2019-10-15,sciencedirect,Efficient processing of top k group skyline queries,https://api.elsevier.com/content/abstract/scopus_id/85067023283,"For a given multi-dimensional data set, a group skyline query returns the optimal groups not dominated by any other group of equal size. The group skyline query is a powerful tool in many applications that call for optimal groups. However, it is common to return a large number of results which make users overwhelmed since it prevents them from making quick and rational decisions. To address this problem, we first identify and formulate a top 
                        k
                      group skyline (T
                        k
                     GSky) query which returns 
                        k
                      optimal groups dominating the highest number of points in the given data set. Next, new pruning strategies are presented to reduce the search space. Then, we propose efficient algorithms by exploiting novel techniques including a grouping strategy, a hybrid strategy, and a point-based replacement strategy, respectively. Finally, we also develop an approximate algorithm to further improve the T
                        k
                     GSky query performance. The performance of the proposed algorithms is studied by extensive experiments over synthetic and real datasets.",space
10.1016/j.cie.2019.07.007,Journal,Computers and Industrial Engineering,scopus,2019-10-01,sciencedirect,Artificial search agents with cognitive intelligence for binary optimization problems,https://api.elsevier.com/content/abstract/scopus_id/85068529900,"Artificial intelligence techniques bring about new opportunities in problem solving. The notion such techniques have in common is learning mechanisms that are mostly problem and environment dependent. Although optimality is not guaranteed by these techniques, they draw attention due to being able to solve challenging optimization problems efficiently. Accordingly, the present study introduces a swarm-based optimization algorithm that is comprised of artificial search agents each with individual cognitive intelligence. In this technique, each agent is allowed to learn from problem space individually. Therefore, each of the search agents exhibits a different search characteristic. Nevertheless, they occasionally share information of the promising regions with each other. Thus, central swarm intelligence is also allowed to lead those independent search agents. Moreover, information-sharing techniques in the developed algorithm are designed as adaptive procedures so that search agents learn throughout generations by avoiding premature convergence and local optima problems as much as possible. The performance of the proposed algorithm is tested on a set of binary optimization problems including the set-union knapsack problem and the uncapacitated facility location problem, which have numerous real-life applications. All reported benchmarking problems are solved by the developed algorithm. As demonstrated by the comprehensive computational study and statistical tests, the proposed swarm-based algorithm significantly improves most of the published results.",space
10.1016/j.jngse.2019.102933,Journal,Journal of Natural Gas Science and Engineering,scopus,2019-09-01,sciencedirect,Machine learning for surveillance of fluid leakage from reservoir using only injection rates and bottomhole pressures,https://api.elsevier.com/content/abstract/scopus_id/85068973220,"Carbon-neutral economies would require preventing the release of industrial-scale CO2 into the atmosphere by injecting into geologic formations. Large-scale injection of CO2 into deep reservoirs carries a potential for its undesired leakage into above zones, which can act as an obstacle to its large-scale implementation. Current methods for surveillance of CO2 leaks are costly and not very robust, especially the methods that simulate expected pressure behavior based on an assumed reservoir model.
                  This study proposes a machine learning method for surveillance of fluid leakage using deconvolution response function (a non-linear function of time varying bottomhole pressure and injection rates) from injection and monitoring wells as a measure of leakage that is simulated via multivariate linear regression of all the wells present in the reservoir. Leakage is detected by comparing “expected” (baseline without leaks) deconvolution response of all monitoring wells with their “observed” deconvolution response. Three key advantages of the proposed method are that it i) uses only injection rates and bottomhole pressure data (with no reservoir or geological model), ii) is independent of physical process parameterization uncertainties, and iii) applicable to both conventional and unconventional (e.g. fractured tight formations) reservoirs with any fluid (e.g. compressible, incompressible). The proposed method is first trained to learn well history with no leakage, followed by its validation after which it can be used to detect leakage by tracking a meaningful deviation error (at least twenty times the error of no leakage base scenario over same time period) between expected well response and observed well response at all monitoring wells. The well history required for the proposed method comes directly from measurements made at wells in a real field, but in absence of field data the proposed method is illustrated through well history simulated by reservoir simulations; no such numerical simulations are required for application of this method in a real world scenario with well measurements.",space
10.1016/j.patrec.2018.02.013,Journal,Pattern Recognition Letters,scopus,2019-09-01,sciencedirect,Biometric surveillance using visual question answering,https://api.elsevier.com/content/abstract/scopus_id/85042474147,"Surveillance of individuals using visual data requires human-level capabilities for understanding the characteristics that differentiate one person from another. However, because the influx of both video and imagery is increasing at a greater rate than humans can cope with, biometric-based surveillance systems are required to assist with the triage of information based on human-generated queries. Unfortunately, current systems are not robust enough to tackle new tasks, as they involve specialized models that do not leverage existing, pre-trained components. To mitigate these issues, we propose a novel system for biometric-based surveillance that utilizes models that are relevance-aware to triage images and videos based on interaction with single or multiple users. As the system is initially focused on detection of people via their appearance and clothing, we have named the system Context and Collaborative (C2) Visual Question Answering (VQA) for Biometric Object-Attribute Relevance and Surveillance (C2VQA-BOARS). To validate the usefulness of C2VQA-BOARS in real-world scenarios, we provide an implementation of two novel components (Relevance and Triage) and apply them in tasks against two datasets created for biometric surveillance. Our results outperform baseline approaches, proving that a system with a minimal amount of fine-tuned components can robustly handle new datasets and problems as needed.",space
10.1016/j.neucom.2018.06.095,Journal,Neurocomputing,scopus,2019-08-18,sciencedirect,Speeding up k-Nearest Neighbors classifier for large-scale multi-label learning on GPUs,https://api.elsevier.com/content/abstract/scopus_id/85065140025,"Multi-label classification is one of the most dynamically growing fields of machine learning, due to its numerous real-life applications in solving problems that can be described by multiple labels at the same time. While most of works in this field focus on proposing novel and accurate classification algorithms, the issue of the computational complexity on growing dataset sizes is somehow marginalized. Owning to the ever-increasing capabilities of data capturing, we are faced with the problem of large-scale data mining that forces learners to be not only highly accurate, but also fast and scalable on high-dimensional spaces of instances, features, and labels. In this paper, we propose a highly efficient parallel approach for computing the multi-label k-Nearest Neighbor classifier on GPUs. While this method is highly effective due to its accuracy and simplicity, its computational complexity makes it prohibitive for large-scale data. We propose a four-step implementation that takes an advantage of the GPU architecture, allowing for an efficient execution of the multi-label k-Nearest Neighbors classifier without any loss of accuracy. Experiments carried out on a number of real and artificial benchmarks show that we are able to achieve speedups up to 200 times when compared to a sequential CPU execution, while efficiently scaling up to varying number of instances and features.",space
10.1016/j.ufug.2019.126365,Journal,Urban Forestry and Urban Greening,scopus,2019-07-01,sciencedirect,Exploring the effect of urban features and immediate environment on body responses,https://api.elsevier.com/content/abstract/scopus_id/85067818650,"This study investigates the relationship between urban features (sky exposure, green spaces, visual complexity, and built-up area), immediate environmental factors (air temperature, relative humidity, Heat Stress Index, Wet Bulb Globe Temperature, wind speed, and noise), personal characteristics (perceived restorativeness) and body reactions (body skin temperature and skin conductance responses). The proposed framework is based on multi-sensor data fusion from wearable physiological sensors, wireless environmental sensors, smartphones, images, geographic information systems datasets, and questionnaires. An experimental setup in a real-world setting is conducted and machine learning algorithms for regression problems and feature selection for variable importance are implemented. The results suggest a significant association between immediate environmental factors and body reactions; however, urban features are found to be weak explanatory variables. A deeper analysis of the identified stress hotspots revealed that locations with more dense green spaces, greater sky exposure, and smaller built-up area tended to report lower levels of stress reaction.",space
10.1016/j.media.2019.05.001,Journal,Medical Image Analysis,scopus,2019-07-01,sciencedirect,Denoising of 3D magnetic resonance images using a residual encoder–decoder Wasserstein generative adversarial network,https://api.elsevier.com/content/abstract/scopus_id/85065426790,"Structure-preserved denoising of 3D magnetic resonance imaging (MRI) images is a critical step in medical image analysis. Over the past few years, many algorithms with impressive performances have been proposed. In this paper, inspired by the idea of deep learning, we introduce an MRI denoising method based on the residual encoder–decoder Wasserstein generative adversarial network (RED-WGAN). Specifically, to explore the structure similarity between neighboring slices, a 3D configuration is utilized as the basic processing unit. Residual autoencoders combined with deconvolution operations are introduced into the generator network. Furthermore, to alleviate the oversmoothing shortcoming of the traditional mean squared error (MSE) loss function, the perceptual similarity, which is implemented by calculating the distances in the feature space extracted by a pretrained VGG-19 network, is incorporated with the MSE and adversarial losses to form the new loss function. Extensive experiments are implemented to assess the performance of the proposed method. The experimental results show that the proposed RED-WGAN achieves performance superior to several state-of-the-art methods in both simulated and real clinical data. In particular, our method demonstrates powerful abilities in both noise suppression and structure preservation.",space
10.1016/j.scs.2019.101523,Journal,Sustainable Cities and Society,scopus,2019-07-01,sciencedirect,Cost efficient resource allocation for real-time tasks in embedded systems,https://api.elsevier.com/content/abstract/scopus_id/85065049277,"Various application classes are being deployed to the cloud these days making use of a pay-as-you-go policy. However, existing cloud technologies are still at an early stage of maturity for applications with real-time constraints. With the emergence of Internet of Things (IoT) deployments and embedded systems in smart infrastructure, requirements for off-loading computation to cloud are increasing. In real-time systems, the resource allocation problem is NP-hard, especially when these systems are deployed in cloud computing environments where task execution involves deadline constraints. As a solution, hybrid approaches provide the opportunities to investigate efficient resource allocation for task scheduling problems. We propose a hybridized form of cuckoo search and genetic algorithms known as HGCS (hybrid genetic and cuckoo search) by embedding genetic operators that optimize makespan and cost of real-time tasks scheduled on cloud virtual machines. The inclusion of genetic operators in the cuckoo search algorithm leads to a rigorous search of the solution space, finding the best feasible schedule that can execute tasks in the lowest time, which in turn reduces the total resources usage cost. The performance of the proposed algorithm is tested by using real-time tasks that need data files for successful completion. The HGCS algorithm is evaluated by comparing the results with genetic and cuckoo search algorithms individually. The experimental results favor HGCS over the other two counterparts in providing a schedule respecting the time constraints of the system with reduced makespan and execution cost.",space
10.1016/j.measurement.2019.03.032,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2019-07-01,sciencedirect,Experimental characterisation of eye-tracking sensors for adaptive human-machine systems,https://api.elsevier.com/content/abstract/scopus_id/85064266296,"Adaptive Human-Machine Interfaces and Interactions (HMI2) are closed-loop cyber-physical systems comprising a network of sensors measuring human, environmental and mission parameters, in conjunction with suitable software for adapting the HMI2 (command, control and display functions) in response to these real-time measurements. Cognitive HMI2 are a particular subclass of these systems, which support dynamic HMI2 adaptations based on the user’s cognitive state. These states are estimated in real-time using various neuro-physiological parameters from gaze, cardiorespiratory and brain signals, which are processed by an Adaptive Neuro-Fuzzy Inference System (ANFIS). However, the accuracy and precision of neuro-physiological measurements are affected by a variety of environmental factors and therefore need to be accurately characterised prior to operational use. This paper describes the characterisation activities performed on two types of eye tracking devices used in the Aerospace Intelligent and Autonomous Systems (AIAS) laboratory of RMIT University to support the development of cognitive human-machine systems. The uncertainty associated with the ANFIS outputs is quantified by propagating the uncertainties in the input data (determined experimentally) through the inference engine. This process is of growing relevance because similar machine learning techniques are now being developed for an increasing number of applications including aerospace, transport, biomedical and defence cyber-physical systems.",space
10.1016/j.cogsys.2019.01.003,Journal,Cognitive Systems Research,scopus,2019-06-01,sciencedirect,The CORTEX cognitive robotics architecture: Use cases,https://api.elsevier.com/content/abstract/scopus_id/85060622773,"CORTEX is a cognitive robotics architecture inspired by three key ideas: modularity, internal modelling and graph representations. CORTEX is also a computational framework designed to support early forms of intelligence in real world, human interacting robots, by selecting an a priori functional decomposition of the capabilities of the robot. This set of abilities was then translated to computational modules or agents, each one built as a network of software interconnected components. The nature of these agents can range from pure reactive modules connected to sensors and/or actuators, to pure deliberative ones, but they can only communicate with each other through a graph structure called Deep State Representation (DSR). DSR is a short-term dynamic representation of the space surrounding the robot, the objects and the humans in it, and the robot itself. All these entities are perceived and transformed into different levels of abstraction, ranging from geometric data to high-level symbolic relations such as “the person is talking and gazing at me”. The combination of symbolic and geometric information endows the architecture with the potential to simulate and anticipate the outcome of the actions executed by the robot. In this paper we present recent advances in the CORTEX architecture and several real-world human-robot interaction scenarios in which they have been tested. We describe our interpretation of the ideas inspiring the architecture and the reasons why this specific computational framework is a promising architecture for the social robots of tomorrow.",space
10.1016/j.datak.2019.05.003,Journal,Data and Knowledge Engineering,scopus,2019-05-01,sciencedirect,SubspaceDB: In-database subspace clustering for analytical query processing,https://api.elsevier.com/content/abstract/scopus_id/85066093010,"High dimensional data analysis within relational database management systems (RDBMS) is challenging because of inadequate support from SQL. Currently, subspace clustering of high dimensional data is implemented either outside DBMS using wrapper code or inside DBMS using SQL User Defined Functions/Aggregates(UDFs/UDAs). However, both these approaches have potential disadvantages from performance, resource usage, and security perspective for voluminous and frequently updated data. Hence, we propose an efficient querying system, named SubspaceDB, that implements subspace clustering directly within an RDBMS. SubspaceDB provides a novel set of query operators, each with an optimization objective, to facilitate interactive analysis for subspace clustering. The query operators focus on retrieving optimal answers to four key query types : (a) Medoid queries, (b) Neighbourhood queries, (c) Partial similarity queries, and (d) Prominence queries, that aid the formation of subspace clusters. Experimental studies on real and synthetic databases of size 
                        15
                        M
                      tuples and 104 attributes show that our proposed approach SubspaceDB can be over 10 times faster as compared to a conventional wrapper-based or SQL UDF approach. The proposed approach is also efficient in retrieving at least 50% data with performance improvement of at least 25%.",space
10.1016/j.enconman.2019.02.086,Journal,Energy Conversion and Management,scopus,2019-05-01,sciencedirect,"Multi-step short-term wind speed forecasting approach based on multi-scale dominant ingredient chaotic analysis, improved hybrid GWO-SCA optimization and ELM",https://api.elsevier.com/content/abstract/scopus_id/85063231962,"Accurate wind speed prediction possesses a significant impact on reasonable scheduling and safe operation of power system. For this purpose, a novel hybrid approach based on multi-scale dominant ingredient chaotic analysis, improved hybrid GWO-SCA (IHGWOSCA) algorithm and extreme learning machine (ELM) is proposed for multi-step short-term wind speed prediction, in which the multi-scale dominant ingredient chaotic analysis combines the proposed optimal variational mode decomposition (OVMD), singular spectrum analysis (SSA) and phase space reconstruction (PSR). To begin with, the mode number and updating step of VMD are pre-determined by center frequency observation method and the proposed least-squares error index (LSEI), thus decomposing the non-stationary wind speed series into a set of intrinsic mode functions (IMFs). Later, the extraction of the dominant ingredient and residuary ingredient for each sub-series is implemented by SSA for the construction of forecasting components. Subsequently, the proposed IHGWOSCA algorithm coded with discrete integers and real-valued are investigated to search optimal parameters in PSR and ELM successively. Lastly, the ultimate forecasting results of the original wind speed are calculated by accumulating results of all the predicted components. Furthermore, seven data sets from Sotavento Galicia and Inner Mongolia have been employed to evaluate the proposed approach. The results illustrate that: (1) the proposed OVMD-based models obtained better RMSE, MAE and MAPE indexes comparing with the benchmark models through weakening the non-stationary of the original signal; (2) the proposed dominant ingredient chaotic analysis combining SSA and PSR enhanced the multi-steps prediction performance effectively; (3) the proposed IHGWOSCA optimization algorithm possessed good capability for optimal parameters searching and fast convergence.",space
10.1016/j.engappai.2019.03.001,Journal,Engineering Applications of Artificial Intelligence,scopus,2019-05-01,sciencedirect,Affective analytics of demonstration sites,https://api.elsevier.com/content/abstract/scopus_id/85062991999,"Multiple-criteria decision-making (MCDM) typically assumes that crowds make completely rational decisions. In MCDM, a crowd as a whole, or its individual members, generally make decisions free from any influence of valence, arousal, emotional state or environment. In contrast, various theories dealing with crowd psychology (Gustave Le Bon, Freudian, Deindividuation, Convergence, Emergent norm, Social identity) analyze, in one form or another, the emotions of the crowd. According to above theories, crowd is influenced by a range of behavioral factors, such as physical, social, psychological, culture, norms, and emotions. It can be argued that the emotional state, valence and arousal of crowds affect their decision making to a considerable degree and multiple criteria crowd behavior modeling must, therefore, consider this impact as well. In this light, the integration of crowd simulation and biometric methods, behavioral operations research and emotions in decision making has taken a prominent place as it leads to a better understanding of crowd emotions and crowd decision making. In this context, the authors developed the Affective Analytics of Demonstration Sites (ANDES) that added to this body of research in four ways. The crowd analysis and simulations conducted with ANDES used a neuro decision matrix. The matrix contains a detailed description of demonstration sites (public spaces) in question and the emotions, valence, arousal and physiological parameters of people present there. With ANDES’s Remote Sensor Network, emotional (emotions, valence, arousal) and physiological (average crowd facial temperature, crowd composition by gender and age group, etc.) parameters of people present at demonstration sites can be mapped. ANDES can assist experts in more effective implementations of public spaces planning and a participation process by attendees by collecting and examining various layers of data on the emotional and physiological parameters of visitors based on a visitors-centric public spaces planning approach. ANDES can determine the public space and real estate values.",space
10.1016/j.robot.2018.11.017,Journal,Robotics and Autonomous Systems,scopus,2019-05-01,sciencedirect,A real-time framework for kinodynamic planning in dynamic environments with application to quadrotor obstacle avoidance,https://api.elsevier.com/content/abstract/scopus_id/85062619374,"The objective of this paper is to present a full-stack, real-time motion planning framework for kinodynamic robots and then show how it is applied and demonstrated on a physical quadrotor system operating in a laboratory environment. The proposed framework utilizes an offline–online computation paradigm, neighborhood classification through machine learning, sampling-based motion planning with an optimal cost distance metric, and trajectory smoothing to achieve real-time planning for aerial vehicles. This framework accounts for dynamic obstacles with an event-based replanning structure and a locally reactive control layer that minimizes replanning events. The approach is demonstrated on a quadrotor navigating moving obstacles in an indoor space and stands as, arguably, one of the first demonstrations of full-online kinodynamic motion planning, with execution cycles of 3 Hz to 5 Hz. For the quadrotor, a simplified dynamics model is used during the planning phase to accelerate online computation. A trajectory smoothing phase, which leverages the differentially flat nature of quadrotor dynamics, is then implemented to guarantee a dynamically feasible trajectory.",space
10.1016/j.cels.2019.03.003,Journal,Cell Systems,scopus,2019-04-24,sciencedirect,DoubletFinder: Doublet Detection in Single-Cell RNA Sequencing Data Using Artificial Nearest Neighbors,https://api.elsevier.com/content/abstract/scopus_id/85064396876,"Single-cell RNA sequencing (scRNA-seq) data are commonly affected by technical artifacts known as “doublets,” which limit cell throughput and lead to spurious biological conclusions. Here, we present a computational doublet detection tool—DoubletFinder—that identifies doublets using only gene expression data. DoubletFinder predicts doublets according to each real cell’s proximity in gene expression space to artificial doublets created by averaging the transcriptional profile of randomly chosen cell pairs. We first use scRNA-seq datasets where the identity of doublets is known to show that DoubletFinder identifies doublets formed from transcriptionally distinct cells. When these doublets are removed, the identification of differentially expressed genes is enhanced. Second, we provide a method for estimating DoubletFinder input parameters, allowing its application across scRNA-seq datasets with diverse distributions of cell types. Lastly, we present “best practices” for DoubletFinder applications and illustrate that DoubletFinder is insensitive to an experimentally validated kidney cell type with “hybrid” expression features.",space
10.1016/j.mfglet.2019.05.003,Journal,Manufacturing Letters,scopus,2019-04-01,sciencedirect,A blockchain enabled Cyber-Physical System architecture for Industry 4.0 manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/85066168835,"Cyber-Physical Production Systems (CPPSs) are complex manufacturing systems which aim to integrate and synchronize machine world and manufacturing facility to the cyber computational space. However, having intensive interconnectivity and a computational platform is crucial for real-world implementation of CPPSs. In this paper, the potential impacts of blockchain technology in development and realization of real-world CPPSs are discussed. A unified three-level blockchain architecture is proposed as a guideline for researchers and industries to clearly identify the potentials of blockchain and adapt, develop, and incorporate this technology with their manufacturing developments towards Industry 4.0.",space
10.1016/j.neucom.2019.01.019,Journal,Neurocomputing,scopus,2019-03-28,sciencedirect,Multi-view transfer learning with privileged learning framework,https://api.elsevier.com/content/abstract/scopus_id/85060576231,"In this paper, we present a multi-view transfer learning model named Multi-view Transfer Discriminative Model (MTDM) for both image and text classification tasks. Transfer learning, which aims to learn a robust classifier for the target domain using data from a different distribution, has been proved to be effective in many real-world applications. However, most of the existing transfer learning methods map across domain data into a high-dimension space which the distance between domains is closed. This strategy always fails in the multi-view scenario. On the contrary, the multi-view learning methods are also difficult to extend in the transfer learning settings. One of our goals in this paper is to develop a model which can perform better in both multi-view and transfer learning settings. On the one hand, the problem of multi-view is implemented by the paradigm of learning using privileged information (LUPI), which could guarantee the principle of complementary and consensus. On the other hand, the model adequately utilizes the source domain data to build a robust classifier for the target domain. We evaluate our model on both image and text classification tasks and show the effectiveness compared with other baseline approaches.",space
10.1016/j.dss.2019.01.003,Journal,Decision Support Systems,scopus,2019-03-01,sciencedirect,Deep learning based personalized recommendation with multi-view information integration,https://api.elsevier.com/content/abstract/scopus_id/85060338648,"With the rapid proliferation of images on e-commerce platforms today, embracing and integrating versatile information sources have become increasingly important in recommender systems. Owing to the heterogeneity in information sources and consumers, it is necessary and meaningful to consider the potential synergy between visual and textual content as well as consumers' different cognitive styles. This paper proposes a multi-view model, namely, Deep Multi-view Information iNtEgration (Deep-MINE), to take multiple sources of content (i.e., product images, descriptions and review texts) into account and design an end-to-end recommendation model. In doing so, stacked auto-encoder networks are deployed to map multi-view information into a unified latent space, a cognition layer is added to depict consumers' heterogeneous cognition styles and an integration module is introduced to reflect the interaction of multi-view latent representations. Extensive experiments on real world data demonstrate that Deep-MINE achieves high accuracy in product ranking, especially in the cold-start case. In addition, Deep-MINE is able to boost overall model performance compared with models taking a single view, further verifying the proposed model's effectiveness on information integration.",space
10.1016/j.solener.2019.01.027,Journal,Solar Energy,scopus,2019-03-01,sciencedirect,ANN based automatic slat angle control of venetian blind for minimized total load in an office building,https://api.elsevier.com/content/abstract/scopus_id/85059868380,"Windows are the only part of a building that can directly penetrate the solar radiation into the occupied space and thus the shading devices are needed to control the solar penetration. A variety of research have been conducted to develop the optimized slat angle control in the existing literature, but the research incorporating artificial intelligence technique with slat angle control is limited thus far. Therefore, in this study, the ANN (Artificial Neural Network) model was applied to minimize the combined total load consisting of lighting, cooling, and heating loads through automatic slat angle control of venetian blinds. A three-story rectangular office building was simulated using EnergyPlus, and dimming control was applied to control the lighting. The interlocked simulation between Matlab and EnergyPlus was conducted through BCVTB. As a result of comparing automatic blind control via the ANN to fixed blind slat angle, the automatic blind control via the ANN showed 9.1% lower total load than the blind angle fixed at 50°. It was confirmed that the cooling and heating load could be significantly reduced by real-time automatic control via the ANN under various operating conditions, rather than fixing the blinds at one angle.",space
10.1016/j.ress.2018.07.024,Journal,Reliability Engineering and System Safety,scopus,2019-03-01,sciencedirect,A new approach for estimating the parameters of Weibull distribution via particle swarm optimization: An application to the strengths of glass fibre data,https://api.elsevier.com/content/abstract/scopus_id/85056745362,"Three-parameter Weibull is one of the most popular and most widely-used distribution in many fields of science. Therefore, many studies have been conducted concerning the statistical inferences of the parameters of Weibull distribution. In general, the maximum likelihood (ML) methodology is used in the estimation process of unknown parameters. In this study, the ML estimation of the parameters of Weibull distribution is considered using particle swarm optimization (PSO). As in other heuristic optimization methods, the performance of PSO is affected by initial conditions. The novelty of this study comes from the fact that we propose a new adaptive search space based on confidence intervals in PSO. The modified maximum likelihood (MML) estimators are utilized for constructing the confidence intervals. MML based confidence intervals allow a narrower search space for the parameters of Weibull distribution than the search space used in the literature. Therefore, the performance of PSO increases, since the search space is wisely narrowed. In order to show the performance of the proposed approach, an extensive Monte-Carlo simulation study is conducted. Simulation results show that the proposed approach works well. In addition, real world data is analyzed to show implementation of the proposed method.",space
10.1016/j.jhydrol.2018.11.069,Journal,Journal of Hydrology,scopus,2019-02-01,sciencedirect,"An enhanced extreme learning machine model for river flow forecasting: State-of-the-art, practical applications in water resource engineering area and future research direction",https://api.elsevier.com/content/abstract/scopus_id/85059162436,"Despite the massive diversity in the modeling requirements for practical hydrological applications, there remains a need to develop more reliable and intelligent expert systems used for real-time prediction purposes. The challenge in meeting the standards of an expert system is primarily due to the influence and behavior of hydrological processes that is driven by natural fluctuations over the physical scale, and the resulting variance in the underlying model input datasets. River flow forecasting is an imperative task for water resources operation and management, water demand assessments, irrigation and agriculture, early flood warning and hydropower generations. This paper aims to investigate the viability of the enhanced version of extreme learning machine (EELM) model in river flow forecasting applied in a tropical environment. Herein, we apply the complete orthogonal decomposition (COD) learning tool to tune the output-hidden layer of the ELM model’s internal neuronal system, instead of the conventional multi-resolution tool (e.g., singular value decomposition). To demonstrate the application of EELM model, the Kelantan River, located in the Malaysian peninsular, selected as a case study. For a comparison of the EELM model, and further model evaluation, two distinct data-intelligent models are developed (i.e., the classical ELM and the support vector regression, SVR model). An exhaustive list of diagnostic indicators are used to evaluate the EELM model in respect to the benchmark algorithms, namely, SVR and ELM. The model performance indicators exhibit superior results for the EELM model relative to ELM and SVR models. In addition, the EELM model is presented as a more accurate, alternative predictive tool for modelling the tropical river flow patterns and its underlying characteristic perturbations in the physical space. Several statistical metrics defined as the coefficient of determination (r), Nash-Sutcliffe efficiency (Ens
                     ), Willmott’s Index (WI), root-mean-square error (RMSE) and mean absolute error (MAE) are computed to assess the model’s effectiveness. In quantitative terms, superiority of EELM over ELM and SVR models was exhibited by Ens
                      = 0.7995, 0.7434 and 0.665, r = 0.894, 0.869 and 0.818 and WI = 0.9380, 0.9180 and 0.8921, respectively. Whereas, EELM model attained lower (RMSE and MAE) values by approximately (11.61–22.53%) and (8.26–8.72%) relative to ELM and SVR models, respectively. The obtained results reveal that the EELM model is a robust expert model and can be embraced practically in real-life water resources management and river sustainability decisions. As a complementary component of this paper, we also review state-of-art research works where scholars have embraced extensive implementation of the ELM model in water resource engineering problems. A comprehensive evaluation is carried out to recognize the current limitations, and also to propose potential opportunities of applying improved variants of the ELM model presented as a future research direction.",space
10.1016/j.gaitpost.2018.11.029,Journal,Gait and Posture,scopus,2019-02-01,sciencedirect,"Three-dimensional cameras and skeleton pose tracking for physical function assessment: A review of uses, validity, current developments and Kinect alternatives",https://api.elsevier.com/content/abstract/scopus_id/85057183966,"Background
                  Three-dimensional camera systems that integrate depth assessment with traditional two-dimensional images, such as the Microsoft Kinect, Intel Realsense, StereoLabs Zed and Orbecc, hold great promise as physical function assessment tools. When combined with point cloud and skeleton pose tracking software they can be used to assess many different aspects of physical function and anatomy. These assessments have received great interest over the past decade, and will likely receive further study as the integration of depth sensing and augmented reality smartphone cameras occurs more in everyday life.
               
                  Research Question
                  The aim of this review is to discuss how these devices work, what options are available, the best methods for performing assessments and how they can be used in the future.
               
                  Methods
                  Firstly, a review of the Microsoft Kinect devices and associated artificial intelligence, automated skeleton tracking algorithms is provided. This includes a narrative critique of the validity and clinical utility of these devices for assessing different aspects of physical function including spatiotemporal, kinematic and inverse dynamics data derived from gait and balance trials, and anatomical assessments performed using the depth sensor information. Methods for improving the accuracy of data are examined, including multiple-camera systems and sensor fusion with inertial monitoring units, model fitting, and marker tracking. Secondly, alternative hardware, including other structured light and time of flight methods, stereoscopic cameras and augmented reality leveraging smartphone and tablet cameras to perform measurements in three-dimensional space are summarised. Software options related to depth sensing cameras are then discussed, focussing on recent advances such as OpenPose and web-based methods such as PoseNet.
               
                  Results and Significance
                  The clinical and non-laboratory utility of these devices holds great promise for physical function assessment, and recent developments could strengthen their ability to provide important and impactful health-related data.",space
10.1016/j.robot.2018.10.015,Journal,Robotics and Autonomous Systems,scopus,2019-02-01,sciencedirect,Time-dependent genetic algorithm and its application to quadruped's locomotion,https://api.elsevier.com/content/abstract/scopus_id/85056925953,"Genetic algorithms (GAs) are widely used in machine learning and optimization. This paper proposes a time-dependent genetic algorithm (TDGA) based on real-coded genetic algorithm (RCGA) to improve the convergence performance of functions over time such as a foot trajectory. TDGA has several distinguishing features when compared with traditional RCGA. First, individuals are arranged over time, and then the individuals are optimized in sequence. Second, search spaces of design variables are newly comprised of processes of reductions for search spaces. Third, the search space for crossover operations is expanded to avoid local minima traps that can occur in new search spaces up to the previous search space before performing any reduction of search space, and boundary mutation operation is performed to the new search spaces. Computer simulations are implemented to verify the convergence performance of the robot locomotion optimized by TDGA. Then, TDGA optimizes the desired feet trajectories of quadruped robots that climb up a slope and the impedance parameters of admittance control so that quadruped robots can trot stably over irregular terrains. Simulation results clearly represent that the convergence performance is improved by TDGA, which also shows that TDGA could be broadly used in robot locomotion research.",space
10.1016/j.comnet.2018.11.013,Journal,Computer Networks,scopus,2019-01-15,sciencedirect,Towards automatic fingerprinting of IoT devices in the cyberspace,https://api.elsevier.com/content/abstract/scopus_id/85056904979,"Nowadays, the cyberspace consists of an increasing number of IoT devices, such as net-printers, webcams, and routers. Illuminating the nature of online devices would provide insights into detecting potentially vulnerable devices on the Internet. However, there is a lack of device discovery in large-scale due to the massive number of device models (i.e., types, vendors, and products). In this paper, we propose an efficient approach to generate fingerprints of IoT devices. We observe that device manufacturers have different network system implementations on their products. We explore features spaces of IoT devices in three network layers, including the network-layer, transport-layer, and application-layer. Utilizing the feature of network protocols, we generate IoT device fingerprints based on neural network algorithms. Furthermore, we implement the prototype system and conduct real experiments to validate the performance of device fingerprints. Results show that our classification can generate device class labels with a 94% precision and 95% recall. We use those device fingerprints to discover 15.3 million network-connected devices and analyze their distribution characteristics in cyberspace.",space
10.1016/j.procs.2019.09.069,Conference Proceeding,Procedia Computer Science,scopus,2019-01-01,sciencedirect,An Innovative Technology: Augmented Reality Based Information Systems,https://api.elsevier.com/content/abstract/scopus_id/85076255225,"In our generation the information systems evolve with new technologies: augmented reality (AR), IoT, artificial intelligence, blockchain etc. Anymore they perform information exchange by sensors. It is estimated that the systems will be in a state of extreme interaction and reach 50 billion devices connected in Internet in 2020. We know that everything around us will be in interaction and they will do everything without any need of human interference. For example, when our dishwasher is full, it will start to wash automatically, or when the run out of the gasoline, our car will drive to the nearest station, or even when a burglar is entered to our house, it will automatically be detected and be announced to the police office. In business life, the processes will be automatical in maximum level and this technology will increase productivity and efficiency. Next to mobile technology, it is thought that these new generation information systems (IS) will take the biggest place in our lives. AR also will be integrated to these systems to augment the information in real world. Humanity will augment its habitat in an innovative way thanks to these AR based IS. This paper surveys the current state-of-the-art AR systems related with aerospace & defense, industry, education, medical and gaming sectors. The connection of AR based IS and innovation is explained with a technological insight. In addition to international use cases HAVELSAN’s use cases are also given that are performed from the aspect of applied open innovation strategy. This strategy is addressed specific to the implemented activities of AR based IS.",space
10.1016/j.asoc.2018.10.010,Journal,Applied Soft Computing Journal,scopus,2019-01-01,sciencedirect,A hybridization of extended Kalman filter and Ant Colony Optimization for state estimation of nonlinear systems,https://api.elsevier.com/content/abstract/scopus_id/85056208326,"In this paper, a new nonlinear heuristic filter based on the hybridization of an extended Kalman filter and an ant colony estimator is proposed to estimate the states of a nonlinear system. In this filter, a group of virtual ants searches the state space stochastically and dynamically to find and track the best state estimation while the position of each ant is updated at the measurement time using the extended Kalman filter. The performance of the proposed filter is compared with well-known heuristic filters using a nonlinear benchmark problem. The statistical results show that this algorithm is able to provide promising and competitive results. Then, the new filter is tested on a nonlinear engineering problem with more than one state. The problem is to estimate simultaneously the states of an unmanned aerial vehicle as well as the wind disturbances, applied to the system. In this case, a processor-in-the-loop experiment is also performed to verify the implementation capability of the proposed approach. This paper also investigates the real-time implementation capability of the proposed filter in the attitude estimation of a three degrees of freedom experimental setup of a quadrotor to further investigate its effectiveness in practice.",space
10.1016/j.euromechsol.2018.10.011,Journal,"European Journal of Mechanics, A/Solids",scopus,2019-01-01,sciencedirect,A dual interpolation boundary face method for elasticity problems,https://api.elsevier.com/content/abstract/scopus_id/85055917199,"A dual interpolation boundary face method (DiBFM) is proposed to unify the conforming and nonconforming elements in boundary element method (BEM) implementation. In the DiBFM, the nodes of a conventional conforming element are sorted into two groups: the nodes on the boundary (called virtual nodes) and the internal nodes (called source nodes). Without virtual nodes, the conforming element turns to be a conventional nonconforming element of a lower order. Physical variables are interpolated using the conforming elements, the same way as conforming BEM. Boundary integral equations are collocated at source nodes, the same way as nonconforming BEM. To make the final system of linear equations solvable, additional constraint equations are required to condense the degrees of freedom for all the virtual nodes. These constraints are constructed using the moving least-squares (MLS). Besides, both boundary integration and MLS are performed in the parametric spaces of curves, namely, the geometric data, such as coordinates, out normals and Jacobians, are calculated directly from curves rather than from elements. Thus, no geometric errors are introduced no matter how coarse the discretization is. The method has been implemented successfully for solving two-dimensional elasticity problems. A number of numerical examples with real engineering background have demonstrated the accuracy and efficiency of the new method.",space
10.1016/j.infsof.2018.08.003,Journal,Information and Software Technology,scopus,2019-01-01,sciencedirect,An extensible framework for software configuration optimization on heterogeneous computing systems: Time and energy case study,https://api.elsevier.com/content/abstract/scopus_id/85051630181,"Context: Application of component based software engineering methods to heterogeneous computing (HC) enables different software configurations to realize the same function with different non–functional properties (NFP). Finding the best software configuration with respect to multiple NFPs is a non–trivial task.
                  
                     Objective: We propose a Software Component Allocation Framework (SCAF) with the goal to acquire a (sub–) optimal software configuration with respect to multiple NFPs, thus providing performance prediction of a software configuration in its early design phase. We focus on the software configuration optimization for the average energy consumption and average execution time.
                  
                     Method: We validated SCAF through its instantiation on a real–world demonstrator and a simulation. Firstly, we verified the correctness of our model through comparing the performance prediction of six software configurations to the actual performance, obtained through extensive measurements with a confidence interval of 95%. Secondly, to demonstrate how SCAF scales up, we performed software configuration optimization on 55 generated use–cases (with solution spaces ranging from 1030 to 3070) and benchmark the results against best performing random configurations.
                  
                     Results: The performance of a configuration as predicted by our framework matched the configuration implemented and measured on a real–world platform. Furthermore, by applying the genetic algorithm and simulated annealing to the weight function given in SCAF, we obtain sub–optimal software configurations differing in performance at most 7% and 13% from the optimal configuration (respectfully).
                  
                     Conclusion: SCAF is capable of correctly describing a HC platform and reliably predict the performance of software configuration in the early design phase. Automated in the form of an Eclipse plugin, SCAF allows software architects to model architectural constraints and preferences, acting as a multi–criterion software architecture decision support system. In addition to said, we also point out several interesting research directions, to further investigate and improve our approach.",space
10.1016/j.jnca.2018.09.023,Journal,Journal of Network and Computer Applications,scopus,2018-12-15,sciencedirect,Dynamic workload patterns prediction for proactive auto-scaling of web applications,https://api.elsevier.com/content/abstract/scopus_id/85054442625,"Proactive auto-scaling methods dynamically manage the resources for an application according to the current and future load predictions to preserve the desired performance at a reduced cost. However, auto-scaling web applications remain challenging mainly due to dynamic workload intensity and characteristics which are difficult to predict. Most existing methods mainly predict the request arrival rate which only partially captures the workload characteristics and the changing system dynamics that influence the resource needs. This may lead to inappropriate resource provisioning decisions. In this paper, we address these challenges by proposing a framework for prediction of dynamic workload patterns as follows. First, we use an unsupervised learning method to analyze the web application access logs to discover URI (Uniform Resource Identifier) space partitions based on the response time and the document size features. Then for each application URI, we compute its distribution across these partitions based on historical access logs to accurately capture the workload characteristics compared to just representing the workload using the request arrival rate. These URI distributions are then used to compute the Probabilistic Workload Pattern (PWP), which is a probability vector describing the overall distribution of incoming requests across URI partitions. Finally, the identified workload patterns for a specific number of last time intervals are used to predict the workload pattern of the next interval. The latter is used for future resource demand prediction and proactive auto-scaling to dynamically control the provisioning of resources. The framework is implemented and experimentally evaluated using historical access logs of three real web applications, each with increasing, decreasing, periodic, and randomly varying arrival rate behaviors. Results show that the proposed solution yields significantly more accurate predictions of workload patterns and resource demands of web applications compared to existing approaches.",space
10.1016/j.neuroimage.2018.08.031,Journal,NeuroImage,scopus,2018-12-01,sciencedirect,Phase shift invariant imaging of coherent sources (PSIICOS) from MEG data,https://api.elsevier.com/content/abstract/scopus_id/85054308167,"Increasing evidence suggests that neuronal communication is a defining property of functionally specialized brain networks and that it is implemented through synchronization between population activities of distinct brain areas. The detection of long-range coupling in electroencephalography (EEG) and magnetoencephalography (MEG) data using conventional metrics (such as coherence or phase-locking value) is by definition contaminated by spatial leakage. Methods such as imaginary coherence, phase-lag index or orthogonalized amplitude correlations tackle spatial leakage by ignoring zero-phase interactions. Although useful, these metrics will by construction lead to false negatives in cases where true zero-phase coupling exists in the data and will underestimate interactions with phase lags in the vicinity of zero. Yet, empirically observed neuronal synchrony in invasive recordings indicates that it is not uncommon to find zero or close-to-zero phase lag between the activity profiles of coupled neuronal assemblies.
                  Here, we introduce a novel method that allows us to mitigate the undesired spatial leakage effects and detect zero and near zero phase interactions. To this end, we propose a projection operation that operates on sensor-space cross-spectrum and suppresses the spatial leakage contribution but retains the true zero-phase interaction component. We then solve the network estimation task as a source estimation problem defined in the product space of interacting source topographies. We show how this framework provides reliable interaction detection for all phase-lag values and we thus refer to the method as Phase Shift Invariant Imaging of Coherent Sources (PSIICOS).
                  Realistic simulations demonstrate that PSIICOS has better detector characteristics than existing interaction metrics. Finally, we illustrate the performance of PSIICOS by applying it to real MEG dataset recorded during a standard mental rotation task. Taken together, using analytical derivations, data simulations and real brain data, this study presents a novel source-space MEG/EEG connectivity method that overcomes previous limitations and for the first time allows for the estimation of true zero-phase coupling via non-invasive electrophysiological recordings.",space
10.1016/j.neucom.2018.06.045,Journal,Neurocomputing,scopus,2018-11-03,sciencedirect,Adaptive Neighborhood MinMax Projections,https://api.elsevier.com/content/abstract/scopus_id/85049433041,"Dimensionality reduction as one of most attractive topics in machine learning research area has aroused extensive attentions in recent years. In order to preserve the local structure of data, most of dimensionality reduction methods consider constructing the relationships among each sample and its k nearest neighbors, and they find the neighbors in original space by using Euclidean distance. Since the data in original space contain some noises and redundant features, finding the neighbors in original space is incorrect and may degrade the subsequent performance. Therefore, how to find the optimal k nearest neighbors for each sample is the key point to improve the robustness of model. In this paper, we propose a novel dimensionality reduction method, named Adaptive Neighborhood MinMax Projections (ANMMP) which finds the neighbors in the optimal subspace by solving Trace Ratio problem in which the noises and redundant features have been removed already. Meanwhile, the samples within same class are pulled together while the samples between different classes are pushed far away in such learned subspace. Besides, proposed model is a general approach which can be implemented easily and applied on other methods to improve the robustness. Extensive experiments conducted on several synthetic data and real-world data sets and achieve some encouraging performance with comparison to metric learning and feature extraction methods, which demonstrates the efficiency of our method.",space
10.1016/j.ijepes.2018.03.031,Journal,International Journal of Electrical Power and Energy Systems,scopus,2018-10-01,sciencedirect,Clustering-based novelty detection for identification of non-technical losses,https://api.elsevier.com/content/abstract/scopus_id/85044607485,"The reduction of non-technical losses is a significant part of the total potential benefits resulting from implementations of the smart grid concept. This paper proposes a data-based method to detect sources of theft and other commercial losses. Prototypes of typical consumption behavior are extracted through clustering of data collected from smart meters. A distance-based novelty detection framework classifies new data samples as malign if their distance to the typical consumption prototypes is significant. The proposed method works on the space of four different indicators of irregular consumption, enabling the easy interpretation of results. A use case based on real data is presented to evaluate the method. The threat model considers sixteen different possible types of changes in consumption pattern that result from non-technical losses, including attacks and defects present since the first day of metering. The proposed clustering-based novelty detection method for identification of non-technical losses, using the Gustafson-Kessel fuzzy clustering algorithm, achieves a true positive rate of 63.6% and false positive rate of 24.3%, outperforming other state-of-the-art unsupervised learning methods.",space
10.1016/j.ins.2018.04.070,Journal,Information Sciences,scopus,2018-08-01,sciencedirect,Continuously maintaining approximate quantile summaries over large uncertain datasets,https://api.elsevier.com/content/abstract/scopus_id/85047087656,"Quantile summarization is a useful tool for management of massive datasets in the rapidly growing number of applications, and its importance is further enhanced with uncertainty in the data being explored. In this paper, we focus on the problem of computing approximate quantile summaries over large uncertain datasets. On the basis of GK [14] algorithm, we propose a novel online algorithm namely uGK. Using only little space, the proposed uGK algorithm maintains a small set of tuples, each of which contains a point value and the “count” of uncertain elements that are not larger than this value, and supports any quantile query within a given error. Experimental evaluation on both synthetic and real-life datasets illustrates the effectiveness of our uGK algorithm.",space
10.1016/j.apacoust.2018.03.012,Journal,Applied Acoustics,scopus,2018-08-01,sciencedirect,Chaotic fractal walk trainer for sonar data set classification using multi-layer perceptron neural network and its hardware implementation,https://api.elsevier.com/content/abstract/scopus_id/85044150028,"First, this study proposes the use of the newly developed Stochastic Fractal Search (SFS) algorithm for training MLP NNs to design the evolutionary classifier. Evolutionary classifiers, often experience problems of slow convergence speed, trapping in local minima, and non-real-time classification. This paper also use four chaotic maps to improve the performance of the SFS. This modified version of SFS has been called Chaotic Fractal Walk Trainer (CFWT). To assess the performance of the proposed classifiers, these networks will be evaluated using the two benchmark datasets and a high-dimensional practical sonar dataset. For endorsement, the results are compared to four popular meta-heuristics trainers. The results show that new classifiers suggest better performance than the other benchmark algorithms, in terms of entrapment in local minima, classification accuracy, and convergence speed. This paper also implements the designed classifier on the Filed Programmable Field Array (FPGA) substrate for testing the real-time processing ability of the proposed method. The results of the real application prove that the designed classifiers are applicable to high-dimension challenging problems with unknown search spaces.",space
10.1016/j.media.2018.04.004,Journal,Medical Image Analysis,scopus,2018-07-01,sciencedirect,VP-Nets: Efficient automatic localization of key brain structures in 3D fetal neurosonography,https://api.elsevier.com/content/abstract/scopus_id/85046367108,"Three-dimensional (3D) fetal neurosonography is used clinically to detect cerebral abnormalities and to assess growth in the developing brain. However, manual identification of key brain structures in 3D ultrasound images requires expertise to perform and even then is tedious. Inspired by how sonographers view and interact with volumes during real-time clinical scanning, we propose an efficient automatic method to simultaneously localize multiple brain structures in 3D fetal neurosonography. The proposed View-based Projection Networks (VP-Nets), uses three view-based Convolutional Neural Networks (CNNs), to simplify 3D localizations by directly predicting 2D projections of the key structures onto three anatomical views.
                  While designed for efficient use of data and GPU memory, the proposed VP-Nets allows for full-resolution 3D prediction. We investigated parameters that influence the performance of VP-Nets, e.g. depth and number of feature channels. Moreover, we demonstrate that the model can pinpoint the structure in 3D space by visualizing the trained VP-Nets, despite only 2D supervision being provided for a single stream during training. For comparison, we implemented two other baseline solutions based on Random Forest and 3D U-Nets. In the reported experiments, VP-Nets consistently outperformed other methods on localization. To test the importance of loss function, two identical models are trained with binary corss-entropy and dice coefficient loss respectively. Our best VP-Net model achieved prediction center deviation: 1.8 ± 1.4 mm, size difference: 1.9 ± 1.5 mm, and 3D Intersection Over Union (IOU): 63.2 ± 14.7% when compared to the ground truth. To make the whole pipeline intervention free, we also implement a skull-stripping tool using 3D CNN, which achieves high segmentation accuracy. As a result, the proposed processing pipeline takes a raw ultrasound brain image as input, and output a skull-stripped image with five detected key brain structures.",space
10.1016/j.eswa.2017.11.011,Journal,Expert Systems with Applications,scopus,2018-06-15,sciencedirect,Towards a common implementation of reinforcement learning for multiple robotic tasks,https://api.elsevier.com/content/abstract/scopus_id/85035079318,"Mobile robots are increasingly being employed for performing complex tasks in dynamic environments. Those tasks can be either explicitly programmed by an engineer or learned by means of some automatic learning method, which improves the adaptability of the robot and reduces the effort of setting it up. In this sense, reinforcement learning (RL) methods are recognized as a promising tool for a machine to learn autonomously how to do tasks that are specified in a relatively simple manner. However, the dependency between these methods and the particular task to learn is a well-known problem that has strongly restricted practical implementations in robotics so far. Breaking this barrier would have a significant impact on these and other intelligent systems; in particular, having a core method that requires little tuning effort for being applicable to diverse tasks would boost their autonomy in learning and self-adaptation capabilities. In this paper we present such a practical core implementation of RL, which enables the learning process for multiple robotic tasks with minimal per-task tuning or none. Based on value iteration methods, we introduce a novel approach for action selection, called Q-biased softmax regression (QBIASSR), that takes advantage of the structure of the state space by attending the physical variables involved (e.g., distances to obstacles, robot pose, etc.), thus experienced sets of states accelerate the decision-making process of unexplored or rarely-explored states. Intensive experiments with both real and simulated robots, carried out with the software framework also introduced here, show that our implementation is able to learn different robotic tasks without tuning the learning method. They also suggest that the combination of true online SARSA(λ) (TOSL) with QBIASSR can outperform the existing RL core algorithms in low-dimensional robotic tasks. All of these are promising results towards the possibility of learning much more complex tasks autonomously by a robotic agent.",space
10.1016/j.compind.2018.03.014,Journal,Computers in Industry,scopus,2018-06-01,sciencedirect,Real-time object detection in agricultural/remote environments using the multiple-expert colour feature extreme learning machine (MEC-ELM),https://api.elsevier.com/content/abstract/scopus_id/85044151304,"It is necessary for autonomous robotics in agriculture to provide real time feedback, but due to a diverse array of objects and lack of landscape uniformity this objective is inherently complex. The current study presents two implementations of the multiple-expert colour feature extreme learning machine (MEC-ELM). The MEC-ELM is a cascading algorithm that has been implemented along side a summed area table (SAT) for fast feature extraction and object classification, for a fully functioning object detection algorithm. The MEC-ELM is an implementation of the colour feature extreme learning machine (CF-ELM), which is an extreme learning machine (ELM) with a partially connected hidden layer; taking three colour bands as inputs. The colour implementation used with the SAT enable the MEC-ELM to find and classify objects quickly, with 84% precision and 91% recall in weed detection in the Y’UV colour space and in 0.5 s per frame. The colour implementation is however limited to low resolution images and for this reason a colour level co-occurrence matrix (CLCM) variant of the MEC-ELM is proposed. This variant uses the SAT to produce a CLCM and texture analyses, with texture values processed as an input to the MEC-ELM. This enabled the MEC-ELM to achieve 78–85% precision and 81–93% recall in cattle, weed and quad bike detection and in times between 1 and 2 s per frame. Both implementations were benchmarked on a standard i7 mobile processor. Thus the results presented in this paper demonstrated that the MEC-ELM with SAT grid and CLCM makes an ideal candidate for fast object detection in complex and/or agricultural landscapes.",space
10.1016/j.knosys.2018.03.005,Journal,Knowledge-Based Systems,scopus,2018-06-01,sciencedirect,Unsupervised geographically discriminative feature learning for landmark tagging,https://api.elsevier.com/content/abstract/scopus_id/85043325506,"Recently, a large number of geo-tagged landmark images have been uploaded through various social media services. Usually, these geo-tagged images are annotated by users with GPS and tags related to the landmarks where they are taken. Landmark tagging aims to automatically annotate an image with the tags to describe the landmark where the image is taken. It has been observed that the images and tags show strong correlation with the geographical locations. The widely used assumption by many existing tagging methods is that images are independently and identically distributed is not effective to capture the geographical correlation. In this paper, we study the novel problem of utilizing the geographical correlation among images and landmarks for better tagging landmark images. In particular, we propose an unsupervised feature learning approach to learn the geographically discriminative features across geographical locations, by integrating latent space learning and geographically structural analysis (LSGSA) into a joint model. A latent space learning model is proposed to effectively fuse the heterogeneous features of visual content and tags. Meanwhile, the geographical structure analysis and group sparsity are applied to learn the geographically discriminative features. Then, a geo-guided sparse reconstruction method is proposed to tag images by utilizing the discriminative information of features, in which the landmark-specific tags are boosted by a weighting method. Experiments on the real-world datasets demonstrate the superiority of our approach.",space
10.1016/j.measurement.2018.02.060,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2018-06-01,sciencedirect,Development of a novel machine vision procedure for rapid and non-contact measurement of soil moisture content,https://api.elsevier.com/content/abstract/scopus_id/85042874145,"Soil moisture measurement is one of the essential management components to decrease water consumption and prevent water stresses in plants. In this study, a fast and non-contact method using machine vision and artificial intelligence was developed so as to make operators capable of having an estimate of soil moisture by taking only one image. Three soil textures along with three levels of added organic matter were applied. Mean comparison and the subsequent stepwise multiple regression were applied to find superior features from different color spaces. ANFIS and stepwise multiple regression were used to predict the soil moisture. Results indicated that the general model could predict the soil moisture with mean absolute error of less than 1.1%. This value reached to 0.3% for some sub-models belonging to the texture–organic matter group. Application of the present method is highly recommended for soil moisture measurement because of simple implementation and potential for online measurements.",space
10.1016/j.patcog.2018.01.012,Journal,Pattern Recognition,scopus,2018-06-01,sciencedirect,Multi-view manifold learning with locality alignment,https://api.elsevier.com/content/abstract/scopus_id/85042389370,"Manifold learning aims to discover the low dimensional space where the input high dimensional data are embedded by preserving the geometric structure. Unfortunately, almost all the existing manifold learning methods were proposed under single view scenario, and they cannot be straightforwardly applied to multiple feature sets. Although concatenating multiple views into a single feature provides a plausible solution, it remains a question on how to better explore the independence and interdependence of different views while conducting manifold learning. In this paper, we propose a multi-view manifold learning with locality alignment (MVML-LA) framework to learn a common yet discriminative low-dimensional latent space that contain sufficient information of original inputs. Both supervised algorithm (S-MVML-LA) and unsupervised algorithm (U-MVML-LA) are developed. Experiments on benchmark real-world datasets demonstrate the superiority of our proposed S-MVML-LA and U-MVML-LA over existing state-of-the-art methods.",space
10.1016/j.apenergy.2018.02.156,Journal,Applied Energy,scopus,2018-05-15,sciencedirect,Approximate model predictive building control via machine learning,https://api.elsevier.com/content/abstract/scopus_id/85043463406,"Many studies have proven that the building sector can significantly benefit from replacing the current practice rule-based controllers (RBC) by more advanced control strategies like model predictive control (MPC). However, the optimization-based control algorithms, like MPC, impose increasing hardware and software requirements, together with more complicated error handling capabilities required from the commissioning staff. In recent years, several studies introduced promising remedy for these problems by using machine learning algorithms. The idea is based on devising simplified control laws learned from MPC. The main advantage of the proposed methods stems from their easy implementation even on low-level hardware. However, most of the reported studies were dealing only with problems with a limited complexity of the parametric space, and devising laws only for a single control variable, which inevitably limits their applicability to more complex building control problems. In this paper, we introduce a versatile framework for synthesis of simple, yet well-performing control strategies that mimic the behavior of optimization-based controllers, also for large scale multiple-input-multiple-output (MIMO) control problems which are common in the building sector. The approach employs multivariate regression and dimensionality reduction algorithms. Particularly, deep time delay neural networks (TDNN) and regression trees (RT) are used to derive the dependency of multiple real-valued control inputs on parameters. The complexity of the problem, as well as implementation cost, are further reduced by selecting the most significant features from the set of parameters. This reduction is based on straightforward manual selection, principal component analysis (PCA) and dynamic analysis of the building model. The approach is demonstrated on a case study employing temperature control in a six-zone building, described by a linear model with 286 states and 42 disturbances, resulting in an MPC problem with more than thousand of parameters. The results show that simplified control laws retain most of the performance of the complex MPC, while significantly decreasing the complexity and implementation cost.",space
10.1016/j.neunet.2018.01.014,Journal,Neural Networks,scopus,2018-04-01,sciencedirect,Accelerated low-rank representation for subspace clustering and semi-supervised classification on large-scale data,https://api.elsevier.com/content/abstract/scopus_id/85042324490,"The scalability of low-rank representation (LRR) to large-scale data is still a major research issue, because it is extremely time-consuming to solve singular value decomposition (SVD) in each optimization iteration especially for large matrices. Several methods were proposed to speed up LRR, but they are still computationally heavy, and the overall representation results were also found degenerated. In this paper, a novel method, called accelerated LRR (ALRR) is proposed for large-scale data. The proposed accelerated method integrates matrix factorization with nuclear-norm minimization to find a low-rank representation. In our proposed method, the large square matrix of representation coefficients is transformed into a significantly smaller square matrix, on which SVD can be efficiently implemented. The size of the transformed matrix is not related to the number of data points and the optimization of ALRR is linear with the number of data points. The proposed ALRR is convex, accurate, robust, and efficient for large-scale data. In this paper, ALRR is compared with state-of-the-art in subspace clustering and semi-supervised classification on real image datasets. The obtained results verify the effectiveness and superiority of the proposed ALRR method.",space
10.1016/j.ins.2016.08.009,Journal,Information Sciences,scopus,2018-04-01,sciencedirect,Decentralized Clustering by Finding Loose and Distributed Density Cores,https://api.elsevier.com/content/abstract/scopus_id/84981714778,"Centroid-based clustering approaches fail to recognize extremely complex patterns that are non-isotropic. We analyze the underlying causes and find some inherent flaws in these approaches, including Shape Loss, False Distances and False Peaks, which typically cause centroid-based approaches to fail when applied to complex patterns. As an alternative to current methods, we propose a hybrid decentralized approach named DCore, which is based on finding density cores instead of centroids, to overcome these flaws. The underlying idea is that we consider each cluster to have a shrunken density core that roughly retains the shape of the cluster. Each such core consists of a set of loosely connected local density peaks of higher density than their surroundings. Borders, edges and outliers are distributed around the outsides of these cores in a hierarchical structure. Experiments demonstrate that the promise of DCore lies in its power to recognize extremely complex patterns and its high performance in real applications, for example, image segmentation and face clustering, regardless of the dimensionality of the space in which the data are embedded.",space
10.1016/j.artint.2017.12.001,Journal,Artificial Intelligence,scopus,2018-03-01,sciencedirect,Decentralized Reinforcement Learning of robot behaviors,https://api.elsevier.com/content/abstract/scopus_id/85038868982,"A multi-agent methodology is proposed for Decentralized Reinforcement Learning (DRL) of individual behaviors in problems where multi-dimensional action spaces are involved. When using this methodology, sub-tasks are learned in parallel by individual agents working toward a common goal. In addition to proposing this methodology, three specific multi agent DRL approaches are considered: DRL-Independent, DRL Cooperative-Adaptive (CA), and DRL-Lenient. These approaches are validated and analyzed with an extensive empirical study using four different problems: 3D Mountain Car, SCARA Real-Time Trajectory Generation, Ball-Dribbling in humanoid soccer robotics, and Ball-Pushing using differential drive robots. The experimental validation provides evidence that DRL implementations show better performances and faster learning times than their centralized counterparts, while using less computational resources. DRL-Lenient and DRL-CA algorithms achieve the best final performances for the four tested problems, outperforming their DRL-Independent counterparts. Furthermore, the benefits of the DRL-Lenient and DRL-CA are more noticeable when the problem complexity increases and the centralized scheme becomes intractable given the available computational resources and training time.",space
10.1016/j.artint.2017.11.006,Journal,Artificial Intelligence,scopus,2018-03-01,sciencedirect,Strong temporal planning with uncontrollable durations,https://api.elsevier.com/content/abstract/scopus_id/85036477809,"Planning in real world domains often involves modeling and reasoning about the duration of actions. Temporal planning allows such modeling and reasoning by looking for plans that specify start and end time points for each action. In many practical cases, however, the duration of actions may be uncertain and not under the full control of the executor. For example, a navigation task may take more or less time, depending on external conditions such as terrain or weather.
                  In this paper, we tackle the problem of strong temporal planning with uncontrollable action durations (STPUD). For actions with uncontrollable durations, the planner is only allowed to choose the start of the actions, while the end is chosen, within known bounds, by the environment. A solution plan must be robust with respect to all uncontrollable action durations, and must achieve the goal on all executions, despite the choices of the environment.
                  We propose two complementary techniques. First, we discuss a dedicated planning method, that generalizes the state-space temporal planning framework, leveraging SMT-based techniques for temporal networks under uncertainty. Second, we present a compilation-based method, that reduces any STPUD problem to an ordinary temporal planning problem. Moreover, we investigate a set of sufficient conditions to simplify domains by removing some of the uncontrollability.
                  We implemented both our approaches, and we experimentally evaluated our techniques on a large number of instances. Our results demonstrate the practical applicability of the two techniques, which show complementary behavior.",space
10.1016/j.ins.2017.11.012,Journal,Information Sciences,scopus,2018-03-01,sciencedirect,Finding the hottest item in data streams,https://api.elsevier.com/content/abstract/scopus_id/85035804309,"We study a problem of finding the hottest item interval in a data stream, where the hotness of an item over an interval is determined by its average frequency. Finding the hottest item interval is particularly helpful in business promotions, such as monitoring the peak sales records, finding the hottest period in an online game, digging the highest click rate of an online music, etc. Existing work focus on finding the most frequent item over a fixed length interval. However, these solutions cannot return the hottest interval since the best length (i.e., maximizing the average frequency) is unknown in advance. To discover the hottest item interval, a straightforward solution is to calculate the average frequencies of items for every possible interval length, which is too costly for stream applications. To efficiently compute the hottest item interval, we propose an algorithm that employs the arrival timestamps of items and reduce the search space by three pruning strategies. Extensive experiments show that the proposed algorithms can efficiently discover the hottest item interval on both real and synthetic datasets.",space
10.1016/j.knosys.2017.09.036,Journal,Knowledge-Based Systems,scopus,2018-03-01,sciencedirect,A temporal consistency method for online review ranking,https://api.elsevier.com/content/abstract/scopus_id/85031315796,"Providing appropriate online review ranking consistently with the entire review set is deemed important for e-commerce services to facilitate consumers decision making. Unlike the existing efforts that often treat online reviews statically, this paper takes the temporal dynamics of online reviews into account, and designs an effective method for time-aware review ranking. In doing so, first of all, a time-aware review consistency ranking (TRCR) problem is formulated, based on a newly defined metric, which aims to derive a compact review list with maximized expected consistency degree to the original review set. Furthermore, this problem is proven to be NP-hard, which leads us to developing an effective approximation by heuristically restricting the search space (i.e., TRCRea). This proposed method with related improvements show strengths on two aspects: one is that the informational decay of the reviews is well addressed at both macro and micro levels; and the other is that the compact review list provided to the consumers is obtained from a combined perspective of consistency and time-awareness in light of product features and sentiment orientations. Finally, the experiments on real-world data demonstrate the effectiveness and efficiency of the proposed method over baseline methods.",space
10.1016/j.jfranklin.2017.07.037,Journal,Journal of the Franklin Institute,scopus,2018-03-01,sciencedirect,System-on-a-chip (SoC)-based hardware acceleration for foreground and background identification,https://api.elsevier.com/content/abstract/scopus_id/85027498536,"The rapid growth of embedded vision applications and accessibility in recent years has instigated a philosophical shift in algorithm and implementation design for artificial intelligence. With the popularization of high-definition video, the amount of data available to be processed has also increased substantially, posing massive computational and communication demands. Hardware acceleration through specialization has received renewed interest in recent years; such acceleration has generally been implemented using two chips, with the image signal processing (ISP) part being performed by a DSP, a GPU or an FPGA and the video content analytics (VCA) part being executed by a processor. GPUs consume a substantial amount of power; thus, it is challenging to deploy them in embedded environments. However, the new generation of SoC-FPGAs that are fabricated with both the microprocessor and FPGA on a single chip consumes less power and can be built into small systems, thereby offering an attractive platform for embedded applications. This study presents the hardware acceleration of a real-time adaptive background and foreground identification algorithm in a SoC, including the capture, processing and display stages. The algorithm can be performed in either 2D or 3D space. The proposed platform uses photometric invariant color, depth data and local binary patterns (LBPs) to distinguish background from foreground. The system uses minimal cell resources, an elastically pipelined architecture is used to absorb variations in processing time, and each pipeline stage is optimized to use the available FPGA primitives. Additionally, the communication-centric architecture used in this work simplifies the implementation of embedded vision algorithms.",space
10.1016/j.inffus.2017.05.003,Journal,Information Fusion,scopus,2018-03-01,sciencedirect,A Social-aware online short-text feature selection technique for social media,https://api.elsevier.com/content/abstract/scopus_id/85019594702,"Large-scale text categorisation in social environments, characterised by the high dimensionality of feature spaces, is one of the most relevant problems in machine learning and data mining nowadays. Short-texts, which are posted at unprecedented rates, accentuate both the importance of learning tasks and the challenges posed by such large feature space. A collection of social media short-texts does not only provide textual information but also topological information given by the relationships between posts and their authors. The linked nature of social data causes new complementary data dimensions to be added to the feature space, which, at the same time, becomes sparser. Additionally, in the context of social media, posts usually arrive simultaneously in streams, which hinders the deployment of efficient traditional feature selection techniques that assume a feature space fully known in advance. Hence, efficient and scalable online feature selection becomes an important requirement in numerous large-scale social applications. This work presents an online feature selection technique for high-dimensional data based on the integration of two information sources, social and content-based, for the real-time classification of short-text streams coming from social media. It focuses on discovering implicit relations amongst new posts, already known ones and their corresponding authors to identify groups of socially related posts. Then, each discovered group is represented by a set of non-redundant and relevant textual features. Finally, such features are used to train different learning models for classifying newly arriving posts. Extensive experiments conducted on real-world short-texts demonstrate that the proposed approach helps to improve classification results when compared to state-of-the-art and traditional online feature selection techniques.",space
10.1016/j.neucom.2017.01.115,Journal,Neurocomputing,scopus,2018-02-14,sciencedirect,Extreme Learning Machine for Joint Embedding and Clustering,https://api.elsevier.com/content/abstract/scopus_id/85029666112,"Clustering generic data, i.e., data not specific to a particular field, is a challenging problem due to their diverse complex structures in the original feature space. Traditional approaches address this problem by complementing clustering with feature learning methods, which either capture the intrinsic structure of the data or represent the data such that clusters are better revealed. In this paper, we propose an approach referred to as Extreme Learning Machine for Joint Embedding and Clustering (ELM-JEC), which incorporates desirable properties of both types of feature learning methods at the same time, specifically by (1) preserving the manifold structure of the data in the original space; (2) maximizing the class separability of the data in the embedded space. Since either type of method has improved clustering performance in some cases, our motivation is to integrate the two desirable properties to further improve the accuracy and robustness of clustering. Additional notable features of ELM-JEC are that it provides nonlinear feature mappings and achieves feature learning and clustering in the same formulation. The proposed approach can be implemented using alternating optimization, and its clustering performance compares favorably with several state-of-the-art methods on the real-world benchmark datasets.",space
10.1016/j.neucom.2017.08.036,Journal,Neurocomputing,scopus,2018-01-31,sciencedirect,Data-driven model-free slip control of anti-lock braking systems using reinforcement Q-learning,https://api.elsevier.com/content/abstract/scopus_id/85029168035,"This paper proposes the design and implementation of a model-free tire slip control for a fast and highly nonlinear Anti-lock Braking System (ABS). A reinforcement Q-learning optimal control approach is inserted in a batch neural fitted scheme using two neural networks to approximate the value function and the controller, respectively. The transition samples required for learning high performance control can be collected by interacting with the process either by online exploiting the current iteration controller (or policy) under an ε-greedy exploration strategy, or by using data collected under any other controller that is capable of ensuring efficient exploration of the action-state space. Both approaches are highlighted in the paper. Fortunately, the ABS process fits this type of learning-by-interaction because it does not need an initial stabilizing controller. The validation case studies conducted on a real laboratory setup reveal that high control system performance can be achieved using the proposed approaches. Insightful comments on the observed control behavior are offered along with performance comparisons with several types of model-based and model-free controllers including relay, model-based optimal PI, an original model-free neural network state-feedback VRFT controller and a model-free neural network adaptive actor-critic one. With the ability to improve control performance starting from different supervisory controllers or to learn high performance controllers from scratch, the proposed Q-learning optimal control approach proves its performance in a wide operating range and is therefore recommended to its industrial application on ABS.",space
10.1016/j.patrec.2017.12.010,Journal,Pattern Recognition Letters,scopus,2018-01-15,sciencedirect,Face alignment with cascaded semi-parametric deep greedy neural forests,https://api.elsevier.com/content/abstract/scopus_id/85038375628,"Face alignment is an active topic in computer vision, consisting in aligning a shape model on the face. To this end, most modern approaches refine the shape in a cascaded manner, starting from an initial guess. Those shape updates can either be applied in the feature point space (i.e. explicit updates) or in a low-dimensional, parametric space. In this paper, we propose a semi-parametric cascade that first aligns a parametric shape, then captures more fine-grained deformations of an explicit shape. For the purpose of learning shape updates at each cascade stage, we introduce a deep greedy neural forest (GNF) model, which is an improved version of deep neural forest (NF). GNF appears as an ideal regressor for face alignment, as it combines differentiability, high expressivity and fast evaluation runtime. The proposed framework is very fast and achieves high accuracies on multiple challenging benchmarks, including small, medium and large pose experiments.",space
10.1016/j.neucom.2017.01.110,Journal,Neurocomputing,scopus,2018-01-03,sciencedirect,Making physical proofs of concept of reinforcement learning control in single robot hose transport task complete,https://api.elsevier.com/content/abstract/scopus_id/85023642304,"This paper deals with the realization of physical proof of concept experiments in the paradigm of Linked Multi-Component Robotic Systems (LMCRS). The main objective is to demonstrate that the controllers learned through Reinforcement Learning (RL) algorithms with different state space formalizations and different spatial discretizations in a simulator are reliable in a real world configuration of the task of transporting a hose by a single robot. This one is a prototypical example of LMCRS task (extendable to much more complex tasks). We describe how the complete system has been designed and implemented. Two different previously learned RL controllers have been tested solving two different LMCRS control problems, using different state space modeling and discretization step in each case. The physical realizations validate previously published simulation based results, giving a strong argument in favor of the suitability of RL techniques to deal with LMCRS systems.",space
10.1016/j.procs.2018.10.493,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Paraphrasing Arabic Metaphor with Neural Machine Translation,https://api.elsevier.com/content/abstract/scopus_id/85065721331,"The task of recognizing and generating paraphrases is an essential component in many Arabic natural language processing (NLP) applications. A well-established machine translation approach for automatically extracting paraphrases, leverages bilingual corpora to find the equivalent meaning of phrases in a single language, is performed by ""pivoting"" over a shared translation in another language. Neural machine translation has recently become a viable alternative approach to the more widely-used statistical machine translation. In this paper, we revisit bilingual pivoting in the context of neural machine translation and present a paraphrasing model based mainly on neural networks. Our model describes paraphrases in a continuous space and generates candidate paraphrases for an Arabic source input. Experimental results across datasets confirm that neural paraphrases significantly outperform those obtained with statistical machine translation, in particular the Google translator, and indicate high similarity correlation between our model and human translation, making our model attractive for real-world deployment.",space
10.1016/j.procs.2018.05.113,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Real Time High Performance of Sliding Mode Controlled Induction Motor Drives,https://api.elsevier.com/content/abstract/scopus_id/85049099142,"Several industrial applications demand high performance speed functioning and require new control techniques so as to ensure a fast dynamic response. The present work investigates real time implementation and experimental sliding mode controlled (SMC) induction motor drives (IM). The strategy of sliding mode control is a powerful tool to ensure robustness. Nevertheless, the chattering phenomenon is a major disadvantage for non linear systems. For this purpose, two different types of analysis such as layer boundary methods are implemented in dSPACE 1104 controller board and compared between them in order to obtain the best method to reduce or eliminate chattering phenomenon. An experimental results using dSPACE 1104 based on TMS320F240 DSP are described in this work.",space
10.1016/j.procs.2018.05.105,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Geo Map Visualization for Frequent Purchaser in Online Shopping Database Using an Algorithm LP-Growth for Mining Closed Frequent Itemsets,https://api.elsevier.com/content/abstract/scopus_id/85049073263,"Numerous frequent itemsets are explored using frequent itemset mining algorithm which contains redundant information. Fortunately, this issue is decreased to the mining of closed frequent itemsets. However, these approaches still have some performance bottlenecks like processing time and storage space. Moreover, new algorithms of closed frequent itemsets are presented. In this paper, the proposed closed frequent itemset (LP-Closed-tree) using Linear prefix growth method is introduced which is a powerful approach for mining frequent itemsets. It builds basic tree and mines frequent itemsets. The proposed LP-Closed-tree is implemented on real and dense database like online shopping database and chess and is evaluated with other existing algorithms. While using online shopping dataset, the frequent purchaser of the dataset is visualized using google map in geographical method. After comprehensive empirical appraisal it is found that the proposed LP-Closed-tree algorithms are faster in many cases. Moreover, these algorithms are known for relatively lesser consumption of time and memory in cases of large and dense database.",space
10.1016/j.procs.2018.05.168,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Detection of A Shadow of Animated Video Frames in RGB Color Space,https://api.elsevier.com/content/abstract/scopus_id/85049060245,"The study is made on the detection of a shadow of animated video frames in RGB color space. The present study is based on color invariant shadow detection accurately as far as practicable within the limitation of the Matlab software. The grayscale histogram is obtained to extract segments of the shadows. The main aim of the study is to accurately visualize the shadows of the animated objects in consecutive animated video frames. Such type of algorithm can be applied to detect the shadows of real-time images. The advantages of shadow detection are to find out the timing of the day which is caused by Sun, to detect the moving object shadow can be used, just to deceive the enemies from the target object shadows can be used.",space
10.1016/j.artmed.2017.12.004,Journal,Artificial Intelligence in Medicine,scopus,2018-01-01,sciencedirect,Random ensemble learning for EEG classification,https://api.elsevier.com/content/abstract/scopus_id/85039865976,"Real-time detection of seizure activity in epilepsy patients is critical in averting seizure activity and improving patients’ quality of life. Accurate evaluation, presurgical assessment, seizure prevention, and emergency alerts all depend on the rapid detection of seizure onset. A new method of feature selection and classification for rapid and precise seizure detection is discussed wherein informative components of electroencephalogram (EEG)-derived data are extracted and an automatic method is presented using infinite independent component analysis (I-ICA) to select independent features. The feature space is divided into subspaces via random selection and multichannel support vector machines (SVMs) are used to classify these subspaces. The result of each classifier is then combined by majority voting to establish the final output. In addition, a random subspace ensemble using a combination of SVM, multilayer perceptron (MLP) neural network and an extended k-nearest neighbors (k-NN), called extended nearest neighbor (ENN), is developed for the EEG and electrocorticography (ECoG) big data problem. To evaluate the solution, a benchmark ECoG of eight patients with temporal and extratemporal epilepsy was implemented in a distributed computing framework as a multitier cloud-computing architecture. Using leave-one-out cross-validation, the accuracy, sensitivity, specificity, and both false positive and false negative ratios of the proposed method were found to be 0.97, 0.98, 0.96, 0.04, and 0.02, respectively. Application of the solution to cases under investigation with ECoG has also been effected to demonstrate its utility.",space
10.1016/j.enbuild.2017.07.077,Journal,Energy and Buildings,scopus,2017-11-01,sciencedirect,Automatic HVAC control with real-time occupancy recognition and simulation-guided model predictive control in low-cost embedded system,https://api.elsevier.com/content/abstract/scopus_id/85028732660,"Intelligent building automation systems can reduce the energy consumption of heating, ventilation and air-conditioning (HVAC) units by sensing the comfort requirements automatically and scheduling the HVAC operations dynamically. Traditional building automation systems rely on fairly inaccurate occupancy sensors and basic predictive control using oversimplified building thermal response models, all of which prevent such systems from reaching their full potential. Such limitations can now be avoided due to the recent developments in embedded system technologies, which provide viable low-cost computing platforms with powerful processors and sizeable memory storage in a small footprint. As a result, building automation systems can now efficiently execute highly sophisticated computational tasks, such as real-time video processing and accurate thermal-response simulations. With this in mind, we designed and implemented an occupancy-predictive HVAC control system in a low-cost yet powerful embedded system (using Raspberry Pi 3) to demonstrate the following key features for building automation: (1) real-time occupancy recognition using video-processing and machine-learning techniques, (2) dynamic analysis and prediction of occupancy patterns, and (3) model predictive control for HVAC operations guided by real-time building thermal response simulations (using an on-board EnergyPlus simulator). We deployed and evaluated our system for providing automatic HVAC control in the large public indoor space of a mosque, thereby achieving significant energy savings.",space
10.1016/j.actaastro.2017.07.038,Journal,Acta Astronautica,scopus,2017-11-01,sciencedirect,Self-supervised learning as an enabling technology for future space exploration robots: ISS experiments on monocular distance learning,https://api.elsevier.com/content/abstract/scopus_id/85026883328,"Although machine learning holds an enormous promise for autonomous space robots, it is currently not employed because of the inherent uncertain outcome of learning processes. In this article we investigate a learning mechanism, Self-Supervised Learning (SSL), which is very reliable and hence an important candidate for real-world deployment even on safety-critical systems such as space robots. To demonstrate this reliability, we introduce a novel SSL setup that allows a stereo vision equipped robot to cope with the failure of one of its cameras. The setup learns to estimate average depth using a monocular image, by using the stereo vision depths from the past as trusted ground truth. We present preliminary results from an experiment on the International Space Station (ISS) performed with the MIT/NASA SPHERES VERTIGO satellite. The presented experiments were performed on October 8th, 2015 on board the ISS. The main goals were (1) data gathering, and (2) navigation based on stereo vision. First the astronaut Kimiya Yui moved the satellite around the Japanese Experiment Module to gather stereo vision data for learning. Subsequently, the satellite freely explored the space in the module based on its (trusted) stereo vision system and a pre-programmed exploration behavior, while simultaneously performing the self-supervised learning of monocular depth estimation on board. The two main goals were successfully achieved, representing the first online learning robotic experiments in space. These results lay the groundwork for a follow-up experiment in which the satellite will use the learned single-camera depth estimation for autonomous exploration in the ISS, and are an advancement towards future space robots that continuously improve their navigation capabilities over time, even in harsh and completely unknown space environments.",space
10.1016/j.imavis.2016.11.020,Journal,Image and Vision Computing,scopus,2017-09-01,sciencedirect,Strength modelling for real-worldautomatic continuous affect recognition from audiovisual signals,https://api.elsevier.com/content/abstract/scopus_id/85008150025,"Automatic continuous affect recognition from audiovisual cues is arguably one of the most active research areas in machine learning. In addressing this regression problem, the advantages of the models, such as the global-optimisation capability of Support Vector Machine for Regression and the context-sensitive capability of memory-enhanced neural networks, have been frequently explored, but in an isolated way. Motivated to leverage the individual advantages of these techniques, this paper proposes and explores a novel framework, Strength Modelling, where two models are concatenated in a hierarchical framework. In doing this, the strength information of the first model, as represented by its predictions, is joined with the original features, and this expanded feature space is then utilised as the input by the successive model. A major advantage of Strength Modelling, besides its ability to hierarchically explore the strength of different machine learning algorithms, is that it can work together with the conventional feature- and decision-level fusion strategies for multimodal affect recognition. To highlight the effectiveness and robustness of the proposed approach, extensive experiments have been carried out on two time- and value-continuous spontaneous emotion databases (RECOLA and SEMAINE) using audio and video signals. The experimental results indicate that employing Strength Modelling can deliver a significant performance improvement for both arousal and valence in the unimodal and bimodal settings. The results further show that the proposed systems is competitive or outperform the other state-of-the-art approaches, but being with a simple implementation.",space
10.1016/j.compmedimag.2016.11.006,Journal,Computerized Medical Imaging and Graphics,scopus,2017-09-01,sciencedirect,Quantitative PET image reconstruction employing nested expectation-maximization deconvolution for motion compensation,https://api.elsevier.com/content/abstract/scopus_id/85007011515,"Bulk body motion may randomly occur during PET acquisitions introducing blurring, attenuation-emission mismatches and, in dynamic PET, discontinuities in the measured time activity curves between consecutive frames. Meanwhile, dynamic PET scans are longer, thus increasing the probability of bulk motion. In this study, we propose a streamlined 3D PET motion-compensated image reconstruction (3D-MCIR) framework, capable of robustly deconvolving intra-frame motion from a static or dynamic 3D sinogram. The presented 3D-MCIR methods need not partition the data into multiple gates, such as 4D MCIR algorithms, or access list-mode (LM) data, such as LM MCIR methods, both associated with increased computation or memory resources. The proposed algorithms can support compensation for any periodic and non-periodic motion, such as cardio-respiratory or bulk motion, the latter including rolling, twisting or drifting. Inspired from the widely adopted point-spread function (PSF) deconvolution 3D PET reconstruction techniques, here we introduce an image-based 3D generalized motion deconvolution method within the standard 3D maximum-likelihood expectation-maximization (ML-EM) reconstruction framework. In particular, we initially integrate a motion blurring kernel, accounting for every tracked motion within a frame, as an additional MLEM modeling component in the image space (integrated 3D-MCIR). Subsequently, we replaced the integrated model component with a nested iterative Richardson-Lucy (RL) image-based deconvolution method to accelerate the MLEM algorithm convergence rate (RL-3D-MCIR). The final method was evaluated with realistic simulations of whole-body dynamic PET data employing the XCAT phantom and real human bulk motion profiles, the latter estimated from volunteer dynamic MRI scans. In addition, metabolic uptake rate Ki
                      parametric images were generated with the standard Patlak method. Our results demonstrate significant improvement in contrast-to-noise ratio (CNR) and noise-bias performance in both dynamic and parametric images. The proposed nested RL-3D-MCIR method is implemented on the Software for Tomographic Image Reconstruction (STIR) open-source platform and is scheduled for public release.",space
10.1016/j.patrec.2016.10.006,Journal,Pattern Recognition Letters,scopus,2017-07-01,sciencedirect,Redundancy-driven modified Tomek-link based undersampling: A solution to class imbalance,https://api.elsevier.com/content/abstract/scopus_id/85002707897,"Class imbalance can be defined as a span among data mining, machine learning and pattern recognition domains that provides to learn from a data-space having unequal class distribution. Common classifiers when trained by imbalanced data tend to bias towards the class possessing bulk instances causing misclassification of upcoming patterns/instances. The study reveals that presence of redundant borderline instances and outliers in the data-space severely catalyzes the effect of class imbalance. The Condensed Nearest Neighbor and Tomek-link undersampling techniques are used as the baseline systems for the present study, and an improved undersampling algorithm is proposed to be employed in the pre-processing stage by amalgamating aspects of outlier and redundancy detection to the baseline system. The proposed scheme imparts to detect outlier, redundant and noisy instances having least contribution in estimating accurate class labels. Thus, a data-level solution has been offered to the concerned problem with novelty in effective elimination of majority instances without losing valuable information. The proposed scheme is implemented and validated with Back Propagation Neural Network (BPNN), K-Nearest-Neighbor (K-NN), Support Vector Machine (SVM) and Naive Bayes classifiers for 10 real-life datasets. The experimental results obtained clearly manifest the superiority of the proposed scheme over the baseline schemes.",space
10.1016/j.comcom.2016.12.015,Journal,Computer Communications,scopus,2017-06-01,sciencedirect,Imola: A decentralised learning-driven protocol for multi-hop White-Fi,https://api.elsevier.com/content/abstract/scopus_id/85028254628,"In this paper we tackle the digital exclusion problem in developing and remote locations by proposing Imola, an inexpensive learning-driven access mechanism for multi-hop wireless networks that operate across TV white-spaces (TVWS). Stations running Imola only rely on passively acquired neighbourhood information to achieve scheduled-like operation in a decentralised way, without explicit synchronisation. Our design overcomes pathological circumstances such as hidden and exposed terminals that arise due to carrier sensing and are exceptionally problematic in low frequency bands. We present a prototype implementation of our proposal and conduct experiments in a real test bed, which confirms the practical feasibility of deploying our solution in mesh networks that build upon the IEEE 802.11af standard. Finally, the extensive system level simulations we perform demonstrate that Imola achieves up to 4× more throughput than the channel access protocol defined by the standard and reduces frame loss rate by up to 100%.",space
10.1016/j.engappai.2016.08.019,Journal,Engineering Applications of Artificial Intelligence,scopus,2017-06-01,sciencedirect,GPU-based parallel optimization of immune convolutional neural network and embedded system,https://api.elsevier.com/content/abstract/scopus_id/84995489085,"Up to now, the image recognition system has been utilized more and more widely in the security monitoring, the industrial intelligent monitoring, the unmanned vehicle, and even the space exploration. In designing the image recognition system, the traditional convolutional neural network has some defects such as long training time, easy over-fitting and high misclassification rate. In order to overcome these defects, we firstly used the immune mechanism to improve the convolutional neural network and put forward a novel immune convolutional neural network algorithm, after we analyzed the network structure and parameters of the convolutional neural network. Our algorithm not only integrated the location data of the network nodes and the adjustable parameters, but also dynamically adjusted the smoothing factor of the basis function. In addition, we utilized the NVIDIA GPU (Graphics Processing Unit) to accelerate the new immune convolutional neural network (ICNN) in parallel computing and built a real-time embedded image recognition system for this ICNN. The immune convolutional neural network algorithm was improved with CUDA programming and was tested with the sample data in the GPU-based environment. The GPU-based implementation of the novel immune convolutional neural network algorithm was made with the cuDNN, which was designed by NVIDIA for GPU-based accelerating of DNNs in machine learning. Experimental results show that our new immune convolutional neural network has higher recognition rate, more stable performance and faster computing speed than the traditional convolutional neural network.",space
10.1016/j.artmed.2017.06.002,Journal,Artificial Intelligence in Medicine,scopus,2017-05-01,sciencedirect,Automatic detection of surgical haemorrhage using computer vision,https://api.elsevier.com/content/abstract/scopus_id/85020769333,"Background and objectives
                  On occasions, a surgical intervention can be associated with serious, potentially life-threatening complications. One of these complications is a haemorrhage during the operation, an unsolved issue that could delay the intervention or even cause the patient's death. On laparoscopic surgery this complication is even more dangerous, due to the limited vision and mobility imposed by the minimally invasive techniques.
               
                  Methods
                  In this paper it is described a computer vision algorithm designed to analyse the images captured by a laparoscopic camera, classifying the pixels of each frame in blood pixels and background pixels and finally detecting a massive haemorrhage. The pixel classification is carried out by comparing the parameter B/R and G/R of the RGB space colour of each pixel with a threshold obtained using the global average of the whole frame of these parameters. The detection of and starting haemorrhage is achieved by analysing the variation of the previous parameters and the amount of pixel blood classified.
               
                  Results
                  When classifying in vitro images, the proposed algorithm obtains accuracy over 96%, but during the analysis of an in vivo images obtained from real operations, the results worsen slightly due to poor illumination, visual interferences or sudden moves of the camera, obtaining accuracy over 88%. The detection of haemorrhages directly depends of the correct classification of blood pixels, so the analysis achieves an accuracy of 78%.
               
                  Conclusions
                  The proposed algorithm turns out to be a good starting point for an automatic detection of blood and bleeding in the surgical environment which can be applied to enhance the surgeon vision, for example showing the last frame previous to a massive haemorrhage where the incision could be seen using augmented reality capabilities.",space
10.1016/j.cmpb.2017.02.016,Journal,Computer Methods and Programs in Biomedicine,scopus,2017-05-01,sciencedirect,A study of EMR-based medical knowledge network and its applications,https://api.elsevier.com/content/abstract/scopus_id/85014111750,"Background and Objective
                  Electronic medical records (EMRs) contain an amount of medical knowledge which can be used for clinical decision support. We attempt to integrate this medical knowledge into a complex network, and then implement a diagnosis model based on this network.
               
                  Methods
                  The dataset of our study contains 992 records which are uniformly sampled from different departments of the hospital. In order to integrate the knowledge of these records, an EMR-based medical knowledge network (EMKN) is constructed. This network takes medical entities as nodes, and co-occurrence relationships between the two entities as edges. Selected properties of this network are analyzed. To make use of this network, a basic diagnosis model is implemented. Seven hundred records are randomly selected to re-construct the network, and the remaining 292 records are used as test records. The vector space model is applied to illustrate the relationships between diseases and symptoms. Because there may exist more than one actual disease in a record, the recall rate of the first ten results, and the average precision are adopted as evaluation measures.
               
                  Results
                  Compared with a random network of the same size, this network has a similar average length but a much higher clustering coefficient. Additionally, it can be observed that there are direct correlations between the community structure and the real department classes in the hospital. For the diagnosis model, the vector space model using disease as a base obtains the best result. At least one accurate disease can be obtained in 73.27% of the records in the first ten results.
               
                  Conclusion
                  We constructed an EMR-based medical knowledge network by extracting the medical entities. This network has the small-world and scale-free properties. Moreover, the community structure showed that entities in the same department have a tendency to be self-aggregated. Based on this network, a diagnosis model was proposed. This model uses only the symptoms as inputs and is not restricted to a specific disease. The experiments conducted demonstrated that EMKN is a simple and universal technique to integrate different medical knowledge from EMRs, and can be used for clinical decision support.",space
10.1016/j.imavis.2016.08.006,Journal,Image and Vision Computing,scopus,2017-04-01,sciencedirect,Random Multi-Graphs: A semi-supervised learning framework for classification of high dimensional data,https://api.elsevier.com/content/abstract/scopus_id/84994719659,"Currently, high dimensional data processing confronts two main difficulties: inefficient similarity measure and high computational complexity in both time and memory space. Common methods to deal with these two difficulties are based on dimensionality reduction and feature selection. In this paper, we present a different way to solve high dimensional data problems by combining the ideas of Random Forests and Anchor Graph semi-supervised learning. We randomly select a subset of features and use the Anchor Graph method to construct a graph. This process is repeated many times to obtain multiple graphs, a process which can be implemented in parallel to ensure runtime efficiency. Then the multiple graphs vote to determine the labels for the unlabeled data. We argue that the randomness can be viewed as a kind of regularization. We evaluate the proposed method on eight real-world data sets by comparing it with two traditional graph-based methods and one state-of-the-art semi-supervised learning method based on Anchor Graph to show its effectiveness. We also apply the proposed method to the subject of face recognition.",space
10.1016/j.neucom.2016.03.107,Journal,Neurocomputing,scopus,2017-03-29,sciencedirect,Influencing over people with a social emotional model,https://api.elsevier.com/content/abstract/scopus_id/85005873049,"This paper presents an approach of a social emotional model, which allows to extract the social emotion of a group of intelligent entities. The emotional model PAD allows to represent the emotion of an intelligent entity in 3-D space, allowing the representation of different emotional states. The social emotional model presented in this paper uses individual emotions of each one of the entities, which are represented in the emotional space PAD. Using a social emotional model within intelligent entities allows the creation of more real simulations, in which emotional states can influence decision-making. The result of this social emotional mode is represented by a series of examples, which are intended to represent a number of situations in which the emotions of each individual modify the emotion of the group. Moreover, the paper introduces an example which employs the proposed model in order to learn and predict future actions trying to influence in the social emotion of a group of people.",space
10.1016/j.isprsjprs.2017.01.002,Journal,ISPRS Journal of Photogrammetry and Remote Sensing,scopus,2017-03-01,sciencedirect,Appearance learning for 3D pose detection of a satellite at close-range,https://api.elsevier.com/content/abstract/scopus_id/85009110439,"In this paper we present a learning-based 3D detection of a highly challenging specular object exposed to a direct sunlight at very close-range. An object detection is one of the most important areas of image processing, and can also be used for initialization of local visual tracking methods. While the object detection in 3D space is generally a difficult problem, it poses more difficulties when the object is specular and exposed to the direct sunlight as in a space environment. Our solution to a such problem relies on an appearance learning of a real satellite mock-up based on a vector quantization and the vocabulary tree. Our method, implemented on a standard computer (CPU), exploits a full perspective projection model and provides near real-time 3D pose detection of a satellite for close-range approach and manipulation. The time consuming part of the training (feature description, building the vocabulary tree and indexing, depth buffering and back-projection) are performed offline, while a fast image retrieval and 3D-2D registration are performed on-line. In contrast, the state of the art image-based 3D pose detection methods are slower on CPU or assume a weak perspective camera projection model. In our case the dimension of the satellite is larger than the distance to the camera, hence the assumption of the weak perspective model does not hold. To evaluate the proposed method, the appearance of a full scale mock-up of the rear part of the TerraSAR-X satellite is trained under various illumination and camera views. The training images are captured with a camera mounted on six degrees of freedom robot, which enables to position the camera in a desired view, sampled over a sphere. The views that are not within the workspace of the robot are interpolated using image-based rendering. Moreover, we generate ground truth poses to verify the accuracy of the detection algorithm. The achieved results are robust and accurate even under noise due to specular reflection, and able to initialize a local tracking method.",space
10.1016/j.ijar.2016.12.003,Journal,International Journal of Approximate Reasoning,scopus,2017-03-01,sciencedirect,Scalable learning and inference in Markov logic networks,https://api.elsevier.com/content/abstract/scopus_id/85006307364,"Markov logic networks (MLNs) have emerged as a powerful representation that incorporates first-order logic and probabilistic graphical models. They have shown very good results in many problem domains. However, current implementations of MLNs do not scale well due to the large search space and the intractable clause groundings, which is preventing their widespread adoption. In this paper, we propose a general framework named Ground Network Sampling (GNS) for scaling up MLN learning and inference. GNS offers a new instantiation perspective by encoding ground substitutions as simple paths in the Herbrand universe, which uses the interactions existing among the objects to constrain the search space. To further make this search tractable for large scale problems, GNS integrates random walks and subgraph pattern mining, gradually building up a representative subset of simple paths. When inference is concerned, a template network is introduced to quickly locate promising paths that can ground given logical statements. The resulting sampled paths are then transformed into ground clauses, which can be used for clause creation and probabilistic inference. The experiments on several real-world datasets demonstrate that our approach offers better scalability while maintaining comparable or better predictive performance compared to state-of-the-art MLN techniques.",space
10.1016/j.procs.2017.06.036,Conference Proceeding,Procedia Computer Science,scopus,2017-01-01,sciencedirect,Domestic water consumption monitoring and behaviour intervention by employing the internet of things technologies,https://api.elsevier.com/content/abstract/scopus_id/85029365048,"As the water resource is becoming scarce, conservation of water has a high priority around the globe, study on water management and conservation becomes an important research problem. People are increasingly becoming more individual households, which tend to be less efficient, requiring more resources per capita than larger households. In order to address these challenges, this paper presents the achievements of monitoring domestic water consumption at the appliance level and intervening people’s water usage behavior which have been made in ISS-EWATUS (http://www.issewatus.eu), an European Commission funded FP7 project. The water amount consumed by every household appliance is wirelessly recorded with the exact consumption time and stored in a central database. People’s water consumption behavior is likely affected by the real-time water consumption awareness, instant practical advices regarding water-saving activities and classification of water consumption behavior for individuals, all of which are provided by a decision support system deployed as a mobile application in a tablet or any other mobile devices. Only the enhanced water consumption awareness is presented in this paper due to the space limitation. The integrated monitoring and decision support system has been deployed and in use in Sosnowiec in Poland and Skiathos in Greece since March 2015. The domestic water consumption monitoring system at appliance level and the local DSS for affecting people’s water consumption behavior are innovative and have little seen before according to the knowledge of the authors.",space
10.1016/j.procs.2017.03.056,Conference Proceeding,Procedia Computer Science,scopus,2017-01-01,sciencedirect,The Current Status and Future Perspectives of Virtual Maintenance,https://api.elsevier.com/content/abstract/scopus_id/85029176726,"Virtual maintenance, which is widely used in aerospace, automobile, military equipment, etc., has been given abroad attention among equipment life-cycle including concept definition, system design, component production, daily operation, troubleshooting, and so on. Virtual maintenance has been given many different definitions and lot of technologies for implementation, but there is no clear systematic conclusion on the both. Based on the review of the current achievements, the elements of virtual maintenance are extracted, the technologies are systematically explored, and the applications are developed. Meanwhile, the future perspectives of virtual maintenance are discussed associated with virtual reality and augmented reality, multi-person collaboration, remote assistance, as well as artificial intelligence.",space
10.1016/j.energy.2017.07.005,Journal,Energy,scopus,2017-01-01,sciencedirect,Mathematical modeling and evolutionary generation of rule sets for energy-efficient flexible job shops,https://api.elsevier.com/content/abstract/scopus_id/85024852274,"As environmental awareness grows, sustainable scheduling is attracting increasing attention. The purposes of this paper are obtain the lower bound of energy-efficient flexible job shops with machine selection, job sequencing, and machine on-off decision making via a new mathematical model and to discover more energy-efficient rules with easy implementation in real practice via an efficient Gene Expression Programming (eGEP) algorithm. This paper first formulates a novel mixed-integer linear mathematical model to achieve effective machine selection, job sequencing, and machine off-on decision making. Then for the purpose of avoiding the empirical combination, five attributes exerting direct influence on the total energy consumption are extracted and consequently involved in the evolutionary process of eGEP. Furthermore, diversified rule mining operations with multi-gene representation and self-study are designed to enhance the search space and solutions quality. And, unsupervised learning is utilized in which global best and current worst are set to guide evolution direction since the learning progress has no prior knowledge. Experimental results show that machine off-on decisions efficiently reduce the total energy consumption; and, the discovered rules reach the lower bound calculated by GAMS/CPLEX in small problems and have significant superiority over other dispatching rules in energy saving.",space
10.1016/j.neunet.2016.09.001,Journal,Neural Networks,scopus,2017-01-01,sciencedirect,A modular architecture for transparent computation in recurrent neural networks,https://api.elsevier.com/content/abstract/scopus_id/84994319086,"Computation is classically studied in terms of automata, formal languages and algorithms; yet, the relation between neural dynamics and symbolic representations and operations is still unclear in traditional eliminative connectionism. Therefore, we suggest a unique perspective on this central issue, to which we would like to refer as transparent connectionism, by proposing accounts of how symbolic computation can be implemented in neural substrates. In this study we first introduce a new model of dynamics on a symbolic space, the versatile shift, showing that it supports the real-time simulation of a range of automata. We then show that the Gödelization of versatile shifts defines nonlinear dynamical automata, dynamical systems evolving on a vectorial space. Finally, we present a mapping between nonlinear dynamical automata and recurrent artificial neural networks. The mapping defines an architecture characterized by its granular modularity, where data, symbolic operations and their control are not only distinguishable in activation space, but also spatially localizable in the network itself, while maintaining a distributed encoding of symbolic representations. The resulting networks simulate automata in real-time and are programmed directly, in the absence of network training. To discuss the unique characteristics of the architecture and their consequences, we present two examples: (i) the design of a Central Pattern Generator from a finite-state locomotive controller, and (ii) the creation of a network simulating a system of interactive automata that supports the parsing of garden-path sentences as investigated in psycholinguistics experiments.",space
10.1016/j.compbiomed.2016.10.006,Journal,Computers in Biology and Medicine,scopus,2016-12-01,sciencedirect,CAFÉ-Map: Context Aware Feature Mapping for mining high dimensional biomedical data,https://api.elsevier.com/content/abstract/scopus_id/84995610155,"Feature selection and ranking is of great importance in the analysis of biomedical data. In addition to reducing the number of features used in classification or other machine learning tasks, it allows us to extract meaningful biological and medical information from a machine learning model. Most existing approaches in this domain do not directly model the fact that the relative importance of features can be different in different regions of the feature space. In this work, we present a context aware feature ranking algorithm called CAFÉ-Map. CAFÉ-Map is a locally linear feature ranking framework that allows recognition of important features in any given region of the feature space or for any individual example. This allows for simultaneous classification and feature ranking in an interpretable manner. We have benchmarked CAFÉ-Map on a number of toy and real world biomedical data sets. Our comparative study with a number of published methods shows that CAFÉ-Map achieves better accuracies on these data sets. The top ranking features obtained through CAFÉ-Map in a gene profiling study correlate very well with the importance of different genes reported in the literature. Furthermore, CAFÉ-Map provides a more in-depth analysis of feature ranking at the level of individual examples.
                  
                     Availability: CAFÉ-Map Python code is available at:
                  
                     http://faculty.pieas.edu.pk/fayyaz/software.html#cafemap .
                  The CAFÉ-Map package supports parallelization and sparse data and provides example scripts for classification. This code can be used to reconstruct the results given in this paper.",space
10.1016/j.trc.2016.10.019,Journal,Transportation Research Part C: Emerging Technologies,scopus,2016-12-01,sciencedirect,Short-term speed predictions exploiting big data on large urban road networks,https://api.elsevier.com/content/abstract/scopus_id/84994322993,"Big data from floating cars supply a frequent, ubiquitous sampling of traffic conditions on the road network and provide great opportunities for enhanced short-term traffic predictions based on real-time information on the whole network. Two network-based machine learning models, a Bayesian network and a neural network, are formulated with a double star framework that reflects time and space correlation among traffic variables and because of its modular structure is suitable for an automatic implementation on large road networks. Among different mono-dimensional time-series models, a seasonal autoregressive moving average model (SARMA) is selected for comparison. The time-series model is also used in a hybrid modeling framework to provide the Bayesian network with an a priori estimation of the predicted speed, which is then corrected exploiting the information collected on other links. A large floating car data set on a sub-area of the road network of Rome is used for validation. To account for the variable accuracy of the speed estimated from floating car data, a new error indicator is introduced that relates accuracy of prediction to accuracy of measure. Validation results highlighted that the spatial architecture of the Bayesian network is advantageous in standard conditions, where a priori knowledge is more significant, while mono-dimensional time series revealed to be more valuable in the few cases of non-recurrent congestion conditions observed in the data set. The results obtained suggested introducing a supervisor framework that selects the most suitable prediction depending on the detected traffic regimes.",space
10.1016/j.ins.2016.08.050,Journal,Information Sciences,scopus,2016-12-01,sciencedirect,Pedestrian detection by learning a mixture mask model and its implementation,https://api.elsevier.com/content/abstract/scopus_id/84989928835,"Pedestrian detection from videos is a useful technique in intelligent transportation systems. Some key challenges of accurate pedestrian detection are the large variations in pedestrian appearance as the pedestrians assume different poses and the different camera views that are involved. This makes the generic visual descriptors unreliable for real-world pedestrian detection. In this paper, we propose a high-level human-specific descriptor for detecting pedestrians in multiple videos. More specifically, by obtaining the feature matrix from a sliding window, we use multiple mapping vectors to project the original feature matrix into different mask spaces. Inspired by the part-based model [12], it is natural to formulate the pedestrian detection into a multiple-instance learning (MIL) framework. Afterward, we adopt an MI-SVM [9] to solve it. To evaluate the proposed detection algorithm, we implement the pedestrian detection algorithm in FPGA, which can process over 30 fps. Moreover, our method outperforms many existing object detection algorithms in terms of accuracy.",space
10.1016/j.cie.2016.07.031,Journal,Computers and Industrial Engineering,scopus,2016-11-01,sciencedirect,TIMSPAT – Reachability graph search-based optimization tool for colored Petri net-based scheduling,https://api.elsevier.com/content/abstract/scopus_id/84991205081,"The combination of Petri net (PN) modeling with AI-based heuristic search (HS) algorithms (PNHS) has been successfully applied as an integrated approach to deal with scheduling problems that can be transformed into a search problem in the reachability graph. While several efficient HS algorithms have been proposed albeit using timed PN, the practical application of these algorithms requires an appropriate tool to facilitate its development and analysis. However, there is a lack of tool support for the optimization of timed colored PN (TCPN) models based on the PNHS approach for schedule generation. Because of its complex data structure, TCPN-based scheduling has often been limited to simulation-based performance analysis only. Also, it is quite difficult to evaluate the strength and tractability of algorithms for different scheduling scenarios due to the different computing platforms, programming languages and data structures employed. In this light, this paper presents a new tool called TIMSPAT, developed to overcome the shortcomings of existing tools. Some features that distinguish this tool are the collection of several HS algorithms, XML-based model integration, the event-driven exploration of the timed state space including its condensed variant, localized enabling of transitions, the introduction of static place, and the easy-to-use syntax statements. The tool is easily extensible and can be integrated as a component into existing PN simulators and software environments. A comparative study is performed on a real-world eyeglass production system to demonstrate the application of the tool for scheduling purposes.",space
10.1016/j.forsciint.2016.09.010,Journal,Forensic Science International,scopus,2016-11-01,sciencedirect,Pornography classification: The hidden clues in video space–time,https://api.elsevier.com/content/abstract/scopus_id/84989311576,"As web technologies and social networks become part of the general public's life, the problem of automatically detecting pornography is into every parent's mind — nobody feels completely safe when their children go online. In this paper, we focus on video-pornography classification, a hard problem in which traditional methods often employ still-image techniques — labeling frames individually prior to a global decision. Frame-based approaches, however, ignore significant cogent information brought by motion. Here, we introduce a space-temporal interest point detector and descriptor called Temporal Robust Features (TRoF). TRoF was custom-tailored for efficient (low processing time and memory footprint) and effective (high classification accuracy and low false negative rate) motion description, particularly suited to the task at hand. We aggregate local information extracted by TRoF into a mid-level representation using Fisher Vectors, the state-of-the-art model of Bags of Visual Words (BoVW). We evaluate our original strategy, contrasting it both to commercial pornography detection solutions, and to BoVW solutions based upon other space-temporal features from the scientific literature. The performance is assessed using the Pornography-2k dataset, a new challenging pornographic benchmark, comprising 2000 web videos and 140h of video footage. The dataset is also a contribution of this work and is very assorted, including both professional and amateur content, and it depicts several genres of pornography, from cartoon to live action, with diverse behavior and ethnicity. The best approach, based on a dense application of TRoF, yields a classification error reduction of almost 79% when compared to the best commercial classifier. A sparse description relying on TRoF detector is also noteworthy, for yielding a classification error reduction of over 69%, with 19× less memory footprint than the dense solution, and yet can also be implemented to meet real-time requirements.",space
10.1016/j.scico.2016.03.009,Journal,Science of Computer Programming,scopus,2016-11-01,sciencedirect,Model-driven processes and tools to design robot-based generative learning objects for computer science education,https://api.elsevier.com/content/abstract/scopus_id/84971228847,"In this paper, we introduce a methodology to design robot-oriented generative learning objects (GLOs) that are, in fact, heterogeneous meta-programs to teach computer science (CS) topics such as programming. The methodology includes CS learning variability modelling using the feature-based approaches borrowed from the SW engineering domain. Firstly, we define the CS learning domain using the known educational framework TPACK (Technology, Pedagogy And Content Knowledge). By learning variability we mean the attributes of the framework extracted and represented as feature models with multiple values. Therefore, the CS learning variability represents the problem domain. Meta-programming is considered as a solution domain. Both are represented by feature models. The GLO design task is formulated as mapping the problem domain model on the solution domain model. Next, we present the design framework to design GLOs manually or semi-automatically. The multi-level separation of concepts, model representation and transformation forms the conceptual background. Its theoretical background includes: (a) a formal definition of feature-based models; (b) a graph-based and set-based definition of meta-programming concepts; (c) transformation rules to support the model mapping; (d) a computational Abstract State Machine model to define the processes and design tool for developing GLOs. We present the architecture and some characteristics of the tool. The tool enables to improve the GLO design process significantly (in terms of time and quality) and to achieve a higher quality and functionality of GLOs themselves (in terms of the parameter space enlargement for reuse and adaptation). We demonstrate the appropriateness of the methodology in the real teaching setting. In this paper, we present the case study that analyses three robot-oriented GLOs as the higher-level specifications. Then, using the meta-language processor, we are able to produce, from the specifications, the concrete robot control programs on demand automatically and to demonstrate teaching algorithms visually by robot's actions. We evaluate the approach from technological and pedagogical perspectives using the known structural metrics. Also, we indicate the merits and demerits of the approach. The main contribution and originality of the paper is the seamless integration of two known technologies (feature modelling and meta-programming) in designing robot-oriented GLOs and their supporting tools.",space
10.1016/j.eswa.2016.04.026,Journal,Expert Systems with Applications,scopus,2016-10-30,sciencedirect,Using dynamical systems tools to detect concept drift in data streams,https://api.elsevier.com/content/abstract/scopus_id/84965132319,"Real-world data streams may change their behaviors along time, what is referred to as concept drift. By detecting those changes, researchers obtain relevant information about the phenomena that produced such streams (e.g. temperatures in a region, bacteria population, disease occurrence, etc.). Many concept drift detection algorithms consider supervised or semi-supervised approaches which tend to be unfeasible when data is collected at high frequencies, due to the difficulties involved in labeling. Complementarily, current studies usually assume data as statistically independent and identically distributed, disregarding any temporal relationship among observations and, consequently, risking the quality of data modeling. In order to tackle both aspects, we employ dynamical system modeling to represent the temporal relationships among data observations and how they modify along time in attempt to detect concept drift. This approach considers Taken’s immersion theorem to unfold consecutive windows of data observations into the phase space in attempt to represent and compare time dependencies. From this perspective, we proposed four new concept drift detection algorithms based on the unsupervised machine learning paradigm. The first algorithm builds dendrograms of consecutive phase spaces (every phase space represents the time relationships for the observations contained in a particular data window) and compare them out by using the Gromov–Hausdorff distance, providing enough guarantees to detect concept drifts. The second algorithm employs the Cross Recurrence Plot and the Recurrence Quantification Analysis to detect relevant changes in consecutive phase spaces and warn about relevant data modifications. We also preprocess data windows by considering the Empirical Mode Decomposition method and Mutual Information in attempt to take only the deterministic stream behavior into account. All algorithms were implemented as plugins for the Massive Online Analysis (MOA) software and then compared to well-known algorithms from literature. Results confirm the proposed algorithms were capable of detecting most of the behavior changes, creating few false alarms.",space
10.1016/j.ijleo.2016.06.126,Journal,Optik,scopus,2016-10-01,sciencedirect,Development of a calibrating algorithm for Delta Robot's visual positioning based on artificial neural network,https://api.elsevier.com/content/abstract/scopus_id/84978140749,"Delta robot with vision system can automatically control the end-actuator to accurately grasp moving objects on the conveyor belt. Establishment of the mapping relationship between the image feature space and the robot working space form a closed-loop chain for transformational link between the robot coordinate, camera coordinate and conveyor belt coordinate. The vision system calibration is a basic problem of robot vision research and implementation. The artificial neural networks (ANN) which has learning ability, adaptive ability and nonlinear function approximation ability can establish the nonlinear relationship between space points and pixel points to complete accurate calibration of the vision system. The convergence speed of calibration algorithm affects the real-time visual servo system. The calibration precision, generalization ability and calibration space of algorithm influence the robot grasping accuracy. Therefore, a new calibration technique for delta robot’s vision system was presented in this paper. The algorithm combines ANN with Faugeras vision system calibration technology. The setting of the initial value, network structure and the choice of the activation function is based on the model of Faugeras vision system calibration algorithm, which makes the actual output of the network closer to the target output. Experiments proved that this algorithm has higher calibration accuracy and generalization ability compared with the conventional calibration algorithm, as well as faster convergence speed compared with the conventional artificial neural network structure in the case of high calibration accuracy.",space
10.1016/j.adhoc.2016.06.011,Journal,Ad Hoc Networks,scopus,2016-10-01,sciencedirect,Feature selection for performance characterization in multi-hop wireless sensor networks,https://api.elsevier.com/content/abstract/scopus_id/84977657790,"Current trends in Wireless Sensor Networks are faced with the challenge of shifting from testbeds in controlled environments to real-life deployments, characterized by unattended and long-term operation. The network performance in such settings depends on various factors, ranging from the operational space, the behavior of the protocol stack, the intra-network dynamics, and the status of each individual node. As such, characterizing the network’s high-level performance based exclusively on link-quality estimation, can yield episodic snapshots on the performance of specific, point-to-point links. The objective of this work is to provide an integrated framework for the unsupervised selection of the dominant features that have crucial impact on the performance of end-to-end links, established over a multi-hop topology. Our focus is on compressing the original feature vector of network parameters, by eliminating redundant network attributes with predictable behavior. The proposed approach is implemented alongside different cases of protocol stacks and evaluated on data collected from real-life deployments in rural and industrial environments. Discussions on the efficacy of the proposed scheme, and the dominant network characteristics per deployment are offered.",space
10.1016/j.jnca.2016.06.010,Journal,Journal of Network and Computer Applications,scopus,2016-09-01,sciencedirect,Traffic and mobility aware resource prediction using cognitive agent in mobile ad hoc networks,https://api.elsevier.com/content/abstract/scopus_id/84989815364,"Mobile Ad hoc NETwork (MANET) characteristics such as limited resources, shared channel, unpredictable mobility, improper load balancing, and variation in signal strength affect the routing of real-time multimedia data that requires Quality of Service (QoS) provisioning. Accurate prediction of the resource availability assists efficient resource allocation before the routing of such data. Most of the published work on resource prediction in MANET focuses on either bandwidth or energy without considering mobility effects. Adoption of intelligent software agent such as Cognitive Agent (CA) for the accurate resource prediction has a significant potential to solve the challenges of resource prediction in MANET. The intelligence provided in CA is similar to the logical thinking like a human for decision-making. The predominant CA architecture is the Belief-Desire-Intention (BDI) model, which performs the various tasks on behalf of the human user as an assistant.
                  In this paper, we propose a CA-based Resource Prediction mechanism considering Mobility (CA-RPM) that predicts the resources using agents through the resource prediction agency consisting of one static agent, one cognitive agent and two mobile agents. Agents predict the traffic, mobility, buffer space, energy, and bandwidth effectively that is necessary for efficient resource allocation to support real-time and multimedia communications. The mobile agents collect and distribute network traffic statistics over MANET whereas a static agent collects the local statistics. CA creates static/mobile agent during the process of resource prediction. Initially, the designed time-series Wavelet Neural Networks (WNNs) predict traffic and mobility. Buffer space, energy, and bandwidth prediction use the predicted mobility and traffic. Simulation results show that the predicted resources closely match with the real values at the cost of little overheads due to the usage of agents. Simulation analysis of predicted traffic and mobility also shows the improvement compared to recurrent WNN in terms of mean square error, covariance, memory overhead, agent overhead and computation overhead. We plan to use these predicted resources for its efficient utilization in QoS routing is our future work.",space
10.1016/j.ijpe.2016.06.005,Journal,International Journal of Production Economics,scopus,2016-09-01,sciencedirect,Hybrid flow shop batching and scheduling with a bi-criteria objective,https://api.elsevier.com/content/abstract/scopus_id/84975859979,"This paper addresses the hybrid flow shop batching and scheduling problem where sequence-dependent family setup times are present and the objective is to simultaneously minimize the weighted sum of the total weighted completion time and total weighted tardiness. In particular, it disregards the group technology assumptions by allowing for the possibility of splitting pre-determined groups of jobs into inconsistent batches in order to improve the operational efficiency. A benchmark of small size problems is considered to show the benefits of batching on group scheduling. Since the problem is strongly NP-hard, several algorithms based upon tabu search are developed at three levels, which move back and forth between batching and scheduling phases. Two algorithms incorporate tabu search into the framework of path-relinking to exploit the information on good solutions. These tabu search/path-relinking algorithms comprise several distinguishing features including two relinking procedures to effectively construct paths and the stage-based improvement procedure to consider the move interdependency. The best tabu search algorithm as a local search algorithm is compared to a population-based algorithm, and the superiority of the former over the latter is shown using a statistical experiment. The initial solution finding mechanism is implemented to trigger the search into the solution space. The efficiency and effectiveness of the best algorithm is verified with the help of the results found by CPLEX. The results show that the best algorithm, based on tabu search/path relinking and the stage-based improvement procedure, could find solutions at least as good as CPLEX, but in drastically shorter computational time. In order to reflect the real industry requirements, dynamic machine availability times, dynamic job release times, machine eligibility and machine capability for processing jobs, desired lower bounds on batch sizes, and job skipping are considered.",space
10.1016/j.eswa.2016.03.012,Journal,Expert Systems with Applications,scopus,2016-09-01,sciencedirect,Optimization of neural networks through grammatical evolution and a genetic algorithm,https://api.elsevier.com/content/abstract/scopus_id/84962738944,"This paper proposes a hybrid neuro-evolutive algorithm (NEA) that uses a compact indirect encoding scheme (IES) for representing its genotypes (a set of ten production rules of a Lindenmayer System with memory), moreover has the ability to reuse the genotypes and automatically build modular, hierarchical and recurrent neural networks. A genetic algorithm (GA) evolves a Lindenmayer System (L-System) that is used to design the neural network’s architecture. This basic neural codification confers scalability and search space reduction in relation to other methods. Furthermore, the system uses a parallel genome scan engine that increases both the implicit parallelism and convergence of the GA. The fitness function of the NEA rewards economical artificial neural networks (ANNs) that are easily implemented. The NEA was tested on five real-world classification datasets and three well-known datasets for time series forecasting (TSF). The results are statistically compared against established state-of-the-art algorithms and various forecasting methods (ADANN, ARIMA, UCM, and Forecast Pro). In most cases, our NEA outperformed the other methods, delivering the most accurate classification and time series forecasting with the least computational effort. These superior results are attributed to the improved effectiveness and efficiency of NEA in the decision-making process. The result is an optimized neural network architecture for solving classification problems and simulating dynamical systems.",space
10.1016/j.bica.2016.07.001,Journal,Biologically Inspired Cognitive Architectures,scopus,2016-07-01,sciencedirect,A dual-process Qualia Modeling Framework (QMF),https://api.elsevier.com/content/abstract/scopus_id/84979681532,"We present the software implementation of our Qualia Modeling Framework (QMF), a computational cognitive model based on the dual-process theory, which theorizes that reasoning and decision making rely on integrated experiences from two interactive minds: the autonomous mind, without the agent’s conscious awareness, and the reflective mind, of which the agent is consciously aware. In the QMF, artificial qualia are the vocabulary of the conscious mind, required to reason over conceptual memory, and generate cognitive inferences. The autonomous mind employs pattern-matching, for fast reasoning over episodic memories. An ACT-R model with conventional declarative memory represents the autonomous mind. A second ACT-R model, with an unconventional implementation of declarative memory utilizing a hypernetwork theory based model of qualia space, represents the reflective mind. Using real-world, non-trivial, data sets, our cognitive model achieved classification accuracy comparable to, or greater than, analogous machine learning classifiers kNN and DT, while providing improvements in flexibility by allowing the Target Attribute to be identified or changed any time during training and testing. We advance the BICA challenge by providing a generalizable, efficient, algorithm which models the phenomenal structure of consciousness as proposed by a contemporary theory, and provides an effective decision aid in complex environments where data are too broad or diverse for a human to evaluate without computational assistance.",space
10.1016/j.biosystems.2016.05.007,Journal,BioSystems,scopus,2016-07-01,sciencedirect,Robotic action acquisition with cognitive biases in coarse-grained state space,https://api.elsevier.com/content/abstract/scopus_id/84973440991,"Some of the authors have previously proposed a cognitively inspired reinforcement learning architecture (LS-Q) that mimics cognitive biases in humans. LS-Q adaptively learns under uniform, coarse-grained state division and performs well without parameter tuning in a giant-swing robot task. However, these results were shown only in simulations. In this study, we test the validity of the LS-Q implemented in a robot in a real environment. In addition, we analyze the learning process to elucidate the mechanism by which the LS-Q adaptively learns under the partially observable environment. We argue that the LS-Q may be a versatile reinforcement learning architecture, which is, despite its simplicity, easily applicable and does not require well-prepared settings.",space
10.1016/j.cmpb.2016.04.005,Journal,Computer Methods and Programs in Biomedicine,scopus,2016-07-01,sciencedirect,A MapReduce approach to diminish imbalance parameters for big deoxyribonucleic acid dataset,https://api.elsevier.com/content/abstract/scopus_id/84964534094,"Background
                  In the age of information superhighway, big data play a significant role in information processing, extractions, retrieving and management. In computational biology, the continuous challenge is to manage the biological data. Data mining techniques are sometimes imperfect for new space and time requirements. Thus, it is critical to process massive amounts of data to retrieve knowledge. The existing software and automated tools to handle big data sets are not sufficient. As a result, an expandable mining technique that enfolds the large storage and processing capability of distributed or parallel processing platforms is essential.
               
                  Method
                  In this analysis, a contemporary distributed clustering methodology for imbalance data reduction using k-nearest neighbor (K-NN) classification approach has been introduced. The pivotal objective of this work is to illustrate real training data sets with reduced amount of elements or instances. These reduced amounts of data sets will ensure faster data classification and standard storage management with less sensitivity. However, general data reduction methods cannot manage very big data sets. To minimize these difficulties, a MapReduce-oriented framework is designed using various clusters of automated contents, comprising multiple algorithmic approaches.
               
                  Results
                  To test the proposed approach, a real DNA (deoxyribonucleic acid) dataset that consists of 90 million pairs has been used. The proposed model reduces the imbalance data sets from large-scale data sets without loss of its accuracy.
               
                  Conclusions
                  The obtained results depict that MapReduce based K-NN classifier provided accurate results for big data of DNA.",space
10.1016/j.ins.2016.01.097,Journal,Information Sciences,scopus,2016-06-20,sciencedirect,Generalized quasi-metric on strings,https://api.elsevier.com/content/abstract/scopus_id/84959352366,"In this paper, we propose a generalized quasi-metric in spaces of strings, which is based on edit operations (insertion and deletions) and taking values as pairs of non-negative integers. We show that with such a generalization is possible to carry more information about similarity between strings than in the usual case where the distance between the strings is a simple real number. An algorithm for the calculation of this quasi-metric is presented and as well as an illustrative example of the application of this quasi-metric in handwritten digit classification. We also show some relations of this quasi-metric with the concept of subsequence of a string.",space
10.1016/j.neucom.2016.02.014,Journal,Neurocomputing,scopus,2016-06-12,sciencedirect,Model reference fractional order control using type-2 fuzzy neural networks structure: Implementation on a 2-DOF helicopter,https://api.elsevier.com/content/abstract/scopus_id/84959905863,"In this paper, an adaptive learning algorithm is proposed for an interval type-2 fuzzy fractional order controller. The use of fractional order controller adds more degrees of freedom which makes it possible to obtain superior performance in comparison with ordinary differential controllers. A fractional order reference model is used to define the desired trajectory of the nonlinear dynamic system. The structure of the system is based on the feedback error learning method. The stability of the adaptation laws is proved using Lyapunov theory. In order to test the efficiency and efficacy of the proposed learning and the control algorithm, the trajectory tracking problem of a magnetic rigid spacecraft is studied. The simulation results show that the proposed control algorithm outperforms the case when ordinary differential fuzzy controller is used. Furthermore, it is shown that it is possible to define a master chaotic system as a reference model and obtain synchronization between the two chaotic systems using the proposed approach. In the simulation part the synchronization between two Duffing–Holmes system is also achieved. In order to show the implementability of the proposed method, it is used to control a real time laboratory setup 2-DOF helicopter. It is shown that the proposed fractional order controller can be implemented in a low cost embedded system and can successfully control a highly nonlinear dynamic system.",space
10.1016/j.future.2015.10.013,Journal,Future Generation Computer Systems,scopus,2016-06-01,sciencedirect,Real-time data mining of massive data streams from synoptic sky surveys,https://api.elsevier.com/content/abstract/scopus_id/84961285522,"The nature of scientific and technological data collection is evolving rapidly: data volumes and rates grow exponentially, with increasing complexity and information content, and there has been a transition from static data sets to data streams that must be analyzed in real time. Interesting or anomalous phenomena must be quickly characterized and followed up with additional measurements via optimal deployment of limited assets. Modern astronomy presents a variety of such phenomena in the form of transient events in digital synoptic sky surveys, including cosmic explosions (supernovae, gamma ray bursts), relativistic phenomena (black hole formation, jets), potentially hazardous asteroids, etc. We have been developing a set of machine learning tools to detect, classify and plan a response to transient events for astronomy applications, using the Catalina Real-time Transient Survey (CRTS) as a scientific and methodological testbed. The ability to respond rapidly to the potentially most interesting events is a key bottleneck that limits the scientific returns from the current and anticipated synoptic sky surveys. Similar challenge arises in other contexts, from environmental monitoring using sensor networks to autonomous spacecraft systems. Given the exponential growth of data rates, and the time-critical response, we need a fully automated and robust approach. We describe the results obtained to date, and the possible future developments.",space
10.1016/j.ijepes.2015.11.077,Journal,International Journal of Electrical Power and Energy Systems,scopus,2016-06-01,sciencedirect,Power distribution network reconfiguration for power loss minimization using novel dynamic fuzzy c-means (dFCM) clustering based ANN approach,https://api.elsevier.com/content/abstract/scopus_id/84949995877,"In this study, a three-layer artificial neural network (ANN) is proposed to reconfigure power distribution networks to obtain the optimal configuration in which the active power loss is minimal. Then, the proposed ANN is reduced in size by transforming the input space with kernels using a proposed modified dynamic fuzzy c-means (dFCM) clustering algorithm to obtain a novel framework. The proposed framework and ANN both are implemented on the two IEEE 33-bus and IEEE 69-bus power distribution networks. The ANN and framework both are trained using the training set consisting of only 64 training samples. The simulated results are compared to the results obtained by performing a selected traditional method which is the switching algorithm. The comparative results explicitly verify that using the proposed framework for distribution networks reconfiguration has some benefits such as a very short process time that is far shorter than the others, a very simple structure including only a minimal number of neurons and higher accuracy compared to the others. These features show that the proposed framework can be effectively used for real-time reconfiguration of power distribution networks.",space
10.1016/j.tics.2015.11.007,Journal,Trends in Cognitive Sciences,scopus,2016-03-01,sciencedirect,Conceptual Alignment: How Brains Achieve Mutual Understanding,https://api.elsevier.com/content/abstract/scopus_id/84958161462,"We share our thoughts with other minds, but we do not understand how. Having a common language certainly helps, but infants’ and tourists’ communicative success clearly illustrates that sharing thoughts does not require signals with a pre-assigned meaning. In fact, human communicators jointly build a fleeting conceptual space in which signals are a means to seek and provide evidence for mutual understanding. Recent work has started to capture the neural mechanisms supporting those fleeting conceptual alignments. The evidence suggests that communicators and addressees achieve mutual understanding by using the same computational procedures, implemented in the same neuronal substrate, and operating over temporal scales independent from the signals’ occurrences.",space
10.1016/j.neucom.2015.12.013,Journal,Neurocomputing,scopus,2016-02-29,sciencedirect,A distributed spatial-temporal weighted model on MapReduce for short-term traffic flow forecasting,https://api.elsevier.com/content/abstract/scopus_id/84955646724,"Accurate and timely traffic flow prediction is crucial to proactive traffic management and control in data-driven intelligent transportation systems (D2ITS), which has attracted great research interest in the last few years. In this paper, we propose a Spatial–Temporal Weighted K-Nearest Neighbor model, named STW-KNN, in a general MapReduce framework of distributed modeling on a Hadoop platform, to enhance the accuracy and efficiency of short-term traffic flow forecasting. More specifically, STW-KNN considers the spatial–temporal correlation and weight of traffic flow with trend adjustment features, to optimize the search mechanisms containing state vector, proximity measure, prediction function, and 
                        K
                      selection. Furthermore, STW-KNN is implemented on a widely adopted Hadoop distributed computing platform with the MapReduce parallel processing paradigm, for parallel prediction of traffic flow in real time. Finally, with extensive experiments on real-world big taxi trajectory data, STW-KNN is compared with the state-of-the-art prediction models including conventional K-Nearest Neighbor (KNN), Artificial Neural Networks (ANNs), Naïve Bayes (NB), Random Forest (RF), and C4.5. The results demonstrate that the proposed model is superior to existing models on accuracy by decreasing the mean absolute percentage error (MAPE) value more than 11.59% only in time domain and even achieves 89.71% accuracy improvement with the MAPEs of between 3.34% and 6.00% in both space and time domains, and also significantly improves the efficiency and scalability of short-term traffic flow forecasting over existing approaches.",space
10.1016/j.ins.2015.09.051,Journal,Information Sciences,scopus,2016-02-01,sciencedirect,Across neighborhood search for numerical optimization,https://api.elsevier.com/content/abstract/scopus_id/84949671034,"Population-based search algorithms (PBSAs), including swarm intelligence algorithms (SIAs) and evolutionary algorithms (EAs), are competitive alternatives for solving complex optimization problems and they have been widely applied to real-world optimization problems in different fields. In this study, a novel population-based across neighborhood search (ANS) is proposed for numerical optimization. ANS is motivated by two straightforward assumptions and three important issues raised in improving and designing efficient PBSAs. In ANS, a group of individuals collaboratively search the solution space for an optimal solution of the optimization problem considered. A collection of superior solutions found by individuals so far is maintained and updated dynamically. At each generation, an individual directly searches across the neighborhoods of multiple superior solutions with the guidance of a Gaussian distribution. This search manner is referred to as across neighborhood search. The characteristics of ANS are discussed and the concept comparisons with other PBSAs are given. The principle behind ANS is simple. Moreover, ANS is easy for implementation and application with three parameters being required to tune. Extensive experiments on 18 benchmark optimization functions of different types show that ANS has well balanced exploration and exploitation capabilities and performs competitively compared with many efficient PBSAs (Related Matlab codes used in the experiments are available from http://guohuawunudt.gotoip2.com/publications.html).",space
10.1016/j.ins.2015.08.055,Journal,Information Sciences,scopus,2016-02-01,sciencedirect,Using tensor products to detect unconditional label dependence in multilabel classifications,https://api.elsevier.com/content/abstract/scopus_id/84949658865,"Multilabel (ML) classification tasks consist of assigning a set of labels to each input. It is well known that detecting label dependencies is crucial in order to improve the performance in ML problems. In this paper, we study a new kernel approach to take into account unconditional label dependence between labels. The aim is to improve the performance measured by a micro-averaged loss function. The core idea is to transform a ML task into a binary classification problem whose inputs are drawn from a tensor space of the original input space and a representation of the labels. In this joint feature space we define a kernel to explicitly involve both labels and object descriptions. In addition to the theoretical contributions, the experimental results of this study provide an interesting conclusion: the performance in terms of Hamming Loss can be improved when unconditional label dependence is considered, as our method does. We report a thoroughly experimentation carried out with real world domains and several synthetic datasets devised to analyze the effect of exploiting label dependence in scenarios with different degrees of dependency.",space
10.1016/j.neucom.2015.02.096,Journal,Neurocomputing,scopus,2016-01-22,sciencedirect,Building feature space of extreme learning machine with sparse denoising stacked-autoencoder,https://api.elsevier.com/content/abstract/scopus_id/84940061662,"The random-hidden-node extreme learning machine (ELM) is a much more generalized cluster of single-hidden-layer feed-forward neural networks (SLFNs) which has three parts: random projection, non-linear transformation, and ridge regression (RR) model. Networks with deep architectures have demonstrated state-of-the-art performance in a variety of settings, especially with computer vision tasks. Deep learning algorithms such as stacked autoencoder (SAE) and deep belief network (DBN) are built on learning several levels of representation of the input. Beyond simply learning features by stacking autoencoders (AE), there is a need for increasing its robustness to noise and reinforcing the sparsity of weights to make it easier to discover interesting and prominent features. The sparse AE and denoising AE was hence developed for this purpose. This paper proposes an approach: SSDAE-RR (stacked sparse denoising autoencoder – ridge regression) that effectively integrates the advantages in SAE, sparse AE, denoising AE, and the RR implementation in ELM algorithm. We conducted experimental study on real-world classification (binary and multiclass) and regression problems with different scales among several relevant approaches: SSDAE-RR, ELM, DBN, neural network (NN), and SAE. The performance analysis shows that the SSDAE-RR tends to achieve a better generalization ability on relatively large datasets (large sample size and high dimension) that were not pre-processed for feature abstraction. For 16 out of 18 tested datasets, the performance of SSDAE-RR is more stable than other tested approaches. We also note that the sparsity regularization and denoising mechanism seem to be mandatory for constructing interpretable feature representations. The fact that a SSDAE-RR approach often has a comparable training time to ELM makes it useful in some real applications.",space
10.1016/j.procs.2016.08.136,Conference Proceeding,Procedia Computer Science,scopus,2016-01-01,sciencedirect,Formal Description and Automatic Generation of Learning Spaces Based on Ontologies,https://api.elsevier.com/content/abstract/scopus_id/84988908359,"A good virtual Learning Space (LS) should convey pertinent learning information to the visitors at the most adequate time and locations to favor their knowledge acquisition.
                  Considering the consolidation of the internet and the improvement of the interaction, searching, and learning mechanisms, we propose a generic architecture, called CaVa, to create virtual Learning Spaces building up on cultural institution documents. More precisely, our proposal is to automatically create ontology-based virtual learning environments.
                  Thus, to impart relevant learning materials to the virtual LS, we propose the use of ontologies to represent the key concepts and semantic relations in an user- and machine-understandable format. These concepts together with the data (extracted from the real documents) stored in a digital storage format (XML datasets, relational databases, etc.) are displayed in an ontology-based learning space that enables the visitors to use the available features and tools to learn about a specific domain.
                  According to the approach here discussed, each desired virtual LS must be specified rigorously through a domain specific language (DSL) that was designed and implemented.
                  To validate the proposed architecture, three case studies will be used as instances of CaVa architecture.",space
10.1016/j.procs.2016.03.044,Conference Proceeding,Procedia Computer Science,scopus,2016-01-01,sciencedirect,A Comparative Analysis of Particle Swarm Optimization and Support Vector Machines for Devnagri Character Recognition: An Android Application,https://api.elsevier.com/content/abstract/scopus_id/84964863522,"Devanagari script is widely used in the Indian subcontinent in several major languages such as Hindi, Sanskrit, Marathi and Nepali. Recognition of unconstrained (Handwritten) Devanagari character is more complex due to shape of constituent strokes. Hence character recognition (CR) has been an active area of research till now and it continues to be a challenging research topic due to its diverse applicable environment. As the size of the vocabulary increases, the complexity of algorithms also increases linearly due to the need for a larger search space. Devnagari script recognition systems using Zernike moments, fuzzy rule and quadratic classifier provide less accuracy and less efficiency. Classification methods based on learning from examples have been widely applied to character recognition from the 1990s and have brought forth significant improvements of recognition accuracies. In this paper techniques like particle swarm optimization and support vector machines are implemented and compared. An android phone is used for taking input character and MATLAB software for showing the recognized Devnagari character. For the connection between android device and MATLAB we are using PHP language. The particle swarm optimization technique provides accuracy up to 90%.",space
10.1016/j.ins.2015.06.027,Journal,Information Sciences,scopus,2015-12-01,sciencedirect,The responsibility weighted Mahalanobis kernel for semi-supervised training of support vector machines for classification,https://api.elsevier.com/content/abstract/scopus_id/84940920656,"Kernel functions in support vector machines (SVM) are needed to assess the similarity of input samples in order to classify these samples, for instance. Besides standard kernels such as Gaussian (i.e., radial basis function, RBF) or polynomial kernels, there are also specific kernels tailored to consider structure in the data for similarity assessment. In this paper, we will capture structure in data by means of probabilistic mixture density models, for example Gaussian mixtures in the case of real-valued input spaces. From the distance measures that are inherently contained in these models, e.g., Mahalanobis distances in the case of Gaussian mixtures, we derive a new kernel, the responsibility weighted Mahalanobis (RWM) kernel. Basically, this kernel emphasizes the influence of model components from which any two samples that are compared are assumed to originate (that is, the “responsible” model components). We will see that this kernel outperforms the RBF kernel and other kernels capturing structure in data (such as the LAP kernel in Laplacian SVM) in many applications where partially labeled data are available, i.e., for semi-supervised training of SVM. Other key advantages are that the RWM kernel can easily be used with standard SVM implementations and training algorithms such as sequential minimal optimization, and heuristics known for the parametrization of RBF kernels in a C-SVM can easily be transferred to this new kernel. Properties of the RWM kernel are demonstrated with 20 benchmark data sets and an increasing percentage of labeled samples in the training data.",space
10.1016/j.amc.2015.04.123,Journal,Applied Mathematics and Computation,scopus,2015-09-15,sciencedirect,An efficient initialization mechanism of neurons for Winner Takes All Neural Network implemented in the CMOS technology,https://api.elsevier.com/content/abstract/scopus_id/84942982088,"The paper presents a new initialization mechanism based on a Convex Combination Method (CCM) for Kohonen self-organizing Neural Networks (NNs) realized in the CMOS technology. A proper selection of initial values of the neuron weights exhibits a strong impact on the quality of the overall learning process. Unfortunately, in case of real input data, e.g. biomedical data, proper initialization is not easy to perform, as an exact data distribution is usually unknown. Bad initialization causes that even 70%–80% of neurons remain inactive, which increases the quantization error and thus limits the classification abilities of the NN. The proposed initialization algorithm has a couple of important advantages. Firstly, it does not require a knowledge of data distribution in the input data space. Secondly, there is no necessity for an initial polarization of the neuron weights before starting the learning process. This feature is very convenient in case of transistor level realizations. In this case the programming lines, which in other approaches occupy a large chip area, are not required. We proposed a modification of the original CCM algorithm. A new parameter which in the proposed analog CMOS realization is represented by an external current, allows to fit the behavior of the mechanism to NNs containing different numbers of neurons. The investigations show that the modified CCM operates properly for the NN containing even 250 neurons. A single CCM block realized in the CMOS 180nm technology occupies an area of 300
                     
                        
                           μ
                        
                     m2 and dissipates an average power of 20
                     
                        
                           μ
                        
                     W and at data rate of up to 20MHz.",space
10.1016/j.patcog.2015.03.017,Journal,Pattern Recognition,scopus,2015-09-01,sciencedirect,IODA: An input/output deep architecture for image labeling,https://api.elsevier.com/content/abstract/scopus_id/84929516076,"In this paper, we propose a deep neural network (DNN) architecture called Input Output Deep Architecture (IODA) for solving the problem of image labeling. IODA directly links a whole image to a whole label map, assigning a label to each pixel using a single neural network forward step. Instead of designing a handcrafted a priori model on labels (such as an atlas in the medical domain), we propose to automatically learn the dependencies between labels. The originality of IODA is to transpose DNN input pre-training trick to the output space, in order to learn a high level representation of labels. It allows a fast image labeling inside a fully neural network framework, without the need of any preprocessing such as feature designing or output coding.
                  In this paper, IODA is applied on both a toy texture problem and a real-world medical image dataset, showing promising results. We provide an open source implementation of IODA.
                        1
                     
                     
                        1
                        
                           http://mloss.org/software/view/562/
                        
                     
                     ,
                     
                        2
                     
                     
                        2
                        
                           https://github.com/jlerouge/crino",space
10.1016/j.echo.2015.02.020,Journal,Journal of the American Society of Echocardiography,scopus,2015-08-01,sciencedirect,Accuracy and Test-Retest Reproducibility of Two-Dimensional Knowledge-Based Volumetric Reconstruction of the Right Ventricle in Pulmonary Hypertension,https://api.elsevier.com/content/abstract/scopus_id/84938546753,"Background
                  Right heart function is the key determinant of symptoms and prognosis in pulmonary hypertension (PH), but the right ventricle has a complex geometry that is challenging to quantify by two-dimensional (2D) echocardiography. A novel 2D echocardiographic technique for right ventricular (RV) quantitation involves knowledge-based reconstruction (KBR), a hybrid of 2D echocardiography–acquired coordinates localized in three-dimensional space and connected by reference to a disease-specific RV shape library. The aim of this study was to determine the accuracy of 2D KBR against cardiac magnetic resonance imaging in PH and the test-retest reproducibility of both conventional 2D echocardiographic RV fractional area change (FAC) and 2D KBR.
               
                  Methods
                  Twenty-eight patients with PH underwent same-day echocardiography and cardiac magnetic resonance imaging. Two operators performed serial RV FAC and 2D KBR acquisition and postprocessing to assess inter- and intraobserver test-retest reproducibility.
               
                  Results
                  Bland-Altman analysis (mean bias ± 95% limits of agreement) showed good agreement for end-diastolic volume (3.5 ± 25.0 mL), end-systolic volume (0.9 ± 19.9 mL), stroke volume (2.6 ± 23.1 mL), and ejection fraction (0.4 ± 10.2%) measured by 2D KBR and cardiac magnetic resonance imaging. There were no significant interobserver or intraobserver test-retest differences for 2D KBR RV metrics, with acceptable limits of agreement (interobserver end-diastolic volume, −0.9 ± 21.8 mL; end-systolic volume, −1.3 ± 25.8 mL; stroke volume, −0.2 ± 24.2 mL; ejection fraction, 0.7 ± 14.4%). Significant test-retest variability was observed for 2D echocardiographic RV areas and FAC.
               
                  Conclusions
                  Two-dimensional KBR is an accurate, novel technique for RV volumetric quantification in PH, with superior test-retest reproducibility compared with conventional 2D echocardiographic RV FAC.",space
10.1016/j.scico.2015.04.002,Journal,Science of Computer Programming,scopus,2015-08-01,sciencedirect,DREMS ML: A wide spectrum architecture design language for distributed computing platforms,https://api.elsevier.com/content/abstract/scopus_id/84930180339,"Complex sensing, processing and control applications running on distributed platforms are difficult to design, develop, analyze, integrate, deploy and operate, especially if resource constraints, fault tolerance and security issues are to be addressed. While technology exists today for engineering distributed, real-time component-based applications, many problems remain unsolved by existing tools. Model-driven development techniques are powerful, but there are very few existing and complete tool chains that offer an end-to-end solution to developers, from design to deployment. There is a need for an integrated model-driven development environment that addresses all phases of application lifecycle including design, development, verification, analysis, integration, deployment, operation and maintenance, with supporting automation in every phase. Arguably, a centerpiece of such a model-driven environment is the modeling language. To that end, this paper presents a wide-spectrum architecture design language called DREMS ML that itself is an integrated collection of individual domain-specific sub-languages. We claim that the language promotes “correct-by-construction” software development and integration by supporting each individual phase of the application lifecycle. Using a case study, we demonstrate how the design of DREMS ML impacts the development of embedded systems.",space
10.1016/j.trb.2015.02.008,Journal,Transportation Research Part B: Methodological,scopus,2015-06-01,sciencedirect,Nonlinear multivariate time-space threshold vector error correction model for short term traffic state prediction,https://api.elsevier.com/content/abstract/scopus_id/84925046498,We propose Time–Space Threshold Vector Error Correction (TS-TVEC) model for short term (hourly) traffic state prediction. The theory and method of cointegration with error correction mechanism is employed in the general design of the new statistical model TS-TVEC. An inherent connection between mathematical form of error correction model and traffic flow theory is revealed through the transformation of the well-known Fundamental Traffic Diagrams. A threshold regime switching framework is implemented to overcome any unknown structural changes in traffic time series. Spatial cross correlated information is incorporated with a piecewise linear vector error correction model. A Neural Network model is also constructed in parallel to comparatively test the effectiveness and robustness of the new statistical model. Our empirical study shows that the TS-TVEC model is an effective tool that is capable of modeling the complexity of stochastic traffic flow processes and potentially applicable to real time traffic state prediction.,space
10.1016/j.knosys.2014.12.010,Journal,Knowledge-Based Systems,scopus,2015-03-01,sciencedirect,Top-k high utility pattern mining with effective threshold raising strategies,https://api.elsevier.com/content/abstract/scopus_id/84923103642,"In pattern mining, users generally set a minimum threshold to find useful patterns from databases. As a result, patterns with higher values than the user-given threshold are discovered. However, it is hard for the users to determine an appropriate minimum threshold. The reason for this is that they cannot predict the exact number of patterns mined by the threshold and control the mining result precisely, which can lead to performance degradation. To address this issue, top-k mining has been proposed for discovering patterns from ones with the highest value to ones with the kth highest value with setting the desired number of patterns, k. Top-k utility mining has emerged to consider characteristics of real-world databases such as relative importance of items and item quantities with the advantages of top-k mining. Although a relevant algorithm has been suggested in recent years, it generates a huge number of candidate patterns, which results in an enormous amount of execution time. In this paper, we propose an efficient algorithm for mining top-k high utility patterns with highly decreased candidates. For this purpose, we develop three strategies that can reduce the search space by raising a minimum threshold effectively in the construction of a global tree, where they utilize exact and pre-evaluated utilities of itemsets. Moreover, we suggest a strategy to identify actual top-k high utility patterns from candidates with the exact and pre-calculated utilities. Comprehensive experimental results on both real and synthetic datasets show that our algorithm with the strategies outperforms state-of-the-art methods.",space
10.1016/j.jbi.2014.10.009,Journal,Journal of Biomedical Informatics,scopus,2015-02-01,sciencedirect,Quantifying the determinants of outbreak detection performance through simulation and machine learning,https://api.elsevier.com/content/abstract/scopus_id/84924493147,"Objective
                  To develop a probabilistic model for discovering and quantifying determinants of outbreak detection and to use the model to predict detection performance for new outbreaks.
               
                  Materials and methods
                  We used an existing software platform to simulate waterborne disease outbreaks of varying duration and magnitude. The simulated data were overlaid on real data from visits to emergency department in Montreal for gastroenteritis. We analyzed the combined data using biosurveillance algorithms, varying their parameters over a wide range. We then applied structure and parameter learning algorithms to the resulting data set to build a Bayesian network model for predicting detection performance as a function of outbreak characteristics and surveillance system parameters. We evaluated the predictions of this model through 5-fold cross-validation.
               
                  Results
                  The model predicted performance metrics of commonly used outbreak detection methods with an accuracy greater than 0.80. The model also quantified the influence of different outbreak characteristics and parameters of biosurveillance algorithms on detection performance in practically relevant surveillance scenarios. In addition to identifying characteristics expected a priori to have a strong influence on detection performance, such as the alerting threshold and the peak size of the outbreak, the model suggested an important role for other algorithm features, such as adjustment for weekly patterns.
               
                  Conclusion
                  We developed a model that accurately predicts how characteristics of disease outbreaks and detection methods will influence on detection. This model can be used to compare the performance of detection methods under different surveillance scenarios, to gain insight into which characteristics of outbreaks and biosurveillance algorithms drive detection performance, and to guide the configuration of surveillance systems.",space
10.1016/j.procs.2015.08.504,Conference Proceeding,Procedia Computer Science,scopus,2015-01-01,sciencedirect,Fitness based position update in spider monkey optimization algorithm,https://api.elsevier.com/content/abstract/scopus_id/84962597795,Spider Monkey Optimization (SMO) technique is most recent member in the family of swarm optimization algorithms.SMO algorithm fall in class of Nature Inspired Algorithm (NIA). SMO algorithm is good in exploration and exploitation of local search space and it is well balanced algorithm most of the times. This paper presents a new strategy to update position of solution during local leader phase using fitness of individuals. The proposed algorithm is named as Fitness based Position Update in SMO (FPSMO) algorithm as it updates position of individuals based on their fitness. The anticipated strategy enhances the rate of convergence. The planned FPSMO approach tested over nineteen benchmark functions and for one real world problem so as to establish superiority of it over basic SMO algorithm.,space
10.1016/j.procs.2015.05.364,Conference Proceeding,Procedia Computer Science,scopus,2015-01-01,sciencedirect,Multiobjective design optimization in the Lightweight Dataflow for DDDAS Environment (LiD4E),https://api.elsevier.com/content/abstract/scopus_id/84939212317,"In this paper, we introduce new methods for multiobjective, system-level optimization that have been incorporated into the Lightweight Dataflow for Dynamic Data Driven Application Systems (DDDAS) Environment (LiD4E). LiD4E is a design tool for optimized implementation of dynamic, data-driven stream mining systems using high-level dataflow models of computation. More specifically, we develop in this paper new methods for integrated modeling and optimization of real-time stream mining constraints, multidimensional stream mining performance (precision and recall), and energy efficiency. Using a design methodology centered on data-driven control of and coordination between alternative dataflow subsystems for stream mining (classification modes), we develop systematic methods for exploring complex, multidimensional design spaces associated with dynamic stream mining systems, and deriving sets of Pareto-optimal system configurations that can be switched among based on data characteristics and operating constraints.",space
10.1016/j.patcog.2014.07.024,Journal,Pattern Recognition,scopus,2015-01-01,sciencedirect,Inexact and incremental bilinear Lanczos components algorithms for high dimensionality reduction and image reconstruction,https://api.elsevier.com/content/abstract/scopus_id/84908031277,"Recently, matrix-based methods have gained wide attentions in pattern recognition and machine learning communities. The generalized low rank approximations of matrices (GLRAM) and the bilinear Lanczos components (BLC) algorithm are two popular algorithms that treat data as the native two-dimensional matrix patterns. However, these two algorithms often require heavy computation time and memory space in practice, especially for large scale problems. In this paper, we propose inexact and incremental bilinear Lanczos components algorithms for high dimensionality reduction and image reconstruction. We first introduce the thick-restarting strategy to the BLC algorithm, and present a thick-restarted Lanczos components (TRBLC) algorithm. In this algorithm, we use the Ritz vectors as approximations to dominant eigenvectors instead of the Lanczos vectors. In our implementation, the iterative matrices are neither formed nor stored explicitly, thanks to the characteristic of the Lanczos procedure. Then, we explore the relationship between the reconstruction error and the accuracy of the Ritz vectors, so that the computational complexities of eigenpairs can be reduced significantly. As a result, we propose an inexact thick-restarted Lanczos components (Inex-TRBLC) algorithm. Moreover, we investigate the problem of incremental generalized low rank approximations of matrices, and propose an incremental and inexact TRBLC (Incr-TRBLC) algorithm. Numerical experiments illustrate the superiority of the new algorithms over the GLRAM algorithm and its variations, as well as the BLC algorithm for some real-world image reconstruction and face recognition problems.",space
10.1016/j.neuron.2014.08.042,Journal,Neuron,scopus,2014-10-22,sciencedirect,Engagement of Neural Circuits Underlying 2D Spatial Navigation in a Rodent Virtual Reality System,https://api.elsevier.com/content/abstract/scopus_id/84908234689,"Virtual reality (VR) enables precise control of an animal’s environment and otherwise impossible experimental manipulations. Neural activity in rodents has been studied on virtual 1D tracks. However, 2D navigation imposes additional requirements, such as the processing of head direction and environment boundaries, and it is unknown whether the neural circuits underlying 2D representations can be sufficiently engaged in VR. We implemented a VR setup for rats, including software and large-scale electrophysiology, that supports 2D navigation by allowing rotation and walking in any direction. The entorhinal-hippocampal circuit, including place, head direction, and grid cells, showed 2D activity patterns similar to those in the real world. Furthermore, border cells were observed, and hippocampal remapping was driven by environment shape, suggesting functional processing of virtual boundaries. These results illustrate that 2D spatial representations can be engaged by visual and rotational vestibular stimuli alone and suggest a novel VR tool for studying rat navigation.",space
10.1016/j.ins.2014.02.024,Journal,Information Sciences,scopus,2014-08-10,sciencedirect,Oppositional extension of reinforcement learning techniques,https://api.elsevier.com/content/abstract/scopus_id/84900824395,"In this paper, we present different opposition schemes for four reinforcement learning methods: Q-learning, Q(
                        
                           λ
                        
                     ), Sarsa, and Sarsa(
                        
                           λ
                        
                     ) under assumptions that are reasonable for many real-world problems where type-II opposites generally better reflect the nature of the problem at hand. It appears that the aggregation of opposition-based schemes with regular learning methods can significantly speed up the learning process, especially where the number of observations is small or the state space is large. We verify the performance of the proposed methods using two different applications: a grid-world problem and a single water reservoir management problem.",space
10.1016/j.neucom.2014.01.012,Journal,Neurocomputing,scopus,2014-07-20,sciencedirect,Text style analysis using trace ratio criterion patch alignment embedding,https://api.elsevier.com/content/abstract/scopus_id/84897963546,"An effective algorithm for extracting cues of text styles is proposed in this paper. When processing document collections, the documents are first converted to a high dimensional data set with the assistant of a group of style markers. We also employ the Trace Ratio Criterion Patch Alignment Embedding (TR-PAE) to obtain lower dimensional representation in a textual space. The TR-PAE has some advantages that the inter-class separability and intra-class compactness are well characterized by the special designed intrinsic graph and penalty graph, which are based on discriminative patch alignment strategy. Another advantage is that the proposed method is based on trace ratio criterion, which directly represents the average between-class distance and average within-class distance in the low-dimensional space. To evaluate our proposed algorithm, three corpuses are designed and collected using existing popular corpuses and real-life data covering diverse topics and genres. Extensive simulations are conducted to illustrate the feasibility and effectiveness of our implementation. Our simulations demonstrate that the proposed method is able to extract the deeply hidden information of styles of given documents, and efficiently conduct reliable text analysis results on text styles can be provided.",space
10.1016/j.ins.2014.01.008,Journal,Information Sciences,scopus,2014-06-10,sciencedirect,Embedded local feature selection within mixture of experts,https://api.elsevier.com/content/abstract/scopus_id/84897053423,"A useful strategy to deal with complex classification scenarios is the “divide and conquer” approach. The mixture of experts (MoE) technique makes use of this strategy by jointly training a set of classifiers, or experts, that are specialized in different regions of the input space. A global model, or gate function, complements the experts by learning a function that weighs their relevance in different parts of the input space. Local feature selection appears as an attractive alternative to improve the specialization of experts and gate function, particularly, in the case of high dimensional data. In general, subsets of dimensions, or subspaces, are usually more appropriate to classify instances located in different regions of the input space. Accordingly, this work contributes with a regularized variant of MoE that incorporates an embedded process for local feature selection using 
                        
                           
                              
                                 L
                              
                              
                                 1
                              
                           
                        
                      regularization. Experiments using artificial and real-world datasets provide evidence that the proposed method improves the classical MoE technique, in terms of accuracy and sparseness of the solution. Furthermore, our results indicate that the advantages of the proposed technique increase with the dimensionality of the data.",space
10.1016/j.neucom.2013.03.060,Journal,Neurocomputing,scopus,2014-05-20,sciencedirect,Autonomous UAV based search operations using constrained sampling evolutionary algorithms,https://api.elsevier.com/content/abstract/scopus_id/84896707806,"This paper introduces and studies the application of Constrained Sampling Evolutionary Algorithms in the framework of an UAV based search and rescue scenario. These algorithms have been developed as a way to harness the power of Evolutionary Algorithms (EA) when operating in complex, noisy, multimodal optimization problems and transfer the advantages of their approach to real time real world problems that can be transformed into search and optimization challenges. These types of problems are denoted as Constrained Sampling problems and are characterized by the fact that the physical limitations of reality do not allow for an instantaneous determination of the fitness of the points present in the population that must be evolved. A general approach to address these problems is presented and a particular implementation using Differential Evolution as an example of CS-EA is created and evaluated using teams of UAVs in search and rescue missions. The results are compared to those of a Swarm Intelligence based strategy in the same type of problem as this approach has been widely used within the UAV path planning field in different variants by many authors.",space
10.1016/j.ins.2014.01.010,Journal,Information Sciences,scopus,2014-05-10,sciencedirect,Traffic sign recognition using group sparse coding,https://api.elsevier.com/content/abstract/scopus_id/84894099092,"Recognizing traffic signs is a challenging problem; and it has captured the attention of the computer vision community for several decades. Essentially, traffic sign recognition is a multi-class classification problem that has become a real challenge for computer vision and machine learning techniques. Although many machine learning approaches are used for traffic sign recognition, they are primarily used for classification, not feature design. Identifying rich features using modern machine learning methods has recently attracted attention and has achieved success in many benchmarks. However these approaches have not been fully implemented in the traffic sign recognition problem. In this paper, we propose a new approach to tackle the traffic sign recognition problem. First, we introduce a new feature learning approach using group sparse coding. The primary goal is to exploit the intrinsic structure of the pre-learned visual codebook. This new coding strategy preserves locality and encourages similar descriptors to share similar sparse representation patterns. Second, we use a non-uniform quantization approach based on log-polar mapping. Using the log-polar mapping of the traffic sign image, rotated and scaled patterns are converted into shifted patterns in the new space. We extract the local descriptors from these patterns to learn the features. Finally, by evaluating the proposed approach using the German Traffic Sign Recognition Benchmark dataset, we show that the proposed coding strategy outperforms existing coding methods and the obtained results are comparable to the state-of-the-art.",space
10.1016/j.tcs.2013.09.027,Journal,Theoretical Computer Science,scopus,2014-01-30,sciencedirect,Domain adaptation and sample bias correction theory and algorithm for regression,https://api.elsevier.com/content/abstract/scopus_id/84892371351,"We present a series of new theoretical, algorithmic, and empirical results for domain adaptation and sample bias correction in regression. We prove that the discrepancy is a distance for the squared loss when the hypothesis set is the reproducing kernel Hilbert space induced by a universal kernel such as the Gaussian kernel. We give new pointwise loss guarantees based on the discrepancy of the empirical source and target distributions for the general class of kernel-based regularization algorithms. These bounds have a simpler form than previous results and hold for a broader class of convex loss functions not necessarily differentiable, including 
                        
                           
                              L
                           
                           
                              q
                           
                        
                      losses and the hinge loss. We also give finer bounds based on the discrepancy and a weighted feature discrepancy parameter. We extend the discrepancy minimization adaptation algorithm to the more significant case where kernels are used and show that the problem can be cast as an SDP similar to the one in the feature space. We also show that techniques from smooth optimization can be used to derive an efficient algorithm for solving such SDPs even for very high-dimensional feature spaces and large samples. We have implemented this algorithm and report the results of experiments both with artificial and real-world data sets demonstrating its benefits both for general scenario of adaptation and the more specific scenario of sample bias correction. Our results show that it can scale to large data sets of tens of thousands or more points and demonstrate its performance improvement benefits.",space
10.1016/j.neunet.2014.07.001,Journal,Neural Networks,scopus,2014-01-01,sciencedirect,Ordinal regression neural networks based on concentric hyperspheres,https://api.elsevier.com/content/abstract/scopus_id/84904878701,"Threshold models are one of the most common approaches for ordinal regression, based on projecting patterns to the real line and dividing this real line in consecutive intervals, one interval for each class. However, finding such one-dimensional projection can be too harsh an imposition for some datasets. This paper proposes a multidimensional latent space representation with the purpose of relaxing this projection, where the different classes are arranged based on concentric hyperspheres, each class containing the previous classes in the ordinal scale. The proposal is implemented through a neural network model, each dimension being a linear combination of a common set of basis functions. The model is compared to a nominal neural network, a neural network based on the proportional odds model and to other state-of-the-art ordinal regression methods for a total of 12 datasets. The proposed latent space shows an improvement on the two performance metrics considered, and the model based on the three-dimensional latent space obtains competitive performance when compared to the other methods.",space
10.1016/j.engappai.2014.01.007,Journal,Engineering Applications of Artificial Intelligence,scopus,2014-01-01,sciencedirect,Adaptive multi-objective reinforcement learning with hybrid exploration for traffic signal control based on cooperative multi-agent framework,https://api.elsevier.com/content/abstract/scopus_id/84894106219,"In this paper, we focus on computing a consistent traffic signal configuration at each junction that optimizes multiple performance indices, i.e., multi-objective traffic signal control. The multi-objective function includes minimizing trip waiting time, total trip time, and junction waiting time. Moreover, the multi-objective function includes maximizing flow rate, satisfying green waves for platoons traveling in main roads, avoiding accidents especially in residential areas, and forcing vehicles to move within moderate speed range of minimum fuel consumption. In particular, we formulate our multi-objective traffic signal control as a multi-agent system (MAS). Traffic signal controllers have a distributed nature in which each traffic signal agent acts individually and possibly cooperatively in a MAS. In addition, agents act autonomously according to the current traffic situation without any human intervention. Thus, we develop a multi-agent multi-objective reinforcement learning (RL) traffic signal control framework that simulates the driver's behavior (acceleration/deceleration) continuously in space and time dimensions. The proposed framework is based on a multi-objective sequential decision making process whose parameters are estimated based on the Bayesian interpretation of probability. Using this interpretation together with a novel adaptive cooperative exploration technique, the proposed traffic signal controller can make real-time adaptation in the sense that it responds effectively to the changing road dynamics. These road dynamics are simulated by the Green Light District (GLD) vehicle traffic simulator that is the testbed of our traffic signal control. We have implemented the Intelligent Driver Model (IDM) acceleration model in the GLD traffic simulator. The change in road conditions is modeled by varying the traffic demand probability distribution and adapting the IDM parameters to the adverse weather conditions. Under the congested and free traffic situations, the proposed multi-objective controller significantly outperforms the underlying single objective controller which only minimizes the trip waiting time (i.e., the total waiting time in the whole vehicle trip rather than at a specific junction). For instance, the average trip and waiting times are ≃8 and 6 times lower respectively when using the multi-objective controller.",space
10.1016/j.chemolab.2013.08.009,Journal,Chemometrics and Intelligent Laboratory Systems,scopus,2013-10-15,sciencedirect,Genetic algorithm search space splicing particle swarm optimization as general-purpose optimizer,https://api.elsevier.com/content/abstract/scopus_id/84884383367,"A heuristic search space splicing scheme has been implemented to aid the convergence of the particle swarm optimization (PSO) algorithm to the global optimum. Genetic algorithm (GA) was used to splice the search space into smaller subspaces, thereby reducing the number of local minima. PSO algorithm was subsequently used to locate the global optima in the subspaces. A set of 11 well-known test functions had been used for the assessment of this novel GA search space splicing PSO (GA-SSS-PSO) architecture. Of the methods tested in this study, the GA-SSS-PSO approach was the only one that could optimize all functions to a desirable level. To demonstrate the algorithm's applicability, three optimization tasks of different categories commonly faced in the field of chemometrics were subjected to optimization by GA-SSS-PSO and results indicated that the novel hybrid algorithm provided robust performance for both theoretical and real life problems and may be suited as general-purpose optimizer for medium-sized optimization tasks.",space
10.1016/j.bica.2013.07.009,Journal,Biologically Inspired Cognitive Architectures,scopus,2013-10-01,sciencedirect,Emotional biologically inspired cognitive architecture,https://api.elsevier.com/content/abstract/scopus_id/84883206490,"Human-like artificial emotional intelligence is vital for integration of future robots into the human society. This work introduces a general framework for representation and processing of emotional contents in a cognitive architecture, called “emotional biologically inspired cognitive architecture” (eBICA). Unlike in previous attempts, in this framework emotional elements are added virtually to all cognitive representations and processes by modifying the main building blocks of the prototype architectures. The key elements are appraisals associated as attributes with schemas and mental states, moral schemas that control patterns of appraisals and represent social emotions, and semantic spaces that give values to appraisals. Proposed principles are tested in an experiment involving human subjects and virtual agents, based on a simple paradigm in imaginary virtual world. It is shown that with moral schemas, but probably not without them, eBICA can account for human behavior in the selected paradigm. The model sheds light on clustering of social emotions and allows for their elegant mathematical description. The new framework will be suitable for implementation of believable emotional intelligence in artifacts, necessary for emotionally informed behavior, collaboration of virtual partners with humans, and self-regulated learning of virtual agents.",space
10.1016/j.neunet.2013.05.010,Journal,Neural Networks,scopus,2013-10-01,sciencedirect,Complexity-reduced implementations of complete and null-space-based linear discriminant analysis,https://api.elsevier.com/content/abstract/scopus_id/84879563722,"Dimensionality reduction has become an important data preprocessing step in a lot of applications. Linear discriminant analysis (LDA) is one of the most well-known dimensionality reduction methods. However, the classical LDA cannot be used directly in the small sample size (SSS) problem where the within-class scatter matrix is singular. In the past, many generalized LDA methods has been reported to address the SSS problem. Among these methods, complete linear discriminant analysis (CLDA) and null-space-based LDA (NLDA) provide good performances. The existing implementations of CLDA are computationally expensive. In this paper, we propose a new and fast implementation of CLDA. Our proposed implementation of CLDA, which is the most efficient one, is equivalent to the existing implementations of CLDA in theory. Since CLDA is an extension of null-space-based LDA (NLDA), our implementation of CLDA also provides a fast implementation of NLDA. Experiments on some real-world data sets demonstrate the effectiveness of our proposed new CLDA and NLDA algorithms.",space
10.1016/j.asoc.2013.07.009,Journal,Applied Soft Computing Journal,scopus,2013-08-27,sciencedirect,Synergizing fitness learning with proximity-based food source selection in artificial bee colony algorithm for numerical optimization,https://api.elsevier.com/content/abstract/scopus_id/84886723307,"Evolutionary computation (EC) paradigm has undergone extensions in the recent years diverging from the natural process of genetic evolution to the simulation of natural life processes exhibited by the living organisms. Bee colonies exemplify a high level of intrinsic interdependence and co-ordination among its members, and algorithms inspired from the bee colonies have gained recent prominence in the field of swarm based metaheuristics. The artificial bee colony (ABC) algorithm was recently developed, by simulating the minimalistic foraging model of honeybees in search of food sources, for solving real-parameter, non-convex, and non-smooth optimization problems. The single parameter perturbation in classical ABC resulted in fairly commendable performance for simple problems without epistasis of variables (separable). However, it suffered from narrow search zone and slow convergence which eventually led to poor exploitation tendency. Even with the increase in dimensionality, a significant deterioration was observed in the ability of ABC to locate the optimum in a huge search volume. Some of the probable shortcomings in the basic ABC approach, as observed, are the single parameter perturbation instead of a multiple one, ignoring the fitness to reward ratio while selecting food sites, and most importantly the absence of environmental factors in the algorithm design. Research has shown that spatial environmental factors play a crucial role in insect locomotion and foragers seem to learn the direction to be undertaken based on the relative analysis of its proximal surroundings. Most importantly, the mapping of the forager locomotion from three dimensional search spaces to a multidimensional solution space calls forth the implementation of multiple modification schemes. Based on the fundamental observation pertaining to the dynamics of ABC, this article proposes an improved variant of ABC aimed at improving the optimizing ability of the algorithm over an extended set of problems. The hybridization of the proposed fitness learning mechanism with a weighted selection scheme and proximity based stimuli helps to achieve a fine blending of explorative and exploitative behaviour by enhancing both local and global searching ability of the algorithm. This enhances the ability of the swarm agents to detect optimal regions in the unexplored fitness basins. With respect to its immediate surroundings, a proximity based component is added to the normal positional modification of the onlookers and is enacted through an improved probability selection scheme that takes the T/E (total reward to distance) ratio metric into account. The biologically-motivated, hybridized variant of ABC achieves a statistically superior performance on majority of the tested benchmark instances, as compared to some of the most prominent state-of-the-art algorithms, as is demonstrated through a detailed experimental evaluation and verified statistically.",space
10.1016/j.eswa.2012.12.080,Journal,Expert Systems with Applications,scopus,2013-08-01,sciencedirect,A decision support system for optimal deployment of sonobuoy networks based on sea current forecasts and multi-objective evolutionary optimization,https://api.elsevier.com/content/abstract/scopus_id/84875365598,"A decision support system for the optimal deployment of drifting acoustic sensor networks for cooperative track detection in underwater surveillance applications is proposed and tested on a simulated scenario. The system integrates sea water current forecasts, sensor range models and simple drifting buoy kinematic models to predict sensor positions and temporal network performance. A multi-objective genetic optimization algorithm is used for searching a set of Pareto optimal deployment solutions (i.e. the initial position of drifting sonobuoys of the network) by simultaneously optimizing two quality of service metrics: the temporal mean of the network area coverage and the tracking coverage. The solutions found after optimization, which represent different efficient tradeoffs between the two metrics, can be conveniently evaluated by the mission planner in order to choose the solution with the desired compromise between the two conflicting objectives. Sensitivity analysis through the Unscented Transform is also performed in order to test the robustness of the solutions with respect to network parameters and environmental uncertainty. Results on a simulated scenario making use of real probabilistic sea water current forecasts are provided showing the effectiveness of the proposed approach. Future work is envisioned to make the tool fully operational and ready to use in real scenarios.",space
10.1016/j.sigpro.2012.10.020,Journal,Signal Processing,scopus,2013-06-01,sciencedirect,3D CBIR with sparse coding for image-guided neurosurgery,https://api.elsevier.com/content/abstract/scopus_id/84875248258,"This research takes an application-specific approach to investigate, extend and implement the state of the art in the fields of both visual information retrieval and machine learning, bridging the gap between theoretical models and real world applications. During an image-guided neurosurgery, path planning remains the foremost and hence the most important step to perform an operation and ensures the maximum resection of an intended target and minimum sacrifice of health tissues. In this investigation, the technique of content-based image retrieval (CBIR) coupled with machine learning algorithms are exploited in designing a computer aided path planning system (CAP) to assist junior doctors in planning surgical paths while sustaining the highest precision. Specifically, after evaluation of approaches of sparse coding and K-means in constructing a codebook, the model of sparse codes of 3D SIFT has been furthered and thereafter employed for retrieving, The novelty of this work lies in the fact that not only the existing algorithms for 2D images have been successfully extended into 3D space, leading to promising results, but also the application of CBIR that is mainly in a research realm, to a clinical sector can be achieved by the integration with machine learning techniques. Comparison with the other four popular existing methods is also conducted, which demonstrates that with the implementation of sparse coding, all methods give better retrieval results than without while constituting the codebook, implying the significant contribution of machine learning techniques.",space
10.1016/j.jhydrol.2012.10.019,Journal,Journal of Hydrology,scopus,2013-01-07,sciencedirect,A comparison of methods to avoid overfitting in neural networks training in the case of catchment runoff modelling,https://api.elsevier.com/content/abstract/scopus_id/84871010255,"Artificial neural networks (ANNs) becomes very popular tool in hydrology, especially in rainfall–runoff modelling. However, a number of issues should be addressed to apply this technique to a particular problem in an efficient way, including selection of network type, its architecture, proper optimization algorithm and a method to deal with overfitting of the data. The present paper addresses the last, rarely considered issue, namely comparison of methods to prevent multi-layer perceptron neural networks from overfitting of the training data in the case of daily catchment runoff modelling. Among a number of methods to avoid overfitting the early stopping, the noise injection and the weight decay have been known for about two decades, however only the first one is frequently applied in practice. Recently a new methodology called optimized approximation algorithm has been proposed in the literature.
                  Overfitting of the training data leads to deterioration of generalization properties of the model and results in its untrustworthy performance when applied to novel measurements. Hence the purpose of the methods to avoid overfitting is somehow contradictory to the goal of optimization algorithms, which aims at finding the best possible solution in parameter space according to pre-defined objective function and available data. Moreover, different optimization algorithms may perform better for simpler or larger ANN architectures. This suggest the importance of proper coupling of different optimization algorithms, ANN architectures and methods to avoid overfitting of real-world data – an issue that is also studied in details in the present paper.
                  The study is performed for Annapolis River catchment, characterized by significant seasonal changes in runoff, rapid floods during winter and spring, moderately dry summers, severe winters with snowfall, snow melting, frequent freeze and thaw, and presence of river ice. The present paper shows that the elaborated noise injection method may prevent overfitting slightly better than the most popular early stopping approach. However, the implementation of noise injection to real-world problems is difficult and the final model performance depends significantly on a number of very technical details, what somehow limits its practical applicability. It is shown that optimized approximation algorithm does not improve the results obtained by older methods, possibly due to over-simplified criterion of stopping the algorithm. Extensive calculations reveal that Evolutionary Computation-based algorithm performs better for simpler ANN architectures, whereas classical gradient-based Levenberg–Marquardt algorithm is able to benefit from additional input variables, representing precipitation and snow cover from one more previous day, and from more complicated ANN architectures. This confirms that the curse of dimensionality has severe impact on the performance of Evolutionary Computing methods.",space
10.1016/j.procs.2013.05.187,Conference Proceeding,Procedia Computer Science,scopus,2013-01-01,sciencedirect,Comparing support vector machines and artificial neural networks in the recognition of steering angle for driving of mobile robots through paths in plantations,https://api.elsevier.com/content/abstract/scopus_id/84896966222,"The use of mobile robots turns out to be interesting in activities where the action of human specialist is difficult or dangerous. Mobile robots are often used for the exploration in areas of difficult access, such as rescue operations and space missions, to avoid human experts exposition to risky situations. Mobile robots are also used in agriculture for planting tasks as well as for keeping the application of pesticides within minimal amounts to mitigate environmental pollution. In this paper we present the development of a system to control the navigation of an autonomous mobile robot through tracks in plantations. Track images are used to control robot direction by pre-processing them to extract image features. Such features are then submitted to a support vector machine and an artificial neural network in order to find out the most appropriate route. A comparison of the two approaches was performed to ascertain the one presenting the best outcome. The overall goal of the project to which this work is connected is to develop a real time robot control system to be embedded into a hardware platform. In this paper we report the software implementation of a support vector machine and of an artificial neural network, which so far presented respectively around 93% and 90% accuracy in predicting the appropriate route.",space
10.3182/20130902-3-CN-3020.00124,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2013-01-01,sciencedirect,Adaptive dynamic programming for solving non-zero-sum differential games,https://api.elsevier.com/content/abstract/scopus_id/84896325180,"In this paper, a novel adaptive dynamic programming algorithm based on policy iteration is developed to solve online multi-player non-zero-sum differential game for continuous-time nonlinear systems. This algorithm is mathematically equivalent to the quasi-Newton's iteration in a Banach space. The implementation using neural networks is given, where a critic neural network is used to learn its value function, and an action neural network sharing the same parameters with the corresponding critic neural network is used to learn its optimal control policy for each player. All the critic and action neural networks are updated online in real-time and continuously. A simulation example is presented to demonstrate the effectiveness of the developed scheme.",space
10.1016/j.pmcj.2012.11.003,Journal,Pervasive and Mobile Computing,scopus,2013-01-01,sciencedirect,Enabling real-time city sensing with kernel stream oracles and MapReduce,https://api.elsevier.com/content/abstract/scopus_id/84883463862,"An algorithmic architecture for kernel-based modelling of data streams from city sensing infrastructures is introduced. It is both applicable for pre-installed, moving and extemporaneous sensors, including the “citizen-as-a-sensor” view on user-generated data. The approach is centred around a kernel dictionary implementing a general hypothesis space which is updated incrementally, accounting for memory and processing capacity limitations. It is general for both kernel-based classification and regression. An extension to area-to-point modelling is introduced to account for the data aggregated over a spatial region. A distributed implementation realised under the Map-Reduce framework is presented to train an ensemble of sequential kernel learners.",space
10.1016/j.patcog.2012.07.018,Journal,Pattern Recognition,scopus,2013-01-01,sciencedirect,Self-taught dimensionality reduction on the high-dimensional small-sized data,https://api.elsevier.com/content/abstract/scopus_id/84866033003,"To build an effective dimensionality reduction model usually requires sufficient data. Otherwise, traditional dimensionality reduction methods might be less effective. However, sufficient data cannot always be guaranteed in real applications. In this paper we focus on performing unsupervised dimensionality reduction on the high-dimensional and small-sized data, in which the dimensionality of target data is high and the number of target data is small. To handle the problem, we propose a novel Self-taught Dimensionality Reduction (STDR) approach, which is able to transfer external knowledge (or information) from freely available external (or auxiliary) data to the high-dimensional and small-sized target data. The proposed STDR consists of three steps: First, the bases are learnt from sufficient external data, which might come from the same “type” or “modality” of target data. The bases are the common part between external data and target data, i.e., the external knowledge (or information). Second, target data are reconstructed by the learnt bases by proposing a novel joint graph sparse coding model, which not only provides robust reconstruction ability but also preserves the local structures amongst target data in the original space. This process transfers the external knowledge (i.e., the learnt bases) to target data. Moreover, the proposed solver to the proposed model is theoretically guaranteed that the objective function of the proposed model converges to the global optimum. After this, target data are mapped into the learnt basis space, and are sparsely represented by the bases, i.e., represented by parts of the bases. Third, the sparse features (that is, the rows with zero (or small) values) of the new representations of target data are deleted for achieving the effectiveness and the efficiency. That is, this step performs feature selection on the new representations of target data. Finally, experimental results at various types of datasets show the proposed STDR outperforms the state-of-the-art algorithms in terms of k-means clustering performance.",space
10.1016/j.camwa.2012.01.079,Journal,Computers and Mathematics with Applications,scopus,2012-12-01,sciencedirect,Machine learning in agent-based stochastic simulation: Inferential theory and evaluation in transportation logistics,https://api.elsevier.com/content/abstract/scopus_id/84870236735,"Multiagent-based simulation is an approach to realize stochastic simulation where both the behavior of the modeled multiagent system and dynamic aspects of its environment are implemented with autonomous agents. Such simulation provides an ideal environment for intelligent agents to learn to perform their tasks before being deployed in a real-world environment. The presented research investigates theoretical and practical aspects of learning by autonomous agents within stochastic agent-based simulation. The theoretical work is based on the Inferential Theory of Learning, which describes learning processes from the perspective of a learner’s goal as a search through knowledge space. The theory is extended for approximate and probabilistic learning to account for the situations encountered when learning in stochastic environments. Practical aspects are exemplified by two use cases in autonomous logistics: learning predictive models for environment conditions in the future, and learning in the context of evolutionary plan optimization.",space
10.1016/j.asoc.2012.01.014,Journal,Applied Soft Computing Journal,scopus,2012-12-01,sciencedirect,Autonomous real-time landing site selection for Venus and Titan using Evolutionary Fuzzy Cognitive Maps,https://api.elsevier.com/content/abstract/scopus_id/84869086510,"Future science-driven landing missions, conceived to collect in situ data on regions of planetary bodies that have the highest potential to yield important scientific discoveries, will require a higher degree of autonomy. The latter includes the ability of the spacecraft to autonomously select the landing site using real-time data acquired during the descent phase. This paper presents the development of an Evolutionary Fuzzy Cognitive Map (E-FCM) model that implements an artificial intelligence system capable of autonomously selecting a landing site with the highest potential for scientific discoveries constrained by the requirement of soft landing in a region with safe terrains. The proposed E-FCM evolves its internal states and interconnections as a function of real-time data collected during the descent phase, therefore improving the decision process as more accurate information becomes available. The E-FCM is constructed using knowledge accumulated by planetary experts and it is tested on scenarios that simulate the decision process during the descent phase toward the Hyndla Regio on Venus. The E-FCM is shown to quickly reach conclusions that are consistent with what would be the choice of a planetary expert if the scientist were presented with the same information. The proposed methodology is fast and efficient and may be suitable for on-board spacecraft implementation and real-time decision making during the course of robotic exploration of the Solar System.",space
10.1016/j.actaastro.2012.06.003,Journal,Acta Astronautica,scopus,2012-11-01,sciencedirect,"Robotic Mission to Mars: Hands-on, minds-on, web-based learning",https://api.elsevier.com/content/abstract/scopus_id/84865224510,"Problem-based learning has been demonstrated as an effective methodology for developing analytical skills and critical thinking. The use of scenario-based learning incorporates problem-based learning whilst encouraging students to collaborate with their colleagues and dynamically adapt to their environment. This increased interaction stimulates a deeper understanding and the generation of new knowledge. The Victorian Space Science Education Centre (VSSEC) uses scenario-based learning in its Mission to Mars, Mission to the Orbiting Space Laboratory and Primary Expedition to the M.A.R.S. Base programs. These programs utilize methodologies such as hands-on applications, immersive-learning, integrated technologies, critical thinking and mentoring to engage students in Science, Technology, Engineering and Mathematics (STEM) and highlight potential career paths in science and engineering. The immersive nature of the programs demands specialist environments such as a simulated Mars environment, Mission Control and Space Laboratory, thus restricting these programs to a physical location and limiting student access to the programs. To move beyond these limitations, VSSEC worked with its university partners to develop a web-based mission that delivered the benefits of scenario-based learning within a school environment. The Robotic Mission to Mars allows students to remotely control a real rover, developed by the Australian Centre for Field Robotics (ACFR), on the VSSEC Mars surface. After completing a pre-mission training program and site selection activity, students take on the roles of scientists and engineers in Mission Control to complete a mission and collect data for further analysis. Mission Control is established using software developed by the ACRI Games Technology Lab at La Trobe University using the principles of serious gaming. The software allows students to control the rover, monitor its systems and collect scientific data for analysis. This program encourages students to work scientifically and explores the interaction between scientists and engineers. This paper presents the development of the program, including the involvement of university students in the development of the rover, the software, and the collation of the scientific data. It also presents the results of the trial phase of this program including the impact on student engagement and learning outcomes.",space
10.1016/j.eswa.2012.04.074,Journal,Expert Systems with Applications,scopus,2012-10-15,sciencedirect,Shared decision support system on dental restoration,https://api.elsevier.com/content/abstract/scopus_id/84861811784,"Shared decision making (SDM) is an approach in which doctor–patient communication regarding available evidence and patient preferences is facilitated to enable the patient to participate in treatment decisions. SDM affords not only the inclusion of the ethical diversities involved in patient-centered care, but also the quality improvements in decision-making process. Though SDM has been studied extensively, there have been few practical implementations in real clinical environments. In this paper, we propose a shared decision-making system with its focus on dental restorative treatment planning. In our system, restorative treatment alternatives for SDM were generated by employing an ontology that had captured the clinical knowledge required for treatments. We considered patient preferences for treatment as an important support for mutual agreements between the patient and the doctor on healthcare decisions. We constructed a consistent and robust hierarchy of preferences using the analytic hierarchy process (AHP) method, to help determine treatment priorities. On the basis of our proposed system, we developed a Web-based application for the visualization of evidence-based treatment recommendations with preference-based weights. We tested our system using a scenario to illustrate how doctors and patients can make shared decisions. The application is of high value in supporting SDM between doctors and patients, and expedites effective treatments and enhanced patient satisfaction.",space
10.1016/j.ijepes.2012.02.008,Journal,International Journal of Electrical Power and Energy Systems,scopus,2012-09-01,sciencedirect,Assessing the relevance of load profiling information in electrical load forecasting based on neural network models,https://api.elsevier.com/content/abstract/scopus_id/84860742606,"The article is focused on evaluating the relevance of load profiling information in electrical load forecasting, using neural networks as the forecasting methodology. Different models, with and without load profiling information, were tested and compared, and, the importance of the different inputs was investigated, using the concept of partial derivatives to understand the relevance of including this type of data in the input space. The paper presents a model for the day ahead load profile prediction for an area with many consumers. The results were analyzed with a simulated load diagram (to illustrate a distribution feeder) and also with a specific output of a 60/15kV real distribution substation that feeds a small town. The adopted methodology was successfully implemented and resulted in reducing the mean absolute percentage error between 0.5% and 16%, depending on the nature of the concurrent methodology used and the forecasted day, with a major benefit regarding the treatment of special days (holidays). The results illustrate an interesting potential for the use of the load profiling information in forecasting.",space
10.1016/j.optcom.2012.05.037,Journal,Optics Communications,scopus,2012-08-15,sciencedirect,Semi-supervised kernel learning based optical image recognition,https://api.elsevier.com/content/abstract/scopus_id/84863471802,"This paper is to propose semi-supervised kernel learning based optical image recognition, called Semi-supervised Graph-based Global and Local Preserving Projection (SGGLPP) through integrating graph construction with the specific DR process into one unified framework. SGGLPP preserves not only the positive and negative constraints but also the local and global structure of the data in the low dimensional space. In SGGLPP, the intrinsic and cost graphs are constructed using the positive and negative constraints from side-information and k nearest neighbor criterion from unlabeled samples. Moreover, kernel trick is applied to extend SGGLPP called KSGGLPP by on the performance of nonlinear feature extraction. Experiments are implemented on UCI database and two real image databases to testify the feasibility and performance of the proposed algorithm.",space
10.1016/S1000-9361(11)60423-8,Journal,Chinese Journal of Aeronautics,scopus,2012-08-01,sciencedirect,Approximate maximum likelihood algorithm for moving source localization using TDOA and FDOA measurements,https://api.elsevier.com/content/abstract/scopus_id/84865610506,"A closed-form approximate maximum likelihood (AML) algorithm for estimating the position and velocity of a moving source is proposed by utilizing the time difference of arrival (TDOA) and frequency difference of arrival (FDOA) measurements of a signal received at a number of receivers. The maximum likelihood (ML) technique is a powerful tool to solve this problem. But a direct approach that uses the ML estimator to solve the localization problem is exhaustive search in the solution space, and it is very computationally expensive, and prohibits real-time processing. On the basis of ML function, a closed-form approximate solution to the ML equations can be obtained, which can allow real-time implementation as well as global convergence. Simulation results show that the proposed estimator achieves better performance than the two-step weighted least squares (WLS) approach, which makes it possible to attain the Cramér-Rao lower bound (CRLB) at a sufficiently high noise level before the threshold effect occurs.",space
10.1016/j.neunet.2012.02.032,Journal,Neural Networks,scopus,2012-08-01,sciencedirect,Autonomous Growing Neural Gas for applications with time constraint: Optimal parameter estimation,https://api.elsevier.com/content/abstract/scopus_id/84861761321,"This paper aims to address the ability of self-organizing neural network models to manage real-time applications. Specifically, we introduce fAGNG (fast Autonomous Growing Neural Gas), a modified learning algorithm for the incremental model Growing Neural Gas (GNG) network. The Growing Neural Gas network with its attributes of growth, flexibility, rapid adaptation, and excellent quality of representation of the input space makes it a suitable model for real time applications. However, under time constraints GNG fails to produce the optimal topological map for any input data set. In contrast to existing algorithms, the proposed fAGNG algorithm introduces multiple neurons per iteration. The number of neurons inserted and input data generated is controlled autonomous and dynamically based on a priory or online learnt model. A detailed study of the topological preservation and quality of representation depending on the neural network parameter selection has been developed to find the best alternatives to represent different linear and non-linear input spaces under time restrictions or specific quality of representation requirements.",space
10.1016/j.patrec.2012.01.015,Journal,Pattern Recognition Letters,scopus,2012-07-01,sciencedirect,A cluster-assumption based batch mode active learning technique,https://api.elsevier.com/content/abstract/scopus_id/84859302340,"In this paper, we propose an active learning technique for solving multiclass problems with support vector machine (SVM) classifiers. The technique is based on both uncertainty and diversity criteria. The uncertainty criterion is implemented by analyzing the one-dimensional output space of the SVM classifier. A simple histogram thresholding algorithm is used to find out the low density region in the SVM output space to identify the most uncertain samples. Then the diversity criterion exploits the kernel k-means clustering algorithm to select uncorrelated informative samples among the selected uncertain samples. To assess the effectiveness of the proposed method we compared it with other batch mode active learning techniques presented in the literature using one toy data set and three real data sets. Experimental results confirmed that the proposed technique provided a very good tradeoff among robustness to biased initial training samples, classification accuracy, computational complexity, and number of new labeled samples necessary to reach the convergence.",space
10.1016/j.jtice.2011.11.006,Journal,Journal of the Taiwan Institute of Chemical Engineers,scopus,2012-05-01,sciencedirect,"Mathematical modeling, optimal design and control of an SCR reactor for NOx removal",https://api.elsevier.com/content/abstract/scopus_id/84860316497,"The elimination of nitrogen oxides (NOx) is an important issue for global environment. This paper deals with the model development, optimal design and feedback control of an SCR (selective catalytic reduction) reactor for NOx removal. A 3D dynamic simulation model for use to investigate the reaction behavior and the transport phenomena in the catalytic filter of the SCR reactor is proposed. To estimate the model's kinetic parameters from experimental data, an optimization technique that integrates Taguchi method, a real-coded genetic algorithm and a neural network auxiliary model is developed. With the proposed dynamic model, we investigated the effects of the key parameters, such as the gas hourly space velocity, operating temperature and the amount of ammonium used, on the NOx conversion and NH3 slip phenomena. To improve the NOx abatement performance, the proposed optimization technique is then applied to search for a set of best operation conditions for the SCR reactor. The obtained results indicate that the optimized SCR can achieve the NOx reduction rate up to 99.93%, which is over 9% better in performance than the previously reported one in the literature. Besides, the optimal operating temperature is considerably lower and the emission of ammonium from the reactor is insignificant. Compared with conventional designs, the proposed design is much better in energy savings and is environment-friendly. To attenuate the negative effects of environmental disturbances on reactor's performance, we implemented a direct adaptive control strategy to the SCR reactor. The stability of the control system is theoretically guaranteed with a Lyapunov-based approach. Extensive simulation results show that the learning-type, nonlinear control strategy presents significantly much better NOx reduction performance than a conventional IMC-PI controller, especially when facing process uncertainties and disturbances.",space
10.1016/j.websem.2011.11.003,Journal,Journal of Web Semantics,scopus,2012-03-01,sciencedirect,Induction of robust classifiers for web ontologies through kernel machines,https://api.elsevier.com/content/abstract/scopus_id/84857784576,"The paper focuses on the task of approximate classification of semantically annotated individual resources in ontological knowledge bases. The method is based on classification models built through kernel methods, a well-known class of effective statistical learning algorithms. Kernel functions encode a notion of similarity among elements of some input space. The definition of a family of parametric language-independent kernel functions for individuals occurring in an ontology allows the application of these statistical learning methods on Semantic Web knowledge bases. The classification models induced by kernel methods offer an alternative way to classify individuals with respect to the typical exact and approximate deductive reasoning procedures. The proposed statistical setting enables further inductive approaches to a variety of other tasks that can better cope with the inherent incompleteness of the knowledge bases in the Semantic Web and with their potential incoherence due to their distributed nature. The effectiveness of the proposed method is empirically proved through experiments on the task of approximate classification with real ontologies collected from standard repositories.",space
10.1016/j.patcog.2011.08.005,Journal,Pattern Recognition,scopus,2012-03-01,sciencedirect,Accurate real-time neural disparity MAP estimation with FPGA,https://api.elsevier.com/content/abstract/scopus_id/80055025069,"We propose in this paper a new method for real-time dense disparity map computing using a stereo pair of rectified images. Based on the neural network and Disparity Space Image (DSI) data structure, the disparity map computing consists of two main steps: initial disparity map estimation by combining the neuronal network and the DSI structure, and its refinement. Four improvements are introduced so that an accurate and fast result will be reached. The first one concerns the proposition of a new strategy in order to optimize the computation time of the initial disparity map. In the second one, a specific treatment is proposed in order to obtain more accurate disparity for the neighboring pixels to boundaries. The third one, it concerns the pixel similarity measure for matching score computation and it consists of using in addition to the traditional pixel intensities, the magnitude and orientation of the gradients providing more accuracy. Finally, the processing time of the method has been decreased consequently to our implementation of some critical steps on FPGAs. Experimental results on real datasets are conducted and a comparative evaluation of the obtained results relative to the state-of-art methods is presented.",space
10.1016/j.neuroimage.2011.03.052,Journal,NeuroImage,scopus,2012-01-02,sciencedirect,Effects of training strategies implemented in a complex videogame on functional connectivity of attentional networks,https://api.elsevier.com/content/abstract/scopus_id/80054109529,"We used the Space Fortress videogame, originally developed by cognitive psychologists to study skill acquisition, as a platform to examine learning-induced plasticity of interacting brain networks. Novice videogame players learned Space Fortress using one of two training strategies: (a) focus on all aspects of the game during learning (fixed priority), or (b) focus on improving separate game components in the context of the whole game (variable priority). Participants were scanned during game play using functional magnetic resonance imaging (fMRI), both before and after 20h of training. As expected, variable priority training enhanced learning, particularly for individuals who initially performed poorly. Functional connectivity analysis revealed changes in brain network interaction reflective of more flexible skill learning and retrieval with variable priority training, compared to procedural learning and skill implementation with fixed priority training. These results provide the first evidence for differences in the interaction of large-scale brain networks when learning with different training strategies. Our approach and findings also provide a foundation for exploring the brain plasticity involved in transfer of trained abilities to novel real-world tasks such as driving, sport, or neurorehabilitation.",space
10.3182/20120403-3-DE-3010.00010,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,RodosVisor - An object-oriented and customizable hypervisor: The CPU virtualization,https://api.elsevier.com/content/abstract/scopus_id/84866091325,"RodosVisor is an object-oriented and bare-metal virtual machine monitor (VMM) or hypervisor designed for the aerospace industry, mainly to provide time and spatial separation to the NetworkCentric core avionics machine, Montenegro and Dittrich (2009). The NetworkCentric core avionics machine consists of several harmonized components working together to implement dependable computing in a simple way, with computing units managed by the local real-time operating system RODOS. To support partitioned software architectures such as AIR, Rufino et al. (2009), and MILS, DeLong, R. (2007), RodosVisor adapted the Popek and Goldberg's fidelity, efficiency and resource control virtualization requirements, Popek and Goldberg (1974), to the space application domain by extending them with extra ones, like timing determinism, reactivity and improved dependability. Another distinctive RodosVisor feature is the customized design based on generative programming techniques, such as aspect oriented programming and template meta-programming.",space
10.1016/B978-0-444-59520-1.50104-4,Book Series,Computer Aided Chemical Engineering,scopus,2012-01-01,sciencedirect,Intelligent Automation Platform for Bioprocess Development,https://api.elsevier.com/content/abstract/scopus_id/84862870640,"High throughput technology has been increasingly adapted for drug screen and bioprocess development, due to the small amount of processing materials and reagents required and parallel experiment execution. It allows a wide design space to be explored in order to discover novel bioprocess solutions. Currently, the high throughput experiments for bioprocess development are implemented in a sequential fashion in which liquid handling system will perform the web lab experiment to prepare the samples; standalone analysis devices detect the data such as protein concentration; and specific software is used to realise the data analysis for process design or further experimentation.
                  The aim of this paper is to show how the efficiency of the high throughput bioprocess development approach can be enhanced by creating an intelligent automation platform that systematically drives liquid handling system, analysis devices and data analysis to perform a closed-loop learning. The first generation prototype has been established which consists of three parts: automated devices, design algorithms and database. In order to prove the concept of prototype, both simulation and real experiments studies have been established. In this case study, the platform is used to investigate the solubility of lysozyme at various ion strengths and pH values. Tecan liquid handling system for experimentation as well as buffer preparation and a plate reader for uv absorption measurement to determine protein concentration were used as the automated devices. The simplex search algorithm and artificial neural network modelling were utilised as design algorithm to iteratively select the experiments to execute and determine the optimal design solution. An entity-relationship database with Tecan system configuration information and experimental data was established. The results demonstrate this integrated approach can implement experiments and data analysis automatically to provide specific bioprocess design solutions in a closed loop strategy at first time. It is a promising approach that may significant increase the level of lab automation to release the engineer from the labour intensive R&D activities and provides the base for sophisticated artificial intelligent learning in the future.",space
10.1016/j.autcon.2011.05.018,Journal,Automation in Construction,scopus,2012-01-01,sciencedirect,Simulation and analytical techniques for construction resource planning and scheduling,https://api.elsevier.com/content/abstract/scopus_id/81355138778,"To date, few construction methods or models in the literature have discussed about helping the project managers decide the near-optimum distributions of manpower, material, equipment and space according to their project objectives and project constraints. Thus, the traditional scheduling methods or models often result in a “seat-of-the-pants” style of management, rather than decision making based on an analysis of real data. This paper presents an intelligent scheduling system (ISS) that can help the project managers to find the near-optimum schedule plan according to their project objectives and project constraints. ISS uses simulation techniques to distribute resources and assign different levels of priorities to different activities in each simulation cycle to find the near-optimal solution. ISS considers and integrates most of the important construction factors (schedule, cost, space, manpower, equipment and material) simultaneously in a unified environment, which makes the resulting schedule that will be closer to optimal. Furthermore, ISS allows for what-if analyses of possible scenarios, and schedule adjustments based on unforeseen conditions (change orders, late material delivery, etc.). Finally, two sample applications and one real-world construction project are utilized to illustrate and compare the effectiveness of ISS with two widely used software packages, Primavera Project Planner and Microsoft Project.",space
10.1016/j.actaastro.2011.05.027,Journal,Acta Astronautica,scopus,2011-09-01,sciencedirect,Dynamic fiber Bragg gratings based health monitoring system of composite aerospace structures,https://api.elsevier.com/content/abstract/scopus_id/79960979180,"The main purpose of the current work is to develop a new system for structural health monitoring of composite aerospace structures based on real-time dynamic measurements, in order to identify the structural state condition. Long-gauge Fibre Bragg Grating (FBG) optical sensors were used for monitoring the dynamic response of the composite structure. The algorithm that was developed for structural damage detection utilizes the collected dynamic response data, analyzes them in various ways and through an artificial neural network identifies the damage state and its location. Damage was simulated by slightly varying locally the mass of the structure (by adding a known mass) at different zones of the structure. Lumped masses in different locations upon the structure alter the eigen-frequencies in a way similar to actual damage. The structural dynamic behaviour has been numerically simulated and experimentally verified by means of modal testing on two different composite aerospace structures.
                  Advanced digital signal processing techniques, e.g. the wavelet transform (WT), were used for the analysis of the dynamic response for feature extraction. WT's capability of separating the different frequency components in the time domain without loosing frequency information makes it a versatile tool for demanding signal processing applications. The use of WT is also suggested by the no-stationary nature of dynamic response signals and the opportunity of evaluating the temporal evolution of their frequency contents. Feature extraction is the first step of the procedure. The extracted features are effective indices of damage size and location. The classification step comprises of a feed-forward back propagation network, whose output determines the simulated damage location. Finally, dedicated training and validation activities were carried out by means of numerical simulations and experimental procedures.
                  Experimental validation was performed initially on a flat stiffened panel, representing a section of a typical aeronautical structure, manufactured and tested in the lab and, as a second step, on a scaled up space oriented structure, which is a composite honeycomb plate, used as a deployment base for antenna arrays. An integrated FBG sensor network, based on the advantage of multiplexing, was mounted on both structures and different excitation positions and boundary conditions were used. The analysis of operational dynamic responses was employed to identify both the damage and its position. The system that was designed and tested initially on the thin composite panel, was successfully validated on the larger honeycomb structure. Numerical simulation of both structures was used as a support tool at all the steps of the work providing among others the location of the optical sensors used. The proposed work will be the base for the whole system qualification and validation on an antenna reflector in future work.",space
10.1016/j.asoc.2011.01.045,Journal,Applied Soft Computing Journal,scopus,2011-09-01,sciencedirect,Knowledge of opposite actions for reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/79956159518,"Abstract
                  Reinforcement learning (RL) is one of the machine intelligence techniques with several characteristics that make it suitable for solving real-world problems. However, RL agents generally face a very large state space in many applications. They must take actions in every state many times to find the optimal policy. In this work, a special type of knowledge about actions is employed to improve the performance of the off-policy, incremental, and model-free reinforcement learning with discrete state and action space. One of the components of RL agent is the action. For each action, its associate opposite action is defined. The actions and opposite actions are implemented in the framework of reinforcement learning to update the value function resulting in a faster convergence. The effects of opposite action on some of the reinforcement learning algorithms are investigated.",space
10.1016/j.jvlc.2011.03.004,Journal,Journal of Visual Languages and Computing,scopus,2011-08-01,sciencedirect,An alternative map of the United States based on an n-dimensional model of geographic space,https://api.elsevier.com/content/abstract/scopus_id/79960409782,"Geographic features have traditionally been visualized with fairly high amount of geometric detail, while relationships among these features in attribute space have been represented at a much coarser resolution. This limits our ability to understand complex high-dimensional relationships and structures existing in attribute space. In this paper, we present an alternative approach aimed at creating a high-resolution representation of geographic features with the help of a self-organizing map (SOM) consisting of a large number of neurons. In a proof-of-concept implementation, we spatialize 200,000+ U.S. Census block groups using a SOM consisting of 250,000 neurons. The geographic attributes considered in this study reflect a more holistic representation of geographic reality than in previous studies. The study includes 69 attributes regarding population statistics, land use/land cover, climate, geology, topography, and soils. This diversity of attributes is informed by our desire to build a comprehensive two-dimensional base map of n-dimensional geographic space. The paper discusses how standard GIS methods and neural network processing are combined towards the creation of an alternative map of the United States.",space
10.1016/j.conengprac.2011.03.002,Journal,Control Engineering Practice,scopus,2011-07-01,sciencedirect,Survey and application of sensor fault detection and isolation schemes,https://api.elsevier.com/content/abstract/scopus_id/79957944993,"Model-based sensor fault detection, isolation and accommodation (SFDIA) is a direction of development in particular with UAVs where sensor redundancy may not be an option due to weight, cost and space implications. SFDIA via neural networks (NNs) have been proposed over the years due to their nonlinear structures and online learning capabilities. The majority of papers tend to consider single sensor faults. While useful, this assumption can limit application to real systems where sensor faults can occur simultaneously or consecutively. In this paper we consider the latter scenario, where it is assumed that a 1s time gap is present between consecutive faults. Furthermore few applications have considered fixed-wing UAVs where full autonomy is most needed. In this paper an EMRAN RBF NN is chosen for modelling purposes due to its ability to adapt well to nonlinear environments while maintaining high computational speeds. A nonlinear UAV model is used for demonstration, where decoupled longitudinal motion is considered. System and measurement noise is also included in the UAV model as wind gust disturbances on the angle of attack and sensor noise, respectively. The UAV is assumed to operate at an initial trimmed condition of speed, 32m/s and altitude, 1000m. After 30 separate SFDIA tests implemented on a 1.6GHz Pentium processor, the NN-SFDIA scheme detected all but 2 faults and the NN processing time was 97% lower than the flight data sampling time.",space
10.1016/j.knosys.2011.01.007,Journal,Knowledge-Based Systems,scopus,2011-05-01,sciencedirect,A knowledge-based problem solving method in GIS application,https://api.elsevier.com/content/abstract/scopus_id/79952451251,"Model design for theme analysis is one of the biggest challenges in GIS. Many real applications in GIS require functioning not only in data management and visualization, but also in analysis and decision-making. Confronted with an application of planning a new metro line in a city, a typical GIS is unable to accomplish the task in the absence of human experts or artificial intelligence technologies. Apart from being models for analyzing in different themes, some applications are also instances of problem solving in AI. Therefore, in order to strengthen its ability in automatic analysis, many theories and technologies from AI can be embedded in the GIS. In this paper, a state space is defined to formalize the metro line planning problem. By utilizing the defined state evaluation function, knowledge-based rules and strategies, a heuristic searching method is developed to optimize the solutions iteratively. Experiments are implemented to illuminate the validity of this AI-enhanced automatic analysis model of GIS.",space
10.1016/j.neucom.2010.11.023,Journal,Neurocomputing,scopus,2011-04-01,sciencedirect,Manifold Mapping Machine,https://api.elsevier.com/content/abstract/scopus_id/79953057596,"Nonlinear classification has been a non-trivial task in machine learning for the past decades. In recent years, kernel machines have successfully generalized the inner-product based linear classifiers to nonlinear ones by transforming data into some high or infinite dimensional feature space. However, due to their implicit space transformation and unobservable latent feature space, it is hard to have an intuitive understanding of their working mechanism. In this paper, we propose a comprehensible framework for nonlinear classifier design, called Manifold Mapping Machine (M3). M3 can generalize any linear classifier to nonlinear by transforming data into some low-dimensional feature space explicitly. To demonstrate the effectiveness of M3 framework, we further present an algorithmic implementation of M3 named Supervised Spectral Space Classifier (S3C). Compared with the kernel classifiers, S3C can achieve similar or even better data separation by mapping data into the low-dimensional spectral space, allowing both of its mapped data and new feature space to be examined directly. Moreover, with the discriminative information integrated into the spectral space transformation, the classification performance of S3C is more robust than that of the kernel classifiers. Experimental results show that S3C is superior to other state-of-the-art nonlinear classifiers on both synthetic and real-world data sets.",space
10.1016/j.asoc.2010.09.007,Journal,Applied Soft Computing Journal,scopus,2011-03-01,sciencedirect,Forecasting stock markets using wavelet transforms and recurrent neural networks: An integrated system based on artificial bee colony algorithm,https://api.elsevier.com/content/abstract/scopus_id/78751613501,"This study presents an integrated system where wavelet transforms and recurrent neural network (RNN) based on artificial bee colony (abc) algorithm (called ABC-RNN) are combined for stock price forecasting. The system comprises three stages. First, the wavelet transform using the Haar wavelet is applied to decompose the stock price time series and thus eliminate noise. Second, the RNN, which has a simple architecture and uses numerous fundamental and technical indicators, is applied to construct the input features chosen via Stepwise Regression-Correlation Selection (SRCS). Third, the Artificial Bee Colony algorithm (ABC) is utilized to optimize the RNN weights and biases under a parameter space design. For illustration and evaluation purposes, this study refers to the simulation results of several international stock markets, including the Dow Jones Industrial Average Index (DJIA), London FTSE-100 Index (FTSE), Tokyo Nikkei-225 Index (Nikkei), and Taiwan Stock Exchange Capitalization Weighted Stock Index (TAIEX). As these simulation results demonstrate, the proposed system is highly promising and can be implemented in a real-time trading system for forecasting stock prices and maximizing profits.",space
10.1016/j.microc.2010.09.008,Journal,Microchemical Journal,scopus,2011-03-01,sciencedirect,Full-range optical pH sensor array based on neural networks,https://api.elsevier.com/content/abstract/scopus_id/78649931852,"A neural network multivariate calibration is used to predict the pH of a solution in the full-range (0–14) from hue (H) values coming from imaging an optical pH sensor array based on 11 sensing elements with immobilized pH indicators. Different colorimetric acid-base indicators were tested for membrane preparation fulfilling the following conditions: 1) no leaching; 2) change in tonal coordinate by reaction and 3) covering the full pH range with overlapping between their pH responses. The sensor array was imaged after equilibration with a solution using a scanner working in transmission mode. Using software developed by us, the H coordinate of the colour space HSV was calculated from the RGB coordinates of each element.
                  The neural network was trained with the calibration data set using the Levenberg–Marquardt training method. The network structure has 11 input neurons (each one matching the hue of a single element in the sensor array), 1 output (the pH approximation value) and 1 hidden layer with 10 hidden neurons. The network provides an MSE=0.0098 in the training data, MSE=0.0183 in the validation data and MSE=0.0426 in the test data coming from a set of real water samples. The resulting correlation coefficient R obtained in the Pearson correlation test is R=0.999.",space
10.3182/20110828-6-IT-1002.03103,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2011-01-01,sciencedirect,UniBot Remote Laboratory: A scalable web-based set-up for education and experimental activities in robotics,https://api.elsevier.com/content/abstract/scopus_id/84866747278,"Abstract
                  The direct work on a real set-ups is an important experience for students in control theory and robotics. On the other hand, for several reasons (space, costs, complexity, etc.), it is not always possible to give students an individual access to laboratory set-ups, for their practical activities. Therefore, in recent years many tele-laboratories have been implemented by different universities, providing experimental set-ups to each student, while minimizing problems related to costs, spaces, and so on. The UniBot Remote Lab has been implemented to provide remote access via TCP connection, to assign to students different time-slots for their experiences, and to reduce the financial effort required by real set-ups. Moreover, the entire framework has been developed with high modularity both from the hardware and software point of view and, even if the basic set-up has been conceived for mobile robotics, different kind of robots or automatic machines can be easily added and be available for experimental activities.",space
10.1016/j.procs.2011.07.038,Conference Proceeding,Procedia Computer Science,scopus,2011-01-01,sciencedirect,From context to micro-context - Issues and challenges in sensorizing smart spaces for assistive living,https://api.elsevier.com/content/abstract/scopus_id/84863045088,"Most smart home based monitoring / assistive systems that attempt to recognize activities within a smart home are targeted towards living-alone elderly, and stop at providing instantaneous coarse grained information such as room-occupancy or provide specific programmed reminders for taking medication etc. In our work, we target multiple residents, while restricting the use of wearable devices / sensors. In addition we do away with video due to privacy concerns. In this paper we present the design challenges and issues in putting together a sensor network for obtaining micro-context information in multi-person smart spaces. In order to support greater levels of ambient intelligence we support fine grained spatio-temporal data and context acquisition. The architecture is being currently developed into a prototype in a modular fashion for deployment and testing in a variety of environments, and is being concurrently evaluated and tested in real conditions, prior to deployment in a facility for elderly residents with mild cognitive disorder.",space
10.1016/j.jas.2011.07.015,Journal,Journal of Archaeological Science,scopus,2011-01-01,sciencedirect,"Living the past: 3D models, virtual reality and game engines as tools for supporting archaeology and the reconstruction of cultural heritage - the case-study of the Roman villa of Casal de Freiria",https://api.elsevier.com/content/abstract/scopus_id/80054111601,"“Learn about the past to better understand the present and predict the future”.
                  Although used abundantly to justify our interest in ancient societies, this statement lacks practical meaning due to the high degree of uncertainty which cloaks archaeological studies and theories, and the fact that there is no real way to prove or validate them. That is why it is so important to approach this study from a multidisciplinary point of view, providing several inputs which complement each other and so maximize the amount of factual information drawn from the analysis. Even then, the study will never be truly complete because there will always be a missing document, a small trace of an object (Verhagen, 2008), that still needs to be analysed.
                  This paper aims to be a useful contribution to historical research, specifically to the study of architectural history. Its purpose is to create a series of methods and tools for testing and analysing theories and hypotheses for historical scenarios (Vasáros, 2008) through the use of 3D modelling tools and Virtual Reality (VR) engines.
                  The project was developed in two stages:
                  The first was the creation of several three-dimensional (3D) models, each representing a different theory or hypothesis. The models were based on accurate Computer Assisted Design (CAD) (Autodesk® AutoCAD) models for the reconstruction of the buildings, and Geographic Information Systems (GIS) (ESRI®
                     ) for the recreation of the terrain, thereby creating a realistic representation of what exists now, and a close approximation to what may have once existed.
                  In the second stage, a simplified version of the models was imported into a Virtual Reality (VR) game engine (Bethesda Softworks®
                     ) to create the ambience of the villa at the time, allowing full exploration of the space. It also includes fauna and flora, as well as Artificial Intelligence (AI)-driven avatars, as can be seen in the Video 1 provided in the electronic version of this manuscript.",space
10.1016/j.atmosenv.2010.07.024,Journal,Atmospheric Environment,scopus,2010-11-01,sciencedirect,Prediction of hourly O<inf>3</inf> concentrations using support vector regression algorithms,https://api.elsevier.com/content/abstract/scopus_id/77956886806,"In this paper we present an application of the Support Vector Regression algorithm (SVMr) to the prediction of hourly ozone values in Madrid urban area. In order to improve the training capacity of SVMrs, we have used a recently proposed approach, based on reductions of the SVMr hyper-parameters search space. Using the modified SVMr, we study different influences which may modify the ozone prediction, such as previous ozone measurements in a given station, measurements in neighbors stations, and the influence of meteorologic variables. We use statistical tests to verify the significance of incorporating different variables into the SVMr. A comparison with the results obtained using a neural network (multi-layer perceptron) is also carried out. This study has been carried out in 5 different stations of the air pollution monitoring network of Madrid, so the conclusions raised are backed by real data. The final result of the work is a robust and powerful software for tropospheric ozone prediction in Madrid. Also, the prediction tool based on SVMr is flexible enough to incorporate any other prediction variable, such as city models, or traffic patters, which may improve the prediction obtained with the SVMr.",space
10.1016/j.simpat.2010.04.010,Journal,Simulation Modelling Practice and Theory,scopus,2010-10-01,sciencedirect,Revisiting state space exploration of timed coloured petri net models to optimize manufacturing system's performance,https://api.elsevier.com/content/abstract/scopus_id/77955305732,"Due to constant fluctuations in market demands, nowadays scheduling of flexible manufacturing systems is taking great importance to improve competitiveness. Coloured Petri Nets (CPN) is a high level modelling formalism which have been widely used to model and verify systems, allowing representing not only the system’s dynamic behaviour but also the information flow. One approach that focuses in performance optimization of industrial systems is the one that uses the CPN formalism extended with time features (Timed Coloured Petri Nets) and explores all the possible states of the model (state space) looking for states of particular interest under industrial scope. Unfortunately, using the time extension, the state space becomes awkward for most industrial problems, reason why there is a recognized need of approaches that could tackle optimization problems such as the scheduling of manufacturing activities without simplifying any important aspect of the real system. In this paper a timed state space approach for properties verification and systems optimization is presented together with new algorithms in order to get better results when time is used as a cost function for optimizing the makespan of manufacturing systems. A benchmarking example of a job-shop is modelled in CPN formalism to illustrate the improvements that can be achieved with the proposed implementations.",space
10.1016/j.cor.2009.11.016,Journal,Computers and Operations Research,scopus,2010-10-01,sciencedirect,A tabu search with an oscillation strategy for the discriminant analysis problem,https://api.elsevier.com/content/abstract/scopus_id/77649180337,"This article proposes a tabu search approach to solve a mathematical programming formulation of the linear classification problem, which consists of determining an hyperplane that separates two groups of points as well as possible in ℜ
                        m
                     . The tabu search approach proposed is based on a non-standard formulation using linear system infeasibility. The search space is the set of bases defined on the matrix that describes the linear system. The moves are performed by pivoting on a specified row and column. On real machine learning databases, our approach compares favorably with implementations based on parametric programming and irreducible infeasible constraint sets. Additional computational results for randomly generated instances confirm that our method provides a suitable alternative to the mixed integer programming formulation that is solved by a commercial code when the number of attributes m increases.",space
10.1016/j.ejor.2010.01.026,Journal,European Journal of Operational Research,scopus,2010-09-16,sciencedirect,A travel demand management strategy: The downtown space reservation system,https://api.elsevier.com/content/abstract/scopus_id/77949310027,"In this paper, a Travel Demand Management strategy known as the Downtown Space Reservation System (DSRS) is introduced. The purpose of this system is to facilitate the mitigation of traffic congestion in a cordon-based downtown area by requiring people who want to drive into this area to make reservations in advance. An integer programming formulation is provided to obtain the optimal mix of vehicles and trips that are characterized by a series of factors such as vehicle occupancy, departure time, and trip length with an objective of maximizing total system throughput and revenue. Based upon the optimal solution, an “intelligent” module is built using artificial neural networks that enables the transportation authority to make decisions in real time on whether to accept an incoming request. An example is provided that demonstrates that the solution of the “intelligent” module resembles the optimal solution with an acceptable error rate. Finally, implementation issues of the DSRS are addressed.",space
10.1016/j.engappai.2010.02.007,Journal,Engineering Applications of Artificial Intelligence,scopus,2010-06-01,sciencedirect,A knowledge-based architecture for distributed fault analysis in power networks,https://api.elsevier.com/content/abstract/scopus_id/77950510844,"Power industry around the world is facing several changes since deregulation with constant pressure put on improving security, reliability and quality of the power supply. Computational fault analysis and diagnosis of power networks have been active research topics with several theories and algorithms proposed. This paper proposes a distributed diagnostic algorithm for fault analysis in power networks. Distributed architecture for power network fault analysis (DAPFA) is an intelligent, model-based diagnostic algorithm that incorporates a hierarchical power network representation and model. The architecture is based on the industry’s substation automation implementation standards. The structural and functional model is a multi-level representation with each level depicting a more complex grouping of components than its predecessor in the hierarchy. The distributed functional representation contains the behavioral knowledge related to the components of that level in the structural model.
                  The diagnostic algorithm of DAPFA is designed to perform fault analysis in pre-diagnostic and diagnostic levels. Pre-diagnostic phase provides real-time analysis while the diagnostic phase provides the final diagnostic analysis. The diagnostic algorithm incorporates knowledge-based and model-based reasoning mechanisms with one of the model levels represented as a network of neural nets. The relevant algorithms and techniques are discussed. The resulting system has been implemented on a New Zealand sub-system and the results are analyzed.",space
10.1016/j.tsf.2009.10.152,Journal,Thin Solid Films,scopus,2010-05-31,sciencedirect,Application of neural classification in ellipsometry for robust thin-film characterizations,https://api.elsevier.com/content/abstract/scopus_id/77953297755,"Nowadays, ellipsometry is a widely used technique for thin-film analyses among the existing methods. It offers a rapid, accurate and non-destructive control. The main task in this technique remains in the inverse problem which goal is to extract the interesting characteristics of the sample from the ellipsometric measurement. This is a purely mathematical step and the common algorithms used to achieve it are based on the gradient method. These latter proceed iteratively and hence require a well-chosen initial solution in order to converge towards a global minimum corresponding to the real physical solution. The main limitation of these algorithms is the risk to slip into a local minimum, leading to an erroneous solution. In this paper, we propose an original method based on neural classification in order to give a first estimation about the location of the solution in the multi-parameter space. This operation generally takes place before the characterization step itself. In this work, the method is implemented experimentally to estimate the thickness range of photoresist thin films deposited on a glass substrate.",space
10.1016/j.landurbplan.2010.03.002,Journal,Landscape and Urban Planning,scopus,2010-05-30,sciencedirect,Incorporating spatio-temporal knowledge in an Intelligent Agent Model for natural resource management,https://api.elsevier.com/content/abstract/scopus_id/77951254750,"Space and time are intrinsic components of the decision-making process in natural resource management. Decisions to extract resources from a specific location have consequences for all future decisions as they may lead to profitable opportunities or, conversely, towards unfavorable outcomes. As such, the spatio-temporal nature of decision-making should be acknowledged and incorporated into models developed to assist the management of natural resources. The objective of this research is to develop an Intelligent Agent Model that is able to learn through repetitive simulation how to make decisions regarding natural resource extraction. Specifically, an agent is guided by heuristic algorithms to search a natural landscape and learn which locations hold the highest profits and when it is best to extract the resource in order to improve the potential of future opportunities. The model is implemented using hypothetical and real data sets to emulate the process of harvesting trees in natural forests in order to maximize profits while respecting spatial constraints that are imposed in order to conserve various aspects of the forest. Simulation results reveal the ability of the Intelligent Agent Model to utilize spatio-temporal knowledge in order learn how to devise optimal solutions in a variety of scenarios. Furthermore, the model demonstrates how the timing of decisions is linked to the spatial constraints imposed on the operation. The findings from this research can be used to inform natural resource management about the importance of the relationship between the location and timing of resource-based activities.",space
10.1016/j.micpro.2010.06.001,Journal,Microprocessors and Microsystems,scopus,2010-01-01,sciencedirect,An FPGA implemented cellular automaton crowd evacuation model inspired by the electrostatic-induced potential fields,https://api.elsevier.com/content/abstract/scopus_id/79957783489,"This paper studies the on-chip realisation of a dynamic model proposed to simulate crowd behaviour, originated from electrostatic-induced potential fields. It is based on cellular automata (CA), thus taking advantage of their inherent ability to represent sufficiently phenomena of arbitrary complexity and, additionally, to be simulated precisely by digital computers. The model combines electrostatic-induced potential fields to incorporate flexibility in the movement of pedestrians. It primarily calculates distances in an obstacle filled space based on the Euclidean metric. Furthermore, it adopts a computationally fast and efficient method to overcome trouble-inducing obstacles by shifting the moving mechanism to a potential field method based on Manhattan-distance. The hardware implementation of the model is based on FPGA logic. Initialisation of the dedicated processor takes place in collaboration with a detecting and tracking algorithm supported by cameras. The instant response of the processor provides the location of pedestrians around exits. Hardware implementation exploits the prominent feature of parallelism that CA structures inherently possess in contrast to the serial computers, thus accelerating the response of the model. Furthermore, FPGA implementation of the model is advantageous in terms of low-cost, high-speed, compactness and portability features. Finally, the processor could be used as a part of an embedded, real-time, decision support system, aiming at the efficient guidance of crowd in cases of mass egress.",space
10.1016/j.comnet.2009.07.012,Journal,Computer Networks,scopus,2009-12-24,sciencedirect,An RL-based scheduling algorithm for video traffic in high-rate wireless personal area networks,https://api.elsevier.com/content/abstract/scopus_id/70449529532,"The emerging high-rate wireless personal area network (WPAN) technology is capable of supporting high-speed and high-quality real-time multimedia applications. In particular, video streams are deemed to be a dominant traffic type, and require quality of service (QoS) support. However, in the current IEEE 802.15.3 standard for MAC (media access control) of high-rate WPANs, the implementation details of some key issues such as scheduling and QoS provisioning have not been addressed. In this paper, we first propose a Markov decision process (MDP) model for optimal scheduling for video flows in high-rate WPANs. Using this model, we also propose a scheduler that incorporates compact state space representation, function approximation, and reinforcement learning (RL). Simulation results show that our proposed RL scheduler achieves nearly optimal performance and performs better than F-SRPT, EDD+SRPT, and PAP scheduling algorithms in terms of a lower decoding failure rate.",space
10.1016/j.jss.2008.06.043,Journal,Journal of Surgical Research,scopus,2009-08-01,sciencedirect,Endotoxin Alters Early Fetal Lung Morphogenesis,https://api.elsevier.com/content/abstract/scopus_id/67651115865,"Background
                  The effects of immaturity and hypoplasia of the premature lung can be affected by proinflammatory stimuli in late gestation or the postnatal period from acute lung injury secondary to intensive ventilatory management or the metabolic consequences of surgery. These stimuli alter alveolarization and contribute to bronchopulmonary dysplasia. While prior research has focused primarily on late gestational effects of inflammation on alveolar development, we sought to study whether early gestational exposure to endotoxin affects branching morphogenesis, during the critical pseudoglandular stage of lung development.
               
                  Method
                  Gestational day 15 (E15) fetal rat lung explants (term = 22 d) were treated with either 200 ng/mL or 2 μg/mL lipopolysaccharides (LPS) with controls and examined daily by phase microscopy. After 5 d, explants were fixed in 4% formaldehyde, paraffin embedded, and sectioned at 5 μm in the coronal plane. Immunohistochemical analysis was performed with platelet endothelial cell adhesion molecule (PECAM) to define endothelial cells, vascular endothelial growth factor (VEGF) to examine endothelial mitogenesis, and COX-2 antibodies as a marker for prostaglandin synthesis. Real-time PCR examined inducible nitric oxide synthase (iNOS), FGF9, FGF10, and FGFr2 gene expression. Air space fraction and airway epithelium were analyzed with Image J software.
               
                  Results
                  Phase contrast microscopy and hematoxylin-eosin histology revealed progressive, dose-related changes in air sac contraction and interstitial thickening. Compared with control E15 explants, day 5 explants incubated with high dose LPS demonstrated thickened and shrunken airway sacs with stunted branching and increased matrix deposition in interstitial areas. By immunohistochemical staining, COX-2 was quantitatively increased after high dose LPS exposure, while PECAM was reduced. VEGF expression was unaltered. LPS increased iNOS, but decreased FGF9, FGF10, and FGFr2 gene expression.
               
                  Conclusions
                  These data support evidence for an inflammatory effect of LPS on the early phase of lung development in the fetal rat, affecting branching morphogenesis during the pseudoglandular phase. Fetal endothelial cells are clearly affected, while COX-2 elevation suggests activation of an as yet undefined fetal pulmonary inflammatory cascade. We speculate that proinflammatory stimuli may ultimately lead to abnormal pulmonary development via fibroblastic growth factor (FGF)-directed mechanisms that affect epithelial-mesenchymal interaction and differentiation at a much earlier gestational age than was previously recognized.",space
10.1016/j.cmpb.2009.01.009,Journal,Computer Methods and Programs in Biomedicine,scopus,2009-08-01,sciencedirect,An event-driven distributed processing architecture for image-guided cardiac ablation therapy,https://api.elsevier.com/content/abstract/scopus_id/67349086267,"Medical imaging data is becoming increasing valuable in interventional medicine, not only for preoperative planning, but also for real-time guidance during clinical procedures. Three key components necessary for image-guided intervention are real-time tracking of the surgical instrument, aligning the real-world patient space with image-space, and creating a meaningful display that integrates the tracked instrument and patient data. Issues to consider when developing image-guided intervention systems include the communication scheme, the ability to distribute CPU intensive tasks, and flexibility to allow for new technologies. In this work, we have designed a communication architecture for use in image-guided catheter ablation therapy. Communication between the system components is through a database which contains an event queue and auxiliary data tables. The communication scheme is unique in that each system component is responsible for querying and responding to relevant events from the centralized database queue. An advantage of the architecture is the flexibility to add new system components without affecting existing software code. In addition, the architecture is intrinsically distributed, in that components can run on different CPU boxes, and even different operating systems. We refer to this Framework for Image-Guided Navigation using a Distributed Event-Driven Database in Real-Time as the FINDER architecture. This architecture has been implemented for the specific application of image-guided cardiac ablation therapy. We describe our prototype image-guidance system and demonstrate its functionality by emulating a cardiac ablation procedure with a patient-specific phantom. The proposed architecture, designed to be modular, flexible, and intuitive, is a key step towards our goal of developing a complete system for visualization and targeting in image-guided cardiac ablation procedures.",space
10.1016/j.neunet.2009.06.038,Journal,Neural Networks,scopus,2009-07-01,sciencedirect,Integrated feature and parameter optimization for an evolving spiking neural network: Exploring heterogeneous probabilistic models,https://api.elsevier.com/content/abstract/scopus_id/68149171764,"This study introduces a quantum-inspired spiking neural network (QiSNN) as an integrated connectionist system, in which the features and parameters of an evolving spiking neural network are optimized together with the use of a quantum-inspired evolutionary algorithm. We propose here a novel optimization method that uses different representations to explore the two search spaces: A binary representation for optimizing feature subsets and a continuous representation for evolving appropriate real-valued configurations of the spiking network. The properties and characteristics of the improved framework are studied on two different synthetic benchmark datasets. Results are compared to traditional methods, namely a multi-layer-perceptron and a naïve Bayesian classifier (NBC). A previously used real world ecological dataset on invasive species establishment prediction is revisited and new results are obtained and analyzed by an ecological expert. The proposed method results in a much faster convergence to an optimal solution (or a close to it), in a better accuracy, and in a more informative set of features selected.",space
10.1016/j.neunet.2009.06.043,Journal,Neural Networks,scopus,2009-07-01,sciencedirect,Neural networks with multiple general neuron models: A hybrid computational intelligence approach using Genetic Programming,https://api.elsevier.com/content/abstract/scopus_id/68149160210,"Classical neural networks are composed of neurons whose nature is determined by a certain function (the neuron model), usually pre-specified. In this paper, a type of neural network (NN-GP) is presented in which: (i) each neuron may have its own neuron model in the form of a general function, (ii) any layout (i.e network interconnection) is possible, and (iii) no bias nodes or weights are associated to the connections, neurons or layers. The general functions associated to a neuron are learned by searching a function space. They are not provided a priori, but are rather built as part of an Evolutionary Computation process based on Genetic Programming. The resulting network solutions are evaluated based on a fitness measure, which may, for example, be based on classification or regression errors. Two real-world examples are presented to illustrate the promising behaviour on classification problems via construction of a low-dimensional representation of a high-dimensional parameter space associated to the set of all network solutions.",space
10.1016/j.ins.2008.12.015,Journal,Information Sciences,scopus,2009-04-29,sciencedirect,"V-detector: An efficient negative selection algorithm with ""probably adequate"" detector coverage",https://api.elsevier.com/content/abstract/scopus_id/61449165664,"This paper describes an enhanced negative selection algorithm (NSA) called V-detector. Several key characteristics make this method a state-of-the-art advance in the decade-old NSA. First, individual-specific size (or matching threshold) of the detectors is utilized to maximize the anomaly coverage at little extra cost. Second, statistical estimation is integrated in the detector generation algorithm so the target coverage can be achieved with given probability. Furthermore, this algorithm is presented in a generic form based on the abstract concepts of data points and matching threshold. Hence it can be extended from the current real-valued implementation to other problem space with different distance measure, data/detector representation schemes, etc. By using one-shot process to generate the detector set, this algorithm is more efficient than strongly evolutionary approaches. It also includes the option to interpret the training data as a whole so the boundary between the self and nonself areas can be detected more distinctly. The discussion is focused on the features attributed to negative selection algorithms instead of combination with other strategies.",space
10.1016/j.advwatres.2009.01.001,Journal,Advances in Water Resources,scopus,2009-04-01,sciencedirect,Pumping optimization of coastal aquifers based on evolutionary algorithms and surrogate modular neural network models,https://api.elsevier.com/content/abstract/scopus_id/62349136438,"Pumping optimization of coastal aquifers involves complex numerical models. In problems with many decision variables, the computational burden for reaching the optimal solution can be excessive. Artificial Neural Networks (ANN) are flexible function approximators and have been used as surrogate models of complex numerical models in groundwater optimization. However, this approach is not practical in cases where the number of decision variables is large, because the required neural network structure can be very complex and difficult to train. The present study develops an optimization method based on modular neural networks, in which several small subnetwork modules, trained using a fast adaptive procedure, cooperate to solve a complex pumping optimization problem with many decision variables. The method utilizes the fact that salinity distribution in the aquifer, depends more on pumping from nearby wells rather than from distant ones. Each subnetwork predicts salinity in only one monitoring well, and is controlled by relatively few pumping wells falling within certain control distance from the monitoring well. While the initial control area is radial, its shape is adaptively improved using a Hermite interpolation procedure. The modular neural subnetworks are trained adaptively during optimization, and it is possible to retrain only the ones not performing well. As optimization progresses, the subnetworks are adapted to maximize performance near the current search space of the optimization algorithm. The modular neural subnetwork models are combined with an efficient optimization algorithm and are applied to a real coastal aquifer in the Greek island of Santorini. The numerical code SEAWAT was selected for solving the partial differential equations of flow and density dependent transport. The decision variables correspond to pumping rates from 34 wells. The modular subnetwork implementation resulted in significant reduction in CPU time and identified an even better solution than the original numerical model.",space
10.1016/j.commatsci.2008.04.030,Journal,Computational Materials Science,scopus,2009-03-01,sciencedirect,Hybrid intelligent approach for modeling and optimization of semiconductor devices and nanostructures,https://api.elsevier.com/content/abstract/scopus_id/59749102668,"In this work, we present a hybrid intelligent approach for parameter extraction and design optimization of semiconductor nanoscale devices and nanostructures. Based on evolutionary algorithms, numerical methods, neural network scheme and parallel computing technique, the optimization methodology is developed and successfully implemented. In the hybrid approach, an evolutionary algorithm, such as genetic algorithm or particle swarm optimization, firstly searches the entire problem space to get a set of roughly estimated solutions. The numerical method, such as Levenberg–Marquardt method, then performs a local optima search and sets the local optima as the suggested values for the genetic algorithm to perform further optimizations. Meanwhile, the neural network is applied to investigate the influence of parameters on the optimized functions which thus guides the evolutionary direction of genetic algorithm. For solving real world problems, all functional blocks are performed under a PC-based Linux cluster system with message-passing interface libraries. This hybrid intelligent approach has experimentally been implemented and validated for different applications in semiconductor nanodevices and nanostructures. For semiconductor nanodevice parameter extraction, this approach shows its capability to automatically extract a set of global parameters among sixteen 90nm complementary metal oxide semiconductor (CMOS) devices. Compared with the measured current–voltage (I–V) curves of fabricated CMOS samples, the optimized I–V results are within 3% of accuracy. The computational examinations including sensitivity, convergence property, and parallelization are discussed. For parameter extraction of organic light emitting diode (OLED), the approach also achieves good accuracy for red, green, blue OLEDs. For the third and fourth applications, optimal structure design of silicon photonic taper waveguide and photonic crystal are further advanced by integrating a simulation-based technique in the developed system. All of these experiments demonstrate interesting results and validate the optimization methodology. The concept of hybrid intelligent approach may benefit modeling and optimization in diverse science and engineering problems.",space
10.1016/j.eswa.2008.08.022,Journal,Expert Systems with Applications,scopus,2009-01-01,sciencedirect,Text feature selection using ant colony optimization,https://api.elsevier.com/content/abstract/scopus_id/58349085948,"Feature selection and feature extraction are the most important steps in classification systems. Feature selection is commonly used to reduce dimensionality of datasets with tens or hundreds of thousands of features which would be impossible to process further. One of the problems in which feature selection is essential is text categorization. A major problem of text categorization is the high dimensionality of the feature space; therefore, feature selection is the most important step in text categorization. At present there are many methods to deal with text feature selection. To improve the performance of text categorization, we present a novel feature selection algorithm that is based on ant colony optimization. Ant colony optimization algorithm is inspired by observation on real ants in their search for the shortest paths to food sources. Proposed algorithm is easily implemented and because of use of a simple classifier in that, its computational complexity is very low. The performance of proposed algorithm is compared to the performance of genetic algorithm, information gain and CHI on the task of feature selection in Reuters-21578 dataset. Simulation results on Reuters-21578 dataset show the superiority of the proposed algorithm.",space
10.1016/j.neucom.2008.02.008,Journal,Neurocomputing,scopus,2008-10-01,sciencedirect,Improve local tangent space alignment using various dimensional local coordinates,https://api.elsevier.com/content/abstract/scopus_id/56549124687,"In the past few years, the problem of nonlinear dimensionality reduction arises in many fields of information processing. The local tangent space alignment (LTSA) is one of the effective and efficient algorithms to perform nonlinear dimensionality reduction. It has a number of attractive features: simple geometric intuitions, straightforward implementation, and global optimization. However, LTSA may fail on the manifold with nonuniformly distributed noise or large curvatures. In this paper, LTSA is improved by introducing various dimensional local coordinates to represent the local geometry for each neighborhood. The modified LTSA (MLTSA) is much stable and theoretical analysis is given to show the improvement of MLTSA on noisy manifold. We also illustrate the effectiveness of our method on both synthetic and real-world data sets.",space
10.1016/j.micpro.2008.04.002,Journal,Microprocessors and Microsystems,scopus,2008-10-01,sciencedirect,A tunable high-performance architecture for enhancement of stream video captured under non-uniform lighting conditions,https://api.elsevier.com/content/abstract/scopus_id/54549122634,"A novel architecture for performing hue-saturation-value (HSV) domain enhancement of digital color images captured under non-uniform lighting conditions is proposed in this paper for video streaming applications. The approach promotes log-domain computation to eliminate all multiplications, divisions and exponentiations utilizing the compact high-speed logarithmic estimation modules. An optimized quadrant symmetric architecture is incorporated into the design of homomorphic filter for the enhancement of intensity value. Efficient modules are also presented for conversion between RGB and HSV color spaces with tunable H and S components in HSV for more flexible color rendering. The design is able to bring out details hidden in shadow regions of the image and preserve the bright parts with adjustable vividness and color shift for improvement of visual quality while maintaining its consistency. It is capable of producing 187.86 million outputs per second (MOPs) on Xilinx’s Virtex II XC2V2000-4ff896 field programmable gate array (FPGA) at a clock frequency of 187.86MHz. It can process over 179.1 (1024×1024) frames per second, which is very suitable for high definition videos, and consumes approximately 70.7% and 76.8% less hardware resource with 127% and 280% performance boost when compared to the designs with machine learning algorithm in [M.Z. Zhang, M.J. Seow, V.K. Asari, A high performance architecture for color image enhancement using a machine learning approach, International Journal of Computational Intelligence Research – Special Issue on Advances in Neural Networks 2(1) (2006) 40–47], and with separated dynamic and contrast enhancements in [H.T. Ngo, M.Z. Zhang, L. Tao, V.K. Asari, Design of a high performance architecture for real-time enhancement of video stream captured in extremely low lighting environment, International Journal of Embedded Systems: Special Issue on Media and Stream Processing, in press], respectively. This approach also provide 83.4 times performance gain with more consistent fidelity in the results compared to some DSP based implementations (256×256 frame size) [G.D. Hines, Z. Rahman, D.J. Jobson, G.A. Woodell, DSP implementation of the retinex image enhancement algorithm, visual information processing XIII, in: Proceedings of the SPIE, vol. 5438, 2004, pp. 13–24; G.D. Hines, Z. Rahman, D.J. Jobson, G.A. Woodell, Single-scale retinex using digital signal processors, in: Proceedings of the Global Signal Processing Conference, September 2004, pp. 1–6] under the reflectance-illuminance category of image enhancement models.",space
10.1016/j.neunet.2007.12.009,Journal,Neural Networks,scopus,2008-03-01,sciencedirect,Compact hardware liquid state machines on FPGA for real-time speech recognition,https://api.elsevier.com/content/abstract/scopus_id/40649092298,"Hardware implementations of Spiking Neural Networks are numerous because they are well suited for implementation in digital and analog hardware, and outperform classic neural networks. This work presents an application driven digital hardware exploration where we implement real-time, isolated digit speech recognition using a Liquid State Machine. The Liquid State Machine is a recurrent neural network of spiking neurons where only the output layer is trained. First we test two existing hardware architectures which we improve and extend, but that appears to be too fast and thus area consuming for this application. Next, we present a scalable, serialized architecture that allows a very compact implementation of spiking neural networks that is still fast enough for real-time processing. All architectures support leaky integrate-and-fire membranes with exponential synaptic models. This work shows that there is actually a large hardware design space of Spiking Neural Network hardware that can be explored. Existing architectures have only spanned part of it.",space
10.1016/j.apacoust.2007.05.007,Journal,Applied Acoustics,scopus,2008-02-01,sciencedirect,HRTF personalization based on artificial neural network in individual virtual auditory space,https://api.elsevier.com/content/abstract/scopus_id/39149101013,"The synthesis of individual virtual auditory space (VAS) is an important and challenging task in virtual reality. One of the key factors for individual VAS is to obtain a set of individual head related transfer functions (HRTFs). A customization method based on back-propagation (BP) artificial neural network (ANN) is proposed to obtain an individual HRTF without complex measurement. The inputs of the neural network are the anthropometric parameters chosen by correlation analysis and the outputs are the characteristic parameters of HRTFs together with the interaural time difference (ITD). Objective simulation experiments and subjective sound localization experiments are implemented to evaluate the performance of the proposed method. Experiments show that the estimated non-individual HRTF has small mean square error, and has similar perception effect to the corresponding one obtained from the database. Furthermore, the localization accuracy of personalized HRTF is increased compared to the non-individual HRTF.",space
10.1016/j.bspc.2007.09.002,Journal,Biomedical Signal Processing and Control,scopus,2008-01-01,sciencedirect,Multi-channel surface EMG classification using support vector machines and signal-based wavelet optimization,https://api.elsevier.com/content/abstract/scopus_id/41149127373,"The study proposes a method for supervised classification of multi-channel surface electromyographic signals with the aim of controlling myoelectric prostheses. The representation space is based on the discrete wavelet transform (DWT) of each recorded EMG signal using unconstrained parameterization of the mother wavelet. The classification is performed with a support vector machine (SVM) approach in a multi-channel representation space. The mother wavelet is optimized with the criterion of minimum classification error, as estimated from the learning signal set. The method was applied to the classification of six hand movements with recording of the surface EMG from eight locations over the forearm. Misclassification rate in six subjects using the eight channels was (mean±S.D.) 4.7±3.7% with the proposed approach while it was 11.1±10.0% without wavelet optimization (Daubechies wavelet). The DWT and SVM can be implemented with fast algorithms, thus, the method is suitable for real-time implementation.",space
10.1016/j.actaastro.2006.12.021,Journal,Acta Astronautica,scopus,2008-01-01,sciencedirect,Abort determination with non-adaptive neural networks for the Mars precision landers,https://api.elsevier.com/content/abstract/scopus_id/36049019599,"The 2009 Mars Science Laboratory (MSL) will attempt the first precision landing on Mars using a modified version of the Apollo Earth entry guidance program. The guidance routine, Entry Terminal Point Controller (ETPC), commands the deployment of a supersonic parachute while converging the range to the landing target. For very dispersed cases, ETPC is unlikely to converge the range to the target and command parachute deployment within Mach number and dynamic pressure constraints. A full-lift-up abort can save 85% of these failed trajectories while abandoning the precision landing objective. In order to implement an abort, a failed trajectory needs to be recognized in real time. The application of artificial neural networks (ANNs) as an abort determination technique was evaluated. An ANN was designed, trained and tested using Monte Carlo simulations of MSL descent for a severe dust storm scenario. When incorporated into ETPC, the ANN correctly classifies 87% of descent trajectories as abort or non-abort, reducing the probability of losing MSL in a severe dust storm from 18% to 3.5%. This research shows that ANNs are capable of recognizing failed descent trajectories and can significantly increase the survivability of MSL for very dispersed cases.",space
10.1016/j.jsv.2007.06.028,Journal,Journal of Sound and Vibration,scopus,2007-11-06,sciencedirect,MIMO adaptive vibration control of smart structures with quickly varying parameters: Neural networks vs classical control approach,https://api.elsevier.com/content/abstract/scopus_id/34548564073,"This paper presents experimental adaptive identification and control of smart structures using neural networks based on system classification technique. An inverted L-structure with surface-bonded piezoceramic sensors/actuators is used for analysis. The state space, as well as matrix fraction description presentation, from control input voltages to output sensor voltage, is established in multivariable form. It is observed that the computational time required for online parameter identification and controller design is generally quite high. For the system, whose parameters change abruptly with large amplitudes, classical adaptive control techniques give poor transient behavior and sometimes instability. Also, for obtaining the ideal closed-loop performance, linear quadratic regulator cannot be re-designed in real-time for changed parameters of the smart structures, even if these parameters are identified in real time. Closed-loop identification of system parameters and control gains using system classification-based neural networks is proposed and implemented. A preliminary experimental study is also done to see the effectiveness of the proposed technique over classical control methods.",space
10.1016/j.enganabound.2006.12.003,Journal,Engineering Analysis with Boundary Elements,scopus,2007-08-01,sciencedirect,Principal component analysis and artificial neural network approach to electrical impedance tomography problems approximated by multi-region boundary element method,https://api.elsevier.com/content/abstract/scopus_id/34347221886,"The idea of electrical impedance tomography (EIT) is to evaluate conductivity or permittivity distribution inside the examined object by measuring the voltages between electrodes placed on its surface. In this paper, EIT as a default 3D diagnostic method of the breast cancer is suggested. The breast was modelled as a hemisphere consists of two spatially homogenous areas with different conductivity. In order to determine the distribution of potential in the breast model, a multi-region boundary element method (BEM) was implemented. In this paper, a multi-region BEM with quadratic interpolation function for the flat, triangular surface elements was introduced. The inverse problem solution provided the identification of the size and the position of the anomalies in the breast tissue. For this purpose the efficient method based on principal component analysis (PCA) and the artificial neural network (ANN) was used. PCA applied to EIT data allows reducing dimensionality of measured data for 3D space and removing the unused part of information, usually corresponding to noise and interrelated variables. ANN method allows to obtain the results of inverse problem solution in real-time.",space
10.1016/j.measurement.2006.09.004,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2007-07-01,sciencedirect,Neuro-fuzzy state modeling of flexible robotic arm employing dynamically varying cognitive and social component based PSO,https://api.elsevier.com/content/abstract/scopus_id/34249704095,The present paper proposes the development of a neuro-fuzzy state-space model for flexible robotic arm on the basis of real sensor data acquired. The training problem of the neuro-fuzzy architecture has been configured as a highly multidimensional stochastic global optimization problem and improved variants of particle swarm optimization (PSO) techniques have been successfully implemented for it. The effects of dynamically varying the “cognitive” and the “social” components of the improved PSOs on the training performance have been studied in detail. The practical utility of such a model development procedure is aptly demonstrated by employing the best trained model to design a stable fuzzy state controller and implementing it in real life for the same flexible robotic arm.,space
10.3182/20070625-5-fr-2916.00025,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2007-01-01,sciencedirect,A smart approach to precision attitude maneuvers of spacecrafts,https://api.elsevier.com/content/abstract/scopus_id/79960998950,"A nonlinear adaptive approach is presented to achieve rest-to-rest attitude maneuvers for spacecrafts in the presence of parameter uncertainties and unknown disturbances. A nonlinear controller, designed on the principle of dynamic inversion achieves the goals for the nominal model but suffers performance degradation in the presence of off-nominal parameter values and unwanted inputs. To address this issue, a model-following neuro-adaptive control design is carried out by taking the help of neural networks. Due to the structured approach followed here, the adaptation is restricted to the momentum level equations. The adaptive technique presented is computationally nonintensive and hence can be implemented in real-time. Because of these features, this new approach is named as structured model-following adaptive real-time technique (SMART). From simulation studies, this SMART approach is found to be very effective in achieving precision attitude maneuvers in the presence of parameter uncertainties and unknown disturbances.",space
10.1016/j.buildenv.2005.07.008,Journal,Building and Environment,scopus,2006-12-01,sciencedirect,Quasi-adaptive fuzzy heating control of solar buildings,https://api.elsevier.com/content/abstract/scopus_id/33745977431,"Significant progress has been made on maximising passive solar heat gains to building spaces in winter. Control of the space heating in these applications is complicated due to the lagging influence of the useful solar heat gain coupled with the wide range of construction materials and heating system choices. Additionally, and in common with most building control applications, there is a need to develop control solutions that permit simple and transparent set-up and commissioning procedures. This paper addresses the development and testing of a quasi-adaptive fuzzy logic control method that addresses these issues. The controller is developed in two steps. A feed-forward neural network is used to predict the internal air temperature, in which a singular value decomposition (SVD) algorithm is used to remove the highly correlated data from the inputs of the neural network to reduce the network structure. The fuzzy controller is then designed to have two inputs: the first input being the error between the set-point temperature and the internal air temperature and the second the predicted future internal air temperature. The controller was implemented in real-time using a test cell with controlled ventilation and a modulating electric heating system. Results, compared with validated simulations of conventionally controlled heating, confirm that the proposed controller achieves superior tracking and reduced overheating when compared with the conventional method of control.",space
10.1016/j.patcog.2005.10.026,Journal,Pattern Recognition,scopus,2006-05-01,sciencedirect,RBF-based neurodynamic nearest neighbor classification in real pattern space,https://api.elsevier.com/content/abstract/scopus_id/33244467462,"Superposition of radial basis functions centered at given prototype patterns constitutes one of the most suitable energy forms for gradient systems that perform nearest neighbor classification with real-valued static prototypes. It is shown in this paper that a continuous-time dynamical neural network model, employing a radial basis function and a sigmoid multi-layer perceptron sub-networks, is capable of maximizing such an energy form locally, thus performing almost perfectly nearest neighbor classification, when initiated by a distorted pattern. The proposed design scheme allows for explicit representation of prototype patterns as network parameters, as well as augmenting additional or forgetting existing memory patterns. The dynamical classification scheme implemented by the network eliminates all comparisons, which are the vital steps of the conventional nearest neighbor classification process. The performance of the proposed network model is demonstrated on binary and gray-scale image reconstruction applications.",space
10.1016/j.neucom.2005.04.008,Journal,Neurocomputing,scopus,2006-03-01,sciencedirect,Cell assemblies for diagnostic problem-solving,https://api.elsevier.com/content/abstract/scopus_id/32544433164,"We describe a neuronal model for diagnostic problem-solving. This model which is inspired by cell assemblies gives some hints on how diagnostic problem-solving might actually be performed by the human brain. The diagnostic process is described by a deduction system that performs an abductive inference. The abductive inference itself is described by the verbal category theory. A mapping of a diagnostic problem into a diagnostic system represented by an associative memory with feedback connections is presented. The associative memory with feedback connections offers a self-contained architecture for the administration and representation of manifestations and disorders. This can be implemented efficiently on a serial computer, requiring low memory space and low computational costs. Because of these advantages, this model was chosen for the implementation of a real embedded diagnostic system for a wire bonder machine. The knowledge base of this system is composed of 350 rules, which are stored in 11 modules. These modules model the error behaviour of the microcontroller based units of the machine and are arranged in a taxonomy which corresponds to the hierarchical chains that describe the relationship between disorders and manifestations.",space
10.1016/j.asr.2005.12.004,Journal,Advances in Space Research,scopus,2006-01-01,sciencedirect,Analysis of Sileye-3/Alteino data with a neural network technique: Particle discrimination and energy reconstruction,https://api.elsevier.com/content/abstract/scopus_id/33744809796,"In this work, we present the data analysis of the Sileye-3/Alteino experiment with neural network technique. Sileye-3/Alteino is composed of two devices: the cosmic ray-advanced silicon telescope (an 8 plane, 32 strip silicon detector) and an electroencephalograph. It was placed on board the ISS on April the 27th 2002 to investigate on the Light Flash phenomenon and the radiation environment in space. We show the possibility of using neural networks as an useful tool for real-time data analysis. A feed-forward neural network (Multi-Layer Perceptron – MLP) has been implemented and trained (with Monte Carlo data) to perform on line particle identification for ions with Atomic Number (Z) ⩽8 and incident kinetic energy reconstruction for ions Z
                     >2. The result of the analysis of Sileye-3/Alteino real data with the neural network and the improvements over classical analysis techniques are discussed.",space
10.1016/j.agsy.2004.07.019,Journal,Agricultural Systems,scopus,2005-12-01,sciencedirect,Merging genomic control networks and soil-plant-atmosphere-continuum models,https://api.elsevier.com/content/abstract/scopus_id/27344439349,"Advances in genomic science make it desirable to include genomic controls in soil-plant-atmosphere-continuum (SPAC) models by methods proposed in this paper. Molecular genetic concepts suggest that a differential equation similar to ones used in neural networks can be used to model single-gene elements of larger systems. Natural modifications to the equation incorporate temperature dependency. Multi-gene components based on this element function as Boolean logic gates, linear arithmetic units, delays, differentiators, integrators, oscillators, coincidence detectors, and bi-stable devices. Related genetic circuitry from real organisms is shown. Genomic integration with SPAC models entails whole-plant modeling with realistic morphology. Plants are networks of parts, iterated in time and space under genetic control, that induce and modulate conservative SPAC mass/energy flows. Network developmental rules can be stated as Lindenmayer grammars whose symbols represent plant parts programmed as software objects. A structure is presented for simulators based on these concepts. The discussion argues that prior object-oriented plant modeling approaches (i) do not reflect how plants actually develop morphologically and (ii) may represent processes in tactically unwise ways at a time when genomics is advancing knowledge of process interactions. Finally, genomics and expanding computing power redefine concepts of model “simplicity” and “complexity” to favor increased realism.",space
10.1016/j.neunet.2005.06.029,Journal,Neural Networks,scopus,2005-07-01,sciencedirect,On-chip visual perception of motion: A bio-inspired connectionist model on FPGA,https://api.elsevier.com/content/abstract/scopus_id/27744451750,"Visual motion provides useful information to understand the dynamics of a scene to allow intelligent systems interact with their environment. Motion computation is usually restricted by real time requirements that need the design and implementation of specific hardware architectures. In this paper, the design of hardware architecture for a bio-inspired neural model for motion estimation is presented. The motion estimation is based on a strongly localized bio-inspired connectionist model with a particular adaptation of spatio-temporal Gabor-like filtering. The architecture is constituted by three main modules that perform spatial, temporal, and excitatory–inhibitory connectionist processing. The biomimetic architecture is modeled, simulated and validated in VHDL. The synthesis results on a Field Programmable Gate Array (FPGA) device show the potential achievement of real-time performance at an affordable silicon area.",space
10.1016/j.compind.2004.06.003,Journal,Computers in Industry,scopus,2005-02-01,sciencedirect,A solution to the unequal area facilities layout problem by genetic algorithm,https://api.elsevier.com/content/abstract/scopus_id/13444304124,"The majority of the issued facilities layout problems (FLPs) minimize the material handling cost and ignore other factors, such as area utilization, department shape and site shape size. These factors, however, might influence greatly the objective function and should give consideration. The research range of this paper is focus on the unequal areas department facilities layout problem, and implement analysis of variance (ANOVA) of statistics to find out the best site size of layout by genetic algorithm. The proposed module takes the minimum total layout cost (TLC) into account. TLC is an objective function combining material flow factor cost (MFFC), shape ratio factor (SRF) and area utilization factor (AUF). In addition, a rule-based of expert system is implemented to create space-filling curve for connecting each unequal area department to be continuously placed without disjoint (partition). In this manner, there is no gap between each unequal area department. The experimental results show that the proposed approach is more feasible in dealing with the facilities layout problems in the real world.",space
10.1016/j.acra.2004.09.012,Journal,Academic Radiology,scopus,2005-01-01,sciencedirect,Hidden Markov event sequence models: Toward unsupervised functional MRI brain mapping,https://api.elsevier.com/content/abstract/scopus_id/13244273665,"Rationale and objectives
                  Most methods used in functional MRI (fMRI) brain mapping require restrictive assumptions about the shape and timing of the fMRI signal in activated voxels. Consequently, fMRI data may be partially and misleadingly characterized, leading to suboptimal or invalid inference. To limit these assumptions and to capture the broad range of possible activation patterns, a novel statistical fMRI brain mapping method is proposed. It relies on hidden semi-Markov event sequence models (HSMESMs), a special class of hidden Markov models (HMMs) dedicated to the modeling and analysis of event-based random processes.
               
                  Materials and methods
                  Activation detection is formulated in terms of time coupling between (1) the observed sequence of hemodynamic response onset (HRO) events detected in the voxel’s fMRI signal and (2) the “hidden” sequence of task-induced neural activation onset (NAO) events underlying the HROs. Both event sequences are modeled within a single HSMESM. The resulting brain activation model is trained to automatically detect neural activity embedded in the input fMRI data set under analysis. The data sets considered in this article are threefold: synthetic epoch-related, real epoch-related (auditory lexical processing task), and real event-related (oddball detection task) fMRI data sets.
               
                  Results
                  
                     Synthetic data: Activation detection results demonstrate the superiority of the HSMESM mapping method with respect to a standard implementation of the statistical parametric mapping (SPM) approach. They are also very close, sometimes equivalent, to those obtained with an “ideal” implementation of SPM in which the activation patterns synthesized are reused for analysis. The HSMESM method appears clearly insensitive to timing variations of the hemodynamic response and exhibits low sensitivity to fluctuations of its shape (unsustained activation during task). Real epoch-related data: HSMESM activation detection results compete with those obtained with SPM, without requiring any prior definition of the expected activation patterns thanks to the unsupervised character of the HSMESM mapping approach. Along with activation maps, the method offers a wide range of additional fMRI analysis functionalities, including activation lag mapping, activation mode visualization, and hemodynamic response function analysis. Real event-related data: Activation detection results confirm and validate the overall strategy that consists in focusing the analysis on the transients, time-localized events that are the HROs.
               
                  Conclusion
                  All the experiments performed on synthetic and real fMRI data demonstrate the relevance of HSMESMs in fMRI brain mapping. In particular, the statistical character of these models, along with their learning and generalizing abilities are of particular interest when dealing with strong variabilities of the active fMRI signal across time, space, experiments, and subjects.",space
10.1016/j.envsoft.2003.10.003,Journal,Environmental Modelling and Software,scopus,2004-08-01,sciencedirect,Modelling SO<inf>2</inf> concentration at a point with statistical approaches,https://api.elsevier.com/content/abstract/scopus_id/3342982389,"In this paper, the results obtained by inter-comparing several statistical techniques for modelling SO2 concentration at a point such as neural networks, fuzzy logic, generalised additive techniques and other recently proposed statistical approaches are reported. The results of the inter-comparison are the fruits of collaboration between some of the partners of the APPETISE project funded under the Framework V Information Societies and Technologies (IST) programme. Two different cases for study were selected: the Siracusa industrial area, in Italy, where the pollution is dominated by industrial emissions and the Belfast urban area, in the UK, where domestic heating makes an important contribution. The different kinds of pollution (industrial/urban) and different locations of the areas considered make the results more general and interesting. In order to make the inter-comparison more objective, all the modellers considered the same datasets. Missing data in the original time series was filled by using appropriate techniques. The inter-comparison work was carried out on a rigorous basis according to the performance indices recommended by the European Topic Centre on Air and Climate Change (ETC/ACC). The targets for the implemented prediction models were defined according to the EC normative relating to limit values for sulphur dioxide. According to this normative, three different kinds of targets were considered namely daily mean values, daily maximum values and hourly mean values. The inter-compared models were tested on real cases of poor air quality. In the paper, the inter-compared techniques are ranked in terms of their capability to predict critical episodes. A ranking in terms of their predictability of the three different targets considered is also proposed. Several key issues are illustrated and discussed such as the role of input variable selection, the use of meteorological data, and the use of interpolated time series. Moreover, a novel approach referred to as the technique of balancing the training pattern set, which was successfully applied to improve the capability of ANN models to predict exceedences is introduced. The results show that there is no single modelling approach, which generates optimum results in terms of the full range of performance indices considered. In view of the implementation of a warning system for air quality control, approaches that are able to work better in the prediction of critical episodes must be preferred. Therefore, the artificial neural network prediction models can be recommended for this purpose. The best forecasts were achieved for daily averages of SO2 while daily maximum and hourly mean values are difficult to predict with acceptable accuracy.",space
10.1016/j.nima.2004.01.052,Journal,"Nuclear Instruments and Methods in Physics Research, Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",scopus,2004-05-21,sciencedirect,A neural network device for on-line particle identification in cosmic ray experiments,https://api.elsevier.com/content/abstract/scopus_id/2342492426,"On-line particle identification is one of the main goals of many experiments in space both for rare event studies and for optimizing measurements along the orbital trajectory. Neural networks can be a useful tool for signal processing and real time data analysis in such experiments. In this document we report on the performances of a programmable neural device which was developed in VLSI analog/digital technology. Neurons and synapses were accomplished by making use of Operational Transconductance Amplifier (OTA) structures. In this paper we report on the results of measurements performed in order to verify the agreement of the characteristic curves of each elementary cell with simulations and on the device performances obtained by implementing simple neural structures on the VLSI chip. A feed-forward neural network (Multi-Layer Perceptron, MLP) was implemented on the VLSI chip and trained to identify particles by processing the signals of two-dimensional position-sensitive Si detectors. The radiation monitoring device consisted of three double-sided silicon strip detectors. From the analysis of a set of simulated data it was found that the MLP implemented on the neural device gave results comparable with those obtained with the standard method of analysis confirming that the implemented neural network could be employed for real time particle identification.",space
10.1016/j.robot.2003.11.006,Journal,Robotics and Autonomous Systems,scopus,2004-02-29,sciencedirect,A reinforcement learning with evolutionary state recruitment strategy for autonomous mobile robots control,https://api.elsevier.com/content/abstract/scopus_id/0742289960,"In recent robotics fields, much attention has been focused on utilizing reinforcement learning (RL) for designing robot controllers, since environments where the robots will be situated in should be unpredictable for human designers in advance. However there exist some difficulties. One of them is well known as ‘curse of dimensionality problem’. Thus, in order to adopt RL for complicated systems, not only ‘adaptability’ but also ‘computational efficiencies’ should be taken into account. The paper proposes an adaptive state recruitment strategy for NGnet-based actor-critic RL. The strategy enables the learning system to rearrange/divide its state space gradually according to the task complexity and the progress of learning. Some simulation results and real robot implementations show the validity of the method.",space
10.1016/S0169-4332(03)00919-X,Journal,Applied Surface Science,scopus,2004-02-15,sciencedirect,Application of a genetic algorithm and a neural network for the discovery and optimization of new solid catalytic materials,https://api.elsevier.com/content/abstract/scopus_id/0346304811,In the process of discovering new catalytic compositions by combinatorial methods in heterogeneous catalysis usually various potential catalytic compounds have to be prepared and tested. To decrease the number of necessary experiments an optimization algorithm based on a genetic algorithm for deriving subsequent generations from the performance of the members of the preceding generation is described. This procedure is supplemented by using an artificial neural network for establishing relationships between catalyst compositions—or more general speaking—materials properties and their catalytic performance. By combining a trained neural network with the genetic algorithm software virtually computer experiments were done aiming at adjusting the control parameters of the optimization algorithm to the special requirement of catalyst development. The approach is illustrated by the search for new catalytic compositions for the oxidative dehydrogenation of propane.,space
10.1016/j.advengsoft.2004.04.002,Journal,Advances in Engineering Software,scopus,2004-01-01,sciencedirect,Intelligent flight support system (IFSS): A real-time intelligent decision support system for future manned spaceflight operations at Mission Control Center,https://api.elsevier.com/content/abstract/scopus_id/2942601776,"The Mission Control Center Systems (MCCS) is a functionally robust set of distributed systems primarily supporting the Space Shuttle Program (SSP) and the International Space Station (ISS) mission operations. Forged around the uniquely complex and demanding requirements of human spaceflight, the MCCS has evolved within the limits of the technological capabilities of the time. The dynamic environment in which MCCS functions has demanded that the systems architecture continue to evolve as well.
                  The MCCS provides the primary means of controlling crewed spacecraft operated by NASA. Flight controllers (FCs) monitor the spacecraft systems through telemetry sent from the spacecraft to the ground and from the ground to the vehicle. FCs utilize several application software to present telemetry data in a variety of output presentations. While most displays simply provide a densely packed screen of telemetry data, only a few provide graphical representations of the vehicle systems' status. New technological advances in user interface design have not penetrated into MCC especially since the SSP and ISS systems were developed when these technologies were not available. The Intelligent Flight Support System (IFSS) described in this paper promotes situational awareness at MCC with an interactive virtual model of the ISS and Space Shuttle combined with data and decision support displays. IFSS also incorporates an intelligent component to model various characteristics of space vehicle systems when predictable results of unknown scenarios are required. IFSS supports FCs in the planning, communications, command, and control operations of the ISS and Space Shuttle by providing knowledge and skills that are unavailable from internal representation.",space
10.1016/j.advengsoft.2004.04.001,Journal,Advances in Engineering Software,scopus,2004-01-01,sciencedirect,Evolutionary optimization of energy systems using population graphing and neural networks,https://api.elsevier.com/content/abstract/scopus_id/2942546504,"This paper examines the simultaneous use of graph based evolutionary algorithms (GBEAs) and a real-time estimate of the final fitness for evolutionary optimization of systems modeled using computational fluid dynamics (CFDs). GBEAs are used to control the rate at which information travels, enabling the diversity of the population to be tuned to match the solution space. During each fitness evaluation, the CFD solver iteratively solves the fluid flow and heat transfer characteristics of the proposed design. In this paper, an artificial neural network is used to develop a real-time estimate of the final fitness and error bounds at each iteration of the solver. Using these estimates, the evolutionary algorithm can determine when the fitness of the design is known with sufficient accuracy for the evolutionary process. This significantly reduces the overall compute time. These techniques are demonstrated by optimizing the spatial temperature profile of the cooking surface of a biomass cookstove. In this cookstove, hot gases from biomass combustion flow under the cooking surface. Within this flow area, a set of baffles direct the flow of hot gases and establish the spatial temperature profile of the stove's cooking surface. The location and size of a series of baffles within the hot gas flow area are determined by the optimization routine. In this design problem, it is found that the two techniques are compatible; both the number of fitness evaluations and the time required for each CFD fitness evaluation are reduced while utilizing GBEAs to preserve the diversity of the population.",space
10.1016/S0004-3702(03)00078-X,Journal,Artificial Intelligence,scopus,2003-12-01,sciencedirect,Enhancing disjunctive logic programming systems by SAT checkers,https://api.elsevier.com/content/abstract/scopus_id/0242365718,"Disjunctive logic programming (DLP) with stable model semantics is a powerful nonmonotonic formalism for knowledge representation and reasoning. Reasoning with DLP is harder than with normal (∨-free) logic programs, because stable model checking—deciding whether a given model is a stable model of a propositional DLP program—is co-NP-complete, while it is polynomial for normal logic programs.
                  This paper proposes a new transformation ΓM(P), which reduces stable model checking to UNSAT—i.e., to deciding whether a given CNF formula is unsatisfiable. The stability of a model M of a program 
                        P
                      thus can be verified by calling a Satisfiability Checker on the CNF formula ΓM(P). The transformation is parsimonious (i.e., no new symbol is added), and efficiently computable, as it runs in logarithmic space (and therefore in polynomial time). Moreover, the size of the generated CNF formula never exceeds the size of the input (and is usually much smaller). We complement this transformation with modular evaluation results, which allow for efficient handling of large real-world reasoning problems.
                  The proposed approach to stable model checking has been implemented in DLV—a state-of-the-art implementation of DLP. A number of experiments and benchmarks have been run using SATZ as Satisfiability checker. The results of the experiments are very positive and confirm the usefulness of our techniques.",space
10.1016/S0045-7906(01)00050-7,Journal,Computers and Electrical Engineering,scopus,2003-06-01,sciencedirect,Two-phase neural network based estimation of degree of insecurity of power system,https://api.elsevier.com/content/abstract/scopus_id/0037409373,"Increased loading and contingencies often lead to situations where the optimal power flow solution no longer remains within the secure region. In such situations there is a need of determining control actions to be taken quickly, as otherwise the system may become unstable. Hence it is important to quantify the degree of insecurity of the power system both in planning as well as at operational stages. The distance in parameter space between an insecure operating point and the closest point on feasible (secure) hyper-surface has been used as a measure of degree of insecurity. A method based on two-phase optimization neural network has been presented to compute the degree of insecurity and the voltages and angles at all the buses of the system corresponding to the closest secure point. Inclusion of security limits on power system variables assures a solution representing a secure system. When compared with conventional non-linear optimization techniques, the proposed neural network is superior, as it can be easily implemented using digital hardware and is highly suitable for real time implementation in energy management system.
                  The proposed method has been tested on IEEE 30-bus test system and a practical 75-bus Indian system. The results achieved are compared with results from a conventional method. Insecurity arising due to increase in load and contingencies has been considered in this work.",space
10.1016/S0378-7753(03)00309-4,Journal,Journal of Power Sources,scopus,2003-05-15,sciencedirect,Power supply quality improvement with a SOFC plant by neural-network-based control,https://api.elsevier.com/content/abstract/scopus_id/0038216724,"This paper demonstrates the potential of a solid-oxide fuel cell (SOFC) to perform functions other than the supply of real power to the grid. These additional functions however require the use of an inverter. The flux-vector control is used very effectively for the control of this inverter, where the space-vector pulsewidth modulation (SVM) is implemented by neural networks (NNs). The results presented in the paper show the effect of the fuel cell on the voltage at the sensitive load point. The performance of the fuel cell was found to be excellent.",space
10.1016/S0196-8904(02)00138-3,Journal,Energy Conversion and Management,scopus,2003-05-01,sciencedirect,A rotor position estimator for switched reluctance motors using CMAC,https://api.elsevier.com/content/abstract/scopus_id/0037400491,"This paper presents an approach to rotor position estimation in switched reluctance motors (SRMs) by using a cerebellum model articulation controller (CMAC). Previous research has shown that an artificial neural network (ANN) forms an efficient mapping structure through measurement of the flux linkages and currents for the phases. A CMAC is investigated in this paper in order to overcome the high computational power requirement problem that is encountered in a feedforward ANN based rotor position estimator. The CMAC structure does not contain neurons with activation functions, and all mathematical operations are performed without multiplication. These simplicities increase the throughput in real time implementation performed with conventional embedded controllers. However, the distributed memory structure of a CMAC requires more space. The issues involved in designing, training and implementing a CMAC are presented. In order to demonstrate the feasibility of the concept, a 20 kW, 6/4, three phase SRM is studied with training and evaluation data, which are obtained from a simulation program. A CMAC that is based on experimentally measured training and testing data for the same SRM is also used to demonstrate the promise of this approach.",space
10.1016/S0925-2312(02)00619-7,Journal,Neurocomputing,scopus,2003-04-01,sciencedirect,Pattern recognition using multilayer neural-genetic algorithm,https://api.elsevier.com/content/abstract/scopus_id/0037381137,"The genetic algorithm implemented with neural network to determine automatically the suitable network architecture and the set of parameters from a restricted region of space. The multilayer neural-genetic algorithm was applied in image processing for pattern recognition, and to determine the object orientation. The library to cover the views of object was build from real images of (10×10) pixels. Which is the smallest image size can be used in this algorithm to recognize the type of aircraft with its direction
                  The multilayer perecptron neural network integrated with the genetic algorithm, the result showed good optimization, by reducing the number of hidden nodes required to train the neural network (the number of epoch's reduced to less than 50%). One of the important results of the implemented algorithm is the reduction in the time required to train the neural network.",space
10.1016/S0893-6080(03)00015-7,Journal,Neural Networks,scopus,2003-01-01,sciencedirect,Improved system for object detection and star/galaxy classification via local subspace analysis,https://api.elsevier.com/content/abstract/scopus_id/0037380809,"The two traditional tasks of object detection and star/galaxy classification in astronomy can be automated by neural networks because the nature of the problems is that of pattern recognition. A typical existing system can be further improved by using one of the local Principal Component Analysis (PCA) models. Our analysis in the context of object detection and star/galaxy classification reveals that local PCA is not only superior to global PCA in feature extraction, but is also superior to gaussian mixture in clustering analysis. Unlike global PCA which performs PCA for the whole data set, local PCA applies PCA individually to each cluster of data. As a result, local PCA often outperforms global PCA for data of multi-modes. Moreover, since local PCA can effectively avoid the trouble of having to specify a large number of free elements of each covariance matrix of gaussian mixture, it can give a better description of local subspace structures of each cluster when applied on high dimensional data with small sample size. In this paper, the local PCA model proposed by Xu [IEEE Trans. Neural Networks 12 (2001) 822] under the general framework of Bayesian Ying Yang (BYY) normalization learning will be adopted. Endowed with the automatic model selection ability of BYY learning, the BYY normalization learning-based local PCA model can cope with those object detection and star/galaxy classification tasks with unknown model complexity. A detailed algorithm for implementation of the local PCA model will be proposed, and experimental results using both synthetic and real astronomical data will be demonstrated.",space
10.1016/S0925-2312(02)00450-2,Journal,Neurocomputing,scopus,2002-07-27,sciencedirect,A real-scale anatomical model of the dentate gyrus based on single cell reconstructions and 3D rendering of a brain atlas,https://api.elsevier.com/content/abstract/scopus_id/0036064039,"As a first step towards the creation of a cellular model of dentate gyrus (DG) anatomy, we distributed 1,000,000 digitized granule cells (gcs) in 3D in a virtual reality reconstruction of Swanson's brain atlas. DG coronal sections were assembled into 3D surfaces using implicit function generation. The resulting file included hilar, granular, and molecular boundaries. 20,000 replicas of each of 50 reconstructed gcs were added to the model by packing the somata in the appropriate layer and then radially orienting the dendritic tree axes. The model can be used to evaluate stereologic parameters such as dendritic overlap probability, space occupancy, and exposure to incoming fibers.",space
10.1016/S0165-0114(01)00034-3,Journal,Fuzzy Sets and Systems,scopus,2002-03-16,sciencedirect,A fast learning algorithm for parsimonious fuzzy neural systems,https://api.elsevier.com/content/abstract/scopus_id/0037117202,"In this paper, a novel learning algorithm for dynamic fuzzy neural networks based on extended radial basis function neural networks, which are functionally equivalent to Takagi–Sugeno–Kang fuzzy systems, is proposed. The algorithm comprises 4 parts: (1) criteria of rules generation; (2) allocation of premise parameters; (3) determination of consequent parameters and (4) pruning technology. The salient characteristics of the approach are: (1) a hierarchical on-line self-organizing learning paradigm is employed so that not only parameters can be adjusted, but also the determination of structure can be self-adaptive without partitioning the input space a priori; (2) fast learning speed can be achieved so that the system can be implemented in real time. Simulation studies and comprehensive comparisons with some other learning algorithms demonstrate that the proposed algorithm is superior in terms of simplicity of structure, learning efficiency and performance.",space
10.1016/S0950-7051(01)00151-4,Journal,Knowledge-Based Systems,scopus,2001-11-01,sciencedirect,Specifying fault tolerance in mission critical intelligent systems,https://api.elsevier.com/content/abstract/scopus_id/0035505448,"Real time intelligent systems are being increasingly used in mission critical applications in domains like military, aerospace, process control industry and medicine. Despite this vast potential, the major concern about deploying mission critical intelligent systems is their dependability. Dependability encompasses such notions as reliability, safety, security, maintainability and portability. A major concern about mission critical intelligent systems is their performance in the presence of failures. Intelligent systems are characterized by often non-existent, imprecise or rapidly changing specifications. This makes the task of characterizing an intelligent system's performance in the presence of failures much more difficult. In this paper, we characterize the failures that are likely in a mission critical intelligent system. We propose an extended I/O automata model to capture these failure specifications. We further demonstrate how these specifications can be realized in a real time expert system by structuring the knowledge base. This formalism can also be used to specify the fault tolerant properties of the underlying hardware and software over which the intelligent system resides. Thus we have an unified formalism to specify fault tolerance properties in hardware, system software and the intelligent system. This will enable us to reason about the performance of the entire system inclusive of all its components in an uniform manner.",space
10.1016/S0952-1976(01)00031-8,Journal,Engineering Applications of Artificial Intelligence,scopus,2001-10-01,sciencedirect,Reinforcement learning control of nonlinear multi-link system,https://api.elsevier.com/content/abstract/scopus_id/0035493967,"In this paper, the effects of basic parameters in reinforcement learning control such as eligibility, action and critic network constrained weights, system nonlinearities, gradient information, state-space partitioning, variance of exploration are studied in detail. It is attempted to increase feasibility for practical applications, implementation, learning efficiency, and enhance performance. Also, a novel adaptive grid algorithm is proposed to overcome the difficulty in partitioning the input space to achieve better performance. Reinforcement learning is applied for control of a nonlinear one and two-link robots. This problem dictates that the learning is performed on-line, based on a binary or real-valued reinforcement signal from a critic network, without knowing the system model or nonlinearity.",space
10.1016/S0921-8890(01)00113-0,Journal,Robotics and Autonomous Systems,scopus,2001-07-31,sciencedirect,Acquisition of stand-up behavior by a real robot using hierarchical reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/0035979437,"In this paper, we propose a hierarchical reinforcement learning architecture that realizes practical learning speed in real hardware control tasks. In order to enable learning in a practical number of trials, we introduce a low-dimensional representation of the state of the robot for higher-level planning. The upper level learns a discrete sequence of sub-goals in a low-dimensional state space for achieving the main goal of the task. The lower-level modules learn local trajectories in the original high-dimensional state space to achieve the sub-goal specified by the upper level.
                  We applied the hierarchical architecture to a three-link, two-joint robot for the task of learning to stand up by trial and error. The upper-level learning was implemented by Q-learning, while the lower-level learning was implemented by a continuous actor–critic method. The robot successfully learned to stand up within 750 trials in simulation and then in an additional 170 trials using real hardware. The effects of the setting of the search steps in the upper level and the use of a supplementary reward for achieving sub-goals are also tested in simulation.",space
10.1016/S0954-1810(01)00003-6,Journal,Artificial Intelligence in Engineering,scopus,2001-07-01,sciencedirect,A low-cost Internet-based telerobotic system for access to remote laboratories,https://api.elsevier.com/content/abstract/scopus_id/0035385318,"This paper presents an account of the design of a low-cost Internet-based teleoperation system implemented on China's Internet. Using a multimedia-rich human-computer interface, combining predictive displays and graphical overlays, a series of simple tasks were performed within a simulated space environment scenario. Internet clients anywhere can monitor the robotic workspace, talk with technicians, and control the Arm/Hand integrated system with 15DOF located in lab to perform tasks (such as grasping a vessel, pouring a liquid, and peg-in-hole assembling, etc.). Our main contributions are to establish a foundation for teleoperated science and engineering research, and we have addressed some issues involving the time-delay associated with the Internet. We also developed several key software adaptation technologies and products used for Internet-Based teleoperation, compatible with the BH-III dexterous hand, BH1 6-DOF mechanical arm and five-finger 11-DOF data glove, constructed in our laboratory. This system has been successfully tested and applied in remote robotic education (Virtual Laboratories) system via China's Internet using our Master/Slave architecture, which combines mixed modes of remote monitor/manipulate and local autonomous control.",space
10.1016/S0020-0255(01)00149-9,Journal,Information Sciences,scopus,2001-01-01,sciencedirect,Learning fuzzy classifier systems for multi-agent coordination,https://api.elsevier.com/content/abstract/scopus_id/0035426729,"We present ELF, a learning fuzzy classifier system (LFCS), and its application to the field of Learning Autonomous Agents. In particular, we will show how this kind of Reinforcement Learning systems can be successfully applied to learn both behaviors and their coordination for Autonomous Agents. We will discuss the importance of knowledge representation approach based on fuzzy sets to reduce the search space without losing the required precision. Moreover, we will show how we have applied ELF to learn the distributed coordination among agents which can exchange information with each other. The experimental validation has been done on software agents interacting in a real-time task.",space
10.1016/S0965-9978(00)00031-4,Journal,Advances in engineering software,scopus,2000-01-01,sciencedirect,CORBA extension for intelligent software environments,https://api.elsevier.com/content/abstract/scopus_id/0343714795,"We describe the implementation of a technology that achieves system-wide properties in large software systems by controlling and modifying inter-component communications. Traditional component-based applications intermix the code for component functionality with support for systematic properties. This produces non-reusable components and inflexible systems. The Object Infrastructure Framework (OIF) separates systematic properties from functional code and provides a mechanism for weaving them together with functional components. This allows a much richer variety of component reuse and system evolution. Key elements of this technology include intercepting inter-component communications with discrete, dynamically configurable “injectors”, annotating communications and processes with additional meta-information, and a high-level, declarative specification language for describing the mapping between desired system properties and services that achieve these properties. We have implemented these ideas in a CORBA/Java framework for distributed computing, and are currently applying them to a distributed system for the analysis of aerospace design (wind-tunnel and CFD) data.",space
10.1016/S0893-6080(99)00102-1,Journal,Neural Networks,scopus,2000-01-01,sciencedirect,Partially pre-calculated weights for the backpropagation learning regime and high accuracy function mapping using continuous input RAM-based sigma-pi nets,https://api.elsevier.com/content/abstract/scopus_id/0033980299,"In this article we present a methodology that partially pre-calculates the weight updates of the backpropagation learning regime and obtains high accuracy function mapping. The paper shows how to implement neural units in a digital formulation which enables the weights to be quantised to 8-bits and the activations to 9-bits. A novel methodology is introduced to enable the accuracy of sigma–pi units to be increased by expanding their internal state space. We, also, introduce a novel means of implementing bit-streams in ring memories instead of utilising shift registers. The investigation utilises digital “Higher Order” sigma–pi nodes and studies continuous input RAM-based sigma–pi units. The units are trained with the backpropagation learning regime to learn functions to a high accuracy. The neural model is the sigma–pi units which can be implemented in digital microelectronic technology.
                  The ability to perform tasks that require the input of real-valued information, is one of the central requirements of any cognitive system that utilises artificial neural network methodologies. In this article we present recent research which investigates a technique that can be used for mapping accurate real-valued functions to RAM-nets. One of our goals was to achieve accuracies of better than 1% for target output functions in the range Y∈[0,1], this is equivalent to an average Mean Square Error (MSE) over all training vectors of 0.0001 or an error modulus of 0.01. We present a development of the sigma–pi node which enables the provision of high accuracy outputs. The sigma–pi neural model was initially developed by Gurney (Learning in nets of structured hypercubes. PhD Thesis, Department of Electrical Engineering, Brunel University, Middlessex, UK, 1989; available as Technical Memo CN/R/144). Gurney's neuron models, the Time Integration Node (TIN), utilises an activation that was derived from a bit-stream. In this article we present a new methodology for storing sigma–pi node's activations as single values which are averages.
                  In the course of the article we state what we define as a real number; how we represent real numbers and input of continuous values in our neural system. We show how to utilise the bounded quantised site-values (weights) of sigma–pi nodes to make training of these neurocomputing systems simple, using pre-calculated look-up tables to train the nets. In order to meet our accuracy goal, we introduce a means of increasing the bandwidth capability of sigma–pi units by expanding their internal state-space. In our implementation we utilise bit-streams when we calculate the real-valued outputs of the net. To simplify the hardware implementation of bit-streams we present a method of mapping them to RAM-based hardware using ‘ring memories’. Finally, we study the sigma–pi units’ ability to generalise once they are trained to map real-valued, high accuracy, continuous functions. We use sigma–pi units as they have been shown to have shorter training times than their analogue counterparts and can also overcome some of the drawbacks of semi-linear units (Gurney, 1992. Neural Networks, 5, 289–303).",space
10.1016/S0030-4018(99)00359-4,Journal,Optics Communications,scopus,1999-09-15,sciencedirect,Vector-product Hopfield model,https://api.elsevier.com/content/abstract/scopus_id/0033344055,"For pattern recognition, we are frequently faced with the problem of recognition for a real-world three-dimensional object. The mathematical vector-product algorithm in three-dimensional space is introduced into the neural network domain, and a new type of neural network model — vector-product Hopfield model — is proposed. Computer simulations show that the vector-product Hopfield model can recall the entire stored three-dimensional vectors at a high recognition rate with the partial information of the stored vectors in one or two dimensions only. Such a performance cannot be realized with the Hopfield model by simply presenting only one-third or two-thirds of the stored vectors. Thus, the proposed model is highly interesting for further developments of neural network models and practical applications. Preliminary optical experimental implementation is also given.",space
10.1016/S0031-3203(98)00159-9,Journal,Pattern Recognition,scopus,1999-07-01,sciencedirect,Curvature scale-space-driven object recognition with an indexing scheme based on artificial neural networks,https://api.elsevier.com/content/abstract/scopus_id/0033167353,"This paper addresses the problem of recognizing real flat objects from two-dimensional images. In particular, a new object recognition technique which performs under occlusion and geometric transformations is presented. The method has mainly been designed to handle complex objects and incorporates two main ideas. First, matching operates hierarchically, guided by a curvature scale space segmentation scheme, and takes advantage of important object features, that is, features which distinguish an object from other objects. This is different from many classical approaches which employ a rather large number of very local features. Second, the model database is built by using artificial neural networks (ANNs). This is also different from traditional approaches where classical indexing schemes, such as hashing, are utilized to organize and search the model database. Important object features are obtained in two steps: first, by segmenting the object boundary at multiple scales using its resampled curvature scale space (RCSS) and second, by concentrating at each scale separately, searching for groups of segments which distinguish an object from other objects. These groups of segments are then used to build a model database which stores associations between segments and models. The model database is implemented using a set of ANNs which provide the essential mechanism not only for establishing correct associations between groups of segments and models but also for enabling efficient searching and robust retrieval. The method has been tested using both artificial and real data illustrating good performance.",space
10.1016/S0164-1212(99)00019-9,Journal,Journal of Systems and Software,scopus,1999-05-01,sciencedirect,Multi-agent tuple-space based problem solving framework,https://api.elsevier.com/content/abstract/scopus_id/0033121515,"Many real-life problems are inherently distributed. The applications may be spatially distributed, such as interpreting and integrating data from spatially distributed sources. It is also possible to have the applications being functionally distributed, such as bringing together a number of specialized medical-diagnosis systems on a particularly difficult case. In addition, the applications might be temporally distributed, as in a factory where the production line consists of several work areas, each having an expert system responsible for scheduling orders. Research in cooperating intelligent systems has led to the development of many computational models (Huhns and Singh 1992, Hewitt 1991, Gasser 1991, Polat et al. 1993, Polat and Guvenir 1993, 1994, Shoham 1993 
                     Sycara, 1989) for coordinating several intelligent systems for solving complex problems involving diverse knowledge and activity. In this paper, a system for coordinating problem solving activities of multiple intelligent agents using the tuple space model of computation is described. The tuple space based problem solving framework is implemented on an Intel Hypercube iPSC/2 allowing multiple rule-based systems performing their dedicated tasks in parallel. The tasks are interrelated, in other words, there exists a partial order among the tasks which have to be performed.",space
10.1016/S0273-1177(99)00318-X,Journal,Advances in Space Research,scopus,1999-01-01,sciencedirect,Development of autonomous control in a closed microbial bioreactor,https://api.elsevier.com/content/abstract/scopus_id/0033374124,"Space-based life support systems which include ecological components will rely on sophisticated hardware and software to monitor and control key system parameters. Autonomous closed artificial ecosystems are useful for research in numerous fields. We are developing a bioreactor designed to study both microbe-environment interactions and autonomous control systems. Currently we are investigating N-cycling and N-mass balance in closed microbial systems. The design features of the system involve real-time monitoring of physical parameters (e.g. temperature, light), growth solution composition (e.g. pH, NOx, CO2), cell density and the status of important hardware components. Control of key system parameters is achieved by incorporation of artificial intelligence software tools that permit autonomous decision-making by the instrument. These developments provide a valuable research tool for terrestrial microbial ecology, as well as a testbed for implementation of artificial intelligence concepts. Autonomous instrumentation will be necessary for robust operation of space-based life support systems, and for use on robotic spacecraft. Sample data acquired from the system, important features of software components, and potential applications for terrestrial and space research will be presented.",space
10.1016/S0167-9236(99)00055-X,Journal,Decision Support Systems,scopus,1999-01-01,sciencedirect,Partitioning-based clustering for Web document categorization,https://api.elsevier.com/content/abstract/scopus_id/0033323923,"Clustering techniques have been used by many intelligent software agents in order to retrieve, filter, and categorize documents available on the World Wide Web. Clustering is also useful in extracting salient features of related Web documents to automatically formulate queries and search for other similar documents on the Web. Traditional clustering algorithms either use a priori knowledge of document structures to define a distance or similarity among these documents, or use probabilistic techniques such as Bayesian classification. Many of these traditional algorithms, however, falter when the dimensionality of the feature space becomes high relative to the size of the document space. In this paper, we introduce two new clustering algorithms that can effectively cluster documents, even in the presence of a very high dimensional feature space. These clustering techniques, which are based on generalizations of graph partitioning, do not require pre-specified ad hoc distance functions, and are capable of automatically discovering document similarities or associations. We conduct several experiments on real Web data using various feature selection heuristics, and compare our clustering schemes to standard distance-based techniques, such as hierarchical agglomeration clustering, and Bayesian classification methods, such as AutoClass.",space
10.1016/S0004-3702(99)00025-9,Journal,Artificial Intelligence,scopus,1999-01-01,sciencedirect,"Task decomposition, dynamic role assignment, and low-bandwidth communication for real-time strategic teamwork",https://api.elsevier.com/content/abstract/scopus_id/0033149662,"Multi-agent domains consisting of teams of agents that need to collaborate in an adversarial environment offer challenging research opportunities. In this article, we introduce periodic team synchronization (PTS) domains as time-critical environments in which agents act autonomously with low communication, but in which they can periodically synchronize in a full-communication setting. The two main contributions of this article are a flexible team agent structure and a method for inter-agent communication. First, the team agent structure allows agents to capture and reason about team agreements. We achieve collaboration between agents through the introduction of formations. A formation decomposes the task space defining a set of roles. Homogeneous agents can flexibly switch roles within formations, and agents can change formations dynamically, according to pre-defined triggers to be evaluated at run-time. This flexibility increases the performance of the overall team. Our teamwork structure further includes pre-planning for frequently occurring situations. Second, the communication method is designed for use during the low-communication periods in PTS domains. It overcomes the obstacles to inter-agent communication in multi-agent environments with unreliable, single-channel, high-cost, low-bandwidth communication. We fully implemented both the flexible teamwork structure and the communication method in the domain of simulated robotic soccer, and conducted controlled empirical experiments to verify their effectiveness. In addition, our simulator team made it to the semi-finals of the RoboCup-97 competition, in which 29 teams participated. It achieved a total score of 67–9 over six different games, and successfully demonstrated its flexible teamwork structure and inter-agent communication.",space
10.1016/S0020-0255(98)10064-6,Journal,Information Sciences,scopus,1999-01-01,sciencedirect,A real-time system-adapted anomaly detector,https://api.elsevier.com/content/abstract/scopus_id/0033117079,"We present techniques for detecting anomalies in the performance of computer systems. We take both “performance” and “computer systems” quite generally. We assume that samples of performance data are available both in real-time and also historically from past normal performance of the target system. In addition we assume that there is some vector space and each sample is a point in that vector space. Detections are made by comparing the most recent sample of data with the historical data, thus yielding detections in real-time which are adaptive to the particular target system. Our techniques are probabilistic but are distribution-free and otherwise make relatively mild assumptions. We present some supporting mathematics that shows how to select the false alarm rate. We address detection rate with a general theorem and also with a specific simulation experiment. The data used can be of wide variety, possibly finely grained, and can be collected from hardware, system software, software subsystems, applications, or distributed applications. Our techniques may be of value in various aspects of future computer systems and in other systems as well.",space
10.1016/S0927-5452(98)80009-9,Book Series,Advances in Parallel Computing,scopus,1998-01-01,sciencedirect,"Application of a multi-processor system for recognition of EEG-activities in amplitude, time and space in real-time",https://api.elsevier.com/content/abstract/scopus_id/85023117179,"The EEG system BrainScope consists of a special amplifier system for high quality signal detection in open field conditions during communicative situations. A high performance multi-processor system which is capable of processing the huge amounts of data produced by a multichannel EEG record to gain information in real-time has also been developed. Algorithms for recognition of events in single channels are implemented in the first level of the multi-processor system. We use high performance image processing algorithms in the second level, interpreting the sampled values of each channel as pixels of the image, 256 up to 2.000 times per second. This patented method describes the EEG activity as sequences of virtual sources in parameters of amplitude, time and space. Fuzzy logic and methods of AI are used to define and recognise sequences of virtual sources in real-time.
                  The network of two or more Personal Computers (PC’s) is co-ordinated through the multiprocessor system for presentation of EEG activity and controlling. Multi-media approaches to the application of psychological tests are possible through the user interface including tests in media of sound, words, pictures and moving pictures. These tests can be arranged and carried out in computer controlled sequences and modified by user interactions. Tools are also provided to allow the user to create his own tests. These methods are integrated into the powerful graphic user interface and uses a database system. Incorporated into this user interface are state of the art EEGSYS algorithms from the NIMH (Washington / USA) for mappings, FFT, etc.
                  The BrainScope demonstrates the impacts and applications of the new strategy for EEG investigation in communicative situations between:
                        
                           -
                           patient and physician for subjective evaluation,
                        
                        
                           -
                           patient and information technology for stimulation and acquisition of signals and reactions,
                        
                        
                           -
                           physician and information technology for quantitative analysis of signals and reactions. The major advantage of this new strategy is that the three processes can be carried out in realtime. It optimises the capacity of humans to interpret information with the capability of modern information technology to manipulate and process data. It therefore requires use with an experienced and trained physician who can make accurate observations during the process of an investigation. The physician can, for example, click on a significant EEG pattern (this makes it a further recognisable phenomenon through fuzzy logic) and correlate it with his own observations. The multi-processor system recognises this EEG activity, i.e. it interprets this as a possible description of the state of the brain, sets a defined stimulus and recognises and evaluates the Event Related Potential (ERP) immediately.",space
10.1016/S0928-4869(97)00003-7,Journal,Simulation Practice and Theory,scopus,1997-10-15,sciencedirect,A virtual testbed for analysis and design of sensorimotoric aspects of agent control,https://api.elsevier.com/content/abstract/scopus_id/0042544183,"In this paper 
                        XRaptor
                      is introduced, an object-oriented simulation tool. It provides a virtual multi-agent world which acts as testbed for agent control mechanisms. This environment encompasses a 3-dimensional space, in which the agents may move. Currently agents are realized modelling some abstract properties of flies and bats. 
                        XRaptor
                      provides different levels of information flow and world manipulation capabilities from the agents' point of view. A further purpose of 
                        XRaptor
                      is educational: Different teams of developers may design control units for agents which can then be subjected to a tournament.",space
10.1016/S0893-6080(97)00029-4,Journal,Neural Networks,scopus,1997-08-01,sciencedirect,Associative list memory,https://api.elsevier.com/content/abstract/scopus_id/0031214817,"This paper introduces an Associative List Memory (ALM) that has high recall fidelity with low memory and low processing requirements. This permits a simple implementation in software on a personal computer or space instrument microprocessor. Associative List Memory has a performance comparable with Sparse Distributed Memory (SDM) but differs from SDM in that convergence occurs during learning, rather than on recall, and in that the memory is in the form of a dynamic list rather than static randomly distributed locations. Associative List Memory is suitable for unsupervised finding of classes of phenomena in large databases. In particular, all of the class exemplars deduced can be easily accessed at any time to provide a summary of current database knowledge, being essentially the contents of the list. Examples are given where patterns of 1000 bits length with > 30% noise can be learned unsupervised to deduce the original pattern's noise free. A second pass through the data in recall mode can be used to assign to each input the appropriate original pattern, effectively removing all noise from the input data. At large input bit sizes the recall fidelity approaches closely to the maximum possible value. Associative List Memory compares well in recall fidelity with SDM and other associative memories. Its processing times on a personal computer are found to be practical for database applications. Implemented within a space instrument processor, ALM would greatly reduce downlink data transmission rates. © 1997 Elsevier Science Ltd.",space
10.1006/jvlc.1997.0042,Journal,Journal of Visual Languages and Computing,scopus,1997-01-01,sciencedirect,"A parallel, distributed and associative approach for searching image patterns with holographic dynamics",https://api.elsevier.com/content/abstract/scopus_id/0031167601,"This paper presents a new associative pattern-matching network based on a digital adaptation of optical holography. Unlike any existing neural network or associative memories, it can localize its search dynamically on any subset of the pattern space and at the same time generate a feedback on the quality of the match. Current associative memories based on neuro computing are unable to support such meta-interactions. The scheme involves adaptive ‘enfolding’ of the raw massive search space into a holograph and direct regeneration of the matched target pattern during a search. The search process is a constant-time operation compared to traditional algorithm approaches, inherently parallelizable, and is an excellent candidate for hardware or optical implementation. This new technique is expected to facilitate significantly applications that require direct pattern matching in massive image repositories in real time.Target recognition,visual query,content-based image retrieveandautomatic index-extractionare just a few of such applications.",space
10.1016/s0952-1976(96)00084-x,Journal,Engineering Applications of Artificial Intelligence,scopus,1997-01-01,sciencedirect,Collision-free motion planning for redundant robots using neural-network processing,https://api.elsevier.com/content/abstract/scopus_id/0031124335,"A computationally efficient obstacle-avoidance algorithm for redundant robots is presented in this paper. The algorithm incorporates Tank-Hopfield networks and the J function in the framework of resolved motion rate control, which is well suited for real-time implementation. Robot-arm kinematic control is carried out by the TH network. The connection weights of the network can be directly obtained from the known matrices at each sampling time, and joint velocity commands are generated from the outputs of the network. The obstacle-avoidance task can be achieved by formulating the performance criterion J > J
                     
                        min
                      (J
                     
                        min
                      represents the minimal distance between the redundant robot and obstacles), and by using a null space to ensure that this criterion is satisfied. Several simulation cases for a four-link planar manipulator are given, to demonstrate how the proposed collision free trajectory planning scheme works.",space
10.1016/S0921-8890(96)00026-7,Journal,Robotics and Autonomous Systems,scopus,1997-01-01,sciencedirect,Efficient parallel processing for depth calculation using stereo,https://api.elsevier.com/content/abstract/scopus_id/0031121096,"Stereo vision generates the depth map of a scene by fusing information in images taken from two or more views. Stereo is a computationally intensive task and an efficient real time stereo application needs either a dedicated hardware or a parallel computer. This paper proposes a parallel stereo algorithm developed at CAIR using both edges and regions as features for matching. The algorithm employs an intelligent stereo matching technique that reduces the search space for correspondence thereby reducing the computational load. A load balancing scheme is also proposed for further improving the efficiency of the algorithm, with the increase in the number of nodal processors. The algorithm is implemented on a PACE parallel processor developed at the DRDO laboratory ANURAG at Hyderabad, India, and its capability is demonstrated through an application example.",space
10.1016/S0968-090X(97)00007-7,Journal,Transportation Research Part C: Emerging Technologies,scopus,1997-01-01,sciencedirect,A parallel algorithm to extract information about the motion of road traffic using image analysis,https://api.elsevier.com/content/abstract/scopus_id/0031120013,"Road traffic movement is a very important source of information in traffic management. Although systems exist which can detect the presence of a vehicle and its speed under certain conditions, there is generally a lack of effective means to measure both the speed and direction of traffic movement. This is particularly true for road junctions, where conflicting traffic shares the same space and where some control strategy could be more effectively applied with the help of speed and direction estimates. The increasing use of closed circuit television (CCTV) systems has provided the opportunity to apply image processing techniques to extract such information. However, such techniques are computationally intensive in general, and the application of parallel processing methods is one of the best choices which could bring the desired acquisition of movement information into practical reality. This paper describes a parallel video-based image analysis system which is capable of extracting movement information, including direction and speed, of road vehicular traffic over any part of a road surface. The prototype has been implemented on an array of 36 transputers and an image grabber with a SUN SPARC IPC as the host machine. The software mainly consists of median filtering, feature extraction, spatio-temporal analysis, matching of image features in successive images by neural networks and aggregation of matched results. This algorithm has been tested using data for a signal-controlled junction aiming to capture an opposed turning traffic movement with promising results. It has also been shown that a real-time system based on the described algorithm is feasible.",space
10.1016/S0360-1323(96)00043-1,Journal,Building and Environment,scopus,1997-01-01,sciencedirect,An artificial intelligence approach to the prediction of natural lighting levels,https://api.elsevier.com/content/abstract/scopus_id/0031095364,"Controlling artificial lights within buildings to act solely as a supplement to available daylighting requires continuous knowledge of natural lighting levels within a space. Although this information is readily obtained by measurement whilst lights are extinguished, once illuminated the determination of the underlying natural light level is not so straightforward. This paper describes the use of a Genetic Algorithm (GA) at the heart of a self-commissioning, adaptive algorithm capable of the real-time prediction of natural light levels at chosen points within a room using external measurements of vertical plane illuminance. The algorithm is extremely compact and efficient and readily implemented on a microprocessor. As such, it is suggested that it could form the basis of a robust and practical lighting controller.",space
10.1016/s0004-3702(96)00031-8,Journal,Artificial Intelligence,scopus,1997-01-01,sciencedirect,Permissive planning: Extending classical planning to uncertain task domains,https://api.elsevier.com/content/abstract/scopus_id/0030712509,"Uncertainty, inherent in most real-world domains, can cause failure of apparently sound classical plans. On the other hand, reasoning with representations that explicitly reflect uncertainty can engender significant, even prohibitive, additional computational costs. This paper contributes a novel approach to planning in uncertain domains. The approach is an extension of classical planning. Machine learning is employed to adjust planner bias in response to execution failures. Thus, the classical planner is conditioned towards producing plans that tend to work when executed in the world.
                  The planner's representations are simple and crisp; uncertainty is represented and reasoned about only during learning. The user-supplied domain theory is left intact. The operator definitions and the planner's projection ability remain as the domain expert intended them. Some structuring of the planner's bias space is required. But with suitable structuring the approach scales well. The learning converges using no more than a polynomial number of examples. The system then probabilistically guarantees that either the plans produced will achieve their goal when executed or that adequate planning is not possible with the domain theory provided. An implemented robotic system is described.",space
10.1016/0165-0114(95)00259-6,Journal,Fuzzy Sets and Systems,scopus,1996-01-01,sciencedirect,A prolog-like inference system based on neural logic - An attempt towards fuzzy neural logic programming,https://api.elsevier.com/content/abstract/scopus_id/0030576822,"Research under the name of Neural Logic Networks is an attempt to integrate connectionist models and logic reasoning [8, 9]. With a Neural Logic Network, a simple neural network structure with suitable weight(s) can be used to represent a set of flexible operations, which offer increased possibilities in dealing with inference in real-world problem solving. They also possess useful properties in an extended logic system which is called Neural Logic. One of the important features of Neural Logic is that all its operations can be defined and realized by neural networks, which form Neural Logic Networks. As one part of the research on Neural Logic Networks, fuzzy neural logic programming has been proposed [6]. This paper introduces a Prolog-like inference system based on Neural Logic as an implementation of fuzzy neural logic programming. In this system, fuzzy reasoning is executed by the Neural Logic inference engine with incomplete or uncertain knowledge. The framework of the system and its inference mechanism are described.",space
10.1016/0376-0421(95)00011-9,Journal,Progress in Aerospace Sciences,scopus,1996-01-01,sciencedirect,Neural networks: Applications and opportunities in aeronautics,https://api.elsevier.com/content/abstract/scopus_id/0030263707,"Technologies based on neural networks are currently being developed which may assist in addressing a wide range of complex problems in aeronautics. The review indicates that the proper utilization of this technology offers a feasible approach to help meet current and future technological needs. This might include, but is not limited to, the following: the implementation of active control devices to harness or suppress unsteady aerodynamic effects such as dynamic stall on helicopter rotor blades; the parallel data processing of 10s to 1000s of sensors either for actuation and control or for system and component ‘health’ monitoring and fault detection; the development of simulators and control algorithms for severe, ‘unsteady’, six degree-of-freedom vehicle maneuvers where the linearized equations of motion do not adequately describe the vehicle dynamics; and the requirement to monitor these sensors, simulate the maneuvers and instigate control all on a time-scale which must be faster than the real-time phenomena. Clearly, neural networks alone will not solve these problems. However, neural networks provide a practical approach for determining solutions of complex nonlinear problems such as equations of motion, for the parallel processing of 1000s of sensors, and this can be achieved with the required computational speed.",space
10.1016/0094-5765(95)00058-8,Journal,Acta Astronautica,scopus,1995-01-01,sciencedirect,"Concepts for automated, intelligent control of advanced μg-Facilities",https://api.elsevier.com/content/abstract/scopus_id/58149210228,"Manned space flights experienced large complexity, long preparation time and high costs. For this reason they are reduced and the operations for micro-g research are changing from crew control toward telescience and automated control. Telescience, however requires nearly permanent real-time data transmission —a condition that cannot be fulfilled by many missions. Then “intelligent and autonomous control” is the only way out. Autonomous supervisor, Artificial intelligence, Real-time video elaboration and Expert system are notions of new techniques coming up in ground based application where they help to find the optimum operational conditions or are used for very fast decisions counter-acting critical phases in complex systems. Today it is natural to ask for an analysis on how these applications can be used in the control of experiments in micro-g research. In the presently running study, called SEMIR and supported by ESA, the possibilities, the chances, but also the constraints of the intelligent control system are investigated. Their implementation in the field of crystal growth, protein crystallization, critical point phenomena and fluid physics is analyzed.
                  In this paper a preliminary output of this study is given.",space
10.1016/0952-1976(95)00044-5,Journal,Engineering Applications of Artificial Intelligence,scopus,1995-01-01,sciencedirect,"Dependable, intelligent voting for real-time control software",https://api.elsevier.com/content/abstract/scopus_id/0029491715,"An intelligent and dependable voting mechanism for use in real-time control applications is presented. Strategies proposed by current safety standards advocate N-version software to minimize the effects of undetected software design faults (bugs). This requires diversity in design but presents a problem in that truly diverse code produces diverse results; that is, differences in output values, timeliness and reliability. Reaching a consensus requires an intelligent voter, especially when non-stop operation is demanded, e.g. in aerospace applications. This paper, therefore, firstly considers the applicable safety standards and the requirements for an intelligent voter service. The use of replicated voters to improve reliability is examined and a mechanism to ensure non-stop operation is presented. The formal mathematical analysis used to verify the crucial behavioural properties of the voting service design is detailed. Finally, the use of neural nets and genetic algorithms to create N- version redundant voters, is considered.",space
10.1016/0004-3702(94)00089-J,Journal,Artificial Intelligence,scopus,1995-01-01,sciencedirect,CABINS: a framework of knowledge acquisition and iterative revision for schedule improvement and reactive repair,https://api.elsevier.com/content/abstract/scopus_id/0029332288,"Practical scheduling problems generally require allocation of resources in the presence of a large, diverse and typically conflicting set of constraints and optimization criteria. The ill-structuredness of both the solution space and the desired objectives make scheduling problems difficult to formalize. This paper describes a case-based learning method for acquiring context-dependent user optimization preferences and tradeoffs and using them to incrementally improve schedule quality in predictive scheduling and reactive schedule management in response to unexpected execution events. The approach, implemented in the CABINS system, uses acquired user preferences to dynamically modify search control to guide schedule improvement. During iterative repair, cases are exploited for: (1) repair action selection, (2) evaluation of intermediate repair results and (3) recovery from revision failures. The method allows the system to dynamically switch between repair heuristic actions, each of which operates with respect to a particular local view of the problem and offers selective repair advantages. Application of a repair action tunes the search procedure to the characteristics of the local repair problem. This is achieved by dynamic modification of the search control bias. There is no a priori characterization of the amount of modification that may be required by repair actions. However, initial experimental results show that the approach is able to (a) capture and effectively utilize user scheduling preferences that were not present in the scheduling model, (b) produce schedules with high quality, without unduly sacrificing efficiency in predictive schedule generation and reactive response to unpredictable execution events along a variety of criteria that have been recognized as important in real operating environments.",space
10.1016/0098-1354(94)E0034-K,Journal,Computers and Chemical Engineering,scopus,1995-01-01,sciencedirect,Building a chemical process design system within soar-1. Design issues,https://api.elsevier.com/content/abstract/scopus_id/0029236841,"We explore the potential to include automatic learning in a design agent by implementing a simple distillation sequencing system, CPD-Soar, within Soar. Soar is an integrated software architecture with a build-in set of mechanisms for exhibiting intelligent behavior, including problem-solving, learning and interaction with the environment. Soar has a number of scientific uses: computer scientists build artificially intelligent agents with Soar as a foundation, and cognitive psychologists use Soar to model human cognition. CPD-Soar illustrates how design-related tasks can be cast within Soar's framework, hence demonstrating the functioning and potential of its problem- solving and learning mechanisms. This simple example system, which involves computations with real numbers, automatically learns things which are too specific, leading to the hypothesis that the generalization an agent infers from specific examples is strongly dependent upon the model the agent brings to the learning process.
                  We introduced the Soar architecture as a vehicle for developing design systems with capabilities seen to be important for chemical process domains but missing in most existing design systems. We reported upon CPD-Soar, a system developed within the Soar framework, and described in depth its tasks, problem-space structure, operation and performance.
                  The construction of CPD-Soar was a valuable exercise for two main reasons: one, it provided evidence that the mechanisms present in Soar can provide design systems with useful abilities, and two, the act of creating the system was useful in distinguishing between those aspects of the task domain that are well understood from those that are not. Selecting among competent evaluation functions and learning within numerically-intensive domains were two areas identified as not being well understood.
                  Our observation of CPD-Soar's learning behavior lead us to postulate an hypothesis about learning: namely, that the richer the model an agent has of its evaluation functions, the more general its learning will be.",space
10.1006/dspr.1994.1016,Journal,Digital Signal Processing,scopus,1994-01-01,sciencedirect,Minimal topology for a radial basis functions neural network for pattern classification,https://api.elsevier.com/content/abstract/scopus_id/33747746017,"In the context of pattern classification, the success of a classification scheme often depends on the geometrical properties of the pattern classes under consideration. As radial basis functions (RBF) neural networks have largely been applied in pattern classification problems, in this paper we present a brief overview of different trends in radial basis functions neural networks and their applications. The meanings of the weights and the processing units for a RBF network applied for pattern classification are given. A new learning algorithm for a RBF neural network is proposed in this paper. This algorithm gives a solution for classifying configurations of patterns in a feature space providing the minimum number of hidden units for the network implementation. The learning is based on the backpropagation algorithm. The performance of the proposed algorithm is assessed on different artificial and real applications. The algorithm is successfully applied for estimating a distribution, as well as for separating signals in a multiple access communication system and for recognizing static speech.",space
10.1016/0004-3702(94)90104-X,Journal,Artificial Intelligence,scopus,1994-01-01,sciencedirect,Exploiting the deep structure of constraint problems,https://api.elsevier.com/content/abstract/scopus_id/0028529156,"We introduce a technique for analyzing the behavior of sophisticated AI search programs working on realistic, large-scale problems. This approach allows us to predict where, in a space of problem instances, the hardest problems are to be found and where the fluctuations in difficulty are greatest. Our key insight is to shift emphasis from modelling sophisticated algorithms directly to modelling a search space that captures their principal effects. We compare our model's predictions with actual data on real problems obtained independently and show that the agreement is quite good. By systematically relaxing our underlying modelling assumptions we identify their relative contribution to the remaining error and then remedy it. We also discuss further applications of our model and suggest how this type of analysis can be generalized to other kinds of AI problems.",space
10.1016/0004-3702(94)90097-3,Journal,Artificial Intelligence,scopus,1994-01-01,sciencedirect,Investigating production system representations for non-combinatorial match,https://api.elsevier.com/content/abstract/scopus_id/0028464184,"Eliminating combinatorics from the match in production systems (or rule-based systems) is important for expert systems, real-time performance, machine learning (particularly with respect to the utility issue), parallel implementations and cognitive modeling. In [74], the unique-attribute representation was introduced to eliminate combinatorics from the match. However, in so doing, unique-attributes engender a sufficiently negative set of trade-offs, so that investigating whether there are alternative representations that yield better trade-offs becomes of critical importance.
                  This article identifies two promising spaces of such alternatives, and explores a number of the alternatives within these spaces. The first space is generated from local syntactic restrictions on working memory. Within this space, unique-attributes is shown to be the best alternative possible. The second space comes from restrictions on the search performed during the match of individual productions (match-search). In particular, this space is derived from the combination of a new, more relaxed, match formulation (instantiationless match) and a set of restrictions derived from the constraint-satisfaction literature. Within this space, new alternatives are found that outperform unique-attributes in some, but not yet all, domains.",space
10.1016/0952-1976(94)90056-6,Journal,Engineering Applications of Artificial Intelligence,scopus,1994-01-01,sciencedirect,Verification of real-time programs by a knowledge-based strategy,https://api.elsevier.com/content/abstract/scopus_id/0028449013,"A verification system for real-time programs must provide a means of showing that the programs are logically correct and of proving that all timing constraints are met. To prove a program correct, according to traditional methods, all possible execution sequences (i.e. traces) must be shown to satisfy the specifications. That, however, will lead to the exponential explosion of the traces. This paper adopts an Artificial Intelligence technique to verify real-time programs, and proposes a method by which the real-time programs are proved just in a so-called “Reasonable Trace Space” (RTS). The RTS is a subset of the set of all traces (in short, the ATS) and is determined by a knowledge base which is provided by the program designer or a software verification expert. If a real-time program is proved correct in the RTS (which is much smaller than the ATS), it can be concluded that it is error free in the sense of the knowledge base. A knowledge-based trace-generation algorithm which enumerates all reasonable traces and a trace-verification algorithm which decides whether a trace is correct or not, are presented. Their performances are also analyzed.",space
10.1016/0005-1098(94)90154-6,Journal,Automatica,scopus,1994-01-01,sciencedirect,FCMAC: A fuzzified cerebellar model articulation controller with self-organizing capacity,https://api.elsevier.com/content/abstract/scopus_id/0028413077,"The Albus's Cerebellar Model Articulation Controller (CMAC) network has been used in many practical areas with considerable success. This paper presents a fuzzified CMAC network (FCMAC) acting as a multivariable adaptive controller with the feature of self-organizing association cells and the further ability of self-learning the required teacher signals in real-time. In particular, the original CMAC has been reformulated within a framework of a simplified fuzzy control algorithm (SFCA) and the associated self-learning algorithms have been developed as a result of incorporating the schemes of competitive learning and iterative learning control into the system. By using a similarity-measure-based, instead of coding-algorithm-based, content-addressable scheme, FCMAC is capable of dealing with arbitrary-dimensional continuous input space in a simple manner without involving complicated discretizing, quantizing, coding, and hashing procedures used in the original CMAC. The learning control system described here can be thought of as either a completely unsupervised fuzzy-neural control strategy without relying on the process model or equivalently an automatic real-time knowledge acquisition scheme for the implementation of fuzzy controllers. The proposed approach has been applied to a multivariable blood pressure control problem which is characterized by strong interaction between variables and large time delays.",space
10.1006/jpdc.1993.1023,Journal,Journal of Parallel and Distributed Computing,scopus,1993-01-01,sciencedirect,An adaptive opto-electronic neural network for associative pattern retrieval,https://api.elsevier.com/content/abstract/scopus_id/38249005040,"A novel adaptive trinary neural network model is proposed for associative pattern retrieval from incomplete data. Systematic analysis of the convergence mechanism for the character recognition problem is provided to illustrate the derivation of this novel adaptive thresholding scheme. The adaptive scheme with trinary input representation outperforms other associative retrieval schemes in terms of convergence and storage capacity. The inherent parallelism of this neural network architecture is exploited for a parallel optical implementation. The tremendous speed and free-space interconnection capability of optics results in a very efficient real-time character recognition system. The adaptive threshold scheme developed here may have far reaching implications for other neural networks in enhancing their learning and retrieval, speed, and accuracy.",space
10.1016/0003-682X(93)90050-G,Journal,Applied Acoustics,scopus,1993-01-01,sciencedirect,Modelling of room acoustics and loudspeakers in JBL's complex array design program CADP2,https://api.elsevier.com/content/abstract/scopus_id/0027242485,"This paper presents a survey of the models and principles that are used in JBL's second generation program CADP2 to predict the performance of complex multi-loudspeaker sound reinforcement systems in real spaces. The paper explains how the image model for polyhydral spaces, statistical room acoustics and complex polar patterns for loudspeakers are combined into time and frequency domain sound field models. The models are applied to predict accurate 
                        
                           1
                           1
                        
                      or 
                        
                           1
                           3
                        
                      octave band SPLs in frequency and space from complex loudspeaker arrays. The models are also the basis for the prediction of speech intelligibility (ALcons, AI, RASTI and C50/80) from sound reinforcement systems taking propagation and electronic delays, discrete reflections, reverberation and noise into account.",space
10.1016/b978-0-08-041275-7.50077-6,Conference Proceeding,IFAC Symposia Series,scopus,1992-01-01,sciencedirect,Real-time fault diagnosis for propulsion systems,https://api.elsevier.com/content/abstract/scopus_id/0027099165,"Current research toward real-time fault diagnosis for propulsion systems at the National Aeronautics and Space Administration's Lewis Research Center is described. The research is being applied to both airbreathing and rocket propulsion systems. Topics include fault detection methods including neural networks, system modeling, and real-time implementations.",space
10.1016/0893-6080(91)90062-A,Journal,Neural Networks,scopus,1991-01-01,sciencedirect,DEFAnet-A deterministic neural network concept for function approximation,https://api.elsevier.com/content/abstract/scopus_id/0026375514,"A deterministic neural network concept for a “universal approximator” is proposed. The network has two hidden layers; only the synapses of the output layer are required to be plastic and only those depend on the function to be approximated. It is shown that a DEterministic Function Approximation Network (DEFAnet) allows to approximate an arbitrary continuous function from the finite-dimensional unit interval into the finite-dimensional real space with arbitrary accuracy; arbitrary Boolean functions may be implemented exactly in a simple subset of DEFAnets. In a supervised learning scheme, convergence to the desired function is guaranteed; back propagation of errors is not required. The concept is also open for reinforcement learning. In addition, when the topology of the network is determined according to the DEFAnet concept, it is possible to calculate all plastic synaptic weights in closed form, thus reducing the training considerably or replacing it altogether. Efficient algorithms for the calculation of synapse weights are given.",space
10.1016/0004-3702(91)90113-X,Journal,Artificial Intelligence,scopus,1991-01-01,sciencedirect,Dynamic across-time measurement interpretation,https://api.elsevier.com/content/abstract/scopus_id/0026243270,"Incrementally maintaining a qualitative understanding of physical system behavior based on observations is crucial to tasks such as real-time control, monitoring, and diagnosis. This paper describes the DATMI theory for interpretation tasks. The key idea of DATMI is to dynamically maintain a concise representation of the space of local and global interpretations across time that are consistent with the observations. This representation has two key advantages. First, a set of possible interpretations is more useful than a single (best) candidate for many tasks, such as conservative monitoring. Second, this representation simplifies switching to alternative interpretations when data are faulty or incomplete. Domain-specific knowledge about state and transition probabilities can be used to suggest the interpretation which is most likely. Domain-specific knowledge about durations of states and paths of states can also be used to further constrain the interpretation space. When no consistent interpretation exists, faulty-data hypotheses are generated and then tested by adjusting the interpretation space. The DATMI theory has been tested via implementation and we describe its performance on two examples.",space
10.1016/0922-338X(90)90030-Z,Journal,Journal of Fermentation and Bioengineering,scopus,1990-01-01,sciencedirect,An expert approach for control of fermentation processes as variable structure plants,https://api.elsevier.com/content/abstract/scopus_id/0025102057,"Structural variability is a fundamental property of biological systems. Fermentation processes, in particular, are often accompanied by physiological phenomena which have the characteristics of structural alterations. The conventional control approach does not have the competence to handle such processes, because it is based on the general assumption for a single-structure plant. In order to overcome this limitation, we consider under some conditions fermentation processes as variable structure plants and propose a two-level hierarchical scheme for their control. At the higher hierarchical level, which performs the organizing functions and operates in the structural space of the plant, the current plant structure is recognized as an element of a finite set of structures defined on the basis of ‘deep’ expert knowledge of the cell physiology and/or ‘shallow’ expert knowledge of the behavioral characteristics of the microbial population. The recognition is based on fuzzy decomposition of the structural space of the plant into several subspaces, mapped isomorphically into a respective set of rational control strategies. A set of expert rules is used for the actual synthesis of the recognition procedure, which is finally tuned by supervised learning. The adopted methodology at this hierarchical level provides the system with a kind of intelligence. To the lower hierarchical level, two distinct functions are assigned. The first one performs the global task of control of the plant structure and operates in its structural space. The second function has a local character and consists of control of the plant in a particular subspace of constant structure. It works in the state space of the plant. Under supervision of the higher level, the control strategy relevant to the current plant structure is picked out from a predefined pool and the corresponding control action is calculated. Extensive use of expert knowledge in the conceptual formulation of proper control strategies is assumed. Compared with the conventional single-structure approach, the variable structure concept results in rational and physiologically motivated control of fermentation processes. Instead of the development of a single but complicated process model and control structure, construction of several simple control structures, often in a ‘model-less’ form, is required. This method naturally incorporates and theoretizes the widely accepted policy of artificial induction of structural transformations in the plant, which is essential for enhanced productivity in many modern fermentation technologies. The proposed approach has been realized as a real-time software system, provided with enough flexibility to be used with different fermentation processes and equipment. This system is orientated to IBM personal computers and works under the QNX multi-tasking real-time operating system.",space
10.1016/S0736-5853(89)80029-2,Journal,Telematics and Informatics,scopus,1989-01-01,sciencedirect,"Expert system development methodology and the transition from prototyping to operations: Fiesta, a case study",https://api.elsevier.com/content/abstract/scopus_id/0024911679,"A major barrier in taking expert systems from prototype to operational status involves instilling end user confidence in the operational system. End users want assurances that their systems have been thoroughly tested, meet all their specifications and requirements, and are built based on designs which are reliable and maintainable. For most software systems, the waterfall life cycle model can provide those assurances. However, this model is inappropriate for expert system development, where an iterative refinement approach is commonly employed. This paper will look at different life cycle models and explore the advantages and disadvantages of each when applied to expert system development. The Fault Isolation Expert System for TDRSS Applications (FIESTA) is presented as a case study example of development of an expert system. FIESTA is planned for use in the Network Control Center (NCC) at Goddard Space Flight Center in Greenbelt, Maryland. The end user confidence necessary for operational use of this system is accentuated by the fact that it will handle real-time data in a secure environment, allowing little tolerance for errors. The paper discusses how FIESTA is dealing with transition problems as it moves from an off-line standalone prototype to an on-line real-time system.",space
10.1016/S0736-5853(89)80026-7,Journal,Telematics and Informatics,scopus,1989-01-01,sciencedirect,Synthetic organisms and self-designing systems,https://api.elsevier.com/content/abstract/scopus_id/0024911678,"This paper examines the need for complex, adaptive solutions to certain types of complex problems typified by the Strategic Defense System and NASA's Space Station and Mars Rover. Since natural systems have evolved with capabilities of intelligent behavior in complex, dynamic situations, it is proposed that biological principles be identified and abstracted for application to certain problems now facing industry, defense, and space exploration. Two classes of artificial neural networks are presented — a nonadaptive network used as a genetically determined “retina,” and a frequency-coded network used as an adaptive “brain.” The role of a specific environment coupled with a system of artificial neural networks having simulated sensors and effectors is seen as an ecosystem. Evolution of synthetic organisms within this ecosystem provides a powerful optimization methodology for creating intelligent systems able to function successfully in any desired environment. A complex software system involving a simulation of an environment and a program designed to cope with that environment are presented. Reliance on adaptive systems, as found in nature, is only part of the proposed answer, though an essential one. The second part of the proposed method makes use of an additional biological metaphor—that of natural selection—to solve the dynamic optimization problems every intelligent system eventually faces. A third area of concern in developing an adaptive, intelligent system is that of real-time computing. It is recognized that many of the problems now being explored in this area have their parallels in biological organisms, and many of the performance issues facing artificial neural networks may find resolution in the methodology of real-time computing.",space
10.1016/S0736-5853(89)80027-9,Journal,Telematics and Informatics,scopus,1989-01-01,sciencedirect,Integration of perception and reasoning in fast neural modules,https://api.elsevier.com/content/abstract/scopus_id/0024904803,"Artificial neural systems promise to integrate symbolic and subsymbolic processing to achieve real-time control of physical systems. Two potential alternatives exist. In one, neural nets can be used to front-end expert systems. The expert systems, in turn, are developed with varying degrees of parallelism, including their implementation in neural nets. In the other, rule-based reasoning and sensor data can be integrated within a single hybrid neural system. The hybrid system reacts as a unit to provide decisions (problem solutions) based on the simultaneous evaluation of data and rules. This paper discusses a model hybrid system based on the fuzzy cognitive map (FCM). The operation of the model is illustrated with the control of a hypothetical satellite that intelligently alters its attitude in space in response to an intersecting micrometeorite shower.",space
10.1016/0094-5765(89)90052-0,Journal,Acta Astronautica,scopus,1989-01-01,sciencedirect,"Telescience and microgravity. Impact on future facilities, ground segments and operations",https://api.elsevier.com/content/abstract/scopus_id/0024856605,"Scientific activities related to experimentation in long duration microgravity missions can only be accomplished by the implementation of the Telescience Concept.
                  Telescience is in fact the logical answer to the need of an intelligent interactive conduct of experiments, to the lack (or very little availability) of crew time on board of the Segments of the Columbus project and to the PIs demand for decentralized operations. Telescience could also be seen as the preparative phase for the ultimate, future exploitation of Microgravity by means of Expert Systems that will utilize AI and Robotics for routine operations (Data Factories, Space Productions and Commercial Enterprises).
                  The implications of Telescience on future Space Activities is reviewed with reference to the Principal Investigator Activities, Crew Members Roles and Facilities. The possibilities offered by newly designed Facilities to be operated in Telescience are pointed out with reference to the scientific objectives that would not be achieved otherwise.
                  Diagnostic facilities (mainly non invasive) that provide digital measurements to be inputted (in real time) into numerical codes for computation of field parameters are being considered. Ground Segment Structure, User Support Centers Organization and Test Bedding activities will be discussed as essential factors of the Telescience Scenario of the Multiuser, permanent platform Facilities for the Microgravity disciplines (Material, Fluid, Life and Engineering Science).",space
10.1016/S0006-3495(88)83038-8,Journal,Biophysical Journal,scopus,1988-01-01,sciencedirect,Principles of odor coding and a neural network for odor discrimination,https://api.elsevier.com/content/abstract/scopus_id/0024272690,"A concept of olfactory coding is proposed. It describes the stimulus responses of all receptor cells by the use of vector spaces. The morphological convergence pattern between receptor cells and glomeruli is given in the same vector space as the receptor cell activities. The overall input of a glomerulus follows as the scalar product of the receptor cell activity vector and the vector of the glomerulus' convergence pattern. The proposed coding concept shows how the network of the olfactory bulb succeeds in discriminating odors with high selectivity. It is concluded that sets of mitral cells coding similar odors work very much in the way of mutually inhibited matched filters. This solves one main problem both in olfaction as well as real-time odor detection by an artificial nose, i.e., how the fairly low degree of selectivity of receptor cells or sensors is overcome by the neural network following the receptor stage. The formal description of olfactory coding suggests that quality perception which is invariant under concentration shifts is accomplished by an associative memory in the olfactory bulb.",space
10.1016/0167-739X(87)90034-3,Journal,Future Generation Computer Systems,scopus,1987-01-01,sciencedirect,DLM - a powerful ai computer for embedded expert systems,https://api.elsevier.com/content/abstract/scopus_id/0023575081,"There is an increasing need to operate computers in a real-time environment whereby the solution path of the task cannot be pre-determined. Such problems (eg intelligent process control, vision understanding, data fusion etc.) can be solved with an expert system approach that may necessitate interaction with procedural processes. Since expert systems are in general more effectively implemented in declarative languages (eg Lisp, Prolog, Hope), the requirement exists for a powerful AI Computer capable of executing declarative languages and, if necessary, directing the operation of real-time equipment.
                  The Declarative Language Machine (DLM) is a dedicated operational computer designed and built by Bristish Aerospace for this purpose and incorporates the following key features: 
                        
                           1.
                           (1) Prolog execution rate of 620 KLIPS (the DLM is believed to be the fastest AI computer in the world).
                        
                        
                           2.
                           (2) Direct support for real-time operation and interaction with real-time equipment.
                        
                        
                           3.
                           (3) Capability for the efficient execution of Prolog, Parlog, Hope and Lisp.
                        
                        
                           4.
                           (4) Capable of being a stand-alone computer or embedded within host equipment.
                        
                        
                           5.
                           (5) Floating-point co-processor for arithmetic operations.
                        
                     
                  
                  This paper describes the architecture of the DLM",space
10.1016/0031-3203(87)90023-9,Journal,Pattern Recognition,scopus,1987-01-01,sciencedirect,VLSI architectures for string matching and pattern matching,https://api.elsevier.com/content/abstract/scopus_id/0023207478,"In this paper, we discuss string-matching and dynamic time-warp pattern-matching. The string-matching problem arises in a number of applications such as in artificial intelligence, pattern recognition and information retrieval. The method of dynamic time-warping is a well-established technique for time alignment and comparison of speech and image patterns. It has found extensive application in speech recognition and related areas of pattern-matching.
                  We propose a VLSI architecture based on the space-time domain expansion approach which can compute the string distance and also give the matching index-pairs which correspond to the edit sequence. The time complexity is O(max(m, n)) by using m× n processing elements array, where m is the length of the input string and n is the length of the reference string. With a uniprocessor the matching process will have the time complexity O(m × n). If there are p reference strings, using the proposed architecture the string-matching problem can be solved in time O(max(m, n, p)). With a uniprocessor the time complexity will be O(m × n × p). We also propose a VLSI architecture for dynamic time-warping based on the space-time expansion method which can obtain high throughput by using extensive pipelining and parallelism. It can measure the dissimilarity between two patterns in time O(max(m, n, N)). If using a uniprocessor the time complexity will be O(m × n × N), where m and n are the numbers of feature vectors of the unknown input pattern and the reference template respectively, and N is the number of elements of the feature vector. If there are p reference templates, the time complexity will be O(max(m, n, N × p)), and if using a uniprocessor the time complexity will be O(m × n × p × N). The algorithm partition problems are discussed. Verifications of the proposed VLSI architectures are also given. The backtracking procedures are discussed in much detail and their hardware implementations are also given. The proposed architectures can be applied to many areas such as pattern recognition, information retrieval, image processing, speech processing, remote sensing, robotics, computer vision, artificial intelligence and office automata. They are useful to real-time information processing.",space
10.1016/0734-189X(83)90095-6,Journal,"Computer Vision, Graphics and Image Processing",scopus,1983-01-01,sciencedirect,An implementation of a computational theory of visual surface interpolation,https://api.elsevier.com/content/abstract/scopus_id/0020642298,"Computational theories of structure-from-motion (Ullman, The Interpretation of Visual Motion, MIT Press, 1979) and stereo vision (Marr and Poggio, Proc. R. Soc. London Ser. B 
                        204, 1979, 301–328) only specify the computation of three-dimensional surface information at particular points in the image. Yet, the visual perception is clearly of complete surfaces. To account for this, a computational theory of the interpolation of surfaces from visual information was presented in Grimson. (From Images to Surfaces: A Computational Study of the Human Early Visual System, MIT Press, 1981; 
                        A Computational Theory of Visual Surface Interpolation, MIT Artificial Intelligence Lab Memo, No. 613, 1981; and 
                        Philos. Trans. R. Soc. London Ser. B 
                        298, 1982, 395–427). The problem is constrained by the fact that the surface must agree with the information from stereo or motion correspondence, and not vary radically between these points. Using the image irradiance equation (Horn, MIT Project MAC Tech. Rep. MACTR-79, 1970; 
                        The Psychology of Computer Vision, McGraw-Hill, 1975; and 
                        Artif. Intell. 
                        8, 1977, 201–231), an explicit form of this surface consistency constraint can be derived (Grimson, MIT Artificial Intelligence Lab Memo, No. 646, 1981). To determine which of two possible surfaces is more consistent with the surface consistency constraint, one must be able to compare the two surfaces. To do this, a functional from the space of possible functions to the real numbers is required. In this way, the surface most consistent with the visual information will be that which minimizes the functional. In Grimson, a set of conditions was derived which ensures that the functional has a unique minimal surface. Based on these conditions, a number of possible functionals were proposed. In Brady and Horn (MIT Artificial Intelligence Lab Memo, No. 654, 1981), it was shown that this set of possible functionals forms a vector space, spanned by the functional of quadratic variation and the functional of the square Laplacian. Analytic arguments were given in Grimson to support the choice of the quadratic variation as the functional whose minimal surface is the “best” interpolation of the known points. In this paper, algorithms for computing the minimal surface are derived. Using this implementation of the computational theory derived in Grimson the differences between minimal surfaces computed using quadratic variation and those computed using the square Laplacian are illustrated. These examples provide additional support for the choice of the quadratic variation. The performance of the algorithm in interpolating both random dot and natural stereograms which have been processed by the Marr-Poggio stereo algorithm (Grimson, Computing Shape Using a Theory of Human Stereo Vision, Ph.D. Thesis, MIT, Cambridge, 1980 and 
                        Philos. Trans. R. Soc. London Ser. B 
                        292, 1981, 217–253) is also illustrated.",space
