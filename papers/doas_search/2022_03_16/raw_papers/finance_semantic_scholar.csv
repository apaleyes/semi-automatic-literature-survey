paperId,url,title,abstract,venue,year,externalIds.DOI,externalIds.CorpusId,database
7db240ccd5e4bbb8a520d24318e8523eda544e9a,https://www.semanticscholar.org/paper/7db240ccd5e4bbb8a520d24318e8523eda544e9a,Ansys Real Time Physics Based Radar Simulation – An Enabler for Machine Learning in the Context of Autonomous Driving,"Throughout the evolution of Advanced Driver Assistance Systems (ADAS), the correct perception of the environment has always been a decisive success factor. Capturing and defining scenarios/edge cases, various and heterogenous datasets, multiple sensors/sensorfusion architectures, and perception algorithms are just a few of the many challenges we are facing when implementing such systems. To cope with such levels of complexity, modular approaches are required. Such approaches target flexibility and standardized interfaces between data provided by various sensor modules/models and driving functions. In the Artificial Intelligence (AI) domain, and more precisely when dealing with supervised training of Neural Networks (NN), obtaining valid and accurately labeled datasets is essential. By enabling Machine Learning (ML) in electromagnetic applications, Ansys physics-based Real Time Radar (RTR) introduces a new paradigm for sensor development and integration that leverages GPU hardware and new algorithms to accelerate simulation by orders of magnitude without compromising accuracy. In this paper, a comprehensive workflow for the generation of virtual datasets using the Open Simulation Interface (OSI) will be presented. This workflow will illustrate how the scenario variation process coupled with RTR facilitates the creation of heterogenous/labeled datasets that are ready for training object detection NN. Finally, this presentation will also show the preliminary results obtained when implementing this process. Introduction Machine Learning (ML) is gradually taking over “conventional” algorithms that were previously designed to help make Autonomous Vehicles (AVs) a reality. Several auto giants like BMW, VDI-Berichte Nr. 2384, 2021 83 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. Volkswagen, and Volvo are at least partially relying on ML algorithms to solve various parts of the sense-plan-act paradigm. Others, like Tesla [1] are solely relying on ML and in some cases Artificial Intelligence (AI) to provide an end-to-end solution. AVs usually rely on several sensors of different types to perceive their surrounding environment. As such sensors continuously scan the environment and generate raw data, the perception stack processes this data and generates a meaningful virtual map of the surrounding environment. In the area of Computer Vision (CV), usually relying on optical systems, ML algorithms are already successfully deployed in commercial systems [2] – [4]. In addition to optical systems, and due to their superior performances in bad weather conditions and dark environments, radars have also made their way into the AV’s sensor stacks. Though not frequently encountered, ML has also been used to replace some of the traditional radar signal processing algorithms. For example, in [5] the author demonstrates how a fully convolutional network can be used for object detection and 3D estimation using a Frequency-Modulated ContinuousWave (FMCW) radar. Contrary to [5], which is using real data for the training process, the author in [6] illustrates the power of physics-based simulation to also demonstrate the feasibility of using ML approaches to solve radar-based perception problems. As the training process of ML algorithms highly relies on labeled training data, Ansys’ Real Time Radar (RTR) automates generation of labeled data sets by shifting data generation and labeling from the real world to the virtual world. In addition, having an API which is compatible with the Open Simulation Interface (OSI) [7] ensures that a standardized interface is being deployed to describe the virtual environment in which generated scenarios are executed. Finally, this paper illustrates how, with the help of Ansys optiSlang [8], a tool chain is developed to orchestrate the scenario variation process and the simulation workflow to automatically generate labeled datasets for training a Neural Network (NN) based object detection algorithm. Ansys Real Time Radar RTR is an all-GPU implementation of the shooting and bouncing rays (SBR) method optimized for the automotive radar application to simulate a scenario in real time/faster than real time. The simulation, which is based on an arbitrary 3D scene/actor geometry, electrical material VDI-Berichte Nr. 2384, 2021 84 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. properties, including transmissive and reflective dielectrics, 3D polarized antenna patterns, directly calculates the scattered electro-magnetic fields as observed by the radar. As an output, RTR can generate raw I and I+Q A/D data for multi-channel radars in dynamically changing driving scenarios. Objects (e.g., vehicles, pedestrians, road, infrastructure, etc.) can be assigned arbitrary positions, orientations, linear and angular velocities in a scene graph hierarchy through a light-weight API to characterize complex traffic scenarios with negligible simulation overhead. To measure Doppler velocity, automotive radars transmit, receive, and process hundreds of chirps over each Coherent Processing Interval (CPI). Fast Fourier Transformations (FFT) and several post processing algorithms are then applied to hundreds of samples from each chirp/CPI to obtain range-Doppler (RD) images, which will be used for Neural Network (NN) training. These images, as represented in Fig. 1, give a visualization of all scattered fields in terms of relative velocity (Doppler) and distance from the radar (range). Fig. 1: Example of Range-Doppler image. As previously mentioned, RTR includes a lightweight C++ and Python API, enabling it to be integrated into nearly any driving simulator available on the market. In Fig. 2 the API’s main interfaces are depicted. VDI-Berichte Nr. 2384, 2021 85 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. Fig. 2: RTR's API representing its main inputs and outputs. Starting from left to right, the API give the user access to the following: 1. “Radar Config” enables the user to configure radar waveforms, radar modes, and antenna patterns. Radar waveforms are defined by parameters such as center frequency, bandwidth, number of frequency samples, CPI duration, number of chirps, number of transmit and receive antennas, and relative antenna positions. 2. “Object and Materials” helps the user build the 3D environment to be simulated and assign dielectric material properties. For example, as presented in Fig. 3, a vehicle can be imported as a set of subcomponents. Users can assign appropriate material properties for each component. VDI-Berichte Nr. 2384, 2021 86 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. Fig. 3: Vehicle subcomponents featuring adequate assignment of material properties. 3. “Object Velocities” represents scene dynamics where position and velocity updates are provided at each simulation time step. Such data is usually obtained from any driving simulator or from a set of pre-recorded/measured GPS/IMU data. At each time step, RTR executes a physics-based radar simulation of the scene and returns either the RD data per channel or the raw I/Q channel data (post A/D conversion). Open Simulation Interface To ensure modularity and interchangeability, RTR’s inputs have been adapted to support OSI. Focusing on environment perception and automated driving functions, OSI is an interface specification for models and components of distributed simulation. It defines a generic interface that ensures modularity, interoperability, and integration of simulation framework’s individual components. In addition, OSI was developed to address and align with the emerging standard for communication interfaces of real sensors, ISO 23150 [9]. This will eventually ensure a better correlation between communication interfaces used in both virtual and real worlds. Corresponding to OSI’s message description, the OSI:SensorView message represented in Fig. 4 contains the ground truth data that can be generated by any 3rd party driving simulator. The message includes information about the states of dynamic and static actors. VDI-Berichte Nr. 2384, 2021 87 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. Fig. 4: Driving Simulator and RTR connection via OSI. Scenario Variation Neural Networks (NN) are designed to behave as low bias and high variance machines that can perform extremely well on training data. To generalize such machines to new environments, heterogeneous datasets are essential for the training process. Using Ansys optiSlang, scenario variations were generated based on a predefined set of parameters. As represented in Table 1, a set of three parameters were chosen. Table 1: Scenario Variation Parameters Parameter Description Model Name Describes the 3D geometry of the vehicle. Initial Speed A range between 0 and 80 km/h Driver Behavior Aggressive, Normal, Cautious 1. “Model Name” defines whether a traffic participant is a car, bus, or a motorcycle. It also describes what vehicle model to use, ensuring a large variety of traffic participants within each generated scenario. 2. “Initial Speed” may randomly vary between 0 and 80 km/h. Considering that the NN is being trained on RD images, this parameter will ensure that RD scattered fields are randomly distributed in the RD image space along the Doppler velocity axis. 3. “Driver Behavior” takes in three different values: Aggressive, Normal and Cautious. Each behaviour ",ELIV 2021,2021,10.51202/9783181023846-83,239556900,semantic_scholar
d557d4b079b89fdd4d149697b38c9160a8234c47,https://www.semanticscholar.org/paper/d557d4b079b89fdd4d149697b38c9160a8234c47,"Rapid development, real-world deployment, and evaluation of projected augmented reality applications","Current interactive projected augmented reality systems are not designed to support rapid development and deployment of applications beyond the confines of research labs. I developed a series of self-contained interactive projector-sensor systems (collectively LuminAR devices) and a web-based software development framework. The design goal of this research work was to advance the state of the art of projected AR interfaces and to explore how they can manifest in day-to-day objects. This novel, tightly integrated approach allows developers who are not versed in computer graphics, vision algorithms, and augmented reality techniques to implement projected AR applications rapidly. In this work, I review several real-world uses of the system for retail presentation, desktop interaction and collaboration applications, manufacturing, and education. The work is evaluated through extensive use of the hardware and software by developers as well as two user studies that specifically explored applications for manufacturing and education. The evaluation methodology focused both on basic interaction and system usability as well as the implications of using augmented interfaces in the specific application domains of education and manufacturing. I also discuss the results of the first large-scale user studies of projected augmented reality rapid application development. Finally, I provide a set of design principles for projected augmented reality applications, and recommendations concerning how to deploy such applications in the real world. This dissertation work was partially supported by research grants from Intel, Steelcase and Pearson. Dissertation Supervisor Pattie Maes Professor of Media Arts and Sciences Program in Media Arts and Sciences Rapid Development, Real-World Deployment, and Evaluation of Projected Augmented Reality Applications",,2017,,57966766,semantic_scholar
1de1fb5a0530c5c144c8a78fb653c8c7bc58c108,https://www.semanticscholar.org/paper/1de1fb5a0530c5c144c8a78fb653c8c7bc58c108,Sensor-based human activity recognition: Overcoming issues in a real world setting,"The rapid growing of the population age in industrialized societies calls for advanced tools to continuous monitor the activities of people. The goals of those tools are usually to support active and healthy ageing, and to early detect possible health issues to enable a long and independent life. Recent advancements in sensor miniaturization and wireless communications have paved the way to unobtrusive activity recognition systems. Hence, many pervasive health care systems have been proposed which monitor activities through unobtrusive sensors and by machine learning or artificial intelligence methods. Unfortunately, while those systems are effective in controlled environments, their actual effectiveness out of the lab is still limited due to different shortcomings of existing approaches. 
 
In this work, we explore such systems and aim to overcome existing limitations and shortcomings. Focusing on physical movements and crucial activities, our goal is to develop robust activity recognition methods based on external and wearable sensors that generate high quality results in a real world setting. Under laboratory conditions, existing research already showed that wearable sensors are suitable to recognize physical activities while external sensors are promising for activities that are more complex. Consequently, we investigate problems that emerge when coming out of the lab. This includes the position handling of wearable devices, the need of large expensive labeled datasets, the requirement to recognize activities in almost real-time, the necessity to adapt deployed systems online to changes in behavior of the user, the variability of executing an activity, and to use data and models across people. As a result, we present feasible solutions for these problems and provide useful insights for implementing corresponding techniques. Further, we introduce approaches and novel methods for both external and wearable sensors where we also clarify limitations and capabilities of the respective sensor types. Thus, we investigate both types separately to clarify their contribution and application use in respect of recognizing different types of activities in a real world scenario. 
 
Overall, our comprehensive experiments and discussions show on the one hand the feasibility of physical activity recognition but also recognizing complex activities in a real world scenario. Comparing our techniques and results with existing works and state-of-the-art techniques also provides evidence concerning the reliability and quality of the proposed techniques. On the other hand, we also identify promising research directions and highlight that combining external and wearable sensors seem to be the next step to go beyond activity recognition. In other words, our results and discussions also show that combining external and wearable sensors would compensate weaknesses of the individual sensors in respect of certain activity types and scenarios. Therefore, by addressing the outlined problems, we pave the way for a hybrid approach. Along with our presented solutions, we conclude our work with a high-level multi-tier activity recognition architecture showing that aspects like physical activity, (emotional) condition, used objects, and environmental features are critical for reliable recognizing complex activities.",,2019,,201711455,semantic_scholar
cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,https://www.semanticscholar.org/paper/cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,Comparing Human-Robot Proxemics between Virtual Reality and the Real World,"Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of Human-Robot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. Comparing Human-Robot Proxemics between Virtual Reality and the Real World Rui Li KTH Royal Institute of Technology Stockholm, Sweden Rui3@kth.se ABSTRACT Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other.Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. INTRODUCTION Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI) [1][2][3][4]. VR has been used to test teleoperation and collect demonstration data to train machine learning algorithms, which showcased the effectiveness of learning visuomotor skills using data collected by consumer-grade devices [1]. VR teleoperation systems were proposed to crowdsource robotic demonstrations at scale [2]. A VR simulation framework was also proposed to replace the physical robot, as VR can enable high level abstraction in embodiment and multimodal interaction [3]. VR has also been used as a rapid prototyping tool to design in-vehicle interactions and interfaces for self-driving cars, which showed the evocation to genuine responses from test participants [4]. Compared to other HRI experiment methods, VR as an emerging interactive media provides unique advantages. VR HRI has the potential of having higher immersion and fidelity than picture based HRI, video-based HRI and simulated HRI. In situations where the perception of the robot is challenging, compared to on-screen viewing, VR display showed significant improvement on collaborative tasks [5]. When comparing VR HRI to the physical, realworld interaction (Live HRI), there is a trade-off between the two. VR experiences still cannot replace physical experiences due to system limitation, and limited interaction modalities etc. [6]. For example, system limitations such as limited field of view and low display resolution could reduce immersion and presence of the VR experience, resulting in different behaviors from Live experiments. Limited interaction modalities, such as the absence of touch, means that the participant could not feel the robot or even go through the robot, which could potentially break the entire interaction. Figure 1: Photograph of the Live experiment setting However, with the help of the distribution of consumer-grade VR devices and online crowdsourcing platforms, VR HRI has the potential to gain massive data for training robotic behavior and studying HRI related issues. Data collection through VR can also reduce noise and improve the data quality [1], which help to ease data processing and algorithm training. Furthermore, VR HRI experiments can test concepts and interactions without physical robots, making it more resource efficient and less expensive than Live HRI. Less hardware also means that the experiment will be less cumbersome to set up, easier to be reproduced and to ensure experiment quality. In this study, HRI Proxemics (the preferred personal space between a human and a robot) was compared to give a better justification and more basic understanding of the relationship between Live and VR. Proxemics preferences rely on lower level intuition [7], therefore, reflect the differences in the perceptions between Live and VR better. Compared to other HRI subject such as conversational (audio) or gaze behavior (visual), which are more modality dependent, proxemics can give a comprehensive understanding of the human responses. In addition, variations of modalities in VR can greatly influence human perception. For example, a higher visual familiarity of the physical environment in VR can decrease the effect of distance distortion [8]. Auditory inputs play another important role in VR, the addition of spatial sound can increase the sense of presence in VR and provide sound localization [9]. Thus, this work also compares VR settings with variance in modalities to evaluate the impacts of visual familiarity and spatial sound on VR HRI experiments. A 2 x 3 mixed design experiment was conducted to evaluate the differences between Live and VR HRI, as well as the influence of visual familiarity and spatial sound in VR. For the Live HRI, the pepper robot from Softbank Robotics was used (Figure 1). In the VR HRI, a 3D model of the same robot was used. To measure visual familiarity, the VR scene was created in Blender based on a 3D scan of the physical lab. The spatial sound was created by enabling the movement of the physical robot, due to the difficulties of engineering spatial sound. The interaction was implemented in Unity. As an objective measurement for proxemics preference, the minimum comfort distance (MCD) was measured. In addition, for the psychological perception of the experience, the feeling of presence was measured with the SUS questionnaire. For the perception of the robot, two relevant factors, competence and discomfort was measured with the ROSAS questionnaire.",,2018,,52210376,semantic_scholar
6e04016b9502b3bf8ccc10f5ae775c5e12531ec8,https://www.semanticscholar.org/paper/6e04016b9502b3bf8ccc10f5ae775c5e12531ec8,Pluggable real world interfaces Physically enabled code deployment for networked sensors,"In this paper we present a novel abstraction and deployment process using real world interfaces, which reflect the realities of pervasive software development. Pluggable real world interfaces support ldquoplugpsilanpsilaplayrdquo deployment for sensor-augmented hardware and provide an object-oriented encapsulation of high-level contextual interfaces. The architecture adds an additional object based abstraction layer between the sensor subsystem (delivering e.g. cues) and the application (delivering the situation context). Component abstraction layers are implemented as code that comes with physical components, e.g. a chair, and provides the functionality for detecting context bundled with the sensory hardware. The approach will lead to pluggable real world interfaces: The functionality of an appliance will be composed from the functionality of its components - just like a meeting room will be composed from many chairs. This paper will present concept, architecture and a first implementation based on a Java run-time system for very tiny, very low-power embedded sensor nodes.",2008 5th International Conference on Networked Sensing Systems,2008,10.1109/INSS.2008.4610909,34946413,semantic_scholar
5e171bf6603a03fbfdf3434abcd29496a2327100,https://www.semanticscholar.org/paper/5e171bf6603a03fbfdf3434abcd29496a2327100,A Learning Analytics Conceptual Framework for Augmented Reality-Supported Educational Case Studies,"The deployment of augmented reality (AR) has attracted educators’ interest and introduced new opportunities in education. Additionally, the advancement of artificial intelligence has enabled educational researchers to apply innovative methods and techniques for the monitoring and evaluation of the teaching and learning process. The so-called learning analytics (LA) discipline emerged with the promise to revolutionize traditional instructional practices by introducing systematic and multidimensional ways to improve the effectiveness of the instructional process. However, the implementation of LA methods is usually associated with web-based platforms, which offer direct access to learners’ data with minimal effort or adjustments. On the other hand, the complex nature of immersive technologies and the diverse instructional approaches which are utilized in different scientific domains have limited the opportunities for research and development in this direction. Within these research contexts, we present a conceptual framework that describes the elements of an LA process tailored to the information that can be gathered from the use of educational applications, and further provide an indicative case study for AR-supported educational interventions. The current work contributes by elucidating and concretizing the design elements of AR-supported applications and provides researchers and designers with guidelines on how to apply instructional strategies in (augmented) real-world projects.",Multimodal Technol. Interact.,2021,10.3390/MTI5030009,233835571,semantic_scholar
7c58a8f8c6611c1fc7191e5cc1b52cc3819387de,https://www.semanticscholar.org/paper/7c58a8f8c6611c1fc7191e5cc1b52cc3819387de,A Cloud-based Deep Learning Framework for Remote Detection of Diabetic Foot Ulcers,"This research proposes a mobile and cloud-based framework for the automatic detection of diabetic foot ulcers and conducts an investigation of its performance. The system uses a cross-platform mobile framework which enables the deployment of mobile apps to multiple platforms using a single TypeScript code base. A deep convolutional neural network was deployed to a cloud-based platform where the mobile app could send photographs of patient’s feet for inference to detect the presence of diabetic foot ulcers. The functionality and usability of the system were tested in two clinical settings: Salford Royal NHS Foundation Trust and Lancashire Teaching Hospitals NHS Foundation Trust. The benefits of the system, such as the potential use of the app by patients to identify and monitor their condition are discussed. May/June 2019 1 ar X iv :2 10 5. 07 76 3v 1 [ cs .L G ] 1 7 M ay 2 02 1 DIABETES MELLITUS is a chronic metabolic disorder, and a growing world-wide epidemic [1]. Diabetic Foot Ulcers (DFU) are wounds developed on the feet that represent serious chronic complications resulting from diabetes, and are prone to high levels of recurrence and infection [2]. There are numerous potential contributing factors to the development of DFU, with diagnosis, monitoring, and treatment programmes requiring multidisciplinary medical expertise. Feet of diabetic patients are more susceptible to injury and chronic wounds, resulting in skin damage and ultimately development of a DFU [3]. Patients with an active DFU, or at high-risk of developing a DFU require frequent foot checks by healthcare professionals and referral to specialists to prevent additional severe complications [4]. DFU can result in serious lifestyle repercussions, resulting in immobility, social stigma, social isolation, increased mortality, and significant costs to healthcare systems [5], with hospitalisation constituting the most expensive part of treating DFU infections [6]. More than half of DFUs become infected, with approximately 20% of moderate or severe DFU infections leading to lower extremity amputation [7]. The cost of health care in England for DFU and amputation in 2014-2015 is estimated at £1 billion. This constitutes approximately 1% of the entire National Health Service (NHS) budget [8]. The lower bound of DFU and associated amputation cost estimates is higher than the combined NHS expenditure in England on breast, prostate, and lung cancers [9]. In the United States, the direct costs of treating DFU exceed the treatment costs of many common cancers [7]. Given the significant and growing impact of DFU, mobile health solutions that target this condition could assist in improving patient quality of life. Up to 80% of DFU are thought to be preventable through early detection [10]. Promotion of patient self care and continuous monitoring for those most at risk, increasing rates of early intervention to reduce the severity and impact of DFU, could provide significant cost savings for healthcare systems. Self-management programmes have been associated with improved health outcomes, with mobile technologies identified as an important factor in helping to deliver self-management interventions that are adaptable, low cost and easily accessible [11]. Due to the continued significant increase in reported global cases of diabetes and DFU, research in this area has also seen notable growth. As a result, the use of deep learning algorithms for automated analysis of DFU have become more prominent, particularly from our group over the last few years [12], [13], [14], [15]. Goyal et al. have created and validated deep convolutional neural networks (CNNs) capable of DFU classification [12], semantic segmentation [13] and localisation [14]. These models have been shown to have high levels of sensitivity, specificity and mean average precision (mAP) in experimental settings. This paper proposes a cloud-based deep learning framework for remote detection of diabetic foot ulcers. To address the issues above, our framework includes: • A cross-platform mobile app used for capturing photographs of DFU (a non-contact solution) capable of sending diagnosis requests to a cloud service • A cloud-platform that mobile clients can connect to capable of inference using one or more CNNs to provide a diagnosis To assess the usability and reliability of such a system, we completed a proof-of-concept clinical evaluation using mobile and cloud technologies at two UK sites: Salford Royal NHS Foundation Trust and Lancashire Teaching Hospitals NHS Foundation Trust. Prior to starting the proofof-concept clinical evaluation, we obtained ethical approval from Salford Royal NHS Foundation Trust (REF: S19HRANA37) and Lancashire Teaching Hospitals NHS Foundation Trust (REF: SE-281). WHY CLOUD? The unprecedented growth of the global smartphone market over the last decade has been mirrored by the more recent emergence and rapid expansion of enterprise Cloud Computing Platforms (CCP). CCP provide on-demand computing, storage and software accessible over the internet, allowing for the remote offloading of process-intensive tasks. This approach to server technology is an increasingly common long-term 2 © 2019 IEEE Published by the IEEE Computer Society IT Professional strategy for replacing the traditional manually maintained client-server hardware set-up [16]. A clear advantage of CCPs are that they allow users of mobile devices to gain access to significant processing power, well beyond the means of any existing mobile device. This approach allows for patients to use even very dated mobile hardware to access the latest advances in automated medical image analysis. This essentially means that continual advances in this field are not tied to the computing capability of mobile devices, as such devices are simply consuming services from CCPs. Additionally, scalability becomes easier to manage, given the virtualised nature of cloud services. There is a growing trend in the use of ensemble CNNs in medical image analysis, whereby multiple CNNs are used to form a final prediction. Distributing mobile apps that use multiple models is not practical or possible given the limited permissible size of apps when distributed via online app stores. There is also the issue of intellectual property protection. Android apps are particularly easy to reverse engineer, so having the CNNs run on the server instead of the user’s mobile device means that trained models are never publicly exposed. SYSTEM ARCHITECTURE The two major components created for the evaluation were (1) a cross-platform mobile app, and (2) a cloud-based deep learning framework that performed inference on foot photographs sent from mobile clients. A cross-platform framework was chosen for the development of the mobile client since the ultimate goal of this research is to provide patients with a means of remotely monitoring and diagnosing DFU using their own smart-phones, which primarily comprise of Android or iOS devices. An overview of the system physical architecture is shown in Fig. 1. The following sections describe how these components were utilised in the creation of our proposed framework. Mobile App Cross-platform development can help to reduce the time and costs associated with developing apps for multiple mobile platforms. The mobile app developed for our evaluation was created using Ionic, a cross-platform framework based on the earlier Cordova framework. Screens within Ionic apps are rendered onto a standard WebView, in the same way that web pages are rendered in web browsers. There are also native elements within the framework however, including the ability to interface with the device’s hardware features, such as sensors and cameras. Fig. 2 shows the main data capture screens within the mobile app. The primary objective of our initial proof of concept evaluation was to determine the usability and reliability of our cross-platform mobile client and cloud-based framework in real-world settings. Ease of use was a primary motivating factor behind the design of the mobile app. Screens within the app display context-sensitive information in the form of an information bar at the top of each screen that was used to guide the user through the process of acquiring and uploading foot photographs. The UI and validation were designed so that it was not possible for the user to take the wrong action. Examples of this include: • It was not possible to retake a photograph for the current foot if one had already been taken and uploaded. • The user could not upload a photograph for any foot more than once. • It was not possible to change left foot “checked” tickbox if the left foot photo had been uploaded. • It was not possible to change right foot “checked” tickbox if the right foot photo had been uploaded. Ionic utilises a Model View Controller (MVC) architecture, implemented using Angualr.js, which separates data, presentation of data and business logic. App data, including application state, is stored in a local SQLite database. Oracle Mobile Cloud Service Software Development Kit (SDK) Oracle provides a SDK for several mobile development platforms, including Ionic, which enables mobile clients to interface with Oracle Mobile Hub (OMH). The Oracle Mobile Cloud Service SDK is a HyperText Transfer Protocol Secure client layer, through which requests can be made to OMH and associated services using JavaScript Object Notation (JSON) via REpre-",IEEE Pervasive Computing,2021,10.1109/MPRV.2021.3135686,234742192,semantic_scholar
a3040b1044097d903991c03cde30a0600ce53d0c,https://www.semanticscholar.org/paper/a3040b1044097d903991c03cde30a0600ce53d0c,RFID and Arduino : Managing RFID Events on a Real World Prototype,"Radio Frequency Identification (RFID) is a technology that is growing fast, becoming part of our daily lives more often and opening way to the Internet of Things. It can identify physical objects automatically, almost in real-time. Unfortunately, the learning curve for the technology is steep and a variety of tools are necessary just to implement a simple prototype. This paper provides an introduction to RFID through the implementation of a prototype, covering both hardware and software. We have deployed open-source software to monitor an Arduino robot carrying tagged objects in a small “warehouse”.",,2012,,203691895,semantic_scholar
976ae262c8e88c61462f673383da3649f935cebc,https://www.semanticscholar.org/paper/976ae262c8e88c61462f673383da3649f935cebc,QoS-Aware Placement of Deep Learning Services on the Edge with Multiple Service Implementations,"Mobile edge computing pushes computationally-intensive services closer to the user to provide reduced delay due to physical proximity. This has led many to consider deploying deep learning models on the edge – commonly known as edge intelligence (EI). EI services can have many model implementations that provide different QoS. For instance, one model can perform inference faster than another (thus reducing latency) while achieving less accuracy when evaluated. In this paper, we study joint service placement and model scheduling of EI services with the goal to maximize Quality-of-Servcice (QoS) for end users where EI services have multiple implementations to serve user requests, each with varying costs and QoS benefits. We cast the problem as an integer linear program and prove that it is NP-hard. We then prove the objective is equivalent to maximizing a monotone increasing, submodular set function and thus can be solved greedily while maintaining a (1 – 1/e)-approximation guarantee. We then propose two greedy algorithms: one that theoretically guarantees this approximation and another that empirically matches its performance with greater efficiency. Finally, we thoroughly evaluate the proposed algorithm for making placement and scheduling decisions in both synthetic and real-world scenarios against the optimal solution and some baselines. In the real-world case, we consider real machine learning models using the ImageNet 2012 data-set for requests. Our numerical experiments empirically show that our more efficient greedy algorithm is able to approximate the optimal solution with a 0.904 approximation on average, while the next closest baseline achieves a 0.607 approximation on average.",2021 International Conference on Computer Communications and Networks (ICCCN),2021,10.1109/ICCCN52240.2021.9522156,233476205,semantic_scholar
b42fd5d2959d327a2eaa28784b744f74f6b4e6b7,https://www.semanticscholar.org/paper/b42fd5d2959d327a2eaa28784b744f74f6b4e6b7,Configuration Management Best Practices: Practical Methods that Work in the Real World,"Successfully Implement High-Value Configuration Management Processes in Any Development Environment As IT systems have grown increasingly complex and mission-critical, effective configuration management (CM) has become critical to an organizations success. Using CM best practices, IT professionals can systematically manage change, avoiding unexpected problems introduced by changes to hardware, software, or networks. Now, todays best CM practices have been gathered in one indispensable resource showing you how to implement them throughout any agile or traditional development organization. Configuration Management Best Practices is practical, easy to understand and apply, and fully reflects the day-to-day realities faced by practitioners. Bob Aiello and Leslie Sachs thoroughly address all six pillars of CM: source code management, build engineering, environment configuration, change control, release engineering, and deployment. They demonstrate how to implement CM in ways that support software and systems development, meet compliance rules such as SOX and SAS-70, anticipate emerging standards such as IEEE/ISO 12207, and integrate with modern frameworks such as ITIL, COBIT, and CMMI. Coverage includes Using CM to meet business objectives, contractual requirements, and compliance rules Enhancing quality and productivity through lean processes and just-in-time process improvement Getting off to a good start in organizations without effective CM Implementing a Core CM Best Practices Framework that supports the entire development lifecycle Mastering the people side of CM: rightsizing processes, overcoming resistance, and understanding workplace psychology Architecting applications to take full advantage of CM best practices Establishing effective IT controls and compliance Managing tradeoffs and costs and avoiding expensive pitfalls Configuration Management Best Practices is the essential resource for everyone concerned with CM: from CTOs and CIOs to development, QA, and project managers and software engineers to analysts, testers, and compliance professionals. Praise for Configuration Management Best Practices Understanding change is critical to any attempt to manage change. Bob Aiello and Leslie Sachss Configuration Management Best Practices presents fundamental definitions and explanations to help practitioners understand change and its potential impact. Mary Lou A. Hines Fritts, CIO and Vice Provost Academic Programs, University of Missouri-Kansas City Few books on software configuration management emphasize the role of people and organizational context in defining and executing an effective SCM process. Bob Aiello and Leslie Sachss book will give you the information you need not only to manage change effectively but also to manage the transition to a better SCM process. Steve Berczuk, Agile Software Developer, and author of Software Configuration Management Patterns: Effective Teamwork, Practical Integration Bob Aiello and Leslie Sachs succeed handsomely in producing an important book, at a practical and balanced level of detail, for this topic that often goes without saying (and hence gets many projects into deep trouble). Their passion for the topic shows as they cover a wonderful range of topicseven culture, personality, and dealing with resistance to changein an accessible form that can be applied to any project. The software industry has needed a book like this for a long time! Jim Brosseau, Clarrus Consulting Group, and author of Software Teamwork: Taking Ownership for Success A must read for anyone developing or managing software or hardware projects. Bob Aiello and Leslie Sachs are able to bridge the language gap between the myriad of communities involved with successful Configuration Management implementations. They describe practical, real world practices that can be implemented by developers, managers, standard makers, and even Classical CM Folk. Bob Ventimiglia, Bobev Consulting A fresh and smart review of todays key concepts of SCM, build management, and related key practices on day-to-day software engineering. From the voice of an expert, Bob Aiello and Leslie Sachs offer an invaluable resource to success in SCM. Pablo Santos Luaces, CEO of Codice Software Bob Aiello and Leslie Sachs have a gift for stimulating the types of conversation and thought that necessarily precede needed organizational change. What they have to say is always interesting and often important. Marianne Bays, Business Consultant, Manager and Educator",,2010,,109967709,semantic_scholar
1690f9b7c9a39357177b4532592ae46b4ea11434,https://www.semanticscholar.org/paper/1690f9b7c9a39357177b4532592ae46b4ea11434,Online Reinforcement Learning Control of an Electromagnetic Manipulator,"Machine Learning Control is a control paradigm that applies Artificial Intelligence methods to control problems. Within this domain, the field of Reinforcement Learning (RL) is particularly promising, since it provides a framework in which a control policy does not have to be programmed explicitly, but can be learned by an intelligent controller directly from real-world data, allowing to control systems that are either arduous or even impossible to model analytically. However, in spite of such considerable potential, the RL paradigm poses a number of challenges that effectively hinder its applications in the real-world and in industry. It is therefore critical that research in this field is advanced until RL-based controllers can be practically demonstrated to be real-world feasible and reliable. This thesis report presents the attempts made at applying control strategies based on Reinforcement Learning to solve a precise positioning task with a physical experimental setup. The setup at hand is a magnetic manipulator (magman) characterized by a high degree of nonlinearity. The controller uses the spatially continuous magnetic field generated by four actuators to displace a steel ball, constrained to move in one dimension, towards a reference position. Two different implementations of the Q-learning algorithm (Sutton, Barto, et al., 1998) were deployed. In spite of the good results obtained in a simplified simulated environment, both implementations failed on the experimental setup. The negative outcome of these experiments is mainly due to the fact that, since the task at hand is an accurate positioning task, the reward obtained by the learner while interacting with the environment is too sparse for it to be able to learn a stabilizing control policy. Other factors have presumably contributed to the controllers’ failure, such as the circumstance that the agent does not have access to the full system state information and a sub-optimal tuning of the algorithms’ hyper-parameters. Besides model-free RL, the Value Iteration model-based method was successfully applied both in simulations and with the experimental setup. The present findings suggest that, in order to solve the magman task with model-free RL, more sophisticated algorithms need to be deployed, such as for example an agent that can naturally deal with continuous state and action spaces, as the DDPG algorithm (Lillicrap et al., 2015), with exploration carried out in the parameter-space rather than in the control action space (Plappert et al., 2017), in addition to a more optimal exploitation of the information extracted from the environment, for example using Hindsight Experience Replay (Andrychowicz et al., 2017).",,2019,,209064277,semantic_scholar
b0ed25630d66d052bff2a9218d141a24b19e4d68,https://www.semanticscholar.org/paper/b0ed25630d66d052bff2a9218d141a24b19e4d68,THE BENEFITS OF APPLYING AI TO COMPRESSION,"Artificial intelligence (AI) is a popular subject today. Currently used across various verticals, from medicine to autonomous vehicles and finance, it is projected to have a significant impact. Today, AI is used for video compression, not just to provide bitrate savings but also to improve the quality of experience (QoE) and savings in processing power. This paper will present three applications of AI for video compression, explaining how each helps with the delivery of video content over broadcast and OTT networks. The applications that will be examined include Dynamic Encoding Style (DES), which enables a better trade-off between video quality and bitrate; Dynamic Resolution Encoding (DRE), which enables a superior QoE and density; and Dynamic Frame Rate Encoding (DFE), which allows for improved density and QoE. After a brief presentation of the methods, the paper will then present the results of implementing these technologies in the real world. INTRODUCTION Video compression for broadcast TV services started more than 20 years ago. Over time, several key improvements, such as dual-pass, statistical multiplexing, and software migration, were made to compression technology in order to boost performance. Artificial Intelligence (AI) is driving the next frontier of video compression enhancements. AI is effective at detecting objects and at surveillance. Machines are capable of detecting cancer cells with excellent accuracy, which can be a great help for medical doctors (1, 2). AI algorithms can also be useful at processing a lot of data. Some companies use it to clean large data sets, an activity called data wrangling. More and more, AI can be used for decision-making. The autonomous vehicle collapses many of these uses. Indeed, detection is important in an autonomous car, as other vehicles, persons, objects, and signs on the road need to be clearly identified along with their motion. Together with the internals of the car, it becomes a lot of data to process. The autonomous car has to constantly make decisions about the speed, direction, signaling, and more. In other terms, AI is very effective at predictions (3). More details on the evolution from human-designed algorithm to using AI for live video compression can be found in (10). In the VOD encoding domain, Netflix has been the pioneer in developing an AI-based system to assist file encoding, known as per-title or per-chunk encoding (4). Those techniques only apply to offline encoding and cannot be used for live video. This paper presents three examples of AI applied to live video encoding to optimize broadcast and OTT content delivery. The first three sections present the three examples. For each example, the paper presents a brief presentation of the methods followed by the results, including real-life effects. In this paper, both “AI” and “machine learning” expressions are used, knowing that machine learning is, in fact, a part of AI. DYNAMIC ENCODING STYLE (DES) OR CONTENT-AWARE ENCODING (CAE) FOR BITRATE SAVINGS In this first application, the video compression algorithm itself has improved thanks to machine learning technology. The goal is to improve the video quality/bitrate trade-off, meaning reducing the bitrate while maintaining the video quality or keeping a bitrate and improving the video quality. This is done by the means of encoding styles. Encoding styles are compression algorithm configurations well-suited for particular content. Results DES has been thoroughly tested across a lot of material, and it has shown a bitrate reduction vs. deployed system from 20% up to 30% on VBR content in broadcast applications, and 35% on average up to 50% compared with CBR for streaming applications. Table 1 shows the comparison of the AI-based algorithm with the deployed solution for a customer’s use case. The AI-based algorithm is run at different lower bitrates compared with the deployed solution, from 10% to 30% lower. At 10% the AI-based algorithm is better, at 20% it is equal and at 30% it is worse. The last two columns provide a comparison of lowering the bitrate for both algorithms for verification purposes. The conclusion is that the AI-based algorithm provides a 20% gain. Prog Channel AI version Pool bitrate -10% AI version Pool bitrate -20% AI version Pool bitrate -30% Both versions Pool bitrate -10% Both versions Pool bitrate -20% 1 Documentary = AI slightly lower AI lower AI better AI better 2 Cartoon = = AI lower = = 3 General Entertainment = = AI lower = AI slightly better 4 Movie = = AI slightly lower = AI slightly better 5 Sport AI better = AI lower AI better AI better 6 High action shows AI better AI slightly better = AI better AI better Table 1 – Video quality comparison on different channels between deployed and AI-based algorithm DES and CAE have been deployed in many streaming situations, with some examples and results shown below. The first example is a large streaming service with more than 1 million subscribers and more than 50 channels. This service supports live, VOD, cloud DVR, time-shift and serverside dynamic ad insertion. Due to the COVID-19 global health crisis, the service provider observed a dramatic increase in the bandwidth use and needed a solution to relieve the pressure without changing its infrastructure. By turning on DES and CAE the service provider saw significant improvements on their network. The backbone traffic was reduced by 50%, and the CDN peak usage was reduced by 30%. Figure 1 shows the backbone traffic reduction after DES/CAE was activated. Figure 1 Backbone traffic reduction thanks to DES and CAE The second example involves a large European streaming provider. The measurements were also made during the lockdown period due to COVID-19. In this example we show the average bitrate variation between normal compression and with DES/CAE turned on. For sports content, a bitrate reduction of 30% was measured, and for studio content a bitrate reduction of 40% was observed. Studio content includes television programs, such as talk shows and games shows. Figure 2 Studio content average bitrate reduction thanks to DES/CAE DES/CAE Activated",,2021,,245443415,semantic_scholar
1592204ecb311b4e70b9d79f9722a7878e04b886,https://www.semanticscholar.org/paper/1592204ecb311b4e70b9d79f9722a7878e04b886,Agile Requirements Engineering and Software Planning for a Digital Health Platform to Engage the Effects of Isolation Caused by Social Distancing: Case Study,"Background Social distancing and shielding measures have been put in place to reduce social interaction and slow the transmission of the coronavirus disease (COVID-19). For older people, self-isolation presents particular challenges for mental health and social relationships. As time progresses, continued social distancing could have a compounding impact on these concerns. Objective This project aims to provide a tool for older people and their families and peers to improve their well-being and health during and after regulated social distancing. First, we will evaluate the tool’s feasibility, acceptability, and usability to encourage positive nutrition, enhance physical activity, and enable virtual interaction while social distancing. Second, we will be implementing the app to provide an online community to assist families and peer groups in maintaining contact with older people using goal setting. Anonymized data from the app will be aggregated with other real-world data sources to develop a machine learning algorithm to improve the identification of patients with COVID-19 and track for real time use by health systems. Methods Development of this project is occurring at the time of publication, and therefore, a case study design was selected to provide a systematic means of capturing software engineering in progress. The app development framework for software design was based on agile methods. The evaluation of the app’s feasibility, acceptability and usability shall be conducted using Public Health England's guidance on evaluating digital health products, Bandura’s model of health promotion, the Reach Effectiveness Adoption Implementation Maintenance (RE-AIM) framework and the Nonadoption, Abandonment and Challenges to the Scale-up, Spread and Suitability (NASSS) framework. Results Making use of a pre-existing software framework for health behavior change, a proof of concept was developed, and a multistage app development and deployment for the solution was created. Grant submissions to fund the project and study execution have been sought at the time of publication, and prediscovery iteration of the solution has begun. Ethical approval for a feasibility study design is being sought. Conclusions This case study lays the foundations for future app development to combat mental and societal issues arising from social distancing measures. The app will be tested and evaluated in future studies to allow continuous improvement of the app. This novel contribution will provide an evidence-based exemplar for future app development in the space of social isolation and loneliness.",JMIR public health and surveillance,2020,10.2196/19297,217549025,semantic_scholar
e93cc9ebed4a2a18b72d30fe5a6dad40442f9130,https://www.semanticscholar.org/paper/e93cc9ebed4a2a18b72d30fe5a6dad40442f9130,Big data analytics in Industry 4.0 ecosystems,"The emergence of advanced technologies has triggered a sweeping digital transformation in the industrial ecosystem. The cutting-edge technologies (like, Internet of things, big data, artificial intelligence, drones, cyber-physical systems, and augmented reality, and computer vision) are key enablers of this industrial revolution. Industry 4.0 has reshaped the conventional manufacturing and production processes into automated operations and workflows. This industrial transition is fueled by advanced computing (cloud and edge computing), analytic (big data analytics and computational analytics), intelligent (machine and deep learning), and communication (programmable and intelligent networks) infrastructure and technologies. The collection, aggregation, analysis, and processing of big data generated from the industrial periphery (like manufacturing equipment and maintenance systems) enable real-time decision-making and autonomous opportunities. However, the voluminous size, variability, and frequency of this data bring a wide array of disputes and oppositions in the resource-limited Industrial systems. Moreover, the continuous decision-making workflow in production and manufacturing segments increases the sharing of data across different functions, systems, and organizational boundaries. For this reason, cloud computing and big data technologies (Hadoop and Map-Reduce) can improve the anticipated response and reaction times. Industry 4.0 will lead toward more devices enriched with embedded computing platforms which boost the capabilities of the overall workflow. But, this also leads towards an increased communication and interaction between these devices which can end up in various challenges for the underlying network infrastructure. However, the conventional communication protocols may end up in various performance bottlenecks which in turn can increase the threat from different kinds of attacks and security challenges. Concluding the above discussion, the industrial ecosystem would rely on two entities: (1) users or infrastructure (physical world) and (2) cloud-enabled algorithms and autonomous systems (virtual world) that are connected through advanced and autonomous communication technologies. The driving force behind the success of these industrial ecosystems relies on the efficient gathering/collection, analysis, and storage of data generated by smart devices and sensors. Under this domain, big data analytics is set to be driving predictive manufacturing and provide timely detection of anomalies and system failures to predict product quality. In this way, big data is bound to play a prominent role in driving the industrial ecosystem. Even more, the only reason for this concern is not limited to the volume of data but the major concern is the contribution of this data for the design and implementation of efficient industrial processes and policies. The interpretation and understanding of the available data help to the design of efficient processes and policies related to industrial systems. The focus of this special issue is to present novel and seminal contributions around the important issues and challenges related to big data management and analytics for industrial 4.0 ecosystems. It provides ground-breaking research from academia and industry, that emphasizes the novel solutions, applications, tools, software, and algorithms designed to handle the industrial big data. A substantial number of submissions were received for the special issue. The papers were reviewed by at least three reviewers and underwent a rigorous two rounds of reviews. After the completion of the peer review process, we have accepted 10 seminal contributions related to big data analytics for Industry 4.0. All the accepted papers either discuss the recent solutions related to big data analytics or proposes an innovative way of handling big data across diverse infrastructure deployments. The outline of these contributions discussed below. The first paper titled “An Efficient Scheme for Secure Feature Location using Data Fusion and Data Mining in IOT Environment” by Balaji et al.1 proposes a secure feature location approach based on data fusion and data mining to overcome the challenges of the existing textual and dynamic approaches. The first step in this approach involves the removal of repeated test cases followed by the selection of important attributes. The artificial flora optimization algorithm was used to remove the repeated test cases. After this, the Caesar Cipher-RSA algorithm was used to encrypt the selected attributes, and thereafter a score value was assigned to them. This score value acts as an input to the K-mean algorithm to normalize it using the min-max approach. The evaluation results show that the proposed approach is superior in comparison to existing variants. The second paper titled “Data Dimensionality Reduction Techniques for Industry 4.0: Research Results, Challenges, and Future Research Directions” by Chhikara et al.2 provides a comprehensive survey on dimensionality reduction techniques.",Softw. Pract. Exp.,2021,10.1002/spe.3008,236305446,semantic_scholar
c3d3d7b56fcc775c96777b1c6123b46a452b4aee,https://www.semanticscholar.org/paper/c3d3d7b56fcc775c96777b1c6123b46a452b4aee,Data-Centric Programming Best Practices: Using DDS to Integrate Real-World Systems,".................................................................................................................................... 3 Real-World Systems Programming ........................................................................................... 4 Defining a Data Model .............................................................................................................. 4 DDS Maintains the State of the World as Defined by the Data Model ..................................... 6 About DDS ................................................................................................................................ 8 Best Practices in DDS Programming ........................................................................................ 9 G1. Start by defining a data model, then map the data model to DDS domains, data types and Topics. .............................................................................................................. 9 G2. Fully define your DDS Types, do not rely on opaque bytes or other custom encapsulations ................................................................................................................ 11 G3. Isolate subsystems into DDS Domains. Use mediation, such as RTI Routing Service, to bridge Domains ............................................................................................. 12 G4. Use keyed Topics. For each data type, indicate to DDS the fields that uniquely identify the data-object .................................................................................................... 13 G5. Large teams should create a targeted application platform with system-wide QoS profiles and limited access to the DDS APIs. .................................................................. 16 G6. Configure QoS using the XML Profiles .................................................................... 17 Conclusions ............................................................................................................................. 18 References .............................................................................................................................. 18 Best-Practices Data-Centric Programming: Using DDS to Integrate Real-World Systems November 2010 3 © 2010 Real-Time Innovations Abstract Systems are often implemented by teams using a variety of technologies, programming languages, and operating systems. Integrating and evolving these systems becomes complex. Traditional approaches rely on low-level messaging technologies, delegating much of the message interpretation and information management services to application logic. This complicates system integration because different applications could use inconsistent interpretations and implementations of information-management services, such as detecting component presence, state management, reliability and availability of the information, handling of component failures, etc. Integrating modern systems requires a new, modular network-centric approach that avoids these historic problems by relying on standard APIs and protocols that provide stronger information-management services. For example, many of these systems are heterogeneous, mixing a variety of computer hardware, operating systems, and programming languages. Developers often use Java, .NET, or web-scripting to develop consoles and other GUI-oriented applications, and C or C++ for specialized hardware, device drivers, and performanceor time-critical applications. The end system might mix computers running Windows, Linux, and other operating systems, such as Mac OS X, Android, or real-time operating systems like VxWorks and INTEGRITY. The use of standard APIs and interoperable protocols allows all these systems to be easily integrated and deployed. Today, these systems are typically developed using a service-oriented approach and integrated using standards-based middleware APIs such as DDS, JMS, and CORBA, and protocols such as DDS-RTPS, Web-Services/SOAP, REST/HTTP, AMQP, and CORBA/IIOP. This whitepaper focuses on “real-world” systems, that is, systems that interact with the external physical world and must live within the constraints imposed by real-world physics. Good examples include air-traffic control systems, real-time stock trading, command and control (C2) systems, unmanned vehicles, robotic and vetronics, and Supervisory Control and Data Acquisition (SCADA) systems. More and more these “real-world” systems are integrated using a Data-Centric PublishSubscribe approach, specifically the programming model defined by the Object Management Group (OMG) Data Distribution Service (DDS) specification. This whitepaper describes the basic characteristics of real-world systems programming, reasons why DDS is the best standard middleware technology to use to integrate these systems, and a set of “best practices” guidelines that should be applied when using DDS to implement these systems. Best-Practices Data-Centric Programming: Using DDS to Integrate Real-World Systems November 2010 4 © 2010 Real-Time Innovations Real-World Systems Programming Real-World systems refer to a class of software systems that operate continuously and interact directly with real-world objects, such as aircraft, trains, stock transactions, weapons, robotic and manufacturing equipment, etc. Unlike systems involving only humans and computers, real-world systems have to live within the constraints imposed by the physics of the external world. Notably, time cannot be slowed, paused, or reversed. The implication is that these systems must be able to handle the information at the pace it arrives at, as well as be robust to changes in the operating environment. In addition to these environmental considerations, the nature of typical real-world applications also places demands on their availability and need to continue operating even in the presence of partial failures. In order to interact with the real world, software must include a reasonable, if simplified, model of the external world. This model typically includes aspects of the “state of the world” relevant to system operations. Here the word “state” is used in the normal sense in software modeling and programming. State summarizes the past inputs to the system from its initial state and contains all the information necessary for a system or program to know how it should react to future events or inputs. Imagine that a new component or application starts and joins a system. The “state of the system” contains the information that this new component needs to acquire before it is ready to start performing its function. A typical component would normally only need access to a subset of that state, the portion that directly affects its operation. For example, in an air-traffic management problem, the relevant aspects of the state of the world might include the current location and trajectory of every aircraft, the flight plans of all flights within a 24-hour window, specific details on each aircraft (type, airline, crew), etc. Once a software component or subsystem is running, it interacts with other components by exposing part of its state, notifying other components when its state changes, and invoking operations on (or sending messages to) other components. Each component reacts to these information exchanges by updating its internal model of the world and using that to perform its necessary actions. Defining a Data Model A data model is simply an organized description of the state of the system. Thus, it includes data types, processes for transferring and updating those types, and methods for accessing the data. It does not typically include functions that can alter the data or (importantly) the application-level logic that affects the data. Governance organizations and system integrators often start their design by designing the system data-model. There are good reasons for this approach: • A data model provides governance across disparate teams and organizations, allowing components developed at different points in time by different organizations to be integrated. This makes it an ideal starting point for a central design or governance authority. Best-Practices Data-Centric Programming: Using DDS to Integrate Real-World Systems November 2010 5 © 2010 Real-Time Innovations • A data model represents the better understood, more invariant aspects of the system. Typically the data model is grounded in the “physics of the system.” That is, it describes the kinds of objects and sensors it manages (like aircraft locations, flight plans, and vehicle positions). The data model is not strongly tied to applicationspecific use cases (e.g., the possible fields in a flight plan are a consequence of the nature of aircraft flight); this makes the data model a good starting point, since the full set of use cases might not be well known in advance or might be the responsibility of a different team. • A data model increases decoupling between systems and components. The data model is grounded in the essential information present in the system and it does not depend so much on the use cases that access the information. For example, an airtraffic control model might include a definition of a “flight plan,” but not whether it is automatically generated using an optimization algorithm, checked for collisions, or altered in mid-flight. Using the data model as the basis for the integration avoids over-constraining the design, leaving it open to allow future evolution and use cases. Contrast this with a design based on defining service invocation APIs which are intimately tied to the details of each service and are likely to change as new use cases are incorporated Example Data Model Imagine designing a simple “chat” application. The underlying Data-Model could be defined to contain four kinds of objects summarized in the table below: Object Kind Key Fields Other Fields Description Person EmailAddress Name, Loc",,2010,,17354633,semantic_scholar
e31589522b066d9096e67a2c21ef593039041795,https://www.semanticscholar.org/paper/e31589522b066d9096e67a2c21ef593039041795,A Battery Digital Twin framework for Predictive Maintenance and State of Health Estimation of Electric Vehicles,"To maximize the performance of Electric Vehicle (EV) battery, the requirements of the battery management system (BMS) are getting higher and higher, especially in terms of safety, predictive maintenance, and battery life. The on-board BMS cannot store or process large amounts of data during the operation of a vehicle, with poor real-time capability and data utilization rate. To effectively manage the battery, it is vital to build an Off-board digital twin that can mimic the actual battery with more intelligence. The paper proposes the digital twin framework of Li-Ion battery packs for a fleet of vehicles. This work presents the digital model of the battery, data driven models, contextual information, operational data and the cloud-based deployment. To extend the lifetime of the battery and bring more security into the system, an anomaly detection technique is proposed by using a machine intelligence approach that captures the temporal and spatial relationship between various battery parameters. Also, a learning-based prediction technique is proposed to estimate the health status of battery. The paper outlines the design methodology followed, challenges faced, drawbacks and further opportunities involved in developing the framework for creating a battery digital twin. The performance of the system is analysed with NASA prognostic data set and from the vehicle’s plant model with various drive cycles. Introduction A digital twin is a virtual representation of a physical asset. The digital twin technology helps the global industries to develop a digital DNA of their assets; ranging from as small as smartphone, to as large as a city and as complicated as a human, spanning across industries and processes. This cutting-edge technology helps the companies to optimize performance, maintenance and achieve better results. It came into prominence only upon the advent of other technologies like cloud, artificial intelligence, machine learning etc. VDI-Berichte Nr. 2384, 2021 579 https://doi.org/10.51202/9783181023846-579 Generiert durch IP '54.191.176.224', am 07.11.2021, 18:57:40. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. The electric vehicles (EV) are the future, and the automakers are investing more on to their EV sector nowadays. The key performance component of an EV is its battery. The electric battery is a merger of chemical, thermal, and mechanical processes. The lifetime of these devices depends greatly on the materials used, the system design and the operating conditions. This complexity has therefore made real-world control of battery systems challenging. Timely preparation for future eventualities is a cornerstone for managing batteries in an EV. The battery management system (BMS) of an EV receives considerable amount of data, and processing them is computationally intensive and requires more memory and processing power. Such processing would be difficult to be contained within the on-board systems, and all this data has to be computed elsewhere. With the advent of the Internet of Things and wireless communication in automotive, the information can be stored in the cloud, offering relentless computational power. However, sending the battery data separates it from the physical battery. The battery is still on-board, while the data is offboard. To effectively manage the battery, it is vital to build an off-board digital system that can resemble the actual battery with more intelligence. Due to aging, the parts of a vehicle are invariably at the risk. Not knowing the risk earlier can keep your car out of the road from sometime, the sooner the better. Electric vehicles reduced the output emissions, but the safety side of the batteries are certainly a costlier affair than the internal combustion engine, considering the multiple facets to include such as the chemicals used, the thermal state, mechanical parts, etc. The frequency and cost of regular maintenance service can be reduced if able to diagnose the issues in advance using digital techniques. Such a system identifies the risks and hinders it from becoming an issue, which not only reduces the downtime, but also the repair costs. If the system is able to warn early, component damage can be eliminated to an extent. The common type of batteries used in EV; the Lithium-ion cells are classified as class 9 hazardous materials. The safety risks involved due to thermal hazards alone from the battery perspective opens up a whole new world of risks; let alone the maintenance due to mechanical failure. Predicting a potential risk and subsequent drive to the mechanic ensures your vehicle in mint condition. Hence, this paper proposes the digital twin framework of Li-Ion battery packs for a fleet of vehicles for predictive maintenance. Our contributions in a nutshell:  Developed a digital twin framework of EV batteries for operating and diagnosing a fleet of vehicles VDI-Berichte Nr. 2384, 2021 580 https://doi.org/10.51202/9783181023846-579 Generiert durch IP '54.191.176.224', am 07.11.2021, 18:57:40. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig.  Developed the State of health (SOH) prediction system of battery for the fleet of vehicles using battery digital twin  Developed an intelligent anomaly detection system to analyse the unusual nature of battery behaviour  Developed the digital twin framework and conducted performance evaluation using NASA dataset and data set generated using plant model. Related Works Even though the battery management system of EVs is well discussed in the literature, digital twin-based battery diagnosis is in the primitive stage only. Tanizawa et al. [6] propose a cloud-connected battery management system that continuously connects the batteries to the cloud, manages their state of charge and monitors changes in its characteristics. Koko Friansa et al. [2] proposes a battery monitoring system to monitor the operational and performance of batteries in a small microgrid system. Taesic Kim et al. [5] proposes a cloud-based battery condition monitoring and fault diagnosis platform for largescale lithium-ion battery energy storage systems. Weihan Li et al. [4] proposes a cloud battery management system with online state-of-charge and state-of-health estimates. Billy Wu et al. [1] discusses their perspectives on battery modelling, data-driven approaches and how these elements can be combined in a framework for creating a battery digital twin. Shichun Yang et al. [3] proposes a framework utilizing a cloud architecture for a cloud-based battery management system based on Cyber Hierarchy and Interactional Network (CHAIN) to leverage the use of algorithms that can be used to realize the state-of-X-estimation, thermal management, and other functions of traditional BMS system. The aforementioned works do not delve upon even the scantiest possibilities of irregularity in the battery data. They all assume that the data from the physical asset is in its pristine form. This paper focus on building a digital twin framework and an anomaly detection system by using a machine intelligence approach that captures the temporal and spatial relationship between various battery parameters. The paper also proposes to estimate the State of the health of the EV battery using Long Short-Term Memory (LSTM), a machine learning approach. Proposed Digital Twin Architecture The digital twin architecture of battery system consists of mainly two parts. First part is the physical system of Electric vehicle battery with real-world data and other is the twin of the system implemented on cloud infrastructure. The vehicle will be added to the digital twin VDI-Berichte Nr. 2384, 2021 581 https://doi.org/10.51202/9783181023846-579 Generiert durch IP '54.191.176.224', am 07.11.2021, 18:57:40. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. eco-system when the driver completes the initial registration process with vehicle information. Successful registration will create an instance of the digital twin in cloud, specific to the vehicle. This way, it is possible to manage multiple vehicles at the same time by creating instances of the digital twin.",ELIV 2021,2021,10.51202/9783181023846-579,239553090,semantic_scholar
bf89fad572188e397c9bea2fae07401069da38de,https://www.semanticscholar.org/paper/bf89fad572188e397c9bea2fae07401069da38de,"Lightweight Cryptography Algorithms for Resource-Constrained IoT Devices: A Review, Comparison and Research Opportunities","IoT is becoming more common and popular due to its wide range of applications in various domains. They collect data from the real environment and transfer it over the networks. There are many challenges while deploying IoT in a real-world, varying from tiny sensors to servers. Security is considered as the number one challenge in IoT deployments, as most of the IoT devices are physically accessible in the real world and many of them are limited in resources (such as energy, memory, processing power and even physical space). In this paper, we are focusing on these resource-constrained IoT devices (such as RFID tags, sensors, smart cards, etc.) as securing them in such circumstances is a challenging task. The communication from such devices can be secured by a mean of lightweight cryptography, a lighter version of cryptography. More than fifty lightweight cryptography (plain encryption) algorithms are available in the market with a focus on a specific application(s), and another 57 algorithms have been submitted by the researchers to the NIST competition recently. To provide a holistic view of the area, in this paper, we have compared the existing algorithms in terms of implementation cost, hardware and software performances and attack resistance properties. Also, we have discussed the demand and a direction for new research in the area of lightweight cryptography to optimize balance amongst cost, performance and security.",IEEE Access,2021,10.1109/ACCESS.2021.3052867,232042514,semantic_scholar
48c6c90f708013906e4fb9e82635a94f1db386db,https://www.semanticscholar.org/paper/48c6c90f708013906e4fb9e82635a94f1db386db,DigiMobot: Digital Twin for Human-Robot Collaboration in Indoor Environments,"Human-robot collaboration and cooperation are critical for Autonomous Mobile Robots (AMRs) in order to use them in indoor environments, such as offices, hospitals, libraries, schools, factories, and warehouses. Since a long transition period might be required to fully automate such facilities, we have to deploy AMRs while improving safety in the mixed environments of human and mobile robots. In addition, human behaviors in such environments might be difficult to predict. In this paper, we present a Digital Twin for Autonomous Mobile Robots system named DigiMobot to support, manage, monitor, and validate AMRs in indoor environments. First, DigiMobot can simulate human behaviors and robot movements to verify and validate AMRs to improve safety in a virtual world. Secondly, DigiMobot can monitor and manage AMRs in the physical world by collecting sensor data from each robot in real-time. Since DigiMobot enables us to test the robot systems in the virtual world, we can deploy and implement AMRs in each facility without any modifications. To show the feasibility of DigiMobot, we develop a software framework and two different types of autonomous mobile robots. Finally, we conduct real-world experiments in a warehouse located in Saitama, Japan, in which more than 400, 000 items are stored for commercial purposes.",2021 IEEE Intelligent Vehicles Symposium (IV),2021,10.1109/iv48863.2021.9575499,240463113,semantic_scholar
2d4b9a86270a24126e734c1bf869665a6496ca8b,https://www.semanticscholar.org/paper/2d4b9a86270a24126e734c1bf869665a6496ca8b,Introduction to the ACSAC’19 Special Issue—Vol. 2,"The Annual Computer Security Applications Conference (ACSAC) brings together cutting-edge researchers, with a broad cross-section of security professionals drawn from academia, industry, and government, gathered to present and discuss the latest security results and topics. ACSAC’s core mission is to investigate practical solutions for computer and network security technology. The 35th Annual Computer Security Applications Conference was held in Puerto Rico on December 9–13, 2019. ACSAC 2019 especially encouraged contributions in the area of Deployable and Impactful Security. Deployable and impactful security solutions aim to address key real-world challenges, which may include accuracy, runtime overhead, ground-truth labeling, human aspects, usability, and energy consumption. Having the deployability and impactfulness goals motivates one to focus on solving the most critical real-world challenges, which may otherwise be ignored by the fast-moving research community. In addition, ACSAC encourages authors of accepted papers to submit software and data artifacts and make them publicly available to the entire community. Releasing software and data artifacts represents an important step toward facilitating the reproducibility of research results and ultimately contributes to the real-world deployment of novel security solutions. This special issue includes extended versions of papers that appeared at ACSAC 2019, focusing especially on research on computer security applications with a high potential for being deployed in real-world environments or that have already been deployed and used to implement practical defense systems. This volume contains six articles on topics including DNS security and privacy, anti-virus and malicious software, and IoT and cyber-physical systems security. In “PREMADOMA: An Operational Solution to Prevent Malicious Domain Name Registrations in the .eu TLD,” Desmet et al. propose a system for detecting malicious domains at registration time, before they have an opportunity to be used. PREMADOMA has already been deployed to defend the .eu country-code top-level domain (ccTLD) registrar. The evaluation includes 11 months of real-world observations from the .eu ccTLD, showing that PREMADOMA is effective at blocking a significant number malicious domain registrations. Nakatsuka et al. propose to enable end-to-end DNS privacy in an article titled “PDoT: Private DNS-overTLS with TEE Support.” The PDoT system is a DNS resolver that can run within a Trusted Execution Environment (TEE). Using remote attestation, clients can verify the TEE-based DNS resolver execution, providing a guarantee that the DNS operator running the resolver will not be able to monitor the clients’ DNS requests. To provide strong privacy guarantees, PDoT relies on the fact that the confidentiality of client-to-resolver and resolver-to-name-servers DNS queries can be protected by DNS-over-TLS (DoT), a recently proposed protocol. The evaluation presents experimental results and measurements using a PDoT prototype, demonstrating that achieving strong DNS privacy is feasible. In “Cut-and-Mouse and Ghost Control: Exploiting Antivirus Software with Synthesized Inputs,” Genç et al. present two classes of attacks against popular antivirus (AV) software: Ghost Control and Cut-and-Mouse. The first attack allows malware to avoid detection by simulating mouse events to disable the AV protection. The latter attack allows malware to trigger whitelisted applications into performing malicious activities on their behalf.",Digital Threats: Research and Practice,2021,10.1145/3437253,232115619,semantic_scholar
c54671ea3d65504d5bc7d976a63d4279bed5e1fe,https://www.semanticscholar.org/paper/c54671ea3d65504d5bc7d976a63d4279bed5e1fe,"IFogSim2: An Extended iFogSim Simulator for Mobility, Clustering, and Microservice Management in Edge and Fog Computing Environments","Internet of Things (IoT) has already proven to be the building block for next-generation Cyber-Physical Systems (CPSs). The considerable amount of data generated by the IoT devices needs latency-sensitive processing, which is not feasible by deploying the respective applications in remote Cloud datacentres. Edge/Fog computing, a promising extension of Cloud at the IoT-proximate network, can meet such requirements for smart CPSs. However, the structural and operational differences of Edge/Fog infrastructure resist employing Cloud-based service regulations directly to these environments. As a result, many research works have been recently conducted, focusing on efficient application and resource management in Edge/Fog computing environments. Scalable Edge/Fog infrastructure is a must to validate these policies, which is also challenging to accommodate in the real-world due to high cost and implementation time. Considering simulation as a key to this constraint, various software has been developed that can imitate the physical behaviour of Edge/Fog computing environments. Nevertheless, the existing simulators often fail to support advanced service management features because of their monolithic architecture, lack of actual dataset, and limited scope for a periodic update. To overcome these issues, we have developed multiple simulation models for service migration, dynamic distributed cluster formation, and microservice orchestration for Edge/Fog computing in this work and integrated with the existing iFogSim simulation toolkit for launching it as iFogSim2. The performance of iFogSim2 and its built-in policies are evaluated using three use case scenarios and compared with the contemporary simulators and benchmark policies under different settings. Results indicate that the proposed solution outperform others in service management time, network usage, ram consumption, and simulation time.",ArXiv,2021,,237494539,semantic_scholar
98d924f82af8c4fb621fca29ab56d3eac5b50c8c,https://www.semanticscholar.org/paper/98d924f82af8c4fb621fca29ab56d3eac5b50c8c,A Blockchain-Supported Framework for Charging Management of Electric Vehicles,"Profound changes driven by decarbonization, decentralization, and digitalization are disrupting the energy industry, bringing new challenges to its key stakeholders. In the attempt to address the climate change issue, increasing penetration of renewables and mobility electrification augment the complexity of the electric grid, thus calling for new management approaches to govern energy exchanges while ensuring reliable and secure operations. The emerging blockchain technology is regarded as one of the most promising solutions to respond to the matter in a decentralized, efficient, fast, and secure way. In this work, we propose an Ethereum-based charging management framework for electric vehicles (EVs), tightly interlinked with physical and software infrastructure and implemented in a real-world demonstration site. With a specifically designed solidity-based smart contract governing the charging process, the proposed framework enables secure and reliable accounting of energy exchanges in a network of trustless peers, thus facilitating the EVs’ deployment and encouraging the adoption of blockchain technology for everyday tasks such as EV charging through private and semi-private charging infrastructure. The results of a multi-actor implementation case study in Switzerland demonstrate the feasibility of the proposed blockchain framework and highlight its potential to reduce costs in a typical EV charging business model. Moreover, the study shows that the suggested framework can speed up the charging and billing processes for EV users, simplify the access to energy markets for charging station owners, and facilitate the interaction between the two through specifically designed mobile and web applications. The implementation presented in this paper can be used as a guideline for future blockchain applications for EV charging and other smart grid projects.",Energies,2021,10.3390/en14217144,242059107,semantic_scholar
83c855157830ae26e3420a8bf44877660451c8e4,https://www.semanticscholar.org/paper/83c855157830ae26e3420a8bf44877660451c8e4,On the suitability of current Augmented Reality head-mounted devices,"Simulation is a recognized and much-appreciated tool in healthcare and education. Advances in simulation have led to the burgeoning of various technologies. In recent years, one such technological advancement has been Augmented Reality (AR). Augmented Reality simulations have been implemented in healthcare on various fronts with the help of a plethora of devices including cellphones, tablets, and wearable AR headsets. AR headsets offer the most immersive experience of the AR simulation as they are head-mounted and offer a stereoscopic view of the superimposed 3D models through the attached goggles overlaid on real-world surfaces. To this effect, it is important to understand the performance capabilities of the AR headsets based on workload. In this paper, our objective is to compare the performances of two prominent AR headsets of today, the Microsoft Hololens and the Magic Leap One. We use surgical AR software that allows the surgeons to show internal structures, such as the rib cage, to assist in the surgery as a reference application to obtain performance numbers for those AR devices. Based on our research, there are no performance measurements and recommendations available for these types of devices in general yet. Introduction In an attempt to measure the feasibility and effectiveness of using AR in surgery and nursing education, we developed an application titled ARiSE (Augmented Reality in Surgery and Education) [39]. We incorporated two facets of this application, one to be used during surgery in the Operating Room (OR), and another to assist in the education of nursing students. Surgeons would use this application in the OR during rib-plating surgery and be able to visualize an accurate model of the patient’s rib cage derived from computerized tomography (CT) scans outside their body. The nursing education application would be used by nursing students during the training of fundamental cardiopulmonary physical assessment skills. The students will be able to visualize stock models of various human organs overlaid on manikins along with visual guides to correct auscultation assessment. The aforementioned applications were developed and deployed into the first generation Microsoft Hololens and Magic Leap One AR headsets for practical use. While the application was deployed successfully and demonstrated accurate usability in both devices, our objective in this paper was to measure and compare the performances between the two AR headsets to derive recommendations for when to use which of these devices. The contributions of this paper are as follows: 1. Direct comparison of head-mounted augmented reality devices from the major brands, namely Microsoft and Magic Leap. 2. Expert evaluation based on real-world application tested with the help of domain specialists. 3. Recommendations for head-mounted Augmented Reality devices. Related Work In this section, we discuss some of the other works that relate to our project and use augmented reality techniques and devices. The related work is split into separated subsections with AR being the common theme applied to different medical areas. Augmented Reality in Mobile Devices for Medical Learning Required clinical content cannot always be imparted in live settings due to various restrictions. Educators have instead started using simulation to enhance clinical education. AR simulations have been used to assist the teaching of emergency situations, procedural training, and anatomy [35]. One such AR simulation is described by Von Jan et al. [1] in their paper. The researchers present an application that may be implemented on cellphones and tablet devices that present life-like scenarios which are overlaid on real-world objects. The trainee would visualize these scenarios through their mobile or tablet devices. This application is called mARble, and the researchers report their findings that indicate that AR enhances learning in medical education settings, specifically for subjects that are visually oriented. Augmented Reality Used in Education In their paper, Steve Chi-Yin Yuen et al. [13] have implemented AR in education and training and evaluated its efficiency. The researchers discuss the applications of AR in various fields including architecture, advertising, entertainment, medicine, gaming, books, travel, and the military. With respect to medical education, their results show AR enhancing surgical procedures and aiding clinical procedures by enhancing efficiency, reducing cost, and improving safety. The researchers also state that AR ahs the potential to invent new clinical and surgical procedures. AR has been integrated with existing medical equipment by Fischer et al. in their research [17]. There has also been research that claims AR to have the potential to make surgery minimally invasive [13] and also to enhance the learning experience in educational settings [29]. Chien et al. have used AR to assist in teaching students the anatomy of a 3D skull [16]. Researchers have also demonstrated that AR may enhance the teaching of human anatomy [19]. AR in Nursing Education Wuller et. al. have reviewed existing AR research to assist nursing education [30]. Foronda et. al. have described 3 types of AR applications used to supplement nursing education [6]. Researchers use the Microsoft Hololens to overlay muscles and bones of the human anatomy on manikins. Rahn et. al., overlay 3D models of human organs in real-time on students using iPads[31]. AR was also used with the help of iPads by Abersold et. al. in their study to assist in the training of the placement of the nasogastric tube(NGT) [32]. Ferguson et. al. have claimed game-based AR applications as having the potential to enhance nursing education [33]. This is also supported by Garrett et. al. who demonstrate improved nursing and clinical skills acquisition in students who participated in AR training scenarios [34]. Simulating Surgeries Scott Delp et al. have reviewed the shortcomings of educating medical personnel in providing appropriate emergency care [2]. In their research Samset et al. [14] developed AR tools for minimal invasive therapies (MIT). Scenarios presented by them include liver surgery, liver tumors, and cardiac surgery. With the help of AR the researchers superimpose real-world objects with 3D models obtained from CT scans. Results demonstrated improved surgical procedures and hence the potential of AR to improve healthcare in terms of utility, quality, and cost-effectiveness. Other research has also been conducted with regards to using AR during surgery. Kawamata et al. describe an AR application in their research that assists in the surgery of pituitary tumors [18]. Their results demonstrated this type of AR navigation allowing surgeons to perform accurate and safe endoscopic operations on these tumors. Memory Retention While Using AR Using Steady State Topography (SST) brain imaging to examine the brain activity of people who participated in AR and non-AR tasks, Heather Andrew et. al. [12] found that the visual attention is almost double when performing AR tasks when compared to non-AR tasks. The author also found that what is stored in memory is 70% higher for AR experiences [12]. Other studies show that the long-term memory of the learner can be enhanced by using multiple media interactions in the learning process [11]. Adedukon-Shittu et. al. have also demonstrated the effectiveness of AR technology with regards to enhancing memory retention and performance [23]. Other studies have also demonstrated the enhanced knowledge acquisition and retention of adequate memory when using AR as a supplemental tool in the education process [24]. Feasibility of Using AR to Train Resuscitation Steve Balian et al. [8] introduced a method of testing the feasibility of using augmented reality to educate healthcare providers about administration of Cardio Pulmonary Resuscitation (CPR). Using the Microsoft Hololens to provide users with audio and visual feedback, the blood flow in the human body was superimposed in real time onto a manikin. The study deployed 51 volunteers for this study. The volunteering health care providers were asked to perform CPR using only the Hololens for two minutes. The chest compression parameters were then recorded for this test. The participants generally responded positively to the system. The approach was perceived to be realistic and the AR was considered a helpful tool for training in medical education. Among the volunteers, 94% stated that they would be willing to use this application for CPR training in the future. The further support the notion of AR’s usefulness in education, Balien et al. successfully demonstrated another augmented reality tool that proved to be valuable for existing education approaches in medical training[16]. Menon et al. [37] developed an augmented reality application to improve the training of nursing students that showed a measurable improvement in student outcomes. Time and again augmented reality has proven to be advantageous when integrated into education in terms of novelty, memory retention, and knowledge gained [14] [12] [23] [24] . AR Triage Training for Multi-Casualty Scenarios The order in which patients are treated can have a detrimental effect on the survival rate of a group of patients. Hence, triage, i.e. selecting the most critical patients based on their chance of survival is crucial. John Hendricks et al. [4] devised a virtual reality simulation that assists medical personnel in their training and military field medics in making appropriate decisions in triage training environments. Their model deploys a scene in which users encounter a virtual patient with multiple injury scenarios. The virtual patients can vary with respect to their injuries as well physiological conditions and these conditions can evolve with timebased on their injuries. The injuries are visually supported by animations, such as bleeding and seizures. Augmented rea",,2021,,246273508,semantic_scholar
d54ad52d1c7ba89e9eb26e55aae36ab8bf65db91,https://www.semanticscholar.org/paper/d54ad52d1c7ba89e9eb26e55aae36ab8bf65db91,Software Physical/Virtual Rx Queue Mapping Toward High-Performance Containerized Networking,"Softwarization of Network Functions (NFs) accelerates automated deployment and management of services on next-gen networks. Combining flexibility and high-performance is a vital requirement for Network Functions Virtualisation (NFV); however, many studies have demonstrated that containerization or virtualization of NFs severely degrades the fundamental efficiency of packet forwarding. Virtual network I/O, a mechanism of packet transferring between a guest and the host, has been seen as the performance bottleneck in the PVP (Physical-Virtual-Physical) datapath, and one of the main causes of this deterioration is packet copy between them. Various techniques, such as zero-copy, pass-through, and hardware offloading, have been examined to alleviate the performance overhead. However, existing designs and implementations incur pragmatic issues, such as compatibility, manageability, and insufficient quality of performance. We propose yet another design and implementation of zero-copy/pass-through acceleration (named IOVTee) to resolve real-world problems as well as to enhance the forwarding efficiency. IOVTee takes advantage of pre-processing of virtual switches with achieving zero-copy on the receive (Rx) path. The pluggable style of IOVTee for vhost-user (the de-facto virtual network I/O) enables our approach to be transparent to both containers/VMs and virtual switches. In this article, we explain the heart of IOVTee, a fully software-based Rx queue mapping mechanism (between physical and virtual) that enables a concept of Virtual DMA Write-through (to the NF). Our evaluation results showed that applying IOVTee to vhost-user drastically increased efficiency of packet forwarding in the PVP datapath (by 45% and 98% for traffic of 64-byte and 1514-byte packets respectively).",IEEE Transactions on Network and Service Management,2021,10.1109/TNSM.2020.3049053,232236560,semantic_scholar
5adc8c5ee72d9bfba627323baab2e600e492ad3b,https://www.semanticscholar.org/paper/5adc8c5ee72d9bfba627323baab2e600e492ad3b,"Design, Development, and Validation of an Augmented Reality-Enabled Production Strategy Process","The Production Strategy Process (PSP) is an integral part of production planning and control as it defines how production processes are structured and designed and outlines how production will be executed. PSP involves massive information transfer and communication among project participants. While BIM can improve the flow of information, the paradox of designing 3D models in 2D space remains. This paradox indicates that new visualization technologies are needed to leverage the use of information in the PSP. As Industry 4.0, the fourth industrial revolution, continues to evolve, it is imperative that construction firms seek, find, and adopt new technologies. This research employed Augmented Reality (AR) as a new user interface in the PSP. The current state of practice of PSP was investigated and current challenges are identified. The opportunities to integrate AR were defined, and an AR-enabled future state was proposed. Next, an AR-enabled PSP prototype using the Microsoft HoloLens was implemented and validated on a real-world healthcare project. Usability testing was then conducted using a one-on-one protocol to validate the prototype with 20 participants. Surveys were the deployed to qualitatively assess the impact of integrating AR into PSP. The difference between the traditional PSP and the AR-enabled PSP was tested through a series of hypotheses comparing both processes. The results demonstrate that the AR-enabled PSP offers significant benefits over the Traditional PSP: improved collaboration, reduced miscommunication, increased quality and detection of errors, enhanced decision-making, better documentation, better information access, improved information flow, increased input accuracy, and increased integration of safety considerations. Additionally, the technology, software, and hardware were also evaluated, and, on average, the findings demonstrated the potential of AR in production planning.",Frontiers in Built Environment,2022,10.3389/fbuil.2022.730098,246491609,semantic_scholar
036a7c42a459eb751bba2f8badec3b62d6328c10,https://www.semanticscholar.org/paper/036a7c42a459eb751bba2f8badec3b62d6328c10,EDITORIAL Wireless sensor networks: design for real-life deployment and deployment experiences Wireless sensor networks: design for real-life deployment and deployment experiences,"Wireless sensor networks (WSNs) are among the most promising technologies of the new millennium. The opportunities afforded by being able to program networks of small, lightweight, low-power, computation- and bandwidth-limited nodes have attracted a large community of researchers and developers. However, the unique set of capabilities offered by the technology produces an exciting but complex design space, which is often difficult to negotiate in an application context. Deploying sensing physical environments produces its own set of challenges, and can push systems into failure modes, thus revealing problems that can be difficult to discover or reproduce in simulation or the laboratory. Sustained efforts in the area of wireless networked sensing over the last 15 years have resulted in a large number of theoretical developments, substantial practical achievements, and a wealth of lessons for the future. It is clear that in order to bridge the gap between (on the one hand) visions of very large scale, autonomous, randomly deployed networks and (on the other) the actual performance of fielded systems, we need to view deployment as an essential component in the process of developing sensor networks: a process that includes hardware and software solutions that serve specific applications and end-user needs. Incorporating deployment into the design process reveals a new and different set of requirements and considerations, whose solutions require innovative thinking, multidisciplinary teams and strong involvement from end-user communities. This special feature uncovers and documents some of the hurdles encountered and solutions offered by experimental scientists when deploying and evaluating wireless sensor networks in situ, in a variety of well specified application scenarios. The papers specifically address issues of generic importance for WSN system designers: (i) data quality, (ii) communications availability and quality, (iii) alternative, low-energy sensing modalities and (iv) system solutions with high end-user added value and cost benefits. The common thread is deployment and deployment evaluation. In particular, satisfaction of application requirements, involvement of the end-user in the design and deployment process, satisfactory system performance and user acceptance are concerns addressed in many of the contributions. The contributions form a valuable set, which help to identify the priorities for research in this burgeoning area: Robust, reliable and efficient data collection in embedded wireless multi-hop networks are essential elements in creating a true deploy-and-forget user experience. Maintaining full connectivity within a WSN, in a real world environment populated by other WSNs, WiFi networks or Bluetooth devices that constitute sources of interference is a key element in any application, but more so for those that are safety-critical, such as disaster response. Awareness of the effects of wireless channel, physical position and line-of-sight on received signal strength in real-world, outdoor environments will shape the design of many outdoor applications. Thus, the quantification of such effects is valuable knowledge for designers. Sensors' failure detection, scalability and commercialization are common challenges in many long-term monitoring applications; transferable solutions are evidenced here in the context of pollutant detection and water quality. Innovative, alternative thinking is often needed to achieve the desired long-lived networks when power-hungry sensors are foreseen components; in some instances, the very problems of wireless technology, such as RF irregularity, can be transformed into advantages. The importance of an iterative design and evaluation methodology—from analysis to simulation to real-life deployment—should be well understood by all WSN developers. The value of this is highlighted in the context of a challenging WPAN video-surveillance application based on a novel Nomadic Access Mechanism. Cost benefits to be drawn from devising a WSN based solution to classic application areas such as surveillance are often a prime motivator for WSN designers; an example is offered here based on the use of intelligent agents for intrusion monitoring. Last but not least, the practicality and usability of the WSN solutions found for novel applications is key to their adoption. This is particularly true when the end-users of the developed technology are medical patients. The importance of feedback, elegant hardware encapsulation and extraction of meaning from data is presented in the context of novel orthopedic rehabilitation aids. Overall, this feature offers wide coverage of most issues encountered in the process of design, implementation and evaluation of deployable WSN systems. We trust that designers and developers of WSN systems will find much work of value, ranging from lessons learned, through solutions to known hurdles, to novel developments that enhance applications. Finally, we would like to thank all authors for their valuable contributions!",,2010,10.1088/0957-0233/21/12/120101,118864811,semantic_scholar
8b68f00f25c143c99ffb2cbeaf485ece497793d6,https://www.semanticscholar.org/paper/8b68f00f25c143c99ffb2cbeaf485ece497793d6,Edge Data Based Trailer Inception Probabilistic Matrix Factorization for Context-Aware Movie Recommendation,"The rapid growth of edge data generated by mobile devices and applications deployed at the edge of the network has exacerbated the problem of information overload. As an effective way to alleviate information overload, recommender system can improve the quality of various services by adding application data generated by users on edge devices, such as visual and textual information, on the basis of sparse rating data. The visual information in the movie trailer is a significant part of the movie recommender system. However, due to the complexity of visual information extraction, data sparsity cannot be remarkably alleviated by merely using the rough visual features to improve the rating prediction accuracy. Fortunately, the convolutional neural network can be used to extract the visual features precisely. Therefore, the end-to-end neural image caption (NIC) model can be utilized to obtain the textual information describing the visual features of movie trailers. This paper proposes a trailer inception probabilistic matrix factorization model called Ti-PMF, which combines NIC, recurrent convolutional neural network, and probabilistic matrix factorization models as the rating prediction model. We implement the proposed Ti-PMF model with extensive experiments on three real-world datasets to validate its effectiveness. The experimental results illustrate that the proposed Ti-PMF outperforms the existing ones. H. Chen, Z. Li, Z. Wang, Z. Ni, and J. Li College of Control Science and Engineering, China University of Petroleum, Qingdao 266580, China. E-mail: chenhl@upc.edu.cn H. Chen and G. Xu College of Computer and Control Engineering, Minjiang University, Fuzhou 350108, China E-mail: xuge@pku.edu.cn A. Aziz School of Software, Dalian University of Technology, Dalian 116620, China. F. Xia School of Engineering, IT and Physical Sciences, Federation University Australia, Ballarat, VIC 3353, Australia. E-mail: f.xia@ieee.org ar X iv :2 20 2. 10 23 6v 1 [ cs .C V ] 1 6 Fe b 20 22 2 Honglong Chen et al.",World Wide Web,2021,10.1007/s11280-021-00974-4,245008789,semantic_scholar
fa1387e89d40c6b68969fa26315da581db6aeb23,https://www.semanticscholar.org/paper/fa1387e89d40c6b68969fa26315da581db6aeb23,Agile Requirements Engineering and Software Planning for a Digital Health Platform to Engage the Effects of Isolation Caused by Social Distancing: Case Study (Preprint),"
 BACKGROUND
 Social distancing and shielding measures have been put in place to reduce social interaction and slow the transmission of the coronavirus disease (COVID-19). For older people, self-isolation presents particular challenges for mental health and social relationships. As time progresses, continued social distancing could have a compounding impact on these concerns.
 
 
 OBJECTIVE
 This project aims to provide a tool for older people and their families and peers to improve their well-being and health during and after regulated social distancing. First, we will evaluate the tool’s feasibility, acceptability, and usability to encourage positive nutrition, enhance physical activity, and enable virtual interaction while social distancing. Second, we will be implementing the app to provide an online community to assist families and peer groups in maintaining contact with older people using goal setting. Anonymized data from the app will be aggregated with other real-world data sources to develop a machine learning algorithm to improve the identification of patients with COVID-19 and track for real time use by health systems.
 
 
 METHODS
 Development of this project is occurring at the time of publication, and therefore, a case study design was selected to provide a systematic means of capturing software engineering in progress. The app development framework for software design was based on agile methods. The evaluation of the app’s feasibility, acceptability and usability shall be conducted using Public Health England's guidance on evaluating digital health products, Bandura’s model of health promotion, the Reach Effectiveness Adoption Implementation Maintenance (RE-AIM) framework and the Nonadoption, Abandonment and Challenges to the Scale-up, Spread and Suitability (NASSS) framework.
 
 
 RESULTS
 Making use of a pre-existing software framework for health behavior change, a proof of concept was developed, and a multistage app development and deployment for the solution was created. Grant submissions to fund the project and study execution have been sought at the time of publication, and prediscovery iteration of the solution has begun. Ethical approval for a feasibility study design is being sought.
 
 
 CONCLUSIONS
 This case study lays the foundations for future app development to combat mental and societal issues arising from social distancing measures. The app will be tested and evaluated in future studies to allow continuous improvement of the app. This novel contribution will provide an evidence-based exemplar for future app development in the space of social isolation and loneliness.
",,2020,10.2196/preprints.19297,219073060,semantic_scholar
3552bef6e5478dfec4a704927f0939e0b938f910,https://www.semanticscholar.org/paper/3552bef6e5478dfec4a704927f0939e0b938f910,SensEH: From simulation to deployment of energy harvesting wireless sensor networks,"Energy autonomy and system lifetime are critical concerns in wireless sensor networks (WSNs), for which energy harvesting (EH) is emerging as a promising solution. Nevertheless, the tools supporting the design of EH-WSNs are limited to a few simulators that require developers to re-implement the application with programming languages different from WSN ones. Further, simulators notoriously provide only a rough approximation of the reality of low-power wireless communication. In this paper we present SENSEH, a software framework that allows developers to move back and forth between the power and speed of a simulated approach and the reality and accuracy of in-field experiments. SENSEH relies on COOJA for emulating the actual, deployment-ready code, and provides two modes of operation that allow the reuse of exactly the same code in real-world WSN deployments. We describe the toolchain and software architecture of SENSEH, and demonstrate its practical use and benefits in the context of a case study where we investigate how the lifetime of a WSN used for adaptive lighting in road tunnels can be extended using harvesters based on photovoltaic panels.",39th Annual IEEE Conference on Local Computer Networks Workshops,2014,10.1109/LCNW.2014.6927704,11680969,semantic_scholar
f4ffe28408602de97f85c9bf3410a7173fd3ed3c,https://www.semanticscholar.org/paper/f4ffe28408602de97f85c9bf3410a7173fd3ed3c,BEST PRACTICES REAL-WORLD EXPERIENCES OFFER INSIGHTS Virtualization,"Propelled by the technologies that VMware and Intel have pioneered, virtualization has rapidly gained market acceptance. Indeed, VMware virtual infrastructure software has some 4 million users and more than 20,000 corporate customers, many of which have been built on Intel-based systems. One of those corporate customers is Solvay Pharmaceuticals, a chemical and pharmaceutical group with 29,000 employees in 50 countries, which identified virtualization as the solution to contain server growth. “Because we’re in a regulated industry, we would need to get three new servers every time we needed a new application—one for development, one for test and one for production,” explains Bruce McMillan, Solvay’s manager, emerging technologies. The company implemented VMware® Infrastructure 3 featuring VMware® ESX server hosts on HP ProLiant servers powered by quad-core Intel® Xeon® processors. By going from 65 physical servers to just 17, McMillan says the virtualization implementation “has saved us $1.5 million in pure hardware costs. That’s not even counting power and cooling costs, or staffing costs.” There are many more examples of successful virtualization deployments across many industries and mixes of applications, leading to the emergence of clear best practices that can benefit organizations implementing virtualization for the first time.",,2008,,43067716,semantic_scholar
b8317ef9bcf24c688c67e5062cb388e53b2dd150,https://www.semanticscholar.org/paper/b8317ef9bcf24c688c67e5062cb388e53b2dd150,Decoding Motor Skills of AI and Human Policies: A Study on Humanoid and Human Balance Control,"In this study, we propose a new paradigm of using a machine learning approach to facilitate a quicker, more efficient and effective control development, as a different approach of utilising the power of machine learning in addition to other options that intent to use learning directly in real-world applications. We first develop a DRL-based control framework to learn rich motor skills of push recovery for humanoid robots that exhibit human-like push recovery behaviour. Next, we propose to take advantage of DRL to quickly discover solutions for very difficult problems, and then extract the principles of those policies as guidelines for developing engineered controllers. Furthermore, a comparison between humanoid and human balancing is conducted to show the characteristics of the learned humanoid behaviour. This comparison will show that DRL algorithms can learn a good policy with short development and training time that may require humans years to learn. We analyse input-output data collected from humanoid and human policies and postulate a Minimum-Jerk ModelPredictive Control (MJMPC) Framework that quantitatively reflects both AI and human push recovery policies. I. SCIENTIFIC MOTIVATION From the advancement in computers, computer-aided design for mechanical and electronic engineering, architecture and many other engineering fields emerged. Foreseeing a similar development curve and technology wave, we forecast a new emerging discipline in the near future that uses learning-aided approaches for catalysing control development, alongside other similar applications such as in medicine discovery. In this study, we propose a new paradigm of using a machine learning approach to facilitate a quicker, more efficient and effective control development, as a different approach of leveraging the power of machine learning in addition to other options that intent to use learning directly in real-world applications. Machine Learning and Deep Reinforcement Learning (DRL) in particular have reached an advanced stage to produce powerful policies with better autonomous performances than many state-of-the-art control and planning approaches in robot locomotion [1], robotic manipulation [2], and even the control of complex morphological machines [3]. Notably, DRL’s ability to solve complex problems with a relatively short development time is especially attractive, which is empowered by training policies that maximise the cumulative reward through the exploration of the action and state space, rather than using prior knowledge of the models about the robot, the world, and their interactions. To leverage the capabilities of DRL, we first develop a DRL-based control framework to learn rich motor skills of push recovery for humanoid robots. The complexity in whole-body balancing arises in challenges such as multi(a) Ankle Strategy (b) Hip Strategy (c) Toe Strategy (d) Step Strategy Fig. 1: Human-like Push Recovery strategies emerging from Deep Reinforcement Learning. The discovered behaviours serve as a guideline for the design of certifiable and safe controllers that replicate advantageous strategies from AI policies. contact coordination based on multi-sensory inputs, state transitions between fullyand under-actuated situations, switching policies, and generalising to external disturbances on any body parts, while accounting for all edge cases that a designer has difficulty to consider beforehand. In such a setting, manually designing the individual control strategies and finding a reliable switching mechanism requires both substantial development time, mathematical rigour, and code implementation. On the other hand, through a well-designed DRL framework and task-specific training procedures, a robust policy can be learned automatically by interacting with the environment, requiring only computational power. In particular, as shown in Fig. 1, our learned policy exhibits human-like push recovery behaviour with four typical push recovery strategies emerging naturally: ankle, hip, toe, and IEEE Robotics and Automation Magazine (RAM) paper, presented at IROS 2020. It should be cited as a RAM paper. stepping strategy. Though the learned control policy could possibly be deployed on the real robotic system, the lack of explainability and analytical reasoning of the Neural Network makes it unsuitable for safety-critical applications in real world. Furthermore, due to the demand of large data and sampleinefficient nature of DRL algorithms, complex policies are typically trained in simulation, which cannot guarantee the same performance while transferred directly to the real system [1], and the challenge of reality gap raises concerning about both the safety and performance. To benefit from both the safety and interpretability for the control policy and the versatility and adaptability from learning, we propose to take advantage of DRL to quickly discover versatile, deployable policies and solutions for very difficult problems, and then study, analyse and extract the principles of those policies as guidelines for developing engineered controllers in a reliable manner. By doing so, we utilise the AI-solutions for rapid control development (Fig. 3) to design safe and certifiable controllers which can be verified and deployed on real-world robots (Fig. 2). While classical control development is based on gradually building knowledge that increases the performance incrementally, using a template policy will provide disruptive, innovative solutions that will escalate performance (green line, Fig. 3). DRL is able to achieve good performance by a number of iterations in the DRL learning framework. However, the achieved performance is still comparatively low to what tuning in control can do. Combining both approaches to “kick-start” the iteration process helps to design good controllers. After knowing the system and the controller, it is straightforward to improve upon due to the fact that we are then able to understand why the performance is lower than the optimum, whereas in the case of DRL, there is little influence from human engineers to improve the performance but reshaping the reward and/or altering the learning framework, and relying on the exploration being sufficiently large to achieve high performance. In this paper, we are motivated to study a viable approach to infer underlying principles of an AI policy by studying its perception-action relation, i.e., to some extent, reverseengineer an equivalent controller in terms of functionality based on a black-box policy. This methodology is not only applicable to AI policies, but also to any black-box policies, such as a human policy. Without knowing exactly how push recovery policies are realised by Artificial Neural Network (ANN) or biological human Neural Network, we can still analyse the behaviour at the functionality level by studying their input-output relationship. Based on evidence of optimality in human manipulation tasks [4], we hypothesise that policies for push recovery in humans and humanoid are both optimal control process that follows certain optimal criteria that can be quantified. Following this hypothesis, we analyse and utilise inputoutput data collected from both humanoid and human policies, and propose a Minimum-Jerk Model-Predictive Control (MJMPC) Framework that is able to quantitatively reflect both the AI and human push recovery policies. The engineered controller has high similarity (Coefficient of Determination more than 90%) with the collected data, and also exhibits the same human-like push recovery strategies, which emerge from the proposed MJMPC without the need of manual switching between the strategies. Furthermore, a comparison between humanoid and human balancing is conducted to show the characteristics of the learned humanoid behaviour. This comparison will show that DRL algorithms are very powerful to learn a policy (e.g., balancing) within a short development and training time that may require humans years to learn. In contrast, in order to design an engineered controller from scratch with similar performance, months or even years are needed for developmental iterations, mainly because of the high-redundancy and a diversity of control actions, which are yet challenging to resolve the physical optimality on a high Degree of Freedom (DoF) robot. In this regard, the learning approach is very attractive because of the significant reduction of manual effort, and the learning architecture requires only the design of input-output and rewards. This article shed some light on a new paradigm: the recent high-profile successes in DRL suggest new high-quality value in learning methods that the discovered policies can be used as a basis for speeding up the development of robotic controllers (Fig. 2). As an outcome in this push recovery study, we obtain a certifiable, analysable optimal controller that does not require any state machine or switching mechanism, while exhibiting human-like push recovery strategies, such as ankle, hip, toe, and stepping strategy all in a coherent optimisation process. II. GENERATING COMPLEX MOTIONS FOR HUMANOID ROBOTS THROUGH DEEP REINFORCEMENT LEARNING To use DRL-policies as a basis for analysis, these policies must reach a certain performance threshold that ideally surpasses traditional control approaches both in the types of motions it can generate and the amount of disturbances that it can withstand. DRL has been shown to be capable of learning locomotion and fall recovery policies that surpasses traditional control approaches for quadruped robots in terms of power efficiency, and versatility of motion [1]. In this section, we present a hierarchical learning framework for achieving versatile behaviours during push recovery for humanoid robots as proposed in [5]. The learned policy exhibits a wide range of balancing strategies that are comparable to human push recovery. In particular, the learned policy is able to withstand external distu",,2020,,219952864,semantic_scholar
ec589937c2bbdb93fcef6e7f99e82d4ca99f0306,https://www.semanticscholar.org/paper/ec589937c2bbdb93fcef6e7f99e82d4ca99f0306,Flight Controller Synthesis Via Deep Reinforcement Learning,"Traditional control methods are inadequate in many deployment settings involving control of Cyber-Physical Systems (CPS). In such settings, CPS controllers must operate and respond to unpredictable interactions, conditions, or failure modes. Dealing with such unpredictability requires the use of executive and cognitive control functions that allow for planning and reasoning. Motivated by the sport of drone racing, this dissertation addresses these concerns for state-of-the-art flight control by investigating the use of deep neural networks to bring essential elements of higher-level cognition for constructing low level flight controllers. 
This thesis reports on the development and release of an open source, full solution stack for building neuro-flight controllers. This stack consists of the methodology for constructing a multicopter digital twin for synthesize the flight controller unique to a specific aircraft, a tuning framework for implementing training environments (GymFC), and a firmware for the world's first neural network supported flight controller (Neuroflight). GymFC's novel approach fuses together the digital twinning paradigm for flight control training to provide seamless transfer to hardware. Additionally, this thesis examines alternative reward system functions as well as changes to the software environment to bridge the gap between the simulation and real world deployment environments. 
Work summarized in this thesis demonstrates that reinforcement learning is able to be leveraged for training neural network controllers capable, not only of maintaining stable flight, but also precision aerobatic maneuvers in real world settings. As such, this work provides a foundation for developing the next generation of flight control systems.",ArXiv,2019,,202578037,semantic_scholar
a725dfbed5b04e1300b17f5e823c2bfd9953b4b5,https://www.semanticscholar.org/paper/a725dfbed5b04e1300b17f5e823c2bfd9953b4b5,SoftTap: A Software-Defined TAP via Switch-Based Traffic Mirroring,"With widespread deployment of virtualization technologies in datacenter networks, traditional tools used for network monitoring, such as hardware taps, become unfit. This is due to the inability of hardware solutions for dynamic deployment and virtual network monitoring. This paper presents the design and evaluation of SoftTap, a scalable alternative to hardware taps which is capable of operating over both physical and virtual switches. SoftTap is based on port and flow mirroring capabilities of commodity OpenFlow switches and is not limited to a specific network architecture or topology. A key design challenge in SoftTap is the fast computation of switch mirroring configurations in large-scale deployments. Our design is based on novel polynomial time approximation algorithms that are shown to achieve bounded approximation ratios compared to optimal solutions. We evaluate SoftTap using model-driven simulations as well as realistic Mininet experiments. Specifically, our simulations consider large networks to show the scalability of SoftTap. Mininet experiments, on the other hand, consider its real-world utility by implementing an intrusion detection system (IDS) and a VoIP metering application on top of SoftTap. In our experiments, under SoftTap, IDS achieves up to 25% higher detection recall, while VoIP metering achieves up to 23% less packet loss compared to existing mirroring-based traffic monitoring approaches.",2021 IEEE 7th International Conference on Network Softwarization (NetSoft),2021,10.1109/NetSoft51509.2021.9492588,236480238,semantic_scholar
d99c8f011375b443237e4b4dad767549ab313f38,https://www.semanticscholar.org/paper/d99c8f011375b443237e4b4dad767549ab313f38,"Design, Analysis, and Experimental Evaluation of a New Secure Rejoin Mechanism for LoRaWAN Using Elliptic-Curve Cryptography","LoRaWAN (Long Range Wide Area Network) is a Low-Power Wide Area Networks (LPWAN) technology with very rapid uptake during the previous years, developed by the LoRa (Long Range) Alliance as an open standard operating over the unlicensed band. Current LoRaWAN architecture foresees specific techniques for bootstrapping end-to-end encryption during network initialization. In particular, this work focuses on the Over-The-Air Activation (OTAA) method, which uses two keys (Network key (NwkKey) and Application key (AppKey)) that are hard-coded into the device and do not change throughout the entire lifetime of the deployment. The inability to refresh these two keys is as a weak point in terms of the overall security of the network especially when considering deployments that are expected to operate for at least 10–15 years. In this paper, the security issues of OTAA are presented in detail highlighting the vulnerabilities against the specific type of attacks. A new scheme for network activation is proposed that builds upon the current LoRaWAN architecture in a way that maintains backwards compatibility while resolving certain vulnerabilities. Under the new mechanism, the devices periodically negotiate new keys securely based on elliptic-curve cryptography. The security properties of the proposed mechanism are analyzed against a specific type of attacks. The analysis indicates that the new secure rejoin mechanism guarantees (i) computational key secrecy, (ii) decisional key secrecy, and (iii) key independence, forward and backward, for both root keys thus properly addressing the considered security vulnerabilities of LoRaWAN. Moreover, the method is implemented in software using the RIOT-OS, a hardware-independent operating system that supports many different architectures for 8 bit, 16 bit, 32 bit and 64 bit processors. The resulting software is evaluated on the FIT IoT-Lab real-world experimentation facility under a diverse set of ARM Cortex-M* devices targeting a broad range of IoT applications, ranging from advanced wearable devices to interactive entertainment devices, home automation and industrial cyber-physical systems. The experiments indicate that the overall overhead incurred in terms of energy and time by the proposed rejoin mechanism is acceptable given the low frequency of execution and the improvements to the overall security of the LoRaWAN1.1 OTAA method.",J. Sens. Actuator Networks,2021,10.3390/jsan10020036,235965215,semantic_scholar
334c96fd3595cf14477eaa52455f14b17d2b8ffc,https://www.semanticscholar.org/paper/334c96fd3595cf14477eaa52455f14b17d2b8ffc,PIE: a Tool for Data-Driven Autonomous UAV Flight Testing,"In this paper, a novel technique is presented to test the flight of an unmanned aerial vehicle autonomously in a real-world scenario using a data-driven technique without intervening with its onboard software. With the growing applications of such vehicles, testing of autonomous flight is a very important task for rapid deployment. There are different tools for modeling and simulating unmanned vehicles in virtual worlds such as Gazebo, MATLAB, Simulink, and Webots to name a few. None of these simulation tools are able to model all possible physical parameters of a real-world environment. Hence, the flight controller or mission planning software has to be tested in the physical world in the presence of an expert before deployment for a specific task. A Perception Inference Engine evaluation tool is presented that can infer internal states of the autonomous system from external observations only. The Gazebo simulation platform is used to collect data to develop the perception model. For real-time data collection, a VICON motion capture system is used to observe the autonomous flight of a small unmanned aerial vehicle. A state-of-the-art decision tree algorithm is used to implement the data-driven approach. The technique was tested using simulation data and verified with real-time data from Intel Aero Ready to Fly and Parrot AR. 2.0 drones. Moreover, we analyzed the robustness of the proposed system by introducing noise in sensor measurement and ambiguity in the testing scenario. We compared the performance of the decision tree classifier with Naïve bayes and support vector machine classifiers. It is shown that the developed system can be used for the performance evaluation of a UAV operating in the physical world by significantly reducing uncertainty in mission failure due to environmental parameters.",J. Intell. Robotic Syst.,2020,10.1007/s10846-019-01078-y,203123838,semantic_scholar
eb0e7c3176f89fa33aa50d9d3ec1fd83ab3cf96a,https://www.semanticscholar.org/paper/eb0e7c3176f89fa33aa50d9d3ec1fd83ab3cf96a,"Health 4.0: Applications, Management, Technologies and Review","The Industry 4.0 Standard (I4S) employs technologies for automation and data exchange through cloud computing, Big Data (BD), Internet of Things (IoT), forms of wireless Internet, 5G technologies, cryptography, the use of semantic database (DB) design, Augmented Reality (AR) and Content-Based Image Retrieval (CBIR). Its healthcare extension is the so-called Health 4.0. 
This study informs about Health 4.0 and its potential to extend, virtualize and enable new healthcare-related processes (e.g., home care, finitude medicine, and personalized/remotely triggered pharmaceutical treatments) and transform them into services. 
In the future, these services will be able to virtualize multiple levels of care, connect devices and move to Personalized Medicine (PM). The Health 4.0 Cyber-Physical System (HCPS) contains several types of computers, communications, storage, interfaces, biosensors, and bioactuators. The HCPS paradigm permits observing processes from the real world, as well as monitoring patients before, during and after surgical procedures using biosensors. Besides, HCPSs contain bioactuators that accomplish the intended interventions along with other novel strategies to deploy PM. A biosensor detects some critical outer and inner patient conditions and sends these signals to a Decision-Making Unit (DMU). Mobile devices and wearables are present examples of gadgets containing biosensors. Once the DMU receives signals, they can be compared to the patient’s medical history and, depending on the protocols, a set of measures to handle a given situation will follow. The part responsible for the implementation of the automated mitigation actions are the bioactuators, which can vary from a buzzer to the remote-controlled release of some elements in a capsule inside the patient’s body. 
            Decentralizing health services is a challenge for the creation of health-related applications. Together, CBIR systems can enable access to information from multimedia and multimodality images, which can aid in patient diagnosis and medical decision-making. 
Currently, the National Health Service addresses the application of communication tools to patients and medical teams to intensify the transfer of treatments from the hospital to the home, without disruption in outpatient services. 
HCPS technologies share tools with remote servers, allowing data embedding and BD analysis and permit easy integration of healthcare professionals expertise with intelligent devices.  However, it is undeniable the need for improvements, multidisciplinary discussions, strong laws/protocols, inventories about the impact of novel techniques on patients/caregivers as well as rigorous tests of accuracy until reaching the level of automating any medical care technological initiative.",Medical Technologies Journal,2019,10.26415/2572-004X-VOL2ISS4P262-276,149726840,semantic_scholar
06a05c5ecdfb9ab6867722752c16b1e8c21361dd,https://www.semanticscholar.org/paper/06a05c5ecdfb9ab6867722752c16b1e8c21361dd,On the practical security of white-box cryptography. (De la théorie à la pratique de la cryptographie en boite blanche),"Cryptography studies how to secure communications and information. The security of a cryptosystem depends on the secrecy of the underlying key. White-box cryptography explores methods to hide a cryptographic key into some software deployed in the real world. 
 
Classical cryptography only assumes that the adversary accesses the target cryptographic primitive in a black-box manner in which she can only observe or manipulate the input and output of the primitive, but cannot know or tamper with its internal details. The gray-box model further allows an adversary to exploit key- dependent sensitive information leaked from the execution of physical implementations. All sorts of side-channel attacks exploit some physical information leakage, such as the power consumption of the device. The white-box model considers the worst-case scenario in which the adversary has complete control over the software and its execution environment. The goal of white-box cryptography is to securely implement a cryptographic primitive against such a powerful adversary. Although the scientific community has proposed some candidate solutions to build white-box cryptography, all have proven ineffective. Consequently, this problem has remained open for almost two decades since the concept was introduced. 
 
The continuous growth in market demand and the emerging potential applications have driven the industry to deploy secretly-designed proprietary solutions. Al- though this paradigm of achieving security through obscurity contradicts the widely accepted Kerckhoffs' principle in cryptography, this is currently the only option for white-box cryptography. Security experts have reported how gray-box attacks could be used to extract keys from several publicly available white-box implementations. In a gray-box attack, the adversary adapts side-channel analysis techniques to the white-box context, i.e., to target computation traces made of noise-free run-time information instead of the noisy physical leakage. Gray-box attacks are generic since they do not require any a priori knowledge of the implementation and hence avoid costly reverse engineering. Some non-publicly scrutinized industrial white-box schemes in the market are believed to be under the threat of gray-box attacks. 
 
This thesis focuses on the analysis and improvement of gray-box attacks and the associated countermeasures for white-box cryptography. We first provide an in- depth analysis of why gray-box attacks are capable of breaking the classical white-box design which is based on table encodings. Next, we introduce a new gray-box attack named linear decoding analysis and show that linearly encoding sensitive information is insufficient to protect the cryptographic software. Afterward, we describe how to combine state-of-the-art countermeasures to resist gray-box attacks and comprehensively elaborate on the (in)effectiveness of these combined countermeasures in terms of computation complexity. Finally, we introduce a new attack technique that exploits the data-dependency of the targeted implementation to substantially lower the complexity of the existing gray-box attacks on white-box cryptography. In addition to the theoretical analyses and new attack techniques introduced in this thesis, we report some attack experiments against practical white-box implementations. In particular, we could break the winning implementations of two consecutive editions of the well-known WhibOx white-box cryptography competition.",,2020,,225666192,semantic_scholar
1704ce2c7bdcd18919d5b7fcdaf29dfd7bac3fae,https://www.semanticscholar.org/paper/1704ce2c7bdcd18919d5b7fcdaf29dfd7bac3fae,Statement of Research Interests,"I develop secure systems that prevent advanced cybersecurity threats targeting hardware vulnerabilities. To that end, my research interests lie at the intersection of cryptography, computer architecture, and digital hardware design. Trusted computing in hardware is fundamental to information security practices. The basis of security guarantees in digital systems is essentially a set of cryptographic operations executing in a hardware root of trust. Advanced cyberattacks therefore deliberately target hardware layer vulnerabilities, especially in the context of security-critical Cyber-Physical Systems (CPS) and Internet-of-Things (IoT) applications—these attacks are difficult to detect and are much harder to thwart from the higher abstraction levels of the system. My research analyzes such vulnerabilities of hardware implementations of cyber-infrastructure. To provide practical security solutions that can be deployed in real-world settings, the systems I develop focus on implementation (side-channel) security, hardware/software efficiency, and end-to-end system demonstration. I ultimately aim to design tools that can quantify a provable security level for a given threat model and enable automated trade-offs for developers between a desired level of security, performance, and cost.",,2020,,221673657,semantic_scholar
cd51bbfd51cde0c089d9dfb7d30bfc124d9b7c55,https://www.semanticscholar.org/paper/cd51bbfd51cde0c089d9dfb7d30bfc124d9b7c55,"Summary for CIFE Seed Proposals for Academic Year 2020-21 Proposal number: 2020-04 Proposal title: Hybrid Physical-Digital Spaces: Transforming the Design, Operation, and Experience of Built Environments to Promote Health and Wellbeing","up to 150 words) Increasing evidence suggests built office features (e.g., lighting, materials, and ventilation) have substantial impacts on occupant wellbeing. A key next direction is field studies at industry partner sites to examine real-world workplaces. We propose to develop innovative Internet of Things (IoT) techniques that integrate data from building instrumentation, personal device sensors, and self-report interfaces and then deploy this platform in-the-wild to capture rich, longitudinal, ecologically-valid data about the status of office workers and the spaces they occupy. Insights will advance scientific knowledge of how buildings impact wellbeing as well as produce practical implications for building designers and operators. A timely component will explore how covid-19 has temporally or fundamentally changed occupant behaviors and operational decisions (e.g., physical distancing desks and ventilation settings that reduce pathogen spread). Overall, our proposed research has the potential to transform the industry’s thinking on how built environments can be designed, operated, and experienced. Hybrid Physical-Digital Spaces: Transforming the Design, Operation, and Experience of Built Environments to Promote Health and Wellbeing Problem and Significance Considering that people in the U.S. spend 87% of their time in indoor spaces , we assert that 1 buildings are powerful yet underleveraged loci for promoting human wellbeing. Imagine an intelligent office that could adapt soundscape systems to manage noise in open floor plans, optimize space reservation or utilization to foster collaborations and save energy, or provide digital information displays that promote employee connectedness and physical activity. Towards actualizing our vision of such hybrid physical-digital spaces, our proposal strives to develop, apply, and evaluate novel scientific and engineering approaches that will transform the industry’s thinking around how built environments can be designed, operated, and experienced. Increasingly, hypotheses suggest that built features of indoor environments (e.g., lighting, materials, and ventilation) have substantial impacts on occupants (e.g., employee recruitment and retention, absenteeism, cognition, creativity, productivity, social interactions, physical activity and health, and psychological wellbeing). In turn, these individual outcomes also drive pivotal organizational outcomes such as product innovation, workforce diversity, employee turnover, market share, and profitability. Examples illustrate how building interventions can have huge impacts : enhancing employee exposure to daylight can save businesses ~$2,000/yr per capita 2 , better air quality can raise cognitive scores of workers by 101% 3 , and increasing indoor access to biophilic elements could recoup $23 billion considering 10% of workplace absenteeism (a $226 billion dollar problem) is attributable to architecture that inadequately connects to nature 4 . However, few of these hypotheses have been tested at scale, over time, and in real world conditions . Instead, most prior efforts are small sample, short-term correlational studies based on potentially biased and sparse self-reported data. A more rigorous, scientific, and human-centered approach to study and engineer buildings that promote wellbeing can have major implications at individual, organizational, and societal levels (see Figure 1), offering both foundational theoretical knowledge as well as practical strategies for building designers and operators. Figure 1. Relations among building features and human outcomes at various levels. Further, “smart buildings” today typically focus on basic sensing and control for energy savings, thermal comfort, and security. Connecting to CIFE’s Vision for the Future of Building Users, we argue buildings of the future can go beyond such bottom line outcomes to be more interactive and human-centered: aware of and responsive to occupants’ cognitive, mental, and physical feelings and needs, while respecting privacy and promoting positive indoor experiences . 1 Klepeis, et al., 2001; 2 Heschong & Mahone, 2003; 3 Allen et al., 2016; 4 Elzeyadi, 2011. <Landay-Billington> < Hybrid Physical-Digital Spaces> 1 Theoretical and Practical Points of Departure It is imperative to increase understanding of exactly what built attributes have what impacts and on whom, in a scalable, longitudinal, and inclusive manner. Thus through technology-driven assessment and hybrid physical-digital interventions, we aim to (a) fundamentally advance the science on how built environments impact human wellbeing and, in turn, (b) generate guidelines that can revolutionize the way spaces are designed, operated, and experienced . Our current scope focuses on office spaces and workers; though an overarching goal is for our developed approaches and insights to establish a foundation that enables future research with additional populations and environments (e.g., physicians and patients in clinical settings, students and teachers in classrooms, and traditionally marginalized shift and temporary workers). In particular, our reusable platform will help others study this wider range of buildings and occupants; and combining these approaches with emerging endeavors such as biophilic design and precision interventions provides a novel opportunity to not only more deeply investigate but also address long-running public health challenges and systemic inequities facing society. In these ways, we hope to positively impact a broad cross-section of stakeholders at individual, organizational, and institutional levels. Moreover, this project will support interdisciplinary fertilization across engineering, computing, psychology, law, and medicine . Research Methods and Work Plan Our research agenda is to support the design and operation of built facilities that augment human capabilities and wellbeing — and have a fundamental positive change on the way indoor spaces are experienced by the people that occupy them. By introducing intelligent systems capable of gathering and interpreting building and occupant data as well as delivering adaptive interventions in response, novel roles will also emerge for managing buildings and the activities that take place inside them. To achieve these goals, our research will comprise three main activities: 1. Developing an extensible and secure data collection and machine learning platform . A key aim of this research is scientifically examining how built spaces impact human wellbeing. To pursue this investigation and develop methods that enable buildings to be more aware of occupants’ states and needs, we have been developing pattern detection software that integrates data from (a) personal devices (smartphones, smartwatches, fitness trackers), (b) building instrumentation or portable environmental sensors (light levels, air quality), and (c) experience sampling interfaces that prompt occupants for subjective information through quick, validated self-report techniques. Figure 2 illustrates examples of these assessment components. This work involves addressing a number of technical challenges, such as selecting sampling rates and window sizes to maximize efficiency, developing methods for analyzing asynchronous and sparse sensor data, and developing privacy-sensitive feature engineering strategies for detecting and predicting wellbeing outcomes of interest. We also plan to package our platform as a reusable toolkit that can be applied by other researchers and building operators. This work is ongoing and a basic version will be ready by summer. Once development is complete, CIFE support would allow us to move onto the next critical phase: moving out of the lab and into the field. <Landay-Billington> < Hybrid Physical-Digital Spaces> 2 Figure 2. Platform to integrate data from personal devices, building sensors, and subjective self-report. 2. Deploying the platform through a mixed-method study with industry partners . The next step in our research is to deploy this platform at field sites in partnership with View, Inc. (specifically, at TIAA offices in Manhattan, this summer/fall) to capture rich, longitudinal, ecologically-valid data about behavioral, psychological, and physiological states of occupants and their everyday work environments. Our plan is to recruit a sample of approximately 150 employees for a period of 18 weeks, which will involve a baseline phase followed by systematic variation of built features (Views/No Views, Plants/No Plants, and Diversity/No Diversity in artwork) and measurement of indicators hypothesized to promote both personal wellbeing and organizational performance, based on the literature and our formative online and lab studies, described below. In combination with the engineering-focused activities to implement and install the platform, deployment will occur in tandem with ethnographic work (e.g., observations, interviews, and surveys) to manually validate reliability of the system’s automated inferences as well as gain a more qualitative portrait of occupant experiences in various spaces. Privacy-centric engagements will additionally investigate stakeholders’ attitudes regarding the capture of various types of information to derive implications about informed consent and personal data management. Along similar lines, it will be critical to responsibly manage captured data, especially potentially sensitive and exploitable data about wellness or performance. Therefore all studies will be conducted with oversight and approval from the Stanford Institutional Review Board (IRB). In addition to obtaining participants’ informed consent, we will also design sensor and data collection mechanisms to use an opt-in model, including partial participation. Our data management systems can also allow individuals to view and delete their personal data, including if purging is desired in the event of study withdrawal. Our research team has exp",,2020,,235651121,semantic_scholar
b8f7fa8d93c5ee4c3df988ba7c7499b1db51706e,https://www.semanticscholar.org/paper/b8f7fa8d93c5ee4c3df988ba7c7499b1db51706e,Towards Autonomous Smart Sensing Systems,"Since the 1990's, researchers in both academia and industry have been exploring ways to exploit the potential for Wireless Sensor Networks (WSNs) to revolutionize our understanding of - and interaction with - the world around us. WSNs have therefore been a major focus of research over the past 20 years. While WSNs offer a persuasive solution for accurate real-time sensing of the physical world, they are yet to be as ubiquitous as originally predicted when the technology was first envisaged. Technical difficulties exist which have inhibited the anticipated uptake in WSN technologies. The most challenging of these have been identified as system reliability, battery lifetime, maintenance requirements, node size and ease of use. Over the past decade, the Wireless Sensor Networks (WSN) group at the Tyndall National Institute, has been at the forefront of driving the vision of ubiquitously deployed, extended lifetime, low power consumption embedded systems providing information rich data streams wirelessly in (close to) real-time. In this time, the WSN group has developed multiple novel, first of kind, wireless multi-sensor systems and deployed these in the world around us, overcoming the technical challenges associated with ensuring robust and reliable long-term data sets from our environment. This work is focused on investigating and addressing these challenges through the development of the new technologies and system integration methodologies required to facilitate and implement WSNs and validate these in real deployments. Specifically, discussed are the development and deployment of novel WSN systems in the built environment, environmental monitoring and fitness and health monitoring systems.The key research challenges identified and discussed are:a)The development of resource-constrained, extremely low power consumption systems incorporating energy-efficient hardware and software algorithms.b)The development of highly reliable extremely long duration deployments which through the use of appropriate energy harvesting solutions facilitate (near) zero maintenance sensor networks.c)The development of low power consumption miniaturized wearable microsysteThe development of technologies to address these challenges in terms of cost, size, power consumption and reliability which need to be tested and validated in real world deployments of wireless sensing systems is discussed. It is clear that when looking at the scale up of deployments of novel WSNs, that to be successful, such systems need to ""be invisible, last forever, cost nothing and work out of the box"". This paper describes these relevant technologies and associated project demonstrators",2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),2020,10.1109/I2MTC43012.2020.9128887,220311463,semantic_scholar
3ee6c2e1aebf16c7b62774700b99a4320ebfefea,https://www.semanticscholar.org/paper/3ee6c2e1aebf16c7b62774700b99a4320ebfefea,MC-SDN: Supporting Mixed-Criticality Real-Time Communication Using Software-Defined Networking,"Despite recent advances, there still remain many problems to design reliable cyber-physical systems. One of the typical problems is to achieve a seemingly conflicting goal, which is to support timely delivery of real-time flows while improving resource efficiency. Recently, the concept of mixed-criticality (MC) has been widely accepted as useful in addressing the goal for real-time resource management. However, it has not been yet studied well for real-time communication. In this paper, we present the first approach to support MC flow scheduling on switched Ethernet networks leveraging an emerging network architecture, software-defined networking (SDN). Though SDN provides flexible and programmatic ways to control packet forwarding and scheduling, it yet raises several challenges to enable real-time MC flow scheduling on SDN, including: 1) how to handle (i.e., drop or re-prioritize) out-of-mode packets in the middle of the network when the criticality mode changes and 2) how the mode change affects end-to-end transmission delays. Addressing such challenges, we develop MC-SDN that supports real-time MC flow scheduling by extending SDN-enabled switches and OpenFlow protocols. It manages and schedules MC packets in different ways depending on the system criticality mode. To this end, we carefully design the mode change protocol that provides analytic mode change delay bound, and then resolve implementation issues for system architecture. For evaluation, we implement a prototype of MC-SDN on top of Open vSwitch, and integrate it into a real world network testbed as well as a 1/10 autonomous vehicle. Our extensive evaluations with the network testbed and vehicle deployment show that MC-SDN supports MC flow scheduling with minimal delays on forwarding rule updates and it brings a significant improvement in safety in a real-world application scenario.",IEEE Internet of Things Journal,2019,10.1109/JIOT.2019.2915921,164760290,semantic_scholar
9aeb35460b423cbf274e0ff1f052a602d9efbb91,https://www.semanticscholar.org/paper/9aeb35460b423cbf274e0ff1f052a602d9efbb91,A Graphical User Interface for the Real World,"A number of typical Ubiquitous Computing (Ubicomp) interface problems are presented. From these, the requirements needed for a Ubicomp meta-user interface are extracted and a user interface architecture that is based on traditional GUI architectures is introduced. MOTIVATION A recent survey of more than 100 Ubiquitous Computing Applications (Rehman 2001) revealed that a lot of these applications violate basic HCI guidelines such as Norman’s design principles (Norman 1990). More specifically, developers often leave the user with too little control, don’t provide appropriate feedback about what the system is doing and fail to show appropriate affordances and constraints to the user. A more detailed analysis is given in (Rehman et al. 2002). For our purposes I shall try to highlight the fundamental problem here. My hypothesis is that these problems are not singular design flaws, but have their root in the very idea of Ubiquitous/Disappearing/Invisible Computing. First, there is a need to define “invisibility”. Some system developers have taken it to mean that the user does not see any system-related part physically. As pleasing it may seem to some developers, this is not what Weiser or Norman meant when they conceived the idea, as is evident from the examples they usually cite. Rather, they meant that the computer stays out of the user’s mind, not necessarily their sight. That said, let us look at whether even this is achievable. For this we shall look at the archetype of a good tool: the pencil. You have control over it, the user knows how to use it, and it provides constant feedback. More importantly, it is invisible in Weiser’s sense (Weiser 1994). I believe that the reason why we have not achieved this quality in our applications is, that there is an asymmetry in our capability to capture input in comparison to providing output. With the advent of sophisticated sensors any real world object, even the human body (as seen in location-aware applications) can be a highly accurate input device. Whereas the above-mentioned pencil can also deliver feedback in the same way as its input (mechanically), we cannot, say, mechanically stop a museum visitor with a contextaware electronic guide from going out of the coverage of the location system. One solution is not to worry about the output at all. Clearly, this leads to ill-designed applications. The other solution is to display this information on a display. This adds another level of indirection a conventional tool does not suffer from and, depending on where the display is placed, may imply an increase in cognitive load. Before we talk about our approach to this problem, we shall look at another problem we became aware of when analysing the applications. Norman (1993) talks about the idea that an interface in its function as a “surface representation” should convey an image of the underlying system. The problem we have in Ubicomp the system is a collection of invisibly connected heterogenous nodes that do not seem to have a “surface” a user can relate to, at all. Our aim is to not only give the user such a smooth “surface”, but also to massively increase the bandwith of the reverse channel, between system and user. In order to do this with as little cognitive load as possible, we advocate the use of Augmented Reality (AR). SYSTEM IDEA In its widest sense any system that connects the real and virtual world can be labelled ""Augmented Reality"" (AR). As such, even tangible interfaces are examples of AR. The narrower definition involves a system that uses a head-mounted display (HMD) and a tracker (Feiner et al. 1993). The tracker continuously measures the position and orientation of the head to some real object and displays a 3D graphics on a see-through HMD that makes the virtual object appear to be placed at a fixed location in the physical world. We are using a head-mounted camera and have deployed markers (Kato & Billinghurst 1999 ) in order to track objects of interest. The marker is a reference point that can be used to overlay graphics on a real world object. A file cabinet can, e.g., have its contents overlaid on it, as long as the marker has a fixed relationship to the file cabinet. The system will infer the position of the file cabinet relative to the user’ s eyes and display a corresponding virtual object on his HMD. In order to design our smooth surface, we want to leverage some of the design principles used in the GUI domain. Before GUIs arrived, computer users were typically confronted with a multitude of applications each with their own user interface. Transferring data between them was difficult and users had to interrupt their tasks in order to adhere to application boundaries. Working across applications was impossible. In a way the situation was similar to the one we now have in Ubicomp. The arrival of a meta-interface was decisive in giving the user the “ unified experience” we wish to provide in our present domain. Figure 1: A meta-interface for ubiquitous computing A VDU (VISUAL DISPLAY UNIT) The first step in building our Graphical User Interface that can send information and receive information from any everyday object, service, appliance or application, was to implement a display that covers the entire space. Of course, this is meant in a virtual sense, using Augmented Reality. This involved constructing a spatial model that abstracts from tracking sensors and sources. We are working with a spatial model that consists of a network of points. The arcs are Cartesian coordinate transformations, some dynamically changing, some fixed. We are using this type of model in order to cope with 6-DOF(degrees-of-freedom) tracking information. In order to give an object output capability, say a loudspeaker, we can attach an electromagnetic sensor or a marker to it and add it as a point of interest to our model, that will keep track of its position and orientation at all times. Figure 2: Every object has an output USER INTERFACE ARCHITECTURE We shall now describe the entire user interface architecture we are in the process of implementing. Figure 3 shows the current design plan. We are assuming that the user has a mobile unit with a head-mounted display and that his head position and orientation are trackable. The environment contains a number of “ active” appliances: devices such as a printer, services such as a web search, Applications such as Microsoft Word and Everyday Objects such as a mug that knows its temperature. All of these devices have interfaces to a tuple space in order to receive commands and send messages to the user. We thought that a tuple space is best suited for a symmetric 2-way communication. The two paths of information flow are shown by the arrows: a forward path from left to right and a feedback path from right to left. The world model and data model can be seen as the Visual Display Unit described above. The world model contains the position(and orientation) of each active object. The data model is a description of interactive information, each piece of which I N T E R F A C E Network Services Applications Devices",,2002,,14235041,semantic_scholar
9dc8a27f10c70c77604470882f5f3336dedd468f,https://www.semanticscholar.org/paper/9dc8a27f10c70c77604470882f5f3336dedd468f,Continuous Experimentation for Automotive Software on the Example of a Heavy Commercial Vehicle in Daily Operation,"As the automotive industry focuses its attention more and more towards the software functionality of vehicles, techniques to deliver new software value at a fast pace are needed. Continuous Experimentation, a practice coming from the web-based systems world, is one of such techniques. It enables researchers and developers to use real-world data to verify their hypothesis and steer the software evolution based on performances and user preferences, reducing the reliance on simulations and guesswork. Several challenges prevent the verbatim adoption of this practice on automotive cyber-physical systems, e.g., safety concerns and limitations from computational resources; nonetheless, the automotive field is starting to take interest in this technique. This work aims at demonstrating and evaluating a prototypical Continuous Experimentation infrastructure, implemented on a distributed computational system housed in a commercial truck tractor that is used in daily operations by a logistic company on public roads. The system comprises computing units and sensors, and software deployment and data retrieval are only possible remotely via a mobile data connection due to the commercial interests of the logistics company. This study shows that the proposed experimentation process resulted in the development team being able to base software development choices on the real-world data collected during the experimental procedure. Additionally, a set of previously identified design criteria to enable Continuous Experimentation on automotive systems was discussed and their validity confirmed in the light of the presented work.",ECSA,2020,10.1007/978-3-030-58923-3_5,212633943,semantic_scholar
a86aed4879a9f76ade522bf95e9bc1237e67452e,https://www.semanticscholar.org/paper/a86aed4879a9f76ade522bf95e9bc1237e67452e,RFID sensing networks for critical infrastructure security: A real testbed in an energy smart grid,"The UHF Radiofrequency Identification technology offers nowadays a viable technological solution for the implementation of low-level environmental monitoring of connected critical infrastructures to be protected from both physical threats and cyber attacks. An RFID sensor network was developed within the H2020 SCISSOR project, by addressing the design of both hardware components, that is a new family of multi-purpose wireless boards, and of control software handling the network topology. The hierarchical system is able to the detect complex, potentially dangerous, events such as the un-authorized access to a restricted area, anomalies of the electrical equipments, or the unusual variation of environmental parameters. The first real-world test-bed has been deployed inside an operational smart-grid on the Favignana Island. Currently, the network is fully working and remotely accessible.",2017 IEEE International Conference on RFID Technology & Application (RFID-TA),2017,10.1109/RFID-TA.2017.8098901,39994881,semantic_scholar
8a24cd6ed1875c73761c9fa0ac76105a03e6e932,https://www.semanticscholar.org/paper/8a24cd6ed1875c73761c9fa0ac76105a03e6e932,A Hypervisor for Shared-Memory FPGA Platforms,"Cloud providers widely deploy FPGAs as application-specific accelerators for customer use. These providers seek to multiplex their FPGAs among customers via virtualization, thereby reducing running costs. Unfortunately, most virtualization support is confined to FPGAs that expose a restrictive, host-centric programming model in which accelerators cannot issue direct memory accesses (DMAs). The host-centric model incurs high runtime overhead for workloads that exhibit pointer chasing. Thus, FPGAs are beginning to support a shared-memory programming model in which accelerators can issue DMAs. However, virtualization support for shared-memory FPGAs is limited. This paper presents Optimus, the first hypervisor that supports scalable shared-memory FPGA virtualization. Optimus offers both spatial multiplexing and temporal multiplexing to provide efficient and flexible sharing of each accelerator on an FPGA. To share the FPGA-CPU interconnect at a high clock frequency, Optimus implements a multiplexer tree. To isolate each guest's address space, Optimus introduces the technique of page table slicing as a hardware-software co-design. To support preemptive temporal multiplexing, Optimus provides an accelerator preemption interface. We show that Optimus supports eight physical accelerators on a single FPGA and improves the aggregate throughput of twelve real-world benchmarks by 1.98x-7x.",ASPLOS,2020,10.1145/3373376.3378482,211105692,semantic_scholar
ea213eb5101ceaf749ca538a4135b879a464c00d,https://www.semanticscholar.org/paper/ea213eb5101ceaf749ca538a4135b879a464c00d,Configurable Framework for managing data produced by multiple PLCs,"The work was carried on in collaboration with the CO.S.E. Centre of Thales Alenia Space in Italy at the Turin site. Thales Alenia Space in Italy has over 40 years of experience in building systems and equipment for space exploration, telecommunications, navigation, Earth observation and science. In the world of software development, which engages this work, telemetry can offer automatic data collection from the real world. The telemetry, therefore, allows to collect important data that becomes valuable and usable when analyzed. The main goal of this work is to develop a Configurable Framework, which manage the communication among different devices and data produced by multiple PLCs. The PLCs are connected to different equipment, contained in a module, which manage the resources needed to survive: the use and recycling of water, the storage and distribution of electrical power, thermal control and air recycling, data management and processing, remote communication and control, medical treatment and many more. The module can be deployed in all kinds of environments (desert camps, oil and gas exploration camps, military outposts and polar bases) and is the ideal solution for survival in remote or hostile areas. The realized architecture, therefore, allows to monitor the status of the module once it has been deployed in a remote location. After the first phase, the requirements were collected in a document to keep track of the data needed to create the appropriate code, which was subsequently used to simulate the various components. Afterwards, it was defined what are the functionalities that a generic Configurable Framework shall implement, then all possible architectures that can satisfy the company needs were explored and finally a working prototype was developed. Furthermore, it is also an opportunity to work with software technologies outside the domain of existing business skills and discover which advantages they can bring to the company. To provide flexibility, the GUI was built as a Web Application, taking advantage from the Bootstrap library. In fact the architecture satisfy the requested modularity and it is compatible with different operating systems and several screen resolutions. The interfaces are suitable for many electronic devices, allowing the use of keyboard, mouse and touch-screens. To speed up the development it was used a Web Framework; different solutions were analyzed, but in the end it was chosen the Django Framework, since it works without having to install additional dependencies. The application was developed using SQLite as database solution, it runs from a folder and can be moved between different machines without worrying about databases and Web Servers configurations. The GUI consist of two Web Pages, the first one allows the users to perform the different requests. The second one shows which setting are available and it allows to interact with the defined models by creating, deleting or updating items. The sampled data is collected into a non-relational Database, using the JSON format, to keep track of it, and it is also displayed in real time in order to visualize updates. After simulating the behavior of the used boards, the next step was to test and validate code using physical devices. The primary purpose of testing is to detect software failures so that defects may be discovered and corrected. At the end of the testing phase a user manual was also produced.",,2020,,226091269,semantic_scholar
c6833112f857c54b83d144d5c48cee6e4868dc22,https://www.semanticscholar.org/paper/c6833112f857c54b83d144d5c48cee6e4868dc22,Mobile Technology for Clinical Operations: Challenges and Opportunities,"Abstract 
On the ground best practices for the use of mobile technology within e-clinical with a focus on practical applications available now for clinical operations and data management. Hear real life implementations and learnings from the field. As mobile becomes more and more common, dialog must focus on actual experiences during the conduct of trials so that clinical teams can make informed decisions on the deployment, security, and usage of mobile applications at various stages of clinical development. 
 
In this discussion, the audience will hear real stories from clinical professionals and their on the ground experiences with mobile applications. The audience will walk away with a deeper understanding of the benefits and limitations of mobile applications in clinical operations and better enable them to adopt mobile applications with practical expectations. Attendees will be guided on the practical use of these technologies. They will learn to determine the use cases that are most relevant and applicable in Clinical Operations. 
 
 
 
Biography: 
 
Jay Smith is Head of Product for TransPerfect’s Trial Interactive E-Clinical platform. Jay brings 25 years of product management, consulting, and engineering experience across verticals such as life sciences, enterprise software, healthcare, government, entertainment and manufacturing. Previously, Jay has led product efforts for Medidata, Sparta Systems, Cureatr, VenueNext, Apogy, and Liquent. Jay holds an MBA from Villanova University and a degree in Computer Science and Physics from Gettysburg College. 
 
  
 
  
 
  
 
  
 
  
 
  
 
Speaker Publications: 
 
 
  “Analysis of Multiple Samples Using Multiplex Sample NMR: Selective Excitation and Chemical Shift Imaging Approaches”; Analytical Chemistry / 2001 / 73(11):2541-6 
 “Variable Temperature Study of the Cross-Relaxation Dynamics in the Hyperpolarized Xenon-Induced Enhancement of Surface Nuclei”; The Journal of Physical Chemistry B / 2001 / 105(7):1412-1421 
 “Cross-relaxation dynamics between laser-polarized xenon and surface species using a simple three-spin model”; Chemical Physics Letters / 2000 / 317(1-2):165-173 
 
 
  
 
10th International Conference on Clinical Research and Clinical Trials; Amsterdam, Netherlands- March 18-19, 2020. 
 
Abstract Citation: 
 
Practical ClinOps Applications of AI and Machine Learning – Real-World Use Cases, Euro Clinical Trials 2020, 10th International Conference on Clinical Research and Clinical Trials; Amsterdam, Netherlands- March 18-19, 2020 (https://clinicaltrials.pharmaceuticalconferences.com/abstract/2020/mobile-technology-for-clinical-operations-challenges-and-opportunities)",,2020,,225321802,semantic_scholar
57165b0eb61847bec87cdd6df7a4eb37bd92fc59,https://www.semanticscholar.org/paper/57165b0eb61847bec87cdd6df7a4eb37bd92fc59,Commercially Available Head-Mounted Displays Are Unsuitable for Augmented Reality Surgical Guidance: A Call for Focused Research for Surgical Applications,"Recent advances in portable computational units, optics, and photonics devices have enabled the scientific community to open many new fronts in biomedical research, with the development of innovative augmented reality (AR) applications exploiting the potentialities offered by head-mounted display (HMD) technology. Such technology has reached the maturity to be translated into commercial products, and published works on HMDs provide glimpses of how AR will disrupt the surgical field, allowing for an ergonomic, intuitive, and 3-dimensional fruition of preoperative and intraoperative information. Nowadays several commercial HMDs, such as Microsoft HoloLens, Meta or Magic Leap, integrate tracking and registration technology, and the deployment of software development kits has reduced technical complexity of custom application development, allowing for a wide range of users to easily create AR applications and attracting researchers to explore their potentialities for the implementation of surgical navigators. The above-mentioned HMDs are designed following an optical see-through (OST) approach, which augments the natural view through the projection of virtual reality information on semitransparent displays in front of the user’s eyes. The OST approach fits well in the surgical domain as it offers an instantaneous full-resolution view of the real world, allowing the natural synchronization of visual and proprioceptive information, and a complete situation awareness. Ongoing research is aimed at the goal of providing a device “conceived as a transparent interface between the user and the environment, a personal and mobile window that fully integrates real and virtual information.”1 Commercial companies are rapidly improving HMD ergonomic aspects, for example, HoloLens 2 features an improved field of view (52° diagonal), which includes eye tracking, and offers more comfortable wearability. However, maximizing surgical accuracy remains a challenge for manufacturers and researchers. Together with ergonomics, the achievement of precision objectives must be addressed to develop a visor suitable for guiding surgical operations, not to mention compliance with medical device regulations. An increasing number of research studies propose the use of commercial HMDs to guide surgical interventions.2 To the best of our knowledge, these works are principally focused on the need to strengthen virtual/real patient registration (eg, use of an external localization system),3 improve virtual content stability,4 and solve calibration issues, and they underestimate the contribution of perceptual issues to the user accuracy. One of the largest obstacles to obtain a perceptually correct augmentation is the inability to render proper focus cues in HMDs; indeed, the majority of systems offers the AR content at a fixed focal distance, failing to stimulate natural eye accommodation and retinal blur effects.5 Our recent work2 suggests to avoid the use of existing HMD-OST, which are not specifically designed for performing tasks in peripersonal space (<1 m), to guide manual tasks requiring a high level of precision, since perceptual issues, particularly “focal rivalry” (ie, inability to see simultaneously in focus the virtual and real content), can affect user performance.5 Most commercial systems (HoloLens, Lumus, Meta, Ora2) indeed have a fixed focal plane at 2 m or more (often infinite). Thus, during manual tasks, virtual content is 903197 SRIXXX10.1177/1553350620903197Surgical InnovationCarbone et al editorial2020",Surgical innovation,2020,10.1177/1553350620903197,211072894,semantic_scholar
ae5db02b396985311457af37e8e9a3a020f7e18f,https://www.semanticscholar.org/paper/ae5db02b396985311457af37e8e9a3a020f7e18f,A Simulation Study for Long-Range Underwater Acoustic Networks in the High North,"In stark contrast to a typical underwater acoustic network (UAN) deployed in mid-latitudes, ice-covered environments make network deployment difficult and expensive. A limited number of nodes must cover ranges of hundreds of kilometers. We tackle the network design in three layers: engineering, physical, and networking. At the engineering layer, we investigate hardware and bandwidth limitations for real-world implementation. Based on the proposed bandwidth, we design a software modem equipped with three waveforms achieving 1.8, 21.4, and 96.2 b/s. The packet error rate performance is computed with a channel simulator that takes realistic environmental parameters. Our simulations show that ranges of more than 100 km can be achieved in two High North areas during summer months provided that the point-to-point links exploit the ducted sound propagation. However, during winter months, this performance may not be always possible and multiple hops may be needed to cover the same range. Finally, based on the outcomes of the physical layer, an adaptive cross-layer routing protocol, termed network-aware adaptive routing (NADIR), is simulated. Link quality, energy consumption, and topological data are used to select the best coded modulation scheme and relay node in the next transmission slot. Our results show that the use of an adaptive strategy offers higher packet delivery and lower energy consumption than a nonadaptive strategy.",IEEE Journal of Oceanic Engineering,2019,10.1109/JOE.2019.2931853,202094973,semantic_scholar
fd94932da9cc80e9cd9f6204d2cae2624d582e91,https://www.semanticscholar.org/paper/fd94932da9cc80e9cd9f6204d2cae2624d582e91,HyperLink: Virtual Machine Introspection and Memory Forensic Analysis without Kernel Source Code,"Virtual Machine Introspection (VMI) is an approach to inspecting and analyzing the software running inside a virtual machine from the hypervisor. Similarly, memory forensics analyzes the memory snapshots or dumps to understand the runtime state of a physical or virtual machine. The existing VMI and memory forensic tools rely on up-to-date kernel information of the target operating system (OS) to work properly, which often requires the availability of the kernel source code. This requirement prevents these tools from being widely deployed in real cloud environments. In this paper, we present a VMI tool called HyperLink that partially retrieves running process information from a guest virtual machine without its source code. While current introspection and memory forensic solutions support only one or a limited number of kernel versions of the target OS, HyperLink is a one-for-many introspection and forensic tool, i.e., it supports most, if not all, popular OSes regardless of their versions. We implement both online and offline versions of HyperLink. We validate the efficacy of HyperLink under different versions of Linux, Windows, FreeBSD, and Mac OS X. For all the OSes we tested, HyperLink can successfully retrieve the process information in one minute or several seconds. Through online and offline analyses, we demonstrate that HyperLink can help users detect real-world kernel rootkits and play an important role in intrusion detection. Due to its version-agnostic property, HyperLink could become the first introspection and forensic tool that works well in autonomic cloud computing environments.",2016 IEEE International Conference on Autonomic Computing (ICAC),2016,10.1109/ICAC.2016.46,7292567,semantic_scholar
70458452066e724e5ff29c0c74046e816765beb5,https://www.semanticscholar.org/paper/70458452066e724e5ff29c0c74046e816765beb5,OpenStack Neat: a framework for dynamic and energy‐efficient consolidation of virtual machines in OpenStack clouds,"Dynamic consolidation of virtual machines (VMs) is an efficient approach for improving the utilization of physical resources and reducing energy consumption in cloud data centers. Despite the large volume of research published on this topic, there are very few open‐source software systems implementing dynamic VM consolidation. In this paper, we propose an architecture and open‐source implementation of OpenStack Neat, a framework for dynamic VM consolidation in OpenStack clouds. OpenStack Neat can be configured to use custom VM consolidation algorithms and transparently integrates with existing OpenStack deployments without the necessity of modifying their configuration. In addition, to foster and encourage further research efforts in the area of dynamic VM consolidation, we propose a benchmark suite for evaluating and comparing dynamic VM consolidation algorithms. The proposed benchmark suite comprises OpenStack Neat as the base software framework, a set of real‐world workload traces, performance metrics and evaluation methodology. As an application of the proposed benchmark suite, we conduct an experimental evaluation of OpenStack Neat and several dynamic VM consolidation algorithms on a five‐node testbed, which shows significant benefits of dynamic VM consolidation resulting in up to 33% energy savings. Copyright © 2014 John Wiley & Sons, Ltd.",Concurr. Comput. Pract. Exp.,2015,10.1002/cpe.3314,8409282,semantic_scholar
67472d5b368783136837d7065162d940535e732e,https://www.semanticscholar.org/paper/67472d5b368783136837d7065162d940535e732e,Supervisory control theory applied to swarm robotics,"Currently, the control software of swarm robotics systems is created by ad hoc development. This makes it hard to deploy these systems in real-world scenarios. In particular, it is difficult to maintain, analyse, or verify the systems. Formal methods can contribute to overcome these problems. However, they usually do not guarantee that the implementation matches the specification, because the system’s control code is typically generated manually. Also, there is cultural resistance to apply formal methods; they may be perceived as an additional step that does not add value to the final product. To address these problems, we propose supervisory control theory for the domain of swarm robotics. The advantages of supervisory control theory, and its associated tools, are a reduction in the amount of ad hoc development, the automatic generation of control code from modelled specifications, proofs of properties over generated control code, and the reusability of formally designed controllers between different robotic platforms. These advantages are demonstrated in four case studies using the e-puck and Kilobot robot platforms. Experiments with up to 600 physical robots are reported, which show that supervisory control theory can be used to formally develop state-of-the-art solutions to a range of problems in swarm robotics.",Swarm Intelligence,2016,10.1007/s11721-016-0119-0,18760258,semantic_scholar
576b4575a2180e5c4a0479bf96e63cf53e672483,https://www.semanticscholar.org/paper/576b4575a2180e5c4a0479bf96e63cf53e672483,Understanding and Statically Detecting Synchronization Performance Bugs in Distributed Cloud Systems,"In such an information society, the Internet of Things (IoT) plays an increasingly important role in our daily lives. With such a huge number of deployed IoT devices, Cyber-Physical System (CPS) calls for powerful distributed infrastructures to supply big data computing, intelligence, and storage services. With the increasingly complex distributed software infrastructures, new intricate bugs continue to manifest, causing huge economic loss. Synchronization performance problems, which means that improper synchronizations may degrade the performance and even lead to service exception, heavily influence the entire distributed cluster, imperiling the reliability of the system. As one kind of performance problems, the synchronization performance problems are acknowledged as difficult to diagnosis and fix. We collect 26 performance issues in three real-world distributed systems: HDFS, Hadoop MapReduce, and HBase, and do analysis on their root cause, fix strategy, and algorithm complexity in order to understand these synchronization performance bugs better. Then, we implement a static detection tool including critical section identifier, loop identifier, inner loop identifier, expensive loop identifier, and pruning component. After that, we evaluate our detection tool on these three distributed systems with sampled bugs. In the evaluation, our detection tool accurately finds out all the target bugs. Besides, it points out more new potential performance problems than the previous works. With the strict performance overhead, our detection tool is proved to be greatly efficient.",IEEE Access,2019,10.1109/ACCESS.2019.2923956,196171384,semantic_scholar
9a762006e2e410c29d8b2d17145fcd9d7f3b44b6,https://www.semanticscholar.org/paper/9a762006e2e410c29d8b2d17145fcd9d7f3b44b6,Structural Health Monitoring and Prognostic of Industrial Plants and Civil Structures: A Sensor to Cloud Architecture,"The deployment of Structural Health Monitoring (SHM) systems is a natively interdisciplinary task that involves joint research contributions from sensing technologies, data science and civil engineering. The capability to assess, also from remote stations, the working conditions of industrial plants or the structural integrity of civil buildings is widely requested in many application fields. The technological development aims to continuously provide innovative tools and approaches to satisfy these demands. As a first instance, reliable monitoring strategies are needed to detect structural damages while filtering out environmental noise. Ongoing solutions to tackle these topics are based on the exploitation of highly customized sensing technologies, such as shaped transducers for Acoustic Emission (AE) testing or Micro-Electro-Mechanical System (MEMS) accelerometers for Operational Modal Analysis (OMA) [1]. On the other hand, effective data acquisition and storage techniques must be employed to cope with the heterogeneity of the sensing devices and with the amount of data produced by collecting raw measured signals. Finally, damage detection and prediction tasks should be computed via data-driven algorithms that can complement the model-based alternatives traditionally used in civil engineering. Layered SHM architectures [2] represent straightforward approaches to address the system complexity originated by this interdisciplinary design; however, few real-world implementations have been presented so far in the literature. In this paper, we overcome these limitations by presenting an Internet of Things (IoT)-based SHM architecture for the predictive maintenance of industrial sites and civil engineering structures and infrastructures. The proposed cyber-physical system includes a monitoring layer, that consists of accelerometer-based sensor networks, a data acquisition layer, built on the recent W3C Web of Things standard [3], and a data storage and analytics layer, which leverages distributed database and Machine Learning tools. We extensively discuss the hardware/software components of the proposed SHM architecture, by stressing its advantages in terms of device versatility, data scalability and interoperability support. Finally, the effectiveness of the system is validated on a real-world use-case, i.e., the monitoring of a metallic frame structure located at the SHM research labs of the University of Bologna, Italy, within the MAC4PRO project [4].",IEEE Instrumentation & Measurement Magazine,2020,10.1109/MIM.2020.9289069,228092404,semantic_scholar
577a8528c2dd27d9c36d9cb1e63c6667c9c3370d,https://www.semanticscholar.org/paper/577a8528c2dd27d9c36d9cb1e63c6667c9c3370d,Challenges and Opportunities in the Future Applications of IoT Technology,"The advent of internet of things (IoT) has influenced and revolutionized the information systems and computing technologies. A computing concept where physical objects used in daily life, will identify themselves by getting connected to the internet is called IoT. Physical objects embedded with electronic, radio-frequency identification, software, sensors, actuators and smart objects converge with the internet to accumulate and share data in IoT. IoT is expected to bring in extreme changes and solutions to most of the daily problems in the real world. Thus, IoT provides connectivity for everyone and everything at any time. The IoT embeds some intelligence in Internet connected objects to communicate, exchange information, take decisions, invoke actions and provide amazing services. It has an imperative economic and societal impact for the future construction of information, network, and communication technology. In the upcoming years, the IoT is expected to bridge various technologies to enable new applications by connecting physical objects together to support the intelligent decision making. As the most cost-effective and performant source of positioning and timing information in outdoor environments, the global navigation satellite systems(GNSS) has become an essential element of major contemporary technology developments notably including the IoT, Big Data, Smart Cities and Multimodal Logistics. By 2020, there will be more than 20 billion interconnected IoT devices, and its market size may reach $1.5 trillion. Projections for the impact of IoT on the Internet and economy are impressive, with some anticipating as many as 100 billion connected IoT devices and a global economic impact of more than $11 trillion by 2025. Regulators can play a role in encouraging the development and adoption of the IoT, by preventing abuse of market dominance, protecting users and protecting Internet networks while promoting efficient markets and the public interest. Regulators can consider and identify some measures to foster development of the IoT. Encourage development of LTE‐A and 5G wireless networks, and keep need for IoT‐specific spectrum under review. Universal IPv6 adoption by governments in their own services and procurements, and other incentives for private sector adoption. Increasing interoperability through competition law and give users a right to easy access to personal data. Support global standardization and deployment of remotely provisioned SIMs for greater machine to machine competition. Particular attention will be needed from regulators to IoT privacy and security issues, which are key to encouraging public trust in and adoption of the technology. This paper focuses specifically on the essential technologies that enable the implementation of IoT and the general layered architecture of IoT, the market of IoT and GNSS technologies and their impact of the world economy, application domain of IoT and finally the Policy and regulatory implications and best practices.",,2019,,211103847,semantic_scholar
944ad40d87fce6566211eeca78fe0b08eee1e34b,https://www.semanticscholar.org/paper/944ad40d87fce6566211eeca78fe0b08eee1e34b,Trustable Environmental Monitoring by Means of Sensors Networks on Swarming Autonomous Marine Vessels and Distributed Ledger Technology,"The article describes a highly trustable environmental monitoring system employing a small scalable swarm of small-sized marine vessels equipped with compact sensors and intended for the monitoring of water resources and infrastructures. The technological foundation of the process which guarantees that any third party can not alter the samples taken by the robot swarm is based on the Robonomics platform. This platform provides encrypted decentralized technologies based on distributed ledger tools, and market mechanisms for organizing the work of heterogeneous multi-vendor cyber-physical systems when automated economical transactions are needed. A small swarm of robots follows the autonomous ship, which is in charge of maintaining the secure transactions. The swarm implements a version of Reynolds' Boids model based on the Belief Space Planning approach. The main contributions of our work consist of: (1) the deployment of a secure sample certification and logging platform based on the blockchain with a small-sized swarm of autonomous vessels performing maneuvers to measure chemical parameters of water in automatic mode; (2) the coordination of a leader-follower framework for the small platoon of robots by means of a Reynolds' Boids model based on a Belief Space Planning approach. In addition, the article describes the process of measuring the chemical parameters of water by using sensors located on the vessels. Both technology testing on experimental vessel and environmental measurements are detailed. The results have been obtained through real world experiments of an autonomous vessel, which was integrated as the “leader” into a mixed reality simulation of a swarm of simulated smaller vessels.The design of the experimental vessel physically deployed in the Volga river to demonstrate the practical viability of the proposed methods is shortly described.",Frontiers in Robotics and AI,2020,10.3389/frobt.2020.00070,218905106,semantic_scholar
8002fff47f40e6126bf3f9f7fabea1ac9e1cbb4e,https://www.semanticscholar.org/paper/8002fff47f40e6126bf3f9f7fabea1ac9e1cbb4e,Hardware-in-the-Loop Testing of Connected and Automated Vehicle Applications: A Use Case for Queue-Aware Signalized Intersection Approach and Departure,"Most existing studies on connected and automated vehicle (CAV) applications apply simulation to evaluate system effectiveness. Model accuracy, limited data for calibration, and simulation assumptions limit the validity of evaluation results. One alternative approach is to use emerging hardware-in-the-loop (HIL) testing methods. HIL test environments enable physical test vehicles to interact with virtual vehicles from traffic simulation models, providing an evaluation environment that can replicate deployment conditions at early stages of CAV technology implementation without incurring excessive costs related to large field tests. In this study, a HIL testing system for vehicle-to-infrastructure (V2I) CAV applications is developed. The involved software and hardware includes a physical CAV controlled in real time, a traffic signal controller, communication devices, and a traffic simulator (VISSIM). Such HIL systems increase validity by considering the physical vehicle’s trajectories—which are constrained by real-world factors such as GPS accuracy, communication delay, and vehicle dynamics—in a simulated traffic environment. The developed HIL system is applied to test a representative early deployment CAV application: queue-aware signalized intersection approach and departure (Q-SIAD). The Q-SIAD algorithm generates recommended speed profiles based on the vehicle’s status, signal phase and timing (SPaT), downstream queue length, and system constraints and parameters (e.g., maximum acceleration and deceleration). The algorithm also considers the status of other vehicles in designing the speed profiles. The experiment successfully demonstrated this functionality with one test CAV driving through one intersection controlled by a fixed-timing traffic signal under various simulated traffic conditions.",Transportation Research Record: Journal of the Transportation Research Board,2018,10.1177/0361198118793001,115588554,semantic_scholar
0618fcca3b177d077401e4b62b495b84cd15e2c5,https://www.semanticscholar.org/paper/0618fcca3b177d077401e4b62b495b84cd15e2c5,A SCADA System Testbed for Cybersecurity and Forensic Research and Pedagogy,"This paper presents a supervisory control and data acquisition (SCADA) testbed recently built at the University of New Orleans. The testbed consists of models of three industrial physical processes: a gas pipeline, a power transmission and distribution system, and a wastewater treatment plant--these systems are fully-functional and implemented at small-scale. It utilizes real-world industrial equipment such as transformers, programmable logic controllers (PLC), aerators, etc., bringing it closer to modeling real-world SCADA systems. Sensors, actuators, and PLCs are deployed at each physical process system for local control and monitoring, and the PLCs are also connected to a computer running human-machine interface (HMI) software for monitoring the status of the physical processes. The testbed is a useful resource for cybersecurity research, forensic research, and education on different aspects of SCADA systems such as PLC programming, protocol analysis, and demonstration of cyber attacks.",ICSS '16,2016,10.1145/3018981.3018984,15327427,semantic_scholar
911f53c6615a08de2d589be5af88baa6f8c7b2b4,https://www.semanticscholar.org/paper/911f53c6615a08de2d589be5af88baa6f8c7b2b4,Understanding Pervasive Games for Purposes of Learning,"Among the manifold of approaches to technology enhanced learning, game based learning is very attractive. In game based learning, the technological systems employed for the purpose of learning are digital games. Stand-alone serious games are rare. Games deployed for learning need to be embedded into suitable contexts. A particular approach promising from certain didactic perspectives and driven by a variety of characteristics of learning contents and training requirements is embedding those games into the surrounding physical world. Games embedded into the physical world are called pervasive games. The ways of embedding are paramount. There have been numerous attempts to design and to implement pervasive games, in general, and to deploy pervasive games for learning purposes, in particular. The majority of those pervasive games failed quite badly. Storyboarding the interaction between the real world and the virtual world of a pervasive game reveals the essential strengths and weaknesses of the game concept and allows for diagnosing didactic flaws of game play. Beyond its diagnostic power, the approach supports the design of more affective and effective pervasive games. Storyboarding is a methodology of anticipating human experience and, thus, a methodology of didactic design. 1 THE AUTHORS’ POSITION All of us–readers and authors of this manuscript–are aware of the fact that so-called digital natives 1 have other expectations when facing digital media than their parents and teachers. Playful learning, whenever possible, and using digital games for learning without any fear belongs to the widespread expectations teachers and trainers have to fulfill. In response, game based learning and serious games are terms naming some prosperous field of technology enhanced learning. When the learning contents is out there in the surrounding world, it seems plausible to bring the games out there as well–pervasive games concepts evolve. In harsh contrast to the promises, most pervasive games failed badly. There will surely be no superficial and short explanation for a large number of finally disappointing game developments. But understanding the past and 1The term digital natives as polemically opposed to denigratingly calleddigital immigrantsis, exactly in this sense, ascribed to Marc Prensky (Prensky 2001), although the idea as a whole dates back to (Barlow, 1996) writing: “You are terrified of your own children, since they are natives in a world where you will always be immigrants.” shaping the future surely needs some pondering, some exchange of opinions, and several innovative ideas. The authors aim at some small contribution to this process by advocating their position, ◦ that there are decisive characteristics of pervasive games which may be well explicated by suitable approaches of storyboarding applied to pervasive games. Using storyboarding a posteriori, it turns out to work as a diagnostic tool. Doing it a priori, storyboarding becomes a tool for design and development fostering to draw conclusions from lessons learned in earlier projects that failed. Based on the authors’ key position above, one is lead to some more viewpoints worth to be considered. ◦ Pervasive games may be classified according to their pervasiveness which is of didactic relevance. ◦ The crucial embedding of learning contents into game play may be characterized quite well by means of storyboarding terminology. ◦ The storyboarding technology, by its very nature, allows for an explication of the context conditions in which learning is likely to take place. The basic terminology will be introduced briefly to be applied to a larger number of pervasive games. 696 P. Jantke K. and Spundflasch S.. Understanding Pervasive Games for Purposes of Learning. DOI: 10.5220/0004413006960701 In Proceedings of the 5th International Conference on Computer Supported Education (CSEDU-2013), pages 696-701 ISBN: 978-989-8565-53-2 Copyright c 2013 SCITEPRESS (Science and Technology Publications, Lda.) 2 INTRODUCTORY CASE STUDY Before going into the details of discussion, the authors are aiming at an intuitive introduction. Instead of presenting notations in a formal way, a certain digital game is used to exemplify what the present paper is about, which concepts are in use, and how typical problems are formulated and attacked. The game selected for an introduction by example is TREASURE(Chalmers et al., 2005) which is one of the earliest pervasive games. The purpose of the game TREASUREis to learnabout wireless communication. The ideas underlying this game are easy in structure. Figure 1: Interface to the T REASUREgame as it appears in some PDA; picture taken from (Chalmers et al., 2005) with the permission of the authors as it appears in (Jantke, 2006) . Some real urban environment such as a park, e.g., is virtually equipped with virtual treasures 2. Teams of players are running around in pursuit of treasures. Team members in the real world are localized by means of GPS technology relating them to the virtual treasures and to each other. In certain areas, there are WLAN connections allowing players to contact their virtual treasure boxes on the server for upload. 2For the borderline between reality and virtuality, in general, and for its relevance to e-learning, in particular , interested readers are directed to (Jantke and Lengyel, 2012) . (Chalmers et al., 2005) describe variations of the game mechanics. The core idea, however, is lucid. The storyboard in fig. 2 is summarizing the essentials. Figure 2: Storyboard of T REASURE’s game mechanics. Every node is an episode or a scene describing some action. Smaller inscriptions describe actions of the computer system as opposed to actions of human players. Solid lines indicate the passing of a human player from one action to another such as, for illustration, from just walking to picking up some treasure. Dashed green lines indicate that the player’s action causes some actions of the computer system. In turn, dotted blue lines indicate the impact of earlier game actions on the player’s current actions. For instance, virtual treasures can only be discovered and picked up where the computer system has placed them virtually. Arrows indicating update operations of the players’ positions have been dropped. Game playing means moving around, collecting virtual treasures, trying to pickpocket each other, and aiming at uploads of the own virtual treasure to the safe virtual treasure box. The bookkeeping of treasure locations and treasure boxes defines the termination of game play. The simplicity of the storyboard above reflects the simple structure of the underlying game concept. Furthermore, it exhibits that there are no actions of interest performed by the game system except bookkeeping and, thus, determining preconditions of player actions. The game system is not perceived as an actor, but more seen as a supervising game master. Understanding Pervasive Games for Purposes of Learning",CSEDU,2013,10.5220/0004413006960701,9682055,semantic_scholar
e988d52c2959d5924862e33076d5af550344c47f,https://www.semanticscholar.org/paper/e988d52c2959d5924862e33076d5af550344c47f,Flow-Based Network Slicing: Mapping the Future Mobile Radio Access Networks,"Nowadays mobile networks are asked to support different applications and services characterized by very specific Quality of Service (QoS) requirements. With this aim in mind, deploying network slices with particular resource allocation policies on a per-service basis becomes extremely relevant. In this regard, we introduce a solution able to dynamically partition the underlying physical infrastructure of a mobile radio access network into multiple logical slices with distinctive service-level agreements. We leverage Software-Defined Networking principles to provide fine-grained flow identification and sophisticated QoS management policies on a generic architecture supporting 4G and 5G networks with the objective of mapping the path towards the future mobile networks. The experimental evaluation of the deployed prototype on a real-world testbed has demonstrated the slicing capabilities of the system while ensuring full performance and functional isolation. We release the entire implementation under a permissive APACHE 2.0 license for academic use.",2019 28th International Conference on Computer Communication and Networks (ICCCN),2019,10.1109/ICCCN.2019.8847068,203566043,semantic_scholar
24b69c53c7aedc28e1a483bc43b14d27f2614c63,https://www.semanticscholar.org/paper/24b69c53c7aedc28e1a483bc43b14d27f2614c63,Urban Edge Computing,"The new paradigm of Edge Computing aims to bring resources for storage and computations closer to end devices, alleviating stress on core networks and enabling low-latency mobile applications. While Cloud Computing carries out processing in large centralized data centers, Edge Computing leverages smaller-scale resources— often termed cloudlets—in the vicinity of users. Edge Computing is expected to support novel applications (e.g., mobile augmented reality) and the growing number of connected devices (e.g., from the domain of the Internet of Things). Today, however, we lack essential building blocks for the widespread public availability of Edge Computing, especially in urban environments. This thesis makes several contributions to the understanding, planning, deployment, and operation of Urban Edge Computing infrastructures. We start from a broad perspective by conducting a thorough analysis of the field of Edge Computing, systematizing use cases, discussing potential benefits, and analyzing the potential of Edge Computing for different types of applications. 
We propose re-using existing physical infrastructures (cellular base stations, WiFi routers, and augmented street lamps) in an urban environment to provide computing resources by upgrading those infrastructures with cloudlets. On the basis of a real-world dataset containing the location of those infrastructures and mobility traces of two mobile applications, we conduct the first large-scale measurement study of urban cloudlet coverage with four different metrics for coverage. After having shown the viability of using those existing infrastructures in an urban environment, we make an algorithmic contribution to the problem of which locations to upgrade with cloudlets, given the heterogeneous nature (with regards to communication range, computing resources, and costs) of the underlying infrastructure. Our proposed solution operates locally on grid cells and is able to adapt to the desired tradeoff between the quality of service and costs for the deployment. Using a simulation experiment on the same mobility traces, we show the effectiveness of our strategy. 
Existing mechanisms for computation offloading typically achieve loose coupling between the client device and the computing resources by requiring prior transfers of heavyweight execution environments. In light of this deficiency, we propose the concept of store-based microservice onloading, embedded in a flexible runtime environment for Edge Computing. Our runtime environment operates on a microservice-level granularity and those services are made available in a repository—the microservice store—and, upon request from a client, transferred from the store to execution agents at the edge. Furthermore, our Edge Computing runtime is able to share running instances with multiple users and supports the seamless definition and execution of service chains through distributed message queues. Empirical measurements of the implemented approach showed up to 13 times reduction in the end-to-end latency and energy savings of up to 94 % for the mobile device. 
We provide three contributions regarding strategies and adaptations of an Edge Computing system at runtime. Existing strategies for the placement of data and computation components are not adapted to the requirements of a heterogeneous (e.g., with regards to varying resources) edge environment. The placement of functional parts of an application is a core component of runtime decisions. This problem is computationally hard and has been insufficiently explored for service chains whose topologies are typical for Edge Computing environments (e.g., with regards to the location of data sources and sinks). To this end, we present two classes of heuristics that make the problem more tractable. We implement representatives for each class and show how they substantially reduce the time it takes to find a solution to the placement problem, while introducing only a small optimality gap. The placement of data (e.g., such captured by mobile devices) in Edge Computing should take into account the user’s context and the possible intent of sharing this data. Especially in the case of overloaded networks, e.g., during large-scale events, edge infrastructure can be beneficial for data storage and local dissemination. To address this challenge, we propose vStore, a middleware that—based on a set of rules—decouples applications from pre-defined storage locations in the cloud. We report on results from a field study with a demonstration application, showing that we were able to reduce cloud storage in favor of proximate micro-storage at the edge. 
As a final contribution, we explore the adaptation possibilities of microservices themselves. We suggest to make microservices adaptable in three dimensions: (i) in the algorithms they use to perform a certain task, (ii) in their parameters, and (iii) in auxiliary data that is required. These adaptations can be leveraged to trade a faster execution time for a decreased quality of the computation (e.g., by producing more inaccurate or partly wrong results). We argue that this is an important building block to be included in an Edge Computing system in view of both constrained resources and strict requirements on computation latencies. We conceptualize an adaptable microservice execution framework and define the problem of choosing the service variant, building upon the design of our previously introduced Edge Computing runtime environment. For a case study, we implement representative examples (e.g., in the field of computer vision and image processing) and outline the practical influence of the abovementioned tradeoff. 
In conclusion, this dissertation systematically analyzes the field of Urban Edge Computing, thereby contributing to its general understanding. Our contributions provide several important building blocks for the realization of a public Edge Computing infrastructure in an urban environment.",,2020,10.25534/TUPRINTS-00013362,225078801,semantic_scholar
c5ca0cfef7767f65c1095253d94be59af280d017,https://www.semanticscholar.org/paper/c5ca0cfef7767f65c1095253d94be59af280d017,Analysis of Software Countermeasures for Whitebox Encryption,"Whitebox cryptography aims to ensure the security of cryptographic algorithms in the whitebox model where the adversary has full access to the execution environment. To attain security in this setting is a challenging problem: Indeed, all published whitebox implementations of standard symmetric-key algorithms such as AES to date have been practically broken. However, as far as we know, no whitebox implementation in real-world products has suffered from a key recovery attack. This is due to the fact that commercial products deploy additional software protection mechanisms on top of the whitebox implementation. This makes practical attacks much less feasible in real-world applications. There are numerous software protection mechanisms which protect against standard whitebox attacks. One such technique is control flow obfuscation which randomizes the order of table lookups for each execution of the whitebox encryption module. Another technique is randomizing the locations of the various Look up tables (LUTs) in the memory address space. In this paper we investigate the effectiveness of these countermeasures against two attack paradigms. The first known as Differential Computational Analysis (DCA) attack was developed by Bos, Hubain, Michiels and Teuwen in CHES 2016. The attack passively collects software execution traces for several plaintext encryptions and uses the collected data to perform an analysis similar to the well known differential power attacks (DPA) to recover the secret key. Since the software execution traces contain time demarcated physical addresses of memory locations being read/written into, they essentially leak the values of the inputs to the various LUTs accessed during the whitebox encryption operation, which as it turns out leaks sufficient information to perform the power attack. We found that if in addition to control flow obfuscation, one were to randomize the locations of the LUTs in the memory, then it is very difficult to perform the DCA on the resultant system using such table inputs and extract the secret key in reasonable time. As an alternative, we investigate the version of the DCA attack which uses the outputs of the tables instead of the inputs to mount the power analysis attack. This modified DCA is able to extract the secret key from the flow obfuscated and location randomized versions of several whitebox binaries available in crypto literature. We develop another attack called the Zero Difference Enumeration (ZDE) attack. The attack records software traces for several pairs of strategically selected plaintexts and performs a simple statistical test on the effective difference of the traces to extract the secret key. We show that ZDE is able to recover the keys of whitebox systems. Finally we propose a new countermeasure for protecting whitebox binaries based on insertion of random delays which aims to make both the ZDE and DCA attackspractically difficult by adding random noise in the information leaked to the attacker.",IACR Trans. Symmetric Cryptol.,2017,10.13154/tosc.v2017.i1.307-328,3627062,semantic_scholar
68feb53085d7df58268e014ee9e14596f57e0f45,https://www.semanticscholar.org/paper/68feb53085d7df58268e014ee9e14596f57e0f45,Deploying W3C Web of Things-Based Interoperable Mash-up Applications for Industry 4.0: A Testbed,"In Industry 4.0 scenarios, novel applications are enabled by the capability to gather large amount of data from pervasive sensors and to process them in order to devise the “digital twin” of a physical equipment. The heterogeneity of hardware sensors, communication protocols and data formats constitutes one of the main challenge toward the large-scale adoption of the Internet of Things (IoT) paradigm on industrial environments. To this purpose, the W3C Web of Things (WoT) group is working on the definition of some reference standards intended to describe in a uniform way the software interfaces of IoT devices and services, and hence to achieve the full interoperability among different IoT components regardless of their implementation. At the same time, due also to the recent appearance of the WoT W3C draft, few testbed and real-world deployments of the W3C WoT architecture has been proposed so far in the literature. In this paper, we attempt to fill such gap by describing the realization of a WoT monitoring application of a generic indoor production site: the system is able to orchestrate the sensing operations from three heterogeneous Wireless Sensor Networks (WSNs). We describe how the components of the W3C WoT architecture have been instantiated in our scenario. Moreover, we demonstrate the possibility to decouple the mash-up policies from the network functionalities, and we evaluate the overhead introduced by the WoT approach.",WWIC,2019,10.1007/978-3-030-30523-9_1,202550324,semantic_scholar
b3bf6ac7bd450eccec62fa45182924a5106eddd3,https://www.semanticscholar.org/paper/b3bf6ac7bd450eccec62fa45182924a5106eddd3,Understanding the bottlenecks in virtualizing cellular core network functions,"Network function virtualization (NFV) promises significant cost savings, flexibility and ease of deployment. However, potential challenges in implementing virtualized network elements that can support real-world performance requirements are still an open question. For example, traditional telecom networks have a lot of complex interdependencies that can affect performance. In this paper, we study the potential bottlenecks in virtualizing cellular core network functions. Using a combination of analysis and experimentation, we quantify the impact of software-based EPC elements on various metrics including physical processing, memory, IO, and bandwidth resource requirements. We use production grade, software-based cellular network elements running on general purpose Linux servers, driven by a variety of realistic workloads derived from a realworld cellular network, to examine the combined effects of control and data planes on an LTE enhanced packet core (EPC). In particular, we discover that the SGW handles about 33% of the control plane transactions and is a potential source for performance bottlenecks as a result of the interdependencies between control and data plane processing. Our results indicate that simply replacing existing EPC elements with virtualized equivalents can have severe performance bottlenecks and that virtualized EPC elements need to be carefully designed.",The 21st IEEE International Workshop on Local and Metropolitan Area Networks,2015,10.1109/LANMAN.2015.7114735,13952593,semantic_scholar
fab176450e9142a31f47c0dbe1990f1e647cb66d,https://www.semanticscholar.org/paper/fab176450e9142a31f47c0dbe1990f1e647cb66d,Guest Editorial: Special Issue on Emerging Technology for Software Define Network Enabled Internet of Things,"The Internet of Things (IoT) has been considered as a technology, which takes the first step towards a smarter world bridging the physical world with the cyber world. Even though IoT notion has gained much attention during the last few decades, real-world implementation of large-scale IoT network is still evolving in its infancy. Since deployment lags far behind the theoretic notion, comprehensive research efforts on innovative applications, architecture, and a network of IoT are required to promote the implementation of large-scale, high-quality, efficient, and secure IoT scenarios. Software Defined Networks (SDN) facilitates a variety of opportunities for network evolution. The key feature of SDN is to decouple data and control planes, which removes control plane from network hardware. As a result, it offers a remarkable resilience in programming, while providing a broad range of opportunities to optimize the utilization of network resources. Owing to the characteristics of SDN, experts in both industry and academia claims SDN as one of the ideal technologies to bridge the gaps and to overcome drawbacks of IoT deployment. Exploiting the benefits of scalable and adaptable network devices, SDN is considered as highly promising in empowering smart, powerful, and open IoT services and communication functionalities. In fact, SDN is capable of addressing numerous challenges in IoT varying from innumerable service requests and responses, the enormous data flow of IoT sensors, devices, and appli-",International Journal of Parallel Programming,2019,10.1007/s10766-019-00643-0,201714349,semantic_scholar
1b2ba411791c8c173050085c44f49deec8d2c953,https://www.semanticscholar.org/paper/1b2ba411791c8c173050085c44f49deec8d2c953,Design and Implementation of TeleAdvisor: a Projection-Based Augmented Reality System for Remote Collaboration,"TeleAdvisor is a versatile projection-based augmented reality system designed for remote collaboration. It allows a remote expert to naturally guide a local user in need of assistance in carrying out physical tasks around real-world objects. The system consists of a small projector and two cameras mounted on top of a tele-operated robotic arm at the worker’s side, and an interface to view the camera stream, control the point-of-view and gesture using projected annotations at the remote expert’s side. TeleAdvisor provides a hands-free, mobile, low-cost solution that supports gesturing by the remote expert while minimizing the cognitive overhead of the local worker. We describe the challenges, design considerations and implementation details of the two phases of the TeleAdvisor prototype, as well as its evaluation and deployment at an industrial manufacturing center. We summarize our understandings from our experiences during the project and discuss the general implications for design of augmented reality remote collaboration systems.",Computer Supported Cooperative Work (CSCW),2015,10.1007/s10606-015-9232-7,14455015,semantic_scholar
3aee2d33e7be0d0d8fbc599dfc9489a5a33c2eae,https://www.semanticscholar.org/paper/3aee2d33e7be0d0d8fbc599dfc9489a5a33c2eae,Implementation of Web AR Applications with Fog Radio Access Networks Based on Openairinterface Platform,"Augmented Reality (AR), is a technology that gives you an interactive experience of the real world where the objects reside in are ‘augmented’ by computer-generated perceptual information. Though some AR applications already exist on our smart devices, their development is not as promising as expected due to the requirement of the installation of a dedicated app. Hence the application of AR on large scales has been hampered to a certain extent. The advent of Web AR is a good complementary of the original in-app AR applications. With the convenience and cross-platform characteristics of Web browser, Web AR is being paid increasingly attention from both manufacturers and researchers. Fog Radio Access Networks (F-RAN) architecture takes full advantage of the storage and processing capabilities in edge devices, which significantly alleviates the burden of the fronthaul and BBU pool and reduces the transmission delay. Openairinterface is an LTE-compatible open source platform provided by Openairinterface Software Alliance (OSA). It is a promising demonstration platform, on which many new system architectures and communication technologies can be deployed. In this article, two applications of Web AR, including Web Face AR and Web Space AR, are successfully demonstrated on our F-RAN hardware platform based on Openairinterface. Performance evaluation results validate that they can provide users with good Quality of Experience (QoE) and very low latency.","2019 5th International Conference on Control, Automation and Robotics (ICCAR)",2019,10.1109/ICCAR.2019.8813710,201814652,semantic_scholar
ef8e465f80f41ec5cb3dbdb527c8509e66abaf5c,https://www.semanticscholar.org/paper/ef8e465f80f41ec5cb3dbdb527c8509e66abaf5c,A Framework to Secure Applications with ISA Heterogeneity,"Software security attacks are evolving from exploiting common code vulnerabilities to exploiting micro architecture side-channels. Traditional software diversity or code randomization techniques diversify the code memory layout and make it difficult for potential attackers to pinpoint the precise location of the target vulnerability. However, those approaches may not be sufficient enough for the new micro architecture attacks (e.g., Spectre). While some architecture researchers have proposed using diverse ISA configurations to defeat code injection or code reuse attacks, most of these works remain in the simulation stage due to legal, licensing, and verification costs involved in bringing a heterogeneous chip design into physical hardware [39]. In this paper, we report our on-going work of HeterSec, a framework to secure applications utilizing real world heterogeneous ISA machines. HeterSec runs on top of the commodity x86_64 and ARM64 machines. It gives the process the ability to dynamically select its underlying ISA environment. Therefore, the protected process would hide the vulnerable targets with the diversified instruction set, or would detect the abnormal behavior by comparing the execution results step-by-step from multiple ISA-diversified instances. To demonstrate the effectiveness of such software framework, we implemented HeterSec on Linux and showed its deployability by running it on a x86_64 and ARM64machine pair, connected using InfiniBand. We then conduct two case studies with HeterSec. In the first case, we timely randomize the process execution path across the ISA, which achieves similar security guarantees as the existing architecture based solutions. In the second case, we implement a multi-ISA based multi-version execution (MVX) system, providing a stronger security guarantee than current homogeneousISA MVX designs.",,2019,,86857561,semantic_scholar
b2f006d98b2b66f0ee1e5c90065e30ec8fd917b6,https://www.semanticscholar.org/paper/b2f006d98b2b66f0ee1e5c90065e30ec8fd917b6,"AR and VR Applications Improve Engineering Collaboration, Personnel Optimization, and Equipment Accuracy for Separation Solutions","
 With the most recent industry downturn still fresh in many minds, the oil and gas E&P sector is approaching this recovery with a commitment to long-term cost discipline. As a result, augmented reality (AR) and virtual reality (VR) technologies are being adopted by operators and service companies alike as a means of cost savings while driving operational efficiency.
 AR technologies employ enhanced visualization hardware, techniques, and methodologies to create new environments wherein digital and physical objects and their data coexist and interact with one another, enhancing the user experience of the real world (Kunkel and Soechti 2017). VR refers to the full immersion of the user intoand interaction with a completely digital environment. Together, these technologies form the core of immersive experience and a new paradigm in industrial interaction.
 Until recently, these technologies were primarily applied as enhanced entertainment products, most notably within the gaming industry. However, during the past several years, and thanks to the introduction of hands-free, head-mounted display (HMD) technologies, such as Microsoft® HoloLens™ and now HoloLens 2, AR and VR are migrating into the enterprise sector.
 While the oil field has not been as quick to integrate AR and VR as other sectors, such as medicine, defense, and aeronautics, operators and service providers alike have increased adoption overthe past 12 months. Motivated by a mandate to keep operating costs low and improve efficiencies in terms of field processes, operators have begun implementing AR/VR applications as collaborative problem-solving, planning, and design tools.
 For example, some operators are initiating ARconcepts to promote internal use development and prototyping for both oilfield applications and remote refinery inspections. Additionally, service companies are embracing the use of smart glasses and wearable technologies to help improve remote work and collaboration to help increase in-field safety and reduce downtime.
 As part of its strategy to help drive the oil and gas industry's digital transformation, one major service provider is developing AR/VR applications to create digital representations of physical oilfield assets on the Microsoft® HoloLens device. One area of focus is the planning, design, and deployment of solids control, fluid separation, and handling technologies for offshore drilling applications.","Day 2 Wed, September 04, 2019",2019,10.2118/195720-MS,202424582,semantic_scholar
2ee588a45093e121d2bc8d09fe0c81992a9fc1d8,https://www.semanticscholar.org/paper/2ee588a45093e121d2bc8d09fe0c81992a9fc1d8,Secure Containers in Android: The Samsung KNOX Case Study,"Bring Your Own Device (BYOD) is a growing trend among enterprises, aiming to improve workers' mobility and productivity via their smartphones. The threats and dangers posed by the smartphones to the enterprise are also ever-growing. Such dangers can be mitigated by running the enterprise software inside a ""secure container"" on the smartphone. In our work we present a systematic assessment of security critical areas in design and implementation of a secure container for Android using reverse engineering and attacker-inspired methods. We do this through a case-study of Samsung KNOX, a real-world product deployed on millions of devices. Our research shows how KNOX security features work behind the scenes and lets us compare the vendor's public security claims against reality. Along the way we identified several design weaknesses and a few vulnerabilities that were disclosed to Samsung.",SPSM@CCS,2016,10.1145/2994459.2994470,8510729,semantic_scholar
3519511ac74b509ef65457c72b81cec0fdc1a256,https://www.semanticscholar.org/paper/3519511ac74b509ef65457c72b81cec0fdc1a256,D Distributed Heterogeneous Tracking for Augmented Reality,"Augmented reality (AR) is a technique in which a user’s view of the real world is enhanced or augmented with additional information generated from a computer model (Azuma et al., 2001). The enhancement may consist of virtual artifacts to be fitted into the environment or a display of non-geometric information about existing real objects. Mobile AR (MAR) systems implement this interaction paradigm in an environment in which the user moves, possibly over wide areas (Feiner, MacIntyre, Hoellerer, & Webster, 1997). This is in contrast to non-mobile AR systems that are utilized in limited spaces such as a computer-aided surgery or by a technician’s aid in a repair shop. There are a number of challenges to implementing successful AR systems. These include a proper calibration of the optical properties of cameras and display systems (Tuceryan et al., 1995; Tuceryan, Genc, & Navab, 2002), and an accurate registration of threedimensional objects with their physical counterparts and environments (Breen, Whitaker, Rose, & Tuceryan, 1996; Whitaker, Crampton, Breen, Tuceryan, & Rose, 1995). In particular, as the observer (or an object of interest) moves over time, the 3D graphics need to be properly updated so that the realism of the resulting scene and/or alignment of necessary objects and graphics are maintained. Furthermore, this has to be done in real time and with high accuracy. The technology that allows this real-time update of the graphics as users and objects move is a tracking system that measures the position and orientation of the tracked objects (Koller et al., 1997). The ability to track objects, therefore, is one of the big challenges in MAR systems. This article describes a software framework for realizing such a distributed tracking environment by discovering independently deployed, possibly heterogeneous trackers and fusing the data from them while roaming over a wide area. In addition to the MAR domain, this kind of a tracking capability would also be useful in other domains such as robotics and locationaware applications. The novelty of this research lies in the amalgamation of the theoretical principles from the domains of AR/VR, data fusion, and the distributed software systems to create a sensor-based, wide-area tracking environment. BACKGROUND",,2019,,59331378,semantic_scholar
506e55521ddd1442ae6eae58534aa971946acc3f,https://www.semanticscholar.org/paper/506e55521ddd1442ae6eae58534aa971946acc3f,Distributed Heterogeneous Tracking for Augmented Reality,"Augmented reality (AR) is a technique in which a user’s view of the real world is enhanced or augmented with additional information generated from a computer model (Azuma et al., 2001). The enhancement may consist of virtual artifacts to be fitted into the environment or a display of non-geometric information about existing real objects. Mobile AR (MAR) systems implement this interaction paradigm in an environment in which the user moves, possibly over wide areas (Feiner, MacIntyre, Hoellerer, & Webster, 1997). This is in contrast to non-mobile AR systems that are utilized in limited spaces such as a computer-aided surgery or by a technician’s aid in a repair shop. There are a number of challenges to implementing successful AR systems. These include a proper calibration of the optical properties of cameras and display systems (Tuceryan et al., 1995; Tuceryan, Genc, & Navab, 2002), and an accurate registration of threedimensional objects with their physical counterparts and environments (Breen, Whitaker, Rose, & Tuceryan, 1996; Whitaker, Crampton, Breen, Tuceryan, & Rose, 1995). In particular, as the observer (or an object of interest) moves over time, the 3D graphics need to be properly updated so that the realism of the resulting scene and/or alignment of necessary objects and graphics are maintained. Furthermore, this has to be done in real time and with high accuracy. The technology that allows this real-time update of the graphics as users and objects move is a tracking system that measures the position and orientation of the tracked objects (Koller et al., 1997). The ability to track objects, therefore, is one of the big challenges in MAR systems. This article describes a software framework for realizing such a distributed tracking environment by discovering independently deployed, possibly heterogeneous trackers and fusing the data from them while roaming over a wide area. In addition to the MAR domain, this kind of a tracking capability would also be useful in other domains such as robotics and locationaware applications. The novelty of this research lies in the amalgamation of the theoretical principles from the domains of AR/VR, data fusion, and the distributed software systems to create a sensor-based, wide-area tracking environment. BACKGROUND",,2019,,207979554,semantic_scholar
333ccba50d00f3e8c4766e0c5e179ff4de31036f,https://www.semanticscholar.org/paper/333ccba50d00f3e8c4766e0c5e179ff4de31036f,KNOWLEDGE EXCHANGE WITHIN THE PARTICLE ACCELERATOR COMMUNITY VIA CLOUD COMPUTING*,"The development, testing and use of particle accelerator modeling codes is a core competency of accelerator research laboratories around the world, and likewise for synchrotron radiation and X-ray optics codes at lightsource facilities. Such codes require time and training to learn a command-line workflow involving multiple input and configuration files, execution on a high-performance server or cluster, post-processing with specialized software and finally visualization. Such workflows are error prone and difficult to reproduce. Cloud computing and UI design are core competencies of RadiaSoft LLC, where the Sirepo framework is being developed to make state of the art codes available in the browser of any desktop, laptop or tablet. We present our initial successes as real world examples of knowledge exchange between industry and the research community. This work is leading to broader knowledge exchange throughout the community by facilitating education of students and enabling instantaneous sharing of simulation details between colleagues. Sirepo design objectives include: seamless integration with legacy codes, low barrier to entry for new users, configuration transfer to command-line mode, catalog of provenance to aid reproducibility, and simplified collaboration through multimodal sharing. The combination of intuitive browserbased GUIs and Sirepo's server-side application container technology enables simplified computational archiving and reproducibility. If embraced by the community, this could become an important asset for the design, commissioning and future upgrade of particle accelerator and Xray beamline facilities. SIREPO – A SOFTWARE FRAMEWORK Sirepo is an open source framework [1] for bringing any scientific, engineering or educational software to the cloud, with a GUI that works in any modern browser on any computing device with sufficient screen size, including tablets. The Sirepo client is built on HTML5 technologies, including the JavaScript libraries Bootstrap [2] and Angular [3]. The D3 library [4, 5] is used for interactive 2D graphics, while VTK [6] is used for 3D. The Sirepo server is built on Flask [7], a lightweight framework for web development with Python. The scientific codes supported by Sirepo, and all of their dependencies, are containerized via Docker [8], which is an open platform for distributed applications. RadiaSoft has developed open source software [9] and expertise for building, deploying and reliably executing scientific codes in Docker containers [10, 11]. RadiaSoft docker images are publicly available [12] as part of the Sirepo ecosystem. SIREPO – A SCIENTIFIC GATEWAY RadiaSoft maintains a free scientific gateway for the particle accelerator community [13], which provides a broad selection of supported codes. The most mature implementations are for SRW (Synchrotron Radiation Workshop) [14-16] and elegant [17, 18]. SRW is an open source [19] physical optics code with powerful features for calculating synchrotron radiation and X-Ray optics, including successful detailed benchmarking against state-of-the-art X-Ray beamlines [20]. Sirepo/SRW has a growing number of users at synchrotron and free electron laser (FEL) user facilities around the world, including: NSLS-II, LCLS, APS and ALS in the USA, ELETTRA in Italy, European XFEL in Germany, ESRF and SOLEIL in France, PSI in Switzerland, Diamond in the UK and LNLS in Brazil. Just as elegant is one of the most widely used codes in the world for particle accelerator simulation and design, Sirepo/elegant has many more users than the other supported accelerator codes, including regular classroom use at the US Particle Accelerator School (USPAS) [21, 22]. Other well-supported codes provide important capabilities: Synergia [23, 24] offers 2D and 3D space charge models and special features for simulating nonlinear integrable optics in the IOTA ring [25, 26], while Zgoubi [27] provides spin tracking and JSPEC [28, 29] models intrabeam scattering and electron cooling. There are two Sirepo implementations of the massivelyparallel open source particle-in-cell (PIC) code Warp [3033]. Warp PBA enables use of Warp’s quasi-3D electromagnetic PIC modeling of beamand laser-driven plasmabased accelerators. Warp VND uses Warp’s electrostatic PIC capabilities to simulate a wide range of problems, with near-term emphasis on thermionic converters and other types of vacuum nanoelectronic devices.",,2019,,236908192,semantic_scholar
55b3a0752036c32bb61d23e7e93a1fddda0c8289,https://www.semanticscholar.org/paper/55b3a0752036c32bb61d23e7e93a1fddda0c8289,Towards trusted execution of multi-modal continuous authentication schemes,"The emergence of powerful, sensor-rich devices has led to the development of continuous authentication (CA) schemes using off-the-shelf hardware, where user behaviour is compared to past experience to produce an authentication decision with the aim of addressing challenges with traditional authentication schemes. Current CA proposals, however, have largely neglected adversaries present in a real-world deployment, namely the ubiquity of mal ware and software attacks. This has particular importance when a device cannot be trusted by a third-party, such as a corporation, that controls access to assets based on that decision. A software compromise, either on the scheme implementation or platform, may enable an adversary to modify authentication scores to alter the status of the device in reality, give insights into user behaviour, or gain unauthorised access to restricted assets. Hence, for the first time, we examine two standardised constructs that offer isolated and trusted execution - Secure Elements (SEs) and Trusted Execution Environments (TEEs) - even when an adversary has root-level privileges, and propose measures for providing trusted CA while retaining deployability. Based on these, we implement the first system for evaluating TEE-based CA on a consumer mobile device using Intel SGX, thus providing confidentiality, integrity and trust while removing the main platform from the TCB. We present an empirical evaluation of TEE-and non-TEE performance using methods proposed in related CA schemes. Our results indicate that trusted CA can be provided with no significant performance penalty, and may even offer performance benefits.",SAC,2017,10.1145/3019612.3019652,22701326,semantic_scholar
2ad3366962d249b7b63c4986ebb0cb22ea212a75,https://www.semanticscholar.org/paper/2ad3366962d249b7b63c4986ebb0cb22ea212a75,"Service-Oriented Architecture Compass: Business Value, Planning, and Enterprise Roadmap","Praise for Service-Oriented Architecture Compass""A comprehensive roadmap to Service-Oriented Architecture (SOA). SOA is, in reality, a business architecture to be used by those enterprises intending to prosper in the 21st century. Decision makers who desire that their business become flexible can jumpstart that process by adopting the best practices and rules of thumb described in SOA Compass.""i¾Bob Laird, MCI IT Chief Architect""The book Service-Oriented Architecture Compass shows very clearly by means of real projects how agile business processes can be implemented using Service-Oriented Architectures. The entire development cycle from planning through implementation is presented very close to practice and the critical success factors are presented very convincingly.""i¾Professor Dr. Thomas Obermeier, Vice Dean of FHDW Bergisch Gladbach, Germany""This book is a major improvement in the field. It gives a clear view and all the key points on how to really face a SOA deployment in today's organizations.""i¾Mario Moreno, IT Architect Leader, Generali France""Service-Oriented Architecture enables organizations to be agile and flexible enough to adopt new business strategies and produce new services to overcome the challenges created by business dynamism today. CIOs have to consider SOA as a foundation of their Enterprise Applications Architecture primarily because it demonstrates that IT aligns to business processes and also because it positions IT as a service enabler and maximizes previous investments on business applications.To understand and profit from SOA, this book provides CIOs with the necessary concepts and knowledge needed to understand and adapt it into their IT organizations.""i¾Sabri Hamed Al-Azazi, CIO of Dubai Holding, Sabri""I am extremely impressed by the depth and scale of this book! The title is perfecti¾when you know where you want to go, you need a compass to guide you there! After good IT strategy leads you to SOA, this book is the perfect vehicle that will drive you from dream to reality. We in DSK Bank will use it as our SOA bible in the ongoing project.""i¾Miro Vichev, CIO, DSK Bank, Bulgaria, member of OTP Group""Service-Oriented Architecture offers a pathway to networking of intra- and inter-corporate business systems. The standards have the potential to create far more flexible and resilient business information systems than have been possible in the past. This book is a must-read for those who care about the future of business IT.""i¾Elizabeth Hackenson, CIO, MCI""Service-Oriented Architecture is key to help customers become on demand businessesi¾a business that can quickly respond to competitive threats and be first to take advantage of marketplace opportunities. SOA Compass is a must-read for those individuals looking to bridge the gap between IT and business in order to help their enterprises become more flexible and responsive.""i¾Michael Liebow, Vice President, Web Services and Service-Oriented Architecture, IBM Business Consulting Services""This book is a welcome addition to SOA literature. It articulates the business case and provides practical proven real-world advice, guidance, tips, and techniques for organizations to make the evolution from simple point-to-point web services to true SOA by addressing such topics as planning, organization, analysis and design, security, and systems management.""i¾Denis O'Sullivan, Fireman's Fund Enterprise ArchitectMaximize the business value and flexibility of your SOA deploymentIn this book, IBM Enterprise Integration Team experts present a start-to-finish guide to planning, implementing, and managing Service-Oriented Architecture. Drawing on their extensive experience helping enterprise customers migrate to SOA, the authors share hard-earned lessons and best practices for architects, project managers, and software development leaders alike.Well-written and practical, Service-Oriented Architecture Compass offers the perfect blend of principles and ""how-to"" guidance for transitioning your infrastructure to SOA. The authors clearly explain what SOA is, the opportunities it offers, and how it differs from earlier approaches. Using detailed examples from IBM consulting engagements, they show how to deploy SOA solutions that tightly integrate with your processes and operations, delivering maximum flexibility and value. With detailed coverage of topics ranging from policy-based management to workflow implementation, no other SOA book offers comparable value to workingIT professionals.Coverage includes SOA from both a business and technical standpointi¾and how to make the business case Planning your SOA project: best practices and pitfalls to avoid SOA analysis and design for superior flexibility and value Securing and managing your SOA environment Using SOA to simplify enterprise application integration Implementing business processes and workflow in SOA environments Case studies in SOA deployment After you've deployed: delivering better collaboration, greater scalability, and more sophisticated applicationsThe IBM Press developerWorks® Series is a unique undertaking in which print books and the Web are mutually supportive. The publications in this series are complemented by resources on the developerWorks Web site on ibm.com. Icons throughout the book alert the reader to these valuable resources.",,2005,,109626518,semantic_scholar
9456732ff41a7e8b125f229d00de12ffb59eaa67,https://www.semanticscholar.org/paper/9456732ff41a7e8b125f229d00de12ffb59eaa67,Analyzing and Securing Embedded Systems,"Author(s): Spensky, Chad | Advisor(s): Vigna, Giovanni; Kruegel, Christopher | Abstract: Embedded systems (i.e., single-purpose computers with tightly-coupled software and hardware) are now pervasive throughout in our increasingly digitized world. Due to the rapid growth of the embedded systems industry and the commercial pressure to implement new features, most of these systems are built using insecure hardware and have numerous latent software vulnerabilities. Unfortunately, the diversity of physical hardware and software implementations on these various systems along with their tight coupling between software and hardware have rendered most of our existing automated security analysis techniques ineffective. Attackers currently have the upper hand, as they need only discover a single vulnerability, whereas defenders must manually identify, and fix, all of the existing vulnerabilities. To make matters worse, many of these vulnerable embedded systems can interact with the physical world and, if compromised, could cause serious damage (e.g., a public utility) or even death (e.g., a medical device). To rectify this calamitous situation that we have created, we must be able to 1) identify and fix problems with the existing systems that are already deployed and 2) create future systems that are fundamentally secure, by design.Embedded systems are more difficult to analyze than traditional computers because the hardware platforms that they run on are far more diverse, have strict hardware dependencies, are equipped peripherals that differ wildly between systems, and their execution typically depends on external phenomena that materialize as hardware interrupts. The depth of the analysis can be improved by developing novel hardware-based introspection techniques, which would provide analysts with the ability to observe the internal state of the real embedded system in real-world scenarios. The scale of the analysis can also be improved by decoupling the firmware from the hardware through emulation techniques, which would permit analysts to parallelize their analyses across numerous emulated systems, without the need for hardware, and also experiment with the embedded system in a zero-risk virtual environment. I developed a novel hardware-based introspection technique for embedded systems that provides real-time, high-level insights into modifications made to both volatile and non-volatile memory using a Field-Programmable Gate Array (FPGA) implementation and novel semantic-gap reconstruction techniques. I also developed an approach to support the decoupling of firmware from its hardware that can use either hardware- or software-based instrumentation of the system to record the hardware interactions on the real system and then convert these recordings into generalized, composable ω-automata that can be used in place of the hardware for emulation.Embedded systems are also difficult to protect against hardware-based attacks, especially glitching. Ideally, firmware could be protected against these attacks using software-only techniques that could be deployed to the billions of existing systems to protect them from physical attacks, without physically replacing them. I developed an approach that permits embedded system developers to automatically inject various software-based glitching defenses into their code at compile-time, producing glitch-resistant firmware without the need for any code annotations or modifications to the embedded system’s hardware.",,2020,,229265516,semantic_scholar
1ab0a05b291c08c69bfadf4118dca3707f820eaf,https://www.semanticscholar.org/paper/1ab0a05b291c08c69bfadf4118dca3707f820eaf,Reality Considerations When Designing a TDMA-FDMA Based Link-Layer for Real-Time WSN,"In this article we elaborate on reality considerations when designing and implementing application tailored TDMA-FDMA medium access protocol with guaranteed end-to-end delay. We highlight importance of considering underlaying hardware and software components when designing communication protocols for resource constrained platforms. We also show that by combining medium access protocol, bootstrapping, and time synchronization mechanisms within the link-layer, we can limit on average clock drift in the network to 0.5 μs, as well as achieve 81% energy efficiency while keeping collision probability at its minimum of 1%. Finally, we conclude with challenges and lessons learned in real-world deployment of TDMA/FDMA based link-layer with guaranteed end-to-end delay in WSN.",MACOM,2012,10.1007/978-3-642-34976-8_11,40499054,semantic_scholar
7654c04648178149dad73ae3b1a93d404e631774,https://www.semanticscholar.org/paper/7654c04648178149dad73ae3b1a93d404e631774,OpenStack in Action,"In the cloud computing model, a cluster of physical computers hosts an environment that provides shared services (public and private) and offers the flexibility to easily add, remove, and expand virtual servers and applications. OpenStack is an open source framework that can be installed on individual physical servers to a cloud platform and enables the building of custom infrastructure (IaaS), platform (PaaS), and software (SaaS) services without the high cost and vendor lock-in associated with proprietary cloud platforms. OpenStack in Action offers real world use cases and step-by-step instructions to develop cloud platforms from inception to deployment. It explains the design of both the physical hardware cluster and the infrastructure services needed to create a custom cloud platform. It shows how to select and set up virtual and physical servers, implement software-defined networking, and the myriad other technical details required to design, deploy, and operate an OpenStack cloud in an enterprise. It also discusses the cloud operation techniques needed to establish security practices, access control, efficient scalability, and day-to-day DevOps practices. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications.",,2016,,114415234,semantic_scholar
09d3a6aae7eecc9d435c04d55e8386ddd27221e5,https://www.semanticscholar.org/paper/09d3a6aae7eecc9d435c04d55e8386ddd27221e5,The IoT hub: a fog node for seamless management of heterogeneous connected smart objects,"The Internet of Things (IoT) will interconnect billions of devices, denoted as “smart objects,” deployed pervasively, which will be extremely heterogeneous, in terms of hardware (i.e., computational power and available memory), software (i.e., operating systems, applications), and communication interfaces. Traditional Internet actors, such as personal computers, mobile devices, and cloud servers, will also communicate with smart objects, thus creating even more complexity. The IoT has so far grown as several vertical silos, with the purpose of demonstrating the concept of the IoT, rather than focusing on the actual construction of a highly interoperable infrastructure for the development of applications. The Internet Protocol (IP) stack (in particular, HTTP and CoAP) has been foreseen as the driver for integration and interoperability among devices and basis for the evolution of the Web of Things. However, in order to manage the physical diversity of devices and to create an IP-based infrastructure, the presence of network elements able to bridge different networks to enable direct end-to-end communication is required. Moreover, effective interaction with applications might require the presence of intermediaries, such as proxies, which may optionally implement protocol and data format translation functionalities. Given the above considerations, we propose a Fog node, denoted as “IoT Hub,” placed at the edge of multiple networks, which enhances the networks capabilities by implementing the following functions: border router; cross-proxy; cache; and resource directory. An implementation of the IoT Hub is presented together with a performance evaluation in a real-world IoT testbed.","2015 12th Annual IEEE International Conference on Sensing, Communication, and Networking - Workshops (SECON Workshops)",2015,10.1109/SECONW.2015.7328145,679112,semantic_scholar
a3d76c4d3718a53465153917bba9b24205d83096,https://www.semanticscholar.org/paper/a3d76c4d3718a53465153917bba9b24205d83096,Health 4.0 as an Application of Industry 4.0 in Healthcare Services and Management,"The Industry 4.0 Standard (I4S) employs technologies for automation and data exchange through cloud computing, Big Data (BD), Internet of Things (IoT), forms of wireless Internet, 5G technologies, cryptography, the use of semantic database (DB) design, Augmented Reality (AR) and Content-Based Image Retrieval (CBIR). Its healthcare extension is the so-called Health 4.0. 
This study informs about Health 4.0 and its potential to extend, virtualize and enable new healthcare-related processes (e.g., home care, finitude medicine, and personalized/remotely triggered pharmaceutical treatments) and transform them into services. 
In the future, these services will be able to virtualize multiple levels of care, connect devices and move to Personalized Medicine (PM). The Health 4.0 Cyber-Physical System (HCPS) contains several types of computers, communications, storage, interfaces, biosensors, and bioactuators. The HCPS paradigm permits observing processes from the real world, as well as monitoring patients before, during and after surgical procedures using biosensors. Besides, HCPSs contain bioactuators that accomplish the intended interventions along with other novel strategies to deploy PM. A biosensor detects some critical outer and inner patient conditions and sends these signals to a Decision-Making Unit (DMU). Mobile devices and wearables are present examples of gadgets containing biosensors. Once the DMU receives signals, they can be compared to the patient’s medical history and, depending on the protocols, a set of measures to handle a given situation will follow. The part responsible for the implementation of the automated mitigation actions are the bioactuators, which can vary from a buzzer to the remote-controlled release of some elements in a capsule inside the patient’s body. 
            Decentralizing health services is a challenge for the creation of health-related applications. Together, CBIR systems can enable access to information from multimedia and multimodality images, which can aid in patient diagnosis and medical decision-making. 
Currently, the National Health Service addresses the application of communication tools to patients and medical teams to intensify the transfer of treatments from the hospital to the home, without disruption in outpatient services. 
HCPS technologies share tools with remote servers, allowing data embedding and BD analysis and permit easy integration of healthcare professionals expertise with intelligent devices.  However, it is undeniable the need for improvements, multidisciplinary discussions, strong laws/protocols, inventories about the impact of novel techniques on patients/caregivers as well as rigorous tests of accuracy until reaching the level of automating any medical care technological initiative.",,2018,,86649549,semantic_scholar
60b5a34a0cf2939c394f4365c4e67cae394a4325,https://www.semanticscholar.org/paper/60b5a34a0cf2939c394f4365c4e67cae394a4325,AdaFT,"Cyber-physical systems (CPS) frequently have to use massive redundancy to meet application requirements for high reliability. While such redundancy is required, it can be activated adaptively, based on the current state of the controlled plant. Most of the time, the plant is in a state that allows for a lower level of fault tolerance. Avoiding the continuous deployment of massive fault tolerance will greatly reduce the workload of the CPS, and lower the operating temperature of the cyber sub-system, thus increasing its reliability. In this article, we extend our prior research by demonstrating a software simulation framework Adaptive Fault Tolerance (AdaFT) that can automatically generate the sub-spaces within which our adaptive fault tolerance can be applied. We also show the theoretical benefits of AdaFT and its actual implementation in several real-world CPSs.",TECS,2017,10.1145/2980763,218483539,semantic_scholar
7be4ed954faba9d34e421b1187d3bafdafd26330,https://www.semanticscholar.org/paper/7be4ed954faba9d34e421b1187d3bafdafd26330,The role of data supported decision-making technology in respiratory care,"Millions of people across the world are affected by Chronic Obstructive Pulmonary Disease (COPD). It is one of the most prevalent chronic health conditions in the world. As a life-long condition that effects breathing, it has a huge physical and mental impact on peoples’ lives every single day. COPD is characterised by periods of respiratory exacerbations which, if are not managed swiftly, can result in hospitalisation for emergency care. However, effective self-management and support can help people with COPD to avoid the distress of requiring emergency care, while supporting their quality of life and independence. In addition to the difficulties that COPD introduces to a plethora of people, it also presents a huge challenge for healthcare services around the world. In the UK, COPD generates a high number of hospital admissions annually, with many of these for emergency care. In this highly demanding and time-pressured context, healthcare professionals are required to make timely and evidence-based decisions to effectively care for patients. This is the challenging reality for all healthcare professionals that collaborate in the ongoing management and support involved for COPD care. Data supported decision-making (DSDM) technology holds potential to support the ongoing care of people with COPD, through connecting them and their healthcare professionals with pertinent data that can inform decision-making around care. Examples of such technologies include patient health monitoring apps that share data with healthcare professionals for personalised care planning, and clinical dashboards that interlink data from different sources to support decision-making about patient treatment. However, there is currently limited research working in partnership with people with COPD and respiratory healthcare professionals to truly understand how these technologies might support care in its real-world context. Specifically, there are three key gaps in knowledge which this thesis addresses. First, there is a need to understand how DSDM technologies can be designed to support healthcare professionals to provide COPD care, while considering the challenges of implementing technology into healthcare systems. Furthering this, there is a need to understand how technology could support the self-management of COPD, considering it is progressive and highly debilitating in nature. Finally, there is a need to understand how technology could support the ongoing care collaboration between healthcare professionals and patients through sharing patient-generated data about COPD symptoms. Each of these three areas are important in developing an understanding about how technology could support the real-world context of COPD care. To advance our knowledge in this space, I conducted three novel pieces of research working with people with COPD and healthcare professionals to understand how DSDM technologies could support everyday challenges related to COPD care. First, I worked with 11 healthcare professionals to co-design a DSDM dashboard by exploring their decision-making needs around COPD care. Then I conducted exploratory research involving 171 people with chronic respiratory conditions to understand how technology may support their self-care. Finally, I conducted a small exploratory case study with eight participants to understand the patient experience of self-monitoring their respiratory symptoms and the healthcare professionals’ experience of receiving this data remotely. The thesis concludes with a synthesis of the key novel findings across the three research studies, providing overarching opportunities and nodes of caution when designing and deploying DSDM technologies in this space. This discussion draws attention to the ways that perceptions of data ‘trustworthiness’ affects how DSDM technologies are used for decision-making, the tensions that occur when technology does not align with the local context of care, the need for self-management technology to support the personal and evolving condition journey of COPD, and how we may consider designing patient facing technologies to better accommodate potential reactive self-care patterns",,2020,10.17635/LANCASTER/THESIS/1056,234639833,semantic_scholar
b3adef935b0bcffff47f8f45900ce2c3b6bbaeed,https://www.semanticscholar.org/paper/b3adef935b0bcffff47f8f45900ce2c3b6bbaeed,Enhancing Interaction with Augmented Reality through Mid-Air Haptic Feedback: Architecture Design and User Feedback,"Nowadays, Augmented-Reality (AR) head-mounted displays (HMD) deliver a more immersive visualization of virtual contents, but the available means of interaction, mainly based on gesture and/or voice, are yet limited and obviously lack realism and expressivity when compared to traditional physical means. In this sense, the integration of haptics within AR may help to deliver an enriched experience, while facilitating the performance of specific actions, such as repositioning or resizing tasks, that are still dependent on the user’s skills. In this direction, this paper gathers the description of a flexible architecture designed to deploy haptically enabled AR applications both for mobile and wearable visualization devices. The haptic feedback may be generated through a variety of devices (e.g., wearable, graspable, or mid-air ones), and the architecture facilitates handling the specificity of each. For this reason, within the paper, it is discussed how to generate a haptic representation of a 3D digital object depending on the application and the target device. Additionally, the paper includes an analysis of practical, relevant issues that arise when setting up a system to work with specific devices like HMD (e.g., HoloLens) and mid-air haptic devices (e.g., Ultrahaptics), such as the alignment between the real world and the virtual one. The architecture applicability is demonstrated through the implementation of two applications: (a) Form Inspector and (b) Simon Game, built for HoloLens and iOS mobile phones for visualization and for UHK for mid-air haptics delivery. These applications have been used to explore with nine users the efficiency, meaningfulness, and usefulness of mid-air haptics for form perception, object resizing, and push interaction tasks. Results show that, although mobile interaction is preferred when this option is available, haptics turn out to be more meaningful in identifying shapes when compared to what users initially expect and in contributing to the execution of resizing tasks. Moreover, this preliminary user study reveals some design issues when working with haptic AR. For example, users may be expecting a tailored interface metaphor, not necessarily inspired in natural interaction. This has been the case of our proposal of virtual pressable buttons, built mimicking real buttons by using haptics, but differently interpreted by the study participants.",,2019,10.3390/app9235123,214145864,semantic_scholar
641bdbf02b3ecf41c66ea4b03356b42c4f9b99ba,https://www.semanticscholar.org/paper/641bdbf02b3ecf41c66ea4b03356b42c4f9b99ba,AdaFT: A Framework for Adaptive Fault Tolerance for Cyber-Physical Systems,"Cyber-physical systems (CPS) frequently have to use massive redundancy to meet application requirements for high reliability. While such redundancy is required, it can be activated adaptively, based on the current state of the controlled plant. Most of the time, the plant is in a state that allows for a lower level of fault tolerance. Avoiding the continuous deployment of massive fault tolerance will greatly reduce the workload of the CPS, and lower the operating temperature of the cyber sub-system, thus increasing its reliability. In this article, we extend our prior research by demonstrating a software simulation framework Adaptive Fault Tolerance (AdaFT) that can automatically generate the sub-spaces within which our adaptive fault tolerance can be applied. We also show the theoretical benefits of AdaFT and its actual implementation in several real-world CPSs.",ACM Trans. Embed. Comput. Syst.,2017,10.1145/2980763,18369100,semantic_scholar
26960f55ddb24b2338a5eae012c63cd13abb7bdd,https://www.semanticscholar.org/paper/26960f55ddb24b2338a5eae012c63cd13abb7bdd,Performance Evaluation for WiFi DCF Networks from Theory to Testbed,"Distributed Coordination Function (DCF) is a basic MAC protocol used in the world-wide WiFi networks and plays a key role in determining the network performance, especially in situations with a large number of users and high-density Access Point (AP) deployed. To achieve a better understanding of the real-world performance of 802.11 DCF networks, we have constructed an emulation platform and a prototype testbed for performance evaluation. The design and implementation of these two platforms are discussed in this paper. The key DCF parameters, i.e., the initial contention window size ($CW_{min}$) and the maximum contention window size ($CW_{max}$), are tuneable so that we are able to study the impact of these DCF parameters on the network performance. These experiment results are compared against with a recently proposed unified analytical framework to examine the model assumptions and system performance bottlenecks. Our results demonstrate that by adapting the values of $CW_{min}$ based on WiFi traffic load, the maximal network throughput can be achieved, and the optimal value of $CW_{min}$ varies when the network size changes. As a reality check, the emerging software defined WiFi network architecture can be optimized for performance enhancement guided by this unified performance model.",2017 IEEE International Symposium on Parallel and Distributed Processing with Applications and 2017 IEEE International Conference on Ubiquitous Computing and Communications (ISPA/IUCC),2017,10.1109/ISPA/IUCC.2017.00207,44093395,semantic_scholar
8bcb1c8534908815702a0db245402a7bdf53e136,https://www.semanticscholar.org/paper/8bcb1c8534908815702a0db245402a7bdf53e136,Cloud Computing in Support of Applied Learning: A Baseline Study of Infrastructure Design at Southern Polytechnic State University.,"Cloud computing represents an architecture and paradigm of computing designed to deliver infrastructure, platforms, and software as constructible computing resources on demand to networked users. As campuses are challenged to better accommodate academic needs for applications and computing environments, cloud computing can provide an accommodating solution for mobile, campus laboratory, and distance computing. The need for ubiquitous software deployments, virtual environments, software acceleration, economies of scale, and on-demand services points to cloud computing solutions for expedient network access to a pool of shared resources. In this baseline study, as part of a nascent research track, the researchers examine a proposed design for cloud computing at Southern Polytechnic State University to support action research, applied learning and practical, real-world student experiences at the university. Access to university cloud computing resources via an academic research network, physically isolated from the current production network, is proposed. Following a system development life-cycle methodology, design criteria are derived from an analysis of focus group data involving questions related to academic research, applied instruction, and experiential and service learning. Presentation of findings occurs in the form of a use case and architectural topology rendering to be used as a basis for follow-on study in this research track. Physical implementation of cloud computing models at the University can follow this roadmap as the research track unfolds and data are collected to analyze and evaluate for optimal cloud architecture in support of research and education.",,2013,,61942758,semantic_scholar
7f27c90447026bbee088a3b58a62ed57b1bd7102,https://www.semanticscholar.org/paper/7f27c90447026bbee088a3b58a62ed57b1bd7102,Automated Optimization of Software Parameters in a Long Term Evolution Radio Base Station,"Radio network optimization is concerned with the configuration of radio base station parameters in order to achieve the desired level of service quality in addition to many other differentiating technical factors. Mobile network operators have different physical locations, levels of traffic profiles, number of connected devices, and the desired quality of service. All of these conditions make the problem of optimizing the parameters of a radio base station specific to the operator’s business goals. The high number of calibration parameters and the complex interaction between them make the system behave as a black-box model for any practical purpose. The computation of relevant operator metrics is often stochastic, and it can take several minutes to compute the effect of changing a single, making it impractical to optimize systems with approaches that require a large number of iterations. Operators want to optimize their already deployed system in online scenarios while minimizing the exposure of the system to a negative set of parameters during the optimization procedure. {This paper presents a novel approach to the optimization of a Long Term Evolution (LTE) radio base station in a large search space with an expensive stochastic objective and a limited regret bounds scenario. We show the feasibility of this approach by implementing it in an industrial testing bed radio base station connected to real User Equipment (UE) in collaboration with Ericsson. Two optimization processes in this experimental setup are executed to show the feasibility of the approach in real-world scenarios.",2019 IEEE International Systems Conference (SysCon),2019,10.1109/SYSCON.2019.8836830,202687917,semantic_scholar
5da0b64bded1123cfd8064b60dc78ca2a4a57075,https://www.semanticscholar.org/paper/5da0b64bded1123cfd8064b60dc78ca2a4a57075,OpenWiNo: An open hardware and software framework for fast-prototyping in the IoT,"The Internet of Things promises an always-connected future where the objects surrounding us will communicate in order to make our lives easier, more secure, etc. This evolution is a research opportunity as new solutions must be found to problems ranging from network interconnection to data mining. In the networking community, innovative solutions are being developed for the Device Layer of the Internet of Things, which includes the IoT wireless protocols. In order to study their performance, researchers turn more often to real world platforms, commonly designated by the term “testbeds”, on which they may implement and test the protocols and algorithms. This is even more important in the Industrial IoT field, where environments are perturbed by industrial systems like automated production systems. In this paper, after a brief presentation of the context of testbeds, we introduce WiNo and OpenWiNo, an open hardware and software framework for fast-prototyping in the field of the Internet of Things. Compared to existing platforms, the solution WiNo+OpenWiNo offers a wide array of Physical layers and easy integration of various sensors as it is developed as part of the Arduino ecosystem. It also allows research teams to easily and quickly deploy their own testbed into real environments.",2016 23rd International Conference on Telecommunications (ICT),2016,10.1109/ICT.2016.7500490,25137851,semantic_scholar
25ea28e7137dc82f157f13d5c26208918ecf38c4,https://www.semanticscholar.org/paper/25ea28e7137dc82f157f13d5c26208918ecf38c4,A testbed for collaborative development of underwater communications and networking,"The possibilities opened with the increased use of autonomous underwater vehicles and their potential interactions with existing or prospective submerged sensor networks create an end-user technological pull on the communication capabilities for the underwater domain. Simulation models, while fundamental in the scientific and technological development process cannot offer the feature richness of the physical environment and may potentially mask software and hardware behaviours exposed by the real world. This paper presents a testbed implemented by the NATO STO Centre for Maritime Research and Experimentation (CMRE), deployed to foster cooperative development of underwater communications and networking by providing an “hardware-in-the-world” capability to scientists and engineers. The data collection infrastructure provides a comprehensive data set of environmental measurements relevant to underwater acoustic propagation, arbitrary waveform generation within two frequency bands (useful for channel probing and testing of modulation schemes), full band raw acoustic data recording and access to two sets of fundamentally different commercially available acoustic modems. This structured data collection allows for a comprehensive analysis of the environment variables, their impact on the acoustic channel evolution and how this affects end-to-end connectivity of acoustic modems which can be used to steer the design choices for networking protocols.",MILCOM 2012 - 2012 IEEE Military Communications Conference,2012,10.1109/MILCOM.2012.6415691,11631735,semantic_scholar
431649836fc44c444b0640b0e9efd2e7eb591f1c,https://www.semanticscholar.org/paper/431649836fc44c444b0640b0e9efd2e7eb591f1c,Automatic classification of natural signals for environmental monitoring.,"This manuscript summarizes a three years work addressing the use of machine learning for the automatic analysis of natural signals. The main goal of this PhD is to produce efficient and operative frameworks for the analysis of environmental signals, in order to gather knowledge and better understand the considered environment. Particularly, we focus on the automatic tasks of detection and classification of natural events.This thesis proposes two tools based on supervised machine learning (Support Vector Machine, Random Forest) for (i) the automatic classification of events and (ii) the automatic detection and classification of events. The success of the proposed approaches lies in the feature space used to represent the signals. This relies on a detailed description of the raw acquisitions in various domains: temporal, spectral and cepstral. A comparison with features extracted using convolutional neural networks (deep learning) is also made, and favours the physical features to the use of deep learning methods to represent transient signals.The proposed tools are tested and validated on real world acquisitions from different environments: (i) underwater and (ii) volcanic areas. The first application considered in this thesis is devoted to the monitoring of coastal underwater areas using acoustic signals: continuous recordings are analysed to automatically detect and classify fish sounds. A day to day pattern in the fish behaviour is revealed. The second application targets volcanoes monitoring: the proposed system classifies seismic events into categories, which can be associated to different phases of the internal activity of volcanoes. The study is conducted on six years of volcano-seismic data recorded on Ubinas volcano (Peru). In particular, the outcomes of the proposed automatic classification system helped in the discovery of misclassifications in the manual annotation of the recordings. In addition, the proposed automatic classification framework of volcano-seismic signals has been deployed and tested in Indonesia for the monitoring of Mount Merapi. The software implementation of the framework developed in this thesis has been collected in the Automatic Analysis Architecture (AAA) package and is freely available.",,2018,,69764436,semantic_scholar
9a7936acb8420f589e457c9dc2a2035f4ecc2809,https://www.semanticscholar.org/paper/9a7936acb8420f589e457c9dc2a2035f4ecc2809,Mixed Reality Cubicles and Cave Automatic Virtual Environment,"In Cave Automatic Virtual Environments (CAVEs), a computer generated environment is projected all around a user to fully immerse or eliminate all reference to the real world. Typically, Virtual Reality (VR) CAVEs also track and respond to the user's physical orientation, movements and gestures. Mixed reality environments instead focus on combining real world objects with computer generated ones. In this paper, we focus on the application of Augmented Reality (AR) as a mixed reality technology via (or to) mobile devices such as head-mounted devices, smart-phones and tablets. We present the development of mixed reality applications for mobile (smart-phone and tablet) devices leading up to the implementation of an mixed reality (AR) cubicle for immersive Three Dimensional (3D) visualizations. We also present the results of a study on the familiarity with both VR and AR technologies among students from two institutions of tertiary education. The paper concludes with a discussion of planned deployment and upgrade of mixed reality cubicles using mobile VR equipment.",2016 15th International Conference on Ubiquitous Computing and Communications and 2016 International Symposium on Cyberspace and Security (IUCC-CSS),2016,10.1109/IUCC-CSS.2016.009,1814688,semantic_scholar
711e2a75dd57d2ae372a806a215f80dd8995c6e9,https://www.semanticscholar.org/paper/711e2a75dd57d2ae372a806a215f80dd8995c6e9,CAn't Touch This: Practical and Generic Software-only Defenses Against Rowhammer Attacks,"Rowhammer is a hardware bug that can be exploited to implement privilege escalation and remote code execution attacks. Previous proposals on rowhammer mitigation either require hardware changes or follow heuristic-based approaches (based on CPU performance counters). To date, there exists no instant protection against rowhammer attacks on legacy systems. 
In this paper, we present the design and implementation of two practical and efficient software-only defenses against rowhammer attacks. Our defenses prevent the attacker from leveraging rowhammer to corrupt physically co-located data in memory that is owned by a different system entity. Our first defense, B-CATT, extends the system bootloader to disable vulnerable physical memory. B-CATT is highly practical, does not require changes to the operating system, and can be deployed on virtually all x86-based systems. While B-CATT is able to stop all known rowhammer attacks, it does not yet tackle the fundamental problem of missing memory isolation in physical memory. To address this problem, we introduce our second defense G-CATT, a generic solution that extends the physical memory allocator of the OS to physically isolate the memory of different system entities (e.g., kernel and user space). 
As proof of concept, we implemented B-CATT on x86, and our generic defense, G-CATT, on x86 and ARM to mitigate rowhammer-based kernel exploits. Our extensive evaluation shows that both mitigation schemes (i) can stop available real- world rowhammer attacks, (ii) impose virtually no run-time overhead for common user and kernel benchmarks as well as commonly used applications, and (iii) do not affect the stability of the overall system.",ArXiv,2016,,8960214,semantic_scholar
eb4bd8ff9ac68485fb8f0f55cb7bd487a348ad40,https://www.semanticscholar.org/paper/eb4bd8ff9ac68485fb8f0f55cb7bd487a348ad40,Smart Connected Buildings Design Automation: Foundations and Trends,"Buildings are the result of a complex integration of multi-physics subsystems. Besides the obvious civil engineering infrastructure, thermal, electrical, mechanical, control, communication and computing subsystems must co-exist and be operated so that the overall operation is smooth and efficient. This is particularly important for commercial buildings but is also very relevant for residential buildings especially apartment buildings. Unfortunately, the design and deployment of these subsystems is rarely synchronized: lighting, security, heating, ventilation and air conditioning systems are often designed independently. However, simply putting together a collection of sub-systems, albeit optimized, has led to the inefficient buildings of today. Worldwide, buildings consume 42% of all electrical power - more than any other asset - and it can be proven that much of this can be reduced if a holistic approach to design, deployment, and operation is taken. Government agencies, academic institutions, building contractors and owners have realized the significant impact of buildings on the global environment, the electrical grid, and the mission of their organizations. However, the economic impact for all constituencies is still difficult to assess. Government regulations can play a fundamental role, as it has been the case for the transportation industry where regulations on emission and fuel consumption have been the single most important factor of innovation in automotive design. We are convinced that by leveraging technology and utilizing a system-level approach to buildings, they will provide comfort, safety and functionality while minimizing energy cost, supporting a robust electric grid and mitigating environmental impact. Realizing this vision requires adding intelligence from the beginning of the design phase, to deployment, from commissioning to operation, all the way to the end of the building's life cycle. In this issue, we attempt to provide an as-complete-as-possible overview of the activities in the field of smart connected building design automation that attempts to make the vision a reality. The overarching range of such activities includes developing simulation tools for modeling and the design of buildings, and consequently control algorithms proposed to make buildings smarter and more efficient. Furthermore, we will review real-world and large-scale implementation of such control strategies on physical buildings. We then present a formal co-design methodology to design buildings, taking the view that buildings are prime examples of cyber-physical systems where the virtual and physical worlds meet as more traditional products such as thermostats are able to connect online and perform complicated computational tasks to control building temperature effectively. We complete the presentation describing the growing role of buildings in the operation of the smart grid where buildings are not only consumers of energy, but are themselves also providers of services and energy to the grid. The audiences for this monograph are industry professionals and researchers who work in the area of smart buildings, smart cities, and smart grid, with emphasis on energy efficiency, simulation tools, optimal control, and cyber-physical systems for the emerging power markets.",Found. Trends Electron. Des. Autom.,2016,10.1561/1000000043,28198344,semantic_scholar
8646ca3ed644b8ae61f9432f28d5e025cb1f18ea,https://www.semanticscholar.org/paper/8646ca3ed644b8ae61f9432f28d5e025cb1f18ea,Green city: A low-cost testbed for distributed control algorithms in Smart Grid,"As a type of Cyber-Physical Systems (CPSs), Smart Grid has been adding more communication and control capabilities to improve power efficiency and availability. Especially, more and more distributed control algorithms have been developed for Smart Grids because of their flexibility and robustness. In order to deploy them in real electric power systems, distributed control algorithms must be tested, not only in theoretical simulations, but also in testbeds subject to real world constraints that can provide feedback to make the algorithm robust. Implementations of these algorithms in a Smart Grid environment are facing many cyber-physical challenges such as possible communication failures or imperfections, noisy signals, etc. These challenges can lead to increasing economical expenditure or cause failure of the power system. There exist different approaches for testing distributed control algorithms, from using state-of-the-art facilities to software or hardware-in-the-loop simulations. To better emulate real-world electric grid operation scenarios with low capital investment, in this paper the Green City (GC) testbed is proposed as a suitable platform for both control theory researchers in Smart Grid, and for engineering education, allowing students to learn through hands-on experiences. GC has been conceived as a multi-agent networked CPS with the following main features: 1- Smart Grid environment emulation with low-cost physical elements; 2- Fast prototyping capability of distributed control algorithms for Smart Grid.",IECON 2015 - 41st Annual Conference of the IEEE Industrial Electronics Society,2015,10.1109/IECON.2015.7392385,13411995,semantic_scholar
e7dd4afd555002393eb68db7f77ee2e1c17e9866,https://www.semanticscholar.org/paper/e7dd4afd555002393eb68db7f77ee2e1c17e9866,An evolving multi-agent scenario generation framework for simulations in preventive medicine education,"We describe the design, implementation and evaluation of a novel multi-agent scenario generation framework for interactive virtual reality simulations towards preventive medicine education. Our scenario generation framework is based on recordings of human movements from a distributed sensor networks deployed in a real-world physical setting. The components of our framework include the generation of unique virtual agent behaviors from the sensor data, and algorithms for the generation of low level or gross movement behaviors such as path determination, directional traffic flows, collision avoidance and overtaking. The framework also includes the generation of high level fine actions for multi-agents such as techniques for interactive activities in pedagogical scenarios based on environment and temporal triggers. We applied our multi-agent scenario generation framework in an interactive simulation for hand hygiene education, and conduct an initial usability study to assess the educational benefits of the simulation to nursing students and evaluated the performance characteristics of our framework. Results of our quantitative and qualitative evaluations suggest that our framework was robust in creating engaging, compelling, and realistic interactive training scenarios with multiple virtual agents in simulated hospital situations.",IHI '12,2012,10.1145/2110363.2110392,16570583,semantic_scholar
55b4107c8d37629d0378671324f56f9e801a6d4e,https://www.semanticscholar.org/paper/55b4107c8d37629d0378671324f56f9e801a6d4e,Efficient Long-Term Degradation Profiling in Time Series for Complex Physical Systems,"The long term operation of physical systems inevitably leads to their wearing out, and may cause degradations in performance or the unexpected failure of the entire system. To reduce the possibility of such unanticipated failures, the system must be monitored for tell-tale symptoms of degradation that are suggestive of imminent failure. In this work, we introduce a novel time series analysis technique that allows the decomposition of the time series into trend and fluctuation components, providing the monitoring software with actionable information about the changes of the system's behavior over time. We analyze the underlying problem and formulate it to a Quadratic Programming (QP) problem that can be solved with existing QP-solvers. However, when the profiling resolution is high, as generally required by real-world applications, such a decomposition becomes intractable to general QP-solvers. To speed up the problem solving, we further transform the problem and present a novel QP formulation, Non-negative QP, for the problem and demonstrate a tractable solution that bypasses the use of slow general QP-solvers. We demonstrate our ideas on both synthetic and real datasets, showing that our method allows us to accurately extract the degradation phenomenon of time series. We further demonstrate the generality of our ideas by applying them beyond classic machine prognostics to problems in identifying the influence of news events on currency exchange rates and stock prices. We fully implement our profiling system and deploy it into several physical systems, such as chemical plants and nuclear power plants, and it greatly helps detect the degradation phenomenon, and diagnose the corresponding components.",KDD,2015,10.1145/2783258.2788572,15867910,semantic_scholar
4dd4afbb17999bdf9e218001e3a6ae2252c10f8f,https://www.semanticscholar.org/paper/4dd4afbb17999bdf9e218001e3a6ae2252c10f8f,Visualizing UAS-collected imagery using augmented reality,"One of the areas where augmented reality will have an impact is in the visualization of 3-D data. 3-D data has traditionally been viewed on a 2-D screen, which has limited its utility. Augmented reality head-mounted displays, such as the Microsoft HoloLens, make it possible to view 3-D data overlaid on the real world. This allows a user to view and interact with the data in ways similar to how they would interact with a physical 3-D object, such as moving, rotating, or walking around it. A type of 3-D data that is particularly useful for military applications is geo-specific 3-D terrain data, and the visualization of this data is critical for training, mission planning, intelligence, and improved situational awareness. Advances in Unmanned Aerial Systems (UAS), photogrammetry software, and rendering hardware have drastically reduced the technological and financial obstacles in collecting aerial imagery and in generating 3-D terrain maps from that imagery. Because of this, there is an increased need to develop new tools for the exploitation of 3-D data. We will demonstrate how the HoloLens can be used as a tool for visualizing 3-D terrain data. We will describe: 1) how UAScollected imagery is used to create 3-D terrain maps, 2) how those maps are deployed to the HoloLens, 3) how a user can view and manipulate the maps, and 4) how multiple users can view the same virtual 3-D object at the same time.",Defense + Security,2017,10.1117/12.2262864,125964869,semantic_scholar
1214515daae90180cf912789250888c6233f4d07,https://www.semanticscholar.org/paper/1214515daae90180cf912789250888c6233f4d07,Three-dimensional wireless ad hoc and sensor networks 2016,"Distinct difference between twoand three-dimensional spaces has led to new research challenges to provide self-organizing communications for wireless ad hoc and sensor networks. Therefore, conventional approaches to extend or modify the existing schemes in twodimensional space cannot meet the specific requirements for three-dimensional networks. Instead, a new design and its implementation are usually demanded to accelerate deployment in real world. Based on research motivation, our previous Special Issue in 2014, ‘‘Three-dimensional wireless ad hoc and sensor networks,’’ seems successful in points of presenting the existing research efforts and attracting the interests from the community. In accordance with achievement, we intend to organize the second Special Issue for the same research area. While previous Special Issue was supposed to address the fundamental design issue, more practical approaches which contribute to advances in three-dimensional wireless ad hoc and sensor networks are our major objective. While considering our objective, editors believe that this Special Issue provides collection of articles on networking technique in three-dimensional ad hoc and sensor networks. We have selected 7 valuable papers out of 12 submissions in several aspects such as relevance to Special Issue and novelty of solution. The topic of these papers is roughly categorized into following areas: error detecting, localization, routing protocol, application, and simulation tool. In the first paper titled ‘‘A hybrid decoding of Reed– Muller codes,’’ Shuang Li et al. proposed hybrid decoding algorithm for Reed–Muller (RM) codes to decrease the number of floating-point multiplications significantly. The proposed algorithm reduced computational complexity for decoding of RM codes by terminating recursion procedure in earlier stage. A simplified maximum-likelihood (ML) decision based on fast Hadamard transform (FHT) is another source of low complexity. Simulation results were given to prove the improved performance of error correction as compared to conventional algorithms. Next two papers are related to localization problem. One is for Bluetooth and the other for wireless sensor networks. In the second paper titled ‘‘Three-dimensional positioning system using Bluetooth low-energy beacons,’’ Hyunwook Park et al. introduced threedimensional positioning scheme for Bluetooth. The proposed scheme used Bluetooth low-energy (BLE) beacons to estimate the distance and calculated threedimensional coordinates based on three-dimensional triangulation. A proposed scheme measures threedimensional location of moving nodes by employing four fixed position beacon nodes to form random sphere to collect position of each node. Simulation results reveal that the proposed method can reduce distance error rate rather than the existing twodimensional triangulation method. In the third paper titled ‘‘A distance-based maximum likelihood estimation method for sensor localization in wireless sensor networks,’’ Jing Xu et al. studied node localization in wireless sensor networks since conventional maximumlikelihood estimation (MLE) scheme based on received signal strength indicator (RSSI) failed to reflect the physical characteristics properly. To address this issue, in this paper, distance-based MLE (DB-MLE) to consider measurement errors was formulated as a complicated nonlinear optimization problem. Furthermore, two solutions based on first-order optimal condition and two-dimensional search method were presented. Simulation results showed that DB-MLE provided higher localization accuracy than the other methods. In the fourth paper titled ‘‘Autonomous drone for delay-tolerant networks in indoor applications,’’ Rados1aw O. Schoeneich et al. introduced interesting idea and application of autonomous drones as mobile message ferries in delay-tolerant networks. In order to prove applicability, universal software architecture of drone based on Android devices and its detail prototype were presented. This implementation was tested with the autonomous movement and observed to pass all relevant tests. In the fifth paper titled ‘‘Cooperative downloading in mobile ad hoc networks: a cost-energy perspective,’’ He Li et al. presented another interesting application, cooperative downloading. To improve file downloading, contents are distributed by the help of mobile",Int. J. Distributed Sens. Networks,2017,10.1177/1550147717715974,13050478,semantic_scholar
497ed558a130464edf5ae4b974e35cb6b374a54d,https://www.semanticscholar.org/paper/497ed558a130464edf5ae4b974e35cb6b374a54d,An Active Learning Environment to Improve First-Year Mechanical Engineering Retention Rates and Software Skills,"This work proposes a foundational change from traditional lecture to an active learning environment in the Colorado State University First-Year Introduction to Mechanical Engineering course of 145 students. The goal of this approach is to improve computational capabilities in Mechanical Engineering and long-term retention rates with a single broad emphasis. Major and minor changes were implemented in the course, from specific day to day in-class activities to the addition of laboratory sessions to replace traditional classroom lecture. These laboratories of no more than fifteen students were delivered by Learning Assistants, which were upper-level undergraduate peer educators. To evaluate proficiency, a MATLAB post-test was delivered to students who were instructed through lecture only (“Lecture”) and those who were instructed with the above changes (“Active”). A survey was also provided upon completion of the course to the Active group for student reflection on their perceived software capability and the usefulness of approaches. Post-test results suggest that the Active group was more proficient in MATLAB than the Lecture group. Survey results suggest that the Active group recognize they had not achieved expert use of the software but that they were likely to use it throughout their careers and that all approaches were useful, in particular the use of Learning Assistants. Future longterm retention statistics will shed light on the possible effectiveness of this approach, which are currently unavailable. Introduction Colorado State University has a total student enrollment in excess of 33,000. As a land grant university, the historic mission of the institution is to provide students with an education in practical fields such as agriculture and engineering. The College of Engineering has a growing student cohort, with an increase from ~450 first-year students Fall 2010 to ~600 students Fall 2015 [1]. However, persistence and graduation rates have remained fairly steady over the last fifteen years. The current six year persistence rate within the college is only ~45% and the six year graduation rate within the college is similar at ~43%. Many students do not remain within the college for even a full year, as the second fall persistence rate is only 70-75% [1]. These data show a significant portion of enrolled first-year engineering students do not remain within the program long enough to be exposed to foundational engineering content, which starts in the sophomore year with engineering specific courses. A current goal of the college is to improve these retention statistics. Additionally, many students do not develop the necessary software skills required to use computational tools such as MATLAB, which are integral to success in the curriculum. Students who do not develop these skills during introductory coursework must “catch up” in later courses, where the technical content is more challenging. We hypothesize this can lead to unpreparedness for challenging content or careers as an engineer and can negatively impact academic standing, leading to decreased retention. Thus, the goals of this work were to 1) improve retention rates for first-year engineering students, specifically mechanical engineering, and 2) improve computational and software skills of first-year students, specifically MATLAB and Microsoft Excel. MATLAB is a common computational package which can be used for a broad range of engineering problems throughout a curriculum [2]. However, learning Excel and MATLAB through lecture is challenging, as these tools are best understood through utilization, not observation [3]. MATLAB and other computational tools are often taught in classrooms with computational equipment, however this is can be a challenge with a large classroom [4]. Some have utilized computer based tutorials which students can complete on their own time [5], while others implemented a large scale deployment of personal computers equipped with MATLAB and other software [6]. Additionally, the use of peer-educators can be an effective approach to facilitating MATLAB development [7]. Thus, we have chosen to employ an approach which utilizes an active environment to learn MATLAB and other introductory content through the use of laboratory sessions and peer-educators, in this case the Learning Assistant model [8]. Similar to previous approaches, we have utilized classroom lectures, hands on in-class activities, and laboratory sessions [9]. The Introduction to Mechanical Engineering Course (MECH 103) was developed to provide students with an overview of the mechanical engineering discipline and as an introduction to the computational packages MATALB [10] and Microsoft Excel. The course consists of between 140 and 250 first-year students and was previously delivered using traditional lecture. While this approach was most efficient for a single instructor due to the enrollment size, this resulted in a static learning environment for a course which should excite students about mechanical engineering and provide foundational technical skills. The overall approach to this work was to thus create an active environment for students within the course, which had an enrollment of 145 students for the Fall 2016 semester. The rationale to this approach was that by providing students with hands-on experiences working with mechanical engineering problems and computational software, the understanding of course content will improve [11,12] whereby improving retention [13]. While some immediate test and survey data were acquired and are shown in this work, it is important to note that the true impact on retention is not currently recognizable and will require future analysis. In-Class Sessions Class sessions were varied throughout the semester and the week, as they typically included lectured course material, guest lectures or panels, and activities. The course met Monday, Wednesday, and Friday from 9-9:50 AM in a large lecture hall with individual stadium seating. Friday lecture was often cancelled and this time was spent in weekly laboratory sessions instead, which are outlined in the next section. Monday class time was assigned to covering course content through lecture, teamwork activities, and in class problems. The content of the course included general introductory material such as teamwork, communication, and design, commonly used units and unit conversions, mathematical models and systems, and an introduction to Microsoft Excel and MATLAB. Active engagement in the class included a teamwork design problem, requiring students to break into groups of three. Due to the theater seating layout of the classroom, groups of four or more made successful teamwork and communication difficult. Each group of students were provided one piece of 8.5” x 11” blank printer paper, one paperclip, and two pieces of scotch tape. The design problem was simple: build the tallest free standing structure possible using only the given materials. This was an inexpensive and simple approach to teamwork design activity. In place of a lecture or even a discussion on how to use design techniques for a simple problem such as this, students were able to actively engage in this process despite the difficulties of class size and layout. While students typically have an excellent understanding of units such as a pound (lb), their physical understanding of units such as a Joule or Watt are less developed within the context of everyday life. To provide students with a meaningful representation of energy (Joule) and power (Watt), they were provided a common object – in this case a softball – and asked to calculate how high they would have to raise the object to exert one Joule of energy – in this case roughly a foot and a half. While simple and inexpensive, this activity provided students with useful knowledge they can apply without a calculator and helps them relate coursework to the real world. For example, if they can place a Joule into real-world context, they could then answer the question “Can I launch a rocket into space using a thousand Joules?”. Wednesday lecture sessions were commonly used for guest lecturers and panels. These class sessions included the College of Engineering Dean, faculty members and graduate students in mechanical engineering, industry panelists, entrepreneurs and small business owners, and an interactive teamwork theatre troupe. The goal of these sessions was to provide students with a broad overview of different disciplines within mechanical engineering and what skills are necessary to succeed in various professional roles. While emphasizing an active learning environment is inherently difficult with each and every guest, student engagement was addressed by delivering variability in all of the presentations and strongly encouraging students to ask questions. For example, the theater troupe was an interactive experience where students were able to act as a team member within a group that mocked to show a diverse team struggling with communication. This session involved humor, discussion, and lively responses from students in place of a traditional static lecture. Laboratory Sessions In place of Friday lecture, students were asked to attend laboratory sessions for one hour [14,3]. A total of eleven sessions were provided throughout the week to accommodate all schedules. Sessions included one instructor, 13-16 students, and were held in laboratories with individual workstations with Microsoft Excel and MATLAB software. Laboratory instructors included a Graduate Teaching Fellow and Undergraduate Learning Assistants (LAs). Laboratory sessions involved a short (<5 minutes) lecture briefly reviewing content from class before students began working on assigned problems. These problems implemented course content such as the use of Excel or MATLAB to analyze and display data through real-world applications. An example of utilizing MATLAB to simulate rolling a die is p",,2017,10.18260/1-2--27546,33114557,semantic_scholar
e5b1b40f91cc77ffe862a7b220483f1a8660e0bf,https://www.semanticscholar.org/paper/e5b1b40f91cc77ffe862a7b220483f1a8660e0bf,Towards middleware security framework for next generation data centers connectivity,"Data Center as a Service (DCaaS) facilitates to clients as an alternate outsourced physical data center, the expectations of business community to fully automate these data centers to run smoothly. Geographically Distributed Data Centers and their connectivity has major role in next generation data centers. In order to deploy the reliable connections between Distributed Data Centers, the SDN based security and logical firewalls are attractive and enviable. We present the middleware security framework for software defined data centers interconnectivity, the proposed security framework will be based on some learning processes, which will reduce the complexity and manage very large number of secure connections in real-world data centers. In this paper we will focus on two main objectives; (1) proposing simple and yet scalable techniques for security and analysis, (2) Implementing and evaluating these techniques on real-world data centers.",2015 Science and Information Conference (SAI),2015,10.1109/SAI.2015.7237308,5713100,semantic_scholar
391f0e6d14e1b98ff759891c72bb45f5b7323792,https://www.semanticscholar.org/paper/391f0e6d14e1b98ff759891c72bb45f5b7323792,Data warehousing fundamentals for IT professionals,"Preface. Part 1 OVERVIEW AND CONCEPTS. 1 The Compelling Need for Data Warehousing. Chapter Objectives. Escalating Need for Strategic Information. Failures of Past Decision Support Systems. Operational Versus Decision-Support Systems. Data Warehousing--The Only Viable Solution. Data Warehouse Defined. The Data Warehousing Movement. Evolution of Business Intelligence. Chapter Summary. Review Questions. Exercises. 2 Data Warehouse: The Building Blocks. Chapter Objectives. Defining Features. Data Warehouses and Data Marts. Architectural Types. Overview of Components. Metadata in the Data Warehouse. Chapter Summary. Review Questions. Exercises. 3 Trends in Data Warehousing. Chapter Objectives. Continued Growth in Data Warehousing. Vendor Solutions and Products. Significant Trends. Emergence of Standards. Web-Enabled Data Warehouse. Chapter Summary. Review Questions. Exercises. Part 2 PLANNING AND REQUIREMENTS. 4 Planning and Project Management. Chapter Objectives. Planning Your Data Warehouse. The Data Warehouse Project. The Project Team. Project Management Considerations. Chapter Summary. Review Questions. Exercises. 5 Defining the Business Requirements. Chapter Objectives. Dimensional Analysis. Information Packages A Useful Concept. Requirements Gathering Methods. Requirements Definition: Scope and Content. Chapter Summary. Review Questions. Exercises. 6 Requirements as the Driving Force for Data Warehousing. Chapter Objectives. Data Design. The Architectural Plan. Data Storage Specifications. Information Delivery Strategy. Chapter Summary. Review Questions. Exercises. Part 3 ARCHITECTURE AND INFRASTRUCTURE. 7 The Architectural Components. Chapter Objectives. Understanding Data Warehouse Architecture. Distinguishing Characteristics. Architectural Framework. Technical Architecture. Architectural Types. Chapter Summary. Review Questions. Exercises. 8 Infrastructure as the Foundation for Data Warehousing. Chapter Objectives. Infrastructure Supporting Architecture. Hardware and Operating Systems. Database Software. Collection of Tools. Data Warehouse Appliances. Chapter Summary. Review Questions. Exercises. 9 The Significant Role of Metadata. Chapter Objectives. Why Metadata is Important. Metadata Types by Functional Areas. Business Metadata. Technical Metadata. How to Provide Metadata. Chapter Summary. Review Questions. Exercises. Part 4 DATA DESIGN AND DATA PREPARATION. 10 Principles of Dimensional Modeling. Chapter Objectives. From Requirements to Data Design. The STAR Schema. STAR Schema Keys. Advantages of the STAR Schema. STAR Schema: Examples. Chapter Summary. Review Questions. Exercises. 11 Dimensional Modeling: Advanced Topics. Chapter Objectives. Updates to the Dimension Tables. Miscellaneous Dimensions. The Snowflake Schema. Aggregate Fact Tables. Families of STARS. Chapter Summary. Review Questions. Exercises. 12 Data Extraction, Transformation, and Loading. Chapter Objectives. ETL Overview. Data Extraction. Data Transformation. Data Loading. ETL Summary. Other Integration Approaches. Chapter Summary. Review Questions. Exercises. 13 Data Quality: A Key to Success. Chapter Objectives. Why is Data Quality Critical?. Data Quality Challenges. Data Quality Tools. Data Quality Initiative. Master Data Management (MDM). Chapter Summary. Review Questions. Exercises. Part 5 INFORMATION ACCESS AND DELIVERY. 14 Matching Information to the Classes of Users. Chapter Objectives. Information from the Data Warehouse. Who Will Use the Information? Information Delivery. Information Delivery Tools. Information Delivery: Special Topics. Chapter Summary. Review Questions. Exercises. 15 OLAP in the Data Warehouse. Chapter Objectives. Demand for Online Analytical Processing. Major Features and Functions. OLAP Models. OLAP Implementation Considerations. Chapter Summary. Review Questions. Exercises. 16 Data Warehousing and the Web. Chapter Objectives. Web-Enabled Data Warehouse. Web-Based Information Delivery. OLAP and the Web. Building a Web-Enabled Data Warehouse. Chapter Summary. Review Questions. Exercises. 17 Data Mining Basics. Chapter Objectives. What is Data Mining?. Major Data Mining Techniques. Data Mining Applications. Chapter Summary. Review Questions. Exercises. Part 6 IMPLEMENTATION AND MAINTENANCE. 18 The Physical Design Process. Chapter Objectives. Physical Design Steps. Physical Design Considerations. Physical Storage. Indexing the Data Warehouse. Performance Enhancement Techniques. Chapter Summary. Review Questions. Exercises. 19 Data Warehouse Deployment. Chapter Objectives. Data Warehouse Testing. Major Deployment Activities. Considerations for a Pilot. Security. Backup and Recovery. Chapter Summary. Review Questions. Exercises. 20 Growth and Maintenance. Chapter Objectives. Monitoring the Data Warehouse. User Training and Support. Managing the Data Warehouse. Chapter Summary. Review Questions. Exercises. Answers to Selected Exercises. Appendix A. Project Life Cycle Steps and Checklists. Appendix B. Critical Factors for Success. Appendix C. Guidelines for Evaluating Vendor Solutions. Appendix D. Highlights of Vendors and Products. Appendix E. Real-World Examples of Best Practices. References. Glossary. Index.",,2010,,60064398,semantic_scholar
47486d010d940cc014bd1df89974b53bd73e91a2,https://www.semanticscholar.org/paper/47486d010d940cc014bd1df89974b53bd73e91a2,Graph Analysis of Fog Computing Systems for Industry 4.0,"Increased adoption of Fog Computing concepts into Cyber Physical Systems (CPS) is a driving force for implementing Industry 4.0. The modern industrial environment focuses on providing a flexible factory floor that suits the needs of modern manufacturing through the reduction of downtimes, reconfiguration times, adoption of new technologies and the increase of its production capabilities and rates. Fog Computing through CPS aims to provide a flexible orchestration and management platform that can meet the needs of this emerging industry model. Proposals on Fog Computing platform and Software Defined Networks (SDN) for Industry allow for resource virtualization and access throughout the system enabling large composite application systems to be deployed on multiple nodes. The increase of reliability, redundancy and runtime parameters as well as the reduction of costs in such systems are of key interest to Industry and researchers as well. The development of optimization algorithms and methods is made difficult by the complexity of such systems and the lack of real-world data on fog systems resulting in algorithms that are not being designed for real world scenarios. We propose a set of use-case scenarios based on our Industrial partner that we analyze to determine the graph based parameters of the system that allows us to scale and generate a more realistic testing scenario for future optimization attempts as well as determine the nature of such systems in comparison to other networks types. To show the differences between these scenarios and our real-world use-case we have selected a set of key graph characteristics based on which we analyze and compare the resulting graphs from the systems.",2017 IEEE 14th International Conference on e-Business Engineering (ICEBE),2017.0,10.1109/ICEBE.2017.17,8296615,semantic_scholar
5f5c5c44c2bff3f2f9e4d53191cbc6d0a67839d9,https://www.semanticscholar.org/paper/5f5c5c44c2bff3f2f9e4d53191cbc6d0a67839d9,Supervisory control theory for controlling swarm robotics systems,"Swarm robotics systems have the potential to tackle many interesting problems. Their control software is mostly created by ad-hoc development. This makes it hard to deploy 
swarm robotics systems in real-world scenarios as it is difficult to analyse, maintain, or extend these systems. Formal methods can contribute to overcome these problems. 
However, they usually do not guarantee that the implementation matches the specification because the system’s control code is typically generated manually. 
 
This thesis studies the application of the supervisory control theory (SCT) framework in swarm robotics systems. SCT is widely applied and well established in the man- 
ufacturing context. It requires the system and the desired behaviours (specifications) to be defined as formal languages. In this thesis, regular languages are used. Regular languages, in the form of deterministic finite state automata, have already been widely applied for controlling swarm robotics systems, enabling a smooth transition from the ad-hoc development currently in practice. This thesis shows that the control code for 
swarm robotics systems can be automatically generated from formal specifications. 
 
Several case studies are presented that serve as guidance for those who want to learn how to specify swarm behaviours using SCT formally. The thesis provides the tools for 
the implementation of controllers using formal specifications. Controllers are validated on swarms of up to 600 physical robots through a series of systematic experiments. 
 
It is also shown that the same controllers can be automatically ported onto different robotics platforms, as long as they offer the required capabilities. The thesis extends and incorporates techniques to the supervisory control theory framework; specifically, the concepts of global events and the use of probabilistic generators. It can be seen as a step towards making formal methods a standard practice in swarm robotics.",,2016.0,,117662445,semantic_scholar
441730142e0ac938f9d656c3ada16118fdcbd6bb,https://www.semanticscholar.org/paper/441730142e0ac938f9d656c3ada16118fdcbd6bb,Usermode OS components on seL4 with rump kernels,"seL4 is a formally-verified high-assurance microkernel that provides isolation to properly designed applications that it executes. Real-world cyber-physical systems can use seL4 for increased security. Many applications rely on the operating system to provide system services, such as device drivers, file systems and networking capabilities, however seL4 only provides these in a limited capacity which limits its deployment. Adding support to this wide array of systems that can benefit from the additional security seL4 provides would require reimplementing the millions of lines of operating system code that these systems require. This is infeasible without an approach that reuses existing components. Current methods either require providing services by running a paravirtualised version of the Linux kernel which provides only coarse isolation or by developing specific services on an as-needed basis which does not scale for many devices. Rump kernels are a NetBSD project for running NetBSD system services in different environments such as in user-mode on a microkernel. This thesis evaluates rump kernels as an approach to provide driver-like operating system components in user-mode on the seL4 microkernel. This will be achieved by adding a seL4 platform to the Rumprun unikernel, an existing project that uses rump kernels. We evaluate our implementation to compare its performance with other software systems and to investigate the level of overhead our implementation adds. We also show that the effort required to use rump kernels is low and by using NetBSD system services we increase the amount of devices that can be used with seL4. This thesis contains background information and related work, details on our design and implementation and our evaluation, future work and conclusion.",,2016.0,,29917336,semantic_scholar
ca54656aecac98f2c688e973e92bfdcf15b04efc,https://www.semanticscholar.org/paper/ca54656aecac98f2c688e973e92bfdcf15b04efc,Multiport antenna performance analysis from ray-traced channels for small cells,"The evaluation of multi-element antenna (MEA) performance in urban environments is particularly important for the planning and design of reliable, spectrally efficient systems. To date, MIMO communication using large dimensions has not been implemented for commercial systems. But the potential capacity of MIMO is much more significant when the MIMO dimensions become large. The hold-up is not just the shortfall in understanding how to deploy large-dimension MIMO, but also how to design the antenna systems. There is no widely agreed method for evaluating MEAs used by MIMO communications. Even the performance evaluation of a well-established MIMO configuration, such as the small scale systems already used, remains limited. The evaluation often defers to OTA tests in artificial environmental conditions. The real-world performance remains unclear, so care must be taken in design optimization. In this paper, we discuss the incorporation of some physical modeling of the propagation environment to evaluating the MEA performance for a typical urban wireless channel. A ray-optic method is applicable to any 3D environment, and any number of antennas with arbitrary patterns. In this sense it offers a practical approach for large-dimension MIMO evaluation by being able to change the antennas and the environment, all in software. The evaluation method is illustrated by using an MEA slot cube (12 antennas) operating in a specific environment, namely downtown Ottawa.",The 8th European Conference on Antennas and Propagation (EuCAP 2014),2014.0,10.1109/EUCAP.2014.6902535,26233783,semantic_scholar
e1184cc1e4725f7736d9944a33ada01a626cedc3,https://www.semanticscholar.org/paper/e1184cc1e4725f7736d9944a33ada01a626cedc3,Learning in Robotics,"For a robot, the ability to get from one place to another is one of the most basic skills. However, locomotion on legged robots is a challenging multidimensional control problem. This paper presents a machine learning approach to legged locomotion, with all training done on the physical robots. The main contributions are a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithms for learning quadrupedal locomotion. The resulting learned walk is considerably faster than all previously reported hand-coded walks for the same robot platform. Introduction The ability to deploy a fully autonomous robot in an unstructured, dynamic environment (the proverbial real world) over an extended period of time remains an open challenge in the field of robotics. Considerable progress is being made towards many components of this task including physical agility, power management, and on-board sensor technology. One such component that has drawn considerable interest recently is the ability for a robot to autonomously learn to improve its own performance (Ng et al. 2004; Bagnell & Schneider 2001; Zhang & Vadakkepat 2003). Despite this interest, considerable work remains due to the di fficulties associated with machine learning in the real world . Compared to other machine learning scenarios such as classification or action learning in simulation, learning o n physical robots presents several formidable challenges, i ncluding the following. Sparse Training Data: It is often prohibitively difficult to generate large amounts of data due to the maintenance required on robots, such as battery changes, hardware repairs, and, usually, constant human supervision. Thus, learning methods designed for physical robots must be effective with small amounts of data. Dynamical Complexity: The dynamics of many robotic control tasks are too complex for faithful simulation to be possible. Furthermore, robots are inherently situated in an unstructured environment with unpredictable sensor and actuator noise, namely the real world. Thus, even when off-line simulation is possible, it can never be fully reflective of the target environment. In this paper, we overcome these challenges for one concrete complex robot task, namely legged locomotion. Using a commercially available quadruped robot, we fully automate the training process (other than battery changes) and Copyright c © 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. employ machine learning algorithms that are sufficiently data efficient to enable productive learning on physical robots in a matter of hours. The resulting learned walk is considerably faster than all previously reported hand-cod ed walks for the same robot platform. This paper contributes both a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithms for learning quadrupedal locomotion. The remainder of the paper is organized as follows. First, we introduce the parameterized walk which our learning process seeks to optimize. We then specify our four learning approaches, and follow with detailed empirical results. We close with a discussion of their implications and possible avenues for future work. A Parameterized Walk The Sony Aibo ERS-210A is a commercially available robot that is equipped with a color CMOS camera and an optional ethernet card that can be used for wireless communication. The Aibo is a quadruped robot, and has three degrees of freedom in each of its four legs (Sony 2004). At the lowest level, the Aibo’s gait is determined by a series of joint positions for the three joints in each of its leg s. An early attempt to develop a gait by Hornby et al. (1999) involved using a genetic algorithm to learn a set of lowlevel parameters that described joint velocities and body p osition.1 More recent attempts to develop gaits for the Aibo have involved adopting a higher-level representation that deals with the trajectories of the Aibo’s four feet through three-dimensional space. An inverse kinematics calculati on is then used to convert these trajectories into joint angles . Among higher-level approaches, most of the differences between gaits that have been developed for the Aibo stem from the shape of the loci through which the feet pass and the exact parameterizations of those loci. For example, a team from the University of New South Wales achieved the fastest known hand-tuned gait using the high-level approach described above with trapezoidal loci. They subsequently generated an even faster walk via learning (Kim & Uther 2003). A team from Germany created a flexible gait implementation that allows them to use a variety of different shapes of loci (Rofer et al. 2003), and the team from the University of Newcastle was able to generate highvelocity gaits using a genetic algorithm and loci of arbitra ry shape (Quinlan, Chalup, & Middleton 2003). Our team (UT Austin Villa, Stone t al. 2004) first approached the gait optimization problem by hand-tuning Developed on an earlier version of the Aibo. a gait described by half-elliptical loci. This gait performed comparably to those of other teams participating in RoboCup 2003. The work reported in this paper uses the hand-tuned UT Austin Villa walk as a starting point for learning. Figure 1 compares the reported speeds of the gaits mentioned above, both hand-tuned and learned, including that of our starting point, the UT Austin Villa walk. The latter walk is described fully in a team technical report (Stone et al. 2004). The remainder of this section describes those details of the UT Austin Villa walk that are important to understand for the purposes of this paper. Hand-tuned gaits Learned gaits CMU Austin Villa UNSW Hornby UNSW NUBots (2002) (2003) (2003) (1999) (2003) (2003) 200 245 254 170 270 296 Figure 1: Maximum forward velocities of the best gaits (in mm/s) for different teams, both learned and hand-tuned. The half-elliptical locus used by our team is shown in Figure 2. By instructing each foot to move through a locus of this shape, with each pair of diagonally opposite legs in phase with each other and perfectly out of phase with the other two (a gait known as a trot), we enable the Aibo to walk. Four parameters define this elliptical locus: 1. The length of the ellipse; 2. The height of the ellipse; 3. The position of the ellipse on the x axis; and 4. The position of the ellipse on the y axis. Since the Aibo is roughly symz",,2006.0,,14687420,semantic_scholar
83c19e91c197218df688172968455ff9d4efc7fe,https://www.semanticscholar.org/paper/83c19e91c197218df688172968455ff9d4efc7fe,Enhancing liveness testing for transferring data packets through using automatic test packet generation,"-Networks are getting bigger and more complex, yet administrators rely on incomplete tools such as and to debug problems. We propose an automated and systematic approach for testing and rectify networks called “Automatic evaluates Package Generation” (ATPG). ATPG reads router configurations and generates a device-independent model. The model is used to generate a minimum set of test packets to minimally exerting every link in the network or maximally exerting every rule in the network. Test packets are sent periodically, and detected failures trigger a separate mechanism to localize the revoke. ATPG can detect both functional and renderings problems. ATPG complements but goes beyond earlier work in static checking for which cannot detect liveness or performance faults or fault localization which only localize revoke given liveness results. We describe our prototype ATPG implementation and results on two real-world data sets: Stanford University’s backbone network and Internet. We find that a small number of test packets suffice to test all rules in these networks. A sending 4000 test packet 10 times per second consumes less than 1% of link capacity. ATPG code and the datasets are publicly available. Keyword: ATPG, liveness, Networks. ________________________________________________________________________________________________________ I.INTRODUCTION Networking is the word fundamentally cogitates to computers and their property. It is very often used in the world of computers and their use in different connections. The term networking express the link between two or more computers and their tendency, with the vital purpose of sharing the data stored in the computers, with each other. The networks between the computing tendencies are very public these days due to the launch of assorted hardware and computer software which aid in making the activity much more convenient to build and use. Fig: 1.1Structure of Networking between the different computers The discuss about Figure: 1.1 Structure of Networking between the different computer. Its main process of share the Internet to different things and devices. General Network Techniques When computers communicate on a network, they send out information packets without knowing if anyone is listening. Computers in a network all have an attached to the network and that is called to be attached to a network bus. What one computer sends out will reach the other computer on the local area network. © 2017 IJEDR | Volume 5, Issue 1 | ISSN: 2321-9939 IJEDR1701073 International Journal of Engineering Development and Research (www.ijedr.org) 476 Fig: 1.2 the clear idea about the networking functions The discus about figure: 1.2 clear ideas of network function and different computers to be able to distinguish between each other, every computer have unique ID called MAC-address Media Access Control Address. This address is not only unique on your network but unique for all tendencies that can be aquiline up to a network. The MAC-address is tied to the hardware and has nothing to do with IP-addresses. Since all computers on the network graduate inversion that is sent out from all other computers the MACaddresses is primarily used by the computers to filter out incoming network traffic that is addressed to the scratcher computer. When a computer expostulation with another computer on the network, it sends out both the other computers MAC-address and the MACaddress of its own. In that way the receiving computer will not only realize that this parcel is for me but also, who sent this data packet so a return response can be sent to the sender. Ethernet network as delineate here, all computers hear all network aggregation since they are attached to the same bus. This network structure is called multi-drop. One problem with this network structure is that when you have, let say ten computers on a network and they expostulation attendance and due to that they sends out there data packets randomly, collisions occur when two or more computers sends data at the same time. When that happens data gets imperfect and has to be resent. On a network that is heavy loaded even the resent packets collide with other packets and have to be resent again. In reality this soon takes affect an information problem. If respective computers communicate with each other at high speed they may not be able to utilize more than 25% of the total network information measure since the rest of the bandwidth is used for regressive antecedently corrupted packets. The way to minimize this problem is to use network switches. II.RELATED NETWORK Detecting the occurrence and location of performance anomalies is critical to ensuring the effective operation of network infrastructures. In this paper we present a framework for detecting and apposition performance anomalies based on using an active investigate measurement infrastructure deployed on the periphery of a network. Our framework has three components: an algorithm for detection performance oddball on a path, an algorithm for environs which paths to probe at a given time in order to detect performance anomalies where a path is defined as the set of links between two sampling nodes, and an algorithm for designation the links that are causing an identified anomaly on a path The path selection algorithm is designed to enable a interchange between insure that all links in a network are of times monitored to detect performance anomalies, while minimizing probing overhead.[1] This paper, we develop failure-resilient techniques for monitoring link detain and imbecility in a Service Provider or endeavor IP network. Our two-phased approach attempts to minimize both the monitoring infrastructure costs as well as the additional aggregation due to probe messages. In the first phase, we compute the particular point of a minimal set of monitoring stations such © 2017 IJEDR | Volume 5, Issue 1 | ISSN: 2321-9939 IJEDR1701073 International Journal of Engineering Development and Research (www.ijedr.org) 477 that all network links are covered, even in the bigness of several link reverting. Afterwards, in the second phase, we compute a minimal set of probe messages that are transmitted by the stations to measure link delays and isolate network faults these approximation ratios are provably very close to the best possible bounds for any algorithm. [2]We present a new symbolic execution tool, KLEE, capable of automatically induce tests that wangle high coverage on a diverse set of complex and environmentallyintense programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment position on millions of UNIX systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage on average over 90% per tool. [3] The emergence of Open Flow-capable switches enables exciting new network functionality, at the risk of programming errors that make communication less reliable. The centralized programming model, where a single accountant program manages the network, seems to reduce the likelihood of bugs. However, the system is inherently scattered and asynchronous, with events happening at different switches and end hosts, and inevitable delays affecting communication with the controller. In this paper, we present economic, regular techniques for testing unqualified controller programs. Our NICE tool applies model checking to explore the state space of the entire system the controller, the switches, and the hosts. [4] Network performance tomography, characteristics of the network interior, such as link loss and packet latency, is inferred from unrelated end-to-end measurements. Most work to date is based on employ packet level correlations. However, these methods are often limited in scope-multicast is not widely deployed-or require deployment of additional hardware or software system. Some recent work has been successful in reaching a less detailed goal: identifying the lossiest network links using only unrelated end-toend sampling. In this paper, we abstract the properties of network performance that allow this to be done and exploit them with a quick and simple deduction algorithm that, with high likely, separate the worst performing links. [5] III. SYSTEM ANALAYS Automatic Test Packet Generation (ATPG) framework that self-loading generates a minimal set of collection to test the liveness of the underlying topology and the congruence between data plane state and redundancy description. The tool can also automatically generate packets to test performance assertions such as packet latency. It can also be specialized to generate a minimal set of packets that merely test every link for network liveness.  A survey of network operators revealing common failures and root causes.  A test packet generation algorithm.  A revoke localization algorithm to isolate faulty devices and rules.  ATPG use cases for functional and performance testing.  Evaluation of a prototype ATPG system using rule sets collected from the Stanford and Internet2 backbones.",,2017.0,,212505389,semantic_scholar
415584ce811fe9ca946336bff433171088d21da7,https://www.semanticscholar.org/paper/415584ce811fe9ca946336bff433171088d21da7,Measurement-Driven Algorithm and System Design for Wireless and Datacenter Networks,"Measurement-Driven Algorithm and System Design for Wireless and Datacenter Networks Varun Gupta The growing number of mobile devices and data-intensive applications pose unique challenges for wireless access networks as well as datacenter networks that enable modern cloudbased services. With the enormous increase in volume and complexity of traffic from applications such as video streaming and cloud computing, the interconnection networks have become a major performance bottleneck. In this thesis, we study algorithms and architectures spanning several layers of the networking protocol stack that enable and accelerate novel applications and that are easily deployable and scalable. The design of these algorithms and architectures is motivated by measurements and observations in real world or experimental testbeds. In the first part of this thesis, we address the challenge of wireless content delivery in crowded areas. We present the AMuSe system, whose objective is to enable scalable and adaptive WiFi multicast. AMuSe is based on accurate receiver feedback and incurs a small control overhead. This feedback information can be used by the multicast sender to optimize multicast service quality, e.g., by dynamically adjusting transmission bitrate. Specifically, we develop an algorithm for dynamic selection of a subset of the multicast receivers as feedback nodes which periodically send information about the channel quality to the multicast sender. Further, we describe the Multicast Dynamic Rate Adaptation (MuDRA) algorithm that utilizes AMuSe’s feedback to optimally tune the physical layer multicast rate. MuDRA balances fast adaptation to channel conditions and stability, which is essential for multimedia applications. We implemented the AMuSe system on the ORBIT testbed and evaluated its performance in large groups with approximately 200 WiFi nodes. Our extensive experiments demonstrate that AMuSe can provide accurate feedback in a dense multicast environment. It outperforms several alternatives even in the case of external interference and changing network conditions. Further, our experimental evaluation of MuDRA on the ORBIT testbed shows that MuDRA outperforms other schemes and supports high throughput multicast flows to hundreds of nodes while meeting quality requirements. As an example application, MuDRA can support multiple high quality video streams, where 90% of the nodes report excellent or very good video quality. Next, we specifically focus on ensuring high Quality of Experience (QoE) for video streaming over WiFi multicast. We formulate the problem of joint adaptation of multicast transmission rate and video rate for ensuring high video QoE as a utility maximization problem and propose an online control algorithm called DYVR which is based on Lyapunov optimization techniques. We evaluated the performance of DYVR through analysis, simulations, and experiments using a testbed composed of Android devices and off the shelf APs. Our evaluation shows that DYVR can ensure high video rates while guaranteeing a low but acceptable number of segment losses, buffer underflows, and video rate switches. We leverage the lessons learnt from AMuSe for WiFi to address the performance issues with LTE evolved Multimedia Broadcast/Multicast Service (eMBMS). We present the Dynamic Monitoring (DyMo) system which provides low-overhead and real-time feedback about eMBMS performance. DyMo employs eMBMS for broadcasting instructions which indicate the reporting rates as a function of the observed Quality of Service (QoS) for each UE. This simple feedback mechanism collects very limited QoS reports which can be used for network optimization. We evaluated the performance of DyMo analytically and via simulations. DyMo infers the optimal eMBMS settings with extremely low overhead, while meeting strict QoS requirements under different UE mobility patterns and presence of network component failures. In the second part of the thesis, we study datacenter networks which are key enablers of the end-user applications such as video streaming and storage. Datacenter applications such as distributed file systems, one-to-many virtual machine migrations, and large-scale data processing involve bulk multicast flows. We propose a hardware and software system for enabling physical layer optical multicast in datacenter networks using passive optical splitters. We built a prototype and developed a simulation environment to evaluate the performance of the system for bulk multicasting. Our evaluation shows that the optical multicast architecture can achieve higher throughput and lower latency than IP multicast and peer-to-peer multicast schemes with lower switching energy consumption. Finally, we study the problem of congestion control in datacenter networks. Quantized Congestion Control (QCN), a switch-supported standard, utilizes direct multi-bit feedback from the network for hardware rate limiting. Although QCN has been shown to be fastreacting and effective, being a Layer-2 technology limits its adoption in IP-routed Layer 3 datacenters. We address several design challenges to overcome QCN feedback’s Layer2 limitation and use it to design window-based congestion control (QCN-CC) and load balancing (QCN-LB) schemes. Our extensive simulations, based on real world workloads, demonstrate the advantages of explicit, multi-bit congestion feedback, especially in a typical environment where intra-datacenter traffic with short Round Trip Times (RTT: tens of μs) run in conjunction with web-facing traffic with long RTTs (tens of milliseconds).",,2017.0,10.7916/D8GB2GH3,57640336,semantic_scholar
5f0a7e99c821847a58157a1c635371f5b5824010,https://www.semanticscholar.org/paper/5f0a7e99c821847a58157a1c635371f5b5824010,fAARS: A Platform for Location-Aware Trans-reality Games,"Users today can easily and intuitively record their real-world experiences through mobile devices, and commodity virtual worlds enable users from around the world to socialize in the context of realistic environments where they simulate real-world activities. This synergy of technological advances makes the design and implementation of trans-reality games, blending the boundaries of the real and virtual worlds, a compelling software-engineering problem. In this paper, we describe fAARS, a platform for developing and deploying trans-reality games that cut across the real and parallel virtual worlds, offering users a range of game-play modalities. We place fAARS in the context of recent related work, and we demonstrate its capabilities by discussing two different games developed on it, one with three different variants.",ICEC,2012.0,10.1007/978-3-642-33542-6_16,12449239,semantic_scholar
d753886538b5a9970a79212f5a995d55a8ee4c33,https://www.semanticscholar.org/paper/d753886538b5a9970a79212f5a995d55a8ee4c33,A AdaFT : A Framework for Adaptive Fault Tolerance for Cyber-Physical Systems,"Cyber-physical systems frequently have to use massive redundancy to meet application requirements for high reliability. While such redundancy is required, it can be activated adaptively, based on the current state of the controlled plant. Most of the time the plant is in a state that allows for a lower level of faulttolerance. Avoiding the continuous deployment of massive fault tolerance will greatly reduce the workload of the CPS, and lower the operating temperature of the cyber sub-system, thus increasing its reliability. In this paper, we extend our prior research by demonstrating a software simulation framework (AdaFT) that can automatically generate the sub-spaces within which our adaptive fault tolerance can be applied. We also show the theoretical benefits of AdaFT, and its actual implementation in several real world CPSs. CCS Concepts: rComputer systems organization→ Embedded systems; Redundancy; Robotics;",,2016.0,,37471369,semantic_scholar
6e7e52c8f59ec975cb9b850cef1ecf8470b9c28a,https://www.semanticscholar.org/paper/6e7e52c8f59ec975cb9b850cef1ecf8470b9c28a,Physical hardware-in-the-loop modelling and simulation,"It is too risky to install a newly-designed device, component, or controller, directly into a real power system without rigorous testing. To help to de-risk the system integration, and to assist in the design process, computer simulation is an accepted and widely-adopted tool. However, in a simulation-only environment, many real-world issues such as noise, randomness of event timings, and hardware design issues are not well explored. In addition, there are limits on the size and fidelity of system which can be simulated, due to the required computational intensity, and because control systems for devices often contain software which is proprietary and cannot be modelled accurately. Physical Hardware in the Loop Simulation provides an interim stage between purely computer-based simulation, and real device deployment. Part of the power system (or “Smart Grid”) is simulated, but specific components are implemented in actual hardware. The hardware may consist of instrumentation, relays or controllers, carrying no primary current. Such testing is termed “Secondary Hardware-in-the-Loop”, as the signals exchanged between the simulation and hardware consist only of measurements and control values. A more advanced environment is created where primary power flow is exchanged with the hardware. This is termed “Primary Hardware-in-the-Loop” or “Power Hardware-in-the-Loop” testing. In addition to measurement and control signals being exchanged with the simulation, an interface is required at which primary power is exchanged between the simulation and the hardware, at the voltage and current levels suitable for the hardware under test. Creation of such environments is complex, but allows steady-state, dynamic, and worst-case scenarios to be re-created in a controlled environment. Therefore hardware-in-the-loop testing offers a cheaper, safer, faster and more comprehensive de-risking process than trying the hardware for the first time on a real network. The complexity and interconnected nature of the Smart Grid means that such Hardware in the Loop based testing is becoming even more critical to understanding the behaviour of systems and schemes, and consequently the safe and secure introduction of new technologies.",,2016.0,10.1002/9781118755471.SGD078,112826757,semantic_scholar
46d8cfaaa23ab333cf43849b0ce9bb58ee87bcd7,https://www.semanticscholar.org/paper/46d8cfaaa23ab333cf43849b0ce9bb58ee87bcd7,Slow wireless communication testbed based on software-defined radio,"The Internet of Things (IoT) extends the virtual cyber world into the real physical world by networking everyday smart physical objects, representing an upgraded version of Internet. The wireless sensor networks (WSN) are playing diverse sensing functions and feeding information from the physical world for IoT. Nowadays, most of the WSNs are deployed to detect slow-changing physical quantities and data from sensor nodes in these WSNs vary very slowly over a long time interval. Accordingly, a low data transmission rate is sufficient for the sensor nodes. Moreover, the low data transmission rate also enables very narrowband (VNB) radio communication with a bandwidth of several kHz to be applied in wireless sensor nodes. 
It is noteworthy that most of wireless sensor nodes are transmitting data in the unlicensed 2.4 GHz frequency band where the dominant interference is characterized by its wideband nature. Therefore, if the very narrowband (VNB) radio is adopted in a wireless sensor node, only a small portion of co-channel wideband interference will overlap with the VNB signal transmitted by the wireless sensor node. Because only a little of the wideband interference is superimposed onto the VNB signal, the VNB signal captured by a receiver has a relatively high signal-to-noise ratio (SNR), making it possible to reduce the power of the transmitter or release the noise of the receiver. 
Power consumption is a key factor that determines the lifetime of a sensor node because most of sensor nodes are powered by batteries. Once a battery is depleted, the lifetime of a sensor node will expire. The radio transceiver on a wireless sensor node consumes a lot of power when it is working but the VNB signal enables the radio transceiver to decrease its transmission power while guaranteeing arbitrarily low bit error ratio (BER). Thus, low power consumption is made possible at the VNB radio transceiver. 
In this research project, the VNB radio transceiver applied in a wireless sensor node is called “slow wireless radio”. The slow wireless radio aims at the wireless sensor nodes that are transmitting data at very-low average bit-rate of 100 bits/s in the 2.4 GHz frequency band full of wideband interference while achieving low power consumption. 
The goal of this project is to build up a point-to-point slow wireless radio communication testbed based on software-defined radio (SDR), where successful VNB wireless communication will be implemented and related communication performances such as signal-to-noise ratio (SNR) and bit error ratio (BER) will be measured in real-time. The testbed will serve as a design reference to investigate the feasibility of the slow-wireless radio communication when it is used in wireless sensor nodes and facilitate the development of a slow-wireless sensor node prototype.",,2017.0,,55730934,semantic_scholar
16af2f3a6d1f07c46bd851aa2899731136fab73e,https://www.semanticscholar.org/paper/16af2f3a6d1f07c46bd851aa2899731136fab73e,Poster: Ziria: language for rapid prototyping of wireless PHY,"Software-defined radio (SDR) brings the flexibility of software to the domain of wireless protocol design, promising an ideal platform both for research and innovation and rapid deployment of new protocols on existing hardware. However, existing SDR programming platforms require either careful hand-tuning of low-level code, negating many of the advantages of software, or are too slow to be useful in the real world. We present Ziria, the first software-defined radio programming platform that is both easily programmable and performant. Ziria introduces a novel programming model tailored to wireless physical layer tasks and captures the inherent and important distinction between data and control paths in this domain. Ziria provides the capability of implementing a real-time WiFi PHY running at 20 MHz.",MobiCom,2014.0,10.1145/2639108.2642893,52007676,semantic_scholar
88aa14a159f0fad0a2b07445c3f091558ffbda62,https://www.semanticscholar.org/paper/88aa14a159f0fad0a2b07445c3f091558ffbda62,Ziria: wireless programming for hardware dummies,"Software-defined radio (SDR) brings the flexibility of software to the domain of wireless protocol design, promising both an ideal platform for research and innovation and the rapid deployment of new protocols on existing hardware. Most existing SDR platforms require careful hand-tuning of low-level code to be useful in the real world. In this talk I will describe Ziria, an SDR platform that is both easily programmable and performant. Ziria introduces a programming model that builds on ideas from functional programming and that is tailored to wireless physical layer tasks. The model captures the inherent and important distinction between data and control paths in this domain. I will describe the programming model, give an overview of the execution model, compiler optimizations, and current work. We have used Ziria to produce an implementation of 802.11a/g and a partial implementation of LTE.",FHPC '14,2014.0,10.1145/2636228.2661115,13121910,semantic_scholar
aa8966777cd1d9b06671c891161f7696e939333a,https://www.semanticscholar.org/paper/aa8966777cd1d9b06671c891161f7696e939333a,Power and Electromagnetic Analysis for Template Attacks,"Elliptic curve cryptography (ECC) is prone to physical attacks exploring side-channel leakages from power consumption or electromagnetic emanations. When performing a side-channel evaluation of ECC implementations one can choose among different side-channels and analysis methods that sometimes implies more accurate results. Typically, electromagnetic analysis performs better when pattern matching algorithms are deployed, while power analysis is more precise with Hamming weight models. In this paper, we perform template attacks with power and electromagnetic side-channels, in order to compare the efficiency of those methods in real-world software implementations. 
More precisely, we perform Online Template Attack, an efficient attack technique applied to regular scalar multiplication algorithms. To retrieve the secret scalar during a scalar-multiplication with Online Template Attack, it is sufficient to acquire one trace per key bit. In order to compare power and electromagnetic analysis, we use the double-and-add-always algorithm on a twisted Edwards curve running on a smart card with an ATmega163 CPU.",,2015.0,,236061208,semantic_scholar
d68bd3b7f44531e596713c918bd97398b2f6fe3b,https://www.semanticscholar.org/paper/d68bd3b7f44531e596713c918bd97398b2f6fe3b,A Distributed Intelligent Sensing Approach for Environmental Monitoring Applications,"Scientific reports from around the world present us with the undeniable fact that the global ecosystem is undergoing severe change. As this shift accelerates, it is ever more critical that we are able to quantify the local effects of such changes, and further, their implications, from our daily life to the biological processes that put food on our tables. In this thesis, we study the application of sensor network technology to the observation and estimation of highly local phenomena---specifically at a scale between ten to several hundred square meters. Embedding knowledge about the observed process directly into the sensor nodes' behavior via dedicated resource management or control algorithms allows us to deploy dense networks with low power requirements. Ecological systems are notoriously complex. In our work we must thus be highly experimental; it is our highest goal that we construct an approach to environmental monitoring that is not only realistic, but practical for real-world use. Our approach is centered on a commercially available sensor network product, aided by an off-the-shelf quadrotor with minimal customization. We validate our approach through a series of experiments performed from simulation all the way to reality, in deployments lasting days to several months. We motivate the need for local data via two case studies examining physical phenomena. First, employing novel modalities, we study the eclosion of a common agricultural pest. We present our efforts to acquire data that is more local than commonly employed methods, culminating in a six month deployment in a Swiss apple orchard. Next, we apply a environmental fluid dynamics model to enable the estimation of sensible heat flux using an inexpensive sensor. We integrate the sensor with a wireless sensor network and validate its capabilities in a short-term deployment. Acquiring meaningful data on a local scale requires that we advance the state of the art in multiple aspects. Static sensor networks present a classical tension between resolution, autonomy, and accuracy. We explore the performance of algorithms aimed at providing all three, showing explicitly what is required to implement these approaches for real-world applications in an autonomous deployment under uncontrolled conditions. Eventually, spatial resolution is limited by network density. Such limits may be overcome by the use of mobile sensors. We explore the use of an off-the-shelf quadrotor, equipped with environmental sensors, as an additional element in system of heterogeneous sensing nodes. Through a series of indoor and outdoor experiments, we quantify the contribution of a such a mobile sensor, and various strategies for planning its path.",,2015.0,10.5075/epfl-thesis-6673,109901930,semantic_scholar
74fab23e3fd31db77d22074ede9de7e8c8a40c38,https://www.semanticscholar.org/paper/74fab23e3fd31db77d22074ede9de7e8c8a40c38,Improving Vehicular Networking Reliability and Efficiency in the Context of Platooning Applications,"Vehicular networking is a technology that enables vehicles communication system. A joint effort from the automobile industry, transportation industry, and government offices is driving the adoption of this technology to build intelligent transportation systems that consist of smart vehicles. This study attempts to improve the reliability and efficiency of vehicular networking. The study assumes the context of platooning applications, but the contributions of this study can be applied to other vehicular applications as well. There are two contributions in this study. First, a wireless emulator is designed and implemented to emulate IEEE 802.11 networks in real-time using the Ethernet infrastructure. The emulator replaces the MAC layer and physical layer of IEEE 802.11 networking stack with a real-time CSMA/CA model, thus reduces the cost of experiments. It provides upper layers the same interfaces as on a real device. As a result, the testing targets in the emulation are real-world software components as opposed to simulation scripts in a discrete event simulator. These software components can be routing protocols, transport protocols, or applications, and are the same code that can be deployed in the real world. Second, an Interframe Compression Transmission Layer is designed and implemented, to provide efficient transmission of periodical messages in vehicular environments. The transmission layer compresses the difference between frames instead of frames themselves, and reduces bandwidth consumption significantly. To improve the behaviors of the transmission layer under different scenarios and configurations studied, an adaptive version of the algorithm is designed, which achieves more than 50% in reduction of bandwidth consumption using real-world platooning data trace. With lower bandwidth consumption, delivery ratio is vastly improved in congested networking environments.",,2016.0,,62827960,semantic_scholar
1bd64a7541e8d34f0d7782f0e786b2ffd78e55b7,https://www.semanticscholar.org/paper/1bd64a7541e8d34f0d7782f0e786b2ffd78e55b7,Interacting in various application domains,"eLearning and Education.- Arab Children's Reading Preference for Different Online Fonts.- Adaptation Decisions and Profiles Exchange among Open Learning Management Systems Based on Agent Negotiations and Machine Learning Techniques.- Accessing e-Learning Systems via Screen Reader: An Example.- Using Tablet PCs and Pen-Based Technologies to Support Engineering Education.- Optimal Affective Conditions for Subconscious Learning in a 3D Intelligent Tutoring System.- Computer-Based Learning to Improve Breast Cancer Detection Skills.- Virtual Classroom and Communicability: Empathy and Interaction for All.- Communicability for Virtual Learning: Evaluation.- Attention and Motivation in Hypermedia Systems.- A Web-Based, Interactive Annotation Editor for the eCampus Development Environment for SCORM Compliant E-Learning Modules.- An Innovative Way of Understanding Learning Processes: Eye Tracking.- A Set of Rules and Strategies for UNSAM Virtual Campus.- HCI Professional Involvement in k-12 Education: On Target or Missing the Mark?.- A Language Learning System Utilizing RFID Technology for Total Physical Response Activities.- Promoting Metacognition in Immersive Cultural Learning Environments.- The Application of the Flexilevel Approach for the Assessment of Computer Science Undergraduates.- Development of Ubiquitous On-Demand Study Support Environment for Nursing Students.- The Effects of Prior Knowledge on the Use of Adaptive Hypermedia Learning Systems.- Supporting Learners in Adaptive Learning Environments through the Enhancement of the Student Model.- The Concept of IMPRESSION: An Interactive Instruction System and Its Practice for Real-Time Distance Lessons between U.S. and Japan.- Improving Children's Writing Ability.- From Paper to Module - An Integrated Environment for Generating SCORM Compliant Moodle Courses Out of Text and Multimedia Elements.- Development of a Simulator of Abacus: Ancient Analog Calculator on a Mobile Phone as a Teaching Material.- A Proposal for a Framework for an e-Alumni Program Using SNS.- Supporting End-User Development of Personalized Mobile Learning Tools.- Didactic Models as Design Representations.- Interactive Learning Panels.- WebELS: A Content-Centered E-Learning Platform for Postgraduate Education in Engineering.- A Pen-Based Teaching System for Children and Its Usability Evaluation.- Development of a Visualised Sound Simulation Environment: An e-Approach to a Constructivist Way of Learning.- Games and Entertainment.- Causal Links of Presence.- Games Design Principles for Improving Social Web Applications.- A Multiple-Level 3D-LEGO Game in Augmented Reality for Improving Spatial Ability.- An Online Survey System on Computer Game Enjoyment and Personality.- Playability Testing of Web-Based Sport Games with Older Children and Teenagers.- Exploring the Elements and Design Criteria of Massively-Multiplayer Online Role-Playing Game (MMORPG) Interfaces.- Healthcare Game Design: Behavioral Modeling of Serious Gaming Design for Children with Chronic Diseases.- Analyzing Human Behaviors in an Interactive Art Installation.- The Effects of Quest Types and Gaming Motivations on Players' Knowledge Acquisitions in an Online Role-Playing Game Environment.- Self-movement Feeling Generation in Sports Watching with Screen Movement via Pan-Tilt Steerable Projector.- Design of Interactive Emotional Sound Edutainment System.- Understanding Online Game Addiction: Connection between Presence and Flow.- The Experience of Presence in 3D Web Environment: An Analysis of Korean Second Life.- Influence of Real-World Ten-Pin Bowling Experience on Performance during First-Time Nintendo Wii Bowling Practice.- Emotionally Adapted Games - An Example of a First Person Shooter.- DiamondTheater: A System for Reproducing Theater and Supporting Creative Activities.- Work, Collaboration and Business.- New Health Information Systems (HIS) Quality-in-Use Model Based on the GQM Approach and HCI Principles.- An Information Visualization Approach to Hospital Shifts Scheduling.- Designed to Fit: Challenges of Interaction Design for Clothes Fitting Room Technologies.- Usability for Poll Workers: A Voting System Usability Test Protocol.- CAD and Communicability: A System That Improves the Human-Computer Interaction.- A Novel Visualization Tool for Evaluating Medication Side-Effects in Multi-drug Regimens.- Design of a Web Intervention to Change Youth Smoking Habits.- Smart Makeup Mirror: Computer-Augmented Mirror to Aid Makeup Application.- Studying Reactive, Risky, Complex, Long-Spanning, and Collaborative Work: The Case of IT Service Delivery.- Human Computer Interaction in Virtual Standardized Patient Systems.- Towards Standardized Pen-Based Annotation of Breast Cancer Findings.- ImproV: A System for Improvisational Construction of Video Processing Flow.- E-Assessment: A Suitable Alternative for Measuring Competences?.- Green Advocate in E-Commerce.- Gesture-Based Sharing of Documents in Face-to-Face Meetings.- Developing, Deploying and Assessing Usage of a Movie Archive System among Students of Film Studies.- Using Activity Descriptions to Generate User Interfaces for ERP Software.- Developing a Nomenclature for EMR Errors.- Mapping for Multi-source Visualization: Scientific Information Retrieval Service (SIRS).- Client-Side Visualization of Internet Forums for Information Retrieval.- Social-Technical Tools for Collaborative Sensemaking and Sketching.- Developing Some User Interfaces of TV under Enormous Channels Environment.- Electronic Glassboard - Conception and Implementation of an Interactive Tele-presence Application.- A New Automatic Teller Machine (ATM) Proposal through the Analysis of ATMs of Three Banks.- Advanced Applications.- Designing Usable Bio-information Architectures.- Run-Time Adaptation of a Universal User Interface for Ambient Intelligent Production Environments.- Heuristic Evaluation of Mission-Critical Software Using a Large Team.- Interface Development for Early Notification Warning System: Full Windshield Head-Up Display Case Study.- Reflections on the Interdisciplinary Collaborative Design of Mapping the Universe.- Distilling Support Opportunities to Improve Urban Search and Rescue Missions.- A New Approach to Design an Interactive System for Molecular Analysis.- The Differences of Aviation Human Factors between Individualism and Collectivism Culture.- Web-Based Training System for Improving Aviation Maintenance Performance.- Allocating Human-System Interfaces Functions by Levels of Automation in an Advanced Control Room.- Development of an Expert System as a User Interface for an RFID Application.- Developing a Validation Methodology for Educational Driving Simulators and a Case Study.- Developing a Usable Mobile Flight Case Learning System in Air Traffic Control Miscommunications.",,2009.0,,107222405,semantic_scholar
6220b68cd721512098b9b14851afe5e660c0e565,https://www.semanticscholar.org/paper/6220b68cd721512098b9b14851afe5e660c0e565,Crypto-day Campeon A8,"Using the properties of a wireless channel is an alternative approach for securing the channel besides pre-shared keys or asymmetric cryptography. Numerous experiments have recently demonstrated that channel-based key establishment (CBKE) is a promising alternative to well-known symmetric/asymmetric approaches. Their run-times for establishing a symmetric key suggest that such methods are highly suitable for real-world applications that operate in a dynamic mobile environment with peer-to-peer association. CBKE is a new paradigm for generating shared secret keys. The approach is based on the estimation of the wireless transmission channel by both the sender and receiver, where the shared secret key is derived from channel parameters. The commonness of the randomness of the secret key relies on the principle of channel reciprocity. Specifically, this means that the channel from Alice to Bob is the same than the channel from Bob to Alice. This symmetry of practical channels is usually sufficiently high, as well as its entropy of spatial, temporal, and spectral characteristics. Security is given if an attacker’s distance to the two communicating nodes is high enough, so that the observed channel parameters to each node are uncorrelated and independent from each other. Typically, in real environments this is given if the distance is greater than about half of the carrier wavelength. For instance, for the frequency used in 2.4 GHz WiFi, this translates to a distance of 6.25 cm. So far, high usability and dynamic key management are very difficult to achieve for wireless devices, which operate under strict resource constraints. CBKE has the potential to significantly reduce the cost of securing small embedded devices, and hence make mass production and deployment more viable. Until now, no research has addressed the requirements for performance evaluation of real-world implementations of CBKE systems. We present a wireless CBKE security system built with standard components, e.g., quantization scheme and error correction codes, presented in recent publications. We introduce necessary implementation properties and requirements of CBKE systems. In order to validate the performance of the key generation algorithms, we define a set of metrics. Finally, we describe an end-to-end implementation on an ARM-Cortex M3 microcontroller to demonstrate the practical feasibility of channel-based key estimation using current embedded hardware. Comparative analysis of pseudorandom generators Aleksei Burlakov, Johannes vom Dorp, Joachim von zur Gathen, Sarah Hillmann, Michael Link, Daniel Loebenberger, Jan Lühr, Simon Schneider & Sven Zemanek {burlakov,dorp,luehr,schneid,zemanek}@cs.uni-bonn.de {sarah.hillmann,michael.link}@uni-bonn.de {gathen,daniel}@bit.uni-bonn.de Bonn-Aachen International Center for Information Technology Dahlmannstr. 2, Bonn We compare random generators (RGs) under controlled conditions regarding their efficiency and statistical properties. For this purpose, we distinguish between physical RGs and software RGs, which can be further subdivided into cryptographically secure and insecure RGs. Physical RGs covered by our study are the hardware generator PRG310-4 and /dev/random as implemented in the Linux kernel. Since /dev/random is fed by system events, we analyze both an idle lab environment and a server hosting several virtual machines. As examples for cryptographically secure RGs our analysis compares the RSA generator and the Blum-Blum-Shub generator, both for 3000-bit moduli. Additionally, we compare them to the Nisan-Wigderson construction with suitably selected parameters. We include two cryptographically insecure RGs, namely a linear congruential generator (LCG) and the Littlewood generator. In order to obtain repeatable and comparable results, our implementations of the software RGs were all run on the same machine and produced 512 kB of output each, using AES post-processed output of the generator PRG310-4 as source for random seed bits. We compare the results in terms of byte entropy and throughput excluding initialization. For further statistical analysis — not shown in the table — we apply the NIST test suite on the outputs. The most important finding is that in our scenarios, number-theoretic generators compete very well against hardware-based ones. byte entropy runtime throughput throughput [bit] [μs] [kB/s] normalized PRG310-4, no post-processing 7.99963 16308400 31.39486 4.34492 AES post-processing 7.99963 36524300 14.01806 1.94004 /dev/random, in the field 7.99979 9.169× 10 5.584× 10−3 7.728× 10−4 in the lab 7.99948 2.671× 10 1.917× 10−4 2.653× 10−5 Littlewood 6.47244 15206550 33.66970 4.61011 Linear congruential generator 7.99969 2644039 193.64313 26.51392 Blum-Blum-Shub 7.99962 17708350 28.91291 3.95880 RSA, e = 2 + 1, 1400 bit/round 7.99966 267604 1913.27484 261.96857 e = 3, 1 bit/round 7.99963 70103838 7.30345 1 Nisan-Wigderson 7.99961 2731227 187.46153 25.66753 Table 1: Overview of the results for generating 512 kB of output.",,2015.0,,12063917,semantic_scholar
949b01c64ba61c94ba0982ffd6abc50658d53874,https://www.semanticscholar.org/paper/949b01c64ba61c94ba0982ffd6abc50658d53874,"Demo: 802.11 a/g PHY implementation in ziria, domain-specific language for wireless programming","Software-defined radio (SDR) brings the flexibility of software to the domain of wireless protocol design, promising an ideal platform both for research and innovation and the rapid deployment of new protocols on existing hardware. However, existing SDR programming platforms require either careful hand-tuning of low-level code, negating many of the advantages of software, or are too slow to be useful in the real world. In this demo we present Ziria, the first software-defined radio programming platform that is both easily programmable and performant. Ziria introduces a novel programming model tailored to wireless physical layer tasks and captures the inherent and important distinction between data and control paths in this domain. We show the capabilities of Ziria by demonstrating a real-time implementation of WiFi PHY running at 20 MHz.",SRIF@SIGCOMM,2014.0,10.1145/2627788.2627799,8185112,semantic_scholar
91bfa56b000e77ba68738c0366436918462ebe38,https://www.semanticscholar.org/paper/91bfa56b000e77ba68738c0366436918462ebe38,"ACS: Specifying ""Smart"" Applications Using Sense-Process-Consume Flows","Smart applications enable pervasive and informed interactions between the physical and digital worlds. These applications are deployed on resource constrained wireless sensor networks and are commonly implemented using software modularization schemes such as components and services. Composing smart applications at the abstraction level offered by embedded software modularization schemes is complex and time consuming. The complexity of composing them is derived from the need to understand many low-level issues, e.g. embedded programming languages, coordination mechanisms, software tool chains. We present an application composition service that reduces composition effort by offering a declarative specification of sense, process and consume flows. We demonstrate reduction in composition effort for three real-world smart applications deployed on a smart office environment.",2013 IEEE 12th International Symposium on Network Computing and Applications,2013.0,10.1109/NCA.2013.28,16895477,semantic_scholar
824afcd7abb4e89ddbc7d547d434cdfe47616c8b,https://www.semanticscholar.org/paper/824afcd7abb4e89ddbc7d547d434cdfe47616c8b,Modeling and Simulation of Radio Signals Attenuation Using Informed Virtual Geographic Environments (IVGE),"A radio communication system is a complex dynamic phenomenon where transmitter and receiver antennas are constantly constrained by the physical environment in which they are deployed. In the real world, radio transmissions are subject to propagation effects which deeply affect the received signals because of geographic and environmental characteristics (foliage and vegetation, buildings, mountains and hills, etc.). Multi-Agent Geo-Simulation aims to simulate such phenomena involving a large number of autonomous situated actors (implemented as software agents) evolving and interacting within a representation of the physical environment. Using a geo-computation approach, we propose to use an Informed Virtual Geographic Environment (IVGE) along with MAGS paradigm. In addition, we propose a multi-agent prototype to analyze the attenuation effect due to the radio signal’s traversal between antennas (simulated as software agents) through terrain shape, vegetation area, and buildings using a 3D line-of-sight computation technique. Keywords-Line of Sight; Excess attenuation; Vegetation and Foliage; Radio Propagation; Informed Virtual Geographic Environments",,2013.0,,229379178,semantic_scholar
72656d9df791499f6bb246f599e462228ff528ad,https://www.semanticscholar.org/paper/72656d9df791499f6bb246f599e462228ff528ad,A channel model and coding for vehicle to vehicle communication based on a developed V-SCME,"Over the recent years, VANET communication has attracted a lot of attention due to its potential in facilitating the implementation of 'Intelligent Transport System'. Vehicular applications need to be completely tested before deploying them in the real world. In this context, VANET simulations would be preferred in order to evaluate and validate the proposed model, these simulations are considered inexpensive compared to the real world (hardware) tests. The development of a more realistic simulation environment for VANET is critical in ensuring high performance. Any environment required for simulating VANET, needs to be more realistic and include a precise representation of vehicle movements, as well as passing signals among different vehicles. In order to achieve efficient results that reflect the reality, a high computational power during the simulation is needed which consumes a lot of time. The existing simulation tools could not simulate the exact physical conditions of the real world, so results can be viewed as unsatisfactory when compared with real world experiments. This thesis describes two approaches to improve such vehicle to vehicle communication. The first one is based on the development of an already existing approach, the Spatial Channel Model Extended (SCME) for cellular communication which is a verified, validated and well-established communication channel model. The new developed model, is called Vehicular - Spatial Channel Model Extended (V-SCME) and can be utilised for Vehicle to Vehicle communication. V-SCME is a statistical channel model which was specifically developed and configured to satisfy the requirements of the highly dynamic network topology such as vehicle to vehicle communication. V-SCME provides a precise channel coefficients library for vehicle to vehicle communication for use by the research community, so as to reduce the overall simulation time. The second approach is to apply V-BLAST (MIMO) coding which can be implemented with vehicle to vehicle communication and improve its performance over the V-SCME. The V- SCME channel model with V-BLAST coding system was used to improve vehicle to vehicle physical layer performance, which is a novel contribution. Based on analysis and simulations, it was found that the developed channel model V-SCME is a good solution to satisfy the requirements of vehicle to vehicle communication, where it has considered a lot of parameters in order to obtain more realistic results compared with the real world tests. In addition, V-BLAST (MIMO) coding with the V-SCME has shown an improvement in the bit error rate. The obtained results were intensively compared with other types of MIMO coding.",,2016.0,,204088939,semantic_scholar
57f88d973d0def2120b5348f3fa074d851933e99,https://www.semanticscholar.org/paper/57f88d973d0def2120b5348f3fa074d851933e99,Realistic frequency coded chipless RFID: physically modulated tags and refectarray readers,"Recently, the chipless Radio Frequency Identiﬁcation (RFID) technology has attracted tremendous attention in the market of item identiﬁcation where the cost is the main concern. However, up to date the technology is at the conceptual level and suffers from a lot of imitations that hinder the technology deployment. The chipless RFID system comprises three major parts which are the reader circuit, the interrogation antennas, and the chipless tags. The contributions of this dissertation are to overcome the challenges that impede the deployment of the chipless RFID system from the perspective of innovating physically modulated tags and developing the reader antenna system. In particular, the system is considered in three novel aspects. 
The ﬁrst aspect is the linear physically modulated tags where the tag is interrogated by Ultra Wideband (UWB) signal and the tag inscribed metallic resonators are physically modulating the interrogation frequencies. Therefore, the UWB waveform is modulated in the form of resonant notches, and/or peaks that are inherently embedded in the tag backscattered Radar Cross Section (RCS) frequency response. In this regard, four innovative physically modulated tags are developed aiming at enhancing the coding efﬁciency, maximizing the coding capacity, conserving the operating frequency range and preserving the tag size. The ﬁrst tag is based on nested circular ring resonators where each resonator codiﬁes a tag coding notch. Terefore, the tag structure is scalable, printable and compact size. Moreover, a novel encoding methodology is employed to preserve the notch width and position while coding. The second developed tag is a depolarizing one where the polarization isolation between the reader interrogation signal and the tag response is utilized to minimize the environmental clutter reﬂections. Furthermore, the tag is scalable, printable, and compact size in the credit card format. Thirdly, a novel Notch Width Modulation (NWM) tag is introduced where the tag-ID is not only based on the notch position but also on the notch width. Hence, the notch width conﬁgures a further dimension to increase the Degree of Freedom (DoF) for coding and modulation. Therefore, the notch width and position are modulated simultaneously aiming at enhancing the coding efﬁciency and capacity. Lastly, a novel On Off Notch/Peak (OO-N/P) and Notch/Peak-Position (N/P-P) modulation tag is introduced. The tag basic idea is to exploit both the co-polarized and cross polarized backscattered signals from a tag excited with a linear polarized wave. Consequently, the tag signature is encoded into Notch/Peak (N/P) format in two orthogonal planes. Thus, the Co/Cross-polarizing N/P modulation scheme presents a novel criterion for enhancing the coding efﬁciency and capacity of the chipless RFID systems. Moreover, the cross-polarized response enhances the tag detection in a realistic environment. The proposed tags and their associated physical modulation schemes are validated using Electro Magnetic (EM) simulations and real-world testbed measurements. 
 
In the second aspect, the Reﬂectarray (RA) antenna is proposed to be utilized in the reader side aiming at increasing the reading range, minimizing the environmental reﬂections, and acquiring a lot of novel capabilities that can not be provided by the conventional antenna arrays. The spatial feeding RA antenna is easily integrated with the RF circuits, lightweight, conformal geometry, and low cost. Hence, in this concern, three different novel designs are developed. The ﬁrst design utilizes the Log Periodic Array (LPDA) antenna to feed the developed RA surface. This introduced prototype operates at 5.8GHz and achieves 300MHz bandwidth. Moreover, the RA antenna radiation beam is 4 times narrower than the feeder beam and thus 6dB higher in gain with −10dB Side Lobe Level (SLL). The second developed prototype uses a constant phase center horn antenna to feed the RA surface. Thus, an UWB RA antenna enabling multiple bits accommodation is designed. This antenna operates from 4GHz to 6GHz with 15° Half Power Beam Width (HPBW), 19dBi gain, and −10dB SLL. Furthermore, this developed UWB RA antenna is successfully integrated with the physically modulated tags and a reading range of 1m is achieved. To the best of my knowledge, this is the highest reading range achieved in the Frequency Coded (FC) chipless RFID systems, considering real-world indoor environment and software deﬁned radio reader. After that, dual-polarized RA antenna with low cross-polarization level is presented. This RA antenna is proposed to be utilized with the Co/Cross-polarizing tags. Finally, a successful implementation of an electronic beam steering RA antenna is introduced. This novel beam steering RA antenna system enhances the reading robustness and can precisely locate the chipless tags. In this concern, a novel unit cell that is able to electronically control the reﬂected phase at different discrete frequencies utilizing a single varactor diode is proposed. Therefore, a scanning range of ±50° is achieved. Moreover, the steered beams are 4 times narrower than the feeder beam and thus 4 times higher in gain. 
 
In the third aspect, the nonlinear physically modulated tags are proposed. The core functionality relies on interrogating the tag with a prescribed set and format of frequencies in a time regulated technique while the tag replies with its unique ID at other frequencies. Therefore, the nonlinearity is exploited to completely isolate the environmental clutter reﬂections, get rid of the necessary reference calibration measurements, overcome the detuning caused by the tagged item materials, and increase the coverage. These objectives are attained by exploiting the nonlinearity generated from a single unbiased diode integrated with the tag structure. The ﬁrst proposed tag category relies on exploiting the second order nonlinear terms. Therefore, in this regard, three novel tags are introduced. The ﬁrst class is the single tone harmonic radar tags. In this class, the reader scans the available tags by sending speciﬁc fundamental tones. Then, the tag receiving antenna is tuned at only one of these fundamentals which is maximally conveyed to the nonlinear device for generating the corresponding harmonics. Consequently, the tag transmitting antenna is tuned at the second harmonic which is retransmitted back towards 
the reader representing the tag-ID. Thus, the narrower is the band-pass ﬁlter provided by the tag receiving antenna or integrated into it, the more the frequencies that can be utilized for coding. After that, the multi-tone interrogation is proposed to increase the coding capacity. 
Hence, the tag is interrogated with a prescribed set of fundamentals that are swept over the time to avoid the generation of the mixing products in the reader and tag as well. The tag in turn which is completely planar based on the Coplanar Waveguide (CPW) technology implements a Notch Position Modulation (NPM) scheme in the second harmonics of these fundamental tones. Therefore, the notches that are existing in the second harmonic response symbolize the tag-ID. Afterward, the simultaneous multi-tone interrogation is explored. In this concern, a set of distinct frequency pairs are used to interrogate the nonlinear tags. As a consequence, these tones are mixed through the nonlinear device. Consequently, the tag transmitting antenna ﬁgures out only one of these mixed products. The second proposed tag category relies on exploiting the inter-modulation communication principle which exhibits a small frequency span. Therefore, the tag is illuminated by two co-located frequencies and respond at an inter-modulated frequency which is retransmitted by the tag transmitting antenna representing the tag-ID. Finally, the phase encoding capability is proposed. Therefore, not only the existence or the non-existence of a harmonic notch or peak used in coding the tag-ID but also the corresponding relative phase states can be considered. The introduced tags and their associated physical modulation schemes are veriﬁed using harmonic balance analysis, EM simulations and realistic testbed measurements. 
 
Lastly, the unique features which are considered in the dissertation bring a signiﬁcant enhancement to the deployment of the chipless RFID system.",,2017.0,,150161814,semantic_scholar
87aed004b3334c17bcd87b55cff6291594e1347c,https://www.semanticscholar.org/paper/87aed004b3334c17bcd87b55cff6291594e1347c,Development of a smart home simulator for use as a heuristic tool for management of sensor distribution.,"Smart Homes offer potential solutions for various forms of independent living for the elderly. The assistive and protective environment afforded by smart homes offer a safe, relatively inexpensive, dependable and viable alternative to vulnerable inhabitants. Nevertheless, the success of a smart home rests upon the quality of information its decision support system receives and this in turn places great importance on the issue of correct sensor deployment. In this article we present a software tool that has been developed to address the elusive issue of sensor distribution within smart homes. Details of the tool will be presented and it will be shown how it can be used to emulate any real world environment whereby virtual sensor distributions can be rapidly implemented and assessed without the requirement for physical deployment for evaluation. As such, this approach offers the potential of tailoring sensor distributions to the specific needs of a patient in a non-evasive manner. The heuristics based tool presented here has been developed as the first part of a three stage project.",Technology and health care : official journal of the European Society for Engineering and Medicine,2009.0,10.3233/THC-2009-0550,207706483,semantic_scholar
08ae139d6890717bea0e6243549d66caf24fa78e,https://www.semanticscholar.org/paper/08ae139d6890717bea0e6243549d66caf24fa78e,Teaching Embedded Systems in a MOOC Format,"We have designed and implemented a Massive Open Online Class (MOOC) with a substantial lab component within the edX platform. We deployed this MOOC three times with a total enrollment of over 100,000 students. If MOOCs are truly going to transform engineering education, then they must be able to deliver classes with laboratory components. Our offering goes a long way in unraveling the perceived complexities in delivering a laboratory experience to thousands of students from around the globe. We believe the techniques developed in this class will significantly transform the MOOC environment. Effective education requires students to learn by doing. In the traditional academic setting this active learning is achieved through a lab component. Translating this to the online environment is a non-trivial task that required several important factors to come together. First, we have significant support from industrial partners ARM Inc. [1] and Texas Instruments [2]. Second, the massive growth of embedded microcontrollers has made the availability of lost-cost development platforms feasible. Third, we have assembled a team with the passion, patience, and experience of delivering quality lab experiences to large classes. Fourth, online tools now exist that allow students to interact and support each other. We used edX for the delivery of videos, interactive animations, text, and quizzes [3]. We used Piazza [4] for discussion boards and Zyante [5] for a programming reference. We partnered with element-14 [6], Digi-Key [7], and Mouser [8] to make the lab kit available and low-cost. Even though there was a $40-$70 cost to purchase the lab kit, the course completion numbers were slightly better than a typical MOOC. 7.3% of the students completed enough of the class to receive a certificate. Students completing end of the course surveys report a 95% overall satisfaction. Demographics show a world-wide reach with India, US, and Egypt being the countries with the most students. In this paper we will present best practices, successes and limitations of teaching a substantial lab across the globe. Background An embedded system combines mechanical, electrical, and chemical components along with a computer, hidden inside, to serve a single dedicated purpose [9-11]. There are over 50 billion processors based on the ARM architecture delivered into products, and most of these computers are single-chip microcontrollers that are the brains of an embedded system. Embedded systems are a ubiquitous component of our everyday lives. We interact with hundreds of tiny computers every day that are embedded into our houses, our cars, our bridges, our toys, and our work. As our world has become more complex, so have the capabilities of the microcontrollers embedded into our devices. Therefore the world needs a trained workforce to develop and manage products based on embedded microcontrollers. Review Other online classes have delivered laboratory experiences. Hesselink at Stanford University developed iLabs as a means to deliver science experiments to online learning. Their lab-in-a-box involves simulations and animations [12]. O’Malley et al. from the University of Manchester developed a Chemistry MOOC with a lab component using virtual labs and simulations [13-14]. University of Washington presented a hardware/software MOOC on Coursera [15]. This course is primarily a programming class without graded physical labs. Ferri et al. from Georgia Institute of Technology created a MOOC for linear circuits [16]. This class had activities to perform with NI’s myDAC, but graded lab circuits were not part of the online experience. Connor, and Huettel at Duke created a Virtual Community of Practice for electric circuits [17]. Cherner et al. created a virtual multifunctional X-Ray diffractometer for teaching science and engineering [18]. Saterbak et al. at Rice University developed online materials to teach freshman design, with the goal to free-up class time for more interactive learning experiences [19]. Harris from University of California at Irvine has a six-course sequence on Introduction to the Internet of Things and Embedded Systems where students build actual embedded devices [20]. Grading for this course uses peer assessment. Lee et al. at Berkeley developed an introduction to embedded systems MOOC with laboratory exercises. The lab itself was a robotic controller in a virtual laboratory environment. Completion of the labs themselves does have an automatic grading component based on the student’s written software [21-22]. All this work emphasizes the need for hands on learning. Pedagogy The overall educational objective of this class is to allow students to discover how computers interact with the environment. The class provides hands-on experiences of how an embedded system could be used to solve problems. The focus of this introductory course is understanding and analysis rather than design, where students learn new techniques by doing them. We feel we have solved the dilemma in learning a laboratory-based topic like embedded systems, where there is a tremendous volume of details that first must be learned before hardware and software systems can be designed. The approach taken in this course is to learn by doing in a bottom-up fashion. One of the advantages of a bottom-up approach to learning is that the student begins by mastering simple concepts. Once the student truly understands simple concepts, he or she can embark on the creative process of design, which involves putting the pieces together to create a more complex system. True creativity involves solving complex problems using effective combinations of simple components. Embedded systems afford an effective platform to teach new engineers how to program for three reasons. First, there is no operating system. Thus, in a bottom-up fashion the student can see, write, and understand all software running on a system that actually does something. Second, embedded systems involve real input/output that is easy for the student to touch, hear, and see. Many engineering students struggle with abstraction. We believe many students learn effectively by using their sense of touch, hearing and sight to first understand and internalize difficult concepts, and then they will be able to develop and appreciate abstractions. Third, embedded systems are employed in many everyday products, motivating students to see firsthand, how engineering processes can be applied in the real world. This course is intended for beginning college students with some knowledge of electricity as would have been taught in an introductory college physics class. Secondly, it is expected students will have some basic knowledge of programming and logic design. No specific language will be assumed as prior knowledge but this class could be taken as their second programming class. We hoped experienced engineers could also use this course to train or retrain in the field of embedded systems. Learning objectives of the course Although the students are engaged with a fun and rewarding lab experience, our educational pedagogy is centered on fundamental learning objectives. After the successful conclusion of this class, students should be able to understand the basic components of a computer, write C language programs that perform input/output interfacing, implement simple data structures, manipulate numbers in multiple formats, and understand how software uses global memory to store permanent information and the stack to store temporary information. Our goal is for students to learn these concepts: 0) How the computer stores and manipulates data; 1) Embedded systems using modular design and abstraction; 2) Design tools like requirements documents, data flow graphs, and call graphs; 3) C programming: considering both function and style; 4) Debugging and verification using a simulator and the real microcontroller; 5) Debugging tools like voltmeters, oscilloscopes, and logic analyzers; 6) How to input/output using switches, LEDs, DACs, ADCs, and serial ports; 7) Implementation of an I/O driver, multithreaded programming, and interrupts; 8) Analog to digital conversion (ADC), periodic sampling, and the Nyquist Theorem; 9) Stepper motors, brushed DC motors, and simple digital controllers; 10) Digital to analog conversion (DAC), used to make simple sounds; 11) Simple distributed systems that connect two microcontrollers; 12) Internet of things, connecting the embedded system to the internet; 13) System-level design that combine multiple components together. Laboratory Kit Active learning requires a platform for the student to learn by doing. Figure 1 shows the components of the basic lab kit. There are two difficulties with a physical lab kit deployed in a world-wide open classroom environment. The first problem is availability of components. We partnered with companies and distributors six months in advance of the course launch to guarantee availability. The companies wanted us to specify the number of students who would buy the kit. In this regard, we were very lucky. Six months prior to our first launch, we estimated 2000 people would register for the class and 1000 would buy the kit. In turns out Texas Instruments produced 10,000 microcontroller boards just in case. Much to our surprise 40,000 people registered and we estimate 11,000 purchased the kit during this first delivery of the course. The second solution to the problem of availability was to have three world-wide distributors (element-14, Mouser, and Digi-Key). Working with these distributors, we created one-click landing pages for students to buy the kit. Furthermore, for each component in the kit (other than the microcontroller board), we had three or more possible parts. The third solution was to design the course with flexible deadlines and pathways. Each lab had a simulation and a real-board requirement. Students who were waiting for the parts to be shipped could proceed with ",,2016.0,10.18260/p.26025,59020150,semantic_scholar
a738fa6670ebcfa1f3348f04cf6722f4d8bbc049,https://www.semanticscholar.org/paper/a738fa6670ebcfa1f3348f04cf6722f4d8bbc049,Physical Layer Cooperation,"Information theory has long pointed to the promise of physical layer cooperation in boosting the spectral efficiency of wireless networks. Yet, the optimum relaying strategy to achieve the network capacity has till date remained elusive. Recently however, a relaying strategy termed Quantize-Map-and-Forward (QMF) was proved to achieve the capacity of arbitrary wireless networks within a bounded additive gap. This thesis contributes to the design, analysis and implementation of QMF relaying by optimizing its performance for small relay networks, proposing low-complexity iteratively decodable codes, and carrying out over-the-air experiments using software-radio testbeds to assess real-world potential and competitiveness. The original QMF scheme has each relay performing the same operation, agnostic to the network topology and the channel state information (CSI); this facilitates the analysis for arbitrary networks, yet comes at a performance penalty for small networks and medium SNR regimes. In this thesis, we demonstrate the benefits one can gain for QMF if we optimize its performance by leveraging topological and channel state information. We show that for the N-relay diamond network, by taking into account topological information, we can exponentially reduce the QMF additive approximation gap from $\Theta(N)$ bits/s/Hz to $\Theta(\log N)$ bits/s/Hz, while for the one-relay and two-relay networks, use of topological information and CSI can help to gain as much as $6$ dB. Moreover, we explore what benefits we can realize if we jointly optimize QMF and half-duplex scheduling, as well as if we employ hybrid schemes that combine QMF and Decode-and-Forward (DF) relay operations. To take QMF from being a purely information-theoretic idea to an implementable strategy, we derive a structure employing Low-Density-Parity-Check (LDPC) ensembles for the relay node operations and message-passing algorithms for decoding. We demonstrate through extensive simulation results over the full-duplex diamond network, that our designs offer a robust performance over fading channels and achieves the full diversity order of our network at moderate SNRs. Next, we explore the potential real-world impact of QMF and present the design and experimental evaluation of a wireless system that exploits relaying in the context of WiFi. We deploy three main competing strategies that have been proposed for relaying, Amplify-and-Forward (AF), DF and QMF, on the WarpLab software radio platform. We present experimental results--to the best of our knowledge, the first ones--that compare QMF, AF and DF in a realistic indoor setting. We find that QMF is a competitive scheme to the other two, offering in some cases up to 12% throughput benefits and up to 60% improvement in frame error-rates over the next best scheme. We then present a more advanced architecture for physical layer cooperation (termed QUILT), that seamlessly adapts to the underlying network configuration to achieve competitive or better performance than the best current approaches. It combines on-demand, opportunistic use of DF or QMF followed by interleaving at the relay, with hybrid decoding at the destination that extracts information from even potentially undecodable received frames. We theoretically quantify how our design choices affect the system performance. We also deploy QUILT on WarpLab and show through over-the-air experiments up to $5$ times FER improvement over the next best cooperative protocol.",,2015.0,10.5075/epfl-thesis-6500,109810278,semantic_scholar
559f9f70126c1cf3b90ec742ff8095ec1b9eb2e9,https://www.semanticscholar.org/paper/559f9f70126c1cf3b90ec742ff8095ec1b9eb2e9,Programming GPS and OpenStreetMap Applications with Java: The RealObject Application Framework,FROM VISION TO MISSION Software Objects and Real-World Representations Introduction Controlling Remote Objects via References Object-Oriented Programming Models in Physics The Vision Introduction Physical Objects Projecting Reality into Virtual Worlds Real-World Objects (ROs) Real Object Applications (RO-Apps) Real Object Application Framework (ROAF) OOA Analysis and Mission Introduction Language Analysis Semantic Network Gathering Information Data Dictionary Problem Statement Candidate Objects The Mission GLOBAL POSITIONING Space and Time Introduction A GeoPoint for Spatial Coordinate Systems A Position Interface for Various Coordinate Systems A Route to Manage an Array of Positions NAVSTAR GPS GPSpoint implements GPSinfo GPStrace extends Route JavaGPS Requirements for a GPSunit Real vs. SimulatedMotion Recording a liveTrace Loading a GPX File to a GPStrace Play Back a GPStrace The GPS Package: roaf.gps From Geography to Cartography Introduction Map Projection Systems Requirements for a Map GUI The Framework Pattern Creating a MapPanel Creating a Swing Mapping Application Interacting via MapEvents Deploying the GPXviewer.jar The GUI Mapping Package REALOBJECTS (ROs) Objects in Motion Introduction Every RealObject has a GPSunit abstract Motion of a RealObject Creating a Motorcycle Observing Motorcycles Processing Digital Maps Introduction Overview A Map Compiler for OSM Data Collecting Geometry in the Field Map Attribution with JOSM Map Formats Processing Maps with Osmosis Parsing OSM Files with the OSMparser toolchain Configuration Rendering OpenStreetMaps with Kosmos Making Maps Navigable Introduction Map Compiler Branches Networking Creating a NavigableMap Conclusion Navigating Objects Introduction Navigation Systems Route Calculation Exploring the Graph with a GameMap The Navigator ROAPPS: REALOBJECT APPLICATIONS Separating the RO Client and the ROA Server Introduction Observing Remote Objects ROAF Client Software The roaf.util.RMI Class Client Server Architecture Introduction Status Report Four-Layer Architecture The ServerEngine The RealObjectsServer SOs: ServerObjects Server Tuning The RealObjectsBox ROAdio Broadcasting Rolling out a ROApp Introduction LC: The Game Scenario Server Architecture Application Layer: ROApp Extends ROServer ROF: The RealObject Framework ROApp clients: LCPlayer extends RealObject More ServerObjects The ROApp Scenario Mission Accomplished: Time to Play Introduction Using the Application Interactive GUIPlayers Intelligent Players Robocode ROAF: REAL OBJECT APPLICATION FRAMEWORK Evolution Introduction Game Scenario versus Real World ROApp Classification RO Classification Decouple Mature ROs The RealObject Hierarchy,,2012.0,10.1201/b11641,59997089,semantic_scholar
879d7ae67e566d2ed81499d22c9676187926996d,https://www.semanticscholar.org/paper/879d7ae67e566d2ed81499d22c9676187926996d,An IEEE 802.22 transceiver framework and its performance analysis on software defined radio for TV white space,"With rapid increase in new applications and services, there is huge demand for internet bandwidth. Several researchers around the world have found that, majority of licensed bands (mostly terrestrial TV band) are either unused or underused. These underutilized bands allocated for TV transmission are known as TV white space (TVWS). For effective utilization of TVWS, the IEEE 802.22 is proposed. The IEEE 802.22 wireless regional area network (WRAN) is the latest standard for effective utilization of TV bands. This standard is based on orthogonal frequency division multiplexing with various modulation techniques to provide different data rates. In this paper, an implementation framework for physical layer of IEEE 802.22 WRAN standard for normal mode is demonstrated and analyzed. This transceiver is implemented using the National Instruments Laboratory Virtual Instrument Engineering Workbench programming software on the National Instruments universal software radio peripheral 2952R. We have also analyzed different blocks of IEEE 802.22 based on their execution time, and identify the critical blocks of IEEE 802.22 that should be optimized for real-time applications for commercial product development and field deployments. We have also highlighted the difference between theoretical and practical performance of the considered error control codes for IEEE 802.22 specified block size. Additionally, various covariance based spectrum sensing methods are also analyzed for real-world environment.",Telecommun. Syst.,2018.0,10.1007/s11235-017-0417-x,49602924,semantic_scholar
8cc243723a5f33a1602efd4b743c4ae605c8e0f9,https://www.semanticscholar.org/paper/8cc243723a5f33a1602efd4b743c4ae605c8e0f9,Open Archive TOULOUSE Archive Ouverte (OATAO),"The Internet of Things promises an alwaysconnected future where the objects surrounding us will communicate in order to make our lives easier, more secure, etc. This evolution is a research opportunity as new solutions must be found to problems ranging from network interconnection to data mining. In the networking community, innovative solutions are being developed for the Device Layer of the Internet of Things, which includes the IoT wireless protocols. In order to study their performance, researchers turn more often to real world platforms, commonly designated by the term “testbeds”, on which they may implement and test the protocols and algorithms. This is even more important in the Industrial IoT field, where environments are perturbed by industrial systems like automated production systems. In this paper, after a brief presentation of the context of testbeds, we introduce WiNo and OpenWiNo, an open hardware and software framework for fast-prototyping in the field of the Internet of Things. Compared to existing platforms, the solution WiNo+OpenWiNo offers a wide array of Physical layers and easy integration of various sensors as it is developed as part of the Arduino ecosystem. It also allows research teams to easily and quickly deploy their own testbed into real environments. Keywords— Internet of Things; Wireless Sensor Networks; Fast prototyping; Testbed; Open Hardware; Arduino",,2012.0,,86868442,semantic_scholar
e8b695ef59db23ec1f74520c2369b4ad1aaab2b1,https://www.semanticscholar.org/paper/e8b695ef59db23ec1f74520c2369b4ad1aaab2b1,Practical issues of implementing a hybrid multi-NIC wireless mesh-network,"Testbeds are a powerful tool to study wireless mesh and sensor networks as close as possible to real world application scenarios. In contrast to simulation or analytical approaches these installations face various kinds of environment parameters. Challenges related to the shared physical medium, operating system, and used hardware components do arise. In this technical report about the work-in-progress Distributed Embedded Systems testbed of 100 routers deployed at the Freie Universitat Berlin we focus on the software architecture and give an introduction to the network protocol stack of the Linux kernel. Furthermore, we discuss our rst experiences with a pilot network setup, the encountered problems and the achieved solutions. This writing continues our rst publication and builds upon the discussed overall testbed architecture, our experiment methodology, and aspired research objectives.",,2008.0,,30677109,semantic_scholar
ee7cf7c6fa6abe30ba73aef48af4a813005501f8,https://www.semanticscholar.org/paper/ee7cf7c6fa6abe30ba73aef48af4a813005501f8,Deploying and Managing State-of-the-Art Workstation Labs Like a Boss!,"Our PACE Computer-Aided Design (CAD) / Manufacturing (CAM) / Engineering (CAE) Lab is one of our flagship computer labs at Lehigh University. The Lab is uniquely supported by both the Mechanical Engineering & Mechanics (MEM) Department and the IT organization. It is also unique in that we make available pools of virtual workstations to supplement the 60 physical high-end Windows workstations that are critical to our teaching and research missions. In addition to the PACE Lab, we also have to deploy and support 25 other lab and research environments, each having their own unique software and requirements. The entire university has been using Symantec Ghost for image management for nearly 15 years, but we needed a more flexible and automated solution to create, deploy, and manage Windows Operating Systems. You will learn why we decided to focus our efforts on the completely free Microsoft Deployment Toolkit Lite Touch solution, and how it has increased the efficiency of our deployments. We are currently expanding these techniques and realizing their time-saving benefits even in our datacenter and support of IT infrastructure. We have also been successful in deploying the Windows 10 Technical Preview in a test lab and are keenly awaiting the final release of the updated deployment tools that will officially support the new Microsoft Operating System. You will walk away with real-world best-practice workflows that you can immediately implement in your own environment to realize some of the benefits that we have already seen.",SIGUCCS,2015.0,10.1145/2815546.2815570,30811598,semantic_scholar
92b72047f2e8aff46d822c9d27233a5a652cb24c,https://www.semanticscholar.org/paper/92b72047f2e8aff46d822c9d27233a5a652cb24c,Prototyping processing-demanding physical layer systems featuring single or multi-antenna schemes,"In the past years numerous algorithms, schemes and techniques have been proposed, in order to improve the performance of the wireless multi-antenna communication systems. Real-time multi-antenna testbeds are offering the means to analyse the real-world performance, implementation cost and feasibility of such novel techniques, accounting for hardware limitations and software constraints. This paper presents the physical-layer (PHY) design, implementation and validation of a high-performance real-time mobile WiMAX transceiver, accounting for low-level deployment issues and signal impairments. A first evaluation of the acquired results for both Single Input Single Output (SISO) and Multiple Input Multiple Output (MIMO) system-configurations, demonstrates the performance of the system using real-time channel emulation.",2011 19th European Signal Processing Conference,2011.0,,14752168,semantic_scholar
2b4cfc0b672aafc526bda6f7e35cf204ece0d34c,https://www.semanticscholar.org/paper/2b4cfc0b672aafc526bda6f7e35cf204ece0d34c,AC 2009-1203: A NOVEL INTERDISCIPLINARY SENSOR NETWORKS LABORATORY,"Today, networks of legacy and newer sophisticated sensors and actuators that combine reconfigurable gigascale semiconductor technology with emerging micro-mechanical systems (MEMS) and nanotechnology subsystems (i.e. bio-systems/chemical/fluidics/photonics/ etc) are being designed and deployed in almost every area of technology that impacts human endeavor and commerce. These smart sensors/actuators are being networked together through: either standards based or industry specific, proprietary, wired networks or newly emerging wireless networking technologies. Presently, at the twoand four-year college level, technicians and technologists in a wide variety of impacted disciplines are not receiving an adequate education about: fundamental sensor theory, basic sensor operation, sensor system deployment planning, appropriate data-transport and networking connectivity schemes, applications software, and impending system maintenance support needs of these increasingly more sophisticated and complex, smart sensor/actuator based systems. This paper will report on the development, organization, and use of a novel interdisciplinary sensor networks laboratory. The heart of the laboratory is a dedicated data-network (SensorNet) that emulates a wide area network or WAN. The SensorNet WAN nodes and other network access points allow for the interconnection of numerous types of industry specific and standard “area networks” typically utilized for the gathering of sensor data and directing other sensor functions, as well as, the associated PC’s and servers used to direct the sensor systems and warehouse the gathered data. This laboratory environment lends itself to real world case-study and problem-based type student-centered learning experiences that can be themselves integrated into established fields of technology that do not normally include this type of activity as part of the field’s traditional educational experience at the undergraduate level. I. Overview Although it is not uncommon for several different technology fields to converge together, it is somewhat unexpected to observe such an amalgamation rapidly triggering other technologic innovations that have widespread potential to change our relationship with the environment and our daily endeavors. However, this is just what is happening today. Only a short time ago, the Internet, the result of a convergence of several technologies, spawned the development of what is commonly known as the “Information Economy.” Today another innovative and important convergence of technologies has recently gained critical mass and recognition by business and industry, government, academia, and professional societies. It is the deployment of intricate systems involving complex sensors with embedded (ambient) intelligence and advanced actuators coupled with modern data-transport and networking technologies and applicationenabling software with data fusion capabilities. This rapidly evolving convergence of technologies, which allows us to implement sensor systems that gather in situ (remote), realtime, statistically relevant information and interpret it in new and novel ways, has already started to transform automation and process control systems. The technology of networked sensor systems has the very genuine potential to significantly impact almost every aspect of human endeavor by increasing system efficiency, reducing energy consumption, permitting the real-time monitoring of the “health” of the nation’s infrastructure and environment, and improving public health and safety. Applications are limitless! P ge 1.77.3 On a global level, the NSF has been calling this “grand convergence,” cyberinfrastructure. One may find many references to this concept, forecasts of potential future applications, reports on inprogress test projects such as HPWREN, NIMS, and ROADnet, and potential research funding opportunities on the NSF’s Web site [1] . However, most of this current, enthusiastic attention and promotion of cyberinfrastructure by the NSF is aimed at senior, graduate-level research institutions. Not surprisingly, most of the NSF’s recent Requests for Proposals (RFPs) in this area have been targeted at basic research about wireless sensor networks and systems and applications of these systems to infrastructure and environmental monitoring and other technology areas. While many applications of networked sensor systems are yet to be even thought of, the reality is that they are being deployed today and will continue to proliferate for many years to come until they eventually become as commonplace as a typical public utility like electricity. This paper describes aspects of an NSF funded CCLI project (DUE 0736888), titled, “The Sensor Networks Education Project” (SNEP) that seeks to develop materials and a model teaching laboratory that will be useful for other faculty and organizations at the twoand even the four-year college level to emulate. This project looks at this evolving convergence on a more practical level and speaks to the lack of engineering technology faculty expertise and teaching materials needed to infuse the newly recognized, exponentially growing knowledge base of networked sensor technology into the curricula and hence into the skill-sets of today’s twoand four-year technical college graduates – the technicians and technologists of tomorrow. This is the community of workers that will most likely deal with the design, deployment, updating, and maintenance of these systems. Today, networks of legacy and newer sophisticated sensors that combine reconfigurable gigascale semiconductor technology with emerging micro-electromechanical systems (MEMS) and nanotechnology subsystems [2] (i.e. bio-systems/chemical/molecular/photonic) are being designed and deployed in almost every area of technology that impacts human endeavor and commerce (i.e. Aerospace, Agriculture, Automotive, Biomedical, Building Automation, Energy Exploration and Production, Environmental Monitoring, Healthcare, Homeland Security, Industrial Automation, Infrastructure Monitoring, Information Technology, Manufacturing, Military, Pharmaceutical, Telecomm, Transportation, Weather Forecasting, etc). These sensors are being networked together through: either standards based or industry proprietary wired networks or emerging wireless networking technologies. Presently, at the twoand four-year college level, technologists and technicians in a wide variety of impacted disciplines are not receiving an adequate education about: fundamental sensor theory, basic sensor operation, sensor system deployment planning, appropriate data-transport and networking connectivity schemes, applications software, and impending system maintenance support needs of these increasingly more sophisticated sensor based systems. Recently, there has been a great deal of public dialogue about the out-sourcing of American manufacturing jobs and the effect of this reality on the nation’s future. Dealing with an ever increasing base of physical sensor networks in all areas of endeavor will not be something that can be done through a call to a help desk located in a foreign country. The apparent curriculum shortcoming regarding these topics within today’s associate and bachelors degree technology oriented programs is primarily due to the extremely rapid evolution and convergence of several key areas of electronics, computer, and MEMS technology (i.e. embedded processing, smart P ge 1.77.4 sensors/actuators, wired and wireless networking, etc), the lack of appropriate up-to-date educational materials, and a lack of appropriate faculty expertise in this rapidly expanding and remarkably cross-disciplinary field. II. Project Overview Over its two-year life-span, this CCLI Phase I project has as its primary goals the creation and testing of interdisciplinary student-centered learning materials primarily designed for a “field laboratory” type environment, the dissemination of these materials, and the development of faculty expertise in the multi-disciplinary field of networked sensors and modern active-learner teaching techniques. To accomplish these goals the project will: (1) develop and deploy a model, innovative, replicate-able, multi-interdisciplinary, case-study and problem-based oriented, networked, distributed sensor laboratory, (2) develop basic and advanced instructional materials and standard and “hybrid” laboratory activities related to the sensor laboratory that can be utilized for introductory courses in sensor technology or more advanced courses in networked sensor systems for use by both twoand four-year technology programs, (3) develop several prototype multifaceted educational modules that integrate traditional science and math based theory, practical real-world laboratory exercises, and science based, high-resolution, interactive simulation software, applicable to several of the major technology areas employing networked sensor technology (i.e. building automation and infrastructure monitoring and industrial automation), and (4) provide on-going local, regional, and national dissemination of these developed materials and laboratory experiences through hands-on faculty workshops and webbased distribution technologies including the National Science Digital Library (NSDL). In addition, for the duration of the project, continuous on-going professional development in the principles and applications of student-centered and active learner techniques will be provided to the recruited college faculty that will take part in the project. Research has shown that long term professional development programs are more effective than short-term workshops. For this project to be successful, the participating faculty must learn how to effectively integrate content and pedagogy in a way that actively engages students in individual and collaborative problem solving, analysis, synthesis, critical thinking, reasoning, and skillfully applying knowledge in real-w",,,,55903198,semantic_scholar
598a92dbe934f935de53dc24fdd0a1ba6a212daf,https://www.semanticscholar.org/paper/598a92dbe934f935de53dc24fdd0a1ba6a212daf,Wireless network virtualization : a techno-economic analysis and a service differentiation strategy,"Virtualization of wireless networks can significantly lower the capital expenditures (CAPEX) and operational expenditures (OPEX) by enabling resource sharing among multiple parties. Virtual network operators (VNOs), i.e. mobile virtual network operators (MVNOs) and over the top service providers (SPs) are becoming prominent players in wireless network markets with their differentiated service provisioning. This changing business model is beneficial for both the network operators and the VNOs, as network operators can increase their revenues by leasing resources to the VNOs who in turn, can implement their own network without having to deploy expensive physical networks. 
 
In future 5G networks, VNOs will play even more important role by providing various differentiated services using different wireless technologies. This requires provisioning of technologyagnostic physical infrastructure on which VNOs will be able to build their customized, isolated network slices tailored for optimal performance of the intended services. Research on Wireless network virtualization is a fairly recent trend and there is lack of an end-to-end solution for wireless network virtualization architectures in the open literature. In this respect, in this thesis, three architectural frameworks for wireless network virtualization have been proposed that differ in their degree of segregation between the signal processing and the radio accessunits. The frameworks also differ in terms of their associated CAPEX & OPEX as well as in terms of their achievable quality of service (QoS). For this reason, selection of a particular virtualization model for a particular service deployment is a multi-dimensional problem. Hence, a multi-criteria utility model has been developed that accounts for network cost & QoS trade-offs in order to enable the design and optimization of wireless access virtualization architectures that best comply with the investment and service-level requirements of network operators (and/or service providers). 
 
The second phase of the thesis focuses on the architectural requirements for provisioning heterogeneous virtual networks on a common physical substrate. It has been argued that software defined network (SDN) and cloud computing technologies are the key enablers for deploying such a network model. The existing proposals in the open literature focus on wireless network solutions for a particular radio access technology (RAT), e.g., WiFi, cellular, wireless sensor network (WSN), etc. or a particular part of a network (e.g., cellular core vs access networks). But an integral solution for programmable, elastic, virtualized heterogeneous networks is not available in the open literature. Hence, a blueprint for the deployment of an end-to-end programmable & flexible heterogeneous virtual wireless network (HVWN) infrastructure using SDN & cloud computing has been laid out in this chapter. Open problems and challenges in realizing a programmable, elastic HVWN have also been identified. 
 
Next, in the third phase of the thesis, the case of provisioning differentiated services in a cloud-based software-defined virtual wireless network environment has been studied. We have focused on a particular part of the generalized architecture proposed in the second part of the thesis, i.e., the case of programmable virtualized wireless networks that consists of cellular and fixed WiFi networks. More specifically we have studied how differentiated services can be provided in such a programmable virtualized platform. We have proposed to use the spare bits of OpenFlow packet structure to implement virtual network entities. Use of northbound APIs has been emphasized for composing complex wireless network applications. Emulation results show that the SDN-based virtualized wireless networks are able to meet the critical performance requirements of carrier networks. 
 
In the final part of the thesis, we focused on full duplex (FD) deployment of multi-cell networks. Current cellular networks are suffering from spectrum ossification problem. In a virtualized environment where multiple VNOs will compete for access to shared radio resources, the spectrum scarcity problem will be more severe. In such context, FD systems can provide an efficient solution by doubling the spectrum efficiency. In our research, we have identified the critical challenges for real world deployment of multi-tier FD multi-cell networks. We have analyzed FD performance trade-offs for a dense urban multi-tier cellular network. We have used the Madrid grid model proposed by METIS project that consists of macro and pico cells. We also have investigated the impact of co-located BS interference in FD performance for a single-tier homogeneous network deployment. We have proposed intelligent proportional fair joint user selection and power control algorithms to harness the gain of FD deployment. We have developed algorithms for both cloud radio access network (C-RAN) and traditional distributed RAN (D-RAN) network models. Extensive system-level simulation results show that using the devised algorithms the FD systems are able to achieve significant performance gain .",,2016.0,,113627971,semantic_scholar
0c177e2385aad71584840a21b23ebd4b58c00132,https://www.semanticscholar.org/paper/0c177e2385aad71584840a21b23ebd4b58c00132,Service Virtualization: Reality Is Overrated,"Software drives innovation and success in todays business world. Yet critical software projects consistently come in late, defective, and way over budget. So whats the problem? Get ready for a shock, because the answer to the problem is to avoid reality altogether. A new IT practice and technology called Service Virtualization (SV) is industrializing the process of simulating everything in our software development and test environments. Yes, fake systems are even better than the real thing for most of the design and development lifecycle, and SV is already making a huge impact at some of the worlds biggest companies. Service Virtualization:Reality Is Overratedis the first book to present this powerful new method for simulating the behavior, data, and responsiveness of specific components in complex applications. By faking out dependency constraints, SV delivers dramatic improvements in speed, cost, performance, and agility to the development of enterprise application software. Writing for executive and technical readers alike, SV inventor John Michelsen and Jason English capture lessons learned from the first five years of applying this game-changing practice in real customer environments. Other industriesfrom aviation to medicinealready understand the power of simulation to solve real-world constraints and deliver new products to market better, faster, and cheaper. Now its time to apply the same thinking to our software. For more information, see servicevirtualization.com. What youll learnYou will learn why, when, where, and how to deploy service virtualization (SV) solutions to mitigate or eliminate the constraints of an unavailable or unready service system by simulating its dependent components in order to deliver better enterprise software faster and at lower cost. In particular, you will learn step-by-step why, when, where, and how to deploy the following SV solutions: shift-left infrastructure availability performance readiness test scenario management Who this book is for This book is not only for IT practitioners on engineering, testing, and environments teams engaged in the development and delivery of enterprise software, but also for executives of companies in all sectors who need to understand and implement emergent opportunities to improve the time to market and overall competitiveness of any outward-facing business strategy that has a software application component. Table of ContentsForeword by Burt Klein Chapter 1. Introduction Service Virtualization Briefly Defined Key Practices Enabled by SV Shift-Left Infrastructure Availability Performance Readiness Test Scenario Management Navigating This Book Chapter 2. The Business Imperative: Innovate or Die Consumers Have No Mercy Business Demands Agile Software Delivery Increased Change and Complexity for IT Simulation Is Not Just for Other Industries Chapter 3. How We Got Here From Monolithic to Composite Apps Todays Complex Service Environments From Waterfall to Agile Development Chapter 4. Constraints: The Enemy of Agility Unavailable Systems and Environments Conflicting Delivery Schedules Data Management and Volatility Third Party Costs and Control Chapter 5. What is Service Virtualization? The Opposite of Server Virtualization Creation of a Virtual Service Maintaining Virtual Services What Kinds of Things You Can virtualize Virtual Service Environments (VSEs) Chapter 6. Where to Start with SV? Pick a Hairy Problem Identify Stakeholders Set Real Value Goals for Releases Avoid Inappropriate Technologies Chapter 7. Capabilities of Service Virtualization Technology Live-Like Development Environment Automation Eliminates Manual Stubbing and Maintenance Enables Parallel Dev and Test No more Availability Problem Platform-Neutrality Chapter 8. Best Practice #1: Shift-Left Reducing Wait Time Early Component and System Testing Define SV from Capture Define Incomplete SV from Requirements Expected Results Customer Example Chapter 9. Best Practice #2: Infrastructure Availability Finding Over-Utilized Resources Virtualizing Mainframes Avoiding Big IT Outlays Expected Results Customer Example Chapter 10. Best Practice #3: Performance Readiness Virtualizing Performance Environments Informing Performance from Production Expected Results Customer Example Chapter 11. Best Practice #4: Test Scenario Management Managing Big Data Shielding Teams from Volatility Massively Parallel Regression Testing Expected Results Customer Example Chapter 12. Rolling out Service Virtualization Who Pays for Service Virtualization? Overcoming Organizational Challenges Who Manages a VSE? Should I Have More Than One? Key Skills and Roles in a Virtual IT World Chapter 13. Service Virtualization in the DevTest Cloud Constraints of Cloud Dev and Test Achieving Elastic Cloud Environments Chapter 14. Assessing the Value Key Metrics for Success Areas for Improvement Chapter 15. Conclusion The Industrialized Software Supply Chain Innovate and Thrive Whats Next for SV? Glossary About the Authors",,2012.0,,113694918,semantic_scholar
d0d2d442baa8320b3a87af64b1a5a6e98497ae3c,https://www.semanticscholar.org/paper/d0d2d442baa8320b3a87af64b1a5a6e98497ae3c,Analysis of Radio Communication Attenuation Using Geoprocessing Techniques,"Multi-Agent Geo-Simulation (MAGS) aims to simulate phenomena involving a large number of autonomous situated actors (implemented as software agents) evolving and interacting within a Virtual representation of the Geographic Environment (VGE). A radio communication system is a typical complex dynamic phenomena where transmitter and receiver antennas are constantly constrained by the physical environment in which it they are deployed. In the real world, radio transmissions are subject to propagation effects which deeply affect the received signals because of geographic and environmental characteristics (foliage and vegetation, buildings, mountains and hills, etc.). Using geoprocessing techniques, we propose an automated approach to build semantically-informed and geometrically-accurate virtual geographic environments which uses Geographic Information System (GIS) data and builds an informed graph-based structure called Informed Virtual Geographic Environment (IVGE). In addition, we propose a multi-agent prototype to analyze the attenuation effect due to the radio signal’s traversal between antennas (simulated as software agents) through terrain shape, vegetation area, and buildings using a 3D line-of-sight computation technique. Keywords-Informed Virtual Geographic Environment (IVGE); Radio Signal Propagation; Line-Of-Sight; Multi-Agent GeoSimulation (MAGS).",,2011.0,,166224141,semantic_scholar
eebc85dcbbf0b5904316256d57cf7e9c3488d024,https://www.semanticscholar.org/paper/eebc85dcbbf0b5904316256d57cf7e9c3488d024,E-science approaches in molecular science,"Computer simulations of the properties of processes and materials are becoming increasingly necessary in several technological and environmental studies. This implies a growing demand of computing resources that severely exploits computational environments in terms of sustainability and reliability of the infrastructure.
 The developments in computing hardware and software, in particular the deployment of world-wide reliable Grid Computing infrastructures, the adoption of innovative computing approaches like the General Purpose Graphic Processing Unit (GPGPU) Computing and the High Performance Network environments, stimulate the exploitation of new approaches and methodologies in Computational Sciences. Furthermore the advances made in the World Wide Web, allow the implementation of Web sites from which the simulation of elementary chemical processes at molecular level is performed combining various techniques and computational approaches which are executed on High Throughput Computing (HTC) and/or High Performance Computing (HPC) platforms.
 The ubiquity of information and computing resources has impacted on the researchers? productivity, in a similar way the same technologies impacted everyone?s daily life. The E-science technologies facilitate the exchange of information among researchers, enhance the collaborative work and increase the quality of dissemination of results. Several European initiatives are devoted to facilitate researchers? work and to establish networks among researchers of the various disciplines, enabling some European research groups to reach leading positions in their disciplines. We have got the support of the EU COST (COllaboration in Science and Technology) Initiative in two Actions devoted to the facilitation of adoption of Grid and Distributed Computing technologies in Molecular and Matter Sciences. In particular I participated to the COST D23 Action: Metachem - Metalaboratories for Complex Computational Applications in Chemistry (2000-2005), and I coordinated the Working Group: Simbex: a metalaboratory for the a priori simulation of crossed molecular Beam Experiments. Furthermore I participated to the COST D37 Action: Grid Computing in Chemistry: GRIDCHEM (2006-2009), and I coordinated the Working Group: ELAMS: E-science and Learning Approaches in Molecular Science.
 The outcomes of both COST Actions contributed significantly to establish an active group of Computational Chemistry and Molecular and Matter Science laboratories which adopted the Grid Computing as an innovative computing paradigm for performing massive computational campaigns.
 Since 2004 the researchers of such laboratories joined the Virtual Organization (VO) CompChem established on the EGEE Grid Infrastructure, the largest distributed computing environment ever established worldwide, and coordinated by CERN (Conseil Europeen pour la Recherche Nucleaire). I served as VO Manager since the VO was established, under the coordination of Prof. Antonio Lagana, Department of Chemistry, University of Perugia.
 The present thesis covered a long period of research work focused on implementing some e-science instruments to the computational chemistry community, in particular the community of users belonging to the COMPCHEM Virtual Organization active in the EGEE/EGI European Grid Initiative.
 The Thesis describes some tools and approaches the author adopted to provide innovative tools to the Computational Chemistry community based on two main pillars:
1. approaches for running large computational campaigns on Grid Infrastructures 2. adopting virtual reality techniques for making more intuitive the interaction with nanoscale computing approaches and simplifying the definition of the initial conditions of the molecular simulations
 The research work originated 10 research papers, several of them produced as a joint work with European laboratories interested in the implementation of e-science tools for a smart, curious and demanding community like the Computational Chemistry one.
 The author has been able to provide a useful view of the molecular world through the use of virtual reality techniques, combined with the most advanced Web technologies, in particular using the ISO standard X3D for the 3D visualization and interaction with a virtual world. These innovative tools enabled the researchers to set up the environment for carrying out complex molecular simulations (as in the case of the Dl-Poly software package) in a intuitive and visual way. Once defined the species interacting in the considered molecular system, represented in a virtual world, the system produce the input file for the simulation and the Dl-Poly program may be launched, possibly on a Grid infrastructure to take benefit of the powerful available computational resources.
 In chapters 1, 2 and 3 the various steps toward the implementation of an a-priori molecular simulator on the EGEE/EGI Grid, for the COMPCHEM VO users, are reported.
 Molecular Virtual Reality applications are really useful as e-learning support tools for Chemistry students. To this purpose we implemented a Learning Management System based on a semantic web approach, described in chapter 7, and an assessment system, described in chapter 8, which have been used several times to assess the competences of students participating to the Erasmus Mundus Master of Science in Theoretical Chemistry and Computational Modeling. The system enables the coordinators of the Master to monitor the progresses made by the students an a daily basis.
 The author has shown how it is possible to use virtual reality approaches to describe a chemical experiment at both human and molecular level using a virtual reality approach. To this end a multi-scale virtual reality approach has been adopted to deal with the description of the physical environment,HVR. The main features of the virtual reality representation of the experiments and the potentiality of associating VRML and X3D with Java engine calculator are outlined in chapter 4.
 In chapter 5 an X3D Molecular Virtual Reality environment in which the researcher is able to interact with it by using immersive devices and dynamic gestures is described. By using the gestures the researcher is able to modify the composition of the molecular system by adding or subtracting functions and the molecular properties of the new species are evaluated in real time by invoking a Web Service implementing the simulation environment. This has required the assemblage of an innovative approach coupling the management of immersive devices with Web Services and molecular dynamics packages.
 In chapter 6 the author has presented an X3D Molecular Virtual Reality environment which makes usage of the most recent and powerful HTML and Web technologies. The approach implemented takes into account the modern approaches followed in implementing Social Networking environments and showed how useful these approaches are also in implementing scientific environments. We think such type of work is important also in consideration of how our lifestyle is changing, thanks to the ubiquity of the information, the availability of an increasing availability of storage and computing power. The social networking showed us how deep may be the impact of the computing and networking facility in the daily life and similarly the computational science, and the computational chemistry in particular, has to reshape the classical approaches and methodologies in order to gain advantage of the modern computing platforms and the powerfulness of the networking, distributed and mobile environments.",,2012.0,,215932175,semantic_scholar
f46a281fee7b6efcc4e1ba05755ea198ad008fdf,https://www.semanticscholar.org/paper/f46a281fee7b6efcc4e1ba05755ea198ad008fdf,Application of intelligent agent technology for knowledge management integration,"Organizations invest in various knowledge management (KM) systems and tools to enable seamless integration of the constantly increasing volume and sources of information. This research first, presents a case study on the existing categories of KM systems and tools, their potential contribution to the KM process, and their pitfalls; second, it proposes a comprehensive methodology for building KM through the organization using software agent technology. This approach aims to address the research issue of how KM can be optimized using intelligent agents and how to enhance decision-making process. The proposed system is applied to a real-world project lifecycle case that is EPC (Engineering Procurement and Construction) project. A prototype of the system is presented where intelligent agents are the building blocks of a peer-to-peer organization wide system. The application was implemented using Eclipse technology, and the agents were deployed on the FIPA-OS (Foundation for Intelligent Physical Agents-Open-Source) environment, we used JESS (Java expert system shell) to develop the knowledge based of the agents' reasoning.",,2004.0,10.1109/ICCI.2004.6,62769875,semantic_scholar
d70a02826a51efc7d133f688a820e73e6a1a1b44,https://www.semanticscholar.org/paper/d70a02826a51efc7d133f688a820e73e6a1a1b44,Semantic High Level Querying in Sensor Networks,"The quick development and deployment of sensor technology within the general frame of the Internet of Things poses relevant opportunity and challenges. The sensor is not a pure data source, but an entity (Semantic Sensor Web) with associated metadata and it is a building block of a “worldwide distributed” real time database, to be processed through real-time queries. Important challenges are to achieve interoperability in connectivity and processing capabilities (queries) and to apply “intelligence” and processing capabilities as close as possible to the source of data. This paper presents the extension of a general architecture for data integration in which we add capabilities for processing of complex queries and discuss how they can be adapted to, and used by, an application in the Semantic Sensor Web, presenting a pilot study in environment and health domains. 1 Background and Motivation The rapid development and deployment of sensor technology involves many different types of sensors, both remote and in situ, with such diverse capabilities as range, modality, and manoeuvrability. It is possible today to utilize networks with multiple sensors to detect and identify objects of interest up close or from a great distance. Connected Objects – or the Internet of Things – is expected to be a significant new market and encompass a large variety of technologies and services in different domains. Transport, environmental management, health, agriculture, domestic appliances, building automation, energy efficiency will benefit of real-time reality mining, personal decision support capabilities provided by the growing information shadow (i.e. data traces) of people, goods and objects supplied by the huge data available from the emerging sensor Web [1]. Vertical applications can be developed to connect to and communicate with objects tailored for specific sub domains, service enablement to face fragmented connectivity, device standards, application information protocols etc. and device management. Building extending connectivity, connectivity tailored for object communication – with regards to business model, service level, billing etc, are possible exploitation areas of the Internet Connected Objects. Important challenges are to achieve interoperability in connectivity and processing capabilities (queries, etc.), to distribute “intelligence” and processing capabilities as close as possible to the source of data (the Giordani I., Toscani D., Archetti F. and Cislaghi M.. Semantic High Level Querying in Sensor Networks. DOI: 10.5220/0003116600720084 In Proceedings of the International Workshop on Semantic Sensor Web (SSW-2010), pages 72-84 ISBN: 978-989-8425-33-1 Copyright c 2010 SCITEPRESS (Science and Technology Publications, Lda.) sensor or mobile device), in order to avoid massive data flows and bottlenecks on the connectivity side. The sensor is not a pure data source, but an entity (Semantic Sensor Web) with associated domain metadata, capable of autonomous processing and it is a building block of a “worldwide distributed” real time database, to be processed through realtime queries. The vision of the Semantic Sensor Web promises to unify the real and the virtual world by integrating sensor technologies and Semantic Web technologies. Sensors and their data will be formally described and annotated in order to facilitate the common integration, discovery and querying of information. Since this semantic information ultimately needs to be communicated by the sensors themselves, one may wonder whether existing techniques for processing, querying and modeling sensor data are still applicable under this increased load of transmitted data. In the following of this paper we introduce the state of the art in data querying over network of data providers. In Sect. 2 we present the software architecture of a data integration system in which we added complex query processing features. Sect. 3 introduces the case study in which we deployed our system: the study of short term effect of air pollution on health. Sect. 4 presents the detailed implementation of the querying features together with results on real data sets. Finally, Sect 5 presents the conclusions and future work. 1.1 State of the Art This paper stems from the work presented in [12], in which is presented a software system aimed at forecasting the demand of patient admissions on health care structures due to environmental pollution. The target users of this decision sup-port tool are health care managers and public administrators, which need help in resource allocation and policies implementation. The key feature of that system was the algorithmic kernel, to perform time series analysis through Autoregressive Hidden Markov Models (AHMM) [7]. The scenario in which the system has been deployed is the research project LENVIS1, which is aimed to create a network of services for data and information sharing based on heterogeneous and distributed data sources and modeling. One of the innovations brought by LENVIS is the “service oriented business intelligence”, i.e. an approach to Business Intelligence in which the information presented to the user comes from data processing that is performed online, i.e. data are extracted under request of the applications, and on the basis of data availability, i.e. data are exchanged through web services, which does not guarantee response time neither availability. Such a complex environment, in which data sources are distributed over the internet, is common to several problems and has been faced by different approaches. One of them is that of [13], in which “monitoring queries” continuously collect data about spatially-related physical phenomena. An algorithm, called Adaptive Pocket Driven Trajectories, is used to select data collection paths based on the spatial layout of sen1 LENVIS Localised environmental and health information services for all. FP7-ICT-2007-2. Project number 223925. www.lenvis.eu 73",SSW,2010.0,10.5220/0003116600720084,16601247,semantic_scholar
3f826f1f349ee707639c39d231259498b14215c5,https://www.semanticscholar.org/paper/3f826f1f349ee707639c39d231259498b14215c5,AC 2011-2689: SMART GRID DEVELOPMENT IN ELECTRICAL DIS- TRIBUTION NETWORK,"This paper will focus on smart grid project design and implementation. The project was developed by students and demonstrates new ideas and teamwork. This project was successfully completed and has been developed, implemented and assessed. Topics covered are: how to build a smart gird by utilizing computer application software tools, design, simulation, and diagnoses of electrical distribution systems. All the real world components in electrical distribution network such as residential, commercial and industrial building are modeled in this project. Background The purpose of this project is to design and implement a small scale electric power network by a team of seven students, supervised by a faculty member. The students’ background is in electrical engineering with emphasis in electric power system. The students conducted a study in the field of Smart Grid technologies for history and background information. This work led to designing and implementing a small model of a smart gird power distribution network. The power grid represents the real world aspirations of both government and private industry geared toward building a more reliable, responsive, and overall efficient network of residential, industrial and commercial buildings. Since the concept of a smart grid is very vague, students chose to implement a time tested and proven aspect of such technology known as smart meters. The smart meter is a wireless device connected to every house, industrial and commercial buildings to provide essential feedback in real time to the power companies. This feedback could be in the form of a fault occurring at that said location, or illegal energy usage. This feedback in real time is very useful to the power companies, given the fact that most rely on feedback via a phone call from the consumer before they know whether or not there is a fault in the system. Another purpose of implementing the grid was to simulate metering technology at the residential, industrial and commercial level. These meters would send data to a computer which is a simulated control room in order to read where certain faults occur in the system. In turn one could control which areas of the grid would be supplying the power. This represents a simulation of the power company’s ability to read and send vital information throughout the grid, thus improving the responsiveness and reliability of the network. Figure 1 illustrates the completed model after it was built and during testing. The lifecycle of this project was implemented in three different phases and started in September of 2009 and it was completed in May of 2010. Planning and analysis was completed in phase I, design and implementation in phase II, and documentation and students’ assessment in phase III. Figure 1. A model of smart grid in electrical distribution system Phase I: Planning and Analysis Initially, each team member worked on individual research on the concepts of smart grid its purpose. Later on, a decision was made as to what the team wanted to demonstrate with the project. The decision was made to show specifically how smart meters would work and help in fault detection as well as saving money by removing the need for meter readers to read the power meters every month. A project leader was elected by the team members to coordinate the team work. Meetings were then set up by the project leader, to brainstorm on how the actual implementation was going to be planned. Microsoft Project software was very instrumental to organize the work of the team. Tasks were assigned with specific due dates to keep the project on schedule and under budget. Requirements Since this project was spread over three quarters, students had many deadlines and task that had to be met in order to have a successful project. There were three phases to this project, research, background study and planning during the 1 quarter, design and building during the 2 quarter and the final stage of testing and troubleshooting during the 3 quarter along with final oral presentation, simulation documentation and assessment of the project. The students made documents and recorded each steps of the project down to each task and timeline by using Microsoft Project software. The project advisor coordinated the project steps and students were required to present a weekly progress report. This step insured that the project was moving smooth and on the track. The group was divided into two teams, one in software teams which consisted of two members and a hardware team which consisted of other five group members. The software team was in charge of all the coding and GUI implementation so the actual grid can communicate back and forth with the computer. The hardware team was in charge of the physical grid which consisted of the circuit that was built using logic chips such as MUXs, and Flip-Flops, wiring, creating a map on the grid with houses, roads, school, power stations, sub-stations, transmission lines, and distributions lines. The commercial site consists of shopping area, factories, stadium, school and so forth. In final stages of the project, testing, debugging and troubleshooting was performed in order to assure that hardware components and related software can communicate back and forth in a proper sequence. Much of the requirements had the made along the way since this was very new to all students. Phase II: Design and Implementation The design started immediately after the clear definition of the project requirement and purpose. To lower the cost and improve the safety, the design would be a DC (Direct Current) representation of an AC (Alternate Current) system. The system was designed by drawing out the model of a city and the specific buildings to exist in that city. The design was based on what took place in the planning stage which defined how the city and buildings will receive their power and the power. Figure 2. The process of building a smart grid The next challenge in the design process was solving the problem of switches and smart meters. Figure 2 shows the design of the smart meters and placement of LEDs (Light Emitting Diode). The LEDs will represent whether a particular house, building or transformer has power on or off. If for any reason an LED was not lit, then that particular item does not have power. The faults were determined by voltages because even if the building wasn’t drawing power, then there still would be a voltage on the line. This voltage was then sent to a 64 to 1 multiplexor which was then sent to the microcontroller to determine faults. To turn the power of buildings “on” and “off” a common NPN transistor (2n2222) and the base current was provided by a flip flop integrated circuit. Flip flops were used due to I/O’s limitations of the PIC. Figure 3 was duplicated for every transformer, with the only difference being the number of buildings being fed from the transformer which is the first LED after the 12V source. Figure 3. Circuit diagram for buildings Implementation A Smart Grid system includes a power meter which enables the communication systems to update the utility about its condition and the electronics to control the meter. The old electromechanical meters that were used are becoming obsolete since they cannot support the features that the utilities desire to have such as monitoring and controlling power supplied to its customers. Utilities wishes to monitor power consumption so that they can accurately predict how much power will be used during peak and down times. This information is helpful in producing sufficient energy and better efficiency in power waste. It can also help to pinpoint locations of power outages leading to a quicker recovery time. Challenges in implementation of the system are based on a couple of issues. First is the cost. It could cost upwards of $1000 for each smart meter, depending on features to be installed for each house or business. The costs can add up quickly, and the utilities don't see any immediate savings or incentive to deploy the smart grid in a very near future. The system designed in this project is using smart meters with a simulated wireless connection to the central servers at the utilities. The meters would send a signal to the central computer to update its status, power consumption, and other things. It can be designed in a way that it will have a battery backup for when the power is interrupted, or have the central computer assume it is off when it doesn't send a signal at the regular time intervals. Obviously the latter option would be the most cost effective and would use less power to run. But having power to the smart meter could also be beneficial because diagnostics could be run to determine if the power went out or if the meter is having its own internal hardware problems. A wireless signal was simulated for this project, but in real world application one can use either wireless, normal phone lines, or communications over power line. Most utilities already have communications systems set up through their power lines and using this method would be most cost effective. Having wireless, on the other hand, frees up usage of the power lines reducing their stress and prolonging the cables life. Companies are developing and testing their own systems using one of those options. In any case, it is based on hardware availability and cost effectiveness. Figure 4 illustrates communication with the smart grid. Figure 4. Communication with the smart grid Phase III: Documentation and Students’ Assessment In phase III of the project, the students provided a detail documentation of the project which includes cost analysis and different phases of the design. An electronic copy of this documentation and demo presentation was produced in a DVD. The following assessment and lessons learned was observed during the life cycle of the project: 1) When the main board that was used in the final project was constructed, the problems of wiring of all o",,2011.0,,73660185,semantic_scholar
3230c340ca4eeb94c7648fd516ed26c70b12a249,https://www.semanticscholar.org/paper/3230c340ca4eeb94c7648fd516ed26c70b12a249,Loading Architecture for a Sensor Web Browser on Digital Earth_final_0,"The world-wide sensor web observes real world phenomena at a particular moment in time with a large number of geo-referenced sensors. Sensor web needs a sensor web browser for accessing distributed and heterogeneous sensor networks in a coherent frontend. The Digital Earth provides a geo-referenced three-dimensional environment for intuitively browsing and displaying sensor observations. However, the major challenge is to load the vast amount of sensor observations from servers to a sensor web browser while minimizing the delay that a user experiences. This research uses two techniques to address the challenge. First, the browser caches transmitted data onto the local hard drive to reduce redundant internet bandwidth consumption. Second, this work designs a loading architecture to decouple sensor data loading, rendering, and browsing. The proposed scheme is implemented in the GeoCENS sensor web browser. To the best of our knowledge, with the proposed loading architecture, GeoCENS is the first Digital Earth-based sensor web browser. Background and Relevance The Digital Earth was envisioned in Gore’s 1998 speech1. The Digital Earth is a threedimensional visualization model of the physical Earth, which contains high resolution imagery and digital elevation models. Users are able to intuitively interact with Digital Earth by navigation features such as flying to places or floating above the surface. In recent years more and more publicly-available Digital Earths are revealed, such as ESRI’s ArcGIS Explorer2, NASA’s World Wind3, Microsoft’s Bing Map4, and Google Earth5. One of the visions that Gore described is that people can access vast amount of scientific information through the Digital Earth to help them understand real world. Therefore, at the same time that Digital Earths were being developed, the deployments of the world-wide sensor web were also put into practice for observing heterogeneous, environmental phenomena. The sensor web concept originated at the NASA/Jet Propulsion Laboratory in 1997 (Delin et al. 2005; Liang et al. 2005) for acquiring environmental information by integrating massive spatially distributed consumermarket sensors. The world-wide sensor web has been applied in a range of applications, including: large-scale monitoring of the environment (Hart and Martinez 2006), civil structures (Xu et al. 2004), roadways (Hsieh 2004), and animal habitats (Mainwaring et al. 2002). Ranging from video camera networks that monitor real-time traffic, matchbox-sized wireless sensor networks embedded in the environment for monitoring habitats, sensor webs generate tremendous volumes of valuable observations, enabling scientists to observe previously unobservable phenomena. Similar to how the World 1 http://www.isde5.org/al_gore_speech.htm 2 http://www.esri.com/software/arcgis/explorer/index.html 3 http://worldwind.arc.nasa.gov/java/ 4 http://www.bing.com/maps/ 5 http://www.google.com/earth/index.html Wide Web needs an Internet browser for viewing web pages, the sensor web needs a coherent frontend for accessing the distributed and heterogeneous sensor networks. This kind of coherent frontend is called sensor web browser. In order to achieve Gore’s Digital Earth and sensor web visions, an online Digital Earthbased sensor web browser for users to browse, search, manage, and exchange sensor data is needed. However, transmitting the vast amount of sensor readings from servers to a sensor web browser with minimum delay is very challenging. Therefore, the goal of this paper is to present a sensor web data loading architecture for addressing the following two issues: (1) transmitting vast amount of sensor data and (2) minimizing the delay time that user might experience. To address the first issue, efficient data loading management utilizing a local cache is applied. To address the second issue, a loading architecture that decouples loading and browsing, a speculation mechanism, and a dynamic priority queue (Xhafa & Tonguz 2001) are applied. With the proposed scheme, efficient sensor web data loading and good user experience are attained. The sensor data used in this work are historical and can be represented as points on the Digital Earth. Methodology Issue 1: Transmitting Vast Amount of Sensor Data To mitigate this issue, we can adopt strategies from Web browsers and how they are able to minimize redundant transmissions. Web browsers cache data, which means they store web page requests and associated responses as key-value pairs on the local disk in order to reduce unnecessary and redundant transmissions, and to improve end-to-end latency. Similarly, this work implements a caching strategy to reduce the unnecessary transmission of sensor data in the sensor web. However, sensor web browsers’ requests and sensor observations are different from web browsers’ requests and web pages. We cannot simply manage sensor web requests and responses as key-value pairs in web caches. A sensor web request can be interpreted as asking for sensor observations of a certain phenomenon in a set of spatio-temporal cubes. The spatio-temporal cubes can be distributed irregularly in space and time. An effective sensor web caching strategy requires efficient management of these spatio-temporal cube requests. Therefore, we develop a new spatio-temporal indexing structure, LOST-Tree (LOading SpatioTemporal indexing tree) to manage sensor data loading with a local cache. A LOST-Tree manages sensor data loading of one phenomenon. A LOST-Tree consists of two techniques. First, instead of indexing massive amounts of raw sensor data, a LOST-Tree will index requests (i.e., spatio-temporal cubes). Because no actual data stored in LOST-Tree, it is small and able to fit into memory for efficient processing. Second, a LOST-Tree applies any two regular and aggregatable structures onto the spatial and temporal domains for transforming irregular spatio-temporal cubes into regular cubes. For instance, this work implements with a Quadtree (Finkel & Bentley 1974) and the Gregorian calendar. These two structures are integrated by using temporal structure (i.e., the Gregorian calendar in this work) as main structure and embedding spatial structure (i.e., Quadtree in this work) in nodes of temporal structure. In this way, a record in LOST-Tree represents a spatio-temporal cube, which allows simple look-up searches (rather than range query) for determining unloaded cubes. Records are inserted into LOST-Tree only if the corresponding cubes have been loaded. Since both structures are aggregatable, the more cubes are loaded, the fewer records remain. Therefore, a LOST-Tree is scalable, light weight, and able to efficiently identify unloaded potions in sensor web browser. By incorporating LOST-Trees into a sensor web browser, previously loaded sensor data can be retrieved from the local cache. The end-to-end latency can be reduced and the Internet bandwidth can be efficiently utilized on transmitting the data that has not been loaded. Issue 2: Minimizing the delay time that users experience In order to minimize the delay users might experience when loading sensor web data, we need to decouple data loading, rendering, and browsing. We implemented the following performance improvement strategies in the GeoCENS sensor web browser (Liang et al. 2010). (1) Checker: In order to reduce the requesting frequency, instead of executing loading processes whenever the Digital Earth is moving, loading processes start when the earth has stopped moving for more than a defined period of time (e.g., 500 milliseconds). This follows the assumption that a user’s area of interest corresponds to where the user stops moving the Digital Earth. (2) Wrapper: After a user requests a spatio-temporal cube, a thread is trigged to determine the portions that are not available in the local cache (by using a LOST-Tree). The wrapper also filters out the requests that have been issued, and composes new loading tasks. (3) Loader: After the loading tasks are created, these tasks are stored into a queue waiting for being sent to the servers. A thread-pool with pre-defined number of threads polls these loading tasks from the queue and issues requests based on the loading tasks to the servers. After the data are transmitted to the browser, a thread is trigged to parse the data and store them in local cache. (4) Poller: The checker, wrapper, and loader are for the data loading, and the poller is for rendering. A timer runs periodically to check if user moves the Digital Earth. If not, a thread will then be trigged to retrieve data from the local cache and render the data on Digital Earth. Besides applying the decoupling architecture, two additional mechanisms are implemented for improving a user’s experience. They are speculation and dynamic priority queue. Firstly, speculation means the system issues requests before a user issues the requests. For instance, we expect users will browse the nearby regions around their initial area of interest. Therefore, when a user requests a spatio-temporal cube, the spatial component (e.g., bounding box) of the cube is then expanded. Secondly, users are allowed to navigate freely within the Digital Earth environment. A user may decide to move to other places before the previous loading tasks are digested. As a result, it is important to prioritize the loading tasks dynamically. For example, if we manage the loading tasks with a first-in-first-out (FIFO) strategy, new requests will not be executed until the old requests are finished. In this case, users may feel slower system performance because the system is background loading data they aren’t expecting. Therefore, instead of following a FIFO queue, we apply a dynamic priority queue (Xhafa & Tonguz 2001) to prioritize the loading tasks. Whenever a user moves the Digital Earth, the priorities of loading tasks in the queue will be re-assigned with the distance between the current area of interest and the tasks’ loadi",,2011.0,,10895330,semantic_scholar
4e9f1e3cec101b425cdb304a98888e4e4db65baa,https://www.semanticscholar.org/paper/4e9f1e3cec101b425cdb304a98888e4e4db65baa,Cognitive and Contextual Enterprise Mobile Computing: Invited Keynote Talk,"The second wave of change presented by the age of mobility, wearables, and IoT focuses on how organizations and enterprises, from a wide variety of commercial areas and industries, will use and leverage the new technologies available. Businesses and industries that don't change with the times will simply cease to exist. Applications need to be powered by cognitive and contextual technologies to support real-time proactive decisions. These decisions will be based on the mobile context of a specific user or group of users, incorporating location, time of day, current user task, and more. Driven by the huge amounts of data produced by mobile and wearables devices, and influenced by privacy concerns, the next wave in computing will need to exploit data and computing at the edge of the network. Future mobile apps will have to be cognitive to 'understand' user intentions based on all the available interactions and unstructured data. Mobile applications are becoming increasingly ubiquitous, going beyond what end users can easily comprehend. Essentially, for both business-to-client (B2C) and business-to-business (B2B) apps, only about 30% of the development efforts appear in the interface of the mobile app. For example, areas such as the collaborative nature of the software or the shortened development cycle and time-to-market are not apparent to end users. The other 70% of the effort invested is dedicated to integrating the applications with back-office systems and developing those aspects of the application that operate behind the scenes. An important, yet often complex, part of the solution and mobile app takes place far from the public eye-in the back-office environment. It is there that various aspects of customer relationship management must be addressed: tracking usage data, pushing out messaging as needed, distributing apps to employees within the enterprise, and handling the wide variety of operational and management tasks-often involving the collection and monitoring of data from sensors and wearable devices. All this must be carried out while addressing security concerns that range from verifying user identities, to data protection, to blocking attempted breaches of the organization, and activation of malicious code. Of course, these tasks must be augmented by a systematic approach and vigilant maintenance of user privacy. The first wave of the mobile revolution focused on development platforms, run-time platforms, deployment, activation, and management tools for multi-platform environments, including comprehensive mobile device management (MDM). To realize the full potential of this revolution, we must capitalize on information about the context within which mobile devices are used. With both employees and customers, this context could be a simple piece of information such as the user location or time of use, the hour of the day, or the day of the week. The context could also be represented by more complex data, such as the amount of time used, type of activity performed, or user preferences. Further insight could include the relationship history with the user and the user's behavior as part of that relationship, as well as a long list of variables to be considered in various scenarios. Today, with the new wave of wearables, the definition of context is being further extended to include environmental factors such as temperature, weather, or pollution, as well as personal factors such as heart rate, movement, or even clothing worn. In both B2E and B2C situations, a context-dependent approach, based on the appropriate context for each specific user, offers a superior tool for working with both employees and clients alike. This mode of operation does not start and end with the individual user. Rather, it takes into account the people surrounding the user, the events taking place nearby, appliances or equipment activated, the user's daily schedule, as well as other, more general information, such as the environment and weather. Developing enterprise-wide, context-dependent, mobile solutions is still a complex challenge. A system of real added-value services must be developed, as well as a comprehensive architecture. These four-tier architectures comprise end-user devices like wearables and smartphones, connected to systems of engagement (SoEs), and systems of record (SoRs). All this is needed to enable data analytics and collection in the context where it is created. The data collected will allow further interaction with employees or customers, analytics, and follow-up actions based on the results of that analysis. We also need to ensure end-to-end (E2E) security across these four tiers, and to keep the data and application contexts in sync. These are just some of the challenges being addressed by IBM Research. As an example, these technologies could be deployed in the retail space, especially in brick-and-mortar stores. Identifying a customer entering a store, detecting her location among the aisles, and cross-referencing that data with the customer's transaction history, could lead to special offers tailor-made for that specific customer or suggestions relevant to her purchasing process. This technology enables real-world implementation of metrics, analytics, and other tools familiar to us from the online realm. We can now measure visits to physical stores in the same way we measure web page hits: analyze time spent in the store, the areas visited by the customer, and the results of those visits. In this way, we can also identify shoppers wandering around the store and understand when they are having trouble finding the product they want to purchase. We can also gain insight into the standard traffic patterns of shoppers and how they navigate a store's floors and departments. We might even consider redesigning the store layout to take advantage of this insight to enhance sales. In healthcare, the context can refer to insight extracted from data received from sensors on the patient, from either his mobile device or wearable technology, and information about the patient's environment and location at that moment in time. This data can help determine if any assistance is required. For example, if a patient is discharged from the hospital for continued at-home care, doctors can continue to remotely monitor his condition via a system of sensors and analytic tools that interpret the sensor readings. This approach can also be applied to the area of safety. Scientists at IBM Research are developing a platform that collects and analyzes data from wearable technology to protect the safety of employees working in construction, heavy industry, manufacturing, or out in the field. This solution can serve as a real-time warning system by analyzing information gathered from wearable sensors embedded in personal protective equipment, such as smart safety helmets and protective vests, and in the workers' individual smartphones. These sensors can continuously monitor a worker's pulse rate, movements, body temperature, and hydration level, as well as environmental factors such as noise level, and other parameters. The system can provide immediate alerts to the worker about any dangers in the work environment to prevent possible injury. It can also be used to prevent accidents before they happen or detect accidents once they occur. For example, with sophisticated algorithms, we can detect if a worker falls based on a sudden difference in elevations detected by an accelerometer, and then send an alert to notify her peers and supervisor or call for help. Monitoring can also help ensure safety in areas where continuous exposure to heat or dangerous materials must be limited based on regulated time periods. Mobile technologies can also help manage events with massive numbers of participants, such as professional soccer games, music festivals, and even large-scale public demonstrations, by sending alerts concerning long and growing lines or specific high-traffic areas. These technologies can be used to detect accidents typical of large-scale gatherings, send warnings about overcrowding, and alert the event organizers. In the same way, they can alleviate parking problems or guide public transportation operators- all via analysis and predictive analytics. IBM Research - Haifa is currently involved in multiple activities as part of IBM's MobileFirst initiative. Haifa researchers have a special expertise in time- and location-based intelligent applications, including visual maps that display activity contexts and predictive analytics systems for mobile data and users. In another area, IBM researchers in Haifa are developing new cognitive services driven from the unique data available on mobile and wearable devices. Looking to the future, the IBM Research team is further advancing the integration of wearable technology, augmented reality systems, and biometric tools for mobile user identity validation. Managing contextual data and analyzing the interaction between the different kinds of data presents fascinating challenges for the development of next-generation programming. For example, we need to rethink when and where data processing and computations should occur: Is it best to leave them at the user-device level, or perhaps they should be moved to the back-office systems, servers, and/or the cloud infrastructures with which the user device is connected? New-age applications are becoming more and more distributed. They operate on a wide range of devices, such as wearable technologies, use a variety of sensors, and depend on cloud-based systems. As a result, a new distributed programming paradigm is emerging to meet the needs of these use-cases and real-time scenarios. This paradigm needs to deal with massive amounts of devices, sensors, and data in business systems, and must be able to shift computation from the cloud to the edge, based on context in close to real-time. By processing data at the edge of the network, close to where the i",ISEC,2016.0,10.1145/2856636.2876471,9933019,semantic_scholar
fa12864c51585278e00cb926f185a2f69b70675b,https://www.semanticscholar.org/paper/fa12864c51585278e00cb926f185a2f69b70675b,EQUIP: a Software Platform for Distributed Interactive Systems,"EQUIP is a new software platform designed and engineered to support the development and deployment of distributed interactive systems, such as mixed reality user interfaces that combine distributed input and output devices to create a coordinated experience. EQUIP emphasises: cross-language development (currently C++ and Java), modularisation, extensibility, interactive performance, and heterogeneity of devices (from handheld devices to large servers and visualisation machines) and networks (including both wired and wireless technologies). A key element of EQUIP is its shared data service, which combines ideas from tuplespaces, general event systems and collaborative virtual environments. This data service provides a uniquely balanced treatment of state and event-based communication. It also supports distributed computation – through remote class loading – as well as passive data distribution. EQUIP has already been used in several projects within the EQUATOR Interdisciplinary Research Collaboration (IRC) in the UK, and is freely available in source form (currently known to work on Windows, IRIX and MacOS-X platforms). INTRODUCTION The development of novel interactive devices and the deployment of mobile communication infrastructures have fuelled a growing focus on ubiquitous interactive systems that support people within real world environments. These systems place digital information in physical spaces [28] focusing on the delivery of information to users through a heterogeneous collection of devices ranging from handheld and wearable computers to large embedded displays. The majority of these systems have exploited a sense of location as a contextual cue to drive the interaction. An equally significant trend has been the growth in the number and diversity of collaborative virtual environments to manage cooperative interaction [2, 12, 26]. Just as ubiquitous computing environments exploit real world location, these systems exploit a sense of location within a virtual world as a contextual cue for interaction. However, despite significant similarities, these two research approaches have often tended to be seen in opposition to each other, with ubiquitous computing embedding computers with the world of users, and virtual environments embedding users within a computer generated world [16]. As part of our ongoing research we are exploring the advantages to be gained through the convergence of these approaches, allowing a collaborative virtual environment to be overlaid on top of a shared physical space. A number of key advantages motivate our desire to combine the physical and virtual to support interactive systems: The ability to exploit the coextensive virtual world as a ‘behind the scenes’ resource for coordinating and managing devices and interaction in the physical space. The opportunity to develop applications that span the physical and digital realms, for example that require collaboration between field operatives and controlroom personnel. The chance to support new kinds of interactive experience, combining elements from virtual worlds (e.g. rich media content, high interactivity) with varied modes of access over extended geographical areas and periods of time (e.g. across a city, over a period of days or weeks). Our ultimate goal is to develop a rich interactive experience that combines physical and digital space, with digital interaction becoming increasingly interwoven with everyday interaction in the physical world. This paper presents the EQUIP platform [9], developed to support the merging of physical and virtual environments as part of the EQUATOR Interdisciplinary Research Collaboration (IRC) in the UK [8]. EQUIP is freely available (including source) for other practitioners to make use of [9]. The rest of this paper gives an overview of EQUIP, and its key elements before",,2002.0,,15536154,semantic_scholar
41d472aaa26abdac72c008ffa206170e88d446e9,https://www.semanticscholar.org/paper/41d472aaa26abdac72c008ffa206170e88d446e9,Replicating augmented reality objects for multi-user interaction,"Augmented Reality (AR) is the combination of virtual objects and the physical world surrounding us. These virtual objects are used to enrich the real world. Because of technical improvements of mobile hardware, there are quite a number of AR applications deployed in the last decade. 
 
To illustrate the potential of the AR technique and to look for a new concept of human-computer interaction, we started the ’Augmented Reality for 3D Multi-user Interaction’ (ARMI) project. The goal of the ARMI project is to build an AR application where, two or more users, can work concurrently on the same virtual maquette. This maquette is visible through a Head Mounted Display (HMD) to display visual objects on top of the real world. The virtual maquette can be used for representing specific traffic situations with roads and cars but also for modelling other 3D scenarios. The following basic actions are supported: creating, selecting, moving, rotating, and deleting virtual objects. 
 
The virtual maquette, build by the ARMI project, depends on four distinct areas and we identify the following research areas: hand tracking, hand-pose estimation, 3D interfacing, and AR-object replication. This thesis looks into the possibilities of object replication for the AR application. First, a number of AR applications are considered and it becomes clear that most AR applications with multiple users are depending on a client-server approach and no object replication is used. Second, a number of replication systems are described in which more than one server is involved to keep the data consistent and a number of fundamental replication algorithms are discussed. Based on the related work the decision is made to use a speculative variant of an asynchronous majority consensus algorithm for the AR-object replication. 
 
Furthermore, in this thesis the development, implementation and evaluation of the AR-object replication is described. From the evaluation it becomes clear that it is difficult to satisfy all the required replication parameters. We notice a number of replication limits. For example a scaling problem, which means that the number of clients is limited, and specific user behaviour in terms of performed operations per second. Based on the evaluation we conclude that the relaxation of a number of replication parameters is necessary to keep the system responsive enough for an AR application.",,2009.0,,59834946,semantic_scholar
a2ca591957d1081bbf4b1a04c565b8f365c014d8,https://www.semanticscholar.org/paper/a2ca591957d1081bbf4b1a04c565b8f365c014d8,Literature Survey: A Design Approach to Smart System based on Internet of Thing (IoT) for Intelligent Transportation,"Recent years, the transportation efficiency and related issues have become one of the main focuses of the global world. Along this line, intelligent transportation systems (ITS) based on Internet of Things (IoT) provided a promising chance to resolve the challenges caused by the increasing transportation problems, such as traffic prediction, road status evaluation, traffic accident detection, etc. In this, The Internet of Things is based on the Internet, network wireless sensing and detection technologies to realize the intelligent recognition on the tagged traffic object, tracking, monitoring, managing and processed automatically. IoT based intelligent transportation systems are designed to support the Smart City vision, which aims at employing the advanced and powerful communication technologies for the administration of the city and the citizens. Keywords—IoT, transportation. I. LITERATURE SURVEY K.Ashokkumar, Baron Sam, R.Arshadprabhu, Britto [1] proposes the advances in cloud computing and web of things (IoT) have provided a promising chance to resolve the challenges caused by the increasing transportation problems. They tend to gift a unique multilayered conveyance knowledge cloud platform by exploitation cloud computing and IoT technologies to resolve the challenges caused by the increasing transportation issues. They present a novel multilayered vehicular data cloud platform by using cloud computing and IoT technologies. Two innovative vehicular data cloud services, an intelligent parking cloud service and a vehicular data mining cloud service in the IoT environment are also presented reviews. Amir-Mohammad Rahmani, Nanda Kumar Thanigaivelan, Tuan Nguyen Gia, Jose Granados, Behailu Negash, Pasi Liljeberg, and Hannu Tenhunen [2] proposes the strategic position of gateways to offer several higherlevel services such as local storage, real-time local data processing, embedded data mining, etc., proposing thus a Smart e-Health Gateway. By taking responsibility for handling some burdens of the sensor network and a remote healthcare center, a Smart e-Health Gateway can cope with many challenges in ubiquitous healthcare systems such as energy efficiency, scalability, and reliability issues. Michele Nitti, Luigi Atzori, and Irena Pletikosa Cvijikj [3] addressed the issue by analyzing possible strategies for the benefit of overall network navigability.They first propose five heuristics, which are based on local network properties and that are expected to have an impact on the overall network structure. Thet then perform extensive experiments, which are intended to analyze the performance in terms of giant components, average degree of connections, local clustering, and average path length. Jianli Pan, Raj Jain, Subharthi Paul, Tam Vu, Abusayeed Saifullah, Mo Sha [4] proposes an IoT framework with smart location-based automated and networked energy control, which uses smartphone platform and cloud-computing technologies to enable multiscale energy proportionality including building-, user-, and organizational-level energy proportionality. They further build a proof-of-concept IoT network and control system prototype and carried out real-world experiments, which demonstrate the effectiveness of the proposed solution. They envision that the broad application of the proposed solution has not only led to significant economic benefits in term of energy saving, improving home/office network intelligence, but also bought in a huge social implication in terms of global sustainability Catarinucci, L. , de Donno, D. , Mainetti, L. , Palano, L. [5] proposes a novel, IoT-aware, smart architecture for automatic monitoring and tracking of patients, personnel, and biomedical devices within hospitals and nursing institutes. Staying true to the IoT vision, they propose a smart hospital system (SHS), which relies on different, yet complementary, technologies, specifically RFID, WSN, and smart mobile, interoperating with each other through a Constrained Application Protocol (CoAP)/IPv6 over lowpower wireless personal area network (6LoWPAN)/representational state transfer (REST) network International Conference on Science and Engineering for Sustainable Development (ICSESD-2017) (www.jit.org.in) International Journal of Advanced Engineering, Management and Science (IJAEMS) Special Issue-1 https://dx.doi.org/10.24001/icsesd2017.49 ISSN : 2454-1311 www.ijaems.com Page | 195 infrastructure. The SHS is able to collect, in real time, both environmental conditions and patients' physiological parameters via an ultra-low-power hybrid sensing network (HSN) composed of 6LoWPAN nodes integrating UHF RFID functionalities. Sensed data are delivered to a control center where an advanced monitoring application (MA) makes them easily accessible by both local and remote users via a REST web service. Al-Fuqaha, A., Kalamazoo, MI, Guizani, M. , Mohammadi, M., Aledhari, M. [6] provides a more thorough summary of the most relevant protocols and application issues to enable researchers and application developers to get up to speed quickly on how the different protocols fit together to deliver desired functionalities without having to go through RFCs and the standards specifications. They also provides an overview of some of the key IoT challenges presented in the recent literature and provide a summary of related research work. Moreover, they explore the relation between the IoT and other emerging technologies including big data analytics and cloud and fog computing. They also presents the need for better horizontal integration among IoT services. Stecca, M., Moiso, C., Fornasa, M., Baglietto, P. [7] presents app execution platform (AEP), a platform that supports the design, deployment, execution, and management of IoT applications in the domain of smart home, smart car, and smart city. AEP was designed to coherently fulfill a set of requirements covered only partially or in a fragmented way by other IoT application platforms. AEP focuses on SO virtualization and on composite application (CA) orchestration and supports dynamic object availability. Yi-Bing Lin, Yun-Wei Lin, Chang-Yen Chih, Tzu-Yi Li [8] proposes an IoT device which is characterized by its “features” (e.g., temperature, vibration, and display) that are manipulated by the network applications. If a network application handles the individual device features independently, then we can write a software module for each device feature, and the network application can be simply constructed by including these brick-like device feature modules. Based on the concept of device feature, brick-like software modules can provide simple and efficient mechanism to develop IoT device applications and interactions. Ganz, F. , Puschmann, D. , Barnaghi, P. , Carrez, F. [9] provides a survey of the requirements and solutions and describes challenges in the area of information abstraction and presents an efficient workflow to extract meaningful information from raw sensor data based on the current stateof-the-art in this area and also identifies research directions at the edge of information abstraction for sensor data. To ease the understanding of the abstraction workflow process, they introduce a software toolkit that implements the introduced techniques and motivates to apply them on various data sets. Aijaz, A. , Aghvami, A.H.[10] provides the state of the art in cognitive M2M communications from a protocol stack perspective, covers the emerging standardization efforts and the latest developments on protocols for cognitive M2M networks which includes a centralized cognitive medium access control (MAC) protocol, a distributed cognitive MAC protocol, and a specially designed routing protocol for cognitive M2M networks. These protocols explicitly account for the peculiarities of cognitive radio environments. Performance evaluation demonstrates that the proposed protocols not only ensure protection to the primary users (PUs) but also fulfil the utility requirements of the secondary M2M networks. Tsirmpas, C., Anastasiou, A., Bountris, P., Koutsouris, D. [11] proposes a new methodology based on self organizing maps (SOMs) and fuzzy C-means (FCM) algorithms for profile generation as regards the activities of the user and their correlation with the available sensors. Moreover, we utilize the provided context to assign the generated profiles to more contextually complex activities. This methodology is being evaluated into an AAL structure equipped with several sensors. More precisely, they assess the proposed method in a data set generated by accelerometers and its performance over a number of everyday activities Mainetti, L., Lecce, Mighali, V. ; Patrono, L. [12] proposes a software architecture to easily mash-up constrained application protocol (CoAP) resources. It is able to discover the available devices and to virtualize them outside the physical network. These virtualizations are then exposed to the upper layers by a REpresentational State Transfer (REST) interface, so that the physical devices interact only with their own virtualization. Furthermore, the system provides simplified tools allowing the development of mash-up applications to different-skilled users. Finally, the architecture allows not only to monitor but also to control the devices, thus establishing a bidirectional communication channel. Hasan Omar Al-Sakran [13] presents a novel intelligent traffic administration system, based on Internet of Things, which is featured by low cost, high scalability, high compatibility, easy to upgrade, to replace traditional traffic management system and the proposed system can improve road traffic tremendously. The Internet of Things is based on the Internet, network wireless sensing and detection technologies to realize the intelligent recognition on the tagged traffic object, tracking, monitoring, managing and processed automatically. The paper proposes an architecture that integrates internet of things with",,2017.0,10.24001/ICSESD2017.49,96461930,semantic_scholar
a33da34007f78b7e24e08de6a3fe3a272c76ad9c,https://www.semanticscholar.org/paper/a33da34007f78b7e24e08de6a3fe3a272c76ad9c,Application of intelligent agent technology for knowledge management integration,"Organizations invest in various knowledge management (KM) systems and tools to enable seamless integration of the constantly increasing volume and sources of information. This research first, presents a case study on the existing categories of KM systems and tools, their potential contribution to the KM process, and their pitfalls; second, it proposes a comprehensive methodology for building KM through the organization using software agent technology. This approach aims to address the research issue of how KM can be optimized using intelligent agents and how to enhance decision-making process. The proposed system is applied to a real-world project lifecycle case that is EPC (Engineering Procurement and Construction) project. A prototype of the system is presented where intelligent agents are the building blocks of a peer-to-peer organization wide system. The application was implemented using Eclipse technology, and the agents were deployed on the FIPA-OS (Foundation for Intelligent Physical Agents-Open-Source) environment, we used JESS (Java expert system shell) to develop the knowledge based of the agents' reasoning.","Proceedings of the Third IEEE International Conference on Cognitive Informatics, 2004.",2004.0,10.1109/COGINF.2004.1327481,25044218,semantic_scholar
a2524b60d8c51af32ee36f9d3ccb0761c8b593f6,https://www.semanticscholar.org/paper/a2524b60d8c51af32ee36f9d3ccb0761c8b593f6,Indexing and retrieving Semantic Web resources: the RDFStore model,"The Semantic Web is a logical evolution of the existing Web. It is based on a common conceptual data model of great generality that allows both humans and machines to work with interrelated, but disjoint, information as if it was a single global database. The design and implementation of a general, scalable, federated and flexible data storage and indexing model, which corresponds to the data model of the Semantic Web, is fundamental for the success and deployment of such a system. The generality of the RDF data model presents unique challenges to efficient storage, indexing and querying engines. This paper presents our experience and work related to RDFStore which implements a new flexible indexing and query model. The model is tailored to RDF data and is designed around the Semantic Web from the ground up. The paper describes the underlying indexing algorithm, together with comparisons to other existing RDF storage and query strategies. Towards a lightweight database architecture The generality of the RDF data model presents unique challenges to efficient storage, indexing and querying software. Even if the Entity-Relational (ER) data model [1] is the dominant technology for database management systems today, it has limitations in modeling RDF constructs. RDF being unbounded, the resulting data structures are irregular, expressed using different data granularity, deeply nested or even cyclic. As a consequence, it is not possible to easily fix the ""structural view"" of a piece of information (object), which is instead one of the fundaments of traditional RDBMS systems trying to be much narrower and precise as possible and where an update not conforming to a single static schema is rejected. Database systems also optimize data storage and retrieval by knowing ahead of time how records are structured and interrelated and tend to use very inefficient nested SQL SELECT statements to process nested and cyclic structures. All this is too restrictive for RDF data. Like most semi-structured formalisms [2][3] RDF is self-describing. This means that the schema information is embedded with the data, and no a priori structure can be assumed, giving a lot of flexibility to manage any data and deal with changes in the data's structure seamlessly at the application level. The only basic structure available is the RDF graph itself, which allows describing RDF vocabularies as groups of related resources and the relationships between these resources [4]. All new data can be ""safely"" accepted, eventually at the cost of tailoring the queries to the data. On the other side, RDF data management systems must be much more generic and polymorphic like most of dynamically-bound object-oriented systems [5]; changes to the schema are expected to be as frequent as changes to the data itself and could happen while the data is being processed or ingested. A drawback of RDF heterogeneity is that the schema is relatively large compared to the data itself [6]; this in contrast to traditional RDBMS where the data schema is generally several orders of magnitude smaller than the data. This also implies that RDF queries over the schema information are as important as queries on the data. Another problem is that most RDF data (e.g. metadata embedded into an HTML page or RSS1.0 news feed) might exist independently of the vocabulary schemas used to mark-up the data, further complicating data structure ""validation"" (RDF Schema validation). This de-coupling aspect also makes the data ""de-normalization"" more difficult [7][8][9]. ""De-normalization"" is needed in RDBMS to overcome query performance penalties caused by the very general ""normalized"" schemas. De-normalization must be done taking into account to how the database will be used and how data is initially structured. In RDF this is not generally possible, unless all the RDF Schema definitions of the classes and properties used are known a-priori and available to the software application. Even if that might be the case, it is not a general rule and it would be too restrictive and make RDF applications extremely fragile. In the simplest and most general case, RDF software must associate the semantics to a given property exclusively using the unique string representation of its URIs. This will not stop of course more advanced and intelligent software to go a step further and retrieve, if available, the schema of the associated namespace declarations for validation, optimization or inference purposes. It is interesting to point out that a large part of queries foreseen for Web applications are information discovery and retrieval queries (e.g. Google) that can ""ignore"" the data schema taxonomy. Simple browsing through the RDF data itself or searching for some sub-string into literals, or using common patterns is generally enough for a large family of RDF applications. On the other hand, we strongly believe that RDBMS has proven to be a very effective and efficient technology to manage large quantities of well-structured data. This will continue to be true for the foreseeable future. We thus see RDF and similar less rigid, or semi-structured data technologies as complementary to traditional RDBMS systems. We expect to see RDF increasingly appear in the middle layer where lightweight systems that focus on interoperability, flexibility and a certain degree of decoupling of rigid formats are desired. We believe that a fundamentally different storage and query architecture is required to support the efficiently and the flexibility of RDF and its query languages. At a minimum such storage system needs to be: Lightweight Native implementation of the graph Fundamentally independent from data structure Allow for very wide ranges in value sizes; where the size distribution is not known in advance, most certainly is not Gaussian and will fluctuate wildly. Be efficient it should not be necessary to retrieve very large volumes of data in order to reconstruct part of the graph. Allow built support for arbitrary complex regular-path-expressions on the graph to match RDF queries like RDQL [50] statement triple-patterns. Have some free-text support Context/provenance/scope or flavoring of triples Furthermore given that RDF and the Semantic Web are relatively new, and will require significant integration and experimentation it is important that its technology matches that of the Internet: Easy to interface to C, Perl and Java at the very least. Ruby, Python, Visual Basic and .NET are a pre. Easy to distribute (part of) the solution across physical machines or locations in order match scaling and operational habits of existing key Internet infrastructure. Very resistant to ""missing links"" and other noise. Contexts and provenance A RDF statement represents a fact that is asserted as true in a certain context space time, situation, scope, etc. The circumstances where the statement has been stated represent its ""contextual"" information [10][11]. For example, it may be useful to track the origin of triples added to the graph, e.g. the URI of the source where triples are defined, e.g. in an RDF/XML file, when and by whom they where added and the expiration date (if any) for the triples. Such context and provenance information can be thought of as an additional and orthogonal dimension to the other components of a triple. The concept is called in the literature ""statement reification"". Context and provenance are currently not included in the RDF standardisation process [48][49], but will hopefully adressed in a next release of the specification. From the application developer point of view there is a clear need for such primitive constructs to layer different levels of semantics on top of RDF which can not be represented in the RDF triples space. Applications normally need to build meta-levels of abstraction over triples to reduce complexity and provide an incremental and scaleable access to information. For example, if a Web robot is processing and syndicating news coming from various on-line newspapers, there will be overlap. An application may decide to filter the news based not only on a timeline or some other property, but perhaps select sources providing only certain information with unique characteristics. This requires the flagging of triples as belonging to different contexts and then describing in the RDF itself the relationships between the contexts. At query time such information can then be used by the application to define a search scope to filter the results. Another common example of the usage of provenance and contextual information is about digital signing RDF triples to provide a basic level of trust over the Semantic. In that case triples could be flagged for example with a PGP key to uniquely identify the source and its properties. There have been several attempts [12][13][14][15] trying to formalize and use contexts and provenance information in RDF but a common agreement has not been reached yet. However, context and provenance information come out as soon as a real application is built using RDF. Some first examples are presented below. Our approach to model contexts and provenance has been simpler and motivated by real-world RDF applications we have developed [16a][16b][16c]. We found that an additional dimension to the RDF triple can be useful or even essential. Given that the usage of full-blown RDF reification is not feasible due to its verbosity and inefficiency we developed a different modeling technique that flags or mark a given statement as belonging to a specific context. First example considers subjective assertions. The Last Minute News (LMN) [16b] and The News Blender (NB) [16c] demos allow an user rating and qualifying the source newspapers. The user can ""say"" that a newspaper is ""liberal"" or ""conservative"". Of course, two users, X and Y, will show two different opinions. Without considering the context, this will result in two triples: Newspaper A -> Quality -> ""liberal"" Newspaper A -> Qu",,2003.0,,15151937,semantic_scholar
a895c3e93c9097eaf4269e3ec246249d89a54436,https://www.semanticscholar.org/paper/a895c3e93c9097eaf4269e3ec246249d89a54436,Designing Pervasive Services for Physical Hypermedia,In this paper we describe the design and implementation of a software substrate for building pervasive services in the context of physical hypermedia applications. We first introduce the main ideas behind physical hypermedia; next we argue that physical navigation requires some software support to improve accessibility to real world objects. We next describe an architectural framework that supports specification and deployment of pervasive services. Some simple examples of use are presented. We conclude by comparing our work with others' and describing further work we are pursuing,2006 ACS/IEEE International Conference on Pervasive Services,2006.0,10.1109/PERSER.2006.1652238,18418153,semantic_scholar
c30ab40bc2a9daa1882837560bc688997bf90f21,https://www.semanticscholar.org/paper/c30ab40bc2a9daa1882837560bc688997bf90f21,Unified Software Engineering with Java,Preface 1. Introduction to Java in the Context of Software Engineering 2. Experimenting With Classes and Objects 3. The Structure and Syntax of Java 4. Design and Development of Java Applications 5. Architecture-Driven Component Development 6. Introduction to Distributed Computing Concepts 7. Interfacing with Users 8. Implementing Java Programs 9. Software Quality Assurance 10. Information Management in Java 11. Reality Check: Java Programs in the Real World 12. Software Integration and Deployment 13. Java on Various Computer Platforms 14. Advanced Topics in Java Software Engineering 15. The Unified Modeling Language: A Primer,,2006.0,,60618591,semantic_scholar
88e866eaab83d42f392182697c3075ece34c9c62,https://www.semanticscholar.org/paper/88e866eaab83d42f392182697c3075ece34c9c62,The Six Pillars of Simulation Architecture,"This paper addresses simulation architecture for real-time Man-In-The-Loop (MITL) and Hardware-In-TheLoop (HITL) simulation laboratories, as used by the Lockheed Martin Aeronautics Company to support the full life cycle for aircraft and air system products. The paper discusses concepts and considerations used to establish the architecture for simulation and systems integration laboratories. The subject is presented via discussion of six pillars of simulation architecture: Composition, Functionality, Structure, Behavior, Mechanization, and Doctrine. The foundation for these pillars, the System Design Process, is also discussed. For each of these major elements, architectural goals and products are reviewed, and some real examples from major programs are provided. Biography Barry Evans is a Senior Fellow and Chief Engineer for Simulation and Systems Integration Laboratories at the Lockheed Martin Aeronautics Company. Mr. Evans provides technical leadership for simulation and integration labs across all programs and sites within the company. Prior to this role, Mr. Evans served two years as the Acting Director for F-35 Laboratories, during which time he successfully led a major reorganization and technical re-plan that resulted in substantial cost reduction and resolution of technical challenges. Prior to this assignment, he led a major program re-plan and served as air vehicle integration manager for the C-5 RERP Program. Mr. Evans was the lead architect for a common suite of laboratory software, hardware, standards, processes, and paradigms, deployed across program domains, including F-35, F-22, C-5, C-130, C-27, P-3, S-3, CRAD, and IRAD programs. He served as system architect and/or project manager for numerous simulation and systems integration projects. His design and development experience includes: operating systems; simulation executives; I/O; air vehicle and mission systems models; tactical combat simulation; OFP re-hosts; visual/audio/motion/feel cueing systems; and hardware-in-the-loop stimulation. Mr. Evans holds a BSEET from Southern Polytechnic State University, and he completed course work in Flight Dynamics from Kansas State University. Introduction The subject of simulation architecture is one that is both broad and deep, and the term “simulation architecture” can summon different views and meanings, depending on the focus of the individual. Additionally, there are different types of simulation and a variety of applications, which can influence what is architecturally important in the simulation. So, in order to discuss simulation architecture, it is necessary to describe the application context and provide a definition of what is intended by the term “architecture”. The Lockheed Martin Aeronautics Company utilizes real-time Man-In-The-Loop (MITL) and Hardware-InThe-Loop (HITL) simulation laboratories to support all phases of air system development and sustainment, including: concept exploration, theater-level analysis, system trade studies, system requirements development, design evaluation, system developmental testing, integration testing, system verification and validation (V&V), and training/familiarization. The simulations employed range from relatively small, low fidelity implementations that may focus on one element of one aircraft, to very large scale, full theater simulations that represent thousands of real-world entities (aircraft, ground vehicles, elements of the environment, weapons, etc.) with very high fidelity. Some of the simulations are oriented about human interaction with simulated systems (such as the airframe and flight controls, pilot-vehicle interface, mission systems, weapon systems, etc.), while other simulations are focused about stimulating air system hardware for integration and verification testing. The discussion below is in the context of this wide range of simulation applications. For the discussion herein, simulation architecture is defined as the design, structure, organization, and behavior of a simulation. The architecture addresses hardware, software, intellectual attributes, human interaction, and/or anything critical to the simulation design. The term “simulation architecture” may apply at different levels within a simulation, including: component, subsystem, system, and system-of-systems. The following discussion is focused on the system-level design of a simulation, addressing the information necessary to define the simulation blueprint, inclusive of component identification and interfaces, but exclusive of component-level internal design. Simulation architecture may be viewed in terms of six pillars (or elements): Composition, Functionality, Structure, Behavior, Mechanization, and Doctrine. Refer to Figure 1. In this view, the six pillars provide direct support of the simulation architecture, and the system design process is portrayed as the underlying foundation for the pillars. A discussion of the goals and products for each of these elements allows the highlighting of various key concepts and considerations that go into the development of a simulation architecture. The six pillars are not meant to represent phases or steps of architectural development, but rather a convenient way to break down and discuss the complex subject of simulation architecture. Also, it should be noted that just as the structural load of a building section may be supported by more than one physical pillar, a product or artifact of simulation architecture may be supported by, (or be the result of), the goals and activities from more than one of the architectural pillars discussed herein. Additionally, it is worth noting that most of elements of system design discussed herein could be applicable to any software-intensive system design. But, the descriptions are cast in the context of simulation, and they include some simulation-specific architectural considerations. Figure 1: The Six Pillars of Simulation Architecture",,2014.0,10.2514/6.2014-2204,61230373,semantic_scholar
4a99aa2ca1dd85ac7b82d79bf8cec1a09c9d8488,https://www.semanticscholar.org/paper/4a99aa2ca1dd85ac7b82d79bf8cec1a09c9d8488,Energy Efficient protocol design in Wireless Sensor Networks – Contributions to make the Ubiquitous Platform Greener,"s all hardware resources as components. For example, calling the getData() command on a sensor component will cause it to later signal a dataReady() event when the hardware interrupt fires. While many components are entirely software based, the combination of split-phase operations and tasks makes this distinction transparent to the programmer. In both cases an event signals that the encryption operation is complete. ADC, ClockC, UART, SlavePin and SpiByteFifo are example hardware abstraction components. TinyOS commands and events are very short, due to limited code space and a finite state machine style of decomposition. The rich event processing model means an event or command call path can traverse several components. The TinyOS component model allows us to easily change the target platform from mote hardware to simulation by only replacing a small number of low-level components. The event-driven execution model can be exploited for efficient eventdriven simulation, and the whole program compilation process can be re-targeted for the simulator‟s storage model and native instruction set. The static component memory model of TinyOS simplifies state management for these large collections. Setting the right level of simulation abstraction can accurately capture the behavior and interactions of TinyOS applications. Figure 1.3: TinyOS Structure (Consist of scheduler and graph of components) 2.3 TOSSIM: A Simulator for TinyOS Sensor Networks The Necessity of Network Simulation: The emergence of wireless sensor networks brought many open issues to network designers. Traditionally, the three main techniques for analyzing the performance of wired and wireless networks are analytical methods, computer simulation, and physical measurement. However, because of many constraints imposed on sensor networks, such as energy limitation, decentralized collaboration and fault tolerance, algorithms for sensor networks tend to be quite complex and usually defy analytical methods that have been proved to be fairly effective for traditional networks. Furthermore, few sensor networks have come into existence, for there are still many unsolved research problems, so measurement is virtually impossible. It appears that simulation is the only feasible approach to the quantitative analysis of sensor networks. The event-driven nature of sensor networks means that testing an individual mote is insufficient. Programs must be tested at scale and in complex and rich conditions to capture a wide range of interactions. Deploying hundreds of motes is a daunting task, the focus of work shifts from research to maintenance, which is time-consuming due to the failure rate of individual motes. A simulator can deal with these difficulties, by providing controlled, reproducible environments, by enabling access to tools such as debuggers, and by postponing deployment until code is well tested and algorithms are understood. TOSSIM: TOSSIM is a discrete event simulator for TinyOS sensor networks. Instead of compiling a TinyOS application for a mote, users can compile it into the TOSSIM framework, which runs on a PC. This allows users to debug, test, and analyze algorithms in a controlled and repeatable environment. As TOSSIM runs on a PC, users can examine their TinyOS code using debuggers and other development tools. TOSSIM‟s primary goal is to provide a high fidelity simulation of TinyOS applications. For this reason, it focuses on simulating TinyOS and its execution, rather than simulating the real world. While TOSSIM can be used to understand the causes of behavior observed in the real world, it does not capture all of them, and should not be used for absolute evaluations. Related Publication: Swarup Kumar Mitra, Ayon Chakraborty, Subhajit Mandal and M.K.Naskar, Simulation of Wireless Sensor Networks using TinyOS A Case Study, In the Proceedings of the National Conference on Modern Trends in Electrical Engineering, pages EC 23 EC 26, Hooghly, West Bengal, July 2009. 3 Data Gathering Schemes in WSNs Data gathering is by far one of the most important aspects of research considering energy efficiency in the routing protocols for wireless sensor networks. Wireless sensor networks have emerged as a ubiquitous platform recently, and issues regarding the efficiency of energy usage by these devices play a very important role. These devices are equipped with negligible or less amount of battery power to sustain for a long time. Not only that, in most of the scenarios, where these networks are deployed it is infeasible or impossible sometimes to replace the battery power of the sensor nodes. One of the most fundamental aspects for energy consumption in sensor nodes is communication, other than sensing and computation costs. Optimization of communication costs is thus essential, which is a direct consequence of betterment of routing techniques in this type of wireless networks. A major portion of my contribution in this project deals with designing data gathering schemes for wireless sensor networks and optimization of routing techniques, described below. The first in this queue was the HDS or “Hybrid Data Gathering Scheme”. Published in the International Conference of Distributed Computing and Internet Technology (ICDCIT‟10), this work is a novel approach in minimizing not only the communication / energy overhead but also guarantees a minimal energy-latency product. It also distributes the energy consumption by the nodes by rotating the leader node, so as to increase the uniformity of energy content in the nodes. The uniform distribution of energy content in the nodes also helps to lessen the chances of a black hole or a sinkhole problem. The HDS protocol is based on the hybrid combination of two algorithms, SHORT and LBERRA. The LBERRA scheme is used to subdivide the sensor field into predefined clusters, and SHORT is applied to form a binary tree spanning the nodes. There are two types of leader nodes: one for each cluster, forming the root of the tree and the other one is the „sink‟ communicating the gathered data to the Base Station. In each of the data gathering rounds the leader node is changed The second work was related to optimization of routing chain through heuristic techniques. Firstly, I applied Particle Swarm Optimization to create the most energy efficient paths for communication in the sensor field. Then, I investigated the use of Genetic Algorithms (hybridized with simulated annealing) in solving the same problem. In these works, I not only devised the algorithm for the minimum-energy path formation, but also coded it in nesC discussed in the earlier section. The implementation and simulation in nesC guarantees the hardware feasibility of the algorithm in sensor nodes. Packet loss rates were also studied with varying network topology and signal strengths in communication between particular sensor nodes. In all the cases, a standard background noise was considered. This work followed a series of publications including three international conferences and two international journals. An extension of this work was to create energy efficient data gathering trees. Most algorithms developed in literature used greedy algorithms to construct routing trees which in most of the cases did not result in near-optimal energy usage. “ROOT” or “ROuting through Optimized Trees” was an answer to this need. Related Publications: International Conference: 1. Ayon Chakraborty, Kaushik Chakraborty, Swarup Mitra and Mrinal Naskar, An Optimized Lifetime Enhancement Scheme for Data Gathering in Wireless Sensor Networks, in the proceedings of The Fifth IEEE Conference on Wireless Communication and Sensor Networks, WCSN'09 Allahabad, India, (December, 2009). 2. Ayon Chakraborty, Swarup Mitra and Mrinal Naskar, An Efficient Hybrid Data Gathering Scheme in Wireless Sensor Networks, in the proceedings of The Sixth International Conference on Distributed Computing and Internet Technology, ICDCIT'10, Bhubaneswar, India. (February, 2010). 3. Ayon Chakraborty, Swarup K. Mitra and M.K. Naskar, Energy Efficient Routing in Wireless Sensor Networks: A Genetic Approach, in the Proceedings of the International Conference on Computer Communications and Devices (ICCCD 2010), IIT Kharagpur (December, 2010) 4. Kaushik Chakraborty, Ayon Chakraborty, Swarup Mitra and Mrinal Naskar, ROOT: Energy Efficient Routing through Optimized Tree in Sensor Networks, in the proceedings of The International Conference on Computer Communications and Devices – ICCCD'10, Kharagpur, India. (December, 2010).",,2012.0,,17171658,semantic_scholar
9c97ed6973ac9c1555e29124642c546464bc85ac,https://www.semanticscholar.org/paper/9c97ed6973ac9c1555e29124642c546464bc85ac,Engaging in a Conversation with Synthetic Agents along the Virtuality Continuum,"During the last decade research groups as well as a number of commercial software developers have started to deploy embodied conversational characters in the user interface especially in those application areas where a close emulation of multimodal human-human communication is needed. Incarnations of such characters differ widely in type and amount of embodiment - starting from simplistic cartoon-style 2D representations of faces, fully embodied virtual humans in 3D virtual worlds to physically embodied androids co-habiting the user's real world. Despite of their variety, most of these characters have one thing in common: In order to enter the user's physical world, they need to be physical themselves. My talk focuses on challenges that arise when embedding synthetic conversational agents in the user's physical world. Following [4], we may classify the contact between synthetic and human agents according to a ""virtuality continuum"" (see Fig. 1). At one extreme, we find android agents that are completely integrated in the user's physical world and even allow for physical contact with the user. Mel, a robotic penguin developed by Sidner and colleagues [5] (see image 1 in Fig. 1), is one of the most sophisticated physical agents that engages in face-to-face communication with a human user. At the other extreme, there are purely virtual environments that are populated by human and synthetic agents. A prominent example is the pedagogical agent Steve [3] (see Image 4 in Fig. 1). Steve is aware of the user's presence in the virtual space, monitors her actions and responds to them, but has no access to the external world. That is it is only able to perceive user actions that are performed in the virtual space. In between, we find a new generation of characters that inhabit a world in which virtual and digital objects are smoothly integrated. In these applications, projections of virtual characters overlay the user's physical environment or projections of real persons are inserted into a virtual world. For instance, Cavazza and colleagues [2] propose a magic mirror paradigm which puts the user both in the role of an actor and a spectator by inserting the user'svideo image in a virtual world that is populated by synthetic agents (see Image 3 in Fig. 1). In the Virtual Augsburg project (see [1]), a synthetic character called Ritchie jointly explores with the user a table-top application that combines virtual buildings of the city center of Augsburg with a real city map being laid out on a real table. Most work so far has concentrated on the design and implementation of conversational agents at the two extremes of the Virtuality Continuum. In my talk, I will report on a new generation of synthetic characters that are no longer bound to a flat screen, but able to enter a physical world and to engage in a conversation with a human user. Users and characters do not inhabit separated spaces, but share an informational and physical reality that is augmented by digital objects. As a consequence, communication has to take into account both the physical and the digital context. New forms of deixis are enabled by the manipulation of objects and movements of characters in the physical space. Further challenges arise from the realization of so-called traversable interfaces that allow human and synthetic agents to cross the border from the digital to the real world and vice versa.",2006 IEEE/WIC/ACM International Conference on Intelligent Agent Technology,2006.0,10.1109/IAT.2006.62,9567460,semantic_scholar
5c666ddee49c09fad988c2d7ec468ae5baf280e7,https://www.semanticscholar.org/paper/5c666ddee49c09fad988c2d7ec468ae5baf280e7,Workshop Mobile and Embedded Interactive Systems ( MEIS ' 06 ),"Automatic identification technology such as RFID promises to connect physical objects with virtual representations or even computational capabilities. However, even though RFID tags are continuously falling in price, their widespread use on consumer items is still several years away, rendering large-scale experiments with such an “internet of things” difficult. Much more ubiquitous are printed bar codes, yet so far their recognition required either specialized scanner equipment, custom-tailored bar codes or costly commercial licenses – all equally significant deployment hurdles. We have developed a freely available EAN-13 bar code recognition and information system that is both lightweight and fast enough for the use on camera-equipped mobile phones, thus significantly lowering the barrier for large-scale, real-world testing of novel information and interaction applications based on “connected” physical objects. We hope that this “low tech” version of bridging the gap will allow the community to quickly develop and try out more realistic and widespread applications, and thus gain real-world experiences for better jump-starting the future internet of things, today. 1 Today’s Role of Barcode Recognition The idea of linking real-world products with virtual information has been around for quite some time. In 1998, Barrett and Maglio already described a system for attaching information to real-world objects [BM98], while 1999 Want et al. expanded upon the idea and linked arbitrary items through the use of RFID tags with both information services and actions [WFGH99]. Since then, a number of research projects have continued to explore this concept of “bridging the gap”, i.e., the automatic identification of individually tagged real-world products in order to quickly look up information or initiate a specific action [KBM02]. With the increasing mobility of powerful computing systems, e.g., mobile phones or handheld PDAs, this bridging can even be done in situ, i.e., right when we need it, where we need it. While RFID potentially offers an unprecedented user experience due to its detailed means for identification (i.e., on a per item basis) and the lack of a line-of-sight requirement for reading, most industry analysts agree that an item-level rollout (e.g., having an RFID tag on every single supermarket product) is still several years away [Jue05]. In contrast, the printed bar codes are practically ubiquitous: Virtually every item sold today carries an internationally standardized bar code on its packaging, enabling not only checkout registers to quickly sum up one’s shopping items, but also to identify a product and look up a wealth of related information. Obviously, using bar codes for linking real-world objects to virtual information has a number of drawbacks when compared to an RFID-enabled future with corresponding mobile RFID readers, such as NFC-enabled1 mobile phones. Due to their sensitivity to soiling, ripping, and lighting conditions, optical bar code recognition can be difficult. Until recently, reading a conventional (i.e., 1D) bar code inevitably required a separate laser scanner or a corresponding mobile phone scanner attachment. The increasing availability of camera phones, i.e., mobile phones with an integrated digital camera, has begun to simplify this process, however. After 2D bar codes have been successfully recognized by most consumer-grade camera phones for quite some time [Roh04], the continuously increasing quality of both the camera resolution and the employed lenses have finally made it feasible to directly read 1D bar codes with such cameras, without the need for special attachments or handheld lasers. This significantly changes the attractiveness of using barcodes for the above physical-to-digital linkage: Instead of waiting several years for a comprehensive item-level roll out of RFID tags, or forcing people to carry around specific scanner attachments for their mobile phones, the support of 1D bar code recognition on any camera phone immediately allows anybody to interact with almost any commercially available product – all it takes is a small application download. The main contribution of this paper is a freely available 1D bar code recognition toolkit that is intended to facilitate the creation of novel applications and services. We believe that the adequate performance of our recognition software, when compared with existing commercial implementations, the ease with which external data sources can be integrated, and the availability of our toolkit under an open source license will help to foster the the use of camera phones as mobile bar code scanners.",,2006.0,,16944891,semantic_scholar
c55e4acdf98b1931bf1e00280639348d51a6283a,https://www.semanticscholar.org/paper/c55e4acdf98b1931bf1e00280639348d51a6283a,A Hierarchical Collective Agents Network for Real-time Sensor Fusion and Decision Support,"This research addresses a problem of how to make effective use of real-time information acquired from multiple sensor and heterogeneous data resources, and reasoning on the gathered information for situation assessment and impact assessment (SA/IA), thus to provide reliable decision support for time-critical operations. A hierarchical collective agents network (HCAN) is employed as a solution to this problem. The agents network supports multi-sensor registration, real-time sensor/platform cueing, level-2 and level-3 information fusion, and has an arm toward the level-4 fusion objectives. An agent component assembly and decision-support-system-development environment, the 21 Century systems’ AEDGE software package, is used for the design and implementation of a HCAN-DSS system. The ability to integrate and correlate a vast amounts of disparate information from multiple sensor and heterogeneous data resources with varying degrees of uncertainty in real-time is an impediment issue for missioncritical decision support systems (DSS). For example, in crucial military operations command officers need real-time information and intelligences from various sensors/data resources in a theater of reconnaissance and surveillance to build a whole picture of the battle-space. It is critical for the commanders to know and to understand the relationships among the information collected. Questions are asked: what are the physical and functional constituencies among the objects in a given geographic sector? Are there sequential or temporal dependencies of the objects and what will trigger them? What are the possible consequences of the action and re-actions? Decision making based on these situation assessment and impact assessment (SA/IA) are particularly important for identifying and prioritizing “gaps” between the operation planning and the real-time interactions. To support making effective SA/IA, a data fusion and decision support system is required to use a set of coherent patterns derived from the available data sets and infer the implications (e.g., causal relations) toward the real world situations. The attribute coherence that is critical to the formation of the meaningful knowledge patterns is often obscure in the data sets obtained from heterogeneous resources. The data collections are often incomplete, imprecise, and inconsistent due to various natural constraints and human faults. Decision makers naturally desire to access large quantities of information expressed in diverse forms. However, as new sensor technology and various information sources have combined to create quantity and diversity, it has become increasingly difficult to provide decision makers with the right information at the right time and in the right quantity and format. Real-time computerized decision support systems are constructed by integrating a number of diverse components from a variety of software modules. Software developers have come to a numerous ways of querying the local and centralized data resources to access and distill the large and diverse information for the purpose of providing effective decision support. Meanwhile DSS are becoming more and more complex in terms of knowing which data resources to connect, how to keep track of the data dynamics, and assess the reliability of information from each resources. These tying links make the use of intelligent agent architecture necessary and desirable for allowing real-time responsibility and adaptive control of the DSS. Many popular agent systems of today deploy agents in a uniform level of operation. The agents respond to the same calls and cooperate at the same time toward the same goal of operation. The architecture endues some difficulties in agent communications and task control. When applied in complex real-time DSS with intensive human and system interactions, the cooperative nature makes the system less robust because the disability of one agent would affect the successive operations of the entire agent assembly. The collective nature of the agents in a HCAN paradigm overcomes some of these difficulties, for example, relieving the burden of data-exchanges between fellow agents by limiting agent communication to vertical layers of the assembly only. The hierarchical architecture simplifies the functional design of the agent interactions and enhances the security and efficiency of the process. The HCAN architecture also strikes a balance between the centralized control and distributed computation by allowing distributive agent operations within layers of the hierarchy and enforcing centralized control between the layers of the hierarchy, thus creating a federated agents integration structure. Basically, the HCAN has the functionalities of. 1). A flexible software architecture for accommodating system augmentation and evolutions; 2). A powerful representation schema for accommodating heterogeneous forms of information; 3). A diverse interface for various input resources, output formats, and human interactions; From: AAAI Technical Report WS-02-15. Compilation copyright © 2002, AAAI (www.aaai.org). All rights reserved. 4). An ability of reasoning on incomplete and inconsistent information, and extracting useful knowledge from the data of heterogeneous resources; 5). An ability of incorporating real-time dynamics of information resources into system at time of operation, and promptly adjusting the reasoning mechanisms; 6). An ability of summarizing and refining knowledge extracted, and distinguishing mission and time critical knowledge from insignificant and redundant ones; 7). A capability of supplying meaningful and accurate explanations, both qualitatively and quantitatively, of the automated system actions; and 8). A capability of providing adequate control and scrutiny of the system operations w.r.t. environment constrains. There are many sources of uncertainty at different levels of the decision support. For example, even if a situationassessor is aware of the presence of certain objects in the operation space, such as the type of contact, intention, reaction rational, etc., the exact dynamics of the object is still uncertain to the decision maker. While the knowledge about the object dynamics is critical in constructing an optimal strategy of action, various statistical methods and knowledge discovery techniques are applied in the reasoning module. The level of uncertainty forces the reasoning agents to operate with different decision strategies. The 21 Century Systems, Inc. has developed the AEDGE as an open DII-COE and CORBA compliant agentbased environment that enables the development of components-based agent systems. The system is implemented in JavaTM, with Java Database ConnectivityTM for DB access, Java Swing, AWT, and Java3D for visual interfaces, Java Media Framework and Java Speech API for audio/speech interface. AEDGE defines Agents, Entities, Avatars and their interactions with each other and with external sources of information. This standardized architecture allows additional components, such as servicespecific DSS tools, to be efficiently built upon the core functionality. Common interfaces and data structures can be exported to interested parties who wish to extend the architecture with new components, agents, servers, or clients. When the core AEDGE components are bundled with customer-specific components, a clean separation of those components, through APIs, is provided. The AEDGE is based on an extensible multi-component DSS architecture (EMDA, also referred to as the AEDGETM Architecture). The architectures describe the data objects, interfaces, communication mechanisms, component interactions, and integration mechanisms for the AEDGE and its extensions. In the AEDGE architecture, components communicate among each other via the Service Provider/Service Requester Protocol (SPSR). Service providers are components that implement an algorithm or need to share their data (data sources). Service requesters are the components that need a function performed for them by some other component or need to import data from another component. Both service requesters and service providers implement remote interfaces, which enables such components to communicate over a TCP/IP network. The remote interface implementation is currently based on Java RMI (remote method invocation), though the Architecture is not dependent on this implementation. AEDGE provides multiple levels of customization. The subject-matter users are able to build scenarios and scripts or to automatically generate them using the AEDGE-based Scenario Editor. Rules and triggers for agent behaviors can be created and modified by the advanced user. AEDGE also provides APIs for custom extensions of agents, data bridges, and the entity framework. The practical user will enjoy AEDGE’s versatile data connectivity and its near-real-time execution and monitoring of DSS functions. As a built-in bonus, AEDGE provides connections to a number of simulators and data formats, including HLA, DIS, DTED, DBDB2, XML, as well as support for multiple modes of distribution (CORBA, RMI, TCP/IP). As an example of the HCAN design using AEDGE for data fusion and DSS applications, the Advanced Battlestation with Decision Support System (ABS/DSS) which was developed as an operational agent-based C2 team decision support platform for command and control centers aboard aircraft carriers.. The ABS/DSS is based on AEDGE’s implementation of HCAN, and provides consolidated situational awareness through real-time, interactive, agent based decision support coupled with a linked 2D/3D battlespace visualization. Additionally, the ABS/DSS supports shipboard distributed training, train-asyou-fight, with a built-in scenario construction and emulation of friendly and hostile entities. Whether the watchstander is in live-feed mode or in training mode the operation of the agent-based decision support system and the 2D/3D visualization is identical. ",,2002.0,,8602112,semantic_scholar
398d9928ef1e9b18ee4e3876d3ea22b1d59ecf74,https://www.semanticscholar.org/paper/398d9928ef1e9b18ee4e3876d3ea22b1d59ecf74,Architectures for system-level applications of adaptive computing,The mission of the Systems-Level Applications of Adaptive Computing (SLAAC) project is to design and implement a distributed adaptive computing systems architecture. This systems-level focus of SLAAC resulted from the realization that scalability and portability are the two main obstructions preventing innovative Adaptive Computing Systems (ACS) research from being directly useful in deployed real-time environments. Scalability is an issue in that many real-world applications are larger than the modern PCI-based FPGA accelerator. Transitioning from a small proof of concept demonstration to large real-world application is often overlooked in ACS research. Portability has both a hardware and software aspect. Physical form-factor and operating system issues can limit the utility ACS research done in the lab with desktop PCs unless there is a development path to more traditional real-time environments. The SLAAC project seeks to remedy these issues of scalability and portability in ACS systems.,Seventh Annual IEEE Symposium on Field-Programmable Custom Computing Machines (Cat. No.PR00375),1999.0,10.1109/FPGA.1999.803693,206656676,semantic_scholar
f6ca00b9e3901f948d7eb6635f407cc3c7a7a6ed,https://www.semanticscholar.org/paper/f6ca00b9e3901f948d7eb6635f407cc3c7a7a6ed,Enterprise Security: The Manager's Defense Guide,"Preface. I. THE FORGING OF A NEW ECONOMY. 1. What is E-Business? The E-Business Sweepstakes. Caesars of E-Business: An Embattled Business Culture. The Lure of Overnight Successes. Crossing the Digital Chasm. The Sobering Reality. Real-World Examples. E-Business: The Shaping and Dynamics of a New Economy. The E-Business Supply Chain. Related E-Business Trends. Summary. 2. What Is E-Security? E-Security at Your Service. Demands on Traditional IT Security: A Changing of the Guard. Principles of E-Security. Risk Management in the New Economy. How E-Security Enables E-Business. The E-Security Dilemma: Open Access versus Asset Protection. 3. The Malicious Opponents of E-Business. The Lure of Hacking. Hackers versus Crackers. Hacker Groups. Why Hackers Love to Target Microsoft. Meeting the Hacker Threat. National Infrastructure Protection Center. Central Intelligence Agency. Other White Hats. II. PROTECTING INFORMATION ASSETS IN AN OPEN SOCIETY. 4. A New Theater of Battle. From the Demilitarized Zone and the Perimeter to Guerilla Warfare. The Triumph of Intranets, Extranets, and Virtual Private Networks. The Vanishing World of Controlled, or Closed, Access. The Impact of Open Access. The Correlation between Open Access and Asset Protection. The Role of Authentication and Privacy in the New Economy. Summary. 5. Reempowering Information Technology in the New Arms Race. The Failings of the Old Paradigm. Infiltration of Rogue Applets. Human Error and Omission. Ongoing Change in the Enterprise Network. Deploying and Maintaining Complex Layer Client/Server Software. Shortage of Human Capital. Rigidity of Enterprise Security Policy. Tools for Rearming the IT Manager. Guidelines for E-Security. Enterprise Security Policy. Summary. III. WAGING WAR FOR CONTROL OF CYBERSPACE. 6. Attacks by Syntax: Hacker and Cracker Tools. Inherent Shortcomings of TCP/IP. Standard ""Ports"" of Call. TCP/IP Implementation Weaknesses. IP Spoofing. Distributed Denial-of-Service Attacks and Tools. Trin00. Tribe Flood Network. Tribe Flood Network 2000. Stacheldraht. ICMP Directed Broadcast, or Smurf Bandwidth Attack. Backdoor Programs and Trojan Horses. Backdoor Program Functions. Examples of Backdoor Programs. Summary. 7. Attacks by Automated Command Sequences. Script Attacks. The Next Generation of E-Mail Attacks. The Bubble Boy Virus. Mainstream JavaScript Attacks. Attacks through Remote Procedure Call Services. Brown Orifice. Summary and Recommendations. 8. Countermeasures and Attack Prevention. Surviving an Attack. Formulate an Emergency Response Plan and an Incident Response Team. Obtain Outside Assistance. Contact Law Enforcement Authorities. Use Intrusion Detection System Software. Countering an Attack. Disconnect Compromised Host/System from Your Network. Copy an Image of the Compromised System(s). Analyze the Intrusion. Recognizing What the Intruder Leaves Behind. 9. Denial-of-Service Attacks. Effects of DoS and DDoS Attacks. General Computing Resources. High-Performance Firewall. Network Bandwidth. Handling a SYN Flood DDoS Attack. Countermeasures. Precautions. Handling a Bandwidth DDoS Attack. Guarding against Being an Accomplice Network. Guarding against Becoming an Intermediary Network. Guarding against Being a Victim. Handling a UDP Flood Bomb. Using an IDS. Recovering from a DDoS Attack. 10. Creating a Functional Model for E-Security. Developing a Blueprint for E-Security. Understanding Business Objectives. Honing in on Your IT Security Policy. Making Good on IT Security's Best Practices. The IT Security Functional Model. Deploying Effective E-Security Architecture: Hardening the Network's Infrastructure. Hardening Your Router. Hardening Your Operating Systems. Summary. 11. Building a Security Architecture. Firewall Architecture Deployment, Controls, and Administration. Types of Firewalls. Hardening Firewalls. Remote-Access Architecture. Encryption Options for Administrators. Securing Remote-Administration Pipes for Administrators. Remote-Access Architecture/Solutions for Users. Vulnerability Assessment Architecture/Solutions. Network-Based Assessment Architecture. Host Vulnerability Assessment. Intrusion Detection Architecture. Network-Based IDS Architecture. Host-Based IDS Solutions. IV. ACTIVE DEFENSE MECHANISMS AND RISK MANAGEMENT. 12. Vulnerability Management. Types of Vulnerabilities. Managing IT Systems Vulnerabilities. Conducting Vulnerability Analysis. Network-Based Vulnerability Analysis. Host-Based Vulnerability Analysis. 13. Risk Management. The Role of Assessment in Risk Management. The Process of Risk Management. Defining the System Boundaries. Threat Analysis. Impact Analysis. Risk Determination. Summary. Appendix A: SANs/fbi Top 20 Internet Security Vulnerabilities. Appendix B: Sample CERT/Coordination Center Incident Response Form. Appendix C: Windows 2000 Security/Hardening Plan. Appendix D: Denial-of-Service Attacks. Glossary. Bibliography. Index. 020171972XT08282002",,2002.0,,106754877,semantic_scholar
6f3834ab566c206d150bd98b2f1e085a9639a185,https://www.semanticscholar.org/paper/6f3834ab566c206d150bd98b2f1e085a9639a185,Unleashing the Power of Wireless Networks Through Information Sharing in the Sensor Internet,"We provide in this presentation in a first part an overview of the research activities of the Swiss National Competence Centre in Research for Mobile Information and Communication Systems (NCCR-MICS) in the area of self-organizing, wireless networks. In the second part we present specific MICS research results from our research group on managing information generated in such networks using self-organizing, logical overlay networks. 
 
Recent advances in wireless communication enable the embedding of sensing and actuation technology into our physical environment at an unprecedented large scale and fine granularity. We show exemplary recent theoretical advances and systems developments on self-organizing, wireless sensor networks and mobile ad-hoc networks achieved in MICS. They provide evidence for the comprehensive scope and high degree of interdisciplinarity required in this area of research. We illustrate the deployment of the resulting technologies in real-world applications. An application class we focus in MICS in particular concerns the monitoring of various typical physical phenomena in the Swiss environment, such as watershed, permafrost, and avalanches. 
 
In the long term, the increasing deployment and application of wireless networks beyond specialized, isolated applications will lead to the production of massive amounts of sensor data requiring further processing support and proper interpretation of data. We argue that self-organizing, logical overlay networks for resource and information sharing will play an important role for achieving this task. Structured overlay networks will be used to support scalable processing of data streams. Semantic overlay networks will be used to overcome heterogeneity in information representation. Finally, social overlay networks will be used to form agreements on meaning and utility of data. We illustrate these developments from our ongoing research: Global Sensor Network, a lightweight implementation of an overlay network for sensor data stream sharing, PicShark, a peer-to-peer image sharing system with support for automated generation and sharing of image annotations, and Semantic Gossiping, a social mechanism based on belief propagation to reconcile heterogeneous annotation schemes. 
 
As a result of these developments, we envision the Internet to develop into a Sensor Internet in which physical reality, information technology and human activity become increasingly intertwined into one common complex system for better understanding and more easily mastering the environment we live in.",EWSN,2006.0,10.1007/11669463_3,34316684,semantic_scholar
e61e75ec370207ed0dca2e81307e8901d09c0493,https://www.semanticscholar.org/paper/e61e75ec370207ed0dca2e81307e8901d09c0493,Enabling Customer Experience and Front-Office Transformation through Business Process Engineering,"In the past, the scope of business processes has been circumscribed to the industrialization of enterprise operations. Indeed, Business Process Management (BPM) has focused on relatively mature operations, with the goal of improving performance through automation. However, in today’s world of customer-centricity and individualized services, the richest source of economic value-creation comes from enterprise-customer contacts beyond transactions. Consequently, process has recently moved out of its traditional court and is becoming prevalent in less traditional competences such as marketing operations, customer-relationship management, campaign creation and monitoring, brand management, sales and advisory services, multi-channel management, service innovation and management life-cycle, among others. These competences host customerenterprise co-creation activities characterized by innovation, human creativity, and new technologies. Above all, these work-practices call for continuous differentiation, instead of “pouring concrete” on emerging business processes. While BPM will continue to make important contributions to the factory of enterprises, Business Process Engineering (BPE) is chartered to provide a holistic approach to new opportunities related to the life-cycle of enterprise customers and the transformation of so-called Front-Office Operations. More broadly, Business Process Engineering fosters a new space for the multidisciplinary study of process, integrating individuals, information and technology, and it does so with the goal of engineering (i.e., designing and running) innovative enterprise operations to serve customers and improve their experiences. Furthermore, given past challenges in the Back-Office, it is imperative that managers focus on processes in the Front-Office where the software industry has jumped into with solutions that bury key processes within applications, thus making differentiation and agility very difficult. BPE stresses the critical importance of the integration of Information and Behavior and it is this goal that links it with Business Informatics: the information process in organizations and society. Since behavior and information are complementary and inseparable domains of concern, current approaches to decision making based on data-only evidence should be reexamined holistically: it may be catastrophic to explicate or predict the behavior of organizations or individuals meaningfully by insisting on the ongoing divorce across the two domains. In particular, Business Informatics and Business Process Engineering offer an opportunity to address potential benefits of “big data” and “business analytics” beyond the IT domain. Having IEEE lead these directions means an opportunity for stimulating new research and practice on the most fundamental problems that enterprises and customers face today in dealing with each other. Keywords— business process engineering; customer experience; business process management; business informatics; enterprise engineering I. PROCESS IS OUT OF THE INDUSTRIALIZATION BOX Business process has been at the center of the stage in both research and industry for several decades. Under the brand of Business Process Management (BPM), business process has attracted a great deal of attention from many practitioners and scholars. BPM has been defined as the analysis, design, implementation, optimization and monitoring of business processes [70], [219], [79], [229], [230]. In [266], Van der Aalst defined some targets of BPM: ‘ ... supports business processes using methods, techniques, and software to design, enact, control and analyze operational processes involving humans, organizations, applications, documents and other sources of information” . While the above definitions are quite comprehensive and broad, in reality most BPM research and industry activity has grwon upon the motivation of reducing operating costs through automation, optimization and outsourcing. There are a several schools of thought and practice (such as lean, lean sixsigma, and others [172], [6], [4], [5]) and a myriad of related literature in the last 40 years that serve to illustrate the focus on cost contention. Around the middle of the past decade, T. Davenport stated in a celebrated Harvard Business Review paper [54] that processes were being “analyzed, standardized, and quality checked”, and that this phenomenon was happening for all sort of activities, stated in Davenport’s own terms: “from making a mouse trap to hiring a CEO”. The actual situation is that industry investment and consequential research have stayed much more on “trapping the mouse” than in differentiating customer services through innovative and more intelligent processes, let alone hiring CEOs. This may be explained partly from Davenport’s own statements: “Process standards could revolutionize how businesses work. They could dramatically increase the level and breadth of outsourcing and reduce the number of processes that organizations decide to perform for themselves” (bold face is added here for emphasis). With the advent of different technologies such as mobile, cloud, social media, and related capabilities that have empowered consumers, the classical approach and scope of business process have begun to change quickly. Organizations are adopting new operating models [100] that will drastically affect the way processes are conceived and deployed. As stated by many authors in the last four decades, business process work is supposed to cover all competences in an organization, irrespective of the specific skills from human beings participating in such operations. However, in an unpublished inspection of about 1,300 papers conducted by 1 Van der Aalst excludes strategy processes from BPM, a remarkable point that will be revisited in more depth later in this paper. the author and some of his collaborators 2 , most process examples shown in the literature deal with rather simple forms of coordination of work, mostly exhibiting a flow structure and addressing administrative tasks (like those captured in early works on office information systems). Furthermore, the examples provided usually deal with rather idealized operations, probably offered as simple examples with the only purpose of illustrating theoretical or foundational research results. Thus, radically simplified versions of “managing an order”, “approving a form”, “processing a claim”, “paying a provider”, “delivering an order” etc. are among the most popular examples of processes found in the literature. The lack of public documentation of substantial collections of real-world processes is remarkable. The authors in [106] both confirmed the dominant focus on simple business processes and also suggested potential practical consequences of related research: “... there is a growing and very active research community looking at process modeling and analysis, reference models, workflow flexibility, process mining and process-centric service-oriented architecture (SOA). However, it is clear that existing approaches have problems dealing with the enormous challenges real-life BPM projects are facing [ ... ] Conventional BPM research seems to focus on situations with just a few isolated processes ...”. Of course, the list of available real-world processes would be a lot richer if one included the set defined by enterprise packaged applications [219]. However, this comprehensive collection is proprietary because it constitutes a key piece of intellectual capital coming from software vendors or integrators in the industry. The traditional focus on process has also raised much controversy. At the S-BPM ONE Conference in 2010, a keynote speaker [176] remarked: “Let me be as undiplomatic as I possibly can be without being offensive [...] The academic community is as much to blame [...] as the vendors of BPM systems, who continue to reduce the task of managing business processes to a purely technological and automation-oriented level”. While other authors in the same conference debated “who is to blame” very animatedly [78], [234] it is important to highlight that the statement from Olbrich (in bold face above for emphasis) reinforces that BPM has mostly followed the obsession of automation and optimization by means of Information Technology. A detailed inspection of the extant literature confirms that business process work has been devoted to a rather small fraction of the actual variety and complexity found in enterprise behavior. This behavior enacts many valuegenerating capabilities that organizations cultivate based on skills provided by their own workforces and through rich interactions with other enterprise stakeholders, particularly customers. The following points offer a simplified summary: 2 At the time of this publication in IEEE, the mentioned work still remains unpublished. The co-authors are L. Flores and V. Becker both from IBM. (1) Business process research in Computer Science has been traditionally focused on certain classes of enterprise operations, mostly involving simple coordination mechanisms across tasks. This type of coordination and the overall behavior represented in underlying models reflect very much an “assembly line” where work is linearly synchronized to deliver a desired artifact or outcome. Simplicity of the choreography is ensured by removing any form of overhead in communication when moving from one stage to the next. Unlike other more complex business processes, many software applications do have this simplified structure. In fact, a trend since the early 2000’s is to separate the specific application logic from the coordination / choreography needed across modules, and both of them from the actual data contained in a data-base management system. Different foundations and a plethora of languages have been created to capture this semantics of coordination such as Business Process Modeling Notation (BPMN), Business Process Execution Languag",,2013.0,,1797243,semantic_scholar
44ce98492713648fef9446f779de56029f432763,https://www.semanticscholar.org/paper/44ce98492713648fef9446f779de56029f432763,Effectiveness Of Collaborative Learning In Online Teaching,"This paper describes how e-learning is becoming popular and used as an alternative means of solving problems in education. E-learning is usually used in distance learning and may be used to replace conventional classroom teaching. Many educational institutions use Internet for collaborative learning in a distributed educational process. It has been known that traditional communications media can be replaced by electronic communication for the whole educational process and in particular, to assess the role of collaborative learning in a distributed education environment. It has been found that a distributed educational process naturally supports collaborative learning environments in which students and tutors interact and provide essential support for students studying at a distance. The tutor’s feedback to students help the learning process and there is indication that tutors are happy to work in the new environment. It is therefore suggested that “blended” online teaching – a combination of the use of the Internet as a medium of instruction and tutors to do face-to-face teaching via a collaborative learning approach – may be implemented to achieve enhanced distance-student performance. INTRODUCTION There is a number of definitions for e-learning. For example, Soekartawi et al. (2002) defined elearning as: ‘... a generic term for all technologically supported learning using an array of teaching and learning tools as phone bridging, audio and videotapes, teleconferencing, satellite transmissions, and the more recognized web-based training or computer aided instruction also commonly referred to as online courses...’. Today, e-learning has become an extremely popular alternative means of delivering educational services worldwide mainly because it is seen as a means of resolving significant educational problems that cannot be solved by conventional means. E-learning is particularly used in open and distance learning (ODL) as it can provide flexible education for those who cannot attend regular schooling particularly because they are unable to leave their work to attend regular conventional classes. Additionally, in an archipelagic country like Indonesia, where the bulk of the population is spread over thousands of islands, educational services are made more accessible through ODL. Most ODL programmes in Southeast Asia, particularly in Indonesia, deploy any one or a combination of the following media: print, radio, television, audiocassettes, videocassettes and computers. Generally, the course materials of ODL programmes in the region are largely print-based. It is likely that this will continue for a long time. However, by increasing the use of computers and information and communication technology (ICT), the role of the printed-based course materials will be replaced gradually by e-learning. MOJIT Effectiveness Of Collaborative Learning In Online Teaching* 69 The development of ICT is moving rapidly because of its pertinent role in providing education to the masses. In fact, by effectively deploying ICT, the educational institutions concerned can cope with, and cater to, the expanding student population. In general, it has been observed that the use of ICT in education in Indonesia has resulted in relatively successful outcomes. There have been numerous problems, however, but there is one significant issue that may be cited here because of its implications to future endeavours in education in Indonesia and in the Southeast Asian region as well. This issue has direct influence over the learning process. It is called transactional distance. The physical separation of teacher and student is no longer a problem given the developments in ICT. However, with ICT use, transactional distance can easily result in a misunderstanding and miscomprehension of the concepts to be learned. In fact, it can even lead to the mal-education of people, following a lack of appropriate communication between learner and teacher. If there is no communication, however, this transactional distance between learner and teacher becomes wider. THE NEW LEARNING PARADIGM In 1995, the International Council on Distance Education (ICDE) conducted what it called an anecdotal, worldwide survey to determine the nature, reality and pace of the shift in the learning paradigm (ICDE, 1996). The survey noted the following clear signs: o A shift from objective knowledge to constructed knowledge. o A shift from an industrial-based to a knowledge-based society. o A shift in education missions from providing instruction to providing learning. o A shift in technologically-mediated procedures of communication and learning. o A shift from “current college and university models to as yet undetermined structures.” Over the last five years, the fifth observation of ICDE has become very clear. The “undetermined structure” at the time of the ICDE survey has come to be known as the virtual learning structure. Experts made similar observations from the ICTsector at about the same time period (Soekartawi et al., 2002). The importance of the virtual campus in the Indonesian context has been reported by Soekartawi (2002, 2002a, 2003) and Haryono & Alatas (2002). According to Garmer & Firestone (1996), due to the development of ICT, the paradigm for learning is shifting away from the traditional notion that ‘‘knowledge” is transferred from teacher to student within the confines of the classroom where the focus of the teaching-learning process before was the teacher; now, it is shifting to the learner. A new understanding of learning places the learner at the centre of the learning process, with the teacher serving an important supporting role in facilitating the process (Garmer & Firestone, 1996). In this new paradigm, successful learning is measured by the individual’s ability to apply appropriate tools and information to solve problems in real life. There is a challenge in this new concept of learning. It is necessary to unlearn old habits and notions of how learning should be structured and to develop new habits of instruction that motivate learners to take greater control over their own education (Garmer & Firestone, 1996). This is what generally known by the “first concern” which should be to understand the learner’s motivations and goals in order that we can expand opportunities for learning. This new paradigm has critical implications for the use of ICT (e-learning) in education, particularly for ODL. For one, it requires that the learner take on more responsibility and autonomy in his learning. This is something that the learner may not be comfortable with or prepared to assume completely at this time. For another, teachers will have to give up control over the learning process and take on a new role similar to that of educational coaches who spend more time on the sidelines MOJIT Effectiveness Of Collaborative Learning In Online Teaching* 70 watching and maybe making plans for a more effective learning environment. The new paradigm has also highlighted an important issue. There is a need for all learners to upgrade their skills, particularly skills to learn on their own. This largely requires an ability to seek, understand and use information, which, in turn, requires the ability to use technology. In today’s world, for example, one must be computer literate to gain access to information and new knowledge. COLLABORATIVE LEARNING One of the critical factors in online learning is the quality issue. Many efforts can be used to meet this issue, among them obtaining feedback evaluation from the student. Further, a crucial aspect of successful distance education is the quality of feedback on student assignments. The electronic assignment handling system pioneered in these trials has now been adopted by the Open University at large and will serve a big number of students. Collaborative learning, therefore, can be used to improve the issue of quality in distance learning. Collaborative learning is an essential ingredient in the recipe to create an ""effective learning environment"" as it provides learners with the opportunity to discuss, argue, negotiate and reflect upon existing beliefs and knowledge. The learner is ""...involved in constructing knowledge through a process of discussion and interaction with learning peers and experts...."" Harasim (in Thomas, 1999, p.51). To facilitate collaboration so that personal knowledge can be constructed, there needs to be a purpose for the collaboration and the purpose needs to be meaningful to the learner. Thus, it is important that an appropriate context is set for the collaborative activity, for example, assigning a ""real world"" task for learners or a problem to which all learners can relate. In addition to setting the context, there needs to be a vehicle through which collaboration can take place. In traditional faceto-face educational settings, collaboration mostly occurs through conversation, that is, individuals interacting with one another via the use of language. Therefore, in terms of creating an effective learning environment, four attributes surface as being paramount: • Providing opportunities to foster personal construction of knowledge • Setting an appropriate context for learning to create the opportunities. • Facilitating collaboration amongst learners. • Using conversation to facilitate collaboration. Petre (1998) argued that while superficially it might appear that distance learning/education is the domain of the distance educator, the aims of educational institutions are similar in ideology; according to Thomas (1999), only the implementations differ. However, what distance educators brings to this arena is the experience of how to interact with students and tutors who are geographically and temporally remote from a campus as well as from one another. The ultimate objective is to diminish and even remove the barriers that remoteness erects. In general, the learning environment in the Open University can be described",,2006.0,,18756902,semantic_scholar
733d5217e1a42999926598617d1849df9f4d4ea2,https://www.semanticscholar.org/paper/733d5217e1a42999926598617d1849df9f4d4ea2,Physical Layer Cooperation: Theory and Practice,"Abstract Information theory has long pointed to the promise of physical layer cooperation inboosting the spectral efﬁciency of wireless networks. Yet, the optimum relaying strategyto achieve the network capacity has till date remained elusive. Recently however, arelaying strategy termed Quantize-Map-and-Forward (QMF) was proved to achievethe capacity of arbitrary wireless networks within a bounded additive gap. This thesiscontributes to the design, analysis and implementation of QMF relaying by optimizing itsperformance for small relay networks, proposing low-complexity iteratively decodablecodes, and carrying out over-the-air experiments using software-radio testbeds to assessreal-world potential and competitiveness.The original QMF scheme has each relay performing the same operation, agnosticto the network topology and the channel state information (CSI); this facilitates theanalysis for arbitrary networks, yet comes at a performance penalty for small networksand medium SNR regimes. In this thesis, we demonstrate the beneﬁts one can gainfor QMF if we optimize its performance by leveraging topological and channel stateinformation. We show that for the N-relay diamond network, by taking into accounttopological information, we can exponentially reduce the QMF additive approximationgap from Q(N) bits/s/Hz to Q(logN) bits/s/Hz, while for the one-relay and two-relaynetworks, use of topological information and CSI can help to gain as much as 6 dB.Moreover, we explore what beneﬁts we can realize if we jointly optimize QMF andhalf-duplex scheduling, as well as if we employ hybrid schemes that combine QMF andDecode-and-Forward (DF) relay operations.To take QMF from being a purely information-theoretic idea to an implementable strategy,we derive a structure employing Low-Density-Parity-Check (LDPC) ensembles for therelay node operations and message-passing algorithms for decoding. We demonstratethrough extensive simulation results over the full-duplex diamond network, that ourdesigns offer a robust performance over fading channels and achieves the full diversityorder of our network at moderate SNRs.Next, we explore the potential real-world impact of QMF and present the design andexperimental evaluation of a wireless system that exploits relaying in the context ofWiFi. We deploy three main competing strategies that have been proposed for relaying,Amplify-and-Forward (AF), DF and QMF, on the WarpLab software radio platform. Wepresent experimental results—to the best of our knowledge, the ﬁrst ones–that compareQMF, AF and DF in a realistic indoor setting. We ﬁnd that QMF is a competitive schemev",,2015.0,,112912924,semantic_scholar
5f3c473a98d99962b62cf044f510f5db64ca2f47,https://www.semanticscholar.org/paper/5f3c473a98d99962b62cf044f510f5db64ca2f47,Is there an extreme world? [Book Review],"Extreme Programming’s core concept is that the client’s requirements for a software system change throughout the system’s development. The iterative method that XP uses requires considerable client involvement and a deep level of commitment to complete discrete sections of the development while meeting all the documentation, testing, and quality requirements before deployment. This nontraditional development method sounds like a silver bullet—too good to be true. So, is anyone using it? Extreme Programming Installed sets out to show that this method has indeed seen realworld deployment. To understand the real-world implementation of these theories, we need practical examples to expand the ideas. This XP book provides them. Backed by experience on XP projects, Ron Jefferies, Ann Anderson, and Chet Hendrickson give insight into the practical issues that development teams face. For example, what do you do about programmers working overtime? How do you use XP on projects to replace legacy systems? One important theme in the book is that of constantly testing the code to ensure that it meets requirements. The authors give practical help on what to test, how to test, and what tools to use in this development environment. Another theme is that the programming pair collectively owns the code. This means the pair is responsible for making it all work and ensuring that it all conforms to standards. If something is missing or needs rework to get the code to release, the pair adds it and tests the revised code. The importance of small tasks and fast turnaround of booked-out code becomes apparent as the book’s examples continue. Fortunately, the authors give guidance on code management tools and the code release process. If developers take away only one thought from this book, it should be this: make estimates for your work based on reality, not best intentions or wishes and hopes. The programming pair works on design, tests, and code on the basis of their own estimates. This gives all programmers and designers the potential to work according to their best speed and abilities rather than expectations imposed on them. The need for reality extends to reporting progress and test results. As the authors put it, “the Extreme way: tell the truth straight out.”",IEEE Software,2002.0,10.1109/MS.2002.1003465,22891722,semantic_scholar
d0f2b863b1af919d08620efe960f708aabe63199,https://www.semanticscholar.org/paper/d0f2b863b1af919d08620efe960f708aabe63199,SECURE DESIGN AND IMPLEMENTATION OF DISTRIBUTED AND INTEROPERABLE INFORMATION SYSTEMS BASED ON OVERLAP KNOWLEDGE PATTERN,"New architectural and technical forms of information systems add a more significant level of complexity due to the decentralization of the constraints, treatment and data. These architectures increase the deployment and the runtime possibilities because of the number of existing sites. Indeed, the simple separation of the various functional levels as it is done in a classical architecture (Data, Treatment, Presentation) is not enough and the choice of the site of deployment or runtime becomes significant for the optimization of the production of the system. In these architectures, these decisions of distribution are generally made during the implementation phase. The conceptual structures offered to designers to allow them to express their needs for distribution (concepts of packages, business component,..) do not match with the rules used by developers for building their distributed components [Snene04B]. In fact, software components represent a single and autonomous concept of real world. They encapsulate all the data concerning this concept including name, goal, behavior and all other information with regard to them. In fact, a software component is a set of objects that can be physically deployed on two or several sites. It is usually made up of one or of several distributed components that offer together the various aspects of distribution necessary to the software component. The distributed components represent the physical modules used for application assembling. They encapsulate given data and treatments and provide their services through well-defined interfaces.",,2005.0,,2178042,semantic_scholar
9cd7293da245d385efb2058faa39dd11678dd572,https://www.semanticscholar.org/paper/9cd7293da245d385efb2058faa39dd11678dd572,Sistemi riconfigurabili a basso consumo per applicazioni di monitoraggio distribuito,"The term Ambient Intelligence (AmI) refers to a vision on the future of the information society where smart, electronic environment are sensitive and responsive 
to the presence of people and their activities (Context awareness). In an ambient intelligence world, devices work in concert to support people in carrying out their everyday life activities, tasks and rituals in an easy, natural way using information and intelligence that is hidden in the network connecting these devices. This promotes the creation of pervasive environments improving the quality of life of the occupants and enhancing the human experience. 
AmI stems from the convergence of three key technologies: ubiquitous computing, ubiquitous communication and natural interfaces. 
Ambient intelligent systems are heterogeneous and require an excellent cooperation between several hardware/software technologies and disciplines, including signal processing, networking and protocols, embedded systems, information 
management, and distributed algorithms. 
Since a large amount of fixed and mobile sensors embedded is deployed into the environment, the Wireless Sensor Networks is one of the most relevant enabling technologies for AmI. WSN are complex systems made up of a number of sensor nodes which can be deployed in a target area to sense physical phenomena and communicate with other nodes and base stations. These simple devices typically embed a low power computational unit (microcontrollers, FPGAs etc.), a wireless communication unit, one or more sensors and a some form of energy supply (either batteries or energy scavenger modules). 
WNS promises of revolutionizing the interactions between the real physical worlds and human beings. Low-cost, low-computational power, low energy consumption and small size are characteristics that must be taken into consideration when designing and dealing with WSNs. 
To fully exploit the potential of distributed sensing approaches, a set of challengesmust be addressed. Sensor nodes are inherently resource-constrained systems with very low power consumption and small size requirements which 
enables than to reduce the interference on the physical phenomena sensed and to allow easy and low-cost deployment. They have limited processing speed,storage capacity and communication bandwidth that must be efficiently used 
to increase the degree of local ”understanding” of the observed phenomena. 
A particular case of sensor nodes are video sensors. This topic holds strong interest for a wide range of contexts such as military, security, robotics and most recently consumer applications. Vision sensors are extremely effective for medium to long-range sensing because vision provides rich information to human operators. However, image sensors generate a huge amount of data, whichmust be heavily processed before it is transmitted due to the scarce 
bandwidth capability of radio interfaces. In particular, in video-surveillance, it has been shown that source-side compression is mandatory due to limited bandwidth and delay constraints. Moreover, there is an ample opportunity 
for performing higher-level processing functions, such as object recognition that has the potential to drastically reduce the required bandwidth (e.g. by transmitting compressed images only when something ‘interesting‘ is detected). The energy cost of image processing must however be carefully minimized. 
Imaging could play and plays an important role in sensing devices for ambient intelligence. Computer vision can for instance be used for recognising persons and objects and recognising behaviour such as illness and rioting. 
Having a wireless camera as a camera mote opens the way for distributed scene analysis. More eyes see more than one and a camera system that can observe a scene from multiple directions would be able to overcome occlusion problems and could describe objects in their true 3D appearance. In real-time, these approaches are a recently opened field of research. 
In this thesis we pay attention to the realities of hardware/software technologies and the design needed to realize systems for distributed monitoring, attempting to propose solutions on open issues and filling the gap between 
AmI scenarios and hardware reality. The physical implementation of an individual wireless node is constrained by three important metrics which are outlined below. 
Despite that the design of the sensor network and its sensor nodes is strictly application dependent, a number of constraints should almost always be considered. Among them: 
• Small form factor to reduce nodes intrusiveness. 
• Low power consumption to reduce battery size and to extend nodes lifetime. 
• Low cost for a widespread diffusion. 
These limitations typically result in the adoption of low power, low cost devices such as low powermicrocontrollers with few kilobytes of RAMand tenth of kilobytes of program memory with whomonly simple data processing algorithms can be implemented. However the overall computational power of the WNS can be very large since the network presents a high degree of parallelism that can be exploited through the adoption of ad-hoc techniques. Furthermore through the fusion of information from the dense mesh of sensors even complex phenomena can be monitored. 
In this dissertation we present our results in building several AmI applications suitable for a WSN implementation. The work can be divided into two main areas:Low Power Video Sensor Node and Video Processing Alghoritm and Multimodal 
Surveillance . 
Low Power Video Sensor Nodes and Video Processing Alghoritms 
In comparison to scalar sensors, such as temperature, pressure, humidity, velocity, and acceleration sensors, vision sensors generate much higher bandwidth data due to the two-dimensional nature of their pixel array. 
We have tackled all the constraints listed above and have proposed solutions to overcome the current WSNlimits for Video sensor node. We have designed and developed wireless video sensor nodes focusing on the small size and the flexibility of reuse in different applications. The video 
nodes target a different design point: the portability (on-board power supply, wireless communication), a scanty power budget (500mW),while still providing a prominent level of intelligence, namely sophisticated classification algorithmand high level of reconfigurability. We developed 
two different video sensor node: The device architecture of the first one is based on a low-cost low-power FPGA+microcontroller system-on-chip. 
The second one is based on ARM9 processor. Both systems designed within the above mentioned power envelope could operate in a continuous fashion with Li-Polymer battery pack and solar panel. Novel low power low cost video sensor nodes which, in contrast to sensors that just watch the world, are capable of comprehending the perceived information in order to interpret it locally, are presented. Featuring such intelligence, these nodes would be able to cope with such tasks as recognition of unattended bags in airports, persons carrying potentially dangerous objects, etc.,which normally require a human operator. Vision algorithms for object detection, acquisition like human detection with Support Vector Machine (SVM) classification and abandoned/removed object detection are implemented, described and illustrated on real world data. 
Multimodal surveillance: In several setup the use of wired video cameras may not be possible. For 
this reason building an energy efficient wireless vision network for monitoring and surveillance is one of the major efforts in the sensor network community. Energy efficiency for wireless smart camera networks is one of the major efforts in distributed monitoring and surveillance community. 
For this reason, building an energy efficient wireless vision network for monitoring and surveillance is one of the major efforts in the sensor network community. The Pyroelectric Infra-Red (PIR) sensors have been used to extend the lifetime of a solar-powered video sensor node by providing an energy level dependent trigger to the video camera and the wireless module. Such approach has shown to be able to extend node lifetime and possibly result in continuous operation of the node.Being low-cost, passive (thus low-power) and presenting a limited form factor, 
PIR sensors are well suited for WSN applications. Moreover techniques to have aggressive power management policies are essential for achieving long-termoperating on standalone distributed cameras needed to improve the power consumption. We have used an adaptive controller like Model Predictive Control (MPC) to help the system to improve the performances outperforming naive power management policies.",,2010.0,10.6092/unibo/amsdottorato/2980,107921835,semantic_scholar
09b7e69ffd621e9025cbc1a313f41a8b63f98738,https://www.semanticscholar.org/paper/09b7e69ffd621e9025cbc1a313f41a8b63f98738,Arms: a decentralised naming model for object-based distributed computing systems,"Entities communicate with one another in distributed computing systems via symbolic names. Implementing such communication requires a naming scheme that dynamically maps these symbolic names to physical nodes and processes. Traditionally, a centralised name server is deployed to perform such translations. However, a collaborative and dynamic environment requires a decentralised naming system due to reasons of efficiency and reliability. 
 
ARMS (Adaptive, Randomised and Migration-enabled Scheme) is a novel decentralised naming scheme for distributed object-oriented computing systems. A notable feature of ARMS is that it provides direct naming supports for the patterns of object communication and object migration processes to achieve greater performance and scalability in executing object-oriented software within a distributed environment. These supports are driven by three key components: 1) an adaptive locating protocol that exploits the patterns of object communication and explores the best routing path in the face of the changing network conditions, 2) a randomised overlay that is a scalable and flexible substrate for routing name queries, and 3) a hybrid relocation scheme that provides a transparent and efficient means of referencing migrated objects. 
 
The performance of ARMS has been examined using a number of real world Java-based benchmarking programs. Based on results in this study, ARMS has found to be superior to its structural counterpart – the Chord model because of the adaptive routing protocol and the resilient overlay. Furthermore, ARMS has shown to be superior in a number of other performance metrics.",,2010.0,,59657940,semantic_scholar
3d80085c2bc6e289c5620bbf06b0878f4b3be001,https://www.semanticscholar.org/paper/3d80085c2bc6e289c5620bbf06b0878f4b3be001,Reliable middleware framework for rfid system,"The reliability of RFID systems depends on a number of factors including: RF interference, deployment environment, configuration of the readers, and placement of readers and tags. While RFID technology is improving rapidly, a reliable deployment of this technology is still a significant challenge impeding wide-spread adoption. This research investigates system software solutions for achieving a highly reliable deployment that mitigates inherent unreliability in RFID technology. 
We have considered two different problem domains for large scale RFID deployment. One is item tracking and the other is guidance-monitoring. 
The basic contribution of our work is providing novel middleware solution that is able to serve the application taking into account the inherent unreliability of RFID technology. Our path abstraction that uses the physical flow of data as an ally to generate a logical system level flow enhances the performance in many ways. The contributions of this dissertation are summarized below: 
Defining novel system architecture for item tracking applications: We have defined a system architecture referred to as Reliable Framework for RFID (RF2ID) that takes into account the unreliability of RFID devices and provides a scalable, reliable system architecture for item tracking applications. It uses a distributed system abstraction named Virtual Reader (VR) that handles RFID data in different geographic locations. Virtual Path (VPath) is the abstraction that creates channels among the VRs and facilitates a data flow oriented data management in the system. 
Implementation of RF2ID: We have implemented RF2ID that is able to incorporate physical RFID devices as well as emulated devices for scalability study taking into account various real world challenges of large scale RFID deployment. 
Load Shedding Based Resource Management: RF2ID requires a mechanism to handle unexpected system load in the presence of asynchronous arrival of data items. Space based load shedding and time based load shedding techniques are used in RF2ID. The basic idea is to exploit the VR and Vpath abstraction to intelligently share the load among the VRs in the presence of high system load, and yet provide some guaranteed Quality of Service (QoS). 
Architecture for GuardianAngel: We define an architecture for an indoor pervasive environment which provides novel system abstraction and communication framework. The layered architecture has distributed computational elements known as the virtual station (VS) that are in charge of serving different regions of the environment. The Mobile Objects (MO) are the physical and logical entities that use sensing device and traverse the environment. The environment itself is tagged with RFID. The MO uses its sensing device to make guidance decisions locally. The VS keeps status information of MOs and keeps coarse grained information of the MO over time and space providing a virtual location for each MO. 
Implementation of GuardianAngel: We have implemented the GuardianAngle system as defined by the architecture. We have used a testbed that uses real RFID readers and tags in the pervasive environment in a limited laboratory setup. We have also developed a distributed system setup using emulated tags for a scalability study of the proposed architecture. We have also implemented a prototype application, to test its feasibility in the real world. 
Evaluation of the system: We have conducted extensive evaluation using the real RFID testbed as well as scalability study using emulated readers and tags. The evaluation using the real RFID tags and readers gives us the credibility of the system under various environmental considerations. The large scale experimentations provide us with scalability and feasibility study to strengthen our limited resource study using real RFID testbed. (Abstract shortened by UMI.)",,2010.0,,113128152,semantic_scholar
e5334acd240594bf4a308887386ba7c4522278f4,https://www.semanticscholar.org/paper/e5334acd240594bf4a308887386ba7c4522278f4,Interim report for Mobile Computing Video transmission using USRP Submitted,"The project envisions a real-time video transmissio n between two points using GNU Radio and Universal Software Radio Peripheral (USRP). In our project, a video signal which could be a realtime signal from a camera or simply a video file is modulated and processed by GNU radio and transmitted using a USRP. There is a USRP receiver node which receives the signal and GNU radio demodulates and re-produces the transmitted video s ignal. This project brings in several challenges like bringing the camera interface to the USRP envi ronment, packets getting lost or corrupted in air, maintaining a constant bit rate as required by the USRP. Introduction USRP is a hardware platform that allows general pur pose computers to function as high bandwidth software radios [1]. Application layer communicates with the physical layer through some intermediate layers. For a stationary host this act ivity seems to be a good option where the communication protocol is systematic and defined ac cording to the environment where it is located. But for a mobile node, the environment conditions c hange with time and hence, the transmit power, bandwidth and quality of the channel has to be cont inuously monitored and passed on to the application layer in order to select the suitable a lgorithm. In turn, the physical layer has to change as per the suggestion from application layer time to t ime. GNU Radio and USRP bring the application developer close to the hardware as near to the ante nna itself and provides user with the flexibility t o change the communication parameters on the fly [2]. USRP aids engineers for rapid prototyping and development of powerful and flexible software radio systems. The applications of USRP come in manifold. It is used widely in prototyping and rese arch applications but it has been deployed in many real-world commercial and defence systems as well [ 3]. In our project, we explore an application of USRP which transmits real-time video from one point to another using USRP. Real-time video transmission finds application in Digital Video Bro adcasting (DVB). Background There has been a tremendous growth in the field of multimedia and mobile communications. The convergence of these two has resulted in mobile mul ti edia communications which has attracted the attention of the research community around the worl d [1]. A lot of researches have been done in this area to find out new methodologies to improvise or innovate new ways to implement the technology with better bandwidth and energy efficiency as thes e two resources are limited. USRP is an emerging technology which provides a platform to excavate th mobile computing environment in different scenarios. GNU radio and USRP provides a powerful radio commun ication platform. GNU Radio is an opensource Software-Defined Radio (SDR) platform which has libraries for various modulation schemes, error-correcting codes and scheduling [3]. GNU Radi o runs as an application which interacts with the USRP hardware. The complex processing like modulati on, and signal processing which are conventionally implemented in hardware, can now be implemented in the software and is easily accessible to the user developing the application. Applications can be created using the GNU Radio blocks or the Python script language which is behin d the blocks. The performance-critical signal processing path is implemented in C++. SWIG interfa ce, which is an interface compiler, is used to link the C++ with the python. The hardware has the antenna, RF (Radio Frequency) front-end, ADC/DAC, USB interface and a user-programmable Fiel d Programmable Gate Array (FPGA) to perform down-conversion. There are newer versions o f USRPs which has Ethernet interfaces and powerful FPGAs for improved speed and processing. Basic block diagram The file source or data can be a webcam output or a video file. Two USRPs are used, one for transmitting and the other for receiving. The sourc e file is Gaussian Minimum Shift Keying (GMSK) modulated and transmitted using transmitter USRP. T he modulation is implemented on GNU Radio. The transmitted signal is received by the receiving USRP and further demodulated by GNU Radio and played back. Figure 1 shows the block diagram Figure 1 Block diagram Experiment set up and plan The project activity is planned in different steps. 1. In the first step we use a stored video file as sou rce. We try to playback this video sent from the transmitter and check whether we are able to re produce the same video at the receiver. This shall be done in simulator mode without the us of USRP and just by the loop back of transmitter and receiver. This set up implemented i s shown in Figure 2. Figure 2 Video transmission in simulator mode 2. The USRPs shall be introduced into picture and loop back removed. The receiver and transmitter side is implemented on each USRP and ag ain a video file shall be transmitted and received. 3. Real-time video capture can be established using a camera which acts as the new video source. GStreamer framework could be used to proces s the video signal and connect to the GNU radio. The block diagram of this idea is shown in Figure 3. Figure 3 Block diagram with real time video [4] The different options available to stream real-time video are: 1. GStream framework which is an open-source software that processes and encodes the video signal from the camera. We have to research other p layers like FFmpeg also available in the market. 2. UDP sockets listening to the camera port at one end and another UDP socket sending to the camera port at the other end. These two options need to be explored a little bit and finalised. After doing some research on the modulation schemes us d in video broadcasting and multimedia applications, we found out that GMSK and Orthogonal Frequency Division Multiplexing (OFDM) are widely used in video communication. Further researches are required on the format of vi deo signals or packets and their encoding schemes. As USRP works on constant bit rate data stream the video signals need to be in the correct format for communication. A good idea on the video encoding wi ll help in debugging in case any problem arises during the development and testing. Expected results and comments As per the experimental set up explained above, the first part of the project is successfully complete d. We were able to transmit and receive a video file w ith a loop back from transmitter to receiver using GMSK modulation in simulator mode. For the second a third part, we need to maintain a constant bit rate data stream for USRP. An H.264 encoder can be used to perform this and we expect that the transmission and receiving of the video file should be smooth and successful. We need to observe the following parameters during the experiment. 1. Delay in reception of the video. 2. Packet loss incurred during transmission. Once the main part of the project is successfully c ompleted, which is proper communication between the two peripherals, we can further analyse and com pare other modulation schemes suitable for video transmission. Also the maximum distance within whic h USRP’s can communicate can be explored. We anticipate that distance between the two USRPs p lay an important role. There could be distortion in the video as the distance increases due to the l oss of packets or erroneous packets. Introducing a suitable error correction scheme can correct the da ta fr mes received. This implementation can be done depending on the time we have at hand. References 1. http://gnuradio.org/redmine/attachments/129/USRP_Do cumentation.pdf 2. http://www.wu.ece.ufl.edu/projects/wirelessVideo/pr ject/H264_USRP/index.htm 3. http://www.ettus.com/downloads/ettus_broch_trifold_ v7b.pdf 4. http://wiki.oz9aec.net/index.php/Simple_DVB_with_Gs treamer_and_GNU_Radio",,2011.0,,32289343,semantic_scholar
a72d2df90c87909159ee5f59811f722b7f6ad4ad,https://www.semanticscholar.org/paper/a72d2df90c87909159ee5f59811f722b7f6ad4ad,"USENIX Association Proceedings of BSDCon ’ 03 San Mateo , CA , USA September","The ever increasing mobility of computers has made protection of data on digital storage media an important requirement in a number of applications and situations. GBDE is a strong cryptographic facility for denying unauthorised access to data stored on a ‘‘cold’’ disk for decades and longer. GBDE operates on the disk(-partition) level allowing any type of file system or database to be protected. A significant focus has been put on the practical aspects in order to make it possible to deploy GBDE in the real world. 1 1. Losing data left and right In the last couple of years, gentlemen of the press have repeatedly been able to expose how laptop computers containing highly sensitive or very valuable information have been lost to carelessness, theft and in some cases espionage. [THEREG] The scope of the problem is very hard to gauge, since it is not a subject which the involved persons and, in particular, institutions are at all keen on having exposed. However, a few data points have been uncovered, revealing that the U.S. Federal Bureau of Investigation loses, on average, one laptop every three days. [DOJ0227] When a computer is lost, stolen or misplaced, it is very often the case that the computer hardware represents a value which is insignificant compared to the value of the disk contents. More often than not, the only reason the press heard about it was that the material on the disk was ‘‘hot’’ enough to make the loss of control rattle people at government level. While it is easy to blame these incidents on ‘‘user error’’, as is generally done, doing so makes it a very hard problem to fix. Human nature being what it is, seems to remain just that. In the absence of technical counter measures, administrative measures have been applied, generally with abysmal results. In one case, a bureaucracy has handled the problem according to what could easily be 1 This software was developed for the FreeBSD Project by Poul-Henning Kamp and NAI Labs, the Security Research Division of Network Associates, Inc. under DARPA/SPAWAR contract N66001-01-C-8035 (‘‘CBOSS’’), as part of the DARPA CHATS research program. mistaken for the plot from a classic Buster Keaton movie: First a laptop was forgotten and lost in a taxi-cab. New policy: always drive your own car if you bring your laptop. Then a car was stolen, including the laptop in the trunk. New policy: always bring your laptop with you. The next laptop was stolen from a pub while the owner was bowing to the pressures of nature. New policy: employees are not to carry their own laptops outside the office at any time. Laptops will be transported from and to the employees home address by the agency security force and will be chained and locked to a ring in the wall installed by the company janitors. All requests must be filed 3 days in advance on form ##-#. [PRIV] 2. Protecting disk contents Protecting the contents of a computer’s disk can in practice be done in two ways: by physically securing the disk or by encrypting its contents. Physical protection is increasingly impossible to implement. It used to be that disk drives could only be moved by forklift, but these days a gigabyte disk is the size, but not quite yet the thickness, of a postage stamp. While computers can be tied down with wires and bars can be put in front of windows, such measures are generally not acceptable, or at least not judged economically justified in any but the most sensitive operations. That leaves encryption of the disk contents as the only practical and viable mode of protection, and both the practicality and the viability has been somewhat in doubt. Until recently, nearly all aspects of cryptography were a highly political issue, this has eased a lot in the last couple of years and there now ‘‘only’’ remain a number of rather fundamental questions in the area of law enforcement and human rights, which are still unsettled. With the political issues mostly out of the way, the next roadblock is practical: While use of cryptography can never be entirely transparent, the overhead and workload it brings must be reasonable. 2.1. Application level encryption Encryption at the application level has been available for a number of years, primarily in the form of the PGP [PGP] program. This is about as intrusive and demanding as things can get: the user is explicitly responsible for doing both encryption and decryption and must enter the pass-phrase for every operation. 2 Apart from the inconvenience of this extra workload, many org anisations would trust their users neither to get this right nor even to want to get it right. From an institutional point of view it is important that cryptographic data protection can be made mandatory. 2.2. Filesystem level encryption Encryption at the file system level is a tried and acknowledged method of providing protection, but it suffers from a number of drawbacks, mainly because no mainstream file systems offer encryption. Encrypting file systems are speciality items, which means increased cost and system administration problems of all sorts. And since practically all operating systems use their own file system format, cross platform fully functional file systems are very rare. This means that a typical organisation will have to operate with a handful of different methods of encryption, which translates to system administration overhead, user confusion and extra effort to pass security and ISO9000 audits. A secondary, but increasingly important issue is that data which are stored in databases on raw disk, operating system paging areas and other such data are not protected by a cryptographic file system. To protect these would mean adding yet another set of encryption methods, which leads to a situation which is very hard to handle practically and administratively. Finally, file systems have a complex programming interface to the operating system, which traditionally 2 Interestingly, this is so impractical in real world use that various applications with PGP support resort to caching the pass-phrase at the application level, thereby weakening the protection a fair bit. has been subject to both version skew and compatibility problems. 2.3. Disk level encryption Encryption at the disk level can protect all data, no matter how they are stored, file system, database or otherwise. To a user, encryption at the disk level would require authentication before the computer can be used, everything functioning transparently thereafter, with all disk content automatically protected. Given that the programming interface for a disk device is very simple and practically identical between operating systems, there are no technical reasons why the same implementation could not be used across several operating systems. All in all this is a close to ideal solution from an operational point of view. There are significant implementation issues however. In difference from the higher levels, encryption at the disk level has no way of knowing a priori which sectors contain data and which sectors do not; neither is knowledge available about access patterns or relationships between individual sectors. Where application level or file system based encryption schemes can key each file individually, a disk based encryption must key each and every sector individually, ev en if it is not currently used to hold data. It has been argued that the encryption ideally should happen in the disk-drive, and while there are steps in this direction, they do unfortunately seem to have been made for the wrong reasons by the wrong people [CPRM], and have consequently not gained acceptance. Provided the owner of the computer remains in control of the encryption, I see no reason why encryption in the disk drives should not gain acceptance in the future. 3. Why this is not quite simple Several implementations have been produced which implement a disk encryption feature by running the user provided passphrase through a good quality one-way hash function and used the output as a key to encrypt all the sectors using a standard block cipher in CBC mode. A per sector IV for the encryption is typically derived from the passphrase and sector address using a one-way hash function. Tw o typical examples are [CGD] and [LOOPAES]. Unfortunately this approach suffers from a number of significant drawbacks, both in terms of cryptographic strength and deployability. For data to stay protected for decades or even lifetimes, sufficient margin must exist not only for technological advances in brute force technology, but also for theoretical advances in cryptoanalytical attacks on the algorithms used. Protecting a modern disk, typically having a few hundred millions of sectors, with the same single 128 or 256 bits of key material offers an incredibly large amount of data for statistical, differential or probabilistic attacks in the future. Worse, because the sectors contain file system or database data and meta data which are optimised for speed, the plaintext sector data typically have both a high degree of structure and a high predictability, offering ample opportunities for statistical and known plaintext attacks. This author would certainly not trust data so protected to be kept secret for more than maybe fiv e or ten years against a determined attacker. But far more damning to this method is that there can only be one single passphrase for the disk. This effectively rules out the ability for an organisation to implement any kind of per-user or multilevel key management scheme: the only possible scheme is ‘‘one key per disk’’. Add to this that to change the passphrase the entire disk would have to be decrypted and re-encrypted, and we have a model which may work in theory, and can be made to work in practice for a determined individual, but which would fast become an operational liability for any org anisation. 4. Designing GBDE The initial design phase of GBDE focused on determining a set of features which would make it both possible and",,2003.0,,14708661,semantic_scholar
333f5f0ec5c5bebbbb4eb77b1eb18da93a4c1c25,https://www.semanticscholar.org/paper/333f5f0ec5c5bebbbb4eb77b1eb18da93a4c1c25,Improving data availability in mobile applications through enhanced cooperative localization,"One of the widely recognized advantages of distributed computing is that through the use of commodity hardware and software, powerful computing systems can be acquired and deployed with lower acquisition costs and shorter application development time than they could be otherwise. While there has been an abundance of research in distributed systems using traditional wired networks, widespread research into the use of mobile ad-hoc networks (MANETs) for distributed computing has only recently emerged. In addition, the integration of off-the-shelf hardware into mobile sensor networks is a relatively recent trend. Thus the impact of commodity hardware integration on the suitability of such systems has not been addressed in any significant way, although from a networking perspective, characteristics and limitations of MANETs are well known. Using MANETS as sensor networks poses unique challenges with respect to localization; many sensor net applications rely on coupling collected data with the location at which sensor readings were taken. Additionally, many applications that rely on data replication, such as peer-to-peer networks, exploit spatial locality to achieve greater communication efficiency, and thus require robust localization schemes. However, they are frequently deployed to places lacking infrastructure to facilitate positioning. Although reliable localization is generally assumed in many works, that is frequently not the case encountered in practice, which leads to poor application performance. This work proposes the idea that reliability and availability of data in location-sensitive applications can be improved by leveraging a cooperative data sharing model for localization, especially beneficial in systems constructed from low-cost, commercial off-the-shelf hardware components. In this proposal I first discuss the TeamTrak platform [23], a mobile testbed implementation for outdoor and urban environments, which is an important component of this work. Operation of physical implementations in noisy, uncontrolled environments presents both system designers and maintainers with unique fault tolerance challenges, many of which are unanticipated and frequently open new avenues for future research. Due to practical considerations, a number of the ideas presented in this work will be explored solely through simulation. However, evaluation tasks will be conducted within the context of the testbed to the greatest extent practicable, with the goal of further validation in real-world settings. Experiences building, deploying, and evaluating ideas using TeamTrak form the basis for much of this work; live tests provide empirical evidence which can improve assumptions about both the system and environment used in simulation as well as in the localization techniques presented. As the foundation for localization work, I first explore the use of low-cost sensor devices for navigation purposes and propose techniques for detecting, reducing, or compensating for errors in reported sensor data. This requires a rigorous evaluation of sensor data under various operational conditions and any resultant failure modes. With the sensor evaluation results providing a basis for understanding when and how failures occur, I next propose techniques for improving data reliability with respect to localization among mobile nodes using a cooperative data sharing model, described in Section 4. Because the Global Positioning System (GPS) has known limitations, dead reckoning can be used to approximate location for each mobile node when GPS is unavailable or unreliable, but such approximations require an accurate starting position and can also be error-prone. System-wide localization can be improved by leveraging relatively reliable data from one or more remote sources.",,2009.0,,6565828,semantic_scholar
a75db8fb465a14f3b86f2c0005662fef61a6b0ca,https://www.semanticscholar.org/paper/a75db8fb465a14f3b86f2c0005662fef61a6b0ca,Optical system evaluation,"Abstract : Optical and infrared sensors have an important role to play in modern military engagements, as the deployment of passive Systems increases. To guarantee the efficient development and usage of such equipment, at a reasonable cost, a reliable and realistic simulation of sensor performance is fundamental. The research project presented in this thesis consists of two parts. First, basic software modules that characterize the target-detector radiative transfer problem were developed. This was accomplished by developing separate modules for each physical aspect of the problem. The second part concerned the viability of implementing the physics of such real-world radiative transfer effects into existing military simulation tools. The chosen simulation environment for this effort was NPS Platform Foundation, an existing simulating software package that was developed at the Naval Postgraduate School.",,1995.0,,107873478,semantic_scholar
635c01ca9f63bac692d2b9e4e6f4bdaf9aef4ce7,https://www.semanticscholar.org/paper/635c01ca9f63bac692d2b9e4e6f4bdaf9aef4ce7,TACTICAL INSERTION MISSION PLANNING AND REHEARSAL USING VIRTUAL REALITY SIMULATION,"Systems Technology, Inc. (STI) has developed a versatile new system for parachute mission planning and rehearsal, combining the validated technology of STI's PC-based PARASIM parachute simulation system with real-time interactive networking, powerful scene generation graphics tools, and terrain-correlated wind fields. This Tactical Insertion Mission Planning and Rehearsal Simulator (TIMPARS) was developed under the SBIR program, funded by the US Special Operations Command (SOCOM). The TIMPARS system rests on four cornerstones: the PARASIM  simulation software, the real-time interactive network, the scene generation toolkit, and the terrain-correlated wind generation module. These elements combine to produce a system with which users can utilize geospecific terrain data and imagery to recreate a real-world site as a simulation scene, input actual or forecasted wind speeds and directions at altitude above the chosen location to generate a terrain-correlated wind field specific to the simulation scene, and then plan and rehearse a mission in a real-time simulation environment with multiple live participants interacting in the same virtual space. BACKGROUND STI's original parachute simulator was developed for use by smokejumpers, US Forestry Service airborne firefighters. Designed to teach round and ramair canopy control, this early version employed rudimentary graphics with a fixed monitor; users stood before the display monitor and pulled simple toggle lines to maneuver in the simulation. Despite the austere configuration, this version provided the minimum cues required to teach parachute flight safely at low cost. In 1996, STI launched a major development effort to incorporate new photo- realistic graphics and head-mounted display/virtual reality technology into the simulator. Subsequent development efforts produced malfunctions procedures software, riser controls, harness switches, and additional simulator improvements. The implementation of these enhanced simulators by the US Marine Corps (USMC) and the Military Freefall School in Yuma, AZ, resulted in a drastic drop in the rate of training injuries. In particular, the USMC First Force Reconnaissance Company experienced a 75% reduction in main canopy cutaways after implementing the enhanced simulator in the MC-5 static line deployed ram-air parachute system (SLDRAPS) transition course at Camp Pendleton, CA. TIMPARS PROJECT",,2003.0,10.2514/6.2003-5610,109365992,semantic_scholar
ec0421dd64fdb0b41759c8d9c2875fa6ade6a526,https://www.semanticscholar.org/paper/ec0421dd64fdb0b41759c8d9c2875fa6ade6a526,Localization and Dynamic Tracking Using Wireless-Networked Sensors and Multi-Agent Technology: First Steps,"SUMMARY We describe in this paper our experience of develop-ing a large-scale, highly distributed multi-agent system using wireless-networked sensors. We provide solutions to the problems of localization(position estimation) and dynamic, real-time mobile object tracking, whichwe call PET problems for short, using wireless sensor networks. We pro-pose system architectures and a set of distributed algorithms for organiz-ing and scheduling cooperative computation in distributed environments,as well as distributed algorithms for localization and real-time object track-ing. Based on these distributed algorithms, we develop and implement ahardware system and software simulator for the PET problems. Finally, wepresent some experimental results on distance measurement accuracy usingradio signal strengths of the wireless sensors and discuss future work.key words: localization and mobile object tracking, distributed algorithms,multi-agent systems, wireless sensor network, and MEMS. 1. IntroductionIn recent years, the technology of micro-, electro-mechanical systems (MEMS) has made rapid advances.Various smart devices, such as sensors and actuators withsome information processing and communication capabili-ties embedded within, have been developed and deployed inmany real-world applications [9]–[11]. The application do-mains of such smart devices are numerous, including newgeneration pervasive computing systems, avionics and plantautomation, building and environmental monitoring, multi-hop routing discovery [6], augmented reality and virtue re-ality systems, active badge [13], and multiple robotics sys-tems. To use smart devices in these applications, it requiresto connect a large number of sensors and actuators, up tothousands, tens of thousands or even millions of units, andto integrate and embed sensing, communication, signal anddata processing, and control functionson individualdevices.The key to large-scale networked, embedded MEMS is themechanism with which individual units are programmed towork as a coherent piece toward achieving common goals.Due to the distributed and real-time nature of most applica-tions and the size of such a large system, information pro-cessing and decision making very often need to be carriedout on the units where actions take place.",IEICE Trans. Fundam. Electron. Commun. Comput. Sci.,2002.0,,114294617,semantic_scholar
3121857e7fb87aed8c95f32eb7840a940736b60d,https://www.semanticscholar.org/paper/3121857e7fb87aed8c95f32eb7840a940736b60d,Projeto de um ambiente 3D de visualização e reprodução de eventos capturados e interpretados a partir de ambientes físicos cientes de contexto para aplicações de preparação para emergência.,"Systems for emergency preparedness support, especially those for accurate monitoring of physical environments subjected to emergency situations, are valuable resources for companies and civil defense public institutions, since these systems can help avoiding and/or reducing lives and patrimony losses. Most of the existing monitoring systems described in the literature have limitations, such as: no posterior visualization of emergency situations that have occurred; limited to specific types of applications; inaccurate identification of risk situations; etc. In this work, a system was proposed and evaluated that aims to overcome these limitations through the integrated use of wireless actor and sensor networks, context aware computing and virtual reality. The work consisted on the creation, implementation and evaluation of a recording and playing 3D media in which physical environments subjected to emergency situations are deployed sensors with processing and communication resources. These sensors capture and interpret contexts, which are mapped, through a visual language, on a 3D virtual environment that mimics the physical environment. The use of virtual reality for visualization and access in real-time or afterwards of situations that are occurring in the physical environment, through a 3D Virtual Environment, can overcome the limitations of hypermedia interfaces or continuous media, like video, when the experiences of the real world are very complex. This work describes the project of a recording and playing system, which allows users to play live experiences gathered from the real world for analysis, evaluation, monitoring and training. The novelty of the system resides in two aspects: it uses an optimized recording technique that saves processing time and storage space; it records scene updating commands independent from 3D Players, allowing the visualization of the collaborative virtual environment (CVE) through any existing 3D web players. In collaboration with the Arts and Communication Department (DAC) of UFSCar, a visual language to prompt identification of emergency situations was created as well as an interface to complex systems. Examples of use include the monitoring of industrial plants, flight rehearsals, petrol exploration platforms, etc. This work is part of a collaborative Project between the Networked Virtual Reality Lab (LRVNet) of the Computer Science Department at UFSCar and PARADISE Lab of SITE at University of Ottawa. 2 All complexity of real-time systems don’t will be considered in this dissertation, real-time will be used indicating that the events must be shown faster as possible.",,2006.0,,131887434,semantic_scholar
ee2be26c5e2bfab3885adc0e6c70e4583435bd3e,https://www.semanticscholar.org/paper/ee2be26c5e2bfab3885adc0e6c70e4583435bd3e,How to harness the Grid with OGSA - Tutorial Proposal,"Summary and Conclusion As a conclusion to this tutorial, we will share lessons learnt from our experience developing with OGSA and highlight the limitations as well as the benefits of deploying OGSA middleware. In particular, we will examine the behaviour of the crystal polymorph prediction system operating in a Grid environment and consider how effectively the implementation exploits available resources. We also discuss practical and political considerations that arise from “real world” Grid environments where technical arguments are often compromised by the needs and preferences of different users, organizations and domain administrators. Conduct of tutorial Delivery The tutorial will consist mostly of a talk support by a Powerpoint presentation. We also intend to illustrate some concepts with live software demonstrations: the first outlining the process of creating a simple OGSA service using the Globus Toolkit 3.0 (GT3) from simple interface description through to stub generation and wrapping an implementation using the delegation model; the second demonstrating the crystal polymorph application running in a simulated Grid environment. This will enable participants to experience Grid middleware from a developer’s perspective and lend some reality to the concepts and mechanisms the tutorial covers.",,2003.0,,16885599,semantic_scholar
e0ff1e95e940e29e7cdc99a84492a37e3a051786,https://www.semanticscholar.org/paper/e0ff1e95e940e29e7cdc99a84492a37e3a051786,RIACS FY2001 Annual Report,"Recently, there has been shift from consideration of optimal decisions in games to a consideration of optimal decision-making programs for dynamic, inaccessible, complex environments such as the real world. Perfect rationality is impossible in these environments, because of prohibiting deliberation complexity. Anytime algorithms attempt to trade off result quality for the time or memory needed to generate results. Bounded rational agents are ones that always take the actions that are expected to optimize their performance measure, given the percept sequence they have seen so far and limited resources they have. Process algebras, with basic programming operators, has been used to study the behaviors of interactive multi-agent systems and leading to more expressive models than Turing Machines, e.g., Interaction Machines. By extending process algebra operators with von Neumann/Morgenstern’s costs/utilities, anytime algorithms can be viewed as a basis for a general theory of computation. As the result we shift a computational paradigm from the design of agents achieving one-time goals, to the agents who persistently attempt to optimize their happiness. We call this approach $-calculus (pronounced “cost-calculus”), which is a higher-order polyadic process algebra with a utility (cost) allowing to capture bounded optimization and metareasoning in distributed interactive AI systems. $-calculus extends performance measures beyond time to include answer quality and uncertainty, using k Omega-optimization to deal with spatial and temporal constraints in a flexible way. This is a very general model, just as neural networks or genetic algorithms, leading to a new programming paradigm (cost languages) and a new class of computer architectures (cost-driven computers). The NSERC supported project on $-calculus aims at investigation, design and implementation of a wide class of adaptive real-time distributed complex systems exhibiting meta-computation and optimization. It has also been applied to the Office of Naval Research SAMON robotics testbed to derive GBML (Generic Behavior Message-passing Language) for behavior planning, control and communication of heterogeneous Autonomous Underwater Vehicles (AUVs). Some preliminary ideas have also been utilized in the 5th Generation ESPRIT SPAN project on integration of objectoriented, logic, procedural and functional styles of programming in parallel architectures. It appears that $-calculus can be useful for the NASA Information Power Grid (IPG) Project. The IPG testbed provides access to a widely distributed network of high performance computers. $calculus resource-bounded optimization allows for flexible allocation of resources and scalability needed to tackle hard computation problems, thus $-calculus could provide a unifying metasystem framework for the Information Power Grid. Biosketch: Dr. Eberbach is a Professor at School of Computer Science, Acadia University and an Adjunct Professor at Faculty of Graduate Studies, Dalhousie University, Canada. Previously he was Senior Scientist at Applied Research Lab, The Pennsylvania State University, Visiting Professor at The University of Memphis, USA, Research Scientist at University College London, U.K., Assistant Professor in Poland, and he also has industrial experience. Professor Eberbach’s current work is in the areas of process algebras, resource bounded optimization, autonomous agents and mobile RIACS FY2001 Annual Report October 2000 through September 2001 -135robotics. General topics of interest are new computing paradigms, languages and architectures, distributed computing, concurrency and interaction, evolutionary computing and neural nets. More information about projects, publications, courses taught can be found at http://cs.acadiau.ca/~eberbach October 27, 2000: Feng Zhao, Ph.D.,Principal Scientist, Xerox PARC “Smart Sensors, Collaborative Sensemaking” Imagine a world in which we live where smart roads would be able to tell us when they need repair and which is the best direction to get to the Giants game, smart factories would stock up just enough inventory, ... The rapid advances in micro-electro-mechanical systems (MEMS) and lower-power wireless networking have enabled a new generation of tiny, cheap, networked sensors that can be “sprayed” on roads, across machines, and on walls. However, these massively distributed sensor networks must overcome a set of technological hurdles before they become widely deployable. Keeping up with the constant onslaught of sensory data from say 100,000 sensors is akin to drinking from a fire hose. The Xerox PARC Smart Matter Diagnostics and Collaborative Sensing Project studies the fundamental problems of distilling high-level, humaninterpretable knowledge from distributed heterogeneous sensor signals in a rapid and scalable manner. We are developing powerful algorithms and software systems to enable a wide range of applications, from sensor-rich health monitoring of electro-mechanical equipment to human-aware environments that leverage sensors to support synergistic interactions with the physical world. Biosketch: Feng Zhao is a Principal Scientist in the Systems and Practices Laboratory at Xerox PARC. Dr. Zhao leads the Smart Matter Diagnostics Project that investigates how sensors and networking technology can change the way we build and interact with physical devices and environments. His research interest includes distributed sensor data analysis, diagnostics, qualitative reasoning, and control of dynamical systems. Dr. Zhao received his PhD in Electrical Engineering and Computer Science from MIT in 1992, where he developed one of the first algorithms for fast N-body computation and phase-space nonlinear control synthesis. From 1992 to 1999, he was Assistant and Associate Professor of Computer and Information Science at Ohio State University. His INSIGHT Group developed the SAL software tool for rapid prototyping of spatio-temporal data analysis applications; the tool is currently used by a number of other research groups. Currently, he is also Consulting Associate Professor of Computer Science at Stanford. Dr. Zhao was National Science Foundation and Office of Naval Research Young Investigator, and an Alfred P. Sloan Research Fellow in Computer Science. He has authored or co-authored about 50 peer-reviewed technical papers in the areas of smart matter, artificial intelligence, nonlinear control, and programming tools. October 12, 2000: Irem Tumer, Intelligent Health and Safety Group NASA/Ames “Influence of Variations on Systems’ Performance And Safety” High-risk aerospace components have to meet very stringent quality, performance, and safety requirements. Any source of variation is of concern, as it may result in scrap or rework (translating into production delays), poor performance (translating into customer dissatisfaction), and potentially unsafe flying conditions (translating into catastrophic failures). As part of the Intelligent RIACS FY2001 Annual Report October 2000 through September 2001 -136Health and Safety group, we have been designing controlled experiments to understand various sources of variations in helicopter transmissions, collecting vibration data, and analyzing the data for indicators of the variations. We are looking for normal and abnormal sources of variation that affect performance and indicators of these variations to provide warning about potential failures during flight. The experiments include: • Flight tests using an AH-1 and an OH-58 helicopter, to determine the variations introduced due to regular maneuvering and the covariance with environmental conditions, engine torque, etc.; • OH-58 transmission test-rig tests to determine the effect of variations due to different levels of torque, mast bending, and mast lifting forces, as well as pinion reinstallation effects; • Machinery Fault Simulator tests to test the effect of prefabricated defects and inherent design and manufacturing variations on gears, bearings, etc. In this talk, I will present an overview of our group’s research goals, discuss the experiments and go over some of the results from the data analyses conducted so far. I will then discuss the current work and future directions in developing formalized methods for design and manufacturing engineers, using the variation information from empirical and analytical studies. RIACS FY2001 Annual Report October 2000 through September 2001 -137III.B RIACS-Supported Workshops As part of its mission of fostering ties with the academic community in IT, RIACS provides financial, administrative, and technical support for selected workshops involving RIACS scientists. The following workshops were supported during this reporting year: Workshop on Verification and Validation of Software The RIACS Workshop on the Verification and Validation of Autonomous and Adaptive Systems took place at Asilomar Conference Center, Pacific Grove, CA, 5-7 Dec 2000. Discussions included: V&V of Intelligent Systems: How to verify and validate systems featuring some form of AI-based technique, such as model-based, rule-based or knowledge-based systems. V&V of Adaptive Systems: How to verify and validate systems featuring adaptive behavior, either in the form of parametric adaptation (e.g. neural nets, reinforcement learning) or control adaptation (e.g. genetic programming). V&V of Complex Systems: How to verify and validate systems with different interacting parts, either within a given location (e.g. layered control architectures) and among several locations (homogenous or heterogeneous multi-agent systems). Workshop on Model-based Validation of Intelligence Lina Khatib (Kestrel) and Charles Pecheur co-organized a symposium on “Model-based Validation of Intelligence” as part of the AAAI Spring Symposium Series in March 2001. We provided the technical content (announcement, reviews and selection of articles, final program) while AAAI provided the logistics (rooms, registra",,2001.0,,133512970,semantic_scholar
cb311ca15659cba872bbd8a2152c814fbb4bb2ce,https://www.semanticscholar.org/paper/cb311ca15659cba872bbd8a2152c814fbb4bb2ce,Integrating QFD with Object Oriented Software Design Methodologies,"Object oriented (OO) methodologies have emerged as a popular paradigm for software design and analysis, both in research and practice. Several variants of OO methods are in use, but they all share significant similarities in their approaches to modeling the application domain. Quality Function Deployment (QFD) is also a design analysis and domain modeling technique with many parallels to OO methods. This paper contains an overview of object oriented design concepts, and shows how familiar QFD techniques are an effective aid for the OO analyst. QFD is a much easier way to approach the initial information collection and provides easy-to-understand structuring tools that do not require extensive training in OO concepts and methods. Overview of Object Oriented Software Concepts The following section is a brief overview of some object oriented concepts. It is beyond the scope and purpose of this article to discuss details of OO methodologies in depth. Many good sources of comprehensive elucidations on object oriented software engineering are given in the bibliography. Objects, messages, and encapsulation The fundamental concept in object oriented methodologies is, appropriately, the object. An object is a representation, or model, of a real-world entity. Objects have both data, which are usually called attributes, and behaviors, which are called methods . Since OO technology was heavily influenced by the analysis and design needs of real time control software applications, it was appropriate to envision objects as software representations of physical devices, such as sensors, actuators, and displays. A stepper motor, for example, has a state attribute ( on or off) and behavior ( turn_left, turn_right, and stop_motion). OO methods have also successfully been applied in more traditional information technology domains such as banking, accounting, personnel, etc. In these data-intensive applications, objects represent business entities and the data and processing operations that are associated with them. For example, a banking system might contain objects that represent individual checking accounts. Each account object contains data attributes for the name of the account owner, address, account number, current balance, and so forth. Objects communicate via messages ent to each other, with the assistance of the underlying language and operating run time support systems. Each message received by an object should have procedural code that interprets and carries out the function requested by the message; if the object does not understand the message, an error notification routine is invoked. In the checking 1 Sponsored by the U.S. Department of Defense 1995 QFD Symposium Page 1 May 14, 1995 account example, methods such as deposit_to_account and debit_from_account are invoked by messages exchanged between the account objects and transaction objects. Because these messages are the only interface that the object presents to the ""external world,"" the implementer is free to design internal object representations of data and procedures in any manner, as long as the message interface remains consistent. By taking this approach, details of the internal representation can be modified as needed without affecting the rest of the system. For example, temperature might be stored internally by the thermometer object in units of Fahrenheit, Celsius, or Kelvin, as long as the object accepts and replies correctly to messages requesting any particular reporting unit. This is known as encapsulation , or information hiding, and is a key concept in object orientation. Classes and instances Obviously, if every object in an application had to be individually and explicitly coded, it would not be practical to build a system consisting of hundreds of separate objects. Hence, every object is associated with a specific class. Classes are templates for objects, which specify the kinds of attributes and methods each object will have. Classes do not hold any of the values for the attributes of specific objects. Objects come into existence by being created as instances of a specific class. When an object instance is built from a class, a complicated process is used to determine the attributes and methods for that object, which involves allocating space, linking the object into various system data structures, and perhaps initializing certain attributes. Users of document processing products such as Ami Pro, Framemaker, or Microsoft Word for Windows have already encountered the concept of classes and instances. These products use the notion of paragraph styles to hold templates for various text characteristics, such as fonts, indentations, line alignment, margins, and so forth. Whenever a new paragraph (object) is created (instantiated) in a document, it inherits the formatting options of the template (class), but does not yet contain any data (text). All instances of paragraphs of a certain type (class) share the same formatting characteristics, but have different data attributes. 1995 QFD Symposium Page 2 May 14, 1995 Instance of a 2-door, red, ... van Class of Vans » # doors » length » color » engine option » VIN",,1995.0,,18370224,semantic_scholar
cb6a0f14963b756058f7b33d2f149a0a7cf9a0bd,https://www.semanticscholar.org/paper/cb6a0f14963b756058f7b33d2f149a0a7cf9a0bd,Best Practices in Enterprise Content Management,"“Enterprise” is a badly misused word, unless you’re Captain Kirk. Alternately applied to mean both “big” and “integrated,” I’m afraid it no longer means either. . . . Eric Stevens, Hummingbird . . . . . . . . . . . 4 Corporate Governance—What Enterprise Content Management Was Meant For Organizations throughout the world have successfully deployed a range of enterprise content management (ECM) solutions to address organizational needs. . . . Dan Ryan, Stellent . . . . . . . . . . . . . . . . 6 Six Ways ECM Can Work for Your Business Unstructured content has doubled in the last three years. As a result, the challenge of managing and effectively leveraging this tremendous volume of content across an enterprise continues to grow. . . . Jeffrey Klein, First Consulting Group. . . . . . 8 ECM Best Practices for the Enlightened Enterprise Information overload in the life sciences industry is driving a pressing need for better content management. Companies are overwhelmed by content. . . . Tom Jenkins, Open Text. . . . . . . . . . . . . 10 ECM: Making Process Possible Over the past few years, ECM has emerged as a defined enterprise software category, one that is clearly capturing more and more attention. . . . Ethan Eisner, LexisNexis . . . . . . . . . . . . 12 Maximize User Satisfaction: Five Steps to ECM Success We have all experienced, or heard from colleagues, the horror stories...The good news is that we can learn from these past experiences. . . . Robert Weideman, ScanSoft. . . . . . . . . . 14 Better PDF for Business The single greatest challenge to streamlining document-based processes in business is the fact that there are two incompatible dominant electronic document formats. . . . Johannes Scholtes, ZyLAB . . . . . . . . . . . 16 Affordability in Content Management and Compliance The impetus on organizations to be transparent and fiscally prudent continues to intensify, motivated by the constant drive for more competitive efficiency. . . . Robert Liscouski, Content Analyst . . . . . . 18 Rising to the Real-World Challenges of ECM Initial efforts to implement ECM systems already have demonstrated the potential for achieving significant benefits. . . . Vernon Imrich, Percussion Software . . . . . 20 Champagne Tastes on a Beer Budget: ECM for the Rest of Us The key to getting a champagne feature set at a beer budget lies in considering how an ECM solution is delivered in addition to what features it advertises. . . . David White, Arbortext . . . . . . . . . . . . . 21 Automated Publishing is More Than ECM What defines a formal publishing process? When should an organization consider adopting an automated approach to publishing? If your content has one or more of the following characteristics. . . . Bill Rogers, Ektron . . . . . . . . . . . . . . . 22 New Realities for Mid-Market Content and Document Management Mid-size organizations have frequently found themselves “stuck in the middle” when it comes to finding the right solution for effectively managing, publishing and sharing online content. . . . George Viebeck, EMC Documentum . . . . . 23 Managing High Volumes of Data in SAP Companies are deluged by application data and documents. Some analysts say this data is growing at 80% per year. . . . Charles Hough, Interwoven . . . . . . . . . 24 A Service-Oriented Architecture for Better ECM New business requirements are changing the way companies think about enterprise software. Initiatives and objectives increasingly cross traditional boundaries. . . . A. J. Hyland, Hyland Software . . . . . . . . 25 ECM for Accounts Payable: It Pays—Faster! Why would I want to pay my bills faster? That’s the question some people ask when we talk about the value of using ECM software, such as document imaging. . . . Todd Peters, PaperThin . . . . . . . . . . . . 26 What You DON’T Know About Web Content Management As digital content continues to grow at an exponential rate, organizations increasingly struggle with ways to affordably and efficiently create and manage content on the Web. . . . Best Practices in Enterprise Content Management",,2005.0,,189811104,semantic_scholar
ef481eb331f6bf705904f527205db1ded8e81401,https://www.semanticscholar.org/paper/ef481eb331f6bf705904f527205db1ded8e81401,The Year 2000 Software Crisis : The Continuing Challenge,"1. Year 2000 Progress Update. Year 2000 Update. Studies Paint Disconcerting Picture. The Real Story. Where Is the Sense of Urgency? Coming to Grips with Reality: Look at the Details. Media Update. The Year 2000 Myth? Silver Bullets Abound. Extremists Hurt the Cause. Business Problem Requires Business Solution. Industry by Industry Status. Banking. Securities Firms. Manufacturing. Retail. Transportation. Telecommunications. Utilities and Energy. Health Care, Insurance, and Pharmaceutical. Service Industries. Small to Mid-Size Companies. Government Update. U.S. Federal Government. U.S. Department of Defense (DOD). U.S. State Governments. U.S. Local Governments. Schools and Universities. International Update. Canada. Europe. Pacific Rim/Japan. Other Regions of the World. Worldwide Economic Impacts. 2. Strategy Update: Shift to Risk Mitigation. It's Late in the Game, Now What? Achieving Full Scale Deployment. The Declining Options Picture. Running Out of Time: Alternative Strategies. Launching Parallel Activities. Year 2000 Business Risk Assessment. Begin Fixing Five Most Critical Systems Now! Solidify Systems Inventory. Identify Data Interchange Points. Identify and Document External Entities. Document Business Functions. Relate Systems to Business Functions. Relate External Data Interfaces to Business Functions. Relate External Entities to Business Functions. Identify Event Horizons. Identify Revenue- or Customer-Related Risks. Identify Legal and/or Regulatory Risk. Prioritize Remediation and Testing Projects. Prioritize Business End-User-Driven Projects. Evolving Application Package Options. Vendor Has Delivered Compliant System. Vendor Fixed the System, But It Is Still Not Compliant. Vendor Is Fixing System, But Delivery Times Misses the Compliance Deadline. Vendor Refuses to Provide Compliance Status. Vendor Is Not Going to Fix the System. Strategies for the Far Behind. Targeting to Reduce Risk. ""Safe"" Corner Cutting. Unacceptable Corner Cutting. Launch Top Five Priority Remediation Projects. Beating the Clock in 1998 and 1999. 3. Legal Issues and Protections. Why You Should Care About This Legal Stuff. Minimizing Costs and Exposure. Contract Issues. Disclosure Obligations. Company Statements. Tax Law Issues. Internal Risk Management. Vendor Considerations. Certification Letters. Supply Chain/Partner Issues. The Paper Trail. Insurance. Government Aspects. Regulatory Agencies. Congress. State Government. Other Legal Aspects. Staffing. Offshore Factories. Copyright. 4. Non-IT Issues and Answers. Defining Non-IT Year 2000 Requirements. Business Partner Impacts. Supply Chain Challenge. Embedded and Other Non-IT Technologies. Non-IT Problems: Consequences and Timing. Industry-Specific Non-IT Challenges. Telecommunications. Energy and Power. Health Care. Manufacturing. Wholesale, Retail, and Service Industries. Transportation Sector. Financial. Government and Defense. Supplier, Business Partner Challenges. Supplier Categories. Supply Chain and the Domino Principal. Supplier Strategies: Methodological Approach. Documenting Multitiered Supply Chains. Supplier Responses. Supplier Contingency Options: Hedging Against Failure. Year 2000 Embedded Technology Challenge. Embedded Systems Challenge. Year 2000 Embedded Systems Impacts. Embedded Systems: Types and Categories. Embedded Systems Project Strategies. Embedded Systems: The Bottom Line. 5. Getting Help: Factories, Outsourcing, and Services. Third-Party ServicesA A Lessons Learned to Date. The Roots of the Dilemma. Identifying the Most Common Mistakes. Ground Rules for Successful Vendor Relationships. Selecting the Right Strategy. Strategy Selection Parameters. Using the Parameters to Select a Strategy. Selecting the Right Partner. Enabling Success. Enabling Factories. The Basic Factory Process. Before Using a Factory. The Factory Package. The Factory QA Process. Internal Factories. Enabling Consultants. Offshore Resources. Market Update. 6. Standards, Tools, and Techniques Update. Standards Update. Date Format Standards. Certification Programs. External Data Interchange. Year 2000 Firewall Strategies. Upgrade Unit Packaging Strategies. Remediation Procedures Update. High-Volume Productivity Targets. Field Expansion Update. Windowing Update. Expansion Using Bridges. Useful Notions and Worst Practices. An Array of Implementation Choices. ""Do It Yourself"" (DIY) Update. Optimizing the DIY Approach. Optimizing the Manual Fix Approach. Optimizing the Tool-Assisted Approach. Optimizing the Internal Factory Approach. Rules Of Engagement-Start Now. Year 2000 Tool Utilization Update. Automated Remediation Tools. Date Routines-Revisited. Bridging Routines. Testing Update. PC and Distributed Systems Tools. Risk Simulation Tools. Tracking Risks and Progress: Repository Utilization. Establishing Year 2000 Tracking Repository. Information Requirements. Enterprisewide Metamodel. Physical Repository Requirements. Information Capture and Loading Requirements. Inquiry and Reporting Requirements. 7. Year 2000 Testing Basics. Year 2000 Testing Differences. Scope of Year 2000 Testing. Focus of Year 2000 Testing. Infrastructure for Year 2000 Testing. What Needs to Be Tested. Compliance Requirement for Business Software. End-User Systems. Compliance Requirement for Embedded Technology. Types and Levels of Testing. Types of Tests. Levels of Tests. How Much Testing is Enough? ""Failsafe"" Testing. Typical Application Testing. Risk-Based Targeting. Elements of Risk-Based Analysis. Components Affected by Risk-Based Testing Strategies. Applying Risk-Based Testing Strategies. Applying Testing. 8. Implementing a Year 2000 Test Program. Managing the Enterprise-Level Testing Effort. The Testing Function of the Project Office. Roles and Responsibilities. Enterprise-Level Test Infrastructures. Assessing Enterprise Test Infrastructures. Determining Enterprise Infrastructure Improvements. Implementing Enterprise Infrastructure Improvements. Enterprise-Level Master Test Plan. High-Level Summary. Testing Strategy. Application Consolidation. Infrastructure Requirements. Test Projects List. Master Project Time Line. Managing Application-Level Testing Efforts. Roles and Responsibilities. Application Test Assessment Activities. Determining Application Infrastructure Improvements. Create an Application Test Plan. Test Data Creation. Test Script Creation. Test Execution. Results Validation. Acceptance and Sign-Off. Creating Application Test Plans. High-Level Summary. Testing Strategy. Test Environment. Detailed Test Descriptions. Application Project Time Line. Creating an Application Test Plan for Packaged Software. Factors Affecting Software Package Testing. Approaches to Testing Packaged Software. Developing a Test Plan for Packaged Software. Creating an Application Test Plan for Factory Remediation. Factors Affecting Testing of Factory-Remediated Software. Approaches to Testing Factory-Remediated Software. Developing a Test Plan for Factory-Remediated Software. 9. Contingency Planning: When Time Runs Out. What Is Contingency Planning. Why Plan for Contingencies. Technical versus Business-Driven Contingency Plans. Internally versus Externally Driven Requirements. Contingency Planning Participants. Identifying Contingency Requirements. Hardware and Infrastructure-Driven Requirements. Software-Driven Requirements. Development/Replacement Projects. Business-Driven Contingency Requirements. Building a Contingency Plan. Planning Overview. Business Model Redesign Options. Business Unit Shutdown Options. Business Function Consolidation Options. Triage: The De Facto Contingency Plan. Contingency Planning by Industry. Financial Institutions. Insurance Companies. Health Care Providers. Manufacturing and Retail Industries. Service Industries. Utilities and Telecommunications. Government Strategies. Small Company Contingency Planning. Contingency Plan Invocation. Time Critically Defines Success. Replacement Project Invocation. Invocation Based on Package-Provider Problems. Internal Failure Identification and Invocation. External Problem Identification and Invocation. Pre-2000 and Post-2000 Contingency Invocation. Who Makes Contingency Decisions? Shift toward Contingency Management. Contingency Management Is Continuous. Contingency Options at an Industry Level. Contingency Options at a National Level. Personal Contingency Planning. 10. Managing the Transition: Surviving the Inevitable. Defining the ""Transition Window."" Problems to Date. Predictions: 1998. Predictions: 1999. January 1, 2000. Cleaning Up the Mess: 2001-2005. Insurance Claims and Legal Action. Looking Forward: Industry by Industry. Financial Industry. Health Care. Manufacturing. Utilities and Telecommunications. Transportation. International Implications. Investment and Economic Impacts. Investment Impacts. Economic Impacts. Building a Crisis Management Team. Year 2000 Crisis Management. Crisis Management Requirements. Internal Planning Considerations. External Impact Considerations. The Crisis Management Plan. Crisis Call-In Center. Contingency Planning and Triage Center. Shifting Business Strategies in Crisis Mode. Cleanup Management Window. Year 2000 Will Change IT Landscape. Institutionalize the Project Office. IT Must Move On. IT Outsourcing: Be Careful. Think Globally-Act Locally. Authors' Note. Vendor Lists. Index.",,1998.0,,107855659,semantic_scholar
9e5b88dc69fe2abbfd4c2e8380a33306fcea4e3e,https://www.semanticscholar.org/paper/9e5b88dc69fe2abbfd4c2e8380a33306fcea4e3e,Robotics research : the eighth international symposium,"1. Advanced Manipulation Session Summary.- Elastic Strips: Real-Time Path Modification for Mobile Manipulation.- Modeling and Control for Mobile Manipulation in Everyday Environments.- Scale-Dependent Grasps.- 2. Dynamics and Control Session Summary.- A General Formulation of Under-Actuated Manipulator Systems.- Towards Precision Robotic Maneuvering, Survey, and Manipulation in Unstructured Undersea Environments.- Where does the Task Frame Go?.- 3. Emergent Motions Session Summary.- Motion Synthesis, Learning and Abstraction through Parameterized Smooth Map from Sensors to Behaviors.- Safe Cooperative Robot Patterns via Dynamics on Graphs.- 4. Motion Planning Session Summary.- Motion Planning with Visibility Constraints: Building Autonomous Observers.- Motion Planning in Humans and Robots.- Local and Global Planning in Sensor Based Navigation of Mobile Robots.- Interleaving Motion Planning and Execution for Mobile Robots.- 5. Manufacturing Session Summary.- Opportunities for Increased Intelligence and Autonomy in Robotic Systems for Manufacturing.- Rapid Deployment Automation: Technical Challenges.- Stability of Assemblies as a Criterion for Cost Evaluation in Robot Assembly.- Towards a New Robot Generation.- 6. New Components Session Summary.- The Design of a Serial Communication Link for Built-in Servo Driver and Sensors in a Robot.- Omnidirectional Vision.- Small Vision Systems: Hardware and Implementation.- 7. Mobile Robots Session Summary.- Exploration of Unknown Environments with a Mobile Robot using Multisensorfusion.- Integration of Topological Map and Behaviors for Efficient Mobile Robot Navigation.- A Robotic Travel Aid for the Blind: Attention and Custom for Safe Behavior.- Automated Highways and the Free Agent Demonstration.- The Design of High Integrity Navigation Systems.- 8. Haptics Session Summary.- Tactile Displays for Increased Spatial and Temporal Bandwidth in Haptic Feedback.- Design of an Anthropomorphic Haptic Interface for the Human Arm.- Testing A Visual Phase Advance Hypothesis for Telerobots.- 9. Medical Session Summary.- Robot Assisted Surgery and Training for Future Minimally Invasive Therapy.- Surgery Simulation with Visual and Haptic Feedback.- Synergistic Mechanical Devices: A New Generation of Medical Robots.- 10. Learning from Human Session Summary.- Vision-based Behavior Learning and Development for Emergence of Robot Intelligence.- Using Human Development as a Model for Adaptive Robotics.- Developmental Processes in Remote-Brained Humanoids.- Animating Human Athletes.- 11. Future Robots Session Summary.- Mechanics and Control of Biomimetic Locomotion.- Robots: A Premature Solution for the Land Mine Problem.- Robots Integrated with Environments: A Perceptual Information Infrastructure for Robot Navigation.- Bio-robotic Systems Based on Insect Fixed Behavior by Artificial Stimulation.- 12. Projects in Japan Session Summary.- Physical Understanding of Manual Dexterity.- Tightly Coupled Sensor and Behavior for Real World Recognition.- Intelligence and Autonomy for Human-machine Cooperative System.- Biologically Inspired Approach to Autonomous Systems.- FNR: Toward a Platform Based Humanoid Project.- Current and Future Perspective of Honda Humanoid Robot.- List of Participants.",,1998.0,,107037609,semantic_scholar
0fb400e80dcda882fc293327c5c12e59fd18af93,https://www.semanticscholar.org/paper/0fb400e80dcda882fc293327c5c12e59fd18af93,On-demand Loading of Pervasive-oriented Applications Using Mass-market Camera Phones,"Camera phones are the first realistic platform for the development of pervasive computing applications: they are personal, ubiquitous, and the builtin camera can be used as a context-sensing equipment. Unfortunately, currently available systems for pervasive computing, emerged from both academic and industrial research, can be adopted only on a small fraction of the devices already deployed or in production in the next future. In this paper we present an extensible programming infrastructure that turns mass-market phones into a platform for pervasive computing. 1 Mobile phone: a platform for pervasive computing Pervasive computing tries to make M. Weiser’s vision [1] a reality by saturating the environment with computing and communication devices: the most of the infrastructure is often invisible and supports user’s activities with an interaction model that is strongly human-centric. Today, almost fifteen years later, despite significant progresses in both hardware and software technologies, this vision is still not completely realizable or economically convenient. Supporting the interaction between users and the environment can be greatly simplified if we relax the interaction model and include a personal device as the access medium. Mobile phones are the most obvious candidates: they are in constant reach of their users, have wireless connectivity capabilities, and are provided with increasing computing power [2]. Even better results can be achieved with those phones that are equipped with a camera. Instead of manually getting information or editing configurations, users can point physical objects to express their will of using them: taking a picture of the objects would suffice to setup the link with the offered services. Relaying on an image acquisition device does not impose a strict limit to the share of possible users, since an always growing number of commercially available mobile phone is equipped with an integrated camera: according to recent studies [3], over 175 million camera phones were shipped in 2004 and, by the end of the decade, the global population of camera phones is expected to surpass 1 billion. ? This work is partially supported by the Italian Ministry for Education and Scientific Research (MIUR) in the framework of the FIRB-VICOM project. However, the acquisition of context-related information through images is not a trivial task, especially with resource-constrained devices. To ease the recognition process, objects can be labeled with visual tags readable by machines. Once decoded, visual tags either directly provide information about the resource they are attached to or, if the amount of information is too large, they act as resource identifiers that can be used to gather information from the network. In this paper, we describe the design and the implementation of POLPO 1 (Polpo is On-demand Loading of Pervasive-Oriented applications), a software system that turns mass-market phones into a platform for the development of pervasive applications. With POLPO, a phone with a built-in camera and compatible with the Java 2 Micro Edition (J2ME) platform is able to get context information by decoding visual tags attached to real-world objects. POLPO supports dynamic loading and installation of custom applications used to interact with the desired resources. 2 Background and contribution In this section we summarize the most relevant solutions based on visual tags and the contribution of our system in this field. Cybercode [4] is a visual tagging system based on a two-dimensional barcode technology. The system has been used to develop several augmented reality applications where the physical world is linked to the digital space trough the use of visual tags. Cybercode is one of the first systems where visual tags can be recognized by low-cost CCD or CMOS cameras, without the need for separate and dedicated readers. Each Cybercode symbol is able to encode 24 or 48 bits of information. The system has been tested with notebook PCs and PDAs. In [5] the author presents a system that turns camera-phones into mobile sensors for two-dimensional visual tags. By recognizing a visual tag, the device can determine the coded value, as well as additional parameters, such as the viewing angle of the camera. The system includes a movement detection scheme which enables to use the mobile phone as a mouse (this is achieved by associating a coordinate scheme to visual tags). The communication capability of the mobile phone is used to retrieve information related to the selected tag and to interact with the corresponding resource. Tag recognition and motion detection algorithms were implemented in C++ for Symbian OS. The Mobile Service Toolkit (MST) [6] is a client-server framework for developing site-specific services that interact with users’ smart phones. Services are advertised by means of machine-readable visual tags, which encode the Bluetooth device address of the machine that hosts the service (Internet protocols addressing could be supported as well). Visual tags also include 15 bits of application-specific data. Once the connection has been established, MST servers can request personal information to the client to provide personalized services. Site-specific services can push user interfaces, expressed with a markup language similar to WML (Wireless Markup Language), to smart phones. MST also provide thin-client functionality: servers can push arbitrary graphics 1 The Italian name for the octopus vulgaris, a cephalopod of the order octopoda, probably the most intelligent of the invertebrates. to the phone’s display which in turn forwards all keypress events to the server. The client-side is written in C++ and requires Symbian OS. A similar approach is described in [7], where the authors propose an architecture for a platform that supports ubiquitous services. Real-world objects are linked to services on the network through visual tags based on geometric invariants that do not depend on the viewing direction [8]. But differently from other solutions, image processing does not take place on the user’s device: pictures are sent to a server where they are elaborated and converted into IDs. Instead of using two-dimensional barcodes, an alternative way of performing object recognition is the one based on radio frequency identification (RFID): small tags, attached to or incorporated into objects, that respond to queries from a reader. However this solution, that can be useful in many pervasive computing scenarios, is not particularly suitable when the interaction is mediated by mobile phones, that lack the capability of reading RFIDs. In our opinion, currently available solutions present two major drawbacks: i) they are limited to specific hw/sw platforms (i.e. Symbian OS), excluding most of the models of mobile phones already shipped and in production in the near future; ii) the software needed to interact with the environment is statically installed onto the mobile phone and cannot be dynamically expanded, e.g. to interact with new classes of resources. We designed and developed a system for pervasive computing based on visual tags that overcomes these constraints as follows. Compatibility with J2ME The system runs on devices compatible with the J2ME platform. This environment is quite limited in terms of both memory and execution speed, but also extremely popular (nearly all mobile phones produced). This required the implementation of a pure Java decoder of visual tags for the J2ME environment. Downloadable applications Our system is based on the idea that the interaction with a given class of resources, e.g. printers, public displays, etc., takes place through a custom application. New custom applications can be downloaded from the network and installed onto the user’s device as needed. This brings two advantages: i) the classes of resources that can be used do not have to be known a priori; ii) the user’s device, that is resource constrained, includes only the software needed to interact with the services actually used. The J2ME platform comprises two configurations, few profiles, and several optional packages. The J2ME configurations identify different classes of devices: the Connected Device Configuration (CDC) is a framework that supports the execution of Java application on embedded devices such as network equipment, set-top boxes, and personal digital assistants; the Connected Limited Device Configuration (CLDC) defines the Java runtime for resource constrained devices as mobile phones and pagers. Our systems runs on top of the version 1.1 of the CLDC, that provides support for floating point arithmetics (unavailable in version 1.0). The adopted profile is the Mobile Information Device Profile (MIDP) that, together with CLDC, provides a complete Java application environment for mobile phones. 3 System architecture POLPO requires that physical resources are labeled with visual tags, and that a program providing access to POLPO functionalities is installed onto the user’s device. This program has the following primary functions: – Decoding of visual tags. The image captured with the built-in camera is processed to extract the data contained into the visual tag. – Management of custom applications. The program downloads and installs the custom application required to interact with a resource. Usually, resources of the same kind share the same custom application (i.e., a single application is used to interact with all printers, another is used with public displays, etc). – Management of user’s personal data. In many cases, applications need information about the user to provide customized services. For this reason, the software installed on mobile phones includes a module that manages user’s personal data and stores them into the persistent memory. Managed data comprise user’s name, telephone number, email address, homepage, etc. Each resource is identified and described by a Data Matrix visual tag. Da",IWUC,2006.0,10.5220/0002501600390048,16708155,semantic_scholar
e2ef0b6e85d1952435e79555ef1791696472a24d,https://www.semanticscholar.org/paper/e2ef0b6e85d1952435e79555ef1791696472a24d,Title Using role-play based simulation to acquire tacit knowledge inorganizations : the case of KreditSim,"Knowledge creation and application is crucial for organisations to cope with ever increasing competition. The business-relevant knowledge is reflected in the processes of a company. A process largely consists of tacit knowledge which is embedded in practice or experiences. Acquiring this type of knowledge is crucial for improving business processes. While formal learning or training programs deliver explicit knowledge and skills, it is much more challenging to generate implicit or tacit knowledge out of everyday work activities. In order to help employees and senior managers to acquire and apply tacit knowledge, a role-play based simulation program has been developed. This kind of simulation allows for a learning environment close to the workplace. The case of KreditSim shows how this coaching method actively involves employees and improves awareness of and participation in business process improvement. After identifying the deficiencies in their early process, the learners improve their process in a new simulation run. In this way, the tacit knowledge about processes is externalised, delivered, refined and reused. Social learning is also supported for process knowledge creation and sharing. Introduction According to the resource based view, each company depends on resources such as employees, machines, IT systems and buildings to produce goods or services for its customers (Wernerfelt, 1984). The resources itself do not incorporate business value but their combination to create an output like goods or services does. This combination is conducted within business processes. A business process (the term process is used synonymously here) is characterised by a set of connected activities necessary to deliver a defined business outcome (Davenport/Short, 1990). Starting point for designing a business process is the aspired business outcome which depends on the customers ́ demands. Within a business process, employees using IT systems and other resources transform input into output. Thus, a company can be seen as a bundle of business processes containing the knowledge to produce goods and services (Inkpen/Dinur, 1998). Business process thinking has become a major topic in management. In fact, turning a company into a processoriented organisation is seen as a competitive advantage and fundamental to its success. Such an organisation is more adaptable to changes in the market, faster in delivering output, superior in terms of quality and more responsive to the needs of customers (Hammer/Champy, 1993). But a process-oriented company requires different knowledge compared to a function-oriented organisation (Kugeler/Vieting, 2003). This processoriented knowledge is mostly tacit knowledge (Hawryszkiewycz, 2010). To ensure a permanent improvement of processes not only explicit knowledge but also tacit knowledge has to be deployed. A major question of process-oriented companies is how the necessary tacit knowledge can be acquired. The aim of this paper is to present KreditSim, a role-play based simulation, which facilitates the acquisition of tacit knowledge in an organisation. The paper is organised as follows. First, we describe the kind of knowledge embodied in business processes. Second, options for knowledge acquisition in the context of business process management are presented. One promising approach to acquire process-aware knowledge is to use role-play based simulations. To demonstrate the effects of this type of simulation the case of KreditSim will be presented. Finally, we will draw conclusions on the usage of role-plays for acquiring tacit knowledge Process-aware knowledge Business processes reflect what a company is doing, i.e. how services are delivered and goods are produced. In this sense, processes can be understood as the DNA of a company – similar to that of human beings. They contain the information how the resources of a company are combined. The interaction of the employees’ activities, their knowledge and the IT applications used leads to a unique combination of resources (the DNA) that distinguishes a company from its competitors. Hence, two companies having the same resources and competing in the same market can perform differently due to better or worse processes based on the varying combinations of resources. Since business processes are the foundation for production, the management of the process-related knowledge is a key factor for the success of a company. If this knowledge is not captured, stored, shared and applied a company is likely to fail (Lucas, 2010; Paroutis/Al Saleh, 2009). But business processes are virtual constructs. The intangibility exacerbates the awareness of employees for a process-oriented view. Trying to identify the knowledge embedded in processes Nonaka (1994) differentiates between explicit and tacit knowledge. Explicit knowledge can be stored and is independent from a certain person. Tacit knowledge can not be stored and is strictly associated to individuals. Still, it is possible to transfer tacit into explicit knowledge to a certain degree. Three basic types of knowledge can be distinguished in the context of business processes (Hawryszkiewycz, 2010): The basic type of knowledge is explicit knowledge in terms of traditional process documentation. Within these documentations the intangible process is made explicit. It is shown how the resources necessary to produce a good or a service have to be combined. This includes information about necessary tasks, their order, responsible employees, IT systems involved et cetera. The way process knowledge is captured differs significantly among organisations. More or less sophisticated approaches range from Excel-based task lists to comprehensive documentation based on professional software using standardised notations like Business Process Modelling Notation (BPMN) or Event-driven Process Chains (EPCs) (Barber et al., 2003). Additional to process documentation, there exists explicit knowledge as a result of monitoring and analysing the process performance. This knowledge mainly addresses the management of a company based on key performance indicators of a certain business process. But this kind of knowledge can also be useful for employees working within a process. It is useful to improve the process but does not substitute the implicit knowledge which is necessary to perform the task itself. The major type of knowledge in processes is tacit knowledge of the employees performing particular tasks. Tacit knowledge is a combination of cognitive processes and physical facts determining how a person behaves to solve a problem (Hawryszkiewycz, 2010). For instance, one employee may perform better than another having the same working conditions and yet it is not possible to capture the reasons for this difference. In summary, tacit knowledge is a major source of knowledge in companies. Often, tacit knowledge vanishes as employees change their jobs or leave the company. In order not to lose this vital knowledge, an organisation may try to increase the percentage of explicit knowledge. Explicit knowledge in terms of process documentation for example is very helpful. Processes are made visible on paper or on screen and employees can better understand the meaning of their activities, tools and information systems within a process. But in real-world settings several problems occur (Nonaka, 1994): Explicit knowledge still remains abstract, as documentations are limited in delivering a true image of reality. Processes themselves are intangible leading to difficulties in understanding processes. The effort to keep explicit knowledge up to date is high. The world is changing fast and so are customers and as a result the company’s employees and processes, too. Tacit knowledge is very hard to learn from explicit sources of knowledge. It is more easily gained through experience and communication with others. Subsequently, explicit knowledge delivers valuable information but is not helpful to acquire the tacit knowledge which is necessary e.g. for process-oriented thinking. Regarding the gap between function-oriented and processoriented thinking, companies have to acquire this tacit knowledge to be successful. Therefore, it is important to support employees in acquiring, sharing and applying tacit knowledge. Acquisition of tacit knowledge In literature three generic options for the acquisition of tacit knowledge, such as process-aware knowledge, can be found: Socialising: Following this approach, events should be set-up, allowing employees to share tacit knowledge through joint activities (Nonaka, 1994). In this context Snowden (1998) proposes that tacit knowledge can be shared through psychosocial mechanisms and released through trust and its dynamics. Experiencing: Acting, e.g. performing tasks, is a vital part of this approach (Earl/Scott, 1999). Knowledge is gained through “learning by doing” (Levitt/March, 1988). According to Nonaka (1994), experiencing is connected to socialising – as experience has to be shared between people – but both are not necessarily intertwined. Using explicit knowledge: According to Nonaka (1991) tacit knowledge can be acquired based on explicit knowledge. The latter can be the result of an externalisation of tacit knowledge in publicly comprehensible forms like documentations (Nonaka, 1991; Snowden, 1998). Thinking about the most effective option, use of explicit knowledge is the weakest one. The amount of tacit knowledge which can be learnt by explicit knowledge is limited (Hawryszkiewycz, 2010). Learning by simply experiencing is useful, but also restricted as employees have to learn on their own. Additionally, experiencing on the job is combined with a high risk of failures due to a trial and error acquisition of knowledge (Levitt/March, 1988). This could be a problem, as an organisation’s success depends on efficient processing of tasks. While the employee is making mistakes customers will become upset or will ",,2010.0,,55371878,semantic_scholar
3a984a19f86877947561b4613b8b40b2efa01d26,https://www.semanticscholar.org/paper/3a984a19f86877947561b4613b8b40b2efa01d26,Understanding storage system problems and diagnosing them through log analysis,"Nowadays, over 90% new information produced are stored on hard disk drives. The explosion of data is making storage system a strategic investment priority in the enterprise world. The revenue created by storage system industry steadily increases from $14.2 Billion in 2004 to over $18.4 Billion in 2007. As a key component of enterprise systems, reliable storage systems are critical. However, despite the efforts put into building robust storage systems, as the size and complexity of storage systems have grown to an unprecedented level, storage system problems are common. Unfortunately, many aspects of storage system problems are still not well understood, and most of previous studies only focus on one component - disk drives. 
To better understand storage system problems, we analyzed the failure characteristics of the core part of storage system - the storage subsystem, which contains disks and all components providing connectivity and usage of disk to the entire storage system. More specifically, we analyzed the storage system logs collected from about 39,000 storage systems commercially deployed at various customer sites. The data set covers a period of 44 months and includes about 1,800,000 disks hosted in about 155,000 storage shelf enclosures. Our study reveals many interesting findings, providing useful guideline for designing reliable storage systems. Some of the major findings include: (1) In addition to disk failures that contribute to 20–55% of storage subsystem failures, other components such as physical interconnects and protocol stacks also account for significant percentages of storage subsystem failures. (2) Each individual storage subsystem failure type and storage subsystem failure as a whole exhibit strong self-correlations. In addition, these failures exhibit bursty patterns. (3) Storage subsystems configured with dual-path interconnects experience 30–40% lower failure rates than those with a single interconnect. (4) Spanning disks of a RAID group across multiple shelves provides a more resilient solution for storage subsystems than within a single shelf. 
As we found out that storage subsystem problems are far beyond disk failures, we extend the scope of study to various storage system problems, and study the characteristics of storage system problem troubleshooting from various dimensions. Using a large set (636,108) of real world customer problem cases reported from 100,000 commercially deployed storage systems in the last two years, the analysis show that while some problems are either benign, or resolved automatically, many others can take hours or days of manual diagnosis to fix. For modern storage systems, hardware failures and misconfigurations dominate customer cases, but software failures take longer time to resolve. Interestingly, a relatively significant percentage of cases are because customers lack sufficient knowledge about the system. We also evaluate the potential of using storage system logs to resolve these problems. Our analysis shows that a failure message alone is a poor indicator of root cause, and that combining failure messages with multiple log events can improve problem root cause prediction by a factor of three. 
One key finding is that storage system logs contain useful information for narrowing down the root cause, while they are challenging to analyze manually because they are noisy and the useful log events are often separated by hundreds of irrelevant log events. Motivated by this finding, we designed and implemented an automatic tool, called Log Analyzer, to improve problem troubleshooting process. By applying statistical analysis techniques, the Log Analyzer can automatically infer the dependency relationship between log events, and identify the key log events that capture the essential system states related to storage system problems. By combining classic unsupervised classification techniques - hierarchical clustering with the event ranking techniques, the Log Analyzer can also identify recurrent storage system problems based on similar log patterns, so that previous diagnosis efforts can be systematically retrieved and leveraged. We train the Log Analyze with 18,878 week-long storage system logs and evaluate it with 164 real-world problem cases. The evaluation indicates that the Log Analyzer can effectively reduce the log event number to 3.4%. For most of the 16 real-world problem cases manually annotated with 1–3 key log events, the Log Analyzer accurately ranked the key log events within top 3 without a priori knowledge on how important the events are. For the other 148 problem cases with diagnosis and with root cause information, the Log Analyzer effectively grouped problem cases with the same root cause together with 63–93% accuracy, significantly outperforming other three alternative solutions which only achieve 30–46% accuracy.",,2009.0,,61561632,semantic_scholar
67bf23a6754c9081f1d4e882175967bd03242474,https://www.semanticscholar.org/paper/67bf23a6754c9081f1d4e882175967bd03242474,Software Support for Ground Control Station for Unmanned Aerial Vehicle,"Uninhabited vehicles can be used in many applications and domains, particularly in environments that humans cannot enter (e.g. deep sea) or prefer not to enter (e.g. war zones). The promise of relatively low cost, highly reliable and effective assets that are not subject to the physical, psychological or training constraints of human pilots has led to much research effort across the world. Due to technological advances and increasing investment, interest in Unmanned Aerial Vehicles (UAVs) as a practical, deployable technological component in many civil applications is rapidly increasing and becoming a reality, as are their capabilities and availability. UAV platforms also offer a unique experimental environment for developing, integrating and experimenting with many other technologies such as automated planners, knowledge representation systems, chronicle recognition systems, etc. UAV performs various kinds of missions such as mobile tactical reconnaissance, surveillance, law enforcement, search and rescue, land management, environmental monitoring, disaster management. UAV is a complex and challenging system to develop. It operates autonomously in unknown and dynamically changing environment. This requires different types of subsystems to cooperate. In order to realize all functionalities of the UAV, the software part becomes very complex real-time system expected to execute real-time tasks concurrently. This paper describes proposed software architecture for GCS (Ground Control Station) for lightweight UAV purpose-built for medium-scale reconnaissance and surveillance missions in civil area. The overall system architecture and implementation are described.© 2009 ASME",,2009.0,10.1115/DETC2009-86456,110868958,semantic_scholar
336d84f6e7a7a398c70e924b4677c1fee7f3b81d,https://www.semanticscholar.org/paper/336d84f6e7a7a398c70e924b4677c1fee7f3b81d,The advantages of micro simulation in traffic modelling with reference to the N4 platinum toll road,"Micro simulation has been used to a limited extend in the past in South Africa, despite major advantages of this tool above static modelling and it’s popularity oversees. The main advantages are dynamic modelling and visual interpretation of the traffic conditions. This tool is ideal to test geometric designs, traffic controls and a variety of traffic management measures. These include incident and congestion management, road works, ramp metering, VMS, etc. It is an extremely suitable tool to use when low cost solutions must be found because of severely limited infrastructure resources. Times that micro simulation was not able to calculate and show reliable traffic situations is over, various traffic simulation models have developed and have reached high quality standards. Micro simulation is about to gain a real market share all around the world; South Africa is following. Modelling toll plazas at interchanges on the N4 Platinum Toll Road is used to illustrate the advantages of micro simulation. Geometric design options, measures effecting toll throughput and traffic control options were evaluated in this example as well as the estimation of the expected life span of various options within a congested network. The package used in this study is AIMSUN2, an advanced micro simulation package widely used internationally that can interact with TRANSYT, SCOOT, EMME/2 and SATURN. AIMSUN2 has been applied to traffic impact analysis, traffic control measures, HOV-lanes, tolling and geometric design within the last three years in South Africa. It has also been used successfully to convey results of investigations to nontechnical people. 1. MICRO SIMULATION 1.1 Background on the development of micro simulation The microscopic traffic simulation models are based on the reproduction of the traffic flows simulating the behavior of the individual vehicles, this not only enables them to capture the full dynamics of time dependent traffic phenomena, but also to deal with behavioral models accounting for drivers’ reactions. The underlying hypothesis is that the dynamics of a stream of traffic is the result of a series of drivers’ attempts to regulate their speed and acceleration accordingly with information received. The driver’s actions resulting from the interpretation of the information received will consist on the control of the acceleration (braking and accelerating), the control of heading (steering) and the decision of overtaking the precedent vehicle either to increase the speed or to position themselves in the right lane to perform a maneuver (i.e. a turning). The origins of microscopic traffic simulation can be traced back to the early stages of digital computers. Although the basic principles were set up many years ago, with the seminal work of, among others, Robert Hermann and the General Motors Group in the early fifties, the computing requirements made them impractical until hardware and software developments made them affordable even on today’s laptop computers. Most of the currently existing microscopic traffic simulators are based on the family of carfollowing, lane changing and gap acceptance models to model the vehicle’s behavior. Carfollowing models are a form of stimulus-response model, where the response is the reaction of the driver (follower) to the motion of the vehicle immediately preceding him (the leader) in the traffic stream. The response of the follower is to accelerate or decelerate in proportion to the magnitude of the stimulus at time t after a reaction time T. The generic form of the conceptual model is: response (t+T) = sensitivity * stimulus (t) Among the most used models are Helly’s model (1), implemented in SITRA-B+, (2), Herman’s model (3), or its improved version by Wicks (4), implemented in MITSIM, (5), the psycho-physical model of Wiedemann, (6), used in VISSIM (7), or the ad hoc version of Gipps (8), used in AIMSUN2 (9, 10). Other microscopic simulators such as INTEGRATION (11) and PARAMICS employ heuristic or other modeling not publicly available in analytic form. A common drawback of most of these models is that the model parameters are global i.e. constant for the entire network whereas it is well know that driver’s behavior is affected by traffic conditions. Therefore a more realistic car-following modeling for microscopic simulation should account for local behavior. This implies that some of the model parameters must be local depending on local geometric and traffic conditions. 1.2 What micro simulation is and the advantages thereof compared to static models The deployment of Intelligent Traffic Systems (ITS) requires support of complementary studies clearly showing the feasibility of the systems and what benefits should be expected from their operation. The large investments required have to be justified in a robust way. That means feasibility studies that validate the proposed systems, assess their expected impacts and provide the basis for sound cost benefit analyses. Microscopic traffic simulation has proven to be a useful tool to achieve these objectives. This is not only due to its ability to capture the full dynamics of time dependent traffic phenomena, but also being capable of dealing with behavioral models accounting for drivers’ reactions when exposed to ITS systems. The advent of ITS has created new objectives and requirements for micro-simulation models. Quoting from Deliverable D3 of the European Commission Project SMARTEST [12]: “The objective of micro-simulation models is essentially, from the model designers point of view, to quantify the benefits of Intelligent Transportation Systems (ITS), primarily Advanced Traveler Information Systems (ATIS) and Advanced Traffic Management Systems (ATMS). Micro-simulation is used for evaluation prior to or in parallel with on-street operation. This covers many objectives such as the study of dynamic traffic control, incident management schemes, real-time route guidance strategies, adaptive intersection signal controls, ramp and mainline metering, etc. Furthermore some models try to assess the impact and sensitivity of alternative design parameters”. The analysis of traffic systems and namely ITS systems, is beyond the capabilities of traditional static transport planning models. Microscopic simulation is then the suitable analysis tool to achieve the required objectives. An example from a real case study where microscopic simulation was used to complement static modeling will help us to understand better how both levels may help the decisionmaker. The city of San Sebastian, in the North of Spain, completed recently a new urban freeway connecting two separated neighborhoods. Figure 1 shows the typical result of the planning study with an close up of the Amara neighborhood. The road network and the demand were modeled using the EMME/2 package. The figure displays the expected impacts of the new infrastructure highlighting in green the average flow reduction due to the redistribution of flows enabled by the new paths on the network, and in red the increase of flows attracted by these paths. A significant discharge in the level of congestion in the main road network was the foreseen impact of the new infrastructure, but the access to the new freeway in the East-West direction shows some undesirable side effects in the neighborhood (Amara) inside the rectangle. The solutions to these problems demands a close up to the subnetwork and take decisions at the level of traffic control and traffic management schemes, not excluding even a partial reshaping of part of the street network. This type of decision requires a more detailed modeling, able of reproducing in a very accurate way the traffic conditions, accounting for the interactions between the vehicular flows and the infrastructure, and obviously including the influence of the traffic lights, objective that can only be achieved by a microscopic traffic simulation model. Figure 1: Expected impact of the new infrastructure in San Sebastian with an close up of the Amara neighborhood Figure 2 displays the corresponding model built with the AIMSUN2 traffic simulation software, the EMME/2 sub model has been built automatically from the AIMSUN2 model by means of an interface between both systems. Figure 2: AIMSUN2 micro simulation model of the Amara neighborhood The type of information that micro simulation can provide for a further analysis is beyond the capabilities of traditional static models. The average flows from sections to sections turning movements) for the allowed movements at selected intersections in the model, speeds and delays for every simulated time interval can be obtained. The dynamic analysis for a time period is completed with values for other traffic variables or indicators of the quality of service as number of stops, time delayed at stops, average queue lengths, etc. Figure 3 provide a further insight on the capacity of analysis provided by dynamic simulation software. The graphic in this figure describes the evolution over time of average flows. The same type of graph can be produced for average queue lengths on a subset of selected sections in the model. Figure 3: The evolution of average flows over time 1.3 The ease of model building and data input (AIMSUN2) The recent evolution of the microscopic simulators has taken advantages of the state-of-theart in the development of object-oriented simulators, and graphical user interfaces, as well as the new trends in software design and the available tools that support it adapted to traffic modeling requirements. A proper achievement of the basic requirements of a microscopic simulator implies building models as close to reality as possible. The closer the model is to reality the more data demanding it become. This has been traditionally the main barrier Section Volumes (Veh/h) 0 200 400 600 80",,2001.0,,108054289,semantic_scholar
3056e72ddb960a635a0121ee3f9f452155bdd70f,https://www.semanticscholar.org/paper/3056e72ddb960a635a0121ee3f9f452155bdd70f,SUPPORT FOR GROUND CONTROL STATION FOR UNMANNED AERIAL VEHICLE,"∗Postgraduate student and author of correspondence, Phone: (+381) 641907205, Email: mladjan@afrodita.rcub.bg.ac.rs ABSTRACT Uninhabited vehicles can be used in many applications and domains, particularly in environments that humans cannot enter (e.g. deep sea) or prefer not to enter (e.g. war zones). The promise of relatively low cost, highly reliable and effective assets that are not subject to the physical, psychological or training constraints of human pilots has led to much research effort across the world. Due to technological advances and increasing investment, interest in Unmanned Aerial Vehicles (UAVs) as a practical, deployable technological component in many civil applications is rapidly increasing and becoming a reality, as are their capabilities and availability. UAV platforms also offer a unique experimental environment for developing, integrating and experimenting with many other technologies such as automated planners, knowledge representation systems, chronicle recognition systems, etc. UAV performs various kinds of missions such as mobile tactical reconnaissance, surveillance, law enforcement, search and rescue, land management, environmental monitoring, disaster management. UAV is a complex and challenging system to develop. It operates autonomously in unknown and dynamically changing environment. This requires different types of subsystems to cooperate. In order to realize all functionalities of the UAV, the software part becomes very complex real-time system expected to execute real-time tasks concurrently. This paper describes proposed software architecture for GCS (Ground Control Station) for lightweight UAV purpose-built for medium-scale reconnaissance and surveillance missions in civil area. The overall system architecture and implementation are described.",,2009.0,,18478483,semantic_scholar
cdd8028ef1569a9c7191e806839ed2bb261efdb9,https://www.semanticscholar.org/paper/cdd8028ef1569a9c7191e806839ed2bb261efdb9,Title: An Integrated Design Environment to Evaluate Power/Performance Tradeoffs for Sensor Network Applications,"Networks of inexpensive, low-power sensing nodes that can monitor the environment, perform limited processing on the samples, and detect events of interest in a collaborative fashion are fast becoming a reality. Examples of such monitoring and detection include target tracking based on acoustic signatures and line-of-bearing estimation, climate control, intrusion detection, etc. The advances in low-power radio technology are making wireless communication within sensor networks an attractive option. However, it is typically difficult or impossible to replenish energy resources available to a portable sensor node, once it is deployed. Maximizing the life of sensor nodes is an overriding priority, and different energy optimization techniques are being developed to addresses computation/communication tradeoffs. A large number of research efforts are focusing on different aspects of the general problem of designing efficient sensor network-based systems where the metrics to measure efficiency vary from system to system. With technological advancements such as silicon-based radios expected to become a reality in a few years, designers of sensor network-based systems will be faced with an extremely large set of design decisions. Each choice will affect the overall system performance in ways that might not always be cleanly modeled. In addition to the research challenges in design and optimization, the practical aspects of designing real-world sensor networks will become equally important. For example, the ability of the design framework to allow rapid specification and evaluation of a particular network configuration is crucial for a more exhaustive exploration of the design space. A design environment for future sensor networks should provide tools and formal methodologies that will allow designers to model, analyze, optimize, and simulate such systems. In the context of our work, design and optimization of a sensor network application involves determining the task allocation to different sensor nodes and the inter-node communication mechanism. Design of the sensor node hardware itself is also an area of active research. However, we assume that a set of node architectures is already available to our enduser, and the design problem is restricted to using the available hardware (with flexibilities, if any, such as dynamic voltage scaling) to efficiently implement the target application. We take a simple, seven-node wireless sensor network for acoustic detection [5] (Automatic Target Recognition) as the case study and demonstrate (i) a modeling and simulation methodology for a class of sensor networks, and (ii) a software framework that implements our methodology. Our formal application model is illustrated in Fig. 1. We use a data flow graph representation to model the computing tasks and their data dependencies. The end-to-end application consists of two types of such data flow graphs: the first type denotes the processing that has to be performed for each sample before it is ready to be ‘fused’ with results from other sensors, and the second type represents the computing involved in data fusion. Specifically , in our case study, a Fast Fourier Transform (FFT) operation is the only task that is performed on each block of sampled data. The outputs of FFTs from all seven nodes are provided as an input to the collaborative computing part, which consists of delay and sum beamforming (BF), and lineof-bearing (LOB) estimation. The result of collaborative computation in such a cluster model of sensor networks has to be transmitted to some observer. This is accomplished by designating one of the nodes as the cluster-head, which could be equipped with more powerful communication facilities than other sensor nodes. All communication within our cluster is one-hop, and processing of a particular data sample (FFT/BF/LOB) occurs either on its home node, or the cluster-head, or partly on both. Simulating a completely specified instance of the above class of sensor networks involves many challenges. None of the existing network simulators to our knowledge models the internal architecture of the processing nodes in the network. This is because the focus of most network simulators is on protocol development and empirical analysis. Except in areas such as high-speed router design, the node internals have little or no impact on decisions related to protocol design. Also, processor simulators do not model the environment outside the chip boundary. Therefore, to obtain detailed and accurate performance estimates for the entire system, we propose a technique to automatically generate network scenarios based on results from low-level node simulations. The network simulator is configured using the generated scenarios, and the individual simulation results are merged and presented to the end-user as a whole. Such a ‘horizontal’ simulation is accomplished through the use of a central data repository for model information, which means that the simulators never have to directly interact with each other. The simulators we integrate provide estimates about energy consumption, thereby assisting in a power/performance analysis of a specific system configuration. Our design framework facilitates multi-granular simulation, i.e., simulating the same system configuration by using simulation models at different levels of abstraction. Typically, coarse-grained models provide rapid estimates, but need to make approximations about system behavior that might not be very accurate. For such a scenario, we demonstrate a form of analytical model refinement (see Fig. 2), i.e. the data from low-level simulations can be automatically processed to ‘distill’ parameter values used by high-level simulators. Naturally, the exact processing has to be specified by someone with knowledge of the analytical model semantics. Our design environment provides the following capabilities to the user: • To graphically describe the target application, node architecture, network configuration, and task-to-node mapping. • To change (reconfigure) the system model to explore alternate designs. Some of the parameters that can be manipulated by the designer include receive/transmit power of the radio, voltage/frequency setting of the processor, cluster geometry, propagation models, etc. • To automatically simulate a design using a coarse system model. • To automatically configure and execute low-level simulators for the node (Wattch [3]) and the network (ns-2 [4]) and obtain system-wide energy and latency estimates. • To automatically update high-level model parameters using low-level simulation statistics. • To graphically visualize simulation results and facilitate (manual) identification of power/performance bottlenecks in the design. This work is an illustration of the general approach of the MILAN [1] project. A modeling and simulation framework based on the first version of the 7-node ATR system model was implemented in [2]. The primary focus of that work was a prototype demonstration of simulator integration and model refinement. Therefore, the system model itself lacked generality. Also, we use a relatively more detailed version of the high-level estimator implemented for [2]. This work represents a significant step towards the ultimate goal of a design environment for automatic optimization and synthesis of sensor network applications.",,2002.0,,5339303,semantic_scholar
9566723d1b3075cd14cabc8519a24a7eaa00cd5a,https://www.semanticscholar.org/paper/9566723d1b3075cd14cabc8519a24a7eaa00cd5a,"Using Simulation Tools for Embedded Software Development Class # 410 , Embedded Systems Conference , Silicon Valley 2008","ion vs. Detail A key insight in building simulations is that you must always make a trade-off between simulator detail and the scope of the simulated system. Looking at some extreme cases, you cannot use the same level of abstraction when simulating the evolution of the universe on a grand scale as when simulating protein folding. You can always trade execution time for increased detail or scope, but assuming you want a result in a reasonable time scale, compromises are necessary. A corollary to the abstraction rule is that simulation is a workload that can always use maximum computer performance (unless it is limited by the speed of interaction from the world or users). A faster computer or less detailed model lets you scale up the size of the system simulated or reduce simulation run times. In general, if the processor in your computer is not loaded to 100%, you are not making optimal use of simulation. The high demands for computer power used to be a limiting factor for the use of simulation, requiring large, expensive, and rare supercomputers to be used. Today, however, even the cheapest PC has sufficient computation power to perform relevant simulations in reasonable time. Thus, the availability of computer equipment is not a problem anymore, and simulation should be a tool considered for deployment to every engineer in a development project. Simulating the Environment Simulation of the physical environment is often done for its own sake, without regard for the eventual use of the simulation model by embedded software developers. It is standard practice in mechanical and electrical engineering to design with computer aided tools and simulation. For example, control engineers developing control algorithms for physical systems such as engines or processing plants often build models of the controlled system in tools such as MatLab/Simulink and Labview. These models are then combined with a model of the controller under development, and control properties like stability and performance evaluated. From a software perspective, this is simulating the specification of the embedded software along with the controlled environment. For a space probe, the environment simulation could comprise a model of the planets, the sun, and the probe itself. This model can be used to evaluate proposed trajectories, since it is possible to work through missions of years in length in a very short time. In conjunction with embedded computer simulations, such a simulator would provide data on the attitude and distance to the sun, the amount of power being generated from solar panels, and the positions of stars seen by the navigation sensors. When the mechanical component of an embedded system is potentially dangerous or impractical to work with, you absolutely want to simulate the effects of the software before committing to physical hardware. For example, control software for heavy machinery or military vehicles are best tested in simulation. Also, the number of physical prototypes available is fairly limited in such circumstances, and not something every developer will have at their desk. Such models can be created using modeling tools, or written in C or C++ (which is quite popular in practice). In many cases, environment simulations can be simple data sequences captured from a real sensor or simply guessed by a developer. It should be noted that a simulated environment can be used for two different purposes. One is to provide “typical” data to the computer system simulation, trying to mimic the behavior of the final physical system under normal operating conditions. The other is to provide “extreme” data, corresponding to boundary cases in the system behavior, and “faulty” data corresponding to broken sensors or similar cases outside normal operating conditions. The ability to inject extreme and faulty cases is a key benefit from simulation. Simulating the Human User Interface The human interface portion of an embedded device is often also simulated during its development. For testing user interface ideas, rapid prototyping and simulation is very worthwhile and can be done in many different ways. One creative example is how the creator of the original Palm Pilot used a wooden block to simulate the effect of carrying the device. Instead of building complete implementations of the interface of a TV, mobile phone, or plant control computer, mockups are built in specialized user interface (UI) tools, in Visual Studio GUI builder on a PC, or even PowerPoint or Flash. Sometimes such simulations have complex behaviors implemented in various scripts or even simple prototype software stacks. Only when the UI design is stable do you commit to implementing it in real code for your real device, since this typically implies a greater programming effort. In later phases of development, when the hardware user interface and most of the software user interface is done, a computer simulation of a device needs to provide input and output facilities to make it possible to test software for the device without hardware. This kind of simulation runs the gamut from simple text consoles showing the output from a serial port to graphical simulations of user interface panels where the user can click on switches, turn knobs, and watch feedback on graphical dials and screens. A typical example is Nokia’s Series 60 development kit, which provides a virtual mobile phone with a keypad and small display. Another example is how virtual PC tools like VmWare and Parallels map the display, keyboard, and mouse of a PC to a target system. In consumer electronics, PC peripherals are often used to provide live test data approximating that of a real system. For example, a webcam is a good test data generator for a simulated mobile phone containing a camera. Even if the optics and sensors are different, it still provides something better than static predetermined images. Same for sound capture and playback – you want to hear the sound the machine is making, not just watch the waveform on a display. Simulating the Network Most embedded computers today are connected to one or more networks. These networks can be internal to a system; for example, in a rack-based system, VME, PCI, PCI Express, RapidIO, Ethernet, IC, serial lines, and ATM can be used to connect the boards. In cars, CAN, LIN, FlexRay, and MOST buses connect body electronics, telematics, and control systems. Aircraft control systems communicate over special bus systems like MIL-STD-1553, ARINC 429, and AFDX. Between the external interfaces of systems, Ethernet running internet standards like UDP and TCP is common. Mobile phones connect to headsets and PCs over Bluetooth, USB, and IR, and to cellular networks using UMTS, CDMA2000, GSM, and other standards. Telephone systems have traffic flowing using many different protocols and physical standards like SS7, SONET, SDH, and ATM. Smartcards connect to card readers using contacts or contact-less interfaces. Sensor nodes communicate over standard wireless networks or lower-power, lower-speed interfaces like Zigbee. Thus, existing in an internal or external network is a reality for most embedded systems. Due to the large scale of a typical network, the network part is almost universally simulated to some extent. You simply cannot test a phone switch or router inside its real deployment network, so you have to provide some kind of simulation for the external world. You don’t want to test mobile phone viruses in the live network for very practical reasons. Often, many other nodes on the network are being developed at the same time. Or you might just want to combine point simulations of several networked systems into a single simulated network. Network simulation can be applied at many levels of the networking stack. The picture below shows the most common levels at which network simulation is being performed. The two levels highlighted in green are the ones that are most useful for embedded software work on a concrete target model. Physical signalling Bit stream Packet transmission Network protocol Application protocol High-level application actions Analog signals, bit errors, radio modeling Clocked zeros and ones, CAN with contention, Ethernet with CSMA model Ethernet packets with MAC address, CAN packets, serial characters, VME data read/write TCP/IP etc. FTP, DHCP, SS7, CANopen Load software, configure node, restart Hardware/software boundary r r / ft r r The most detailed modeling level is the physical signal level. Here, the analog properties of the transmission medium and how signals pass through it is modeled. This makes it possible to simulate radio propagation, echoes, and signal degradation, or the electronic interference caused by signals on a CAN bus. It is quite rarely used in the setting of developing embedded systems software, since it complex and provides more details than strictly needed. Bit stream simulation looks at the ones and zeroes transmitted on a bus or other medium. It is possible to detect events like transmission collisions on Ethernet and the resulting back-off, priorities being clocked onto a CAN bus, and signal garbling due to simultaneous transmissions in radio networks. An open example of such a simulator is the VMNet simulator for sensor networks. Considering the abstraction levels for computer system simulation discussed below, this is at an abstraction level similar to cycle-accurate simulation. Another example is the simulation of the precise clock-by-clock communication between units inside a system-on-a-chip. Packet transmission passes entire packets around, where the definition of a packet depends on the network type. In Ethernet, packets can be up to 65kB large, while serial lines usually transmit single bytes in each “packet”. It is the network simulation equivalent of transaction-level modeling, as discussed below for computer boards. The network simulation has no knowledge of the meaning of the packets. It just passes opaqu",,2008.0,,11785333,semantic_scholar
df7fb406e31a6057e2ea8f4997c3b71895e44093,https://www.semanticscholar.org/paper/df7fb406e31a6057e2ea8f4997c3b71895e44093,Design and Implementation of a Microprocessor-Based Sequencer for a Small-Scale Groundnut Oil Production Plant,"A microprocessor-based Sequencer for a small-scale groundnut oil production plant was designed and a test model was implemented. The microprocessor-based Sequencer is meant to replace traditional, electromechanical sequencers, which are based on relays, contactors, limit switches and other similar devices. The INTEL 8085A microprocessor, combined with interface chips like the AD7575 ADC, the MAX378 multiplexer was used to implement the sequencer. Software was programmed into 2716 EPROM. Actuators and signal conditioning circuits were also designed and implemented. The implemented system was tested and the performance was found to be satisfactory. Introduction Background to the problem : According to Nwachuku[1] microprocessors are the state of the art in electronics digital systems’ design and the Nigerian Engineer, like his counterpart the world over, has no choice but to become interested in them. He contended further that, because of the nature of the microprocessor and its versatility, it becomes possible for the Nigerian engineer to device products to meet local needs using imported chips. This according to him, is the direction of technological development ..In the advanced industrialized and newly industrialized countries, the last couple of decades have seen the extensive application of microprocessors and computers to automate production processes. Specifically, the microprocessor acts as a micro-controller with a fixed program. Here, the microprocessor application system in most cases, involves the determination of values of physical parameters like temperature, pressure, and so on. In Nigeria, not much seems to have been achieved in this area. Some big manufacturing companies in both the public and private sectors of the Nigerian economy have had to change from along monitoring and control of plant operations to microprocessor/computer-based systems. Most of them are based on the programmable logic controllers (PLCs). But all these analog to digital (microprocessor /computer-based) implementations are mostly carried out by foreign firms and personnel, bringing along with them, their designed hardware and software. Even at that, not much is known to have been achieved in the area of deploying this latest-in-technology to improve the production processes of small-scale industries. This project was conceived as a result of a realization of this obvious short-coming. The nature of the problem: All over the world, countries have come to recognize the leading role which small-scale industries play in their economic development. They furnish over forty-percent (40%) of a nation’s output of goods and they also provide a substantial amount of total employment in an economy. A realization of this obvious fact has made the Federal Government of Nigeria to lay emphasis on the need to support Small and Medium Scale Enterprises in order that they act as catalyst for Nigeria’s industrial and economic growth. For example, the Federal Government of Nigeria has floated a Bank of Industries and has set up a Small and Medium Enterprises Development Agency of Nigeria (SMEDAN) with the objective of improving the performance of Small and Medium Enterprises (SMEs) towards achieving rapid industrialization of Nigeria and for the reversal of its over dependence on imports. Advanced Materials Research Online: 2007-06-15 ISSN: 1662-8985, Vols. 18-19, pp 107-110 doi:10.4028/www.scientific.net/AMR.18-19.107 © 2007 Trans Tech Publications Ltd, Switzerland All rights reserved. No part of contents of this paper may be reproduced or transmitted in any form or by any means without the written permission of Trans Tech Publications Ltd, www.scientific.net. (Semanticscholar.org-19/03/20,17:18:59) However, it must be realized that, the success of these initiatives and of Small and Medium Scale Industries (SMIs) themselves, would depend on indigenous locally developed production processes and technologies. Failures of this class of industries in the past have largely been attributed to their over dependence on imported production processes, technologies, and by extension, on imported plants, equipments and machines. The Raw Materials Research and Development Council (RMRDC) of Nigeria have identified the dearth of process equipment and machinery as the bane of Nigeria’s under-utilization of her agricultural and mineral raw materials. This now places a huge burden on the Nigerian engineer to now start to design, implement, and practicalize new production processes, machines, equipment and plants that can be deployed by the old and emerging SMIs. The Federal Government of Nigeria seems to have set the ball rolling by the establishment of a National Office for Technology Acquisition and Promotion (NOTAP). The literature is replete with works on interfacing microprocessors with real life situations which is exemplified by the following: Hosier[2] reviewed briefly, the issues that has to be addressed when interfacing a microprocessor to perform real world monitoring and control operations. Anazia [3] gave an overview of the status of microprocessor applications in industrial process control as it relates to the Guinness Plant in Benin-City, Nigeria. A PC-Based Data Acquisition and Supervisory Control system for a Small-Scale Industry was designed and implemented by [4]. Mansfield[5] described the use of transducers for industrial measurement purposes. Program and Data Stores design and Input/Output interfacing were well treated by Short[6]. Gregory[7] explained in good details signal conditioning circuits design. General description of the microprocessor based sequencer The groundnut-oil production plant operates as a series of logically controlled sequence of states; for example, agitator ON, crusher OFF and so on; the duration of each state being determined by sensor signals, for example, temperature or pressure; these sequence of states were transferred into software. The software part (application program), in consonance with the process-sequential flowchart (SFC) that describes the operation of the plant was written in assembler. A major part of the plant consists of electrical and mechanical components like motors which are controlled by electro-mechanical relays, these in turn are operated by signals from the microprocessor; therefore, OUTPUT INTERFACE circuits were designed. Other signals are fed to the microprocessor from temperature, pressure, and other sensors; INPUT INTERFACE circuits were therefore, also designed. The microprocessor system must have memory for storing the application program and for implementing intermediate computations and such other operations. INPUT transducers’ outputs are memory-mapped. The Y2 output of the 74ALS138 decoder that was used addresses a memory address range of 1000Hex to 17FFHex. Therefore, the six (6) input transducers reside at addresses 1000H, 1001H, 1002H, 1003H, 1004H and 1005H respectively. The analog-to-digital converter converts input voltages of between 0volts and 5volts to binary outputs of between 00000000 and 11111111, that is, between 00Hex to FFHex. The output ports (74LS373s) were assigned to a separate address space different from that occupied by main memory; hence, they were isolated or standard I/O. Therefore, the designed and implemented system is as shown in block diagrammatic form, in Fig. 1. Description of the Designed and Implemented System In Fig. 1, block 1 represents the transducers. Block 2 represents the signal conditioning circuits that condition the transducers’ signals to match the multiplexer’s-MAX 378(block 3) inputs. The combination of the multiplexer and the sample and hold-AD781 JN(block 4) selects and holds an input for the analog-to-digital converter-AD7575 JN (block 5) to convert for the input ports (block 6). Block 7 (INTEL 8085A) is the microprocessor subsystem. Block 8 is an address/data bus decoder (74ALS373), while block 9 is an address decoder (74ALS138) for the memory subsystem(blocks 10-4118 and 11-2716). Block 12 represents the output ports (74ALS373) and 108 Advances in Materials and Systems Technologies",,2007.0,10.4028/www.scientific.net/AMR.18-19.107,60470034,semantic_scholar
7dbe61b811bfc91e247dd4c949543468c394a1fd,https://www.semanticscholar.org/paper/7dbe61b811bfc91e247dd4c949543468c394a1fd,Integration of CORBA and WEB technologies in the VEGA DIS,"Distributed client/server architectures nowadays appear as a must for the wide information systems of the future virtual enterprise. At the same time, the continued growth of the Internet/WEB and its associated standard developments leads to new ways of world-wide information communication, distribution and access to information. This paper introduces to the various developments undertaken in the VEGA 1 project for a tighter integration of STEP, CORBA and WEB technologies within a DIS . Thus, the VEGA platform will allow both the support of distributed heterogeneous and interoperable client/server information systems (through CORBA) and the support of WEB based access to information and services through an Internet based navigation, building upon CGI and Java technologies. 1. Context and problematical issues The Large Scale Engineering (LSE) and Manufacturing universe is nowadays facing an increasing competitive environment where flexibility and adaptability to change are the bound paths to success, leading companies to renew their way of working. This is due to economical and technological drivers. Indeed, industrial enterprises are now devoted to the specification, design and construction of still larger and more complex manufacturing products: it has become no more possible to a single even-wide enterprise to take in charge the realisation of the whole products, both for financial reasons and because the required skills are not all within the enterprise. Thus, companies and SMEs ( Small and Medium Enterprises ) are now on the way of constituting virtual enterprises (VEs) for each new project. In a VE, contractors, partners, suppliers and customers form a temporary aggregation of non co-located actors dealing with the same product, but focusing on their core domain of competence for the shared profitability of industrial projects. At the same time, current progress in IT , providing more reliable and relevant mechanisms and software tools, the development of sophisticated new frameworks in client/server applications, and the continued growth of the Internet enable improved business processes and provide organisations with new business opportunities. Companies are now widely deploying their applications in Internet and Intranet 4 environments, assembling advanced IT based architectures encompassing among others networking, distributed information systems and concurrent engineering. In such a context, the VEGA project is developing a mandatory infrastructure for the support of the LSE business processes, particularly on the base of main standards for information modelling and exchange, with STEP (ISO 10303) or the IFC developed by the IAI , distribution and interoperability mechanisms with the OMG 6 CORBA and IIOP de facto standards, and WEB technology based on HTTP /CGI and Java. VEGA intends to cover the general needs of companies in VEs and Intranets, with a focus on the Building & Construction sector. Its main objective is to provide an information management architecture to guaranty interoperability among various software components running on different platforms, targeting STEP distributed technologies as an approach to bridge the gap between multiple and delocalised software systems in the construction industry. Within VEGA, the development of the DIS is a first approach towards an end-user oriented service for access to information through various forms. In the AEC field, large projects require the involvement of many body entities (client, architect, design engineers, various technical engineers, etc.) sitting at various locations, with different views and needs on the project, and managing different forms of documents like textual documents, structured documents, drawings, and so on. Dealing with all these kinds of information representations on the client side lead to consider, as far as possible, standardised front-end services. Access to information can be related to EDI 9 too. Until now, EDI has always been considered as a technology typically based on EDIFACT syntax and rules. But EDI can be considered as a kind of generic term, including all aspects of technical and commercial information exchange without mandatory requirements with respect to specific communication technology. Indeed, EDI can be considered from two different points of view: • the first one is a rather conceptual one: the EDI is a structured electronic exchange of data of any type between computer applications of parties involved in a transaction ; • the second is an operational one, where we consider means to realise the task as announced in the first point of view. With respect to these operational means, the Internet is nowadays more and more acknowledged as the common medium to support communication facilities within the development of client/server applications that deserve the larger audience. The WEB technology builds upon HTTP to provide the users with high-level graphical applications independent of the underlying client platform. Furthermore, the Java language now simplifies the development of WEB applications through the power and flexibility of a real object-oriented language. The Intranet perspective enfavours the use of WEB technologies on LANs 10 on a company scale, whether it be real or virtual. Therefore, a different approach has been applied in the VEGA DIS, considering EDI in the broad sense and consequently managing EDI messages through distributed networked infrastructures, emerging WEB standards (mainly HTML and VRML formats), and WEB technologies like CGI and Java. 2. Integration of new technologies in the VEGA project 2.1 Overview of the VEGA project As previously mentioned, the VEGA 11 project objective is to develop an IT platform enabling companies in a VE to work together. VEGA leans on available technology, and extend their capabilities as needed for engineering collaboration in a flexible distributed environment. To address the problem of information sharing, VEGA deals with 3 different technologies: • product-data modelling for the specification of meaningful project information; • middleware technology for the distribution of project information; • workflow management for the control of the flow of information and work in the VE. The VEGA platform relies on high level open standards for the three technologies listed above, including STEP and particularly EXPRESS for the neutral specification of product model data, CORBA for communication between distributed applications and distribution of objects over networks, workflow technology as defined by the WfMC 12 for design of process control, and information standards like SGML 13 or various WEB de facto standards for the support of valueadded distributed information services (exchange of administrative messages, document handling, presentation of information). Thus, VEGA is currently elaborating the fundamental grounds for distributed architectures, defining a service layered on top of CORBA (the COAST – COrba Access to STEP models [2]), for remote data access and manipulation of information models defined by explicit meta-models (or schemata) satisfying the STEP EXPRESS semantics. 2.2 STEP and the IAI If different software applications need to communicate and inter-operate, they first need to share the same information, without misunderstanding or loss of semantics. This implies a common way to represent and exchange the semantics. Such issues have led to dramatic research efforts to achieve effective product data exchanges, standardisation of methodologies, languages and technologies, especially in the context of PDT , among them: • STEP ([3], [4]), developed in ISO TC184/SC4 16 for the product data representation and exchange. It allows the expression in a uniform and complete way of all the information required for a product life-cycle (especially through the EXPRESS language [5]), and supplies means for exchanging data physical files [6] and sharing product databases [7] with models and applications independent mechanisms. STEP is today deeply used for real world product information modelling, communication and interpretation. • IFC [8], another major effort currently undertaken by a non-profit alliance (IAI) of the building industry including architects and engineers, building clients, software vendors, and so on. The main IAI objective is to specify the IFC as a universal model to be a basis for collaborative work in the building industry and consequently to improve communication, productivity, delivery time, cost, and quality throughout the design, construction, operation and maintenance life cycle of buildings. STEP and IAI share the same goals, i.e. application interoperability; data exchange and actor cooperation, but differ in their respective processes. The IAI promote a bottom-up approach, with an iterative and incremental development for fast implementation. STEP is a long-term project, with a top-down approach and is concerned with broad standardisation. The IAI, having a formal liaison status with STEP has partially adopted the STEP technology, mainly through an EXPRESS representation of the IFC. In the future, an integration of the IFC within STEP is planned. As an initiative driven by leading companies, the IAI is pushing the IFC as a de facto standard in the Building industry in a very near future. A key point of the VEGA infrastructure is to use PDT and especially EXPRESS to describe and store meaningful product information. Implementing the VEGA vision requires a tight coupling between STEP models and all the various components of the VEGA platform (including distribution layer, workflow, data storage). Eventually, the current IFC 1.5 release will be used in the final VEGA demonstration . 2.3 The CORBA standard CORBA ([9], [10], [11]) is an OMG specification for application interoperability and portability in distributed architectures, allowing objects described in any language to be shared across heterogeneous operating system",,1999.0,,10152884,semantic_scholar
79dd4e21811c1399a4525d82e16c8fbf23db3d51,https://www.semanticscholar.org/paper/79dd4e21811c1399a4525d82e16c8fbf23db3d51,Human-Computer Interaction,"Contents Foreword Preface to the third edition Preface to the second edition Preface to the first edition Introduction Part 1 Foundations Chapter 1 The human 1.1 Introduction 1.2 Input-output channels Design Focus: Getting noticed Design Focus: Where's the middle? 1.3 Human memory Design Focus: Cashing in Design Focus: 7 +- 2 revisited 1.4 Thinking: reasoning and problem solving Design Focus: Human error and false memories 1.5 Emotion 1.6 Individual differences 1.7 Psychology and the design of interactive systems 1.8 Summary Exercises Recommended reading Chapter 2 The computer 2.1 Introduction Design Focus: Numeric keypads 2.2 Text entry devices 2.3 Positioning, pointing and drawing 2.4 Display devices Design Focus: Hermes: a situated display 2.5 Devices for virtual reality and 3D interaction 2.6 Physical controls, sensors and special devices Design Focus: Feeling the road Design Focus: Smart-Its - making sensors easy 2.7 Paper: printing and scanning Design Focus: Readability of text 2.8 Memory 2.9 Processing and networks Design Focus: The myth of the infinitely fast machine 2.10 Summary Exercises Recommended reading Chapter 3 The interaction 3.1 Introduction 3.2 Models of interaction Design Focus: Video recorder 3.3 Frameworks and HCI 3.4 Ergonomics Design Focus: Industrial interfaces 3.5 Interaction styles Design Focus: Navigation in 3D and 2D 3.6 Elements of the WIMP interface Design Focus: Learning toolbars 3.7 Interactivity 3.8 The context of the interaction Design Focus: Half the picture? 3.9 Experience, engagement and fun 3.10 Summary Exercises Recommended reading Chapter 4 Paradigms 4.1 Introduction 4.2 Paradigms for interaction 4.3 Summary Exercises Recommended reading Part 2 Design process Chapter 5 Interaction design basics 5.1 Introduction 5.2 What is design? 5.3 The process of design 5.4 User focus Design Focus: Cultural probes 5.5 Scenarios 5.6 Navigation design Design Focus: Beware the big button trap Design Focus: Modes 5.7 Screen design and layout Design Focus: Alignment and layout matter Design Focus: Checking screen colors 5.8 Iteration and prototyping 5.9 Summary Exercises Recommended reading Chapter 6 HCI in the software process 6.1 Introduction 6.2 The software life cycle 6.3 Usability engineering 6.4 Iterative design and prototyping Design Focus: Prototyping in practice 6.5 Design rationale 6.6 Summary Exercises Recommended reading Chapter 7 Design rules 7.1 Introduction 7.2 Principles to support usability 7.3 Standards 7.4 Guidelines 7.5 Golden rules and heuristics 7.6 HCI patterns 7.7 Summary Exercises Recommended reading Chapter 8 Implementation support 8.1 Introduction 8.2 Elements of windowing systems 8.3 Programming the application Design Focus: Going with the grain 8.4 Using toolkits Design Focus: Java and AWT 8.5 User interface management systems 8.6 Summary Exercises Recommended reading Chapter 9 Evaluation techniques 9.1 What is evaluation? 9.2 Goals of evaluation 9.3 Evaluation through expert analysis 9.4 Evaluation through user participation 9.5 Choosing an evaluation method 9.6 Summary Exercises Recommended reading Chapter 10 Universal design 10.1 Introduction 10.2 Universal design principles 10.3 Multi-modal interaction Design Focus: Designing websites for screen readers Design Focus: Choosing the right kind of speech Design Focus: Apple Newton 10.4 Designing for diversity Design Focus: Mathematics for the blind 10.5 Summary Exercises Recommended reading Chapter 11 User support 11.1 Introduction 11.2 Requirements of user support 11.3 Approaches to user support 11.4 Adaptive help systems Design Focus: It's good to talk - help from real people 11.5 Designing user support systems 11.6 Summary Exercises Recommended reading Part 3 Models and theories Chapter 12 Cognitive models 12.1 Introduction 12.2 Goal and task hierarchies Design Focus: GOMS saves money 12.3 Linguistic models 12.4 The challenge of display-based systems 12.5 Physical and device models 12.6 Cognitive architectures 12.7 Summary Exercises Recommended reading Chapter 13 Socio-organizational issues and stakeholder requirements 13.1 Introduction 13.2 Organizational issues Design Focus: Implementing workflow in Lotus Notes 13.3 Capturing requirements Design Focus: Tomorrow's hospital - using participatory design 13.4 Summary Exercises Recommended reading Chapter 14 Communication and collaboration models 14.1 Introduction 14.2 Face-to-face communication Design Focus: Looking real - Avatar Conference 14.3 Conversation 14.4 Text-based communication 14.5 Group working 14.6 Summary Exercises Recommended reading Chapter 15 Task analysis 15.1 Introduction 15.2 Differences between task analysis and other techniques 15.3 Task decomposition 15.4 Knowledge-based analysis 15.5 Entity-relationship-based techniques 15.6 Sources of information and data collection 15.7 Uses of task analysis 15.8 Summary Exercises Recommended reading Chapter 16 Dialog notations and design 16.1 What is dialog? 16.2 Dialog design notations 16.3 Diagrammatic notations Design Focus: Using STNs in prototyping Design Focus: Digital watch - documentation and analysis 16.4 Textual dialog notations 16.5 Dialog semantics 16.6 Dialog analysis and design 16.7 Summary Exercises Recommended reading Chapter 17 Models of the system 17.1 Introduction 17.2 Standard formalisms 17.3 Interaction models 17.4 Continuous behavior 17.5 Summary Exercises Recommended reading Chapter 18 Modeling rich interaction 18.1 Introduction 18.2 Status-event analysis 18.3 Rich contexts 18.4 Low intention and sensor-based interaction Design Focus: Designing a car courtesy light 18.5 Summary Exercises Recommended reading Part 4 Outside the box Chapter 19 Groupware 19.1 Introduction 19.2 Groupware systems 19.3 Computer-mediated communication Design Focus: SMS in action 19.4 Meeting and decision support systems 19.5 Shared applications and artifacts 19.6 Frameworks for groupware Design Focus: TOWER - workspace awareness Exercises Recommended reading Chapter 20 Ubiquitous computing and augmented realities 20.1 Introduction 20.2 Ubiquitous computing applications research Design Focus: Ambient Wood - augmenting the physical Design Focus: Classroom 2000/eClass - deploying and evaluating ubicomp 20.3 Virtual and augmented reality Design Focus: Shared experience Design Focus: Applications of augmented reality 20.4 Information and data visualization Design Focus: Getting the size right 20.5 Summary Exercises Recommended reading Chapter 21 Hypertext, multimedia and the world wide web 21.1 Introduction 21.2 Understanding hypertext 21.3 Finding things 21.4 Web technology and issues 21.5 Static web content 21.6 Dynamic web content 21.7 Summary Exercises Recommended reading References Index",Encyclopedia of Database Systems,1993.0,10.1007/978-1-4614-8265-9_192,61154755,semantic_scholar
469427dae41a91263631a3105a859b210b5d6912,https://www.semanticscholar.org/paper/469427dae41a91263631a3105a859b210b5d6912,BTMMU: an efficient and versatile cross-ISA memory virtualization,"Full system dynamic binary translation (DBT) has many important applications, but it is typically much slower than the native host. One major overhead in full system DBT comes from cross-ISA memory virtualization, where multi-level memory address translation is needed to map guest virtual address into host physical address. Like the SoftMMU used in the popular open-source emulator QEMU, software-based memory virtualization solutions are not efficient. Meanwhile, mature techniques for same-ISA virtualization such as shadow page table or second level address translation are not directly applicable due to cross-ISA difficulties. Some previous studies achieved significant speedup by utilizing existing hardware (TLB or virtualization hardware) of the host. However, since the hardware is not designed with cross-ISA in mind, those solutions had some limitations that were hard to overcome. Most of them only supported guests with smaller virtual address space than the host. Some supported only guests with the same page size. And some did not support privileged memory accesses. This paper proposes a new solution named BTMMU (Binary Translation Memory Management Unit). BTMMU composes of a low-cost hardware extension of host MMU, a kernel module and a patched QEMU version. BTMMU is able to solve most known limitations of previous hardware-assisted solutions and thus versatile enough for real deployments. Meanwhile, BTMMU achieves high efficiency by directly accessing guest address space, implementing shadow page table in kernel module, utilizing dedicated entrance for guest-related MMU exceptions and various software optimizations. Evaluations on SPEC CINT2006 benchmark suite and some real-world applications show that BTMMU achieves 1.40x and 1.36x speedup on IA32-to-MIPS64 and X86_64-to-MIPS64 configurations respectively when comparing with the base QEMU version. The result is compared to a representative previous work and shows its advantage.",VEE,2021.0,10.1145/3453933.3454015,233174939,semantic_scholar
1890380a9e14e0a82dc105c9b8ce251107af9ddf,https://www.semanticscholar.org/paper/1890380a9e14e0a82dc105c9b8ce251107af9ddf,2nd CfP: 7th IEEE International Conference on Self-Adaptive and Self-Organizing Systems (SASO2013),"submission: May 3, 2013 Paper submission: May 10, 2013 Notification: June 21, 2013 Camera ready copy due: July 19, 2013 Early registration: August 21, 2013 Conference: September 9-13, 2013 ------------------------------Topics of Interest ------------------------------The topics of interest to SASO include, but are not limited to: Self-* systems theory: theoretical frameworks and models; biologicallyand socially-inspired paradigms; inter-operation of self-* mechanisms; Self-* systems engineering: hardware, software and middleware development frameworks and methods, platforms and toolkits; self-* materials; Self-* system properties: robustness, resilience and stability; emergence; computational awareness and self-awareness; reflection; Self-* cyber-physical and socio-technical systems: human factors and visualization; self-* social computers; crowdsourcing and collective awareness; Applications and experiences of self-* systems: cyber security, transportation, computational sustainability, big data and creative commons, power systems. --------------------------------------Submission Instructions --------------------------------------All submissions should be 10 pages and formatted according to the IEEE Computer Society Press proceedings style guide and submitted electronically in PDF format. Please register as authors and submit your papers using the SASO 2013 conference management system. The proceedings will be published by IEEE Computer Society Press, and made available as a part of the IEEE digital library. Note that a separate call for poster submissions has also been issued. ---------------------------Review Criteria ---------------------------Papers should present novel ideas in the cross-disciplinary research context described in this call, clearly motivated by problems from current practice or applied research. We expect both theoretical and empirical contributions to be clearly stated, substantiated by formal analysis, animation or simulation, experimental evaluations, comparative studies, and so on. Appropriate reference must be made to related work. Because SASO is a cross-disciplinary conference, papers must be intelligible and relevant to researchers who are not members of the same specialized sub-field. Authors are also encouraged to submit papers describing applications. Application papers are expected to provide an indication of the real world relevance of the problem that is solved, including a description of the deployment domain, and some form of evaluation of performance, usability, or comparison to alternative approaches. Experience papers are also welcome but they must clearly state the insight into any aspect of design, implementation or management of self-* systems which is of benefit to practitioners and the SASO community. ---------------------------Program Chairs ---------------------------Tom Holvoet KU Leuven, Belgium Jeremy Pitt Imperial College London, England Ichiro Satoh National Institute of Informatics, Tokyo, Japan ? CFP RSP-2013 at ESWeek CfP: Workshop on Model-Driven and Agile Engineering for the Web (MDWE) @ ICWE 2013 ? Calls for Papers CPS Domains Architectures Secure Control Systems Multi-models Communication Embedded Software Model Integration Platforms Systems Engineering Modeling Science of Security Transportation CPS Technologies Announcement",,2020.0,,218913932,semantic_scholar
18e1739f511ab6ca4eba3c836a3ba34a5deb1972,https://www.semanticscholar.org/paper/18e1739f511ab6ca4eba3c836a3ba34a5deb1972,A Reproducible Solution for Implementing Online Laboratory Systems Through Inexpensive and Open-source Technology,"Laboratory experiences are a crucial part of the undergraduate engineering curriculum. With coursework, college programs, and professional interactions increasingly being performed online the natural evolution of a ‘digital-first’ culture suggests that traditionally hands-on educational activities should find themselves represented online as well. Transitioning laboratory-based exercises online is difficult, time consuming, and sometimes costly. In addition, the efficacy of an online laboratory experience as a worthwhile educational tool has not been explored with depth. This study focuses on the details and benefits of incorporating laboratory experiences with online infrastructure with the perspective of optimizing development time and cost. The purpose is to use FOSS (free and open source software) in addition to other open source solutions to develop modular, scalable, and easily deployable remote laboratory infrastructure capable of interacting with traditional equipment over network connections. Introduction It is commonly accepted that one of the best ways to learn technical skills is through hands-on experiences. Be it through apprenticeships, internships, laboratories, or bootcamps, an interactive experience provides concrete, engaging, and fulfilling learning opportunities. By spending time personally carrying out a task, the brain forms neural connections which make it easier to remember and duplicate the task. The understanding of cognition and epistemology has grown throughout the entirety of the history of the human race. Masters pass down skills by having pupils perform those skills according to their instruction. However, with the rise of the digital age, the question becomes, can the dissemination of all concrete knowledge be conducted via computers just as well as through physical interactions. And if so, then how? The Impact of Remote Laboratory Systems on Education The digital world has become an integral part of the lives of faculty and their students and is now irrevocably intertwined with daily routines. As such, society grows ever more comfortable interacting with the world through a digital medium and seeks to find new avenues to do so and new virtual environments to explore. Therefore, it naturally follows that transitioning the whole of education towards a system which is more frequently used by digital natives may be in the best interest of future generations. The purpose of this study is to create a case for implementing remote, on-line laboratory experiences that can successfully fill the same intellectual need as their physical counterparts. The benefit of achieving this goal is similar to that of all on-line instruction, to reach more students and to make education accessible. The chief drawback is that creating the network infrastructure necessary to implement on-line experiences as a substitute for physical laboratory work is difficult and costly. This study also seeks to find and build open-source solutions to this problem using inexpensive hardware, open-source software, and simple network configurations that may add to the list of best practices built by previous and current researchers. Impact on Students Remote laboratory systems offer unique benefits to how students retain information. By providing students with a more open platform to access knowledge, rather than traditional physical interactions, it is possible to see positive effects on engagement and learning. Nabil Lehlou et al. (2009) conducted a study in which students in two different fields (Industrial Statistics and Manufacturing Systems) performed lab exercises and recorded how the students felt they understood the material before and after the lab. The results provided a clear indicator that the students felt the remote lab system provided a beneficial educational experience as six out of eight in Industrial Statistics and eight out of eight students in Manufacturing Systems reported an increase in confidence in the subject material. In addition, five out of eight students in Industrial Statistics and eight out of eight students in Manufacturing Systems reported a drastic improvement in their confidence for their respective fields. A separate study performed by H. Vargas et al. (2010) found similar positive results. They provided 120 students across seven universities with remote laboratory experiences. The research indicated that the full lab experience included performing an actual lab over the internet, which required students to reserve a time to use the lab resources. The response by the students indicated that they enjoyed the system as well as found it useful in understanding the respective course content. According to the results, 69% of the students felt satisfied with the system and 19% felt strongly satisfied. Additionally, 51% of students felt that the remote lab was better than traditional methods, 25% felt it was equal to traditional methods, and 15% felt it was much better than traditional methods. These results strongly assert that remote laboratory experiences not only have a place in the future of education but can have a large impact on its quality. Key Features Needed To better understand what makes remote lab systems effective and their impact on students potent, it is critical to understand what key features are common among these systems. In a study performed by P. Bisták et al. (2011) at the Slovak University of Technology in Bratislava, Slovakia, it was outlined that a remote lab system server could provide the client (user) with text messages, numerical data, graphs, animations, and video clips. The system could interface with sensors and cameras in order to collect useful information and statistics for the client. The setup involved a front-end GUI being served to a client, which in turn communicated over TCP/IP to a remote server. Information could be transmitted in either direction between the server and the client with data and commands running back and forth. The server would have access to the local hardware of the lab system and be able to send any commands received from the client to the hardware. Additionally, it would be able to collect output data from the lab hardware and send it back to the client. Another remote lab study was performed by T.J. Mateo Sanguino et al. (2012) at the University of Huelva in Palos de la Frontera, Spain. In this study a similar setup was implemented with a client providing user access to a remote server, which was in turn connected to a lab system. The user would have the ability to control computer devices on a rack through this setup and perform multiple remote labs. An interesting point to make which differentiates this study from the above is that it does not send photos back to the user. The labs performed did not require cameras or video, instead relying on numerical data to provide the pertinent observation. This is an important point to make as it shows that every lab system is different and there might not be a “generic” or “one-size-fits-all” approach. If this is the case, then a truly reproducible lab system must provide means by which different hardware or software peripherals may be added or removed depending on the needed application. However, at a minimum it appears that a remote laboratory needs a client-server system and some basic means by which to send text or commands between the client and server. Making Labs More Personal As humans are social and emotional creatures, it could be argued that experiences which leverage those traits would aid in the retention of information. It could also help explain why recent concepts such as social media have become fast staples in cultures around the world. They simply exploit the natural desires of people. Similarly, despite being called “remote”, it might be possible to use remote lab systems to improve learning through social, emotional and personal growth. A study performed by C. Terkowsky et al. (2013) at TU Dortmund University in Dortmund, Germany focused on the personalization of the remote laboratory experience. They referenced a theory on education and learning called “Kolb’s Experiential Learning Cycle” wherein multiple stages of learning are introduced. These stages are Concrete Experience, Reflective Observation, Abstract Conceptualization and Active Experimentation. According to the theory, they create the “learning experience”. Armed with this information, the study introduces the concept of an E-Portfolio. This E-Portfolio provides users of remote labs with the ability to record the work they performed and document their findings. The concept of this portfolio does not stop at being a simple digital notebook, however. The study asserts that this portfolio can be used by professors to check on students’ work or be opened to the public in order to add a social dynamic. The study calls the social aspect a “community” and says that it can facilitate learning. To reinforce the main point, by adding a social aspect, be it with classmates or with the world, users will have a greater feeling of connection with their work and might retain more information. Another study performed by Z. Nedic (2013) at the University of South Australia shines light on the collaborative aspect of remote labs. The study saw international students organize themselves autonomously to complete group lab assignments and recorded their planning and communication. The results showed that students, despite being from different countries, exhibited politeness when trying to create social groups and complete the remote labs. The study gives hope to the notion of creating a more connected educational system where students from around the world participate in the same curriculum. This in turn also facilitates international cooperation and communication in the real world, as a large amount of professional communication is done remotely. One study performed by Qing Ding et al. (2017) as joint research between C",,2020.0,10.18260/1-2--34043,230678993,semantic_scholar
22d3f48350aedf87a48daafad96ea8d92c9099f3,https://www.semanticscholar.org/paper/22d3f48350aedf87a48daafad96ea8d92c9099f3,Distributed IoT Attestation via Blockchain (Extended Version),"The growing number and nature of Internet of Things (IoT) devices makes these resource-constrained appliances particularly vulnerable and increasingly impactful in their exploitation. Current estimates for the number of connected “things” commonly reach the tens of billions. The low-cost and limited computational strength of these devices can preclude security features. Additionally, economic forces and a lack of industry expertise in security often contribute to a rush to market with minimal consideration for security implications. It is essential that users of these emerging technologies, from consumers to IT professionals, be able to establish and retain trust in the multitude of diverse and pervasive compute devices that are ever more responsible for our critical infrastructure and personal information. Remote attestation is a well-known technique for building such trust between devices. In standard implementations, a potentially untrustworthy prover attests, using public key infrastructure, to a verifier about its configuration or properties of its current state. Attestation is often performed on an ad hoc basis with little concern for historicity. However, controls and sensors manufactured for the Industrial IoT (IIoT) may be expected to operate for decades. Even in the consumer market, so-called “smart” things can be expected to outlive their manufacturers. This longevity combined with limited software or firmware patching creates an ideal environment for long-lived zero-day vulnerabilities. Knowing both if a device is vulnerable and if so when it became vulnerable is a management nightmare as IoT deployments scale. For network connected machines, with access to sensitive information and real-world physical controls, maintaining some sense of a device’s lifecycle would be insightful. In this paper, we propose a novel attestation architecture, DAN: a distributed attestation network, utilizing blockchain to store and share device information. We present the design of this new attestation architecture, and describe a virtualized simulation, as well as a prototype system chosen to emulate an IoT deployment with a network of Raspberry Pi, Infineon TPMs, and a Hyperledger Fabric blockchain. We discuss the implications and potential challenges of such a network for various applications such as identity management, intrusion detection, forensic audits, and regulatory certification.",,2020.0,,214593683,semantic_scholar
fb70b32eaae4987e7230d03ec82858a8163d66c3,https://www.semanticscholar.org/paper/fb70b32eaae4987e7230d03ec82858a8163d66c3,Industrial Control System Security ( ICSS ) Workshop,"One of the hardest decisions an asset owner must make when faced with known vulnerabilities or exploits is whether to take down their plant in order to apply patches and upgrade end of life process control components. There are risks if you do (productions loss, opportunity costs, failed upgrades) and (cyber)risks if you do not. In this presentation we will discuss several options that could be considered when presented with known cyber-risks. Note: On the surface this may feel like a standard defense in depth strategy, and in some respects it is. But these strategies are meant to address specific attack techniques, known vulnerabilities and exploits, so it is better to think of these techniques as reactive rather than the defense in depth, proactive approach. Runtime Application Self Protection (RASP) is an emerging collection of approaches to address the fundamental issue with cyber-exploits, that is the ability for malicious processes to access memory where they should not be able. If you control memory access, you control an entire class of exploits (memorybased attacks) Patching – Our most traditional approach to defend against exploits in the wild Signatures – Antivirus is the most common signature-based solution. YARA rules are a way of identifying malware (or other files) by creating rules that look for certain characteristics. There are several signaturebased solutions that can be shared to slow or stop the exploitation of certain vulnerability types. Mitigations – frequently OEMs advise their customers to take specific actions in order to close off known attack vectors. Closing ports on a firewall, disabling unused services, implementing access controls, network segmentation, implementing secure protocols, etc. are all common recommendations to react to specific vulnerabilities. Security Tools – OEMS such as Schneider Electric take time to partner with security tool vendors who often bring their own unique approach to addressing active exploits Network Anomaly detection – similar to signature checking, the ability to identify an exploit on the wire before it reaches the device. Good examples are, “magic” packets that can cause crashes, buffer overflows, RCE, etc. AI/ML – an emerging technology, maligned somewhat today, but do not underestimate how this can and will be used in the future Upgrades – whether this is a component by component upgrade or a rip and replace, one way to eliminate legacy cybersecurity issues is to upgrade to the current generation Biography: Andrew Kling is an Industry Automation Product Security Officer at Schneider Electric. Andy has over three decades of software development experience, having worked in multiple industries. He has worked in the R&D organization of Schneider Electric since 2001. Currently, Andy leads the Industry Automation business unit in cybersecurity. At Schneider Electric, Andy has managed many process control engineering teams. As a result of this experience, Andy has ushered the Schneider Electric Development organization to comply with ISA 62443 standards at the process, product, and system levels, achieving several world firsts along the way. Andy has a Master’s Degree in Software Engineering – Artificial Intelligence from Northeastern University and a Bachelor’s of Science in Information Sciences from the University of Massachusetts, Lowell. In addition, Andy is a participating Senior member of ISA, primarily contributing to the ISA 62443 cybersecurity standards and the ISA Global Cybersecurity Alliance. 9:15 K7: A Protected Protocol for Industrial Control Systems that Fits Large Organizations, Eli Biham, Sara Bitan, and Alon Dankner, Israel Technion ABSTRACT: One of the main obstacles of securing industrial control systems is the lack of an appropriate security model that is both implementable by vendors and addresses the inherent security and usability issues needed by organizations. Current solutions such as device passwords and IPSec lack scalable key management infrastructure and [inc granularity access control mechanisms. In this paper we propose a novel security model for industrial control systems that supports organizational level authorizations and authentication requirements, while hiding the low—level details (e.g., keys and passwords) from the One of the main obstacles of securing industrial control systems is the lack of an appropriate security model that is both implementable by vendors and addresses the inherent security and usability issues needed by organizations. Current solutions such as device passwords and IPSec lack scalable key management infrastructure and [inc granularity access control mechanisms. In this paper we propose a novel security model for industrial control systems that supports organizational level authorizations and authentication requirements, while hiding the low—level details (e.g., keys and passwords) from the users. It also allows to easily add and remove l’I.Cs, engineering stations, HMI devices and users, and assign permissions to them. A major advantage is its support for hybrid ICS systems, and the simple ability to upgrade the security of legacy devices to functionality of new secure protocol. Without loss of generality, we base our protocol, K7, on the Siemens S7 protocol, and enhance it with new cryptographic features to support the extra functionality. We use a ticket-based system (e.g., Kerberos with LDAP server) to support the exchange of permissions and keys, and incorporate it into our protocol. To prove our solution, we implemented K7 as a protocol converter add-on to standard Siemens clients and PLCs that transform them into augmented devices that use K7. We hope that Siemens and other vendors will add direct support for K7 on their ICS systems. 9:40 What and Where to Monitor for Intrusion Detection in Industrial Control Networks, Alvaro Cardenas, University California Santa Cruz ABSTRACT: In this presentation we will look at two related problems for intrusion detection in control systems: where to monitor the system to detect anomalies, and what to monitor, in order to obtain an accurate picture of the real world. We first discuss what we can and cannot detect depending on the location of our network monitor, and identify locations that maximize our visibility to attacks. We also propose the addition of hidden sensor measurements to a system to improve its security. Hidden sensor measurements are by our definition measurements that were not considered in the original design of the system, and are not used for any operational reason. We only add them to improve the security of the system and using them in anomaly detection and mitigation. In this presentation we will look at two related problems for intrusion detection in control systems: where to monitor the system to detect anomalies, and what to monitor, in order to obtain an accurate picture of the real world. We first discuss what we can and cannot detect depending on the location of our network monitor, and identify locations that maximize our visibility to attacks. We also propose the addition of hidden sensor measurements to a system to improve its security. Hidden sensor measurements are by our definition measurements that were not considered in the original design of the system, and are not used for any operational reason. We only add them to improve the security of the system and using them in anomaly detection and mitigation. Biography: Alvaro A. Cardenas is an Associate Professor of Computer Science and Engineering at the University of California, Santa Cruz. Before joining UCSC, he was the Eugene McDermott Associate Professor of Computer Science at the University of Texas at Dallas. Earlier in his career, he was a postdoctoral scholar at the University of California, Berkeley, and a research staff member at Fujitsu Laboratories. He holds M.S. and Ph.D. degrees from the University of Maryland, College Park, and a B.S. from Universidad de Los Andes in Colombia. His research interests focus on cyber-physical systems and IoT security and privacy. He is the recipient of the NSF CAREER award, the 2018 faculty excellence in research award from the Erik Johnson School of Engineering and Computer Science, and the Eugene McDermott Fellow Endowed Chair at the University of Texas at Dallas. He has also received best paper awards from the IEEE Smart Grid Communications Conference and the U.S. Army Research Conference. One of his papers was also a finalist to the CSAW competition in Israel. 10:05 Break (30 minutes) 10:35 Securing Critical Infrastructure: Challenges and Opportunities, Jianying Zhou, Singapore University of Technology and Design ABSTRACT: Critical infrastructure becomes a strategic target in the midst of a cyber-war. Governments are investing significantly in response to the risks and challenges while researchers and vendors are aggressively developing and marketing new technologies aimed at protecting critical infrastructure. In this talk, I will briefly describe the framework and features of a cyber-physical system (CPS) which serves as the core to provide critical services in different industrial domains. Then I will discuss the challenges we face and the approaches we can take to defend against cyber attacks. After that I will present a few novel technologies developed in iTrust for preventing and detecting attacks to CPS. I will further introduce the fully operational CPS testbeds in iTrust, and show how the testbeds are used to validate the security technologies so that the owners and operators of critical infrastructure can be confident that the technologies to be deployed will actually protect their systems in the event of a cyber-war. Critical infrastructure becomes a strategic target in the midst of a cyber-war. Governments are investing significantly in response to the risks and challenges while researchers and vendors are aggressively developing and marketing new technologies aimed at protecting critical infrastru",,2020.0,,227319385,semantic_scholar
e8ad5f723fe13574614be08c5a8aeed5602e7b97,https://www.semanticscholar.org/paper/e8ad5f723fe13574614be08c5a8aeed5602e7b97,Vulnerability Discovery in Embedded Systems,"Objective Embedded systems are omnipresent in our everyday life. For example, they are the core of various Common-Off-The-Shelf (COTS) devices such as printers, mobile phones, home appliances, and computer components and peripherals. They are also present in many devices that are less consumer oriented such as video surveillance systems, medical implants, automotive elements, military systems, SCADA and PLC devices, and basically anything we usually call “electronics”. Moreover, the emerging phenomenon of the Internet-of-Things (IoT) will make them even more widespread and interconnected. The security and reliability of this broad range of devices is paramount to ensure both the proper functioning of our society and our physical safety. Unfortunately, embedded devices did not reach yet the same level of security we obtained for software of typical personal computers. For example, because of the very heterogeneous set of platforms and architectures, embedded systems still lack a solid set of vulnerability discovery and analysis techniques. The goal of this thesis is to bridge this gap by improving the state of the art of vulnerability discovery in software binaries. In particular, the student will focus on the development of novel static and dynamic analysis techniques that can be applied to the study of real-world, complex firmware images. The proposed approach will need to cope with a number of challenges, ranging from scalability issues, heterogeneity of targets, need for low false positive rates, and the intrinsic difficulty of running dynamic analysis on real embedded devices. To ensure the deployability of the developed techniques, real examples and test cases for the Ph.D. research will be provided by a close industrial support and collaboration with Siemens. Background and PreviousWork The work performed in this thesis builds upon two lines of research which are ongoing in our group at Eurecom. The first is related to the use and application of advanced dynamic analysis techniques on the firmware of embedded devices. Zaddach et al. designed and implemented and open source system named Avatar [1], whose goal is to execute a firmware inside an instrumented emulator. Emulating firmwares of embedded devices requires accurate models of all hardware components used by the system under analysis. Unfortunately, the lack of documentation and the large variety of hardware on the market make this approach infeasible in practice. Avatar fills this gap and overcomes the limitation of pure firmware emulation by acting as an orchestration engine between the physical device and an external emulator [7]. By injecting a 1 special software proxy in the embedded device, Avatar can execute the firmware instructions inside the emulator while channeling the I/O operations to the physical hardware. Since it is infeasible to perfectly emulate an entire embedded system and it is currently impossible to perform advanced dynamic analysis by running code on the device itself, Avatar takes a hybrid approach. It leverages the real hardware to handle I/O operations, but extracts the firmware code from the embedded device and emulates it on an external machine. A similar architecture, but supported by a FPGA bridge to increase the throughput, was used by Koscher et al. [5] in their Surrogates system.",,2020.0,,211526969,semantic_scholar
21a98f40124f50173239ff63601426b5d948d76a,https://www.semanticscholar.org/paper/21a98f40124f50173239ff63601426b5d948d76a,Modeling and Simulation in the Systems Engineering Life Cycle,"structural component that it represented. For example, the sorin centrifugal pump is a particular kind of blood pump used in the CHOA centrifugal pump circuit. In the SysML model, the part property for the sorin centrifugal pump redefines the abstract blood pump part property of the ECMO circuit block. With these crosscutting relationships defined, any element of an ECMO circuit design is traceable from source materials to requirements, from requirements to behaviors, from behaviors to abstract structural element, and finally from abstract structural element to a specific make and model of a component. This end-to-end traceability, shown in Fig. 27.10, provides an objective mechanism for determining whether a particular circuit design meets its requirements and a mechanism for performing impact analysis in the event that requirements, best practices, or technology change. Future iterations of the model may incorporate more full life cycle aspects of the ECMO domain such as training and maintenance and may expand impact analysis. 27.10 Future Updates to the ECMO MBSE Framework The SysML diagram not currently represented in the ECMO MBSE framework is the parametric diagram. A parametric diagram allows for applying constraints to block value properties, and allows external engineering models to be plugged into the SysML model for analysis. For example, we could add parametric diagrams to compare the throughput of blood or medications through various circuit designs, to analyze the amount of pressure the circuit exerts on blood, to analyze which circuit designs minimize the amount of foreign surface area that the patient’s blood contacts, or to analyze the volume of blood required to prime each circuit before it is used. Fig. 27.10  Crosscutting relationships 364 N. L. Adams et al. 27.11 Leveraging the ECMO MBSE Framework One of the goals of the ECMO SE project is to leverage ELSO registry case study data to improve ECMO medical decision making. One way to accomplish this is to provide a way for ECMO specialists to capture circuit designs and then associate a circuit design with each case study submitted to the ELSO registry. The addition of circuit design details will enhance the ELSO registry data so that practitioners can perform analysis to determine which existing ECMO circuit design, if any, outperforms other existing circuit designs. SysML provides systems engineers with a rich environment for capturing and modeling systems of interest. However, a “user friendly” tool is required for ECMO specialists to capture and submit their circuit designs. For example, while the IBD shown in Fig. 27.9 accurately captures all aspects of the circuit architecture and integrates into the formal MBSE framework for ECMO, it is a highly technical view and does not address the customer need for a simple “non-technical” interface. The following two statements capture the need to improve ECMO safety by providing a way to evaluate existing ECMO circuits to identify a possible best of breed. ECMO program need: “The medical community needs a safer ECMO circuit that is more reliable, more autonomous, and easier to deploy in order to increase the quality of ECMO treatments, promote survivability of patients with severe respiratory and cardiac conditions, and to reduce the use of lung-damaging ventilators on patients.” EDCT project need: “The ECMO community needs a way to characterize the performance of current ECMO circuits across the world in order to identify and correct design flaws, increase system reliability, and to support the development of a standardized next-generation ECMO circuit.” The solution for this problem became known as the EDCT, and we designed the EDCT as an easy-to-use, web-based interface for constructing and capturing ECMO circuit designs. We created two models to support concept development. First, we created a comprehensive SysML model to define key ECMO components and their interrelationships. Second, we created a consumer-friendly model in the form of a prototype of the web-based user interface (UI). This prototype presents the end-user with the look and feel of the application. Key features of this model include visually accurate component icons, drag-and-drop functionality, and UI layout. We then used these models to refine design concepts and evaluate the user interface to optimize the user experience. We leveraged the existing SysML ECMO framework to explore possible solutions, refine concepts, and reduce development risk of the EDCT software application. Working closely with an ELSO representative, we used SysML to create a comprehensive model for the EDCT. Figure 27.11 shows the high-level use case for the EDCT web application. We then created a prototype of the EDCT UI, shown in Fig. 27.12, to reduce much of the uncertainty regarding “look and feel,” host hardware and software, and web browser compatibility. Most importantly, the prototype model provides a 365 27 Model-Based Systems Engineering mechanism for communication between our team, the project sponsor, and potential end users. We represent the physical ECMO circuit graphically using the EDCT user interface. Using this application, the specialist creates a virtual circuit to document the layout and component detail of the actual circuit. The UI provides a userfriendly experience but it does not sufficiently describe the circuit. To mitigate this, we developed a high-fidelity engineering representation of the circuit to accurately Fig. 27.11  EDCT use case diagram Fig. 27.12  EDCT prototype website 366 N. L. Adams et al. record the circuit architecture so that components and the assembly order can be stored in a database and retrieved for later analysis. The bridge between the UI and a database compatible schema is a circuit node map, shown in Fig. 27.13. We devised the node map to define each component and its relationship to other components. In this representation, we assign each component an identifier such as GS1 (gas sensor #1), Pat1 (patient #1), and T11 (tube #11). Nodes between each component possess an integer identifier. In this way, we associate each component with the nodes and the architecture can be stored electronically without ambiguity. Fig. 27.13 shows the node map for the UI circuit shown in Fig. 27.12 and is useful when rendering circuit assemblies. This model representation is not intended for the end user, and when used in the EDCT user interface, each component is replaced with its corresponding device icon for easier understanding. The design of the EDCT provides a way for ECMO specialists and physicians to gather real-world circuit data to improve decision making for future ECMO circuit implementations. Key features desired for the actual implementation of the tool include integration with the ELSO registry, a simple user interface, vetted circuit elements, provisional device capability, integrated circuit error checking and low/ no cost to implement at ECMO centers around the world.","Simulation Foundations, Methods and Applications",2015.0,10.1007/978-1-4471-5634-5,42687410,semantic_scholar
de3ec2ed31c2a2cbe4d38d044b90ef5f1b36648d,https://www.semanticscholar.org/paper/de3ec2ed31c2a2cbe4d38d044b90ef5f1b36648d,"Mobile technologies in the study, assessment, and treatment of schizophrenia.","Mobile technologies are developing at a phenomenal rate and hold tremendous promise for transforming schizophrenia research and treatment. Over the last decade, mobile devices including microcomputers, mobile phones, and smartphones have become ubiquitous. The United Nations’ telecommunication agency recently reported that mobile phone subscriptions have reached almost 6 billion worldwide.1 Developing countries now account for close to 3 quarters of the mobile phones in use, and in some developed countries, the number of mobile phones already exceeds the size of the population, with many individuals owning multiple mobile devices. Recent national surveys in the United States found that mobile devices are helping bridge the digital information divide between various socioeconomic groups, as several traditionally underserved populations who typically could not afford access to home computers and internet packages now often use smartphones as their primary connection to information on the Internet. Remarkably, there is emerging evidence that many chronically homeless individuals now also use mobile devices regularly. The characteristics of contemporary mobile technologies (ie, portability, self-contained power source, increasingly user-friendly design) allow for something quite revolutionary—they enable us to transport research, assessment, and treatment out of the laboratories and clinics and into the real-time/real-world context in which individuals negotiate their daily lives and contend with chronic psychiatric illnesses and functional impairment. As infrastructure for mobile telecommunication continues to develop globally, it will create opportunities for far-reaching implementation of evidence-based interventions and wide-scale dissemination of information and resources in a manner that is unprecedented. 
 
The inherent advantages of mobile technologies are not going unnoticed by researchers, clinicians, and forward-thinking policy makers. The incorporation of various mobile devices in support of prevention and treatment initiatives across biomedical and behavioral disciplines is growing rapidly,2,3 the National Institutes of Health has recently begun to offer specialized training and funding opportunities focusing specifically on Mobile Health (mHealth) research, The Center for Medicare and Medicaid Services is exploring new payment models that may allow for expanded reimbursement of technology-based services, and the US Food and Drug Administration has already released statements regarding guidelines for regulating the use of some mobile devices and applications. 
 
While enthusiasm for utilization of mobile technology in research and clinical care is gaining momentum across a wide array of physical and mental health disciplines, many schizophrenia researchers and clinicians remain skeptical about the ability or willingness of patients with psychotic illnesses to comply with mobile research protocols or engage in mobile interventions. This apprehension is largely unfounded, and evidence suggests that given opportunity and appropriate training, many individuals with schizophrenia can and will use various mobile technologies successfully, even when they are quite symptomatic. The purpose of this special issue is to cultivate discussion about new opportunities for leveraging existing and emerging mobile technologies in the study of psychotic illnesses and to encourage investigators to think creatively about how these novel approaches can improve our understanding of the etiological risk factors, contextual influences, and possible treatments for schizophrenia. 
 
In the first article in this collection, we have asked Kimhy et al4 to discuss the rationale for mobile technology research in schizophrenia and provide concrete guidelines and practical suggestions for studies with this population. Their expert insights and shared collective experiences will undoubtedly be useful to investigators who are unfamiliar with mobile technology study design, hardware and software requirements, and statistical approaches necessary to successfully analyze the rich data that are characteristic of these paradigms. In the articles that follow, investigators demonstrate how 3 generations of mobile devices, including preprogrammed wristwatches used in conjunction with paper-and-pencil diaries, microcomputers, and mobile phones, can be effectively deployed for mobile research and treatment development. Ben-Zeev and colleagues5 use mobile technology to assist in the evaluation of patient clinical self-reports. They compare real-time/real-place momentary mobile assessments collected over 7 days in individuals with schizophrenia and a nonclinical comparison group to retrospective reports provided by both groups for the same period of time. Their findings indicate that study compliance in individuals with schizophrenia can be exceptionally high and that various dimensions of one’s symptomological and emotional experience are not well captured by traditional reports and better captured by momentary mobile assessments. Oorschot and colleagues6 and Granholm and colleagues7 deploy mobile devices in a therapeutic context. Oorschot and associates demonstrate how mobile data can be used to elucidate idiosyncratic symptom patterns and dynamic changes within individuals longitudinally, and articulate how this approach can augment face-to-face treatments by improving the therapeutic relationship between clinician and patient, providing important information for psychoeducation and treatment personalization. Granholm and colleagues7 report on an innovative automated mobile assessment and intervention for schizophrenia. In their pilot study, text messages sent from a remote preprogrammed server to patients’ mobile phones are used to administer cognitive-behavioral interventions in support of medication adherence, social functioning, and coping with auditory hallucinations. In the context of a growing body of evidence suggesting symptoms, affect, and functioning in schizophrenia are not nearly as static as previously believed, such paradigms may prove to be an especially powerful tool for identification of risk or preventive factors that could perhaps be targeted with real-time mobile interventions. 
 
As mobile devices infiltrate more and more areas of life of the general population, they will undoubtedly become more prevalent among those with schizophrenia as well. Statistically, many of those who are currently growing up with these technologies in hand will go on to develop serious mental illnesses in the future. Looking forward, now is the opportune time for innovative investigators and clinicians to examine how these emerging technologies can be harnessed as a powerful new platform for research and treatment approaches that can be made available in the years ahead.",Schizophrenia bulletin,2012.0,10.1093/schbul/sbr179,32016055,semantic_scholar
e49d64c86166a828381df783776cabd30d78b902,https://www.semanticscholar.org/paper/e49d64c86166a828381df783776cabd30d78b902,Resource-Aware Predictive Models in Cyber-Physical Systems,"Author(s): AMIR, MARAL | Advisor(s): Givargis, Tony | Abstract: Cyber-Physical Systems (CPS) are composed of computing devices interacting with physical systems. Model-based design is a powerful methodology in CPS design in the implementation of control systems. For instance, Model Predictive Control (MPC) is typically implemented in CPS applications, e.g., in path tracking of autonomous vehicles. MPC deploys a model to estimate the behavior of the physical system at future time instants for a specific time horizon. Ordinary Differential Equations (ODE) are the most commonly used models to emulate the behavior of continuous-time (non-)linear dynamical systems. A complex physical model may comprise thousands of ODEs that pose scalability, performance and power consumption challenges. One approach to address these model complexity challenges are frameworks that automate the development of model-to-model transformation. In this dissertation, a state-based model with tunable parameters is proposed to operate as a reconfigurable predictive model of the physical system. Moreover, we propose a run-time switching algorithm that selects the best model using machine learning. We employed a metric that formulates the trade-off between the error and computational savings due to model reduction. Building statistical models are constrained to having expert knowledge and an actual understanding of the modeled phenomenon or process. Also, statistical models may not produce solutions that are as robust in a real-world context as factors outside the model, like disruptions would not be taken into account. Machine learning models have emerged as a solution to account for the dynamic behavior of the environment and automate intelligence acquisition and refinement. Neural networks are machine learning models, well-known to have the ability to learn linear and nonlinear relations between input and output variables without prior knowledge. However, the ability to efficiently exploit resource-hungry neural networks in embedded resource-bound settings is a major challenge.Here, we proposed Priority Neuron Network (PNN), a resource-aware neural networks model that can be reconfigured into smaller sub-networks at runtime. This approach enables a trade-off between the model's computation time and accuracy based on available resources. The PNN model is memory efficient since it stores only one set of parameters to account for various sub-network sizes. We propose a training algorithm that applies regularization techniques to constrain the activation value of neurons and assigns a priority to each one. We consider the neuron's ordinal number as our priority criteria in that the priority of the neuron is inversely proportional to its ordinal number in the layer. This imposes a relatively sorted order on the activation values. We conduct experiments to employ our PNN as the predictive model in a CPS application. We can see that not only our technique will resolve the memory overhead of DNN architectures but it also reduces the computation overhead for the training process substantially. The training time is a critical matter especially in embedded systems where many NN models are trained on the fly.",,2019.0,,238130416,semantic_scholar
c613158391c247d42c45c79a83120a817183611b,https://www.semanticscholar.org/paper/c613158391c247d42c45c79a83120a817183611b,Proposition de recherche doctorale Vulnerability Discovery in Embedded Systems,"Objective Embedded systems are omnipresent in our everyday life. For example, they are the core of various Common-Off-The-Shelf (COTS) devices such as printers, mobile phones, home appliances, and computer components and peripherals. They are also present in many devices that are less consumer oriented such as video surveillance systems, medical implants, automotive elements, military systems, SCADA and PLC devices, and basically anything we usually call “electronics”. Moreover, the emerging phenomenon of the Internet-of-Things (IoT) will make them even more widespread and interconnected. The security and reliability of this broad range of devices is paramount to ensure both the proper functioning of our society and our physical safety. Unfortunately, embedded devices did not reach yet the same level of security we obtained for software of typical personal computers. For example, because of the very heterogeneous set of platforms and architectures, embedded systems still lack a solid set of vulnerability discovery and analysis techniques. The goal of this thesis is to bridge this gap by improving the state of the art of vulnerability discovery in software binaries. In particular, the student will focus on the development of novel static and dynamic analysis techniques that can be applied to the study of real-world, complex firmware images. The proposed approach will need to cope with a number of challenges, ranging from scalability issues, heterogeneity of targets, need for low false positive rates, and the intrinsic difficulty of running dynamic analysis on real embedded devices. To ensure the deployability of the developed techniques, real examples and test cases for the Ph.D. research will be provided by a close industrial support and collaboration with Siemens. Background and PreviousWork The work performed in this thesis builds upon two lines of research which are ongoing in our group at Eurecom. The first is related to the use and application of advanced dynamic analysis techniques on the firmware of embedded devices. Zaddach et al. designed and implemented and open source system named Avatar [1], whose goal is to execute a firmware inside an instrumented emulator. Emulating firmwares of embedded devices requires accurate models of all hardware components used by the system under analysis. Unfortunately, the lack of documentation and the large variety of hardware on the market make this approach infeasible in practice. Avatar fills this gap and overcomes the limitation of pure firmware emulation by acting as an orchestration engine between the physical device and an external emulator [7]. By injecting a 1 special software proxy in the embedded device, Avatar can execute the firmware instructions inside the emulator while channeling the I/O operations to the physical hardware. Since it is infeasible to perfectly emulate an entire embedded system and it is currently impossible to perform advanced dynamic analysis by running code on the device itself, Avatar takes a hybrid approach. It leverages the real hardware to handle I/O operations, but extracts the firmware code from the embedded device and emulates it on an external machine. A similar architecture, but supported by a FPGA bridge to increase the throughput, was used by Koscher et al. [5] in their Surrogates system.",,2019.0,,201110343,semantic_scholar
c7e55c148f1de403cdb8068c329ec8a039ba5ab9,https://www.semanticscholar.org/paper/c7e55c148f1de403cdb8068c329ec8a039ba5ab9,Preparing for Operational Use of C2-Simulation Interoperation,"Technical Activities in the NATO MSG are completing sustained development of a standards-based capability, known as C2SIM, for coalitions to interoperate their national command and control (C2) and simulation systems collectively as part of NATO’s Federated Mission Network (FMN). This form of synthetic battlespace can have a great impact on the effectiveness of coalition military operations. The second generation of C2SIM standards from SISO is ready for balloting and afterwards will form the basis of a STANAG. MSG-145 is conducting extensive testing to validate these standards. Several closely related capabilities developed in testing also can ease the path to operational use. This paper describes those capabilities: (1) C2SIM within Modelling and Simulation as a Service (MSaaS); (2) adoption by the MSCOE of the C2SIM Sandbox distributed development platform; (3) using C2SIM to support operational training in a cyber-active environment; and (4) extension of C2SIM into different domains, exemplified by an Autonomous Systems Extension. C2SIM provides a powerful new, standardsbased capability for coalition simulations to support collective training (including cyber effects), planning (including mission analysis) and increasingly important in command and control of Autonomous Systems for military operations. This paper provides important information to prepare for its operational military use. 1.0 INTRODUCTION: C2SIM OVERVIEW The ability to interoperate command and control (C2 or Mission Command) systems with simulation systems has been an important goal for more than a decade [1]. Over those years the NATO Modelling and Simulation Group (NMSG) has been cooperating with the Simulation Interoperability Standards Organization (SISO) to develop, prototype, and test standards that support that capability. Their shared vision is that members of a coalition will be able to combine their C2 systems and simulation systems collectively into a system-ofsystems where simulations are tasked by the C2 systems and in turn provide reports that are displayed on the C2 system just as they would appear due to real-world operations. The resulting system of systems can support training, course of action analysis, and mission rehearsal for the coalition. Each force element uses the C2 system with which it has trained and is represented by a simulation that represents well its doctrine, resources, and tactics/techniques/procedures. Sharing information this way will result in more effective coalition operations that can happen sooner [2, 3]. Preparing for Operational Use of C2-Simulation Interoperation 2 STO-MP-MSG-171 APPROVED FOR PUBLIC RELEASE APPROVED FOR PUBLIC RELEASE Standards enabling the vision described above are well along in development by SISO and expected to reach the balloting phase by the end of 2019. In order to finalize effective standards, the NATO Technical Activity MSG-145 Operationalization of Command and Control – Simulation Interoperation (C2SIM) undertook a validation process. This paper describes that process, beginning with the roles and motivations of NATO and SISO, then providing background on C2SIM. After that we will look at the activities of the eight national teams involved most recently and then explain how they enabled validation of C2SIM through a coordinated effort that provided compliant interfaces on six different simulations and one C2 system as well as supporting software. The validation effort took these C2SIM-enabled systems to the NATO Coalition Warrior Interoperability Exercise (CWIX) for detailed testing and then culminated with experimentation, structured as a miniature exercise in distributed mission planning. The paper concludes with lessons learned from the validation process and a view toward the future of C2SIM-based coalition interoperability. Figure 1 shows the general architecture of a C2SIM coalition. The C2 systems interoperate using a C2 standard; the simulation systems interoperate using a simulation standard; and the system of systems interoperates using C2SIM. A web service is used to replicate C2SIM messages for distribution among the constituent systems and to produce a log that documents results of the operation. The draft C2SIM standard consists of a text document defining rules and procedures for interoperation and for maintenance of the ontologies; a Core ontology consisting of data classes expected to be needed by any operational simulation; a Standard Military Extension (SMX) with classes applicable to all domains of military activity; and a Land Operations Extension (LOX) to encompass the capability originally provided by MSDL and C-BML and also to serve as an exemplar for future extensions. SMX is logically part of the main C2SIM standard, while LOX forms a new layer on top of Core+SMX. Figure 1: C2SIM Coalition General Architecture 2.0 NATO AND SISO ROLES DEVELOPING C2SIM The partnership between MSG-145 and the SISO C2SIM Product Development Group (PDG) has been critical in reaching a point where C2SIM can be validated, and even more critical in the validation process. As Preparing for Operational Use of C2-Simulation Interoperation STO-MP-MSG-171 3 APPROVED FOR PUBLIC RELEASE NATO UNCLASSIFIED RELEASABLE TO AUSTRALIA AND SWEDEN a collaborative organization of industry, academic and government people, SISO does not have the ability to develop working prototype systems-of-systems or to validate them with international military participation. Conversely, NATO is not in a position to develop industry-based standards. Cooperative work between the two has been needed to create the C2SIM standard. MSG-145 is the third in a sequence of NATO Technical Activities that has supported development of C2SIM. The first, MSG-048 Coalition Battle Management Language, completed validation of the technical feasibility of coalition C2-simulation interoperation. The second, MS-085 Standardization for C2-Simulation Interoperation supported and tested the first generation of C2-simulation interoperation standards: the Military Scenario Definition Language (MSDL) [4] and the Coalition Battle Management Language (C-BML) [5]. A key outcome of MSG-085 was the determination that while MSDL and C-BML can be made to work together, a second-generation standard was needed to achieve effective harmonization; also that the second generation should be designed for extensibility [6]. SISO responded by forming a merged PDG with a charter to achieve these things under the unified name C2SIM [7]. A goal of MSG-145 is to base a NATO Standardization Agreement (STANAG) on the C2SIM industry standard. SISO’s activities to create C2SIM have been based on a complete bottom-up review of both C-BML and MSDL with a view to the result serving as the basis for a family of extensions. The C2SIM PDG concluded that the best way to approach this was developing a consistent family of ontologies. Development has been underway since 2014 and recently produced a set of draft ontologies that is ready for implementation, along with an approach to extracting a standard XML schema from the ontologies to support implementation for validation. 3.0 C2SIM AS A SERVICE UNDER MSAAS Modelling and Simulation as a Service (MSaaS) is a new approach being explored by the STO NMSG Panel for a permanently available, flexible, service-based framework to provide more cost effective availability of Modelling and Simulation (M&S) products, data and processes to a large number of users on-demand. The NATO MSG-136 Modelling and Simulation as a Service Implementation defined MSaaS as “the combination of service-based approaches with ideas taken from cloud computing” [9]. MSG-145 defined the C2SIM Integration Platform (IP) Reference Architecture (RA) using both the NATO C3 Taxonomy [10] and the MSaaS Reference Architecture from NATO MSG-136 [9] as a source for Architecture Building Blocks (ABBs) and Architecture Patterns. The basic idea is to provide C2SIM as a service, defining ABBs linked to the NATO C3 Taxonomy and the M&S extensions defined by NATO MSG136. Examples of defined ABBs are Message-Oriented Middleware Service (functionality to support the exchange of messages between data producers and consumers, independent of the message format and content) or Mediation Services (middle layer between incompatible producers and consumers of information), built in the system-of-systems experimented by MSG 145. An experimental platform to provide “C2SIM as a service” is that developed by NATO Modelling and Simulation Centre of Excellence (MSCOE) in collaboration with the Leonardo company. This is a MSaaS cloud-based testbed prototype, named Open Cloud Environment ApplicatioN (OCEAN). It offers an embryonic framework made of a combination of hardware, software and services to automate the deployment of M&S tools and applications in a cloud environment. The OCEAN platform offers a unique point of access through a web portal with secure access granted by a user identity management system. The availability of services is managed by an M&S services management system that facilitates the delivery, versioning, testing, Preparing for Operational Use of C2-Simulation Interoperation 4 STO-MP-MSG-171 APPROVED FOR PUBLIC RELEASE APPROVED FOR PUBLIC RELEASE consumption, termination and disposal of services as shown in Figure 2. The system architecture involves the use of a hybrid cloud where the user can mix use of physical machines, virtual machines and containers (Figure 2) by means of a Platform as a Service solution based on OpenStack installed inside a VMware cluster. OCEAN is expected to provide C2SIM as a Service for experimental purposes by the end of 2019. Figure 2: OCEAN architecture 4.0 TRANSITIONING C2SIM SANDBOX TO MSCOE Under the MSG-145 activity, an integration and testing environment called C2SIM Sandbox was developed by George Mason University (GMU) to provide a full C2SIM capability, available via a virtual private network (VPN) by remote desk",,2019.0,,208540295,semantic_scholar
59e10d1d4cd454635914cfd0ac5160a318fd0473,https://www.semanticscholar.org/paper/59e10d1d4cd454635914cfd0ac5160a318fd0473,UB09 Session 9,"In the domain of Wireless Sensor Networks (WSN), providing an effective security solution to protect the motes and their communications is challenging. Due to the hard constraints on performance, storage and energy consumption, normal network-security related techniques cannot be applied. Focusing on the ""Intrusion Detection"" problem, we propose a realworld application of our WSN Intrusion Detection System (WIDS). WIDS exploits the Weak Process Models to classify potential security issues in the WSN and to notify the operators when an attack tentative is detected. In this demonstration, we show how our IDS works, how it detects some basic attacks and how the IDS can evolve to fullfil the needs of secure WSN deployments. Download Paper (PDF) UB09.2 RESCUE: EDA TOOLSET FOR INTERDEPENDENT ASPECTS OF RELIABILITY, SECURITY AND QUALITY IN NANOELECTRONIC SYSTEMS DESIGN Authors: Cemil Cem Gürsoy1, Guilherme Cardoso Medeiros2, Junchao Chen3, Nevin George4, Josie Esteban Rodriguez Condia5, Thomas Lange6, Aleksa Damljanovic5, Raphael Segabinazzi Ferreira4, Aneesh Balakrishnan6, Xinhui Anna Lai1, Shayesteh Masoumian7, Dmytro Petryk3, Troya Cagil Koylu2, Felipe Augusto da Silva8, Ahmet Cagri Bagbaba8 and Maksim Jenihhin1 1Tallinn University of Technology, EE; 2Delft University of Technology, NL; 3IHP, DE; 4BTU Cottbus-Senftenberg, DE; 5Politecnico di Torino, IT; 6IROC Technologies, FR; 7Intrinsic ID B.V., NL; 8Cadence Design Systems GmbH, DE Abstract The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF)The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF) UB09.3 ASAM: AUTOMATIC SYNTHESIS OF ALGORITHMS ON MULTI CHIP/FPGA WITH COMMUNICATION CONSTRAINTS Authors: Amir Masoud Gharehbaghi, Tomohiro Maruoka, Yukio Miyasaka, Akihiro Goda, Amir Masoud Gharehbaghi and Masahiro Fujita, The University of Tokyo, JP Abstract Mapping of large systems/computations on multiple chips/multiple cores needs sophisticated compilation methods. In this demonstration, we present our compiler tools for multi-chip and multi-core systems that considers communication architecture and the related constraints for optimal mapping. Specifically, we demonstrate compilation methods for multi-chip connected with ring topology, and multi-core connected with mesh topology, assuming fine-grained reconfigurable cores, as well as generalization techniques for large problems size as convolutional neural networks. We will demonstrate our mappings methods starting from data-flow graphs (DFGs) and equations, specifically with applications to convolutional neural networks (CNNs) for convolution layers as well as fully connected layers. Download Paper (PDF) UB09.4 HEPSYCODE-MC: ELECTRONIC SYSTEM-LEVEL METHODOLOGY FOR HW/SW CO-DESIGN OF MIXED-CRITICALITY EMBEDDED SYSTEMS Authors: Luigi Pomante1, Vittoriano Muttillo1, Marco Santic1 and Emilio Incerto2 1Università degli Studi dell'Aquila DEWS, IT; 2IMT Lucca, IT Abstract Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF)Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF) UB09.5 CS: CRAZYSQUARE Authors: Federica Caruso1, Federica Caruso1, Tania Di Mascio1, Alessandro D'Errico1, Marco Pennese2, Luigi Pomante1, Claudia Rinaldi1 and Marco Santic1 1University of L'Aquila, IT; 2Ministry of Education, IT Abstract CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF)CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF) UB09.6 LABSMILING: A SAAS FRAMEWORK, COMPOSED OF A NUMBER OF REMOTELY ACCESSIBLE TESTBEDS AND RELATED SW TOOLS, FOR ANALYSIS, DESIGN AND MANAGEMENT OF LOW DATA-RATE WIRELESS PERSONAL AREA NETWORKS BASED ON IEEE 802.15.4 Authors: Carlo Centofanti, Luigi Pomante, Marco Santic and Walter Tiberti, University of L'Aquila, IT Abstract Low data-rate wireless personal area networks (LR-WPANs) are constantly increasing their presence in the fields of IoT, wearable, home automation, health monitoring. The development, deployment and testing of SW based on IEEE 802.15.4 standard (and derivations, e.g. 15.4e), require the exploitation of a testbed as the network grows in complexity and heterogeneity. This demo shows LabSmiling: a SaaS framework which connects testbeds deployed in a real-world-environment and the related SW tools that make available a meaningful (but still scalable) number of physical devices (sensor nodes) to developers. It provides a comforta",,2019.0,195809644,,semantic_scholar
d0057c131d7356ef14863c03c6a5e1063755d657,https://www.semanticscholar.org/paper/d0057c131d7356ef14863c03c6a5e1063755d657,A4WSN: an architecture-driven modelling platform for analysing and developing WSNs,"This paper proposes A4WSN, an architecture-driven modelling platform for the development and the analysis of wireless sensor networks (WSNs). A WSN consists of spatially distributed sensor nodes that cooperate in order to accomplish a specific task. Sensor nodes are cheap, small, and battery-powered devices with limited processing capabilities and memory. WSNs are mostly developed directly on the top of the operating system. They are tied to the hardware configuration of the sensor nodes, and their design and implementation can require cooperation with a myriad of system stakeholders with different backgrounds. The peculiarities of WSNs and current development practices bring a number of challenges, ranging from hardware and software coupling, limited reuse, and the late assessment of WSN quality properties. As a way to overcome a number of existing limitations, this study presents a multi-view modelling approach that supports the development and analysis of WSNs. The framework uses different models to describe the software architecture, hardware configuration, and physical deployment of a WSN. A4WSN allows engineers to perform analysis and code generation in earlier stages of the WSN development life cycle. The A4WSN platform can be extended with third-party plug-ins providing additional analysis or code generation engines. We provide evidence of the applicability of the proposed platform by developing PlaceLife, an A4WSN plug-in for estimating the WSN lifetime by taking various physical obstacles in the deployment environment into account. In turn, PlaceLife has been applied to a real-world case study in the health care domain as a running example.",Software & Systems Modeling,2018.0,49869385,10.1007/s10270-018-0687-0,semantic_scholar
5516eea2b00bba86118be6f90d7c678b4c022226,https://www.semanticscholar.org/paper/5516eea2b00bba86118be6f90d7c678b4c022226,Developments in the United Kingdom road transport from a smart cities perspective,"Purpose: Smart city is a city which functions in a sustainable and intelligent way, by integrating all of its infrastructures and services in a cohesive way using intelligent devices for monitoring and control, to ensure efficiency and better quality of life for its citizens. As other countries globally, UK is keen for economic development and investment in smart city missions to create interest in monetary environment and inward investment. This paper explores the driving forces of smart road transport transformation and implementation in the UK. Design/methodology/approach: The study involved interviews with sixteen professionals from the UK road transport sector. A semi-structured interview technique was used to collect experts’ perception, which was then examined using content analysis. Findings: The results of the study revealed that the technological advancement is a key driver. The main challenges faced for the implementation of smart city elements in the UK road network are: lack of investment; maintenance; state of readiness and the awareness of the smart road transport concept. The study concludes that an understanding of the concept of smart cities from a road transport perspective is very important to create awareness of the benefits and the way it works. A wider collaboration between every sector is crucial to create a successful smart city. Originality/value: The study contributes to the field of digitalisation of road transport sector. This paper reveals the key driving forces of smart road transport transformation, the current status of smart road transport implementation in UK and challenges of the smart road transport development in the UK. Engineering, Construction and Architectural Management http://mc.manuscriptcentral.com/ecaam 2 Introduction Enormous global urbanisation and growth has caused migration of people in urban areas and spatial development of urban infrastructure. According to Obaidat and Nicopolitidis (2016), 85-90% of the world population evolution is a result of a 21 st century cities. Therefore, smart cities are being created from scratch or being developed gradually by improving the prevailing cities infrastructure and primary resources. A study of McKinsey Global Institute’s by Department for Business innovation and Skills (2013) shows that more than 50% of the global GDP (Gross Domestic Product) is generated in the 190 major cities in the developed countries compare to 22 largest cities in the developing countries. However, the increase of growth in the developed countries is healthy but this phenomena also set a high expectation on public services and the quality of the urban infrastructure and environment. Due to the urbanisation, more problems such as overpopulation, pollutions, scarcity of natural resources, public and private services are being created (Yigitcanlar et. al. 2020, Dameri, 2014). Smart cities are an emerging strategy to mitigate the problems generated by the rapid urban population growth and rapid urbanization (Prakash, 2019). A ‘smart city’ is defined as an urban area that is highly innovative in terms of overall facilities, ecological real estate, communication and market feasibility (Chirisa et al, 2016). Also, smart cities is defined as “a place where the traditional networks and services are made more efficient with the use of digital and telecommunication technologies, for the benefits of its inhabitants and businesses” (Kumar et al., 2018). Whereas BSI (2014) noted that smart cities as an effective integration of physical, digital and human systems in a built environment to deliver sustainable, prosperous and inclusive future for its citizens. From the above three definitions, it could be inferred that smart cities are cities that make use of physical, digital and human systems in order to enable sustainability and efficiency for the citizens within that city. The economic growth of any country is supported by its good infrastructure. The United Kingdom (UK) has strategic roads, railway and airports; however, it is one of the top 10 most congested country in the world (Korosec, 2018). According to ONS (2017), the population in the UK in 2016 was 65.6 million which was the largest ever. It is also projected that the population in the UK would grow, reaching over 74 million by 2039. Due to the population rise amalgamated with increase of cars on the road, which has increased by 4.6% higher than the previous peak in 2016 (Department of Transport, 2017), the present UK transport system faces many challenges. The UK road transportation system is gradually taking steps to overcome the problems. As road transport is a significant source of both safety and environmental concerns. This research aims to explore the driving forces of smart road transport transformation, and implementation in the UK along with the challenges faced. In doing so the next section delves into the relevant literature review followed by methodology and findings. The paper finally concludes with recommendations. 2. Literature review An extensive review of literature on drivers, current status of smart cities and the challenges are discussed in this section. Three main drivers include: technology, environment, and socioeconomy (See Table 1). The technological advancement is clearly seen as strong impact on the cities in the recent years. It can be seen clearly that, the communication technologies (ICT) develops city management, enhances technical and social networks, and urban affordability. Within the technology as a driver, the review of literature revealed that technologies such as Big Data, Internet of Things and Artificial Intelligence are widely Engineering, Construction and Architectural Management http://mc.manuscriptcentral.com/ecaam 3 implemented within the smart cities context. However, there is a lack of studies focusing on the UK smart road transport sector. Table 1: Literature on classification of drivers of smart cities development Drivers Description Source Technology Big data Big volume of digital data contents through online services such as Facebook, Twitter, You Tube, Instagram and SnapChat Olshannikova et al , 2017 Data is transmitted to various smart applications through sensor devices or other cloud computing integrated devices Hashem et al , 2016 Big data development highlights information and communication tools in the cyber physical farm management cycle and it identifies the interconnection related to socio-economic challenges Wolfert et al , 2017 Internet of Things IoT is widely in use for many multimedia application, smart transportation system and smart city design and deployment issues KeertiKumar et al , 2016 The implementation of IoT in transportation system means underlying technology and creating smart environments to increase their efficiency in tackling the transportation and environmental issues Kyriazis et al , 2013 Artificial Intelligence Artificial Intelligence (AI) is a technology that is influencing every pace of life which enable people to reconsider how to integrate information, analyse data, and real time data usage for the purpose of improve decision making West and Allen , 2018 Artificial neutral networks, expert system and hybrid intelligent system are computer based modelling tools that have recently aroused and found extensive recognition in many discipline for modelling complex real-world problem. Bahrammirzaee , 2010 China is a leading competitor and primarily focusing in the use of AI in military vehicles. While, Russia actively looking for opportunity in AI development and focused on robots in military Hoadley and Lucas , 2018 Environment Songdo, the Korean model of smart city was subjected to become one of the sustainable smart cities around the world Yigitcanlar , et al, 2018 Songdo, consist 40% of parks and green spaces and waste processing centre placed by the underground system and to recycle Benedikt , 2016 ; Shwayri , 2013 European Union aimed to reduce greenhouse gas emission in urban design through the implementation of innovation technologies Ahvenniemi et al , 2017 Engineering, Construction and Architectural Management http://mc.manuscriptcentral.com/ecaam 4 Cities have been setting high expectation on reaching the target of creating a clean future as shared by Covenant of Mayors’ vision for 2050 to accelerate the decarbonisation Covenant of Mayors for Climate & Energy , 2018 Socioeconomy By 2050, six hundred cities will generate almost 65% of world economic growth by contribute to a higher global GDP Dobbs et al, 2012 Smart cities indicated as the next evolution of ‘new community management where urban problems converted into opportunities for business investment and profit earning Anand and Marcco , 2018 Global smart cities market size to grow reaching USD 2.57 trillion by 2050 Grand View Research , 2018 Governments are obligated to protect citizen and their control over the active connection of local public groups, the police force, and the citizen such as the senior and disabled Neirotti et al ,2014 Level of observing has been focused by an increasing safety and security that desires to manage risks Kitchin , 2014 Safety and security features implemented with help of big data and data controls centres that joined and bind data stream collectively for example the installation of CCTV and street monitored camera are to monitor activity remotely Firmino and Duarte, 2014 Big data is a trend of utilising software tools to store data and share data collected with the use of technology. Nowadays, big data has been a tool that facilitating individual, businesses and government to discover new solutions to certain problems. Data is a crucial aspect as when an asset is built, asset management goes on and the more data collected in, the asset is being constructed, the more the asset can be maintained in an efficient manner (Loshin, 2018). Furthermore, KeertiKumar et al (2016) mention that IoT is widely used for many multim",,2020.0,235349937,,semantic_scholar
d239d3668f8653d5790d7029a344c18b1bfae493,https://www.semanticscholar.org/paper/d239d3668f8653d5790d7029a344c18b1bfae493,"Geoworldsim: a time-asynchronous, distributed and intelligent environment based geosimulation platform","Imagine that the accessibility of the population to public infrastructures needs to be evaluated. A possible solution would be to calculate every distance and analyse which percentage of the population is in less than 300 meters. However, this solution does not take into account issues such as: the age distribution, functional diversity or people’s preferences when transiting the city. It is in these cases when it is necessary to go a step further and integrate Geographic Information Systems with other perspectives such as Multi-Agent Systems which represent the particular characteristics of each individual and their decision making processes. This integration is known as Geosimulation and builds more accurate simulations to model the physical reality together with social, demo graphic and economic components. 
Geosimulations aim at modelling systems at the scale of individuals and entity-level units of the built environment and provides a way to simulate big amounts of agents interacting in a virtual geographic environment and endowed with spatial cognitive capabilities (perception, navigation, reasoning). This dissertation presents a new Geosimulation platform design and implementation that allows analysing and simulating different urban infrastructures. The platform manages to put into practice the latest theories in Multi-Agent Systems along with the new techniques in cloud computing and asynchronism. The proposed design is evaluated for three case studies; ubiquitous IoT, sustainable transport policies and resilience of the power grid. The methodologies presented, provide progress to their respective research areas by improving state-of-the-art techniques or designing new mechanisms. 
Furthermore, by connecting these Geosimulations to the real world by sensors and actuators, the concept of mixed reality arises; simulations where changes in the real world are transferred to the virtual world through sensors and agents can influence the real world through actuators. Mixed realities allow developing distributed control systems, which not only take into account the physical reality and social preferences but also the state, where to deploy intelligent agents that provide services to citizens.",,2018.0,215847803,,semantic_scholar
3b71fb8879e0d5f480b1379f58b902e904b743fb,https://www.semanticscholar.org/paper/3b71fb8879e0d5f480b1379f58b902e904b743fb,Towards Network Softwarization: A Modular Approach for Network Control Delegation. (Une Approche Modulaire avec Délégation de contrôle pour les Réseaux Programmables),"Network operators are facing great challenges in terms of cost and complexity in order to incorporate new communication technologies (e.g., 4G, 5G, fiber) and to keep up with increasing demands of new network services to address emerging use cases. Softwarizing the network operations using SoftwareDefined Networking (SDN) and Network Function Virtualization (NFV) paradigms can simplify control and management of networks and provide network services in a cost effective way. SDN decouples control and data traffic processing in the network and centralizes the control traffic processing to simplify the network management, but may face scalability issues due to the same reasons. NFV decouples hardware and software of network appliances for cost effective operations of network services, but faces performance degradation issues due to data traffic processing in software. In order to address scalability and performance issues in SDN/NFV, we propose in the first part of the thesis, a modular network control and management architecture, in which the SDN controller delegates part of its responsibilities to specific network functions instantiated in network devices at strategic locations in the infrastructure. We have chosen to focus on a modern application using an IP multicast service for live video streaming applications (e.g., Facebook Live or Periscope) that illustrates well the SDN scalability problems. Our solution exploits benefits of the NFV paradigm to address the scalability issue of centralized SDN control plane by offloading processing of multicast service specific control traffic to Multicast Network Functions (MNFs) implemented in software and executed in NFV environment at the edge of the network. Our approach provides smart, flexible and scalable group management and leverages centralized control of SDN for Lazy Load Balance Multicast (L2BM) traffic engineering policy in software defined ISP networks. Evaluation of this approach is tricky, as real world SDN testbeds are costly and not easily available for the research community. So, we designed a tool that leverages the huge amount of resources available in the grid, to easily emulate such scenarios. Our tool, called DiG, takes into account the physical resources (memory, CPU, link capacity) constraints to provide a realistic evaluation environment with controlled conditions. Our NFV-based approach requires multiple application specific functions (e.g., MNFs) to control and manage the network devices and process the related data traffic in an independent way. Ideally, these specific functions should be implemented directly on hardware programmable routers. In this case, new routers must be able to execute multiple independently developed programs. Packet-level programming language P4, one of the promising SDN-enabling technologies, allows applications to program their data traffic processing on P4 compatible network devices. In the second part of the thesis, we propose a novel approach to deploy and execute multiple independently developed and compiled applications programs on the same network device. This solution, called P4Bricks, allows multiple applications to control and manage their data traffic, independently. P4Bricks merges programmable blocks (parsers/deparsers and packet processing pipelines) of P4 programs according to processing semantics (parallel or sequential) provided at the time of deployment.",,2018.0,52954853,,semantic_scholar
3a7a35c3e64c8e4a546ad3656ffb78cc92f42a7a,https://www.semanticscholar.org/paper/3a7a35c3e64c8e4a546ad3656ffb78cc92f42a7a,A REVIEW ON INTRUSION DETECTION AND ITS ANALYSIS,"Now a day wireless detection network could be a unit that is loosely used in environmental management, investigation tasks, observing military applications, health connected applications, pursuit and dominant etc. A wireless intrusion detection additionally aids among the detection of a range of attacks. so as to identify gaps and attacks in wireless network intrusion detection analysis, this paper survey the literature of this area. This paper is to classify existing up to date wireless intrusion detection system (IDS); techniques based on target wireless network, detection technique, assortment method, trust model and analysis technique. This paper summarize pros and cons of a similar or differing types of considerations and concerns for wireless intrusion detection with respect to specific attributes of target wireless networks together with wireless local area networks (WLANs), wireless personal area networks (WPANs), wireless sensor networks (WSNs), ad hoc networks, mobile telecommunication, wireless mesh networks (WMNs) and cyber physical systems (CPSs). This paper is to outline the fundamentals of intrusion detection in wireless network, describing the kinds of attacks and state the motivation for intrusion detection in wireless network. A REVIEW ON INTRUSION DETECTION AND ITS ANALYSIS Pradeep Kumar, N. Sreema Iswarya, S. Sharmila Jeyarani 318 [1] INTRODUCTION Intrusion detection is a significant exploring topic with many prospective applications. Along with intrusion prevention, response and tolerance, intrusion detection is one tool that can defend against the real-world cyber attacks threatening critical systems. Vulnerabilities in most computer systems. And, it can be exploited by either non authorized or authorized users. Having said that, several tools are being designed and implemented for a variety of exploitations in diverse range of security attacks. Among these tools is the intrusion detection systems (IDS) which allow us to monitor a range of computer systems: an information system, a network or a cloud computing. These IDS detect intrusions and defined as attempts to break the security objectives such as confidentiality, integrity and availability and non-repudiation. The objective of this paper is to compare the different type of intrusion detection systems and describe their mode of use. In addition, we will include the different approaches currently proposed by others on IDS system, network and cloud computing based vulnerabilities in most computer systems. And, it can be exploited by either non authorized or authorized users. [2] LITERATURE REVIEW Wireless networks are not immune to the risks of destruction and decommissioning. Some of these risks are identical to those in Ad-Hoc networks, and others are specific to the sensors. Several articles [1][2][3][4][5] have presented security attacks and issues in WSNs. Intrusion detection system (IDS) defined as the second line of defense after cryptography, allows the detection and prevention of internal and external attacks. In [6], it is presented a Rule-based IDS called also Signature-based. Most of the techniques in these schemes follow three main phases: data acquisition phase, rule application phase and intrusion detection phase. In [7], it is proposed two approaches to improve the security of clusters for sensor networks using IDS. The first approach uses a model-based on authentication, and the second scheme is called Energy-Saving. IN [8] a hybrid intrusion detection system (HIDS) model has been anticipated for wireless sensor networks. The paper does not promote a solution. Rather, it is a comparative study of existing model of intrusion detection in wireless sensor networks. The paper aim is to provide a better understanding of the current research issues in this field. The paper [9] focus on detecting intrusion or anomalous behavior of nodes in WLAN’s Using a modular technique. We explore the security vulnerabilities of 802.11, numerous intrusion detection techniques, and different network traffic metrics also called as features. Based on the study of metrics, propose a modular based intrusion detection approach. The intrusion detection is a mechanism for a WSN to detect the existence of improper, inaccurate, or anomalous moving attackers. In the paper [10], consider the issue according to heterogeneous WSN models. Furthermore, consider two sensing detection models: single-sensing detection and multiplesensing detection. [3] SURVEY ON SECURITY ATTACKS AND INTRUSION TYPES 3.1 INTRUDER TYPE: Computer security specialists normally distinguish between internal and external network attacks. This is because intruder profiles, methods of attack and intruder objectives can vary significantly between internal and external attacks. International Journal of Computer Engineering and Applications, Volume XII, Issue I, Jan. 18, www.ijcea.com ISSN 2321-3469 Pradeep Kumar, N. Sreema Iswarya, S. Sharmila Jeyarani 319 3.1.1 External intruder  External intruder attacks can be made against the internal network, using the target’s own computers.  This is often done with the active or passive collusion of the members of the target’s own staff.  However, if the ultimate initiator of the attacks is someone holding no legitimate privileges on the network, then it is considered an external attack.  Attacks where the intruder has no privileges on the target network, and either gains access from outside the network perimeter or by evading or undermining the target’s physical and/or network security measures to achieve some degree of access to the target’s internal network. 3.1.2 Internal intruder  Attacks where the intruder has legitimate privileges on the target network.  Access is obtained using existing privileges, privileges the intruder has extended without permission, or privileges stolen from other users.  To gain access to data and resources to which the intruder is not authorized.  Internal attacks are typically far more common than external ones. 3.2 INTRUSION TYPE Several kinds of IDS technologies exist because of the variance of network configurations. Mainly, there are 3 necessary distinct families of IDS: the categories of IDPS technologies are differentiated primarily by the types of events that they monitor and therefore the ways that during which they're deployed. 3.2.1 Attempted break-in  A Firewall has the task to examine data traffic across borders between networks, and to reject those packets, which do not have a permission for transmission.  Beside attempts to access directly a computer in the protected network, there are also attacks against the Firewall itself, or attempts to outwit a Firewall with falsified data packets. Such break-in attempts are recognized, repelled and logged by the Intrusion Detection system (IDS).  Thereby it can be selected between logging within the device, email notification, SNMP traps or SYSLOG alarms. IDS checks the data traffic for certain properties and detects in this way also new attacks proceeding with conspicuous patterns. 3.2.2 Masquerade  A masquerade attack is an attack that uses a fake identity, such as a network identity, to gain unauthorized access to personal computer information through legitimate access identification.  If an authorization process is not fully protected, it can become extremely vulnerable to a masquerade attack. 3.2.3 Leakage  DoS Attack designed to cause an interruption or suspension of services of a specific host/server by flooding it with large quantities of useless traffic or external communication requests.  When the DoS attack succeeds the server is not able to answer even to legitimate requests any more this can be observed in numbers of ways: slow response of the server, slow network performance, unavailability of software or web page, inability to access data, website or other A REVIEW ON INTRUSION DETECTION AND ITS ANALYSIS Pradeep Kumar, N. Sreema Iswarya, S. Sharmila Jeyarani 320 resources.  Distributed Denial of Service Attack (DDoS) occurs where multiple compromised or infected systems (botnet) flood a particular host with traffic simultaneously. 3.2.4 Phishing attack  This type of attack use social engineering techniques to steal confidential information the most common purpose of such attack targets victim's banking account details and credentials.  Phishing attacks tend to use schemes involving spoofed emails send to users that lead them to malware infected websites designed to appear as real on-line banking websites.  Emails received by users in most cases will look authentic sent from sources known to the user (very often with appropriate company logo and localised information) those emails will contain a direct request to verify some account information, credentials or credit card numbers by following the provided link and confirming the information online.  The request will be accompanied by a threat that the account may become disabled or suspended if the mentioned details are not being verified by the user. 3.3 DETECTION METHODOLOGIES The Attempt The d Information Leak rule deals with signatures from potentdetection method defines the characteristics of analyzer. It is categorized on the basis of information being used by IDS. 3.3.1 Anomaly based detection  The anomaly based detection is based on defining the network behavior.  The network behavior is in accordance with the predefined behavior, then it is accepted or else it triggers the event in the anomaly detection.  The accepted network behavior is prepared or learned by the specifications of the network administrators. 3.3.2 Misuse based detection  Misused based detection involves searching network traffic for a series of malicious bytes or packet sequences.  The main advantage of this technique is that signatures are very easy to develop and understand if we know what network behavior we are trying to identify.  For instance, we might use a signature that looks for part",,2018.0,212602953,,semantic_scholar
19f66130a77d96ab599e51cf67ae00fc2064fe6a,https://www.semanticscholar.org/paper/19f66130a77d96ab599e51cf67ae00fc2064fe6a,A Survey on Cloud Computing,"Cloud computing provides customers the illusion of infinite computing resources which are available from anywhere, anytime, on demand. Computing at such an immense scale requires a framework that can support extremely large datasets housed on clusters of commodity hardware. Two examples of such frameworks are Google’s MapReduce and Microsoft’s Dryad. First we discuss implementation details of these frameworks and drawbacks where future work is required. Next we discuss the challenges of computing at such a large scale. In particular, we focus on the security issues which arise in the cloud: the confidentiality of data, the retrievability and availability of data, and issues surrounding the correctness and confidentiality of computation executing on third party hardware. Today, the most popular applications are Internet services with millions of users. Websites like Google, Yahoo! and Facebook receive millions of clicks daily. This generates terabytes of invaluable data which can be used to improve online advertising strategies and user satisfaction. Real time capturing, storage, and analysis of this data are common needs of all high-end online applications. To address these problems, a number of cloud computing technologies have emerged in last few years. Cloud computing is a style of computing where dynamically scalable and virtualized resources are provided as a service over the Internet. The cloud refers to the datacenter hardware and software that supports a clients needs, often in the form of datastores and remotely hosted applications. These infrastructures enable companies to cut costs by eliminating the need for physical hardware, allowing companies to outsource data and computations on demand. Developers with innovative ideas for Internet services no longer need large capital outlays in hardware to deploy their services; this paradigm shift is transforming the IT industry. The operation of large scale, commodity computer datacenters was the key enabler of cloud computing, as these datacenters take advantage of economies of scale, allowing for decreases in the cost of electricity, bandwidth, operations, and hardware [Armbrust et al. 2009]. It is well known that writing efficient parallel and distributed applications is complex. Google proposed the MapReduce [Dean and Ghemawat 2004] programming framework in 2004 to address this complexity. It allows programmers to specify a map function that processes a key/value pair to generate an intermediate key/value pairs, and a reduce function that merges all the intermediate key/value pairs to produce the required output. Many real world tasks, especially in the domain of search can be expressed in this model. Hadoop 1 is the most popular open source implementation of MapReduce. It has been widely adopted both in academic and industrial users, including at organiza",,2009.0,15081783,,semantic_scholar
60c0f69a4d36f1ad95ae8e2ad217163bcb338ee6,https://www.semanticscholar.org/paper/60c0f69a4d36f1ad95ae8e2ad217163bcb338ee6,Trust and integrity in distributed systems,"In the last decades, we have witnessed an exploding growth of the Internet. The massive adoption of distributed systems on the Internet allows users to offload their computing intensive work to remote servers, e.g. cloud. In this context, distributed systems are pervasively used in a number of difference scenarios, such as web-based services that receive and process data, cloud nodes where company data and processes are executed, and softwarised networks that process packets. In these systems, all the computing entities need to trust each other and co-operate in order to work properly. While the communication channels can be well protected by protocols like TLS or IPsec, the problem lies in the expected behaviour of the remote computing platforms, because they are not under the direct control of end users and do not offer any guarantee that they will behave as agreed. For example, the remote party may use non-legitimate services for its own convenience (e.g. illegally storing received data and routed packets), or the remote system may misbehave due to an attack (e.g. changing deployed services). This is especially important because most of these computing entities need to expose interfaces towards the Internet, which makes them easier to be attacked. Hence, software-based security solutions alone are insufficient to deal with the current scenario of distributed systems. They must be coupled with stronger means such as hardware-assisted protection. In order to allow the nodes in distributed system to trust each other, their integrity must be presented and assessed to predict their behaviour. The remote attestation technique of trusted computing was proposed to specifically deal with the integrity issue of remote entities, e.g. whether the platform is compromised with bootkit attacks or cracked kernel and services. This technique relies on a hardware chip called Trusted Platform Module (TPM), which is available in most business class laptops, desktops and servers. The TPM plays as the hardware root of trust, which provides a special set of capabilities that allows a physical platform to present its integrity state. With a TPM equipped in the motherboard, the remote attestation is the procedure that a physical node provides hardware-based proof of the software components loaded in this platform, which can be evaluated by other entities to conclude its integrity state. Thanks to the hardware TPM, the remote attestation procedure is resistant to software attacks. However, even though the availability of this chip is high, its actual usage is low. The major reason is that trusted computing has very little flexibility, since its goal is to provide strong integrity guarantees. For instance, remote attestation result is positive if and only if the software components loaded in the platform are expected and loaded in a specific order, which limits its applicability in real-world scenarios. For such reasons, this technique is especially hard to be applied on software services running in application layer, that are loaded in random order and constantly updated. Because of this, current remote attestation techniques provide incomplete solution. They only focus on the boot phase of physical platforms but not on the services, not to mention the services running in virtual instances. This work first proposes a new remote attestation framework with the capability of presenting and evaluating the integrity state not only of the boot phase of physical platforms but also of software services at load time, e.g. whether the software is legitimate or not. The framework allows users to know and understand the integrity state of the whole life cycle of the services they are interacting with, thus the users can make informed decision whether to send their data or trust the received results. Second, based on the remote attestation framework this thesis proposes a method to bind the identity of secure channel endpoint to a specific physical platform and its integrity state. Secure channels are extensively adopted in distributed systems to protect data transmitted from one platform to another. However, they do not convey any information about the integrity state of the platform or the service that generates and receives this data, which leaves ample space for various attacks. With the binding of the secure channel endpoint and the hardware TPM, users are protected from relay attacks (with hardware-based identity) and malicious or cracked platform and software (with remote attestation). Third, with the help of the remote attestation framework, this thesis introduces a new method to include the integrity state of software services running in virtual containers in the evidence generated by the hardware TPM. This solution is especially important for softwarised network environments. Softwarised network was proposed to provide dynamic and flexible network deployment which is an ever complex task nowadays. Its main idea is to switch hardware appliances to softwarised network functions running inside virtual instances, that are full-fledged computational systems and accessible from the Internet, thus their integrity is at stake. Unfortunately, currently remote attestation work is not able to provide hardware-based integrity evidence for software services running inside virtual instances, because the direct link between the internal of virtual instances and hardware root of trust is missing. With the solution proposed in this thesis, the integrity state of the softwarised network functions running in virtual containers can be presented and evaluated with hardware-based evidence, implying the integrity of the whole softwarised network. The proposed remote attestation framework, trusted channel and trusted softwarised network are implemented in separate working prototypes. Their performance was evaluated and proved to be excellent, allowing them to be applied in real-world scenarios. Moreover, the implementation also exposes various APIs to simplify future integration with different management platforms, such as OpenStack and OpenMANO.",,2017.0,116120592,10.6092/POLITO/PORTO/2676918,semantic_scholar
04dc3ef332c7f83e66f50653945e621d4bf403b1,https://www.semanticscholar.org/paper/04dc3ef332c7f83e66f50653945e621d4bf403b1,Simulation of Hierarchical Storage Systems for TCO and QoS,"Due to the variety of storage technologies deep storage hierarchies turn out to be the most feasible choice to meet performance and cost requirements when handling vast amounts of data. Long-term archives employed by scientific users are mainly reliant on tape storage, as it remains the most cost-efficient option. Archival systems are often loosely integrated into the HPC storage infrastructure. In expectation of exascale systems and in situ analysis also burst buffers will require integration with the archive. Exploring new strategies and developing open software for tape systems is a hurdle due to the lack of affordable storage silos and availability outside of large organizations and due to increased wariness requirements when dealing with ultra-durable data. Lessening these problems by providing virtual storage silos should enable community-driven innovation and enable site operators to add features where they see fit while being able to verify strategies before deploying on production systems. Different models for the individual components in tape systems are developed. The models are then implemented in a prototype simulation using discrete event simulation. The work shows that the simulations can be used to approximate the behavior of tape systems deployed in the real world and to conduct experiments without requiring a physical tape system.",ISC Workshops,2017.0,30316798,10.1007/978-3-319-67630-2_12,semantic_scholar
d86e9c9f339bf976eb4e4ee882aee3f5fb5858ba,https://www.semanticscholar.org/paper/d86e9c9f339bf976eb4e4ee882aee3f5fb5858ba,Using physical layer emulation to understand and improve wireless networks,"Researchers and developers have long faced a fundamental tension between the experimental realism of wireless testbeds on one hand, and the control and repeatability of simulation on the other hand. This thesis introduces physical layer wireless net work emulation---a new approach to wireless network experimentation that balances the stark tradeoff of traditional alternatives by enabling both realistic and repeatable experimentation. 
The design and implementation of a functional wireless emulator are presented along with a discussion of how this implementation overcomes the challenges necessary to meet operational requirements. In particular, solutions to the problems of developing a hardware architecture for emulation, and software control of that architecture will be presented. 
To illustrate the power of physical layer wireless network emulation, case studies are presented. First, physical layer emulation is used to analyze several aspects of wireless LAN link-level behavior. Physical layer emulation is then used to investigate wireless LAN access point selection performance, and to develop improvements. 
This thesis shows that---compared to traditional approaches---physical layer wireless network emulation provides a better understanding of real-world wireless network performance, shortens the development cycle of wireless networking software, and facilitates the deployment of research into operational wireless networks without sacrificing a controlled experimental environment.",,2006.0,107864466,,semantic_scholar
877faa6486db570bdbe7aa24d5b40cac6017843d,https://www.semanticscholar.org/paper/877faa6486db570bdbe7aa24d5b40cac6017843d,Software and System Engineering for Cyber-Physical Systems: technical challenges and collaboration opportunities,"of presentations Holger Pfeifer (FORTISS) – The European Smart Anything Everywhere initiative and funding opportunities by CPSE-Labs experiments The European ‘Smart Anything Everywhere’ (SAE) initiative supports the innovation on smart digital systems thanks to networks of competence centres. The ecosystems built under these initiatives are based on collaboration between researchers, large industries and SMEs which will help to transfer knowledge and resources available to a much wider group of companies. SMEs and middle size companies can experiment with new technologies, try them out in their processes and work together with the suppliers of the technology to adapt it to their specific needs. CPSELabs is one SAE innovation action which provides an open forum for sharing platforms, architectures and SW tools for the engineering of dependable and trustworthy CPS. It provides funding for focussed experiments (36 partners) and fast-track (12-18 months) with innovation objective. Next call for experiment will be published in Spring 2016 http://www.cpse-labs.eu/calls.php Holger Pfeifer (FORTISS) CPSE-Labs experiments of Germany South centre: Model-driven engineering for industrial automation systems The importance of software in industrial automation is continuously increasing. New approaches to the development and maintenance are needed to cope with the growing complexity of control software for future automation systems. 4DIAC Framework for Distributed Industrial Automation & Control is an open source solution for the programming of programmable logic controllers (PLCs) according to the standard IEC 61499. With this standard it provides higher level modelling means and better support for networked control devices. The main components of 4DIAC are the Eclipse-based integrated development environment 4DIAC-IDE and the real-time capable run-time environment FORTE. Martin Grimheden (KTH) – CPSE-Labs experiments of Sweden centre: Overcoming thresholds for data integration in CPS engineering environments The talk will describe the Kth model-based approach to data integration based on the OSLC interoperability standard. Patrick Leserf (ESTACA) Trade-off analysis with SysML and Papyrus : a drone application Obtaining the set of trade-off architectures from a SysML model is an important objective for the system designer. To achieve this goal, we propose a methodology combining SysML with the variability concept and multiobjectives optimization techniques. An initial SysML model is completed with variability information to show up the different alternatives for component redundancy and selection from a library. The constraints and objective functions are also added to the initial SysML model, with an optimization context. Then a representation of a constraint satisfaction problem (CSP) is generated with an algorithm from the optimization context and solved with an existing solver. The presentation will illustrate our methodology by designing an Embedded Cognitive Safety System (ECSS). From a component repository and redundancy alternatives, the best design alternatives are generated in order to minimize the total cost and maximize the estimated system reliability. Benoît Combemale (IRISA) Using models for a broader engagement in smart systems. Various disciplines use models for different purposes. While engineering models, including software engineering models, are often developed to guide the construction of a non-existent system, scientific models, in contrast, are created to better understand a natural phenomenon (i.e., an already existing system). An engineering model may incorporate scientific models to build a smart system. This talk proposes a vision promoting an approach that synergistically combines engineering and scientific models to enable broader engagement of end users in smart systems, informed decision-making based on more-accessible scientific models and data, and automated feedback to the engineering models to support dynamic adaptation of smart systems. To support this vision, we identify a number of challenges to be addressed with particular emphasis on the socio-technical benefits of modeling. Claire Ingram (Newcastle University) CPSE-Labs experiments of UK centre: Pragmatic techniques for modelbased Engineering of Cyber-Physical Systems Newcastle University's Cyber-Physical Systems Lab conducts research into pragmatic techniques for model-based engineering of Cyber-Physical Systems (CPSs). In this talk I will introduce some platforms supported by Newcastle's CPS Lab, including an approach for co-modelling which allows separate design teams working with discrete-event and continuous-time formalisms to develop CPS designs collaboratively. I will also introduce an experiment which has been funded previously under the CPSE Labs initiative. Michael Siegel (OFFIS) CPSE-Labs experiments of Germany North centre: The Maritime Architecture Framework (MAF) and eMaritime Integrated Reference Platform (eMIR) The Maritime Architecture Framework (MAF) is a stakeholder-oriented CPS architecture framework for existing and future maritime CPS and services. MAF provides the conceptual basis, methods, tools and technologies to define, document, and align existing or future CPS architectures and architectural reference models for e-navigation and e-maritime applications. The MAF is a key enabler in the maritime domain for system harmonization, interoperability and standardization. The MAF is also a reference for the definition and design of testbeds for enavigation and e-maritime systems and services. It helps to define the context, to check completeness and provides a semantic basis to discuss the outcome and results. Additionally it offers a semantic basis for integration of testbeds e.g. for larger demonstrators. To support the development of maritime CPS – i.e. the integration of heterogeneous systems of the maritime transportation space , the Design Centre North provides the eMaritime Integrated Reference Platform (eMIR) for rapid prototyping in simulation environments and testing in real world environments. This talk gives an overview about the background, context and concepts of the MAF and why testbed environments (e.g. eMIR) for the development, integration testing and demonstration for CPS must take into account and support the design aspects of such an architecture framework. Andre Pierre Mattei (SCA-ITA) SysML Design of an observation satellite for agriculture surveillance in Brazil François Fouquet (SnT, Interdisciplinary Centre for Security, Reliability and Trust)Models for managing IoT data Internet of Things applications analyze our past habits through sensor measurements to anticipate future trends. To yield accurate predictions, intelligent systems not only rely on single numerical values, but also on structured models aggregated from different sensors. Computation theory, based on the discretization of observable data into timed events, can easily lead to millions of values. Time series and similar database structures can efficiently index the mere data, but quickly reach computation and storage limits when it comes to structuring and processing IoT data. During this talk, I will present various results presented at Models’15 and SmartGridCom’15 that tackles this complexity by exploiting IoT data characteristics. Notably, I will present a concept of continuous models that can handle high-volatile IoT data by defining a meta type for continuous attributes. In addition to traditional discrete object-oriented modeling APIs, we enable models to represent very large sequences of sensor values by inferring mathematical models that can efficiently replace raw values. We show on various IoT datasets that sequences of polynomials can significantly improve storage and reasoning efficiency. I will present an application of this IoT model extension for suspicious value detection in the SmartGrid domain. We proposed a method to learn and store a profile of “typical” values and their probability in IoT context models. We show that using such profiles together with a contextual checker we can improve suspicious value detection, both in terms of efficiency and effectiveness. Juan Garbajosa (UPM) Experiments of Spain centre: Open CPS platform for building and deploying smart city services Bran Selic (Simula) Modeling uncertainty in cyber-physical systems For the past year, we have been working on a core model of Uncertainty and its application to requirements specification and system testing in the context of the European Commission's H2020 UTEST project. (More information on this project, which involves a number of industrial and research partners from Europe, can be found at: http://certus-sfi.no/u-test-a-horizon-2020-funding-recipient/). Fabien Peureux Fabien Peureux (Femto-st/EGM/Smartesting S&S) Model-Based Testing for Internet of Things and Cyber-Physical Systems Testing Cyber-Physical Systems (CPS) is challenging due to the various uncertainties in their behaviour. The purpose of this talk is to present our ongoing work on a model-based testing framework for automatic generation of executable test cases for CPS in the presence of various uncertainties. Basically, uncertainties can be described as a lack of certainty about the current state or about the future outcome of the system. Within CPS, it can be due to the stimulus and data sent from the user environment to the physical units (application level), to the interactions between the physical units and the network services (infrastructure level), or a combination of the both (integration level). To test such issues, the proposed model-based testing approach is implemented on the EMF framework, and based on the test generation tool Smartesting CertifyIt1. It relies on a UML behavioral model of the system under test, from which abstract test cases are automatically generated by applying dedicated coverage strategies focusing on uncertainty testing. Afterwards, ",,2016.0,222303274,,semantic_scholar
1e4380f2aec7922062899b4becff1c1db53679c7,https://www.semanticscholar.org/paper/1e4380f2aec7922062899b4becff1c1db53679c7,Modeling and Simulation of Tape Libraries for Hierarchical Storage Systems,"The variety of storage technologies results in deep storage hierarchies to be the only feasible choice to meet performance and cost requirements when handling vast amounts of data. Long-term storage systems employed by scientific users are mainly reliant on tape storage, as it remained the most costefficient option. Archival systems are often loosely integrated into the HPC storage infrastructure. With the rise of exascale systems and in situ analysis also burst buffers will require integration with the archive. Exploring new strategies and developing open software for tape systems is a hurdle due to the lack of affordable storage silos and availability outside of large organizations and due to increased wariness requirements when dealing with ultradurable data. Lessening these problems by providing virtual storage silos should enable community-driven innovation, and enable site operators to add features where they see fit while being able to verify strategies before deploying on production systems. Different models for the individual components in tape systems are developed. The models are then implemented in a prototype simulation using discrete event simulation. It is shown that the simulation can be used to approximate the behavior of tape systems deployed in the real world and to conduct experiments without requiring a physical tape system.",,2016.0,30811547,,semantic_scholar
f77d59740dfeb10e9b650ec8b1baba91fca70279,https://www.semanticscholar.org/paper/f77d59740dfeb10e9b650ec8b1baba91fca70279,NaviBeam: Indoor assistance and navigation for shopping malls through projector phones,"We present our concept of an indoor assistance and navigation system for pedestrians that leverages projector phones. Digitally enhanced guides have many advantages over traditional paper-based indoor guides, most of all that they can be aware of their current context and display dynamic information. That is why particularly shopping malls recently started deploying indoor assistance applications for mobile phones, which also include support for navigation. Moreover, as we show in the paper, projected interfaces offer additional distinct advantages over static guides and even traditional or augmented reality mobile applications. We describe five concepts for a shopping mall indoor assistance system based on projector phones, comprising support for shop selection, precise way finding, “virtual fitting” of clothes, and context-aware and ambient advertisements. We then apply the concepts to a typical shopping scenario, where users wear the phone at their belt and constantly project the interface in front of them. Expected benefits of our system are that people find their way quicker, easier, and less distracted from their usual shopping experience. Finally, we discuss the technical feasibility of our envisioned implementation and research questions we are particularly interested in. INTRODUCTION Navigation and location-based services for pedestrians recently gained a lot of attention and are becoming rapidly adopted. Very popular among these are applications for location-based places recommendations and turn-by-turn navigation. While these applications mostly focus on outdoor navigation, less attention has been paid to the opportunities for providing indoor assistance with mobile devices. Especially in large complex buildings, e.g. shopping centers, in most cases static signs, You-Are-Here maps, or paper flyer maps are still the only available navigation assistance for visitors. Preliminary observations and interviews we conducted in nearby shopping centers show that, at least in Germany, available navigational aids are still as insufficient as Levine reported them to be almost 30 years ago [8]. Despite a lot of research projects that explored indoor assistance over the last decade, it was not before the beginning of 2011 that we saw the first mobile shopping applications reaching consumer markets, such as the Sam’s Club mobile application [1], which provides indoor navigation to specific items and/or shops in some selected American shopping malls. Similarly, some shopping centers in Asia introduced mobile AR applications for shopping assistance [2]. In our research group we study future application areas of projector phones, i.e. mobile phones with integrated projectors (see [11] for a detailed survey). We found projector phones to provide some distinguished advantages for indoor navigation assistance, e.g., that interaction can be handsfree and the projection can serve as ambient display, thereby not contradicting the traditional shopping experience. Further that the surrounding world can be directly augmented, freeing the user from mapping between mobile display and real world and that the output space is much larger than on mobile displays. And finally, that bystander can see and attach to the projected output. RELATED WORK We present relevant work dealing with mobile shopping, recommender systems, indoor positioning and navigation. One of the first works on digital mobile shopping assistants has been done by Asthana et al. [3]. They presented main usage scenarios, e.g., telling people where to find certain products or informing them on discounts and special offers. Similar can be recognized in aforementioned mobile shopping applications and as well in recently filed patents, e.g. from Apple Inc. (US 2010/0198626 A1, US 2010/0191578 A1), which include navigation, service interactions (e.g., parking tickets), and support for social networking. Yang et al. [15] developed a location-aware recommender system. It learns a customer’s interests from previously visited product websites and continuously presents a list of products around the customer’s current location, that are likely to interest him. The software also takes into account the distance to shops and is able to learn customer’s preference between highly interesting products and far distances. Butz et al. [4] present a hybrid indoor navigation system that is able to adapt route instructions to different output devices (screen resolution, device resources) and based on the precision of available location information. Results from [7,14] indicate that intelligent fusion of infrastructure techniques, e.g. GPS, GSM triangulation, and sensors like accelerometers, magnetometers, and gyroscopes, enables precise indoor location tracking with current commercially available smartphones in unaltered environments. Kray et al. [6] explore the design space of routing instructions for pedestrians on mobile devices. Narzt et al. present a specific mobile application for augmented reality (AR) [10]. Alternative systems for pedestrian navigation are the Rotating Compass by Rukzio et al. [12], showing personalised navigation information on public displays and the CabBoots system by Frey [5], which guides users by means of tactile output in the shoes. Aforementioned techniques, with the exception of the last two, rely on holding a handset device. However, we feel strongly inclined that holding a device in hand for a longer time does not fit well the traditional shopping experience. Negative side effects, e.g., arm fatigue, regular switching of the field of view, do not allow using the shopping assistance application as constant companion. CONCEPT In our envisioned prototype, people wear their projector phone on their belt, projecting a display right in front of their feet (Figure 1). In the following we present five concepts for mobile shopping assistance that are enabled by projector phones. Later we apply these concepts to a typical shopping scenario. Shadow Interaction Since our concept expects people to wear the projector fixed to the belt most of the time, direct interaction with the mobile device would not be sufficient as the only interaction technique. Audio is not an alternative because of the noisy environment in a shopping mall. This leaves users with the option to directly interact with the projection, either by feet or hands (or gaze in the future). In preliminary studies we discovered that foot-based interaction is not well suited due to the fact that foot movement inherently involves movement of the body at the same time, which makes interaction cumbersome. We found interacting with the shadow of a finger in front of the projection a much more promising solution. Figure 2 shows the stroke of the resulting shadow on the projection. With the tip of the shadow, all points on the projection can be reached. Items should be highlighted once the shadow reaches their bounding box to give adequate feedback to the user. Although the tracking of the finger’s height to enable traditional press/release behaviors would be possible, this would require the user to maintain a complex mental model of different finger height levels. Instead, our concept builds on the fact, that the tip of the index finger can be moved well without changing the shape of the rest of the hand or even the middle knuckle of the index finger. Thus, to select an item, a user moves the index finger to point on the desired item and then bends the index finger and unbends it again. Alternatively, one could use the finger’s dwell time as in Microsoft’s Kinect. To the best of our knowledge, shadow interaction with projections has not been reported before. Radar of Recommendations Another concept that is particularly useful with projected navigation is a radar of recommendations. Building on [15], we want to constantly show and update a personal radar of products the user might be interested in (Figure 2). Based on the information available from accounts with online stores (e.g., Amazon) and items recently explored with our system (see fourth concept), users see offers of nearby stores in front of them and can select these items to start a navigation in the middle of the circle. The size of an item conveys the expected interest of the user, the distance from the middle depicts the walking distance (not air path) from the user’s current location. The size of the radius of interest (distance to shops and items) can be adjusted by slightly changing the angle of the projector, similarly to looking further ahead. Different from [15], the projection provides a much larger output space and the radar serves as ambient display in the user’s periphery. Projected (augmented) Navigation When the user selected an item or shop he is interested in, the assistance system starts a projected navigation. In outdoor navigation, turn-by-turn navigation is still the most prominent, though we have seen augmented reality been used in research [10]. Especially for indoor navigation, where there are more tight and subsequent turns or small decision spaces (take the left stairs up, not the right stairs down) our experience is that turn-by-turn navigation does not work well. Therefore, we want the user not to follow Figure 1: The projector is worn at different locations on the belt (left and middle) and can optionally be taken into hand to change the angle of the projection (right). Figure 2: The user interacts with his radar of recommendations through the shadow of his finger. The orange outline shows the shadow, the left red circle the fingertip that, for clicking, can be changed independently of the finger’s middle knuckle (right circle) or the rest of the hand’s shape. this type of directions, but instead simply follow a blue line projected in front of her (see Figure 3). Since the projector phone is spatially-aware, both in terms of location and orientation, the blue line is projected as a static augmentation of the real world, i",,2011.0,216619517,,semantic_scholar
30f346032407b43e811107a46bf07a38f30527d7,https://www.semanticscholar.org/paper/30f346032407b43e811107a46bf07a38f30527d7,The Build Master: Microsoft's Software Configuration Management Best Practices,"""Wow, what can I say? Chapter 4, 'The Build Lab and Personnel,' by itself is enough justification to purchase the book! Vince is obviously a 'Dirty Finger Nails' build meister and there is a lot we can all learn from how he got them dirty! There are so many gems of wisdom throughout this book it's hard to know where to start describing them! It starts where SCM should start, at the end, and works its way forward. This book is a perfect complement to the 'Follow the Files' approach to SCM that I espouse. I will recommend that every software lead and software configuration management person I work with be required to read this book!""-Bob Ventimiglia, autonomic logistics software configuration manager, Lockheed Martin Aeronautics""The Build Master contains some truly new information; most of the chapters discuss points that many people in the industry don't have a full understanding of and need to know. It's written in a way that is easy to read and will help a reader fill holes in their vision regarding software build management. I especially liked Vince's use of Microsoft stories to make his points throughout the book. I will purchase the book and make certain chapters mandatory reading for my build manager consultants.""-Steve Konieczka, SCM consultant""Vince does a great job of providing the details of an actual working build process. It can be very useful for those who must tackle this task within their own organization. Also the 'Microsoft Notes' found throughout the book provide a very keen insight into the workings of Microsoft. This alone is worth purchasing this book.""-Mario E. Moreira, author of Software Configuration Management Implementation Roadmap and columnist at CM Crossroads""Software configuration management professionals will find this book presents practical ideas for managing code throughout the software development and deployment lifecycles. Drawing on lessons learned, the author provides real-world examples and solutions to help you avoid the traps and pitfalls common in today's environments that require advanced and elegant software controls.""-Sean W. Sides, senior technical configuration manager, Great-West Healthcare Information Systems""If you think compiling your application is a build process, then this book is for you. Vince gives us a real look at the build process. With his extensive experience in the area at Microsoft, a reader will get a look in at the Microsoft machine and also how a mature build process should work. This is a must read for anyone doing serious software development.""-Jon Box, Microsoft regional director, ProTech Systems Group""Did you ever wonder how Microsoft manages to ship increasingly complex software? In The Build Master, specialist Vince Maraia provides an insider's look.""-Bernard Vander Beken, software developer, jawn.net""This book offers an interesting look into how Microsoft manages internal development of large projects and provides excellent insight into the kinds of build/SCM things you can do for your large-scale projects.""-Lance Johnston, vice president of Software Development, SCM Labs, Inc.""The Build Master provides an interesting insight into how large software systems are built at Microsoft covering the set up of their build labs and the current and future tools used. The sections on security, globalization, and versioning were quite helpful as these areas tend to be overlooked.""-Chris Brown, ThoughtWorks, consultant""The Build Master is a great read. Managing builds is crucial to the profitable delivery of high-quality software. Until now, the build process has been one of the least-understood stages of the entire development lifecycle. This book helps you implement a smoother, faster, more effective build process and use it to deliver better software.""-Robert J. Shimonski, Networking and Security Expert, http://www.rsnetworks.netThe first best-practice, start-to-finish guide for the software build processManaging builds is crucial to the profitable delivery of high-quality software; however, the build process has been one of the least-understood stages of the entire development lifecycle. Now, one of Microsoft's leading software build experts introduces step-by-step best practices for maximizing the reliability, effectiveness, timeliness, quality, and security of every build you create.Drawing on his extensive experience working with Microsoft's enterprise and development customers, Vincent Maraia covers all facets of the build process-introducing techniques that will work on any platform, on projects of any size. Maraia places software builds in context, showing how they integrate with configuration management, setup, and even customer support. Coverage includes How Microsoft manages builds: process flows, check-in windows, reporting status, and more Understanding developer and project builds, pre- and post-build steps, clean builds, incremental builds, continuous integration builds, and more Choosing the right build tools for your projects Configuring source trees and establishing your build environment-introducing Virtual Build Labs (VBLs) Planning builds for multiple-site development projects or teams Determining what should (and shouldn't) be kept under source control Managing versioning, including build, file, and .NET assembly versions Using automation as effectively as possible Securing builds: a four layer approach-physical, tracking sources, binary/release bits assurance, and beyondBuilds powerfully impact every software professional: developers, architects, managers, project leaders, configuration specialists, testers, and release managers. Whatever your role, this book will help you implement a smoother, faster, more effective build process-and use it to deliver better software.© Copyright Pearson Education. All rights reserved.",,2005.0,107761717,,semantic_scholar
3c8b0f17ca1ad55900a1bc35f7e588db15e6ec8d,https://www.semanticscholar.org/paper/3c8b0f17ca1ad55900a1bc35f7e588db15e6ec8d,Improving QoS for large-scale WSNs,"The advancements in information and communication technologies have been triggering an increase in miniaturization and ubiquity, paving the way towards new paradigms in embedded computing systems. Modern embedded systems are enabling a number of smaller, smarter and ubiquitous devices, creating an eagerness for monitoring and controlling everything, everywhere. These facts are pushing forward the design of new Wireless Sensor Network (WSN) infrastructures that will tightly interact with the physical environment, in a ubiquitous and pervasive fashion. However, such cyber-physical systems require a rethinking of the usual computing and networking concepts, and given that these computing entities closely interact with their environment, timeliness is of increasing importance. Nevertheless, many other QoS properties such as scalability, energyefficiency and robustness must also be addressed if these infrastructures are to become a reality. This Thesis addresses the use of standard protocols, particularly IEEE 802.15.4 and ZigBee, combined with commercial technologies as a baseline to enable WSN infrastructures capable of supporting the QoS requirements that future large-scale networked embedded systems will impose. Hence, several architectural solutions (mechanisms, algorithms, protocol add-ons) are hereby proposed to address some of the most prominent QoS challenges, such as timeliness, scalability, robustness and energy-efficiency. Importantly, in order to clearly identify the most prominent QoS challenges and to provide effective QoS solutions with close contact with reality, a hands-on approach is followed throughout this Thesis. Hence, we rely upon two real-world application scenarios (i.e. a Datacentre Monitoring (DM) scenario and a Structural Health Monitoring (SHM) scenario), which were engineered, implemented and deployed in the course of this work, to validate and demonstrate this Thesis’ QoS proposals. This strategy enables a deeper understanding of these infrastructures at a more practical level, and provides the proposals with a real-world application context, showing that these network infrastructures have the potential to be used in real-world cyber-physical applications in the near future, if provided with the necessary QoS management mechanisms. Among the proposals, concerning timeliness, for instance, ZigBee cluster-tree topologies are known for a lack of flexibility in adapting to changes in the traffic or bandwidth requirements at runtime, making these infrastructures not capable of allocating more bandwidth to a set of nodes sensing a particular phenomena, or reducing the latency of a data stream. This Thesis proposes a way of dynamically addressing this problem via a mechanism to re-schedule the clusters’ active periods. Concerning the MAC sub-layer of the IEEE 802.15.4 protocol, in this Thesis we carry out an experimental evaluation of a traffic differentiation mechanism, providing the support of different traffic classes to the legacy protocol. This mechanism is also extended to support intra-cluster communications. In addition to timeliness, this mechanism provides and improvement in terms of energy-efficiency. The IEEE 802.15.4 Guaranteed Time Slot mechanism, missing from most stack implementations, is also",,2015.0,111616016,,semantic_scholar
92affd2809bedc05661178892f7b063a9cbcfcb3,https://www.semanticscholar.org/paper/92affd2809bedc05661178892f7b063a9cbcfcb3,A MODULAR OPEN SYSTEM ARCHITECTURE STRATEGY FOR ROBOTICS AND AUTONOMOUS SYSTEMS,"This paper describes the results of a study funded by the National Advanced Mobility Consortium (NAMC) to develop a strategy for establishing interoperability as the norm in military ground robotic and autonomous system (RAS) programs. It briefly provides background explaining the current practices and the reason the study was conducted. It outlines the types of interoperability targeted in ground RAS programs, and describes the findings of a survey of current efforts aimed at creating interoperability through a modular open system architecture approach. It recommends a path forward for creating interoperability in military ground RAS program based on maturing and propagating the ground robotics interoperability profile (IOP) currently being developed and matured at Project Manager, Force Projection (PM FP). Finally it lays out specific steps to be taken and proposes that responsibility for IOP be transitioned to a consortium-style organization as it progresses through an “Iteration and Maturation” phase over the next 3-5 years towards its eventual adoption by an enduring standards body. The views expressed in this paper do not constitute official Department of Defense policy. INTRODUCTION This is a crucial time for the future of Robotic and Autonomous Systems (RAS) in general and Robotic and Autonomous Ground Vehicle Systems (“RA-GVS”) in particular. Historically, the vast majority of military ground robotics systems have been procured to support immediate operational needs. This has resulted in a number of built-topurpose, tightly integrated systems that have proven their operational utility and become indispensable. However, as current conflicts wind down and budgets shrink, maintaining and adding new capabilities to such built-to-purpose ground robotic systems will prove an expensive and difficult proposition. The difficulty in tightly integrated, built-topurpose systems is that they are costly to maintain and extend, and it is not possible to select and integrate “best of breed” components. Moving forward, the community must lessen the life-cycle costs, shorten the technology update cycles, and enable operational flexibility. In the case of autonomous systems, science and technology (S&T) projects have proven the technological feasibility and potential benefits of autonomy to a range of applications, from automated convoys, to fully self-driving vehicles. Successful transition of autonomous systems into real-world deployment will require development of well-understood, modular packages that can be integrated inexpensively with legacy platforms and worked into military operational processes. The continued development of tightly integrated, built-to-purpose autonomous systems is not a tenable approach, and delays the transition of these technologies. Proceedings of the 2015 Ground Vehicle Systems Engineering and Technology Symposium (GVSETS) A Modular Open System Architecture Strategy for Robotics and Autonomous Systems, Moore, et al. Page 2 of 17 To mitigate these difficulties, and to move to the next level in deployment of robotic and autonomous systems, the community of interest (COI) must proactively develop and embrace a modular open systems architecture (MOSA) for robotics and autonomous systems. A MOSA will provide a shared architectural framework and a set of interface standards, and will promote modularity, commonality, and interoperability between sub-systems and components. A MOSA for RA-GVS will enable an environment of competition and innovation in the community, and “grease the skids” for developing, integrating, deploying, and maintaining a wide variety of interoperable, robotic, and autonomous ground vehicles and platforms. In 2010, the Army and Marine Corps ground vehicle community initiated the process of creating a MOSA for RA-GVS when a group operating out of the then Robotic Systems-Joint Program Office (RS-JPO) commenced work on developing a collection of Interoperability Profiles (IOP) for unmanned ground vehicles. IOP is intended to provide program managers (PMs), and eventually others, with a standardized library of physical, electrical, and logical (messaging) interfaces, and a common set of supporting documentation and materials that they can use to define a common interoperability profile, or “instantiation”, specific to a certain robotic vehicle or platform. The instantiation specifies which interfaces and interoperability attributes, from among those defined in the overarching IOP, are to be implemented on a particular RAS. Over the past 5 years, work has focused on developing initial versions of the IOP and evaluating the technical feasibility of utilizing such an open interface standard without sacrificing operational performance. In the meantime, responsibility for IOP has transitioned to an IOP group operating under the Project Manager, Force Projection organization, under the Army’s Program Executive Office for Combat Support and Combat Service Support (“PEO CS&CSS”). With the pending release of IOP Version 2.0, the Government will have completed the Initial Development stage, which has resulted in a well-defined set of IOP documents and initial demonstration of technically sound underpinnings for a RAS MOSA. Definitions This section provides a set of definitions for several terms used throughout the remainder of the text. Note that many of these are used ambiguously within unmanned systems or focus on an alternate interpretation across different domains. The purpose here is not to provide a complete definitions document, but rather to focus on specific terms that may cause confusion. • Open Architecture (or Open Systems Architecture): “a type of computer hardware or software architecture [...] that allows adding, upgrading, modifying, and swapping components. [It provides] a varied combination of interoperability, portability, and open software standards.” [1] • Open Standard: “standards made available to the general public and are developed (or approved) and maintained via a collaborative and consensus driven process. [They] facilitate interoperability and data exchange among different products or services and are intended for widespread adoption.” [2] Note that for purposes of this document, a standard will still be considered “open” if a small fee is required to obtain it, such as Society and Automotive Engineering (SAE) documents. • Interface specification: a complete, unambiguous, and testable description of an interface. In robotics, this may include physical (mounting points, dimensions, weight), electrical (voltage, current), and logical (software, communication bus) interfaces. • Interface: “a point where two systems, sub-systems, components, subjects, organizations, etc., meet and interact”. [3] • Modular: “having parts that can be connected or combined in different ways” [4] • Interoperability: “the predictable performance of a capability across an interface through compliance to a selected set of specifications” [5] • Platform: the base vehicle or mobility chassis of a robotic system. • Payload: “a device carried by a [platform], usually in a bay or attached to a hardpoint” [5] • End Effector: “last link of a manipulator, often modular to accept various tools or instruments” • Controller (or Operator Control Unit): “A hardware and/or software interface that allows a human to command or monitor one or more unmanned systems.” • Appliqué: “The augmentation of a manned vehicle such that it can be semi-autonomously controlled.” [6] While most acronyms within this paper are defined within the paragraph of their specific use, others are used extensively throughout the document. A partial reference list is provided here for convenience: • RAS : Robotics and Autonomous System • UGV : Unmanned Ground Vehicle • IOP : Interoperability Profiles • JAUS: Joint Architecture for Unmanned Systems • AEODRS: Advanced EOD Robotic System • NAMC: National Advanced Mobility Consortium • ISR: Intelligence, Surveillance, and Reconnaissance • MOSA: Modular Open Systems Architecture Proceedings of the 2015 Ground Vehicle Systems Engineering and Technology Symposium (GVSETS) A Modular Open System Architecture Strategy for Robotics and Autonomous Systems, Moore, et al. Page 3 of 17 GROUND ROBOTICS AND AUTONOMOUS SYSTEMS INTEROPERABILITY The Architecture Framework for Unmanned Systems defines interoperability as “the predictable performance of a capability across an interface through compliance to a selected set of specifications” [5]. Therefore, to promote interoperability, an acquisition process must determine the appropriate set of specifications to define the physical, electrical, and logical interfaces between two or more entities. Equally important, however, is that the specific goal of interoperability must be determined. While interoperability generally leads to lower maintenance costs, longer lifespan, and an open marketplace for third party suppliers, specific use cases can be considered as representative types of interoperability. These use cases reflect the boundaries between unmanned systems components for which interfaces must be defined. In-Field Swap of Vehicle Payloads A common goal of interoperability is the ability to quickly switch out payloads on a robotic platform in the field. For the purposes of this document, “in the field” is considered to be any location in which full engineering and technical services are not available, but swapping may still require a power cycle or minor configuration changes performed by the vehicle operator. This type of interoperability may include functionally equivalent payloads from different manufacturers, or switching to a payload with a different mission function. In all cases, the payloads must have welldefined physical, electrical, and logical interfaces. It is important to note, however, that compliance to well-defined interfaces may still not be sufficient for interoperability. A large, high capacity manipulator arm may be too hea",,2015.0,64627754,,semantic_scholar
60ad8f69b0d150763afe9fde95ff61c475ba6acf,https://www.semanticscholar.org/paper/60ad8f69b0d150763afe9fde95ff61c475ba6acf,A Study of 802 . 11 Bitrate Selection in Linux,"This paper investigates rate adaptation in 802.11 wireless networks, with a focus on algorithms currently available in the Linux operating system. The algorithms are compared with a simple rate adaptation algorithm from the literature, and modifications are presented that increase the performance of th e existing routines in the studied scenarios. In order to compare simulated results with physical results, and to leverage the Linux software ecosystem, a new software simulator based on a virtual 802.11 device is presented. I. I NTRODUCTION In 802.11 wireless networks, data may be transmitted with any of a number of rates, from low-speed bitrates that are resilient under poor channel conditions, to high-speed rat es that require a high signal level to function. The process of a utomatically selecting the rate that maximizes throughput, given the current channel conditions, is known as r te adaptation[1] and has been studied extensively. The first published rate adaptation algorithm, Auto-Rate Fallback (ARF), is an extremely simple state machine that predicts the rate based on the most recent successful rate. SampleRate [2] is a popular algorithm that builds a statisti cal model of rates based on frame success rate and computed throughput. These two algorithms form the basis for others examined later in the paper. Other algorithms attempt to predict the rate based on direct measurements of the signal level at either the transmitter o r receiver [3], [4]. Unfortunately, these solutions often re quire changes to the MAC layer, or expensive low-bitrate broadcas t packets. Recently, loss differentiation has emerged as a promising improvement to frame-loss based algorithms, particularly in congested networks. As these also usually require changes t o the 802.11 specifications [5], [6], or modifications to physi cal hardware [7], uptake of these algorithms in deployed system has been slow. Consequently, it is instructive to study the algorithms currently in wide use in 802.11 LANs. Simulation of rate algorithms has typically been performed using network simulators, such as ns2, originally develope d for wired networks. While these systems work well for comparing different algorithms under controlled circumstances, by t heir nature it is difficult to compare experimental results with real-world trials. Moreover, simulations from the literat ure often fail to account for cross-layer effects that would imp act practical implementations, such as routing delays, and TCP timeouts. One observation is that a simulator may account for crosslayer effects implicitly, by directly using the networking stack of the operating system. One prior attempt to bridge the gap between research simulators and deployed systems is given in [8]. The authors present the library libmac, which allows experimenters to capture and inject frames using modified 802.11 device drivers. This system utilizes physical radio s for packet collection and transport. For simulation purposes, it would be advantageous to instead use virtual radios and mode l the medium. Thus, this paper introduces a simulator based on a virtualized 802.11 device driver, using the Linux Mac80211 [9] wireless stack. In addition to capturing cross-layer effec ts, the proposed simulator provides the ability to directly compar e experiments utilizing virtual devices with those from phys ical devices. This simulator is then used in an investigation of r ate adaptation algorithms used in the Linux operating system. Section III describes the assumptions made about the network and typical hardware devices. In section IV, the rate lgorithms are briefly described. Section V formulates the channel models used in simulation. In section VI, a new 802.11 simulator that utilizes the Linux wireless stack is presented. In section VII, the rate algorithms are compared both in the simulator and in real world experiments. Finally , in section VIII, modifications to the Minstrel algorithm are proposed. II. D IFFERENCES FROMPREVIOUS WORK In this paper, three rate adaptation algorithms are examine d: Minstrel, PID, and AARF [10]. AARF has been presented and reviewed in the literature, as has SampleRate [11], the pred ecessor of Minstrel. Yet, the author is unaware of published comparisons of Minstrel and PID, the two rate adaptation algorithms presently available in the Linux kernel 2.6.32. Simulations of rate adaptation algorithms have previously been carried out in network simulators with the same or similar channel models as those used in this work. The cross-layer accuracy of such simulations relies in some par t on the accuracy of models of other network layers. A new simulator is introduced that models only the 802.11 device and wireless medium while using the existing infrastructur e for the remaining layers. The virtual wireless device driver mac80211_hwsim existed prior to this project for testing Mac80211. In its more limited role as an API testing tool, the driver performed onl y TABLE I: 802.11a Rate Set Rate (Mbps) Modulation Coding Rate Bits per OFDM symbol 6 BPSK 1/2 48 9 BPSK 3/4 48 12 QPSK 1/2 96 18 QPSK 3/4 96 24 16-QAM 1/2 192 36 16-QAM 3/4 192 48 64-QAM 2/3 288 54 64-QAM 3/4 288 basic operations and did not attempt to simulate the wireles s medium. This kernel driver was rewritten to pass frames to user programs to ease development of the channel simulator. III. N ETWORK MODEL For this paper, the 802.11a PHY is used as the basis for experimentation. The newest standard, 802.11n, has recent ly been approved and provides 32 additional rates; however, th Linux rate adaptation API for 802.11n rates is still evolvin g at this time. The 802.11a rate set (Table I) is still in use as part of 802.11g, and provides a variety of speeds. In addition, this paper is primarily interested in applicat ons to small infrastructure networks. In ad-hoc and mesh system , both the range of the network and number of nodes is often large. As a result, rates that work over long distances may be preferred to high throughput, short range rates. Also, in large networks, hidden terminals are common, leading to the frequent use of low-bitrate RTS/CTS protection. A trend in consumer-oriented 802.11 hardware is the increasing use of so-called soft-MAC designs: devices consis ti g primarily of radios and small embedded CPUs where most of the 802.11 MAC Layer Management Entity (MLME) features are performed off-chip by the host computer. These designs a re low cost and have the advantage of being software-updateabl e. Such designs often omit explicit rate control features, rel ying on the host to provide a rate or a set of candidate rates for a frame. A typical design is the Atheros 5212, in which each frame is accompanied by a multi-rate retry (MRR) descriptor . The descriptor consists of four candidate rates, r1, r2, r3, r4, along with a set of retry counts, c1, c2, c3, c4. The device will attempt to transmit a frame c1 times at rater1, thenc2 times at r2, etc., until the retry counts are exhausted or until an ACK is received. The simulator assumes a similar design. IV. RATE ALGORITHMS ARF [12] is among the earliest developed automatic rate selection algorithms. In ARF, if packets are transmitted su ccessfully a fixed number of times, then the rate is raised. If there is a frame loss immediately after a rate change, or if there are two consecutive failures, the rate is lowered. Ada ptive Auto-Rate Fallback (AARF) [10] utilizes the basic results of ARF, but adds the notion of an exponentially increasing threshold for raising the rate. This is intended to correct the observed problem that the periodic failed transmission attempts at higher rates led to decreased overall throughpu t. Minstrel, based on [2], takes a probabilistic approach. Ten percent of sent frames include a random probe rate as the first rate in the MRR chain. Success at each rate is recorded as packets are sent. Every 100 ms, the probabilities of success and computed throughput are updated for all packets, and the se are combined with previous results using an exponentially weighted moving average. The MRR descriptor includes the two best throughputs followed by the best probability rate, then followed by the lowest available rate. The MRR retry counts are selected such that transmissions at a given rate f or all attempts should take no more than 6 ms, and the entire transmission takes less than 24 ms. PID is based on the concept of the proportional-integralderivative feedback controller [13]. The algorithm adjust s the transmission rate to achieve a maximum of 14% transmission failures. Every 125 ms, the controller recomputes the avera ge number of failed transmissions with an exponentially weigh ted moving average. If a large amount of frame loss is detected, the controller can enter a sharpening mode, in which large adjustments to the rate can be made to more quickly approach the targeted success percentage.",,2012.0,4992831,,semantic_scholar
5a9a330290d3a3bd2d8593ec38c5e4d0fdceb1c1,https://www.semanticscholar.org/paper/5a9a330290d3a3bd2d8593ec38c5e4d0fdceb1c1,3rd Workshop on Smart Surveillance System Applications,"Motivation and Justification: 
 
Automatic recognition of people and their activities has very important social implications, because it is related to the extremely sensitive topic of civil liberties. Society needs to address this issue of automatic recognition and find a balanced solution that is able to meet its various needs and concerns. In the post 9/11 period, population security and safety considerations have given rise to research needs for identification of threatening human activities and emotional behaviours. 
 
Timely identification of human intent is one of the most challenging areas of ""all-hazards"" risk assessment in the protection of critical infrastructure, business continuity planning and community safety. The ""all-hazards"" approach is used extensively by the public and private sector, including Public Safety Canada (PS Canada -- formerly PSEPC), Emergency Management Ontario (EMO), US Federal Emergency Management Agency (FEMA) and US Department of Homeland Security (DHS). 
 
There is a clear need for industry and the research community to addresses fundamental issues involved in the prevention of human-made disasters, namely the variable context-dependent, real-time detection/identification of potential threatening behaviour of humans, acting individually or in crowded environments. 
 
Such an industry and academia forum will have to discuss development and commercialization of new multimodal (video and infrared, voice and sound, RFID and perimeter intrusion) intelligent sensor technologies for location and socio-cultural context-aware security risk assessment and decision support in human-crowd surveillance applications in environments such as school campuses, hospitals, shopping centers, subways or railway stations, airports, sports and artistic arenas etc. Due to the complexity of the surveillance task there is a clear need for the development of a distributed intelligent surveillance system architecture, which combines visual and audio surveillance based on wireless sensor nodes equipped with video or infrared (IR) cameras, audio detectors, or other object detection and motion sensors with location aware wireless sensor network solutions. The integration of visual, sound and radio tracking methods results in a highly intelligent, proactive, and adaptive surveillance and security solution sensor networks. Task-directed sensor data collection and observation planning algorithms need to be developed to allow for a more elastic and efficient use of the inherently limited sensing and processing capabilities. Each task a sensor has to carry out determines the nature and the level of the information that is actually needed. There is a need for ""selective environment perception"" methods that focus on object parameters that are important for the specific decision to be made for the task at hand and avoid wasting effort to process irrelevant data. 
 
Multisensor data fusion techniques should be investigated for the dynamic integration of the multi-thread flow of information provided by the heterogeneous network of surveillance sensors into a coherent multimodal model of the monitored human crowd. 
 
In the context of crowds, robust tracking of people represents an important challenge. The numerous sources of occlusions and the large diversity of interactions that might occur make difficult the long-term tracking of a particular individual over an extended period of time and using a network of sensors. Realtime image processing and computer-vision algorithms need to be studied for the identification, tracking and recognition of gait and other relevant body-language patterns of the human agents who can be deemed of interest for security reasons. Real-time signal processing algorithms have to be designed for the identification and evaluation of environmental and human-subject multimodal parameters (such as human gait, gestures, facial emotions, human voice, background sound, ambient light, etc.) that provide the contextual information for the specific surveillance activity. 
 
A multidisciplinary, context-aware, situation-assessment system, including human behaviour, cognitive psychology, multicultural sociology, learning systems, artificial intelligence, distributed multimedia and software design elements, has to be ultimately developed for the real-time evaluation of the activity and emotional behaviour of the human subjects identified as being potentially of security interest in the monitored dynamic environment. 
 
The development of such a complex system requires the seamless integration of new and improved surveillance techniques and methodologies supporting both functional and non functional requirements for surveillance networks. Functional requirements are signal processing functions and data fusion, archiving and tracking human behaviours, assessment and interpretation functions of the data, and supporting human decision makers, among others. Non-functional requirements include interoperability, scalability, availability, and manageability. 
 
The partial and heterogeneous sensor-views of the environment have to fuse into a coherent Virtualized Reality Environment (VRE) model of the explored environment. Being based on information about real/physical world objects and phenomena, as captured by a variety of sensors, VREs have more ""real content"" than the pure Virtual Reality environments entirely based on computer simulations. The VREs model of the explored environment allows human operators to combine their intrinsic reactive-behavior with higher-order world model representations of the immersive VRE systems. 
 
A synthetic environment will eventually be needed to provide efficient multi granularity-level function-specific feedback and human-computer interaction interfaces for the human users who are the final assessors and decision makers in the specific security monitoring situation. 
 
An ideal system should provide efficient multi granularity-level function-specific feedback for the human users who are the final assessors and decision makers in the specific security monitoring situation. 
 
The rate at which surveillance systems can currently disseminate data to evaluate new threats is mainly limited due to the developed and implemented nature of existing systems and their limited ability to operate with other systems. IBM's Service-Oriented Architecture (SOA) provides the much needed deployment ready solution which supports the integration of external systems developed by diverse industrial and institutional partners.",CASCON,2011.0,39995229,,semantic_scholar
e5587c05c33aff79bdef5c6610778868498feb81,https://www.semanticscholar.org/paper/e5587c05c33aff79bdef5c6610778868498feb81,Theme 7 : Intelligent project lifecycle knowledge management and decision support,"Rapid advances in technologies such as networks, database analysis techniques, and high speed processing helped the growing importance of organizational decision support systems (DSSs). Today's organizations invest in various knowledge management (KM) systems and tools to enable seamless integration of the constantly increasing volume and sources of information. However, with the myriad of commercially available DSSs, organizations are facing the problem of a growing gap between the solely available applications and the business users' requirements for effectively take advantages of their support in the decision making process. To narrow this gap, an integrated and flexible environment is needed to assimilate the use of many of these systems and satisfy the growing users and needs. In this paper, we argue that decision support systems need to use software agents as the natural means of representing the broadly categorized business users of an organization. They hide from their users the complexity of underlying technology of data extraction, analysis, information retrieval and knowledge sharing, and enabling them to make fact-based and timely decision with more degree of confidence. This research approach aims to address the research issue of how KM can be optimized using intelligent agents and how to enhance decision-making process. Second, we delineate the design and implementation of such environment, based on software agent development technology, and endowed with various types of data access capabilities and information retrieval. The system supports multiples business users. The proposed system is applied to a real-world project lifecycle case that is SE (Software Engineering) project. A prototype of the system is presented where intelligent agents are the building blocks of a peer-to-peer organization wide system. The application was implemented using Eclipse technology, and the agents were deployed on the FIPA-OS (Foundation for Intelligent Physical Agents-Open-Source) environment, we used JESS (Java Expert System Shell) to develop the knowledge based of the agents' reasoning.",,,15067876,,semantic_scholar
abb0cd9f718bf472e16bee39f525aa6ee219e411,https://www.semanticscholar.org/paper/abb0cd9f718bf472e16bee39f525aa6ee219e411,Intelligent Network Design of intelligent multinode Sensor networking,". The paper deals with the self configured intelligent sensor networking. The individual sensors are acting on the body or an object to measure different parameters. Although the sensors are measuring parameters accurately, but they are failed to act depending on different situations. For example a robot is moving on a surface can take decision to turn left or right when an obstacle come across. But the same robots take wrong decision when the obstacle is not static. The robot can wait till the obstacle passed away from its way. But the robot still follows the traditional way, which is turning left or turn. In this case the robot is failed to take correct decision depending on the situation. If we consider other example such as traditional automatic water supply to plants or crops, the system supplies the water at regular intervals of time with accurate quantity. But the system takes same decisions in all seasons irrespective of the soil type and crop type. In our system we are proposing a Wireless Distributing sensor system design which is able to take wise decisions as a farmer. A farmer can understands how much water the soil needs and at what time it need to apply. In our work, we are developing, (1) Home Area Networking (2)software supporting above functions; (3) Wireless Sensor Networking. Introduction: My paper describes about advanced self configured Wireless Distributed Sensor networking. My project support universal sensors, network management, GUI software, house area network (HAN) [1]. The smart environment relies first and foremost on sensory data from the real world. Sensory data comes from multiple sensors of different modalities in distributed locations. The smart environment needs information about its surroundings as well as about its internal workings. Our wireless sensor networks is involved with challenging issues wireless sensor systems, self-organization, signal processing and decision-making, and finally some concepts for home automation, We have identified some facts are : 1. Most networks are application specific, extensive secondary development is necessary to adapt to different circumstances. 2. Most of them are run by developers professionals rather than end users. 4. It is difficult for end users to configure and deploy a practical sensor network. 5. Systematic compatibility for diverse sensors and communication channels is limited. 6. The aquatic sensor network technology lags behind terrestrial development in terms of use of modern technology. Often, a single sensor cannot fully capture the measured phenomenon, so researchers develop multisensor systems to obtain more accurate information, as shown in Figure 2-1. Smart Sensor Enhanced functions include “compensation of secondary parameters (e.g. temperature), failure prevention and detection, self-testing, auto-calibration”. Figure 2-1: Multi-sensor Sensing Model Figure 2-2: Smart Sensor Network A sensor network consists of multiple detection stations called sensor nodes [3].The transducer generates electrical signals based on sensed physical effects and phenomena. The microcomputer processes and stores the sensor output. The transceiver, which can be hard-wired or wireless, receives commands from a central computer and transmits data to that computer. The power for each sensor node is derived from the electric utility or from a battery. The observations made against the characteristics of DSN are: Extended wider coverage of the environment Better fault tolerance Higher quality of measurements liminate ambient interference. Shorter response delay for changing events. Flexible size of network ISSN : 0975-3397 468 N. Suresh kumar et al. / (IJCSE) International Journal on Computer Science and Engineering Vol. 02, No. 03, 2010, 468-472 Wireless House Area Network WHAN The research group started related investigations under Low Power Wireless Sensor networking. We are integrating DSN with WHAN in house area network system. Based on this DSN platform, we are implementing wireless house area network design. Research Issues and Challenges Many research issues and challenges have been exposed. Design considerations for sensor networks [7]. Sensing aspect Computation part Wireless Sensor Networking Faster algorithm for data tranceiving. Signal Conditioning [2] Smart Sensor includes basic blocks for signal conditioning (SC), digital signal processing (DSP), and A/D conversion. Signal conditioning [6] is performed using electronic circuitry analog low pass filter. Temperature compensation can also be added during the Signal Conditioning stage. A basic technique for improving the signal-to-noise ratio (SNR) is low-pass filtering, since noise generally dominates the desirable signals at high frequencies. Shown in the figure is an analog LPF that also amplifies, constructed from an operational amplifier. The transfer function of this filter is with 3 dB cutoff frequency given by rad.",,2010.0,15689740,,semantic_scholar
ab60ca1c14bfd348b10074258db61003349cf2de,https://www.semanticscholar.org/paper/ab60ca1c14bfd348b10074258db61003349cf2de,An Empirically Derived Taxonomy of Information Systems Integration,"Information systems integration (ISI) represents the degree of cooperation in information system practices between business functions within a firm and between a firm and its trading partners. Although the establishment of information systems integration objective has been reported as one of the key concerns of top management because ISI enhances the firms’ competitiveness and growth, the classification of the information system practices and its managerial implications are still vaguely developed. The two objectives of this paper are: (1) to develop a taxonomy of information systems integration (ISI) called ISI-Matrix, and (2) to report managerial implications for matching each information system class with business process applications. By using a systematic research investigation approach, two ISI structures are identified: Internal ISI (IISI) and External ISI (EISI) from the responses of 220 firms. The ability to identify and understand the implications of the ISI-Matrix is of critical importance to both academic and management practitioners. INTRODUCTION The rapid changes in perspective toward globalization of markets and manufacturing has forced management to re-configure the traditional views of business functions and replace them with business processes. The process view of organizations embraces cross-functional teams which penetrate networks of inter-organizations and intra-organizations. Within the process, a project team performs many tasks across functional barriers (with a firm and between a firm and its trading partners) to meet corporate goals in a more seamless way. This increased emphasis on improving business processes has triggered the need for placing information systems (IS) in a strategic role of corporate strategy as opposed to a supportive role in the traditional view (Raghunathan & Raghunathan, 1990; Chan et al., 1997; Goodhue et al., 1992). A review of the empirical literature reveals that one issue, the linkage of IS practices with organizational objectives, has been among the top problems reported by information systems (IS) managers and business executives (Reich and Benbasat, 1996; Computerworld, 1994; Lederer and Mendelow, 1986; Earl, 1989). Information Systems Integration (ISI) is the degree of cooperation between business functions within the firm and between a firm and its trading partners on an internally consistent set of strategic, operational, and infrastructural information systems practices using information systems (IS). In a broader sense, ISI often represents as a pressing concern of misalignment of information system practices between two business processes (King and Teo, 1997; Segars and Grover, 1998). In this context, information system practices, which are utilized to accomplish process tasks at each end, lack degree of congruence when certain processes/tasks involve cross-functional boundaries at the other end. Consequently, the first objective of this paper is to identify a set of IS practices that is shared by process team members. Therefore, ISI represents the degree of cooperation in information system practices between business functions within a firm and between a firm and its trading partners. ISI has been reported to facilitate the possibilities of increased productivity, customer responsiveness, and the synchronization of diverse organizational settings. It has been documented that the introduction and utilization of ISI enhance firms’ competitiveness and growth, product quality, productivity, machine utilization, space management, and logistics efficiency and flexibility (Gross, 1984; Kaltwasser, 1990; Noori and Mavaddat, 1998). A higher degree of ISI creates information visibility and captures the moments of information which enable T. Jitpaiboon, T.S. Ragu-Nathan & M. Vonderembse 2005 Volume 15 Number 2 18 collaborative members of the supply chain to manage their business processes and share information better (Lummus and Vokkurka, 1999; Gunasekaran and Ngai, 2004; Bourdreau and Couillard, 1999; Williams et al., 1997; Gangopadhyay and Huang, 2004). Although ISI has been reported to positively impact firm performance, the classification of the information system practices and its managerial implications are still vaguely developed. The classifications of ISI in the current literature are extremely broad and fragmented. There is no consensus on what constitutes an ISI taxonomy. Our goal is to develop a comprehensive ISI taxonomy to aid organizations whose ability to harness the power of IS practices is critical to their success. Development of valid and reliable instruments to be used in large-scale surveys is an important first step toward this goal. The resulting taxonomy should help organizations to match information system class with their current business process applications which should enhance a firm’s internal and external integration. In a narrow sense, focusing on the survey approach, this study arguably classified ISI into two main categories namely Internal Information Systems Integration (IISI) and External Information Systems Integration (EISI). In each category, ISI construct is also clustered into three levels namely Strategic Integration, Operational Integration, and Infrastructural Integration. Infrastructural integration is also subdivided into two sub-constructs: Data Integration and Network Integration. Table 1 shows the components of ISI classifications. By deploying this classification scheme, the second objective of this paper is to propose a classification matrix (ISI-Matrix) which will be used to provide managerial implications for both researchers and practitioners. The next section of this paper defines and discusses the ISI taxonomy. The following sections describe the research design and discuss the candidate measured used to evaluate the degree at different ISI levels. The subsequent section presents the results, and the final section discusses the implications of our findings for researchers and practitioners. Table 1: Information System Classifications. Internal Information System Integration (IISI) External Information System Integration (EISI) • Strategic Integration – Internal • Operation Integration – Internal • Infrastructural Integration – Internal o Data Integration – Internal o Network Integration – Internal • Strategic Integration – External • Operation Integration – External • Infrastructural Integration – External o Data Integration – External o Network Integration – External INFORMATION SYSTEMS INTEGRATION TAXONOMY A description of previous taxonomies The rapid changes in the role of information systems (IS) are presenting firms with significant challenges and dramatic opportunities. Revolutionary advances in hardware and software capabilities coupled with reduced prices have shifted numerous applications from infeasible to feasible, changed the structure of organizations, and forced management to rethink the classification of IS. The terms taxonomy and framework are sometimes used interchangeably in the literature (Doke and Barrier, 1994). However, a clear distinction between the two is identified: taxonomy is generally used to describe a classification scheme for “things” such as IS. Although framework is sometimes used as a synonym for taxonomy, it is more often used to describe models that organize and group “concepts and relationships” (Doke and Barrier, 1994). The taxonomy is derived from the characteristics of the measured subjects, so the categories are both exhaustive and mutually exclusive (Fiedler, Grover, and Teng, 1996). This method is especially useful when one is examining unexplored phenomena because both methods must be empirically examined to evaluate the representativeness and generalizability of the classifications to the population they are mean to describe. Unlike the predetermined, idealized categories of a typological methodology that lend themselves to prescriptive hypothesizing, taxonomy’s classifications emerge from analysis so that the classification is derived (Doty and Glick, 1994; Hair, Anderson, Tatham, and Black, 1998). Developing a taxonomy An Empirically Derived Taxonomy Journal of International Technology and Information Management 19 can be viewed as a multistep process including the classification scheme, measurement development, multivariate analysis of the classification criteria to produce the item groupings, and the evaluation of the classification groupings (Fiedler, Grover, and Teng, 1996). The classification systems should “mirror the real world...describe organizational reality in a way that is recognizable to and consistent with the vision of practitioners and researchers alike as a viable reproduction of the diverse world in which we live in” (Rich, 1992). Since the focus here is the classification of ISI, a taxonomy will be used to classify these models Dimensions of a Taxonomy Integration is “to make into a whole” (Oxford English dictionary). The study of ISI classifications started as early as 1985 by Mudie and Schafer. They analyzed ISI in process terms, as they believed ISI should not only facilitate the process of development and use of data, applications, and other processing technology, but also should provide the flexibility to meet the future business demands in workstations, processing types, and applications. Wyse and Higgins (1993) defined ISI as the extent to which data and applications through different communication networks can be shared and accessed for organizational use. They defined ISI into two components: data integration and technical integration. Data integration refers to the relevancy of the information that is collected, processed, and disseminated throughout the firm. Technical integration concerns the physical or formal linkage of information systems and subsystems that are used by the firm. Webber and Pliskin (1996) defined ISI in the merger or acquisition context as the extent of the integration of IS and data processing functi",,2006.0,40863671,,semantic_scholar
e349556d5302749ee79643792e60b192020a42b2,https://www.semanticscholar.org/paper/e349556d5302749ee79643792e60b192020a42b2,Special Issue on Wireless Sensor Network: Theory and Practice,"Wireless sensor network (WSN) is an emergent multi-disciplinary science, and it may be considered as the foundation of pervasive computing, mobile computing and wearable computing. WSN is a very active and competitive research area due to its diverse and unlimited potential applications: air, underground and underwater. In spite of its young age, economic impact of WSN is important, for examples the industrial control segment market will be worth $5.3B by 2010 and the smart home market will be worth $2.8 billion worldwide by 2012 (Source: Stamatis Karnouskos, EU-US 08 Workshop). WSN is a set of wireless nodes. On one hand, each wireless node (WN) has similar hardware and software functionalities as a PC: CPU, memory, operating system, and communication protocol to fulfill a specific task. On the other hand, a WN has a limited power supply (embedded battery) and consumes approximately 1 million less power (~100μW instead of ~100W) than a PC. Due to resources constraints: energy consumption and form factor, the approaches applied in general purpose computer systems are not adapted to the requirements of WSN. When it comes to the design of energy efficient oriented hardware and software components of WSN, cross-layering optimization approaches are generally adopted such as application-specific unified hardware and software by taking into account the following criteria: trade-off between complexity, efficiency and resource consumption, and application context (context-aware) etc. Currently two main hardware development and design trends are carried out to implement the WN: Commercial OffThe-Shelf ‘COTS’ and System on Chip ‘SoC’. The first and second generations of WN were designed by using low power 8-bit or 16-bit microcontroller processor core, Bluetooth and non standard wireless access medium (MICA Mote). The current trend of WN design such as Tmotesky, iMote and LiveNode are based on low power 16-bit or 32-bit RISC microcontroller, and full compliance IEEE802.14.5 standard. However the ultimate goal of all the researchers in the world is the implementation of long life, low cost and invisible WN integrated and embedded into environment. Three key technologies make possible to achieve this objective: MEMS ‘MicroElectroMEchanical systems’, UWB ’Ultra-Wide Band’ and low power CMOS technology. Different WN prototypes are realized by Intel (iMOTE2), University of Michigan (MOTE: Michigan Uni Prototype) and University of Berkeley (Pico-Mote). WN hardware seems easier to solve than embedded software for diverse WSN applications. The main questions which are related to WSN basic software design is how to keep modularity, high level abstraction and reliability to enable to implement complex massively distributed WSNs to meet resource constraint requirements. Real-time operating system (RTOS) plays a key role to support high level abstraction and distributed collaborative processing. Currently four categories of WSN’s RTOS are developed: Event driven (TinyOS), Multitask (RETOS, tKernel, NutOS, MANTIS), Data-Centric (AmbientRT), and Hybrid (Contiki, LIMOS). Note that TinyOS is very popular but it not adapted to complex hard real-time application. The development challenges of the WSN RTOS are energy-efficiency (context aware, configurable, small footprint), robustness, fault tolerance, support hard real-time constraint, and support component based model (high level of abstraction to ease the integration of high level SW such as protocol, middleware, application, and simulator). Furthermore, for WSN applications, message sending is energy consuming. Thus it is important to implement embedded energy efficient wireless routing protocol to increase WSN lifetime. It is clear that general purpose MANET routing protocol such as AODV (active), OLSR (proactive) and ZBR (hybrid) etc. are not suitable for WSN due to resource constraints. For example optimal routing path is well adapted to general purpose MANET but not suitable for WSN because the repetitive use of the same path will exhaust the battery of WNs belonging to the optimal routing path (black hole). Many WSN dedicated protocols are implemented (spin, cougar, gear, leach, speed ...) but it is currently very difficult to have a clear idea concerning their performance (energy consumption, scalability, connectivity, lifetime ...) because of the lack of large scale WSN real world experimentation results and because the simulation model does not reflect the real-world ones (physical layer). Note that, there is no standard scenario and the application program (with a known number of WNs) enables to evaluate rationally the performance of wireless routing protocols. In addition routing protocol relies on the WSN topology. On one hand, an optimal WSN topology facilitates the implementation of routing and administration protocols. On the other hand, the deployment of large scale WS nodes in a large area is random and its topology is a priori unknown. Then, it is important to investigate the auto-configuration algorithms to increase the efficiency of routing and administration protocols. However the frontier between the administration protocol and the routing protocol is not as clearly defined as in a classical network (e.g. TCP/IP and SNMP) due to cross-layering approach. Moreover WSN security, reliability, and fault tolerance are still an open problem. In this special issue 5 papers are selected among 40 submitted papers for the NTMS workshop on wireless sensor network: theory and practice, held at Tangier at Morocco in 2008, the rest of the papers are selected from an open call for paper. WSN is a multi-disciplinary science. It impossible to present all its topics but this special issue addresses most of the WSN embedded software problems dealing with real-world applications (EU NeT-ADDED FP6 project, French ANR research project and industrial projects). JOURNAL OF NETWORKS, VOL. 4, NO. 6, AUGUST 2009 379",,2009.0,18352686,,semantic_scholar
368687003560b21e53865cd604aae8c00dc62c4b,https://www.semanticscholar.org/paper/368687003560b21e53865cd604aae8c00dc62c4b,PassItOn: An Opportunistic Messaging Prototype on Mobile Devices,"With the increasing popularity of mobile handheld devices and the growing capability of these devices, it is becoming possible that information sharing/dissemination is carried out through human networks, as a complement to the traditional computer networks. In such human networks, people come across one another, while their mobile devices exchange and store information in a spontaneous and transparent way. Such an encounter could be established through direct device-todevice connectivity when two devices come into each other’s communication range, or be enabled by, e.g., a Wi-Fi access point, when the devices both enter its coverage. A new form of dissemination, which we call opportunistic messaging, is such an application that is based on human encounters and mobilities. When human encounters are exploited for communications, the reliance on network infrastructure access is eliminated; communications can be performed even where infrastructure is absent or infrastructure access is intermittent. By leveraging human mobilities, data delivery does not require an end-to-end path from the source to a recipient; instead, people carrying mobile devices serve as relays – they cache others’ data and forward/deliver the data when appropriate. Thus, the propagation of information is tied to people’s physical proximity when they move around, and incorporates the social aspects of communications as people tend to spend more time co-locating with their social relations. Opportunistic messaging is applicable anywhere, and is especially appealing where network infrastructure access is limited or intermittent (e.g., on cruise ships, in national parks, after disasters). Another intriguing characteristic of it is its ease of deployment – no central server is needed, but only a single piece of software on users’ mobile devices. However, as human encounters and mobilities are unpredictable, when used for social applications, opportunistic messaging is most suited for disseminating user-generated information that is non-formal, less important, and thus not time-critical. In recent years, a considerable amount of efforts have been invested in the research on opportunistic networking and delay-tolerant networking (which encompasses opportunistic networking but is a broader concept). A large portion of prior work has focused on routing issues, e.g., through whom as intermediate carriers to deliver a message to the destinations [1] [2] [3] [4]. The routing issues have been further explored in various contexts, such as in vehicular networks [5] [6] [7] and in social networking applications [8] [9] [10] [11]. However, serious real-world application development, deployment and evaluation of the opportunistic networking concepts still fall behind [12], in which many challenging issues remain to be addressed (to name a few, location-awareness, user incentives and preferences, power preservation, encounter controls, etc.). In this work, we design and prototype PassItOn, a fully distributed opportunistic messaging system. Our goal is to build up a proof-of-concept platform on real mobile devices, and thus show the feasibility and potentials of utilizing human movements for dissemination applications. Meanwhile, we seek to shed lights on the design, implementation and deployment issues in building such systems, and thus stimulate new ideas and perspectives on addressing these issues. Moreover, we aim to offer a real testbed on which new mechanisms, protocols and use cases can be tested and evaluated.",2009 6th IEEE Consumer Communications and Networking Conference,2009.0,17079094,10.1109/CCNC.2009.4785009,semantic_scholar
225b6e4de88e9be8caa5c36224d135e1ed6f00ee,https://www.semanticscholar.org/paper/225b6e4de88e9be8caa5c36224d135e1ed6f00ee,Extended Abstract : A Framework for Virtual Surgery,"Surgical simulation for medical education and preoperative planning has attracted more and more attention in recent years and a number of such virtual environments have been developed and validated. However, almost all of them are focusing on simulating the surgical scene and haptic interaction to provide users with freedom to perform surgery in the virtual environment. We propose that critical surgical procedures and motion path could be guided by an information intensive process model, especially those being trained in such a virtual surgical environment. In this paper, we outline the virtual surgery framework and the design of the software environment for suturing procedure. The preliminary system that incorporates the above functionality with realistic surgical scene and haptic interaction is still on the development stage. Potentially, the information based modeling process of the surgical motion could also help automate the process of the procedural operation of surgical robot. THE CONCEPTUAL FRAMEWORK The general view of our framework architecture is shown in Figure 1, where the whole system is composed of the hardware part and the software part. In the hardware side(refer to Figure 2), besides such tradition input and output device like keyboard, mouse, haptic device and display, we leave room for sensors to record surgical motion as system input or drive micro assembly work cell to validate and measure our virtual environment in precisely simulating the microsurgery process quantitatively. The software architecture is generally composed of the following three modules. Fig. 1. Framework Architecture A. The IDEF-0 Parser In our approach, the emphasis is on the creation of an Information Intensive Process Model using the IDEF-0 modeling methodology. This model will be used to identify critical and non critical categories of information encompassing the core steps in a neurosurgical (diagnosis) and surgery process; these will possibly include the key or driving assumptions, information inputs, skill constraints, the intermediate ’attribute’ outcomes between various steps or stages of the process in reference as well as the crucial performers (which can range from the medical personnel involved in the diagnosis and surgery D/S itself to the medical assisting devices which play a key role in the outcome of various steps in this D/S process). This information model will not only capture the functional relationships among related tasks at various levels of abstraction but will also enable the representation of temporal precedence constraints among sub-tasks. They will provide a valuable insight into the process of micro surgery; our background in engineering and IT and our prior work in such modeling activities will be helpful in (a) Haptic: Phantom Premium (b) Microassembly Work Cell Fig. 2. Hardware Setup developing such a model. A lower level decomposition of this model is shown in Figure 3. Fig. 3. IDEF-0 Model (A2 Level Decomposition) We seek to develop a robust understanding of these surgical processes using information modeling strategies that have proven to be successful in understanding functional relationships within complex processes that range from performing micro devices assembly to designing virtual reality based simulation environments for satellite assembly (among others). B. Surgical Mode Selection After the framework reads and parse the customizable IDEFL-0 model file, the user will be having the option to further customize the system mode, which is composed of training mode, planning mode, and evaluation mode. The default mode is the virtual environment without any constraints. Those three modes in Figure 1 are described as: • Training and Evaluation Mode: students are able to practice surgery following those constraints defined in the information intensive process model (IIPM) by instructors or experienced surgeons. Meanwhile, the system could evaluate students’ performance based on the comparison of their operation with those constraints defined in the model. • Planning Mode: surgeons could first freely explore different potential surgical strategies in the virtual environment, and then decide a couple of final procedural surgical plans and define those procedural constraints in the IIPM for future validation. • Validation Mode: this is where users could validate those surgical operations in the virtual environment by driving the physical validation system. In our system, since we are focusing on microsurgery, a microassembly work cell (in Figure 2(b)) is used as validation purpose. C. The Virtual Reality Engine This is where the graphics visualization, collision detection and haptic rendering happens, and the FEM Analysis module in VR Engine is used to simulate the soft tissue deformation. Meanwhile, the surgical path generator module is used to define haptical constraints and visualize critical path. The red line in Figure 6 in section IV illustrates this idea. CASE STUDY: VIRTUAL SUTURING In this section, we give a overview of the model and development process of our virtual suturing system based on the above framework. During the early modeling stage, we got our first hand suturing information by learning the microsurgery lab manual and direct clinical microsurgery experience one of our author of this paper has over 20 years clinical microsurgery experience, and the real sutured scene in Figure 4 was taken by a micro lens camera after he was done with the suture. Fig. 4. Real Suturing Scene A. Information Intensive Process Model and Constraints After collecting all those real world information, we are able to model the whole process. The IDEF0 diagram in Figure 3 gives a general model of the real surgical Suturing process. For simplicity of this paper, we will not give the further decomposition here. Meanwhile, two critical steps in performing suturing is illustrated in Figure 5, where Figure 5(a) shows the critical path when the needle starts to insert into the vessel the needle has to be as perpendicular as possible to the target surface, and Figure 5(b) shows the force constraints of how to guide the needle go through the vessel wall. B. Preliminary Results From Figure 5, we can notice that the surgical path during needle insertion and going through vessel wall are critical steps of a successful suture, so to speak, they are important evaluation criteria in the training and evaluation mode, potential procedural constraints in the planning mode, as well as (a) Insertion Angle (b) Shear Force Fig. 5. Suturing (Courtesy: Internet) Fig. 6. Linear Constraints precision motion control in the validation mode. Because the framework is still in the early development stage, in order to illustrate the idea how we incorporate constraints for those critical steps, we developed a much simplified scene to show how the linear constraint guide a surgical cut procedure in Figure 6, where the red line represents the linear constraints -when the scalpel is about to cut the blood vessel (represented by a hollow cylinder), the user will be able to feel a force that guide their movement along the line. RELATED WORK Due to the huge volume of recent research papers on virtual reality system for surgical training and preoperative planning, [8],[6] and [12] have given a relatively detailed review of surgical simulation applications and technology. We limit our survey on the state of art of virtual reality system for training and complex microsurgery preoperative planing, and those key modeling and visualization techniques that enabled such virtual reality system. Surgical Training and Planning System:Though technology has been evolved a lot for the past decades, it is still a challenging task to efficiently simulate the complex surgical operation environment. To avoid the situation of starting an over ambitious project that includes everything but eventually get nothing done, most of the virtual surgery system focus on creating a realistic surgical scenario for some specific purpose, e.g. suturing training [8] [7][19] [1][13] or preoperative planning [21][18] [9][16][17]. Soft Tissue Modeling: Simulating the soft tissue deformation under surgical operation is not trivial, even a simple procedure involves great effort, eg. [3][11] [4][10] were specifically devoted to simulate the cutting procedure. [14] gives a detailed survey of the real-time deformable models used for surgery simulation. Among those models mentioned in [14], MassSpring model and FEM model are the two dominant models currently used in the research community. For its simplicity and low computational cost, the heuristic Mass-Spring model has attracted lots of research and was extensively optimized and deployed in the early years, such as in SPRING system developed by Kevin et all [15]. However, due to its lacks of realism, recent research work are shifting to center around the later continuum mechanics based FEM model as in [1] [5][2] for its accuracy and realistic mechanic behavior, though it is computational costly and needs lots of optimization to achieve real time performance.",,2009.0,12527909,,semantic_scholar
59304bc73b6d24f18d23404e0d408c462b66c4d0,https://www.semanticscholar.org/paper/59304bc73b6d24f18d23404e0d408c462b66c4d0,Simulationsbasierte Analyse und Entwicklung von Peer-to-Peer-Systemen,"Peer-to-Peer (P2P) systems are distributed systems composed of up to millions of functionally equivalent entities (peers), which form P2P overlay networks on top of physical networks to communicate. The functionality of a peer is implemented by a P2P application which de nes the behavior of the whole P2P system. The equivalence of peers is realized by providing client functionality as well as server functionality. Implementing a P2P system with speci ed behavior is a di cult task because the behavior depends on many factors, such as the used P2P search methods and the underlying physical network. Some factors cannot be taken into account completely because of their complexity or unknown or not understood parts. For instance, the prospective user behavior may only be estimated based on observed data. When engineering complex, dynamic software systems such as P2P systems, simulation is often used to analyze the properties of these systems based on simulation models in an early development phase. With simulations in natural sciences, the separation of reality and (simulation) model is clear: the reality exists in nature, while the model exists as software within some computer system. When simulating software systems, this separation is not so obvious: the simulated model is itself a software system. With P2P systems, for instance, a simpli ed P2P system is modeled and simulated for predicting properties of real P2P systems. The new software engineering contribution of this work is the Peer Software Engineering (PeerSE) method, which allows a controlled transition from simulation models to real-world software systems. The method starts with a comparative analysis of simulation models for P2P systems and proceeds iteratively toward the experimental implementation in a laboratory setting and nally a real-world P2P system deployed in a target environment. The method includes a simulation model for P2P systems and a tool supporting the execution of simulation and laboratory experiments. Simulation is an essential part of the PeerSE method used to identify and to compare models ful lling given requirements. When an appropriate model has been found, model components can be reused and further re ned to implement a laboratory P2P system. To allow for a controlled transition of model components to laboratory components, the results of simulation and laboratory experiments are directly compared using the same metrics. The applicability of the PeerSE method has been successfully evaluated by analyzing and realizing a P2P system for distributed software development.",Softwaretechnik-Trends,2008.0,34594930,,semantic_scholar
1fbf5bfc48293616106a189a3c8874070cbd57ec,https://www.semanticscholar.org/paper/1fbf5bfc48293616106a189a3c8874070cbd57ec,An investigation of techniques to assist with reliable specification and successful simulation of fire field modelling scenarios,"Computational fluid dynamics (CFD) based Fire Field Modelling (FFM) codes offer powerful tools for fire safety engineers but their operation requires a high level of skill and an understanding of the mode of operation and limitations, in order to obtain meaningful results in complex scenarios. This problem is compounded by the fact that many FFM cases are barely stable and poor quality set-up can lead to solution failure. There are considerable dangers of misuse of FFM techniques if they are used without adequate knowledge of both the underlying fire science and the associated numerical modelling. CFD modelling can be difficult to set up effectively since there are a number of potential problems: it is not always clear what controls are needed for optimal solution performance, typically there will be no optimal static set of controls for the whole solution period to cover all stages of a complex simulation, there is the generic problem of requiring a high quality mesh - which cannot usually be ascertained until the mesh is actually used for the particular simulation for which it is intended and there are potential handling issues, e.g. for transitional events (and extremes of physical behaviour) which are likely to break the solution process. 
 
In order to tackle these key problems, the research described in this thesis has identified and investigated a methodology for analysing, applying and automating a CFD Expert user's knowledge to support various stages of the simulation process - including the key stages of creating a mesh and performing the simulation. This research has also indicated an approach for the control of a FFM CFD simulation which is analogous to the way that a FFM CFD Expert would approach the modelling of a previously unseen scenario. These investigations have led to the identification of a set of requirements and appropriate knowledge which have been instantiated as the, so called, Experiment Engine (EE). This prototype component (which has been built and tested within the SMARTFIRE FFM environment) is capable, both of emulating an Expert users' ability to produce a high quality and appropriate mesh for arbitrary scenarios, and is also able to automatically adjust a key control factor of the solution process. 
 
This research has demonstrated that it is possible to emulate an Experts' ability to analyse a series of simulation trials (starting from a simplified, coarse mesh test run) in order to improve subsequent modelling attempts and to improve the scenario specification and/or meshing solution in order to allow the software to recover from a complete solution failure. The research has also shown that it is possible to emulate an Expert user's ability to provide continual run-time control of a simulation and to provide significant benefits in terms of performance, overall reliability and accuracy of the results. 
 
The instantiation and testing of the Experiment Engine concept, on a chosen FFM environment - SMARTFIRE, has demonstrated significant performance and stability gains when compared to non Experiment Engine controlled simulations, for a range of complex ""real world"" fire scenarios. Preliminary tests have shown that the Experiment Engine controlled simulation was generally able to finish the simulations successfully without experiencing any difficulty, even for very complex scenarios, and that the run-time solution control adjustments, made to the time step size by both the Experiment Engine and by the Expert, showed similar trends and responses in reacting to the physical and/or numerical changes in the solution. This was also noticed for transitional events seen during the simulation. It has also been shown that the Experiment Engine (EE) controlled simulation demonstrates a saving of up to 40% of simulation sweeps for complex fire scenarios when compared with non-EE controlled simulations. Analysis of the results has demonstrated that the control technique, deployed by the EE, have no significant impact on the final solution results - hence, the Experiment Engine controlled simulations are able to produce physically sound results, which are almost identical to Expert controlled simulations. 
 
The research has investigated a number of new methods and algorithms (e.g. case categorisation, case recognition, block-wise mesh justification, local adaptive mesh refinements, etc.) that are combined into a novel approach to enhance the robustness, efficiency and the ease-of-use of the existing FFM software package. The instantiation of these methods as a prototype control system (within the target FFM environment - SMARTFIRE) has enhanced the software with a valuable tool-set and arguably will make the FFM techniques more accessible and reliable for novice users. 
 
The component based design and implementation of the Experiment Engine has proved to be highly robust and flexible. The Experiment Engine (EE) provides a bi­directional communication channel between the existing SMARTFIRE Case Specification Environment and the solution module (the CFD Engine). These key components can now communicate directly via status- and control- messages. In this way, it is possible to maintain the original Case Specification Environment and the CFD Engine processes completely independently. The two components interact with each other when the EE is operating. This componentization has enabled rapid prototyping and implementation of new development requirements (as well as the integration of other support techniques) as they have been identified.",,2007.0,109296222,,semantic_scholar
d0270dcfa7ca058cea59512b832be6c91408676f,https://www.semanticscholar.org/paper/d0270dcfa7ca058cea59512b832be6c91408676f,Challenges Concerning Symbolic Computations on Grids,"Challenges concerning symbolic computations on grids Symbolic and algebraic computations are currently ones of fastest growing areas of scientific computing. For a long time, the numerical approach to computational solution of mathematical problems had an advantage of being capable of solving a substantially larger set of problems than the other approach, the symbolic one. Only recently the symbolic approach gained more recognition as a viable tool for solving large-scale problems from physics, engineering or economics, reasoning, robotics or life sciences. Developments in symbolic computing were lagging relative to numerical computing, mainly due to the inadequacy of available computational resources, most importantly computer memory, but also processor power. Continuous growth in the capabilities of computer hardware led naturally to an increasing interest in symbolic calculations and resulted, among others things, in development of sophisticated Computer Algebra Systems (CASs). CASs allow users to study computational problems on the basis of their mathematical formulations and to focus on the problems themselves instead of spending time transforming the problems into forms that are numerically solvable. While their major purpose is to manipulate formulas symbolically, many systems have substantially extended their capabilities, offering nowadays functionalities like graphics allowing a comprehensive approach to problem solving. While, typically, CAS systems are utilized in an interactive mode, in order to solve large problems they can be also used in a batch mode and programmed using languages that are close to common mathematical notation. As CASs become capable of solving large problems, they follow the course of development that has already been taken by numerical software: from sequential computers to parallel machines to distributed computing and finally to the grid. It is particularly the grid that has the highest potential as a discovery accelerator. Currently, its widespread adoption is still impeded by a number of problems, one of which is difficulty of developing and implementing grid-enabled programs. That it is also the case for grid-enabled symbolic computations. There are several classes of symbolic and algebraic algorithms that can perform better in parallel and distributing computing environments. For example for multiprecision integer arithmetic, that appears among others in factorizations, were developed already twenty years ago systolic algorithms and implementations on massive parallel processors, and more recently, on the Internet. Another class that utilize significant amount of computational resources is related to the implementations of polynomial arithmetic: knowledge based algorithms such as symbolic differentiation, factorization of polynomials, greatest common divisor, or, more complicated, Groebner base computations. For example, in the latest case, the size of the computation and the irregular data structures make the parallel or distributed implementation not only an attractive option for improving the algorithm performance, but also a challenge for the computational environment. A third class of algorithms that can benefit from multiple resources in parallel and distributed environments is concerning the exact solvers of large systems of equations. The main reason driving the development of parallel and distributed algorithms for symbolic computations is the ability to solve problems that are memory bound, i.e. that cannot fit into memory of a single computer. An argument for this statement relies on the observation that the input size of a symbolic or algebraic computation can be small, but the memory used in the intermediate stages of the computation may grow considerably. Modern CASs increase their utility not only through new symbolic capabilities, but also expending their applicability using visualization or numerical modules and becoming more than only specific computational kernels. They are real problem solving environments based on interfaces to a significant number of computational engines. In this context it appears also the need to address the ability to reduce the wall-clock time by using parallel or distributed computing environment. A simple example is the case of rendering the images for a simulation animation. Several approaches can be identified in the historical evolution of parallel and distributed CASs: developing versions for shared memory architectures, developing computer algebra hardware, adding facilities for communication and cooperation between existing CASs, or building distributed systems for distributed memory parallel machines or even across Internet. Developing completely new parallel or distributed systems, although efficient, in most cases is rather difficult. Only a few parallel or distributed algorithms within such a system are fully implemented and tested. Still there are several successful special libraries and systems falling in this category: ParSac-2 system, the parallel version of SAC-2, Paclib system, the parallel extension of Saclib, FLATS based on special hardware, STAR/MPI, the parallel version of GAP, ParForm, the parallel version of Form, Cabal, MuPAD, or the recent Givaro, for parallel computing environments, FoxBox or DSC, for distributed computing environments. An alternative approach to build parallel and distributed CASs is to add the new value, the parallelism or the distribution, to an existing system. The number of parallel and distributed versions of most popular CASs is impressive and it can be explained by the different requirements or targeted architectures. For example, for Maple there are several implementations on parallel machines, like the one for Intel Paragon or ||Maple||, and several implementations on networks of workstations, like Distributed Maple or PVMaple. For Mathematica there is a Parallel Computing Toolkit, a Distributed Mathematica and a gridMathematica (for dedicated clusters). Matlab that provides a Symbolic Math Toolbox based on a Maple kernel has more than twenty different parallel or distributed versions: DP-Toolbox, MPITB/PVMTB, MultiMatlab, Matlab Parallelization Toolkit, ParMatlab, PMI, MatlabMPI, MATmarks, Matlab*p, Conlab, Otter and others. More recent web-enabled systems were proved to be efficient in number theory for finding large prime numbers, factoring large numbers, or finding collisions on known encryption algorithms. Online systems for complicated symbolic computations were also built: e.g. OGB for Groebner basis computations. A framework for description and provision of web-based mathematical services was recently designed within the Monet project and a symbolic solver wrapper was build to provide an environment that encapsulates CASs and expose their functionalities through symbolic services (Maple and Axiom were chosen as computing engines). Another platform is MapleNet build on client-server architecture: the server manages concurrent Maple instances launched to server client requests for mathematical computations. WebMathematica is a similar system that offers access to Mathematica applications through a web browser. Grid-oriented projects that involve CASs were only recent initiated. The well-known NetSolve system was one of the earliest grid system developed. Version 2 released in 2003 introduces GridSolve for interoperability with the grid based on agent technologies. APIs are available for Mathematica, Octave and Matlab. The Genss project (Grid Enabled Numerical and Symbolic Services) follows the ideas of the Monet project and intends also to combine grid computing and mathematical web services using a common agent-based framework. Several projects are porting Matlab on grids: from small ones, like Matlab*g, to very complex ones, like Geodise. Maple2g and MathGridLink are two different approaches for grid-enabled version of Maple and Mathematica. Simple to use front-end were recently build in projects like Gemlca and Websolve to deploy legacy code applications as grid services and to allows the submission of computational requests. The vision of grid computing is that of a simple and low cost access to computing resources without artificial barriers of physical location or ownership. Unfortunately, none of the above mentioned grid-enabled CAS is responding simultaneously to some elementary requirements of a possible implementation of this vision: deploy grid symbolic services, access within CAS to available grid services, and couple different grid symbolic services. Moreover a number of major obstacles remain to be addressed. Amongst the most important are mechanisms for adapting to dynamic changes in either computations or systems. This is especially important for symbolic computations, which may be highly irregular in terms of data and general computational demands. Such demands received until now relatively little attention from the research community. In the context of a growing interest in symbolic computations, powerful computer algebra systems are required for complex applications. Freshly started projects shows that porting a CAS to a current distributed environment like a grid is not a trivial task not only from technological point of view but also from algorithmic point of view. Already existing tools are allowing experimental work to be initiated, but a long way is still to be cross until real-world problems will be solved using symbolic computations on grids. Dana Petcu, Western University of Timisoara",Scalable Comput. Pract. Exp.,2005.0,36193099,10.12694/SCPE.V6I3.330,semantic_scholar
40b8656563e93c01a2723717e1c2342e79599507,https://www.semanticscholar.org/paper/40b8656563e93c01a2723717e1c2342e79599507,JSP¿ and XML: Integrating XML and Web Services in Your JSP Application,"Introduction. I. DATA, XML, AND WEB SERVICES INTRODUCTION. 1. Integrating JSP and Data. Using JSP with a Database. Entering the Data. Reviewing the Code for Entering Data. Viewing the Data. Other Considerations. Connection Pooling. Testing Components. Testing for Scale. Basic Design Concepts. Using a Tag library. Summary. 2. Introduction to XML/XSL. What Is XML? Rules of XML. Tags and Elements. The XML Declaration. Document Type Declaration. Schemas. Character Entities. CDATA Sections. Comments. Well-Formed and Validated Documents. On to Using XML. Processing. XSL. Stylesheet Linking. Namespaces. Templates. Stylesheet Errors. Whitespace and Encoding. Entity Declarations. Trees, Nodes, and Family. XPath. Summary. 3. Understanding Web Services. What Is a Web Service? Crystal Ball Readings. The ABCs of Web Services. The Basic Building Blocks. Service Management Initiatives. Java APIs. How to Use a Web Service. Using SOAP. Roaming the Internet. Summary. II. INTEGRATING JSP AND XML. 4. A Quick Start to JSP and XML Together. The Relationship Between XML and JSP. A Warning. JAXP, Xerces, and Xalan. JSP and XML: An Overview. Java XML/XSL APIs. DOM (XML Document Object Model). SAX (XML Parser). JDOM (XML Document Representation). dom4j (XML Document Representation). JAXB (Parser and XML Document Representation). Summary. 5. Using DOM. What Is the DOM? Strengths of DOM. Weaknesses of DOM. Nodes and Tree Structure. The Document Node. Programming with DOM. Attributes. Namespaces. Removing a Node. Moving Nodes. Copying and Appending Nodes. Programmatically Creating an XML Document. Moving Nodes Between Documents. TreeWalker. NodeIterator. Ranges. JDOM, dom4j, and Deferred DOM. Summary. 6. Programming SAX. What Is SAX? The Workings of SAX. SAX Interfaces. Downsides to SAX. Differences Between SAX1 and SAX2. First SAX Example. Characters and Ignorable Whitespace. Processing Versus Validation. Characters Revisited. Error Handling. Ignorable Whitespace. Entity References. The Document Locator. Breaking the System to See How It Works. Processing Versus Validation Revisited. Using SAX to Output HTML. Summary. 7. Successfully Using JSP and XML in an Application. Using a Java Representation of an XML Document. Why Not Just Use SAX or DOM? Installing JDOM and dom4j. JDOM. dom4j. Notes. Why Both JDOM and dom4j? JDOM and dom4j: A Quick Comparison. Common Ways to Use XML. Using a Database with XML. XML Initialization Files. Storing the Initialization Data. Using a Listener to Store the DatabaseParameter Object. Using a Java XML Model. Threading Issues. Getting the Row Count. XML and the WebRowSet. Building a dom4j Helper Class. Creating a Banner Handler. Creating a Test JSP Page. Using a Java Representation of an XML Document. Using JAXB. HashMap Versus Java XML Representation. Pulling in XML Files. Defining an XML File. Pondering XML Design. Reading XML Files and Creating New Output. Using JDOM. Building the Final JSP Page. Summary. 8. Integrating JSP and Web Services. Thinking in JSP and Web Services. Tag Libraries Versus Web Services. Using Tag Libraries to Access a Web Service. Integrating a Web Service into a JSP Page. A Tag Library/Service Warning. Fixing Some Network Issues. Web Service Reliability. When Should You Build Your Own Web Service? JSP Pages Versus Web Service. Building a Corporate Web Service. Goal of This Web Service Example. Realities of Building a Web Service. Setting Up the Example. Initializing Data. Accessing Application Data. Building the Actual Web Service. Deploying a Web Service. Where Is the WSDL? Writing a JSP Page to Deploy the Descriptor File. More on Security. Building a Page to Access the Service. Apache SOAP Help. Summary. 9. Advanced JSP and XML Techniques. Accessing Web Services from a Browser. Using an Applet. Handling Large XML Documents. JDOM. dom4j. Handling Special Characters and Encoding. Using XML Tag Libraries. XSL Tag Library. XTags Library for XML. Summary. III. BUILDING JSP SITES TO USE XML. 10. Using XSL/JSP in Web Site Design. Handling XML Files Directly. How Servlet Mappings Work. Building an XML Servlet Handler. Building a SAX Reader. Creating a Servlet to Process XML Files. Register the Servlet. Building the Error Page. Creating Some Test Files. Accessing XML Directly. Summary. 11. Using XML in Reporting Systems. Architecture of Reporting Systems. When to Use XML with Reports. Data Source for Reports. Creating Database Data. ResultSet to XML. What It Does. Bringing It All Together. The Sorting Table Stylesheet. The Cross Tab Stylesheet. Summary. 12. Advanced XML in Reporting Systems. Multiple-Page Reports. The JSP for the Multiple-Page Report. The Stylesheet for the Multiple-Page Report. Reports on Data with One-to-Many Relationships. The JSP for the One-to-Many Report. The Stylesheet for the One-to-Many Report. Real-World Reporting Systems. Well-Formed Documents Revisited. Summary. 13. Browser Considerations with XML. Client-Side XML and Browser Support. Client-Side JavaScript and XML. The JSP. Client-Side Transformations and XML. The Cross-Browser JavaScript Source File. The JSP. The Two XSL Stylesheets. Summary. 14. Building a Web Service. Designing a Web Service. What Is the Goal? What Are the Requirements? What Data Does the Service Need? Building the Web Service. Building a File Handler. Building a Search Utility. Creating an ElementHandler. Building a Document Object. Applying a Stylesheet. Creating a Stylesheet. Building the Web Service at Last. Registering the Web Service with Apache SOAP. Creating a WSDL File. WSDL Namespaces. Creating the JSPBuzz WSDL File. WSDL Implementation File. WSDL Documentation. Registering Within UDDI. Registering a Service. Using Java to Access a WSDL Document. Summary. 15. Advanced Application Design. Dynamic JSP. When Not to Use Dynamic JSP. Building a Dynamic JSP Example. SOAP Server Security Concerns. Using Tomcat Security Zones. Servlet Filtering. Other Apache SOAP-Specific Security Steps. Quick Takes. Web Services-SSL and Data Encryption. Using Cocoon. Summary. IV. APPENDIXES. A. Setting Up. Installing the JSP Environment. The Java Software Development Kit (SDK). The Tomcat Server. Creating a Web Site for the Book. NetBeans. The MySQL Database Server. Creating a MySQL Database. Installing a JDBC Driver. Summary. B. Introduction to JSP and How Things Work. JSP Basics. JSP Banner Example. Actions. JSP Actions, Directives, and Implicit Objects. A More Robust JSP Example. Additional Information About JSP. What Is JSP and How Does It Work? JSP XML Syntax. JSP Documentation Resources. Summary. C. Tag Library. Tag Library Overview. What Is a Tag Library? Advantages. Disadvantages. The Six Steps to Building Tag Libraries. Tag Library Concepts. Isolating the Business Logic. The Tag Handler. The Tag Library Descriptor (TLD). Creating a Distribution File. Registering the Tag Library. Using the Tag Library Declaration on a JSP Page. Building a Tag Library. Isolating the Business Logic. Building a Tag Handler. The Tag Library Descriptor. Registering the Tag Library. Using the Tag Library on a JSP Page. General Notes. Body Data. Design Notes. Empty Tags. Threading. Summary. D. XSL Reference. XSLT and XPath. Context and Current Nodes. Reference. XSLT Elements. XPath Functions. Index. 0672323540T03282002",,2002.0,60396373,,semantic_scholar
b5feeee7c1e5e6acf7b6ee0314415102b09068e0,https://www.semanticscholar.org/paper/b5feeee7c1e5e6acf7b6ee0314415102b09068e0,A CLASS LIBRARY FOR MANUFACTURING SYSTEMS,"This work presents a class library for manufacturing systems that aims at facilitating the construction of simulation models, allowing reuse and speeding up the modeling process. This library implements a modeling approach that differs from the majority of similar works in this area. It is based on the application of well known manufacturing concepts, like production routings and activities. It allows the creation of new simulations faster than other methodologies, since complex translations from the reality to simulated applications are not necessary. The development of this library was validated by modeling the production line of tractor parts. The production line case study, modeled using both Automod and the proposed class library, allowed a quantitative comparison for the validation of this work. INTRODUCTION The fast dissemination and widely acceptance of simulation methods contributed to the emergence of specialised environments for programming and simulation of manufacturing systems. Simulation is virtually the only methodology at present which is capable to provide accurate performance estimates for manufacturing systems design (Govindaraj et al. 1990). These environments offer tools that allow the efficient analysis from simple to complex systems. Their main limitation is the difficulty to develop new simulation models. Their model building activity usually requires specific information that is related to the internals of the adopted simulation tool. They are usually restricted to highly specialised users, since the systems usually adopt proprietary languages and particular simulation methodologies. On the other hand, the object-oriented paradigm has taken the attention of the scientific community. Properties like encapsulation, inheritance, and reuse have contributed to make the object-oriented technology very popular for manufacturing systems simulation and deployment. The oneto-one mapping between objects in the manufacturing systems being modelled and their abstractions in the simulation model offer conditions for a better modelling. The object-oriented paradigm have been explored to allow the construction of high-fidelity simulation software for supporting the modelling and control of integrated and complex manufacturing systems. The object-oriented paradigm and distributed objects have emerged as some of the most promising technologies since they allow a natural structure for the problem domain of industrial automation systems. This occurs because the industrial components can be easily mapped into the oriented-objects model diagrams using classes and objects. Objects encapsulate the functions of many components, modularizing the description, encouraging reuse, and allowing implementations as well in software as in hardware. There are many commercial simulation environments for manufacturing systems. They offer many features that permit to build powerful simulations in order to develop advanced solutions for new projects through experimenting many alternative configurations. One of these simulation tools is Automod (Auto Simulations, 2002). Automod (Auto Simulations, 1999) is a process-oriented simulator. The simulation method used in Automod is based on moving parts, representing the work in process, on resources, and on transportation entities. To create a simulation using Automod, the designer must control the traffic of parts through the system using a process specification, which indicates the correct pathway that the part must follow. This approach gives to the simulator robust and reliable features, allowing complex model constructions. The major problem of these tools is the difficulty to build new simulation models. The modeling activity in these environments needs a detailed previous study about the tools, generally demanding highly specialised users. Moreover, these environments do not use important modeling concepts like reuse and extensions. Analysing the current state of the art, this fact is unacceptable. An integration between advantages of the commercial simulators and of the new modeling paradigms becomes mandatory. This fact may increase the quality of produced models, since developers only need to worry about the manufacturing problems, without having to worry about particularities of simulation environments. The main goal of this work is the design of a class library for manufacturing systems that allows the development of simulation models to evaluate these systems. Basic components of these systems should be available, and it should be possible to extend them through high-level modeling features found in object-oriented systems, like aggregation and abstraction, aiming at an easier modeling of complex systems, by encouraging reuse and extensions. STATE OF THE ART IN MANUFACTURING SYSTEMS SIMULATION Bertotto (2001) presents a complete study of the state of the art in manufacturing systems simulation. Modeling, control, and simulation methodologies have attracted the attention of researchers and non-governmental consortia to find new ways of developing manufacturing systems. Narayanan (1998) describes relevant efforts in the specification of object-oriented hierarchies applied to manufacturing systems simulation. Among other efforts, it is worth mentioning the application of Distributed Artificial Intelligence (DAI) and multi-agent systems, the ideas proposed by a consortium called Holonic Manufacturing Systems (HMS), and CORBAManufacturing. Park (1997) introduces an object-oriented modeling schema for the design of automated manufacturing systems (MAS) called JR-net. To propose this schema, Park relies on the fact that the modern flexible manufacturing systems are modular and hierarchical structures, built from standard resources. This proposal is formally based on processes and resource definitions, relationships among objects, procedures to build the model, and on the model structure. Its purpose is to shown that the JR-net modeling framework could be used in the same way as a graphical modeling tool for commercial simulators, such as Automod. A CLASS LIBRARY FOR MANUFACTURING SYSTEMS SIMULATION The main feature of this library is its modeling approach, which is different from other efforts in this area. This approach is based on activities and production routings. The production control is established by production routings that own activities and activate the respective resources. This approach makes the design easier, since the current theory of manufacturing systems is based on this kind of concepts. New simulations may be developed faster than in other methodologies, since complex translations between the reality and the simulation models are unnecessary. The class diagram of the proposed library is represented in Figure 1. This library contains a set of classes that supports both the physical and logical modeling of a manufacturing system. The physical modeling is based on classes that represent the manufacturing system entities, like machines, robots, and tasks. The classes that represent the physical resources can be easily extended to represent other equipments that were not yet created. The logical modeling is based on classes, methods, and attributes also similar to real world logic concepts. Class Library Specification This library was created using UML, a standard language for object modeling containing various diagrams. The library is composed by 30 classes, linked by relationships, aggregations, and specializations. The root of the hierarchy is the Production_Line class. This class aggregates two other classes: Resource and Master_Plan. The Resource class is the root of all classes that execute some transformation on manufacturing parts. The Master_Plan class is the root of all classes that logically control the manufacturing system. In this class, the production plan based on client orders is generated. To create the master plan, the Master_Plan class uses services provided by other classes, such as Order, Scheduling_Algorithm, Part, Resource and Daily_Capacity. The user interacts with the model using the Order class, inserting orders to be produced. The Scheduling_Algorithm class implements the production scheduling algorithms that will guide the master plan creation. When the master plan is generated, the production orders should be created to control the production. Each instance of Production_Order will order the production of one part. If, for example, there is an order of 30 parts, there will be 30 instances of Production_Order. This approach makes the production flow easy. Moreover, it is necessary to update the capacity information of the involved resources. Before the creation of the production orders, Production_Order estimates the daily capacity of the resources, using the Daily_Capacity class related to the Resource class. A variation of a production routing was used in the library to control the production in the various resources. The production routing is represented by an aggregation of the Activity class within the Part class. The aggregated Activity classes describe a logical sequence represented by a relationship that shows the next and the previous activity. An activity can only be executed if all the previous activities were executed. This aggregated structure builds a production graph that may control various activities at different levels of the production hierarchy. When an activity is completed, it activates the next instance of the Activity class. If there are no next activities, an answer is returned to the calling instance. This action indicates that the production of the mentioned part is concluded. Each instance of the Activity class must own a default resource to control. Each activity has a relationship to the Resource class that will execute it. To each resource a wait queue, represented by the Queue class, was added. This class holds the scheduled parts if the respective resource is busy. The Position class stores the ph",,2003.0,18991496,,semantic_scholar
784cb6b4c7a6e3f6a2f17584cf0e1e6f07968955,https://www.semanticscholar.org/paper/784cb6b4c7a6e3f6a2f17584cf0e1e6f07968955,Paper Session II-D - Internet Based Training to Support a Changing Workforce,"Changes are now underway at the Kennedy Space Center to re-engineer a smaller but more technically advanced and more productive workforce. A key component of this change is the training to give the workforce the skills needed to implement future space flight programs both effectively and efficiently. The Safety and Mission Assurance Directorate at KSC has developed training classes which are delivered over the Internet for its employees, and are available to all agency employees. The courses developed to date include An Overview of Non-Destructive Evaluation, Introduction to Statistical Process Control, Statistical Process Control, and Radiography. These courses are interactive and can be completed at the optimum pace for the students at any time or location with computer access to the Internet. Overview The re-engineering now underway at the Kennedy Space Center and across all of NASA is a familiar story. The processes that worked so effectively in the past, especially in manned spaceflight applications, are now too expensive to meet the new efficiency goals brought about by today’s budget realities. The new challenge is to maintain or improve effectiveness while dramatically improving efficiency. This means developing new, lower cost techniques in all areas of spaceflight development and operations. New skills must be brought to the workforce to reach these new efficiencies and the means of supplying these skills must itself be accomplished as efficiently as possibly. One area in which technology is delivering increased capability at steadily lower prices is information technology. Information technologies have made training at the desktop cost effective. Desktop training could augment or supplant existing training methods. Interactive learner control enhances training and facilitates Just-In-Time learning and reference. Web Interactive Training (Figure 1) uses time and resources more efficiently than many current training methods in use. The WIT project uses the Web to deliver training directly to the learner’s desktop computers. The primary users of the system are NASA (National Aeronautics and Space Administration) personnel at both KSC (Kennedy Space Center) and other NASA centers. The objective of the project is train a large base of NASA learners; efficiently and effectively, using state of the art technology to enhance learning. Training modules consisting of text, graphics, animation, video, simulations and tests are delivered over the Internet through a Web browser interface. This approach is expected to reduce training costs and associated travel and time-off task costs. The training is available 24 hours a day, seven days a week for learner convenience and follow-up job performance support after the training is completed. The WIT project began in July of 1995 and has continued throughout fiscal year 1996 (October 1995September 1996) and fiscal year 1997 (October 1996September 1997). Four courses have been completed to verify and validate the design, technologies incorporated and developed. These courses include Nondestructive Evaluation (NDE) Overview, Introduction to Statistical Process Control course, Radiography-NDE and Statistical Process Control. The current phase of this project is the development of two advanced courses. One will be Nondestructive Evaluation-Ultrasonics and the other will be Advanced Statistical Process Control-Design of Experiments. The project incorporates state-of-the-art multimedia technologies to meet the defined learning objectives. Instructional Design In order to develop an effective Web-based training system that accomplishes the goals of providing sound instruction over the Web, it is necessary to understand key instructional features that will contribute to the development and deployment of the WIT system (Alexander, 1995). Using multimedia in an effective way on the Web and especially in Web-based training applications is a challenge (Kilby, 1996). The project also defines a functional educational design model that takes into account the advantages and disadvantages of the Web The model for each instructional section generally follows an expansion of the Topic, Task, Test model (Chopping, 1995). For example a section might include: introduction and definitions, key concepts and theoretical foundation, practical application and case studies, an interactive simulation or practice exercise, and testing or evaluation. The WIT system presents information in an interactive and informative way. The learners will engage primarily in guided discovery learning. Learners will have a clear learning objective presented and their path choices will be limited to pertinent information. A structured approach is determined in each course by instructional system designers and subject matter experts (SME’s). The instructional objectives, content, and methodologies are used to determine the best approach for a particular subject area module. The learner will have some flexibility in the depth to which they wish to explore the information, but an acceptable level of proficiency must be met to prove completion of a module. For example, in the Nondestructive Evaluation (NDE) Overview course, learners must take a test with results posted back to a database in order to advance to further sections by the specified path (Figure 5). There is some flexibility built in for the user to explore detailed information on a subject area. Further references and resources are provided as well as more advanced followon modules in a particular NDE method currently under development. Technical Disciplines There are many technical considerations and approaches to this project. The majority of the effort involved advanced HTML scripting, hardware and software setup and design. This effort also includes instructional system design, digital photography, scanning, media conversion, audio and video recording, compression, animation, formatting, scripting, programming, and beta testing. The process includes research and implementation of late-breaking technologies like streaming digital video for topic introductions, CGI interfaces for forms and testing feedback, Shockwave simulation modules, Java and other advanced client/server features. Unique Attributes and Innovations Simulations The simulations developed for the NDE modules are designed to simulate a real-world process in a simple, elegant manner while minimizing the learner’s download time. (Each simulation is approximately 30 kilobytes in size.) These simulations incorporate repetition, learning by example, and positive feedback (Campbell, 1995). Each simulation follows the same general model as the Eddy Current Simulation shown in Figure 4, where the student searches for discontinuities that are randomly scattered throughout a test object. The Eddy Current simulation shows a sample plate on the right and the feedback screen on the left. These simulations were designed and developed using Macromedia Director and its native programming language, Lingo (NASA, In Press). Testing Testing will aid in comprehension of the information presented on NDE and SPC and reinforce the most important points in the section of training presented. Test answers are cataloged in a database to show student progress and adequate completion. Learners are presented with a short multiple-choice quiz. The quiz is randomly generated from a database of questions and is different every time the student takes the quiz. After submitting his or her answers, the student is immediately presented with his or her score, a brief explanation of the answers, and a link to the place in the course where that topic was covered. Feedback and remediation are immediate providing excellent response and reinforcement (NASA, In Press). Learners cannot cheat. The program prevents them from returning to the same quiz and retaking it. The testing database was written in Perl and partially converted to Java. Eventually the program will be entirely converted to Java for ease of future expandability and possible cross-platform server deployment (NASA, In Press). Security and Student Tracking The system is only available to the NASA Centers unless special password authorization is granted by NASA. The courses may be opened up to a broader audience in the future at NASA’s discretion. Current activities include enhancing the learner tracking to include a placeholder for reentering the training space at the same point of exit from the last use. This will avoid unnecessary navigation and the disorientation often associated with large hypertext systems. A good computer managed instruction database to track completion of sections is an essential component of the WIT system. Instructional designers will have a much clearer idea about the effectiveness of the instruction from the answers received. Tracking could be automated to send an electronic mail reminder to individuals who need to finish instruction by a certain date for certification. Performance Support Some of the interactive calculation simulators like the normal distribution calculator shown below (figure 6) provide performance support after the training. The system also contains an electronic version of relevant reference material and procedures used for each discipline. A search engine allows users to pinpoint specific information and get to it in seconds. This makes the modules usable as a reference after the training has been completed. In this way, the same tool that is used for training can be extended to provide reference support to the application of the training on the job. Future Developments Future efforts involve advanced security functions, enhanced student tracking capabilities, additional performance support functions, adaptive learning through dynamic Web pages and objects, integration into centralized NASA training activities, front-end “push” technology, synchronous instructional communication aids (videoconferencing and live bulletin ",,1998.0,78744263,,semantic_scholar
7db240ccd5e4bbb8a520d24318e8523eda544e9a,https://www.semanticscholar.org/paper/7db240ccd5e4bbb8a520d24318e8523eda544e9a,Ansys Real Time Physics Based Radar Simulation – An Enabler for Machine Learning in the Context of Autonomous Driving,"Throughout the evolution of Advanced Driver Assistance Systems (ADAS), the correct perception of the environment has always been a decisive success factor. Capturing and defining scenarios/edge cases, various and heterogenous datasets, multiple sensors/sensorfusion architectures, and perception algorithms are just a few of the many challenges we are facing when implementing such systems. To cope with such levels of complexity, modular approaches are required. Such approaches target flexibility and standardized interfaces between data provided by various sensor modules/models and driving functions. In the Artificial Intelligence (AI) domain, and more precisely when dealing with supervised training of Neural Networks (NN), obtaining valid and accurately labeled datasets is essential. By enabling Machine Learning (ML) in electromagnetic applications, Ansys physics-based Real Time Radar (RTR) introduces a new paradigm for sensor development and integration that leverages GPU hardware and new algorithms to accelerate simulation by orders of magnitude without compromising accuracy. In this paper, a comprehensive workflow for the generation of virtual datasets using the Open Simulation Interface (OSI) will be presented. This workflow will illustrate how the scenario variation process coupled with RTR facilitates the creation of heterogenous/labeled datasets that are ready for training object detection NN. Finally, this presentation will also show the preliminary results obtained when implementing this process. Introduction Machine Learning (ML) is gradually taking over “conventional” algorithms that were previously designed to help make Autonomous Vehicles (AVs) a reality. Several auto giants like BMW, VDI-Berichte Nr. 2384, 2021 83 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. Volkswagen, and Volvo are at least partially relying on ML algorithms to solve various parts of the sense-plan-act paradigm. Others, like Tesla [1] are solely relying on ML and in some cases Artificial Intelligence (AI) to provide an end-to-end solution. AVs usually rely on several sensors of different types to perceive their surrounding environment. As such sensors continuously scan the environment and generate raw data, the perception stack processes this data and generates a meaningful virtual map of the surrounding environment. In the area of Computer Vision (CV), usually relying on optical systems, ML algorithms are already successfully deployed in commercial systems [2] – [4]. In addition to optical systems, and due to their superior performances in bad weather conditions and dark environments, radars have also made their way into the AV’s sensor stacks. Though not frequently encountered, ML has also been used to replace some of the traditional radar signal processing algorithms. For example, in [5] the author demonstrates how a fully convolutional network can be used for object detection and 3D estimation using a Frequency-Modulated ContinuousWave (FMCW) radar. Contrary to [5], which is using real data for the training process, the author in [6] illustrates the power of physics-based simulation to also demonstrate the feasibility of using ML approaches to solve radar-based perception problems. As the training process of ML algorithms highly relies on labeled training data, Ansys’ Real Time Radar (RTR) automates generation of labeled data sets by shifting data generation and labeling from the real world to the virtual world. In addition, having an API which is compatible with the Open Simulation Interface (OSI) [7] ensures that a standardized interface is being deployed to describe the virtual environment in which generated scenarios are executed. Finally, this paper illustrates how, with the help of Ansys optiSlang [8], a tool chain is developed to orchestrate the scenario variation process and the simulation workflow to automatically generate labeled datasets for training a Neural Network (NN) based object detection algorithm. Ansys Real Time Radar RTR is an all-GPU implementation of the shooting and bouncing rays (SBR) method optimized for the automotive radar application to simulate a scenario in real time/faster than real time. The simulation, which is based on an arbitrary 3D scene/actor geometry, electrical material VDI-Berichte Nr. 2384, 2021 84 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. properties, including transmissive and reflective dielectrics, 3D polarized antenna patterns, directly calculates the scattered electro-magnetic fields as observed by the radar. As an output, RTR can generate raw I and I+Q A/D data for multi-channel radars in dynamically changing driving scenarios. Objects (e.g., vehicles, pedestrians, road, infrastructure, etc.) can be assigned arbitrary positions, orientations, linear and angular velocities in a scene graph hierarchy through a light-weight API to characterize complex traffic scenarios with negligible simulation overhead. To measure Doppler velocity, automotive radars transmit, receive, and process hundreds of chirps over each Coherent Processing Interval (CPI). Fast Fourier Transformations (FFT) and several post processing algorithms are then applied to hundreds of samples from each chirp/CPI to obtain range-Doppler (RD) images, which will be used for Neural Network (NN) training. These images, as represented in Fig. 1, give a visualization of all scattered fields in terms of relative velocity (Doppler) and distance from the radar (range). Fig. 1: Example of Range-Doppler image. As previously mentioned, RTR includes a lightweight C++ and Python API, enabling it to be integrated into nearly any driving simulator available on the market. In Fig. 2 the API’s main interfaces are depicted. VDI-Berichte Nr. 2384, 2021 85 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. Fig. 2: RTR's API representing its main inputs and outputs. Starting from left to right, the API give the user access to the following: 1. “Radar Config” enables the user to configure radar waveforms, radar modes, and antenna patterns. Radar waveforms are defined by parameters such as center frequency, bandwidth, number of frequency samples, CPI duration, number of chirps, number of transmit and receive antennas, and relative antenna positions. 2. “Object and Materials” helps the user build the 3D environment to be simulated and assign dielectric material properties. For example, as presented in Fig. 3, a vehicle can be imported as a set of subcomponents. Users can assign appropriate material properties for each component. VDI-Berichte Nr. 2384, 2021 86 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. Fig. 3: Vehicle subcomponents featuring adequate assignment of material properties. 3. “Object Velocities” represents scene dynamics where position and velocity updates are provided at each simulation time step. Such data is usually obtained from any driving simulator or from a set of pre-recorded/measured GPS/IMU data. At each time step, RTR executes a physics-based radar simulation of the scene and returns either the RD data per channel or the raw I/Q channel data (post A/D conversion). Open Simulation Interface To ensure modularity and interchangeability, RTR’s inputs have been adapted to support OSI. Focusing on environment perception and automated driving functions, OSI is an interface specification for models and components of distributed simulation. It defines a generic interface that ensures modularity, interoperability, and integration of simulation framework’s individual components. In addition, OSI was developed to address and align with the emerging standard for communication interfaces of real sensors, ISO 23150 [9]. This will eventually ensure a better correlation between communication interfaces used in both virtual and real worlds. Corresponding to OSI’s message description, the OSI:SensorView message represented in Fig. 4 contains the ground truth data that can be generated by any 3rd party driving simulator. The message includes information about the states of dynamic and static actors. VDI-Berichte Nr. 2384, 2021 87 https://doi.org/10.51202/9783181023846-83 Generiert durch IP '54.190.42.255', am 08.11.2021, 13:26:19. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. Fig. 4: Driving Simulator and RTR connection via OSI. Scenario Variation Neural Networks (NN) are designed to behave as low bias and high variance machines that can perform extremely well on training data. To generalize such machines to new environments, heterogeneous datasets are essential for the training process. Using Ansys optiSlang, scenario variations were generated based on a predefined set of parameters. As represented in Table 1, a set of three parameters were chosen. Table 1: Scenario Variation Parameters Parameter Description Model Name Describes the 3D geometry of the vehicle. Initial Speed A range between 0 and 80 km/h Driver Behavior Aggressive, Normal, Cautious 1. “Model Name” defines whether a traffic participant is a car, bus, or a motorcycle. It also describes what vehicle model to use, ensuring a large variety of traffic participants within each generated scenario. 2. “Initial Speed” may randomly vary between 0 and 80 km/h. Considering that the NN is being trained on RD images, this parameter will ensure that RD scattered fields are randomly distributed in the RD image space along the Doppler velocity axis. 3. “Driver Behavior” takes in three different values: Aggressive, Normal and Cautious. Each behaviour ",ELIV 2021,2021,10.51202/9783181023846-83,239556900,semantic_scholar
d557d4b079b89fdd4d149697b38c9160a8234c47,https://www.semanticscholar.org/paper/d557d4b079b89fdd4d149697b38c9160a8234c47,"Rapid development, real-world deployment, and evaluation of projected augmented reality applications","Current interactive projected augmented reality systems are not designed to support rapid development and deployment of applications beyond the confines of research labs. I developed a series of self-contained interactive projector-sensor systems (collectively LuminAR devices) and a web-based software development framework. The design goal of this research work was to advance the state of the art of projected AR interfaces and to explore how they can manifest in day-to-day objects. This novel, tightly integrated approach allows developers who are not versed in computer graphics, vision algorithms, and augmented reality techniques to implement projected AR applications rapidly. In this work, I review several real-world uses of the system for retail presentation, desktop interaction and collaboration applications, manufacturing, and education. The work is evaluated through extensive use of the hardware and software by developers as well as two user studies that specifically explored applications for manufacturing and education. The evaluation methodology focused both on basic interaction and system usability as well as the implications of using augmented interfaces in the specific application domains of education and manufacturing. I also discuss the results of the first large-scale user studies of projected augmented reality rapid application development. Finally, I provide a set of design principles for projected augmented reality applications, and recommendations concerning how to deploy such applications in the real world. This dissertation work was partially supported by research grants from Intel, Steelcase and Pearson. Dissertation Supervisor Pattie Maes Professor of Media Arts and Sciences Program in Media Arts and Sciences Rapid Development, Real-World Deployment, and Evaluation of Projected Augmented Reality Applications",,2017,,57966766,semantic_scholar
1de1fb5a0530c5c144c8a78fb653c8c7bc58c108,https://www.semanticscholar.org/paper/1de1fb5a0530c5c144c8a78fb653c8c7bc58c108,Sensor-based human activity recognition: Overcoming issues in a real world setting,"The rapid growing of the population age in industrialized societies calls for advanced tools to continuous monitor the activities of people. The goals of those tools are usually to support active and healthy ageing, and to early detect possible health issues to enable a long and independent life. Recent advancements in sensor miniaturization and wireless communications have paved the way to unobtrusive activity recognition systems. Hence, many pervasive health care systems have been proposed which monitor activities through unobtrusive sensors and by machine learning or artificial intelligence methods. Unfortunately, while those systems are effective in controlled environments, their actual effectiveness out of the lab is still limited due to different shortcomings of existing approaches. 
 
In this work, we explore such systems and aim to overcome existing limitations and shortcomings. Focusing on physical movements and crucial activities, our goal is to develop robust activity recognition methods based on external and wearable sensors that generate high quality results in a real world setting. Under laboratory conditions, existing research already showed that wearable sensors are suitable to recognize physical activities while external sensors are promising for activities that are more complex. Consequently, we investigate problems that emerge when coming out of the lab. This includes the position handling of wearable devices, the need of large expensive labeled datasets, the requirement to recognize activities in almost real-time, the necessity to adapt deployed systems online to changes in behavior of the user, the variability of executing an activity, and to use data and models across people. As a result, we present feasible solutions for these problems and provide useful insights for implementing corresponding techniques. Further, we introduce approaches and novel methods for both external and wearable sensors where we also clarify limitations and capabilities of the respective sensor types. Thus, we investigate both types separately to clarify their contribution and application use in respect of recognizing different types of activities in a real world scenario. 
 
Overall, our comprehensive experiments and discussions show on the one hand the feasibility of physical activity recognition but also recognizing complex activities in a real world scenario. Comparing our techniques and results with existing works and state-of-the-art techniques also provides evidence concerning the reliability and quality of the proposed techniques. On the other hand, we also identify promising research directions and highlight that combining external and wearable sensors seem to be the next step to go beyond activity recognition. In other words, our results and discussions also show that combining external and wearable sensors would compensate weaknesses of the individual sensors in respect of certain activity types and scenarios. Therefore, by addressing the outlined problems, we pave the way for a hybrid approach. Along with our presented solutions, we conclude our work with a high-level multi-tier activity recognition architecture showing that aspects like physical activity, (emotional) condition, used objects, and environmental features are critical for reliable recognizing complex activities.",,2019,,201711455,semantic_scholar
cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,https://www.semanticscholar.org/paper/cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,Comparing Human-Robot Proxemics between Virtual Reality and the Real World,"Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of Human-Robot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. Comparing Human-Robot Proxemics between Virtual Reality and the Real World Rui Li KTH Royal Institute of Technology Stockholm, Sweden Rui3@kth.se ABSTRACT Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other.Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. INTRODUCTION Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI) [1][2][3][4]. VR has been used to test teleoperation and collect demonstration data to train machine learning algorithms, which showcased the effectiveness of learning visuomotor skills using data collected by consumer-grade devices [1]. VR teleoperation systems were proposed to crowdsource robotic demonstrations at scale [2]. A VR simulation framework was also proposed to replace the physical robot, as VR can enable high level abstraction in embodiment and multimodal interaction [3]. VR has also been used as a rapid prototyping tool to design in-vehicle interactions and interfaces for self-driving cars, which showed the evocation to genuine responses from test participants [4]. Compared to other HRI experiment methods, VR as an emerging interactive media provides unique advantages. VR HRI has the potential of having higher immersion and fidelity than picture based HRI, video-based HRI and simulated HRI. In situations where the perception of the robot is challenging, compared to on-screen viewing, VR display showed significant improvement on collaborative tasks [5]. When comparing VR HRI to the physical, realworld interaction (Live HRI), there is a trade-off between the two. VR experiences still cannot replace physical experiences due to system limitation, and limited interaction modalities etc. [6]. For example, system limitations such as limited field of view and low display resolution could reduce immersion and presence of the VR experience, resulting in different behaviors from Live experiments. Limited interaction modalities, such as the absence of touch, means that the participant could not feel the robot or even go through the robot, which could potentially break the entire interaction. Figure 1: Photograph of the Live experiment setting However, with the help of the distribution of consumer-grade VR devices and online crowdsourcing platforms, VR HRI has the potential to gain massive data for training robotic behavior and studying HRI related issues. Data collection through VR can also reduce noise and improve the data quality [1], which help to ease data processing and algorithm training. Furthermore, VR HRI experiments can test concepts and interactions without physical robots, making it more resource efficient and less expensive than Live HRI. Less hardware also means that the experiment will be less cumbersome to set up, easier to be reproduced and to ensure experiment quality. In this study, HRI Proxemics (the preferred personal space between a human and a robot) was compared to give a better justification and more basic understanding of the relationship between Live and VR. Proxemics preferences rely on lower level intuition [7], therefore, reflect the differences in the perceptions between Live and VR better. Compared to other HRI subject such as conversational (audio) or gaze behavior (visual), which are more modality dependent, proxemics can give a comprehensive understanding of the human responses. In addition, variations of modalities in VR can greatly influence human perception. For example, a higher visual familiarity of the physical environment in VR can decrease the effect of distance distortion [8]. Auditory inputs play another important role in VR, the addition of spatial sound can increase the sense of presence in VR and provide sound localization [9]. Thus, this work also compares VR settings with variance in modalities to evaluate the impacts of visual familiarity and spatial sound on VR HRI experiments. A 2 x 3 mixed design experiment was conducted to evaluate the differences between Live and VR HRI, as well as the influence of visual familiarity and spatial sound in VR. For the Live HRI, the pepper robot from Softbank Robotics was used (Figure 1). In the VR HRI, a 3D model of the same robot was used. To measure visual familiarity, the VR scene was created in Blender based on a 3D scan of the physical lab. The spatial sound was created by enabling the movement of the physical robot, due to the difficulties of engineering spatial sound. The interaction was implemented in Unity. As an objective measurement for proxemics preference, the minimum comfort distance (MCD) was measured. In addition, for the psychological perception of the experience, the feeling of presence was measured with the SUS questionnaire. For the perception of the robot, two relevant factors, competence and discomfort was measured with the ROSAS questionnaire.",,2018,,52210376,semantic_scholar
6e04016b9502b3bf8ccc10f5ae775c5e12531ec8,https://www.semanticscholar.org/paper/6e04016b9502b3bf8ccc10f5ae775c5e12531ec8,Pluggable real world interfaces Physically enabled code deployment for networked sensors,"In this paper we present a novel abstraction and deployment process using real world interfaces, which reflect the realities of pervasive software development. Pluggable real world interfaces support ldquoplugpsilanpsilaplayrdquo deployment for sensor-augmented hardware and provide an object-oriented encapsulation of high-level contextual interfaces. The architecture adds an additional object based abstraction layer between the sensor subsystem (delivering e.g. cues) and the application (delivering the situation context). Component abstraction layers are implemented as code that comes with physical components, e.g. a chair, and provides the functionality for detecting context bundled with the sensory hardware. The approach will lead to pluggable real world interfaces: The functionality of an appliance will be composed from the functionality of its components - just like a meeting room will be composed from many chairs. This paper will present concept, architecture and a first implementation based on a Java run-time system for very tiny, very low-power embedded sensor nodes.",2008 5th International Conference on Networked Sensing Systems,2008,10.1109/INSS.2008.4610909,34946413,semantic_scholar
5e171bf6603a03fbfdf3434abcd29496a2327100,https://www.semanticscholar.org/paper/5e171bf6603a03fbfdf3434abcd29496a2327100,A Learning Analytics Conceptual Framework for Augmented Reality-Supported Educational Case Studies,"The deployment of augmented reality (AR) has attracted educators’ interest and introduced new opportunities in education. Additionally, the advancement of artificial intelligence has enabled educational researchers to apply innovative methods and techniques for the monitoring and evaluation of the teaching and learning process. The so-called learning analytics (LA) discipline emerged with the promise to revolutionize traditional instructional practices by introducing systematic and multidimensional ways to improve the effectiveness of the instructional process. However, the implementation of LA methods is usually associated with web-based platforms, which offer direct access to learners’ data with minimal effort or adjustments. On the other hand, the complex nature of immersive technologies and the diverse instructional approaches which are utilized in different scientific domains have limited the opportunities for research and development in this direction. Within these research contexts, we present a conceptual framework that describes the elements of an LA process tailored to the information that can be gathered from the use of educational applications, and further provide an indicative case study for AR-supported educational interventions. The current work contributes by elucidating and concretizing the design elements of AR-supported applications and provides researchers and designers with guidelines on how to apply instructional strategies in (augmented) real-world projects.",Multimodal Technol. Interact.,2021,10.3390/MTI5030009,233835571,semantic_scholar
7c58a8f8c6611c1fc7191e5cc1b52cc3819387de,https://www.semanticscholar.org/paper/7c58a8f8c6611c1fc7191e5cc1b52cc3819387de,A Cloud-based Deep Learning Framework for Remote Detection of Diabetic Foot Ulcers,"This research proposes a mobile and cloud-based framework for the automatic detection of diabetic foot ulcers and conducts an investigation of its performance. The system uses a cross-platform mobile framework which enables the deployment of mobile apps to multiple platforms using a single TypeScript code base. A deep convolutional neural network was deployed to a cloud-based platform where the mobile app could send photographs of patient’s feet for inference to detect the presence of diabetic foot ulcers. The functionality and usability of the system were tested in two clinical settings: Salford Royal NHS Foundation Trust and Lancashire Teaching Hospitals NHS Foundation Trust. The benefits of the system, such as the potential use of the app by patients to identify and monitor their condition are discussed. May/June 2019 1 ar X iv :2 10 5. 07 76 3v 1 [ cs .L G ] 1 7 M ay 2 02 1 DIABETES MELLITUS is a chronic metabolic disorder, and a growing world-wide epidemic [1]. Diabetic Foot Ulcers (DFU) are wounds developed on the feet that represent serious chronic complications resulting from diabetes, and are prone to high levels of recurrence and infection [2]. There are numerous potential contributing factors to the development of DFU, with diagnosis, monitoring, and treatment programmes requiring multidisciplinary medical expertise. Feet of diabetic patients are more susceptible to injury and chronic wounds, resulting in skin damage and ultimately development of a DFU [3]. Patients with an active DFU, or at high-risk of developing a DFU require frequent foot checks by healthcare professionals and referral to specialists to prevent additional severe complications [4]. DFU can result in serious lifestyle repercussions, resulting in immobility, social stigma, social isolation, increased mortality, and significant costs to healthcare systems [5], with hospitalisation constituting the most expensive part of treating DFU infections [6]. More than half of DFUs become infected, with approximately 20% of moderate or severe DFU infections leading to lower extremity amputation [7]. The cost of health care in England for DFU and amputation in 2014-2015 is estimated at £1 billion. This constitutes approximately 1% of the entire National Health Service (NHS) budget [8]. The lower bound of DFU and associated amputation cost estimates is higher than the combined NHS expenditure in England on breast, prostate, and lung cancers [9]. In the United States, the direct costs of treating DFU exceed the treatment costs of many common cancers [7]. Given the significant and growing impact of DFU, mobile health solutions that target this condition could assist in improving patient quality of life. Up to 80% of DFU are thought to be preventable through early detection [10]. Promotion of patient self care and continuous monitoring for those most at risk, increasing rates of early intervention to reduce the severity and impact of DFU, could provide significant cost savings for healthcare systems. Self-management programmes have been associated with improved health outcomes, with mobile technologies identified as an important factor in helping to deliver self-management interventions that are adaptable, low cost and easily accessible [11]. Due to the continued significant increase in reported global cases of diabetes and DFU, research in this area has also seen notable growth. As a result, the use of deep learning algorithms for automated analysis of DFU have become more prominent, particularly from our group over the last few years [12], [13], [14], [15]. Goyal et al. have created and validated deep convolutional neural networks (CNNs) capable of DFU classification [12], semantic segmentation [13] and localisation [14]. These models have been shown to have high levels of sensitivity, specificity and mean average precision (mAP) in experimental settings. This paper proposes a cloud-based deep learning framework for remote detection of diabetic foot ulcers. To address the issues above, our framework includes: • A cross-platform mobile app used for capturing photographs of DFU (a non-contact solution) capable of sending diagnosis requests to a cloud service • A cloud-platform that mobile clients can connect to capable of inference using one or more CNNs to provide a diagnosis To assess the usability and reliability of such a system, we completed a proof-of-concept clinical evaluation using mobile and cloud technologies at two UK sites: Salford Royal NHS Foundation Trust and Lancashire Teaching Hospitals NHS Foundation Trust. Prior to starting the proofof-concept clinical evaluation, we obtained ethical approval from Salford Royal NHS Foundation Trust (REF: S19HRANA37) and Lancashire Teaching Hospitals NHS Foundation Trust (REF: SE-281). WHY CLOUD? The unprecedented growth of the global smartphone market over the last decade has been mirrored by the more recent emergence and rapid expansion of enterprise Cloud Computing Platforms (CCP). CCP provide on-demand computing, storage and software accessible over the internet, allowing for the remote offloading of process-intensive tasks. This approach to server technology is an increasingly common long-term 2 © 2019 IEEE Published by the IEEE Computer Society IT Professional strategy for replacing the traditional manually maintained client-server hardware set-up [16]. A clear advantage of CCPs are that they allow users of mobile devices to gain access to significant processing power, well beyond the means of any existing mobile device. This approach allows for patients to use even very dated mobile hardware to access the latest advances in automated medical image analysis. This essentially means that continual advances in this field are not tied to the computing capability of mobile devices, as such devices are simply consuming services from CCPs. Additionally, scalability becomes easier to manage, given the virtualised nature of cloud services. There is a growing trend in the use of ensemble CNNs in medical image analysis, whereby multiple CNNs are used to form a final prediction. Distributing mobile apps that use multiple models is not practical or possible given the limited permissible size of apps when distributed via online app stores. There is also the issue of intellectual property protection. Android apps are particularly easy to reverse engineer, so having the CNNs run on the server instead of the user’s mobile device means that trained models are never publicly exposed. SYSTEM ARCHITECTURE The two major components created for the evaluation were (1) a cross-platform mobile app, and (2) a cloud-based deep learning framework that performed inference on foot photographs sent from mobile clients. A cross-platform framework was chosen for the development of the mobile client since the ultimate goal of this research is to provide patients with a means of remotely monitoring and diagnosing DFU using their own smart-phones, which primarily comprise of Android or iOS devices. An overview of the system physical architecture is shown in Fig. 1. The following sections describe how these components were utilised in the creation of our proposed framework. Mobile App Cross-platform development can help to reduce the time and costs associated with developing apps for multiple mobile platforms. The mobile app developed for our evaluation was created using Ionic, a cross-platform framework based on the earlier Cordova framework. Screens within Ionic apps are rendered onto a standard WebView, in the same way that web pages are rendered in web browsers. There are also native elements within the framework however, including the ability to interface with the device’s hardware features, such as sensors and cameras. Fig. 2 shows the main data capture screens within the mobile app. The primary objective of our initial proof of concept evaluation was to determine the usability and reliability of our cross-platform mobile client and cloud-based framework in real-world settings. Ease of use was a primary motivating factor behind the design of the mobile app. Screens within the app display context-sensitive information in the form of an information bar at the top of each screen that was used to guide the user through the process of acquiring and uploading foot photographs. The UI and validation were designed so that it was not possible for the user to take the wrong action. Examples of this include: • It was not possible to retake a photograph for the current foot if one had already been taken and uploaded. • The user could not upload a photograph for any foot more than once. • It was not possible to change left foot “checked” tickbox if the left foot photo had been uploaded. • It was not possible to change right foot “checked” tickbox if the right foot photo had been uploaded. Ionic utilises a Model View Controller (MVC) architecture, implemented using Angualr.js, which separates data, presentation of data and business logic. App data, including application state, is stored in a local SQLite database. Oracle Mobile Cloud Service Software Development Kit (SDK) Oracle provides a SDK for several mobile development platforms, including Ionic, which enables mobile clients to interface with Oracle Mobile Hub (OMH). The Oracle Mobile Cloud Service SDK is a HyperText Transfer Protocol Secure client layer, through which requests can be made to OMH and associated services using JavaScript Object Notation (JSON) via REpre-",IEEE Pervasive Computing,2021,10.1109/MPRV.2021.3135686,234742192,semantic_scholar
a3040b1044097d903991c03cde30a0600ce53d0c,https://www.semanticscholar.org/paper/a3040b1044097d903991c03cde30a0600ce53d0c,RFID and Arduino : Managing RFID Events on a Real World Prototype,"Radio Frequency Identification (RFID) is a technology that is growing fast, becoming part of our daily lives more often and opening way to the Internet of Things. It can identify physical objects automatically, almost in real-time. Unfortunately, the learning curve for the technology is steep and a variety of tools are necessary just to implement a simple prototype. This paper provides an introduction to RFID through the implementation of a prototype, covering both hardware and software. We have deployed open-source software to monitor an Arduino robot carrying tagged objects in a small “warehouse”.",,2012,,203691895,semantic_scholar
976ae262c8e88c61462f673383da3649f935cebc,https://www.semanticscholar.org/paper/976ae262c8e88c61462f673383da3649f935cebc,QoS-Aware Placement of Deep Learning Services on the Edge with Multiple Service Implementations,"Mobile edge computing pushes computationally-intensive services closer to the user to provide reduced delay due to physical proximity. This has led many to consider deploying deep learning models on the edge – commonly known as edge intelligence (EI). EI services can have many model implementations that provide different QoS. For instance, one model can perform inference faster than another (thus reducing latency) while achieving less accuracy when evaluated. In this paper, we study joint service placement and model scheduling of EI services with the goal to maximize Quality-of-Servcice (QoS) for end users where EI services have multiple implementations to serve user requests, each with varying costs and QoS benefits. We cast the problem as an integer linear program and prove that it is NP-hard. We then prove the objective is equivalent to maximizing a monotone increasing, submodular set function and thus can be solved greedily while maintaining a (1 – 1/e)-approximation guarantee. We then propose two greedy algorithms: one that theoretically guarantees this approximation and another that empirically matches its performance with greater efficiency. Finally, we thoroughly evaluate the proposed algorithm for making placement and scheduling decisions in both synthetic and real-world scenarios against the optimal solution and some baselines. In the real-world case, we consider real machine learning models using the ImageNet 2012 data-set for requests. Our numerical experiments empirically show that our more efficient greedy algorithm is able to approximate the optimal solution with a 0.904 approximation on average, while the next closest baseline achieves a 0.607 approximation on average.",2021 International Conference on Computer Communications and Networks (ICCCN),2021,10.1109/ICCCN52240.2021.9522156,233476205,semantic_scholar
b42fd5d2959d327a2eaa28784b744f74f6b4e6b7,https://www.semanticscholar.org/paper/b42fd5d2959d327a2eaa28784b744f74f6b4e6b7,Configuration Management Best Practices: Practical Methods that Work in the Real World,"Successfully Implement High-Value Configuration Management Processes in Any Development Environment As IT systems have grown increasingly complex and mission-critical, effective configuration management (CM) has become critical to an organizations success. Using CM best practices, IT professionals can systematically manage change, avoiding unexpected problems introduced by changes to hardware, software, or networks. Now, todays best CM practices have been gathered in one indispensable resource showing you how to implement them throughout any agile or traditional development organization. Configuration Management Best Practices is practical, easy to understand and apply, and fully reflects the day-to-day realities faced by practitioners. Bob Aiello and Leslie Sachs thoroughly address all six pillars of CM: source code management, build engineering, environment configuration, change control, release engineering, and deployment. They demonstrate how to implement CM in ways that support software and systems development, meet compliance rules such as SOX and SAS-70, anticipate emerging standards such as IEEE/ISO 12207, and integrate with modern frameworks such as ITIL, COBIT, and CMMI. Coverage includes Using CM to meet business objectives, contractual requirements, and compliance rules Enhancing quality and productivity through lean processes and just-in-time process improvement Getting off to a good start in organizations without effective CM Implementing a Core CM Best Practices Framework that supports the entire development lifecycle Mastering the people side of CM: rightsizing processes, overcoming resistance, and understanding workplace psychology Architecting applications to take full advantage of CM best practices Establishing effective IT controls and compliance Managing tradeoffs and costs and avoiding expensive pitfalls Configuration Management Best Practices is the essential resource for everyone concerned with CM: from CTOs and CIOs to development, QA, and project managers and software engineers to analysts, testers, and compliance professionals. Praise for Configuration Management Best Practices Understanding change is critical to any attempt to manage change. Bob Aiello and Leslie Sachss Configuration Management Best Practices presents fundamental definitions and explanations to help practitioners understand change and its potential impact. Mary Lou A. Hines Fritts, CIO and Vice Provost Academic Programs, University of Missouri-Kansas City Few books on software configuration management emphasize the role of people and organizational context in defining and executing an effective SCM process. Bob Aiello and Leslie Sachss book will give you the information you need not only to manage change effectively but also to manage the transition to a better SCM process. Steve Berczuk, Agile Software Developer, and author of Software Configuration Management Patterns: Effective Teamwork, Practical Integration Bob Aiello and Leslie Sachs succeed handsomely in producing an important book, at a practical and balanced level of detail, for this topic that often goes without saying (and hence gets many projects into deep trouble). Their passion for the topic shows as they cover a wonderful range of topicseven culture, personality, and dealing with resistance to changein an accessible form that can be applied to any project. The software industry has needed a book like this for a long time! Jim Brosseau, Clarrus Consulting Group, and author of Software Teamwork: Taking Ownership for Success A must read for anyone developing or managing software or hardware projects. Bob Aiello and Leslie Sachs are able to bridge the language gap between the myriad of communities involved with successful Configuration Management implementations. They describe practical, real world practices that can be implemented by developers, managers, standard makers, and even Classical CM Folk. Bob Ventimiglia, Bobev Consulting A fresh and smart review of todays key concepts of SCM, build management, and related key practices on day-to-day software engineering. From the voice of an expert, Bob Aiello and Leslie Sachs offer an invaluable resource to success in SCM. Pablo Santos Luaces, CEO of Codice Software Bob Aiello and Leslie Sachs have a gift for stimulating the types of conversation and thought that necessarily precede needed organizational change. What they have to say is always interesting and often important. Marianne Bays, Business Consultant, Manager and Educator",,2010,,109967709,semantic_scholar
1690f9b7c9a39357177b4532592ae46b4ea11434,https://www.semanticscholar.org/paper/1690f9b7c9a39357177b4532592ae46b4ea11434,Online Reinforcement Learning Control of an Electromagnetic Manipulator,"Machine Learning Control is a control paradigm that applies Artificial Intelligence methods to control problems. Within this domain, the field of Reinforcement Learning (RL) is particularly promising, since it provides a framework in which a control policy does not have to be programmed explicitly, but can be learned by an intelligent controller directly from real-world data, allowing to control systems that are either arduous or even impossible to model analytically. However, in spite of such considerable potential, the RL paradigm poses a number of challenges that effectively hinder its applications in the real-world and in industry. It is therefore critical that research in this field is advanced until RL-based controllers can be practically demonstrated to be real-world feasible and reliable. This thesis report presents the attempts made at applying control strategies based on Reinforcement Learning to solve a precise positioning task with a physical experimental setup. The setup at hand is a magnetic manipulator (magman) characterized by a high degree of nonlinearity. The controller uses the spatially continuous magnetic field generated by four actuators to displace a steel ball, constrained to move in one dimension, towards a reference position. Two different implementations of the Q-learning algorithm (Sutton, Barto, et al., 1998) were deployed. In spite of the good results obtained in a simplified simulated environment, both implementations failed on the experimental setup. The negative outcome of these experiments is mainly due to the fact that, since the task at hand is an accurate positioning task, the reward obtained by the learner while interacting with the environment is too sparse for it to be able to learn a stabilizing control policy. Other factors have presumably contributed to the controllers’ failure, such as the circumstance that the agent does not have access to the full system state information and a sub-optimal tuning of the algorithms’ hyper-parameters. Besides model-free RL, the Value Iteration model-based method was successfully applied both in simulations and with the experimental setup. The present findings suggest that, in order to solve the magman task with model-free RL, more sophisticated algorithms need to be deployed, such as for example an agent that can naturally deal with continuous state and action spaces, as the DDPG algorithm (Lillicrap et al., 2015), with exploration carried out in the parameter-space rather than in the control action space (Plappert et al., 2017), in addition to a more optimal exploitation of the information extracted from the environment, for example using Hindsight Experience Replay (Andrychowicz et al., 2017).",,2019,,209064277,semantic_scholar
1592204ecb311b4e70b9d79f9722a7878e04b886,https://www.semanticscholar.org/paper/1592204ecb311b4e70b9d79f9722a7878e04b886,Agile Requirements Engineering and Software Planning for a Digital Health Platform to Engage the Effects of Isolation Caused by Social Distancing: Case Study,"Background Social distancing and shielding measures have been put in place to reduce social interaction and slow the transmission of the coronavirus disease (COVID-19). For older people, self-isolation presents particular challenges for mental health and social relationships. As time progresses, continued social distancing could have a compounding impact on these concerns. Objective This project aims to provide a tool for older people and their families and peers to improve their well-being and health during and after regulated social distancing. First, we will evaluate the tool’s feasibility, acceptability, and usability to encourage positive nutrition, enhance physical activity, and enable virtual interaction while social distancing. Second, we will be implementing the app to provide an online community to assist families and peer groups in maintaining contact with older people using goal setting. Anonymized data from the app will be aggregated with other real-world data sources to develop a machine learning algorithm to improve the identification of patients with COVID-19 and track for real time use by health systems. Methods Development of this project is occurring at the time of publication, and therefore, a case study design was selected to provide a systematic means of capturing software engineering in progress. The app development framework for software design was based on agile methods. The evaluation of the app’s feasibility, acceptability and usability shall be conducted using Public Health England's guidance on evaluating digital health products, Bandura’s model of health promotion, the Reach Effectiveness Adoption Implementation Maintenance (RE-AIM) framework and the Nonadoption, Abandonment and Challenges to the Scale-up, Spread and Suitability (NASSS) framework. Results Making use of a pre-existing software framework for health behavior change, a proof of concept was developed, and a multistage app development and deployment for the solution was created. Grant submissions to fund the project and study execution have been sought at the time of publication, and prediscovery iteration of the solution has begun. Ethical approval for a feasibility study design is being sought. Conclusions This case study lays the foundations for future app development to combat mental and societal issues arising from social distancing measures. The app will be tested and evaluated in future studies to allow continuous improvement of the app. This novel contribution will provide an evidence-based exemplar for future app development in the space of social isolation and loneliness.",JMIR public health and surveillance,2020,10.2196/19297,217549025,semantic_scholar
e93cc9ebed4a2a18b72d30fe5a6dad40442f9130,https://www.semanticscholar.org/paper/e93cc9ebed4a2a18b72d30fe5a6dad40442f9130,Big data analytics in Industry 4.0 ecosystems,"The emergence of advanced technologies has triggered a sweeping digital transformation in the industrial ecosystem. The cutting-edge technologies (like, Internet of things, big data, artificial intelligence, drones, cyber-physical systems, and augmented reality, and computer vision) are key enablers of this industrial revolution. Industry 4.0 has reshaped the conventional manufacturing and production processes into automated operations and workflows. This industrial transition is fueled by advanced computing (cloud and edge computing), analytic (big data analytics and computational analytics), intelligent (machine and deep learning), and communication (programmable and intelligent networks) infrastructure and technologies. The collection, aggregation, analysis, and processing of big data generated from the industrial periphery (like manufacturing equipment and maintenance systems) enable real-time decision-making and autonomous opportunities. However, the voluminous size, variability, and frequency of this data bring a wide array of disputes and oppositions in the resource-limited Industrial systems. Moreover, the continuous decision-making workflow in production and manufacturing segments increases the sharing of data across different functions, systems, and organizational boundaries. For this reason, cloud computing and big data technologies (Hadoop and Map-Reduce) can improve the anticipated response and reaction times. Industry 4.0 will lead toward more devices enriched with embedded computing platforms which boost the capabilities of the overall workflow. But, this also leads towards an increased communication and interaction between these devices which can end up in various challenges for the underlying network infrastructure. However, the conventional communication protocols may end up in various performance bottlenecks which in turn can increase the threat from different kinds of attacks and security challenges. Concluding the above discussion, the industrial ecosystem would rely on two entities: (1) users or infrastructure (physical world) and (2) cloud-enabled algorithms and autonomous systems (virtual world) that are connected through advanced and autonomous communication technologies. The driving force behind the success of these industrial ecosystems relies on the efficient gathering/collection, analysis, and storage of data generated by smart devices and sensors. Under this domain, big data analytics is set to be driving predictive manufacturing and provide timely detection of anomalies and system failures to predict product quality. In this way, big data is bound to play a prominent role in driving the industrial ecosystem. Even more, the only reason for this concern is not limited to the volume of data but the major concern is the contribution of this data for the design and implementation of efficient industrial processes and policies. The interpretation and understanding of the available data help to the design of efficient processes and policies related to industrial systems. The focus of this special issue is to present novel and seminal contributions around the important issues and challenges related to big data management and analytics for industrial 4.0 ecosystems. It provides ground-breaking research from academia and industry, that emphasizes the novel solutions, applications, tools, software, and algorithms designed to handle the industrial big data. A substantial number of submissions were received for the special issue. The papers were reviewed by at least three reviewers and underwent a rigorous two rounds of reviews. After the completion of the peer review process, we have accepted 10 seminal contributions related to big data analytics for Industry 4.0. All the accepted papers either discuss the recent solutions related to big data analytics or proposes an innovative way of handling big data across diverse infrastructure deployments. The outline of these contributions discussed below. The first paper titled “An Efficient Scheme for Secure Feature Location using Data Fusion and Data Mining in IOT Environment” by Balaji et al.1 proposes a secure feature location approach based on data fusion and data mining to overcome the challenges of the existing textual and dynamic approaches. The first step in this approach involves the removal of repeated test cases followed by the selection of important attributes. The artificial flora optimization algorithm was used to remove the repeated test cases. After this, the Caesar Cipher-RSA algorithm was used to encrypt the selected attributes, and thereafter a score value was assigned to them. This score value acts as an input to the K-mean algorithm to normalize it using the min-max approach. The evaluation results show that the proposed approach is superior in comparison to existing variants. The second paper titled “Data Dimensionality Reduction Techniques for Industry 4.0: Research Results, Challenges, and Future Research Directions” by Chhikara et al.2 provides a comprehensive survey on dimensionality reduction techniques.",Softw. Pract. Exp.,2021,10.1002/spe.3008,236305446,semantic_scholar
e31589522b066d9096e67a2c21ef593039041795,https://www.semanticscholar.org/paper/e31589522b066d9096e67a2c21ef593039041795,A Battery Digital Twin framework for Predictive Maintenance and State of Health Estimation of Electric Vehicles,"To maximize the performance of Electric Vehicle (EV) battery, the requirements of the battery management system (BMS) are getting higher and higher, especially in terms of safety, predictive maintenance, and battery life. The on-board BMS cannot store or process large amounts of data during the operation of a vehicle, with poor real-time capability and data utilization rate. To effectively manage the battery, it is vital to build an Off-board digital twin that can mimic the actual battery with more intelligence. The paper proposes the digital twin framework of Li-Ion battery packs for a fleet of vehicles. This work presents the digital model of the battery, data driven models, contextual information, operational data and the cloud-based deployment. To extend the lifetime of the battery and bring more security into the system, an anomaly detection technique is proposed by using a machine intelligence approach that captures the temporal and spatial relationship between various battery parameters. Also, a learning-based prediction technique is proposed to estimate the health status of battery. The paper outlines the design methodology followed, challenges faced, drawbacks and further opportunities involved in developing the framework for creating a battery digital twin. The performance of the system is analysed with NASA prognostic data set and from the vehicle’s plant model with various drive cycles. Introduction A digital twin is a virtual representation of a physical asset. The digital twin technology helps the global industries to develop a digital DNA of their assets; ranging from as small as smartphone, to as large as a city and as complicated as a human, spanning across industries and processes. This cutting-edge technology helps the companies to optimize performance, maintenance and achieve better results. It came into prominence only upon the advent of other technologies like cloud, artificial intelligence, machine learning etc. VDI-Berichte Nr. 2384, 2021 579 https://doi.org/10.51202/9783181023846-579 Generiert durch IP '54.191.176.224', am 07.11.2021, 18:57:40. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. The electric vehicles (EV) are the future, and the automakers are investing more on to their EV sector nowadays. The key performance component of an EV is its battery. The electric battery is a merger of chemical, thermal, and mechanical processes. The lifetime of these devices depends greatly on the materials used, the system design and the operating conditions. This complexity has therefore made real-world control of battery systems challenging. Timely preparation for future eventualities is a cornerstone for managing batteries in an EV. The battery management system (BMS) of an EV receives considerable amount of data, and processing them is computationally intensive and requires more memory and processing power. Such processing would be difficult to be contained within the on-board systems, and all this data has to be computed elsewhere. With the advent of the Internet of Things and wireless communication in automotive, the information can be stored in the cloud, offering relentless computational power. However, sending the battery data separates it from the physical battery. The battery is still on-board, while the data is offboard. To effectively manage the battery, it is vital to build an off-board digital system that can resemble the actual battery with more intelligence. Due to aging, the parts of a vehicle are invariably at the risk. Not knowing the risk earlier can keep your car out of the road from sometime, the sooner the better. Electric vehicles reduced the output emissions, but the safety side of the batteries are certainly a costlier affair than the internal combustion engine, considering the multiple facets to include such as the chemicals used, the thermal state, mechanical parts, etc. The frequency and cost of regular maintenance service can be reduced if able to diagnose the issues in advance using digital techniques. Such a system identifies the risks and hinders it from becoming an issue, which not only reduces the downtime, but also the repair costs. If the system is able to warn early, component damage can be eliminated to an extent. The common type of batteries used in EV; the Lithium-ion cells are classified as class 9 hazardous materials. The safety risks involved due to thermal hazards alone from the battery perspective opens up a whole new world of risks; let alone the maintenance due to mechanical failure. Predicting a potential risk and subsequent drive to the mechanic ensures your vehicle in mint condition. Hence, this paper proposes the digital twin framework of Li-Ion battery packs for a fleet of vehicles for predictive maintenance. Our contributions in a nutshell:  Developed a digital twin framework of EV batteries for operating and diagnosing a fleet of vehicles VDI-Berichte Nr. 2384, 2021 580 https://doi.org/10.51202/9783181023846-579 Generiert durch IP '54.191.176.224', am 07.11.2021, 18:57:40. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig.  Developed the State of health (SOH) prediction system of battery for the fleet of vehicles using battery digital twin  Developed an intelligent anomaly detection system to analyse the unusual nature of battery behaviour  Developed the digital twin framework and conducted performance evaluation using NASA dataset and data set generated using plant model. Related Works Even though the battery management system of EVs is well discussed in the literature, digital twin-based battery diagnosis is in the primitive stage only. Tanizawa et al. [6] propose a cloud-connected battery management system that continuously connects the batteries to the cloud, manages their state of charge and monitors changes in its characteristics. Koko Friansa et al. [2] proposes a battery monitoring system to monitor the operational and performance of batteries in a small microgrid system. Taesic Kim et al. [5] proposes a cloud-based battery condition monitoring and fault diagnosis platform for largescale lithium-ion battery energy storage systems. Weihan Li et al. [4] proposes a cloud battery management system with online state-of-charge and state-of-health estimates. Billy Wu et al. [1] discusses their perspectives on battery modelling, data-driven approaches and how these elements can be combined in a framework for creating a battery digital twin. Shichun Yang et al. [3] proposes a framework utilizing a cloud architecture for a cloud-based battery management system based on Cyber Hierarchy and Interactional Network (CHAIN) to leverage the use of algorithms that can be used to realize the state-of-X-estimation, thermal management, and other functions of traditional BMS system. The aforementioned works do not delve upon even the scantiest possibilities of irregularity in the battery data. They all assume that the data from the physical asset is in its pristine form. This paper focus on building a digital twin framework and an anomaly detection system by using a machine intelligence approach that captures the temporal and spatial relationship between various battery parameters. The paper also proposes to estimate the State of the health of the EV battery using Long Short-Term Memory (LSTM), a machine learning approach. Proposed Digital Twin Architecture The digital twin architecture of battery system consists of mainly two parts. First part is the physical system of Electric vehicle battery with real-world data and other is the twin of the system implemented on cloud infrastructure. The vehicle will be added to the digital twin VDI-Berichte Nr. 2384, 2021 581 https://doi.org/10.51202/9783181023846-579 Generiert durch IP '54.191.176.224', am 07.11.2021, 18:57:40. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. eco-system when the driver completes the initial registration process with vehicle information. Successful registration will create an instance of the digital twin in cloud, specific to the vehicle. This way, it is possible to manage multiple vehicles at the same time by creating instances of the digital twin.",ELIV 2021,2021,10.51202/9783181023846-579,239553090,semantic_scholar
c3d3d7b56fcc775c96777b1c6123b46a452b4aee,https://www.semanticscholar.org/paper/c3d3d7b56fcc775c96777b1c6123b46a452b4aee,Data-Centric Programming Best Practices: Using DDS to Integrate Real-World Systems,".................................................................................................................................... 3 Real-World Systems Programming ........................................................................................... 4 Defining a Data Model .............................................................................................................. 4 DDS Maintains the State of the World as Defined by the Data Model ..................................... 6 About DDS ................................................................................................................................ 8 Best Practices in DDS Programming ........................................................................................ 9 G1. Start by defining a data model, then map the data model to DDS domains, data types and Topics. .............................................................................................................. 9 G2. Fully define your DDS Types, do not rely on opaque bytes or other custom encapsulations ................................................................................................................ 11 G3. Isolate subsystems into DDS Domains. Use mediation, such as RTI Routing Service, to bridge Domains ............................................................................................. 12 G4. Use keyed Topics. For each data type, indicate to DDS the fields that uniquely identify the data-object .................................................................................................... 13 G5. Large teams should create a targeted application platform with system-wide QoS profiles and limited access to the DDS APIs. .................................................................. 16 G6. Configure QoS using the XML Profiles .................................................................... 17 Conclusions ............................................................................................................................. 18 References .............................................................................................................................. 18 Best-Practices Data-Centric Programming: Using DDS to Integrate Real-World Systems November 2010 3 © 2010 Real-Time Innovations Abstract Systems are often implemented by teams using a variety of technologies, programming languages, and operating systems. Integrating and evolving these systems becomes complex. Traditional approaches rely on low-level messaging technologies, delegating much of the message interpretation and information management services to application logic. This complicates system integration because different applications could use inconsistent interpretations and implementations of information-management services, such as detecting component presence, state management, reliability and availability of the information, handling of component failures, etc. Integrating modern systems requires a new, modular network-centric approach that avoids these historic problems by relying on standard APIs and protocols that provide stronger information-management services. For example, many of these systems are heterogeneous, mixing a variety of computer hardware, operating systems, and programming languages. Developers often use Java, .NET, or web-scripting to develop consoles and other GUI-oriented applications, and C or C++ for specialized hardware, device drivers, and performanceor time-critical applications. The end system might mix computers running Windows, Linux, and other operating systems, such as Mac OS X, Android, or real-time operating systems like VxWorks and INTEGRITY. The use of standard APIs and interoperable protocols allows all these systems to be easily integrated and deployed. Today, these systems are typically developed using a service-oriented approach and integrated using standards-based middleware APIs such as DDS, JMS, and CORBA, and protocols such as DDS-RTPS, Web-Services/SOAP, REST/HTTP, AMQP, and CORBA/IIOP. This whitepaper focuses on “real-world” systems, that is, systems that interact with the external physical world and must live within the constraints imposed by real-world physics. Good examples include air-traffic control systems, real-time stock trading, command and control (C2) systems, unmanned vehicles, robotic and vetronics, and Supervisory Control and Data Acquisition (SCADA) systems. More and more these “real-world” systems are integrated using a Data-Centric PublishSubscribe approach, specifically the programming model defined by the Object Management Group (OMG) Data Distribution Service (DDS) specification. This whitepaper describes the basic characteristics of real-world systems programming, reasons why DDS is the best standard middleware technology to use to integrate these systems, and a set of “best practices” guidelines that should be applied when using DDS to implement these systems. Best-Practices Data-Centric Programming: Using DDS to Integrate Real-World Systems November 2010 4 © 2010 Real-Time Innovations Real-World Systems Programming Real-World systems refer to a class of software systems that operate continuously and interact directly with real-world objects, such as aircraft, trains, stock transactions, weapons, robotic and manufacturing equipment, etc. Unlike systems involving only humans and computers, real-world systems have to live within the constraints imposed by the physics of the external world. Notably, time cannot be slowed, paused, or reversed. The implication is that these systems must be able to handle the information at the pace it arrives at, as well as be robust to changes in the operating environment. In addition to these environmental considerations, the nature of typical real-world applications also places demands on their availability and need to continue operating even in the presence of partial failures. In order to interact with the real world, software must include a reasonable, if simplified, model of the external world. This model typically includes aspects of the “state of the world” relevant to system operations. Here the word “state” is used in the normal sense in software modeling and programming. State summarizes the past inputs to the system from its initial state and contains all the information necessary for a system or program to know how it should react to future events or inputs. Imagine that a new component or application starts and joins a system. The “state of the system” contains the information that this new component needs to acquire before it is ready to start performing its function. A typical component would normally only need access to a subset of that state, the portion that directly affects its operation. For example, in an air-traffic management problem, the relevant aspects of the state of the world might include the current location and trajectory of every aircraft, the flight plans of all flights within a 24-hour window, specific details on each aircraft (type, airline, crew), etc. Once a software component or subsystem is running, it interacts with other components by exposing part of its state, notifying other components when its state changes, and invoking operations on (or sending messages to) other components. Each component reacts to these information exchanges by updating its internal model of the world and using that to perform its necessary actions. Defining a Data Model A data model is simply an organized description of the state of the system. Thus, it includes data types, processes for transferring and updating those types, and methods for accessing the data. It does not typically include functions that can alter the data or (importantly) the application-level logic that affects the data. Governance organizations and system integrators often start their design by designing the system data-model. There are good reasons for this approach: • A data model provides governance across disparate teams and organizations, allowing components developed at different points in time by different organizations to be integrated. This makes it an ideal starting point for a central design or governance authority. Best-Practices Data-Centric Programming: Using DDS to Integrate Real-World Systems November 2010 5 © 2010 Real-Time Innovations • A data model represents the better understood, more invariant aspects of the system. Typically the data model is grounded in the “physics of the system.” That is, it describes the kinds of objects and sensors it manages (like aircraft locations, flight plans, and vehicle positions). The data model is not strongly tied to applicationspecific use cases (e.g., the possible fields in a flight plan are a consequence of the nature of aircraft flight); this makes the data model a good starting point, since the full set of use cases might not be well known in advance or might be the responsibility of a different team. • A data model increases decoupling between systems and components. The data model is grounded in the essential information present in the system and it does not depend so much on the use cases that access the information. For example, an airtraffic control model might include a definition of a “flight plan,” but not whether it is automatically generated using an optimization algorithm, checked for collisions, or altered in mid-flight. Using the data model as the basis for the integration avoids over-constraining the design, leaving it open to allow future evolution and use cases. Contrast this with a design based on defining service invocation APIs which are intimately tied to the details of each service and are likely to change as new use cases are incorporated Example Data Model Imagine designing a simple “chat” application. The underlying Data-Model could be defined to contain four kinds of objects summarized in the table below: Object Kind Key Fields Other Fields Description Person EmailAddress Name, Loc",,2010,,17354633,semantic_scholar
bf89fad572188e397c9bea2fae07401069da38de,https://www.semanticscholar.org/paper/bf89fad572188e397c9bea2fae07401069da38de,"Lightweight Cryptography Algorithms for Resource-Constrained IoT Devices: A Review, Comparison and Research Opportunities","IoT is becoming more common and popular due to its wide range of applications in various domains. They collect data from the real environment and transfer it over the networks. There are many challenges while deploying IoT in a real-world, varying from tiny sensors to servers. Security is considered as the number one challenge in IoT deployments, as most of the IoT devices are physically accessible in the real world and many of them are limited in resources (such as energy, memory, processing power and even physical space). In this paper, we are focusing on these resource-constrained IoT devices (such as RFID tags, sensors, smart cards, etc.) as securing them in such circumstances is a challenging task. The communication from such devices can be secured by a mean of lightweight cryptography, a lighter version of cryptography. More than fifty lightweight cryptography (plain encryption) algorithms are available in the market with a focus on a specific application(s), and another 57 algorithms have been submitted by the researchers to the NIST competition recently. To provide a holistic view of the area, in this paper, we have compared the existing algorithms in terms of implementation cost, hardware and software performances and attack resistance properties. Also, we have discussed the demand and a direction for new research in the area of lightweight cryptography to optimize balance amongst cost, performance and security.",IEEE Access,2021,10.1109/ACCESS.2021.3052867,232042514,semantic_scholar
48c6c90f708013906e4fb9e82635a94f1db386db,https://www.semanticscholar.org/paper/48c6c90f708013906e4fb9e82635a94f1db386db,DigiMobot: Digital Twin for Human-Robot Collaboration in Indoor Environments,"Human-robot collaboration and cooperation are critical for Autonomous Mobile Robots (AMRs) in order to use them in indoor environments, such as offices, hospitals, libraries, schools, factories, and warehouses. Since a long transition period might be required to fully automate such facilities, we have to deploy AMRs while improving safety in the mixed environments of human and mobile robots. In addition, human behaviors in such environments might be difficult to predict. In this paper, we present a Digital Twin for Autonomous Mobile Robots system named DigiMobot to support, manage, monitor, and validate AMRs in indoor environments. First, DigiMobot can simulate human behaviors and robot movements to verify and validate AMRs to improve safety in a virtual world. Secondly, DigiMobot can monitor and manage AMRs in the physical world by collecting sensor data from each robot in real-time. Since DigiMobot enables us to test the robot systems in the virtual world, we can deploy and implement AMRs in each facility without any modifications. To show the feasibility of DigiMobot, we develop a software framework and two different types of autonomous mobile robots. Finally, we conduct real-world experiments in a warehouse located in Saitama, Japan, in which more than 400, 000 items are stored for commercial purposes.",2021 IEEE Intelligent Vehicles Symposium (IV),2021,10.1109/iv48863.2021.9575499,240463113,semantic_scholar
2d4b9a86270a24126e734c1bf869665a6496ca8b,https://www.semanticscholar.org/paper/2d4b9a86270a24126e734c1bf869665a6496ca8b,Introduction to the ACSAC’19 Special Issue—Vol. 2,"The Annual Computer Security Applications Conference (ACSAC) brings together cutting-edge researchers, with a broad cross-section of security professionals drawn from academia, industry, and government, gathered to present and discuss the latest security results and topics. ACSAC’s core mission is to investigate practical solutions for computer and network security technology. The 35th Annual Computer Security Applications Conference was held in Puerto Rico on December 9–13, 2019. ACSAC 2019 especially encouraged contributions in the area of Deployable and Impactful Security. Deployable and impactful security solutions aim to address key real-world challenges, which may include accuracy, runtime overhead, ground-truth labeling, human aspects, usability, and energy consumption. Having the deployability and impactfulness goals motivates one to focus on solving the most critical real-world challenges, which may otherwise be ignored by the fast-moving research community. In addition, ACSAC encourages authors of accepted papers to submit software and data artifacts and make them publicly available to the entire community. Releasing software and data artifacts represents an important step toward facilitating the reproducibility of research results and ultimately contributes to the real-world deployment of novel security solutions. This special issue includes extended versions of papers that appeared at ACSAC 2019, focusing especially on research on computer security applications with a high potential for being deployed in real-world environments or that have already been deployed and used to implement practical defense systems. This volume contains six articles on topics including DNS security and privacy, anti-virus and malicious software, and IoT and cyber-physical systems security. In “PREMADOMA: An Operational Solution to Prevent Malicious Domain Name Registrations in the .eu TLD,” Desmet et al. propose a system for detecting malicious domains at registration time, before they have an opportunity to be used. PREMADOMA has already been deployed to defend the .eu country-code top-level domain (ccTLD) registrar. The evaluation includes 11 months of real-world observations from the .eu ccTLD, showing that PREMADOMA is effective at blocking a significant number malicious domain registrations. Nakatsuka et al. propose to enable end-to-end DNS privacy in an article titled “PDoT: Private DNS-overTLS with TEE Support.” The PDoT system is a DNS resolver that can run within a Trusted Execution Environment (TEE). Using remote attestation, clients can verify the TEE-based DNS resolver execution, providing a guarantee that the DNS operator running the resolver will not be able to monitor the clients’ DNS requests. To provide strong privacy guarantees, PDoT relies on the fact that the confidentiality of client-to-resolver and resolver-to-name-servers DNS queries can be protected by DNS-over-TLS (DoT), a recently proposed protocol. The evaluation presents experimental results and measurements using a PDoT prototype, demonstrating that achieving strong DNS privacy is feasible. In “Cut-and-Mouse and Ghost Control: Exploiting Antivirus Software with Synthesized Inputs,” Genç et al. present two classes of attacks against popular antivirus (AV) software: Ghost Control and Cut-and-Mouse. The first attack allows malware to avoid detection by simulating mouse events to disable the AV protection. The latter attack allows malware to trigger whitelisted applications into performing malicious activities on their behalf.",Digital Threats: Research and Practice,2021,10.1145/3437253,232115619,semantic_scholar
d54ad52d1c7ba89e9eb26e55aae36ab8bf65db91,https://www.semanticscholar.org/paper/d54ad52d1c7ba89e9eb26e55aae36ab8bf65db91,Software Physical/Virtual Rx Queue Mapping Toward High-Performance Containerized Networking,"Softwarization of Network Functions (NFs) accelerates automated deployment and management of services on next-gen networks. Combining flexibility and high-performance is a vital requirement for Network Functions Virtualisation (NFV); however, many studies have demonstrated that containerization or virtualization of NFs severely degrades the fundamental efficiency of packet forwarding. Virtual network I/O, a mechanism of packet transferring between a guest and the host, has been seen as the performance bottleneck in the PVP (Physical-Virtual-Physical) datapath, and one of the main causes of this deterioration is packet copy between them. Various techniques, such as zero-copy, pass-through, and hardware offloading, have been examined to alleviate the performance overhead. However, existing designs and implementations incur pragmatic issues, such as compatibility, manageability, and insufficient quality of performance. We propose yet another design and implementation of zero-copy/pass-through acceleration (named IOVTee) to resolve real-world problems as well as to enhance the forwarding efficiency. IOVTee takes advantage of pre-processing of virtual switches with achieving zero-copy on the receive (Rx) path. The pluggable style of IOVTee for vhost-user (the de-facto virtual network I/O) enables our approach to be transparent to both containers/VMs and virtual switches. In this article, we explain the heart of IOVTee, a fully software-based Rx queue mapping mechanism (between physical and virtual) that enables a concept of Virtual DMA Write-through (to the NF). Our evaluation results showed that applying IOVTee to vhost-user drastically increased efficiency of packet forwarding in the PVP datapath (by 45% and 98% for traffic of 64-byte and 1514-byte packets respectively).",IEEE Transactions on Network and Service Management,2021,10.1109/TNSM.2020.3049053,232236560,semantic_scholar
c54671ea3d65504d5bc7d976a63d4279bed5e1fe,https://www.semanticscholar.org/paper/c54671ea3d65504d5bc7d976a63d4279bed5e1fe,"IFogSim2: An Extended iFogSim Simulator for Mobility, Clustering, and Microservice Management in Edge and Fog Computing Environments","Internet of Things (IoT) has already proven to be the building block for next-generation Cyber-Physical Systems (CPSs). The considerable amount of data generated by the IoT devices needs latency-sensitive processing, which is not feasible by deploying the respective applications in remote Cloud datacentres. Edge/Fog computing, a promising extension of Cloud at the IoT-proximate network, can meet such requirements for smart CPSs. However, the structural and operational differences of Edge/Fog infrastructure resist employing Cloud-based service regulations directly to these environments. As a result, many research works have been recently conducted, focusing on efficient application and resource management in Edge/Fog computing environments. Scalable Edge/Fog infrastructure is a must to validate these policies, which is also challenging to accommodate in the real-world due to high cost and implementation time. Considering simulation as a key to this constraint, various software has been developed that can imitate the physical behaviour of Edge/Fog computing environments. Nevertheless, the existing simulators often fail to support advanced service management features because of their monolithic architecture, lack of actual dataset, and limited scope for a periodic update. To overcome these issues, we have developed multiple simulation models for service migration, dynamic distributed cluster formation, and microservice orchestration for Edge/Fog computing in this work and integrated with the existing iFogSim simulation toolkit for launching it as iFogSim2. The performance of iFogSim2 and its built-in policies are evaluated using three use case scenarios and compared with the contemporary simulators and benchmark policies under different settings. Results indicate that the proposed solution outperform others in service management time, network usage, ram consumption, and simulation time.",ArXiv,2021,,237494539,semantic_scholar
98d924f82af8c4fb621fca29ab56d3eac5b50c8c,https://www.semanticscholar.org/paper/98d924f82af8c4fb621fca29ab56d3eac5b50c8c,A Blockchain-Supported Framework for Charging Management of Electric Vehicles,"Profound changes driven by decarbonization, decentralization, and digitalization are disrupting the energy industry, bringing new challenges to its key stakeholders. In the attempt to address the climate change issue, increasing penetration of renewables and mobility electrification augment the complexity of the electric grid, thus calling for new management approaches to govern energy exchanges while ensuring reliable and secure operations. The emerging blockchain technology is regarded as one of the most promising solutions to respond to the matter in a decentralized, efficient, fast, and secure way. In this work, we propose an Ethereum-based charging management framework for electric vehicles (EVs), tightly interlinked with physical and software infrastructure and implemented in a real-world demonstration site. With a specifically designed solidity-based smart contract governing the charging process, the proposed framework enables secure and reliable accounting of energy exchanges in a network of trustless peers, thus facilitating the EVs’ deployment and encouraging the adoption of blockchain technology for everyday tasks such as EV charging through private and semi-private charging infrastructure. The results of a multi-actor implementation case study in Switzerland demonstrate the feasibility of the proposed blockchain framework and highlight its potential to reduce costs in a typical EV charging business model. Moreover, the study shows that the suggested framework can speed up the charging and billing processes for EV users, simplify the access to energy markets for charging station owners, and facilitate the interaction between the two through specifically designed mobile and web applications. The implementation presented in this paper can be used as a guideline for future blockchain applications for EV charging and other smart grid projects.",Energies,2021,10.3390/en14217144,242059107,semantic_scholar
83c855157830ae26e3420a8bf44877660451c8e4,https://www.semanticscholar.org/paper/83c855157830ae26e3420a8bf44877660451c8e4,On the suitability of current Augmented Reality head-mounted devices,"Simulation is a recognized and much-appreciated tool in healthcare and education. Advances in simulation have led to the burgeoning of various technologies. In recent years, one such technological advancement has been Augmented Reality (AR). Augmented Reality simulations have been implemented in healthcare on various fronts with the help of a plethora of devices including cellphones, tablets, and wearable AR headsets. AR headsets offer the most immersive experience of the AR simulation as they are head-mounted and offer a stereoscopic view of the superimposed 3D models through the attached goggles overlaid on real-world surfaces. To this effect, it is important to understand the performance capabilities of the AR headsets based on workload. In this paper, our objective is to compare the performances of two prominent AR headsets of today, the Microsoft Hololens and the Magic Leap One. We use surgical AR software that allows the surgeons to show internal structures, such as the rib cage, to assist in the surgery as a reference application to obtain performance numbers for those AR devices. Based on our research, there are no performance measurements and recommendations available for these types of devices in general yet. Introduction In an attempt to measure the feasibility and effectiveness of using AR in surgery and nursing education, we developed an application titled ARiSE (Augmented Reality in Surgery and Education) [39]. We incorporated two facets of this application, one to be used during surgery in the Operating Room (OR), and another to assist in the education of nursing students. Surgeons would use this application in the OR during rib-plating surgery and be able to visualize an accurate model of the patient’s rib cage derived from computerized tomography (CT) scans outside their body. The nursing education application would be used by nursing students during the training of fundamental cardiopulmonary physical assessment skills. The students will be able to visualize stock models of various human organs overlaid on manikins along with visual guides to correct auscultation assessment. The aforementioned applications were developed and deployed into the first generation Microsoft Hololens and Magic Leap One AR headsets for practical use. While the application was deployed successfully and demonstrated accurate usability in both devices, our objective in this paper was to measure and compare the performances between the two AR headsets to derive recommendations for when to use which of these devices. The contributions of this paper are as follows: 1. Direct comparison of head-mounted augmented reality devices from the major brands, namely Microsoft and Magic Leap. 2. Expert evaluation based on real-world application tested with the help of domain specialists. 3. Recommendations for head-mounted Augmented Reality devices. Related Work In this section, we discuss some of the other works that relate to our project and use augmented reality techniques and devices. The related work is split into separated subsections with AR being the common theme applied to different medical areas. Augmented Reality in Mobile Devices for Medical Learning Required clinical content cannot always be imparted in live settings due to various restrictions. Educators have instead started using simulation to enhance clinical education. AR simulations have been used to assist the teaching of emergency situations, procedural training, and anatomy [35]. One such AR simulation is described by Von Jan et al. [1] in their paper. The researchers present an application that may be implemented on cellphones and tablet devices that present life-like scenarios which are overlaid on real-world objects. The trainee would visualize these scenarios through their mobile or tablet devices. This application is called mARble, and the researchers report their findings that indicate that AR enhances learning in medical education settings, specifically for subjects that are visually oriented. Augmented Reality Used in Education In their paper, Steve Chi-Yin Yuen et al. [13] have implemented AR in education and training and evaluated its efficiency. The researchers discuss the applications of AR in various fields including architecture, advertising, entertainment, medicine, gaming, books, travel, and the military. With respect to medical education, their results show AR enhancing surgical procedures and aiding clinical procedures by enhancing efficiency, reducing cost, and improving safety. The researchers also state that AR ahs the potential to invent new clinical and surgical procedures. AR has been integrated with existing medical equipment by Fischer et al. in their research [17]. There has also been research that claims AR to have the potential to make surgery minimally invasive [13] and also to enhance the learning experience in educational settings [29]. Chien et al. have used AR to assist in teaching students the anatomy of a 3D skull [16]. Researchers have also demonstrated that AR may enhance the teaching of human anatomy [19]. AR in Nursing Education Wuller et. al. have reviewed existing AR research to assist nursing education [30]. Foronda et. al. have described 3 types of AR applications used to supplement nursing education [6]. Researchers use the Microsoft Hololens to overlay muscles and bones of the human anatomy on manikins. Rahn et. al., overlay 3D models of human organs in real-time on students using iPads[31]. AR was also used with the help of iPads by Abersold et. al. in their study to assist in the training of the placement of the nasogastric tube(NGT) [32]. Ferguson et. al. have claimed game-based AR applications as having the potential to enhance nursing education [33]. This is also supported by Garrett et. al. who demonstrate improved nursing and clinical skills acquisition in students who participated in AR training scenarios [34]. Simulating Surgeries Scott Delp et al. have reviewed the shortcomings of educating medical personnel in providing appropriate emergency care [2]. In their research Samset et al. [14] developed AR tools for minimal invasive therapies (MIT). Scenarios presented by them include liver surgery, liver tumors, and cardiac surgery. With the help of AR the researchers superimpose real-world objects with 3D models obtained from CT scans. Results demonstrated improved surgical procedures and hence the potential of AR to improve healthcare in terms of utility, quality, and cost-effectiveness. Other research has also been conducted with regards to using AR during surgery. Kawamata et al. describe an AR application in their research that assists in the surgery of pituitary tumors [18]. Their results demonstrated this type of AR navigation allowing surgeons to perform accurate and safe endoscopic operations on these tumors. Memory Retention While Using AR Using Steady State Topography (SST) brain imaging to examine the brain activity of people who participated in AR and non-AR tasks, Heather Andrew et. al. [12] found that the visual attention is almost double when performing AR tasks when compared to non-AR tasks. The author also found that what is stored in memory is 70% higher for AR experiences [12]. Other studies show that the long-term memory of the learner can be enhanced by using multiple media interactions in the learning process [11]. Adedukon-Shittu et. al. have also demonstrated the effectiveness of AR technology with regards to enhancing memory retention and performance [23]. Other studies have also demonstrated the enhanced knowledge acquisition and retention of adequate memory when using AR as a supplemental tool in the education process [24]. Feasibility of Using AR to Train Resuscitation Steve Balian et al. [8] introduced a method of testing the feasibility of using augmented reality to educate healthcare providers about administration of Cardio Pulmonary Resuscitation (CPR). Using the Microsoft Hololens to provide users with audio and visual feedback, the blood flow in the human body was superimposed in real time onto a manikin. The study deployed 51 volunteers for this study. The volunteering health care providers were asked to perform CPR using only the Hololens for two minutes. The chest compression parameters were then recorded for this test. The participants generally responded positively to the system. The approach was perceived to be realistic and the AR was considered a helpful tool for training in medical education. Among the volunteers, 94% stated that they would be willing to use this application for CPR training in the future. The further support the notion of AR’s usefulness in education, Balien et al. successfully demonstrated another augmented reality tool that proved to be valuable for existing education approaches in medical training[16]. Menon et al. [37] developed an augmented reality application to improve the training of nursing students that showed a measurable improvement in student outcomes. Time and again augmented reality has proven to be advantageous when integrated into education in terms of novelty, memory retention, and knowledge gained [14] [12] [23] [24] . AR Triage Training for Multi-Casualty Scenarios The order in which patients are treated can have a detrimental effect on the survival rate of a group of patients. Hence, triage, i.e. selecting the most critical patients based on their chance of survival is crucial. John Hendricks et al. [4] devised a virtual reality simulation that assists medical personnel in their training and military field medics in making appropriate decisions in triage training environments. Their model deploys a scene in which users encounter a virtual patient with multiple injury scenarios. The virtual patients can vary with respect to their injuries as well physiological conditions and these conditions can evolve with timebased on their injuries. The injuries are visually supported by animations, such as bleeding and seizures. Augmented rea",,2021,,246273508,semantic_scholar
5adc8c5ee72d9bfba627323baab2e600e492ad3b,https://www.semanticscholar.org/paper/5adc8c5ee72d9bfba627323baab2e600e492ad3b,"Design, Development, and Validation of an Augmented Reality-Enabled Production Strategy Process","The Production Strategy Process (PSP) is an integral part of production planning and control as it defines how production processes are structured and designed and outlines how production will be executed. PSP involves massive information transfer and communication among project participants. While BIM can improve the flow of information, the paradox of designing 3D models in 2D space remains. This paradox indicates that new visualization technologies are needed to leverage the use of information in the PSP. As Industry 4.0, the fourth industrial revolution, continues to evolve, it is imperative that construction firms seek, find, and adopt new technologies. This research employed Augmented Reality (AR) as a new user interface in the PSP. The current state of practice of PSP was investigated and current challenges are identified. The opportunities to integrate AR were defined, and an AR-enabled future state was proposed. Next, an AR-enabled PSP prototype using the Microsoft HoloLens was implemented and validated on a real-world healthcare project. Usability testing was then conducted using a one-on-one protocol to validate the prototype with 20 participants. Surveys were the deployed to qualitatively assess the impact of integrating AR into PSP. The difference between the traditional PSP and the AR-enabled PSP was tested through a series of hypotheses comparing both processes. The results demonstrate that the AR-enabled PSP offers significant benefits over the Traditional PSP: improved collaboration, reduced miscommunication, increased quality and detection of errors, enhanced decision-making, better documentation, better information access, improved information flow, increased input accuracy, and increased integration of safety considerations. Additionally, the technology, software, and hardware were also evaluated, and, on average, the findings demonstrated the potential of AR in production planning.",Frontiers in Built Environment,2022,10.3389/fbuil.2022.730098,246491609,semantic_scholar
036a7c42a459eb751bba2f8badec3b62d6328c10,https://www.semanticscholar.org/paper/036a7c42a459eb751bba2f8badec3b62d6328c10,EDITORIAL Wireless sensor networks: design for real-life deployment and deployment experiences Wireless sensor networks: design for real-life deployment and deployment experiences,"Wireless sensor networks (WSNs) are among the most promising technologies of the new millennium. The opportunities afforded by being able to program networks of small, lightweight, low-power, computation- and bandwidth-limited nodes have attracted a large community of researchers and developers. However, the unique set of capabilities offered by the technology produces an exciting but complex design space, which is often difficult to negotiate in an application context. Deploying sensing physical environments produces its own set of challenges, and can push systems into failure modes, thus revealing problems that can be difficult to discover or reproduce in simulation or the laboratory. Sustained efforts in the area of wireless networked sensing over the last 15 years have resulted in a large number of theoretical developments, substantial practical achievements, and a wealth of lessons for the future. It is clear that in order to bridge the gap between (on the one hand) visions of very large scale, autonomous, randomly deployed networks and (on the other) the actual performance of fielded systems, we need to view deployment as an essential component in the process of developing sensor networks: a process that includes hardware and software solutions that serve specific applications and end-user needs. Incorporating deployment into the design process reveals a new and different set of requirements and considerations, whose solutions require innovative thinking, multidisciplinary teams and strong involvement from end-user communities. This special feature uncovers and documents some of the hurdles encountered and solutions offered by experimental scientists when deploying and evaluating wireless sensor networks in situ, in a variety of well specified application scenarios. The papers specifically address issues of generic importance for WSN system designers: (i) data quality, (ii) communications availability and quality, (iii) alternative, low-energy sensing modalities and (iv) system solutions with high end-user added value and cost benefits. The common thread is deployment and deployment evaluation. In particular, satisfaction of application requirements, involvement of the end-user in the design and deployment process, satisfactory system performance and user acceptance are concerns addressed in many of the contributions. The contributions form a valuable set, which help to identify the priorities for research in this burgeoning area: Robust, reliable and efficient data collection in embedded wireless multi-hop networks are essential elements in creating a true deploy-and-forget user experience. Maintaining full connectivity within a WSN, in a real world environment populated by other WSNs, WiFi networks or Bluetooth devices that constitute sources of interference is a key element in any application, but more so for those that are safety-critical, such as disaster response. Awareness of the effects of wireless channel, physical position and line-of-sight on received signal strength in real-world, outdoor environments will shape the design of many outdoor applications. Thus, the quantification of such effects is valuable knowledge for designers. Sensors' failure detection, scalability and commercialization are common challenges in many long-term monitoring applications; transferable solutions are evidenced here in the context of pollutant detection and water quality. Innovative, alternative thinking is often needed to achieve the desired long-lived networks when power-hungry sensors are foreseen components; in some instances, the very problems of wireless technology, such as RF irregularity, can be transformed into advantages. The importance of an iterative design and evaluation methodology—from analysis to simulation to real-life deployment—should be well understood by all WSN developers. The value of this is highlighted in the context of a challenging WPAN video-surveillance application based on a novel Nomadic Access Mechanism. Cost benefits to be drawn from devising a WSN based solution to classic application areas such as surveillance are often a prime motivator for WSN designers; an example is offered here based on the use of intelligent agents for intrusion monitoring. Last but not least, the practicality and usability of the WSN solutions found for novel applications is key to their adoption. This is particularly true when the end-users of the developed technology are medical patients. The importance of feedback, elegant hardware encapsulation and extraction of meaning from data is presented in the context of novel orthopedic rehabilitation aids. Overall, this feature offers wide coverage of most issues encountered in the process of design, implementation and evaluation of deployable WSN systems. We trust that designers and developers of WSN systems will find much work of value, ranging from lessons learned, through solutions to known hurdles, to novel developments that enhance applications. Finally, we would like to thank all authors for their valuable contributions!",,2010,10.1088/0957-0233/21/12/120101,118864811,semantic_scholar
8b68f00f25c143c99ffb2cbeaf485ece497793d6,https://www.semanticscholar.org/paper/8b68f00f25c143c99ffb2cbeaf485ece497793d6,Edge Data Based Trailer Inception Probabilistic Matrix Factorization for Context-Aware Movie Recommendation,"The rapid growth of edge data generated by mobile devices and applications deployed at the edge of the network has exacerbated the problem of information overload. As an effective way to alleviate information overload, recommender system can improve the quality of various services by adding application data generated by users on edge devices, such as visual and textual information, on the basis of sparse rating data. The visual information in the movie trailer is a significant part of the movie recommender system. However, due to the complexity of visual information extraction, data sparsity cannot be remarkably alleviated by merely using the rough visual features to improve the rating prediction accuracy. Fortunately, the convolutional neural network can be used to extract the visual features precisely. Therefore, the end-to-end neural image caption (NIC) model can be utilized to obtain the textual information describing the visual features of movie trailers. This paper proposes a trailer inception probabilistic matrix factorization model called Ti-PMF, which combines NIC, recurrent convolutional neural network, and probabilistic matrix factorization models as the rating prediction model. We implement the proposed Ti-PMF model with extensive experiments on three real-world datasets to validate its effectiveness. The experimental results illustrate that the proposed Ti-PMF outperforms the existing ones. H. Chen, Z. Li, Z. Wang, Z. Ni, and J. Li College of Control Science and Engineering, China University of Petroleum, Qingdao 266580, China. E-mail: chenhl@upc.edu.cn H. Chen and G. Xu College of Computer and Control Engineering, Minjiang University, Fuzhou 350108, China E-mail: xuge@pku.edu.cn A. Aziz School of Software, Dalian University of Technology, Dalian 116620, China. F. Xia School of Engineering, IT and Physical Sciences, Federation University Australia, Ballarat, VIC 3353, Australia. E-mail: f.xia@ieee.org ar X iv :2 20 2. 10 23 6v 1 [ cs .C V ] 1 6 Fe b 20 22 2 Honglong Chen et al.",World Wide Web,2021,10.1007/s11280-021-00974-4,245008789,semantic_scholar
fa1387e89d40c6b68969fa26315da581db6aeb23,https://www.semanticscholar.org/paper/fa1387e89d40c6b68969fa26315da581db6aeb23,Agile Requirements Engineering and Software Planning for a Digital Health Platform to Engage the Effects of Isolation Caused by Social Distancing: Case Study (Preprint),"
 BACKGROUND
 Social distancing and shielding measures have been put in place to reduce social interaction and slow the transmission of the coronavirus disease (COVID-19). For older people, self-isolation presents particular challenges for mental health and social relationships. As time progresses, continued social distancing could have a compounding impact on these concerns.
 
 
 OBJECTIVE
 This project aims to provide a tool for older people and their families and peers to improve their well-being and health during and after regulated social distancing. First, we will evaluate the tool’s feasibility, acceptability, and usability to encourage positive nutrition, enhance physical activity, and enable virtual interaction while social distancing. Second, we will be implementing the app to provide an online community to assist families and peer groups in maintaining contact with older people using goal setting. Anonymized data from the app will be aggregated with other real-world data sources to develop a machine learning algorithm to improve the identification of patients with COVID-19 and track for real time use by health systems.
 
 
 METHODS
 Development of this project is occurring at the time of publication, and therefore, a case study design was selected to provide a systematic means of capturing software engineering in progress. The app development framework for software design was based on agile methods. The evaluation of the app’s feasibility, acceptability and usability shall be conducted using Public Health England's guidance on evaluating digital health products, Bandura’s model of health promotion, the Reach Effectiveness Adoption Implementation Maintenance (RE-AIM) framework and the Nonadoption, Abandonment and Challenges to the Scale-up, Spread and Suitability (NASSS) framework.
 
 
 RESULTS
 Making use of a pre-existing software framework for health behavior change, a proof of concept was developed, and a multistage app development and deployment for the solution was created. Grant submissions to fund the project and study execution have been sought at the time of publication, and prediscovery iteration of the solution has begun. Ethical approval for a feasibility study design is being sought.
 
 
 CONCLUSIONS
 This case study lays the foundations for future app development to combat mental and societal issues arising from social distancing measures. The app will be tested and evaluated in future studies to allow continuous improvement of the app. This novel contribution will provide an evidence-based exemplar for future app development in the space of social isolation and loneliness.
",,2020,10.2196/preprints.19297,219073060,semantic_scholar
3552bef6e5478dfec4a704927f0939e0b938f910,https://www.semanticscholar.org/paper/3552bef6e5478dfec4a704927f0939e0b938f910,SensEH: From simulation to deployment of energy harvesting wireless sensor networks,"Energy autonomy and system lifetime are critical concerns in wireless sensor networks (WSNs), for which energy harvesting (EH) is emerging as a promising solution. Nevertheless, the tools supporting the design of EH-WSNs are limited to a few simulators that require developers to re-implement the application with programming languages different from WSN ones. Further, simulators notoriously provide only a rough approximation of the reality of low-power wireless communication. In this paper we present SENSEH, a software framework that allows developers to move back and forth between the power and speed of a simulated approach and the reality and accuracy of in-field experiments. SENSEH relies on COOJA for emulating the actual, deployment-ready code, and provides two modes of operation that allow the reuse of exactly the same code in real-world WSN deployments. We describe the toolchain and software architecture of SENSEH, and demonstrate its practical use and benefits in the context of a case study where we investigate how the lifetime of a WSN used for adaptive lighting in road tunnels can be extended using harvesters based on photovoltaic panels.",39th Annual IEEE Conference on Local Computer Networks Workshops,2014,10.1109/LCNW.2014.6927704,11680969,semantic_scholar
f4ffe28408602de97f85c9bf3410a7173fd3ed3c,https://www.semanticscholar.org/paper/f4ffe28408602de97f85c9bf3410a7173fd3ed3c,BEST PRACTICES REAL-WORLD EXPERIENCES OFFER INSIGHTS Virtualization,"Propelled by the technologies that VMware and Intel have pioneered, virtualization has rapidly gained market acceptance. Indeed, VMware virtual infrastructure software has some 4 million users and more than 20,000 corporate customers, many of which have been built on Intel-based systems. One of those corporate customers is Solvay Pharmaceuticals, a chemical and pharmaceutical group with 29,000 employees in 50 countries, which identified virtualization as the solution to contain server growth. “Because we’re in a regulated industry, we would need to get three new servers every time we needed a new application—one for development, one for test and one for production,” explains Bruce McMillan, Solvay’s manager, emerging technologies. The company implemented VMware® Infrastructure 3 featuring VMware® ESX server hosts on HP ProLiant servers powered by quad-core Intel® Xeon® processors. By going from 65 physical servers to just 17, McMillan says the virtualization implementation “has saved us $1.5 million in pure hardware costs. That’s not even counting power and cooling costs, or staffing costs.” There are many more examples of successful virtualization deployments across many industries and mixes of applications, leading to the emergence of clear best practices that can benefit organizations implementing virtualization for the first time.",,2008,,43067716,semantic_scholar
b8317ef9bcf24c688c67e5062cb388e53b2dd150,https://www.semanticscholar.org/paper/b8317ef9bcf24c688c67e5062cb388e53b2dd150,Decoding Motor Skills of AI and Human Policies: A Study on Humanoid and Human Balance Control,"In this study, we propose a new paradigm of using a machine learning approach to facilitate a quicker, more efficient and effective control development, as a different approach of utilising the power of machine learning in addition to other options that intent to use learning directly in real-world applications. We first develop a DRL-based control framework to learn rich motor skills of push recovery for humanoid robots that exhibit human-like push recovery behaviour. Next, we propose to take advantage of DRL to quickly discover solutions for very difficult problems, and then extract the principles of those policies as guidelines for developing engineered controllers. Furthermore, a comparison between humanoid and human balancing is conducted to show the characteristics of the learned humanoid behaviour. This comparison will show that DRL algorithms can learn a good policy with short development and training time that may require humans years to learn. We analyse input-output data collected from humanoid and human policies and postulate a Minimum-Jerk ModelPredictive Control (MJMPC) Framework that quantitatively reflects both AI and human push recovery policies. I. SCIENTIFIC MOTIVATION From the advancement in computers, computer-aided design for mechanical and electronic engineering, architecture and many other engineering fields emerged. Foreseeing a similar development curve and technology wave, we forecast a new emerging discipline in the near future that uses learning-aided approaches for catalysing control development, alongside other similar applications such as in medicine discovery. In this study, we propose a new paradigm of using a machine learning approach to facilitate a quicker, more efficient and effective control development, as a different approach of leveraging the power of machine learning in addition to other options that intent to use learning directly in real-world applications. Machine Learning and Deep Reinforcement Learning (DRL) in particular have reached an advanced stage to produce powerful policies with better autonomous performances than many state-of-the-art control and planning approaches in robot locomotion [1], robotic manipulation [2], and even the control of complex morphological machines [3]. Notably, DRL’s ability to solve complex problems with a relatively short development time is especially attractive, which is empowered by training policies that maximise the cumulative reward through the exploration of the action and state space, rather than using prior knowledge of the models about the robot, the world, and their interactions. To leverage the capabilities of DRL, we first develop a DRL-based control framework to learn rich motor skills of push recovery for humanoid robots. The complexity in whole-body balancing arises in challenges such as multi(a) Ankle Strategy (b) Hip Strategy (c) Toe Strategy (d) Step Strategy Fig. 1: Human-like Push Recovery strategies emerging from Deep Reinforcement Learning. The discovered behaviours serve as a guideline for the design of certifiable and safe controllers that replicate advantageous strategies from AI policies. contact coordination based on multi-sensory inputs, state transitions between fullyand under-actuated situations, switching policies, and generalising to external disturbances on any body parts, while accounting for all edge cases that a designer has difficulty to consider beforehand. In such a setting, manually designing the individual control strategies and finding a reliable switching mechanism requires both substantial development time, mathematical rigour, and code implementation. On the other hand, through a well-designed DRL framework and task-specific training procedures, a robust policy can be learned automatically by interacting with the environment, requiring only computational power. In particular, as shown in Fig. 1, our learned policy exhibits human-like push recovery behaviour with four typical push recovery strategies emerging naturally: ankle, hip, toe, and IEEE Robotics and Automation Magazine (RAM) paper, presented at IROS 2020. It should be cited as a RAM paper. stepping strategy. Though the learned control policy could possibly be deployed on the real robotic system, the lack of explainability and analytical reasoning of the Neural Network makes it unsuitable for safety-critical applications in real world. Furthermore, due to the demand of large data and sampleinefficient nature of DRL algorithms, complex policies are typically trained in simulation, which cannot guarantee the same performance while transferred directly to the real system [1], and the challenge of reality gap raises concerning about both the safety and performance. To benefit from both the safety and interpretability for the control policy and the versatility and adaptability from learning, we propose to take advantage of DRL to quickly discover versatile, deployable policies and solutions for very difficult problems, and then study, analyse and extract the principles of those policies as guidelines for developing engineered controllers in a reliable manner. By doing so, we utilise the AI-solutions for rapid control development (Fig. 3) to design safe and certifiable controllers which can be verified and deployed on real-world robots (Fig. 2). While classical control development is based on gradually building knowledge that increases the performance incrementally, using a template policy will provide disruptive, innovative solutions that will escalate performance (green line, Fig. 3). DRL is able to achieve good performance by a number of iterations in the DRL learning framework. However, the achieved performance is still comparatively low to what tuning in control can do. Combining both approaches to “kick-start” the iteration process helps to design good controllers. After knowing the system and the controller, it is straightforward to improve upon due to the fact that we are then able to understand why the performance is lower than the optimum, whereas in the case of DRL, there is little influence from human engineers to improve the performance but reshaping the reward and/or altering the learning framework, and relying on the exploration being sufficiently large to achieve high performance. In this paper, we are motivated to study a viable approach to infer underlying principles of an AI policy by studying its perception-action relation, i.e., to some extent, reverseengineer an equivalent controller in terms of functionality based on a black-box policy. This methodology is not only applicable to AI policies, but also to any black-box policies, such as a human policy. Without knowing exactly how push recovery policies are realised by Artificial Neural Network (ANN) or biological human Neural Network, we can still analyse the behaviour at the functionality level by studying their input-output relationship. Based on evidence of optimality in human manipulation tasks [4], we hypothesise that policies for push recovery in humans and humanoid are both optimal control process that follows certain optimal criteria that can be quantified. Following this hypothesis, we analyse and utilise inputoutput data collected from both humanoid and human policies, and propose a Minimum-Jerk Model-Predictive Control (MJMPC) Framework that is able to quantitatively reflect both the AI and human push recovery policies. The engineered controller has high similarity (Coefficient of Determination more than 90%) with the collected data, and also exhibits the same human-like push recovery strategies, which emerge from the proposed MJMPC without the need of manual switching between the strategies. Furthermore, a comparison between humanoid and human balancing is conducted to show the characteristics of the learned humanoid behaviour. This comparison will show that DRL algorithms are very powerful to learn a policy (e.g., balancing) within a short development and training time that may require humans years to learn. In contrast, in order to design an engineered controller from scratch with similar performance, months or even years are needed for developmental iterations, mainly because of the high-redundancy and a diversity of control actions, which are yet challenging to resolve the physical optimality on a high Degree of Freedom (DoF) robot. In this regard, the learning approach is very attractive because of the significant reduction of manual effort, and the learning architecture requires only the design of input-output and rewards. This article shed some light on a new paradigm: the recent high-profile successes in DRL suggest new high-quality value in learning methods that the discovered policies can be used as a basis for speeding up the development of robotic controllers (Fig. 2). As an outcome in this push recovery study, we obtain a certifiable, analysable optimal controller that does not require any state machine or switching mechanism, while exhibiting human-like push recovery strategies, such as ankle, hip, toe, and stepping strategy all in a coherent optimisation process. II. GENERATING COMPLEX MOTIONS FOR HUMANOID ROBOTS THROUGH DEEP REINFORCEMENT LEARNING To use DRL-policies as a basis for analysis, these policies must reach a certain performance threshold that ideally surpasses traditional control approaches both in the types of motions it can generate and the amount of disturbances that it can withstand. DRL has been shown to be capable of learning locomotion and fall recovery policies that surpasses traditional control approaches for quadruped robots in terms of power efficiency, and versatility of motion [1]. In this section, we present a hierarchical learning framework for achieving versatile behaviours during push recovery for humanoid robots as proposed in [5]. The learned policy exhibits a wide range of balancing strategies that are comparable to human push recovery. In particular, the learned policy is able to withstand external distu",,2020,,219952864,semantic_scholar
ec589937c2bbdb93fcef6e7f99e82d4ca99f0306,https://www.semanticscholar.org/paper/ec589937c2bbdb93fcef6e7f99e82d4ca99f0306,Flight Controller Synthesis Via Deep Reinforcement Learning,"Traditional control methods are inadequate in many deployment settings involving control of Cyber-Physical Systems (CPS). In such settings, CPS controllers must operate and respond to unpredictable interactions, conditions, or failure modes. Dealing with such unpredictability requires the use of executive and cognitive control functions that allow for planning and reasoning. Motivated by the sport of drone racing, this dissertation addresses these concerns for state-of-the-art flight control by investigating the use of deep neural networks to bring essential elements of higher-level cognition for constructing low level flight controllers. 
This thesis reports on the development and release of an open source, full solution stack for building neuro-flight controllers. This stack consists of the methodology for constructing a multicopter digital twin for synthesize the flight controller unique to a specific aircraft, a tuning framework for implementing training environments (GymFC), and a firmware for the world's first neural network supported flight controller (Neuroflight). GymFC's novel approach fuses together the digital twinning paradigm for flight control training to provide seamless transfer to hardware. Additionally, this thesis examines alternative reward system functions as well as changes to the software environment to bridge the gap between the simulation and real world deployment environments. 
Work summarized in this thesis demonstrates that reinforcement learning is able to be leveraged for training neural network controllers capable, not only of maintaining stable flight, but also precision aerobatic maneuvers in real world settings. As such, this work provides a foundation for developing the next generation of flight control systems.",ArXiv,2019,,202578037,semantic_scholar
a725dfbed5b04e1300b17f5e823c2bfd9953b4b5,https://www.semanticscholar.org/paper/a725dfbed5b04e1300b17f5e823c2bfd9953b4b5,SoftTap: A Software-Defined TAP via Switch-Based Traffic Mirroring,"With widespread deployment of virtualization technologies in datacenter networks, traditional tools used for network monitoring, such as hardware taps, become unfit. This is due to the inability of hardware solutions for dynamic deployment and virtual network monitoring. This paper presents the design and evaluation of SoftTap, a scalable alternative to hardware taps which is capable of operating over both physical and virtual switches. SoftTap is based on port and flow mirroring capabilities of commodity OpenFlow switches and is not limited to a specific network architecture or topology. A key design challenge in SoftTap is the fast computation of switch mirroring configurations in large-scale deployments. Our design is based on novel polynomial time approximation algorithms that are shown to achieve bounded approximation ratios compared to optimal solutions. We evaluate SoftTap using model-driven simulations as well as realistic Mininet experiments. Specifically, our simulations consider large networks to show the scalability of SoftTap. Mininet experiments, on the other hand, consider its real-world utility by implementing an intrusion detection system (IDS) and a VoIP metering application on top of SoftTap. In our experiments, under SoftTap, IDS achieves up to 25% higher detection recall, while VoIP metering achieves up to 23% less packet loss compared to existing mirroring-based traffic monitoring approaches.",2021 IEEE 7th International Conference on Network Softwarization (NetSoft),2021,10.1109/NetSoft51509.2021.9492588,236480238,semantic_scholar
d99c8f011375b443237e4b4dad767549ab313f38,https://www.semanticscholar.org/paper/d99c8f011375b443237e4b4dad767549ab313f38,"Design, Analysis, and Experimental Evaluation of a New Secure Rejoin Mechanism for LoRaWAN Using Elliptic-Curve Cryptography","LoRaWAN (Long Range Wide Area Network) is a Low-Power Wide Area Networks (LPWAN) technology with very rapid uptake during the previous years, developed by the LoRa (Long Range) Alliance as an open standard operating over the unlicensed band. Current LoRaWAN architecture foresees specific techniques for bootstrapping end-to-end encryption during network initialization. In particular, this work focuses on the Over-The-Air Activation (OTAA) method, which uses two keys (Network key (NwkKey) and Application key (AppKey)) that are hard-coded into the device and do not change throughout the entire lifetime of the deployment. The inability to refresh these two keys is as a weak point in terms of the overall security of the network especially when considering deployments that are expected to operate for at least 10–15 years. In this paper, the security issues of OTAA are presented in detail highlighting the vulnerabilities against the specific type of attacks. A new scheme for network activation is proposed that builds upon the current LoRaWAN architecture in a way that maintains backwards compatibility while resolving certain vulnerabilities. Under the new mechanism, the devices periodically negotiate new keys securely based on elliptic-curve cryptography. The security properties of the proposed mechanism are analyzed against a specific type of attacks. The analysis indicates that the new secure rejoin mechanism guarantees (i) computational key secrecy, (ii) decisional key secrecy, and (iii) key independence, forward and backward, for both root keys thus properly addressing the considered security vulnerabilities of LoRaWAN. Moreover, the method is implemented in software using the RIOT-OS, a hardware-independent operating system that supports many different architectures for 8 bit, 16 bit, 32 bit and 64 bit processors. The resulting software is evaluated on the FIT IoT-Lab real-world experimentation facility under a diverse set of ARM Cortex-M* devices targeting a broad range of IoT applications, ranging from advanced wearable devices to interactive entertainment devices, home automation and industrial cyber-physical systems. The experiments indicate that the overall overhead incurred in terms of energy and time by the proposed rejoin mechanism is acceptable given the low frequency of execution and the improvements to the overall security of the LoRaWAN1.1 OTAA method.",J. Sens. Actuator Networks,2021,10.3390/jsan10020036,235965215,semantic_scholar
334c96fd3595cf14477eaa52455f14b17d2b8ffc,https://www.semanticscholar.org/paper/334c96fd3595cf14477eaa52455f14b17d2b8ffc,PIE: a Tool for Data-Driven Autonomous UAV Flight Testing,"In this paper, a novel technique is presented to test the flight of an unmanned aerial vehicle autonomously in a real-world scenario using a data-driven technique without intervening with its onboard software. With the growing applications of such vehicles, testing of autonomous flight is a very important task for rapid deployment. There are different tools for modeling and simulating unmanned vehicles in virtual worlds such as Gazebo, MATLAB, Simulink, and Webots to name a few. None of these simulation tools are able to model all possible physical parameters of a real-world environment. Hence, the flight controller or mission planning software has to be tested in the physical world in the presence of an expert before deployment for a specific task. A Perception Inference Engine evaluation tool is presented that can infer internal states of the autonomous system from external observations only. The Gazebo simulation platform is used to collect data to develop the perception model. For real-time data collection, a VICON motion capture system is used to observe the autonomous flight of a small unmanned aerial vehicle. A state-of-the-art decision tree algorithm is used to implement the data-driven approach. The technique was tested using simulation data and verified with real-time data from Intel Aero Ready to Fly and Parrot AR. 2.0 drones. Moreover, we analyzed the robustness of the proposed system by introducing noise in sensor measurement and ambiguity in the testing scenario. We compared the performance of the decision tree classifier with Naïve bayes and support vector machine classifiers. It is shown that the developed system can be used for the performance evaluation of a UAV operating in the physical world by significantly reducing uncertainty in mission failure due to environmental parameters.",J. Intell. Robotic Syst.,2020,10.1007/s10846-019-01078-y,203123838,semantic_scholar
eb0e7c3176f89fa33aa50d9d3ec1fd83ab3cf96a,https://www.semanticscholar.org/paper/eb0e7c3176f89fa33aa50d9d3ec1fd83ab3cf96a,"Health 4.0: Applications, Management, Technologies and Review","The Industry 4.0 Standard (I4S) employs technologies for automation and data exchange through cloud computing, Big Data (BD), Internet of Things (IoT), forms of wireless Internet, 5G technologies, cryptography, the use of semantic database (DB) design, Augmented Reality (AR) and Content-Based Image Retrieval (CBIR). Its healthcare extension is the so-called Health 4.0. 
This study informs about Health 4.0 and its potential to extend, virtualize and enable new healthcare-related processes (e.g., home care, finitude medicine, and personalized/remotely triggered pharmaceutical treatments) and transform them into services. 
In the future, these services will be able to virtualize multiple levels of care, connect devices and move to Personalized Medicine (PM). The Health 4.0 Cyber-Physical System (HCPS) contains several types of computers, communications, storage, interfaces, biosensors, and bioactuators. The HCPS paradigm permits observing processes from the real world, as well as monitoring patients before, during and after surgical procedures using biosensors. Besides, HCPSs contain bioactuators that accomplish the intended interventions along with other novel strategies to deploy PM. A biosensor detects some critical outer and inner patient conditions and sends these signals to a Decision-Making Unit (DMU). Mobile devices and wearables are present examples of gadgets containing biosensors. Once the DMU receives signals, they can be compared to the patient’s medical history and, depending on the protocols, a set of measures to handle a given situation will follow. The part responsible for the implementation of the automated mitigation actions are the bioactuators, which can vary from a buzzer to the remote-controlled release of some elements in a capsule inside the patient’s body. 
            Decentralizing health services is a challenge for the creation of health-related applications. Together, CBIR systems can enable access to information from multimedia and multimodality images, which can aid in patient diagnosis and medical decision-making. 
Currently, the National Health Service addresses the application of communication tools to patients and medical teams to intensify the transfer of treatments from the hospital to the home, without disruption in outpatient services. 
HCPS technologies share tools with remote servers, allowing data embedding and BD analysis and permit easy integration of healthcare professionals expertise with intelligent devices.  However, it is undeniable the need for improvements, multidisciplinary discussions, strong laws/protocols, inventories about the impact of novel techniques on patients/caregivers as well as rigorous tests of accuracy until reaching the level of automating any medical care technological initiative.",Medical Technologies Journal,2019,10.26415/2572-004X-VOL2ISS4P262-276,149726840,semantic_scholar
1704ce2c7bdcd18919d5b7fcdaf29dfd7bac3fae,https://www.semanticscholar.org/paper/1704ce2c7bdcd18919d5b7fcdaf29dfd7bac3fae,Statement of Research Interests,"I develop secure systems that prevent advanced cybersecurity threats targeting hardware vulnerabilities. To that end, my research interests lie at the intersection of cryptography, computer architecture, and digital hardware design. Trusted computing in hardware is fundamental to information security practices. The basis of security guarantees in digital systems is essentially a set of cryptographic operations executing in a hardware root of trust. Advanced cyberattacks therefore deliberately target hardware layer vulnerabilities, especially in the context of security-critical Cyber-Physical Systems (CPS) and Internet-of-Things (IoT) applications—these attacks are difficult to detect and are much harder to thwart from the higher abstraction levels of the system. My research analyzes such vulnerabilities of hardware implementations of cyber-infrastructure. To provide practical security solutions that can be deployed in real-world settings, the systems I develop focus on implementation (side-channel) security, hardware/software efficiency, and end-to-end system demonstration. I ultimately aim to design tools that can quantify a provable security level for a given threat model and enable automated trade-offs for developers between a desired level of security, performance, and cost.",,2020,,221673657,semantic_scholar
cd51bbfd51cde0c089d9dfb7d30bfc124d9b7c55,https://www.semanticscholar.org/paper/cd51bbfd51cde0c089d9dfb7d30bfc124d9b7c55,"Summary for CIFE Seed Proposals for Academic Year 2020-21 Proposal number: 2020-04 Proposal title: Hybrid Physical-Digital Spaces: Transforming the Design, Operation, and Experience of Built Environments to Promote Health and Wellbeing","up to 150 words) Increasing evidence suggests built office features (e.g., lighting, materials, and ventilation) have substantial impacts on occupant wellbeing. A key next direction is field studies at industry partner sites to examine real-world workplaces. We propose to develop innovative Internet of Things (IoT) techniques that integrate data from building instrumentation, personal device sensors, and self-report interfaces and then deploy this platform in-the-wild to capture rich, longitudinal, ecologically-valid data about the status of office workers and the spaces they occupy. Insights will advance scientific knowledge of how buildings impact wellbeing as well as produce practical implications for building designers and operators. A timely component will explore how covid-19 has temporally or fundamentally changed occupant behaviors and operational decisions (e.g., physical distancing desks and ventilation settings that reduce pathogen spread). Overall, our proposed research has the potential to transform the industry’s thinking on how built environments can be designed, operated, and experienced. Hybrid Physical-Digital Spaces: Transforming the Design, Operation, and Experience of Built Environments to Promote Health and Wellbeing Problem and Significance Considering that people in the U.S. spend 87% of their time in indoor spaces , we assert that 1 buildings are powerful yet underleveraged loci for promoting human wellbeing. Imagine an intelligent office that could adapt soundscape systems to manage noise in open floor plans, optimize space reservation or utilization to foster collaborations and save energy, or provide digital information displays that promote employee connectedness and physical activity. Towards actualizing our vision of such hybrid physical-digital spaces, our proposal strives to develop, apply, and evaluate novel scientific and engineering approaches that will transform the industry’s thinking around how built environments can be designed, operated, and experienced. Increasingly, hypotheses suggest that built features of indoor environments (e.g., lighting, materials, and ventilation) have substantial impacts on occupants (e.g., employee recruitment and retention, absenteeism, cognition, creativity, productivity, social interactions, physical activity and health, and psychological wellbeing). In turn, these individual outcomes also drive pivotal organizational outcomes such as product innovation, workforce diversity, employee turnover, market share, and profitability. Examples illustrate how building interventions can have huge impacts : enhancing employee exposure to daylight can save businesses ~$2,000/yr per capita 2 , better air quality can raise cognitive scores of workers by 101% 3 , and increasing indoor access to biophilic elements could recoup $23 billion considering 10% of workplace absenteeism (a $226 billion dollar problem) is attributable to architecture that inadequately connects to nature 4 . However, few of these hypotheses have been tested at scale, over time, and in real world conditions . Instead, most prior efforts are small sample, short-term correlational studies based on potentially biased and sparse self-reported data. A more rigorous, scientific, and human-centered approach to study and engineer buildings that promote wellbeing can have major implications at individual, organizational, and societal levels (see Figure 1), offering both foundational theoretical knowledge as well as practical strategies for building designers and operators. Figure 1. Relations among building features and human outcomes at various levels. Further, “smart buildings” today typically focus on basic sensing and control for energy savings, thermal comfort, and security. Connecting to CIFE’s Vision for the Future of Building Users, we argue buildings of the future can go beyond such bottom line outcomes to be more interactive and human-centered: aware of and responsive to occupants’ cognitive, mental, and physical feelings and needs, while respecting privacy and promoting positive indoor experiences . 1 Klepeis, et al., 2001; 2 Heschong & Mahone, 2003; 3 Allen et al., 2016; 4 Elzeyadi, 2011. <Landay-Billington> < Hybrid Physical-Digital Spaces> 1 Theoretical and Practical Points of Departure It is imperative to increase understanding of exactly what built attributes have what impacts and on whom, in a scalable, longitudinal, and inclusive manner. Thus through technology-driven assessment and hybrid physical-digital interventions, we aim to (a) fundamentally advance the science on how built environments impact human wellbeing and, in turn, (b) generate guidelines that can revolutionize the way spaces are designed, operated, and experienced . Our current scope focuses on office spaces and workers; though an overarching goal is for our developed approaches and insights to establish a foundation that enables future research with additional populations and environments (e.g., physicians and patients in clinical settings, students and teachers in classrooms, and traditionally marginalized shift and temporary workers). In particular, our reusable platform will help others study this wider range of buildings and occupants; and combining these approaches with emerging endeavors such as biophilic design and precision interventions provides a novel opportunity to not only more deeply investigate but also address long-running public health challenges and systemic inequities facing society. In these ways, we hope to positively impact a broad cross-section of stakeholders at individual, organizational, and institutional levels. Moreover, this project will support interdisciplinary fertilization across engineering, computing, psychology, law, and medicine . Research Methods and Work Plan Our research agenda is to support the design and operation of built facilities that augment human capabilities and wellbeing — and have a fundamental positive change on the way indoor spaces are experienced by the people that occupy them. By introducing intelligent systems capable of gathering and interpreting building and occupant data as well as delivering adaptive interventions in response, novel roles will also emerge for managing buildings and the activities that take place inside them. To achieve these goals, our research will comprise three main activities: 1. Developing an extensible and secure data collection and machine learning platform . A key aim of this research is scientifically examining how built spaces impact human wellbeing. To pursue this investigation and develop methods that enable buildings to be more aware of occupants’ states and needs, we have been developing pattern detection software that integrates data from (a) personal devices (smartphones, smartwatches, fitness trackers), (b) building instrumentation or portable environmental sensors (light levels, air quality), and (c) experience sampling interfaces that prompt occupants for subjective information through quick, validated self-report techniques. Figure 2 illustrates examples of these assessment components. This work involves addressing a number of technical challenges, such as selecting sampling rates and window sizes to maximize efficiency, developing methods for analyzing asynchronous and sparse sensor data, and developing privacy-sensitive feature engineering strategies for detecting and predicting wellbeing outcomes of interest. We also plan to package our platform as a reusable toolkit that can be applied by other researchers and building operators. This work is ongoing and a basic version will be ready by summer. Once development is complete, CIFE support would allow us to move onto the next critical phase: moving out of the lab and into the field. <Landay-Billington> < Hybrid Physical-Digital Spaces> 2 Figure 2. Platform to integrate data from personal devices, building sensors, and subjective self-report. 2. Deploying the platform through a mixed-method study with industry partners . The next step in our research is to deploy this platform at field sites in partnership with View, Inc. (specifically, at TIAA offices in Manhattan, this summer/fall) to capture rich, longitudinal, ecologically-valid data about behavioral, psychological, and physiological states of occupants and their everyday work environments. Our plan is to recruit a sample of approximately 150 employees for a period of 18 weeks, which will involve a baseline phase followed by systematic variation of built features (Views/No Views, Plants/No Plants, and Diversity/No Diversity in artwork) and measurement of indicators hypothesized to promote both personal wellbeing and organizational performance, based on the literature and our formative online and lab studies, described below. In combination with the engineering-focused activities to implement and install the platform, deployment will occur in tandem with ethnographic work (e.g., observations, interviews, and surveys) to manually validate reliability of the system’s automated inferences as well as gain a more qualitative portrait of occupant experiences in various spaces. Privacy-centric engagements will additionally investigate stakeholders’ attitudes regarding the capture of various types of information to derive implications about informed consent and personal data management. Along similar lines, it will be critical to responsibly manage captured data, especially potentially sensitive and exploitable data about wellness or performance. Therefore all studies will be conducted with oversight and approval from the Stanford Institutional Review Board (IRB). In addition to obtaining participants’ informed consent, we will also design sensor and data collection mechanisms to use an opt-in model, including partial participation. Our data management systems can also allow individuals to view and delete their personal data, including if purging is desired in the event of study withdrawal. Our research team has exp",,2020,,235651121,semantic_scholar
06a05c5ecdfb9ab6867722752c16b1e8c21361dd,https://www.semanticscholar.org/paper/06a05c5ecdfb9ab6867722752c16b1e8c21361dd,On the practical security of white-box cryptography. (De la théorie à la pratique de la cryptographie en boite blanche),"Cryptography studies how to secure communications and information. The security of a cryptosystem depends on the secrecy of the underlying key. White-box cryptography explores methods to hide a cryptographic key into some software deployed in the real world. 
 
Classical cryptography only assumes that the adversary accesses the target cryptographic primitive in a black-box manner in which she can only observe or manipulate the input and output of the primitive, but cannot know or tamper with its internal details. The gray-box model further allows an adversary to exploit key- dependent sensitive information leaked from the execution of physical implementations. All sorts of side-channel attacks exploit some physical information leakage, such as the power consumption of the device. The white-box model considers the worst-case scenario in which the adversary has complete control over the software and its execution environment. The goal of white-box cryptography is to securely implement a cryptographic primitive against such a powerful adversary. Although the scientific community has proposed some candidate solutions to build white-box cryptography, all have proven ineffective. Consequently, this problem has remained open for almost two decades since the concept was introduced. 
 
The continuous growth in market demand and the emerging potential applications have driven the industry to deploy secretly-designed proprietary solutions. Al- though this paradigm of achieving security through obscurity contradicts the widely accepted Kerckhoffs' principle in cryptography, this is currently the only option for white-box cryptography. Security experts have reported how gray-box attacks could be used to extract keys from several publicly available white-box implementations. In a gray-box attack, the adversary adapts side-channel analysis techniques to the white-box context, i.e., to target computation traces made of noise-free run-time information instead of the noisy physical leakage. Gray-box attacks are generic since they do not require any a priori knowledge of the implementation and hence avoid costly reverse engineering. Some non-publicly scrutinized industrial white-box schemes in the market are believed to be under the threat of gray-box attacks. 
 
This thesis focuses on the analysis and improvement of gray-box attacks and the associated countermeasures for white-box cryptography. We first provide an in- depth analysis of why gray-box attacks are capable of breaking the classical white-box design which is based on table encodings. Next, we introduce a new gray-box attack named linear decoding analysis and show that linearly encoding sensitive information is insufficient to protect the cryptographic software. Afterward, we describe how to combine state-of-the-art countermeasures to resist gray-box attacks and comprehensively elaborate on the (in)effectiveness of these combined countermeasures in terms of computation complexity. Finally, we introduce a new attack technique that exploits the data-dependency of the targeted implementation to substantially lower the complexity of the existing gray-box attacks on white-box cryptography. In addition to the theoretical analyses and new attack techniques introduced in this thesis, we report some attack experiments against practical white-box implementations. In particular, we could break the winning implementations of two consecutive editions of the well-known WhibOx white-box cryptography competition.",,2020,,225666192,semantic_scholar
b8f7fa8d93c5ee4c3df988ba7c7499b1db51706e,https://www.semanticscholar.org/paper/b8f7fa8d93c5ee4c3df988ba7c7499b1db51706e,Towards Autonomous Smart Sensing Systems,"Since the 1990's, researchers in both academia and industry have been exploring ways to exploit the potential for Wireless Sensor Networks (WSNs) to revolutionize our understanding of - and interaction with - the world around us. WSNs have therefore been a major focus of research over the past 20 years. While WSNs offer a persuasive solution for accurate real-time sensing of the physical world, they are yet to be as ubiquitous as originally predicted when the technology was first envisaged. Technical difficulties exist which have inhibited the anticipated uptake in WSN technologies. The most challenging of these have been identified as system reliability, battery lifetime, maintenance requirements, node size and ease of use. Over the past decade, the Wireless Sensor Networks (WSN) group at the Tyndall National Institute, has been at the forefront of driving the vision of ubiquitously deployed, extended lifetime, low power consumption embedded systems providing information rich data streams wirelessly in (close to) real-time. In this time, the WSN group has developed multiple novel, first of kind, wireless multi-sensor systems and deployed these in the world around us, overcoming the technical challenges associated with ensuring robust and reliable long-term data sets from our environment. This work is focused on investigating and addressing these challenges through the development of the new technologies and system integration methodologies required to facilitate and implement WSNs and validate these in real deployments. Specifically, discussed are the development and deployment of novel WSN systems in the built environment, environmental monitoring and fitness and health monitoring systems.The key research challenges identified and discussed are:a)The development of resource-constrained, extremely low power consumption systems incorporating energy-efficient hardware and software algorithms.b)The development of highly reliable extremely long duration deployments which through the use of appropriate energy harvesting solutions facilitate (near) zero maintenance sensor networks.c)The development of low power consumption miniaturized wearable microsysteThe development of technologies to address these challenges in terms of cost, size, power consumption and reliability which need to be tested and validated in real world deployments of wireless sensing systems is discussed. It is clear that when looking at the scale up of deployments of novel WSNs, that to be successful, such systems need to ""be invisible, last forever, cost nothing and work out of the box"". This paper describes these relevant technologies and associated project demonstrators",2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),2020,10.1109/I2MTC43012.2020.9128887,220311463,semantic_scholar
3ee6c2e1aebf16c7b62774700b99a4320ebfefea,https://www.semanticscholar.org/paper/3ee6c2e1aebf16c7b62774700b99a4320ebfefea,MC-SDN: Supporting Mixed-Criticality Real-Time Communication Using Software-Defined Networking,"Despite recent advances, there still remain many problems to design reliable cyber-physical systems. One of the typical problems is to achieve a seemingly conflicting goal, which is to support timely delivery of real-time flows while improving resource efficiency. Recently, the concept of mixed-criticality (MC) has been widely accepted as useful in addressing the goal for real-time resource management. However, it has not been yet studied well for real-time communication. In this paper, we present the first approach to support MC flow scheduling on switched Ethernet networks leveraging an emerging network architecture, software-defined networking (SDN). Though SDN provides flexible and programmatic ways to control packet forwarding and scheduling, it yet raises several challenges to enable real-time MC flow scheduling on SDN, including: 1) how to handle (i.e., drop or re-prioritize) out-of-mode packets in the middle of the network when the criticality mode changes and 2) how the mode change affects end-to-end transmission delays. Addressing such challenges, we develop MC-SDN that supports real-time MC flow scheduling by extending SDN-enabled switches and OpenFlow protocols. It manages and schedules MC packets in different ways depending on the system criticality mode. To this end, we carefully design the mode change protocol that provides analytic mode change delay bound, and then resolve implementation issues for system architecture. For evaluation, we implement a prototype of MC-SDN on top of Open vSwitch, and integrate it into a real world network testbed as well as a 1/10 autonomous vehicle. Our extensive evaluations with the network testbed and vehicle deployment show that MC-SDN supports MC flow scheduling with minimal delays on forwarding rule updates and it brings a significant improvement in safety in a real-world application scenario.",IEEE Internet of Things Journal,2019,10.1109/JIOT.2019.2915921,164760290,semantic_scholar
9aeb35460b423cbf274e0ff1f052a602d9efbb91,https://www.semanticscholar.org/paper/9aeb35460b423cbf274e0ff1f052a602d9efbb91,A Graphical User Interface for the Real World,"A number of typical Ubiquitous Computing (Ubicomp) interface problems are presented. From these, the requirements needed for a Ubicomp meta-user interface are extracted and a user interface architecture that is based on traditional GUI architectures is introduced. MOTIVATION A recent survey of more than 100 Ubiquitous Computing Applications (Rehman 2001) revealed that a lot of these applications violate basic HCI guidelines such as Norman’s design principles (Norman 1990). More specifically, developers often leave the user with too little control, don’t provide appropriate feedback about what the system is doing and fail to show appropriate affordances and constraints to the user. A more detailed analysis is given in (Rehman et al. 2002). For our purposes I shall try to highlight the fundamental problem here. My hypothesis is that these problems are not singular design flaws, but have their root in the very idea of Ubiquitous/Disappearing/Invisible Computing. First, there is a need to define “invisibility”. Some system developers have taken it to mean that the user does not see any system-related part physically. As pleasing it may seem to some developers, this is not what Weiser or Norman meant when they conceived the idea, as is evident from the examples they usually cite. Rather, they meant that the computer stays out of the user’s mind, not necessarily their sight. That said, let us look at whether even this is achievable. For this we shall look at the archetype of a good tool: the pencil. You have control over it, the user knows how to use it, and it provides constant feedback. More importantly, it is invisible in Weiser’s sense (Weiser 1994). I believe that the reason why we have not achieved this quality in our applications is, that there is an asymmetry in our capability to capture input in comparison to providing output. With the advent of sophisticated sensors any real world object, even the human body (as seen in location-aware applications) can be a highly accurate input device. Whereas the above-mentioned pencil can also deliver feedback in the same way as its input (mechanically), we cannot, say, mechanically stop a museum visitor with a contextaware electronic guide from going out of the coverage of the location system. One solution is not to worry about the output at all. Clearly, this leads to ill-designed applications. The other solution is to display this information on a display. This adds another level of indirection a conventional tool does not suffer from and, depending on where the display is placed, may imply an increase in cognitive load. Before we talk about our approach to this problem, we shall look at another problem we became aware of when analysing the applications. Norman (1993) talks about the idea that an interface in its function as a “surface representation” should convey an image of the underlying system. The problem we have in Ubicomp the system is a collection of invisibly connected heterogenous nodes that do not seem to have a “surface” a user can relate to, at all. Our aim is to not only give the user such a smooth “surface”, but also to massively increase the bandwith of the reverse channel, between system and user. In order to do this with as little cognitive load as possible, we advocate the use of Augmented Reality (AR). SYSTEM IDEA In its widest sense any system that connects the real and virtual world can be labelled ""Augmented Reality"" (AR). As such, even tangible interfaces are examples of AR. The narrower definition involves a system that uses a head-mounted display (HMD) and a tracker (Feiner et al. 1993). The tracker continuously measures the position and orientation of the head to some real object and displays a 3D graphics on a see-through HMD that makes the virtual object appear to be placed at a fixed location in the physical world. We are using a head-mounted camera and have deployed markers (Kato & Billinghurst 1999 ) in order to track objects of interest. The marker is a reference point that can be used to overlay graphics on a real world object. A file cabinet can, e.g., have its contents overlaid on it, as long as the marker has a fixed relationship to the file cabinet. The system will infer the position of the file cabinet relative to the user’ s eyes and display a corresponding virtual object on his HMD. In order to design our smooth surface, we want to leverage some of the design principles used in the GUI domain. Before GUIs arrived, computer users were typically confronted with a multitude of applications each with their own user interface. Transferring data between them was difficult and users had to interrupt their tasks in order to adhere to application boundaries. Working across applications was impossible. In a way the situation was similar to the one we now have in Ubicomp. The arrival of a meta-interface was decisive in giving the user the “ unified experience” we wish to provide in our present domain. Figure 1: A meta-interface for ubiquitous computing A VDU (VISUAL DISPLAY UNIT) The first step in building our Graphical User Interface that can send information and receive information from any everyday object, service, appliance or application, was to implement a display that covers the entire space. Of course, this is meant in a virtual sense, using Augmented Reality. This involved constructing a spatial model that abstracts from tracking sensors and sources. We are working with a spatial model that consists of a network of points. The arcs are Cartesian coordinate transformations, some dynamically changing, some fixed. We are using this type of model in order to cope with 6-DOF(degrees-of-freedom) tracking information. In order to give an object output capability, say a loudspeaker, we can attach an electromagnetic sensor or a marker to it and add it as a point of interest to our model, that will keep track of its position and orientation at all times. Figure 2: Every object has an output USER INTERFACE ARCHITECTURE We shall now describe the entire user interface architecture we are in the process of implementing. Figure 3 shows the current design plan. We are assuming that the user has a mobile unit with a head-mounted display and that his head position and orientation are trackable. The environment contains a number of “ active” appliances: devices such as a printer, services such as a web search, Applications such as Microsoft Word and Everyday Objects such as a mug that knows its temperature. All of these devices have interfaces to a tuple space in order to receive commands and send messages to the user. We thought that a tuple space is best suited for a symmetric 2-way communication. The two paths of information flow are shown by the arrows: a forward path from left to right and a feedback path from right to left. The world model and data model can be seen as the Visual Display Unit described above. The world model contains the position(and orientation) of each active object. The data model is a description of interactive information, each piece of which I N T E R F A C E Network Services Applications Devices",,2002,,14235041,semantic_scholar
a86aed4879a9f76ade522bf95e9bc1237e67452e,https://www.semanticscholar.org/paper/a86aed4879a9f76ade522bf95e9bc1237e67452e,RFID sensing networks for critical infrastructure security: A real testbed in an energy smart grid,"The UHF Radiofrequency Identification technology offers nowadays a viable technological solution for the implementation of low-level environmental monitoring of connected critical infrastructures to be protected from both physical threats and cyber attacks. An RFID sensor network was developed within the H2020 SCISSOR project, by addressing the design of both hardware components, that is a new family of multi-purpose wireless boards, and of control software handling the network topology. The hierarchical system is able to the detect complex, potentially dangerous, events such as the un-authorized access to a restricted area, anomalies of the electrical equipments, or the unusual variation of environmental parameters. The first real-world test-bed has been deployed inside an operational smart-grid on the Favignana Island. Currently, the network is fully working and remotely accessible.",2017 IEEE International Conference on RFID Technology & Application (RFID-TA),2017,10.1109/RFID-TA.2017.8098901,39994881,semantic_scholar
9dc8a27f10c70c77604470882f5f3336dedd468f,https://www.semanticscholar.org/paper/9dc8a27f10c70c77604470882f5f3336dedd468f,Continuous Experimentation for Automotive Software on the Example of a Heavy Commercial Vehicle in Daily Operation,"As the automotive industry focuses its attention more and more towards the software functionality of vehicles, techniques to deliver new software value at a fast pace are needed. Continuous Experimentation, a practice coming from the web-based systems world, is one of such techniques. It enables researchers and developers to use real-world data to verify their hypothesis and steer the software evolution based on performances and user preferences, reducing the reliance on simulations and guesswork. Several challenges prevent the verbatim adoption of this practice on automotive cyber-physical systems, e.g., safety concerns and limitations from computational resources; nonetheless, the automotive field is starting to take interest in this technique. This work aims at demonstrating and evaluating a prototypical Continuous Experimentation infrastructure, implemented on a distributed computational system housed in a commercial truck tractor that is used in daily operations by a logistic company on public roads. The system comprises computing units and sensors, and software deployment and data retrieval are only possible remotely via a mobile data connection due to the commercial interests of the logistics company. This study shows that the proposed experimentation process resulted in the development team being able to base software development choices on the real-world data collected during the experimental procedure. Additionally, a set of previously identified design criteria to enable Continuous Experimentation on automotive systems was discussed and their validity confirmed in the light of the presented work.",ECSA,2020,10.1007/978-3-030-58923-3_5,212633943,semantic_scholar
8a24cd6ed1875c73761c9fa0ac76105a03e6e932,https://www.semanticscholar.org/paper/8a24cd6ed1875c73761c9fa0ac76105a03e6e932,A Hypervisor for Shared-Memory FPGA Platforms,"Cloud providers widely deploy FPGAs as application-specific accelerators for customer use. These providers seek to multiplex their FPGAs among customers via virtualization, thereby reducing running costs. Unfortunately, most virtualization support is confined to FPGAs that expose a restrictive, host-centric programming model in which accelerators cannot issue direct memory accesses (DMAs). The host-centric model incurs high runtime overhead for workloads that exhibit pointer chasing. Thus, FPGAs are beginning to support a shared-memory programming model in which accelerators can issue DMAs. However, virtualization support for shared-memory FPGAs is limited. This paper presents Optimus, the first hypervisor that supports scalable shared-memory FPGA virtualization. Optimus offers both spatial multiplexing and temporal multiplexing to provide efficient and flexible sharing of each accelerator on an FPGA. To share the FPGA-CPU interconnect at a high clock frequency, Optimus implements a multiplexer tree. To isolate each guest's address space, Optimus introduces the technique of page table slicing as a hardware-software co-design. To support preemptive temporal multiplexing, Optimus provides an accelerator preemption interface. We show that Optimus supports eight physical accelerators on a single FPGA and improves the aggregate throughput of twelve real-world benchmarks by 1.98x-7x.",ASPLOS,2020,10.1145/3373376.3378482,211105692,semantic_scholar
c6833112f857c54b83d144d5c48cee6e4868dc22,https://www.semanticscholar.org/paper/c6833112f857c54b83d144d5c48cee6e4868dc22,Mobile Technology for Clinical Operations: Challenges and Opportunities,"Abstract 
On the ground best practices for the use of mobile technology within e-clinical with a focus on practical applications available now for clinical operations and data management. Hear real life implementations and learnings from the field. As mobile becomes more and more common, dialog must focus on actual experiences during the conduct of trials so that clinical teams can make informed decisions on the deployment, security, and usage of mobile applications at various stages of clinical development. 
 
In this discussion, the audience will hear real stories from clinical professionals and their on the ground experiences with mobile applications. The audience will walk away with a deeper understanding of the benefits and limitations of mobile applications in clinical operations and better enable them to adopt mobile applications with practical expectations. Attendees will be guided on the practical use of these technologies. They will learn to determine the use cases that are most relevant and applicable in Clinical Operations. 
 
 
 
Biography: 
 
Jay Smith is Head of Product for TransPerfect’s Trial Interactive E-Clinical platform. Jay brings 25 years of product management, consulting, and engineering experience across verticals such as life sciences, enterprise software, healthcare, government, entertainment and manufacturing. Previously, Jay has led product efforts for Medidata, Sparta Systems, Cureatr, VenueNext, Apogy, and Liquent. Jay holds an MBA from Villanova University and a degree in Computer Science and Physics from Gettysburg College. 
 
  
 
  
 
  
 
  
 
  
 
  
 
Speaker Publications: 
 
 
  “Analysis of Multiple Samples Using Multiplex Sample NMR: Selective Excitation and Chemical Shift Imaging Approaches”; Analytical Chemistry / 2001 / 73(11):2541-6 
 “Variable Temperature Study of the Cross-Relaxation Dynamics in the Hyperpolarized Xenon-Induced Enhancement of Surface Nuclei”; The Journal of Physical Chemistry B / 2001 / 105(7):1412-1421 
 “Cross-relaxation dynamics between laser-polarized xenon and surface species using a simple three-spin model”; Chemical Physics Letters / 2000 / 317(1-2):165-173 
 
 
  
 
10th International Conference on Clinical Research and Clinical Trials; Amsterdam, Netherlands- March 18-19, 2020. 
 
Abstract Citation: 
 
Practical ClinOps Applications of AI and Machine Learning – Real-World Use Cases, Euro Clinical Trials 2020, 10th International Conference on Clinical Research and Clinical Trials; Amsterdam, Netherlands- March 18-19, 2020 (https://clinicaltrials.pharmaceuticalconferences.com/abstract/2020/mobile-technology-for-clinical-operations-challenges-and-opportunities)",,2020,,225321802,semantic_scholar
57165b0eb61847bec87cdd6df7a4eb37bd92fc59,https://www.semanticscholar.org/paper/57165b0eb61847bec87cdd6df7a4eb37bd92fc59,Commercially Available Head-Mounted Displays Are Unsuitable for Augmented Reality Surgical Guidance: A Call for Focused Research for Surgical Applications,"Recent advances in portable computational units, optics, and photonics devices have enabled the scientific community to open many new fronts in biomedical research, with the development of innovative augmented reality (AR) applications exploiting the potentialities offered by head-mounted display (HMD) technology. Such technology has reached the maturity to be translated into commercial products, and published works on HMDs provide glimpses of how AR will disrupt the surgical field, allowing for an ergonomic, intuitive, and 3-dimensional fruition of preoperative and intraoperative information. Nowadays several commercial HMDs, such as Microsoft HoloLens, Meta or Magic Leap, integrate tracking and registration technology, and the deployment of software development kits has reduced technical complexity of custom application development, allowing for a wide range of users to easily create AR applications and attracting researchers to explore their potentialities for the implementation of surgical navigators. The above-mentioned HMDs are designed following an optical see-through (OST) approach, which augments the natural view through the projection of virtual reality information on semitransparent displays in front of the user’s eyes. The OST approach fits well in the surgical domain as it offers an instantaneous full-resolution view of the real world, allowing the natural synchronization of visual and proprioceptive information, and a complete situation awareness. Ongoing research is aimed at the goal of providing a device “conceived as a transparent interface between the user and the environment, a personal and mobile window that fully integrates real and virtual information.”1 Commercial companies are rapidly improving HMD ergonomic aspects, for example, HoloLens 2 features an improved field of view (52° diagonal), which includes eye tracking, and offers more comfortable wearability. However, maximizing surgical accuracy remains a challenge for manufacturers and researchers. Together with ergonomics, the achievement of precision objectives must be addressed to develop a visor suitable for guiding surgical operations, not to mention compliance with medical device regulations. An increasing number of research studies propose the use of commercial HMDs to guide surgical interventions.2 To the best of our knowledge, these works are principally focused on the need to strengthen virtual/real patient registration (eg, use of an external localization system),3 improve virtual content stability,4 and solve calibration issues, and they underestimate the contribution of perceptual issues to the user accuracy. One of the largest obstacles to obtain a perceptually correct augmentation is the inability to render proper focus cues in HMDs; indeed, the majority of systems offers the AR content at a fixed focal distance, failing to stimulate natural eye accommodation and retinal blur effects.5 Our recent work2 suggests to avoid the use of existing HMD-OST, which are not specifically designed for performing tasks in peripersonal space (<1 m), to guide manual tasks requiring a high level of precision, since perceptual issues, particularly “focal rivalry” (ie, inability to see simultaneously in focus the virtual and real content), can affect user performance.5 Most commercial systems (HoloLens, Lumus, Meta, Ora2) indeed have a fixed focal plane at 2 m or more (often infinite). Thus, during manual tasks, virtual content is 903197 SRIXXX10.1177/1553350620903197Surgical InnovationCarbone et al editorial2020",Surgical innovation,2020,10.1177/1553350620903197,211072894,semantic_scholar
ea213eb5101ceaf749ca538a4135b879a464c00d,https://www.semanticscholar.org/paper/ea213eb5101ceaf749ca538a4135b879a464c00d,Configurable Framework for managing data produced by multiple PLCs,"The work was carried on in collaboration with the CO.S.E. Centre of Thales Alenia Space in Italy at the Turin site. Thales Alenia Space in Italy has over 40 years of experience in building systems and equipment for space exploration, telecommunications, navigation, Earth observation and science. In the world of software development, which engages this work, telemetry can offer automatic data collection from the real world. The telemetry, therefore, allows to collect important data that becomes valuable and usable when analyzed. The main goal of this work is to develop a Configurable Framework, which manage the communication among different devices and data produced by multiple PLCs. The PLCs are connected to different equipment, contained in a module, which manage the resources needed to survive: the use and recycling of water, the storage and distribution of electrical power, thermal control and air recycling, data management and processing, remote communication and control, medical treatment and many more. The module can be deployed in all kinds of environments (desert camps, oil and gas exploration camps, military outposts and polar bases) and is the ideal solution for survival in remote or hostile areas. The realized architecture, therefore, allows to monitor the status of the module once it has been deployed in a remote location. After the first phase, the requirements were collected in a document to keep track of the data needed to create the appropriate code, which was subsequently used to simulate the various components. Afterwards, it was defined what are the functionalities that a generic Configurable Framework shall implement, then all possible architectures that can satisfy the company needs were explored and finally a working prototype was developed. Furthermore, it is also an opportunity to work with software technologies outside the domain of existing business skills and discover which advantages they can bring to the company. To provide flexibility, the GUI was built as a Web Application, taking advantage from the Bootstrap library. In fact the architecture satisfy the requested modularity and it is compatible with different operating systems and several screen resolutions. The interfaces are suitable for many electronic devices, allowing the use of keyboard, mouse and touch-screens. To speed up the development it was used a Web Framework; different solutions were analyzed, but in the end it was chosen the Django Framework, since it works without having to install additional dependencies. The application was developed using SQLite as database solution, it runs from a folder and can be moved between different machines without worrying about databases and Web Servers configurations. The GUI consist of two Web Pages, the first one allows the users to perform the different requests. The second one shows which setting are available and it allows to interact with the defined models by creating, deleting or updating items. The sampled data is collected into a non-relational Database, using the JSON format, to keep track of it, and it is also displayed in real time in order to visualize updates. After simulating the behavior of the used boards, the next step was to test and validate code using physical devices. The primary purpose of testing is to detect software failures so that defects may be discovered and corrected. At the end of the testing phase a user manual was also produced.",,2020,,226091269,semantic_scholar
ae5db02b396985311457af37e8e9a3a020f7e18f,https://www.semanticscholar.org/paper/ae5db02b396985311457af37e8e9a3a020f7e18f,A Simulation Study for Long-Range Underwater Acoustic Networks in the High North,"In stark contrast to a typical underwater acoustic network (UAN) deployed in mid-latitudes, ice-covered environments make network deployment difficult and expensive. A limited number of nodes must cover ranges of hundreds of kilometers. We tackle the network design in three layers: engineering, physical, and networking. At the engineering layer, we investigate hardware and bandwidth limitations for real-world implementation. Based on the proposed bandwidth, we design a software modem equipped with three waveforms achieving 1.8, 21.4, and 96.2 b/s. The packet error rate performance is computed with a channel simulator that takes realistic environmental parameters. Our simulations show that ranges of more than 100 km can be achieved in two High North areas during summer months provided that the point-to-point links exploit the ducted sound propagation. However, during winter months, this performance may not be always possible and multiple hops may be needed to cover the same range. Finally, based on the outcomes of the physical layer, an adaptive cross-layer routing protocol, termed network-aware adaptive routing (NADIR), is simulated. Link quality, energy consumption, and topological data are used to select the best coded modulation scheme and relay node in the next transmission slot. Our results show that the use of an adaptive strategy offers higher packet delivery and lower energy consumption than a nonadaptive strategy.",IEEE Journal of Oceanic Engineering,2019,10.1109/JOE.2019.2931853,202094973,semantic_scholar
fd94932da9cc80e9cd9f6204d2cae2624d582e91,https://www.semanticscholar.org/paper/fd94932da9cc80e9cd9f6204d2cae2624d582e91,HyperLink: Virtual Machine Introspection and Memory Forensic Analysis without Kernel Source Code,"Virtual Machine Introspection (VMI) is an approach to inspecting and analyzing the software running inside a virtual machine from the hypervisor. Similarly, memory forensics analyzes the memory snapshots or dumps to understand the runtime state of a physical or virtual machine. The existing VMI and memory forensic tools rely on up-to-date kernel information of the target operating system (OS) to work properly, which often requires the availability of the kernel source code. This requirement prevents these tools from being widely deployed in real cloud environments. In this paper, we present a VMI tool called HyperLink that partially retrieves running process information from a guest virtual machine without its source code. While current introspection and memory forensic solutions support only one or a limited number of kernel versions of the target OS, HyperLink is a one-for-many introspection and forensic tool, i.e., it supports most, if not all, popular OSes regardless of their versions. We implement both online and offline versions of HyperLink. We validate the efficacy of HyperLink under different versions of Linux, Windows, FreeBSD, and Mac OS X. For all the OSes we tested, HyperLink can successfully retrieve the process information in one minute or several seconds. Through online and offline analyses, we demonstrate that HyperLink can help users detect real-world kernel rootkits and play an important role in intrusion detection. Due to its version-agnostic property, HyperLink could become the first introspection and forensic tool that works well in autonomic cloud computing environments.",2016 IEEE International Conference on Autonomic Computing (ICAC),2016,10.1109/ICAC.2016.46,7292567,semantic_scholar
70458452066e724e5ff29c0c74046e816765beb5,https://www.semanticscholar.org/paper/70458452066e724e5ff29c0c74046e816765beb5,OpenStack Neat: a framework for dynamic and energy‐efficient consolidation of virtual machines in OpenStack clouds,"Dynamic consolidation of virtual machines (VMs) is an efficient approach for improving the utilization of physical resources and reducing energy consumption in cloud data centers. Despite the large volume of research published on this topic, there are very few open‐source software systems implementing dynamic VM consolidation. In this paper, we propose an architecture and open‐source implementation of OpenStack Neat, a framework for dynamic VM consolidation in OpenStack clouds. OpenStack Neat can be configured to use custom VM consolidation algorithms and transparently integrates with existing OpenStack deployments without the necessity of modifying their configuration. In addition, to foster and encourage further research efforts in the area of dynamic VM consolidation, we propose a benchmark suite for evaluating and comparing dynamic VM consolidation algorithms. The proposed benchmark suite comprises OpenStack Neat as the base software framework, a set of real‐world workload traces, performance metrics and evaluation methodology. As an application of the proposed benchmark suite, we conduct an experimental evaluation of OpenStack Neat and several dynamic VM consolidation algorithms on a five‐node testbed, which shows significant benefits of dynamic VM consolidation resulting in up to 33% energy savings. Copyright © 2014 John Wiley & Sons, Ltd.",Concurr. Comput. Pract. Exp.,2015,10.1002/cpe.3314,8409282,semantic_scholar
67472d5b368783136837d7065162d940535e732e,https://www.semanticscholar.org/paper/67472d5b368783136837d7065162d940535e732e,Supervisory control theory applied to swarm robotics,"Currently, the control software of swarm robotics systems is created by ad hoc development. This makes it hard to deploy these systems in real-world scenarios. In particular, it is difficult to maintain, analyse, or verify the systems. Formal methods can contribute to overcome these problems. However, they usually do not guarantee that the implementation matches the specification, because the system’s control code is typically generated manually. Also, there is cultural resistance to apply formal methods; they may be perceived as an additional step that does not add value to the final product. To address these problems, we propose supervisory control theory for the domain of swarm robotics. The advantages of supervisory control theory, and its associated tools, are a reduction in the amount of ad hoc development, the automatic generation of control code from modelled specifications, proofs of properties over generated control code, and the reusability of formally designed controllers between different robotic platforms. These advantages are demonstrated in four case studies using the e-puck and Kilobot robot platforms. Experiments with up to 600 physical robots are reported, which show that supervisory control theory can be used to formally develop state-of-the-art solutions to a range of problems in swarm robotics.",Swarm Intelligence,2016,10.1007/s11721-016-0119-0,18760258,semantic_scholar
576b4575a2180e5c4a0479bf96e63cf53e672483,https://www.semanticscholar.org/paper/576b4575a2180e5c4a0479bf96e63cf53e672483,Understanding and Statically Detecting Synchronization Performance Bugs in Distributed Cloud Systems,"In such an information society, the Internet of Things (IoT) plays an increasingly important role in our daily lives. With such a huge number of deployed IoT devices, Cyber-Physical System (CPS) calls for powerful distributed infrastructures to supply big data computing, intelligence, and storage services. With the increasingly complex distributed software infrastructures, new intricate bugs continue to manifest, causing huge economic loss. Synchronization performance problems, which means that improper synchronizations may degrade the performance and even lead to service exception, heavily influence the entire distributed cluster, imperiling the reliability of the system. As one kind of performance problems, the synchronization performance problems are acknowledged as difficult to diagnosis and fix. We collect 26 performance issues in three real-world distributed systems: HDFS, Hadoop MapReduce, and HBase, and do analysis on their root cause, fix strategy, and algorithm complexity in order to understand these synchronization performance bugs better. Then, we implement a static detection tool including critical section identifier, loop identifier, inner loop identifier, expensive loop identifier, and pruning component. After that, we evaluate our detection tool on these three distributed systems with sampled bugs. In the evaluation, our detection tool accurately finds out all the target bugs. Besides, it points out more new potential performance problems than the previous works. With the strict performance overhead, our detection tool is proved to be greatly efficient.",IEEE Access,2019,10.1109/ACCESS.2019.2923956,196171384,semantic_scholar
9a762006e2e410c29d8b2d17145fcd9d7f3b44b6,https://www.semanticscholar.org/paper/9a762006e2e410c29d8b2d17145fcd9d7f3b44b6,Structural Health Monitoring and Prognostic of Industrial Plants and Civil Structures: A Sensor to Cloud Architecture,"The deployment of Structural Health Monitoring (SHM) systems is a natively interdisciplinary task that involves joint research contributions from sensing technologies, data science and civil engineering. The capability to assess, also from remote stations, the working conditions of industrial plants or the structural integrity of civil buildings is widely requested in many application fields. The technological development aims to continuously provide innovative tools and approaches to satisfy these demands. As a first instance, reliable monitoring strategies are needed to detect structural damages while filtering out environmental noise. Ongoing solutions to tackle these topics are based on the exploitation of highly customized sensing technologies, such as shaped transducers for Acoustic Emission (AE) testing or Micro-Electro-Mechanical System (MEMS) accelerometers for Operational Modal Analysis (OMA) [1]. On the other hand, effective data acquisition and storage techniques must be employed to cope with the heterogeneity of the sensing devices and with the amount of data produced by collecting raw measured signals. Finally, damage detection and prediction tasks should be computed via data-driven algorithms that can complement the model-based alternatives traditionally used in civil engineering. Layered SHM architectures [2] represent straightforward approaches to address the system complexity originated by this interdisciplinary design; however, few real-world implementations have been presented so far in the literature. In this paper, we overcome these limitations by presenting an Internet of Things (IoT)-based SHM architecture for the predictive maintenance of industrial sites and civil engineering structures and infrastructures. The proposed cyber-physical system includes a monitoring layer, that consists of accelerometer-based sensor networks, a data acquisition layer, built on the recent W3C Web of Things standard [3], and a data storage and analytics layer, which leverages distributed database and Machine Learning tools. We extensively discuss the hardware/software components of the proposed SHM architecture, by stressing its advantages in terms of device versatility, data scalability and interoperability support. Finally, the effectiveness of the system is validated on a real-world use-case, i.e., the monitoring of a metallic frame structure located at the SHM research labs of the University of Bologna, Italy, within the MAC4PRO project [4].",IEEE Instrumentation & Measurement Magazine,2020,10.1109/MIM.2020.9289069,228092404,semantic_scholar
944ad40d87fce6566211eeca78fe0b08eee1e34b,https://www.semanticscholar.org/paper/944ad40d87fce6566211eeca78fe0b08eee1e34b,Trustable Environmental Monitoring by Means of Sensors Networks on Swarming Autonomous Marine Vessels and Distributed Ledger Technology,"The article describes a highly trustable environmental monitoring system employing a small scalable swarm of small-sized marine vessels equipped with compact sensors and intended for the monitoring of water resources and infrastructures. The technological foundation of the process which guarantees that any third party can not alter the samples taken by the robot swarm is based on the Robonomics platform. This platform provides encrypted decentralized technologies based on distributed ledger tools, and market mechanisms for organizing the work of heterogeneous multi-vendor cyber-physical systems when automated economical transactions are needed. A small swarm of robots follows the autonomous ship, which is in charge of maintaining the secure transactions. The swarm implements a version of Reynolds' Boids model based on the Belief Space Planning approach. The main contributions of our work consist of: (1) the deployment of a secure sample certification and logging platform based on the blockchain with a small-sized swarm of autonomous vessels performing maneuvers to measure chemical parameters of water in automatic mode; (2) the coordination of a leader-follower framework for the small platoon of robots by means of a Reynolds' Boids model based on a Belief Space Planning approach. In addition, the article describes the process of measuring the chemical parameters of water by using sensors located on the vessels. Both technology testing on experimental vessel and environmental measurements are detailed. The results have been obtained through real world experiments of an autonomous vessel, which was integrated as the “leader” into a mixed reality simulation of a swarm of simulated smaller vessels.The design of the experimental vessel physically deployed in the Volga river to demonstrate the practical viability of the proposed methods is shortly described.",Frontiers in Robotics and AI,2020,10.3389/frobt.2020.00070,218905106,semantic_scholar
577a8528c2dd27d9c36d9cb1e63c6667c9c3370d,https://www.semanticscholar.org/paper/577a8528c2dd27d9c36d9cb1e63c6667c9c3370d,Challenges and Opportunities in the Future Applications of IoT Technology,"The advent of internet of things (IoT) has influenced and revolutionized the information systems and computing technologies. A computing concept where physical objects used in daily life, will identify themselves by getting connected to the internet is called IoT. Physical objects embedded with electronic, radio-frequency identification, software, sensors, actuators and smart objects converge with the internet to accumulate and share data in IoT. IoT is expected to bring in extreme changes and solutions to most of the daily problems in the real world. Thus, IoT provides connectivity for everyone and everything at any time. The IoT embeds some intelligence in Internet connected objects to communicate, exchange information, take decisions, invoke actions and provide amazing services. It has an imperative economic and societal impact for the future construction of information, network, and communication technology. In the upcoming years, the IoT is expected to bridge various technologies to enable new applications by connecting physical objects together to support the intelligent decision making. As the most cost-effective and performant source of positioning and timing information in outdoor environments, the global navigation satellite systems(GNSS) has become an essential element of major contemporary technology developments notably including the IoT, Big Data, Smart Cities and Multimodal Logistics. By 2020, there will be more than 20 billion interconnected IoT devices, and its market size may reach $1.5 trillion. Projections for the impact of IoT on the Internet and economy are impressive, with some anticipating as many as 100 billion connected IoT devices and a global economic impact of more than $11 trillion by 2025. Regulators can play a role in encouraging the development and adoption of the IoT, by preventing abuse of market dominance, protecting users and protecting Internet networks while promoting efficient markets and the public interest. Regulators can consider and identify some measures to foster development of the IoT. Encourage development of LTE‐A and 5G wireless networks, and keep need for IoT‐specific spectrum under review. Universal IPv6 adoption by governments in their own services and procurements, and other incentives for private sector adoption. Increasing interoperability through competition law and give users a right to easy access to personal data. Support global standardization and deployment of remotely provisioned SIMs for greater machine to machine competition. Particular attention will be needed from regulators to IoT privacy and security issues, which are key to encouraging public trust in and adoption of the technology. This paper focuses specifically on the essential technologies that enable the implementation of IoT and the general layered architecture of IoT, the market of IoT and GNSS technologies and their impact of the world economy, application domain of IoT and finally the Policy and regulatory implications and best practices.",,2019,,211103847,semantic_scholar
8002fff47f40e6126bf3f9f7fabea1ac9e1cbb4e,https://www.semanticscholar.org/paper/8002fff47f40e6126bf3f9f7fabea1ac9e1cbb4e,Hardware-in-the-Loop Testing of Connected and Automated Vehicle Applications: A Use Case for Queue-Aware Signalized Intersection Approach and Departure,"Most existing studies on connected and automated vehicle (CAV) applications apply simulation to evaluate system effectiveness. Model accuracy, limited data for calibration, and simulation assumptions limit the validity of evaluation results. One alternative approach is to use emerging hardware-in-the-loop (HIL) testing methods. HIL test environments enable physical test vehicles to interact with virtual vehicles from traffic simulation models, providing an evaluation environment that can replicate deployment conditions at early stages of CAV technology implementation without incurring excessive costs related to large field tests. In this study, a HIL testing system for vehicle-to-infrastructure (V2I) CAV applications is developed. The involved software and hardware includes a physical CAV controlled in real time, a traffic signal controller, communication devices, and a traffic simulator (VISSIM). Such HIL systems increase validity by considering the physical vehicle’s trajectories—which are constrained by real-world factors such as GPS accuracy, communication delay, and vehicle dynamics—in a simulated traffic environment. The developed HIL system is applied to test a representative early deployment CAV application: queue-aware signalized intersection approach and departure (Q-SIAD). The Q-SIAD algorithm generates recommended speed profiles based on the vehicle’s status, signal phase and timing (SPaT), downstream queue length, and system constraints and parameters (e.g., maximum acceleration and deceleration). The algorithm also considers the status of other vehicles in designing the speed profiles. The experiment successfully demonstrated this functionality with one test CAV driving through one intersection controlled by a fixed-timing traffic signal under various simulated traffic conditions.",Transportation Research Record: Journal of the Transportation Research Board,2018,10.1177/0361198118793001,115588554,semantic_scholar
0618fcca3b177d077401e4b62b495b84cd15e2c5,https://www.semanticscholar.org/paper/0618fcca3b177d077401e4b62b495b84cd15e2c5,A SCADA System Testbed for Cybersecurity and Forensic Research and Pedagogy,"This paper presents a supervisory control and data acquisition (SCADA) testbed recently built at the University of New Orleans. The testbed consists of models of three industrial physical processes: a gas pipeline, a power transmission and distribution system, and a wastewater treatment plant--these systems are fully-functional and implemented at small-scale. It utilizes real-world industrial equipment such as transformers, programmable logic controllers (PLC), aerators, etc., bringing it closer to modeling real-world SCADA systems. Sensors, actuators, and PLCs are deployed at each physical process system for local control and monitoring, and the PLCs are also connected to a computer running human-machine interface (HMI) software for monitoring the status of the physical processes. The testbed is a useful resource for cybersecurity research, forensic research, and education on different aspects of SCADA systems such as PLC programming, protocol analysis, and demonstration of cyber attacks.",ICSS '16,2016,10.1145/3018981.3018984,15327427,semantic_scholar
911f53c6615a08de2d589be5af88baa6f8c7b2b4,https://www.semanticscholar.org/paper/911f53c6615a08de2d589be5af88baa6f8c7b2b4,Understanding Pervasive Games for Purposes of Learning,"Among the manifold of approaches to technology enhanced learning, game based learning is very attractive. In game based learning, the technological systems employed for the purpose of learning are digital games. Stand-alone serious games are rare. Games deployed for learning need to be embedded into suitable contexts. A particular approach promising from certain didactic perspectives and driven by a variety of characteristics of learning contents and training requirements is embedding those games into the surrounding physical world. Games embedded into the physical world are called pervasive games. The ways of embedding are paramount. There have been numerous attempts to design and to implement pervasive games, in general, and to deploy pervasive games for learning purposes, in particular. The majority of those pervasive games failed quite badly. Storyboarding the interaction between the real world and the virtual world of a pervasive game reveals the essential strengths and weaknesses of the game concept and allows for diagnosing didactic flaws of game play. Beyond its diagnostic power, the approach supports the design of more affective and effective pervasive games. Storyboarding is a methodology of anticipating human experience and, thus, a methodology of didactic design. 1 THE AUTHORS’ POSITION All of us–readers and authors of this manuscript–are aware of the fact that so-called digital natives 1 have other expectations when facing digital media than their parents and teachers. Playful learning, whenever possible, and using digital games for learning without any fear belongs to the widespread expectations teachers and trainers have to fulfill. In response, game based learning and serious games are terms naming some prosperous field of technology enhanced learning. When the learning contents is out there in the surrounding world, it seems plausible to bring the games out there as well–pervasive games concepts evolve. In harsh contrast to the promises, most pervasive games failed badly. There will surely be no superficial and short explanation for a large number of finally disappointing game developments. But understanding the past and 1The term digital natives as polemically opposed to denigratingly calleddigital immigrantsis, exactly in this sense, ascribed to Marc Prensky (Prensky 2001), although the idea as a whole dates back to (Barlow, 1996) writing: “You are terrified of your own children, since they are natives in a world where you will always be immigrants.” shaping the future surely needs some pondering, some exchange of opinions, and several innovative ideas. The authors aim at some small contribution to this process by advocating their position, ◦ that there are decisive characteristics of pervasive games which may be well explicated by suitable approaches of storyboarding applied to pervasive games. Using storyboarding a posteriori, it turns out to work as a diagnostic tool. Doing it a priori, storyboarding becomes a tool for design and development fostering to draw conclusions from lessons learned in earlier projects that failed. Based on the authors’ key position above, one is lead to some more viewpoints worth to be considered. ◦ Pervasive games may be classified according to their pervasiveness which is of didactic relevance. ◦ The crucial embedding of learning contents into game play may be characterized quite well by means of storyboarding terminology. ◦ The storyboarding technology, by its very nature, allows for an explication of the context conditions in which learning is likely to take place. The basic terminology will be introduced briefly to be applied to a larger number of pervasive games. 696 P. Jantke K. and Spundflasch S.. Understanding Pervasive Games for Purposes of Learning. DOI: 10.5220/0004413006960701 In Proceedings of the 5th International Conference on Computer Supported Education (CSEDU-2013), pages 696-701 ISBN: 978-989-8565-53-2 Copyright c 2013 SCITEPRESS (Science and Technology Publications, Lda.) 2 INTRODUCTORY CASE STUDY Before going into the details of discussion, the authors are aiming at an intuitive introduction. Instead of presenting notations in a formal way, a certain digital game is used to exemplify what the present paper is about, which concepts are in use, and how typical problems are formulated and attacked. The game selected for an introduction by example is TREASURE(Chalmers et al., 2005) which is one of the earliest pervasive games. The purpose of the game TREASUREis to learnabout wireless communication. The ideas underlying this game are easy in structure. Figure 1: Interface to the T REASUREgame as it appears in some PDA; picture taken from (Chalmers et al., 2005) with the permission of the authors as it appears in (Jantke, 2006) . Some real urban environment such as a park, e.g., is virtually equipped with virtual treasures 2. Teams of players are running around in pursuit of treasures. Team members in the real world are localized by means of GPS technology relating them to the virtual treasures and to each other. In certain areas, there are WLAN connections allowing players to contact their virtual treasure boxes on the server for upload. 2For the borderline between reality and virtuality, in general, and for its relevance to e-learning, in particular , interested readers are directed to (Jantke and Lengyel, 2012) . (Chalmers et al., 2005) describe variations of the game mechanics. The core idea, however, is lucid. The storyboard in fig. 2 is summarizing the essentials. Figure 2: Storyboard of T REASURE’s game mechanics. Every node is an episode or a scene describing some action. Smaller inscriptions describe actions of the computer system as opposed to actions of human players. Solid lines indicate the passing of a human player from one action to another such as, for illustration, from just walking to picking up some treasure. Dashed green lines indicate that the player’s action causes some actions of the computer system. In turn, dotted blue lines indicate the impact of earlier game actions on the player’s current actions. For instance, virtual treasures can only be discovered and picked up where the computer system has placed them virtually. Arrows indicating update operations of the players’ positions have been dropped. Game playing means moving around, collecting virtual treasures, trying to pickpocket each other, and aiming at uploads of the own virtual treasure to the safe virtual treasure box. The bookkeeping of treasure locations and treasure boxes defines the termination of game play. The simplicity of the storyboard above reflects the simple structure of the underlying game concept. Furthermore, it exhibits that there are no actions of interest performed by the game system except bookkeeping and, thus, determining preconditions of player actions. The game system is not perceived as an actor, but more seen as a supervising game master. Understanding Pervasive Games for Purposes of Learning",CSEDU,2013,10.5220/0004413006960701,9682055,semantic_scholar
e988d52c2959d5924862e33076d5af550344c47f,https://www.semanticscholar.org/paper/e988d52c2959d5924862e33076d5af550344c47f,Flow-Based Network Slicing: Mapping the Future Mobile Radio Access Networks,"Nowadays mobile networks are asked to support different applications and services characterized by very specific Quality of Service (QoS) requirements. With this aim in mind, deploying network slices with particular resource allocation policies on a per-service basis becomes extremely relevant. In this regard, we introduce a solution able to dynamically partition the underlying physical infrastructure of a mobile radio access network into multiple logical slices with distinctive service-level agreements. We leverage Software-Defined Networking principles to provide fine-grained flow identification and sophisticated QoS management policies on a generic architecture supporting 4G and 5G networks with the objective of mapping the path towards the future mobile networks. The experimental evaluation of the deployed prototype on a real-world testbed has demonstrated the slicing capabilities of the system while ensuring full performance and functional isolation. We release the entire implementation under a permissive APACHE 2.0 license for academic use.",2019 28th International Conference on Computer Communication and Networks (ICCCN),2019,10.1109/ICCCN.2019.8847068,203566043,semantic_scholar
24b69c53c7aedc28e1a483bc43b14d27f2614c63,https://www.semanticscholar.org/paper/24b69c53c7aedc28e1a483bc43b14d27f2614c63,Urban Edge Computing,"The new paradigm of Edge Computing aims to bring resources for storage and computations closer to end devices, alleviating stress on core networks and enabling low-latency mobile applications. While Cloud Computing carries out processing in large centralized data centers, Edge Computing leverages smaller-scale resources— often termed cloudlets—in the vicinity of users. Edge Computing is expected to support novel applications (e.g., mobile augmented reality) and the growing number of connected devices (e.g., from the domain of the Internet of Things). Today, however, we lack essential building blocks for the widespread public availability of Edge Computing, especially in urban environments. This thesis makes several contributions to the understanding, planning, deployment, and operation of Urban Edge Computing infrastructures. We start from a broad perspective by conducting a thorough analysis of the field of Edge Computing, systematizing use cases, discussing potential benefits, and analyzing the potential of Edge Computing for different types of applications. 
We propose re-using existing physical infrastructures (cellular base stations, WiFi routers, and augmented street lamps) in an urban environment to provide computing resources by upgrading those infrastructures with cloudlets. On the basis of a real-world dataset containing the location of those infrastructures and mobility traces of two mobile applications, we conduct the first large-scale measurement study of urban cloudlet coverage with four different metrics for coverage. After having shown the viability of using those existing infrastructures in an urban environment, we make an algorithmic contribution to the problem of which locations to upgrade with cloudlets, given the heterogeneous nature (with regards to communication range, computing resources, and costs) of the underlying infrastructure. Our proposed solution operates locally on grid cells and is able to adapt to the desired tradeoff between the quality of service and costs for the deployment. Using a simulation experiment on the same mobility traces, we show the effectiveness of our strategy. 
Existing mechanisms for computation offloading typically achieve loose coupling between the client device and the computing resources by requiring prior transfers of heavyweight execution environments. In light of this deficiency, we propose the concept of store-based microservice onloading, embedded in a flexible runtime environment for Edge Computing. Our runtime environment operates on a microservice-level granularity and those services are made available in a repository—the microservice store—and, upon request from a client, transferred from the store to execution agents at the edge. Furthermore, our Edge Computing runtime is able to share running instances with multiple users and supports the seamless definition and execution of service chains through distributed message queues. Empirical measurements of the implemented approach showed up to 13 times reduction in the end-to-end latency and energy savings of up to 94 % for the mobile device. 
We provide three contributions regarding strategies and adaptations of an Edge Computing system at runtime. Existing strategies for the placement of data and computation components are not adapted to the requirements of a heterogeneous (e.g., with regards to varying resources) edge environment. The placement of functional parts of an application is a core component of runtime decisions. This problem is computationally hard and has been insufficiently explored for service chains whose topologies are typical for Edge Computing environments (e.g., with regards to the location of data sources and sinks). To this end, we present two classes of heuristics that make the problem more tractable. We implement representatives for each class and show how they substantially reduce the time it takes to find a solution to the placement problem, while introducing only a small optimality gap. The placement of data (e.g., such captured by mobile devices) in Edge Computing should take into account the user’s context and the possible intent of sharing this data. Especially in the case of overloaded networks, e.g., during large-scale events, edge infrastructure can be beneficial for data storage and local dissemination. To address this challenge, we propose vStore, a middleware that—based on a set of rules—decouples applications from pre-defined storage locations in the cloud. We report on results from a field study with a demonstration application, showing that we were able to reduce cloud storage in favor of proximate micro-storage at the edge. 
As a final contribution, we explore the adaptation possibilities of microservices themselves. We suggest to make microservices adaptable in three dimensions: (i) in the algorithms they use to perform a certain task, (ii) in their parameters, and (iii) in auxiliary data that is required. These adaptations can be leveraged to trade a faster execution time for a decreased quality of the computation (e.g., by producing more inaccurate or partly wrong results). We argue that this is an important building block to be included in an Edge Computing system in view of both constrained resources and strict requirements on computation latencies. We conceptualize an adaptable microservice execution framework and define the problem of choosing the service variant, building upon the design of our previously introduced Edge Computing runtime environment. For a case study, we implement representative examples (e.g., in the field of computer vision and image processing) and outline the practical influence of the abovementioned tradeoff. 
In conclusion, this dissertation systematically analyzes the field of Urban Edge Computing, thereby contributing to its general understanding. Our contributions provide several important building blocks for the realization of a public Edge Computing infrastructure in an urban environment.",,2020,10.25534/TUPRINTS-00013362,225078801,semantic_scholar
c5ca0cfef7767f65c1095253d94be59af280d017,https://www.semanticscholar.org/paper/c5ca0cfef7767f65c1095253d94be59af280d017,Analysis of Software Countermeasures for Whitebox Encryption,"Whitebox cryptography aims to ensure the security of cryptographic algorithms in the whitebox model where the adversary has full access to the execution environment. To attain security in this setting is a challenging problem: Indeed, all published whitebox implementations of standard symmetric-key algorithms such as AES to date have been practically broken. However, as far as we know, no whitebox implementation in real-world products has suffered from a key recovery attack. This is due to the fact that commercial products deploy additional software protection mechanisms on top of the whitebox implementation. This makes practical attacks much less feasible in real-world applications. There are numerous software protection mechanisms which protect against standard whitebox attacks. One such technique is control flow obfuscation which randomizes the order of table lookups for each execution of the whitebox encryption module. Another technique is randomizing the locations of the various Look up tables (LUTs) in the memory address space. In this paper we investigate the effectiveness of these countermeasures against two attack paradigms. The first known as Differential Computational Analysis (DCA) attack was developed by Bos, Hubain, Michiels and Teuwen in CHES 2016. The attack passively collects software execution traces for several plaintext encryptions and uses the collected data to perform an analysis similar to the well known differential power attacks (DPA) to recover the secret key. Since the software execution traces contain time demarcated physical addresses of memory locations being read/written into, they essentially leak the values of the inputs to the various LUTs accessed during the whitebox encryption operation, which as it turns out leaks sufficient information to perform the power attack. We found that if in addition to control flow obfuscation, one were to randomize the locations of the LUTs in the memory, then it is very difficult to perform the DCA on the resultant system using such table inputs and extract the secret key in reasonable time. As an alternative, we investigate the version of the DCA attack which uses the outputs of the tables instead of the inputs to mount the power analysis attack. This modified DCA is able to extract the secret key from the flow obfuscated and location randomized versions of several whitebox binaries available in crypto literature. We develop another attack called the Zero Difference Enumeration (ZDE) attack. The attack records software traces for several pairs of strategically selected plaintexts and performs a simple statistical test on the effective difference of the traces to extract the secret key. We show that ZDE is able to recover the keys of whitebox systems. Finally we propose a new countermeasure for protecting whitebox binaries based on insertion of random delays which aims to make both the ZDE and DCA attackspractically difficult by adding random noise in the information leaked to the attacker.",IACR Trans. Symmetric Cryptol.,2017,10.13154/tosc.v2017.i1.307-328,3627062,semantic_scholar
b3bf6ac7bd450eccec62fa45182924a5106eddd3,https://www.semanticscholar.org/paper/b3bf6ac7bd450eccec62fa45182924a5106eddd3,Understanding the bottlenecks in virtualizing cellular core network functions,"Network function virtualization (NFV) promises significant cost savings, flexibility and ease of deployment. However, potential challenges in implementing virtualized network elements that can support real-world performance requirements are still an open question. For example, traditional telecom networks have a lot of complex interdependencies that can affect performance. In this paper, we study the potential bottlenecks in virtualizing cellular core network functions. Using a combination of analysis and experimentation, we quantify the impact of software-based EPC elements on various metrics including physical processing, memory, IO, and bandwidth resource requirements. We use production grade, software-based cellular network elements running on general purpose Linux servers, driven by a variety of realistic workloads derived from a realworld cellular network, to examine the combined effects of control and data planes on an LTE enhanced packet core (EPC). In particular, we discover that the SGW handles about 33% of the control plane transactions and is a potential source for performance bottlenecks as a result of the interdependencies between control and data plane processing. Our results indicate that simply replacing existing EPC elements with virtualized equivalents can have severe performance bottlenecks and that virtualized EPC elements need to be carefully designed.",The 21st IEEE International Workshop on Local and Metropolitan Area Networks,2015,10.1109/LANMAN.2015.7114735,13952593,semantic_scholar
fab176450e9142a31f47c0dbe1990f1e647cb66d,https://www.semanticscholar.org/paper/fab176450e9142a31f47c0dbe1990f1e647cb66d,Guest Editorial: Special Issue on Emerging Technology for Software Define Network Enabled Internet of Things,"The Internet of Things (IoT) has been considered as a technology, which takes the first step towards a smarter world bridging the physical world with the cyber world. Even though IoT notion has gained much attention during the last few decades, real-world implementation of large-scale IoT network is still evolving in its infancy. Since deployment lags far behind the theoretic notion, comprehensive research efforts on innovative applications, architecture, and a network of IoT are required to promote the implementation of large-scale, high-quality, efficient, and secure IoT scenarios. Software Defined Networks (SDN) facilitates a variety of opportunities for network evolution. The key feature of SDN is to decouple data and control planes, which removes control plane from network hardware. As a result, it offers a remarkable resilience in programming, while providing a broad range of opportunities to optimize the utilization of network resources. Owing to the characteristics of SDN, experts in both industry and academia claims SDN as one of the ideal technologies to bridge the gaps and to overcome drawbacks of IoT deployment. Exploiting the benefits of scalable and adaptable network devices, SDN is considered as highly promising in empowering smart, powerful, and open IoT services and communication functionalities. In fact, SDN is capable of addressing numerous challenges in IoT varying from innumerable service requests and responses, the enormous data flow of IoT sensors, devices, and appli-",International Journal of Parallel Programming,2019,10.1007/s10766-019-00643-0,201714349,semantic_scholar
1b2ba411791c8c173050085c44f49deec8d2c953,https://www.semanticscholar.org/paper/1b2ba411791c8c173050085c44f49deec8d2c953,Design and Implementation of TeleAdvisor: a Projection-Based Augmented Reality System for Remote Collaboration,"TeleAdvisor is a versatile projection-based augmented reality system designed for remote collaboration. It allows a remote expert to naturally guide a local user in need of assistance in carrying out physical tasks around real-world objects. The system consists of a small projector and two cameras mounted on top of a tele-operated robotic arm at the worker’s side, and an interface to view the camera stream, control the point-of-view and gesture using projected annotations at the remote expert’s side. TeleAdvisor provides a hands-free, mobile, low-cost solution that supports gesturing by the remote expert while minimizing the cognitive overhead of the local worker. We describe the challenges, design considerations and implementation details of the two phases of the TeleAdvisor prototype, as well as its evaluation and deployment at an industrial manufacturing center. We summarize our understandings from our experiences during the project and discuss the general implications for design of augmented reality remote collaboration systems.",Computer Supported Cooperative Work (CSCW),2015,10.1007/s10606-015-9232-7,14455015,semantic_scholar
68feb53085d7df58268e014ee9e14596f57e0f45,https://www.semanticscholar.org/paper/68feb53085d7df58268e014ee9e14596f57e0f45,Deploying W3C Web of Things-Based Interoperable Mash-up Applications for Industry 4.0: A Testbed,"In Industry 4.0 scenarios, novel applications are enabled by the capability to gather large amount of data from pervasive sensors and to process them in order to devise the “digital twin” of a physical equipment. The heterogeneity of hardware sensors, communication protocols and data formats constitutes one of the main challenge toward the large-scale adoption of the Internet of Things (IoT) paradigm on industrial environments. To this purpose, the W3C Web of Things (WoT) group is working on the definition of some reference standards intended to describe in a uniform way the software interfaces of IoT devices and services, and hence to achieve the full interoperability among different IoT components regardless of their implementation. At the same time, due also to the recent appearance of the WoT W3C draft, few testbed and real-world deployments of the W3C WoT architecture has been proposed so far in the literature. In this paper, we attempt to fill such gap by describing the realization of a WoT monitoring application of a generic indoor production site: the system is able to orchestrate the sensing operations from three heterogeneous Wireless Sensor Networks (WSNs). We describe how the components of the W3C WoT architecture have been instantiated in our scenario. Moreover, we demonstrate the possibility to decouple the mash-up policies from the network functionalities, and we evaluate the overhead introduced by the WoT approach.",WWIC,2019,10.1007/978-3-030-30523-9_1,202550324,semantic_scholar
3aee2d33e7be0d0d8fbc599dfc9489a5a33c2eae,https://www.semanticscholar.org/paper/3aee2d33e7be0d0d8fbc599dfc9489a5a33c2eae,Implementation of Web AR Applications with Fog Radio Access Networks Based on Openairinterface Platform,"Augmented Reality (AR), is a technology that gives you an interactive experience of the real world where the objects reside in are ‘augmented’ by computer-generated perceptual information. Though some AR applications already exist on our smart devices, their development is not as promising as expected due to the requirement of the installation of a dedicated app. Hence the application of AR on large scales has been hampered to a certain extent. The advent of Web AR is a good complementary of the original in-app AR applications. With the convenience and cross-platform characteristics of Web browser, Web AR is being paid increasingly attention from both manufacturers and researchers. Fog Radio Access Networks (F-RAN) architecture takes full advantage of the storage and processing capabilities in edge devices, which significantly alleviates the burden of the fronthaul and BBU pool and reduces the transmission delay. Openairinterface is an LTE-compatible open source platform provided by Openairinterface Software Alliance (OSA). It is a promising demonstration platform, on which many new system architectures and communication technologies can be deployed. In this article, two applications of Web AR, including Web Face AR and Web Space AR, are successfully demonstrated on our F-RAN hardware platform based on Openairinterface. Performance evaluation results validate that they can provide users with good Quality of Experience (QoE) and very low latency.","2019 5th International Conference on Control, Automation and Robotics (ICCAR)",2019,10.1109/ICCAR.2019.8813710,201814652,semantic_scholar
ef8e465f80f41ec5cb3dbdb527c8509e66abaf5c,https://www.semanticscholar.org/paper/ef8e465f80f41ec5cb3dbdb527c8509e66abaf5c,A Framework to Secure Applications with ISA Heterogeneity,"Software security attacks are evolving from exploiting common code vulnerabilities to exploiting micro architecture side-channels. Traditional software diversity or code randomization techniques diversify the code memory layout and make it difficult for potential attackers to pinpoint the precise location of the target vulnerability. However, those approaches may not be sufficient enough for the new micro architecture attacks (e.g., Spectre). While some architecture researchers have proposed using diverse ISA configurations to defeat code injection or code reuse attacks, most of these works remain in the simulation stage due to legal, licensing, and verification costs involved in bringing a heterogeneous chip design into physical hardware [39]. In this paper, we report our on-going work of HeterSec, a framework to secure applications utilizing real world heterogeneous ISA machines. HeterSec runs on top of the commodity x86_64 and ARM64 machines. It gives the process the ability to dynamically select its underlying ISA environment. Therefore, the protected process would hide the vulnerable targets with the diversified instruction set, or would detect the abnormal behavior by comparing the execution results step-by-step from multiple ISA-diversified instances. To demonstrate the effectiveness of such software framework, we implemented HeterSec on Linux and showed its deployability by running it on a x86_64 and ARM64machine pair, connected using InfiniBand. We then conduct two case studies with HeterSec. In the first case, we timely randomize the process execution path across the ISA, which achieves similar security guarantees as the existing architecture based solutions. In the second case, we implement a multi-ISA based multi-version execution (MVX) system, providing a stronger security guarantee than current homogeneousISA MVX designs.",,2019,,86857561,semantic_scholar
b2f006d98b2b66f0ee1e5c90065e30ec8fd917b6,https://www.semanticscholar.org/paper/b2f006d98b2b66f0ee1e5c90065e30ec8fd917b6,"AR and VR Applications Improve Engineering Collaboration, Personnel Optimization, and Equipment Accuracy for Separation Solutions","
 With the most recent industry downturn still fresh in many minds, the oil and gas E&P sector is approaching this recovery with a commitment to long-term cost discipline. As a result, augmented reality (AR) and virtual reality (VR) technologies are being adopted by operators and service companies alike as a means of cost savings while driving operational efficiency.
 AR technologies employ enhanced visualization hardware, techniques, and methodologies to create new environments wherein digital and physical objects and their data coexist and interact with one another, enhancing the user experience of the real world (Kunkel and Soechti 2017). VR refers to the full immersion of the user intoand interaction with a completely digital environment. Together, these technologies form the core of immersive experience and a new paradigm in industrial interaction.
 Until recently, these technologies were primarily applied as enhanced entertainment products, most notably within the gaming industry. However, during the past several years, and thanks to the introduction of hands-free, head-mounted display (HMD) technologies, such as Microsoft® HoloLens™ and now HoloLens 2, AR and VR are migrating into the enterprise sector.
 While the oil field has not been as quick to integrate AR and VR as other sectors, such as medicine, defense, and aeronautics, operators and service providers alike have increased adoption overthe past 12 months. Motivated by a mandate to keep operating costs low and improve efficiencies in terms of field processes, operators have begun implementing AR/VR applications as collaborative problem-solving, planning, and design tools.
 For example, some operators are initiating ARconcepts to promote internal use development and prototyping for both oilfield applications and remote refinery inspections. Additionally, service companies are embracing the use of smart glasses and wearable technologies to help improve remote work and collaboration to help increase in-field safety and reduce downtime.
 As part of its strategy to help drive the oil and gas industry's digital transformation, one major service provider is developing AR/VR applications to create digital representations of physical oilfield assets on the Microsoft® HoloLens device. One area of focus is the planning, design, and deployment of solids control, fluid separation, and handling technologies for offshore drilling applications.","Day 2 Wed, September 04, 2019",2019,10.2118/195720-MS,202424582,semantic_scholar
2ee588a45093e121d2bc8d09fe0c81992a9fc1d8,https://www.semanticscholar.org/paper/2ee588a45093e121d2bc8d09fe0c81992a9fc1d8,Secure Containers in Android: The Samsung KNOX Case Study,"Bring Your Own Device (BYOD) is a growing trend among enterprises, aiming to improve workers' mobility and productivity via their smartphones. The threats and dangers posed by the smartphones to the enterprise are also ever-growing. Such dangers can be mitigated by running the enterprise software inside a ""secure container"" on the smartphone. In our work we present a systematic assessment of security critical areas in design and implementation of a secure container for Android using reverse engineering and attacker-inspired methods. We do this through a case-study of Samsung KNOX, a real-world product deployed on millions of devices. Our research shows how KNOX security features work behind the scenes and lets us compare the vendor's public security claims against reality. Along the way we identified several design weaknesses and a few vulnerabilities that were disclosed to Samsung.",SPSM@CCS,2016,10.1145/2994459.2994470,8510729,semantic_scholar
55b3a0752036c32bb61d23e7e93a1fddda0c8289,https://www.semanticscholar.org/paper/55b3a0752036c32bb61d23e7e93a1fddda0c8289,Towards trusted execution of multi-modal continuous authentication schemes,"The emergence of powerful, sensor-rich devices has led to the development of continuous authentication (CA) schemes using off-the-shelf hardware, where user behaviour is compared to past experience to produce an authentication decision with the aim of addressing challenges with traditional authentication schemes. Current CA proposals, however, have largely neglected adversaries present in a real-world deployment, namely the ubiquity of mal ware and software attacks. This has particular importance when a device cannot be trusted by a third-party, such as a corporation, that controls access to assets based on that decision. A software compromise, either on the scheme implementation or platform, may enable an adversary to modify authentication scores to alter the status of the device in reality, give insights into user behaviour, or gain unauthorised access to restricted assets. Hence, for the first time, we examine two standardised constructs that offer isolated and trusted execution - Secure Elements (SEs) and Trusted Execution Environments (TEEs) - even when an adversary has root-level privileges, and propose measures for providing trusted CA while retaining deployability. Based on these, we implement the first system for evaluating TEE-based CA on a consumer mobile device using Intel SGX, thus providing confidentiality, integrity and trust while removing the main platform from the TCB. We present an empirical evaluation of TEE-and non-TEE performance using methods proposed in related CA schemes. Our results indicate that trusted CA can be provided with no significant performance penalty, and may even offer performance benefits.",SAC,2017,10.1145/3019612.3019652,22701326,semantic_scholar
333ccba50d00f3e8c4766e0c5e179ff4de31036f,https://www.semanticscholar.org/paper/333ccba50d00f3e8c4766e0c5e179ff4de31036f,KNOWLEDGE EXCHANGE WITHIN THE PARTICLE ACCELERATOR COMMUNITY VIA CLOUD COMPUTING*,"The development, testing and use of particle accelerator modeling codes is a core competency of accelerator research laboratories around the world, and likewise for synchrotron radiation and X-ray optics codes at lightsource facilities. Such codes require time and training to learn a command-line workflow involving multiple input and configuration files, execution on a high-performance server or cluster, post-processing with specialized software and finally visualization. Such workflows are error prone and difficult to reproduce. Cloud computing and UI design are core competencies of RadiaSoft LLC, where the Sirepo framework is being developed to make state of the art codes available in the browser of any desktop, laptop or tablet. We present our initial successes as real world examples of knowledge exchange between industry and the research community. This work is leading to broader knowledge exchange throughout the community by facilitating education of students and enabling instantaneous sharing of simulation details between colleagues. Sirepo design objectives include: seamless integration with legacy codes, low barrier to entry for new users, configuration transfer to command-line mode, catalog of provenance to aid reproducibility, and simplified collaboration through multimodal sharing. The combination of intuitive browserbased GUIs and Sirepo's server-side application container technology enables simplified computational archiving and reproducibility. If embraced by the community, this could become an important asset for the design, commissioning and future upgrade of particle accelerator and Xray beamline facilities. SIREPO – A SOFTWARE FRAMEWORK Sirepo is an open source framework [1] for bringing any scientific, engineering or educational software to the cloud, with a GUI that works in any modern browser on any computing device with sufficient screen size, including tablets. The Sirepo client is built on HTML5 technologies, including the JavaScript libraries Bootstrap [2] and Angular [3]. The D3 library [4, 5] is used for interactive 2D graphics, while VTK [6] is used for 3D. The Sirepo server is built on Flask [7], a lightweight framework for web development with Python. The scientific codes supported by Sirepo, and all of their dependencies, are containerized via Docker [8], which is an open platform for distributed applications. RadiaSoft has developed open source software [9] and expertise for building, deploying and reliably executing scientific codes in Docker containers [10, 11]. RadiaSoft docker images are publicly available [12] as part of the Sirepo ecosystem. SIREPO – A SCIENTIFIC GATEWAY RadiaSoft maintains a free scientific gateway for the particle accelerator community [13], which provides a broad selection of supported codes. The most mature implementations are for SRW (Synchrotron Radiation Workshop) [14-16] and elegant [17, 18]. SRW is an open source [19] physical optics code with powerful features for calculating synchrotron radiation and X-Ray optics, including successful detailed benchmarking against state-of-the-art X-Ray beamlines [20]. Sirepo/SRW has a growing number of users at synchrotron and free electron laser (FEL) user facilities around the world, including: NSLS-II, LCLS, APS and ALS in the USA, ELETTRA in Italy, European XFEL in Germany, ESRF and SOLEIL in France, PSI in Switzerland, Diamond in the UK and LNLS in Brazil. Just as elegant is one of the most widely used codes in the world for particle accelerator simulation and design, Sirepo/elegant has many more users than the other supported accelerator codes, including regular classroom use at the US Particle Accelerator School (USPAS) [21, 22]. Other well-supported codes provide important capabilities: Synergia [23, 24] offers 2D and 3D space charge models and special features for simulating nonlinear integrable optics in the IOTA ring [25, 26], while Zgoubi [27] provides spin tracking and JSPEC [28, 29] models intrabeam scattering and electron cooling. There are two Sirepo implementations of the massivelyparallel open source particle-in-cell (PIC) code Warp [3033]. Warp PBA enables use of Warp’s quasi-3D electromagnetic PIC modeling of beamand laser-driven plasmabased accelerators. Warp VND uses Warp’s electrostatic PIC capabilities to simulate a wide range of problems, with near-term emphasis on thermionic converters and other types of vacuum nanoelectronic devices.",,2019,,236908192,semantic_scholar
3519511ac74b509ef65457c72b81cec0fdc1a256,https://www.semanticscholar.org/paper/3519511ac74b509ef65457c72b81cec0fdc1a256,D Distributed Heterogeneous Tracking for Augmented Reality,"Augmented reality (AR) is a technique in which a user’s view of the real world is enhanced or augmented with additional information generated from a computer model (Azuma et al., 2001). The enhancement may consist of virtual artifacts to be fitted into the environment or a display of non-geometric information about existing real objects. Mobile AR (MAR) systems implement this interaction paradigm in an environment in which the user moves, possibly over wide areas (Feiner, MacIntyre, Hoellerer, & Webster, 1997). This is in contrast to non-mobile AR systems that are utilized in limited spaces such as a computer-aided surgery or by a technician’s aid in a repair shop. There are a number of challenges to implementing successful AR systems. These include a proper calibration of the optical properties of cameras and display systems (Tuceryan et al., 1995; Tuceryan, Genc, & Navab, 2002), and an accurate registration of threedimensional objects with their physical counterparts and environments (Breen, Whitaker, Rose, & Tuceryan, 1996; Whitaker, Crampton, Breen, Tuceryan, & Rose, 1995). In particular, as the observer (or an object of interest) moves over time, the 3D graphics need to be properly updated so that the realism of the resulting scene and/or alignment of necessary objects and graphics are maintained. Furthermore, this has to be done in real time and with high accuracy. The technology that allows this real-time update of the graphics as users and objects move is a tracking system that measures the position and orientation of the tracked objects (Koller et al., 1997). The ability to track objects, therefore, is one of the big challenges in MAR systems. This article describes a software framework for realizing such a distributed tracking environment by discovering independently deployed, possibly heterogeneous trackers and fusing the data from them while roaming over a wide area. In addition to the MAR domain, this kind of a tracking capability would also be useful in other domains such as robotics and locationaware applications. The novelty of this research lies in the amalgamation of the theoretical principles from the domains of AR/VR, data fusion, and the distributed software systems to create a sensor-based, wide-area tracking environment. BACKGROUND",,2019,,59331378,semantic_scholar
506e55521ddd1442ae6eae58534aa971946acc3f,https://www.semanticscholar.org/paper/506e55521ddd1442ae6eae58534aa971946acc3f,Distributed Heterogeneous Tracking for Augmented Reality,"Augmented reality (AR) is a technique in which a user’s view of the real world is enhanced or augmented with additional information generated from a computer model (Azuma et al., 2001). The enhancement may consist of virtual artifacts to be fitted into the environment or a display of non-geometric information about existing real objects. Mobile AR (MAR) systems implement this interaction paradigm in an environment in which the user moves, possibly over wide areas (Feiner, MacIntyre, Hoellerer, & Webster, 1997). This is in contrast to non-mobile AR systems that are utilized in limited spaces such as a computer-aided surgery or by a technician’s aid in a repair shop. There are a number of challenges to implementing successful AR systems. These include a proper calibration of the optical properties of cameras and display systems (Tuceryan et al., 1995; Tuceryan, Genc, & Navab, 2002), and an accurate registration of threedimensional objects with their physical counterparts and environments (Breen, Whitaker, Rose, & Tuceryan, 1996; Whitaker, Crampton, Breen, Tuceryan, & Rose, 1995). In particular, as the observer (or an object of interest) moves over time, the 3D graphics need to be properly updated so that the realism of the resulting scene and/or alignment of necessary objects and graphics are maintained. Furthermore, this has to be done in real time and with high accuracy. The technology that allows this real-time update of the graphics as users and objects move is a tracking system that measures the position and orientation of the tracked objects (Koller et al., 1997). The ability to track objects, therefore, is one of the big challenges in MAR systems. This article describes a software framework for realizing such a distributed tracking environment by discovering independently deployed, possibly heterogeneous trackers and fusing the data from them while roaming over a wide area. In addition to the MAR domain, this kind of a tracking capability would also be useful in other domains such as robotics and locationaware applications. The novelty of this research lies in the amalgamation of the theoretical principles from the domains of AR/VR, data fusion, and the distributed software systems to create a sensor-based, wide-area tracking environment. BACKGROUND",,2019,,207979554,semantic_scholar
2ad3366962d249b7b63c4986ebb0cb22ea212a75,https://www.semanticscholar.org/paper/2ad3366962d249b7b63c4986ebb0cb22ea212a75,"Service-Oriented Architecture Compass: Business Value, Planning, and Enterprise Roadmap","Praise for Service-Oriented Architecture Compass""A comprehensive roadmap to Service-Oriented Architecture (SOA). SOA is, in reality, a business architecture to be used by those enterprises intending to prosper in the 21st century. Decision makers who desire that their business become flexible can jumpstart that process by adopting the best practices and rules of thumb described in SOA Compass.""i¾Bob Laird, MCI IT Chief Architect""The book Service-Oriented Architecture Compass shows very clearly by means of real projects how agile business processes can be implemented using Service-Oriented Architectures. The entire development cycle from planning through implementation is presented very close to practice and the critical success factors are presented very convincingly.""i¾Professor Dr. Thomas Obermeier, Vice Dean of FHDW Bergisch Gladbach, Germany""This book is a major improvement in the field. It gives a clear view and all the key points on how to really face a SOA deployment in today's organizations.""i¾Mario Moreno, IT Architect Leader, Generali France""Service-Oriented Architecture enables organizations to be agile and flexible enough to adopt new business strategies and produce new services to overcome the challenges created by business dynamism today. CIOs have to consider SOA as a foundation of their Enterprise Applications Architecture primarily because it demonstrates that IT aligns to business processes and also because it positions IT as a service enabler and maximizes previous investments on business applications.To understand and profit from SOA, this book provides CIOs with the necessary concepts and knowledge needed to understand and adapt it into their IT organizations.""i¾Sabri Hamed Al-Azazi, CIO of Dubai Holding, Sabri""I am extremely impressed by the depth and scale of this book! The title is perfecti¾when you know where you want to go, you need a compass to guide you there! After good IT strategy leads you to SOA, this book is the perfect vehicle that will drive you from dream to reality. We in DSK Bank will use it as our SOA bible in the ongoing project.""i¾Miro Vichev, CIO, DSK Bank, Bulgaria, member of OTP Group""Service-Oriented Architecture offers a pathway to networking of intra- and inter-corporate business systems. The standards have the potential to create far more flexible and resilient business information systems than have been possible in the past. This book is a must-read for those who care about the future of business IT.""i¾Elizabeth Hackenson, CIO, MCI""Service-Oriented Architecture is key to help customers become on demand businessesi¾a business that can quickly respond to competitive threats and be first to take advantage of marketplace opportunities. SOA Compass is a must-read for those individuals looking to bridge the gap between IT and business in order to help their enterprises become more flexible and responsive.""i¾Michael Liebow, Vice President, Web Services and Service-Oriented Architecture, IBM Business Consulting Services""This book is a welcome addition to SOA literature. It articulates the business case and provides practical proven real-world advice, guidance, tips, and techniques for organizations to make the evolution from simple point-to-point web services to true SOA by addressing such topics as planning, organization, analysis and design, security, and systems management.""i¾Denis O'Sullivan, Fireman's Fund Enterprise ArchitectMaximize the business value and flexibility of your SOA deploymentIn this book, IBM Enterprise Integration Team experts present a start-to-finish guide to planning, implementing, and managing Service-Oriented Architecture. Drawing on their extensive experience helping enterprise customers migrate to SOA, the authors share hard-earned lessons and best practices for architects, project managers, and software development leaders alike.Well-written and practical, Service-Oriented Architecture Compass offers the perfect blend of principles and ""how-to"" guidance for transitioning your infrastructure to SOA. The authors clearly explain what SOA is, the opportunities it offers, and how it differs from earlier approaches. Using detailed examples from IBM consulting engagements, they show how to deploy SOA solutions that tightly integrate with your processes and operations, delivering maximum flexibility and value. With detailed coverage of topics ranging from policy-based management to workflow implementation, no other SOA book offers comparable value to workingIT professionals.Coverage includes SOA from both a business and technical standpointi¾and how to make the business case Planning your SOA project: best practices and pitfalls to avoid SOA analysis and design for superior flexibility and value Securing and managing your SOA environment Using SOA to simplify enterprise application integration Implementing business processes and workflow in SOA environments Case studies in SOA deployment After you've deployed: delivering better collaboration, greater scalability, and more sophisticated applicationsThe IBM Press developerWorks® Series is a unique undertaking in which print books and the Web are mutually supportive. The publications in this series are complemented by resources on the developerWorks Web site on ibm.com. Icons throughout the book alert the reader to these valuable resources.",,2005,,109626518,semantic_scholar
9456732ff41a7e8b125f229d00de12ffb59eaa67,https://www.semanticscholar.org/paper/9456732ff41a7e8b125f229d00de12ffb59eaa67,Analyzing and Securing Embedded Systems,"Author(s): Spensky, Chad | Advisor(s): Vigna, Giovanni; Kruegel, Christopher | Abstract: Embedded systems (i.e., single-purpose computers with tightly-coupled software and hardware) are now pervasive throughout in our increasingly digitized world. Due to the rapid growth of the embedded systems industry and the commercial pressure to implement new features, most of these systems are built using insecure hardware and have numerous latent software vulnerabilities. Unfortunately, the diversity of physical hardware and software implementations on these various systems along with their tight coupling between software and hardware have rendered most of our existing automated security analysis techniques ineffective. Attackers currently have the upper hand, as they need only discover a single vulnerability, whereas defenders must manually identify, and fix, all of the existing vulnerabilities. To make matters worse, many of these vulnerable embedded systems can interact with the physical world and, if compromised, could cause serious damage (e.g., a public utility) or even death (e.g., a medical device). To rectify this calamitous situation that we have created, we must be able to 1) identify and fix problems with the existing systems that are already deployed and 2) create future systems that are fundamentally secure, by design.Embedded systems are more difficult to analyze than traditional computers because the hardware platforms that they run on are far more diverse, have strict hardware dependencies, are equipped peripherals that differ wildly between systems, and their execution typically depends on external phenomena that materialize as hardware interrupts. The depth of the analysis can be improved by developing novel hardware-based introspection techniques, which would provide analysts with the ability to observe the internal state of the real embedded system in real-world scenarios. The scale of the analysis can also be improved by decoupling the firmware from the hardware through emulation techniques, which would permit analysts to parallelize their analyses across numerous emulated systems, without the need for hardware, and also experiment with the embedded system in a zero-risk virtual environment. I developed a novel hardware-based introspection technique for embedded systems that provides real-time, high-level insights into modifications made to both volatile and non-volatile memory using a Field-Programmable Gate Array (FPGA) implementation and novel semantic-gap reconstruction techniques. I also developed an approach to support the decoupling of firmware from its hardware that can use either hardware- or software-based instrumentation of the system to record the hardware interactions on the real system and then convert these recordings into generalized, composable ω-automata that can be used in place of the hardware for emulation.Embedded systems are also difficult to protect against hardware-based attacks, especially glitching. Ideally, firmware could be protected against these attacks using software-only techniques that could be deployed to the billions of existing systems to protect them from physical attacks, without physically replacing them. I developed an approach that permits embedded system developers to automatically inject various software-based glitching defenses into their code at compile-time, producing glitch-resistant firmware without the need for any code annotations or modifications to the embedded system’s hardware.",,2020,,229265516,semantic_scholar
1ab0a05b291c08c69bfadf4118dca3707f820eaf,https://www.semanticscholar.org/paper/1ab0a05b291c08c69bfadf4118dca3707f820eaf,Reality Considerations When Designing a TDMA-FDMA Based Link-Layer for Real-Time WSN,"In this article we elaborate on reality considerations when designing and implementing application tailored TDMA-FDMA medium access protocol with guaranteed end-to-end delay. We highlight importance of considering underlaying hardware and software components when designing communication protocols for resource constrained platforms. We also show that by combining medium access protocol, bootstrapping, and time synchronization mechanisms within the link-layer, we can limit on average clock drift in the network to 0.5 μs, as well as achieve 81% energy efficiency while keeping collision probability at its minimum of 1%. Finally, we conclude with challenges and lessons learned in real-world deployment of TDMA/FDMA based link-layer with guaranteed end-to-end delay in WSN.",MACOM,2012,10.1007/978-3-642-34976-8_11,40499054,semantic_scholar
7654c04648178149dad73ae3b1a93d404e631774,https://www.semanticscholar.org/paper/7654c04648178149dad73ae3b1a93d404e631774,OpenStack in Action,"In the cloud computing model, a cluster of physical computers hosts an environment that provides shared services (public and private) and offers the flexibility to easily add, remove, and expand virtual servers and applications. OpenStack is an open source framework that can be installed on individual physical servers to a cloud platform and enables the building of custom infrastructure (IaaS), platform (PaaS), and software (SaaS) services without the high cost and vendor lock-in associated with proprietary cloud platforms. OpenStack in Action offers real world use cases and step-by-step instructions to develop cloud platforms from inception to deployment. It explains the design of both the physical hardware cluster and the infrastructure services needed to create a custom cloud platform. It shows how to select and set up virtual and physical servers, implement software-defined networking, and the myriad other technical details required to design, deploy, and operate an OpenStack cloud in an enterprise. It also discusses the cloud operation techniques needed to establish security practices, access control, efficient scalability, and day-to-day DevOps practices. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications.",,2016,,114415234,semantic_scholar
09d3a6aae7eecc9d435c04d55e8386ddd27221e5,https://www.semanticscholar.org/paper/09d3a6aae7eecc9d435c04d55e8386ddd27221e5,The IoT hub: a fog node for seamless management of heterogeneous connected smart objects,"The Internet of Things (IoT) will interconnect billions of devices, denoted as “smart objects,” deployed pervasively, which will be extremely heterogeneous, in terms of hardware (i.e., computational power and available memory), software (i.e., operating systems, applications), and communication interfaces. Traditional Internet actors, such as personal computers, mobile devices, and cloud servers, will also communicate with smart objects, thus creating even more complexity. The IoT has so far grown as several vertical silos, with the purpose of demonstrating the concept of the IoT, rather than focusing on the actual construction of a highly interoperable infrastructure for the development of applications. The Internet Protocol (IP) stack (in particular, HTTP and CoAP) has been foreseen as the driver for integration and interoperability among devices and basis for the evolution of the Web of Things. However, in order to manage the physical diversity of devices and to create an IP-based infrastructure, the presence of network elements able to bridge different networks to enable direct end-to-end communication is required. Moreover, effective interaction with applications might require the presence of intermediaries, such as proxies, which may optionally implement protocol and data format translation functionalities. Given the above considerations, we propose a Fog node, denoted as “IoT Hub,” placed at the edge of multiple networks, which enhances the networks capabilities by implementing the following functions: border router; cross-proxy; cache; and resource directory. An implementation of the IoT Hub is presented together with a performance evaluation in a real-world IoT testbed.","2015 12th Annual IEEE International Conference on Sensing, Communication, and Networking - Workshops (SECON Workshops)",2015,10.1109/SECONW.2015.7328145,679112,semantic_scholar
60b5a34a0cf2939c394f4365c4e67cae394a4325,https://www.semanticscholar.org/paper/60b5a34a0cf2939c394f4365c4e67cae394a4325,AdaFT,"Cyber-physical systems (CPS) frequently have to use massive redundancy to meet application requirements for high reliability. While such redundancy is required, it can be activated adaptively, based on the current state of the controlled plant. Most of the time, the plant is in a state that allows for a lower level of fault tolerance. Avoiding the continuous deployment of massive fault tolerance will greatly reduce the workload of the CPS, and lower the operating temperature of the cyber sub-system, thus increasing its reliability. In this article, we extend our prior research by demonstrating a software simulation framework Adaptive Fault Tolerance (AdaFT) that can automatically generate the sub-spaces within which our adaptive fault tolerance can be applied. We also show the theoretical benefits of AdaFT and its actual implementation in several real-world CPSs.",TECS,2017,10.1145/2980763,218483539,semantic_scholar
7be4ed954faba9d34e421b1187d3bafdafd26330,https://www.semanticscholar.org/paper/7be4ed954faba9d34e421b1187d3bafdafd26330,The role of data supported decision-making technology in respiratory care,"Millions of people across the world are affected by Chronic Obstructive Pulmonary Disease (COPD). It is one of the most prevalent chronic health conditions in the world. As a life-long condition that effects breathing, it has a huge physical and mental impact on peoples’ lives every single day. COPD is characterised by periods of respiratory exacerbations which, if are not managed swiftly, can result in hospitalisation for emergency care. However, effective self-management and support can help people with COPD to avoid the distress of requiring emergency care, while supporting their quality of life and independence. In addition to the difficulties that COPD introduces to a plethora of people, it also presents a huge challenge for healthcare services around the world. In the UK, COPD generates a high number of hospital admissions annually, with many of these for emergency care. In this highly demanding and time-pressured context, healthcare professionals are required to make timely and evidence-based decisions to effectively care for patients. This is the challenging reality for all healthcare professionals that collaborate in the ongoing management and support involved for COPD care. Data supported decision-making (DSDM) technology holds potential to support the ongoing care of people with COPD, through connecting them and their healthcare professionals with pertinent data that can inform decision-making around care. Examples of such technologies include patient health monitoring apps that share data with healthcare professionals for personalised care planning, and clinical dashboards that interlink data from different sources to support decision-making about patient treatment. However, there is currently limited research working in partnership with people with COPD and respiratory healthcare professionals to truly understand how these technologies might support care in its real-world context. Specifically, there are three key gaps in knowledge which this thesis addresses. First, there is a need to understand how DSDM technologies can be designed to support healthcare professionals to provide COPD care, while considering the challenges of implementing technology into healthcare systems. Furthering this, there is a need to understand how technology could support the self-management of COPD, considering it is progressive and highly debilitating in nature. Finally, there is a need to understand how technology could support the ongoing care collaboration between healthcare professionals and patients through sharing patient-generated data about COPD symptoms. Each of these three areas are important in developing an understanding about how technology could support the real-world context of COPD care. To advance our knowledge in this space, I conducted three novel pieces of research working with people with COPD and healthcare professionals to understand how DSDM technologies could support everyday challenges related to COPD care. First, I worked with 11 healthcare professionals to co-design a DSDM dashboard by exploring their decision-making needs around COPD care. Then I conducted exploratory research involving 171 people with chronic respiratory conditions to understand how technology may support their self-care. Finally, I conducted a small exploratory case study with eight participants to understand the patient experience of self-monitoring their respiratory symptoms and the healthcare professionals’ experience of receiving this data remotely. The thesis concludes with a synthesis of the key novel findings across the three research studies, providing overarching opportunities and nodes of caution when designing and deploying DSDM technologies in this space. This discussion draws attention to the ways that perceptions of data ‘trustworthiness’ affects how DSDM technologies are used for decision-making, the tensions that occur when technology does not align with the local context of care, the need for self-management technology to support the personal and evolving condition journey of COPD, and how we may consider designing patient facing technologies to better accommodate potential reactive self-care patterns",,2020,10.17635/LANCASTER/THESIS/1056,234639833,semantic_scholar
b3adef935b0bcffff47f8f45900ce2c3b6bbaeed,https://www.semanticscholar.org/paper/b3adef935b0bcffff47f8f45900ce2c3b6bbaeed,Enhancing Interaction with Augmented Reality through Mid-Air Haptic Feedback: Architecture Design and User Feedback,"Nowadays, Augmented-Reality (AR) head-mounted displays (HMD) deliver a more immersive visualization of virtual contents, but the available means of interaction, mainly based on gesture and/or voice, are yet limited and obviously lack realism and expressivity when compared to traditional physical means. In this sense, the integration of haptics within AR may help to deliver an enriched experience, while facilitating the performance of specific actions, such as repositioning or resizing tasks, that are still dependent on the user’s skills. In this direction, this paper gathers the description of a flexible architecture designed to deploy haptically enabled AR applications both for mobile and wearable visualization devices. The haptic feedback may be generated through a variety of devices (e.g., wearable, graspable, or mid-air ones), and the architecture facilitates handling the specificity of each. For this reason, within the paper, it is discussed how to generate a haptic representation of a 3D digital object depending on the application and the target device. Additionally, the paper includes an analysis of practical, relevant issues that arise when setting up a system to work with specific devices like HMD (e.g., HoloLens) and mid-air haptic devices (e.g., Ultrahaptics), such as the alignment between the real world and the virtual one. The architecture applicability is demonstrated through the implementation of two applications: (a) Form Inspector and (b) Simon Game, built for HoloLens and iOS mobile phones for visualization and for UHK for mid-air haptics delivery. These applications have been used to explore with nine users the efficiency, meaningfulness, and usefulness of mid-air haptics for form perception, object resizing, and push interaction tasks. Results show that, although mobile interaction is preferred when this option is available, haptics turn out to be more meaningful in identifying shapes when compared to what users initially expect and in contributing to the execution of resizing tasks. Moreover, this preliminary user study reveals some design issues when working with haptic AR. For example, users may be expecting a tailored interface metaphor, not necessarily inspired in natural interaction. This has been the case of our proposal of virtual pressable buttons, built mimicking real buttons by using haptics, but differently interpreted by the study participants.",,2019,10.3390/app9235123,214145864,semantic_scholar
641bdbf02b3ecf41c66ea4b03356b42c4f9b99ba,https://www.semanticscholar.org/paper/641bdbf02b3ecf41c66ea4b03356b42c4f9b99ba,AdaFT: A Framework for Adaptive Fault Tolerance for Cyber-Physical Systems,"Cyber-physical systems (CPS) frequently have to use massive redundancy to meet application requirements for high reliability. While such redundancy is required, it can be activated adaptively, based on the current state of the controlled plant. Most of the time, the plant is in a state that allows for a lower level of fault tolerance. Avoiding the continuous deployment of massive fault tolerance will greatly reduce the workload of the CPS, and lower the operating temperature of the cyber sub-system, thus increasing its reliability. In this article, we extend our prior research by demonstrating a software simulation framework Adaptive Fault Tolerance (AdaFT) that can automatically generate the sub-spaces within which our adaptive fault tolerance can be applied. We also show the theoretical benefits of AdaFT and its actual implementation in several real-world CPSs.",ACM Trans. Embed. Comput. Syst.,2017,10.1145/2980763,18369100,semantic_scholar
26960f55ddb24b2338a5eae012c63cd13abb7bdd,https://www.semanticscholar.org/paper/26960f55ddb24b2338a5eae012c63cd13abb7bdd,Performance Evaluation for WiFi DCF Networks from Theory to Testbed,"Distributed Coordination Function (DCF) is a basic MAC protocol used in the world-wide WiFi networks and plays a key role in determining the network performance, especially in situations with a large number of users and high-density Access Point (AP) deployed. To achieve a better understanding of the real-world performance of 802.11 DCF networks, we have constructed an emulation platform and a prototype testbed for performance evaluation. The design and implementation of these two platforms are discussed in this paper. The key DCF parameters, i.e., the initial contention window size ($CW_{min}$) and the maximum contention window size ($CW_{max}$), are tuneable so that we are able to study the impact of these DCF parameters on the network performance. These experiment results are compared against with a recently proposed unified analytical framework to examine the model assumptions and system performance bottlenecks. Our results demonstrate that by adapting the values of $CW_{min}$ based on WiFi traffic load, the maximal network throughput can be achieved, and the optimal value of $CW_{min}$ varies when the network size changes. As a reality check, the emerging software defined WiFi network architecture can be optimized for performance enhancement guided by this unified performance model.",2017 IEEE International Symposium on Parallel and Distributed Processing with Applications and 2017 IEEE International Conference on Ubiquitous Computing and Communications (ISPA/IUCC),2017,10.1109/ISPA/IUCC.2017.00207,44093395,semantic_scholar
a3d76c4d3718a53465153917bba9b24205d83096,https://www.semanticscholar.org/paper/a3d76c4d3718a53465153917bba9b24205d83096,Health 4.0 as an Application of Industry 4.0 in Healthcare Services and Management,"The Industry 4.0 Standard (I4S) employs technologies for automation and data exchange through cloud computing, Big Data (BD), Internet of Things (IoT), forms of wireless Internet, 5G technologies, cryptography, the use of semantic database (DB) design, Augmented Reality (AR) and Content-Based Image Retrieval (CBIR). Its healthcare extension is the so-called Health 4.0. 
This study informs about Health 4.0 and its potential to extend, virtualize and enable new healthcare-related processes (e.g., home care, finitude medicine, and personalized/remotely triggered pharmaceutical treatments) and transform them into services. 
In the future, these services will be able to virtualize multiple levels of care, connect devices and move to Personalized Medicine (PM). The Health 4.0 Cyber-Physical System (HCPS) contains several types of computers, communications, storage, interfaces, biosensors, and bioactuators. The HCPS paradigm permits observing processes from the real world, as well as monitoring patients before, during and after surgical procedures using biosensors. Besides, HCPSs contain bioactuators that accomplish the intended interventions along with other novel strategies to deploy PM. A biosensor detects some critical outer and inner patient conditions and sends these signals to a Decision-Making Unit (DMU). Mobile devices and wearables are present examples of gadgets containing biosensors. Once the DMU receives signals, they can be compared to the patient’s medical history and, depending on the protocols, a set of measures to handle a given situation will follow. The part responsible for the implementation of the automated mitigation actions are the bioactuators, which can vary from a buzzer to the remote-controlled release of some elements in a capsule inside the patient’s body. 
            Decentralizing health services is a challenge for the creation of health-related applications. Together, CBIR systems can enable access to information from multimedia and multimodality images, which can aid in patient diagnosis and medical decision-making. 
Currently, the National Health Service addresses the application of communication tools to patients and medical teams to intensify the transfer of treatments from the hospital to the home, without disruption in outpatient services. 
HCPS technologies share tools with remote servers, allowing data embedding and BD analysis and permit easy integration of healthcare professionals expertise with intelligent devices.  However, it is undeniable the need for improvements, multidisciplinary discussions, strong laws/protocols, inventories about the impact of novel techniques on patients/caregivers as well as rigorous tests of accuracy until reaching the level of automating any medical care technological initiative.",,2018,,86649549,semantic_scholar
8bcb1c8534908815702a0db245402a7bdf53e136,https://www.semanticscholar.org/paper/8bcb1c8534908815702a0db245402a7bdf53e136,Cloud Computing in Support of Applied Learning: A Baseline Study of Infrastructure Design at Southern Polytechnic State University.,"Cloud computing represents an architecture and paradigm of computing designed to deliver infrastructure, platforms, and software as constructible computing resources on demand to networked users. As campuses are challenged to better accommodate academic needs for applications and computing environments, cloud computing can provide an accommodating solution for mobile, campus laboratory, and distance computing. The need for ubiquitous software deployments, virtual environments, software acceleration, economies of scale, and on-demand services points to cloud computing solutions for expedient network access to a pool of shared resources. In this baseline study, as part of a nascent research track, the researchers examine a proposed design for cloud computing at Southern Polytechnic State University to support action research, applied learning and practical, real-world student experiences at the university. Access to university cloud computing resources via an academic research network, physically isolated from the current production network, is proposed. Following a system development life-cycle methodology, design criteria are derived from an analysis of focus group data involving questions related to academic research, applied instruction, and experiential and service learning. Presentation of findings occurs in the form of a use case and architectural topology rendering to be used as a basis for follow-on study in this research track. Physical implementation of cloud computing models at the University can follow this roadmap as the research track unfolds and data are collected to analyze and evaluate for optimal cloud architecture in support of research and education.",,2013,,61942758,semantic_scholar
7f27c90447026bbee088a3b58a62ed57b1bd7102,https://www.semanticscholar.org/paper/7f27c90447026bbee088a3b58a62ed57b1bd7102,Automated Optimization of Software Parameters in a Long Term Evolution Radio Base Station,"Radio network optimization is concerned with the configuration of radio base station parameters in order to achieve the desired level of service quality in addition to many other differentiating technical factors. Mobile network operators have different physical locations, levels of traffic profiles, number of connected devices, and the desired quality of service. All of these conditions make the problem of optimizing the parameters of a radio base station specific to the operator’s business goals. The high number of calibration parameters and the complex interaction between them make the system behave as a black-box model for any practical purpose. The computation of relevant operator metrics is often stochastic, and it can take several minutes to compute the effect of changing a single, making it impractical to optimize systems with approaches that require a large number of iterations. Operators want to optimize their already deployed system in online scenarios while minimizing the exposure of the system to a negative set of parameters during the optimization procedure. {This paper presents a novel approach to the optimization of a Long Term Evolution (LTE) radio base station in a large search space with an expensive stochastic objective and a limited regret bounds scenario. We show the feasibility of this approach by implementing it in an industrial testing bed radio base station connected to real User Equipment (UE) in collaboration with Ericsson. Two optimization processes in this experimental setup are executed to show the feasibility of the approach in real-world scenarios.",2019 IEEE International Systems Conference (SysCon),2019,10.1109/SYSCON.2019.8836830,202687917,semantic_scholar
5da0b64bded1123cfd8064b60dc78ca2a4a57075,https://www.semanticscholar.org/paper/5da0b64bded1123cfd8064b60dc78ca2a4a57075,OpenWiNo: An open hardware and software framework for fast-prototyping in the IoT,"The Internet of Things promises an always-connected future where the objects surrounding us will communicate in order to make our lives easier, more secure, etc. This evolution is a research opportunity as new solutions must be found to problems ranging from network interconnection to data mining. In the networking community, innovative solutions are being developed for the Device Layer of the Internet of Things, which includes the IoT wireless protocols. In order to study their performance, researchers turn more often to real world platforms, commonly designated by the term “testbeds”, on which they may implement and test the protocols and algorithms. This is even more important in the Industrial IoT field, where environments are perturbed by industrial systems like automated production systems. In this paper, after a brief presentation of the context of testbeds, we introduce WiNo and OpenWiNo, an open hardware and software framework for fast-prototyping in the field of the Internet of Things. Compared to existing platforms, the solution WiNo+OpenWiNo offers a wide array of Physical layers and easy integration of various sensors as it is developed as part of the Arduino ecosystem. It also allows research teams to easily and quickly deploy their own testbed into real environments.",2016 23rd International Conference on Telecommunications (ICT),2016,10.1109/ICT.2016.7500490,25137851,semantic_scholar
25ea28e7137dc82f157f13d5c26208918ecf38c4,https://www.semanticscholar.org/paper/25ea28e7137dc82f157f13d5c26208918ecf38c4,A testbed for collaborative development of underwater communications and networking,"The possibilities opened with the increased use of autonomous underwater vehicles and their potential interactions with existing or prospective submerged sensor networks create an end-user technological pull on the communication capabilities for the underwater domain. Simulation models, while fundamental in the scientific and technological development process cannot offer the feature richness of the physical environment and may potentially mask software and hardware behaviours exposed by the real world. This paper presents a testbed implemented by the NATO STO Centre for Maritime Research and Experimentation (CMRE), deployed to foster cooperative development of underwater communications and networking by providing an “hardware-in-the-world” capability to scientists and engineers. The data collection infrastructure provides a comprehensive data set of environmental measurements relevant to underwater acoustic propagation, arbitrary waveform generation within two frequency bands (useful for channel probing and testing of modulation schemes), full band raw acoustic data recording and access to two sets of fundamentally different commercially available acoustic modems. This structured data collection allows for a comprehensive analysis of the environment variables, their impact on the acoustic channel evolution and how this affects end-to-end connectivity of acoustic modems which can be used to steer the design choices for networking protocols.",MILCOM 2012 - 2012 IEEE Military Communications Conference,2012,10.1109/MILCOM.2012.6415691,11631735,semantic_scholar
431649836fc44c444b0640b0e9efd2e7eb591f1c,https://www.semanticscholar.org/paper/431649836fc44c444b0640b0e9efd2e7eb591f1c,Automatic classification of natural signals for environmental monitoring.,"This manuscript summarizes a three years work addressing the use of machine learning for the automatic analysis of natural signals. The main goal of this PhD is to produce efficient and operative frameworks for the analysis of environmental signals, in order to gather knowledge and better understand the considered environment. Particularly, we focus on the automatic tasks of detection and classification of natural events.This thesis proposes two tools based on supervised machine learning (Support Vector Machine, Random Forest) for (i) the automatic classification of events and (ii) the automatic detection and classification of events. The success of the proposed approaches lies in the feature space used to represent the signals. This relies on a detailed description of the raw acquisitions in various domains: temporal, spectral and cepstral. A comparison with features extracted using convolutional neural networks (deep learning) is also made, and favours the physical features to the use of deep learning methods to represent transient signals.The proposed tools are tested and validated on real world acquisitions from different environments: (i) underwater and (ii) volcanic areas. The first application considered in this thesis is devoted to the monitoring of coastal underwater areas using acoustic signals: continuous recordings are analysed to automatically detect and classify fish sounds. A day to day pattern in the fish behaviour is revealed. The second application targets volcanoes monitoring: the proposed system classifies seismic events into categories, which can be associated to different phases of the internal activity of volcanoes. The study is conducted on six years of volcano-seismic data recorded on Ubinas volcano (Peru). In particular, the outcomes of the proposed automatic classification system helped in the discovery of misclassifications in the manual annotation of the recordings. In addition, the proposed automatic classification framework of volcano-seismic signals has been deployed and tested in Indonesia for the monitoring of Mount Merapi. The software implementation of the framework developed in this thesis has been collected in the Automatic Analysis Architecture (AAA) package and is freely available.",,2018,,69764436,semantic_scholar
9a7936acb8420f589e457c9dc2a2035f4ecc2809,https://www.semanticscholar.org/paper/9a7936acb8420f589e457c9dc2a2035f4ecc2809,Mixed Reality Cubicles and Cave Automatic Virtual Environment,"In Cave Automatic Virtual Environments (CAVEs), a computer generated environment is projected all around a user to fully immerse or eliminate all reference to the real world. Typically, Virtual Reality (VR) CAVEs also track and respond to the user's physical orientation, movements and gestures. Mixed reality environments instead focus on combining real world objects with computer generated ones. In this paper, we focus on the application of Augmented Reality (AR) as a mixed reality technology via (or to) mobile devices such as head-mounted devices, smart-phones and tablets. We present the development of mixed reality applications for mobile (smart-phone and tablet) devices leading up to the implementation of an mixed reality (AR) cubicle for immersive Three Dimensional (3D) visualizations. We also present the results of a study on the familiarity with both VR and AR technologies among students from two institutions of tertiary education. The paper concludes with a discussion of planned deployment and upgrade of mixed reality cubicles using mobile VR equipment.",2016 15th International Conference on Ubiquitous Computing and Communications and 2016 International Symposium on Cyberspace and Security (IUCC-CSS),2016,10.1109/IUCC-CSS.2016.009,1814688,semantic_scholar
711e2a75dd57d2ae372a806a215f80dd8995c6e9,https://www.semanticscholar.org/paper/711e2a75dd57d2ae372a806a215f80dd8995c6e9,CAn't Touch This: Practical and Generic Software-only Defenses Against Rowhammer Attacks,"Rowhammer is a hardware bug that can be exploited to implement privilege escalation and remote code execution attacks. Previous proposals on rowhammer mitigation either require hardware changes or follow heuristic-based approaches (based on CPU performance counters). To date, there exists no instant protection against rowhammer attacks on legacy systems. 
In this paper, we present the design and implementation of two practical and efficient software-only defenses against rowhammer attacks. Our defenses prevent the attacker from leveraging rowhammer to corrupt physically co-located data in memory that is owned by a different system entity. Our first defense, B-CATT, extends the system bootloader to disable vulnerable physical memory. B-CATT is highly practical, does not require changes to the operating system, and can be deployed on virtually all x86-based systems. While B-CATT is able to stop all known rowhammer attacks, it does not yet tackle the fundamental problem of missing memory isolation in physical memory. To address this problem, we introduce our second defense G-CATT, a generic solution that extends the physical memory allocator of the OS to physically isolate the memory of different system entities (e.g., kernel and user space). 
As proof of concept, we implemented B-CATT on x86, and our generic defense, G-CATT, on x86 and ARM to mitigate rowhammer-based kernel exploits. Our extensive evaluation shows that both mitigation schemes (i) can stop available real- world rowhammer attacks, (ii) impose virtually no run-time overhead for common user and kernel benchmarks as well as commonly used applications, and (iii) do not affect the stability of the overall system.",ArXiv,2016,,8960214,semantic_scholar
eb4bd8ff9ac68485fb8f0f55cb7bd487a348ad40,https://www.semanticscholar.org/paper/eb4bd8ff9ac68485fb8f0f55cb7bd487a348ad40,Smart Connected Buildings Design Automation: Foundations and Trends,"Buildings are the result of a complex integration of multi-physics subsystems. Besides the obvious civil engineering infrastructure, thermal, electrical, mechanical, control, communication and computing subsystems must co-exist and be operated so that the overall operation is smooth and efficient. This is particularly important for commercial buildings but is also very relevant for residential buildings especially apartment buildings. Unfortunately, the design and deployment of these subsystems is rarely synchronized: lighting, security, heating, ventilation and air conditioning systems are often designed independently. However, simply putting together a collection of sub-systems, albeit optimized, has led to the inefficient buildings of today. Worldwide, buildings consume 42% of all electrical power - more than any other asset - and it can be proven that much of this can be reduced if a holistic approach to design, deployment, and operation is taken. Government agencies, academic institutions, building contractors and owners have realized the significant impact of buildings on the global environment, the electrical grid, and the mission of their organizations. However, the economic impact for all constituencies is still difficult to assess. Government regulations can play a fundamental role, as it has been the case for the transportation industry where regulations on emission and fuel consumption have been the single most important factor of innovation in automotive design. We are convinced that by leveraging technology and utilizing a system-level approach to buildings, they will provide comfort, safety and functionality while minimizing energy cost, supporting a robust electric grid and mitigating environmental impact. Realizing this vision requires adding intelligence from the beginning of the design phase, to deployment, from commissioning to operation, all the way to the end of the building's life cycle. In this issue, we attempt to provide an as-complete-as-possible overview of the activities in the field of smart connected building design automation that attempts to make the vision a reality. The overarching range of such activities includes developing simulation tools for modeling and the design of buildings, and consequently control algorithms proposed to make buildings smarter and more efficient. Furthermore, we will review real-world and large-scale implementation of such control strategies on physical buildings. We then present a formal co-design methodology to design buildings, taking the view that buildings are prime examples of cyber-physical systems where the virtual and physical worlds meet as more traditional products such as thermostats are able to connect online and perform complicated computational tasks to control building temperature effectively. We complete the presentation describing the growing role of buildings in the operation of the smart grid where buildings are not only consumers of energy, but are themselves also providers of services and energy to the grid. The audiences for this monograph are industry professionals and researchers who work in the area of smart buildings, smart cities, and smart grid, with emphasis on energy efficiency, simulation tools, optimal control, and cyber-physical systems for the emerging power markets.",Found. Trends Electron. Des. Autom.,2016,10.1561/1000000043,28198344,semantic_scholar
8646ca3ed644b8ae61f9432f28d5e025cb1f18ea,https://www.semanticscholar.org/paper/8646ca3ed644b8ae61f9432f28d5e025cb1f18ea,Green city: A low-cost testbed for distributed control algorithms in Smart Grid,"As a type of Cyber-Physical Systems (CPSs), Smart Grid has been adding more communication and control capabilities to improve power efficiency and availability. Especially, more and more distributed control algorithms have been developed for Smart Grids because of their flexibility and robustness. In order to deploy them in real electric power systems, distributed control algorithms must be tested, not only in theoretical simulations, but also in testbeds subject to real world constraints that can provide feedback to make the algorithm robust. Implementations of these algorithms in a Smart Grid environment are facing many cyber-physical challenges such as possible communication failures or imperfections, noisy signals, etc. These challenges can lead to increasing economical expenditure or cause failure of the power system. There exist different approaches for testing distributed control algorithms, from using state-of-the-art facilities to software or hardware-in-the-loop simulations. To better emulate real-world electric grid operation scenarios with low capital investment, in this paper the Green City (GC) testbed is proposed as a suitable platform for both control theory researchers in Smart Grid, and for engineering education, allowing students to learn through hands-on experiences. GC has been conceived as a multi-agent networked CPS with the following main features: 1- Smart Grid environment emulation with low-cost physical elements; 2- Fast prototyping capability of distributed control algorithms for Smart Grid.",IECON 2015 - 41st Annual Conference of the IEEE Industrial Electronics Society,2015,10.1109/IECON.2015.7392385,13411995,semantic_scholar
e7dd4afd555002393eb68db7f77ee2e1c17e9866,https://www.semanticscholar.org/paper/e7dd4afd555002393eb68db7f77ee2e1c17e9866,An evolving multi-agent scenario generation framework for simulations in preventive medicine education,"We describe the design, implementation and evaluation of a novel multi-agent scenario generation framework for interactive virtual reality simulations towards preventive medicine education. Our scenario generation framework is based on recordings of human movements from a distributed sensor networks deployed in a real-world physical setting. The components of our framework include the generation of unique virtual agent behaviors from the sensor data, and algorithms for the generation of low level or gross movement behaviors such as path determination, directional traffic flows, collision avoidance and overtaking. The framework also includes the generation of high level fine actions for multi-agents such as techniques for interactive activities in pedagogical scenarios based on environment and temporal triggers. We applied our multi-agent scenario generation framework in an interactive simulation for hand hygiene education, and conduct an initial usability study to assess the educational benefits of the simulation to nursing students and evaluated the performance characteristics of our framework. Results of our quantitative and qualitative evaluations suggest that our framework was robust in creating engaging, compelling, and realistic interactive training scenarios with multiple virtual agents in simulated hospital situations.",IHI '12,2012,10.1145/2110363.2110392,16570583,semantic_scholar
55b4107c8d37629d0378671324f56f9e801a6d4e,https://www.semanticscholar.org/paper/55b4107c8d37629d0378671324f56f9e801a6d4e,Efficient Long-Term Degradation Profiling in Time Series for Complex Physical Systems,"The long term operation of physical systems inevitably leads to their wearing out, and may cause degradations in performance or the unexpected failure of the entire system. To reduce the possibility of such unanticipated failures, the system must be monitored for tell-tale symptoms of degradation that are suggestive of imminent failure. In this work, we introduce a novel time series analysis technique that allows the decomposition of the time series into trend and fluctuation components, providing the monitoring software with actionable information about the changes of the system's behavior over time. We analyze the underlying problem and formulate it to a Quadratic Programming (QP) problem that can be solved with existing QP-solvers. However, when the profiling resolution is high, as generally required by real-world applications, such a decomposition becomes intractable to general QP-solvers. To speed up the problem solving, we further transform the problem and present a novel QP formulation, Non-negative QP, for the problem and demonstrate a tractable solution that bypasses the use of slow general QP-solvers. We demonstrate our ideas on both synthetic and real datasets, showing that our method allows us to accurately extract the degradation phenomenon of time series. We further demonstrate the generality of our ideas by applying them beyond classic machine prognostics to problems in identifying the influence of news events on currency exchange rates and stock prices. We fully implement our profiling system and deploy it into several physical systems, such as chemical plants and nuclear power plants, and it greatly helps detect the degradation phenomenon, and diagnose the corresponding components.",KDD,2015,10.1145/2783258.2788572,15867910,semantic_scholar
1214515daae90180cf912789250888c6233f4d07,https://www.semanticscholar.org/paper/1214515daae90180cf912789250888c6233f4d07,Three-dimensional wireless ad hoc and sensor networks 2016,"Distinct difference between twoand three-dimensional spaces has led to new research challenges to provide self-organizing communications for wireless ad hoc and sensor networks. Therefore, conventional approaches to extend or modify the existing schemes in twodimensional space cannot meet the specific requirements for three-dimensional networks. Instead, a new design and its implementation are usually demanded to accelerate deployment in real world. Based on research motivation, our previous Special Issue in 2014, ‘‘Three-dimensional wireless ad hoc and sensor networks,’’ seems successful in points of presenting the existing research efforts and attracting the interests from the community. In accordance with achievement, we intend to organize the second Special Issue for the same research area. While previous Special Issue was supposed to address the fundamental design issue, more practical approaches which contribute to advances in three-dimensional wireless ad hoc and sensor networks are our major objective. While considering our objective, editors believe that this Special Issue provides collection of articles on networking technique in three-dimensional ad hoc and sensor networks. We have selected 7 valuable papers out of 12 submissions in several aspects such as relevance to Special Issue and novelty of solution. The topic of these papers is roughly categorized into following areas: error detecting, localization, routing protocol, application, and simulation tool. In the first paper titled ‘‘A hybrid decoding of Reed– Muller codes,’’ Shuang Li et al. proposed hybrid decoding algorithm for Reed–Muller (RM) codes to decrease the number of floating-point multiplications significantly. The proposed algorithm reduced computational complexity for decoding of RM codes by terminating recursion procedure in earlier stage. A simplified maximum-likelihood (ML) decision based on fast Hadamard transform (FHT) is another source of low complexity. Simulation results were given to prove the improved performance of error correction as compared to conventional algorithms. Next two papers are related to localization problem. One is for Bluetooth and the other for wireless sensor networks. In the second paper titled ‘‘Three-dimensional positioning system using Bluetooth low-energy beacons,’’ Hyunwook Park et al. introduced threedimensional positioning scheme for Bluetooth. The proposed scheme used Bluetooth low-energy (BLE) beacons to estimate the distance and calculated threedimensional coordinates based on three-dimensional triangulation. A proposed scheme measures threedimensional location of moving nodes by employing four fixed position beacon nodes to form random sphere to collect position of each node. Simulation results reveal that the proposed method can reduce distance error rate rather than the existing twodimensional triangulation method. In the third paper titled ‘‘A distance-based maximum likelihood estimation method for sensor localization in wireless sensor networks,’’ Jing Xu et al. studied node localization in wireless sensor networks since conventional maximumlikelihood estimation (MLE) scheme based on received signal strength indicator (RSSI) failed to reflect the physical characteristics properly. To address this issue, in this paper, distance-based MLE (DB-MLE) to consider measurement errors was formulated as a complicated nonlinear optimization problem. Furthermore, two solutions based on first-order optimal condition and two-dimensional search method were presented. Simulation results showed that DB-MLE provided higher localization accuracy than the other methods. In the fourth paper titled ‘‘Autonomous drone for delay-tolerant networks in indoor applications,’’ Rados1aw O. Schoeneich et al. introduced interesting idea and application of autonomous drones as mobile message ferries in delay-tolerant networks. In order to prove applicability, universal software architecture of drone based on Android devices and its detail prototype were presented. This implementation was tested with the autonomous movement and observed to pass all relevant tests. In the fifth paper titled ‘‘Cooperative downloading in mobile ad hoc networks: a cost-energy perspective,’’ He Li et al. presented another interesting application, cooperative downloading. To improve file downloading, contents are distributed by the help of mobile",Int. J. Distributed Sens. Networks,2017,10.1177/1550147717715974,13050478,semantic_scholar
4dd4afbb17999bdf9e218001e3a6ae2252c10f8f,https://www.semanticscholar.org/paper/4dd4afbb17999bdf9e218001e3a6ae2252c10f8f,Visualizing UAS-collected imagery using augmented reality,"One of the areas where augmented reality will have an impact is in the visualization of 3-D data. 3-D data has traditionally been viewed on a 2-D screen, which has limited its utility. Augmented reality head-mounted displays, such as the Microsoft HoloLens, make it possible to view 3-D data overlaid on the real world. This allows a user to view and interact with the data in ways similar to how they would interact with a physical 3-D object, such as moving, rotating, or walking around it. A type of 3-D data that is particularly useful for military applications is geo-specific 3-D terrain data, and the visualization of this data is critical for training, mission planning, intelligence, and improved situational awareness. Advances in Unmanned Aerial Systems (UAS), photogrammetry software, and rendering hardware have drastically reduced the technological and financial obstacles in collecting aerial imagery and in generating 3-D terrain maps from that imagery. Because of this, there is an increased need to develop new tools for the exploitation of 3-D data. We will demonstrate how the HoloLens can be used as a tool for visualizing 3-D terrain data. We will describe: 1) how UAScollected imagery is used to create 3-D terrain maps, 2) how those maps are deployed to the HoloLens, 3) how a user can view and manipulate the maps, and 4) how multiple users can view the same virtual 3-D object at the same time.",Defense + Security,2017,10.1117/12.2262864,125964869,semantic_scholar
e5b1b40f91cc77ffe862a7b220483f1a8660e0bf,https://www.semanticscholar.org/paper/e5b1b40f91cc77ffe862a7b220483f1a8660e0bf,Towards middleware security framework for next generation data centers connectivity,"Data Center as a Service (DCaaS) facilitates to clients as an alternate outsourced physical data center, the expectations of business community to fully automate these data centers to run smoothly. Geographically Distributed Data Centers and their connectivity has major role in next generation data centers. In order to deploy the reliable connections between Distributed Data Centers, the SDN based security and logical firewalls are attractive and enviable. We present the middleware security framework for software defined data centers interconnectivity, the proposed security framework will be based on some learning processes, which will reduce the complexity and manage very large number of secure connections in real-world data centers. In this paper we will focus on two main objectives; (1) proposing simple and yet scalable techniques for security and analysis, (2) Implementing and evaluating these techniques on real-world data centers.",2015 Science and Information Conference (SAI),2015,10.1109/SAI.2015.7237308,5713100,semantic_scholar
391f0e6d14e1b98ff759891c72bb45f5b7323792,https://www.semanticscholar.org/paper/391f0e6d14e1b98ff759891c72bb45f5b7323792,Data warehousing fundamentals for IT professionals,"Preface. Part 1 OVERVIEW AND CONCEPTS. 1 The Compelling Need for Data Warehousing. Chapter Objectives. Escalating Need for Strategic Information. Failures of Past Decision Support Systems. Operational Versus Decision-Support Systems. Data Warehousing--The Only Viable Solution. Data Warehouse Defined. The Data Warehousing Movement. Evolution of Business Intelligence. Chapter Summary. Review Questions. Exercises. 2 Data Warehouse: The Building Blocks. Chapter Objectives. Defining Features. Data Warehouses and Data Marts. Architectural Types. Overview of Components. Metadata in the Data Warehouse. Chapter Summary. Review Questions. Exercises. 3 Trends in Data Warehousing. Chapter Objectives. Continued Growth in Data Warehousing. Vendor Solutions and Products. Significant Trends. Emergence of Standards. Web-Enabled Data Warehouse. Chapter Summary. Review Questions. Exercises. Part 2 PLANNING AND REQUIREMENTS. 4 Planning and Project Management. Chapter Objectives. Planning Your Data Warehouse. The Data Warehouse Project. The Project Team. Project Management Considerations. Chapter Summary. Review Questions. Exercises. 5 Defining the Business Requirements. Chapter Objectives. Dimensional Analysis. Information Packages A Useful Concept. Requirements Gathering Methods. Requirements Definition: Scope and Content. Chapter Summary. Review Questions. Exercises. 6 Requirements as the Driving Force for Data Warehousing. Chapter Objectives. Data Design. The Architectural Plan. Data Storage Specifications. Information Delivery Strategy. Chapter Summary. Review Questions. Exercises. Part 3 ARCHITECTURE AND INFRASTRUCTURE. 7 The Architectural Components. Chapter Objectives. Understanding Data Warehouse Architecture. Distinguishing Characteristics. Architectural Framework. Technical Architecture. Architectural Types. Chapter Summary. Review Questions. Exercises. 8 Infrastructure as the Foundation for Data Warehousing. Chapter Objectives. Infrastructure Supporting Architecture. Hardware and Operating Systems. Database Software. Collection of Tools. Data Warehouse Appliances. Chapter Summary. Review Questions. Exercises. 9 The Significant Role of Metadata. Chapter Objectives. Why Metadata is Important. Metadata Types by Functional Areas. Business Metadata. Technical Metadata. How to Provide Metadata. Chapter Summary. Review Questions. Exercises. Part 4 DATA DESIGN AND DATA PREPARATION. 10 Principles of Dimensional Modeling. Chapter Objectives. From Requirements to Data Design. The STAR Schema. STAR Schema Keys. Advantages of the STAR Schema. STAR Schema: Examples. Chapter Summary. Review Questions. Exercises. 11 Dimensional Modeling: Advanced Topics. Chapter Objectives. Updates to the Dimension Tables. Miscellaneous Dimensions. The Snowflake Schema. Aggregate Fact Tables. Families of STARS. Chapter Summary. Review Questions. Exercises. 12 Data Extraction, Transformation, and Loading. Chapter Objectives. ETL Overview. Data Extraction. Data Transformation. Data Loading. ETL Summary. Other Integration Approaches. Chapter Summary. Review Questions. Exercises. 13 Data Quality: A Key to Success. Chapter Objectives. Why is Data Quality Critical?. Data Quality Challenges. Data Quality Tools. Data Quality Initiative. Master Data Management (MDM). Chapter Summary. Review Questions. Exercises. Part 5 INFORMATION ACCESS AND DELIVERY. 14 Matching Information to the Classes of Users. Chapter Objectives. Information from the Data Warehouse. Who Will Use the Information? Information Delivery. Information Delivery Tools. Information Delivery: Special Topics. Chapter Summary. Review Questions. Exercises. 15 OLAP in the Data Warehouse. Chapter Objectives. Demand for Online Analytical Processing. Major Features and Functions. OLAP Models. OLAP Implementation Considerations. Chapter Summary. Review Questions. Exercises. 16 Data Warehousing and the Web. Chapter Objectives. Web-Enabled Data Warehouse. Web-Based Information Delivery. OLAP and the Web. Building a Web-Enabled Data Warehouse. Chapter Summary. Review Questions. Exercises. 17 Data Mining Basics. Chapter Objectives. What is Data Mining?. Major Data Mining Techniques. Data Mining Applications. Chapter Summary. Review Questions. Exercises. Part 6 IMPLEMENTATION AND MAINTENANCE. 18 The Physical Design Process. Chapter Objectives. Physical Design Steps. Physical Design Considerations. Physical Storage. Indexing the Data Warehouse. Performance Enhancement Techniques. Chapter Summary. Review Questions. Exercises. 19 Data Warehouse Deployment. Chapter Objectives. Data Warehouse Testing. Major Deployment Activities. Considerations for a Pilot. Security. Backup and Recovery. Chapter Summary. Review Questions. Exercises. 20 Growth and Maintenance. Chapter Objectives. Monitoring the Data Warehouse. User Training and Support. Managing the Data Warehouse. Chapter Summary. Review Questions. Exercises. Answers to Selected Exercises. Appendix A. Project Life Cycle Steps and Checklists. Appendix B. Critical Factors for Success. Appendix C. Guidelines for Evaluating Vendor Solutions. Appendix D. Highlights of Vendors and Products. Appendix E. Real-World Examples of Best Practices. References. Glossary. Index.",,2010,,60064398,semantic_scholar
497ed558a130464edf5ae4b974e35cb6b374a54d,https://www.semanticscholar.org/paper/497ed558a130464edf5ae4b974e35cb6b374a54d,An Active Learning Environment to Improve First-Year Mechanical Engineering Retention Rates and Software Skills,"This work proposes a foundational change from traditional lecture to an active learning environment in the Colorado State University First-Year Introduction to Mechanical Engineering course of 145 students. The goal of this approach is to improve computational capabilities in Mechanical Engineering and long-term retention rates with a single broad emphasis. Major and minor changes were implemented in the course, from specific day to day in-class activities to the addition of laboratory sessions to replace traditional classroom lecture. These laboratories of no more than fifteen students were delivered by Learning Assistants, which were upper-level undergraduate peer educators. To evaluate proficiency, a MATLAB post-test was delivered to students who were instructed through lecture only (“Lecture”) and those who were instructed with the above changes (“Active”). A survey was also provided upon completion of the course to the Active group for student reflection on their perceived software capability and the usefulness of approaches. Post-test results suggest that the Active group was more proficient in MATLAB than the Lecture group. Survey results suggest that the Active group recognize they had not achieved expert use of the software but that they were likely to use it throughout their careers and that all approaches were useful, in particular the use of Learning Assistants. Future longterm retention statistics will shed light on the possible effectiveness of this approach, which are currently unavailable. Introduction Colorado State University has a total student enrollment in excess of 33,000. As a land grant university, the historic mission of the institution is to provide students with an education in practical fields such as agriculture and engineering. The College of Engineering has a growing student cohort, with an increase from ~450 first-year students Fall 2010 to ~600 students Fall 2015 [1]. However, persistence and graduation rates have remained fairly steady over the last fifteen years. The current six year persistence rate within the college is only ~45% and the six year graduation rate within the college is similar at ~43%. Many students do not remain within the college for even a full year, as the second fall persistence rate is only 70-75% [1]. These data show a significant portion of enrolled first-year engineering students do not remain within the program long enough to be exposed to foundational engineering content, which starts in the sophomore year with engineering specific courses. A current goal of the college is to improve these retention statistics. Additionally, many students do not develop the necessary software skills required to use computational tools such as MATLAB, which are integral to success in the curriculum. Students who do not develop these skills during introductory coursework must “catch up” in later courses, where the technical content is more challenging. We hypothesize this can lead to unpreparedness for challenging content or careers as an engineer and can negatively impact academic standing, leading to decreased retention. Thus, the goals of this work were to 1) improve retention rates for first-year engineering students, specifically mechanical engineering, and 2) improve computational and software skills of first-year students, specifically MATLAB and Microsoft Excel. MATLAB is a common computational package which can be used for a broad range of engineering problems throughout a curriculum [2]. However, learning Excel and MATLAB through lecture is challenging, as these tools are best understood through utilization, not observation [3]. MATLAB and other computational tools are often taught in classrooms with computational equipment, however this is can be a challenge with a large classroom [4]. Some have utilized computer based tutorials which students can complete on their own time [5], while others implemented a large scale deployment of personal computers equipped with MATLAB and other software [6]. Additionally, the use of peer-educators can be an effective approach to facilitating MATLAB development [7]. Thus, we have chosen to employ an approach which utilizes an active environment to learn MATLAB and other introductory content through the use of laboratory sessions and peer-educators, in this case the Learning Assistant model [8]. Similar to previous approaches, we have utilized classroom lectures, hands on in-class activities, and laboratory sessions [9]. The Introduction to Mechanical Engineering Course (MECH 103) was developed to provide students with an overview of the mechanical engineering discipline and as an introduction to the computational packages MATALB [10] and Microsoft Excel. The course consists of between 140 and 250 first-year students and was previously delivered using traditional lecture. While this approach was most efficient for a single instructor due to the enrollment size, this resulted in a static learning environment for a course which should excite students about mechanical engineering and provide foundational technical skills. The overall approach to this work was to thus create an active environment for students within the course, which had an enrollment of 145 students for the Fall 2016 semester. The rationale to this approach was that by providing students with hands-on experiences working with mechanical engineering problems and computational software, the understanding of course content will improve [11,12] whereby improving retention [13]. While some immediate test and survey data were acquired and are shown in this work, it is important to note that the true impact on retention is not currently recognizable and will require future analysis. In-Class Sessions Class sessions were varied throughout the semester and the week, as they typically included lectured course material, guest lectures or panels, and activities. The course met Monday, Wednesday, and Friday from 9-9:50 AM in a large lecture hall with individual stadium seating. Friday lecture was often cancelled and this time was spent in weekly laboratory sessions instead, which are outlined in the next section. Monday class time was assigned to covering course content through lecture, teamwork activities, and in class problems. The content of the course included general introductory material such as teamwork, communication, and design, commonly used units and unit conversions, mathematical models and systems, and an introduction to Microsoft Excel and MATLAB. Active engagement in the class included a teamwork design problem, requiring students to break into groups of three. Due to the theater seating layout of the classroom, groups of four or more made successful teamwork and communication difficult. Each group of students were provided one piece of 8.5” x 11” blank printer paper, one paperclip, and two pieces of scotch tape. The design problem was simple: build the tallest free standing structure possible using only the given materials. This was an inexpensive and simple approach to teamwork design activity. In place of a lecture or even a discussion on how to use design techniques for a simple problem such as this, students were able to actively engage in this process despite the difficulties of class size and layout. While students typically have an excellent understanding of units such as a pound (lb), their physical understanding of units such as a Joule or Watt are less developed within the context of everyday life. To provide students with a meaningful representation of energy (Joule) and power (Watt), they were provided a common object – in this case a softball – and asked to calculate how high they would have to raise the object to exert one Joule of energy – in this case roughly a foot and a half. While simple and inexpensive, this activity provided students with useful knowledge they can apply without a calculator and helps them relate coursework to the real world. For example, if they can place a Joule into real-world context, they could then answer the question “Can I launch a rocket into space using a thousand Joules?”. Wednesday lecture sessions were commonly used for guest lecturers and panels. These class sessions included the College of Engineering Dean, faculty members and graduate students in mechanical engineering, industry panelists, entrepreneurs and small business owners, and an interactive teamwork theatre troupe. The goal of these sessions was to provide students with a broad overview of different disciplines within mechanical engineering and what skills are necessary to succeed in various professional roles. While emphasizing an active learning environment is inherently difficult with each and every guest, student engagement was addressed by delivering variability in all of the presentations and strongly encouraging students to ask questions. For example, the theater troupe was an interactive experience where students were able to act as a team member within a group that mocked to show a diverse team struggling with communication. This session involved humor, discussion, and lively responses from students in place of a traditional static lecture. Laboratory Sessions In place of Friday lecture, students were asked to attend laboratory sessions for one hour [14,3]. A total of eleven sessions were provided throughout the week to accommodate all schedules. Sessions included one instructor, 13-16 students, and were held in laboratories with individual workstations with Microsoft Excel and MATLAB software. Laboratory instructors included a Graduate Teaching Fellow and Undergraduate Learning Assistants (LAs). Laboratory sessions involved a short (<5 minutes) lecture briefly reviewing content from class before students began working on assigned problems. These problems implemented course content such as the use of Excel or MATLAB to analyze and display data through real-world applications. An example of utilizing MATLAB to simulate rolling a die is p",,2017,10.18260/1-2--27546,33114557,semantic_scholar
5f5c5c44c2bff3f2f9e4d53191cbc6d0a67839d9,https://www.semanticscholar.org/paper/5f5c5c44c2bff3f2f9e4d53191cbc6d0a67839d9,Supervisory control theory for controlling swarm robotics systems,"Swarm robotics systems have the potential to tackle many interesting problems. Their control software is mostly created by ad-hoc development. This makes it hard to deploy 
swarm robotics systems in real-world scenarios as it is difficult to analyse, maintain, or extend these systems. Formal methods can contribute to overcome these problems. 
However, they usually do not guarantee that the implementation matches the specification because the system’s control code is typically generated manually. 
 
This thesis studies the application of the supervisory control theory (SCT) framework in swarm robotics systems. SCT is widely applied and well established in the man- 
ufacturing context. It requires the system and the desired behaviours (specifications) to be defined as formal languages. In this thesis, regular languages are used. Regular languages, in the form of deterministic finite state automata, have already been widely applied for controlling swarm robotics systems, enabling a smooth transition from the ad-hoc development currently in practice. This thesis shows that the control code for 
swarm robotics systems can be automatically generated from formal specifications. 
 
Several case studies are presented that serve as guidance for those who want to learn how to specify swarm behaviours using SCT formally. The thesis provides the tools for 
the implementation of controllers using formal specifications. Controllers are validated on swarms of up to 600 physical robots through a series of systematic experiments. 
 
It is also shown that the same controllers can be automatically ported onto different robotics platforms, as long as they offer the required capabilities. The thesis extends and incorporates techniques to the supervisory control theory framework; specifically, the concepts of global events and the use of probabilistic generators. It can be seen as a step towards making formal methods a standard practice in swarm robotics.",,2016,,117662445,semantic_scholar
47486d010d940cc014bd1df89974b53bd73e91a2,https://www.semanticscholar.org/paper/47486d010d940cc014bd1df89974b53bd73e91a2,Graph Analysis of Fog Computing Systems for Industry 4.0,"Increased adoption of Fog Computing concepts into Cyber Physical Systems (CPS) is a driving force for implementing Industry 4.0. The modern industrial environment focuses on providing a flexible factory floor that suits the needs of modern manufacturing through the reduction of downtimes, reconfiguration times, adoption of new technologies and the increase of its production capabilities and rates. Fog Computing through CPS aims to provide a flexible orchestration and management platform that can meet the needs of this emerging industry model. Proposals on Fog Computing platform and Software Defined Networks (SDN) for Industry allow for resource virtualization and access throughout the system enabling large composite application systems to be deployed on multiple nodes. The increase of reliability, redundancy and runtime parameters as well as the reduction of costs in such systems are of key interest to Industry and researchers as well. The development of optimization algorithms and methods is made difficult by the complexity of such systems and the lack of real-world data on fog systems resulting in algorithms that are not being designed for real world scenarios. We propose a set of use-case scenarios based on our Industrial partner that we analyze to determine the graph based parameters of the system that allows us to scale and generate a more realistic testing scenario for future optimization attempts as well as determine the nature of such systems in comparison to other networks types. To show the differences between these scenarios and our real-world use-case we have selected a set of key graph characteristics based on which we analyze and compare the resulting graphs from the systems.",2017 IEEE 14th International Conference on e-Business Engineering (ICEBE),2017.0,10.1109/ICEBE.2017.17,8296615,semantic_scholar
441730142e0ac938f9d656c3ada16118fdcbd6bb,https://www.semanticscholar.org/paper/441730142e0ac938f9d656c3ada16118fdcbd6bb,Usermode OS components on seL4 with rump kernels,"seL4 is a formally-verified high-assurance microkernel that provides isolation to properly designed applications that it executes. Real-world cyber-physical systems can use seL4 for increased security. Many applications rely on the operating system to provide system services, such as device drivers, file systems and networking capabilities, however seL4 only provides these in a limited capacity which limits its deployment. Adding support to this wide array of systems that can benefit from the additional security seL4 provides would require reimplementing the millions of lines of operating system code that these systems require. This is infeasible without an approach that reuses existing components. Current methods either require providing services by running a paravirtualised version of the Linux kernel which provides only coarse isolation or by developing specific services on an as-needed basis which does not scale for many devices. Rump kernels are a NetBSD project for running NetBSD system services in different environments such as in user-mode on a microkernel. This thesis evaluates rump kernels as an approach to provide driver-like operating system components in user-mode on the seL4 microkernel. This will be achieved by adding a seL4 platform to the Rumprun unikernel, an existing project that uses rump kernels. We evaluate our implementation to compare its performance with other software systems and to investigate the level of overhead our implementation adds. We also show that the effort required to use rump kernels is low and by using NetBSD system services we increase the amount of devices that can be used with seL4. This thesis contains background information and related work, details on our design and implementation and our evaluation, future work and conclusion.",,2016.0,,29917336,semantic_scholar
ca54656aecac98f2c688e973e92bfdcf15b04efc,https://www.semanticscholar.org/paper/ca54656aecac98f2c688e973e92bfdcf15b04efc,Multiport antenna performance analysis from ray-traced channels for small cells,"The evaluation of multi-element antenna (MEA) performance in urban environments is particularly important for the planning and design of reliable, spectrally efficient systems. To date, MIMO communication using large dimensions has not been implemented for commercial systems. But the potential capacity of MIMO is much more significant when the MIMO dimensions become large. The hold-up is not just the shortfall in understanding how to deploy large-dimension MIMO, but also how to design the antenna systems. There is no widely agreed method for evaluating MEAs used by MIMO communications. Even the performance evaluation of a well-established MIMO configuration, such as the small scale systems already used, remains limited. The evaluation often defers to OTA tests in artificial environmental conditions. The real-world performance remains unclear, so care must be taken in design optimization. In this paper, we discuss the incorporation of some physical modeling of the propagation environment to evaluating the MEA performance for a typical urban wireless channel. A ray-optic method is applicable to any 3D environment, and any number of antennas with arbitrary patterns. In this sense it offers a practical approach for large-dimension MIMO evaluation by being able to change the antennas and the environment, all in software. The evaluation method is illustrated by using an MEA slot cube (12 antennas) operating in a specific environment, namely downtown Ottawa.",The 8th European Conference on Antennas and Propagation (EuCAP 2014),2014.0,10.1109/EUCAP.2014.6902535,26233783,semantic_scholar
e1184cc1e4725f7736d9944a33ada01a626cedc3,https://www.semanticscholar.org/paper/e1184cc1e4725f7736d9944a33ada01a626cedc3,Learning in Robotics,"For a robot, the ability to get from one place to another is one of the most basic skills. However, locomotion on legged robots is a challenging multidimensional control problem. This paper presents a machine learning approach to legged locomotion, with all training done on the physical robots. The main contributions are a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithms for learning quadrupedal locomotion. The resulting learned walk is considerably faster than all previously reported hand-coded walks for the same robot platform. Introduction The ability to deploy a fully autonomous robot in an unstructured, dynamic environment (the proverbial real world) over an extended period of time remains an open challenge in the field of robotics. Considerable progress is being made towards many components of this task including physical agility, power management, and on-board sensor technology. One such component that has drawn considerable interest recently is the ability for a robot to autonomously learn to improve its own performance (Ng et al. 2004; Bagnell & Schneider 2001; Zhang & Vadakkepat 2003). Despite this interest, considerable work remains due to the di fficulties associated with machine learning in the real world . Compared to other machine learning scenarios such as classification or action learning in simulation, learning o n physical robots presents several formidable challenges, i ncluding the following. Sparse Training Data: It is often prohibitively difficult to generate large amounts of data due to the maintenance required on robots, such as battery changes, hardware repairs, and, usually, constant human supervision. Thus, learning methods designed for physical robots must be effective with small amounts of data. Dynamical Complexity: The dynamics of many robotic control tasks are too complex for faithful simulation to be possible. Furthermore, robots are inherently situated in an unstructured environment with unpredictable sensor and actuator noise, namely the real world. Thus, even when off-line simulation is possible, it can never be fully reflective of the target environment. In this paper, we overcome these challenges for one concrete complex robot task, namely legged locomotion. Using a commercially available quadruped robot, we fully automate the training process (other than battery changes) and Copyright c © 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. employ machine learning algorithms that are sufficiently data efficient to enable productive learning on physical robots in a matter of hours. The resulting learned walk is considerably faster than all previously reported hand-cod ed walks for the same robot platform. This paper contributes both a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithms for learning quadrupedal locomotion. The remainder of the paper is organized as follows. First, we introduce the parameterized walk which our learning process seeks to optimize. We then specify our four learning approaches, and follow with detailed empirical results. We close with a discussion of their implications and possible avenues for future work. A Parameterized Walk The Sony Aibo ERS-210A is a commercially available robot that is equipped with a color CMOS camera and an optional ethernet card that can be used for wireless communication. The Aibo is a quadruped robot, and has three degrees of freedom in each of its four legs (Sony 2004). At the lowest level, the Aibo’s gait is determined by a series of joint positions for the three joints in each of its leg s. An early attempt to develop a gait by Hornby et al. (1999) involved using a genetic algorithm to learn a set of lowlevel parameters that described joint velocities and body p osition.1 More recent attempts to develop gaits for the Aibo have involved adopting a higher-level representation that deals with the trajectories of the Aibo’s four feet through three-dimensional space. An inverse kinematics calculati on is then used to convert these trajectories into joint angles . Among higher-level approaches, most of the differences between gaits that have been developed for the Aibo stem from the shape of the loci through which the feet pass and the exact parameterizations of those loci. For example, a team from the University of New South Wales achieved the fastest known hand-tuned gait using the high-level approach described above with trapezoidal loci. They subsequently generated an even faster walk via learning (Kim & Uther 2003). A team from Germany created a flexible gait implementation that allows them to use a variety of different shapes of loci (Rofer et al. 2003), and the team from the University of Newcastle was able to generate highvelocity gaits using a genetic algorithm and loci of arbitra ry shape (Quinlan, Chalup, & Middleton 2003). Our team (UT Austin Villa, Stone t al. 2004) first approached the gait optimization problem by hand-tuning Developed on an earlier version of the Aibo. a gait described by half-elliptical loci. This gait performed comparably to those of other teams participating in RoboCup 2003. The work reported in this paper uses the hand-tuned UT Austin Villa walk as a starting point for learning. Figure 1 compares the reported speeds of the gaits mentioned above, both hand-tuned and learned, including that of our starting point, the UT Austin Villa walk. The latter walk is described fully in a team technical report (Stone et al. 2004). The remainder of this section describes those details of the UT Austin Villa walk that are important to understand for the purposes of this paper. Hand-tuned gaits Learned gaits CMU Austin Villa UNSW Hornby UNSW NUBots (2002) (2003) (2003) (1999) (2003) (2003) 200 245 254 170 270 296 Figure 1: Maximum forward velocities of the best gaits (in mm/s) for different teams, both learned and hand-tuned. The half-elliptical locus used by our team is shown in Figure 2. By instructing each foot to move through a locus of this shape, with each pair of diagonally opposite legs in phase with each other and perfectly out of phase with the other two (a gait known as a trot), we enable the Aibo to walk. Four parameters define this elliptical locus: 1. The length of the ellipse; 2. The height of the ellipse; 3. The position of the ellipse on the x axis; and 4. The position of the ellipse on the y axis. Since the Aibo is roughly symz",,2006.0,,14687420,semantic_scholar
6e7e52c8f59ec975cb9b850cef1ecf8470b9c28a,https://www.semanticscholar.org/paper/6e7e52c8f59ec975cb9b850cef1ecf8470b9c28a,Physical hardware-in-the-loop modelling and simulation,"It is too risky to install a newly-designed device, component, or controller, directly into a real power system without rigorous testing. To help to de-risk the system integration, and to assist in the design process, computer simulation is an accepted and widely-adopted tool. However, in a simulation-only environment, many real-world issues such as noise, randomness of event timings, and hardware design issues are not well explored. In addition, there are limits on the size and fidelity of system which can be simulated, due to the required computational intensity, and because control systems for devices often contain software which is proprietary and cannot be modelled accurately. Physical Hardware in the Loop Simulation provides an interim stage between purely computer-based simulation, and real device deployment. Part of the power system (or “Smart Grid”) is simulated, but specific components are implemented in actual hardware. The hardware may consist of instrumentation, relays or controllers, carrying no primary current. Such testing is termed “Secondary Hardware-in-the-Loop”, as the signals exchanged between the simulation and hardware consist only of measurements and control values. A more advanced environment is created where primary power flow is exchanged with the hardware. This is termed “Primary Hardware-in-the-Loop” or “Power Hardware-in-the-Loop” testing. In addition to measurement and control signals being exchanged with the simulation, an interface is required at which primary power is exchanged between the simulation and the hardware, at the voltage and current levels suitable for the hardware under test. Creation of such environments is complex, but allows steady-state, dynamic, and worst-case scenarios to be re-created in a controlled environment. Therefore hardware-in-the-loop testing offers a cheaper, safer, faster and more comprehensive de-risking process than trying the hardware for the first time on a real network. The complexity and interconnected nature of the Smart Grid means that such Hardware in the Loop based testing is becoming even more critical to understanding the behaviour of systems and schemes, and consequently the safe and secure introduction of new technologies.",,2016.0,10.1002/9781118755471.SGD078,112826757,semantic_scholar
d753886538b5a9970a79212f5a995d55a8ee4c33,https://www.semanticscholar.org/paper/d753886538b5a9970a79212f5a995d55a8ee4c33,A AdaFT : A Framework for Adaptive Fault Tolerance for Cyber-Physical Systems,"Cyber-physical systems frequently have to use massive redundancy to meet application requirements for high reliability. While such redundancy is required, it can be activated adaptively, based on the current state of the controlled plant. Most of the time the plant is in a state that allows for a lower level of faulttolerance. Avoiding the continuous deployment of massive fault tolerance will greatly reduce the workload of the CPS, and lower the operating temperature of the cyber sub-system, thus increasing its reliability. In this paper, we extend our prior research by demonstrating a software simulation framework (AdaFT) that can automatically generate the sub-spaces within which our adaptive fault tolerance can be applied. We also show the theoretical benefits of AdaFT, and its actual implementation in several real world CPSs. CCS Concepts: rComputer systems organization→ Embedded systems; Redundancy; Robotics;",,2016.0,,37471369,semantic_scholar
83c19e91c197218df688172968455ff9d4efc7fe,https://www.semanticscholar.org/paper/83c19e91c197218df688172968455ff9d4efc7fe,Enhancing liveness testing for transferring data packets through using automatic test packet generation,"-Networks are getting bigger and more complex, yet administrators rely on incomplete tools such as and to debug problems. We propose an automated and systematic approach for testing and rectify networks called “Automatic evaluates Package Generation” (ATPG). ATPG reads router configurations and generates a device-independent model. The model is used to generate a minimum set of test packets to minimally exerting every link in the network or maximally exerting every rule in the network. Test packets are sent periodically, and detected failures trigger a separate mechanism to localize the revoke. ATPG can detect both functional and renderings problems. ATPG complements but goes beyond earlier work in static checking for which cannot detect liveness or performance faults or fault localization which only localize revoke given liveness results. We describe our prototype ATPG implementation and results on two real-world data sets: Stanford University’s backbone network and Internet. We find that a small number of test packets suffice to test all rules in these networks. A sending 4000 test packet 10 times per second consumes less than 1% of link capacity. ATPG code and the datasets are publicly available. Keyword: ATPG, liveness, Networks. ________________________________________________________________________________________________________ I.INTRODUCTION Networking is the word fundamentally cogitates to computers and their property. It is very often used in the world of computers and their use in different connections. The term networking express the link between two or more computers and their tendency, with the vital purpose of sharing the data stored in the computers, with each other. The networks between the computing tendencies are very public these days due to the launch of assorted hardware and computer software which aid in making the activity much more convenient to build and use. Fig: 1.1Structure of Networking between the different computers The discuss about Figure: 1.1 Structure of Networking between the different computer. Its main process of share the Internet to different things and devices. General Network Techniques When computers communicate on a network, they send out information packets without knowing if anyone is listening. Computers in a network all have an attached to the network and that is called to be attached to a network bus. What one computer sends out will reach the other computer on the local area network. © 2017 IJEDR | Volume 5, Issue 1 | ISSN: 2321-9939 IJEDR1701073 International Journal of Engineering Development and Research (www.ijedr.org) 476 Fig: 1.2 the clear idea about the networking functions The discus about figure: 1.2 clear ideas of network function and different computers to be able to distinguish between each other, every computer have unique ID called MAC-address Media Access Control Address. This address is not only unique on your network but unique for all tendencies that can be aquiline up to a network. The MAC-address is tied to the hardware and has nothing to do with IP-addresses. Since all computers on the network graduate inversion that is sent out from all other computers the MACaddresses is primarily used by the computers to filter out incoming network traffic that is addressed to the scratcher computer. When a computer expostulation with another computer on the network, it sends out both the other computers MAC-address and the MACaddress of its own. In that way the receiving computer will not only realize that this parcel is for me but also, who sent this data packet so a return response can be sent to the sender. Ethernet network as delineate here, all computers hear all network aggregation since they are attached to the same bus. This network structure is called multi-drop. One problem with this network structure is that when you have, let say ten computers on a network and they expostulation attendance and due to that they sends out there data packets randomly, collisions occur when two or more computers sends data at the same time. When that happens data gets imperfect and has to be resent. On a network that is heavy loaded even the resent packets collide with other packets and have to be resent again. In reality this soon takes affect an information problem. If respective computers communicate with each other at high speed they may not be able to utilize more than 25% of the total network information measure since the rest of the bandwidth is used for regressive antecedently corrupted packets. The way to minimize this problem is to use network switches. II.RELATED NETWORK Detecting the occurrence and location of performance anomalies is critical to ensuring the effective operation of network infrastructures. In this paper we present a framework for detecting and apposition performance anomalies based on using an active investigate measurement infrastructure deployed on the periphery of a network. Our framework has three components: an algorithm for detection performance oddball on a path, an algorithm for environs which paths to probe at a given time in order to detect performance anomalies where a path is defined as the set of links between two sampling nodes, and an algorithm for designation the links that are causing an identified anomaly on a path The path selection algorithm is designed to enable a interchange between insure that all links in a network are of times monitored to detect performance anomalies, while minimizing probing overhead.[1] This paper, we develop failure-resilient techniques for monitoring link detain and imbecility in a Service Provider or endeavor IP network. Our two-phased approach attempts to minimize both the monitoring infrastructure costs as well as the additional aggregation due to probe messages. In the first phase, we compute the particular point of a minimal set of monitoring stations such © 2017 IJEDR | Volume 5, Issue 1 | ISSN: 2321-9939 IJEDR1701073 International Journal of Engineering Development and Research (www.ijedr.org) 477 that all network links are covered, even in the bigness of several link reverting. Afterwards, in the second phase, we compute a minimal set of probe messages that are transmitted by the stations to measure link delays and isolate network faults these approximation ratios are provably very close to the best possible bounds for any algorithm. [2]We present a new symbolic execution tool, KLEE, capable of automatically induce tests that wangle high coverage on a diverse set of complex and environmentallyintense programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment position on millions of UNIX systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage on average over 90% per tool. [3] The emergence of Open Flow-capable switches enables exciting new network functionality, at the risk of programming errors that make communication less reliable. The centralized programming model, where a single accountant program manages the network, seems to reduce the likelihood of bugs. However, the system is inherently scattered and asynchronous, with events happening at different switches and end hosts, and inevitable delays affecting communication with the controller. In this paper, we present economic, regular techniques for testing unqualified controller programs. Our NICE tool applies model checking to explore the state space of the entire system the controller, the switches, and the hosts. [4] Network performance tomography, characteristics of the network interior, such as link loss and packet latency, is inferred from unrelated end-to-end measurements. Most work to date is based on employ packet level correlations. However, these methods are often limited in scope-multicast is not widely deployed-or require deployment of additional hardware or software system. Some recent work has been successful in reaching a less detailed goal: identifying the lossiest network links using only unrelated end-toend sampling. In this paper, we abstract the properties of network performance that allow this to be done and exploit them with a quick and simple deduction algorithm that, with high likely, separate the worst performing links. [5] III. SYSTEM ANALAYS Automatic Test Packet Generation (ATPG) framework that self-loading generates a minimal set of collection to test the liveness of the underlying topology and the congruence between data plane state and redundancy description. The tool can also automatically generate packets to test performance assertions such as packet latency. It can also be specialized to generate a minimal set of packets that merely test every link for network liveness.  A survey of network operators revealing common failures and root causes.  A test packet generation algorithm.  A revoke localization algorithm to isolate faulty devices and rules.  ATPG use cases for functional and performance testing.  Evaluation of a prototype ATPG system using rule sets collected from the Stanford and Internet2 backbones.",,2017.0,,212505389,semantic_scholar
415584ce811fe9ca946336bff433171088d21da7,https://www.semanticscholar.org/paper/415584ce811fe9ca946336bff433171088d21da7,Measurement-Driven Algorithm and System Design for Wireless and Datacenter Networks,"Measurement-Driven Algorithm and System Design for Wireless and Datacenter Networks Varun Gupta The growing number of mobile devices and data-intensive applications pose unique challenges for wireless access networks as well as datacenter networks that enable modern cloudbased services. With the enormous increase in volume and complexity of traffic from applications such as video streaming and cloud computing, the interconnection networks have become a major performance bottleneck. In this thesis, we study algorithms and architectures spanning several layers of the networking protocol stack that enable and accelerate novel applications and that are easily deployable and scalable. The design of these algorithms and architectures is motivated by measurements and observations in real world or experimental testbeds. In the first part of this thesis, we address the challenge of wireless content delivery in crowded areas. We present the AMuSe system, whose objective is to enable scalable and adaptive WiFi multicast. AMuSe is based on accurate receiver feedback and incurs a small control overhead. This feedback information can be used by the multicast sender to optimize multicast service quality, e.g., by dynamically adjusting transmission bitrate. Specifically, we develop an algorithm for dynamic selection of a subset of the multicast receivers as feedback nodes which periodically send information about the channel quality to the multicast sender. Further, we describe the Multicast Dynamic Rate Adaptation (MuDRA) algorithm that utilizes AMuSe’s feedback to optimally tune the physical layer multicast rate. MuDRA balances fast adaptation to channel conditions and stability, which is essential for multimedia applications. We implemented the AMuSe system on the ORBIT testbed and evaluated its performance in large groups with approximately 200 WiFi nodes. Our extensive experiments demonstrate that AMuSe can provide accurate feedback in a dense multicast environment. It outperforms several alternatives even in the case of external interference and changing network conditions. Further, our experimental evaluation of MuDRA on the ORBIT testbed shows that MuDRA outperforms other schemes and supports high throughput multicast flows to hundreds of nodes while meeting quality requirements. As an example application, MuDRA can support multiple high quality video streams, where 90% of the nodes report excellent or very good video quality. Next, we specifically focus on ensuring high Quality of Experience (QoE) for video streaming over WiFi multicast. We formulate the problem of joint adaptation of multicast transmission rate and video rate for ensuring high video QoE as a utility maximization problem and propose an online control algorithm called DYVR which is based on Lyapunov optimization techniques. We evaluated the performance of DYVR through analysis, simulations, and experiments using a testbed composed of Android devices and off the shelf APs. Our evaluation shows that DYVR can ensure high video rates while guaranteeing a low but acceptable number of segment losses, buffer underflows, and video rate switches. We leverage the lessons learnt from AMuSe for WiFi to address the performance issues with LTE evolved Multimedia Broadcast/Multicast Service (eMBMS). We present the Dynamic Monitoring (DyMo) system which provides low-overhead and real-time feedback about eMBMS performance. DyMo employs eMBMS for broadcasting instructions which indicate the reporting rates as a function of the observed Quality of Service (QoS) for each UE. This simple feedback mechanism collects very limited QoS reports which can be used for network optimization. We evaluated the performance of DyMo analytically and via simulations. DyMo infers the optimal eMBMS settings with extremely low overhead, while meeting strict QoS requirements under different UE mobility patterns and presence of network component failures. In the second part of the thesis, we study datacenter networks which are key enablers of the end-user applications such as video streaming and storage. Datacenter applications such as distributed file systems, one-to-many virtual machine migrations, and large-scale data processing involve bulk multicast flows. We propose a hardware and software system for enabling physical layer optical multicast in datacenter networks using passive optical splitters. We built a prototype and developed a simulation environment to evaluate the performance of the system for bulk multicasting. Our evaluation shows that the optical multicast architecture can achieve higher throughput and lower latency than IP multicast and peer-to-peer multicast schemes with lower switching energy consumption. Finally, we study the problem of congestion control in datacenter networks. Quantized Congestion Control (QCN), a switch-supported standard, utilizes direct multi-bit feedback from the network for hardware rate limiting. Although QCN has been shown to be fastreacting and effective, being a Layer-2 technology limits its adoption in IP-routed Layer 3 datacenters. We address several design challenges to overcome QCN feedback’s Layer2 limitation and use it to design window-based congestion control (QCN-CC) and load balancing (QCN-LB) schemes. Our extensive simulations, based on real world workloads, demonstrate the advantages of explicit, multi-bit congestion feedback, especially in a typical environment where intra-datacenter traffic with short Round Trip Times (RTT: tens of μs) run in conjunction with web-facing traffic with long RTTs (tens of milliseconds).",,2017.0,10.7916/D8GB2GH3,57640336,semantic_scholar
5f0a7e99c821847a58157a1c635371f5b5824010,https://www.semanticscholar.org/paper/5f0a7e99c821847a58157a1c635371f5b5824010,fAARS: A Platform for Location-Aware Trans-reality Games,"Users today can easily and intuitively record their real-world experiences through mobile devices, and commodity virtual worlds enable users from around the world to socialize in the context of realistic environments where they simulate real-world activities. This synergy of technological advances makes the design and implementation of trans-reality games, blending the boundaries of the real and virtual worlds, a compelling software-engineering problem. In this paper, we describe fAARS, a platform for developing and deploying trans-reality games that cut across the real and parallel virtual worlds, offering users a range of game-play modalities. We place fAARS in the context of recent related work, and we demonstrate its capabilities by discussing two different games developed on it, one with three different variants.",ICEC,2012.0,10.1007/978-3-642-33542-6_16,12449239,semantic_scholar
46d8cfaaa23ab333cf43849b0ce9bb58ee87bcd7,https://www.semanticscholar.org/paper/46d8cfaaa23ab333cf43849b0ce9bb58ee87bcd7,Slow wireless communication testbed based on software-defined radio,"The Internet of Things (IoT) extends the virtual cyber world into the real physical world by networking everyday smart physical objects, representing an upgraded version of Internet. The wireless sensor networks (WSN) are playing diverse sensing functions and feeding information from the physical world for IoT. Nowadays, most of the WSNs are deployed to detect slow-changing physical quantities and data from sensor nodes in these WSNs vary very slowly over a long time interval. Accordingly, a low data transmission rate is sufficient for the sensor nodes. Moreover, the low data transmission rate also enables very narrowband (VNB) radio communication with a bandwidth of several kHz to be applied in wireless sensor nodes. 
It is noteworthy that most of wireless sensor nodes are transmitting data in the unlicensed 2.4 GHz frequency band where the dominant interference is characterized by its wideband nature. Therefore, if the very narrowband (VNB) radio is adopted in a wireless sensor node, only a small portion of co-channel wideband interference will overlap with the VNB signal transmitted by the wireless sensor node. Because only a little of the wideband interference is superimposed onto the VNB signal, the VNB signal captured by a receiver has a relatively high signal-to-noise ratio (SNR), making it possible to reduce the power of the transmitter or release the noise of the receiver. 
Power consumption is a key factor that determines the lifetime of a sensor node because most of sensor nodes are powered by batteries. Once a battery is depleted, the lifetime of a sensor node will expire. The radio transceiver on a wireless sensor node consumes a lot of power when it is working but the VNB signal enables the radio transceiver to decrease its transmission power while guaranteeing arbitrarily low bit error ratio (BER). Thus, low power consumption is made possible at the VNB radio transceiver. 
In this research project, the VNB radio transceiver applied in a wireless sensor node is called “slow wireless radio”. The slow wireless radio aims at the wireless sensor nodes that are transmitting data at very-low average bit-rate of 100 bits/s in the 2.4 GHz frequency band full of wideband interference while achieving low power consumption. 
The goal of this project is to build up a point-to-point slow wireless radio communication testbed based on software-defined radio (SDR), where successful VNB wireless communication will be implemented and related communication performances such as signal-to-noise ratio (SNR) and bit error ratio (BER) will be measured in real-time. The testbed will serve as a design reference to investigate the feasibility of the slow-wireless radio communication when it is used in wireless sensor nodes and facilitate the development of a slow-wireless sensor node prototype.",,2017.0,,55730934,semantic_scholar
16af2f3a6d1f07c46bd851aa2899731136fab73e,https://www.semanticscholar.org/paper/16af2f3a6d1f07c46bd851aa2899731136fab73e,Poster: Ziria: language for rapid prototyping of wireless PHY,"Software-defined radio (SDR) brings the flexibility of software to the domain of wireless protocol design, promising an ideal platform both for research and innovation and rapid deployment of new protocols on existing hardware. However, existing SDR programming platforms require either careful hand-tuning of low-level code, negating many of the advantages of software, or are too slow to be useful in the real world. We present Ziria, the first software-defined radio programming platform that is both easily programmable and performant. Ziria introduces a novel programming model tailored to wireless physical layer tasks and captures the inherent and important distinction between data and control paths in this domain. Ziria provides the capability of implementing a real-time WiFi PHY running at 20 MHz.",MobiCom,2014.0,10.1145/2639108.2642893,52007676,semantic_scholar
88aa14a159f0fad0a2b07445c3f091558ffbda62,https://www.semanticscholar.org/paper/88aa14a159f0fad0a2b07445c3f091558ffbda62,Ziria: wireless programming for hardware dummies,"Software-defined radio (SDR) brings the flexibility of software to the domain of wireless protocol design, promising both an ideal platform for research and innovation and the rapid deployment of new protocols on existing hardware. Most existing SDR platforms require careful hand-tuning of low-level code to be useful in the real world. In this talk I will describe Ziria, an SDR platform that is both easily programmable and performant. Ziria introduces a programming model that builds on ideas from functional programming and that is tailored to wireless physical layer tasks. The model captures the inherent and important distinction between data and control paths in this domain. I will describe the programming model, give an overview of the execution model, compiler optimizations, and current work. We have used Ziria to produce an implementation of 802.11a/g and a partial implementation of LTE.",FHPC '14,2014.0,10.1145/2636228.2661115,13121910,semantic_scholar
aa8966777cd1d9b06671c891161f7696e939333a,https://www.semanticscholar.org/paper/aa8966777cd1d9b06671c891161f7696e939333a,Power and Electromagnetic Analysis for Template Attacks,"Elliptic curve cryptography (ECC) is prone to physical attacks exploring side-channel leakages from power consumption or electromagnetic emanations. When performing a side-channel evaluation of ECC implementations one can choose among different side-channels and analysis methods that sometimes implies more accurate results. Typically, electromagnetic analysis performs better when pattern matching algorithms are deployed, while power analysis is more precise with Hamming weight models. In this paper, we perform template attacks with power and electromagnetic side-channels, in order to compare the efficiency of those methods in real-world software implementations. 
More precisely, we perform Online Template Attack, an efficient attack technique applied to regular scalar multiplication algorithms. To retrieve the secret scalar during a scalar-multiplication with Online Template Attack, it is sufficient to acquire one trace per key bit. In order to compare power and electromagnetic analysis, we use the double-and-add-always algorithm on a twisted Edwards curve running on a smart card with an ATmega163 CPU.",,2015.0,,236061208,semantic_scholar
d68bd3b7f44531e596713c918bd97398b2f6fe3b,https://www.semanticscholar.org/paper/d68bd3b7f44531e596713c918bd97398b2f6fe3b,A Distributed Intelligent Sensing Approach for Environmental Monitoring Applications,"Scientific reports from around the world present us with the undeniable fact that the global ecosystem is undergoing severe change. As this shift accelerates, it is ever more critical that we are able to quantify the local effects of such changes, and further, their implications, from our daily life to the biological processes that put food on our tables. In this thesis, we study the application of sensor network technology to the observation and estimation of highly local phenomena---specifically at a scale between ten to several hundred square meters. Embedding knowledge about the observed process directly into the sensor nodes' behavior via dedicated resource management or control algorithms allows us to deploy dense networks with low power requirements. Ecological systems are notoriously complex. In our work we must thus be highly experimental; it is our highest goal that we construct an approach to environmental monitoring that is not only realistic, but practical for real-world use. Our approach is centered on a commercially available sensor network product, aided by an off-the-shelf quadrotor with minimal customization. We validate our approach through a series of experiments performed from simulation all the way to reality, in deployments lasting days to several months. We motivate the need for local data via two case studies examining physical phenomena. First, employing novel modalities, we study the eclosion of a common agricultural pest. We present our efforts to acquire data that is more local than commonly employed methods, culminating in a six month deployment in a Swiss apple orchard. Next, we apply a environmental fluid dynamics model to enable the estimation of sensible heat flux using an inexpensive sensor. We integrate the sensor with a wireless sensor network and validate its capabilities in a short-term deployment. Acquiring meaningful data on a local scale requires that we advance the state of the art in multiple aspects. Static sensor networks present a classical tension between resolution, autonomy, and accuracy. We explore the performance of algorithms aimed at providing all three, showing explicitly what is required to implement these approaches for real-world applications in an autonomous deployment under uncontrolled conditions. Eventually, spatial resolution is limited by network density. Such limits may be overcome by the use of mobile sensors. We explore the use of an off-the-shelf quadrotor, equipped with environmental sensors, as an additional element in system of heterogeneous sensing nodes. Through a series of indoor and outdoor experiments, we quantify the contribution of a such a mobile sensor, and various strategies for planning its path.",,2015.0,10.5075/epfl-thesis-6673,109901930,semantic_scholar
1bd64a7541e8d34f0d7782f0e786b2ffd78e55b7,https://www.semanticscholar.org/paper/1bd64a7541e8d34f0d7782f0e786b2ffd78e55b7,Interacting in various application domains,"eLearning and Education.- Arab Children's Reading Preference for Different Online Fonts.- Adaptation Decisions and Profiles Exchange among Open Learning Management Systems Based on Agent Negotiations and Machine Learning Techniques.- Accessing e-Learning Systems via Screen Reader: An Example.- Using Tablet PCs and Pen-Based Technologies to Support Engineering Education.- Optimal Affective Conditions for Subconscious Learning in a 3D Intelligent Tutoring System.- Computer-Based Learning to Improve Breast Cancer Detection Skills.- Virtual Classroom and Communicability: Empathy and Interaction for All.- Communicability for Virtual Learning: Evaluation.- Attention and Motivation in Hypermedia Systems.- A Web-Based, Interactive Annotation Editor for the eCampus Development Environment for SCORM Compliant E-Learning Modules.- An Innovative Way of Understanding Learning Processes: Eye Tracking.- A Set of Rules and Strategies for UNSAM Virtual Campus.- HCI Professional Involvement in k-12 Education: On Target or Missing the Mark?.- A Language Learning System Utilizing RFID Technology for Total Physical Response Activities.- Promoting Metacognition in Immersive Cultural Learning Environments.- The Application of the Flexilevel Approach for the Assessment of Computer Science Undergraduates.- Development of Ubiquitous On-Demand Study Support Environment for Nursing Students.- The Effects of Prior Knowledge on the Use of Adaptive Hypermedia Learning Systems.- Supporting Learners in Adaptive Learning Environments through the Enhancement of the Student Model.- The Concept of IMPRESSION: An Interactive Instruction System and Its Practice for Real-Time Distance Lessons between U.S. and Japan.- Improving Children's Writing Ability.- From Paper to Module - An Integrated Environment for Generating SCORM Compliant Moodle Courses Out of Text and Multimedia Elements.- Development of a Simulator of Abacus: Ancient Analog Calculator on a Mobile Phone as a Teaching Material.- A Proposal for a Framework for an e-Alumni Program Using SNS.- Supporting End-User Development of Personalized Mobile Learning Tools.- Didactic Models as Design Representations.- Interactive Learning Panels.- WebELS: A Content-Centered E-Learning Platform for Postgraduate Education in Engineering.- A Pen-Based Teaching System for Children and Its Usability Evaluation.- Development of a Visualised Sound Simulation Environment: An e-Approach to a Constructivist Way of Learning.- Games and Entertainment.- Causal Links of Presence.- Games Design Principles for Improving Social Web Applications.- A Multiple-Level 3D-LEGO Game in Augmented Reality for Improving Spatial Ability.- An Online Survey System on Computer Game Enjoyment and Personality.- Playability Testing of Web-Based Sport Games with Older Children and Teenagers.- Exploring the Elements and Design Criteria of Massively-Multiplayer Online Role-Playing Game (MMORPG) Interfaces.- Healthcare Game Design: Behavioral Modeling of Serious Gaming Design for Children with Chronic Diseases.- Analyzing Human Behaviors in an Interactive Art Installation.- The Effects of Quest Types and Gaming Motivations on Players' Knowledge Acquisitions in an Online Role-Playing Game Environment.- Self-movement Feeling Generation in Sports Watching with Screen Movement via Pan-Tilt Steerable Projector.- Design of Interactive Emotional Sound Edutainment System.- Understanding Online Game Addiction: Connection between Presence and Flow.- The Experience of Presence in 3D Web Environment: An Analysis of Korean Second Life.- Influence of Real-World Ten-Pin Bowling Experience on Performance during First-Time Nintendo Wii Bowling Practice.- Emotionally Adapted Games - An Example of a First Person Shooter.- DiamondTheater: A System for Reproducing Theater and Supporting Creative Activities.- Work, Collaboration and Business.- New Health Information Systems (HIS) Quality-in-Use Model Based on the GQM Approach and HCI Principles.- An Information Visualization Approach to Hospital Shifts Scheduling.- Designed to Fit: Challenges of Interaction Design for Clothes Fitting Room Technologies.- Usability for Poll Workers: A Voting System Usability Test Protocol.- CAD and Communicability: A System That Improves the Human-Computer Interaction.- A Novel Visualization Tool for Evaluating Medication Side-Effects in Multi-drug Regimens.- Design of a Web Intervention to Change Youth Smoking Habits.- Smart Makeup Mirror: Computer-Augmented Mirror to Aid Makeup Application.- Studying Reactive, Risky, Complex, Long-Spanning, and Collaborative Work: The Case of IT Service Delivery.- Human Computer Interaction in Virtual Standardized Patient Systems.- Towards Standardized Pen-Based Annotation of Breast Cancer Findings.- ImproV: A System for Improvisational Construction of Video Processing Flow.- E-Assessment: A Suitable Alternative for Measuring Competences?.- Green Advocate in E-Commerce.- Gesture-Based Sharing of Documents in Face-to-Face Meetings.- Developing, Deploying and Assessing Usage of a Movie Archive System among Students of Film Studies.- Using Activity Descriptions to Generate User Interfaces for ERP Software.- Developing a Nomenclature for EMR Errors.- Mapping for Multi-source Visualization: Scientific Information Retrieval Service (SIRS).- Client-Side Visualization of Internet Forums for Information Retrieval.- Social-Technical Tools for Collaborative Sensemaking and Sketching.- Developing Some User Interfaces of TV under Enormous Channels Environment.- Electronic Glassboard - Conception and Implementation of an Interactive Tele-presence Application.- A New Automatic Teller Machine (ATM) Proposal through the Analysis of ATMs of Three Banks.- Advanced Applications.- Designing Usable Bio-information Architectures.- Run-Time Adaptation of a Universal User Interface for Ambient Intelligent Production Environments.- Heuristic Evaluation of Mission-Critical Software Using a Large Team.- Interface Development for Early Notification Warning System: Full Windshield Head-Up Display Case Study.- Reflections on the Interdisciplinary Collaborative Design of Mapping the Universe.- Distilling Support Opportunities to Improve Urban Search and Rescue Missions.- A New Approach to Design an Interactive System for Molecular Analysis.- The Differences of Aviation Human Factors between Individualism and Collectivism Culture.- Web-Based Training System for Improving Aviation Maintenance Performance.- Allocating Human-System Interfaces Functions by Levels of Automation in an Advanced Control Room.- Development of an Expert System as a User Interface for an RFID Application.- Developing a Validation Methodology for Educational Driving Simulators and a Case Study.- Developing a Usable Mobile Flight Case Learning System in Air Traffic Control Miscommunications.",,2009.0,,107222405,semantic_scholar
949b01c64ba61c94ba0982ffd6abc50658d53874,https://www.semanticscholar.org/paper/949b01c64ba61c94ba0982ffd6abc50658d53874,"Demo: 802.11 a/g PHY implementation in ziria, domain-specific language for wireless programming","Software-defined radio (SDR) brings the flexibility of software to the domain of wireless protocol design, promising an ideal platform both for research and innovation and the rapid deployment of new protocols on existing hardware. However, existing SDR programming platforms require either careful hand-tuning of low-level code, negating many of the advantages of software, or are too slow to be useful in the real world. In this demo we present Ziria, the first software-defined radio programming platform that is both easily programmable and performant. Ziria introduces a novel programming model tailored to wireless physical layer tasks and captures the inherent and important distinction between data and control paths in this domain. We show the capabilities of Ziria by demonstrating a real-time implementation of WiFi PHY running at 20 MHz.",SRIF@SIGCOMM,2014.0,10.1145/2627788.2627799,8185112,semantic_scholar
91bfa56b000e77ba68738c0366436918462ebe38,https://www.semanticscholar.org/paper/91bfa56b000e77ba68738c0366436918462ebe38,"ACS: Specifying ""Smart"" Applications Using Sense-Process-Consume Flows","Smart applications enable pervasive and informed interactions between the physical and digital worlds. These applications are deployed on resource constrained wireless sensor networks and are commonly implemented using software modularization schemes such as components and services. Composing smart applications at the abstraction level offered by embedded software modularization schemes is complex and time consuming. The complexity of composing them is derived from the need to understand many low-level issues, e.g. embedded programming languages, coordination mechanisms, software tool chains. We present an application composition service that reduces composition effort by offering a declarative specification of sense, process and consume flows. We demonstrate reduction in composition effort for three real-world smart applications deployed on a smart office environment.",2013 IEEE 12th International Symposium on Network Computing and Applications,2013.0,10.1109/NCA.2013.28,16895477,semantic_scholar
824afcd7abb4e89ddbc7d547d434cdfe47616c8b,https://www.semanticscholar.org/paper/824afcd7abb4e89ddbc7d547d434cdfe47616c8b,Modeling and Simulation of Radio Signals Attenuation Using Informed Virtual Geographic Environments (IVGE),"A radio communication system is a complex dynamic phenomenon where transmitter and receiver antennas are constantly constrained by the physical environment in which they are deployed. In the real world, radio transmissions are subject to propagation effects which deeply affect the received signals because of geographic and environmental characteristics (foliage and vegetation, buildings, mountains and hills, etc.). Multi-Agent Geo-Simulation aims to simulate such phenomena involving a large number of autonomous situated actors (implemented as software agents) evolving and interacting within a representation of the physical environment. Using a geo-computation approach, we propose to use an Informed Virtual Geographic Environment (IVGE) along with MAGS paradigm. In addition, we propose a multi-agent prototype to analyze the attenuation effect due to the radio signal’s traversal between antennas (simulated as software agents) through terrain shape, vegetation area, and buildings using a 3D line-of-sight computation technique. Keywords-Line of Sight; Excess attenuation; Vegetation and Foliage; Radio Propagation; Informed Virtual Geographic Environments",,2013.0,,229379178,semantic_scholar
74fab23e3fd31db77d22074ede9de7e8c8a40c38,https://www.semanticscholar.org/paper/74fab23e3fd31db77d22074ede9de7e8c8a40c38,Improving Vehicular Networking Reliability and Efficiency in the Context of Platooning Applications,"Vehicular networking is a technology that enables vehicles communication system. A joint effort from the automobile industry, transportation industry, and government offices is driving the adoption of this technology to build intelligent transportation systems that consist of smart vehicles. This study attempts to improve the reliability and efficiency of vehicular networking. The study assumes the context of platooning applications, but the contributions of this study can be applied to other vehicular applications as well. There are two contributions in this study. First, a wireless emulator is designed and implemented to emulate IEEE 802.11 networks in real-time using the Ethernet infrastructure. The emulator replaces the MAC layer and physical layer of IEEE 802.11 networking stack with a real-time CSMA/CA model, thus reduces the cost of experiments. It provides upper layers the same interfaces as on a real device. As a result, the testing targets in the emulation are real-world software components as opposed to simulation scripts in a discrete event simulator. These software components can be routing protocols, transport protocols, or applications, and are the same code that can be deployed in the real world. Second, an Interframe Compression Transmission Layer is designed and implemented, to provide efficient transmission of periodical messages in vehicular environments. The transmission layer compresses the difference between frames instead of frames themselves, and reduces bandwidth consumption significantly. To improve the behaviors of the transmission layer under different scenarios and configurations studied, an adaptive version of the algorithm is designed, which achieves more than 50% in reduction of bandwidth consumption using real-world platooning data trace. With lower bandwidth consumption, delivery ratio is vastly improved in congested networking environments.",,2016.0,,62827960,semantic_scholar
6220b68cd721512098b9b14851afe5e660c0e565,https://www.semanticscholar.org/paper/6220b68cd721512098b9b14851afe5e660c0e565,Crypto-day Campeon A8,"Using the properties of a wireless channel is an alternative approach for securing the channel besides pre-shared keys or asymmetric cryptography. Numerous experiments have recently demonstrated that channel-based key establishment (CBKE) is a promising alternative to well-known symmetric/asymmetric approaches. Their run-times for establishing a symmetric key suggest that such methods are highly suitable for real-world applications that operate in a dynamic mobile environment with peer-to-peer association. CBKE is a new paradigm for generating shared secret keys. The approach is based on the estimation of the wireless transmission channel by both the sender and receiver, where the shared secret key is derived from channel parameters. The commonness of the randomness of the secret key relies on the principle of channel reciprocity. Specifically, this means that the channel from Alice to Bob is the same than the channel from Bob to Alice. This symmetry of practical channels is usually sufficiently high, as well as its entropy of spatial, temporal, and spectral characteristics. Security is given if an attacker’s distance to the two communicating nodes is high enough, so that the observed channel parameters to each node are uncorrelated and independent from each other. Typically, in real environments this is given if the distance is greater than about half of the carrier wavelength. For instance, for the frequency used in 2.4 GHz WiFi, this translates to a distance of 6.25 cm. So far, high usability and dynamic key management are very difficult to achieve for wireless devices, which operate under strict resource constraints. CBKE has the potential to significantly reduce the cost of securing small embedded devices, and hence make mass production and deployment more viable. Until now, no research has addressed the requirements for performance evaluation of real-world implementations of CBKE systems. We present a wireless CBKE security system built with standard components, e.g., quantization scheme and error correction codes, presented in recent publications. We introduce necessary implementation properties and requirements of CBKE systems. In order to validate the performance of the key generation algorithms, we define a set of metrics. Finally, we describe an end-to-end implementation on an ARM-Cortex M3 microcontroller to demonstrate the practical feasibility of channel-based key estimation using current embedded hardware. Comparative analysis of pseudorandom generators Aleksei Burlakov, Johannes vom Dorp, Joachim von zur Gathen, Sarah Hillmann, Michael Link, Daniel Loebenberger, Jan Lühr, Simon Schneider & Sven Zemanek {burlakov,dorp,luehr,schneid,zemanek}@cs.uni-bonn.de {sarah.hillmann,michael.link}@uni-bonn.de {gathen,daniel}@bit.uni-bonn.de Bonn-Aachen International Center for Information Technology Dahlmannstr. 2, Bonn We compare random generators (RGs) under controlled conditions regarding their efficiency and statistical properties. For this purpose, we distinguish between physical RGs and software RGs, which can be further subdivided into cryptographically secure and insecure RGs. Physical RGs covered by our study are the hardware generator PRG310-4 and /dev/random as implemented in the Linux kernel. Since /dev/random is fed by system events, we analyze both an idle lab environment and a server hosting several virtual machines. As examples for cryptographically secure RGs our analysis compares the RSA generator and the Blum-Blum-Shub generator, both for 3000-bit moduli. Additionally, we compare them to the Nisan-Wigderson construction with suitably selected parameters. We include two cryptographically insecure RGs, namely a linear congruential generator (LCG) and the Littlewood generator. In order to obtain repeatable and comparable results, our implementations of the software RGs were all run on the same machine and produced 512 kB of output each, using AES post-processed output of the generator PRG310-4 as source for random seed bits. We compare the results in terms of byte entropy and throughput excluding initialization. For further statistical analysis — not shown in the table — we apply the NIST test suite on the outputs. The most important finding is that in our scenarios, number-theoretic generators compete very well against hardware-based ones. byte entropy runtime throughput throughput [bit] [μs] [kB/s] normalized PRG310-4, no post-processing 7.99963 16308400 31.39486 4.34492 AES post-processing 7.99963 36524300 14.01806 1.94004 /dev/random, in the field 7.99979 9.169× 10 5.584× 10−3 7.728× 10−4 in the lab 7.99948 2.671× 10 1.917× 10−4 2.653× 10−5 Littlewood 6.47244 15206550 33.66970 4.61011 Linear congruential generator 7.99969 2644039 193.64313 26.51392 Blum-Blum-Shub 7.99962 17708350 28.91291 3.95880 RSA, e = 2 + 1, 1400 bit/round 7.99966 267604 1913.27484 261.96857 e = 3, 1 bit/round 7.99963 70103838 7.30345 1 Nisan-Wigderson 7.99961 2731227 187.46153 25.66753 Table 1: Overview of the results for generating 512 kB of output.",,2015.0,,12063917,semantic_scholar
72656d9df791499f6bb246f599e462228ff528ad,https://www.semanticscholar.org/paper/72656d9df791499f6bb246f599e462228ff528ad,A channel model and coding for vehicle to vehicle communication based on a developed V-SCME,"Over the recent years, VANET communication has attracted a lot of attention due to its potential in facilitating the implementation of 'Intelligent Transport System'. Vehicular applications need to be completely tested before deploying them in the real world. In this context, VANET simulations would be preferred in order to evaluate and validate the proposed model, these simulations are considered inexpensive compared to the real world (hardware) tests. The development of a more realistic simulation environment for VANET is critical in ensuring high performance. Any environment required for simulating VANET, needs to be more realistic and include a precise representation of vehicle movements, as well as passing signals among different vehicles. In order to achieve efficient results that reflect the reality, a high computational power during the simulation is needed which consumes a lot of time. The existing simulation tools could not simulate the exact physical conditions of the real world, so results can be viewed as unsatisfactory when compared with real world experiments. This thesis describes two approaches to improve such vehicle to vehicle communication. The first one is based on the development of an already existing approach, the Spatial Channel Model Extended (SCME) for cellular communication which is a verified, validated and well-established communication channel model. The new developed model, is called Vehicular - Spatial Channel Model Extended (V-SCME) and can be utilised for Vehicle to Vehicle communication. V-SCME is a statistical channel model which was specifically developed and configured to satisfy the requirements of the highly dynamic network topology such as vehicle to vehicle communication. V-SCME provides a precise channel coefficients library for vehicle to vehicle communication for use by the research community, so as to reduce the overall simulation time. The second approach is to apply V-BLAST (MIMO) coding which can be implemented with vehicle to vehicle communication and improve its performance over the V-SCME. The V- SCME channel model with V-BLAST coding system was used to improve vehicle to vehicle physical layer performance, which is a novel contribution. Based on analysis and simulations, it was found that the developed channel model V-SCME is a good solution to satisfy the requirements of vehicle to vehicle communication, where it has considered a lot of parameters in order to obtain more realistic results compared with the real world tests. In addition, V-BLAST (MIMO) coding with the V-SCME has shown an improvement in the bit error rate. The obtained results were intensively compared with other types of MIMO coding.",,2016.0,,204088939,semantic_scholar
57f88d973d0def2120b5348f3fa074d851933e99,https://www.semanticscholar.org/paper/57f88d973d0def2120b5348f3fa074d851933e99,Realistic frequency coded chipless RFID: physically modulated tags and refectarray readers,"Recently, the chipless Radio Frequency Identiﬁcation (RFID) technology has attracted tremendous attention in the market of item identiﬁcation where the cost is the main concern. However, up to date the technology is at the conceptual level and suffers from a lot of imitations that hinder the technology deployment. The chipless RFID system comprises three major parts which are the reader circuit, the interrogation antennas, and the chipless tags. The contributions of this dissertation are to overcome the challenges that impede the deployment of the chipless RFID system from the perspective of innovating physically modulated tags and developing the reader antenna system. In particular, the system is considered in three novel aspects. 
The ﬁrst aspect is the linear physically modulated tags where the tag is interrogated by Ultra Wideband (UWB) signal and the tag inscribed metallic resonators are physically modulating the interrogation frequencies. Therefore, the UWB waveform is modulated in the form of resonant notches, and/or peaks that are inherently embedded in the tag backscattered Radar Cross Section (RCS) frequency response. In this regard, four innovative physically modulated tags are developed aiming at enhancing the coding efﬁciency, maximizing the coding capacity, conserving the operating frequency range and preserving the tag size. The ﬁrst tag is based on nested circular ring resonators where each resonator codiﬁes a tag coding notch. Terefore, the tag structure is scalable, printable and compact size. Moreover, a novel encoding methodology is employed to preserve the notch width and position while coding. The second developed tag is a depolarizing one where the polarization isolation between the reader interrogation signal and the tag response is utilized to minimize the environmental clutter reﬂections. Furthermore, the tag is scalable, printable, and compact size in the credit card format. Thirdly, a novel Notch Width Modulation (NWM) tag is introduced where the tag-ID is not only based on the notch position but also on the notch width. Hence, the notch width conﬁgures a further dimension to increase the Degree of Freedom (DoF) for coding and modulation. Therefore, the notch width and position are modulated simultaneously aiming at enhancing the coding efﬁciency and capacity. Lastly, a novel On Off Notch/Peak (OO-N/P) and Notch/Peak-Position (N/P-P) modulation tag is introduced. The tag basic idea is to exploit both the co-polarized and cross polarized backscattered signals from a tag excited with a linear polarized wave. Consequently, the tag signature is encoded into Notch/Peak (N/P) format in two orthogonal planes. Thus, the Co/Cross-polarizing N/P modulation scheme presents a novel criterion for enhancing the coding efﬁciency and capacity of the chipless RFID systems. Moreover, the cross-polarized response enhances the tag detection in a realistic environment. The proposed tags and their associated physical modulation schemes are validated using Electro Magnetic (EM) simulations and real-world testbed measurements. 
 
In the second aspect, the Reﬂectarray (RA) antenna is proposed to be utilized in the reader side aiming at increasing the reading range, minimizing the environmental reﬂections, and acquiring a lot of novel capabilities that can not be provided by the conventional antenna arrays. The spatial feeding RA antenna is easily integrated with the RF circuits, lightweight, conformal geometry, and low cost. Hence, in this concern, three different novel designs are developed. The ﬁrst design utilizes the Log Periodic Array (LPDA) antenna to feed the developed RA surface. This introduced prototype operates at 5.8GHz and achieves 300MHz bandwidth. Moreover, the RA antenna radiation beam is 4 times narrower than the feeder beam and thus 6dB higher in gain with −10dB Side Lobe Level (SLL). The second developed prototype uses a constant phase center horn antenna to feed the RA surface. Thus, an UWB RA antenna enabling multiple bits accommodation is designed. This antenna operates from 4GHz to 6GHz with 15° Half Power Beam Width (HPBW), 19dBi gain, and −10dB SLL. Furthermore, this developed UWB RA antenna is successfully integrated with the physically modulated tags and a reading range of 1m is achieved. To the best of my knowledge, this is the highest reading range achieved in the Frequency Coded (FC) chipless RFID systems, considering real-world indoor environment and software deﬁned radio reader. After that, dual-polarized RA antenna with low cross-polarization level is presented. This RA antenna is proposed to be utilized with the Co/Cross-polarizing tags. Finally, a successful implementation of an electronic beam steering RA antenna is introduced. This novel beam steering RA antenna system enhances the reading robustness and can precisely locate the chipless tags. In this concern, a novel unit cell that is able to electronically control the reﬂected phase at different discrete frequencies utilizing a single varactor diode is proposed. Therefore, a scanning range of ±50° is achieved. Moreover, the steered beams are 4 times narrower than the feeder beam and thus 4 times higher in gain. 
 
In the third aspect, the nonlinear physically modulated tags are proposed. The core functionality relies on interrogating the tag with a prescribed set and format of frequencies in a time regulated technique while the tag replies with its unique ID at other frequencies. Therefore, the nonlinearity is exploited to completely isolate the environmental clutter reﬂections, get rid of the necessary reference calibration measurements, overcome the detuning caused by the tagged item materials, and increase the coverage. These objectives are attained by exploiting the nonlinearity generated from a single unbiased diode integrated with the tag structure. The ﬁrst proposed tag category relies on exploiting the second order nonlinear terms. Therefore, in this regard, three novel tags are introduced. The ﬁrst class is the single tone harmonic radar tags. In this class, the reader scans the available tags by sending speciﬁc fundamental tones. Then, the tag receiving antenna is tuned at only one of these fundamentals which is maximally conveyed to the nonlinear device for generating the corresponding harmonics. Consequently, the tag transmitting antenna is tuned at the second harmonic which is retransmitted back towards 
the reader representing the tag-ID. Thus, the narrower is the band-pass ﬁlter provided by the tag receiving antenna or integrated into it, the more the frequencies that can be utilized for coding. After that, the multi-tone interrogation is proposed to increase the coding capacity. 
Hence, the tag is interrogated with a prescribed set of fundamentals that are swept over the time to avoid the generation of the mixing products in the reader and tag as well. The tag in turn which is completely planar based on the Coplanar Waveguide (CPW) technology implements a Notch Position Modulation (NPM) scheme in the second harmonics of these fundamental tones. Therefore, the notches that are existing in the second harmonic response symbolize the tag-ID. Afterward, the simultaneous multi-tone interrogation is explored. In this concern, a set of distinct frequency pairs are used to interrogate the nonlinear tags. As a consequence, these tones are mixed through the nonlinear device. Consequently, the tag transmitting antenna ﬁgures out only one of these mixed products. The second proposed tag category relies on exploiting the inter-modulation communication principle which exhibits a small frequency span. Therefore, the tag is illuminated by two co-located frequencies and respond at an inter-modulated frequency which is retransmitted by the tag transmitting antenna representing the tag-ID. Finally, the phase encoding capability is proposed. Therefore, not only the existence or the non-existence of a harmonic notch or peak used in coding the tag-ID but also the corresponding relative phase states can be considered. The introduced tags and their associated physical modulation schemes are veriﬁed using harmonic balance analysis, EM simulations and realistic testbed measurements. 
 
Lastly, the unique features which are considered in the dissertation bring a signiﬁcant enhancement to the deployment of the chipless RFID system.",,2017.0,,150161814,semantic_scholar
87aed004b3334c17bcd87b55cff6291594e1347c,https://www.semanticscholar.org/paper/87aed004b3334c17bcd87b55cff6291594e1347c,Development of a smart home simulator for use as a heuristic tool for management of sensor distribution.,"Smart Homes offer potential solutions for various forms of independent living for the elderly. The assistive and protective environment afforded by smart homes offer a safe, relatively inexpensive, dependable and viable alternative to vulnerable inhabitants. Nevertheless, the success of a smart home rests upon the quality of information its decision support system receives and this in turn places great importance on the issue of correct sensor deployment. In this article we present a software tool that has been developed to address the elusive issue of sensor distribution within smart homes. Details of the tool will be presented and it will be shown how it can be used to emulate any real world environment whereby virtual sensor distributions can be rapidly implemented and assessed without the requirement for physical deployment for evaluation. As such, this approach offers the potential of tailoring sensor distributions to the specific needs of a patient in a non-evasive manner. The heuristics based tool presented here has been developed as the first part of a three stage project.",Technology and health care : official journal of the European Society for Engineering and Medicine,2009.0,10.3233/THC-2009-0550,207706483,semantic_scholar
08ae139d6890717bea0e6243549d66caf24fa78e,https://www.semanticscholar.org/paper/08ae139d6890717bea0e6243549d66caf24fa78e,Teaching Embedded Systems in a MOOC Format,"We have designed and implemented a Massive Open Online Class (MOOC) with a substantial lab component within the edX platform. We deployed this MOOC three times with a total enrollment of over 100,000 students. If MOOCs are truly going to transform engineering education, then they must be able to deliver classes with laboratory components. Our offering goes a long way in unraveling the perceived complexities in delivering a laboratory experience to thousands of students from around the globe. We believe the techniques developed in this class will significantly transform the MOOC environment. Effective education requires students to learn by doing. In the traditional academic setting this active learning is achieved through a lab component. Translating this to the online environment is a non-trivial task that required several important factors to come together. First, we have significant support from industrial partners ARM Inc. [1] and Texas Instruments [2]. Second, the massive growth of embedded microcontrollers has made the availability of lost-cost development platforms feasible. Third, we have assembled a team with the passion, patience, and experience of delivering quality lab experiences to large classes. Fourth, online tools now exist that allow students to interact and support each other. We used edX for the delivery of videos, interactive animations, text, and quizzes [3]. We used Piazza [4] for discussion boards and Zyante [5] for a programming reference. We partnered with element-14 [6], Digi-Key [7], and Mouser [8] to make the lab kit available and low-cost. Even though there was a $40-$70 cost to purchase the lab kit, the course completion numbers were slightly better than a typical MOOC. 7.3% of the students completed enough of the class to receive a certificate. Students completing end of the course surveys report a 95% overall satisfaction. Demographics show a world-wide reach with India, US, and Egypt being the countries with the most students. In this paper we will present best practices, successes and limitations of teaching a substantial lab across the globe. Background An embedded system combines mechanical, electrical, and chemical components along with a computer, hidden inside, to serve a single dedicated purpose [9-11]. There are over 50 billion processors based on the ARM architecture delivered into products, and most of these computers are single-chip microcontrollers that are the brains of an embedded system. Embedded systems are a ubiquitous component of our everyday lives. We interact with hundreds of tiny computers every day that are embedded into our houses, our cars, our bridges, our toys, and our work. As our world has become more complex, so have the capabilities of the microcontrollers embedded into our devices. Therefore the world needs a trained workforce to develop and manage products based on embedded microcontrollers. Review Other online classes have delivered laboratory experiences. Hesselink at Stanford University developed iLabs as a means to deliver science experiments to online learning. Their lab-in-a-box involves simulations and animations [12]. O’Malley et al. from the University of Manchester developed a Chemistry MOOC with a lab component using virtual labs and simulations [13-14]. University of Washington presented a hardware/software MOOC on Coursera [15]. This course is primarily a programming class without graded physical labs. Ferri et al. from Georgia Institute of Technology created a MOOC for linear circuits [16]. This class had activities to perform with NI’s myDAC, but graded lab circuits were not part of the online experience. Connor, and Huettel at Duke created a Virtual Community of Practice for electric circuits [17]. Cherner et al. created a virtual multifunctional X-Ray diffractometer for teaching science and engineering [18]. Saterbak et al. at Rice University developed online materials to teach freshman design, with the goal to free-up class time for more interactive learning experiences [19]. Harris from University of California at Irvine has a six-course sequence on Introduction to the Internet of Things and Embedded Systems where students build actual embedded devices [20]. Grading for this course uses peer assessment. Lee et al. at Berkeley developed an introduction to embedded systems MOOC with laboratory exercises. The lab itself was a robotic controller in a virtual laboratory environment. Completion of the labs themselves does have an automatic grading component based on the student’s written software [21-22]. All this work emphasizes the need for hands on learning. Pedagogy The overall educational objective of this class is to allow students to discover how computers interact with the environment. The class provides hands-on experiences of how an embedded system could be used to solve problems. The focus of this introductory course is understanding and analysis rather than design, where students learn new techniques by doing them. We feel we have solved the dilemma in learning a laboratory-based topic like embedded systems, where there is a tremendous volume of details that first must be learned before hardware and software systems can be designed. The approach taken in this course is to learn by doing in a bottom-up fashion. One of the advantages of a bottom-up approach to learning is that the student begins by mastering simple concepts. Once the student truly understands simple concepts, he or she can embark on the creative process of design, which involves putting the pieces together to create a more complex system. True creativity involves solving complex problems using effective combinations of simple components. Embedded systems afford an effective platform to teach new engineers how to program for three reasons. First, there is no operating system. Thus, in a bottom-up fashion the student can see, write, and understand all software running on a system that actually does something. Second, embedded systems involve real input/output that is easy for the student to touch, hear, and see. Many engineering students struggle with abstraction. We believe many students learn effectively by using their sense of touch, hearing and sight to first understand and internalize difficult concepts, and then they will be able to develop and appreciate abstractions. Third, embedded systems are employed in many everyday products, motivating students to see firsthand, how engineering processes can be applied in the real world. This course is intended for beginning college students with some knowledge of electricity as would have been taught in an introductory college physics class. Secondly, it is expected students will have some basic knowledge of programming and logic design. No specific language will be assumed as prior knowledge but this class could be taken as their second programming class. We hoped experienced engineers could also use this course to train or retrain in the field of embedded systems. Learning objectives of the course Although the students are engaged with a fun and rewarding lab experience, our educational pedagogy is centered on fundamental learning objectives. After the successful conclusion of this class, students should be able to understand the basic components of a computer, write C language programs that perform input/output interfacing, implement simple data structures, manipulate numbers in multiple formats, and understand how software uses global memory to store permanent information and the stack to store temporary information. Our goal is for students to learn these concepts: 0) How the computer stores and manipulates data; 1) Embedded systems using modular design and abstraction; 2) Design tools like requirements documents, data flow graphs, and call graphs; 3) C programming: considering both function and style; 4) Debugging and verification using a simulator and the real microcontroller; 5) Debugging tools like voltmeters, oscilloscopes, and logic analyzers; 6) How to input/output using switches, LEDs, DACs, ADCs, and serial ports; 7) Implementation of an I/O driver, multithreaded programming, and interrupts; 8) Analog to digital conversion (ADC), periodic sampling, and the Nyquist Theorem; 9) Stepper motors, brushed DC motors, and simple digital controllers; 10) Digital to analog conversion (DAC), used to make simple sounds; 11) Simple distributed systems that connect two microcontrollers; 12) Internet of things, connecting the embedded system to the internet; 13) System-level design that combine multiple components together. Laboratory Kit Active learning requires a platform for the student to learn by doing. Figure 1 shows the components of the basic lab kit. There are two difficulties with a physical lab kit deployed in a world-wide open classroom environment. The first problem is availability of components. We partnered with companies and distributors six months in advance of the course launch to guarantee availability. The companies wanted us to specify the number of students who would buy the kit. In this regard, we were very lucky. Six months prior to our first launch, we estimated 2000 people would register for the class and 1000 would buy the kit. In turns out Texas Instruments produced 10,000 microcontroller boards just in case. Much to our surprise 40,000 people registered and we estimate 11,000 purchased the kit during this first delivery of the course. The second solution to the problem of availability was to have three world-wide distributors (element-14, Mouser, and Digi-Key). Working with these distributors, we created one-click landing pages for students to buy the kit. Furthermore, for each component in the kit (other than the microcontroller board), we had three or more possible parts. The third solution was to design the course with flexible deadlines and pathways. Each lab had a simulation and a real-board requirement. Students who were waiting for the parts to be shipped could proceed with ",,2016.0,10.18260/p.26025,59020150,semantic_scholar
a738fa6670ebcfa1f3348f04cf6722f4d8bbc049,https://www.semanticscholar.org/paper/a738fa6670ebcfa1f3348f04cf6722f4d8bbc049,Physical Layer Cooperation,"Information theory has long pointed to the promise of physical layer cooperation in boosting the spectral efficiency of wireless networks. Yet, the optimum relaying strategy to achieve the network capacity has till date remained elusive. Recently however, a relaying strategy termed Quantize-Map-and-Forward (QMF) was proved to achieve the capacity of arbitrary wireless networks within a bounded additive gap. This thesis contributes to the design, analysis and implementation of QMF relaying by optimizing its performance for small relay networks, proposing low-complexity iteratively decodable codes, and carrying out over-the-air experiments using software-radio testbeds to assess real-world potential and competitiveness. The original QMF scheme has each relay performing the same operation, agnostic to the network topology and the channel state information (CSI); this facilitates the analysis for arbitrary networks, yet comes at a performance penalty for small networks and medium SNR regimes. In this thesis, we demonstrate the benefits one can gain for QMF if we optimize its performance by leveraging topological and channel state information. We show that for the N-relay diamond network, by taking into account topological information, we can exponentially reduce the QMF additive approximation gap from $\Theta(N)$ bits/s/Hz to $\Theta(\log N)$ bits/s/Hz, while for the one-relay and two-relay networks, use of topological information and CSI can help to gain as much as $6$ dB. Moreover, we explore what benefits we can realize if we jointly optimize QMF and half-duplex scheduling, as well as if we employ hybrid schemes that combine QMF and Decode-and-Forward (DF) relay operations. To take QMF from being a purely information-theoretic idea to an implementable strategy, we derive a structure employing Low-Density-Parity-Check (LDPC) ensembles for the relay node operations and message-passing algorithms for decoding. We demonstrate through extensive simulation results over the full-duplex diamond network, that our designs offer a robust performance over fading channels and achieves the full diversity order of our network at moderate SNRs. Next, we explore the potential real-world impact of QMF and present the design and experimental evaluation of a wireless system that exploits relaying in the context of WiFi. We deploy three main competing strategies that have been proposed for relaying, Amplify-and-Forward (AF), DF and QMF, on the WarpLab software radio platform. We present experimental results--to the best of our knowledge, the first ones--that compare QMF, AF and DF in a realistic indoor setting. We find that QMF is a competitive scheme to the other two, offering in some cases up to 12% throughput benefits and up to 60% improvement in frame error-rates over the next best scheme. We then present a more advanced architecture for physical layer cooperation (termed QUILT), that seamlessly adapts to the underlying network configuration to achieve competitive or better performance than the best current approaches. It combines on-demand, opportunistic use of DF or QMF followed by interleaving at the relay, with hybrid decoding at the destination that extracts information from even potentially undecodable received frames. We theoretically quantify how our design choices affect the system performance. We also deploy QUILT on WarpLab and show through over-the-air experiments up to $5$ times FER improvement over the next best cooperative protocol.",,2015.0,10.5075/epfl-thesis-6500,109810278,semantic_scholar
559f9f70126c1cf3b90ec742ff8095ec1b9eb2e9,https://www.semanticscholar.org/paper/559f9f70126c1cf3b90ec742ff8095ec1b9eb2e9,Programming GPS and OpenStreetMap Applications with Java: The RealObject Application Framework,FROM VISION TO MISSION Software Objects and Real-World Representations Introduction Controlling Remote Objects via References Object-Oriented Programming Models in Physics The Vision Introduction Physical Objects Projecting Reality into Virtual Worlds Real-World Objects (ROs) Real Object Applications (RO-Apps) Real Object Application Framework (ROAF) OOA Analysis and Mission Introduction Language Analysis Semantic Network Gathering Information Data Dictionary Problem Statement Candidate Objects The Mission GLOBAL POSITIONING Space and Time Introduction A GeoPoint for Spatial Coordinate Systems A Position Interface for Various Coordinate Systems A Route to Manage an Array of Positions NAVSTAR GPS GPSpoint implements GPSinfo GPStrace extends Route JavaGPS Requirements for a GPSunit Real vs. SimulatedMotion Recording a liveTrace Loading a GPX File to a GPStrace Play Back a GPStrace The GPS Package: roaf.gps From Geography to Cartography Introduction Map Projection Systems Requirements for a Map GUI The Framework Pattern Creating a MapPanel Creating a Swing Mapping Application Interacting via MapEvents Deploying the GPXviewer.jar The GUI Mapping Package REALOBJECTS (ROs) Objects in Motion Introduction Every RealObject has a GPSunit abstract Motion of a RealObject Creating a Motorcycle Observing Motorcycles Processing Digital Maps Introduction Overview A Map Compiler for OSM Data Collecting Geometry in the Field Map Attribution with JOSM Map Formats Processing Maps with Osmosis Parsing OSM Files with the OSMparser toolchain Configuration Rendering OpenStreetMaps with Kosmos Making Maps Navigable Introduction Map Compiler Branches Networking Creating a NavigableMap Conclusion Navigating Objects Introduction Navigation Systems Route Calculation Exploring the Graph with a GameMap The Navigator ROAPPS: REALOBJECT APPLICATIONS Separating the RO Client and the ROA Server Introduction Observing Remote Objects ROAF Client Software The roaf.util.RMI Class Client Server Architecture Introduction Status Report Four-Layer Architecture The ServerEngine The RealObjectsServer SOs: ServerObjects Server Tuning The RealObjectsBox ROAdio Broadcasting Rolling out a ROApp Introduction LC: The Game Scenario Server Architecture Application Layer: ROApp Extends ROServer ROF: The RealObject Framework ROApp clients: LCPlayer extends RealObject More ServerObjects The ROApp Scenario Mission Accomplished: Time to Play Introduction Using the Application Interactive GUIPlayers Intelligent Players Robocode ROAF: REAL OBJECT APPLICATION FRAMEWORK Evolution Introduction Game Scenario versus Real World ROApp Classification RO Classification Decouple Mature ROs The RealObject Hierarchy,,2012.0,10.1201/b11641,59997089,semantic_scholar
879d7ae67e566d2ed81499d22c9676187926996d,https://www.semanticscholar.org/paper/879d7ae67e566d2ed81499d22c9676187926996d,An IEEE 802.22 transceiver framework and its performance analysis on software defined radio for TV white space,"With rapid increase in new applications and services, there is huge demand for internet bandwidth. Several researchers around the world have found that, majority of licensed bands (mostly terrestrial TV band) are either unused or underused. These underutilized bands allocated for TV transmission are known as TV white space (TVWS). For effective utilization of TVWS, the IEEE 802.22 is proposed. The IEEE 802.22 wireless regional area network (WRAN) is the latest standard for effective utilization of TV bands. This standard is based on orthogonal frequency division multiplexing with various modulation techniques to provide different data rates. In this paper, an implementation framework for physical layer of IEEE 802.22 WRAN standard for normal mode is demonstrated and analyzed. This transceiver is implemented using the National Instruments Laboratory Virtual Instrument Engineering Workbench programming software on the National Instruments universal software radio peripheral 2952R. We have also analyzed different blocks of IEEE 802.22 based on their execution time, and identify the critical blocks of IEEE 802.22 that should be optimized for real-time applications for commercial product development and field deployments. We have also highlighted the difference between theoretical and practical performance of the considered error control codes for IEEE 802.22 specified block size. Additionally, various covariance based spectrum sensing methods are also analyzed for real-world environment.",Telecommun. Syst.,2018.0,10.1007/s11235-017-0417-x,49602924,semantic_scholar
e8b695ef59db23ec1f74520c2369b4ad1aaab2b1,https://www.semanticscholar.org/paper/e8b695ef59db23ec1f74520c2369b4ad1aaab2b1,Practical issues of implementing a hybrid multi-NIC wireless mesh-network,"Testbeds are a powerful tool to study wireless mesh and sensor networks as close as possible to real world application scenarios. In contrast to simulation or analytical approaches these installations face various kinds of environment parameters. Challenges related to the shared physical medium, operating system, and used hardware components do arise. In this technical report about the work-in-progress Distributed Embedded Systems testbed of 100 routers deployed at the Freie Universitat Berlin we focus on the software architecture and give an introduction to the network protocol stack of the Linux kernel. Furthermore, we discuss our rst experiences with a pilot network setup, the encountered problems and the achieved solutions. This writing continues our rst publication and builds upon the discussed overall testbed architecture, our experiment methodology, and aspired research objectives.",,2008.0,,30677109,semantic_scholar
8cc243723a5f33a1602efd4b743c4ae605c8e0f9,https://www.semanticscholar.org/paper/8cc243723a5f33a1602efd4b743c4ae605c8e0f9,Open Archive TOULOUSE Archive Ouverte (OATAO),"The Internet of Things promises an alwaysconnected future where the objects surrounding us will communicate in order to make our lives easier, more secure, etc. This evolution is a research opportunity as new solutions must be found to problems ranging from network interconnection to data mining. In the networking community, innovative solutions are being developed for the Device Layer of the Internet of Things, which includes the IoT wireless protocols. In order to study their performance, researchers turn more often to real world platforms, commonly designated by the term “testbeds”, on which they may implement and test the protocols and algorithms. This is even more important in the Industrial IoT field, where environments are perturbed by industrial systems like automated production systems. In this paper, after a brief presentation of the context of testbeds, we introduce WiNo and OpenWiNo, an open hardware and software framework for fast-prototyping in the field of the Internet of Things. Compared to existing platforms, the solution WiNo+OpenWiNo offers a wide array of Physical layers and easy integration of various sensors as it is developed as part of the Arduino ecosystem. It also allows research teams to easily and quickly deploy their own testbed into real environments. Keywords— Internet of Things; Wireless Sensor Networks; Fast prototyping; Testbed; Open Hardware; Arduino",,2012.0,,86868442,semantic_scholar
ee7cf7c6fa6abe30ba73aef48af4a813005501f8,https://www.semanticscholar.org/paper/ee7cf7c6fa6abe30ba73aef48af4a813005501f8,Deploying and Managing State-of-the-Art Workstation Labs Like a Boss!,"Our PACE Computer-Aided Design (CAD) / Manufacturing (CAM) / Engineering (CAE) Lab is one of our flagship computer labs at Lehigh University. The Lab is uniquely supported by both the Mechanical Engineering & Mechanics (MEM) Department and the IT organization. It is also unique in that we make available pools of virtual workstations to supplement the 60 physical high-end Windows workstations that are critical to our teaching and research missions. In addition to the PACE Lab, we also have to deploy and support 25 other lab and research environments, each having their own unique software and requirements. The entire university has been using Symantec Ghost for image management for nearly 15 years, but we needed a more flexible and automated solution to create, deploy, and manage Windows Operating Systems. You will learn why we decided to focus our efforts on the completely free Microsoft Deployment Toolkit Lite Touch solution, and how it has increased the efficiency of our deployments. We are currently expanding these techniques and realizing their time-saving benefits even in our datacenter and support of IT infrastructure. We have also been successful in deploying the Windows 10 Technical Preview in a test lab and are keenly awaiting the final release of the updated deployment tools that will officially support the new Microsoft Operating System. You will walk away with real-world best-practice workflows that you can immediately implement in your own environment to realize some of the benefits that we have already seen.",SIGUCCS,2015.0,10.1145/2815546.2815570,30811598,semantic_scholar
92b72047f2e8aff46d822c9d27233a5a652cb24c,https://www.semanticscholar.org/paper/92b72047f2e8aff46d822c9d27233a5a652cb24c,Prototyping processing-demanding physical layer systems featuring single or multi-antenna schemes,"In the past years numerous algorithms, schemes and techniques have been proposed, in order to improve the performance of the wireless multi-antenna communication systems. Real-time multi-antenna testbeds are offering the means to analyse the real-world performance, implementation cost and feasibility of such novel techniques, accounting for hardware limitations and software constraints. This paper presents the physical-layer (PHY) design, implementation and validation of a high-performance real-time mobile WiMAX transceiver, accounting for low-level deployment issues and signal impairments. A first evaluation of the acquired results for both Single Input Single Output (SISO) and Multiple Input Multiple Output (MIMO) system-configurations, demonstrates the performance of the system using real-time channel emulation.",2011 19th European Signal Processing Conference,2011.0,,14752168,semantic_scholar
2b4cfc0b672aafc526bda6f7e35cf204ece0d34c,https://www.semanticscholar.org/paper/2b4cfc0b672aafc526bda6f7e35cf204ece0d34c,AC 2009-1203: A NOVEL INTERDISCIPLINARY SENSOR NETWORKS LABORATORY,"Today, networks of legacy and newer sophisticated sensors and actuators that combine reconfigurable gigascale semiconductor technology with emerging micro-mechanical systems (MEMS) and nanotechnology subsystems (i.e. bio-systems/chemical/fluidics/photonics/ etc) are being designed and deployed in almost every area of technology that impacts human endeavor and commerce. These smart sensors/actuators are being networked together through: either standards based or industry specific, proprietary, wired networks or newly emerging wireless networking technologies. Presently, at the twoand four-year college level, technicians and technologists in a wide variety of impacted disciplines are not receiving an adequate education about: fundamental sensor theory, basic sensor operation, sensor system deployment planning, appropriate data-transport and networking connectivity schemes, applications software, and impending system maintenance support needs of these increasingly more sophisticated and complex, smart sensor/actuator based systems. This paper will report on the development, organization, and use of a novel interdisciplinary sensor networks laboratory. The heart of the laboratory is a dedicated data-network (SensorNet) that emulates a wide area network or WAN. The SensorNet WAN nodes and other network access points allow for the interconnection of numerous types of industry specific and standard “area networks” typically utilized for the gathering of sensor data and directing other sensor functions, as well as, the associated PC’s and servers used to direct the sensor systems and warehouse the gathered data. This laboratory environment lends itself to real world case-study and problem-based type student-centered learning experiences that can be themselves integrated into established fields of technology that do not normally include this type of activity as part of the field’s traditional educational experience at the undergraduate level. I. Overview Although it is not uncommon for several different technology fields to converge together, it is somewhat unexpected to observe such an amalgamation rapidly triggering other technologic innovations that have widespread potential to change our relationship with the environment and our daily endeavors. However, this is just what is happening today. Only a short time ago, the Internet, the result of a convergence of several technologies, spawned the development of what is commonly known as the “Information Economy.” Today another innovative and important convergence of technologies has recently gained critical mass and recognition by business and industry, government, academia, and professional societies. It is the deployment of intricate systems involving complex sensors with embedded (ambient) intelligence and advanced actuators coupled with modern data-transport and networking technologies and applicationenabling software with data fusion capabilities. This rapidly evolving convergence of technologies, which allows us to implement sensor systems that gather in situ (remote), realtime, statistically relevant information and interpret it in new and novel ways, has already started to transform automation and process control systems. The technology of networked sensor systems has the very genuine potential to significantly impact almost every aspect of human endeavor by increasing system efficiency, reducing energy consumption, permitting the real-time monitoring of the “health” of the nation’s infrastructure and environment, and improving public health and safety. Applications are limitless! P ge 1.77.3 On a global level, the NSF has been calling this “grand convergence,” cyberinfrastructure. One may find many references to this concept, forecasts of potential future applications, reports on inprogress test projects such as HPWREN, NIMS, and ROADnet, and potential research funding opportunities on the NSF’s Web site [1] . However, most of this current, enthusiastic attention and promotion of cyberinfrastructure by the NSF is aimed at senior, graduate-level research institutions. Not surprisingly, most of the NSF’s recent Requests for Proposals (RFPs) in this area have been targeted at basic research about wireless sensor networks and systems and applications of these systems to infrastructure and environmental monitoring and other technology areas. While many applications of networked sensor systems are yet to be even thought of, the reality is that they are being deployed today and will continue to proliferate for many years to come until they eventually become as commonplace as a typical public utility like electricity. This paper describes aspects of an NSF funded CCLI project (DUE 0736888), titled, “The Sensor Networks Education Project” (SNEP) that seeks to develop materials and a model teaching laboratory that will be useful for other faculty and organizations at the twoand even the four-year college level to emulate. This project looks at this evolving convergence on a more practical level and speaks to the lack of engineering technology faculty expertise and teaching materials needed to infuse the newly recognized, exponentially growing knowledge base of networked sensor technology into the curricula and hence into the skill-sets of today’s twoand four-year technical college graduates – the technicians and technologists of tomorrow. This is the community of workers that will most likely deal with the design, deployment, updating, and maintenance of these systems. Today, networks of legacy and newer sophisticated sensors that combine reconfigurable gigascale semiconductor technology with emerging micro-electromechanical systems (MEMS) and nanotechnology subsystems [2] (i.e. bio-systems/chemical/molecular/photonic) are being designed and deployed in almost every area of technology that impacts human endeavor and commerce (i.e. Aerospace, Agriculture, Automotive, Biomedical, Building Automation, Energy Exploration and Production, Environmental Monitoring, Healthcare, Homeland Security, Industrial Automation, Infrastructure Monitoring, Information Technology, Manufacturing, Military, Pharmaceutical, Telecomm, Transportation, Weather Forecasting, etc). These sensors are being networked together through: either standards based or industry proprietary wired networks or emerging wireless networking technologies. Presently, at the twoand four-year college level, technologists and technicians in a wide variety of impacted disciplines are not receiving an adequate education about: fundamental sensor theory, basic sensor operation, sensor system deployment planning, appropriate data-transport and networking connectivity schemes, applications software, and impending system maintenance support needs of these increasingly more sophisticated sensor based systems. Recently, there has been a great deal of public dialogue about the out-sourcing of American manufacturing jobs and the effect of this reality on the nation’s future. Dealing with an ever increasing base of physical sensor networks in all areas of endeavor will not be something that can be done through a call to a help desk located in a foreign country. The apparent curriculum shortcoming regarding these topics within today’s associate and bachelors degree technology oriented programs is primarily due to the extremely rapid evolution and convergence of several key areas of electronics, computer, and MEMS technology (i.e. embedded processing, smart P ge 1.77.4 sensors/actuators, wired and wireless networking, etc), the lack of appropriate up-to-date educational materials, and a lack of appropriate faculty expertise in this rapidly expanding and remarkably cross-disciplinary field. II. Project Overview Over its two-year life-span, this CCLI Phase I project has as its primary goals the creation and testing of interdisciplinary student-centered learning materials primarily designed for a “field laboratory” type environment, the dissemination of these materials, and the development of faculty expertise in the multi-disciplinary field of networked sensors and modern active-learner teaching techniques. To accomplish these goals the project will: (1) develop and deploy a model, innovative, replicate-able, multi-interdisciplinary, case-study and problem-based oriented, networked, distributed sensor laboratory, (2) develop basic and advanced instructional materials and standard and “hybrid” laboratory activities related to the sensor laboratory that can be utilized for introductory courses in sensor technology or more advanced courses in networked sensor systems for use by both twoand four-year technology programs, (3) develop several prototype multifaceted educational modules that integrate traditional science and math based theory, practical real-world laboratory exercises, and science based, high-resolution, interactive simulation software, applicable to several of the major technology areas employing networked sensor technology (i.e. building automation and infrastructure monitoring and industrial automation), and (4) provide on-going local, regional, and national dissemination of these developed materials and laboratory experiences through hands-on faculty workshops and webbased distribution technologies including the National Science Digital Library (NSDL). In addition, for the duration of the project, continuous on-going professional development in the principles and applications of student-centered and active learner techniques will be provided to the recruited college faculty that will take part in the project. Research has shown that long term professional development programs are more effective than short-term workshops. For this project to be successful, the participating faculty must learn how to effectively integrate content and pedagogy in a way that actively engages students in individual and collaborative problem solving, analysis, synthesis, critical thinking, reasoning, and skillfully applying knowledge in real-w",,,,55903198,semantic_scholar
598a92dbe934f935de53dc24fdd0a1ba6a212daf,https://www.semanticscholar.org/paper/598a92dbe934f935de53dc24fdd0a1ba6a212daf,Wireless network virtualization : a techno-economic analysis and a service differentiation strategy,"Virtualization of wireless networks can significantly lower the capital expenditures (CAPEX) and operational expenditures (OPEX) by enabling resource sharing among multiple parties. Virtual network operators (VNOs), i.e. mobile virtual network operators (MVNOs) and over the top service providers (SPs) are becoming prominent players in wireless network markets with their differentiated service provisioning. This changing business model is beneficial for both the network operators and the VNOs, as network operators can increase their revenues by leasing resources to the VNOs who in turn, can implement their own network without having to deploy expensive physical networks. 
 
In future 5G networks, VNOs will play even more important role by providing various differentiated services using different wireless technologies. This requires provisioning of technologyagnostic physical infrastructure on which VNOs will be able to build their customized, isolated network slices tailored for optimal performance of the intended services. Research on Wireless network virtualization is a fairly recent trend and there is lack of an end-to-end solution for wireless network virtualization architectures in the open literature. In this respect, in this thesis, three architectural frameworks for wireless network virtualization have been proposed that differ in their degree of segregation between the signal processing and the radio accessunits. The frameworks also differ in terms of their associated CAPEX & OPEX as well as in terms of their achievable quality of service (QoS). For this reason, selection of a particular virtualization model for a particular service deployment is a multi-dimensional problem. Hence, a multi-criteria utility model has been developed that accounts for network cost & QoS trade-offs in order to enable the design and optimization of wireless access virtualization architectures that best comply with the investment and service-level requirements of network operators (and/or service providers). 
 
The second phase of the thesis focuses on the architectural requirements for provisioning heterogeneous virtual networks on a common physical substrate. It has been argued that software defined network (SDN) and cloud computing technologies are the key enablers for deploying such a network model. The existing proposals in the open literature focus on wireless network solutions for a particular radio access technology (RAT), e.g., WiFi, cellular, wireless sensor network (WSN), etc. or a particular part of a network (e.g., cellular core vs access networks). But an integral solution for programmable, elastic, virtualized heterogeneous networks is not available in the open literature. Hence, a blueprint for the deployment of an end-to-end programmable & flexible heterogeneous virtual wireless network (HVWN) infrastructure using SDN & cloud computing has been laid out in this chapter. Open problems and challenges in realizing a programmable, elastic HVWN have also been identified. 
 
Next, in the third phase of the thesis, the case of provisioning differentiated services in a cloud-based software-defined virtual wireless network environment has been studied. We have focused on a particular part of the generalized architecture proposed in the second part of the thesis, i.e., the case of programmable virtualized wireless networks that consists of cellular and fixed WiFi networks. More specifically we have studied how differentiated services can be provided in such a programmable virtualized platform. We have proposed to use the spare bits of OpenFlow packet structure to implement virtual network entities. Use of northbound APIs has been emphasized for composing complex wireless network applications. Emulation results show that the SDN-based virtualized wireless networks are able to meet the critical performance requirements of carrier networks. 
 
In the final part of the thesis, we focused on full duplex (FD) deployment of multi-cell networks. Current cellular networks are suffering from spectrum ossification problem. In a virtualized environment where multiple VNOs will compete for access to shared radio resources, the spectrum scarcity problem will be more severe. In such context, FD systems can provide an efficient solution by doubling the spectrum efficiency. In our research, we have identified the critical challenges for real world deployment of multi-tier FD multi-cell networks. We have analyzed FD performance trade-offs for a dense urban multi-tier cellular network. We have used the Madrid grid model proposed by METIS project that consists of macro and pico cells. We also have investigated the impact of co-located BS interference in FD performance for a single-tier homogeneous network deployment. We have proposed intelligent proportional fair joint user selection and power control algorithms to harness the gain of FD deployment. We have developed algorithms for both cloud radio access network (C-RAN) and traditional distributed RAN (D-RAN) network models. Extensive system-level simulation results show that using the devised algorithms the FD systems are able to achieve significant performance gain .",,2016.0,,113627971,semantic_scholar
d0d2d442baa8320b3a87af64b1a5a6e98497ae3c,https://www.semanticscholar.org/paper/d0d2d442baa8320b3a87af64b1a5a6e98497ae3c,Analysis of Radio Communication Attenuation Using Geoprocessing Techniques,"Multi-Agent Geo-Simulation (MAGS) aims to simulate phenomena involving a large number of autonomous situated actors (implemented as software agents) evolving and interacting within a Virtual representation of the Geographic Environment (VGE). A radio communication system is a typical complex dynamic phenomena where transmitter and receiver antennas are constantly constrained by the physical environment in which it they are deployed. In the real world, radio transmissions are subject to propagation effects which deeply affect the received signals because of geographic and environmental characteristics (foliage and vegetation, buildings, mountains and hills, etc.). Using geoprocessing techniques, we propose an automated approach to build semantically-informed and geometrically-accurate virtual geographic environments which uses Geographic Information System (GIS) data and builds an informed graph-based structure called Informed Virtual Geographic Environment (IVGE). In addition, we propose a multi-agent prototype to analyze the attenuation effect due to the radio signal’s traversal between antennas (simulated as software agents) through terrain shape, vegetation area, and buildings using a 3D line-of-sight computation technique. Keywords-Informed Virtual Geographic Environment (IVGE); Radio Signal Propagation; Line-Of-Sight; Multi-Agent GeoSimulation (MAGS).",,2011.0,,166224141,semantic_scholar
0c177e2385aad71584840a21b23ebd4b58c00132,https://www.semanticscholar.org/paper/0c177e2385aad71584840a21b23ebd4b58c00132,Service Virtualization: Reality Is Overrated,"Software drives innovation and success in todays business world. Yet critical software projects consistently come in late, defective, and way over budget. So whats the problem? Get ready for a shock, because the answer to the problem is to avoid reality altogether. A new IT practice and technology called Service Virtualization (SV) is industrializing the process of simulating everything in our software development and test environments. Yes, fake systems are even better than the real thing for most of the design and development lifecycle, and SV is already making a huge impact at some of the worlds biggest companies. Service Virtualization:Reality Is Overratedis the first book to present this powerful new method for simulating the behavior, data, and responsiveness of specific components in complex applications. By faking out dependency constraints, SV delivers dramatic improvements in speed, cost, performance, and agility to the development of enterprise application software. Writing for executive and technical readers alike, SV inventor John Michelsen and Jason English capture lessons learned from the first five years of applying this game-changing practice in real customer environments. Other industriesfrom aviation to medicinealready understand the power of simulation to solve real-world constraints and deliver new products to market better, faster, and cheaper. Now its time to apply the same thinking to our software. For more information, see servicevirtualization.com. What youll learnYou will learn why, when, where, and how to deploy service virtualization (SV) solutions to mitigate or eliminate the constraints of an unavailable or unready service system by simulating its dependent components in order to deliver better enterprise software faster and at lower cost. In particular, you will learn step-by-step why, when, where, and how to deploy the following SV solutions: shift-left infrastructure availability performance readiness test scenario management Who this book is for This book is not only for IT practitioners on engineering, testing, and environments teams engaged in the development and delivery of enterprise software, but also for executives of companies in all sectors who need to understand and implement emergent opportunities to improve the time to market and overall competitiveness of any outward-facing business strategy that has a software application component. Table of ContentsForeword by Burt Klein Chapter 1. Introduction Service Virtualization Briefly Defined Key Practices Enabled by SV Shift-Left Infrastructure Availability Performance Readiness Test Scenario Management Navigating This Book Chapter 2. The Business Imperative: Innovate or Die Consumers Have No Mercy Business Demands Agile Software Delivery Increased Change and Complexity for IT Simulation Is Not Just for Other Industries Chapter 3. How We Got Here From Monolithic to Composite Apps Todays Complex Service Environments From Waterfall to Agile Development Chapter 4. Constraints: The Enemy of Agility Unavailable Systems and Environments Conflicting Delivery Schedules Data Management and Volatility Third Party Costs and Control Chapter 5. What is Service Virtualization? The Opposite of Server Virtualization Creation of a Virtual Service Maintaining Virtual Services What Kinds of Things You Can virtualize Virtual Service Environments (VSEs) Chapter 6. Where to Start with SV? Pick a Hairy Problem Identify Stakeholders Set Real Value Goals for Releases Avoid Inappropriate Technologies Chapter 7. Capabilities of Service Virtualization Technology Live-Like Development Environment Automation Eliminates Manual Stubbing and Maintenance Enables Parallel Dev and Test No more Availability Problem Platform-Neutrality Chapter 8. Best Practice #1: Shift-Left Reducing Wait Time Early Component and System Testing Define SV from Capture Define Incomplete SV from Requirements Expected Results Customer Example Chapter 9. Best Practice #2: Infrastructure Availability Finding Over-Utilized Resources Virtualizing Mainframes Avoiding Big IT Outlays Expected Results Customer Example Chapter 10. Best Practice #3: Performance Readiness Virtualizing Performance Environments Informing Performance from Production Expected Results Customer Example Chapter 11. Best Practice #4: Test Scenario Management Managing Big Data Shielding Teams from Volatility Massively Parallel Regression Testing Expected Results Customer Example Chapter 12. Rolling out Service Virtualization Who Pays for Service Virtualization? Overcoming Organizational Challenges Who Manages a VSE? Should I Have More Than One? Key Skills and Roles in a Virtual IT World Chapter 13. Service Virtualization in the DevTest Cloud Constraints of Cloud Dev and Test Achieving Elastic Cloud Environments Chapter 14. Assessing the Value Key Metrics for Success Areas for Improvement Chapter 15. Conclusion The Industrialized Software Supply Chain Innovate and Thrive Whats Next for SV? Glossary About the Authors",,2012.0,,113694918,semantic_scholar
eebc85dcbbf0b5904316256d57cf7e9c3488d024,https://www.semanticscholar.org/paper/eebc85dcbbf0b5904316256d57cf7e9c3488d024,E-science approaches in molecular science,"Computer simulations of the properties of processes and materials are becoming increasingly necessary in several technological and environmental studies. This implies a growing demand of computing resources that severely exploits computational environments in terms of sustainability and reliability of the infrastructure.
 The developments in computing hardware and software, in particular the deployment of world-wide reliable Grid Computing infrastructures, the adoption of innovative computing approaches like the General Purpose Graphic Processing Unit (GPGPU) Computing and the High Performance Network environments, stimulate the exploitation of new approaches and methodologies in Computational Sciences. Furthermore the advances made in the World Wide Web, allow the implementation of Web sites from which the simulation of elementary chemical processes at molecular level is performed combining various techniques and computational approaches which are executed on High Throughput Computing (HTC) and/or High Performance Computing (HPC) platforms.
 The ubiquity of information and computing resources has impacted on the researchers? productivity, in a similar way the same technologies impacted everyone?s daily life. The E-science technologies facilitate the exchange of information among researchers, enhance the collaborative work and increase the quality of dissemination of results. Several European initiatives are devoted to facilitate researchers? work and to establish networks among researchers of the various disciplines, enabling some European research groups to reach leading positions in their disciplines. We have got the support of the EU COST (COllaboration in Science and Technology) Initiative in two Actions devoted to the facilitation of adoption of Grid and Distributed Computing technologies in Molecular and Matter Sciences. In particular I participated to the COST D23 Action: Metachem - Metalaboratories for Complex Computational Applications in Chemistry (2000-2005), and I coordinated the Working Group: Simbex: a metalaboratory for the a priori simulation of crossed molecular Beam Experiments. Furthermore I participated to the COST D37 Action: Grid Computing in Chemistry: GRIDCHEM (2006-2009), and I coordinated the Working Group: ELAMS: E-science and Learning Approaches in Molecular Science.
 The outcomes of both COST Actions contributed significantly to establish an active group of Computational Chemistry and Molecular and Matter Science laboratories which adopted the Grid Computing as an innovative computing paradigm for performing massive computational campaigns.
 Since 2004 the researchers of such laboratories joined the Virtual Organization (VO) CompChem established on the EGEE Grid Infrastructure, the largest distributed computing environment ever established worldwide, and coordinated by CERN (Conseil Europeen pour la Recherche Nucleaire). I served as VO Manager since the VO was established, under the coordination of Prof. Antonio Lagana, Department of Chemistry, University of Perugia.
 The present thesis covered a long period of research work focused on implementing some e-science instruments to the computational chemistry community, in particular the community of users belonging to the COMPCHEM Virtual Organization active in the EGEE/EGI European Grid Initiative.
 The Thesis describes some tools and approaches the author adopted to provide innovative tools to the Computational Chemistry community based on two main pillars:
1. approaches for running large computational campaigns on Grid Infrastructures 2. adopting virtual reality techniques for making more intuitive the interaction with nanoscale computing approaches and simplifying the definition of the initial conditions of the molecular simulations
 The research work originated 10 research papers, several of them produced as a joint work with European laboratories interested in the implementation of e-science tools for a smart, curious and demanding community like the Computational Chemistry one.
 The author has been able to provide a useful view of the molecular world through the use of virtual reality techniques, combined with the most advanced Web technologies, in particular using the ISO standard X3D for the 3D visualization and interaction with a virtual world. These innovative tools enabled the researchers to set up the environment for carrying out complex molecular simulations (as in the case of the Dl-Poly software package) in a intuitive and visual way. Once defined the species interacting in the considered molecular system, represented in a virtual world, the system produce the input file for the simulation and the Dl-Poly program may be launched, possibly on a Grid infrastructure to take benefit of the powerful available computational resources.
 In chapters 1, 2 and 3 the various steps toward the implementation of an a-priori molecular simulator on the EGEE/EGI Grid, for the COMPCHEM VO users, are reported.
 Molecular Virtual Reality applications are really useful as e-learning support tools for Chemistry students. To this purpose we implemented a Learning Management System based on a semantic web approach, described in chapter 7, and an assessment system, described in chapter 8, which have been used several times to assess the competences of students participating to the Erasmus Mundus Master of Science in Theoretical Chemistry and Computational Modeling. The system enables the coordinators of the Master to monitor the progresses made by the students an a daily basis.
 The author has shown how it is possible to use virtual reality approaches to describe a chemical experiment at both human and molecular level using a virtual reality approach. To this end a multi-scale virtual reality approach has been adopted to deal with the description of the physical environment,HVR. The main features of the virtual reality representation of the experiments and the potentiality of associating VRML and X3D with Java engine calculator are outlined in chapter 4.
 In chapter 5 an X3D Molecular Virtual Reality environment in which the researcher is able to interact with it by using immersive devices and dynamic gestures is described. By using the gestures the researcher is able to modify the composition of the molecular system by adding or subtracting functions and the molecular properties of the new species are evaluated in real time by invoking a Web Service implementing the simulation environment. This has required the assemblage of an innovative approach coupling the management of immersive devices with Web Services and molecular dynamics packages.
 In chapter 6 the author has presented an X3D Molecular Virtual Reality environment which makes usage of the most recent and powerful HTML and Web technologies. The approach implemented takes into account the modern approaches followed in implementing Social Networking environments and showed how useful these approaches are also in implementing scientific environments. We think such type of work is important also in consideration of how our lifestyle is changing, thanks to the ubiquity of the information, the availability of an increasing availability of storage and computing power. The social networking showed us how deep may be the impact of the computing and networking facility in the daily life and similarly the computational science, and the computational chemistry in particular, has to reshape the classical approaches and methodologies in order to gain advantage of the modern computing platforms and the powerfulness of the networking, distributed and mobile environments.",,2012.0,,215932175,semantic_scholar
f46a281fee7b6efcc4e1ba05755ea198ad008fdf,https://www.semanticscholar.org/paper/f46a281fee7b6efcc4e1ba05755ea198ad008fdf,Application of intelligent agent technology for knowledge management integration,"Organizations invest in various knowledge management (KM) systems and tools to enable seamless integration of the constantly increasing volume and sources of information. This research first, presents a case study on the existing categories of KM systems and tools, their potential contribution to the KM process, and their pitfalls; second, it proposes a comprehensive methodology for building KM through the organization using software agent technology. This approach aims to address the research issue of how KM can be optimized using intelligent agents and how to enhance decision-making process. The proposed system is applied to a real-world project lifecycle case that is EPC (Engineering Procurement and Construction) project. A prototype of the system is presented where intelligent agents are the building blocks of a peer-to-peer organization wide system. The application was implemented using Eclipse technology, and the agents were deployed on the FIPA-OS (Foundation for Intelligent Physical Agents-Open-Source) environment, we used JESS (Java expert system shell) to develop the knowledge based of the agents' reasoning.",,2004.0,10.1109/ICCI.2004.6,62769875,semantic_scholar
d70a02826a51efc7d133f688a820e73e6a1a1b44,https://www.semanticscholar.org/paper/d70a02826a51efc7d133f688a820e73e6a1a1b44,Semantic High Level Querying in Sensor Networks,"The quick development and deployment of sensor technology within the general frame of the Internet of Things poses relevant opportunity and challenges. The sensor is not a pure data source, but an entity (Semantic Sensor Web) with associated metadata and it is a building block of a “worldwide distributed” real time database, to be processed through real-time queries. Important challenges are to achieve interoperability in connectivity and processing capabilities (queries) and to apply “intelligence” and processing capabilities as close as possible to the source of data. This paper presents the extension of a general architecture for data integration in which we add capabilities for processing of complex queries and discuss how they can be adapted to, and used by, an application in the Semantic Sensor Web, presenting a pilot study in environment and health domains. 1 Background and Motivation The rapid development and deployment of sensor technology involves many different types of sensors, both remote and in situ, with such diverse capabilities as range, modality, and manoeuvrability. It is possible today to utilize networks with multiple sensors to detect and identify objects of interest up close or from a great distance. Connected Objects – or the Internet of Things – is expected to be a significant new market and encompass a large variety of technologies and services in different domains. Transport, environmental management, health, agriculture, domestic appliances, building automation, energy efficiency will benefit of real-time reality mining, personal decision support capabilities provided by the growing information shadow (i.e. data traces) of people, goods and objects supplied by the huge data available from the emerging sensor Web [1]. Vertical applications can be developed to connect to and communicate with objects tailored for specific sub domains, service enablement to face fragmented connectivity, device standards, application information protocols etc. and device management. Building extending connectivity, connectivity tailored for object communication – with regards to business model, service level, billing etc, are possible exploitation areas of the Internet Connected Objects. Important challenges are to achieve interoperability in connectivity and processing capabilities (queries, etc.), to distribute “intelligence” and processing capabilities as close as possible to the source of data (the Giordani I., Toscani D., Archetti F. and Cislaghi M.. Semantic High Level Querying in Sensor Networks. DOI: 10.5220/0003116600720084 In Proceedings of the International Workshop on Semantic Sensor Web (SSW-2010), pages 72-84 ISBN: 978-989-8425-33-1 Copyright c 2010 SCITEPRESS (Science and Technology Publications, Lda.) sensor or mobile device), in order to avoid massive data flows and bottlenecks on the connectivity side. The sensor is not a pure data source, but an entity (Semantic Sensor Web) with associated domain metadata, capable of autonomous processing and it is a building block of a “worldwide distributed” real time database, to be processed through realtime queries. The vision of the Semantic Sensor Web promises to unify the real and the virtual world by integrating sensor technologies and Semantic Web technologies. Sensors and their data will be formally described and annotated in order to facilitate the common integration, discovery and querying of information. Since this semantic information ultimately needs to be communicated by the sensors themselves, one may wonder whether existing techniques for processing, querying and modeling sensor data are still applicable under this increased load of transmitted data. In the following of this paper we introduce the state of the art in data querying over network of data providers. In Sect. 2 we present the software architecture of a data integration system in which we added complex query processing features. Sect. 3 introduces the case study in which we deployed our system: the study of short term effect of air pollution on health. Sect. 4 presents the detailed implementation of the querying features together with results on real data sets. Finally, Sect 5 presents the conclusions and future work. 1.1 State of the Art This paper stems from the work presented in [12], in which is presented a software system aimed at forecasting the demand of patient admissions on health care structures due to environmental pollution. The target users of this decision sup-port tool are health care managers and public administrators, which need help in resource allocation and policies implementation. The key feature of that system was the algorithmic kernel, to perform time series analysis through Autoregressive Hidden Markov Models (AHMM) [7]. The scenario in which the system has been deployed is the research project LENVIS1, which is aimed to create a network of services for data and information sharing based on heterogeneous and distributed data sources and modeling. One of the innovations brought by LENVIS is the “service oriented business intelligence”, i.e. an approach to Business Intelligence in which the information presented to the user comes from data processing that is performed online, i.e. data are extracted under request of the applications, and on the basis of data availability, i.e. data are exchanged through web services, which does not guarantee response time neither availability. Such a complex environment, in which data sources are distributed over the internet, is common to several problems and has been faced by different approaches. One of them is that of [13], in which “monitoring queries” continuously collect data about spatially-related physical phenomena. An algorithm, called Adaptive Pocket Driven Trajectories, is used to select data collection paths based on the spatial layout of sen1 LENVIS Localised environmental and health information services for all. FP7-ICT-2007-2. Project number 223925. www.lenvis.eu 73",SSW,2010.0,10.5220/0003116600720084,16601247,semantic_scholar
3230c340ca4eeb94c7648fd516ed26c70b12a249,https://www.semanticscholar.org/paper/3230c340ca4eeb94c7648fd516ed26c70b12a249,Loading Architecture for a Sensor Web Browser on Digital Earth_final_0,"The world-wide sensor web observes real world phenomena at a particular moment in time with a large number of geo-referenced sensors. Sensor web needs a sensor web browser for accessing distributed and heterogeneous sensor networks in a coherent frontend. The Digital Earth provides a geo-referenced three-dimensional environment for intuitively browsing and displaying sensor observations. However, the major challenge is to load the vast amount of sensor observations from servers to a sensor web browser while minimizing the delay that a user experiences. This research uses two techniques to address the challenge. First, the browser caches transmitted data onto the local hard drive to reduce redundant internet bandwidth consumption. Second, this work designs a loading architecture to decouple sensor data loading, rendering, and browsing. The proposed scheme is implemented in the GeoCENS sensor web browser. To the best of our knowledge, with the proposed loading architecture, GeoCENS is the first Digital Earth-based sensor web browser. Background and Relevance The Digital Earth was envisioned in Gore’s 1998 speech1. The Digital Earth is a threedimensional visualization model of the physical Earth, which contains high resolution imagery and digital elevation models. Users are able to intuitively interact with Digital Earth by navigation features such as flying to places or floating above the surface. In recent years more and more publicly-available Digital Earths are revealed, such as ESRI’s ArcGIS Explorer2, NASA’s World Wind3, Microsoft’s Bing Map4, and Google Earth5. One of the visions that Gore described is that people can access vast amount of scientific information through the Digital Earth to help them understand real world. Therefore, at the same time that Digital Earths were being developed, the deployments of the world-wide sensor web were also put into practice for observing heterogeneous, environmental phenomena. The sensor web concept originated at the NASA/Jet Propulsion Laboratory in 1997 (Delin et al. 2005; Liang et al. 2005) for acquiring environmental information by integrating massive spatially distributed consumermarket sensors. The world-wide sensor web has been applied in a range of applications, including: large-scale monitoring of the environment (Hart and Martinez 2006), civil structures (Xu et al. 2004), roadways (Hsieh 2004), and animal habitats (Mainwaring et al. 2002). Ranging from video camera networks that monitor real-time traffic, matchbox-sized wireless sensor networks embedded in the environment for monitoring habitats, sensor webs generate tremendous volumes of valuable observations, enabling scientists to observe previously unobservable phenomena. Similar to how the World 1 http://www.isde5.org/al_gore_speech.htm 2 http://www.esri.com/software/arcgis/explorer/index.html 3 http://worldwind.arc.nasa.gov/java/ 4 http://www.bing.com/maps/ 5 http://www.google.com/earth/index.html Wide Web needs an Internet browser for viewing web pages, the sensor web needs a coherent frontend for accessing the distributed and heterogeneous sensor networks. This kind of coherent frontend is called sensor web browser. In order to achieve Gore’s Digital Earth and sensor web visions, an online Digital Earthbased sensor web browser for users to browse, search, manage, and exchange sensor data is needed. However, transmitting the vast amount of sensor readings from servers to a sensor web browser with minimum delay is very challenging. Therefore, the goal of this paper is to present a sensor web data loading architecture for addressing the following two issues: (1) transmitting vast amount of sensor data and (2) minimizing the delay time that user might experience. To address the first issue, efficient data loading management utilizing a local cache is applied. To address the second issue, a loading architecture that decouples loading and browsing, a speculation mechanism, and a dynamic priority queue (Xhafa & Tonguz 2001) are applied. With the proposed scheme, efficient sensor web data loading and good user experience are attained. The sensor data used in this work are historical and can be represented as points on the Digital Earth. Methodology Issue 1: Transmitting Vast Amount of Sensor Data To mitigate this issue, we can adopt strategies from Web browsers and how they are able to minimize redundant transmissions. Web browsers cache data, which means they store web page requests and associated responses as key-value pairs on the local disk in order to reduce unnecessary and redundant transmissions, and to improve end-to-end latency. Similarly, this work implements a caching strategy to reduce the unnecessary transmission of sensor data in the sensor web. However, sensor web browsers’ requests and sensor observations are different from web browsers’ requests and web pages. We cannot simply manage sensor web requests and responses as key-value pairs in web caches. A sensor web request can be interpreted as asking for sensor observations of a certain phenomenon in a set of spatio-temporal cubes. The spatio-temporal cubes can be distributed irregularly in space and time. An effective sensor web caching strategy requires efficient management of these spatio-temporal cube requests. Therefore, we develop a new spatio-temporal indexing structure, LOST-Tree (LOading SpatioTemporal indexing tree) to manage sensor data loading with a local cache. A LOST-Tree manages sensor data loading of one phenomenon. A LOST-Tree consists of two techniques. First, instead of indexing massive amounts of raw sensor data, a LOST-Tree will index requests (i.e., spatio-temporal cubes). Because no actual data stored in LOST-Tree, it is small and able to fit into memory for efficient processing. Second, a LOST-Tree applies any two regular and aggregatable structures onto the spatial and temporal domains for transforming irregular spatio-temporal cubes into regular cubes. For instance, this work implements with a Quadtree (Finkel & Bentley 1974) and the Gregorian calendar. These two structures are integrated by using temporal structure (i.e., the Gregorian calendar in this work) as main structure and embedding spatial structure (i.e., Quadtree in this work) in nodes of temporal structure. In this way, a record in LOST-Tree represents a spatio-temporal cube, which allows simple look-up searches (rather than range query) for determining unloaded cubes. Records are inserted into LOST-Tree only if the corresponding cubes have been loaded. Since both structures are aggregatable, the more cubes are loaded, the fewer records remain. Therefore, a LOST-Tree is scalable, light weight, and able to efficiently identify unloaded potions in sensor web browser. By incorporating LOST-Trees into a sensor web browser, previously loaded sensor data can be retrieved from the local cache. The end-to-end latency can be reduced and the Internet bandwidth can be efficiently utilized on transmitting the data that has not been loaded. Issue 2: Minimizing the delay time that users experience In order to minimize the delay users might experience when loading sensor web data, we need to decouple data loading, rendering, and browsing. We implemented the following performance improvement strategies in the GeoCENS sensor web browser (Liang et al. 2010). (1) Checker: In order to reduce the requesting frequency, instead of executing loading processes whenever the Digital Earth is moving, loading processes start when the earth has stopped moving for more than a defined period of time (e.g., 500 milliseconds). This follows the assumption that a user’s area of interest corresponds to where the user stops moving the Digital Earth. (2) Wrapper: After a user requests a spatio-temporal cube, a thread is trigged to determine the portions that are not available in the local cache (by using a LOST-Tree). The wrapper also filters out the requests that have been issued, and composes new loading tasks. (3) Loader: After the loading tasks are created, these tasks are stored into a queue waiting for being sent to the servers. A thread-pool with pre-defined number of threads polls these loading tasks from the queue and issues requests based on the loading tasks to the servers. After the data are transmitted to the browser, a thread is trigged to parse the data and store them in local cache. (4) Poller: The checker, wrapper, and loader are for the data loading, and the poller is for rendering. A timer runs periodically to check if user moves the Digital Earth. If not, a thread will then be trigged to retrieve data from the local cache and render the data on Digital Earth. Besides applying the decoupling architecture, two additional mechanisms are implemented for improving a user’s experience. They are speculation and dynamic priority queue. Firstly, speculation means the system issues requests before a user issues the requests. For instance, we expect users will browse the nearby regions around their initial area of interest. Therefore, when a user requests a spatio-temporal cube, the spatial component (e.g., bounding box) of the cube is then expanded. Secondly, users are allowed to navigate freely within the Digital Earth environment. A user may decide to move to other places before the previous loading tasks are digested. As a result, it is important to prioritize the loading tasks dynamically. For example, if we manage the loading tasks with a first-in-first-out (FIFO) strategy, new requests will not be executed until the old requests are finished. In this case, users may feel slower system performance because the system is background loading data they aren’t expecting. Therefore, instead of following a FIFO queue, we apply a dynamic priority queue (Xhafa & Tonguz 2001) to prioritize the loading tasks. Whenever a user moves the Digital Earth, the priorities of loading tasks in the queue will be re-assigned with the distance between the current area of interest and the tasks’ loadi",,2011.0,,10895330,semantic_scholar
3f826f1f349ee707639c39d231259498b14215c5,https://www.semanticscholar.org/paper/3f826f1f349ee707639c39d231259498b14215c5,AC 2011-2689: SMART GRID DEVELOPMENT IN ELECTRICAL DIS- TRIBUTION NETWORK,"This paper will focus on smart grid project design and implementation. The project was developed by students and demonstrates new ideas and teamwork. This project was successfully completed and has been developed, implemented and assessed. Topics covered are: how to build a smart gird by utilizing computer application software tools, design, simulation, and diagnoses of electrical distribution systems. All the real world components in electrical distribution network such as residential, commercial and industrial building are modeled in this project. Background The purpose of this project is to design and implement a small scale electric power network by a team of seven students, supervised by a faculty member. The students’ background is in electrical engineering with emphasis in electric power system. The students conducted a study in the field of Smart Grid technologies for history and background information. This work led to designing and implementing a small model of a smart gird power distribution network. The power grid represents the real world aspirations of both government and private industry geared toward building a more reliable, responsive, and overall efficient network of residential, industrial and commercial buildings. Since the concept of a smart grid is very vague, students chose to implement a time tested and proven aspect of such technology known as smart meters. The smart meter is a wireless device connected to every house, industrial and commercial buildings to provide essential feedback in real time to the power companies. This feedback could be in the form of a fault occurring at that said location, or illegal energy usage. This feedback in real time is very useful to the power companies, given the fact that most rely on feedback via a phone call from the consumer before they know whether or not there is a fault in the system. Another purpose of implementing the grid was to simulate metering technology at the residential, industrial and commercial level. These meters would send data to a computer which is a simulated control room in order to read where certain faults occur in the system. In turn one could control which areas of the grid would be supplying the power. This represents a simulation of the power company’s ability to read and send vital information throughout the grid, thus improving the responsiveness and reliability of the network. Figure 1 illustrates the completed model after it was built and during testing. The lifecycle of this project was implemented in three different phases and started in September of 2009 and it was completed in May of 2010. Planning and analysis was completed in phase I, design and implementation in phase II, and documentation and students’ assessment in phase III. Figure 1. A model of smart grid in electrical distribution system Phase I: Planning and Analysis Initially, each team member worked on individual research on the concepts of smart grid its purpose. Later on, a decision was made as to what the team wanted to demonstrate with the project. The decision was made to show specifically how smart meters would work and help in fault detection as well as saving money by removing the need for meter readers to read the power meters every month. A project leader was elected by the team members to coordinate the team work. Meetings were then set up by the project leader, to brainstorm on how the actual implementation was going to be planned. Microsoft Project software was very instrumental to organize the work of the team. Tasks were assigned with specific due dates to keep the project on schedule and under budget. Requirements Since this project was spread over three quarters, students had many deadlines and task that had to be met in order to have a successful project. There were three phases to this project, research, background study and planning during the 1 quarter, design and building during the 2 quarter and the final stage of testing and troubleshooting during the 3 quarter along with final oral presentation, simulation documentation and assessment of the project. The students made documents and recorded each steps of the project down to each task and timeline by using Microsoft Project software. The project advisor coordinated the project steps and students were required to present a weekly progress report. This step insured that the project was moving smooth and on the track. The group was divided into two teams, one in software teams which consisted of two members and a hardware team which consisted of other five group members. The software team was in charge of all the coding and GUI implementation so the actual grid can communicate back and forth with the computer. The hardware team was in charge of the physical grid which consisted of the circuit that was built using logic chips such as MUXs, and Flip-Flops, wiring, creating a map on the grid with houses, roads, school, power stations, sub-stations, transmission lines, and distributions lines. The commercial site consists of shopping area, factories, stadium, school and so forth. In final stages of the project, testing, debugging and troubleshooting was performed in order to assure that hardware components and related software can communicate back and forth in a proper sequence. Much of the requirements had the made along the way since this was very new to all students. Phase II: Design and Implementation The design started immediately after the clear definition of the project requirement and purpose. To lower the cost and improve the safety, the design would be a DC (Direct Current) representation of an AC (Alternate Current) system. The system was designed by drawing out the model of a city and the specific buildings to exist in that city. The design was based on what took place in the planning stage which defined how the city and buildings will receive their power and the power. Figure 2. The process of building a smart grid The next challenge in the design process was solving the problem of switches and smart meters. Figure 2 shows the design of the smart meters and placement of LEDs (Light Emitting Diode). The LEDs will represent whether a particular house, building or transformer has power on or off. If for any reason an LED was not lit, then that particular item does not have power. The faults were determined by voltages because even if the building wasn’t drawing power, then there still would be a voltage on the line. This voltage was then sent to a 64 to 1 multiplexor which was then sent to the microcontroller to determine faults. To turn the power of buildings “on” and “off” a common NPN transistor (2n2222) and the base current was provided by a flip flop integrated circuit. Flip flops were used due to I/O’s limitations of the PIC. Figure 3 was duplicated for every transformer, with the only difference being the number of buildings being fed from the transformer which is the first LED after the 12V source. Figure 3. Circuit diagram for buildings Implementation A Smart Grid system includes a power meter which enables the communication systems to update the utility about its condition and the electronics to control the meter. The old electromechanical meters that were used are becoming obsolete since they cannot support the features that the utilities desire to have such as monitoring and controlling power supplied to its customers. Utilities wishes to monitor power consumption so that they can accurately predict how much power will be used during peak and down times. This information is helpful in producing sufficient energy and better efficiency in power waste. It can also help to pinpoint locations of power outages leading to a quicker recovery time. Challenges in implementation of the system are based on a couple of issues. First is the cost. It could cost upwards of $1000 for each smart meter, depending on features to be installed for each house or business. The costs can add up quickly, and the utilities don't see any immediate savings or incentive to deploy the smart grid in a very near future. The system designed in this project is using smart meters with a simulated wireless connection to the central servers at the utilities. The meters would send a signal to the central computer to update its status, power consumption, and other things. It can be designed in a way that it will have a battery backup for when the power is interrupted, or have the central computer assume it is off when it doesn't send a signal at the regular time intervals. Obviously the latter option would be the most cost effective and would use less power to run. But having power to the smart meter could also be beneficial because diagnostics could be run to determine if the power went out or if the meter is having its own internal hardware problems. A wireless signal was simulated for this project, but in real world application one can use either wireless, normal phone lines, or communications over power line. Most utilities already have communications systems set up through their power lines and using this method would be most cost effective. Having wireless, on the other hand, frees up usage of the power lines reducing their stress and prolonging the cables life. Companies are developing and testing their own systems using one of those options. In any case, it is based on hardware availability and cost effectiveness. Figure 4 illustrates communication with the smart grid. Figure 4. Communication with the smart grid Phase III: Documentation and Students’ Assessment In phase III of the project, the students provided a detail documentation of the project which includes cost analysis and different phases of the design. An electronic copy of this documentation and demo presentation was produced in a DVD. The following assessment and lessons learned was observed during the life cycle of the project: 1) When the main board that was used in the final project was constructed, the problems of wiring of all o",,2011.0,,73660185,semantic_scholar
4e9f1e3cec101b425cdb304a98888e4e4db65baa,https://www.semanticscholar.org/paper/4e9f1e3cec101b425cdb304a98888e4e4db65baa,Cognitive and Contextual Enterprise Mobile Computing: Invited Keynote Talk,"The second wave of change presented by the age of mobility, wearables, and IoT focuses on how organizations and enterprises, from a wide variety of commercial areas and industries, will use and leverage the new technologies available. Businesses and industries that don't change with the times will simply cease to exist. Applications need to be powered by cognitive and contextual technologies to support real-time proactive decisions. These decisions will be based on the mobile context of a specific user or group of users, incorporating location, time of day, current user task, and more. Driven by the huge amounts of data produced by mobile and wearables devices, and influenced by privacy concerns, the next wave in computing will need to exploit data and computing at the edge of the network. Future mobile apps will have to be cognitive to 'understand' user intentions based on all the available interactions and unstructured data. Mobile applications are becoming increasingly ubiquitous, going beyond what end users can easily comprehend. Essentially, for both business-to-client (B2C) and business-to-business (B2B) apps, only about 30% of the development efforts appear in the interface of the mobile app. For example, areas such as the collaborative nature of the software or the shortened development cycle and time-to-market are not apparent to end users. The other 70% of the effort invested is dedicated to integrating the applications with back-office systems and developing those aspects of the application that operate behind the scenes. An important, yet often complex, part of the solution and mobile app takes place far from the public eye-in the back-office environment. It is there that various aspects of customer relationship management must be addressed: tracking usage data, pushing out messaging as needed, distributing apps to employees within the enterprise, and handling the wide variety of operational and management tasks-often involving the collection and monitoring of data from sensors and wearable devices. All this must be carried out while addressing security concerns that range from verifying user identities, to data protection, to blocking attempted breaches of the organization, and activation of malicious code. Of course, these tasks must be augmented by a systematic approach and vigilant maintenance of user privacy. The first wave of the mobile revolution focused on development platforms, run-time platforms, deployment, activation, and management tools for multi-platform environments, including comprehensive mobile device management (MDM). To realize the full potential of this revolution, we must capitalize on information about the context within which mobile devices are used. With both employees and customers, this context could be a simple piece of information such as the user location or time of use, the hour of the day, or the day of the week. The context could also be represented by more complex data, such as the amount of time used, type of activity performed, or user preferences. Further insight could include the relationship history with the user and the user's behavior as part of that relationship, as well as a long list of variables to be considered in various scenarios. Today, with the new wave of wearables, the definition of context is being further extended to include environmental factors such as temperature, weather, or pollution, as well as personal factors such as heart rate, movement, or even clothing worn. In both B2E and B2C situations, a context-dependent approach, based on the appropriate context for each specific user, offers a superior tool for working with both employees and clients alike. This mode of operation does not start and end with the individual user. Rather, it takes into account the people surrounding the user, the events taking place nearby, appliances or equipment activated, the user's daily schedule, as well as other, more general information, such as the environment and weather. Developing enterprise-wide, context-dependent, mobile solutions is still a complex challenge. A system of real added-value services must be developed, as well as a comprehensive architecture. These four-tier architectures comprise end-user devices like wearables and smartphones, connected to systems of engagement (SoEs), and systems of record (SoRs). All this is needed to enable data analytics and collection in the context where it is created. The data collected will allow further interaction with employees or customers, analytics, and follow-up actions based on the results of that analysis. We also need to ensure end-to-end (E2E) security across these four tiers, and to keep the data and application contexts in sync. These are just some of the challenges being addressed by IBM Research. As an example, these technologies could be deployed in the retail space, especially in brick-and-mortar stores. Identifying a customer entering a store, detecting her location among the aisles, and cross-referencing that data with the customer's transaction history, could lead to special offers tailor-made for that specific customer or suggestions relevant to her purchasing process. This technology enables real-world implementation of metrics, analytics, and other tools familiar to us from the online realm. We can now measure visits to physical stores in the same way we measure web page hits: analyze time spent in the store, the areas visited by the customer, and the results of those visits. In this way, we can also identify shoppers wandering around the store and understand when they are having trouble finding the product they want to purchase. We can also gain insight into the standard traffic patterns of shoppers and how they navigate a store's floors and departments. We might even consider redesigning the store layout to take advantage of this insight to enhance sales. In healthcare, the context can refer to insight extracted from data received from sensors on the patient, from either his mobile device or wearable technology, and information about the patient's environment and location at that moment in time. This data can help determine if any assistance is required. For example, if a patient is discharged from the hospital for continued at-home care, doctors can continue to remotely monitor his condition via a system of sensors and analytic tools that interpret the sensor readings. This approach can also be applied to the area of safety. Scientists at IBM Research are developing a platform that collects and analyzes data from wearable technology to protect the safety of employees working in construction, heavy industry, manufacturing, or out in the field. This solution can serve as a real-time warning system by analyzing information gathered from wearable sensors embedded in personal protective equipment, such as smart safety helmets and protective vests, and in the workers' individual smartphones. These sensors can continuously monitor a worker's pulse rate, movements, body temperature, and hydration level, as well as environmental factors such as noise level, and other parameters. The system can provide immediate alerts to the worker about any dangers in the work environment to prevent possible injury. It can also be used to prevent accidents before they happen or detect accidents once they occur. For example, with sophisticated algorithms, we can detect if a worker falls based on a sudden difference in elevations detected by an accelerometer, and then send an alert to notify her peers and supervisor or call for help. Monitoring can also help ensure safety in areas where continuous exposure to heat or dangerous materials must be limited based on regulated time periods. Mobile technologies can also help manage events with massive numbers of participants, such as professional soccer games, music festivals, and even large-scale public demonstrations, by sending alerts concerning long and growing lines or specific high-traffic areas. These technologies can be used to detect accidents typical of large-scale gatherings, send warnings about overcrowding, and alert the event organizers. In the same way, they can alleviate parking problems or guide public transportation operators- all via analysis and predictive analytics. IBM Research - Haifa is currently involved in multiple activities as part of IBM's MobileFirst initiative. Haifa researchers have a special expertise in time- and location-based intelligent applications, including visual maps that display activity contexts and predictive analytics systems for mobile data and users. In another area, IBM researchers in Haifa are developing new cognitive services driven from the unique data available on mobile and wearable devices. Looking to the future, the IBM Research team is further advancing the integration of wearable technology, augmented reality systems, and biometric tools for mobile user identity validation. Managing contextual data and analyzing the interaction between the different kinds of data presents fascinating challenges for the development of next-generation programming. For example, we need to rethink when and where data processing and computations should occur: Is it best to leave them at the user-device level, or perhaps they should be moved to the back-office systems, servers, and/or the cloud infrastructures with which the user device is connected? New-age applications are becoming more and more distributed. They operate on a wide range of devices, such as wearable technologies, use a variety of sensors, and depend on cloud-based systems. As a result, a new distributed programming paradigm is emerging to meet the needs of these use-cases and real-time scenarios. This paradigm needs to deal with massive amounts of devices, sensors, and data in business systems, and must be able to shift computation from the cloud to the edge, based on context in close to real-time. By processing data at the edge of the network, close to where the i",ISEC,2016.0,10.1145/2856636.2876471,9933019,semantic_scholar
fa12864c51585278e00cb926f185a2f69b70675b,https://www.semanticscholar.org/paper/fa12864c51585278e00cb926f185a2f69b70675b,EQUIP: a Software Platform for Distributed Interactive Systems,"EQUIP is a new software platform designed and engineered to support the development and deployment of distributed interactive systems, such as mixed reality user interfaces that combine distributed input and output devices to create a coordinated experience. EQUIP emphasises: cross-language development (currently C++ and Java), modularisation, extensibility, interactive performance, and heterogeneity of devices (from handheld devices to large servers and visualisation machines) and networks (including both wired and wireless technologies). A key element of EQUIP is its shared data service, which combines ideas from tuplespaces, general event systems and collaborative virtual environments. This data service provides a uniquely balanced treatment of state and event-based communication. It also supports distributed computation – through remote class loading – as well as passive data distribution. EQUIP has already been used in several projects within the EQUATOR Interdisciplinary Research Collaboration (IRC) in the UK, and is freely available in source form (currently known to work on Windows, IRIX and MacOS-X platforms). INTRODUCTION The development of novel interactive devices and the deployment of mobile communication infrastructures have fuelled a growing focus on ubiquitous interactive systems that support people within real world environments. These systems place digital information in physical spaces [28] focusing on the delivery of information to users through a heterogeneous collection of devices ranging from handheld and wearable computers to large embedded displays. The majority of these systems have exploited a sense of location as a contextual cue to drive the interaction. An equally significant trend has been the growth in the number and diversity of collaborative virtual environments to manage cooperative interaction [2, 12, 26]. Just as ubiquitous computing environments exploit real world location, these systems exploit a sense of location within a virtual world as a contextual cue for interaction. However, despite significant similarities, these two research approaches have often tended to be seen in opposition to each other, with ubiquitous computing embedding computers with the world of users, and virtual environments embedding users within a computer generated world [16]. As part of our ongoing research we are exploring the advantages to be gained through the convergence of these approaches, allowing a collaborative virtual environment to be overlaid on top of a shared physical space. A number of key advantages motivate our desire to combine the physical and virtual to support interactive systems: The ability to exploit the coextensive virtual world as a ‘behind the scenes’ resource for coordinating and managing devices and interaction in the physical space. The opportunity to develop applications that span the physical and digital realms, for example that require collaboration between field operatives and controlroom personnel. The chance to support new kinds of interactive experience, combining elements from virtual worlds (e.g. rich media content, high interactivity) with varied modes of access over extended geographical areas and periods of time (e.g. across a city, over a period of days or weeks). Our ultimate goal is to develop a rich interactive experience that combines physical and digital space, with digital interaction becoming increasingly interwoven with everyday interaction in the physical world. This paper presents the EQUIP platform [9], developed to support the merging of physical and virtual environments as part of the EQUATOR Interdisciplinary Research Collaboration (IRC) in the UK [8]. EQUIP is freely available (including source) for other practitioners to make use of [9]. The rest of this paper gives an overview of EQUIP, and its key elements before",,2002.0,,15536154,semantic_scholar
41d472aaa26abdac72c008ffa206170e88d446e9,https://www.semanticscholar.org/paper/41d472aaa26abdac72c008ffa206170e88d446e9,Replicating augmented reality objects for multi-user interaction,"Augmented Reality (AR) is the combination of virtual objects and the physical world surrounding us. These virtual objects are used to enrich the real world. Because of technical improvements of mobile hardware, there are quite a number of AR applications deployed in the last decade. 
 
To illustrate the potential of the AR technique and to look for a new concept of human-computer interaction, we started the ’Augmented Reality for 3D Multi-user Interaction’ (ARMI) project. The goal of the ARMI project is to build an AR application where, two or more users, can work concurrently on the same virtual maquette. This maquette is visible through a Head Mounted Display (HMD) to display visual objects on top of the real world. The virtual maquette can be used for representing specific traffic situations with roads and cars but also for modelling other 3D scenarios. The following basic actions are supported: creating, selecting, moving, rotating, and deleting virtual objects. 
 
The virtual maquette, build by the ARMI project, depends on four distinct areas and we identify the following research areas: hand tracking, hand-pose estimation, 3D interfacing, and AR-object replication. This thesis looks into the possibilities of object replication for the AR application. First, a number of AR applications are considered and it becomes clear that most AR applications with multiple users are depending on a client-server approach and no object replication is used. Second, a number of replication systems are described in which more than one server is involved to keep the data consistent and a number of fundamental replication algorithms are discussed. Based on the related work the decision is made to use a speculative variant of an asynchronous majority consensus algorithm for the AR-object replication. 
 
Furthermore, in this thesis the development, implementation and evaluation of the AR-object replication is described. From the evaluation it becomes clear that it is difficult to satisfy all the required replication parameters. We notice a number of replication limits. For example a scaling problem, which means that the number of clients is limited, and specific user behaviour in terms of performed operations per second. Based on the evaluation we conclude that the relaxation of a number of replication parameters is necessary to keep the system responsive enough for an AR application.",,2009.0,,59834946,semantic_scholar
a2ca591957d1081bbf4b1a04c565b8f365c014d8,https://www.semanticscholar.org/paper/a2ca591957d1081bbf4b1a04c565b8f365c014d8,Literature Survey: A Design Approach to Smart System based on Internet of Thing (IoT) for Intelligent Transportation,"Recent years, the transportation efficiency and related issues have become one of the main focuses of the global world. Along this line, intelligent transportation systems (ITS) based on Internet of Things (IoT) provided a promising chance to resolve the challenges caused by the increasing transportation problems, such as traffic prediction, road status evaluation, traffic accident detection, etc. In this, The Internet of Things is based on the Internet, network wireless sensing and detection technologies to realize the intelligent recognition on the tagged traffic object, tracking, monitoring, managing and processed automatically. IoT based intelligent transportation systems are designed to support the Smart City vision, which aims at employing the advanced and powerful communication technologies for the administration of the city and the citizens. Keywords—IoT, transportation. I. LITERATURE SURVEY K.Ashokkumar, Baron Sam, R.Arshadprabhu, Britto [1] proposes the advances in cloud computing and web of things (IoT) have provided a promising chance to resolve the challenges caused by the increasing transportation problems. They tend to gift a unique multilayered conveyance knowledge cloud platform by exploitation cloud computing and IoT technologies to resolve the challenges caused by the increasing transportation issues. They present a novel multilayered vehicular data cloud platform by using cloud computing and IoT technologies. Two innovative vehicular data cloud services, an intelligent parking cloud service and a vehicular data mining cloud service in the IoT environment are also presented reviews. Amir-Mohammad Rahmani, Nanda Kumar Thanigaivelan, Tuan Nguyen Gia, Jose Granados, Behailu Negash, Pasi Liljeberg, and Hannu Tenhunen [2] proposes the strategic position of gateways to offer several higherlevel services such as local storage, real-time local data processing, embedded data mining, etc., proposing thus a Smart e-Health Gateway. By taking responsibility for handling some burdens of the sensor network and a remote healthcare center, a Smart e-Health Gateway can cope with many challenges in ubiquitous healthcare systems such as energy efficiency, scalability, and reliability issues. Michele Nitti, Luigi Atzori, and Irena Pletikosa Cvijikj [3] addressed the issue by analyzing possible strategies for the benefit of overall network navigability.They first propose five heuristics, which are based on local network properties and that are expected to have an impact on the overall network structure. Thet then perform extensive experiments, which are intended to analyze the performance in terms of giant components, average degree of connections, local clustering, and average path length. Jianli Pan, Raj Jain, Subharthi Paul, Tam Vu, Abusayeed Saifullah, Mo Sha [4] proposes an IoT framework with smart location-based automated and networked energy control, which uses smartphone platform and cloud-computing technologies to enable multiscale energy proportionality including building-, user-, and organizational-level energy proportionality. They further build a proof-of-concept IoT network and control system prototype and carried out real-world experiments, which demonstrate the effectiveness of the proposed solution. They envision that the broad application of the proposed solution has not only led to significant economic benefits in term of energy saving, improving home/office network intelligence, but also bought in a huge social implication in terms of global sustainability Catarinucci, L. , de Donno, D. , Mainetti, L. , Palano, L. [5] proposes a novel, IoT-aware, smart architecture for automatic monitoring and tracking of patients, personnel, and biomedical devices within hospitals and nursing institutes. Staying true to the IoT vision, they propose a smart hospital system (SHS), which relies on different, yet complementary, technologies, specifically RFID, WSN, and smart mobile, interoperating with each other through a Constrained Application Protocol (CoAP)/IPv6 over lowpower wireless personal area network (6LoWPAN)/representational state transfer (REST) network International Conference on Science and Engineering for Sustainable Development (ICSESD-2017) (www.jit.org.in) International Journal of Advanced Engineering, Management and Science (IJAEMS) Special Issue-1 https://dx.doi.org/10.24001/icsesd2017.49 ISSN : 2454-1311 www.ijaems.com Page | 195 infrastructure. The SHS is able to collect, in real time, both environmental conditions and patients' physiological parameters via an ultra-low-power hybrid sensing network (HSN) composed of 6LoWPAN nodes integrating UHF RFID functionalities. Sensed data are delivered to a control center where an advanced monitoring application (MA) makes them easily accessible by both local and remote users via a REST web service. Al-Fuqaha, A., Kalamazoo, MI, Guizani, M. , Mohammadi, M., Aledhari, M. [6] provides a more thorough summary of the most relevant protocols and application issues to enable researchers and application developers to get up to speed quickly on how the different protocols fit together to deliver desired functionalities without having to go through RFCs and the standards specifications. They also provides an overview of some of the key IoT challenges presented in the recent literature and provide a summary of related research work. Moreover, they explore the relation between the IoT and other emerging technologies including big data analytics and cloud and fog computing. They also presents the need for better horizontal integration among IoT services. Stecca, M., Moiso, C., Fornasa, M., Baglietto, P. [7] presents app execution platform (AEP), a platform that supports the design, deployment, execution, and management of IoT applications in the domain of smart home, smart car, and smart city. AEP was designed to coherently fulfill a set of requirements covered only partially or in a fragmented way by other IoT application platforms. AEP focuses on SO virtualization and on composite application (CA) orchestration and supports dynamic object availability. Yi-Bing Lin, Yun-Wei Lin, Chang-Yen Chih, Tzu-Yi Li [8] proposes an IoT device which is characterized by its “features” (e.g., temperature, vibration, and display) that are manipulated by the network applications. If a network application handles the individual device features independently, then we can write a software module for each device feature, and the network application can be simply constructed by including these brick-like device feature modules. Based on the concept of device feature, brick-like software modules can provide simple and efficient mechanism to develop IoT device applications and interactions. Ganz, F. , Puschmann, D. , Barnaghi, P. , Carrez, F. [9] provides a survey of the requirements and solutions and describes challenges in the area of information abstraction and presents an efficient workflow to extract meaningful information from raw sensor data based on the current stateof-the-art in this area and also identifies research directions at the edge of information abstraction for sensor data. To ease the understanding of the abstraction workflow process, they introduce a software toolkit that implements the introduced techniques and motivates to apply them on various data sets. Aijaz, A. , Aghvami, A.H.[10] provides the state of the art in cognitive M2M communications from a protocol stack perspective, covers the emerging standardization efforts and the latest developments on protocols for cognitive M2M networks which includes a centralized cognitive medium access control (MAC) protocol, a distributed cognitive MAC protocol, and a specially designed routing protocol for cognitive M2M networks. These protocols explicitly account for the peculiarities of cognitive radio environments. Performance evaluation demonstrates that the proposed protocols not only ensure protection to the primary users (PUs) but also fulfil the utility requirements of the secondary M2M networks. Tsirmpas, C., Anastasiou, A., Bountris, P., Koutsouris, D. [11] proposes a new methodology based on self organizing maps (SOMs) and fuzzy C-means (FCM) algorithms for profile generation as regards the activities of the user and their correlation with the available sensors. Moreover, we utilize the provided context to assign the generated profiles to more contextually complex activities. This methodology is being evaluated into an AAL structure equipped with several sensors. More precisely, they assess the proposed method in a data set generated by accelerometers and its performance over a number of everyday activities Mainetti, L., Lecce, Mighali, V. ; Patrono, L. [12] proposes a software architecture to easily mash-up constrained application protocol (CoAP) resources. It is able to discover the available devices and to virtualize them outside the physical network. These virtualizations are then exposed to the upper layers by a REpresentational State Transfer (REST) interface, so that the physical devices interact only with their own virtualization. Furthermore, the system provides simplified tools allowing the development of mash-up applications to different-skilled users. Finally, the architecture allows not only to monitor but also to control the devices, thus establishing a bidirectional communication channel. Hasan Omar Al-Sakran [13] presents a novel intelligent traffic administration system, based on Internet of Things, which is featured by low cost, high scalability, high compatibility, easy to upgrade, to replace traditional traffic management system and the proposed system can improve road traffic tremendously. The Internet of Things is based on the Internet, network wireless sensing and detection technologies to realize the intelligent recognition on the tagged traffic object, tracking, monitoring, managing and processed automatically. The paper proposes an architecture that integrates internet of things with",,2017.0,10.24001/ICSESD2017.49,96461930,semantic_scholar
a33da34007f78b7e24e08de6a3fe3a272c76ad9c,https://www.semanticscholar.org/paper/a33da34007f78b7e24e08de6a3fe3a272c76ad9c,Application of intelligent agent technology for knowledge management integration,"Organizations invest in various knowledge management (KM) systems and tools to enable seamless integration of the constantly increasing volume and sources of information. This research first, presents a case study on the existing categories of KM systems and tools, their potential contribution to the KM process, and their pitfalls; second, it proposes a comprehensive methodology for building KM through the organization using software agent technology. This approach aims to address the research issue of how KM can be optimized using intelligent agents and how to enhance decision-making process. The proposed system is applied to a real-world project lifecycle case that is EPC (Engineering Procurement and Construction) project. A prototype of the system is presented where intelligent agents are the building blocks of a peer-to-peer organization wide system. The application was implemented using Eclipse technology, and the agents were deployed on the FIPA-OS (Foundation for Intelligent Physical Agents-Open-Source) environment, we used JESS (Java expert system shell) to develop the knowledge based of the agents' reasoning.","Proceedings of the Third IEEE International Conference on Cognitive Informatics, 2004.",2004.0,10.1109/COGINF.2004.1327481,25044218,semantic_scholar
a2524b60d8c51af32ee36f9d3ccb0761c8b593f6,https://www.semanticscholar.org/paper/a2524b60d8c51af32ee36f9d3ccb0761c8b593f6,Indexing and retrieving Semantic Web resources: the RDFStore model,"The Semantic Web is a logical evolution of the existing Web. It is based on a common conceptual data model of great generality that allows both humans and machines to work with interrelated, but disjoint, information as if it was a single global database. The design and implementation of a general, scalable, federated and flexible data storage and indexing model, which corresponds to the data model of the Semantic Web, is fundamental for the success and deployment of such a system. The generality of the RDF data model presents unique challenges to efficient storage, indexing and querying engines. This paper presents our experience and work related to RDFStore which implements a new flexible indexing and query model. The model is tailored to RDF data and is designed around the Semantic Web from the ground up. The paper describes the underlying indexing algorithm, together with comparisons to other existing RDF storage and query strategies. Towards a lightweight database architecture The generality of the RDF data model presents unique challenges to efficient storage, indexing and querying software. Even if the Entity-Relational (ER) data model [1] is the dominant technology for database management systems today, it has limitations in modeling RDF constructs. RDF being unbounded, the resulting data structures are irregular, expressed using different data granularity, deeply nested or even cyclic. As a consequence, it is not possible to easily fix the ""structural view"" of a piece of information (object), which is instead one of the fundaments of traditional RDBMS systems trying to be much narrower and precise as possible and where an update not conforming to a single static schema is rejected. Database systems also optimize data storage and retrieval by knowing ahead of time how records are structured and interrelated and tend to use very inefficient nested SQL SELECT statements to process nested and cyclic structures. All this is too restrictive for RDF data. Like most semi-structured formalisms [2][3] RDF is self-describing. This means that the schema information is embedded with the data, and no a priori structure can be assumed, giving a lot of flexibility to manage any data and deal with changes in the data's structure seamlessly at the application level. The only basic structure available is the RDF graph itself, which allows describing RDF vocabularies as groups of related resources and the relationships between these resources [4]. All new data can be ""safely"" accepted, eventually at the cost of tailoring the queries to the data. On the other side, RDF data management systems must be much more generic and polymorphic like most of dynamically-bound object-oriented systems [5]; changes to the schema are expected to be as frequent as changes to the data itself and could happen while the data is being processed or ingested. A drawback of RDF heterogeneity is that the schema is relatively large compared to the data itself [6]; this in contrast to traditional RDBMS where the data schema is generally several orders of magnitude smaller than the data. This also implies that RDF queries over the schema information are as important as queries on the data. Another problem is that most RDF data (e.g. metadata embedded into an HTML page or RSS1.0 news feed) might exist independently of the vocabulary schemas used to mark-up the data, further complicating data structure ""validation"" (RDF Schema validation). This de-coupling aspect also makes the data ""de-normalization"" more difficult [7][8][9]. ""De-normalization"" is needed in RDBMS to overcome query performance penalties caused by the very general ""normalized"" schemas. De-normalization must be done taking into account to how the database will be used and how data is initially structured. In RDF this is not generally possible, unless all the RDF Schema definitions of the classes and properties used are known a-priori and available to the software application. Even if that might be the case, it is not a general rule and it would be too restrictive and make RDF applications extremely fragile. In the simplest and most general case, RDF software must associate the semantics to a given property exclusively using the unique string representation of its URIs. This will not stop of course more advanced and intelligent software to go a step further and retrieve, if available, the schema of the associated namespace declarations for validation, optimization or inference purposes. It is interesting to point out that a large part of queries foreseen for Web applications are information discovery and retrieval queries (e.g. Google) that can ""ignore"" the data schema taxonomy. Simple browsing through the RDF data itself or searching for some sub-string into literals, or using common patterns is generally enough for a large family of RDF applications. On the other hand, we strongly believe that RDBMS has proven to be a very effective and efficient technology to manage large quantities of well-structured data. This will continue to be true for the foreseeable future. We thus see RDF and similar less rigid, or semi-structured data technologies as complementary to traditional RDBMS systems. We expect to see RDF increasingly appear in the middle layer where lightweight systems that focus on interoperability, flexibility and a certain degree of decoupling of rigid formats are desired. We believe that a fundamentally different storage and query architecture is required to support the efficiently and the flexibility of RDF and its query languages. At a minimum such storage system needs to be: Lightweight Native implementation of the graph Fundamentally independent from data structure Allow for very wide ranges in value sizes; where the size distribution is not known in advance, most certainly is not Gaussian and will fluctuate wildly. Be efficient it should not be necessary to retrieve very large volumes of data in order to reconstruct part of the graph. Allow built support for arbitrary complex regular-path-expressions on the graph to match RDF queries like RDQL [50] statement triple-patterns. Have some free-text support Context/provenance/scope or flavoring of triples Furthermore given that RDF and the Semantic Web are relatively new, and will require significant integration and experimentation it is important that its technology matches that of the Internet: Easy to interface to C, Perl and Java at the very least. Ruby, Python, Visual Basic and .NET are a pre. Easy to distribute (part of) the solution across physical machines or locations in order match scaling and operational habits of existing key Internet infrastructure. Very resistant to ""missing links"" and other noise. Contexts and provenance A RDF statement represents a fact that is asserted as true in a certain context space time, situation, scope, etc. The circumstances where the statement has been stated represent its ""contextual"" information [10][11]. For example, it may be useful to track the origin of triples added to the graph, e.g. the URI of the source where triples are defined, e.g. in an RDF/XML file, when and by whom they where added and the expiration date (if any) for the triples. Such context and provenance information can be thought of as an additional and orthogonal dimension to the other components of a triple. The concept is called in the literature ""statement reification"". Context and provenance are currently not included in the RDF standardisation process [48][49], but will hopefully adressed in a next release of the specification. From the application developer point of view there is a clear need for such primitive constructs to layer different levels of semantics on top of RDF which can not be represented in the RDF triples space. Applications normally need to build meta-levels of abstraction over triples to reduce complexity and provide an incremental and scaleable access to information. For example, if a Web robot is processing and syndicating news coming from various on-line newspapers, there will be overlap. An application may decide to filter the news based not only on a timeline or some other property, but perhaps select sources providing only certain information with unique characteristics. This requires the flagging of triples as belonging to different contexts and then describing in the RDF itself the relationships between the contexts. At query time such information can then be used by the application to define a search scope to filter the results. Another common example of the usage of provenance and contextual information is about digital signing RDF triples to provide a basic level of trust over the Semantic. In that case triples could be flagged for example with a PGP key to uniquely identify the source and its properties. There have been several attempts [12][13][14][15] trying to formalize and use contexts and provenance information in RDF but a common agreement has not been reached yet. However, context and provenance information come out as soon as a real application is built using RDF. Some first examples are presented below. Our approach to model contexts and provenance has been simpler and motivated by real-world RDF applications we have developed [16a][16b][16c]. We found that an additional dimension to the RDF triple can be useful or even essential. Given that the usage of full-blown RDF reification is not feasible due to its verbosity and inefficiency we developed a different modeling technique that flags or mark a given statement as belonging to a specific context. First example considers subjective assertions. The Last Minute News (LMN) [16b] and The News Blender (NB) [16c] demos allow an user rating and qualifying the source newspapers. The user can ""say"" that a newspaper is ""liberal"" or ""conservative"". Of course, two users, X and Y, will show two different opinions. Without considering the context, this will result in two triples: Newspaper A -> Quality -> ""liberal"" Newspaper A -> Qu",,2003.0,,15151937,semantic_scholar
a895c3e93c9097eaf4269e3ec246249d89a54436,https://www.semanticscholar.org/paper/a895c3e93c9097eaf4269e3ec246249d89a54436,Designing Pervasive Services for Physical Hypermedia,In this paper we describe the design and implementation of a software substrate for building pervasive services in the context of physical hypermedia applications. We first introduce the main ideas behind physical hypermedia; next we argue that physical navigation requires some software support to improve accessibility to real world objects. We next describe an architectural framework that supports specification and deployment of pervasive services. Some simple examples of use are presented. We conclude by comparing our work with others' and describing further work we are pursuing,2006 ACS/IEEE International Conference on Pervasive Services,2006.0,10.1109/PERSER.2006.1652238,18418153,semantic_scholar
c30ab40bc2a9daa1882837560bc688997bf90f21,https://www.semanticscholar.org/paper/c30ab40bc2a9daa1882837560bc688997bf90f21,Unified Software Engineering with Java,Preface 1. Introduction to Java in the Context of Software Engineering 2. Experimenting With Classes and Objects 3. The Structure and Syntax of Java 4. Design and Development of Java Applications 5. Architecture-Driven Component Development 6. Introduction to Distributed Computing Concepts 7. Interfacing with Users 8. Implementing Java Programs 9. Software Quality Assurance 10. Information Management in Java 11. Reality Check: Java Programs in the Real World 12. Software Integration and Deployment 13. Java on Various Computer Platforms 14. Advanced Topics in Java Software Engineering 15. The Unified Modeling Language: A Primer,,2006.0,,60618591,semantic_scholar
88e866eaab83d42f392182697c3075ece34c9c62,https://www.semanticscholar.org/paper/88e866eaab83d42f392182697c3075ece34c9c62,The Six Pillars of Simulation Architecture,"This paper addresses simulation architecture for real-time Man-In-The-Loop (MITL) and Hardware-In-TheLoop (HITL) simulation laboratories, as used by the Lockheed Martin Aeronautics Company to support the full life cycle for aircraft and air system products. The paper discusses concepts and considerations used to establish the architecture for simulation and systems integration laboratories. The subject is presented via discussion of six pillars of simulation architecture: Composition, Functionality, Structure, Behavior, Mechanization, and Doctrine. The foundation for these pillars, the System Design Process, is also discussed. For each of these major elements, architectural goals and products are reviewed, and some real examples from major programs are provided. Biography Barry Evans is a Senior Fellow and Chief Engineer for Simulation and Systems Integration Laboratories at the Lockheed Martin Aeronautics Company. Mr. Evans provides technical leadership for simulation and integration labs across all programs and sites within the company. Prior to this role, Mr. Evans served two years as the Acting Director for F-35 Laboratories, during which time he successfully led a major reorganization and technical re-plan that resulted in substantial cost reduction and resolution of technical challenges. Prior to this assignment, he led a major program re-plan and served as air vehicle integration manager for the C-5 RERP Program. Mr. Evans was the lead architect for a common suite of laboratory software, hardware, standards, processes, and paradigms, deployed across program domains, including F-35, F-22, C-5, C-130, C-27, P-3, S-3, CRAD, and IRAD programs. He served as system architect and/or project manager for numerous simulation and systems integration projects. His design and development experience includes: operating systems; simulation executives; I/O; air vehicle and mission systems models; tactical combat simulation; OFP re-hosts; visual/audio/motion/feel cueing systems; and hardware-in-the-loop stimulation. Mr. Evans holds a BSEET from Southern Polytechnic State University, and he completed course work in Flight Dynamics from Kansas State University. Introduction The subject of simulation architecture is one that is both broad and deep, and the term “simulation architecture” can summon different views and meanings, depending on the focus of the individual. Additionally, there are different types of simulation and a variety of applications, which can influence what is architecturally important in the simulation. So, in order to discuss simulation architecture, it is necessary to describe the application context and provide a definition of what is intended by the term “architecture”. The Lockheed Martin Aeronautics Company utilizes real-time Man-In-The-Loop (MITL) and Hardware-InThe-Loop (HITL) simulation laboratories to support all phases of air system development and sustainment, including: concept exploration, theater-level analysis, system trade studies, system requirements development, design evaluation, system developmental testing, integration testing, system verification and validation (V&V), and training/familiarization. The simulations employed range from relatively small, low fidelity implementations that may focus on one element of one aircraft, to very large scale, full theater simulations that represent thousands of real-world entities (aircraft, ground vehicles, elements of the environment, weapons, etc.) with very high fidelity. Some of the simulations are oriented about human interaction with simulated systems (such as the airframe and flight controls, pilot-vehicle interface, mission systems, weapon systems, etc.), while other simulations are focused about stimulating air system hardware for integration and verification testing. The discussion below is in the context of this wide range of simulation applications. For the discussion herein, simulation architecture is defined as the design, structure, organization, and behavior of a simulation. The architecture addresses hardware, software, intellectual attributes, human interaction, and/or anything critical to the simulation design. The term “simulation architecture” may apply at different levels within a simulation, including: component, subsystem, system, and system-of-systems. The following discussion is focused on the system-level design of a simulation, addressing the information necessary to define the simulation blueprint, inclusive of component identification and interfaces, but exclusive of component-level internal design. Simulation architecture may be viewed in terms of six pillars (or elements): Composition, Functionality, Structure, Behavior, Mechanization, and Doctrine. Refer to Figure 1. In this view, the six pillars provide direct support of the simulation architecture, and the system design process is portrayed as the underlying foundation for the pillars. A discussion of the goals and products for each of these elements allows the highlighting of various key concepts and considerations that go into the development of a simulation architecture. The six pillars are not meant to represent phases or steps of architectural development, but rather a convenient way to break down and discuss the complex subject of simulation architecture. Also, it should be noted that just as the structural load of a building section may be supported by more than one physical pillar, a product or artifact of simulation architecture may be supported by, (or be the result of), the goals and activities from more than one of the architectural pillars discussed herein. Additionally, it is worth noting that most of elements of system design discussed herein could be applicable to any software-intensive system design. But, the descriptions are cast in the context of simulation, and they include some simulation-specific architectural considerations. Figure 1: The Six Pillars of Simulation Architecture",,2014.0,10.2514/6.2014-2204,61230373,semantic_scholar
4a99aa2ca1dd85ac7b82d79bf8cec1a09c9d8488,https://www.semanticscholar.org/paper/4a99aa2ca1dd85ac7b82d79bf8cec1a09c9d8488,Energy Efficient protocol design in Wireless Sensor Networks – Contributions to make the Ubiquitous Platform Greener,"s all hardware resources as components. For example, calling the getData() command on a sensor component will cause it to later signal a dataReady() event when the hardware interrupt fires. While many components are entirely software based, the combination of split-phase operations and tasks makes this distinction transparent to the programmer. In both cases an event signals that the encryption operation is complete. ADC, ClockC, UART, SlavePin and SpiByteFifo are example hardware abstraction components. TinyOS commands and events are very short, due to limited code space and a finite state machine style of decomposition. The rich event processing model means an event or command call path can traverse several components. The TinyOS component model allows us to easily change the target platform from mote hardware to simulation by only replacing a small number of low-level components. The event-driven execution model can be exploited for efficient eventdriven simulation, and the whole program compilation process can be re-targeted for the simulator‟s storage model and native instruction set. The static component memory model of TinyOS simplifies state management for these large collections. Setting the right level of simulation abstraction can accurately capture the behavior and interactions of TinyOS applications. Figure 1.3: TinyOS Structure (Consist of scheduler and graph of components) 2.3 TOSSIM: A Simulator for TinyOS Sensor Networks The Necessity of Network Simulation: The emergence of wireless sensor networks brought many open issues to network designers. Traditionally, the three main techniques for analyzing the performance of wired and wireless networks are analytical methods, computer simulation, and physical measurement. However, because of many constraints imposed on sensor networks, such as energy limitation, decentralized collaboration and fault tolerance, algorithms for sensor networks tend to be quite complex and usually defy analytical methods that have been proved to be fairly effective for traditional networks. Furthermore, few sensor networks have come into existence, for there are still many unsolved research problems, so measurement is virtually impossible. It appears that simulation is the only feasible approach to the quantitative analysis of sensor networks. The event-driven nature of sensor networks means that testing an individual mote is insufficient. Programs must be tested at scale and in complex and rich conditions to capture a wide range of interactions. Deploying hundreds of motes is a daunting task, the focus of work shifts from research to maintenance, which is time-consuming due to the failure rate of individual motes. A simulator can deal with these difficulties, by providing controlled, reproducible environments, by enabling access to tools such as debuggers, and by postponing deployment until code is well tested and algorithms are understood. TOSSIM: TOSSIM is a discrete event simulator for TinyOS sensor networks. Instead of compiling a TinyOS application for a mote, users can compile it into the TOSSIM framework, which runs on a PC. This allows users to debug, test, and analyze algorithms in a controlled and repeatable environment. As TOSSIM runs on a PC, users can examine their TinyOS code using debuggers and other development tools. TOSSIM‟s primary goal is to provide a high fidelity simulation of TinyOS applications. For this reason, it focuses on simulating TinyOS and its execution, rather than simulating the real world. While TOSSIM can be used to understand the causes of behavior observed in the real world, it does not capture all of them, and should not be used for absolute evaluations. Related Publication: Swarup Kumar Mitra, Ayon Chakraborty, Subhajit Mandal and M.K.Naskar, Simulation of Wireless Sensor Networks using TinyOS A Case Study, In the Proceedings of the National Conference on Modern Trends in Electrical Engineering, pages EC 23 EC 26, Hooghly, West Bengal, July 2009. 3 Data Gathering Schemes in WSNs Data gathering is by far one of the most important aspects of research considering energy efficiency in the routing protocols for wireless sensor networks. Wireless sensor networks have emerged as a ubiquitous platform recently, and issues regarding the efficiency of energy usage by these devices play a very important role. These devices are equipped with negligible or less amount of battery power to sustain for a long time. Not only that, in most of the scenarios, where these networks are deployed it is infeasible or impossible sometimes to replace the battery power of the sensor nodes. One of the most fundamental aspects for energy consumption in sensor nodes is communication, other than sensing and computation costs. Optimization of communication costs is thus essential, which is a direct consequence of betterment of routing techniques in this type of wireless networks. A major portion of my contribution in this project deals with designing data gathering schemes for wireless sensor networks and optimization of routing techniques, described below. The first in this queue was the HDS or “Hybrid Data Gathering Scheme”. Published in the International Conference of Distributed Computing and Internet Technology (ICDCIT‟10), this work is a novel approach in minimizing not only the communication / energy overhead but also guarantees a minimal energy-latency product. It also distributes the energy consumption by the nodes by rotating the leader node, so as to increase the uniformity of energy content in the nodes. The uniform distribution of energy content in the nodes also helps to lessen the chances of a black hole or a sinkhole problem. The HDS protocol is based on the hybrid combination of two algorithms, SHORT and LBERRA. The LBERRA scheme is used to subdivide the sensor field into predefined clusters, and SHORT is applied to form a binary tree spanning the nodes. There are two types of leader nodes: one for each cluster, forming the root of the tree and the other one is the „sink‟ communicating the gathered data to the Base Station. In each of the data gathering rounds the leader node is changed The second work was related to optimization of routing chain through heuristic techniques. Firstly, I applied Particle Swarm Optimization to create the most energy efficient paths for communication in the sensor field. Then, I investigated the use of Genetic Algorithms (hybridized with simulated annealing) in solving the same problem. In these works, I not only devised the algorithm for the minimum-energy path formation, but also coded it in nesC discussed in the earlier section. The implementation and simulation in nesC guarantees the hardware feasibility of the algorithm in sensor nodes. Packet loss rates were also studied with varying network topology and signal strengths in communication between particular sensor nodes. In all the cases, a standard background noise was considered. This work followed a series of publications including three international conferences and two international journals. An extension of this work was to create energy efficient data gathering trees. Most algorithms developed in literature used greedy algorithms to construct routing trees which in most of the cases did not result in near-optimal energy usage. “ROOT” or “ROuting through Optimized Trees” was an answer to this need. Related Publications: International Conference: 1. Ayon Chakraborty, Kaushik Chakraborty, Swarup Mitra and Mrinal Naskar, An Optimized Lifetime Enhancement Scheme for Data Gathering in Wireless Sensor Networks, in the proceedings of The Fifth IEEE Conference on Wireless Communication and Sensor Networks, WCSN'09 Allahabad, India, (December, 2009). 2. Ayon Chakraborty, Swarup Mitra and Mrinal Naskar, An Efficient Hybrid Data Gathering Scheme in Wireless Sensor Networks, in the proceedings of The Sixth International Conference on Distributed Computing and Internet Technology, ICDCIT'10, Bhubaneswar, India. (February, 2010). 3. Ayon Chakraborty, Swarup K. Mitra and M.K. Naskar, Energy Efficient Routing in Wireless Sensor Networks: A Genetic Approach, in the Proceedings of the International Conference on Computer Communications and Devices (ICCCD 2010), IIT Kharagpur (December, 2010) 4. Kaushik Chakraborty, Ayon Chakraborty, Swarup Mitra and Mrinal Naskar, ROOT: Energy Efficient Routing through Optimized Tree in Sensor Networks, in the proceedings of The International Conference on Computer Communications and Devices – ICCCD'10, Kharagpur, India. (December, 2010).",,2012.0,,17171658,semantic_scholar
5c666ddee49c09fad988c2d7ec468ae5baf280e7,https://www.semanticscholar.org/paper/5c666ddee49c09fad988c2d7ec468ae5baf280e7,Workshop Mobile and Embedded Interactive Systems ( MEIS ' 06 ),"Automatic identification technology such as RFID promises to connect physical objects with virtual representations or even computational capabilities. However, even though RFID tags are continuously falling in price, their widespread use on consumer items is still several years away, rendering large-scale experiments with such an “internet of things” difficult. Much more ubiquitous are printed bar codes, yet so far their recognition required either specialized scanner equipment, custom-tailored bar codes or costly commercial licenses – all equally significant deployment hurdles. We have developed a freely available EAN-13 bar code recognition and information system that is both lightweight and fast enough for the use on camera-equipped mobile phones, thus significantly lowering the barrier for large-scale, real-world testing of novel information and interaction applications based on “connected” physical objects. We hope that this “low tech” version of bridging the gap will allow the community to quickly develop and try out more realistic and widespread applications, and thus gain real-world experiences for better jump-starting the future internet of things, today. 1 Today’s Role of Barcode Recognition The idea of linking real-world products with virtual information has been around for quite some time. In 1998, Barrett and Maglio already described a system for attaching information to real-world objects [BM98], while 1999 Want et al. expanded upon the idea and linked arbitrary items through the use of RFID tags with both information services and actions [WFGH99]. Since then, a number of research projects have continued to explore this concept of “bridging the gap”, i.e., the automatic identification of individually tagged real-world products in order to quickly look up information or initiate a specific action [KBM02]. With the increasing mobility of powerful computing systems, e.g., mobile phones or handheld PDAs, this bridging can even be done in situ, i.e., right when we need it, where we need it. While RFID potentially offers an unprecedented user experience due to its detailed means for identification (i.e., on a per item basis) and the lack of a line-of-sight requirement for reading, most industry analysts agree that an item-level rollout (e.g., having an RFID tag on every single supermarket product) is still several years away [Jue05]. In contrast, the printed bar codes are practically ubiquitous: Virtually every item sold today carries an internationally standardized bar code on its packaging, enabling not only checkout registers to quickly sum up one’s shopping items, but also to identify a product and look up a wealth of related information. Obviously, using bar codes for linking real-world objects to virtual information has a number of drawbacks when compared to an RFID-enabled future with corresponding mobile RFID readers, such as NFC-enabled1 mobile phones. Due to their sensitivity to soiling, ripping, and lighting conditions, optical bar code recognition can be difficult. Until recently, reading a conventional (i.e., 1D) bar code inevitably required a separate laser scanner or a corresponding mobile phone scanner attachment. The increasing availability of camera phones, i.e., mobile phones with an integrated digital camera, has begun to simplify this process, however. After 2D bar codes have been successfully recognized by most consumer-grade camera phones for quite some time [Roh04], the continuously increasing quality of both the camera resolution and the employed lenses have finally made it feasible to directly read 1D bar codes with such cameras, without the need for special attachments or handheld lasers. This significantly changes the attractiveness of using barcodes for the above physical-to-digital linkage: Instead of waiting several years for a comprehensive item-level roll out of RFID tags, or forcing people to carry around specific scanner attachments for their mobile phones, the support of 1D bar code recognition on any camera phone immediately allows anybody to interact with almost any commercially available product – all it takes is a small application download. The main contribution of this paper is a freely available 1D bar code recognition toolkit that is intended to facilitate the creation of novel applications and services. We believe that the adequate performance of our recognition software, when compared with existing commercial implementations, the ease with which external data sources can be integrated, and the availability of our toolkit under an open source license will help to foster the the use of camera phones as mobile bar code scanners.",,2006.0,,16944891,semantic_scholar
9c97ed6973ac9c1555e29124642c546464bc85ac,https://www.semanticscholar.org/paper/9c97ed6973ac9c1555e29124642c546464bc85ac,Engaging in a Conversation with Synthetic Agents along the Virtuality Continuum,"During the last decade research groups as well as a number of commercial software developers have started to deploy embodied conversational characters in the user interface especially in those application areas where a close emulation of multimodal human-human communication is needed. Incarnations of such characters differ widely in type and amount of embodiment - starting from simplistic cartoon-style 2D representations of faces, fully embodied virtual humans in 3D virtual worlds to physically embodied androids co-habiting the user's real world. Despite of their variety, most of these characters have one thing in common: In order to enter the user's physical world, they need to be physical themselves. My talk focuses on challenges that arise when embedding synthetic conversational agents in the user's physical world. Following [4], we may classify the contact between synthetic and human agents according to a ""virtuality continuum"" (see Fig. 1). At one extreme, we find android agents that are completely integrated in the user's physical world and even allow for physical contact with the user. Mel, a robotic penguin developed by Sidner and colleagues [5] (see image 1 in Fig. 1), is one of the most sophisticated physical agents that engages in face-to-face communication with a human user. At the other extreme, there are purely virtual environments that are populated by human and synthetic agents. A prominent example is the pedagogical agent Steve [3] (see Image 4 in Fig. 1). Steve is aware of the user's presence in the virtual space, monitors her actions and responds to them, but has no access to the external world. That is it is only able to perceive user actions that are performed in the virtual space. In between, we find a new generation of characters that inhabit a world in which virtual and digital objects are smoothly integrated. In these applications, projections of virtual characters overlay the user's physical environment or projections of real persons are inserted into a virtual world. For instance, Cavazza and colleagues [2] propose a magic mirror paradigm which puts the user both in the role of an actor and a spectator by inserting the user'svideo image in a virtual world that is populated by synthetic agents (see Image 3 in Fig. 1). In the Virtual Augsburg project (see [1]), a synthetic character called Ritchie jointly explores with the user a table-top application that combines virtual buildings of the city center of Augsburg with a real city map being laid out on a real table. Most work so far has concentrated on the design and implementation of conversational agents at the two extremes of the Virtuality Continuum. In my talk, I will report on a new generation of synthetic characters that are no longer bound to a flat screen, but able to enter a physical world and to engage in a conversation with a human user. Users and characters do not inhabit separated spaces, but share an informational and physical reality that is augmented by digital objects. As a consequence, communication has to take into account both the physical and the digital context. New forms of deixis are enabled by the manipulation of objects and movements of characters in the physical space. Further challenges arise from the realization of so-called traversable interfaces that allow human and synthetic agents to cross the border from the digital to the real world and vice versa.",2006 IEEE/WIC/ACM International Conference on Intelligent Agent Technology,2006.0,10.1109/IAT.2006.62,9567460,semantic_scholar
c55e4acdf98b1931bf1e00280639348d51a6283a,https://www.semanticscholar.org/paper/c55e4acdf98b1931bf1e00280639348d51a6283a,A Hierarchical Collective Agents Network for Real-time Sensor Fusion and Decision Support,"This research addresses a problem of how to make effective use of real-time information acquired from multiple sensor and heterogeneous data resources, and reasoning on the gathered information for situation assessment and impact assessment (SA/IA), thus to provide reliable decision support for time-critical operations. A hierarchical collective agents network (HCAN) is employed as a solution to this problem. The agents network supports multi-sensor registration, real-time sensor/platform cueing, level-2 and level-3 information fusion, and has an arm toward the level-4 fusion objectives. An agent component assembly and decision-support-system-development environment, the 21 Century systems’ AEDGE software package, is used for the design and implementation of a HCAN-DSS system. The ability to integrate and correlate a vast amounts of disparate information from multiple sensor and heterogeneous data resources with varying degrees of uncertainty in real-time is an impediment issue for missioncritical decision support systems (DSS). For example, in crucial military operations command officers need real-time information and intelligences from various sensors/data resources in a theater of reconnaissance and surveillance to build a whole picture of the battle-space. It is critical for the commanders to know and to understand the relationships among the information collected. Questions are asked: what are the physical and functional constituencies among the objects in a given geographic sector? Are there sequential or temporal dependencies of the objects and what will trigger them? What are the possible consequences of the action and re-actions? Decision making based on these situation assessment and impact assessment (SA/IA) are particularly important for identifying and prioritizing “gaps” between the operation planning and the real-time interactions. To support making effective SA/IA, a data fusion and decision support system is required to use a set of coherent patterns derived from the available data sets and infer the implications (e.g., causal relations) toward the real world situations. The attribute coherence that is critical to the formation of the meaningful knowledge patterns is often obscure in the data sets obtained from heterogeneous resources. The data collections are often incomplete, imprecise, and inconsistent due to various natural constraints and human faults. Decision makers naturally desire to access large quantities of information expressed in diverse forms. However, as new sensor technology and various information sources have combined to create quantity and diversity, it has become increasingly difficult to provide decision makers with the right information at the right time and in the right quantity and format. Real-time computerized decision support systems are constructed by integrating a number of diverse components from a variety of software modules. Software developers have come to a numerous ways of querying the local and centralized data resources to access and distill the large and diverse information for the purpose of providing effective decision support. Meanwhile DSS are becoming more and more complex in terms of knowing which data resources to connect, how to keep track of the data dynamics, and assess the reliability of information from each resources. These tying links make the use of intelligent agent architecture necessary and desirable for allowing real-time responsibility and adaptive control of the DSS. Many popular agent systems of today deploy agents in a uniform level of operation. The agents respond to the same calls and cooperate at the same time toward the same goal of operation. The architecture endues some difficulties in agent communications and task control. When applied in complex real-time DSS with intensive human and system interactions, the cooperative nature makes the system less robust because the disability of one agent would affect the successive operations of the entire agent assembly. The collective nature of the agents in a HCAN paradigm overcomes some of these difficulties, for example, relieving the burden of data-exchanges between fellow agents by limiting agent communication to vertical layers of the assembly only. The hierarchical architecture simplifies the functional design of the agent interactions and enhances the security and efficiency of the process. The HCAN architecture also strikes a balance between the centralized control and distributed computation by allowing distributive agent operations within layers of the hierarchy and enforcing centralized control between the layers of the hierarchy, thus creating a federated agents integration structure. Basically, the HCAN has the functionalities of. 1). A flexible software architecture for accommodating system augmentation and evolutions; 2). A powerful representation schema for accommodating heterogeneous forms of information; 3). A diverse interface for various input resources, output formats, and human interactions; From: AAAI Technical Report WS-02-15. Compilation copyright © 2002, AAAI (www.aaai.org). All rights reserved. 4). An ability of reasoning on incomplete and inconsistent information, and extracting useful knowledge from the data of heterogeneous resources; 5). An ability of incorporating real-time dynamics of information resources into system at time of operation, and promptly adjusting the reasoning mechanisms; 6). An ability of summarizing and refining knowledge extracted, and distinguishing mission and time critical knowledge from insignificant and redundant ones; 7). A capability of supplying meaningful and accurate explanations, both qualitatively and quantitatively, of the automated system actions; and 8). A capability of providing adequate control and scrutiny of the system operations w.r.t. environment constrains. There are many sources of uncertainty at different levels of the decision support. For example, even if a situationassessor is aware of the presence of certain objects in the operation space, such as the type of contact, intention, reaction rational, etc., the exact dynamics of the object is still uncertain to the decision maker. While the knowledge about the object dynamics is critical in constructing an optimal strategy of action, various statistical methods and knowledge discovery techniques are applied in the reasoning module. The level of uncertainty forces the reasoning agents to operate with different decision strategies. The 21 Century Systems, Inc. has developed the AEDGE as an open DII-COE and CORBA compliant agentbased environment that enables the development of components-based agent systems. The system is implemented in JavaTM, with Java Database ConnectivityTM for DB access, Java Swing, AWT, and Java3D for visual interfaces, Java Media Framework and Java Speech API for audio/speech interface. AEDGE defines Agents, Entities, Avatars and their interactions with each other and with external sources of information. This standardized architecture allows additional components, such as servicespecific DSS tools, to be efficiently built upon the core functionality. Common interfaces and data structures can be exported to interested parties who wish to extend the architecture with new components, agents, servers, or clients. When the core AEDGE components are bundled with customer-specific components, a clean separation of those components, through APIs, is provided. The AEDGE is based on an extensible multi-component DSS architecture (EMDA, also referred to as the AEDGETM Architecture). The architectures describe the data objects, interfaces, communication mechanisms, component interactions, and integration mechanisms for the AEDGE and its extensions. In the AEDGE architecture, components communicate among each other via the Service Provider/Service Requester Protocol (SPSR). Service providers are components that implement an algorithm or need to share their data (data sources). Service requesters are the components that need a function performed for them by some other component or need to import data from another component. Both service requesters and service providers implement remote interfaces, which enables such components to communicate over a TCP/IP network. The remote interface implementation is currently based on Java RMI (remote method invocation), though the Architecture is not dependent on this implementation. AEDGE provides multiple levels of customization. The subject-matter users are able to build scenarios and scripts or to automatically generate them using the AEDGE-based Scenario Editor. Rules and triggers for agent behaviors can be created and modified by the advanced user. AEDGE also provides APIs for custom extensions of agents, data bridges, and the entity framework. The practical user will enjoy AEDGE’s versatile data connectivity and its near-real-time execution and monitoring of DSS functions. As a built-in bonus, AEDGE provides connections to a number of simulators and data formats, including HLA, DIS, DTED, DBDB2, XML, as well as support for multiple modes of distribution (CORBA, RMI, TCP/IP). As an example of the HCAN design using AEDGE for data fusion and DSS applications, the Advanced Battlestation with Decision Support System (ABS/DSS) which was developed as an operational agent-based C2 team decision support platform for command and control centers aboard aircraft carriers.. The ABS/DSS is based on AEDGE’s implementation of HCAN, and provides consolidated situational awareness through real-time, interactive, agent based decision support coupled with a linked 2D/3D battlespace visualization. Additionally, the ABS/DSS supports shipboard distributed training, train-asyou-fight, with a built-in scenario construction and emulation of friendly and hostile entities. Whether the watchstander is in live-feed mode or in training mode the operation of the agent-based decision support system and the 2D/3D visualization is identical. ",,2002.0,,8602112,semantic_scholar
398d9928ef1e9b18ee4e3876d3ea22b1d59ecf74,https://www.semanticscholar.org/paper/398d9928ef1e9b18ee4e3876d3ea22b1d59ecf74,Architectures for system-level applications of adaptive computing,The mission of the Systems-Level Applications of Adaptive Computing (SLAAC) project is to design and implement a distributed adaptive computing systems architecture. This systems-level focus of SLAAC resulted from the realization that scalability and portability are the two main obstructions preventing innovative Adaptive Computing Systems (ACS) research from being directly useful in deployed real-time environments. Scalability is an issue in that many real-world applications are larger than the modern PCI-based FPGA accelerator. Transitioning from a small proof of concept demonstration to large real-world application is often overlooked in ACS research. Portability has both a hardware and software aspect. Physical form-factor and operating system issues can limit the utility ACS research done in the lab with desktop PCs unless there is a development path to more traditional real-time environments. The SLAAC project seeks to remedy these issues of scalability and portability in ACS systems.,Seventh Annual IEEE Symposium on Field-Programmable Custom Computing Machines (Cat. No.PR00375),1999.0,10.1109/FPGA.1999.803693,206656676,semantic_scholar
f6ca00b9e3901f948d7eb6635f407cc3c7a7a6ed,https://www.semanticscholar.org/paper/f6ca00b9e3901f948d7eb6635f407cc3c7a7a6ed,Enterprise Security: The Manager's Defense Guide,"Preface. I. THE FORGING OF A NEW ECONOMY. 1. What is E-Business? The E-Business Sweepstakes. Caesars of E-Business: An Embattled Business Culture. The Lure of Overnight Successes. Crossing the Digital Chasm. The Sobering Reality. Real-World Examples. E-Business: The Shaping and Dynamics of a New Economy. The E-Business Supply Chain. Related E-Business Trends. Summary. 2. What Is E-Security? E-Security at Your Service. Demands on Traditional IT Security: A Changing of the Guard. Principles of E-Security. Risk Management in the New Economy. How E-Security Enables E-Business. The E-Security Dilemma: Open Access versus Asset Protection. 3. The Malicious Opponents of E-Business. The Lure of Hacking. Hackers versus Crackers. Hacker Groups. Why Hackers Love to Target Microsoft. Meeting the Hacker Threat. National Infrastructure Protection Center. Central Intelligence Agency. Other White Hats. II. PROTECTING INFORMATION ASSETS IN AN OPEN SOCIETY. 4. A New Theater of Battle. From the Demilitarized Zone and the Perimeter to Guerilla Warfare. The Triumph of Intranets, Extranets, and Virtual Private Networks. The Vanishing World of Controlled, or Closed, Access. The Impact of Open Access. The Correlation between Open Access and Asset Protection. The Role of Authentication and Privacy in the New Economy. Summary. 5. Reempowering Information Technology in the New Arms Race. The Failings of the Old Paradigm. Infiltration of Rogue Applets. Human Error and Omission. Ongoing Change in the Enterprise Network. Deploying and Maintaining Complex Layer Client/Server Software. Shortage of Human Capital. Rigidity of Enterprise Security Policy. Tools for Rearming the IT Manager. Guidelines for E-Security. Enterprise Security Policy. Summary. III. WAGING WAR FOR CONTROL OF CYBERSPACE. 6. Attacks by Syntax: Hacker and Cracker Tools. Inherent Shortcomings of TCP/IP. Standard ""Ports"" of Call. TCP/IP Implementation Weaknesses. IP Spoofing. Distributed Denial-of-Service Attacks and Tools. Trin00. Tribe Flood Network. Tribe Flood Network 2000. Stacheldraht. ICMP Directed Broadcast, or Smurf Bandwidth Attack. Backdoor Programs and Trojan Horses. Backdoor Program Functions. Examples of Backdoor Programs. Summary. 7. Attacks by Automated Command Sequences. Script Attacks. The Next Generation of E-Mail Attacks. The Bubble Boy Virus. Mainstream JavaScript Attacks. Attacks through Remote Procedure Call Services. Brown Orifice. Summary and Recommendations. 8. Countermeasures and Attack Prevention. Surviving an Attack. Formulate an Emergency Response Plan and an Incident Response Team. Obtain Outside Assistance. Contact Law Enforcement Authorities. Use Intrusion Detection System Software. Countering an Attack. Disconnect Compromised Host/System from Your Network. Copy an Image of the Compromised System(s). Analyze the Intrusion. Recognizing What the Intruder Leaves Behind. 9. Denial-of-Service Attacks. Effects of DoS and DDoS Attacks. General Computing Resources. High-Performance Firewall. Network Bandwidth. Handling a SYN Flood DDoS Attack. Countermeasures. Precautions. Handling a Bandwidth DDoS Attack. Guarding against Being an Accomplice Network. Guarding against Becoming an Intermediary Network. Guarding against Being a Victim. Handling a UDP Flood Bomb. Using an IDS. Recovering from a DDoS Attack. 10. Creating a Functional Model for E-Security. Developing a Blueprint for E-Security. Understanding Business Objectives. Honing in on Your IT Security Policy. Making Good on IT Security's Best Practices. The IT Security Functional Model. Deploying Effective E-Security Architecture: Hardening the Network's Infrastructure. Hardening Your Router. Hardening Your Operating Systems. Summary. 11. Building a Security Architecture. Firewall Architecture Deployment, Controls, and Administration. Types of Firewalls. Hardening Firewalls. Remote-Access Architecture. Encryption Options for Administrators. Securing Remote-Administration Pipes for Administrators. Remote-Access Architecture/Solutions for Users. Vulnerability Assessment Architecture/Solutions. Network-Based Assessment Architecture. Host Vulnerability Assessment. Intrusion Detection Architecture. Network-Based IDS Architecture. Host-Based IDS Solutions. IV. ACTIVE DEFENSE MECHANISMS AND RISK MANAGEMENT. 12. Vulnerability Management. Types of Vulnerabilities. Managing IT Systems Vulnerabilities. Conducting Vulnerability Analysis. Network-Based Vulnerability Analysis. Host-Based Vulnerability Analysis. 13. Risk Management. The Role of Assessment in Risk Management. The Process of Risk Management. Defining the System Boundaries. Threat Analysis. Impact Analysis. Risk Determination. Summary. Appendix A: SANs/fbi Top 20 Internet Security Vulnerabilities. Appendix B: Sample CERT/Coordination Center Incident Response Form. Appendix C: Windows 2000 Security/Hardening Plan. Appendix D: Denial-of-Service Attacks. Glossary. Bibliography. Index. 020171972XT08282002",,2002.0,,106754877,semantic_scholar
6f3834ab566c206d150bd98b2f1e085a9639a185,https://www.semanticscholar.org/paper/6f3834ab566c206d150bd98b2f1e085a9639a185,Unleashing the Power of Wireless Networks Through Information Sharing in the Sensor Internet,"We provide in this presentation in a first part an overview of the research activities of the Swiss National Competence Centre in Research for Mobile Information and Communication Systems (NCCR-MICS) in the area of self-organizing, wireless networks. In the second part we present specific MICS research results from our research group on managing information generated in such networks using self-organizing, logical overlay networks. 
 
Recent advances in wireless communication enable the embedding of sensing and actuation technology into our physical environment at an unprecedented large scale and fine granularity. We show exemplary recent theoretical advances and systems developments on self-organizing, wireless sensor networks and mobile ad-hoc networks achieved in MICS. They provide evidence for the comprehensive scope and high degree of interdisciplinarity required in this area of research. We illustrate the deployment of the resulting technologies in real-world applications. An application class we focus in MICS in particular concerns the monitoring of various typical physical phenomena in the Swiss environment, such as watershed, permafrost, and avalanches. 
 
In the long term, the increasing deployment and application of wireless networks beyond specialized, isolated applications will lead to the production of massive amounts of sensor data requiring further processing support and proper interpretation of data. We argue that self-organizing, logical overlay networks for resource and information sharing will play an important role for achieving this task. Structured overlay networks will be used to support scalable processing of data streams. Semantic overlay networks will be used to overcome heterogeneity in information representation. Finally, social overlay networks will be used to form agreements on meaning and utility of data. We illustrate these developments from our ongoing research: Global Sensor Network, a lightweight implementation of an overlay network for sensor data stream sharing, PicShark, a peer-to-peer image sharing system with support for automated generation and sharing of image annotations, and Semantic Gossiping, a social mechanism based on belief propagation to reconcile heterogeneous annotation schemes. 
 
As a result of these developments, we envision the Internet to develop into a Sensor Internet in which physical reality, information technology and human activity become increasingly intertwined into one common complex system for better understanding and more easily mastering the environment we live in.",EWSN,2006.0,10.1007/11669463_3,34316684,semantic_scholar
e61e75ec370207ed0dca2e81307e8901d09c0493,https://www.semanticscholar.org/paper/e61e75ec370207ed0dca2e81307e8901d09c0493,Enabling Customer Experience and Front-Office Transformation through Business Process Engineering,"In the past, the scope of business processes has been circumscribed to the industrialization of enterprise operations. Indeed, Business Process Management (BPM) has focused on relatively mature operations, with the goal of improving performance through automation. However, in today’s world of customer-centricity and individualized services, the richest source of economic value-creation comes from enterprise-customer contacts beyond transactions. Consequently, process has recently moved out of its traditional court and is becoming prevalent in less traditional competences such as marketing operations, customer-relationship management, campaign creation and monitoring, brand management, sales and advisory services, multi-channel management, service innovation and management life-cycle, among others. These competences host customerenterprise co-creation activities characterized by innovation, human creativity, and new technologies. Above all, these work-practices call for continuous differentiation, instead of “pouring concrete” on emerging business processes. While BPM will continue to make important contributions to the factory of enterprises, Business Process Engineering (BPE) is chartered to provide a holistic approach to new opportunities related to the life-cycle of enterprise customers and the transformation of so-called Front-Office Operations. More broadly, Business Process Engineering fosters a new space for the multidisciplinary study of process, integrating individuals, information and technology, and it does so with the goal of engineering (i.e., designing and running) innovative enterprise operations to serve customers and improve their experiences. Furthermore, given past challenges in the Back-Office, it is imperative that managers focus on processes in the Front-Office where the software industry has jumped into with solutions that bury key processes within applications, thus making differentiation and agility very difficult. BPE stresses the critical importance of the integration of Information and Behavior and it is this goal that links it with Business Informatics: the information process in organizations and society. Since behavior and information are complementary and inseparable domains of concern, current approaches to decision making based on data-only evidence should be reexamined holistically: it may be catastrophic to explicate or predict the behavior of organizations or individuals meaningfully by insisting on the ongoing divorce across the two domains. In particular, Business Informatics and Business Process Engineering offer an opportunity to address potential benefits of “big data” and “business analytics” beyond the IT domain. Having IEEE lead these directions means an opportunity for stimulating new research and practice on the most fundamental problems that enterprises and customers face today in dealing with each other. Keywords— business process engineering; customer experience; business process management; business informatics; enterprise engineering I. PROCESS IS OUT OF THE INDUSTRIALIZATION BOX Business process has been at the center of the stage in both research and industry for several decades. Under the brand of Business Process Management (BPM), business process has attracted a great deal of attention from many practitioners and scholars. BPM has been defined as the analysis, design, implementation, optimization and monitoring of business processes [70], [219], [79], [229], [230]. In [266], Van der Aalst defined some targets of BPM: ‘ ... supports business processes using methods, techniques, and software to design, enact, control and analyze operational processes involving humans, organizations, applications, documents and other sources of information” . While the above definitions are quite comprehensive and broad, in reality most BPM research and industry activity has grwon upon the motivation of reducing operating costs through automation, optimization and outsourcing. There are a several schools of thought and practice (such as lean, lean sixsigma, and others [172], [6], [4], [5]) and a myriad of related literature in the last 40 years that serve to illustrate the focus on cost contention. Around the middle of the past decade, T. Davenport stated in a celebrated Harvard Business Review paper [54] that processes were being “analyzed, standardized, and quality checked”, and that this phenomenon was happening for all sort of activities, stated in Davenport’s own terms: “from making a mouse trap to hiring a CEO”. The actual situation is that industry investment and consequential research have stayed much more on “trapping the mouse” than in differentiating customer services through innovative and more intelligent processes, let alone hiring CEOs. This may be explained partly from Davenport’s own statements: “Process standards could revolutionize how businesses work. They could dramatically increase the level and breadth of outsourcing and reduce the number of processes that organizations decide to perform for themselves” (bold face is added here for emphasis). With the advent of different technologies such as mobile, cloud, social media, and related capabilities that have empowered consumers, the classical approach and scope of business process have begun to change quickly. Organizations are adopting new operating models [100] that will drastically affect the way processes are conceived and deployed. As stated by many authors in the last four decades, business process work is supposed to cover all competences in an organization, irrespective of the specific skills from human beings participating in such operations. However, in an unpublished inspection of about 1,300 papers conducted by 1 Van der Aalst excludes strategy processes from BPM, a remarkable point that will be revisited in more depth later in this paper. the author and some of his collaborators 2 , most process examples shown in the literature deal with rather simple forms of coordination of work, mostly exhibiting a flow structure and addressing administrative tasks (like those captured in early works on office information systems). Furthermore, the examples provided usually deal with rather idealized operations, probably offered as simple examples with the only purpose of illustrating theoretical or foundational research results. Thus, radically simplified versions of “managing an order”, “approving a form”, “processing a claim”, “paying a provider”, “delivering an order” etc. are among the most popular examples of processes found in the literature. The lack of public documentation of substantial collections of real-world processes is remarkable. The authors in [106] both confirmed the dominant focus on simple business processes and also suggested potential practical consequences of related research: “... there is a growing and very active research community looking at process modeling and analysis, reference models, workflow flexibility, process mining and process-centric service-oriented architecture (SOA). However, it is clear that existing approaches have problems dealing with the enormous challenges real-life BPM projects are facing [ ... ] Conventional BPM research seems to focus on situations with just a few isolated processes ...”. Of course, the list of available real-world processes would be a lot richer if one included the set defined by enterprise packaged applications [219]. However, this comprehensive collection is proprietary because it constitutes a key piece of intellectual capital coming from software vendors or integrators in the industry. The traditional focus on process has also raised much controversy. At the S-BPM ONE Conference in 2010, a keynote speaker [176] remarked: “Let me be as undiplomatic as I possibly can be without being offensive [...] The academic community is as much to blame [...] as the vendors of BPM systems, who continue to reduce the task of managing business processes to a purely technological and automation-oriented level”. While other authors in the same conference debated “who is to blame” very animatedly [78], [234] it is important to highlight that the statement from Olbrich (in bold face above for emphasis) reinforces that BPM has mostly followed the obsession of automation and optimization by means of Information Technology. A detailed inspection of the extant literature confirms that business process work has been devoted to a rather small fraction of the actual variety and complexity found in enterprise behavior. This behavior enacts many valuegenerating capabilities that organizations cultivate based on skills provided by their own workforces and through rich interactions with other enterprise stakeholders, particularly customers. The following points offer a simplified summary: 2 At the time of this publication in IEEE, the mentioned work still remains unpublished. The co-authors are L. Flores and V. Becker both from IBM. (1) Business process research in Computer Science has been traditionally focused on certain classes of enterprise operations, mostly involving simple coordination mechanisms across tasks. This type of coordination and the overall behavior represented in underlying models reflect very much an “assembly line” where work is linearly synchronized to deliver a desired artifact or outcome. Simplicity of the choreography is ensured by removing any form of overhead in communication when moving from one stage to the next. Unlike other more complex business processes, many software applications do have this simplified structure. In fact, a trend since the early 2000’s is to separate the specific application logic from the coordination / choreography needed across modules, and both of them from the actual data contained in a data-base management system. Different foundations and a plethora of languages have been created to capture this semantics of coordination such as Business Process Modeling Notation (BPMN), Business Process Execution Languag",,2013.0,,1797243,semantic_scholar
733d5217e1a42999926598617d1849df9f4d4ea2,https://www.semanticscholar.org/paper/733d5217e1a42999926598617d1849df9f4d4ea2,Physical Layer Cooperation: Theory and Practice,"Abstract Information theory has long pointed to the promise of physical layer cooperation inboosting the spectral efﬁciency of wireless networks. Yet, the optimum relaying strategyto achieve the network capacity has till date remained elusive. Recently however, arelaying strategy termed Quantize-Map-and-Forward (QMF) was proved to achievethe capacity of arbitrary wireless networks within a bounded additive gap. This thesiscontributes to the design, analysis and implementation of QMF relaying by optimizing itsperformance for small relay networks, proposing low-complexity iteratively decodablecodes, and carrying out over-the-air experiments using software-radio testbeds to assessreal-world potential and competitiveness.The original QMF scheme has each relay performing the same operation, agnosticto the network topology and the channel state information (CSI); this facilitates theanalysis for arbitrary networks, yet comes at a performance penalty for small networksand medium SNR regimes. In this thesis, we demonstrate the beneﬁts one can gainfor QMF if we optimize its performance by leveraging topological and channel stateinformation. We show that for the N-relay diamond network, by taking into accounttopological information, we can exponentially reduce the QMF additive approximationgap from Q(N) bits/s/Hz to Q(logN) bits/s/Hz, while for the one-relay and two-relaynetworks, use of topological information and CSI can help to gain as much as 6 dB.Moreover, we explore what beneﬁts we can realize if we jointly optimize QMF andhalf-duplex scheduling, as well as if we employ hybrid schemes that combine QMF andDecode-and-Forward (DF) relay operations.To take QMF from being a purely information-theoretic idea to an implementable strategy,we derive a structure employing Low-Density-Parity-Check (LDPC) ensembles for therelay node operations and message-passing algorithms for decoding. We demonstratethrough extensive simulation results over the full-duplex diamond network, that ourdesigns offer a robust performance over fading channels and achieves the full diversityorder of our network at moderate SNRs.Next, we explore the potential real-world impact of QMF and present the design andexperimental evaluation of a wireless system that exploits relaying in the context ofWiFi. We deploy three main competing strategies that have been proposed for relaying,Amplify-and-Forward (AF), DF and QMF, on the WarpLab software radio platform. Wepresent experimental results—to the best of our knowledge, the ﬁrst ones–that compareQMF, AF and DF in a realistic indoor setting. We ﬁnd that QMF is a competitive schemev",,2015.0,,112912924,semantic_scholar
44ce98492713648fef9446f779de56029f432763,https://www.semanticscholar.org/paper/44ce98492713648fef9446f779de56029f432763,Effectiveness Of Collaborative Learning In Online Teaching,"This paper describes how e-learning is becoming popular and used as an alternative means of solving problems in education. E-learning is usually used in distance learning and may be used to replace conventional classroom teaching. Many educational institutions use Internet for collaborative learning in a distributed educational process. It has been known that traditional communications media can be replaced by electronic communication for the whole educational process and in particular, to assess the role of collaborative learning in a distributed education environment. It has been found that a distributed educational process naturally supports collaborative learning environments in which students and tutors interact and provide essential support for students studying at a distance. The tutor’s feedback to students help the learning process and there is indication that tutors are happy to work in the new environment. It is therefore suggested that “blended” online teaching – a combination of the use of the Internet as a medium of instruction and tutors to do face-to-face teaching via a collaborative learning approach – may be implemented to achieve enhanced distance-student performance. INTRODUCTION There is a number of definitions for e-learning. For example, Soekartawi et al. (2002) defined elearning as: ‘... a generic term for all technologically supported learning using an array of teaching and learning tools as phone bridging, audio and videotapes, teleconferencing, satellite transmissions, and the more recognized web-based training or computer aided instruction also commonly referred to as online courses...’. Today, e-learning has become an extremely popular alternative means of delivering educational services worldwide mainly because it is seen as a means of resolving significant educational problems that cannot be solved by conventional means. E-learning is particularly used in open and distance learning (ODL) as it can provide flexible education for those who cannot attend regular schooling particularly because they are unable to leave their work to attend regular conventional classes. Additionally, in an archipelagic country like Indonesia, where the bulk of the population is spread over thousands of islands, educational services are made more accessible through ODL. Most ODL programmes in Southeast Asia, particularly in Indonesia, deploy any one or a combination of the following media: print, radio, television, audiocassettes, videocassettes and computers. Generally, the course materials of ODL programmes in the region are largely print-based. It is likely that this will continue for a long time. However, by increasing the use of computers and information and communication technology (ICT), the role of the printed-based course materials will be replaced gradually by e-learning. MOJIT Effectiveness Of Collaborative Learning In Online Teaching* 69 The development of ICT is moving rapidly because of its pertinent role in providing education to the masses. In fact, by effectively deploying ICT, the educational institutions concerned can cope with, and cater to, the expanding student population. In general, it has been observed that the use of ICT in education in Indonesia has resulted in relatively successful outcomes. There have been numerous problems, however, but there is one significant issue that may be cited here because of its implications to future endeavours in education in Indonesia and in the Southeast Asian region as well. This issue has direct influence over the learning process. It is called transactional distance. The physical separation of teacher and student is no longer a problem given the developments in ICT. However, with ICT use, transactional distance can easily result in a misunderstanding and miscomprehension of the concepts to be learned. In fact, it can even lead to the mal-education of people, following a lack of appropriate communication between learner and teacher. If there is no communication, however, this transactional distance between learner and teacher becomes wider. THE NEW LEARNING PARADIGM In 1995, the International Council on Distance Education (ICDE) conducted what it called an anecdotal, worldwide survey to determine the nature, reality and pace of the shift in the learning paradigm (ICDE, 1996). The survey noted the following clear signs: o A shift from objective knowledge to constructed knowledge. o A shift from an industrial-based to a knowledge-based society. o A shift in education missions from providing instruction to providing learning. o A shift in technologically-mediated procedures of communication and learning. o A shift from “current college and university models to as yet undetermined structures.” Over the last five years, the fifth observation of ICDE has become very clear. The “undetermined structure” at the time of the ICDE survey has come to be known as the virtual learning structure. Experts made similar observations from the ICTsector at about the same time period (Soekartawi et al., 2002). The importance of the virtual campus in the Indonesian context has been reported by Soekartawi (2002, 2002a, 2003) and Haryono & Alatas (2002). According to Garmer & Firestone (1996), due to the development of ICT, the paradigm for learning is shifting away from the traditional notion that ‘‘knowledge” is transferred from teacher to student within the confines of the classroom where the focus of the teaching-learning process before was the teacher; now, it is shifting to the learner. A new understanding of learning places the learner at the centre of the learning process, with the teacher serving an important supporting role in facilitating the process (Garmer & Firestone, 1996). In this new paradigm, successful learning is measured by the individual’s ability to apply appropriate tools and information to solve problems in real life. There is a challenge in this new concept of learning. It is necessary to unlearn old habits and notions of how learning should be structured and to develop new habits of instruction that motivate learners to take greater control over their own education (Garmer & Firestone, 1996). This is what generally known by the “first concern” which should be to understand the learner’s motivations and goals in order that we can expand opportunities for learning. This new paradigm has critical implications for the use of ICT (e-learning) in education, particularly for ODL. For one, it requires that the learner take on more responsibility and autonomy in his learning. This is something that the learner may not be comfortable with or prepared to assume completely at this time. For another, teachers will have to give up control over the learning process and take on a new role similar to that of educational coaches who spend more time on the sidelines MOJIT Effectiveness Of Collaborative Learning In Online Teaching* 70 watching and maybe making plans for a more effective learning environment. The new paradigm has also highlighted an important issue. There is a need for all learners to upgrade their skills, particularly skills to learn on their own. This largely requires an ability to seek, understand and use information, which, in turn, requires the ability to use technology. In today’s world, for example, one must be computer literate to gain access to information and new knowledge. COLLABORATIVE LEARNING One of the critical factors in online learning is the quality issue. Many efforts can be used to meet this issue, among them obtaining feedback evaluation from the student. Further, a crucial aspect of successful distance education is the quality of feedback on student assignments. The electronic assignment handling system pioneered in these trials has now been adopted by the Open University at large and will serve a big number of students. Collaborative learning, therefore, can be used to improve the issue of quality in distance learning. Collaborative learning is an essential ingredient in the recipe to create an ""effective learning environment"" as it provides learners with the opportunity to discuss, argue, negotiate and reflect upon existing beliefs and knowledge. The learner is ""...involved in constructing knowledge through a process of discussion and interaction with learning peers and experts...."" Harasim (in Thomas, 1999, p.51). To facilitate collaboration so that personal knowledge can be constructed, there needs to be a purpose for the collaboration and the purpose needs to be meaningful to the learner. Thus, it is important that an appropriate context is set for the collaborative activity, for example, assigning a ""real world"" task for learners or a problem to which all learners can relate. In addition to setting the context, there needs to be a vehicle through which collaboration can take place. In traditional faceto-face educational settings, collaboration mostly occurs through conversation, that is, individuals interacting with one another via the use of language. Therefore, in terms of creating an effective learning environment, four attributes surface as being paramount: • Providing opportunities to foster personal construction of knowledge • Setting an appropriate context for learning to create the opportunities. • Facilitating collaboration amongst learners. • Using conversation to facilitate collaboration. Petre (1998) argued that while superficially it might appear that distance learning/education is the domain of the distance educator, the aims of educational institutions are similar in ideology; according to Thomas (1999), only the implementations differ. However, what distance educators brings to this arena is the experience of how to interact with students and tutors who are geographically and temporally remote from a campus as well as from one another. The ultimate objective is to diminish and even remove the barriers that remoteness erects. In general, the learning environment in the Open University can be described",,2006.0,,18756902,semantic_scholar
5f3c473a98d99962b62cf044f510f5db64ca2f47,https://www.semanticscholar.org/paper/5f3c473a98d99962b62cf044f510f5db64ca2f47,Is there an extreme world? [Book Review],"Extreme Programming’s core concept is that the client’s requirements for a software system change throughout the system’s development. The iterative method that XP uses requires considerable client involvement and a deep level of commitment to complete discrete sections of the development while meeting all the documentation, testing, and quality requirements before deployment. This nontraditional development method sounds like a silver bullet—too good to be true. So, is anyone using it? Extreme Programming Installed sets out to show that this method has indeed seen realworld deployment. To understand the real-world implementation of these theories, we need practical examples to expand the ideas. This XP book provides them. Backed by experience on XP projects, Ron Jefferies, Ann Anderson, and Chet Hendrickson give insight into the practical issues that development teams face. For example, what do you do about programmers working overtime? How do you use XP on projects to replace legacy systems? One important theme in the book is that of constantly testing the code to ensure that it meets requirements. The authors give practical help on what to test, how to test, and what tools to use in this development environment. Another theme is that the programming pair collectively owns the code. This means the pair is responsible for making it all work and ensuring that it all conforms to standards. If something is missing or needs rework to get the code to release, the pair adds it and tests the revised code. The importance of small tasks and fast turnaround of booked-out code becomes apparent as the book’s examples continue. Fortunately, the authors give guidance on code management tools and the code release process. If developers take away only one thought from this book, it should be this: make estimates for your work based on reality, not best intentions or wishes and hopes. The programming pair works on design, tests, and code on the basis of their own estimates. This gives all programmers and designers the potential to work according to their best speed and abilities rather than expectations imposed on them. The need for reality extends to reporting progress and test results. As the authors put it, “the Extreme way: tell the truth straight out.”",IEEE Software,2002.0,10.1109/MS.2002.1003465,22891722,semantic_scholar
d0f2b863b1af919d08620efe960f708aabe63199,https://www.semanticscholar.org/paper/d0f2b863b1af919d08620efe960f708aabe63199,SECURE DESIGN AND IMPLEMENTATION OF DISTRIBUTED AND INTEROPERABLE INFORMATION SYSTEMS BASED ON OVERLAP KNOWLEDGE PATTERN,"New architectural and technical forms of information systems add a more significant level of complexity due to the decentralization of the constraints, treatment and data. These architectures increase the deployment and the runtime possibilities because of the number of existing sites. Indeed, the simple separation of the various functional levels as it is done in a classical architecture (Data, Treatment, Presentation) is not enough and the choice of the site of deployment or runtime becomes significant for the optimization of the production of the system. In these architectures, these decisions of distribution are generally made during the implementation phase. The conceptual structures offered to designers to allow them to express their needs for distribution (concepts of packages, business component,..) do not match with the rules used by developers for building their distributed components [Snene04B]. In fact, software components represent a single and autonomous concept of real world. They encapsulate all the data concerning this concept including name, goal, behavior and all other information with regard to them. In fact, a software component is a set of objects that can be physically deployed on two or several sites. It is usually made up of one or of several distributed components that offer together the various aspects of distribution necessary to the software component. The distributed components represent the physical modules used for application assembling. They encapsulate given data and treatments and provide their services through well-defined interfaces.",,2005.0,,2178042,semantic_scholar
9cd7293da245d385efb2058faa39dd11678dd572,https://www.semanticscholar.org/paper/9cd7293da245d385efb2058faa39dd11678dd572,Sistemi riconfigurabili a basso consumo per applicazioni di monitoraggio distribuito,"The term Ambient Intelligence (AmI) refers to a vision on the future of the information society where smart, electronic environment are sensitive and responsive 
to the presence of people and their activities (Context awareness). In an ambient intelligence world, devices work in concert to support people in carrying out their everyday life activities, tasks and rituals in an easy, natural way using information and intelligence that is hidden in the network connecting these devices. This promotes the creation of pervasive environments improving the quality of life of the occupants and enhancing the human experience. 
AmI stems from the convergence of three key technologies: ubiquitous computing, ubiquitous communication and natural interfaces. 
Ambient intelligent systems are heterogeneous and require an excellent cooperation between several hardware/software technologies and disciplines, including signal processing, networking and protocols, embedded systems, information 
management, and distributed algorithms. 
Since a large amount of fixed and mobile sensors embedded is deployed into the environment, the Wireless Sensor Networks is one of the most relevant enabling technologies for AmI. WSN are complex systems made up of a number of sensor nodes which can be deployed in a target area to sense physical phenomena and communicate with other nodes and base stations. These simple devices typically embed a low power computational unit (microcontrollers, FPGAs etc.), a wireless communication unit, one or more sensors and a some form of energy supply (either batteries or energy scavenger modules). 
WNS promises of revolutionizing the interactions between the real physical worlds and human beings. Low-cost, low-computational power, low energy consumption and small size are characteristics that must be taken into consideration when designing and dealing with WSNs. 
To fully exploit the potential of distributed sensing approaches, a set of challengesmust be addressed. Sensor nodes are inherently resource-constrained systems with very low power consumption and small size requirements which 
enables than to reduce the interference on the physical phenomena sensed and to allow easy and low-cost deployment. They have limited processing speed,storage capacity and communication bandwidth that must be efficiently used 
to increase the degree of local ”understanding” of the observed phenomena. 
A particular case of sensor nodes are video sensors. This topic holds strong interest for a wide range of contexts such as military, security, robotics and most recently consumer applications. Vision sensors are extremely effective for medium to long-range sensing because vision provides rich information to human operators. However, image sensors generate a huge amount of data, whichmust be heavily processed before it is transmitted due to the scarce 
bandwidth capability of radio interfaces. In particular, in video-surveillance, it has been shown that source-side compression is mandatory due to limited bandwidth and delay constraints. Moreover, there is an ample opportunity 
for performing higher-level processing functions, such as object recognition that has the potential to drastically reduce the required bandwidth (e.g. by transmitting compressed images only when something ‘interesting‘ is detected). The energy cost of image processing must however be carefully minimized. 
Imaging could play and plays an important role in sensing devices for ambient intelligence. Computer vision can for instance be used for recognising persons and objects and recognising behaviour such as illness and rioting. 
Having a wireless camera as a camera mote opens the way for distributed scene analysis. More eyes see more than one and a camera system that can observe a scene from multiple directions would be able to overcome occlusion problems and could describe objects in their true 3D appearance. In real-time, these approaches are a recently opened field of research. 
In this thesis we pay attention to the realities of hardware/software technologies and the design needed to realize systems for distributed monitoring, attempting to propose solutions on open issues and filling the gap between 
AmI scenarios and hardware reality. The physical implementation of an individual wireless node is constrained by three important metrics which are outlined below. 
Despite that the design of the sensor network and its sensor nodes is strictly application dependent, a number of constraints should almost always be considered. Among them: 
• Small form factor to reduce nodes intrusiveness. 
• Low power consumption to reduce battery size and to extend nodes lifetime. 
• Low cost for a widespread diffusion. 
These limitations typically result in the adoption of low power, low cost devices such as low powermicrocontrollers with few kilobytes of RAMand tenth of kilobytes of program memory with whomonly simple data processing algorithms can be implemented. However the overall computational power of the WNS can be very large since the network presents a high degree of parallelism that can be exploited through the adoption of ad-hoc techniques. Furthermore through the fusion of information from the dense mesh of sensors even complex phenomena can be monitored. 
In this dissertation we present our results in building several AmI applications suitable for a WSN implementation. The work can be divided into two main areas:Low Power Video Sensor Node and Video Processing Alghoritm and Multimodal 
Surveillance . 
Low Power Video Sensor Nodes and Video Processing Alghoritms 
In comparison to scalar sensors, such as temperature, pressure, humidity, velocity, and acceleration sensors, vision sensors generate much higher bandwidth data due to the two-dimensional nature of their pixel array. 
We have tackled all the constraints listed above and have proposed solutions to overcome the current WSNlimits for Video sensor node. We have designed and developed wireless video sensor nodes focusing on the small size and the flexibility of reuse in different applications. The video 
nodes target a different design point: the portability (on-board power supply, wireless communication), a scanty power budget (500mW),while still providing a prominent level of intelligence, namely sophisticated classification algorithmand high level of reconfigurability. We developed 
two different video sensor node: The device architecture of the first one is based on a low-cost low-power FPGA+microcontroller system-on-chip. 
The second one is based on ARM9 processor. Both systems designed within the above mentioned power envelope could operate in a continuous fashion with Li-Polymer battery pack and solar panel. Novel low power low cost video sensor nodes which, in contrast to sensors that just watch the world, are capable of comprehending the perceived information in order to interpret it locally, are presented. Featuring such intelligence, these nodes would be able to cope with such tasks as recognition of unattended bags in airports, persons carrying potentially dangerous objects, etc.,which normally require a human operator. Vision algorithms for object detection, acquisition like human detection with Support Vector Machine (SVM) classification and abandoned/removed object detection are implemented, described and illustrated on real world data. 
Multimodal surveillance: In several setup the use of wired video cameras may not be possible. For 
this reason building an energy efficient wireless vision network for monitoring and surveillance is one of the major efforts in the sensor network community. Energy efficiency for wireless smart camera networks is one of the major efforts in distributed monitoring and surveillance community. 
For this reason, building an energy efficient wireless vision network for monitoring and surveillance is one of the major efforts in the sensor network community. The Pyroelectric Infra-Red (PIR) sensors have been used to extend the lifetime of a solar-powered video sensor node by providing an energy level dependent trigger to the video camera and the wireless module. Such approach has shown to be able to extend node lifetime and possibly result in continuous operation of the node.Being low-cost, passive (thus low-power) and presenting a limited form factor, 
PIR sensors are well suited for WSN applications. Moreover techniques to have aggressive power management policies are essential for achieving long-termoperating on standalone distributed cameras needed to improve the power consumption. We have used an adaptive controller like Model Predictive Control (MPC) to help the system to improve the performances outperforming naive power management policies.",,2010.0,10.6092/unibo/amsdottorato/2980,107921835,semantic_scholar
09b7e69ffd621e9025cbc1a313f41a8b63f98738,https://www.semanticscholar.org/paper/09b7e69ffd621e9025cbc1a313f41a8b63f98738,Arms: a decentralised naming model for object-based distributed computing systems,"Entities communicate with one another in distributed computing systems via symbolic names. Implementing such communication requires a naming scheme that dynamically maps these symbolic names to physical nodes and processes. Traditionally, a centralised name server is deployed to perform such translations. However, a collaborative and dynamic environment requires a decentralised naming system due to reasons of efficiency and reliability. 
 
ARMS (Adaptive, Randomised and Migration-enabled Scheme) is a novel decentralised naming scheme for distributed object-oriented computing systems. A notable feature of ARMS is that it provides direct naming supports for the patterns of object communication and object migration processes to achieve greater performance and scalability in executing object-oriented software within a distributed environment. These supports are driven by three key components: 1) an adaptive locating protocol that exploits the patterns of object communication and explores the best routing path in the face of the changing network conditions, 2) a randomised overlay that is a scalable and flexible substrate for routing name queries, and 3) a hybrid relocation scheme that provides a transparent and efficient means of referencing migrated objects. 
 
The performance of ARMS has been examined using a number of real world Java-based benchmarking programs. Based on results in this study, ARMS has found to be superior to its structural counterpart – the Chord model because of the adaptive routing protocol and the resilient overlay. Furthermore, ARMS has shown to be superior in a number of other performance metrics.",,2010.0,,59657940,semantic_scholar
3d80085c2bc6e289c5620bbf06b0878f4b3be001,https://www.semanticscholar.org/paper/3d80085c2bc6e289c5620bbf06b0878f4b3be001,Reliable middleware framework for rfid system,"The reliability of RFID systems depends on a number of factors including: RF interference, deployment environment, configuration of the readers, and placement of readers and tags. While RFID technology is improving rapidly, a reliable deployment of this technology is still a significant challenge impeding wide-spread adoption. This research investigates system software solutions for achieving a highly reliable deployment that mitigates inherent unreliability in RFID technology. 
We have considered two different problem domains for large scale RFID deployment. One is item tracking and the other is guidance-monitoring. 
The basic contribution of our work is providing novel middleware solution that is able to serve the application taking into account the inherent unreliability of RFID technology. Our path abstraction that uses the physical flow of data as an ally to generate a logical system level flow enhances the performance in many ways. The contributions of this dissertation are summarized below: 
Defining novel system architecture for item tracking applications: We have defined a system architecture referred to as Reliable Framework for RFID (RF2ID) that takes into account the unreliability of RFID devices and provides a scalable, reliable system architecture for item tracking applications. It uses a distributed system abstraction named Virtual Reader (VR) that handles RFID data in different geographic locations. Virtual Path (VPath) is the abstraction that creates channels among the VRs and facilitates a data flow oriented data management in the system. 
Implementation of RF2ID: We have implemented RF2ID that is able to incorporate physical RFID devices as well as emulated devices for scalability study taking into account various real world challenges of large scale RFID deployment. 
Load Shedding Based Resource Management: RF2ID requires a mechanism to handle unexpected system load in the presence of asynchronous arrival of data items. Space based load shedding and time based load shedding techniques are used in RF2ID. The basic idea is to exploit the VR and Vpath abstraction to intelligently share the load among the VRs in the presence of high system load, and yet provide some guaranteed Quality of Service (QoS). 
Architecture for GuardianAngel: We define an architecture for an indoor pervasive environment which provides novel system abstraction and communication framework. The layered architecture has distributed computational elements known as the virtual station (VS) that are in charge of serving different regions of the environment. The Mobile Objects (MO) are the physical and logical entities that use sensing device and traverse the environment. The environment itself is tagged with RFID. The MO uses its sensing device to make guidance decisions locally. The VS keeps status information of MOs and keeps coarse grained information of the MO over time and space providing a virtual location for each MO. 
Implementation of GuardianAngel: We have implemented the GuardianAngle system as defined by the architecture. We have used a testbed that uses real RFID readers and tags in the pervasive environment in a limited laboratory setup. We have also developed a distributed system setup using emulated tags for a scalability study of the proposed architecture. We have also implemented a prototype application, to test its feasibility in the real world. 
Evaluation of the system: We have conducted extensive evaluation using the real RFID testbed as well as scalability study using emulated readers and tags. The evaluation using the real RFID tags and readers gives us the credibility of the system under various environmental considerations. The large scale experimentations provide us with scalability and feasibility study to strengthen our limited resource study using real RFID testbed. (Abstract shortened by UMI.)",,2010.0,,113128152,semantic_scholar
e5334acd240594bf4a308887386ba7c4522278f4,https://www.semanticscholar.org/paper/e5334acd240594bf4a308887386ba7c4522278f4,Interim report for Mobile Computing Video transmission using USRP Submitted,"The project envisions a real-time video transmissio n between two points using GNU Radio and Universal Software Radio Peripheral (USRP). In our project, a video signal which could be a realtime signal from a camera or simply a video file is modulated and processed by GNU radio and transmitted using a USRP. There is a USRP receiver node which receives the signal and GNU radio demodulates and re-produces the transmitted video s ignal. This project brings in several challenges like bringing the camera interface to the USRP envi ronment, packets getting lost or corrupted in air, maintaining a constant bit rate as required by the USRP. Introduction USRP is a hardware platform that allows general pur pose computers to function as high bandwidth software radios [1]. Application layer communicates with the physical layer through some intermediate layers. For a stationary host this act ivity seems to be a good option where the communication protocol is systematic and defined ac cording to the environment where it is located. But for a mobile node, the environment conditions c hange with time and hence, the transmit power, bandwidth and quality of the channel has to be cont inuously monitored and passed on to the application layer in order to select the suitable a lgorithm. In turn, the physical layer has to change as per the suggestion from application layer time to t ime. GNU Radio and USRP bring the application developer close to the hardware as near to the ante nna itself and provides user with the flexibility t o change the communication parameters on the fly [2]. USRP aids engineers for rapid prototyping and development of powerful and flexible software radio systems. The applications of USRP come in manifold. It is used widely in prototyping and rese arch applications but it has been deployed in many real-world commercial and defence systems as well [ 3]. In our project, we explore an application of USRP which transmits real-time video from one point to another using USRP. Real-time video transmission finds application in Digital Video Bro adcasting (DVB). Background There has been a tremendous growth in the field of multimedia and mobile communications. The convergence of these two has resulted in mobile mul ti edia communications which has attracted the attention of the research community around the worl d [1]. A lot of researches have been done in this area to find out new methodologies to improvise or innovate new ways to implement the technology with better bandwidth and energy efficiency as thes e two resources are limited. USRP is an emerging technology which provides a platform to excavate th mobile computing environment in different scenarios. GNU radio and USRP provides a powerful radio commun ication platform. GNU Radio is an opensource Software-Defined Radio (SDR) platform which has libraries for various modulation schemes, error-correcting codes and scheduling [3]. GNU Radi o runs as an application which interacts with the USRP hardware. The complex processing like modulati on, and signal processing which are conventionally implemented in hardware, can now be implemented in the software and is easily accessible to the user developing the application. Applications can be created using the GNU Radio blocks or the Python script language which is behin d the blocks. The performance-critical signal processing path is implemented in C++. SWIG interfa ce, which is an interface compiler, is used to link the C++ with the python. The hardware has the antenna, RF (Radio Frequency) front-end, ADC/DAC, USB interface and a user-programmable Fiel d Programmable Gate Array (FPGA) to perform down-conversion. There are newer versions o f USRPs which has Ethernet interfaces and powerful FPGAs for improved speed and processing. Basic block diagram The file source or data can be a webcam output or a video file. Two USRPs are used, one for transmitting and the other for receiving. The sourc e file is Gaussian Minimum Shift Keying (GMSK) modulated and transmitted using transmitter USRP. T he modulation is implemented on GNU Radio. The transmitted signal is received by the receiving USRP and further demodulated by GNU Radio and played back. Figure 1 shows the block diagram Figure 1 Block diagram Experiment set up and plan The project activity is planned in different steps. 1. In the first step we use a stored video file as sou rce. We try to playback this video sent from the transmitter and check whether we are able to re produce the same video at the receiver. This shall be done in simulator mode without the us of USRP and just by the loop back of transmitter and receiver. This set up implemented i s shown in Figure 2. Figure 2 Video transmission in simulator mode 2. The USRPs shall be introduced into picture and loop back removed. The receiver and transmitter side is implemented on each USRP and ag ain a video file shall be transmitted and received. 3. Real-time video capture can be established using a camera which acts as the new video source. GStreamer framework could be used to proces s the video signal and connect to the GNU radio. The block diagram of this idea is shown in Figure 3. Figure 3 Block diagram with real time video [4] The different options available to stream real-time video are: 1. GStream framework which is an open-source software that processes and encodes the video signal from the camera. We have to research other p layers like FFmpeg also available in the market. 2. UDP sockets listening to the camera port at one end and another UDP socket sending to the camera port at the other end. These two options need to be explored a little bit and finalised. After doing some research on the modulation schemes us d in video broadcasting and multimedia applications, we found out that GMSK and Orthogonal Frequency Division Multiplexing (OFDM) are widely used in video communication. Further researches are required on the format of vi deo signals or packets and their encoding schemes. As USRP works on constant bit rate data stream the video signals need to be in the correct format for communication. A good idea on the video encoding wi ll help in debugging in case any problem arises during the development and testing. Expected results and comments As per the experimental set up explained above, the first part of the project is successfully complete d. We were able to transmit and receive a video file w ith a loop back from transmitter to receiver using GMSK modulation in simulator mode. For the second a third part, we need to maintain a constant bit rate data stream for USRP. An H.264 encoder can be used to perform this and we expect that the transmission and receiving of the video file should be smooth and successful. We need to observe the following parameters during the experiment. 1. Delay in reception of the video. 2. Packet loss incurred during transmission. Once the main part of the project is successfully c ompleted, which is proper communication between the two peripherals, we can further analyse and com pare other modulation schemes suitable for video transmission. Also the maximum distance within whic h USRP’s can communicate can be explored. We anticipate that distance between the two USRPs p lay an important role. There could be distortion in the video as the distance increases due to the l oss of packets or erroneous packets. Introducing a suitable error correction scheme can correct the da ta fr mes received. This implementation can be done depending on the time we have at hand. References 1. http://gnuradio.org/redmine/attachments/129/USRP_Do cumentation.pdf 2. http://www.wu.ece.ufl.edu/projects/wirelessVideo/pr ject/H264_USRP/index.htm 3. http://www.ettus.com/downloads/ettus_broch_trifold_ v7b.pdf 4. http://wiki.oz9aec.net/index.php/Simple_DVB_with_Gs treamer_and_GNU_Radio",,2011.0,,32289343,semantic_scholar
a72d2df90c87909159ee5f59811f722b7f6ad4ad,https://www.semanticscholar.org/paper/a72d2df90c87909159ee5f59811f722b7f6ad4ad,"USENIX Association Proceedings of BSDCon ’ 03 San Mateo , CA , USA September","The ever increasing mobility of computers has made protection of data on digital storage media an important requirement in a number of applications and situations. GBDE is a strong cryptographic facility for denying unauthorised access to data stored on a ‘‘cold’’ disk for decades and longer. GBDE operates on the disk(-partition) level allowing any type of file system or database to be protected. A significant focus has been put on the practical aspects in order to make it possible to deploy GBDE in the real world. 1 1. Losing data left and right In the last couple of years, gentlemen of the press have repeatedly been able to expose how laptop computers containing highly sensitive or very valuable information have been lost to carelessness, theft and in some cases espionage. [THEREG] The scope of the problem is very hard to gauge, since it is not a subject which the involved persons and, in particular, institutions are at all keen on having exposed. However, a few data points have been uncovered, revealing that the U.S. Federal Bureau of Investigation loses, on average, one laptop every three days. [DOJ0227] When a computer is lost, stolen or misplaced, it is very often the case that the computer hardware represents a value which is insignificant compared to the value of the disk contents. More often than not, the only reason the press heard about it was that the material on the disk was ‘‘hot’’ enough to make the loss of control rattle people at government level. While it is easy to blame these incidents on ‘‘user error’’, as is generally done, doing so makes it a very hard problem to fix. Human nature being what it is, seems to remain just that. In the absence of technical counter measures, administrative measures have been applied, generally with abysmal results. In one case, a bureaucracy has handled the problem according to what could easily be 1 This software was developed for the FreeBSD Project by Poul-Henning Kamp and NAI Labs, the Security Research Division of Network Associates, Inc. under DARPA/SPAWAR contract N66001-01-C-8035 (‘‘CBOSS’’), as part of the DARPA CHATS research program. mistaken for the plot from a classic Buster Keaton movie: First a laptop was forgotten and lost in a taxi-cab. New policy: always drive your own car if you bring your laptop. Then a car was stolen, including the laptop in the trunk. New policy: always bring your laptop with you. The next laptop was stolen from a pub while the owner was bowing to the pressures of nature. New policy: employees are not to carry their own laptops outside the office at any time. Laptops will be transported from and to the employees home address by the agency security force and will be chained and locked to a ring in the wall installed by the company janitors. All requests must be filed 3 days in advance on form ##-#. [PRIV] 2. Protecting disk contents Protecting the contents of a computer’s disk can in practice be done in two ways: by physically securing the disk or by encrypting its contents. Physical protection is increasingly impossible to implement. It used to be that disk drives could only be moved by forklift, but these days a gigabyte disk is the size, but not quite yet the thickness, of a postage stamp. While computers can be tied down with wires and bars can be put in front of windows, such measures are generally not acceptable, or at least not judged economically justified in any but the most sensitive operations. That leaves encryption of the disk contents as the only practical and viable mode of protection, and both the practicality and the viability has been somewhat in doubt. Until recently, nearly all aspects of cryptography were a highly political issue, this has eased a lot in the last couple of years and there now ‘‘only’’ remain a number of rather fundamental questions in the area of law enforcement and human rights, which are still unsettled. With the political issues mostly out of the way, the next roadblock is practical: While use of cryptography can never be entirely transparent, the overhead and workload it brings must be reasonable. 2.1. Application level encryption Encryption at the application level has been available for a number of years, primarily in the form of the PGP [PGP] program. This is about as intrusive and demanding as things can get: the user is explicitly responsible for doing both encryption and decryption and must enter the pass-phrase for every operation. 2 Apart from the inconvenience of this extra workload, many org anisations would trust their users neither to get this right nor even to want to get it right. From an institutional point of view it is important that cryptographic data protection can be made mandatory. 2.2. Filesystem level encryption Encryption at the file system level is a tried and acknowledged method of providing protection, but it suffers from a number of drawbacks, mainly because no mainstream file systems offer encryption. Encrypting file systems are speciality items, which means increased cost and system administration problems of all sorts. And since practically all operating systems use their own file system format, cross platform fully functional file systems are very rare. This means that a typical organisation will have to operate with a handful of different methods of encryption, which translates to system administration overhead, user confusion and extra effort to pass security and ISO9000 audits. A secondary, but increasingly important issue is that data which are stored in databases on raw disk, operating system paging areas and other such data are not protected by a cryptographic file system. To protect these would mean adding yet another set of encryption methods, which leads to a situation which is very hard to handle practically and administratively. Finally, file systems have a complex programming interface to the operating system, which traditionally 2 Interestingly, this is so impractical in real world use that various applications with PGP support resort to caching the pass-phrase at the application level, thereby weakening the protection a fair bit. has been subject to both version skew and compatibility problems. 2.3. Disk level encryption Encryption at the disk level can protect all data, no matter how they are stored, file system, database or otherwise. To a user, encryption at the disk level would require authentication before the computer can be used, everything functioning transparently thereafter, with all disk content automatically protected. Given that the programming interface for a disk device is very simple and practically identical between operating systems, there are no technical reasons why the same implementation could not be used across several operating systems. All in all this is a close to ideal solution from an operational point of view. There are significant implementation issues however. In difference from the higher levels, encryption at the disk level has no way of knowing a priori which sectors contain data and which sectors do not; neither is knowledge available about access patterns or relationships between individual sectors. Where application level or file system based encryption schemes can key each file individually, a disk based encryption must key each and every sector individually, ev en if it is not currently used to hold data. It has been argued that the encryption ideally should happen in the disk-drive, and while there are steps in this direction, they do unfortunately seem to have been made for the wrong reasons by the wrong people [CPRM], and have consequently not gained acceptance. Provided the owner of the computer remains in control of the encryption, I see no reason why encryption in the disk drives should not gain acceptance in the future. 3. Why this is not quite simple Several implementations have been produced which implement a disk encryption feature by running the user provided passphrase through a good quality one-way hash function and used the output as a key to encrypt all the sectors using a standard block cipher in CBC mode. A per sector IV for the encryption is typically derived from the passphrase and sector address using a one-way hash function. Tw o typical examples are [CGD] and [LOOPAES]. Unfortunately this approach suffers from a number of significant drawbacks, both in terms of cryptographic strength and deployability. For data to stay protected for decades or even lifetimes, sufficient margin must exist not only for technological advances in brute force technology, but also for theoretical advances in cryptoanalytical attacks on the algorithms used. Protecting a modern disk, typically having a few hundred millions of sectors, with the same single 128 or 256 bits of key material offers an incredibly large amount of data for statistical, differential or probabilistic attacks in the future. Worse, because the sectors contain file system or database data and meta data which are optimised for speed, the plaintext sector data typically have both a high degree of structure and a high predictability, offering ample opportunities for statistical and known plaintext attacks. This author would certainly not trust data so protected to be kept secret for more than maybe fiv e or ten years against a determined attacker. But far more damning to this method is that there can only be one single passphrase for the disk. This effectively rules out the ability for an organisation to implement any kind of per-user or multilevel key management scheme: the only possible scheme is ‘‘one key per disk’’. Add to this that to change the passphrase the entire disk would have to be decrypted and re-encrypted, and we have a model which may work in theory, and can be made to work in practice for a determined individual, but which would fast become an operational liability for any org anisation. 4. Designing GBDE The initial design phase of GBDE focused on determining a set of features which would make it both possible and",,2003.0,,14708661,semantic_scholar
a75db8fb465a14f3b86f2c0005662fef61a6b0ca,https://www.semanticscholar.org/paper/a75db8fb465a14f3b86f2c0005662fef61a6b0ca,Optical system evaluation,"Abstract : Optical and infrared sensors have an important role to play in modern military engagements, as the deployment of passive Systems increases. To guarantee the efficient development and usage of such equipment, at a reasonable cost, a reliable and realistic simulation of sensor performance is fundamental. The research project presented in this thesis consists of two parts. First, basic software modules that characterize the target-detector radiative transfer problem were developed. This was accomplished by developing separate modules for each physical aspect of the problem. The second part concerned the viability of implementing the physics of such real-world radiative transfer effects into existing military simulation tools. The chosen simulation environment for this effort was NPS Platform Foundation, an existing simulating software package that was developed at the Naval Postgraduate School.",,1995.0,,107873478,semantic_scholar
333f5f0ec5c5bebbbb4eb77b1eb18da93a4c1c25,https://www.semanticscholar.org/paper/333f5f0ec5c5bebbbb4eb77b1eb18da93a4c1c25,Improving data availability in mobile applications through enhanced cooperative localization,"One of the widely recognized advantages of distributed computing is that through the use of commodity hardware and software, powerful computing systems can be acquired and deployed with lower acquisition costs and shorter application development time than they could be otherwise. While there has been an abundance of research in distributed systems using traditional wired networks, widespread research into the use of mobile ad-hoc networks (MANETs) for distributed computing has only recently emerged. In addition, the integration of off-the-shelf hardware into mobile sensor networks is a relatively recent trend. Thus the impact of commodity hardware integration on the suitability of such systems has not been addressed in any significant way, although from a networking perspective, characteristics and limitations of MANETs are well known. Using MANETS as sensor networks poses unique challenges with respect to localization; many sensor net applications rely on coupling collected data with the location at which sensor readings were taken. Additionally, many applications that rely on data replication, such as peer-to-peer networks, exploit spatial locality to achieve greater communication efficiency, and thus require robust localization schemes. However, they are frequently deployed to places lacking infrastructure to facilitate positioning. Although reliable localization is generally assumed in many works, that is frequently not the case encountered in practice, which leads to poor application performance. This work proposes the idea that reliability and availability of data in location-sensitive applications can be improved by leveraging a cooperative data sharing model for localization, especially beneficial in systems constructed from low-cost, commercial off-the-shelf hardware components. In this proposal I first discuss the TeamTrak platform [23], a mobile testbed implementation for outdoor and urban environments, which is an important component of this work. Operation of physical implementations in noisy, uncontrolled environments presents both system designers and maintainers with unique fault tolerance challenges, many of which are unanticipated and frequently open new avenues for future research. Due to practical considerations, a number of the ideas presented in this work will be explored solely through simulation. However, evaluation tasks will be conducted within the context of the testbed to the greatest extent practicable, with the goal of further validation in real-world settings. Experiences building, deploying, and evaluating ideas using TeamTrak form the basis for much of this work; live tests provide empirical evidence which can improve assumptions about both the system and environment used in simulation as well as in the localization techniques presented. As the foundation for localization work, I first explore the use of low-cost sensor devices for navigation purposes and propose techniques for detecting, reducing, or compensating for errors in reported sensor data. This requires a rigorous evaluation of sensor data under various operational conditions and any resultant failure modes. With the sensor evaluation results providing a basis for understanding when and how failures occur, I next propose techniques for improving data reliability with respect to localization among mobile nodes using a cooperative data sharing model, described in Section 4. Because the Global Positioning System (GPS) has known limitations, dead reckoning can be used to approximate location for each mobile node when GPS is unavailable or unreliable, but such approximations require an accurate starting position and can also be error-prone. System-wide localization can be improved by leveraging relatively reliable data from one or more remote sources.",,2009.0,,6565828,semantic_scholar
635c01ca9f63bac692d2b9e4e6f4bdaf9aef4ce7,https://www.semanticscholar.org/paper/635c01ca9f63bac692d2b9e4e6f4bdaf9aef4ce7,TACTICAL INSERTION MISSION PLANNING AND REHEARSAL USING VIRTUAL REALITY SIMULATION,"Systems Technology, Inc. (STI) has developed a versatile new system for parachute mission planning and rehearsal, combining the validated technology of STI's PC-based PARASIM parachute simulation system with real-time interactive networking, powerful scene generation graphics tools, and terrain-correlated wind fields. This Tactical Insertion Mission Planning and Rehearsal Simulator (TIMPARS) was developed under the SBIR program, funded by the US Special Operations Command (SOCOM). The TIMPARS system rests on four cornerstones: the PARASIM  simulation software, the real-time interactive network, the scene generation toolkit, and the terrain-correlated wind generation module. These elements combine to produce a system with which users can utilize geospecific terrain data and imagery to recreate a real-world site as a simulation scene, input actual or forecasted wind speeds and directions at altitude above the chosen location to generate a terrain-correlated wind field specific to the simulation scene, and then plan and rehearse a mission in a real-time simulation environment with multiple live participants interacting in the same virtual space. BACKGROUND STI's original parachute simulator was developed for use by smokejumpers, US Forestry Service airborne firefighters. Designed to teach round and ramair canopy control, this early version employed rudimentary graphics with a fixed monitor; users stood before the display monitor and pulled simple toggle lines to maneuver in the simulation. Despite the austere configuration, this version provided the minimum cues required to teach parachute flight safely at low cost. In 1996, STI launched a major development effort to incorporate new photo- realistic graphics and head-mounted display/virtual reality technology into the simulator. Subsequent development efforts produced malfunctions procedures software, riser controls, harness switches, and additional simulator improvements. The implementation of these enhanced simulators by the US Marine Corps (USMC) and the Military Freefall School in Yuma, AZ, resulted in a drastic drop in the rate of training injuries. In particular, the USMC First Force Reconnaissance Company experienced a 75% reduction in main canopy cutaways after implementing the enhanced simulator in the MC-5 static line deployed ram-air parachute system (SLDRAPS) transition course at Camp Pendleton, CA. TIMPARS PROJECT",,2003.0,10.2514/6.2003-5610,109365992,semantic_scholar
ec0421dd64fdb0b41759c8d9c2875fa6ade6a526,https://www.semanticscholar.org/paper/ec0421dd64fdb0b41759c8d9c2875fa6ade6a526,Localization and Dynamic Tracking Using Wireless-Networked Sensors and Multi-Agent Technology: First Steps,"SUMMARY We describe in this paper our experience of develop-ing a large-scale, highly distributed multi-agent system using wireless-networked sensors. We provide solutions to the problems of localization(position estimation) and dynamic, real-time mobile object tracking, whichwe call PET problems for short, using wireless sensor networks. We pro-pose system architectures and a set of distributed algorithms for organiz-ing and scheduling cooperative computation in distributed environments,as well as distributed algorithms for localization and real-time object track-ing. Based on these distributed algorithms, we develop and implement ahardware system and software simulator for the PET problems. Finally, wepresent some experimental results on distance measurement accuracy usingradio signal strengths of the wireless sensors and discuss future work.key words: localization and mobile object tracking, distributed algorithms,multi-agent systems, wireless sensor network, and MEMS. 1. IntroductionIn recent years, the technology of micro-, electro-mechanical systems (MEMS) has made rapid advances.Various smart devices, such as sensors and actuators withsome information processing and communication capabili-ties embedded within, have been developed and deployed inmany real-world applications [9]–[11]. The application do-mains of such smart devices are numerous, including newgeneration pervasive computing systems, avionics and plantautomation, building and environmental monitoring, multi-hop routing discovery [6], augmented reality and virtue re-ality systems, active badge [13], and multiple robotics sys-tems. To use smart devices in these applications, it requiresto connect a large number of sensors and actuators, up tothousands, tens of thousands or even millions of units, andto integrate and embed sensing, communication, signal anddata processing, and control functionson individualdevices.The key to large-scale networked, embedded MEMS is themechanism with which individual units are programmed towork as a coherent piece toward achieving common goals.Due to the distributed and real-time nature of most applica-tions and the size of such a large system, information pro-cessing and decision making very often need to be carriedout on the units where actions take place.",IEICE Trans. Fundam. Electron. Commun. Comput. Sci.,2002.0,,114294617,semantic_scholar
ef481eb331f6bf705904f527205db1ded8e81401,https://www.semanticscholar.org/paper/ef481eb331f6bf705904f527205db1ded8e81401,The Year 2000 Software Crisis : The Continuing Challenge,"1. Year 2000 Progress Update. Year 2000 Update. Studies Paint Disconcerting Picture. The Real Story. Where Is the Sense of Urgency? Coming to Grips with Reality: Look at the Details. Media Update. The Year 2000 Myth? Silver Bullets Abound. Extremists Hurt the Cause. Business Problem Requires Business Solution. Industry by Industry Status. Banking. Securities Firms. Manufacturing. Retail. Transportation. Telecommunications. Utilities and Energy. Health Care, Insurance, and Pharmaceutical. Service Industries. Small to Mid-Size Companies. Government Update. U.S. Federal Government. U.S. Department of Defense (DOD). U.S. State Governments. U.S. Local Governments. Schools and Universities. International Update. Canada. Europe. Pacific Rim/Japan. Other Regions of the World. Worldwide Economic Impacts. 2. Strategy Update: Shift to Risk Mitigation. It's Late in the Game, Now What? Achieving Full Scale Deployment. The Declining Options Picture. Running Out of Time: Alternative Strategies. Launching Parallel Activities. Year 2000 Business Risk Assessment. Begin Fixing Five Most Critical Systems Now! Solidify Systems Inventory. Identify Data Interchange Points. Identify and Document External Entities. Document Business Functions. Relate Systems to Business Functions. Relate External Data Interfaces to Business Functions. Relate External Entities to Business Functions. Identify Event Horizons. Identify Revenue- or Customer-Related Risks. Identify Legal and/or Regulatory Risk. Prioritize Remediation and Testing Projects. Prioritize Business End-User-Driven Projects. Evolving Application Package Options. Vendor Has Delivered Compliant System. Vendor Fixed the System, But It Is Still Not Compliant. Vendor Is Fixing System, But Delivery Times Misses the Compliance Deadline. Vendor Refuses to Provide Compliance Status. Vendor Is Not Going to Fix the System. Strategies for the Far Behind. Targeting to Reduce Risk. ""Safe"" Corner Cutting. Unacceptable Corner Cutting. Launch Top Five Priority Remediation Projects. Beating the Clock in 1998 and 1999. 3. Legal Issues and Protections. Why You Should Care About This Legal Stuff. Minimizing Costs and Exposure. Contract Issues. Disclosure Obligations. Company Statements. Tax Law Issues. Internal Risk Management. Vendor Considerations. Certification Letters. Supply Chain/Partner Issues. The Paper Trail. Insurance. Government Aspects. Regulatory Agencies. Congress. State Government. Other Legal Aspects. Staffing. Offshore Factories. Copyright. 4. Non-IT Issues and Answers. Defining Non-IT Year 2000 Requirements. Business Partner Impacts. Supply Chain Challenge. Embedded and Other Non-IT Technologies. Non-IT Problems: Consequences and Timing. Industry-Specific Non-IT Challenges. Telecommunications. Energy and Power. Health Care. Manufacturing. Wholesale, Retail, and Service Industries. Transportation Sector. Financial. Government and Defense. Supplier, Business Partner Challenges. Supplier Categories. Supply Chain and the Domino Principal. Supplier Strategies: Methodological Approach. Documenting Multitiered Supply Chains. Supplier Responses. Supplier Contingency Options: Hedging Against Failure. Year 2000 Embedded Technology Challenge. Embedded Systems Challenge. Year 2000 Embedded Systems Impacts. Embedded Systems: Types and Categories. Embedded Systems Project Strategies. Embedded Systems: The Bottom Line. 5. Getting Help: Factories, Outsourcing, and Services. Third-Party ServicesA A Lessons Learned to Date. The Roots of the Dilemma. Identifying the Most Common Mistakes. Ground Rules for Successful Vendor Relationships. Selecting the Right Strategy. Strategy Selection Parameters. Using the Parameters to Select a Strategy. Selecting the Right Partner. Enabling Success. Enabling Factories. The Basic Factory Process. Before Using a Factory. The Factory Package. The Factory QA Process. Internal Factories. Enabling Consultants. Offshore Resources. Market Update. 6. Standards, Tools, and Techniques Update. Standards Update. Date Format Standards. Certification Programs. External Data Interchange. Year 2000 Firewall Strategies. Upgrade Unit Packaging Strategies. Remediation Procedures Update. High-Volume Productivity Targets. Field Expansion Update. Windowing Update. Expansion Using Bridges. Useful Notions and Worst Practices. An Array of Implementation Choices. ""Do It Yourself"" (DIY) Update. Optimizing the DIY Approach. Optimizing the Manual Fix Approach. Optimizing the Tool-Assisted Approach. Optimizing the Internal Factory Approach. Rules Of Engagement-Start Now. Year 2000 Tool Utilization Update. Automated Remediation Tools. Date Routines-Revisited. Bridging Routines. Testing Update. PC and Distributed Systems Tools. Risk Simulation Tools. Tracking Risks and Progress: Repository Utilization. Establishing Year 2000 Tracking Repository. Information Requirements. Enterprisewide Metamodel. Physical Repository Requirements. Information Capture and Loading Requirements. Inquiry and Reporting Requirements. 7. Year 2000 Testing Basics. Year 2000 Testing Differences. Scope of Year 2000 Testing. Focus of Year 2000 Testing. Infrastructure for Year 2000 Testing. What Needs to Be Tested. Compliance Requirement for Business Software. End-User Systems. Compliance Requirement for Embedded Technology. Types and Levels of Testing. Types of Tests. Levels of Tests. How Much Testing is Enough? ""Failsafe"" Testing. Typical Application Testing. Risk-Based Targeting. Elements of Risk-Based Analysis. Components Affected by Risk-Based Testing Strategies. Applying Risk-Based Testing Strategies. Applying Testing. 8. Implementing a Year 2000 Test Program. Managing the Enterprise-Level Testing Effort. The Testing Function of the Project Office. Roles and Responsibilities. Enterprise-Level Test Infrastructures. Assessing Enterprise Test Infrastructures. Determining Enterprise Infrastructure Improvements. Implementing Enterprise Infrastructure Improvements. Enterprise-Level Master Test Plan. High-Level Summary. Testing Strategy. Application Consolidation. Infrastructure Requirements. Test Projects List. Master Project Time Line. Managing Application-Level Testing Efforts. Roles and Responsibilities. Application Test Assessment Activities. Determining Application Infrastructure Improvements. Create an Application Test Plan. Test Data Creation. Test Script Creation. Test Execution. Results Validation. Acceptance and Sign-Off. Creating Application Test Plans. High-Level Summary. Testing Strategy. Test Environment. Detailed Test Descriptions. Application Project Time Line. Creating an Application Test Plan for Packaged Software. Factors Affecting Software Package Testing. Approaches to Testing Packaged Software. Developing a Test Plan for Packaged Software. Creating an Application Test Plan for Factory Remediation. Factors Affecting Testing of Factory-Remediated Software. Approaches to Testing Factory-Remediated Software. Developing a Test Plan for Factory-Remediated Software. 9. Contingency Planning: When Time Runs Out. What Is Contingency Planning. Why Plan for Contingencies. Technical versus Business-Driven Contingency Plans. Internally versus Externally Driven Requirements. Contingency Planning Participants. Identifying Contingency Requirements. Hardware and Infrastructure-Driven Requirements. Software-Driven Requirements. Development/Replacement Projects. Business-Driven Contingency Requirements. Building a Contingency Plan. Planning Overview. Business Model Redesign Options. Business Unit Shutdown Options. Business Function Consolidation Options. Triage: The De Facto Contingency Plan. Contingency Planning by Industry. Financial Institutions. Insurance Companies. Health Care Providers. Manufacturing and Retail Industries. Service Industries. Utilities and Telecommunications. Government Strategies. Small Company Contingency Planning. Contingency Plan Invocation. Time Critically Defines Success. Replacement Project Invocation. Invocation Based on Package-Provider Problems. Internal Failure Identification and Invocation. External Problem Identification and Invocation. Pre-2000 and Post-2000 Contingency Invocation. Who Makes Contingency Decisions? Shift toward Contingency Management. Contingency Management Is Continuous. Contingency Options at an Industry Level. Contingency Options at a National Level. Personal Contingency Planning. 10. Managing the Transition: Surviving the Inevitable. Defining the ""Transition Window."" Problems to Date. Predictions: 1998. Predictions: 1999. January 1, 2000. Cleaning Up the Mess: 2001-2005. Insurance Claims and Legal Action. Looking Forward: Industry by Industry. Financial Industry. Health Care. Manufacturing. Utilities and Telecommunications. Transportation. International Implications. Investment and Economic Impacts. Investment Impacts. Economic Impacts. Building a Crisis Management Team. Year 2000 Crisis Management. Crisis Management Requirements. Internal Planning Considerations. External Impact Considerations. The Crisis Management Plan. Crisis Call-In Center. Contingency Planning and Triage Center. Shifting Business Strategies in Crisis Mode. Cleanup Management Window. Year 2000 Will Change IT Landscape. Institutionalize the Project Office. IT Must Move On. IT Outsourcing: Be Careful. Think Globally-Act Locally. Authors' Note. Vendor Lists. Index.",,1998.0,,107855659,semantic_scholar
3121857e7fb87aed8c95f32eb7840a940736b60d,https://www.semanticscholar.org/paper/3121857e7fb87aed8c95f32eb7840a940736b60d,Projeto de um ambiente 3D de visualização e reprodução de eventos capturados e interpretados a partir de ambientes físicos cientes de contexto para aplicações de preparação para emergência.,"Systems for emergency preparedness support, especially those for accurate monitoring of physical environments subjected to emergency situations, are valuable resources for companies and civil defense public institutions, since these systems can help avoiding and/or reducing lives and patrimony losses. Most of the existing monitoring systems described in the literature have limitations, such as: no posterior visualization of emergency situations that have occurred; limited to specific types of applications; inaccurate identification of risk situations; etc. In this work, a system was proposed and evaluated that aims to overcome these limitations through the integrated use of wireless actor and sensor networks, context aware computing and virtual reality. The work consisted on the creation, implementation and evaluation of a recording and playing 3D media in which physical environments subjected to emergency situations are deployed sensors with processing and communication resources. These sensors capture and interpret contexts, which are mapped, through a visual language, on a 3D virtual environment that mimics the physical environment. The use of virtual reality for visualization and access in real-time or afterwards of situations that are occurring in the physical environment, through a 3D Virtual Environment, can overcome the limitations of hypermedia interfaces or continuous media, like video, when the experiences of the real world are very complex. This work describes the project of a recording and playing system, which allows users to play live experiences gathered from the real world for analysis, evaluation, monitoring and training. The novelty of the system resides in two aspects: it uses an optimized recording technique that saves processing time and storage space; it records scene updating commands independent from 3D Players, allowing the visualization of the collaborative virtual environment (CVE) through any existing 3D web players. In collaboration with the Arts and Communication Department (DAC) of UFSCar, a visual language to prompt identification of emergency situations was created as well as an interface to complex systems. Examples of use include the monitoring of industrial plants, flight rehearsals, petrol exploration platforms, etc. This work is part of a collaborative Project between the Networked Virtual Reality Lab (LRVNet) of the Computer Science Department at UFSCar and PARADISE Lab of SITE at University of Ottawa. 2 All complexity of real-time systems don’t will be considered in this dissertation, real-time will be used indicating that the events must be shown faster as possible.",,2006.0,,131887434,semantic_scholar
ee2be26c5e2bfab3885adc0e6c70e4583435bd3e,https://www.semanticscholar.org/paper/ee2be26c5e2bfab3885adc0e6c70e4583435bd3e,How to harness the Grid with OGSA - Tutorial Proposal,"Summary and Conclusion As a conclusion to this tutorial, we will share lessons learnt from our experience developing with OGSA and highlight the limitations as well as the benefits of deploying OGSA middleware. In particular, we will examine the behaviour of the crystal polymorph prediction system operating in a Grid environment and consider how effectively the implementation exploits available resources. We also discuss practical and political considerations that arise from “real world” Grid environments where technical arguments are often compromised by the needs and preferences of different users, organizations and domain administrators. Conduct of tutorial Delivery The tutorial will consist mostly of a talk support by a Powerpoint presentation. We also intend to illustrate some concepts with live software demonstrations: the first outlining the process of creating a simple OGSA service using the Globus Toolkit 3.0 (GT3) from simple interface description through to stub generation and wrapping an implementation using the delegation model; the second demonstrating the crystal polymorph application running in a simulated Grid environment. This will enable participants to experience Grid middleware from a developer’s perspective and lend some reality to the concepts and mechanisms the tutorial covers.",,2003.0,,16885599,semantic_scholar
cb311ca15659cba872bbd8a2152c814fbb4bb2ce,https://www.semanticscholar.org/paper/cb311ca15659cba872bbd8a2152c814fbb4bb2ce,Integrating QFD with Object Oriented Software Design Methodologies,"Object oriented (OO) methodologies have emerged as a popular paradigm for software design and analysis, both in research and practice. Several variants of OO methods are in use, but they all share significant similarities in their approaches to modeling the application domain. Quality Function Deployment (QFD) is also a design analysis and domain modeling technique with many parallels to OO methods. This paper contains an overview of object oriented design concepts, and shows how familiar QFD techniques are an effective aid for the OO analyst. QFD is a much easier way to approach the initial information collection and provides easy-to-understand structuring tools that do not require extensive training in OO concepts and methods. Overview of Object Oriented Software Concepts The following section is a brief overview of some object oriented concepts. It is beyond the scope and purpose of this article to discuss details of OO methodologies in depth. Many good sources of comprehensive elucidations on object oriented software engineering are given in the bibliography. Objects, messages, and encapsulation The fundamental concept in object oriented methodologies is, appropriately, the object. An object is a representation, or model, of a real-world entity. Objects have both data, which are usually called attributes, and behaviors, which are called methods . Since OO technology was heavily influenced by the analysis and design needs of real time control software applications, it was appropriate to envision objects as software representations of physical devices, such as sensors, actuators, and displays. A stepper motor, for example, has a state attribute ( on or off) and behavior ( turn_left, turn_right, and stop_motion). OO methods have also successfully been applied in more traditional information technology domains such as banking, accounting, personnel, etc. In these data-intensive applications, objects represent business entities and the data and processing operations that are associated with them. For example, a banking system might contain objects that represent individual checking accounts. Each account object contains data attributes for the name of the account owner, address, account number, current balance, and so forth. Objects communicate via messages ent to each other, with the assistance of the underlying language and operating run time support systems. Each message received by an object should have procedural code that interprets and carries out the function requested by the message; if the object does not understand the message, an error notification routine is invoked. In the checking 1 Sponsored by the U.S. Department of Defense 1995 QFD Symposium Page 1 May 14, 1995 account example, methods such as deposit_to_account and debit_from_account are invoked by messages exchanged between the account objects and transaction objects. Because these messages are the only interface that the object presents to the ""external world,"" the implementer is free to design internal object representations of data and procedures in any manner, as long as the message interface remains consistent. By taking this approach, details of the internal representation can be modified as needed without affecting the rest of the system. For example, temperature might be stored internally by the thermometer object in units of Fahrenheit, Celsius, or Kelvin, as long as the object accepts and replies correctly to messages requesting any particular reporting unit. This is known as encapsulation , or information hiding, and is a key concept in object orientation. Classes and instances Obviously, if every object in an application had to be individually and explicitly coded, it would not be practical to build a system consisting of hundreds of separate objects. Hence, every object is associated with a specific class. Classes are templates for objects, which specify the kinds of attributes and methods each object will have. Classes do not hold any of the values for the attributes of specific objects. Objects come into existence by being created as instances of a specific class. When an object instance is built from a class, a complicated process is used to determine the attributes and methods for that object, which involves allocating space, linking the object into various system data structures, and perhaps initializing certain attributes. Users of document processing products such as Ami Pro, Framemaker, or Microsoft Word for Windows have already encountered the concept of classes and instances. These products use the notion of paragraph styles to hold templates for various text characteristics, such as fonts, indentations, line alignment, margins, and so forth. Whenever a new paragraph (object) is created (instantiated) in a document, it inherits the formatting options of the template (class), but does not yet contain any data (text). All instances of paragraphs of a certain type (class) share the same formatting characteristics, but have different data attributes. 1995 QFD Symposium Page 2 May 14, 1995 Instance of a 2-door, red, ... van Class of Vans » # doors » length » color » engine option » VIN",,1995.0,,18370224,semantic_scholar
e0ff1e95e940e29e7cdc99a84492a37e3a051786,https://www.semanticscholar.org/paper/e0ff1e95e940e29e7cdc99a84492a37e3a051786,RIACS FY2001 Annual Report,"Recently, there has been shift from consideration of optimal decisions in games to a consideration of optimal decision-making programs for dynamic, inaccessible, complex environments such as the real world. Perfect rationality is impossible in these environments, because of prohibiting deliberation complexity. Anytime algorithms attempt to trade off result quality for the time or memory needed to generate results. Bounded rational agents are ones that always take the actions that are expected to optimize their performance measure, given the percept sequence they have seen so far and limited resources they have. Process algebras, with basic programming operators, has been used to study the behaviors of interactive multi-agent systems and leading to more expressive models than Turing Machines, e.g., Interaction Machines. By extending process algebra operators with von Neumann/Morgenstern’s costs/utilities, anytime algorithms can be viewed as a basis for a general theory of computation. As the result we shift a computational paradigm from the design of agents achieving one-time goals, to the agents who persistently attempt to optimize their happiness. We call this approach $-calculus (pronounced “cost-calculus”), which is a higher-order polyadic process algebra with a utility (cost) allowing to capture bounded optimization and metareasoning in distributed interactive AI systems. $-calculus extends performance measures beyond time to include answer quality and uncertainty, using k Omega-optimization to deal with spatial and temporal constraints in a flexible way. This is a very general model, just as neural networks or genetic algorithms, leading to a new programming paradigm (cost languages) and a new class of computer architectures (cost-driven computers). The NSERC supported project on $-calculus aims at investigation, design and implementation of a wide class of adaptive real-time distributed complex systems exhibiting meta-computation and optimization. It has also been applied to the Office of Naval Research SAMON robotics testbed to derive GBML (Generic Behavior Message-passing Language) for behavior planning, control and communication of heterogeneous Autonomous Underwater Vehicles (AUVs). Some preliminary ideas have also been utilized in the 5th Generation ESPRIT SPAN project on integration of objectoriented, logic, procedural and functional styles of programming in parallel architectures. It appears that $-calculus can be useful for the NASA Information Power Grid (IPG) Project. The IPG testbed provides access to a widely distributed network of high performance computers. $calculus resource-bounded optimization allows for flexible allocation of resources and scalability needed to tackle hard computation problems, thus $-calculus could provide a unifying metasystem framework for the Information Power Grid. Biosketch: Dr. Eberbach is a Professor at School of Computer Science, Acadia University and an Adjunct Professor at Faculty of Graduate Studies, Dalhousie University, Canada. Previously he was Senior Scientist at Applied Research Lab, The Pennsylvania State University, Visiting Professor at The University of Memphis, USA, Research Scientist at University College London, U.K., Assistant Professor in Poland, and he also has industrial experience. Professor Eberbach’s current work is in the areas of process algebras, resource bounded optimization, autonomous agents and mobile RIACS FY2001 Annual Report October 2000 through September 2001 -135robotics. General topics of interest are new computing paradigms, languages and architectures, distributed computing, concurrency and interaction, evolutionary computing and neural nets. More information about projects, publications, courses taught can be found at http://cs.acadiau.ca/~eberbach October 27, 2000: Feng Zhao, Ph.D.,Principal Scientist, Xerox PARC “Smart Sensors, Collaborative Sensemaking” Imagine a world in which we live where smart roads would be able to tell us when they need repair and which is the best direction to get to the Giants game, smart factories would stock up just enough inventory, ... The rapid advances in micro-electro-mechanical systems (MEMS) and lower-power wireless networking have enabled a new generation of tiny, cheap, networked sensors that can be “sprayed” on roads, across machines, and on walls. However, these massively distributed sensor networks must overcome a set of technological hurdles before they become widely deployable. Keeping up with the constant onslaught of sensory data from say 100,000 sensors is akin to drinking from a fire hose. The Xerox PARC Smart Matter Diagnostics and Collaborative Sensing Project studies the fundamental problems of distilling high-level, humaninterpretable knowledge from distributed heterogeneous sensor signals in a rapid and scalable manner. We are developing powerful algorithms and software systems to enable a wide range of applications, from sensor-rich health monitoring of electro-mechanical equipment to human-aware environments that leverage sensors to support synergistic interactions with the physical world. Biosketch: Feng Zhao is a Principal Scientist in the Systems and Practices Laboratory at Xerox PARC. Dr. Zhao leads the Smart Matter Diagnostics Project that investigates how sensors and networking technology can change the way we build and interact with physical devices and environments. His research interest includes distributed sensor data analysis, diagnostics, qualitative reasoning, and control of dynamical systems. Dr. Zhao received his PhD in Electrical Engineering and Computer Science from MIT in 1992, where he developed one of the first algorithms for fast N-body computation and phase-space nonlinear control synthesis. From 1992 to 1999, he was Assistant and Associate Professor of Computer and Information Science at Ohio State University. His INSIGHT Group developed the SAL software tool for rapid prototyping of spatio-temporal data analysis applications; the tool is currently used by a number of other research groups. Currently, he is also Consulting Associate Professor of Computer Science at Stanford. Dr. Zhao was National Science Foundation and Office of Naval Research Young Investigator, and an Alfred P. Sloan Research Fellow in Computer Science. He has authored or co-authored about 50 peer-reviewed technical papers in the areas of smart matter, artificial intelligence, nonlinear control, and programming tools. October 12, 2000: Irem Tumer, Intelligent Health and Safety Group NASA/Ames “Influence of Variations on Systems’ Performance And Safety” High-risk aerospace components have to meet very stringent quality, performance, and safety requirements. Any source of variation is of concern, as it may result in scrap or rework (translating into production delays), poor performance (translating into customer dissatisfaction), and potentially unsafe flying conditions (translating into catastrophic failures). As part of the Intelligent RIACS FY2001 Annual Report October 2000 through September 2001 -136Health and Safety group, we have been designing controlled experiments to understand various sources of variations in helicopter transmissions, collecting vibration data, and analyzing the data for indicators of the variations. We are looking for normal and abnormal sources of variation that affect performance and indicators of these variations to provide warning about potential failures during flight. The experiments include: • Flight tests using an AH-1 and an OH-58 helicopter, to determine the variations introduced due to regular maneuvering and the covariance with environmental conditions, engine torque, etc.; • OH-58 transmission test-rig tests to determine the effect of variations due to different levels of torque, mast bending, and mast lifting forces, as well as pinion reinstallation effects; • Machinery Fault Simulator tests to test the effect of prefabricated defects and inherent design and manufacturing variations on gears, bearings, etc. In this talk, I will present an overview of our group’s research goals, discuss the experiments and go over some of the results from the data analyses conducted so far. I will then discuss the current work and future directions in developing formalized methods for design and manufacturing engineers, using the variation information from empirical and analytical studies. RIACS FY2001 Annual Report October 2000 through September 2001 -137III.B RIACS-Supported Workshops As part of its mission of fostering ties with the academic community in IT, RIACS provides financial, administrative, and technical support for selected workshops involving RIACS scientists. The following workshops were supported during this reporting year: Workshop on Verification and Validation of Software The RIACS Workshop on the Verification and Validation of Autonomous and Adaptive Systems took place at Asilomar Conference Center, Pacific Grove, CA, 5-7 Dec 2000. Discussions included: V&V of Intelligent Systems: How to verify and validate systems featuring some form of AI-based technique, such as model-based, rule-based or knowledge-based systems. V&V of Adaptive Systems: How to verify and validate systems featuring adaptive behavior, either in the form of parametric adaptation (e.g. neural nets, reinforcement learning) or control adaptation (e.g. genetic programming). V&V of Complex Systems: How to verify and validate systems with different interacting parts, either within a given location (e.g. layered control architectures) and among several locations (homogenous or heterogeneous multi-agent systems). Workshop on Model-based Validation of Intelligence Lina Khatib (Kestrel) and Charles Pecheur co-organized a symposium on “Model-based Validation of Intelligence” as part of the AAAI Spring Symposium Series in March 2001. We provided the technical content (announcement, reviews and selection of articles, final program) while AAAI provided the logistics (rooms, registra",,2001.0,,133512970,semantic_scholar
cb6a0f14963b756058f7b33d2f149a0a7cf9a0bd,https://www.semanticscholar.org/paper/cb6a0f14963b756058f7b33d2f149a0a7cf9a0bd,Best Practices in Enterprise Content Management,"“Enterprise” is a badly misused word, unless you’re Captain Kirk. Alternately applied to mean both “big” and “integrated,” I’m afraid it no longer means either. . . . Eric Stevens, Hummingbird . . . . . . . . . . . 4 Corporate Governance—What Enterprise Content Management Was Meant For Organizations throughout the world have successfully deployed a range of enterprise content management (ECM) solutions to address organizational needs. . . . Dan Ryan, Stellent . . . . . . . . . . . . . . . . 6 Six Ways ECM Can Work for Your Business Unstructured content has doubled in the last three years. As a result, the challenge of managing and effectively leveraging this tremendous volume of content across an enterprise continues to grow. . . . Jeffrey Klein, First Consulting Group. . . . . . 8 ECM Best Practices for the Enlightened Enterprise Information overload in the life sciences industry is driving a pressing need for better content management. Companies are overwhelmed by content. . . . Tom Jenkins, Open Text. . . . . . . . . . . . . 10 ECM: Making Process Possible Over the past few years, ECM has emerged as a defined enterprise software category, one that is clearly capturing more and more attention. . . . Ethan Eisner, LexisNexis . . . . . . . . . . . . 12 Maximize User Satisfaction: Five Steps to ECM Success We have all experienced, or heard from colleagues, the horror stories...The good news is that we can learn from these past experiences. . . . Robert Weideman, ScanSoft. . . . . . . . . . 14 Better PDF for Business The single greatest challenge to streamlining document-based processes in business is the fact that there are two incompatible dominant electronic document formats. . . . Johannes Scholtes, ZyLAB . . . . . . . . . . . 16 Affordability in Content Management and Compliance The impetus on organizations to be transparent and fiscally prudent continues to intensify, motivated by the constant drive for more competitive efficiency. . . . Robert Liscouski, Content Analyst . . . . . . 18 Rising to the Real-World Challenges of ECM Initial efforts to implement ECM systems already have demonstrated the potential for achieving significant benefits. . . . Vernon Imrich, Percussion Software . . . . . 20 Champagne Tastes on a Beer Budget: ECM for the Rest of Us The key to getting a champagne feature set at a beer budget lies in considering how an ECM solution is delivered in addition to what features it advertises. . . . David White, Arbortext . . . . . . . . . . . . . 21 Automated Publishing is More Than ECM What defines a formal publishing process? When should an organization consider adopting an automated approach to publishing? If your content has one or more of the following characteristics. . . . Bill Rogers, Ektron . . . . . . . . . . . . . . . 22 New Realities for Mid-Market Content and Document Management Mid-size organizations have frequently found themselves “stuck in the middle” when it comes to finding the right solution for effectively managing, publishing and sharing online content. . . . George Viebeck, EMC Documentum . . . . . 23 Managing High Volumes of Data in SAP Companies are deluged by application data and documents. Some analysts say this data is growing at 80% per year. . . . Charles Hough, Interwoven . . . . . . . . . 24 A Service-Oriented Architecture for Better ECM New business requirements are changing the way companies think about enterprise software. Initiatives and objectives increasingly cross traditional boundaries. . . . A. J. Hyland, Hyland Software . . . . . . . . 25 ECM for Accounts Payable: It Pays—Faster! Why would I want to pay my bills faster? That’s the question some people ask when we talk about the value of using ECM software, such as document imaging. . . . Todd Peters, PaperThin . . . . . . . . . . . . 26 What You DON’T Know About Web Content Management As digital content continues to grow at an exponential rate, organizations increasingly struggle with ways to affordably and efficiently create and manage content on the Web. . . . Best Practices in Enterprise Content Management",,2005.0,,189811104,semantic_scholar
9e5b88dc69fe2abbfd4c2e8380a33306fcea4e3e,https://www.semanticscholar.org/paper/9e5b88dc69fe2abbfd4c2e8380a33306fcea4e3e,Robotics research : the eighth international symposium,"1. Advanced Manipulation Session Summary.- Elastic Strips: Real-Time Path Modification for Mobile Manipulation.- Modeling and Control for Mobile Manipulation in Everyday Environments.- Scale-Dependent Grasps.- 2. Dynamics and Control Session Summary.- A General Formulation of Under-Actuated Manipulator Systems.- Towards Precision Robotic Maneuvering, Survey, and Manipulation in Unstructured Undersea Environments.- Where does the Task Frame Go?.- 3. Emergent Motions Session Summary.- Motion Synthesis, Learning and Abstraction through Parameterized Smooth Map from Sensors to Behaviors.- Safe Cooperative Robot Patterns via Dynamics on Graphs.- 4. Motion Planning Session Summary.- Motion Planning with Visibility Constraints: Building Autonomous Observers.- Motion Planning in Humans and Robots.- Local and Global Planning in Sensor Based Navigation of Mobile Robots.- Interleaving Motion Planning and Execution for Mobile Robots.- 5. Manufacturing Session Summary.- Opportunities for Increased Intelligence and Autonomy in Robotic Systems for Manufacturing.- Rapid Deployment Automation: Technical Challenges.- Stability of Assemblies as a Criterion for Cost Evaluation in Robot Assembly.- Towards a New Robot Generation.- 6. New Components Session Summary.- The Design of a Serial Communication Link for Built-in Servo Driver and Sensors in a Robot.- Omnidirectional Vision.- Small Vision Systems: Hardware and Implementation.- 7. Mobile Robots Session Summary.- Exploration of Unknown Environments with a Mobile Robot using Multisensorfusion.- Integration of Topological Map and Behaviors for Efficient Mobile Robot Navigation.- A Robotic Travel Aid for the Blind: Attention and Custom for Safe Behavior.- Automated Highways and the Free Agent Demonstration.- The Design of High Integrity Navigation Systems.- 8. Haptics Session Summary.- Tactile Displays for Increased Spatial and Temporal Bandwidth in Haptic Feedback.- Design of an Anthropomorphic Haptic Interface for the Human Arm.- Testing A Visual Phase Advance Hypothesis for Telerobots.- 9. Medical Session Summary.- Robot Assisted Surgery and Training for Future Minimally Invasive Therapy.- Surgery Simulation with Visual and Haptic Feedback.- Synergistic Mechanical Devices: A New Generation of Medical Robots.- 10. Learning from Human Session Summary.- Vision-based Behavior Learning and Development for Emergence of Robot Intelligence.- Using Human Development as a Model for Adaptive Robotics.- Developmental Processes in Remote-Brained Humanoids.- Animating Human Athletes.- 11. Future Robots Session Summary.- Mechanics and Control of Biomimetic Locomotion.- Robots: A Premature Solution for the Land Mine Problem.- Robots Integrated with Environments: A Perceptual Information Infrastructure for Robot Navigation.- Bio-robotic Systems Based on Insect Fixed Behavior by Artificial Stimulation.- 12. Projects in Japan Session Summary.- Physical Understanding of Manual Dexterity.- Tightly Coupled Sensor and Behavior for Real World Recognition.- Intelligence and Autonomy for Human-machine Cooperative System.- Biologically Inspired Approach to Autonomous Systems.- FNR: Toward a Platform Based Humanoid Project.- Current and Future Perspective of Honda Humanoid Robot.- List of Participants.",,1998.0,,107037609,semantic_scholar
0fb400e80dcda882fc293327c5c12e59fd18af93,https://www.semanticscholar.org/paper/0fb400e80dcda882fc293327c5c12e59fd18af93,On-demand Loading of Pervasive-oriented Applications Using Mass-market Camera Phones,"Camera phones are the first realistic platform for the development of pervasive computing applications: they are personal, ubiquitous, and the builtin camera can be used as a context-sensing equipment. Unfortunately, currently available systems for pervasive computing, emerged from both academic and industrial research, can be adopted only on a small fraction of the devices already deployed or in production in the next future. In this paper we present an extensible programming infrastructure that turns mass-market phones into a platform for pervasive computing. 1 Mobile phone: a platform for pervasive computing Pervasive computing tries to make M. Weiser’s vision [1] a reality by saturating the environment with computing and communication devices: the most of the infrastructure is often invisible and supports user’s activities with an interaction model that is strongly human-centric. Today, almost fifteen years later, despite significant progresses in both hardware and software technologies, this vision is still not completely realizable or economically convenient. Supporting the interaction between users and the environment can be greatly simplified if we relax the interaction model and include a personal device as the access medium. Mobile phones are the most obvious candidates: they are in constant reach of their users, have wireless connectivity capabilities, and are provided with increasing computing power [2]. Even better results can be achieved with those phones that are equipped with a camera. Instead of manually getting information or editing configurations, users can point physical objects to express their will of using them: taking a picture of the objects would suffice to setup the link with the offered services. Relaying on an image acquisition device does not impose a strict limit to the share of possible users, since an always growing number of commercially available mobile phone is equipped with an integrated camera: according to recent studies [3], over 175 million camera phones were shipped in 2004 and, by the end of the decade, the global population of camera phones is expected to surpass 1 billion. ? This work is partially supported by the Italian Ministry for Education and Scientific Research (MIUR) in the framework of the FIRB-VICOM project. However, the acquisition of context-related information through images is not a trivial task, especially with resource-constrained devices. To ease the recognition process, objects can be labeled with visual tags readable by machines. Once decoded, visual tags either directly provide information about the resource they are attached to or, if the amount of information is too large, they act as resource identifiers that can be used to gather information from the network. In this paper, we describe the design and the implementation of POLPO 1 (Polpo is On-demand Loading of Pervasive-Oriented applications), a software system that turns mass-market phones into a platform for the development of pervasive applications. With POLPO, a phone with a built-in camera and compatible with the Java 2 Micro Edition (J2ME) platform is able to get context information by decoding visual tags attached to real-world objects. POLPO supports dynamic loading and installation of custom applications used to interact with the desired resources. 2 Background and contribution In this section we summarize the most relevant solutions based on visual tags and the contribution of our system in this field. Cybercode [4] is a visual tagging system based on a two-dimensional barcode technology. The system has been used to develop several augmented reality applications where the physical world is linked to the digital space trough the use of visual tags. Cybercode is one of the first systems where visual tags can be recognized by low-cost CCD or CMOS cameras, without the need for separate and dedicated readers. Each Cybercode symbol is able to encode 24 or 48 bits of information. The system has been tested with notebook PCs and PDAs. In [5] the author presents a system that turns camera-phones into mobile sensors for two-dimensional visual tags. By recognizing a visual tag, the device can determine the coded value, as well as additional parameters, such as the viewing angle of the camera. The system includes a movement detection scheme which enables to use the mobile phone as a mouse (this is achieved by associating a coordinate scheme to visual tags). The communication capability of the mobile phone is used to retrieve information related to the selected tag and to interact with the corresponding resource. Tag recognition and motion detection algorithms were implemented in C++ for Symbian OS. The Mobile Service Toolkit (MST) [6] is a client-server framework for developing site-specific services that interact with users’ smart phones. Services are advertised by means of machine-readable visual tags, which encode the Bluetooth device address of the machine that hosts the service (Internet protocols addressing could be supported as well). Visual tags also include 15 bits of application-specific data. Once the connection has been established, MST servers can request personal information to the client to provide personalized services. Site-specific services can push user interfaces, expressed with a markup language similar to WML (Wireless Markup Language), to smart phones. MST also provide thin-client functionality: servers can push arbitrary graphics 1 The Italian name for the octopus vulgaris, a cephalopod of the order octopoda, probably the most intelligent of the invertebrates. to the phone’s display which in turn forwards all keypress events to the server. The client-side is written in C++ and requires Symbian OS. A similar approach is described in [7], where the authors propose an architecture for a platform that supports ubiquitous services. Real-world objects are linked to services on the network through visual tags based on geometric invariants that do not depend on the viewing direction [8]. But differently from other solutions, image processing does not take place on the user’s device: pictures are sent to a server where they are elaborated and converted into IDs. Instead of using two-dimensional barcodes, an alternative way of performing object recognition is the one based on radio frequency identification (RFID): small tags, attached to or incorporated into objects, that respond to queries from a reader. However this solution, that can be useful in many pervasive computing scenarios, is not particularly suitable when the interaction is mediated by mobile phones, that lack the capability of reading RFIDs. In our opinion, currently available solutions present two major drawbacks: i) they are limited to specific hw/sw platforms (i.e. Symbian OS), excluding most of the models of mobile phones already shipped and in production in the near future; ii) the software needed to interact with the environment is statically installed onto the mobile phone and cannot be dynamically expanded, e.g. to interact with new classes of resources. We designed and developed a system for pervasive computing based on visual tags that overcomes these constraints as follows. Compatibility with J2ME The system runs on devices compatible with the J2ME platform. This environment is quite limited in terms of both memory and execution speed, but also extremely popular (nearly all mobile phones produced). This required the implementation of a pure Java decoder of visual tags for the J2ME environment. Downloadable applications Our system is based on the idea that the interaction with a given class of resources, e.g. printers, public displays, etc., takes place through a custom application. New custom applications can be downloaded from the network and installed onto the user’s device as needed. This brings two advantages: i) the classes of resources that can be used do not have to be known a priori; ii) the user’s device, that is resource constrained, includes only the software needed to interact with the services actually used. The J2ME platform comprises two configurations, few profiles, and several optional packages. The J2ME configurations identify different classes of devices: the Connected Device Configuration (CDC) is a framework that supports the execution of Java application on embedded devices such as network equipment, set-top boxes, and personal digital assistants; the Connected Limited Device Configuration (CLDC) defines the Java runtime for resource constrained devices as mobile phones and pagers. Our systems runs on top of the version 1.1 of the CLDC, that provides support for floating point arithmetics (unavailable in version 1.0). The adopted profile is the Mobile Information Device Profile (MIDP) that, together with CLDC, provides a complete Java application environment for mobile phones. 3 System architecture POLPO requires that physical resources are labeled with visual tags, and that a program providing access to POLPO functionalities is installed onto the user’s device. This program has the following primary functions: – Decoding of visual tags. The image captured with the built-in camera is processed to extract the data contained into the visual tag. – Management of custom applications. The program downloads and installs the custom application required to interact with a resource. Usually, resources of the same kind share the same custom application (i.e., a single application is used to interact with all printers, another is used with public displays, etc). – Management of user’s personal data. In many cases, applications need information about the user to provide customized services. For this reason, the software installed on mobile phones includes a module that manages user’s personal data and stores them into the persistent memory. Managed data comprise user’s name, telephone number, email address, homepage, etc. Each resource is identified and described by a Data Matrix visual tag. Da",IWUC,2006.0,10.5220/0002501600390048,16708155,semantic_scholar
e2ef0b6e85d1952435e79555ef1791696472a24d,https://www.semanticscholar.org/paper/e2ef0b6e85d1952435e79555ef1791696472a24d,Title Using role-play based simulation to acquire tacit knowledge inorganizations : the case of KreditSim,"Knowledge creation and application is crucial for organisations to cope with ever increasing competition. The business-relevant knowledge is reflected in the processes of a company. A process largely consists of tacit knowledge which is embedded in practice or experiences. Acquiring this type of knowledge is crucial for improving business processes. While formal learning or training programs deliver explicit knowledge and skills, it is much more challenging to generate implicit or tacit knowledge out of everyday work activities. In order to help employees and senior managers to acquire and apply tacit knowledge, a role-play based simulation program has been developed. This kind of simulation allows for a learning environment close to the workplace. The case of KreditSim shows how this coaching method actively involves employees and improves awareness of and participation in business process improvement. After identifying the deficiencies in their early process, the learners improve their process in a new simulation run. In this way, the tacit knowledge about processes is externalised, delivered, refined and reused. Social learning is also supported for process knowledge creation and sharing. Introduction According to the resource based view, each company depends on resources such as employees, machines, IT systems and buildings to produce goods or services for its customers (Wernerfelt, 1984). The resources itself do not incorporate business value but their combination to create an output like goods or services does. This combination is conducted within business processes. A business process (the term process is used synonymously here) is characterised by a set of connected activities necessary to deliver a defined business outcome (Davenport/Short, 1990). Starting point for designing a business process is the aspired business outcome which depends on the customers ́ demands. Within a business process, employees using IT systems and other resources transform input into output. Thus, a company can be seen as a bundle of business processes containing the knowledge to produce goods and services (Inkpen/Dinur, 1998). Business process thinking has become a major topic in management. In fact, turning a company into a processoriented organisation is seen as a competitive advantage and fundamental to its success. Such an organisation is more adaptable to changes in the market, faster in delivering output, superior in terms of quality and more responsive to the needs of customers (Hammer/Champy, 1993). But a process-oriented company requires different knowledge compared to a function-oriented organisation (Kugeler/Vieting, 2003). This processoriented knowledge is mostly tacit knowledge (Hawryszkiewycz, 2010). To ensure a permanent improvement of processes not only explicit knowledge but also tacit knowledge has to be deployed. A major question of process-oriented companies is how the necessary tacit knowledge can be acquired. The aim of this paper is to present KreditSim, a role-play based simulation, which facilitates the acquisition of tacit knowledge in an organisation. The paper is organised as follows. First, we describe the kind of knowledge embodied in business processes. Second, options for knowledge acquisition in the context of business process management are presented. One promising approach to acquire process-aware knowledge is to use role-play based simulations. To demonstrate the effects of this type of simulation the case of KreditSim will be presented. Finally, we will draw conclusions on the usage of role-plays for acquiring tacit knowledge Process-aware knowledge Business processes reflect what a company is doing, i.e. how services are delivered and goods are produced. In this sense, processes can be understood as the DNA of a company – similar to that of human beings. They contain the information how the resources of a company are combined. The interaction of the employees’ activities, their knowledge and the IT applications used leads to a unique combination of resources (the DNA) that distinguishes a company from its competitors. Hence, two companies having the same resources and competing in the same market can perform differently due to better or worse processes based on the varying combinations of resources. Since business processes are the foundation for production, the management of the process-related knowledge is a key factor for the success of a company. If this knowledge is not captured, stored, shared and applied a company is likely to fail (Lucas, 2010; Paroutis/Al Saleh, 2009). But business processes are virtual constructs. The intangibility exacerbates the awareness of employees for a process-oriented view. Trying to identify the knowledge embedded in processes Nonaka (1994) differentiates between explicit and tacit knowledge. Explicit knowledge can be stored and is independent from a certain person. Tacit knowledge can not be stored and is strictly associated to individuals. Still, it is possible to transfer tacit into explicit knowledge to a certain degree. Three basic types of knowledge can be distinguished in the context of business processes (Hawryszkiewycz, 2010): The basic type of knowledge is explicit knowledge in terms of traditional process documentation. Within these documentations the intangible process is made explicit. It is shown how the resources necessary to produce a good or a service have to be combined. This includes information about necessary tasks, their order, responsible employees, IT systems involved et cetera. The way process knowledge is captured differs significantly among organisations. More or less sophisticated approaches range from Excel-based task lists to comprehensive documentation based on professional software using standardised notations like Business Process Modelling Notation (BPMN) or Event-driven Process Chains (EPCs) (Barber et al., 2003). Additional to process documentation, there exists explicit knowledge as a result of monitoring and analysing the process performance. This knowledge mainly addresses the management of a company based on key performance indicators of a certain business process. But this kind of knowledge can also be useful for employees working within a process. It is useful to improve the process but does not substitute the implicit knowledge which is necessary to perform the task itself. The major type of knowledge in processes is tacit knowledge of the employees performing particular tasks. Tacit knowledge is a combination of cognitive processes and physical facts determining how a person behaves to solve a problem (Hawryszkiewycz, 2010). For instance, one employee may perform better than another having the same working conditions and yet it is not possible to capture the reasons for this difference. In summary, tacit knowledge is a major source of knowledge in companies. Often, tacit knowledge vanishes as employees change their jobs or leave the company. In order not to lose this vital knowledge, an organisation may try to increase the percentage of explicit knowledge. Explicit knowledge in terms of process documentation for example is very helpful. Processes are made visible on paper or on screen and employees can better understand the meaning of their activities, tools and information systems within a process. But in real-world settings several problems occur (Nonaka, 1994): Explicit knowledge still remains abstract, as documentations are limited in delivering a true image of reality. Processes themselves are intangible leading to difficulties in understanding processes. The effort to keep explicit knowledge up to date is high. The world is changing fast and so are customers and as a result the company’s employees and processes, too. Tacit knowledge is very hard to learn from explicit sources of knowledge. It is more easily gained through experience and communication with others. Subsequently, explicit knowledge delivers valuable information but is not helpful to acquire the tacit knowledge which is necessary e.g. for process-oriented thinking. Regarding the gap between function-oriented and processoriented thinking, companies have to acquire this tacit knowledge to be successful. Therefore, it is important to support employees in acquiring, sharing and applying tacit knowledge. Acquisition of tacit knowledge In literature three generic options for the acquisition of tacit knowledge, such as process-aware knowledge, can be found: Socialising: Following this approach, events should be set-up, allowing employees to share tacit knowledge through joint activities (Nonaka, 1994). In this context Snowden (1998) proposes that tacit knowledge can be shared through psychosocial mechanisms and released through trust and its dynamics. Experiencing: Acting, e.g. performing tasks, is a vital part of this approach (Earl/Scott, 1999). Knowledge is gained through “learning by doing” (Levitt/March, 1988). According to Nonaka (1994), experiencing is connected to socialising – as experience has to be shared between people – but both are not necessarily intertwined. Using explicit knowledge: According to Nonaka (1991) tacit knowledge can be acquired based on explicit knowledge. The latter can be the result of an externalisation of tacit knowledge in publicly comprehensible forms like documentations (Nonaka, 1991; Snowden, 1998). Thinking about the most effective option, use of explicit knowledge is the weakest one. The amount of tacit knowledge which can be learnt by explicit knowledge is limited (Hawryszkiewycz, 2010). Learning by simply experiencing is useful, but also restricted as employees have to learn on their own. Additionally, experiencing on the job is combined with a high risk of failures due to a trial and error acquisition of knowledge (Levitt/March, 1988). This could be a problem, as an organisation’s success depends on efficient processing of tasks. While the employee is making mistakes customers will become upset or will ",,2010.0,,55371878,semantic_scholar
3a984a19f86877947561b4613b8b40b2efa01d26,https://www.semanticscholar.org/paper/3a984a19f86877947561b4613b8b40b2efa01d26,Understanding storage system problems and diagnosing them through log analysis,"Nowadays, over 90% new information produced are stored on hard disk drives. The explosion of data is making storage system a strategic investment priority in the enterprise world. The revenue created by storage system industry steadily increases from $14.2 Billion in 2004 to over $18.4 Billion in 2007. As a key component of enterprise systems, reliable storage systems are critical. However, despite the efforts put into building robust storage systems, as the size and complexity of storage systems have grown to an unprecedented level, storage system problems are common. Unfortunately, many aspects of storage system problems are still not well understood, and most of previous studies only focus on one component - disk drives. 
To better understand storage system problems, we analyzed the failure characteristics of the core part of storage system - the storage subsystem, which contains disks and all components providing connectivity and usage of disk to the entire storage system. More specifically, we analyzed the storage system logs collected from about 39,000 storage systems commercially deployed at various customer sites. The data set covers a period of 44 months and includes about 1,800,000 disks hosted in about 155,000 storage shelf enclosures. Our study reveals many interesting findings, providing useful guideline for designing reliable storage systems. Some of the major findings include: (1) In addition to disk failures that contribute to 20–55% of storage subsystem failures, other components such as physical interconnects and protocol stacks also account for significant percentages of storage subsystem failures. (2) Each individual storage subsystem failure type and storage subsystem failure as a whole exhibit strong self-correlations. In addition, these failures exhibit bursty patterns. (3) Storage subsystems configured with dual-path interconnects experience 30–40% lower failure rates than those with a single interconnect. (4) Spanning disks of a RAID group across multiple shelves provides a more resilient solution for storage subsystems than within a single shelf. 
As we found out that storage subsystem problems are far beyond disk failures, we extend the scope of study to various storage system problems, and study the characteristics of storage system problem troubleshooting from various dimensions. Using a large set (636,108) of real world customer problem cases reported from 100,000 commercially deployed storage systems in the last two years, the analysis show that while some problems are either benign, or resolved automatically, many others can take hours or days of manual diagnosis to fix. For modern storage systems, hardware failures and misconfigurations dominate customer cases, but software failures take longer time to resolve. Interestingly, a relatively significant percentage of cases are because customers lack sufficient knowledge about the system. We also evaluate the potential of using storage system logs to resolve these problems. Our analysis shows that a failure message alone is a poor indicator of root cause, and that combining failure messages with multiple log events can improve problem root cause prediction by a factor of three. 
One key finding is that storage system logs contain useful information for narrowing down the root cause, while they are challenging to analyze manually because they are noisy and the useful log events are often separated by hundreds of irrelevant log events. Motivated by this finding, we designed and implemented an automatic tool, called Log Analyzer, to improve problem troubleshooting process. By applying statistical analysis techniques, the Log Analyzer can automatically infer the dependency relationship between log events, and identify the key log events that capture the essential system states related to storage system problems. By combining classic unsupervised classification techniques - hierarchical clustering with the event ranking techniques, the Log Analyzer can also identify recurrent storage system problems based on similar log patterns, so that previous diagnosis efforts can be systematically retrieved and leveraged. We train the Log Analyze with 18,878 week-long storage system logs and evaluate it with 164 real-world problem cases. The evaluation indicates that the Log Analyzer can effectively reduce the log event number to 3.4%. For most of the 16 real-world problem cases manually annotated with 1–3 key log events, the Log Analyzer accurately ranked the key log events within top 3 without a priori knowledge on how important the events are. For the other 148 problem cases with diagnosis and with root cause information, the Log Analyzer effectively grouped problem cases with the same root cause together with 63–93% accuracy, significantly outperforming other three alternative solutions which only achieve 30–46% accuracy.",,2009.0,,61561632,semantic_scholar
67bf23a6754c9081f1d4e882175967bd03242474,https://www.semanticscholar.org/paper/67bf23a6754c9081f1d4e882175967bd03242474,Software Support for Ground Control Station for Unmanned Aerial Vehicle,"Uninhabited vehicles can be used in many applications and domains, particularly in environments that humans cannot enter (e.g. deep sea) or prefer not to enter (e.g. war zones). The promise of relatively low cost, highly reliable and effective assets that are not subject to the physical, psychological or training constraints of human pilots has led to much research effort across the world. Due to technological advances and increasing investment, interest in Unmanned Aerial Vehicles (UAVs) as a practical, deployable technological component in many civil applications is rapidly increasing and becoming a reality, as are their capabilities and availability. UAV platforms also offer a unique experimental environment for developing, integrating and experimenting with many other technologies such as automated planners, knowledge representation systems, chronicle recognition systems, etc. UAV performs various kinds of missions such as mobile tactical reconnaissance, surveillance, law enforcement, search and rescue, land management, environmental monitoring, disaster management. UAV is a complex and challenging system to develop. It operates autonomously in unknown and dynamically changing environment. This requires different types of subsystems to cooperate. In order to realize all functionalities of the UAV, the software part becomes very complex real-time system expected to execute real-time tasks concurrently. This paper describes proposed software architecture for GCS (Ground Control Station) for lightweight UAV purpose-built for medium-scale reconnaissance and surveillance missions in civil area. The overall system architecture and implementation are described.© 2009 ASME",,2009.0,10.1115/DETC2009-86456,110868958,semantic_scholar
3056e72ddb960a635a0121ee3f9f452155bdd70f,https://www.semanticscholar.org/paper/3056e72ddb960a635a0121ee3f9f452155bdd70f,SUPPORT FOR GROUND CONTROL STATION FOR UNMANNED AERIAL VEHICLE,"∗Postgraduate student and author of correspondence, Phone: (+381) 641907205, Email: mladjan@afrodita.rcub.bg.ac.rs ABSTRACT Uninhabited vehicles can be used in many applications and domains, particularly in environments that humans cannot enter (e.g. deep sea) or prefer not to enter (e.g. war zones). The promise of relatively low cost, highly reliable and effective assets that are not subject to the physical, psychological or training constraints of human pilots has led to much research effort across the world. Due to technological advances and increasing investment, interest in Unmanned Aerial Vehicles (UAVs) as a practical, deployable technological component in many civil applications is rapidly increasing and becoming a reality, as are their capabilities and availability. UAV platforms also offer a unique experimental environment for developing, integrating and experimenting with many other technologies such as automated planners, knowledge representation systems, chronicle recognition systems, etc. UAV performs various kinds of missions such as mobile tactical reconnaissance, surveillance, law enforcement, search and rescue, land management, environmental monitoring, disaster management. UAV is a complex and challenging system to develop. It operates autonomously in unknown and dynamically changing environment. This requires different types of subsystems to cooperate. In order to realize all functionalities of the UAV, the software part becomes very complex real-time system expected to execute real-time tasks concurrently. This paper describes proposed software architecture for GCS (Ground Control Station) for lightweight UAV purpose-built for medium-scale reconnaissance and surveillance missions in civil area. The overall system architecture and implementation are described.",,2009.0,,18478483,semantic_scholar
336d84f6e7a7a398c70e924b4677c1fee7f3b81d,https://www.semanticscholar.org/paper/336d84f6e7a7a398c70e924b4677c1fee7f3b81d,The advantages of micro simulation in traffic modelling with reference to the N4 platinum toll road,"Micro simulation has been used to a limited extend in the past in South Africa, despite major advantages of this tool above static modelling and it’s popularity oversees. The main advantages are dynamic modelling and visual interpretation of the traffic conditions. This tool is ideal to test geometric designs, traffic controls and a variety of traffic management measures. These include incident and congestion management, road works, ramp metering, VMS, etc. It is an extremely suitable tool to use when low cost solutions must be found because of severely limited infrastructure resources. Times that micro simulation was not able to calculate and show reliable traffic situations is over, various traffic simulation models have developed and have reached high quality standards. Micro simulation is about to gain a real market share all around the world; South Africa is following. Modelling toll plazas at interchanges on the N4 Platinum Toll Road is used to illustrate the advantages of micro simulation. Geometric design options, measures effecting toll throughput and traffic control options were evaluated in this example as well as the estimation of the expected life span of various options within a congested network. The package used in this study is AIMSUN2, an advanced micro simulation package widely used internationally that can interact with TRANSYT, SCOOT, EMME/2 and SATURN. AIMSUN2 has been applied to traffic impact analysis, traffic control measures, HOV-lanes, tolling and geometric design within the last three years in South Africa. It has also been used successfully to convey results of investigations to nontechnical people. 1. MICRO SIMULATION 1.1 Background on the development of micro simulation The microscopic traffic simulation models are based on the reproduction of the traffic flows simulating the behavior of the individual vehicles, this not only enables them to capture the full dynamics of time dependent traffic phenomena, but also to deal with behavioral models accounting for drivers’ reactions. The underlying hypothesis is that the dynamics of a stream of traffic is the result of a series of drivers’ attempts to regulate their speed and acceleration accordingly with information received. The driver’s actions resulting from the interpretation of the information received will consist on the control of the acceleration (braking and accelerating), the control of heading (steering) and the decision of overtaking the precedent vehicle either to increase the speed or to position themselves in the right lane to perform a maneuver (i.e. a turning). The origins of microscopic traffic simulation can be traced back to the early stages of digital computers. Although the basic principles were set up many years ago, with the seminal work of, among others, Robert Hermann and the General Motors Group in the early fifties, the computing requirements made them impractical until hardware and software developments made them affordable even on today’s laptop computers. Most of the currently existing microscopic traffic simulators are based on the family of carfollowing, lane changing and gap acceptance models to model the vehicle’s behavior. Carfollowing models are a form of stimulus-response model, where the response is the reaction of the driver (follower) to the motion of the vehicle immediately preceding him (the leader) in the traffic stream. The response of the follower is to accelerate or decelerate in proportion to the magnitude of the stimulus at time t after a reaction time T. The generic form of the conceptual model is: response (t+T) = sensitivity * stimulus (t) Among the most used models are Helly’s model (1), implemented in SITRA-B+, (2), Herman’s model (3), or its improved version by Wicks (4), implemented in MITSIM, (5), the psycho-physical model of Wiedemann, (6), used in VISSIM (7), or the ad hoc version of Gipps (8), used in AIMSUN2 (9, 10). Other microscopic simulators such as INTEGRATION (11) and PARAMICS employ heuristic or other modeling not publicly available in analytic form. A common drawback of most of these models is that the model parameters are global i.e. constant for the entire network whereas it is well know that driver’s behavior is affected by traffic conditions. Therefore a more realistic car-following modeling for microscopic simulation should account for local behavior. This implies that some of the model parameters must be local depending on local geometric and traffic conditions. 1.2 What micro simulation is and the advantages thereof compared to static models The deployment of Intelligent Traffic Systems (ITS) requires support of complementary studies clearly showing the feasibility of the systems and what benefits should be expected from their operation. The large investments required have to be justified in a robust way. That means feasibility studies that validate the proposed systems, assess their expected impacts and provide the basis for sound cost benefit analyses. Microscopic traffic simulation has proven to be a useful tool to achieve these objectives. This is not only due to its ability to capture the full dynamics of time dependent traffic phenomena, but also being capable of dealing with behavioral models accounting for drivers’ reactions when exposed to ITS systems. The advent of ITS has created new objectives and requirements for micro-simulation models. Quoting from Deliverable D3 of the European Commission Project SMARTEST [12]: “The objective of micro-simulation models is essentially, from the model designers point of view, to quantify the benefits of Intelligent Transportation Systems (ITS), primarily Advanced Traveler Information Systems (ATIS) and Advanced Traffic Management Systems (ATMS). Micro-simulation is used for evaluation prior to or in parallel with on-street operation. This covers many objectives such as the study of dynamic traffic control, incident management schemes, real-time route guidance strategies, adaptive intersection signal controls, ramp and mainline metering, etc. Furthermore some models try to assess the impact and sensitivity of alternative design parameters”. The analysis of traffic systems and namely ITS systems, is beyond the capabilities of traditional static transport planning models. Microscopic simulation is then the suitable analysis tool to achieve the required objectives. An example from a real case study where microscopic simulation was used to complement static modeling will help us to understand better how both levels may help the decisionmaker. The city of San Sebastian, in the North of Spain, completed recently a new urban freeway connecting two separated neighborhoods. Figure 1 shows the typical result of the planning study with an close up of the Amara neighborhood. The road network and the demand were modeled using the EMME/2 package. The figure displays the expected impacts of the new infrastructure highlighting in green the average flow reduction due to the redistribution of flows enabled by the new paths on the network, and in red the increase of flows attracted by these paths. A significant discharge in the level of congestion in the main road network was the foreseen impact of the new infrastructure, but the access to the new freeway in the East-West direction shows some undesirable side effects in the neighborhood (Amara) inside the rectangle. The solutions to these problems demands a close up to the subnetwork and take decisions at the level of traffic control and traffic management schemes, not excluding even a partial reshaping of part of the street network. This type of decision requires a more detailed modeling, able of reproducing in a very accurate way the traffic conditions, accounting for the interactions between the vehicular flows and the infrastructure, and obviously including the influence of the traffic lights, objective that can only be achieved by a microscopic traffic simulation model. Figure 1: Expected impact of the new infrastructure in San Sebastian with an close up of the Amara neighborhood Figure 2 displays the corresponding model built with the AIMSUN2 traffic simulation software, the EMME/2 sub model has been built automatically from the AIMSUN2 model by means of an interface between both systems. Figure 2: AIMSUN2 micro simulation model of the Amara neighborhood The type of information that micro simulation can provide for a further analysis is beyond the capabilities of traditional static models. The average flows from sections to sections turning movements) for the allowed movements at selected intersections in the model, speeds and delays for every simulated time interval can be obtained. The dynamic analysis for a time period is completed with values for other traffic variables or indicators of the quality of service as number of stops, time delayed at stops, average queue lengths, etc. Figure 3 provide a further insight on the capacity of analysis provided by dynamic simulation software. The graphic in this figure describes the evolution over time of average flows. The same type of graph can be produced for average queue lengths on a subset of selected sections in the model. Figure 3: The evolution of average flows over time 1.3 The ease of model building and data input (AIMSUN2) The recent evolution of the microscopic simulators has taken advantages of the state-of-theart in the development of object-oriented simulators, and graphical user interfaces, as well as the new trends in software design and the available tools that support it adapted to traffic modeling requirements. A proper achievement of the basic requirements of a microscopic simulator implies building models as close to reality as possible. The closer the model is to reality the more data demanding it become. This has been traditionally the main barrier Section Volumes (Veh/h) 0 200 400 600 80",,2001.0,,108054289,semantic_scholar
cdd8028ef1569a9c7191e806839ed2bb261efdb9,https://www.semanticscholar.org/paper/cdd8028ef1569a9c7191e806839ed2bb261efdb9,Title: An Integrated Design Environment to Evaluate Power/Performance Tradeoffs for Sensor Network Applications,"Networks of inexpensive, low-power sensing nodes that can monitor the environment, perform limited processing on the samples, and detect events of interest in a collaborative fashion are fast becoming a reality. Examples of such monitoring and detection include target tracking based on acoustic signatures and line-of-bearing estimation, climate control, intrusion detection, etc. The advances in low-power radio technology are making wireless communication within sensor networks an attractive option. However, it is typically difficult or impossible to replenish energy resources available to a portable sensor node, once it is deployed. Maximizing the life of sensor nodes is an overriding priority, and different energy optimization techniques are being developed to addresses computation/communication tradeoffs. A large number of research efforts are focusing on different aspects of the general problem of designing efficient sensor network-based systems where the metrics to measure efficiency vary from system to system. With technological advancements such as silicon-based radios expected to become a reality in a few years, designers of sensor network-based systems will be faced with an extremely large set of design decisions. Each choice will affect the overall system performance in ways that might not always be cleanly modeled. In addition to the research challenges in design and optimization, the practical aspects of designing real-world sensor networks will become equally important. For example, the ability of the design framework to allow rapid specification and evaluation of a particular network configuration is crucial for a more exhaustive exploration of the design space. A design environment for future sensor networks should provide tools and formal methodologies that will allow designers to model, analyze, optimize, and simulate such systems. In the context of our work, design and optimization of a sensor network application involves determining the task allocation to different sensor nodes and the inter-node communication mechanism. Design of the sensor node hardware itself is also an area of active research. However, we assume that a set of node architectures is already available to our enduser, and the design problem is restricted to using the available hardware (with flexibilities, if any, such as dynamic voltage scaling) to efficiently implement the target application. We take a simple, seven-node wireless sensor network for acoustic detection [5] (Automatic Target Recognition) as the case study and demonstrate (i) a modeling and simulation methodology for a class of sensor networks, and (ii) a software framework that implements our methodology. Our formal application model is illustrated in Fig. 1. We use a data flow graph representation to model the computing tasks and their data dependencies. The end-to-end application consists of two types of such data flow graphs: the first type denotes the processing that has to be performed for each sample before it is ready to be ‘fused’ with results from other sensors, and the second type represents the computing involved in data fusion. Specifically , in our case study, a Fast Fourier Transform (FFT) operation is the only task that is performed on each block of sampled data. The outputs of FFTs from all seven nodes are provided as an input to the collaborative computing part, which consists of delay and sum beamforming (BF), and lineof-bearing (LOB) estimation. The result of collaborative computation in such a cluster model of sensor networks has to be transmitted to some observer. This is accomplished by designating one of the nodes as the cluster-head, which could be equipped with more powerful communication facilities than other sensor nodes. All communication within our cluster is one-hop, and processing of a particular data sample (FFT/BF/LOB) occurs either on its home node, or the cluster-head, or partly on both. Simulating a completely specified instance of the above class of sensor networks involves many challenges. None of the existing network simulators to our knowledge models the internal architecture of the processing nodes in the network. This is because the focus of most network simulators is on protocol development and empirical analysis. Except in areas such as high-speed router design, the node internals have little or no impact on decisions related to protocol design. Also, processor simulators do not model the environment outside the chip boundary. Therefore, to obtain detailed and accurate performance estimates for the entire system, we propose a technique to automatically generate network scenarios based on results from low-level node simulations. The network simulator is configured using the generated scenarios, and the individual simulation results are merged and presented to the end-user as a whole. Such a ‘horizontal’ simulation is accomplished through the use of a central data repository for model information, which means that the simulators never have to directly interact with each other. The simulators we integrate provide estimates about energy consumption, thereby assisting in a power/performance analysis of a specific system configuration. Our design framework facilitates multi-granular simulation, i.e., simulating the same system configuration by using simulation models at different levels of abstraction. Typically, coarse-grained models provide rapid estimates, but need to make approximations about system behavior that might not be very accurate. For such a scenario, we demonstrate a form of analytical model refinement (see Fig. 2), i.e. the data from low-level simulations can be automatically processed to ‘distill’ parameter values used by high-level simulators. Naturally, the exact processing has to be specified by someone with knowledge of the analytical model semantics. Our design environment provides the following capabilities to the user: • To graphically describe the target application, node architecture, network configuration, and task-to-node mapping. • To change (reconfigure) the system model to explore alternate designs. Some of the parameters that can be manipulated by the designer include receive/transmit power of the radio, voltage/frequency setting of the processor, cluster geometry, propagation models, etc. • To automatically simulate a design using a coarse system model. • To automatically configure and execute low-level simulators for the node (Wattch [3]) and the network (ns-2 [4]) and obtain system-wide energy and latency estimates. • To automatically update high-level model parameters using low-level simulation statistics. • To graphically visualize simulation results and facilitate (manual) identification of power/performance bottlenecks in the design. This work is an illustration of the general approach of the MILAN [1] project. A modeling and simulation framework based on the first version of the 7-node ATR system model was implemented in [2]. The primary focus of that work was a prototype demonstration of simulator integration and model refinement. Therefore, the system model itself lacked generality. Also, we use a relatively more detailed version of the high-level estimator implemented for [2]. This work represents a significant step towards the ultimate goal of a design environment for automatic optimization and synthesis of sensor network applications.",,2002.0,,5339303,semantic_scholar
9566723d1b3075cd14cabc8519a24a7eaa00cd5a,https://www.semanticscholar.org/paper/9566723d1b3075cd14cabc8519a24a7eaa00cd5a,"Using Simulation Tools for Embedded Software Development Class # 410 , Embedded Systems Conference , Silicon Valley 2008","ion vs. Detail A key insight in building simulations is that you must always make a trade-off between simulator detail and the scope of the simulated system. Looking at some extreme cases, you cannot use the same level of abstraction when simulating the evolution of the universe on a grand scale as when simulating protein folding. You can always trade execution time for increased detail or scope, but assuming you want a result in a reasonable time scale, compromises are necessary. A corollary to the abstraction rule is that simulation is a workload that can always use maximum computer performance (unless it is limited by the speed of interaction from the world or users). A faster computer or less detailed model lets you scale up the size of the system simulated or reduce simulation run times. In general, if the processor in your computer is not loaded to 100%, you are not making optimal use of simulation. The high demands for computer power used to be a limiting factor for the use of simulation, requiring large, expensive, and rare supercomputers to be used. Today, however, even the cheapest PC has sufficient computation power to perform relevant simulations in reasonable time. Thus, the availability of computer equipment is not a problem anymore, and simulation should be a tool considered for deployment to every engineer in a development project. Simulating the Environment Simulation of the physical environment is often done for its own sake, without regard for the eventual use of the simulation model by embedded software developers. It is standard practice in mechanical and electrical engineering to design with computer aided tools and simulation. For example, control engineers developing control algorithms for physical systems such as engines or processing plants often build models of the controlled system in tools such as MatLab/Simulink and Labview. These models are then combined with a model of the controller under development, and control properties like stability and performance evaluated. From a software perspective, this is simulating the specification of the embedded software along with the controlled environment. For a space probe, the environment simulation could comprise a model of the planets, the sun, and the probe itself. This model can be used to evaluate proposed trajectories, since it is possible to work through missions of years in length in a very short time. In conjunction with embedded computer simulations, such a simulator would provide data on the attitude and distance to the sun, the amount of power being generated from solar panels, and the positions of stars seen by the navigation sensors. When the mechanical component of an embedded system is potentially dangerous or impractical to work with, you absolutely want to simulate the effects of the software before committing to physical hardware. For example, control software for heavy machinery or military vehicles are best tested in simulation. Also, the number of physical prototypes available is fairly limited in such circumstances, and not something every developer will have at their desk. Such models can be created using modeling tools, or written in C or C++ (which is quite popular in practice). In many cases, environment simulations can be simple data sequences captured from a real sensor or simply guessed by a developer. It should be noted that a simulated environment can be used for two different purposes. One is to provide “typical” data to the computer system simulation, trying to mimic the behavior of the final physical system under normal operating conditions. The other is to provide “extreme” data, corresponding to boundary cases in the system behavior, and “faulty” data corresponding to broken sensors or similar cases outside normal operating conditions. The ability to inject extreme and faulty cases is a key benefit from simulation. Simulating the Human User Interface The human interface portion of an embedded device is often also simulated during its development. For testing user interface ideas, rapid prototyping and simulation is very worthwhile and can be done in many different ways. One creative example is how the creator of the original Palm Pilot used a wooden block to simulate the effect of carrying the device. Instead of building complete implementations of the interface of a TV, mobile phone, or plant control computer, mockups are built in specialized user interface (UI) tools, in Visual Studio GUI builder on a PC, or even PowerPoint or Flash. Sometimes such simulations have complex behaviors implemented in various scripts or even simple prototype software stacks. Only when the UI design is stable do you commit to implementing it in real code for your real device, since this typically implies a greater programming effort. In later phases of development, when the hardware user interface and most of the software user interface is done, a computer simulation of a device needs to provide input and output facilities to make it possible to test software for the device without hardware. This kind of simulation runs the gamut from simple text consoles showing the output from a serial port to graphical simulations of user interface panels where the user can click on switches, turn knobs, and watch feedback on graphical dials and screens. A typical example is Nokia’s Series 60 development kit, which provides a virtual mobile phone with a keypad and small display. Another example is how virtual PC tools like VmWare and Parallels map the display, keyboard, and mouse of a PC to a target system. In consumer electronics, PC peripherals are often used to provide live test data approximating that of a real system. For example, a webcam is a good test data generator for a simulated mobile phone containing a camera. Even if the optics and sensors are different, it still provides something better than static predetermined images. Same for sound capture and playback – you want to hear the sound the machine is making, not just watch the waveform on a display. Simulating the Network Most embedded computers today are connected to one or more networks. These networks can be internal to a system; for example, in a rack-based system, VME, PCI, PCI Express, RapidIO, Ethernet, IC, serial lines, and ATM can be used to connect the boards. In cars, CAN, LIN, FlexRay, and MOST buses connect body electronics, telematics, and control systems. Aircraft control systems communicate over special bus systems like MIL-STD-1553, ARINC 429, and AFDX. Between the external interfaces of systems, Ethernet running internet standards like UDP and TCP is common. Mobile phones connect to headsets and PCs over Bluetooth, USB, and IR, and to cellular networks using UMTS, CDMA2000, GSM, and other standards. Telephone systems have traffic flowing using many different protocols and physical standards like SS7, SONET, SDH, and ATM. Smartcards connect to card readers using contacts or contact-less interfaces. Sensor nodes communicate over standard wireless networks or lower-power, lower-speed interfaces like Zigbee. Thus, existing in an internal or external network is a reality for most embedded systems. Due to the large scale of a typical network, the network part is almost universally simulated to some extent. You simply cannot test a phone switch or router inside its real deployment network, so you have to provide some kind of simulation for the external world. You don’t want to test mobile phone viruses in the live network for very practical reasons. Often, many other nodes on the network are being developed at the same time. Or you might just want to combine point simulations of several networked systems into a single simulated network. Network simulation can be applied at many levels of the networking stack. The picture below shows the most common levels at which network simulation is being performed. The two levels highlighted in green are the ones that are most useful for embedded software work on a concrete target model. Physical signalling Bit stream Packet transmission Network protocol Application protocol High-level application actions Analog signals, bit errors, radio modeling Clocked zeros and ones, CAN with contention, Ethernet with CSMA model Ethernet packets with MAC address, CAN packets, serial characters, VME data read/write TCP/IP etc. FTP, DHCP, SS7, CANopen Load software, configure node, restart Hardware/software boundary r r / ft r r The most detailed modeling level is the physical signal level. Here, the analog properties of the transmission medium and how signals pass through it is modeled. This makes it possible to simulate radio propagation, echoes, and signal degradation, or the electronic interference caused by signals on a CAN bus. It is quite rarely used in the setting of developing embedded systems software, since it complex and provides more details than strictly needed. Bit stream simulation looks at the ones and zeroes transmitted on a bus or other medium. It is possible to detect events like transmission collisions on Ethernet and the resulting back-off, priorities being clocked onto a CAN bus, and signal garbling due to simultaneous transmissions in radio networks. An open example of such a simulator is the VMNet simulator for sensor networks. Considering the abstraction levels for computer system simulation discussed below, this is at an abstraction level similar to cycle-accurate simulation. Another example is the simulation of the precise clock-by-clock communication between units inside a system-on-a-chip. Packet transmission passes entire packets around, where the definition of a packet depends on the network type. In Ethernet, packets can be up to 65kB large, while serial lines usually transmit single bytes in each “packet”. It is the network simulation equivalent of transaction-level modeling, as discussed below for computer boards. The network simulation has no knowledge of the meaning of the packets. It just passes opaqu",,2008.0,,11785333,semantic_scholar
df7fb406e31a6057e2ea8f4997c3b71895e44093,https://www.semanticscholar.org/paper/df7fb406e31a6057e2ea8f4997c3b71895e44093,Design and Implementation of a Microprocessor-Based Sequencer for a Small-Scale Groundnut Oil Production Plant,"A microprocessor-based Sequencer for a small-scale groundnut oil production plant was designed and a test model was implemented. The microprocessor-based Sequencer is meant to replace traditional, electromechanical sequencers, which are based on relays, contactors, limit switches and other similar devices. The INTEL 8085A microprocessor, combined with interface chips like the AD7575 ADC, the MAX378 multiplexer was used to implement the sequencer. Software was programmed into 2716 EPROM. Actuators and signal conditioning circuits were also designed and implemented. The implemented system was tested and the performance was found to be satisfactory. Introduction Background to the problem : According to Nwachuku[1] microprocessors are the state of the art in electronics digital systems’ design and the Nigerian Engineer, like his counterpart the world over, has no choice but to become interested in them. He contended further that, because of the nature of the microprocessor and its versatility, it becomes possible for the Nigerian engineer to device products to meet local needs using imported chips. This according to him, is the direction of technological development ..In the advanced industrialized and newly industrialized countries, the last couple of decades have seen the extensive application of microprocessors and computers to automate production processes. Specifically, the microprocessor acts as a micro-controller with a fixed program. Here, the microprocessor application system in most cases, involves the determination of values of physical parameters like temperature, pressure, and so on. In Nigeria, not much seems to have been achieved in this area. Some big manufacturing companies in both the public and private sectors of the Nigerian economy have had to change from along monitoring and control of plant operations to microprocessor/computer-based systems. Most of them are based on the programmable logic controllers (PLCs). But all these analog to digital (microprocessor /computer-based) implementations are mostly carried out by foreign firms and personnel, bringing along with them, their designed hardware and software. Even at that, not much is known to have been achieved in the area of deploying this latest-in-technology to improve the production processes of small-scale industries. This project was conceived as a result of a realization of this obvious short-coming. The nature of the problem: All over the world, countries have come to recognize the leading role which small-scale industries play in their economic development. They furnish over forty-percent (40%) of a nation’s output of goods and they also provide a substantial amount of total employment in an economy. A realization of this obvious fact has made the Federal Government of Nigeria to lay emphasis on the need to support Small and Medium Scale Enterprises in order that they act as catalyst for Nigeria’s industrial and economic growth. For example, the Federal Government of Nigeria has floated a Bank of Industries and has set up a Small and Medium Enterprises Development Agency of Nigeria (SMEDAN) with the objective of improving the performance of Small and Medium Enterprises (SMEs) towards achieving rapid industrialization of Nigeria and for the reversal of its over dependence on imports. Advanced Materials Research Online: 2007-06-15 ISSN: 1662-8985, Vols. 18-19, pp 107-110 doi:10.4028/www.scientific.net/AMR.18-19.107 © 2007 Trans Tech Publications Ltd, Switzerland All rights reserved. No part of contents of this paper may be reproduced or transmitted in any form or by any means without the written permission of Trans Tech Publications Ltd, www.scientific.net. (Semanticscholar.org-19/03/20,17:18:59) However, it must be realized that, the success of these initiatives and of Small and Medium Scale Industries (SMIs) themselves, would depend on indigenous locally developed production processes and technologies. Failures of this class of industries in the past have largely been attributed to their over dependence on imported production processes, technologies, and by extension, on imported plants, equipments and machines. The Raw Materials Research and Development Council (RMRDC) of Nigeria have identified the dearth of process equipment and machinery as the bane of Nigeria’s under-utilization of her agricultural and mineral raw materials. This now places a huge burden on the Nigerian engineer to now start to design, implement, and practicalize new production processes, machines, equipment and plants that can be deployed by the old and emerging SMIs. The Federal Government of Nigeria seems to have set the ball rolling by the establishment of a National Office for Technology Acquisition and Promotion (NOTAP). The literature is replete with works on interfacing microprocessors with real life situations which is exemplified by the following: Hosier[2] reviewed briefly, the issues that has to be addressed when interfacing a microprocessor to perform real world monitoring and control operations. Anazia [3] gave an overview of the status of microprocessor applications in industrial process control as it relates to the Guinness Plant in Benin-City, Nigeria. A PC-Based Data Acquisition and Supervisory Control system for a Small-Scale Industry was designed and implemented by [4]. Mansfield[5] described the use of transducers for industrial measurement purposes. Program and Data Stores design and Input/Output interfacing were well treated by Short[6]. Gregory[7] explained in good details signal conditioning circuits design. General description of the microprocessor based sequencer The groundnut-oil production plant operates as a series of logically controlled sequence of states; for example, agitator ON, crusher OFF and so on; the duration of each state being determined by sensor signals, for example, temperature or pressure; these sequence of states were transferred into software. The software part (application program), in consonance with the process-sequential flowchart (SFC) that describes the operation of the plant was written in assembler. A major part of the plant consists of electrical and mechanical components like motors which are controlled by electro-mechanical relays, these in turn are operated by signals from the microprocessor; therefore, OUTPUT INTERFACE circuits were designed. Other signals are fed to the microprocessor from temperature, pressure, and other sensors; INPUT INTERFACE circuits were therefore, also designed. The microprocessor system must have memory for storing the application program and for implementing intermediate computations and such other operations. INPUT transducers’ outputs are memory-mapped. The Y2 output of the 74ALS138 decoder that was used addresses a memory address range of 1000Hex to 17FFHex. Therefore, the six (6) input transducers reside at addresses 1000H, 1001H, 1002H, 1003H, 1004H and 1005H respectively. The analog-to-digital converter converts input voltages of between 0volts and 5volts to binary outputs of between 00000000 and 11111111, that is, between 00Hex to FFHex. The output ports (74LS373s) were assigned to a separate address space different from that occupied by main memory; hence, they were isolated or standard I/O. Therefore, the designed and implemented system is as shown in block diagrammatic form, in Fig. 1. Description of the Designed and Implemented System In Fig. 1, block 1 represents the transducers. Block 2 represents the signal conditioning circuits that condition the transducers’ signals to match the multiplexer’s-MAX 378(block 3) inputs. The combination of the multiplexer and the sample and hold-AD781 JN(block 4) selects and holds an input for the analog-to-digital converter-AD7575 JN (block 5) to convert for the input ports (block 6). Block 7 (INTEL 8085A) is the microprocessor subsystem. Block 8 is an address/data bus decoder (74ALS373), while block 9 is an address decoder (74ALS138) for the memory subsystem(blocks 10-4118 and 11-2716). Block 12 represents the output ports (74ALS373) and 108 Advances in Materials and Systems Technologies",,2007.0,10.4028/www.scientific.net/AMR.18-19.107,60470034,semantic_scholar
7dbe61b811bfc91e247dd4c949543468c394a1fd,https://www.semanticscholar.org/paper/7dbe61b811bfc91e247dd4c949543468c394a1fd,Integration of CORBA and WEB technologies in the VEGA DIS,"Distributed client/server architectures nowadays appear as a must for the wide information systems of the future virtual enterprise. At the same time, the continued growth of the Internet/WEB and its associated standard developments leads to new ways of world-wide information communication, distribution and access to information. This paper introduces to the various developments undertaken in the VEGA 1 project for a tighter integration of STEP, CORBA and WEB technologies within a DIS . Thus, the VEGA platform will allow both the support of distributed heterogeneous and interoperable client/server information systems (through CORBA) and the support of WEB based access to information and services through an Internet based navigation, building upon CGI and Java technologies. 1. Context and problematical issues The Large Scale Engineering (LSE) and Manufacturing universe is nowadays facing an increasing competitive environment where flexibility and adaptability to change are the bound paths to success, leading companies to renew their way of working. This is due to economical and technological drivers. Indeed, industrial enterprises are now devoted to the specification, design and construction of still larger and more complex manufacturing products: it has become no more possible to a single even-wide enterprise to take in charge the realisation of the whole products, both for financial reasons and because the required skills are not all within the enterprise. Thus, companies and SMEs ( Small and Medium Enterprises ) are now on the way of constituting virtual enterprises (VEs) for each new project. In a VE, contractors, partners, suppliers and customers form a temporary aggregation of non co-located actors dealing with the same product, but focusing on their core domain of competence for the shared profitability of industrial projects. At the same time, current progress in IT , providing more reliable and relevant mechanisms and software tools, the development of sophisticated new frameworks in client/server applications, and the continued growth of the Internet enable improved business processes and provide organisations with new business opportunities. Companies are now widely deploying their applications in Internet and Intranet 4 environments, assembling advanced IT based architectures encompassing among others networking, distributed information systems and concurrent engineering. In such a context, the VEGA project is developing a mandatory infrastructure for the support of the LSE business processes, particularly on the base of main standards for information modelling and exchange, with STEP (ISO 10303) or the IFC developed by the IAI , distribution and interoperability mechanisms with the OMG 6 CORBA and IIOP de facto standards, and WEB technology based on HTTP /CGI and Java. VEGA intends to cover the general needs of companies in VEs and Intranets, with a focus on the Building & Construction sector. Its main objective is to provide an information management architecture to guaranty interoperability among various software components running on different platforms, targeting STEP distributed technologies as an approach to bridge the gap between multiple and delocalised software systems in the construction industry. Within VEGA, the development of the DIS is a first approach towards an end-user oriented service for access to information through various forms. In the AEC field, large projects require the involvement of many body entities (client, architect, design engineers, various technical engineers, etc.) sitting at various locations, with different views and needs on the project, and managing different forms of documents like textual documents, structured documents, drawings, and so on. Dealing with all these kinds of information representations on the client side lead to consider, as far as possible, standardised front-end services. Access to information can be related to EDI 9 too. Until now, EDI has always been considered as a technology typically based on EDIFACT syntax and rules. But EDI can be considered as a kind of generic term, including all aspects of technical and commercial information exchange without mandatory requirements with respect to specific communication technology. Indeed, EDI can be considered from two different points of view: • the first one is a rather conceptual one: the EDI is a structured electronic exchange of data of any type between computer applications of parties involved in a transaction ; • the second is an operational one, where we consider means to realise the task as announced in the first point of view. With respect to these operational means, the Internet is nowadays more and more acknowledged as the common medium to support communication facilities within the development of client/server applications that deserve the larger audience. The WEB technology builds upon HTTP to provide the users with high-level graphical applications independent of the underlying client platform. Furthermore, the Java language now simplifies the development of WEB applications through the power and flexibility of a real object-oriented language. The Intranet perspective enfavours the use of WEB technologies on LANs 10 on a company scale, whether it be real or virtual. Therefore, a different approach has been applied in the VEGA DIS, considering EDI in the broad sense and consequently managing EDI messages through distributed networked infrastructures, emerging WEB standards (mainly HTML and VRML formats), and WEB technologies like CGI and Java. 2. Integration of new technologies in the VEGA project 2.1 Overview of the VEGA project As previously mentioned, the VEGA 11 project objective is to develop an IT platform enabling companies in a VE to work together. VEGA leans on available technology, and extend their capabilities as needed for engineering collaboration in a flexible distributed environment. To address the problem of information sharing, VEGA deals with 3 different technologies: • product-data modelling for the specification of meaningful project information; • middleware technology for the distribution of project information; • workflow management for the control of the flow of information and work in the VE. The VEGA platform relies on high level open standards for the three technologies listed above, including STEP and particularly EXPRESS for the neutral specification of product model data, CORBA for communication between distributed applications and distribution of objects over networks, workflow technology as defined by the WfMC 12 for design of process control, and information standards like SGML 13 or various WEB de facto standards for the support of valueadded distributed information services (exchange of administrative messages, document handling, presentation of information). Thus, VEGA is currently elaborating the fundamental grounds for distributed architectures, defining a service layered on top of CORBA (the COAST – COrba Access to STEP models [2]), for remote data access and manipulation of information models defined by explicit meta-models (or schemata) satisfying the STEP EXPRESS semantics. 2.2 STEP and the IAI If different software applications need to communicate and inter-operate, they first need to share the same information, without misunderstanding or loss of semantics. This implies a common way to represent and exchange the semantics. Such issues have led to dramatic research efforts to achieve effective product data exchanges, standardisation of methodologies, languages and technologies, especially in the context of PDT , among them: • STEP ([3], [4]), developed in ISO TC184/SC4 16 for the product data representation and exchange. It allows the expression in a uniform and complete way of all the information required for a product life-cycle (especially through the EXPRESS language [5]), and supplies means for exchanging data physical files [6] and sharing product databases [7] with models and applications independent mechanisms. STEP is today deeply used for real world product information modelling, communication and interpretation. • IFC [8], another major effort currently undertaken by a non-profit alliance (IAI) of the building industry including architects and engineers, building clients, software vendors, and so on. The main IAI objective is to specify the IFC as a universal model to be a basis for collaborative work in the building industry and consequently to improve communication, productivity, delivery time, cost, and quality throughout the design, construction, operation and maintenance life cycle of buildings. STEP and IAI share the same goals, i.e. application interoperability; data exchange and actor cooperation, but differ in their respective processes. The IAI promote a bottom-up approach, with an iterative and incremental development for fast implementation. STEP is a long-term project, with a top-down approach and is concerned with broad standardisation. The IAI, having a formal liaison status with STEP has partially adopted the STEP technology, mainly through an EXPRESS representation of the IFC. In the future, an integration of the IFC within STEP is planned. As an initiative driven by leading companies, the IAI is pushing the IFC as a de facto standard in the Building industry in a very near future. A key point of the VEGA infrastructure is to use PDT and especially EXPRESS to describe and store meaningful product information. Implementing the VEGA vision requires a tight coupling between STEP models and all the various components of the VEGA platform (including distribution layer, workflow, data storage). Eventually, the current IFC 1.5 release will be used in the final VEGA demonstration . 2.3 The CORBA standard CORBA ([9], [10], [11]) is an OMG specification for application interoperability and portability in distributed architectures, allowing objects described in any language to be shared across heterogeneous operating system",,1999.0,,10152884,semantic_scholar
79dd4e21811c1399a4525d82e16c8fbf23db3d51,https://www.semanticscholar.org/paper/79dd4e21811c1399a4525d82e16c8fbf23db3d51,Human-Computer Interaction,"Contents Foreword Preface to the third edition Preface to the second edition Preface to the first edition Introduction Part 1 Foundations Chapter 1 The human 1.1 Introduction 1.2 Input-output channels Design Focus: Getting noticed Design Focus: Where's the middle? 1.3 Human memory Design Focus: Cashing in Design Focus: 7 +- 2 revisited 1.4 Thinking: reasoning and problem solving Design Focus: Human error and false memories 1.5 Emotion 1.6 Individual differences 1.7 Psychology and the design of interactive systems 1.8 Summary Exercises Recommended reading Chapter 2 The computer 2.1 Introduction Design Focus: Numeric keypads 2.2 Text entry devices 2.3 Positioning, pointing and drawing 2.4 Display devices Design Focus: Hermes: a situated display 2.5 Devices for virtual reality and 3D interaction 2.6 Physical controls, sensors and special devices Design Focus: Feeling the road Design Focus: Smart-Its - making sensors easy 2.7 Paper: printing and scanning Design Focus: Readability of text 2.8 Memory 2.9 Processing and networks Design Focus: The myth of the infinitely fast machine 2.10 Summary Exercises Recommended reading Chapter 3 The interaction 3.1 Introduction 3.2 Models of interaction Design Focus: Video recorder 3.3 Frameworks and HCI 3.4 Ergonomics Design Focus: Industrial interfaces 3.5 Interaction styles Design Focus: Navigation in 3D and 2D 3.6 Elements of the WIMP interface Design Focus: Learning toolbars 3.7 Interactivity 3.8 The context of the interaction Design Focus: Half the picture? 3.9 Experience, engagement and fun 3.10 Summary Exercises Recommended reading Chapter 4 Paradigms 4.1 Introduction 4.2 Paradigms for interaction 4.3 Summary Exercises Recommended reading Part 2 Design process Chapter 5 Interaction design basics 5.1 Introduction 5.2 What is design? 5.3 The process of design 5.4 User focus Design Focus: Cultural probes 5.5 Scenarios 5.6 Navigation design Design Focus: Beware the big button trap Design Focus: Modes 5.7 Screen design and layout Design Focus: Alignment and layout matter Design Focus: Checking screen colors 5.8 Iteration and prototyping 5.9 Summary Exercises Recommended reading Chapter 6 HCI in the software process 6.1 Introduction 6.2 The software life cycle 6.3 Usability engineering 6.4 Iterative design and prototyping Design Focus: Prototyping in practice 6.5 Design rationale 6.6 Summary Exercises Recommended reading Chapter 7 Design rules 7.1 Introduction 7.2 Principles to support usability 7.3 Standards 7.4 Guidelines 7.5 Golden rules and heuristics 7.6 HCI patterns 7.7 Summary Exercises Recommended reading Chapter 8 Implementation support 8.1 Introduction 8.2 Elements of windowing systems 8.3 Programming the application Design Focus: Going with the grain 8.4 Using toolkits Design Focus: Java and AWT 8.5 User interface management systems 8.6 Summary Exercises Recommended reading Chapter 9 Evaluation techniques 9.1 What is evaluation? 9.2 Goals of evaluation 9.3 Evaluation through expert analysis 9.4 Evaluation through user participation 9.5 Choosing an evaluation method 9.6 Summary Exercises Recommended reading Chapter 10 Universal design 10.1 Introduction 10.2 Universal design principles 10.3 Multi-modal interaction Design Focus: Designing websites for screen readers Design Focus: Choosing the right kind of speech Design Focus: Apple Newton 10.4 Designing for diversity Design Focus: Mathematics for the blind 10.5 Summary Exercises Recommended reading Chapter 11 User support 11.1 Introduction 11.2 Requirements of user support 11.3 Approaches to user support 11.4 Adaptive help systems Design Focus: It's good to talk - help from real people 11.5 Designing user support systems 11.6 Summary Exercises Recommended reading Part 3 Models and theories Chapter 12 Cognitive models 12.1 Introduction 12.2 Goal and task hierarchies Design Focus: GOMS saves money 12.3 Linguistic models 12.4 The challenge of display-based systems 12.5 Physical and device models 12.6 Cognitive architectures 12.7 Summary Exercises Recommended reading Chapter 13 Socio-organizational issues and stakeholder requirements 13.1 Introduction 13.2 Organizational issues Design Focus: Implementing workflow in Lotus Notes 13.3 Capturing requirements Design Focus: Tomorrow's hospital - using participatory design 13.4 Summary Exercises Recommended reading Chapter 14 Communication and collaboration models 14.1 Introduction 14.2 Face-to-face communication Design Focus: Looking real - Avatar Conference 14.3 Conversation 14.4 Text-based communication 14.5 Group working 14.6 Summary Exercises Recommended reading Chapter 15 Task analysis 15.1 Introduction 15.2 Differences between task analysis and other techniques 15.3 Task decomposition 15.4 Knowledge-based analysis 15.5 Entity-relationship-based techniques 15.6 Sources of information and data collection 15.7 Uses of task analysis 15.8 Summary Exercises Recommended reading Chapter 16 Dialog notations and design 16.1 What is dialog? 16.2 Dialog design notations 16.3 Diagrammatic notations Design Focus: Using STNs in prototyping Design Focus: Digital watch - documentation and analysis 16.4 Textual dialog notations 16.5 Dialog semantics 16.6 Dialog analysis and design 16.7 Summary Exercises Recommended reading Chapter 17 Models of the system 17.1 Introduction 17.2 Standard formalisms 17.3 Interaction models 17.4 Continuous behavior 17.5 Summary Exercises Recommended reading Chapter 18 Modeling rich interaction 18.1 Introduction 18.2 Status-event analysis 18.3 Rich contexts 18.4 Low intention and sensor-based interaction Design Focus: Designing a car courtesy light 18.5 Summary Exercises Recommended reading Part 4 Outside the box Chapter 19 Groupware 19.1 Introduction 19.2 Groupware systems 19.3 Computer-mediated communication Design Focus: SMS in action 19.4 Meeting and decision support systems 19.5 Shared applications and artifacts 19.6 Frameworks for groupware Design Focus: TOWER - workspace awareness Exercises Recommended reading Chapter 20 Ubiquitous computing and augmented realities 20.1 Introduction 20.2 Ubiquitous computing applications research Design Focus: Ambient Wood - augmenting the physical Design Focus: Classroom 2000/eClass - deploying and evaluating ubicomp 20.3 Virtual and augmented reality Design Focus: Shared experience Design Focus: Applications of augmented reality 20.4 Information and data visualization Design Focus: Getting the size right 20.5 Summary Exercises Recommended reading Chapter 21 Hypertext, multimedia and the world wide web 21.1 Introduction 21.2 Understanding hypertext 21.3 Finding things 21.4 Web technology and issues 21.5 Static web content 21.6 Dynamic web content 21.7 Summary Exercises Recommended reading References Index",Encyclopedia of Database Systems,1993.0,10.1007/978-1-4614-8265-9_192,61154755,semantic_scholar
469427dae41a91263631a3105a859b210b5d6912,https://www.semanticscholar.org/paper/469427dae41a91263631a3105a859b210b5d6912,BTMMU: an efficient and versatile cross-ISA memory virtualization,"Full system dynamic binary translation (DBT) has many important applications, but it is typically much slower than the native host. One major overhead in full system DBT comes from cross-ISA memory virtualization, where multi-level memory address translation is needed to map guest virtual address into host physical address. Like the SoftMMU used in the popular open-source emulator QEMU, software-based memory virtualization solutions are not efficient. Meanwhile, mature techniques for same-ISA virtualization such as shadow page table or second level address translation are not directly applicable due to cross-ISA difficulties. Some previous studies achieved significant speedup by utilizing existing hardware (TLB or virtualization hardware) of the host. However, since the hardware is not designed with cross-ISA in mind, those solutions had some limitations that were hard to overcome. Most of them only supported guests with smaller virtual address space than the host. Some supported only guests with the same page size. And some did not support privileged memory accesses. This paper proposes a new solution named BTMMU (Binary Translation Memory Management Unit). BTMMU composes of a low-cost hardware extension of host MMU, a kernel module and a patched QEMU version. BTMMU is able to solve most known limitations of previous hardware-assisted solutions and thus versatile enough for real deployments. Meanwhile, BTMMU achieves high efficiency by directly accessing guest address space, implementing shadow page table in kernel module, utilizing dedicated entrance for guest-related MMU exceptions and various software optimizations. Evaluations on SPEC CINT2006 benchmark suite and some real-world applications show that BTMMU achieves 1.40x and 1.36x speedup on IA32-to-MIPS64 and X86_64-to-MIPS64 configurations respectively when comparing with the base QEMU version. The result is compared to a representative previous work and shows its advantage.",VEE,2021.0,10.1145/3453933.3454015,233174939,semantic_scholar
1890380a9e14e0a82dc105c9b8ce251107af9ddf,https://www.semanticscholar.org/paper/1890380a9e14e0a82dc105c9b8ce251107af9ddf,2nd CfP: 7th IEEE International Conference on Self-Adaptive and Self-Organizing Systems (SASO2013),"submission: May 3, 2013 Paper submission: May 10, 2013 Notification: June 21, 2013 Camera ready copy due: July 19, 2013 Early registration: August 21, 2013 Conference: September 9-13, 2013 ------------------------------Topics of Interest ------------------------------The topics of interest to SASO include, but are not limited to: Self-* systems theory: theoretical frameworks and models; biologicallyand socially-inspired paradigms; inter-operation of self-* mechanisms; Self-* systems engineering: hardware, software and middleware development frameworks and methods, platforms and toolkits; self-* materials; Self-* system properties: robustness, resilience and stability; emergence; computational awareness and self-awareness; reflection; Self-* cyber-physical and socio-technical systems: human factors and visualization; self-* social computers; crowdsourcing and collective awareness; Applications and experiences of self-* systems: cyber security, transportation, computational sustainability, big data and creative commons, power systems. --------------------------------------Submission Instructions --------------------------------------All submissions should be 10 pages and formatted according to the IEEE Computer Society Press proceedings style guide and submitted electronically in PDF format. Please register as authors and submit your papers using the SASO 2013 conference management system. The proceedings will be published by IEEE Computer Society Press, and made available as a part of the IEEE digital library. Note that a separate call for poster submissions has also been issued. ---------------------------Review Criteria ---------------------------Papers should present novel ideas in the cross-disciplinary research context described in this call, clearly motivated by problems from current practice or applied research. We expect both theoretical and empirical contributions to be clearly stated, substantiated by formal analysis, animation or simulation, experimental evaluations, comparative studies, and so on. Appropriate reference must be made to related work. Because SASO is a cross-disciplinary conference, papers must be intelligible and relevant to researchers who are not members of the same specialized sub-field. Authors are also encouraged to submit papers describing applications. Application papers are expected to provide an indication of the real world relevance of the problem that is solved, including a description of the deployment domain, and some form of evaluation of performance, usability, or comparison to alternative approaches. Experience papers are also welcome but they must clearly state the insight into any aspect of design, implementation or management of self-* systems which is of benefit to practitioners and the SASO community. ---------------------------Program Chairs ---------------------------Tom Holvoet KU Leuven, Belgium Jeremy Pitt Imperial College London, England Ichiro Satoh National Institute of Informatics, Tokyo, Japan ? CFP RSP-2013 at ESWeek CfP: Workshop on Model-Driven and Agile Engineering for the Web (MDWE) @ ICWE 2013 ? Calls for Papers CPS Domains Architectures Secure Control Systems Multi-models Communication Embedded Software Model Integration Platforms Systems Engineering Modeling Science of Security Transportation CPS Technologies Announcement",,2020.0,,218913932,semantic_scholar
18e1739f511ab6ca4eba3c836a3ba34a5deb1972,https://www.semanticscholar.org/paper/18e1739f511ab6ca4eba3c836a3ba34a5deb1972,A Reproducible Solution for Implementing Online Laboratory Systems Through Inexpensive and Open-source Technology,"Laboratory experiences are a crucial part of the undergraduate engineering curriculum. With coursework, college programs, and professional interactions increasingly being performed online the natural evolution of a ‘digital-first’ culture suggests that traditionally hands-on educational activities should find themselves represented online as well. Transitioning laboratory-based exercises online is difficult, time consuming, and sometimes costly. In addition, the efficacy of an online laboratory experience as a worthwhile educational tool has not been explored with depth. This study focuses on the details and benefits of incorporating laboratory experiences with online infrastructure with the perspective of optimizing development time and cost. The purpose is to use FOSS (free and open source software) in addition to other open source solutions to develop modular, scalable, and easily deployable remote laboratory infrastructure capable of interacting with traditional equipment over network connections. Introduction It is commonly accepted that one of the best ways to learn technical skills is through hands-on experiences. Be it through apprenticeships, internships, laboratories, or bootcamps, an interactive experience provides concrete, engaging, and fulfilling learning opportunities. By spending time personally carrying out a task, the brain forms neural connections which make it easier to remember and duplicate the task. The understanding of cognition and epistemology has grown throughout the entirety of the history of the human race. Masters pass down skills by having pupils perform those skills according to their instruction. However, with the rise of the digital age, the question becomes, can the dissemination of all concrete knowledge be conducted via computers just as well as through physical interactions. And if so, then how? The Impact of Remote Laboratory Systems on Education The digital world has become an integral part of the lives of faculty and their students and is now irrevocably intertwined with daily routines. As such, society grows ever more comfortable interacting with the world through a digital medium and seeks to find new avenues to do so and new virtual environments to explore. Therefore, it naturally follows that transitioning the whole of education towards a system which is more frequently used by digital natives may be in the best interest of future generations. The purpose of this study is to create a case for implementing remote, on-line laboratory experiences that can successfully fill the same intellectual need as their physical counterparts. The benefit of achieving this goal is similar to that of all on-line instruction, to reach more students and to make education accessible. The chief drawback is that creating the network infrastructure necessary to implement on-line experiences as a substitute for physical laboratory work is difficult and costly. This study also seeks to find and build open-source solutions to this problem using inexpensive hardware, open-source software, and simple network configurations that may add to the list of best practices built by previous and current researchers. Impact on Students Remote laboratory systems offer unique benefits to how students retain information. By providing students with a more open platform to access knowledge, rather than traditional physical interactions, it is possible to see positive effects on engagement and learning. Nabil Lehlou et al. (2009) conducted a study in which students in two different fields (Industrial Statistics and Manufacturing Systems) performed lab exercises and recorded how the students felt they understood the material before and after the lab. The results provided a clear indicator that the students felt the remote lab system provided a beneficial educational experience as six out of eight in Industrial Statistics and eight out of eight students in Manufacturing Systems reported an increase in confidence in the subject material. In addition, five out of eight students in Industrial Statistics and eight out of eight students in Manufacturing Systems reported a drastic improvement in their confidence for their respective fields. A separate study performed by H. Vargas et al. (2010) found similar positive results. They provided 120 students across seven universities with remote laboratory experiences. The research indicated that the full lab experience included performing an actual lab over the internet, which required students to reserve a time to use the lab resources. The response by the students indicated that they enjoyed the system as well as found it useful in understanding the respective course content. According to the results, 69% of the students felt satisfied with the system and 19% felt strongly satisfied. Additionally, 51% of students felt that the remote lab was better than traditional methods, 25% felt it was equal to traditional methods, and 15% felt it was much better than traditional methods. These results strongly assert that remote laboratory experiences not only have a place in the future of education but can have a large impact on its quality. Key Features Needed To better understand what makes remote lab systems effective and their impact on students potent, it is critical to understand what key features are common among these systems. In a study performed by P. Bisták et al. (2011) at the Slovak University of Technology in Bratislava, Slovakia, it was outlined that a remote lab system server could provide the client (user) with text messages, numerical data, graphs, animations, and video clips. The system could interface with sensors and cameras in order to collect useful information and statistics for the client. The setup involved a front-end GUI being served to a client, which in turn communicated over TCP/IP to a remote server. Information could be transmitted in either direction between the server and the client with data and commands running back and forth. The server would have access to the local hardware of the lab system and be able to send any commands received from the client to the hardware. Additionally, it would be able to collect output data from the lab hardware and send it back to the client. Another remote lab study was performed by T.J. Mateo Sanguino et al. (2012) at the University of Huelva in Palos de la Frontera, Spain. In this study a similar setup was implemented with a client providing user access to a remote server, which was in turn connected to a lab system. The user would have the ability to control computer devices on a rack through this setup and perform multiple remote labs. An interesting point to make which differentiates this study from the above is that it does not send photos back to the user. The labs performed did not require cameras or video, instead relying on numerical data to provide the pertinent observation. This is an important point to make as it shows that every lab system is different and there might not be a “generic” or “one-size-fits-all” approach. If this is the case, then a truly reproducible lab system must provide means by which different hardware or software peripherals may be added or removed depending on the needed application. However, at a minimum it appears that a remote laboratory needs a client-server system and some basic means by which to send text or commands between the client and server. Making Labs More Personal As humans are social and emotional creatures, it could be argued that experiences which leverage those traits would aid in the retention of information. It could also help explain why recent concepts such as social media have become fast staples in cultures around the world. They simply exploit the natural desires of people. Similarly, despite being called “remote”, it might be possible to use remote lab systems to improve learning through social, emotional and personal growth. A study performed by C. Terkowsky et al. (2013) at TU Dortmund University in Dortmund, Germany focused on the personalization of the remote laboratory experience. They referenced a theory on education and learning called “Kolb’s Experiential Learning Cycle” wherein multiple stages of learning are introduced. These stages are Concrete Experience, Reflective Observation, Abstract Conceptualization and Active Experimentation. According to the theory, they create the “learning experience”. Armed with this information, the study introduces the concept of an E-Portfolio. This E-Portfolio provides users of remote labs with the ability to record the work they performed and document their findings. The concept of this portfolio does not stop at being a simple digital notebook, however. The study asserts that this portfolio can be used by professors to check on students’ work or be opened to the public in order to add a social dynamic. The study calls the social aspect a “community” and says that it can facilitate learning. To reinforce the main point, by adding a social aspect, be it with classmates or with the world, users will have a greater feeling of connection with their work and might retain more information. Another study performed by Z. Nedic (2013) at the University of South Australia shines light on the collaborative aspect of remote labs. The study saw international students organize themselves autonomously to complete group lab assignments and recorded their planning and communication. The results showed that students, despite being from different countries, exhibited politeness when trying to create social groups and complete the remote labs. The study gives hope to the notion of creating a more connected educational system where students from around the world participate in the same curriculum. This in turn also facilitates international cooperation and communication in the real world, as a large amount of professional communication is done remotely. One study performed by Qing Ding et al. (2017) as joint research between C",,2020.0,10.18260/1-2--34043,230678993,semantic_scholar
22d3f48350aedf87a48daafad96ea8d92c9099f3,https://www.semanticscholar.org/paper/22d3f48350aedf87a48daafad96ea8d92c9099f3,Distributed IoT Attestation via Blockchain (Extended Version),"The growing number and nature of Internet of Things (IoT) devices makes these resource-constrained appliances particularly vulnerable and increasingly impactful in their exploitation. Current estimates for the number of connected “things” commonly reach the tens of billions. The low-cost and limited computational strength of these devices can preclude security features. Additionally, economic forces and a lack of industry expertise in security often contribute to a rush to market with minimal consideration for security implications. It is essential that users of these emerging technologies, from consumers to IT professionals, be able to establish and retain trust in the multitude of diverse and pervasive compute devices that are ever more responsible for our critical infrastructure and personal information. Remote attestation is a well-known technique for building such trust between devices. In standard implementations, a potentially untrustworthy prover attests, using public key infrastructure, to a verifier about its configuration or properties of its current state. Attestation is often performed on an ad hoc basis with little concern for historicity. However, controls and sensors manufactured for the Industrial IoT (IIoT) may be expected to operate for decades. Even in the consumer market, so-called “smart” things can be expected to outlive their manufacturers. This longevity combined with limited software or firmware patching creates an ideal environment for long-lived zero-day vulnerabilities. Knowing both if a device is vulnerable and if so when it became vulnerable is a management nightmare as IoT deployments scale. For network connected machines, with access to sensitive information and real-world physical controls, maintaining some sense of a device’s lifecycle would be insightful. In this paper, we propose a novel attestation architecture, DAN: a distributed attestation network, utilizing blockchain to store and share device information. We present the design of this new attestation architecture, and describe a virtualized simulation, as well as a prototype system chosen to emulate an IoT deployment with a network of Raspberry Pi, Infineon TPMs, and a Hyperledger Fabric blockchain. We discuss the implications and potential challenges of such a network for various applications such as identity management, intrusion detection, forensic audits, and regulatory certification.",,2020.0,,214593683,semantic_scholar
fb70b32eaae4987e7230d03ec82858a8163d66c3,https://www.semanticscholar.org/paper/fb70b32eaae4987e7230d03ec82858a8163d66c3,Industrial Control System Security ( ICSS ) Workshop,"One of the hardest decisions an asset owner must make when faced with known vulnerabilities or exploits is whether to take down their plant in order to apply patches and upgrade end of life process control components. There are risks if you do (productions loss, opportunity costs, failed upgrades) and (cyber)risks if you do not. In this presentation we will discuss several options that could be considered when presented with known cyber-risks. Note: On the surface this may feel like a standard defense in depth strategy, and in some respects it is. But these strategies are meant to address specific attack techniques, known vulnerabilities and exploits, so it is better to think of these techniques as reactive rather than the defense in depth, proactive approach. Runtime Application Self Protection (RASP) is an emerging collection of approaches to address the fundamental issue with cyber-exploits, that is the ability for malicious processes to access memory where they should not be able. If you control memory access, you control an entire class of exploits (memorybased attacks) Patching – Our most traditional approach to defend against exploits in the wild Signatures – Antivirus is the most common signature-based solution. YARA rules are a way of identifying malware (or other files) by creating rules that look for certain characteristics. There are several signaturebased solutions that can be shared to slow or stop the exploitation of certain vulnerability types. Mitigations – frequently OEMs advise their customers to take specific actions in order to close off known attack vectors. Closing ports on a firewall, disabling unused services, implementing access controls, network segmentation, implementing secure protocols, etc. are all common recommendations to react to specific vulnerabilities. Security Tools – OEMS such as Schneider Electric take time to partner with security tool vendors who often bring their own unique approach to addressing active exploits Network Anomaly detection – similar to signature checking, the ability to identify an exploit on the wire before it reaches the device. Good examples are, “magic” packets that can cause crashes, buffer overflows, RCE, etc. AI/ML – an emerging technology, maligned somewhat today, but do not underestimate how this can and will be used in the future Upgrades – whether this is a component by component upgrade or a rip and replace, one way to eliminate legacy cybersecurity issues is to upgrade to the current generation Biography: Andrew Kling is an Industry Automation Product Security Officer at Schneider Electric. Andy has over three decades of software development experience, having worked in multiple industries. He has worked in the R&D organization of Schneider Electric since 2001. Currently, Andy leads the Industry Automation business unit in cybersecurity. At Schneider Electric, Andy has managed many process control engineering teams. As a result of this experience, Andy has ushered the Schneider Electric Development organization to comply with ISA 62443 standards at the process, product, and system levels, achieving several world firsts along the way. Andy has a Master’s Degree in Software Engineering – Artificial Intelligence from Northeastern University and a Bachelor’s of Science in Information Sciences from the University of Massachusetts, Lowell. In addition, Andy is a participating Senior member of ISA, primarily contributing to the ISA 62443 cybersecurity standards and the ISA Global Cybersecurity Alliance. 9:15 K7: A Protected Protocol for Industrial Control Systems that Fits Large Organizations, Eli Biham, Sara Bitan, and Alon Dankner, Israel Technion ABSTRACT: One of the main obstacles of securing industrial control systems is the lack of an appropriate security model that is both implementable by vendors and addresses the inherent security and usability issues needed by organizations. Current solutions such as device passwords and IPSec lack scalable key management infrastructure and [inc granularity access control mechanisms. In this paper we propose a novel security model for industrial control systems that supports organizational level authorizations and authentication requirements, while hiding the low—level details (e.g., keys and passwords) from the One of the main obstacles of securing industrial control systems is the lack of an appropriate security model that is both implementable by vendors and addresses the inherent security and usability issues needed by organizations. Current solutions such as device passwords and IPSec lack scalable key management infrastructure and [inc granularity access control mechanisms. In this paper we propose a novel security model for industrial control systems that supports organizational level authorizations and authentication requirements, while hiding the low—level details (e.g., keys and passwords) from the users. It also allows to easily add and remove l’I.Cs, engineering stations, HMI devices and users, and assign permissions to them. A major advantage is its support for hybrid ICS systems, and the simple ability to upgrade the security of legacy devices to functionality of new secure protocol. Without loss of generality, we base our protocol, K7, on the Siemens S7 protocol, and enhance it with new cryptographic features to support the extra functionality. We use a ticket-based system (e.g., Kerberos with LDAP server) to support the exchange of permissions and keys, and incorporate it into our protocol. To prove our solution, we implemented K7 as a protocol converter add-on to standard Siemens clients and PLCs that transform them into augmented devices that use K7. We hope that Siemens and other vendors will add direct support for K7 on their ICS systems. 9:40 What and Where to Monitor for Intrusion Detection in Industrial Control Networks, Alvaro Cardenas, University California Santa Cruz ABSTRACT: In this presentation we will look at two related problems for intrusion detection in control systems: where to monitor the system to detect anomalies, and what to monitor, in order to obtain an accurate picture of the real world. We first discuss what we can and cannot detect depending on the location of our network monitor, and identify locations that maximize our visibility to attacks. We also propose the addition of hidden sensor measurements to a system to improve its security. Hidden sensor measurements are by our definition measurements that were not considered in the original design of the system, and are not used for any operational reason. We only add them to improve the security of the system and using them in anomaly detection and mitigation. In this presentation we will look at two related problems for intrusion detection in control systems: where to monitor the system to detect anomalies, and what to monitor, in order to obtain an accurate picture of the real world. We first discuss what we can and cannot detect depending on the location of our network monitor, and identify locations that maximize our visibility to attacks. We also propose the addition of hidden sensor measurements to a system to improve its security. Hidden sensor measurements are by our definition measurements that were not considered in the original design of the system, and are not used for any operational reason. We only add them to improve the security of the system and using them in anomaly detection and mitigation. Biography: Alvaro A. Cardenas is an Associate Professor of Computer Science and Engineering at the University of California, Santa Cruz. Before joining UCSC, he was the Eugene McDermott Associate Professor of Computer Science at the University of Texas at Dallas. Earlier in his career, he was a postdoctoral scholar at the University of California, Berkeley, and a research staff member at Fujitsu Laboratories. He holds M.S. and Ph.D. degrees from the University of Maryland, College Park, and a B.S. from Universidad de Los Andes in Colombia. His research interests focus on cyber-physical systems and IoT security and privacy. He is the recipient of the NSF CAREER award, the 2018 faculty excellence in research award from the Erik Johnson School of Engineering and Computer Science, and the Eugene McDermott Fellow Endowed Chair at the University of Texas at Dallas. He has also received best paper awards from the IEEE Smart Grid Communications Conference and the U.S. Army Research Conference. One of his papers was also a finalist to the CSAW competition in Israel. 10:05 Break (30 minutes) 10:35 Securing Critical Infrastructure: Challenges and Opportunities, Jianying Zhou, Singapore University of Technology and Design ABSTRACT: Critical infrastructure becomes a strategic target in the midst of a cyber-war. Governments are investing significantly in response to the risks and challenges while researchers and vendors are aggressively developing and marketing new technologies aimed at protecting critical infrastructure. In this talk, I will briefly describe the framework and features of a cyber-physical system (CPS) which serves as the core to provide critical services in different industrial domains. Then I will discuss the challenges we face and the approaches we can take to defend against cyber attacks. After that I will present a few novel technologies developed in iTrust for preventing and detecting attacks to CPS. I will further introduce the fully operational CPS testbeds in iTrust, and show how the testbeds are used to validate the security technologies so that the owners and operators of critical infrastructure can be confident that the technologies to be deployed will actually protect their systems in the event of a cyber-war. Critical infrastructure becomes a strategic target in the midst of a cyber-war. Governments are investing significantly in response to the risks and challenges while researchers and vendors are aggressively developing and marketing new technologies aimed at protecting critical infrastru",,2020.0,,227319385,semantic_scholar
e8ad5f723fe13574614be08c5a8aeed5602e7b97,https://www.semanticscholar.org/paper/e8ad5f723fe13574614be08c5a8aeed5602e7b97,Vulnerability Discovery in Embedded Systems,"Objective Embedded systems are omnipresent in our everyday life. For example, they are the core of various Common-Off-The-Shelf (COTS) devices such as printers, mobile phones, home appliances, and computer components and peripherals. They are also present in many devices that are less consumer oriented such as video surveillance systems, medical implants, automotive elements, military systems, SCADA and PLC devices, and basically anything we usually call “electronics”. Moreover, the emerging phenomenon of the Internet-of-Things (IoT) will make them even more widespread and interconnected. The security and reliability of this broad range of devices is paramount to ensure both the proper functioning of our society and our physical safety. Unfortunately, embedded devices did not reach yet the same level of security we obtained for software of typical personal computers. For example, because of the very heterogeneous set of platforms and architectures, embedded systems still lack a solid set of vulnerability discovery and analysis techniques. The goal of this thesis is to bridge this gap by improving the state of the art of vulnerability discovery in software binaries. In particular, the student will focus on the development of novel static and dynamic analysis techniques that can be applied to the study of real-world, complex firmware images. The proposed approach will need to cope with a number of challenges, ranging from scalability issues, heterogeneity of targets, need for low false positive rates, and the intrinsic difficulty of running dynamic analysis on real embedded devices. To ensure the deployability of the developed techniques, real examples and test cases for the Ph.D. research will be provided by a close industrial support and collaboration with Siemens. Background and PreviousWork The work performed in this thesis builds upon two lines of research which are ongoing in our group at Eurecom. The first is related to the use and application of advanced dynamic analysis techniques on the firmware of embedded devices. Zaddach et al. designed and implemented and open source system named Avatar [1], whose goal is to execute a firmware inside an instrumented emulator. Emulating firmwares of embedded devices requires accurate models of all hardware components used by the system under analysis. Unfortunately, the lack of documentation and the large variety of hardware on the market make this approach infeasible in practice. Avatar fills this gap and overcomes the limitation of pure firmware emulation by acting as an orchestration engine between the physical device and an external emulator [7]. By injecting a 1 special software proxy in the embedded device, Avatar can execute the firmware instructions inside the emulator while channeling the I/O operations to the physical hardware. Since it is infeasible to perfectly emulate an entire embedded system and it is currently impossible to perform advanced dynamic analysis by running code on the device itself, Avatar takes a hybrid approach. It leverages the real hardware to handle I/O operations, but extracts the firmware code from the embedded device and emulates it on an external machine. A similar architecture, but supported by a FPGA bridge to increase the throughput, was used by Koscher et al. [5] in their Surrogates system.",,2020.0,,211526969,semantic_scholar
21a98f40124f50173239ff63601426b5d948d76a,https://www.semanticscholar.org/paper/21a98f40124f50173239ff63601426b5d948d76a,Modeling and Simulation in the Systems Engineering Life Cycle,"structural component that it represented. For example, the sorin centrifugal pump is a particular kind of blood pump used in the CHOA centrifugal pump circuit. In the SysML model, the part property for the sorin centrifugal pump redefines the abstract blood pump part property of the ECMO circuit block. With these crosscutting relationships defined, any element of an ECMO circuit design is traceable from source materials to requirements, from requirements to behaviors, from behaviors to abstract structural element, and finally from abstract structural element to a specific make and model of a component. This end-to-end traceability, shown in Fig. 27.10, provides an objective mechanism for determining whether a particular circuit design meets its requirements and a mechanism for performing impact analysis in the event that requirements, best practices, or technology change. Future iterations of the model may incorporate more full life cycle aspects of the ECMO domain such as training and maintenance and may expand impact analysis. 27.10 Future Updates to the ECMO MBSE Framework The SysML diagram not currently represented in the ECMO MBSE framework is the parametric diagram. A parametric diagram allows for applying constraints to block value properties, and allows external engineering models to be plugged into the SysML model for analysis. For example, we could add parametric diagrams to compare the throughput of blood or medications through various circuit designs, to analyze the amount of pressure the circuit exerts on blood, to analyze which circuit designs minimize the amount of foreign surface area that the patient’s blood contacts, or to analyze the volume of blood required to prime each circuit before it is used. Fig. 27.10  Crosscutting relationships 364 N. L. Adams et al. 27.11 Leveraging the ECMO MBSE Framework One of the goals of the ECMO SE project is to leverage ELSO registry case study data to improve ECMO medical decision making. One way to accomplish this is to provide a way for ECMO specialists to capture circuit designs and then associate a circuit design with each case study submitted to the ELSO registry. The addition of circuit design details will enhance the ELSO registry data so that practitioners can perform analysis to determine which existing ECMO circuit design, if any, outperforms other existing circuit designs. SysML provides systems engineers with a rich environment for capturing and modeling systems of interest. However, a “user friendly” tool is required for ECMO specialists to capture and submit their circuit designs. For example, while the IBD shown in Fig. 27.9 accurately captures all aspects of the circuit architecture and integrates into the formal MBSE framework for ECMO, it is a highly technical view and does not address the customer need for a simple “non-technical” interface. The following two statements capture the need to improve ECMO safety by providing a way to evaluate existing ECMO circuits to identify a possible best of breed. ECMO program need: “The medical community needs a safer ECMO circuit that is more reliable, more autonomous, and easier to deploy in order to increase the quality of ECMO treatments, promote survivability of patients with severe respiratory and cardiac conditions, and to reduce the use of lung-damaging ventilators on patients.” EDCT project need: “The ECMO community needs a way to characterize the performance of current ECMO circuits across the world in order to identify and correct design flaws, increase system reliability, and to support the development of a standardized next-generation ECMO circuit.” The solution for this problem became known as the EDCT, and we designed the EDCT as an easy-to-use, web-based interface for constructing and capturing ECMO circuit designs. We created two models to support concept development. First, we created a comprehensive SysML model to define key ECMO components and their interrelationships. Second, we created a consumer-friendly model in the form of a prototype of the web-based user interface (UI). This prototype presents the end-user with the look and feel of the application. Key features of this model include visually accurate component icons, drag-and-drop functionality, and UI layout. We then used these models to refine design concepts and evaluate the user interface to optimize the user experience. We leveraged the existing SysML ECMO framework to explore possible solutions, refine concepts, and reduce development risk of the EDCT software application. Working closely with an ELSO representative, we used SysML to create a comprehensive model for the EDCT. Figure 27.11 shows the high-level use case for the EDCT web application. We then created a prototype of the EDCT UI, shown in Fig. 27.12, to reduce much of the uncertainty regarding “look and feel,” host hardware and software, and web browser compatibility. Most importantly, the prototype model provides a 365 27 Model-Based Systems Engineering mechanism for communication between our team, the project sponsor, and potential end users. We represent the physical ECMO circuit graphically using the EDCT user interface. Using this application, the specialist creates a virtual circuit to document the layout and component detail of the actual circuit. The UI provides a userfriendly experience but it does not sufficiently describe the circuit. To mitigate this, we developed a high-fidelity engineering representation of the circuit to accurately Fig. 27.11  EDCT use case diagram Fig. 27.12  EDCT prototype website 366 N. L. Adams et al. record the circuit architecture so that components and the assembly order can be stored in a database and retrieved for later analysis. The bridge between the UI and a database compatible schema is a circuit node map, shown in Fig. 27.13. We devised the node map to define each component and its relationship to other components. In this representation, we assign each component an identifier such as GS1 (gas sensor #1), Pat1 (patient #1), and T11 (tube #11). Nodes between each component possess an integer identifier. In this way, we associate each component with the nodes and the architecture can be stored electronically without ambiguity. Fig. 27.13 shows the node map for the UI circuit shown in Fig. 27.12 and is useful when rendering circuit assemblies. This model representation is not intended for the end user, and when used in the EDCT user interface, each component is replaced with its corresponding device icon for easier understanding. The design of the EDCT provides a way for ECMO specialists and physicians to gather real-world circuit data to improve decision making for future ECMO circuit implementations. Key features desired for the actual implementation of the tool include integration with the ELSO registry, a simple user interface, vetted circuit elements, provisional device capability, integrated circuit error checking and low/ no cost to implement at ECMO centers around the world.","Simulation Foundations, Methods and Applications",2015.0,10.1007/978-1-4471-5634-5,42687410,semantic_scholar
de3ec2ed31c2a2cbe4d38d044b90ef5f1b36648d,https://www.semanticscholar.org/paper/de3ec2ed31c2a2cbe4d38d044b90ef5f1b36648d,"Mobile technologies in the study, assessment, and treatment of schizophrenia.","Mobile technologies are developing at a phenomenal rate and hold tremendous promise for transforming schizophrenia research and treatment. Over the last decade, mobile devices including microcomputers, mobile phones, and smartphones have become ubiquitous. The United Nations’ telecommunication agency recently reported that mobile phone subscriptions have reached almost 6 billion worldwide.1 Developing countries now account for close to 3 quarters of the mobile phones in use, and in some developed countries, the number of mobile phones already exceeds the size of the population, with many individuals owning multiple mobile devices. Recent national surveys in the United States found that mobile devices are helping bridge the digital information divide between various socioeconomic groups, as several traditionally underserved populations who typically could not afford access to home computers and internet packages now often use smartphones as their primary connection to information on the Internet. Remarkably, there is emerging evidence that many chronically homeless individuals now also use mobile devices regularly. The characteristics of contemporary mobile technologies (ie, portability, self-contained power source, increasingly user-friendly design) allow for something quite revolutionary—they enable us to transport research, assessment, and treatment out of the laboratories and clinics and into the real-time/real-world context in which individuals negotiate their daily lives and contend with chronic psychiatric illnesses and functional impairment. As infrastructure for mobile telecommunication continues to develop globally, it will create opportunities for far-reaching implementation of evidence-based interventions and wide-scale dissemination of information and resources in a manner that is unprecedented. 
 
The inherent advantages of mobile technologies are not going unnoticed by researchers, clinicians, and forward-thinking policy makers. The incorporation of various mobile devices in support of prevention and treatment initiatives across biomedical and behavioral disciplines is growing rapidly,2,3 the National Institutes of Health has recently begun to offer specialized training and funding opportunities focusing specifically on Mobile Health (mHealth) research, The Center for Medicare and Medicaid Services is exploring new payment models that may allow for expanded reimbursement of technology-based services, and the US Food and Drug Administration has already released statements regarding guidelines for regulating the use of some mobile devices and applications. 
 
While enthusiasm for utilization of mobile technology in research and clinical care is gaining momentum across a wide array of physical and mental health disciplines, many schizophrenia researchers and clinicians remain skeptical about the ability or willingness of patients with psychotic illnesses to comply with mobile research protocols or engage in mobile interventions. This apprehension is largely unfounded, and evidence suggests that given opportunity and appropriate training, many individuals with schizophrenia can and will use various mobile technologies successfully, even when they are quite symptomatic. The purpose of this special issue is to cultivate discussion about new opportunities for leveraging existing and emerging mobile technologies in the study of psychotic illnesses and to encourage investigators to think creatively about how these novel approaches can improve our understanding of the etiological risk factors, contextual influences, and possible treatments for schizophrenia. 
 
In the first article in this collection, we have asked Kimhy et al4 to discuss the rationale for mobile technology research in schizophrenia and provide concrete guidelines and practical suggestions for studies with this population. Their expert insights and shared collective experiences will undoubtedly be useful to investigators who are unfamiliar with mobile technology study design, hardware and software requirements, and statistical approaches necessary to successfully analyze the rich data that are characteristic of these paradigms. In the articles that follow, investigators demonstrate how 3 generations of mobile devices, including preprogrammed wristwatches used in conjunction with paper-and-pencil diaries, microcomputers, and mobile phones, can be effectively deployed for mobile research and treatment development. Ben-Zeev and colleagues5 use mobile technology to assist in the evaluation of patient clinical self-reports. They compare real-time/real-place momentary mobile assessments collected over 7 days in individuals with schizophrenia and a nonclinical comparison group to retrospective reports provided by both groups for the same period of time. Their findings indicate that study compliance in individuals with schizophrenia can be exceptionally high and that various dimensions of one’s symptomological and emotional experience are not well captured by traditional reports and better captured by momentary mobile assessments. Oorschot and colleagues6 and Granholm and colleagues7 deploy mobile devices in a therapeutic context. Oorschot and associates demonstrate how mobile data can be used to elucidate idiosyncratic symptom patterns and dynamic changes within individuals longitudinally, and articulate how this approach can augment face-to-face treatments by improving the therapeutic relationship between clinician and patient, providing important information for psychoeducation and treatment personalization. Granholm and colleagues7 report on an innovative automated mobile assessment and intervention for schizophrenia. In their pilot study, text messages sent from a remote preprogrammed server to patients’ mobile phones are used to administer cognitive-behavioral interventions in support of medication adherence, social functioning, and coping with auditory hallucinations. In the context of a growing body of evidence suggesting symptoms, affect, and functioning in schizophrenia are not nearly as static as previously believed, such paradigms may prove to be an especially powerful tool for identification of risk or preventive factors that could perhaps be targeted with real-time mobile interventions. 
 
As mobile devices infiltrate more and more areas of life of the general population, they will undoubtedly become more prevalent among those with schizophrenia as well. Statistically, many of those who are currently growing up with these technologies in hand will go on to develop serious mental illnesses in the future. Looking forward, now is the opportune time for innovative investigators and clinicians to examine how these emerging technologies can be harnessed as a powerful new platform for research and treatment approaches that can be made available in the years ahead.",Schizophrenia bulletin,2012.0,10.1093/schbul/sbr179,32016055,semantic_scholar
c7e55c148f1de403cdb8068c329ec8a039ba5ab9,https://www.semanticscholar.org/paper/c7e55c148f1de403cdb8068c329ec8a039ba5ab9,Preparing for Operational Use of C2-Simulation Interoperation,"Technical Activities in the NATO MSG are completing sustained development of a standards-based capability, known as C2SIM, for coalitions to interoperate their national command and control (C2) and simulation systems collectively as part of NATO’s Federated Mission Network (FMN). This form of synthetic battlespace can have a great impact on the effectiveness of coalition military operations. The second generation of C2SIM standards from SISO is ready for balloting and afterwards will form the basis of a STANAG. MSG-145 is conducting extensive testing to validate these standards. Several closely related capabilities developed in testing also can ease the path to operational use. This paper describes those capabilities: (1) C2SIM within Modelling and Simulation as a Service (MSaaS); (2) adoption by the MSCOE of the C2SIM Sandbox distributed development platform; (3) using C2SIM to support operational training in a cyber-active environment; and (4) extension of C2SIM into different domains, exemplified by an Autonomous Systems Extension. C2SIM provides a powerful new, standardsbased capability for coalition simulations to support collective training (including cyber effects), planning (including mission analysis) and increasingly important in command and control of Autonomous Systems for military operations. This paper provides important information to prepare for its operational military use. 1.0 INTRODUCTION: C2SIM OVERVIEW The ability to interoperate command and control (C2 or Mission Command) systems with simulation systems has been an important goal for more than a decade [1]. Over those years the NATO Modelling and Simulation Group (NMSG) has been cooperating with the Simulation Interoperability Standards Organization (SISO) to develop, prototype, and test standards that support that capability. Their shared vision is that members of a coalition will be able to combine their C2 systems and simulation systems collectively into a system-ofsystems where simulations are tasked by the C2 systems and in turn provide reports that are displayed on the C2 system just as they would appear due to real-world operations. The resulting system of systems can support training, course of action analysis, and mission rehearsal for the coalition. Each force element uses the C2 system with which it has trained and is represented by a simulation that represents well its doctrine, resources, and tactics/techniques/procedures. Sharing information this way will result in more effective coalition operations that can happen sooner [2, 3]. Preparing for Operational Use of C2-Simulation Interoperation 2 STO-MP-MSG-171 APPROVED FOR PUBLIC RELEASE APPROVED FOR PUBLIC RELEASE Standards enabling the vision described above are well along in development by SISO and expected to reach the balloting phase by the end of 2019. In order to finalize effective standards, the NATO Technical Activity MSG-145 Operationalization of Command and Control – Simulation Interoperation (C2SIM) undertook a validation process. This paper describes that process, beginning with the roles and motivations of NATO and SISO, then providing background on C2SIM. After that we will look at the activities of the eight national teams involved most recently and then explain how they enabled validation of C2SIM through a coordinated effort that provided compliant interfaces on six different simulations and one C2 system as well as supporting software. The validation effort took these C2SIM-enabled systems to the NATO Coalition Warrior Interoperability Exercise (CWIX) for detailed testing and then culminated with experimentation, structured as a miniature exercise in distributed mission planning. The paper concludes with lessons learned from the validation process and a view toward the future of C2SIM-based coalition interoperability. Figure 1 shows the general architecture of a C2SIM coalition. The C2 systems interoperate using a C2 standard; the simulation systems interoperate using a simulation standard; and the system of systems interoperates using C2SIM. A web service is used to replicate C2SIM messages for distribution among the constituent systems and to produce a log that documents results of the operation. The draft C2SIM standard consists of a text document defining rules and procedures for interoperation and for maintenance of the ontologies; a Core ontology consisting of data classes expected to be needed by any operational simulation; a Standard Military Extension (SMX) with classes applicable to all domains of military activity; and a Land Operations Extension (LOX) to encompass the capability originally provided by MSDL and C-BML and also to serve as an exemplar for future extensions. SMX is logically part of the main C2SIM standard, while LOX forms a new layer on top of Core+SMX. Figure 1: C2SIM Coalition General Architecture 2.0 NATO AND SISO ROLES DEVELOPING C2SIM The partnership between MSG-145 and the SISO C2SIM Product Development Group (PDG) has been critical in reaching a point where C2SIM can be validated, and even more critical in the validation process. As Preparing for Operational Use of C2-Simulation Interoperation STO-MP-MSG-171 3 APPROVED FOR PUBLIC RELEASE NATO UNCLASSIFIED RELEASABLE TO AUSTRALIA AND SWEDEN a collaborative organization of industry, academic and government people, SISO does not have the ability to develop working prototype systems-of-systems or to validate them with international military participation. Conversely, NATO is not in a position to develop industry-based standards. Cooperative work between the two has been needed to create the C2SIM standard. MSG-145 is the third in a sequence of NATO Technical Activities that has supported development of C2SIM. The first, MSG-048 Coalition Battle Management Language, completed validation of the technical feasibility of coalition C2-simulation interoperation. The second, MS-085 Standardization for C2-Simulation Interoperation supported and tested the first generation of C2-simulation interoperation standards: the Military Scenario Definition Language (MSDL) [4] and the Coalition Battle Management Language (C-BML) [5]. A key outcome of MSG-085 was the determination that while MSDL and C-BML can be made to work together, a second-generation standard was needed to achieve effective harmonization; also that the second generation should be designed for extensibility [6]. SISO responded by forming a merged PDG with a charter to achieve these things under the unified name C2SIM [7]. A goal of MSG-145 is to base a NATO Standardization Agreement (STANAG) on the C2SIM industry standard. SISO’s activities to create C2SIM have been based on a complete bottom-up review of both C-BML and MSDL with a view to the result serving as the basis for a family of extensions. The C2SIM PDG concluded that the best way to approach this was developing a consistent family of ontologies. Development has been underway since 2014 and recently produced a set of draft ontologies that is ready for implementation, along with an approach to extracting a standard XML schema from the ontologies to support implementation for validation. 3.0 C2SIM AS A SERVICE UNDER MSAAS Modelling and Simulation as a Service (MSaaS) is a new approach being explored by the STO NMSG Panel for a permanently available, flexible, service-based framework to provide more cost effective availability of Modelling and Simulation (M&S) products, data and processes to a large number of users on-demand. The NATO MSG-136 Modelling and Simulation as a Service Implementation defined MSaaS as “the combination of service-based approaches with ideas taken from cloud computing” [9]. MSG-145 defined the C2SIM Integration Platform (IP) Reference Architecture (RA) using both the NATO C3 Taxonomy [10] and the MSaaS Reference Architecture from NATO MSG-136 [9] as a source for Architecture Building Blocks (ABBs) and Architecture Patterns. The basic idea is to provide C2SIM as a service, defining ABBs linked to the NATO C3 Taxonomy and the M&S extensions defined by NATO MSG136. Examples of defined ABBs are Message-Oriented Middleware Service (functionality to support the exchange of messages between data producers and consumers, independent of the message format and content) or Mediation Services (middle layer between incompatible producers and consumers of information), built in the system-of-systems experimented by MSG 145. An experimental platform to provide “C2SIM as a service” is that developed by NATO Modelling and Simulation Centre of Excellence (MSCOE) in collaboration with the Leonardo company. This is a MSaaS cloud-based testbed prototype, named Open Cloud Environment ApplicatioN (OCEAN). It offers an embryonic framework made of a combination of hardware, software and services to automate the deployment of M&S tools and applications in a cloud environment. The OCEAN platform offers a unique point of access through a web portal with secure access granted by a user identity management system. The availability of services is managed by an M&S services management system that facilitates the delivery, versioning, testing, Preparing for Operational Use of C2-Simulation Interoperation 4 STO-MP-MSG-171 APPROVED FOR PUBLIC RELEASE APPROVED FOR PUBLIC RELEASE consumption, termination and disposal of services as shown in Figure 2. The system architecture involves the use of a hybrid cloud where the user can mix use of physical machines, virtual machines and containers (Figure 2) by means of a Platform as a Service solution based on OpenStack installed inside a VMware cluster. OCEAN is expected to provide C2SIM as a Service for experimental purposes by the end of 2019. Figure 2: OCEAN architecture 4.0 TRANSITIONING C2SIM SANDBOX TO MSCOE Under the MSG-145 activity, an integration and testing environment called C2SIM Sandbox was developed by George Mason University (GMU) to provide a full C2SIM capability, available via a virtual private network (VPN) by remote desk",,2019.0,,208540295,semantic_scholar
e49d64c86166a828381df783776cabd30d78b902,https://www.semanticscholar.org/paper/e49d64c86166a828381df783776cabd30d78b902,Resource-Aware Predictive Models in Cyber-Physical Systems,"Author(s): AMIR, MARAL | Advisor(s): Givargis, Tony | Abstract: Cyber-Physical Systems (CPS) are composed of computing devices interacting with physical systems. Model-based design is a powerful methodology in CPS design in the implementation of control systems. For instance, Model Predictive Control (MPC) is typically implemented in CPS applications, e.g., in path tracking of autonomous vehicles. MPC deploys a model to estimate the behavior of the physical system at future time instants for a specific time horizon. Ordinary Differential Equations (ODE) are the most commonly used models to emulate the behavior of continuous-time (non-)linear dynamical systems. A complex physical model may comprise thousands of ODEs that pose scalability, performance and power consumption challenges. One approach to address these model complexity challenges are frameworks that automate the development of model-to-model transformation. In this dissertation, a state-based model with tunable parameters is proposed to operate as a reconfigurable predictive model of the physical system. Moreover, we propose a run-time switching algorithm that selects the best model using machine learning. We employed a metric that formulates the trade-off between the error and computational savings due to model reduction. Building statistical models are constrained to having expert knowledge and an actual understanding of the modeled phenomenon or process. Also, statistical models may not produce solutions that are as robust in a real-world context as factors outside the model, like disruptions would not be taken into account. Machine learning models have emerged as a solution to account for the dynamic behavior of the environment and automate intelligence acquisition and refinement. Neural networks are machine learning models, well-known to have the ability to learn linear and nonlinear relations between input and output variables without prior knowledge. However, the ability to efficiently exploit resource-hungry neural networks in embedded resource-bound settings is a major challenge.Here, we proposed Priority Neuron Network (PNN), a resource-aware neural networks model that can be reconfigured into smaller sub-networks at runtime. This approach enables a trade-off between the model's computation time and accuracy based on available resources. The PNN model is memory efficient since it stores only one set of parameters to account for various sub-network sizes. We propose a training algorithm that applies regularization techniques to constrain the activation value of neurons and assigns a priority to each one. We consider the neuron's ordinal number as our priority criteria in that the priority of the neuron is inversely proportional to its ordinal number in the layer. This imposes a relatively sorted order on the activation values. We conduct experiments to employ our PNN as the predictive model in a CPS application. We can see that not only our technique will resolve the memory overhead of DNN architectures but it also reduces the computation overhead for the training process substantially. The training time is a critical matter especially in embedded systems where many NN models are trained on the fly.",,2019.0,,238130416,semantic_scholar
59e10d1d4cd454635914cfd0ac5160a318fd0473,https://www.semanticscholar.org/paper/59e10d1d4cd454635914cfd0ac5160a318fd0473,UB09 Session 9,"In the domain of Wireless Sensor Networks (WSN), providing an effective security solution to protect the motes and their communications is challenging. Due to the hard constraints on performance, storage and energy consumption, normal network-security related techniques cannot be applied. Focusing on the ""Intrusion Detection"" problem, we propose a realworld application of our WSN Intrusion Detection System (WIDS). WIDS exploits the Weak Process Models to classify potential security issues in the WSN and to notify the operators when an attack tentative is detected. In this demonstration, we show how our IDS works, how it detects some basic attacks and how the IDS can evolve to fullfil the needs of secure WSN deployments. Download Paper (PDF) UB09.2 RESCUE: EDA TOOLSET FOR INTERDEPENDENT ASPECTS OF RELIABILITY, SECURITY AND QUALITY IN NANOELECTRONIC SYSTEMS DESIGN Authors: Cemil Cem Gürsoy1, Guilherme Cardoso Medeiros2, Junchao Chen3, Nevin George4, Josie Esteban Rodriguez Condia5, Thomas Lange6, Aleksa Damljanovic5, Raphael Segabinazzi Ferreira4, Aneesh Balakrishnan6, Xinhui Anna Lai1, Shayesteh Masoumian7, Dmytro Petryk3, Troya Cagil Koylu2, Felipe Augusto da Silva8, Ahmet Cagri Bagbaba8 and Maksim Jenihhin1 1Tallinn University of Technology, EE; 2Delft University of Technology, NL; 3IHP, DE; 4BTU Cottbus-Senftenberg, DE; 5Politecnico di Torino, IT; 6IROC Technologies, FR; 7Intrinsic ID B.V., NL; 8Cadence Design Systems GmbH, DE Abstract The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF)The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF) UB09.3 ASAM: AUTOMATIC SYNTHESIS OF ALGORITHMS ON MULTI CHIP/FPGA WITH COMMUNICATION CONSTRAINTS Authors: Amir Masoud Gharehbaghi, Tomohiro Maruoka, Yukio Miyasaka, Akihiro Goda, Amir Masoud Gharehbaghi and Masahiro Fujita, The University of Tokyo, JP Abstract Mapping of large systems/computations on multiple chips/multiple cores needs sophisticated compilation methods. In this demonstration, we present our compiler tools for multi-chip and multi-core systems that considers communication architecture and the related constraints for optimal mapping. Specifically, we demonstrate compilation methods for multi-chip connected with ring topology, and multi-core connected with mesh topology, assuming fine-grained reconfigurable cores, as well as generalization techniques for large problems size as convolutional neural networks. We will demonstrate our mappings methods starting from data-flow graphs (DFGs) and equations, specifically with applications to convolutional neural networks (CNNs) for convolution layers as well as fully connected layers. Download Paper (PDF) UB09.4 HEPSYCODE-MC: ELECTRONIC SYSTEM-LEVEL METHODOLOGY FOR HW/SW CO-DESIGN OF MIXED-CRITICALITY EMBEDDED SYSTEMS Authors: Luigi Pomante1, Vittoriano Muttillo1, Marco Santic1 and Emilio Incerto2 1Università degli Studi dell'Aquila DEWS, IT; 2IMT Lucca, IT Abstract Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF)Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF) UB09.5 CS: CRAZYSQUARE Authors: Federica Caruso1, Federica Caruso1, Tania Di Mascio1, Alessandro D'Errico1, Marco Pennese2, Luigi Pomante1, Claudia Rinaldi1 and Marco Santic1 1University of L'Aquila, IT; 2Ministry of Education, IT Abstract CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF)CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF) UB09.6 LABSMILING: A SAAS FRAMEWORK, COMPOSED OF A NUMBER OF REMOTELY ACCESSIBLE TESTBEDS AND RELATED SW TOOLS, FOR ANALYSIS, DESIGN AND MANAGEMENT OF LOW DATA-RATE WIRELESS PERSONAL AREA NETWORKS BASED ON IEEE 802.15.4 Authors: Carlo Centofanti, Luigi Pomante, Marco Santic and Walter Tiberti, University of L'Aquila, IT Abstract Low data-rate wireless personal area networks (LR-WPANs) are constantly increasing their presence in the fields of IoT, wearable, home automation, health monitoring. The development, deployment and testing of SW based on IEEE 802.15.4 standard (and derivations, e.g. 15.4e), require the exploitation of a testbed as the network grows in complexity and heterogeneity. This demo shows LabSmiling: a SaaS framework which connects testbeds deployed in a real-world-environment and the related SW tools that make available a meaningful (but still scalable) number of physical devices (sensor nodes) to developers. It provides a comforta",,2019.0,,195809644,semantic_scholar
c613158391c247d42c45c79a83120a817183611b,https://www.semanticscholar.org/paper/c613158391c247d42c45c79a83120a817183611b,Proposition de recherche doctorale Vulnerability Discovery in Embedded Systems,"Objective Embedded systems are omnipresent in our everyday life. For example, they are the core of various Common-Off-The-Shelf (COTS) devices such as printers, mobile phones, home appliances, and computer components and peripherals. They are also present in many devices that are less consumer oriented such as video surveillance systems, medical implants, automotive elements, military systems, SCADA and PLC devices, and basically anything we usually call “electronics”. Moreover, the emerging phenomenon of the Internet-of-Things (IoT) will make them even more widespread and interconnected. The security and reliability of this broad range of devices is paramount to ensure both the proper functioning of our society and our physical safety. Unfortunately, embedded devices did not reach yet the same level of security we obtained for software of typical personal computers. For example, because of the very heterogeneous set of platforms and architectures, embedded systems still lack a solid set of vulnerability discovery and analysis techniques. The goal of this thesis is to bridge this gap by improving the state of the art of vulnerability discovery in software binaries. In particular, the student will focus on the development of novel static and dynamic analysis techniques that can be applied to the study of real-world, complex firmware images. The proposed approach will need to cope with a number of challenges, ranging from scalability issues, heterogeneity of targets, need for low false positive rates, and the intrinsic difficulty of running dynamic analysis on real embedded devices. To ensure the deployability of the developed techniques, real examples and test cases for the Ph.D. research will be provided by a close industrial support and collaboration with Siemens. Background and PreviousWork The work performed in this thesis builds upon two lines of research which are ongoing in our group at Eurecom. The first is related to the use and application of advanced dynamic analysis techniques on the firmware of embedded devices. Zaddach et al. designed and implemented and open source system named Avatar [1], whose goal is to execute a firmware inside an instrumented emulator. Emulating firmwares of embedded devices requires accurate models of all hardware components used by the system under analysis. Unfortunately, the lack of documentation and the large variety of hardware on the market make this approach infeasible in practice. Avatar fills this gap and overcomes the limitation of pure firmware emulation by acting as an orchestration engine between the physical device and an external emulator [7]. By injecting a 1 special software proxy in the embedded device, Avatar can execute the firmware instructions inside the emulator while channeling the I/O operations to the physical hardware. Since it is infeasible to perfectly emulate an entire embedded system and it is currently impossible to perform advanced dynamic analysis by running code on the device itself, Avatar takes a hybrid approach. It leverages the real hardware to handle I/O operations, but extracts the firmware code from the embedded device and emulates it on an external machine. A similar architecture, but supported by a FPGA bridge to increase the throughput, was used by Koscher et al. [5] in their Surrogates system.",,2019.0,,201110343,semantic_scholar
d0057c131d7356ef14863c03c6a5e1063755d657,https://www.semanticscholar.org/paper/d0057c131d7356ef14863c03c6a5e1063755d657,A4WSN: an architecture-driven modelling platform for analysing and developing WSNs,"This paper proposes A4WSN, an architecture-driven modelling platform for the development and the analysis of wireless sensor networks (WSNs). A WSN consists of spatially distributed sensor nodes that cooperate in order to accomplish a specific task. Sensor nodes are cheap, small, and battery-powered devices with limited processing capabilities and memory. WSNs are mostly developed directly on the top of the operating system. They are tied to the hardware configuration of the sensor nodes, and their design and implementation can require cooperation with a myriad of system stakeholders with different backgrounds. The peculiarities of WSNs and current development practices bring a number of challenges, ranging from hardware and software coupling, limited reuse, and the late assessment of WSN quality properties. As a way to overcome a number of existing limitations, this study presents a multi-view modelling approach that supports the development and analysis of WSNs. The framework uses different models to describe the software architecture, hardware configuration, and physical deployment of a WSN. A4WSN allows engineers to perform analysis and code generation in earlier stages of the WSN development life cycle. The A4WSN platform can be extended with third-party plug-ins providing additional analysis or code generation engines. We provide evidence of the applicability of the proposed platform by developing PlaceLife, an A4WSN plug-in for estimating the WSN lifetime by taking various physical obstacles in the deployment environment into account. In turn, PlaceLife has been applied to a real-world case study in the health care domain as a running example.",Software & Systems Modeling,2018.0,10.1007/s10270-018-0687-0,49869385,semantic_scholar
5516eea2b00bba86118be6f90d7c678b4c022226,https://www.semanticscholar.org/paper/5516eea2b00bba86118be6f90d7c678b4c022226,Developments in the United Kingdom road transport from a smart cities perspective,"Purpose: Smart city is a city which functions in a sustainable and intelligent way, by integrating all of its infrastructures and services in a cohesive way using intelligent devices for monitoring and control, to ensure efficiency and better quality of life for its citizens. As other countries globally, UK is keen for economic development and investment in smart city missions to create interest in monetary environment and inward investment. This paper explores the driving forces of smart road transport transformation and implementation in the UK. Design/methodology/approach: The study involved interviews with sixteen professionals from the UK road transport sector. A semi-structured interview technique was used to collect experts’ perception, which was then examined using content analysis. Findings: The results of the study revealed that the technological advancement is a key driver. The main challenges faced for the implementation of smart city elements in the UK road network are: lack of investment; maintenance; state of readiness and the awareness of the smart road transport concept. The study concludes that an understanding of the concept of smart cities from a road transport perspective is very important to create awareness of the benefits and the way it works. A wider collaboration between every sector is crucial to create a successful smart city. Originality/value: The study contributes to the field of digitalisation of road transport sector. This paper reveals the key driving forces of smart road transport transformation, the current status of smart road transport implementation in UK and challenges of the smart road transport development in the UK. Engineering, Construction and Architectural Management http://mc.manuscriptcentral.com/ecaam 2 Introduction Enormous global urbanisation and growth has caused migration of people in urban areas and spatial development of urban infrastructure. According to Obaidat and Nicopolitidis (2016), 85-90% of the world population evolution is a result of a 21 st century cities. Therefore, smart cities are being created from scratch or being developed gradually by improving the prevailing cities infrastructure and primary resources. A study of McKinsey Global Institute’s by Department for Business innovation and Skills (2013) shows that more than 50% of the global GDP (Gross Domestic Product) is generated in the 190 major cities in the developed countries compare to 22 largest cities in the developing countries. However, the increase of growth in the developed countries is healthy but this phenomena also set a high expectation on public services and the quality of the urban infrastructure and environment. Due to the urbanisation, more problems such as overpopulation, pollutions, scarcity of natural resources, public and private services are being created (Yigitcanlar et. al. 2020, Dameri, 2014). Smart cities are an emerging strategy to mitigate the problems generated by the rapid urban population growth and rapid urbanization (Prakash, 2019). A ‘smart city’ is defined as an urban area that is highly innovative in terms of overall facilities, ecological real estate, communication and market feasibility (Chirisa et al, 2016). Also, smart cities is defined as “a place where the traditional networks and services are made more efficient with the use of digital and telecommunication technologies, for the benefits of its inhabitants and businesses” (Kumar et al., 2018). Whereas BSI (2014) noted that smart cities as an effective integration of physical, digital and human systems in a built environment to deliver sustainable, prosperous and inclusive future for its citizens. From the above three definitions, it could be inferred that smart cities are cities that make use of physical, digital and human systems in order to enable sustainability and efficiency for the citizens within that city. The economic growth of any country is supported by its good infrastructure. The United Kingdom (UK) has strategic roads, railway and airports; however, it is one of the top 10 most congested country in the world (Korosec, 2018). According to ONS (2017), the population in the UK in 2016 was 65.6 million which was the largest ever. It is also projected that the population in the UK would grow, reaching over 74 million by 2039. Due to the population rise amalgamated with increase of cars on the road, which has increased by 4.6% higher than the previous peak in 2016 (Department of Transport, 2017), the present UK transport system faces many challenges. The UK road transportation system is gradually taking steps to overcome the problems. As road transport is a significant source of both safety and environmental concerns. This research aims to explore the driving forces of smart road transport transformation, and implementation in the UK along with the challenges faced. In doing so the next section delves into the relevant literature review followed by methodology and findings. The paper finally concludes with recommendations. 2. Literature review An extensive review of literature on drivers, current status of smart cities and the challenges are discussed in this section. Three main drivers include: technology, environment, and socioeconomy (See Table 1). The technological advancement is clearly seen as strong impact on the cities in the recent years. It can be seen clearly that, the communication technologies (ICT) develops city management, enhances technical and social networks, and urban affordability. Within the technology as a driver, the review of literature revealed that technologies such as Big Data, Internet of Things and Artificial Intelligence are widely Engineering, Construction and Architectural Management http://mc.manuscriptcentral.com/ecaam 3 implemented within the smart cities context. However, there is a lack of studies focusing on the UK smart road transport sector. Table 1: Literature on classification of drivers of smart cities development Drivers Description Source Technology Big data Big volume of digital data contents through online services such as Facebook, Twitter, You Tube, Instagram and SnapChat Olshannikova et al , 2017 Data is transmitted to various smart applications through sensor devices or other cloud computing integrated devices Hashem et al , 2016 Big data development highlights information and communication tools in the cyber physical farm management cycle and it identifies the interconnection related to socio-economic challenges Wolfert et al , 2017 Internet of Things IoT is widely in use for many multimedia application, smart transportation system and smart city design and deployment issues KeertiKumar et al , 2016 The implementation of IoT in transportation system means underlying technology and creating smart environments to increase their efficiency in tackling the transportation and environmental issues Kyriazis et al , 2013 Artificial Intelligence Artificial Intelligence (AI) is a technology that is influencing every pace of life which enable people to reconsider how to integrate information, analyse data, and real time data usage for the purpose of improve decision making West and Allen , 2018 Artificial neutral networks, expert system and hybrid intelligent system are computer based modelling tools that have recently aroused and found extensive recognition in many discipline for modelling complex real-world problem. Bahrammirzaee , 2010 China is a leading competitor and primarily focusing in the use of AI in military vehicles. While, Russia actively looking for opportunity in AI development and focused on robots in military Hoadley and Lucas , 2018 Environment Songdo, the Korean model of smart city was subjected to become one of the sustainable smart cities around the world Yigitcanlar , et al, 2018 Songdo, consist 40% of parks and green spaces and waste processing centre placed by the underground system and to recycle Benedikt , 2016 ; Shwayri , 2013 European Union aimed to reduce greenhouse gas emission in urban design through the implementation of innovation technologies Ahvenniemi et al , 2017 Engineering, Construction and Architectural Management http://mc.manuscriptcentral.com/ecaam 4 Cities have been setting high expectation on reaching the target of creating a clean future as shared by Covenant of Mayors’ vision for 2050 to accelerate the decarbonisation Covenant of Mayors for Climate & Energy , 2018 Socioeconomy By 2050, six hundred cities will generate almost 65% of world economic growth by contribute to a higher global GDP Dobbs et al, 2012 Smart cities indicated as the next evolution of ‘new community management where urban problems converted into opportunities for business investment and profit earning Anand and Marcco , 2018 Global smart cities market size to grow reaching USD 2.57 trillion by 2050 Grand View Research , 2018 Governments are obligated to protect citizen and their control over the active connection of local public groups, the police force, and the citizen such as the senior and disabled Neirotti et al ,2014 Level of observing has been focused by an increasing safety and security that desires to manage risks Kitchin , 2014 Safety and security features implemented with help of big data and data controls centres that joined and bind data stream collectively for example the installation of CCTV and street monitored camera are to monitor activity remotely Firmino and Duarte, 2014 Big data is a trend of utilising software tools to store data and share data collected with the use of technology. Nowadays, big data has been a tool that facilitating individual, businesses and government to discover new solutions to certain problems. Data is a crucial aspect as when an asset is built, asset management goes on and the more data collected in, the asset is being constructed, the more the asset can be maintained in an efficient manner (Loshin, 2018). Furthermore, KeertiKumar et al (2016) mention that IoT is widely used for many multim",,2020.0,,235349937,semantic_scholar
3b71fb8879e0d5f480b1379f58b902e904b743fb,https://www.semanticscholar.org/paper/3b71fb8879e0d5f480b1379f58b902e904b743fb,Towards Network Softwarization: A Modular Approach for Network Control Delegation. (Une Approche Modulaire avec Délégation de contrôle pour les Réseaux Programmables),"Network operators are facing great challenges in terms of cost and complexity in order to incorporate new communication technologies (e.g., 4G, 5G, fiber) and to keep up with increasing demands of new network services to address emerging use cases. Softwarizing the network operations using SoftwareDefined Networking (SDN) and Network Function Virtualization (NFV) paradigms can simplify control and management of networks and provide network services in a cost effective way. SDN decouples control and data traffic processing in the network and centralizes the control traffic processing to simplify the network management, but may face scalability issues due to the same reasons. NFV decouples hardware and software of network appliances for cost effective operations of network services, but faces performance degradation issues due to data traffic processing in software. In order to address scalability and performance issues in SDN/NFV, we propose in the first part of the thesis, a modular network control and management architecture, in which the SDN controller delegates part of its responsibilities to specific network functions instantiated in network devices at strategic locations in the infrastructure. We have chosen to focus on a modern application using an IP multicast service for live video streaming applications (e.g., Facebook Live or Periscope) that illustrates well the SDN scalability problems. Our solution exploits benefits of the NFV paradigm to address the scalability issue of centralized SDN control plane by offloading processing of multicast service specific control traffic to Multicast Network Functions (MNFs) implemented in software and executed in NFV environment at the edge of the network. Our approach provides smart, flexible and scalable group management and leverages centralized control of SDN for Lazy Load Balance Multicast (L2BM) traffic engineering policy in software defined ISP networks. Evaluation of this approach is tricky, as real world SDN testbeds are costly and not easily available for the research community. So, we designed a tool that leverages the huge amount of resources available in the grid, to easily emulate such scenarios. Our tool, called DiG, takes into account the physical resources (memory, CPU, link capacity) constraints to provide a realistic evaluation environment with controlled conditions. Our NFV-based approach requires multiple application specific functions (e.g., MNFs) to control and manage the network devices and process the related data traffic in an independent way. Ideally, these specific functions should be implemented directly on hardware programmable routers. In this case, new routers must be able to execute multiple independently developed programs. Packet-level programming language P4, one of the promising SDN-enabling technologies, allows applications to program their data traffic processing on P4 compatible network devices. In the second part of the thesis, we propose a novel approach to deploy and execute multiple independently developed and compiled applications programs on the same network device. This solution, called P4Bricks, allows multiple applications to control and manage their data traffic, independently. P4Bricks merges programmable blocks (parsers/deparsers and packet processing pipelines) of P4 programs according to processing semantics (parallel or sequential) provided at the time of deployment.",,2018.0,,52954853,semantic_scholar
3a7a35c3e64c8e4a546ad3656ffb78cc92f42a7a,https://www.semanticscholar.org/paper/3a7a35c3e64c8e4a546ad3656ffb78cc92f42a7a,A REVIEW ON INTRUSION DETECTION AND ITS ANALYSIS,"Now a day wireless detection network could be a unit that is loosely used in environmental management, investigation tasks, observing military applications, health connected applications, pursuit and dominant etc. A wireless intrusion detection additionally aids among the detection of a range of attacks. so as to identify gaps and attacks in wireless network intrusion detection analysis, this paper survey the literature of this area. This paper is to classify existing up to date wireless intrusion detection system (IDS); techniques based on target wireless network, detection technique, assortment method, trust model and analysis technique. This paper summarize pros and cons of a similar or differing types of considerations and concerns for wireless intrusion detection with respect to specific attributes of target wireless networks together with wireless local area networks (WLANs), wireless personal area networks (WPANs), wireless sensor networks (WSNs), ad hoc networks, mobile telecommunication, wireless mesh networks (WMNs) and cyber physical systems (CPSs). This paper is to outline the fundamentals of intrusion detection in wireless network, describing the kinds of attacks and state the motivation for intrusion detection in wireless network. A REVIEW ON INTRUSION DETECTION AND ITS ANALYSIS Pradeep Kumar, N. Sreema Iswarya, S. Sharmila Jeyarani 318 [1] INTRODUCTION Intrusion detection is a significant exploring topic with many prospective applications. Along with intrusion prevention, response and tolerance, intrusion detection is one tool that can defend against the real-world cyber attacks threatening critical systems. Vulnerabilities in most computer systems. And, it can be exploited by either non authorized or authorized users. Having said that, several tools are being designed and implemented for a variety of exploitations in diverse range of security attacks. Among these tools is the intrusion detection systems (IDS) which allow us to monitor a range of computer systems: an information system, a network or a cloud computing. These IDS detect intrusions and defined as attempts to break the security objectives such as confidentiality, integrity and availability and non-repudiation. The objective of this paper is to compare the different type of intrusion detection systems and describe their mode of use. In addition, we will include the different approaches currently proposed by others on IDS system, network and cloud computing based vulnerabilities in most computer systems. And, it can be exploited by either non authorized or authorized users. [2] LITERATURE REVIEW Wireless networks are not immune to the risks of destruction and decommissioning. Some of these risks are identical to those in Ad-Hoc networks, and others are specific to the sensors. Several articles [1][2][3][4][5] have presented security attacks and issues in WSNs. Intrusion detection system (IDS) defined as the second line of defense after cryptography, allows the detection and prevention of internal and external attacks. In [6], it is presented a Rule-based IDS called also Signature-based. Most of the techniques in these schemes follow three main phases: data acquisition phase, rule application phase and intrusion detection phase. In [7], it is proposed two approaches to improve the security of clusters for sensor networks using IDS. The first approach uses a model-based on authentication, and the second scheme is called Energy-Saving. IN [8] a hybrid intrusion detection system (HIDS) model has been anticipated for wireless sensor networks. The paper does not promote a solution. Rather, it is a comparative study of existing model of intrusion detection in wireless sensor networks. The paper aim is to provide a better understanding of the current research issues in this field. The paper [9] focus on detecting intrusion or anomalous behavior of nodes in WLAN’s Using a modular technique. We explore the security vulnerabilities of 802.11, numerous intrusion detection techniques, and different network traffic metrics also called as features. Based on the study of metrics, propose a modular based intrusion detection approach. The intrusion detection is a mechanism for a WSN to detect the existence of improper, inaccurate, or anomalous moving attackers. In the paper [10], consider the issue according to heterogeneous WSN models. Furthermore, consider two sensing detection models: single-sensing detection and multiplesensing detection. [3] SURVEY ON SECURITY ATTACKS AND INTRUSION TYPES 3.1 INTRUDER TYPE: Computer security specialists normally distinguish between internal and external network attacks. This is because intruder profiles, methods of attack and intruder objectives can vary significantly between internal and external attacks. International Journal of Computer Engineering and Applications, Volume XII, Issue I, Jan. 18, www.ijcea.com ISSN 2321-3469 Pradeep Kumar, N. Sreema Iswarya, S. Sharmila Jeyarani 319 3.1.1 External intruder  External intruder attacks can be made against the internal network, using the target’s own computers.  This is often done with the active or passive collusion of the members of the target’s own staff.  However, if the ultimate initiator of the attacks is someone holding no legitimate privileges on the network, then it is considered an external attack.  Attacks where the intruder has no privileges on the target network, and either gains access from outside the network perimeter or by evading or undermining the target’s physical and/or network security measures to achieve some degree of access to the target’s internal network. 3.1.2 Internal intruder  Attacks where the intruder has legitimate privileges on the target network.  Access is obtained using existing privileges, privileges the intruder has extended without permission, or privileges stolen from other users.  To gain access to data and resources to which the intruder is not authorized.  Internal attacks are typically far more common than external ones. 3.2 INTRUSION TYPE Several kinds of IDS technologies exist because of the variance of network configurations. Mainly, there are 3 necessary distinct families of IDS: the categories of IDPS technologies are differentiated primarily by the types of events that they monitor and therefore the ways that during which they're deployed. 3.2.1 Attempted break-in  A Firewall has the task to examine data traffic across borders between networks, and to reject those packets, which do not have a permission for transmission.  Beside attempts to access directly a computer in the protected network, there are also attacks against the Firewall itself, or attempts to outwit a Firewall with falsified data packets. Such break-in attempts are recognized, repelled and logged by the Intrusion Detection system (IDS).  Thereby it can be selected between logging within the device, email notification, SNMP traps or SYSLOG alarms. IDS checks the data traffic for certain properties and detects in this way also new attacks proceeding with conspicuous patterns. 3.2.2 Masquerade  A masquerade attack is an attack that uses a fake identity, such as a network identity, to gain unauthorized access to personal computer information through legitimate access identification.  If an authorization process is not fully protected, it can become extremely vulnerable to a masquerade attack. 3.2.3 Leakage  DoS Attack designed to cause an interruption or suspension of services of a specific host/server by flooding it with large quantities of useless traffic or external communication requests.  When the DoS attack succeeds the server is not able to answer even to legitimate requests any more this can be observed in numbers of ways: slow response of the server, slow network performance, unavailability of software or web page, inability to access data, website or other A REVIEW ON INTRUSION DETECTION AND ITS ANALYSIS Pradeep Kumar, N. Sreema Iswarya, S. Sharmila Jeyarani 320 resources.  Distributed Denial of Service Attack (DDoS) occurs where multiple compromised or infected systems (botnet) flood a particular host with traffic simultaneously. 3.2.4 Phishing attack  This type of attack use social engineering techniques to steal confidential information the most common purpose of such attack targets victim's banking account details and credentials.  Phishing attacks tend to use schemes involving spoofed emails send to users that lead them to malware infected websites designed to appear as real on-line banking websites.  Emails received by users in most cases will look authentic sent from sources known to the user (very often with appropriate company logo and localised information) those emails will contain a direct request to verify some account information, credentials or credit card numbers by following the provided link and confirming the information online.  The request will be accompanied by a threat that the account may become disabled or suspended if the mentioned details are not being verified by the user. 3.3 DETECTION METHODOLOGIES The Attempt The d Information Leak rule deals with signatures from potentdetection method defines the characteristics of analyzer. It is categorized on the basis of information being used by IDS. 3.3.1 Anomaly based detection  The anomaly based detection is based on defining the network behavior.  The network behavior is in accordance with the predefined behavior, then it is accepted or else it triggers the event in the anomaly detection.  The accepted network behavior is prepared or learned by the specifications of the network administrators. 3.3.2 Misuse based detection  Misused based detection involves searching network traffic for a series of malicious bytes or packet sequences.  The main advantage of this technique is that signatures are very easy to develop and understand if we know what network behavior we are trying to identify.  For instance, we might use a signature that looks for part",,2018.0,,212602953,semantic_scholar
d239d3668f8653d5790d7029a344c18b1bfae493,https://www.semanticscholar.org/paper/d239d3668f8653d5790d7029a344c18b1bfae493,"Geoworldsim: a time-asynchronous, distributed and intelligent environment based geosimulation platform","Imagine that the accessibility of the population to public infrastructures needs to be evaluated. A possible solution would be to calculate every distance and analyse which percentage of the population is in less than 300 meters. However, this solution does not take into account issues such as: the age distribution, functional diversity or people’s preferences when transiting the city. It is in these cases when it is necessary to go a step further and integrate Geographic Information Systems with other perspectives such as Multi-Agent Systems which represent the particular characteristics of each individual and their decision making processes. This integration is known as Geosimulation and builds more accurate simulations to model the physical reality together with social, demo graphic and economic components. 
Geosimulations aim at modelling systems at the scale of individuals and entity-level units of the built environment and provides a way to simulate big amounts of agents interacting in a virtual geographic environment and endowed with spatial cognitive capabilities (perception, navigation, reasoning). This dissertation presents a new Geosimulation platform design and implementation that allows analysing and simulating different urban infrastructures. The platform manages to put into practice the latest theories in Multi-Agent Systems along with the new techniques in cloud computing and asynchronism. The proposed design is evaluated for three case studies; ubiquitous IoT, sustainable transport policies and resilience of the power grid. The methodologies presented, provide progress to their respective research areas by improving state-of-the-art techniques or designing new mechanisms. 
Furthermore, by connecting these Geosimulations to the real world by sensors and actuators, the concept of mixed reality arises; simulations where changes in the real world are transferred to the virtual world through sensors and agents can influence the real world through actuators. Mixed realities allow developing distributed control systems, which not only take into account the physical reality and social preferences but also the state, where to deploy intelligent agents that provide services to citizens.",,2018.0,,215847803,semantic_scholar
19f66130a77d96ab599e51cf67ae00fc2064fe6a,https://www.semanticscholar.org/paper/19f66130a77d96ab599e51cf67ae00fc2064fe6a,A Survey on Cloud Computing,"Cloud computing provides customers the illusion of infinite computing resources which are available from anywhere, anytime, on demand. Computing at such an immense scale requires a framework that can support extremely large datasets housed on clusters of commodity hardware. Two examples of such frameworks are Google’s MapReduce and Microsoft’s Dryad. First we discuss implementation details of these frameworks and drawbacks where future work is required. Next we discuss the challenges of computing at such a large scale. In particular, we focus on the security issues which arise in the cloud: the confidentiality of data, the retrievability and availability of data, and issues surrounding the correctness and confidentiality of computation executing on third party hardware. Today, the most popular applications are Internet services with millions of users. Websites like Google, Yahoo! and Facebook receive millions of clicks daily. This generates terabytes of invaluable data which can be used to improve online advertising strategies and user satisfaction. Real time capturing, storage, and analysis of this data are common needs of all high-end online applications. To address these problems, a number of cloud computing technologies have emerged in last few years. Cloud computing is a style of computing where dynamically scalable and virtualized resources are provided as a service over the Internet. The cloud refers to the datacenter hardware and software that supports a clients needs, often in the form of datastores and remotely hosted applications. These infrastructures enable companies to cut costs by eliminating the need for physical hardware, allowing companies to outsource data and computations on demand. Developers with innovative ideas for Internet services no longer need large capital outlays in hardware to deploy their services; this paradigm shift is transforming the IT industry. The operation of large scale, commodity computer datacenters was the key enabler of cloud computing, as these datacenters take advantage of economies of scale, allowing for decreases in the cost of electricity, bandwidth, operations, and hardware [Armbrust et al. 2009]. It is well known that writing efficient parallel and distributed applications is complex. Google proposed the MapReduce [Dean and Ghemawat 2004] programming framework in 2004 to address this complexity. It allows programmers to specify a map function that processes a key/value pair to generate an intermediate key/value pairs, and a reduce function that merges all the intermediate key/value pairs to produce the required output. Many real world tasks, especially in the domain of search can be expressed in this model. Hadoop 1 is the most popular open source implementation of MapReduce. It has been widely adopted both in academic and industrial users, including at organiza",,2009.0,,15081783,semantic_scholar
60c0f69a4d36f1ad95ae8e2ad217163bcb338ee6,https://www.semanticscholar.org/paper/60c0f69a4d36f1ad95ae8e2ad217163bcb338ee6,Trust and integrity in distributed systems,"In the last decades, we have witnessed an exploding growth of the Internet. The massive adoption of distributed systems on the Internet allows users to offload their computing intensive work to remote servers, e.g. cloud. In this context, distributed systems are pervasively used in a number of difference scenarios, such as web-based services that receive and process data, cloud nodes where company data and processes are executed, and softwarised networks that process packets. In these systems, all the computing entities need to trust each other and co-operate in order to work properly. While the communication channels can be well protected by protocols like TLS or IPsec, the problem lies in the expected behaviour of the remote computing platforms, because they are not under the direct control of end users and do not offer any guarantee that they will behave as agreed. For example, the remote party may use non-legitimate services for its own convenience (e.g. illegally storing received data and routed packets), or the remote system may misbehave due to an attack (e.g. changing deployed services). This is especially important because most of these computing entities need to expose interfaces towards the Internet, which makes them easier to be attacked. Hence, software-based security solutions alone are insufficient to deal with the current scenario of distributed systems. They must be coupled with stronger means such as hardware-assisted protection. In order to allow the nodes in distributed system to trust each other, their integrity must be presented and assessed to predict their behaviour. The remote attestation technique of trusted computing was proposed to specifically deal with the integrity issue of remote entities, e.g. whether the platform is compromised with bootkit attacks or cracked kernel and services. This technique relies on a hardware chip called Trusted Platform Module (TPM), which is available in most business class laptops, desktops and servers. The TPM plays as the hardware root of trust, which provides a special set of capabilities that allows a physical platform to present its integrity state. With a TPM equipped in the motherboard, the remote attestation is the procedure that a physical node provides hardware-based proof of the software components loaded in this platform, which can be evaluated by other entities to conclude its integrity state. Thanks to the hardware TPM, the remote attestation procedure is resistant to software attacks. However, even though the availability of this chip is high, its actual usage is low. The major reason is that trusted computing has very little flexibility, since its goal is to provide strong integrity guarantees. For instance, remote attestation result is positive if and only if the software components loaded in the platform are expected and loaded in a specific order, which limits its applicability in real-world scenarios. For such reasons, this technique is especially hard to be applied on software services running in application layer, that are loaded in random order and constantly updated. Because of this, current remote attestation techniques provide incomplete solution. They only focus on the boot phase of physical platforms but not on the services, not to mention the services running in virtual instances. This work first proposes a new remote attestation framework with the capability of presenting and evaluating the integrity state not only of the boot phase of physical platforms but also of software services at load time, e.g. whether the software is legitimate or not. The framework allows users to know and understand the integrity state of the whole life cycle of the services they are interacting with, thus the users can make informed decision whether to send their data or trust the received results. Second, based on the remote attestation framework this thesis proposes a method to bind the identity of secure channel endpoint to a specific physical platform and its integrity state. Secure channels are extensively adopted in distributed systems to protect data transmitted from one platform to another. However, they do not convey any information about the integrity state of the platform or the service that generates and receives this data, which leaves ample space for various attacks. With the binding of the secure channel endpoint and the hardware TPM, users are protected from relay attacks (with hardware-based identity) and malicious or cracked platform and software (with remote attestation). Third, with the help of the remote attestation framework, this thesis introduces a new method to include the integrity state of software services running in virtual containers in the evidence generated by the hardware TPM. This solution is especially important for softwarised network environments. Softwarised network was proposed to provide dynamic and flexible network deployment which is an ever complex task nowadays. Its main idea is to switch hardware appliances to softwarised network functions running inside virtual instances, that are full-fledged computational systems and accessible from the Internet, thus their integrity is at stake. Unfortunately, currently remote attestation work is not able to provide hardware-based integrity evidence for software services running inside virtual instances, because the direct link between the internal of virtual instances and hardware root of trust is missing. With the solution proposed in this thesis, the integrity state of the softwarised network functions running in virtual containers can be presented and evaluated with hardware-based evidence, implying the integrity of the whole softwarised network. The proposed remote attestation framework, trusted channel and trusted softwarised network are implemented in separate working prototypes. Their performance was evaluated and proved to be excellent, allowing them to be applied in real-world scenarios. Moreover, the implementation also exposes various APIs to simplify future integration with different management platforms, such as OpenStack and OpenMANO.",,2017.0,10.6092/POLITO/PORTO/2676918,116120592,semantic_scholar
04dc3ef332c7f83e66f50653945e621d4bf403b1,https://www.semanticscholar.org/paper/04dc3ef332c7f83e66f50653945e621d4bf403b1,Simulation of Hierarchical Storage Systems for TCO and QoS,"Due to the variety of storage technologies deep storage hierarchies turn out to be the most feasible choice to meet performance and cost requirements when handling vast amounts of data. Long-term archives employed by scientific users are mainly reliant on tape storage, as it remains the most cost-efficient option. Archival systems are often loosely integrated into the HPC storage infrastructure. In expectation of exascale systems and in situ analysis also burst buffers will require integration with the archive. Exploring new strategies and developing open software for tape systems is a hurdle due to the lack of affordable storage silos and availability outside of large organizations and due to increased wariness requirements when dealing with ultra-durable data. Lessening these problems by providing virtual storage silos should enable community-driven innovation and enable site operators to add features where they see fit while being able to verify strategies before deploying on production systems. Different models for the individual components in tape systems are developed. The models are then implemented in a prototype simulation using discrete event simulation. The work shows that the simulations can be used to approximate the behavior of tape systems deployed in the real world and to conduct experiments without requiring a physical tape system.",ISC Workshops,2017.0,10.1007/978-3-319-67630-2_12,30316798,semantic_scholar
d86e9c9f339bf976eb4e4ee882aee3f5fb5858ba,https://www.semanticscholar.org/paper/d86e9c9f339bf976eb4e4ee882aee3f5fb5858ba,Using physical layer emulation to understand and improve wireless networks,"Researchers and developers have long faced a fundamental tension between the experimental realism of wireless testbeds on one hand, and the control and repeatability of simulation on the other hand. This thesis introduces physical layer wireless net work emulation---a new approach to wireless network experimentation that balances the stark tradeoff of traditional alternatives by enabling both realistic and repeatable experimentation. 
The design and implementation of a functional wireless emulator are presented along with a discussion of how this implementation overcomes the challenges necessary to meet operational requirements. In particular, solutions to the problems of developing a hardware architecture for emulation, and software control of that architecture will be presented. 
To illustrate the power of physical layer wireless network emulation, case studies are presented. First, physical layer emulation is used to analyze several aspects of wireless LAN link-level behavior. Physical layer emulation is then used to investigate wireless LAN access point selection performance, and to develop improvements. 
This thesis shows that---compared to traditional approaches---physical layer wireless network emulation provides a better understanding of real-world wireless network performance, shortens the development cycle of wireless networking software, and facilitates the deployment of research into operational wireless networks without sacrificing a controlled experimental environment.",,2006.0,,107864466,semantic_scholar
877faa6486db570bdbe7aa24d5b40cac6017843d,https://www.semanticscholar.org/paper/877faa6486db570bdbe7aa24d5b40cac6017843d,Software and System Engineering for Cyber-Physical Systems: technical challenges and collaboration opportunities,"of presentations Holger Pfeifer (FORTISS) – The European Smart Anything Everywhere initiative and funding opportunities by CPSE-Labs experiments The European ‘Smart Anything Everywhere’ (SAE) initiative supports the innovation on smart digital systems thanks to networks of competence centres. The ecosystems built under these initiatives are based on collaboration between researchers, large industries and SMEs which will help to transfer knowledge and resources available to a much wider group of companies. SMEs and middle size companies can experiment with new technologies, try them out in their processes and work together with the suppliers of the technology to adapt it to their specific needs. CPSELabs is one SAE innovation action which provides an open forum for sharing platforms, architectures and SW tools for the engineering of dependable and trustworthy CPS. It provides funding for focussed experiments (36 partners) and fast-track (12-18 months) with innovation objective. Next call for experiment will be published in Spring 2016 http://www.cpse-labs.eu/calls.php Holger Pfeifer (FORTISS) CPSE-Labs experiments of Germany South centre: Model-driven engineering for industrial automation systems The importance of software in industrial automation is continuously increasing. New approaches to the development and maintenance are needed to cope with the growing complexity of control software for future automation systems. 4DIAC Framework for Distributed Industrial Automation & Control is an open source solution for the programming of programmable logic controllers (PLCs) according to the standard IEC 61499. With this standard it provides higher level modelling means and better support for networked control devices. The main components of 4DIAC are the Eclipse-based integrated development environment 4DIAC-IDE and the real-time capable run-time environment FORTE. Martin Grimheden (KTH) – CPSE-Labs experiments of Sweden centre: Overcoming thresholds for data integration in CPS engineering environments The talk will describe the Kth model-based approach to data integration based on the OSLC interoperability standard. Patrick Leserf (ESTACA) Trade-off analysis with SysML and Papyrus : a drone application Obtaining the set of trade-off architectures from a SysML model is an important objective for the system designer. To achieve this goal, we propose a methodology combining SysML with the variability concept and multiobjectives optimization techniques. An initial SysML model is completed with variability information to show up the different alternatives for component redundancy and selection from a library. The constraints and objective functions are also added to the initial SysML model, with an optimization context. Then a representation of a constraint satisfaction problem (CSP) is generated with an algorithm from the optimization context and solved with an existing solver. The presentation will illustrate our methodology by designing an Embedded Cognitive Safety System (ECSS). From a component repository and redundancy alternatives, the best design alternatives are generated in order to minimize the total cost and maximize the estimated system reliability. Benoît Combemale (IRISA) Using models for a broader engagement in smart systems. Various disciplines use models for different purposes. While engineering models, including software engineering models, are often developed to guide the construction of a non-existent system, scientific models, in contrast, are created to better understand a natural phenomenon (i.e., an already existing system). An engineering model may incorporate scientific models to build a smart system. This talk proposes a vision promoting an approach that synergistically combines engineering and scientific models to enable broader engagement of end users in smart systems, informed decision-making based on more-accessible scientific models and data, and automated feedback to the engineering models to support dynamic adaptation of smart systems. To support this vision, we identify a number of challenges to be addressed with particular emphasis on the socio-technical benefits of modeling. Claire Ingram (Newcastle University) CPSE-Labs experiments of UK centre: Pragmatic techniques for modelbased Engineering of Cyber-Physical Systems Newcastle University's Cyber-Physical Systems Lab conducts research into pragmatic techniques for model-based engineering of Cyber-Physical Systems (CPSs). In this talk I will introduce some platforms supported by Newcastle's CPS Lab, including an approach for co-modelling which allows separate design teams working with discrete-event and continuous-time formalisms to develop CPS designs collaboratively. I will also introduce an experiment which has been funded previously under the CPSE Labs initiative. Michael Siegel (OFFIS) CPSE-Labs experiments of Germany North centre: The Maritime Architecture Framework (MAF) and eMaritime Integrated Reference Platform (eMIR) The Maritime Architecture Framework (MAF) is a stakeholder-oriented CPS architecture framework for existing and future maritime CPS and services. MAF provides the conceptual basis, methods, tools and technologies to define, document, and align existing or future CPS architectures and architectural reference models for e-navigation and e-maritime applications. The MAF is a key enabler in the maritime domain for system harmonization, interoperability and standardization. The MAF is also a reference for the definition and design of testbeds for enavigation and e-maritime systems and services. It helps to define the context, to check completeness and provides a semantic basis to discuss the outcome and results. Additionally it offers a semantic basis for integration of testbeds e.g. for larger demonstrators. To support the development of maritime CPS – i.e. the integration of heterogeneous systems of the maritime transportation space , the Design Centre North provides the eMaritime Integrated Reference Platform (eMIR) for rapid prototyping in simulation environments and testing in real world environments. This talk gives an overview about the background, context and concepts of the MAF and why testbed environments (e.g. eMIR) for the development, integration testing and demonstration for CPS must take into account and support the design aspects of such an architecture framework. Andre Pierre Mattei (SCA-ITA) SysML Design of an observation satellite for agriculture surveillance in Brazil François Fouquet (SnT, Interdisciplinary Centre for Security, Reliability and Trust)Models for managing IoT data Internet of Things applications analyze our past habits through sensor measurements to anticipate future trends. To yield accurate predictions, intelligent systems not only rely on single numerical values, but also on structured models aggregated from different sensors. Computation theory, based on the discretization of observable data into timed events, can easily lead to millions of values. Time series and similar database structures can efficiently index the mere data, but quickly reach computation and storage limits when it comes to structuring and processing IoT data. During this talk, I will present various results presented at Models’15 and SmartGridCom’15 that tackles this complexity by exploiting IoT data characteristics. Notably, I will present a concept of continuous models that can handle high-volatile IoT data by defining a meta type for continuous attributes. In addition to traditional discrete object-oriented modeling APIs, we enable models to represent very large sequences of sensor values by inferring mathematical models that can efficiently replace raw values. We show on various IoT datasets that sequences of polynomials can significantly improve storage and reasoning efficiency. I will present an application of this IoT model extension for suspicious value detection in the SmartGrid domain. We proposed a method to learn and store a profile of “typical” values and their probability in IoT context models. We show that using such profiles together with a contextual checker we can improve suspicious value detection, both in terms of efficiency and effectiveness. Juan Garbajosa (UPM) Experiments of Spain centre: Open CPS platform for building and deploying smart city services Bran Selic (Simula) Modeling uncertainty in cyber-physical systems For the past year, we have been working on a core model of Uncertainty and its application to requirements specification and system testing in the context of the European Commission's H2020 UTEST project. (More information on this project, which involves a number of industrial and research partners from Europe, can be found at: http://certus-sfi.no/u-test-a-horizon-2020-funding-recipient/). Fabien Peureux Fabien Peureux (Femto-st/EGM/Smartesting S&S) Model-Based Testing for Internet of Things and Cyber-Physical Systems Testing Cyber-Physical Systems (CPS) is challenging due to the various uncertainties in their behaviour. The purpose of this talk is to present our ongoing work on a model-based testing framework for automatic generation of executable test cases for CPS in the presence of various uncertainties. Basically, uncertainties can be described as a lack of certainty about the current state or about the future outcome of the system. Within CPS, it can be due to the stimulus and data sent from the user environment to the physical units (application level), to the interactions between the physical units and the network services (infrastructure level), or a combination of the both (integration level). To test such issues, the proposed model-based testing approach is implemented on the EMF framework, and based on the test generation tool Smartesting CertifyIt1. It relies on a UML behavioral model of the system under test, from which abstract test cases are automatically generated by applying dedicated coverage strategies focusing on uncertainty testing. Afterwards, ",,2016.0,,222303274,semantic_scholar
1e4380f2aec7922062899b4becff1c1db53679c7,https://www.semanticscholar.org/paper/1e4380f2aec7922062899b4becff1c1db53679c7,Modeling and Simulation of Tape Libraries for Hierarchical Storage Systems,"The variety of storage technologies results in deep storage hierarchies to be the only feasible choice to meet performance and cost requirements when handling vast amounts of data. Long-term storage systems employed by scientific users are mainly reliant on tape storage, as it remained the most costefficient option. Archival systems are often loosely integrated into the HPC storage infrastructure. With the rise of exascale systems and in situ analysis also burst buffers will require integration with the archive. Exploring new strategies and developing open software for tape systems is a hurdle due to the lack of affordable storage silos and availability outside of large organizations and due to increased wariness requirements when dealing with ultradurable data. Lessening these problems by providing virtual storage silos should enable community-driven innovation, and enable site operators to add features where they see fit while being able to verify strategies before deploying on production systems. Different models for the individual components in tape systems are developed. The models are then implemented in a prototype simulation using discrete event simulation. It is shown that the simulation can be used to approximate the behavior of tape systems deployed in the real world and to conduct experiments without requiring a physical tape system.",,2016.0,,30811547,semantic_scholar
f77d59740dfeb10e9b650ec8b1baba91fca70279,https://www.semanticscholar.org/paper/f77d59740dfeb10e9b650ec8b1baba91fca70279,NaviBeam: Indoor assistance and navigation for shopping malls through projector phones,"We present our concept of an indoor assistance and navigation system for pedestrians that leverages projector phones. Digitally enhanced guides have many advantages over traditional paper-based indoor guides, most of all that they can be aware of their current context and display dynamic information. That is why particularly shopping malls recently started deploying indoor assistance applications for mobile phones, which also include support for navigation. Moreover, as we show in the paper, projected interfaces offer additional distinct advantages over static guides and even traditional or augmented reality mobile applications. We describe five concepts for a shopping mall indoor assistance system based on projector phones, comprising support for shop selection, precise way finding, “virtual fitting” of clothes, and context-aware and ambient advertisements. We then apply the concepts to a typical shopping scenario, where users wear the phone at their belt and constantly project the interface in front of them. Expected benefits of our system are that people find their way quicker, easier, and less distracted from their usual shopping experience. Finally, we discuss the technical feasibility of our envisioned implementation and research questions we are particularly interested in. INTRODUCTION Navigation and location-based services for pedestrians recently gained a lot of attention and are becoming rapidly adopted. Very popular among these are applications for location-based places recommendations and turn-by-turn navigation. While these applications mostly focus on outdoor navigation, less attention has been paid to the opportunities for providing indoor assistance with mobile devices. Especially in large complex buildings, e.g. shopping centers, in most cases static signs, You-Are-Here maps, or paper flyer maps are still the only available navigation assistance for visitors. Preliminary observations and interviews we conducted in nearby shopping centers show that, at least in Germany, available navigational aids are still as insufficient as Levine reported them to be almost 30 years ago [8]. Despite a lot of research projects that explored indoor assistance over the last decade, it was not before the beginning of 2011 that we saw the first mobile shopping applications reaching consumer markets, such as the Sam’s Club mobile application [1], which provides indoor navigation to specific items and/or shops in some selected American shopping malls. Similarly, some shopping centers in Asia introduced mobile AR applications for shopping assistance [2]. In our research group we study future application areas of projector phones, i.e. mobile phones with integrated projectors (see [11] for a detailed survey). We found projector phones to provide some distinguished advantages for indoor navigation assistance, e.g., that interaction can be handsfree and the projection can serve as ambient display, thereby not contradicting the traditional shopping experience. Further that the surrounding world can be directly augmented, freeing the user from mapping between mobile display and real world and that the output space is much larger than on mobile displays. And finally, that bystander can see and attach to the projected output. RELATED WORK We present relevant work dealing with mobile shopping, recommender systems, indoor positioning and navigation. One of the first works on digital mobile shopping assistants has been done by Asthana et al. [3]. They presented main usage scenarios, e.g., telling people where to find certain products or informing them on discounts and special offers. Similar can be recognized in aforementioned mobile shopping applications and as well in recently filed patents, e.g. from Apple Inc. (US 2010/0198626 A1, US 2010/0191578 A1), which include navigation, service interactions (e.g., parking tickets), and support for social networking. Yang et al. [15] developed a location-aware recommender system. It learns a customer’s interests from previously visited product websites and continuously presents a list of products around the customer’s current location, that are likely to interest him. The software also takes into account the distance to shops and is able to learn customer’s preference between highly interesting products and far distances. Butz et al. [4] present a hybrid indoor navigation system that is able to adapt route instructions to different output devices (screen resolution, device resources) and based on the precision of available location information. Results from [7,14] indicate that intelligent fusion of infrastructure techniques, e.g. GPS, GSM triangulation, and sensors like accelerometers, magnetometers, and gyroscopes, enables precise indoor location tracking with current commercially available smartphones in unaltered environments. Kray et al. [6] explore the design space of routing instructions for pedestrians on mobile devices. Narzt et al. present a specific mobile application for augmented reality (AR) [10]. Alternative systems for pedestrian navigation are the Rotating Compass by Rukzio et al. [12], showing personalised navigation information on public displays and the CabBoots system by Frey [5], which guides users by means of tactile output in the shoes. Aforementioned techniques, with the exception of the last two, rely on holding a handset device. However, we feel strongly inclined that holding a device in hand for a longer time does not fit well the traditional shopping experience. Negative side effects, e.g., arm fatigue, regular switching of the field of view, do not allow using the shopping assistance application as constant companion. CONCEPT In our envisioned prototype, people wear their projector phone on their belt, projecting a display right in front of their feet (Figure 1). In the following we present five concepts for mobile shopping assistance that are enabled by projector phones. Later we apply these concepts to a typical shopping scenario. Shadow Interaction Since our concept expects people to wear the projector fixed to the belt most of the time, direct interaction with the mobile device would not be sufficient as the only interaction technique. Audio is not an alternative because of the noisy environment in a shopping mall. This leaves users with the option to directly interact with the projection, either by feet or hands (or gaze in the future). In preliminary studies we discovered that foot-based interaction is not well suited due to the fact that foot movement inherently involves movement of the body at the same time, which makes interaction cumbersome. We found interacting with the shadow of a finger in front of the projection a much more promising solution. Figure 2 shows the stroke of the resulting shadow on the projection. With the tip of the shadow, all points on the projection can be reached. Items should be highlighted once the shadow reaches their bounding box to give adequate feedback to the user. Although the tracking of the finger’s height to enable traditional press/release behaviors would be possible, this would require the user to maintain a complex mental model of different finger height levels. Instead, our concept builds on the fact, that the tip of the index finger can be moved well without changing the shape of the rest of the hand or even the middle knuckle of the index finger. Thus, to select an item, a user moves the index finger to point on the desired item and then bends the index finger and unbends it again. Alternatively, one could use the finger’s dwell time as in Microsoft’s Kinect. To the best of our knowledge, shadow interaction with projections has not been reported before. Radar of Recommendations Another concept that is particularly useful with projected navigation is a radar of recommendations. Building on [15], we want to constantly show and update a personal radar of products the user might be interested in (Figure 2). Based on the information available from accounts with online stores (e.g., Amazon) and items recently explored with our system (see fourth concept), users see offers of nearby stores in front of them and can select these items to start a navigation in the middle of the circle. The size of an item conveys the expected interest of the user, the distance from the middle depicts the walking distance (not air path) from the user’s current location. The size of the radius of interest (distance to shops and items) can be adjusted by slightly changing the angle of the projector, similarly to looking further ahead. Different from [15], the projection provides a much larger output space and the radar serves as ambient display in the user’s periphery. Projected (augmented) Navigation When the user selected an item or shop he is interested in, the assistance system starts a projected navigation. In outdoor navigation, turn-by-turn navigation is still the most prominent, though we have seen augmented reality been used in research [10]. Especially for indoor navigation, where there are more tight and subsequent turns or small decision spaces (take the left stairs up, not the right stairs down) our experience is that turn-by-turn navigation does not work well. Therefore, we want the user not to follow Figure 1: The projector is worn at different locations on the belt (left and middle) and can optionally be taken into hand to change the angle of the projection (right). Figure 2: The user interacts with his radar of recommendations through the shadow of his finger. The orange outline shows the shadow, the left red circle the fingertip that, for clicking, can be changed independently of the finger’s middle knuckle (right circle) or the rest of the hand’s shape. this type of directions, but instead simply follow a blue line projected in front of her (see Figure 3). Since the projector phone is spatially-aware, both in terms of location and orientation, the blue line is projected as a static augmentation of the real world, i",,2011.0,,216619517,semantic_scholar
30f346032407b43e811107a46bf07a38f30527d7,https://www.semanticscholar.org/paper/30f346032407b43e811107a46bf07a38f30527d7,The Build Master: Microsoft's Software Configuration Management Best Practices,"""Wow, what can I say? Chapter 4, 'The Build Lab and Personnel,' by itself is enough justification to purchase the book! Vince is obviously a 'Dirty Finger Nails' build meister and there is a lot we can all learn from how he got them dirty! There are so many gems of wisdom throughout this book it's hard to know where to start describing them! It starts where SCM should start, at the end, and works its way forward. This book is a perfect complement to the 'Follow the Files' approach to SCM that I espouse. I will recommend that every software lead and software configuration management person I work with be required to read this book!""-Bob Ventimiglia, autonomic logistics software configuration manager, Lockheed Martin Aeronautics""The Build Master contains some truly new information; most of the chapters discuss points that many people in the industry don't have a full understanding of and need to know. It's written in a way that is easy to read and will help a reader fill holes in their vision regarding software build management. I especially liked Vince's use of Microsoft stories to make his points throughout the book. I will purchase the book and make certain chapters mandatory reading for my build manager consultants.""-Steve Konieczka, SCM consultant""Vince does a great job of providing the details of an actual working build process. It can be very useful for those who must tackle this task within their own organization. Also the 'Microsoft Notes' found throughout the book provide a very keen insight into the workings of Microsoft. This alone is worth purchasing this book.""-Mario E. Moreira, author of Software Configuration Management Implementation Roadmap and columnist at CM Crossroads""Software configuration management professionals will find this book presents practical ideas for managing code throughout the software development and deployment lifecycles. Drawing on lessons learned, the author provides real-world examples and solutions to help you avoid the traps and pitfalls common in today's environments that require advanced and elegant software controls.""-Sean W. Sides, senior technical configuration manager, Great-West Healthcare Information Systems""If you think compiling your application is a build process, then this book is for you. Vince gives us a real look at the build process. With his extensive experience in the area at Microsoft, a reader will get a look in at the Microsoft machine and also how a mature build process should work. This is a must read for anyone doing serious software development.""-Jon Box, Microsoft regional director, ProTech Systems Group""Did you ever wonder how Microsoft manages to ship increasingly complex software? In The Build Master, specialist Vince Maraia provides an insider's look.""-Bernard Vander Beken, software developer, jawn.net""This book offers an interesting look into how Microsoft manages internal development of large projects and provides excellent insight into the kinds of build/SCM things you can do for your large-scale projects.""-Lance Johnston, vice president of Software Development, SCM Labs, Inc.""The Build Master provides an interesting insight into how large software systems are built at Microsoft covering the set up of their build labs and the current and future tools used. The sections on security, globalization, and versioning were quite helpful as these areas tend to be overlooked.""-Chris Brown, ThoughtWorks, consultant""The Build Master is a great read. Managing builds is crucial to the profitable delivery of high-quality software. Until now, the build process has been one of the least-understood stages of the entire development lifecycle. This book helps you implement a smoother, faster, more effective build process and use it to deliver better software.""-Robert J. Shimonski, Networking and Security Expert, http://www.rsnetworks.netThe first best-practice, start-to-finish guide for the software build processManaging builds is crucial to the profitable delivery of high-quality software; however, the build process has been one of the least-understood stages of the entire development lifecycle. Now, one of Microsoft's leading software build experts introduces step-by-step best practices for maximizing the reliability, effectiveness, timeliness, quality, and security of every build you create.Drawing on his extensive experience working with Microsoft's enterprise and development customers, Vincent Maraia covers all facets of the build process-introducing techniques that will work on any platform, on projects of any size. Maraia places software builds in context, showing how they integrate with configuration management, setup, and even customer support. Coverage includes How Microsoft manages builds: process flows, check-in windows, reporting status, and more Understanding developer and project builds, pre- and post-build steps, clean builds, incremental builds, continuous integration builds, and more Choosing the right build tools for your projects Configuring source trees and establishing your build environment-introducing Virtual Build Labs (VBLs) Planning builds for multiple-site development projects or teams Determining what should (and shouldn't) be kept under source control Managing versioning, including build, file, and .NET assembly versions Using automation as effectively as possible Securing builds: a four layer approach-physical, tracking sources, binary/release bits assurance, and beyondBuilds powerfully impact every software professional: developers, architects, managers, project leaders, configuration specialists, testers, and release managers. Whatever your role, this book will help you implement a smoother, faster, more effective build process-and use it to deliver better software.© Copyright Pearson Education. All rights reserved.",,2005.0,,107761717,semantic_scholar
3c8b0f17ca1ad55900a1bc35f7e588db15e6ec8d,https://www.semanticscholar.org/paper/3c8b0f17ca1ad55900a1bc35f7e588db15e6ec8d,Improving QoS for large-scale WSNs,"The advancements in information and communication technologies have been triggering an increase in miniaturization and ubiquity, paving the way towards new paradigms in embedded computing systems. Modern embedded systems are enabling a number of smaller, smarter and ubiquitous devices, creating an eagerness for monitoring and controlling everything, everywhere. These facts are pushing forward the design of new Wireless Sensor Network (WSN) infrastructures that will tightly interact with the physical environment, in a ubiquitous and pervasive fashion. However, such cyber-physical systems require a rethinking of the usual computing and networking concepts, and given that these computing entities closely interact with their environment, timeliness is of increasing importance. Nevertheless, many other QoS properties such as scalability, energyefficiency and robustness must also be addressed if these infrastructures are to become a reality. This Thesis addresses the use of standard protocols, particularly IEEE 802.15.4 and ZigBee, combined with commercial technologies as a baseline to enable WSN infrastructures capable of supporting the QoS requirements that future large-scale networked embedded systems will impose. Hence, several architectural solutions (mechanisms, algorithms, protocol add-ons) are hereby proposed to address some of the most prominent QoS challenges, such as timeliness, scalability, robustness and energy-efficiency. Importantly, in order to clearly identify the most prominent QoS challenges and to provide effective QoS solutions with close contact with reality, a hands-on approach is followed throughout this Thesis. Hence, we rely upon two real-world application scenarios (i.e. a Datacentre Monitoring (DM) scenario and a Structural Health Monitoring (SHM) scenario), which were engineered, implemented and deployed in the course of this work, to validate and demonstrate this Thesis’ QoS proposals. This strategy enables a deeper understanding of these infrastructures at a more practical level, and provides the proposals with a real-world application context, showing that these network infrastructures have the potential to be used in real-world cyber-physical applications in the near future, if provided with the necessary QoS management mechanisms. Among the proposals, concerning timeliness, for instance, ZigBee cluster-tree topologies are known for a lack of flexibility in adapting to changes in the traffic or bandwidth requirements at runtime, making these infrastructures not capable of allocating more bandwidth to a set of nodes sensing a particular phenomena, or reducing the latency of a data stream. This Thesis proposes a way of dynamically addressing this problem via a mechanism to re-schedule the clusters’ active periods. Concerning the MAC sub-layer of the IEEE 802.15.4 protocol, in this Thesis we carry out an experimental evaluation of a traffic differentiation mechanism, providing the support of different traffic classes to the legacy protocol. This mechanism is also extended to support intra-cluster communications. In addition to timeliness, this mechanism provides and improvement in terms of energy-efficiency. The IEEE 802.15.4 Guaranteed Time Slot mechanism, missing from most stack implementations, is also",,2015.0,,111616016,semantic_scholar
92affd2809bedc05661178892f7b063a9cbcfcb3,https://www.semanticscholar.org/paper/92affd2809bedc05661178892f7b063a9cbcfcb3,A MODULAR OPEN SYSTEM ARCHITECTURE STRATEGY FOR ROBOTICS AND AUTONOMOUS SYSTEMS,"This paper describes the results of a study funded by the National Advanced Mobility Consortium (NAMC) to develop a strategy for establishing interoperability as the norm in military ground robotic and autonomous system (RAS) programs. It briefly provides background explaining the current practices and the reason the study was conducted. It outlines the types of interoperability targeted in ground RAS programs, and describes the findings of a survey of current efforts aimed at creating interoperability through a modular open system architecture approach. It recommends a path forward for creating interoperability in military ground RAS program based on maturing and propagating the ground robotics interoperability profile (IOP) currently being developed and matured at Project Manager, Force Projection (PM FP). Finally it lays out specific steps to be taken and proposes that responsibility for IOP be transitioned to a consortium-style organization as it progresses through an “Iteration and Maturation” phase over the next 3-5 years towards its eventual adoption by an enduring standards body. The views expressed in this paper do not constitute official Department of Defense policy. INTRODUCTION This is a crucial time for the future of Robotic and Autonomous Systems (RAS) in general and Robotic and Autonomous Ground Vehicle Systems (“RA-GVS”) in particular. Historically, the vast majority of military ground robotics systems have been procured to support immediate operational needs. This has resulted in a number of built-topurpose, tightly integrated systems that have proven their operational utility and become indispensable. However, as current conflicts wind down and budgets shrink, maintaining and adding new capabilities to such built-to-purpose ground robotic systems will prove an expensive and difficult proposition. The difficulty in tightly integrated, built-topurpose systems is that they are costly to maintain and extend, and it is not possible to select and integrate “best of breed” components. Moving forward, the community must lessen the life-cycle costs, shorten the technology update cycles, and enable operational flexibility. In the case of autonomous systems, science and technology (S&T) projects have proven the technological feasibility and potential benefits of autonomy to a range of applications, from automated convoys, to fully self-driving vehicles. Successful transition of autonomous systems into real-world deployment will require development of well-understood, modular packages that can be integrated inexpensively with legacy platforms and worked into military operational processes. The continued development of tightly integrated, built-to-purpose autonomous systems is not a tenable approach, and delays the transition of these technologies. Proceedings of the 2015 Ground Vehicle Systems Engineering and Technology Symposium (GVSETS) A Modular Open System Architecture Strategy for Robotics and Autonomous Systems, Moore, et al. Page 2 of 17 To mitigate these difficulties, and to move to the next level in deployment of robotic and autonomous systems, the community of interest (COI) must proactively develop and embrace a modular open systems architecture (MOSA) for robotics and autonomous systems. A MOSA will provide a shared architectural framework and a set of interface standards, and will promote modularity, commonality, and interoperability between sub-systems and components. A MOSA for RA-GVS will enable an environment of competition and innovation in the community, and “grease the skids” for developing, integrating, deploying, and maintaining a wide variety of interoperable, robotic, and autonomous ground vehicles and platforms. In 2010, the Army and Marine Corps ground vehicle community initiated the process of creating a MOSA for RA-GVS when a group operating out of the then Robotic Systems-Joint Program Office (RS-JPO) commenced work on developing a collection of Interoperability Profiles (IOP) for unmanned ground vehicles. IOP is intended to provide program managers (PMs), and eventually others, with a standardized library of physical, electrical, and logical (messaging) interfaces, and a common set of supporting documentation and materials that they can use to define a common interoperability profile, or “instantiation”, specific to a certain robotic vehicle or platform. The instantiation specifies which interfaces and interoperability attributes, from among those defined in the overarching IOP, are to be implemented on a particular RAS. Over the past 5 years, work has focused on developing initial versions of the IOP and evaluating the technical feasibility of utilizing such an open interface standard without sacrificing operational performance. In the meantime, responsibility for IOP has transitioned to an IOP group operating under the Project Manager, Force Projection organization, under the Army’s Program Executive Office for Combat Support and Combat Service Support (“PEO CS&CSS”). With the pending release of IOP Version 2.0, the Government will have completed the Initial Development stage, which has resulted in a well-defined set of IOP documents and initial demonstration of technically sound underpinnings for a RAS MOSA. Definitions This section provides a set of definitions for several terms used throughout the remainder of the text. Note that many of these are used ambiguously within unmanned systems or focus on an alternate interpretation across different domains. The purpose here is not to provide a complete definitions document, but rather to focus on specific terms that may cause confusion. • Open Architecture (or Open Systems Architecture): “a type of computer hardware or software architecture [...] that allows adding, upgrading, modifying, and swapping components. [It provides] a varied combination of interoperability, portability, and open software standards.” [1] • Open Standard: “standards made available to the general public and are developed (or approved) and maintained via a collaborative and consensus driven process. [They] facilitate interoperability and data exchange among different products or services and are intended for widespread adoption.” [2] Note that for purposes of this document, a standard will still be considered “open” if a small fee is required to obtain it, such as Society and Automotive Engineering (SAE) documents. • Interface specification: a complete, unambiguous, and testable description of an interface. In robotics, this may include physical (mounting points, dimensions, weight), electrical (voltage, current), and logical (software, communication bus) interfaces. • Interface: “a point where two systems, sub-systems, components, subjects, organizations, etc., meet and interact”. [3] • Modular: “having parts that can be connected or combined in different ways” [4] • Interoperability: “the predictable performance of a capability across an interface through compliance to a selected set of specifications” [5] • Platform: the base vehicle or mobility chassis of a robotic system. • Payload: “a device carried by a [platform], usually in a bay or attached to a hardpoint” [5] • End Effector: “last link of a manipulator, often modular to accept various tools or instruments” • Controller (or Operator Control Unit): “A hardware and/or software interface that allows a human to command or monitor one or more unmanned systems.” • Appliqué: “The augmentation of a manned vehicle such that it can be semi-autonomously controlled.” [6] While most acronyms within this paper are defined within the paragraph of their specific use, others are used extensively throughout the document. A partial reference list is provided here for convenience: • RAS : Robotics and Autonomous System • UGV : Unmanned Ground Vehicle • IOP : Interoperability Profiles • JAUS: Joint Architecture for Unmanned Systems • AEODRS: Advanced EOD Robotic System • NAMC: National Advanced Mobility Consortium • ISR: Intelligence, Surveillance, and Reconnaissance • MOSA: Modular Open Systems Architecture Proceedings of the 2015 Ground Vehicle Systems Engineering and Technology Symposium (GVSETS) A Modular Open System Architecture Strategy for Robotics and Autonomous Systems, Moore, et al. Page 3 of 17 GROUND ROBOTICS AND AUTONOMOUS SYSTEMS INTEROPERABILITY The Architecture Framework for Unmanned Systems defines interoperability as “the predictable performance of a capability across an interface through compliance to a selected set of specifications” [5]. Therefore, to promote interoperability, an acquisition process must determine the appropriate set of specifications to define the physical, electrical, and logical interfaces between two or more entities. Equally important, however, is that the specific goal of interoperability must be determined. While interoperability generally leads to lower maintenance costs, longer lifespan, and an open marketplace for third party suppliers, specific use cases can be considered as representative types of interoperability. These use cases reflect the boundaries between unmanned systems components for which interfaces must be defined. In-Field Swap of Vehicle Payloads A common goal of interoperability is the ability to quickly switch out payloads on a robotic platform in the field. For the purposes of this document, “in the field” is considered to be any location in which full engineering and technical services are not available, but swapping may still require a power cycle or minor configuration changes performed by the vehicle operator. This type of interoperability may include functionally equivalent payloads from different manufacturers, or switching to a payload with a different mission function. In all cases, the payloads must have welldefined physical, electrical, and logical interfaces. It is important to note, however, that compliance to well-defined interfaces may still not be sufficient for interoperability. A large, high capacity manipulator arm may be too hea",,2015.0,,64627754,semantic_scholar
60ad8f69b0d150763afe9fde95ff61c475ba6acf,https://www.semanticscholar.org/paper/60ad8f69b0d150763afe9fde95ff61c475ba6acf,A Study of 802 . 11 Bitrate Selection in Linux,"This paper investigates rate adaptation in 802.11 wireless networks, with a focus on algorithms currently available in the Linux operating system. The algorithms are compared with a simple rate adaptation algorithm from the literature, and modifications are presented that increase the performance of th e existing routines in the studied scenarios. In order to compare simulated results with physical results, and to leverage the Linux software ecosystem, a new software simulator based on a virtual 802.11 device is presented. I. I NTRODUCTION In 802.11 wireless networks, data may be transmitted with any of a number of rates, from low-speed bitrates that are resilient under poor channel conditions, to high-speed rat es that require a high signal level to function. The process of a utomatically selecting the rate that maximizes throughput, given the current channel conditions, is known as r te adaptation[1] and has been studied extensively. The first published rate adaptation algorithm, Auto-Rate Fallback (ARF), is an extremely simple state machine that predicts the rate based on the most recent successful rate. SampleRate [2] is a popular algorithm that builds a statisti cal model of rates based on frame success rate and computed throughput. These two algorithms form the basis for others examined later in the paper. Other algorithms attempt to predict the rate based on direct measurements of the signal level at either the transmitter o r receiver [3], [4]. Unfortunately, these solutions often re quire changes to the MAC layer, or expensive low-bitrate broadcas t packets. Recently, loss differentiation has emerged as a promising improvement to frame-loss based algorithms, particularly in congested networks. As these also usually require changes t o the 802.11 specifications [5], [6], or modifications to physi cal hardware [7], uptake of these algorithms in deployed system has been slow. Consequently, it is instructive to study the algorithms currently in wide use in 802.11 LANs. Simulation of rate algorithms has typically been performed using network simulators, such as ns2, originally develope d for wired networks. While these systems work well for comparing different algorithms under controlled circumstances, by t heir nature it is difficult to compare experimental results with real-world trials. Moreover, simulations from the literat ure often fail to account for cross-layer effects that would imp act practical implementations, such as routing delays, and TCP timeouts. One observation is that a simulator may account for crosslayer effects implicitly, by directly using the networking stack of the operating system. One prior attempt to bridge the gap between research simulators and deployed systems is given in [8]. The authors present the library libmac, which allows experimenters to capture and inject frames using modified 802.11 device drivers. This system utilizes physical radio s for packet collection and transport. For simulation purposes, it would be advantageous to instead use virtual radios and mode l the medium. Thus, this paper introduces a simulator based on a virtualized 802.11 device driver, using the Linux Mac80211 [9] wireless stack. In addition to capturing cross-layer effec ts, the proposed simulator provides the ability to directly compar e experiments utilizing virtual devices with those from phys ical devices. This simulator is then used in an investigation of r ate adaptation algorithms used in the Linux operating system. Section III describes the assumptions made about the network and typical hardware devices. In section IV, the rate lgorithms are briefly described. Section V formulates the channel models used in simulation. In section VI, a new 802.11 simulator that utilizes the Linux wireless stack is presented. In section VII, the rate algorithms are compared both in the simulator and in real world experiments. Finally , in section VIII, modifications to the Minstrel algorithm are proposed. II. D IFFERENCES FROMPREVIOUS WORK In this paper, three rate adaptation algorithms are examine d: Minstrel, PID, and AARF [10]. AARF has been presented and reviewed in the literature, as has SampleRate [11], the pred ecessor of Minstrel. Yet, the author is unaware of published comparisons of Minstrel and PID, the two rate adaptation algorithms presently available in the Linux kernel 2.6.32. Simulations of rate adaptation algorithms have previously been carried out in network simulators with the same or similar channel models as those used in this work. The cross-layer accuracy of such simulations relies in some par t on the accuracy of models of other network layers. A new simulator is introduced that models only the 802.11 device and wireless medium while using the existing infrastructur e for the remaining layers. The virtual wireless device driver mac80211_hwsim existed prior to this project for testing Mac80211. In its more limited role as an API testing tool, the driver performed onl y TABLE I: 802.11a Rate Set Rate (Mbps) Modulation Coding Rate Bits per OFDM symbol 6 BPSK 1/2 48 9 BPSK 3/4 48 12 QPSK 1/2 96 18 QPSK 3/4 96 24 16-QAM 1/2 192 36 16-QAM 3/4 192 48 64-QAM 2/3 288 54 64-QAM 3/4 288 basic operations and did not attempt to simulate the wireles s medium. This kernel driver was rewritten to pass frames to user programs to ease development of the channel simulator. III. N ETWORK MODEL For this paper, the 802.11a PHY is used as the basis for experimentation. The newest standard, 802.11n, has recent ly been approved and provides 32 additional rates; however, th Linux rate adaptation API for 802.11n rates is still evolvin g at this time. The 802.11a rate set (Table I) is still in use as part of 802.11g, and provides a variety of speeds. In addition, this paper is primarily interested in applicat ons to small infrastructure networks. In ad-hoc and mesh system , both the range of the network and number of nodes is often large. As a result, rates that work over long distances may be preferred to high throughput, short range rates. Also, in large networks, hidden terminals are common, leading to the frequent use of low-bitrate RTS/CTS protection. A trend in consumer-oriented 802.11 hardware is the increasing use of so-called soft-MAC designs: devices consis ti g primarily of radios and small embedded CPUs where most of the 802.11 MAC Layer Management Entity (MLME) features are performed off-chip by the host computer. These designs a re low cost and have the advantage of being software-updateabl e. Such designs often omit explicit rate control features, rel ying on the host to provide a rate or a set of candidate rates for a frame. A typical design is the Atheros 5212, in which each frame is accompanied by a multi-rate retry (MRR) descriptor . The descriptor consists of four candidate rates, r1, r2, r3, r4, along with a set of retry counts, c1, c2, c3, c4. The device will attempt to transmit a frame c1 times at rater1, thenc2 times at r2, etc., until the retry counts are exhausted or until an ACK is received. The simulator assumes a similar design. IV. RATE ALGORITHMS ARF [12] is among the earliest developed automatic rate selection algorithms. In ARF, if packets are transmitted su ccessfully a fixed number of times, then the rate is raised. If there is a frame loss immediately after a rate change, or if there are two consecutive failures, the rate is lowered. Ada ptive Auto-Rate Fallback (AARF) [10] utilizes the basic results of ARF, but adds the notion of an exponentially increasing threshold for raising the rate. This is intended to correct the observed problem that the periodic failed transmission attempts at higher rates led to decreased overall throughpu t. Minstrel, based on [2], takes a probabilistic approach. Ten percent of sent frames include a random probe rate as the first rate in the MRR chain. Success at each rate is recorded as packets are sent. Every 100 ms, the probabilities of success and computed throughput are updated for all packets, and the se are combined with previous results using an exponentially weighted moving average. The MRR descriptor includes the two best throughputs followed by the best probability rate, then followed by the lowest available rate. The MRR retry counts are selected such that transmissions at a given rate f or all attempts should take no more than 6 ms, and the entire transmission takes less than 24 ms. PID is based on the concept of the proportional-integralderivative feedback controller [13]. The algorithm adjust s the transmission rate to achieve a maximum of 14% transmission failures. Every 125 ms, the controller recomputes the avera ge number of failed transmissions with an exponentially weigh ted moving average. If a large amount of frame loss is detected, the controller can enter a sharpening mode, in which large adjustments to the rate can be made to more quickly approach the targeted success percentage.",,2012.0,,4992831,semantic_scholar
5a9a330290d3a3bd2d8593ec38c5e4d0fdceb1c1,https://www.semanticscholar.org/paper/5a9a330290d3a3bd2d8593ec38c5e4d0fdceb1c1,3rd Workshop on Smart Surveillance System Applications,"Motivation and Justification: 
 
Automatic recognition of people and their activities has very important social implications, because it is related to the extremely sensitive topic of civil liberties. Society needs to address this issue of automatic recognition and find a balanced solution that is able to meet its various needs and concerns. In the post 9/11 period, population security and safety considerations have given rise to research needs for identification of threatening human activities and emotional behaviours. 
 
Timely identification of human intent is one of the most challenging areas of ""all-hazards"" risk assessment in the protection of critical infrastructure, business continuity planning and community safety. The ""all-hazards"" approach is used extensively by the public and private sector, including Public Safety Canada (PS Canada -- formerly PSEPC), Emergency Management Ontario (EMO), US Federal Emergency Management Agency (FEMA) and US Department of Homeland Security (DHS). 
 
There is a clear need for industry and the research community to addresses fundamental issues involved in the prevention of human-made disasters, namely the variable context-dependent, real-time detection/identification of potential threatening behaviour of humans, acting individually or in crowded environments. 
 
Such an industry and academia forum will have to discuss development and commercialization of new multimodal (video and infrared, voice and sound, RFID and perimeter intrusion) intelligent sensor technologies for location and socio-cultural context-aware security risk assessment and decision support in human-crowd surveillance applications in environments such as school campuses, hospitals, shopping centers, subways or railway stations, airports, sports and artistic arenas etc. Due to the complexity of the surveillance task there is a clear need for the development of a distributed intelligent surveillance system architecture, which combines visual and audio surveillance based on wireless sensor nodes equipped with video or infrared (IR) cameras, audio detectors, or other object detection and motion sensors with location aware wireless sensor network solutions. The integration of visual, sound and radio tracking methods results in a highly intelligent, proactive, and adaptive surveillance and security solution sensor networks. Task-directed sensor data collection and observation planning algorithms need to be developed to allow for a more elastic and efficient use of the inherently limited sensing and processing capabilities. Each task a sensor has to carry out determines the nature and the level of the information that is actually needed. There is a need for ""selective environment perception"" methods that focus on object parameters that are important for the specific decision to be made for the task at hand and avoid wasting effort to process irrelevant data. 
 
Multisensor data fusion techniques should be investigated for the dynamic integration of the multi-thread flow of information provided by the heterogeneous network of surveillance sensors into a coherent multimodal model of the monitored human crowd. 
 
In the context of crowds, robust tracking of people represents an important challenge. The numerous sources of occlusions and the large diversity of interactions that might occur make difficult the long-term tracking of a particular individual over an extended period of time and using a network of sensors. Realtime image processing and computer-vision algorithms need to be studied for the identification, tracking and recognition of gait and other relevant body-language patterns of the human agents who can be deemed of interest for security reasons. Real-time signal processing algorithms have to be designed for the identification and evaluation of environmental and human-subject multimodal parameters (such as human gait, gestures, facial emotions, human voice, background sound, ambient light, etc.) that provide the contextual information for the specific surveillance activity. 
 
A multidisciplinary, context-aware, situation-assessment system, including human behaviour, cognitive psychology, multicultural sociology, learning systems, artificial intelligence, distributed multimedia and software design elements, has to be ultimately developed for the real-time evaluation of the activity and emotional behaviour of the human subjects identified as being potentially of security interest in the monitored dynamic environment. 
 
The development of such a complex system requires the seamless integration of new and improved surveillance techniques and methodologies supporting both functional and non functional requirements for surveillance networks. Functional requirements are signal processing functions and data fusion, archiving and tracking human behaviours, assessment and interpretation functions of the data, and supporting human decision makers, among others. Non-functional requirements include interoperability, scalability, availability, and manageability. 
 
The partial and heterogeneous sensor-views of the environment have to fuse into a coherent Virtualized Reality Environment (VRE) model of the explored environment. Being based on information about real/physical world objects and phenomena, as captured by a variety of sensors, VREs have more ""real content"" than the pure Virtual Reality environments entirely based on computer simulations. The VREs model of the explored environment allows human operators to combine their intrinsic reactive-behavior with higher-order world model representations of the immersive VRE systems. 
 
A synthetic environment will eventually be needed to provide efficient multi granularity-level function-specific feedback and human-computer interaction interfaces for the human users who are the final assessors and decision makers in the specific security monitoring situation. 
 
An ideal system should provide efficient multi granularity-level function-specific feedback for the human users who are the final assessors and decision makers in the specific security monitoring situation. 
 
The rate at which surveillance systems can currently disseminate data to evaluate new threats is mainly limited due to the developed and implemented nature of existing systems and their limited ability to operate with other systems. IBM's Service-Oriented Architecture (SOA) provides the much needed deployment ready solution which supports the integration of external systems developed by diverse industrial and institutional partners.",CASCON,2011.0,,39995229,semantic_scholar
e5587c05c33aff79bdef5c6610778868498feb81,https://www.semanticscholar.org/paper/e5587c05c33aff79bdef5c6610778868498feb81,Theme 7 : Intelligent project lifecycle knowledge management and decision support,"Rapid advances in technologies such as networks, database analysis techniques, and high speed processing helped the growing importance of organizational decision support systems (DSSs). Today's organizations invest in various knowledge management (KM) systems and tools to enable seamless integration of the constantly increasing volume and sources of information. However, with the myriad of commercially available DSSs, organizations are facing the problem of a growing gap between the solely available applications and the business users' requirements for effectively take advantages of their support in the decision making process. To narrow this gap, an integrated and flexible environment is needed to assimilate the use of many of these systems and satisfy the growing users and needs. In this paper, we argue that decision support systems need to use software agents as the natural means of representing the broadly categorized business users of an organization. They hide from their users the complexity of underlying technology of data extraction, analysis, information retrieval and knowledge sharing, and enabling them to make fact-based and timely decision with more degree of confidence. This research approach aims to address the research issue of how KM can be optimized using intelligent agents and how to enhance decision-making process. Second, we delineate the design and implementation of such environment, based on software agent development technology, and endowed with various types of data access capabilities and information retrieval. The system supports multiples business users. The proposed system is applied to a real-world project lifecycle case that is SE (Software Engineering) project. A prototype of the system is presented where intelligent agents are the building blocks of a peer-to-peer organization wide system. The application was implemented using Eclipse technology, and the agents were deployed on the FIPA-OS (Foundation for Intelligent Physical Agents-Open-Source) environment, we used JESS (Java Expert System Shell) to develop the knowledge based of the agents' reasoning.",,,,15067876,semantic_scholar
abb0cd9f718bf472e16bee39f525aa6ee219e411,https://www.semanticscholar.org/paper/abb0cd9f718bf472e16bee39f525aa6ee219e411,Intelligent Network Design of intelligent multinode Sensor networking,". The paper deals with the self configured intelligent sensor networking. The individual sensors are acting on the body or an object to measure different parameters. Although the sensors are measuring parameters accurately, but they are failed to act depending on different situations. For example a robot is moving on a surface can take decision to turn left or right when an obstacle come across. But the same robots take wrong decision when the obstacle is not static. The robot can wait till the obstacle passed away from its way. But the robot still follows the traditional way, which is turning left or turn. In this case the robot is failed to take correct decision depending on the situation. If we consider other example such as traditional automatic water supply to plants or crops, the system supplies the water at regular intervals of time with accurate quantity. But the system takes same decisions in all seasons irrespective of the soil type and crop type. In our system we are proposing a Wireless Distributing sensor system design which is able to take wise decisions as a farmer. A farmer can understands how much water the soil needs and at what time it need to apply. In our work, we are developing, (1) Home Area Networking (2)software supporting above functions; (3) Wireless Sensor Networking. Introduction: My paper describes about advanced self configured Wireless Distributed Sensor networking. My project support universal sensors, network management, GUI software, house area network (HAN) [1]. The smart environment relies first and foremost on sensory data from the real world. Sensory data comes from multiple sensors of different modalities in distributed locations. The smart environment needs information about its surroundings as well as about its internal workings. Our wireless sensor networks is involved with challenging issues wireless sensor systems, self-organization, signal processing and decision-making, and finally some concepts for home automation, We have identified some facts are : 1. Most networks are application specific, extensive secondary development is necessary to adapt to different circumstances. 2. Most of them are run by developers professionals rather than end users. 4. It is difficult for end users to configure and deploy a practical sensor network. 5. Systematic compatibility for diverse sensors and communication channels is limited. 6. The aquatic sensor network technology lags behind terrestrial development in terms of use of modern technology. Often, a single sensor cannot fully capture the measured phenomenon, so researchers develop multisensor systems to obtain more accurate information, as shown in Figure 2-1. Smart Sensor Enhanced functions include “compensation of secondary parameters (e.g. temperature), failure prevention and detection, self-testing, auto-calibration”. Figure 2-1: Multi-sensor Sensing Model Figure 2-2: Smart Sensor Network A sensor network consists of multiple detection stations called sensor nodes [3].The transducer generates electrical signals based on sensed physical effects and phenomena. The microcomputer processes and stores the sensor output. The transceiver, which can be hard-wired or wireless, receives commands from a central computer and transmits data to that computer. The power for each sensor node is derived from the electric utility or from a battery. The observations made against the characteristics of DSN are: Extended wider coverage of the environment Better fault tolerance Higher quality of measurements liminate ambient interference. Shorter response delay for changing events. Flexible size of network ISSN : 0975-3397 468 N. Suresh kumar et al. / (IJCSE) International Journal on Computer Science and Engineering Vol. 02, No. 03, 2010, 468-472 Wireless House Area Network WHAN The research group started related investigations under Low Power Wireless Sensor networking. We are integrating DSN with WHAN in house area network system. Based on this DSN platform, we are implementing wireless house area network design. Research Issues and Challenges Many research issues and challenges have been exposed. Design considerations for sensor networks [7]. Sensing aspect Computation part Wireless Sensor Networking Faster algorithm for data tranceiving. Signal Conditioning [2] Smart Sensor includes basic blocks for signal conditioning (SC), digital signal processing (DSP), and A/D conversion. Signal conditioning [6] is performed using electronic circuitry analog low pass filter. Temperature compensation can also be added during the Signal Conditioning stage. A basic technique for improving the signal-to-noise ratio (SNR) is low-pass filtering, since noise generally dominates the desirable signals at high frequencies. Shown in the figure is an analog LPF that also amplifies, constructed from an operational amplifier. The transfer function of this filter is with 3 dB cutoff frequency given by rad.",,2010.0,,15689740,semantic_scholar
ab60ca1c14bfd348b10074258db61003349cf2de,https://www.semanticscholar.org/paper/ab60ca1c14bfd348b10074258db61003349cf2de,An Empirically Derived Taxonomy of Information Systems Integration,"Information systems integration (ISI) represents the degree of cooperation in information system practices between business functions within a firm and between a firm and its trading partners. Although the establishment of information systems integration objective has been reported as one of the key concerns of top management because ISI enhances the firms’ competitiveness and growth, the classification of the information system practices and its managerial implications are still vaguely developed. The two objectives of this paper are: (1) to develop a taxonomy of information systems integration (ISI) called ISI-Matrix, and (2) to report managerial implications for matching each information system class with business process applications. By using a systematic research investigation approach, two ISI structures are identified: Internal ISI (IISI) and External ISI (EISI) from the responses of 220 firms. The ability to identify and understand the implications of the ISI-Matrix is of critical importance to both academic and management practitioners. INTRODUCTION The rapid changes in perspective toward globalization of markets and manufacturing has forced management to re-configure the traditional views of business functions and replace them with business processes. The process view of organizations embraces cross-functional teams which penetrate networks of inter-organizations and intra-organizations. Within the process, a project team performs many tasks across functional barriers (with a firm and between a firm and its trading partners) to meet corporate goals in a more seamless way. This increased emphasis on improving business processes has triggered the need for placing information systems (IS) in a strategic role of corporate strategy as opposed to a supportive role in the traditional view (Raghunathan & Raghunathan, 1990; Chan et al., 1997; Goodhue et al., 1992). A review of the empirical literature reveals that one issue, the linkage of IS practices with organizational objectives, has been among the top problems reported by information systems (IS) managers and business executives (Reich and Benbasat, 1996; Computerworld, 1994; Lederer and Mendelow, 1986; Earl, 1989). Information Systems Integration (ISI) is the degree of cooperation between business functions within the firm and between a firm and its trading partners on an internally consistent set of strategic, operational, and infrastructural information systems practices using information systems (IS). In a broader sense, ISI often represents as a pressing concern of misalignment of information system practices between two business processes (King and Teo, 1997; Segars and Grover, 1998). In this context, information system practices, which are utilized to accomplish process tasks at each end, lack degree of congruence when certain processes/tasks involve cross-functional boundaries at the other end. Consequently, the first objective of this paper is to identify a set of IS practices that is shared by process team members. Therefore, ISI represents the degree of cooperation in information system practices between business functions within a firm and between a firm and its trading partners. ISI has been reported to facilitate the possibilities of increased productivity, customer responsiveness, and the synchronization of diverse organizational settings. It has been documented that the introduction and utilization of ISI enhance firms’ competitiveness and growth, product quality, productivity, machine utilization, space management, and logistics efficiency and flexibility (Gross, 1984; Kaltwasser, 1990; Noori and Mavaddat, 1998). A higher degree of ISI creates information visibility and captures the moments of information which enable T. Jitpaiboon, T.S. Ragu-Nathan & M. Vonderembse 2005 Volume 15 Number 2 18 collaborative members of the supply chain to manage their business processes and share information better (Lummus and Vokkurka, 1999; Gunasekaran and Ngai, 2004; Bourdreau and Couillard, 1999; Williams et al., 1997; Gangopadhyay and Huang, 2004). Although ISI has been reported to positively impact firm performance, the classification of the information system practices and its managerial implications are still vaguely developed. The classifications of ISI in the current literature are extremely broad and fragmented. There is no consensus on what constitutes an ISI taxonomy. Our goal is to develop a comprehensive ISI taxonomy to aid organizations whose ability to harness the power of IS practices is critical to their success. Development of valid and reliable instruments to be used in large-scale surveys is an important first step toward this goal. The resulting taxonomy should help organizations to match information system class with their current business process applications which should enhance a firm’s internal and external integration. In a narrow sense, focusing on the survey approach, this study arguably classified ISI into two main categories namely Internal Information Systems Integration (IISI) and External Information Systems Integration (EISI). In each category, ISI construct is also clustered into three levels namely Strategic Integration, Operational Integration, and Infrastructural Integration. Infrastructural integration is also subdivided into two sub-constructs: Data Integration and Network Integration. Table 1 shows the components of ISI classifications. By deploying this classification scheme, the second objective of this paper is to propose a classification matrix (ISI-Matrix) which will be used to provide managerial implications for both researchers and practitioners. The next section of this paper defines and discusses the ISI taxonomy. The following sections describe the research design and discuss the candidate measured used to evaluate the degree at different ISI levels. The subsequent section presents the results, and the final section discusses the implications of our findings for researchers and practitioners. Table 1: Information System Classifications. Internal Information System Integration (IISI) External Information System Integration (EISI) • Strategic Integration – Internal • Operation Integration – Internal • Infrastructural Integration – Internal o Data Integration – Internal o Network Integration – Internal • Strategic Integration – External • Operation Integration – External • Infrastructural Integration – External o Data Integration – External o Network Integration – External INFORMATION SYSTEMS INTEGRATION TAXONOMY A description of previous taxonomies The rapid changes in the role of information systems (IS) are presenting firms with significant challenges and dramatic opportunities. Revolutionary advances in hardware and software capabilities coupled with reduced prices have shifted numerous applications from infeasible to feasible, changed the structure of organizations, and forced management to rethink the classification of IS. The terms taxonomy and framework are sometimes used interchangeably in the literature (Doke and Barrier, 1994). However, a clear distinction between the two is identified: taxonomy is generally used to describe a classification scheme for “things” such as IS. Although framework is sometimes used as a synonym for taxonomy, it is more often used to describe models that organize and group “concepts and relationships” (Doke and Barrier, 1994). The taxonomy is derived from the characteristics of the measured subjects, so the categories are both exhaustive and mutually exclusive (Fiedler, Grover, and Teng, 1996). This method is especially useful when one is examining unexplored phenomena because both methods must be empirically examined to evaluate the representativeness and generalizability of the classifications to the population they are mean to describe. Unlike the predetermined, idealized categories of a typological methodology that lend themselves to prescriptive hypothesizing, taxonomy’s classifications emerge from analysis so that the classification is derived (Doty and Glick, 1994; Hair, Anderson, Tatham, and Black, 1998). Developing a taxonomy An Empirically Derived Taxonomy Journal of International Technology and Information Management 19 can be viewed as a multistep process including the classification scheme, measurement development, multivariate analysis of the classification criteria to produce the item groupings, and the evaluation of the classification groupings (Fiedler, Grover, and Teng, 1996). The classification systems should “mirror the real world...describe organizational reality in a way that is recognizable to and consistent with the vision of practitioners and researchers alike as a viable reproduction of the diverse world in which we live in” (Rich, 1992). Since the focus here is the classification of ISI, a taxonomy will be used to classify these models Dimensions of a Taxonomy Integration is “to make into a whole” (Oxford English dictionary). The study of ISI classifications started as early as 1985 by Mudie and Schafer. They analyzed ISI in process terms, as they believed ISI should not only facilitate the process of development and use of data, applications, and other processing technology, but also should provide the flexibility to meet the future business demands in workstations, processing types, and applications. Wyse and Higgins (1993) defined ISI as the extent to which data and applications through different communication networks can be shared and accessed for organizational use. They defined ISI into two components: data integration and technical integration. Data integration refers to the relevancy of the information that is collected, processed, and disseminated throughout the firm. Technical integration concerns the physical or formal linkage of information systems and subsystems that are used by the firm. Webber and Pliskin (1996) defined ISI in the merger or acquisition context as the extent of the integration of IS and data processing functi",,2006.0,,40863671,semantic_scholar
e349556d5302749ee79643792e60b192020a42b2,https://www.semanticscholar.org/paper/e349556d5302749ee79643792e60b192020a42b2,Special Issue on Wireless Sensor Network: Theory and Practice,"Wireless sensor network (WSN) is an emergent multi-disciplinary science, and it may be considered as the foundation of pervasive computing, mobile computing and wearable computing. WSN is a very active and competitive research area due to its diverse and unlimited potential applications: air, underground and underwater. In spite of its young age, economic impact of WSN is important, for examples the industrial control segment market will be worth $5.3B by 2010 and the smart home market will be worth $2.8 billion worldwide by 2012 (Source: Stamatis Karnouskos, EU-US 08 Workshop). WSN is a set of wireless nodes. On one hand, each wireless node (WN) has similar hardware and software functionalities as a PC: CPU, memory, operating system, and communication protocol to fulfill a specific task. On the other hand, a WN has a limited power supply (embedded battery) and consumes approximately 1 million less power (~100μW instead of ~100W) than a PC. Due to resources constraints: energy consumption and form factor, the approaches applied in general purpose computer systems are not adapted to the requirements of WSN. When it comes to the design of energy efficient oriented hardware and software components of WSN, cross-layering optimization approaches are generally adopted such as application-specific unified hardware and software by taking into account the following criteria: trade-off between complexity, efficiency and resource consumption, and application context (context-aware) etc. Currently two main hardware development and design trends are carried out to implement the WN: Commercial OffThe-Shelf ‘COTS’ and System on Chip ‘SoC’. The first and second generations of WN were designed by using low power 8-bit or 16-bit microcontroller processor core, Bluetooth and non standard wireless access medium (MICA Mote). The current trend of WN design such as Tmotesky, iMote and LiveNode are based on low power 16-bit or 32-bit RISC microcontroller, and full compliance IEEE802.14.5 standard. However the ultimate goal of all the researchers in the world is the implementation of long life, low cost and invisible WN integrated and embedded into environment. Three key technologies make possible to achieve this objective: MEMS ‘MicroElectroMEchanical systems’, UWB ’Ultra-Wide Band’ and low power CMOS technology. Different WN prototypes are realized by Intel (iMOTE2), University of Michigan (MOTE: Michigan Uni Prototype) and University of Berkeley (Pico-Mote). WN hardware seems easier to solve than embedded software for diverse WSN applications. The main questions which are related to WSN basic software design is how to keep modularity, high level abstraction and reliability to enable to implement complex massively distributed WSNs to meet resource constraint requirements. Real-time operating system (RTOS) plays a key role to support high level abstraction and distributed collaborative processing. Currently four categories of WSN’s RTOS are developed: Event driven (TinyOS), Multitask (RETOS, tKernel, NutOS, MANTIS), Data-Centric (AmbientRT), and Hybrid (Contiki, LIMOS). Note that TinyOS is very popular but it not adapted to complex hard real-time application. The development challenges of the WSN RTOS are energy-efficiency (context aware, configurable, small footprint), robustness, fault tolerance, support hard real-time constraint, and support component based model (high level of abstraction to ease the integration of high level SW such as protocol, middleware, application, and simulator). Furthermore, for WSN applications, message sending is energy consuming. Thus it is important to implement embedded energy efficient wireless routing protocol to increase WSN lifetime. It is clear that general purpose MANET routing protocol such as AODV (active), OLSR (proactive) and ZBR (hybrid) etc. are not suitable for WSN due to resource constraints. For example optimal routing path is well adapted to general purpose MANET but not suitable for WSN because the repetitive use of the same path will exhaust the battery of WNs belonging to the optimal routing path (black hole). Many WSN dedicated protocols are implemented (spin, cougar, gear, leach, speed ...) but it is currently very difficult to have a clear idea concerning their performance (energy consumption, scalability, connectivity, lifetime ...) because of the lack of large scale WSN real world experimentation results and because the simulation model does not reflect the real-world ones (physical layer). Note that, there is no standard scenario and the application program (with a known number of WNs) enables to evaluate rationally the performance of wireless routing protocols. In addition routing protocol relies on the WSN topology. On one hand, an optimal WSN topology facilitates the implementation of routing and administration protocols. On the other hand, the deployment of large scale WS nodes in a large area is random and its topology is a priori unknown. Then, it is important to investigate the auto-configuration algorithms to increase the efficiency of routing and administration protocols. However the frontier between the administration protocol and the routing protocol is not as clearly defined as in a classical network (e.g. TCP/IP and SNMP) due to cross-layering approach. Moreover WSN security, reliability, and fault tolerance are still an open problem. In this special issue 5 papers are selected among 40 submitted papers for the NTMS workshop on wireless sensor network: theory and practice, held at Tangier at Morocco in 2008, the rest of the papers are selected from an open call for paper. WSN is a multi-disciplinary science. It impossible to present all its topics but this special issue addresses most of the WSN embedded software problems dealing with real-world applications (EU NeT-ADDED FP6 project, French ANR research project and industrial projects). JOURNAL OF NETWORKS, VOL. 4, NO. 6, AUGUST 2009 379",,2009.0,,18352686,semantic_scholar
368687003560b21e53865cd604aae8c00dc62c4b,https://www.semanticscholar.org/paper/368687003560b21e53865cd604aae8c00dc62c4b,PassItOn: An Opportunistic Messaging Prototype on Mobile Devices,"With the increasing popularity of mobile handheld devices and the growing capability of these devices, it is becoming possible that information sharing/dissemination is carried out through human networks, as a complement to the traditional computer networks. In such human networks, people come across one another, while their mobile devices exchange and store information in a spontaneous and transparent way. Such an encounter could be established through direct device-todevice connectivity when two devices come into each other’s communication range, or be enabled by, e.g., a Wi-Fi access point, when the devices both enter its coverage. A new form of dissemination, which we call opportunistic messaging, is such an application that is based on human encounters and mobilities. When human encounters are exploited for communications, the reliance on network infrastructure access is eliminated; communications can be performed even where infrastructure is absent or infrastructure access is intermittent. By leveraging human mobilities, data delivery does not require an end-to-end path from the source to a recipient; instead, people carrying mobile devices serve as relays – they cache others’ data and forward/deliver the data when appropriate. Thus, the propagation of information is tied to people’s physical proximity when they move around, and incorporates the social aspects of communications as people tend to spend more time co-locating with their social relations. Opportunistic messaging is applicable anywhere, and is especially appealing where network infrastructure access is limited or intermittent (e.g., on cruise ships, in national parks, after disasters). Another intriguing characteristic of it is its ease of deployment – no central server is needed, but only a single piece of software on users’ mobile devices. However, as human encounters and mobilities are unpredictable, when used for social applications, opportunistic messaging is most suited for disseminating user-generated information that is non-formal, less important, and thus not time-critical. In recent years, a considerable amount of efforts have been invested in the research on opportunistic networking and delay-tolerant networking (which encompasses opportunistic networking but is a broader concept). A large portion of prior work has focused on routing issues, e.g., through whom as intermediate carriers to deliver a message to the destinations [1] [2] [3] [4]. The routing issues have been further explored in various contexts, such as in vehicular networks [5] [6] [7] and in social networking applications [8] [9] [10] [11]. However, serious real-world application development, deployment and evaluation of the opportunistic networking concepts still fall behind [12], in which many challenging issues remain to be addressed (to name a few, location-awareness, user incentives and preferences, power preservation, encounter controls, etc.). In this work, we design and prototype PassItOn, a fully distributed opportunistic messaging system. Our goal is to build up a proof-of-concept platform on real mobile devices, and thus show the feasibility and potentials of utilizing human movements for dissemination applications. Meanwhile, we seek to shed lights on the design, implementation and deployment issues in building such systems, and thus stimulate new ideas and perspectives on addressing these issues. Moreover, we aim to offer a real testbed on which new mechanisms, protocols and use cases can be tested and evaluated.",2009 6th IEEE Consumer Communications and Networking Conference,2009.0,10.1109/CCNC.2009.4785009,17079094,semantic_scholar
225b6e4de88e9be8caa5c36224d135e1ed6f00ee,https://www.semanticscholar.org/paper/225b6e4de88e9be8caa5c36224d135e1ed6f00ee,Extended Abstract : A Framework for Virtual Surgery,"Surgical simulation for medical education and preoperative planning has attracted more and more attention in recent years and a number of such virtual environments have been developed and validated. However, almost all of them are focusing on simulating the surgical scene and haptic interaction to provide users with freedom to perform surgery in the virtual environment. We propose that critical surgical procedures and motion path could be guided by an information intensive process model, especially those being trained in such a virtual surgical environment. In this paper, we outline the virtual surgery framework and the design of the software environment for suturing procedure. The preliminary system that incorporates the above functionality with realistic surgical scene and haptic interaction is still on the development stage. Potentially, the information based modeling process of the surgical motion could also help automate the process of the procedural operation of surgical robot. THE CONCEPTUAL FRAMEWORK The general view of our framework architecture is shown in Figure 1, where the whole system is composed of the hardware part and the software part. In the hardware side(refer to Figure 2), besides such tradition input and output device like keyboard, mouse, haptic device and display, we leave room for sensors to record surgical motion as system input or drive micro assembly work cell to validate and measure our virtual environment in precisely simulating the microsurgery process quantitatively. The software architecture is generally composed of the following three modules. Fig. 1. Framework Architecture A. The IDEF-0 Parser In our approach, the emphasis is on the creation of an Information Intensive Process Model using the IDEF-0 modeling methodology. This model will be used to identify critical and non critical categories of information encompassing the core steps in a neurosurgical (diagnosis) and surgery process; these will possibly include the key or driving assumptions, information inputs, skill constraints, the intermediate ’attribute’ outcomes between various steps or stages of the process in reference as well as the crucial performers (which can range from the medical personnel involved in the diagnosis and surgery D/S itself to the medical assisting devices which play a key role in the outcome of various steps in this D/S process). This information model will not only capture the functional relationships among related tasks at various levels of abstraction but will also enable the representation of temporal precedence constraints among sub-tasks. They will provide a valuable insight into the process of micro surgery; our background in engineering and IT and our prior work in such modeling activities will be helpful in (a) Haptic: Phantom Premium (b) Microassembly Work Cell Fig. 2. Hardware Setup developing such a model. A lower level decomposition of this model is shown in Figure 3. Fig. 3. IDEF-0 Model (A2 Level Decomposition) We seek to develop a robust understanding of these surgical processes using information modeling strategies that have proven to be successful in understanding functional relationships within complex processes that range from performing micro devices assembly to designing virtual reality based simulation environments for satellite assembly (among others). B. Surgical Mode Selection After the framework reads and parse the customizable IDEFL-0 model file, the user will be having the option to further customize the system mode, which is composed of training mode, planning mode, and evaluation mode. The default mode is the virtual environment without any constraints. Those three modes in Figure 1 are described as: • Training and Evaluation Mode: students are able to practice surgery following those constraints defined in the information intensive process model (IIPM) by instructors or experienced surgeons. Meanwhile, the system could evaluate students’ performance based on the comparison of their operation with those constraints defined in the model. • Planning Mode: surgeons could first freely explore different potential surgical strategies in the virtual environment, and then decide a couple of final procedural surgical plans and define those procedural constraints in the IIPM for future validation. • Validation Mode: this is where users could validate those surgical operations in the virtual environment by driving the physical validation system. In our system, since we are focusing on microsurgery, a microassembly work cell (in Figure 2(b)) is used as validation purpose. C. The Virtual Reality Engine This is where the graphics visualization, collision detection and haptic rendering happens, and the FEM Analysis module in VR Engine is used to simulate the soft tissue deformation. Meanwhile, the surgical path generator module is used to define haptical constraints and visualize critical path. The red line in Figure 6 in section IV illustrates this idea. CASE STUDY: VIRTUAL SUTURING In this section, we give a overview of the model and development process of our virtual suturing system based on the above framework. During the early modeling stage, we got our first hand suturing information by learning the microsurgery lab manual and direct clinical microsurgery experience one of our author of this paper has over 20 years clinical microsurgery experience, and the real sutured scene in Figure 4 was taken by a micro lens camera after he was done with the suture. Fig. 4. Real Suturing Scene A. Information Intensive Process Model and Constraints After collecting all those real world information, we are able to model the whole process. The IDEF0 diagram in Figure 3 gives a general model of the real surgical Suturing process. For simplicity of this paper, we will not give the further decomposition here. Meanwhile, two critical steps in performing suturing is illustrated in Figure 5, where Figure 5(a) shows the critical path when the needle starts to insert into the vessel the needle has to be as perpendicular as possible to the target surface, and Figure 5(b) shows the force constraints of how to guide the needle go through the vessel wall. B. Preliminary Results From Figure 5, we can notice that the surgical path during needle insertion and going through vessel wall are critical steps of a successful suture, so to speak, they are important evaluation criteria in the training and evaluation mode, potential procedural constraints in the planning mode, as well as (a) Insertion Angle (b) Shear Force Fig. 5. Suturing (Courtesy: Internet) Fig. 6. Linear Constraints precision motion control in the validation mode. Because the framework is still in the early development stage, in order to illustrate the idea how we incorporate constraints for those critical steps, we developed a much simplified scene to show how the linear constraint guide a surgical cut procedure in Figure 6, where the red line represents the linear constraints -when the scalpel is about to cut the blood vessel (represented by a hollow cylinder), the user will be able to feel a force that guide their movement along the line. RELATED WORK Due to the huge volume of recent research papers on virtual reality system for surgical training and preoperative planning, [8],[6] and [12] have given a relatively detailed review of surgical simulation applications and technology. We limit our survey on the state of art of virtual reality system for training and complex microsurgery preoperative planing, and those key modeling and visualization techniques that enabled such virtual reality system. Surgical Training and Planning System:Though technology has been evolved a lot for the past decades, it is still a challenging task to efficiently simulate the complex surgical operation environment. To avoid the situation of starting an over ambitious project that includes everything but eventually get nothing done, most of the virtual surgery system focus on creating a realistic surgical scenario for some specific purpose, e.g. suturing training [8] [7][19] [1][13] or preoperative planning [21][18] [9][16][17]. Soft Tissue Modeling: Simulating the soft tissue deformation under surgical operation is not trivial, even a simple procedure involves great effort, eg. [3][11] [4][10] were specifically devoted to simulate the cutting procedure. [14] gives a detailed survey of the real-time deformable models used for surgery simulation. Among those models mentioned in [14], MassSpring model and FEM model are the two dominant models currently used in the research community. For its simplicity and low computational cost, the heuristic Mass-Spring model has attracted lots of research and was extensively optimized and deployed in the early years, such as in SPRING system developed by Kevin et all [15]. However, due to its lacks of realism, recent research work are shifting to center around the later continuum mechanics based FEM model as in [1] [5][2] for its accuracy and realistic mechanic behavior, though it is computational costly and needs lots of optimization to achieve real time performance.",,2009.0,,12527909,semantic_scholar
59304bc73b6d24f18d23404e0d408c462b66c4d0,https://www.semanticscholar.org/paper/59304bc73b6d24f18d23404e0d408c462b66c4d0,Simulationsbasierte Analyse und Entwicklung von Peer-to-Peer-Systemen,"Peer-to-Peer (P2P) systems are distributed systems composed of up to millions of functionally equivalent entities (peers), which form P2P overlay networks on top of physical networks to communicate. The functionality of a peer is implemented by a P2P application which de nes the behavior of the whole P2P system. The equivalence of peers is realized by providing client functionality as well as server functionality. Implementing a P2P system with speci ed behavior is a di cult task because the behavior depends on many factors, such as the used P2P search methods and the underlying physical network. Some factors cannot be taken into account completely because of their complexity or unknown or not understood parts. For instance, the prospective user behavior may only be estimated based on observed data. When engineering complex, dynamic software systems such as P2P systems, simulation is often used to analyze the properties of these systems based on simulation models in an early development phase. With simulations in natural sciences, the separation of reality and (simulation) model is clear: the reality exists in nature, while the model exists as software within some computer system. When simulating software systems, this separation is not so obvious: the simulated model is itself a software system. With P2P systems, for instance, a simpli ed P2P system is modeled and simulated for predicting properties of real P2P systems. The new software engineering contribution of this work is the Peer Software Engineering (PeerSE) method, which allows a controlled transition from simulation models to real-world software systems. The method starts with a comparative analysis of simulation models for P2P systems and proceeds iteratively toward the experimental implementation in a laboratory setting and nally a real-world P2P system deployed in a target environment. The method includes a simulation model for P2P systems and a tool supporting the execution of simulation and laboratory experiments. Simulation is an essential part of the PeerSE method used to identify and to compare models ful lling given requirements. When an appropriate model has been found, model components can be reused and further re ned to implement a laboratory P2P system. To allow for a controlled transition of model components to laboratory components, the results of simulation and laboratory experiments are directly compared using the same metrics. The applicability of the PeerSE method has been successfully evaluated by analyzing and realizing a P2P system for distributed software development.",Softwaretechnik-Trends,2008.0,,34594930,semantic_scholar
1fbf5bfc48293616106a189a3c8874070cbd57ec,https://www.semanticscholar.org/paper/1fbf5bfc48293616106a189a3c8874070cbd57ec,An investigation of techniques to assist with reliable specification and successful simulation of fire field modelling scenarios,"Computational fluid dynamics (CFD) based Fire Field Modelling (FFM) codes offer powerful tools for fire safety engineers but their operation requires a high level of skill and an understanding of the mode of operation and limitations, in order to obtain meaningful results in complex scenarios. This problem is compounded by the fact that many FFM cases are barely stable and poor quality set-up can lead to solution failure. There are considerable dangers of misuse of FFM techniques if they are used without adequate knowledge of both the underlying fire science and the associated numerical modelling. CFD modelling can be difficult to set up effectively since there are a number of potential problems: it is not always clear what controls are needed for optimal solution performance, typically there will be no optimal static set of controls for the whole solution period to cover all stages of a complex simulation, there is the generic problem of requiring a high quality mesh - which cannot usually be ascertained until the mesh is actually used for the particular simulation for which it is intended and there are potential handling issues, e.g. for transitional events (and extremes of physical behaviour) which are likely to break the solution process. 
 
In order to tackle these key problems, the research described in this thesis has identified and investigated a methodology for analysing, applying and automating a CFD Expert user's knowledge to support various stages of the simulation process - including the key stages of creating a mesh and performing the simulation. This research has also indicated an approach for the control of a FFM CFD simulation which is analogous to the way that a FFM CFD Expert would approach the modelling of a previously unseen scenario. These investigations have led to the identification of a set of requirements and appropriate knowledge which have been instantiated as the, so called, Experiment Engine (EE). This prototype component (which has been built and tested within the SMARTFIRE FFM environment) is capable, both of emulating an Expert users' ability to produce a high quality and appropriate mesh for arbitrary scenarios, and is also able to automatically adjust a key control factor of the solution process. 
 
This research has demonstrated that it is possible to emulate an Experts' ability to analyse a series of simulation trials (starting from a simplified, coarse mesh test run) in order to improve subsequent modelling attempts and to improve the scenario specification and/or meshing solution in order to allow the software to recover from a complete solution failure. The research has also shown that it is possible to emulate an Expert user's ability to provide continual run-time control of a simulation and to provide significant benefits in terms of performance, overall reliability and accuracy of the results. 
 
The instantiation and testing of the Experiment Engine concept, on a chosen FFM environment - SMARTFIRE, has demonstrated significant performance and stability gains when compared to non Experiment Engine controlled simulations, for a range of complex ""real world"" fire scenarios. Preliminary tests have shown that the Experiment Engine controlled simulation was generally able to finish the simulations successfully without experiencing any difficulty, even for very complex scenarios, and that the run-time solution control adjustments, made to the time step size by both the Experiment Engine and by the Expert, showed similar trends and responses in reacting to the physical and/or numerical changes in the solution. This was also noticed for transitional events seen during the simulation. It has also been shown that the Experiment Engine (EE) controlled simulation demonstrates a saving of up to 40% of simulation sweeps for complex fire scenarios when compared with non-EE controlled simulations. Analysis of the results has demonstrated that the control technique, deployed by the EE, have no significant impact on the final solution results - hence, the Experiment Engine controlled simulations are able to produce physically sound results, which are almost identical to Expert controlled simulations. 
 
The research has investigated a number of new methods and algorithms (e.g. case categorisation, case recognition, block-wise mesh justification, local adaptive mesh refinements, etc.) that are combined into a novel approach to enhance the robustness, efficiency and the ease-of-use of the existing FFM software package. The instantiation of these methods as a prototype control system (within the target FFM environment - SMARTFIRE) has enhanced the software with a valuable tool-set and arguably will make the FFM techniques more accessible and reliable for novice users. 
 
The component based design and implementation of the Experiment Engine has proved to be highly robust and flexible. The Experiment Engine (EE) provides a bi­directional communication channel between the existing SMARTFIRE Case Specification Environment and the solution module (the CFD Engine). These key components can now communicate directly via status- and control- messages. In this way, it is possible to maintain the original Case Specification Environment and the CFD Engine processes completely independently. The two components interact with each other when the EE is operating. This componentization has enabled rapid prototyping and implementation of new development requirements (as well as the integration of other support techniques) as they have been identified.",,2007.0,,109296222,semantic_scholar
d0270dcfa7ca058cea59512b832be6c91408676f,https://www.semanticscholar.org/paper/d0270dcfa7ca058cea59512b832be6c91408676f,Challenges Concerning Symbolic Computations on Grids,"Challenges concerning symbolic computations on grids Symbolic and algebraic computations are currently ones of fastest growing areas of scientific computing. For a long time, the numerical approach to computational solution of mathematical problems had an advantage of being capable of solving a substantially larger set of problems than the other approach, the symbolic one. Only recently the symbolic approach gained more recognition as a viable tool for solving large-scale problems from physics, engineering or economics, reasoning, robotics or life sciences. Developments in symbolic computing were lagging relative to numerical computing, mainly due to the inadequacy of available computational resources, most importantly computer memory, but also processor power. Continuous growth in the capabilities of computer hardware led naturally to an increasing interest in symbolic calculations and resulted, among others things, in development of sophisticated Computer Algebra Systems (CASs). CASs allow users to study computational problems on the basis of their mathematical formulations and to focus on the problems themselves instead of spending time transforming the problems into forms that are numerically solvable. While their major purpose is to manipulate formulas symbolically, many systems have substantially extended their capabilities, offering nowadays functionalities like graphics allowing a comprehensive approach to problem solving. While, typically, CAS systems are utilized in an interactive mode, in order to solve large problems they can be also used in a batch mode and programmed using languages that are close to common mathematical notation. As CASs become capable of solving large problems, they follow the course of development that has already been taken by numerical software: from sequential computers to parallel machines to distributed computing and finally to the grid. It is particularly the grid that has the highest potential as a discovery accelerator. Currently, its widespread adoption is still impeded by a number of problems, one of which is difficulty of developing and implementing grid-enabled programs. That it is also the case for grid-enabled symbolic computations. There are several classes of symbolic and algebraic algorithms that can perform better in parallel and distributing computing environments. For example for multiprecision integer arithmetic, that appears among others in factorizations, were developed already twenty years ago systolic algorithms and implementations on massive parallel processors, and more recently, on the Internet. Another class that utilize significant amount of computational resources is related to the implementations of polynomial arithmetic: knowledge based algorithms such as symbolic differentiation, factorization of polynomials, greatest common divisor, or, more complicated, Groebner base computations. For example, in the latest case, the size of the computation and the irregular data structures make the parallel or distributed implementation not only an attractive option for improving the algorithm performance, but also a challenge for the computational environment. A third class of algorithms that can benefit from multiple resources in parallel and distributed environments is concerning the exact solvers of large systems of equations. The main reason driving the development of parallel and distributed algorithms for symbolic computations is the ability to solve problems that are memory bound, i.e. that cannot fit into memory of a single computer. An argument for this statement relies on the observation that the input size of a symbolic or algebraic computation can be small, but the memory used in the intermediate stages of the computation may grow considerably. Modern CASs increase their utility not only through new symbolic capabilities, but also expending their applicability using visualization or numerical modules and becoming more than only specific computational kernels. They are real problem solving environments based on interfaces to a significant number of computational engines. In this context it appears also the need to address the ability to reduce the wall-clock time by using parallel or distributed computing environment. A simple example is the case of rendering the images for a simulation animation. Several approaches can be identified in the historical evolution of parallel and distributed CASs: developing versions for shared memory architectures, developing computer algebra hardware, adding facilities for communication and cooperation between existing CASs, or building distributed systems for distributed memory parallel machines or even across Internet. Developing completely new parallel or distributed systems, although efficient, in most cases is rather difficult. Only a few parallel or distributed algorithms within such a system are fully implemented and tested. Still there are several successful special libraries and systems falling in this category: ParSac-2 system, the parallel version of SAC-2, Paclib system, the parallel extension of Saclib, FLATS based on special hardware, STAR/MPI, the parallel version of GAP, ParForm, the parallel version of Form, Cabal, MuPAD, or the recent Givaro, for parallel computing environments, FoxBox or DSC, for distributed computing environments. An alternative approach to build parallel and distributed CASs is to add the new value, the parallelism or the distribution, to an existing system. The number of parallel and distributed versions of most popular CASs is impressive and it can be explained by the different requirements or targeted architectures. For example, for Maple there are several implementations on parallel machines, like the one for Intel Paragon or ||Maple||, and several implementations on networks of workstations, like Distributed Maple or PVMaple. For Mathematica there is a Parallel Computing Toolkit, a Distributed Mathematica and a gridMathematica (for dedicated clusters). Matlab that provides a Symbolic Math Toolbox based on a Maple kernel has more than twenty different parallel or distributed versions: DP-Toolbox, MPITB/PVMTB, MultiMatlab, Matlab Parallelization Toolkit, ParMatlab, PMI, MatlabMPI, MATmarks, Matlab*p, Conlab, Otter and others. More recent web-enabled systems were proved to be efficient in number theory for finding large prime numbers, factoring large numbers, or finding collisions on known encryption algorithms. Online systems for complicated symbolic computations were also built: e.g. OGB for Groebner basis computations. A framework for description and provision of web-based mathematical services was recently designed within the Monet project and a symbolic solver wrapper was build to provide an environment that encapsulates CASs and expose their functionalities through symbolic services (Maple and Axiom were chosen as computing engines). Another platform is MapleNet build on client-server architecture: the server manages concurrent Maple instances launched to server client requests for mathematical computations. WebMathematica is a similar system that offers access to Mathematica applications through a web browser. Grid-oriented projects that involve CASs were only recent initiated. The well-known NetSolve system was one of the earliest grid system developed. Version 2 released in 2003 introduces GridSolve for interoperability with the grid based on agent technologies. APIs are available for Mathematica, Octave and Matlab. The Genss project (Grid Enabled Numerical and Symbolic Services) follows the ideas of the Monet project and intends also to combine grid computing and mathematical web services using a common agent-based framework. Several projects are porting Matlab on grids: from small ones, like Matlab*g, to very complex ones, like Geodise. Maple2g and MathGridLink are two different approaches for grid-enabled version of Maple and Mathematica. Simple to use front-end were recently build in projects like Gemlca and Websolve to deploy legacy code applications as grid services and to allows the submission of computational requests. The vision of grid computing is that of a simple and low cost access to computing resources without artificial barriers of physical location or ownership. Unfortunately, none of the above mentioned grid-enabled CAS is responding simultaneously to some elementary requirements of a possible implementation of this vision: deploy grid symbolic services, access within CAS to available grid services, and couple different grid symbolic services. Moreover a number of major obstacles remain to be addressed. Amongst the most important are mechanisms for adapting to dynamic changes in either computations or systems. This is especially important for symbolic computations, which may be highly irregular in terms of data and general computational demands. Such demands received until now relatively little attention from the research community. In the context of a growing interest in symbolic computations, powerful computer algebra systems are required for complex applications. Freshly started projects shows that porting a CAS to a current distributed environment like a grid is not a trivial task not only from technological point of view but also from algorithmic point of view. Already existing tools are allowing experimental work to be initiated, but a long way is still to be cross until real-world problems will be solved using symbolic computations on grids. Dana Petcu, Western University of Timisoara",Scalable Comput. Pract. Exp.,2005.0,10.12694/SCPE.V6I3.330,36193099,semantic_scholar
40b8656563e93c01a2723717e1c2342e79599507,https://www.semanticscholar.org/paper/40b8656563e93c01a2723717e1c2342e79599507,JSP¿ and XML: Integrating XML and Web Services in Your JSP Application,"Introduction. I. DATA, XML, AND WEB SERVICES INTRODUCTION. 1. Integrating JSP and Data. Using JSP with a Database. Entering the Data. Reviewing the Code for Entering Data. Viewing the Data. Other Considerations. Connection Pooling. Testing Components. Testing for Scale. Basic Design Concepts. Using a Tag library. Summary. 2. Introduction to XML/XSL. What Is XML? Rules of XML. Tags and Elements. The XML Declaration. Document Type Declaration. Schemas. Character Entities. CDATA Sections. Comments. Well-Formed and Validated Documents. On to Using XML. Processing. XSL. Stylesheet Linking. Namespaces. Templates. Stylesheet Errors. Whitespace and Encoding. Entity Declarations. Trees, Nodes, and Family. XPath. Summary. 3. Understanding Web Services. What Is a Web Service? Crystal Ball Readings. The ABCs of Web Services. The Basic Building Blocks. Service Management Initiatives. Java APIs. How to Use a Web Service. Using SOAP. Roaming the Internet. Summary. II. INTEGRATING JSP AND XML. 4. A Quick Start to JSP and XML Together. The Relationship Between XML and JSP. A Warning. JAXP, Xerces, and Xalan. JSP and XML: An Overview. Java XML/XSL APIs. DOM (XML Document Object Model). SAX (XML Parser). JDOM (XML Document Representation). dom4j (XML Document Representation). JAXB (Parser and XML Document Representation). Summary. 5. Using DOM. What Is the DOM? Strengths of DOM. Weaknesses of DOM. Nodes and Tree Structure. The Document Node. Programming with DOM. Attributes. Namespaces. Removing a Node. Moving Nodes. Copying and Appending Nodes. Programmatically Creating an XML Document. Moving Nodes Between Documents. TreeWalker. NodeIterator. Ranges. JDOM, dom4j, and Deferred DOM. Summary. 6. Programming SAX. What Is SAX? The Workings of SAX. SAX Interfaces. Downsides to SAX. Differences Between SAX1 and SAX2. First SAX Example. Characters and Ignorable Whitespace. Processing Versus Validation. Characters Revisited. Error Handling. Ignorable Whitespace. Entity References. The Document Locator. Breaking the System to See How It Works. Processing Versus Validation Revisited. Using SAX to Output HTML. Summary. 7. Successfully Using JSP and XML in an Application. Using a Java Representation of an XML Document. Why Not Just Use SAX or DOM? Installing JDOM and dom4j. JDOM. dom4j. Notes. Why Both JDOM and dom4j? JDOM and dom4j: A Quick Comparison. Common Ways to Use XML. Using a Database with XML. XML Initialization Files. Storing the Initialization Data. Using a Listener to Store the DatabaseParameter Object. Using a Java XML Model. Threading Issues. Getting the Row Count. XML and the WebRowSet. Building a dom4j Helper Class. Creating a Banner Handler. Creating a Test JSP Page. Using a Java Representation of an XML Document. Using JAXB. HashMap Versus Java XML Representation. Pulling in XML Files. Defining an XML File. Pondering XML Design. Reading XML Files and Creating New Output. Using JDOM. Building the Final JSP Page. Summary. 8. Integrating JSP and Web Services. Thinking in JSP and Web Services. Tag Libraries Versus Web Services. Using Tag Libraries to Access a Web Service. Integrating a Web Service into a JSP Page. A Tag Library/Service Warning. Fixing Some Network Issues. Web Service Reliability. When Should You Build Your Own Web Service? JSP Pages Versus Web Service. Building a Corporate Web Service. Goal of This Web Service Example. Realities of Building a Web Service. Setting Up the Example. Initializing Data. Accessing Application Data. Building the Actual Web Service. Deploying a Web Service. Where Is the WSDL? Writing a JSP Page to Deploy the Descriptor File. More on Security. Building a Page to Access the Service. Apache SOAP Help. Summary. 9. Advanced JSP and XML Techniques. Accessing Web Services from a Browser. Using an Applet. Handling Large XML Documents. JDOM. dom4j. Handling Special Characters and Encoding. Using XML Tag Libraries. XSL Tag Library. XTags Library for XML. Summary. III. BUILDING JSP SITES TO USE XML. 10. Using XSL/JSP in Web Site Design. Handling XML Files Directly. How Servlet Mappings Work. Building an XML Servlet Handler. Building a SAX Reader. Creating a Servlet to Process XML Files. Register the Servlet. Building the Error Page. Creating Some Test Files. Accessing XML Directly. Summary. 11. Using XML in Reporting Systems. Architecture of Reporting Systems. When to Use XML with Reports. Data Source for Reports. Creating Database Data. ResultSet to XML. What It Does. Bringing It All Together. The Sorting Table Stylesheet. The Cross Tab Stylesheet. Summary. 12. Advanced XML in Reporting Systems. Multiple-Page Reports. The JSP for the Multiple-Page Report. The Stylesheet for the Multiple-Page Report. Reports on Data with One-to-Many Relationships. The JSP for the One-to-Many Report. The Stylesheet for the One-to-Many Report. Real-World Reporting Systems. Well-Formed Documents Revisited. Summary. 13. Browser Considerations with XML. Client-Side XML and Browser Support. Client-Side JavaScript and XML. The JSP. Client-Side Transformations and XML. The Cross-Browser JavaScript Source File. The JSP. The Two XSL Stylesheets. Summary. 14. Building a Web Service. Designing a Web Service. What Is the Goal? What Are the Requirements? What Data Does the Service Need? Building the Web Service. Building a File Handler. Building a Search Utility. Creating an ElementHandler. Building a Document Object. Applying a Stylesheet. Creating a Stylesheet. Building the Web Service at Last. Registering the Web Service with Apache SOAP. Creating a WSDL File. WSDL Namespaces. Creating the JSPBuzz WSDL File. WSDL Implementation File. WSDL Documentation. Registering Within UDDI. Registering a Service. Using Java to Access a WSDL Document. Summary. 15. Advanced Application Design. Dynamic JSP. When Not to Use Dynamic JSP. Building a Dynamic JSP Example. SOAP Server Security Concerns. Using Tomcat Security Zones. Servlet Filtering. Other Apache SOAP-Specific Security Steps. Quick Takes. Web Services-SSL and Data Encryption. Using Cocoon. Summary. IV. APPENDIXES. A. Setting Up. Installing the JSP Environment. The Java Software Development Kit (SDK). The Tomcat Server. Creating a Web Site for the Book. NetBeans. The MySQL Database Server. Creating a MySQL Database. Installing a JDBC Driver. Summary. B. Introduction to JSP and How Things Work. JSP Basics. JSP Banner Example. Actions. JSP Actions, Directives, and Implicit Objects. A More Robust JSP Example. Additional Information About JSP. What Is JSP and How Does It Work? JSP XML Syntax. JSP Documentation Resources. Summary. C. Tag Library. Tag Library Overview. What Is a Tag Library? Advantages. Disadvantages. The Six Steps to Building Tag Libraries. Tag Library Concepts. Isolating the Business Logic. The Tag Handler. The Tag Library Descriptor (TLD). Creating a Distribution File. Registering the Tag Library. Using the Tag Library Declaration on a JSP Page. Building a Tag Library. Isolating the Business Logic. Building a Tag Handler. The Tag Library Descriptor. Registering the Tag Library. Using the Tag Library on a JSP Page. General Notes. Body Data. Design Notes. Empty Tags. Threading. Summary. D. XSL Reference. XSLT and XPath. Context and Current Nodes. Reference. XSLT Elements. XPath Functions. Index. 0672323540T03282002",,2002.0,,60396373,semantic_scholar
b5feeee7c1e5e6acf7b6ee0314415102b09068e0,https://www.semanticscholar.org/paper/b5feeee7c1e5e6acf7b6ee0314415102b09068e0,A CLASS LIBRARY FOR MANUFACTURING SYSTEMS,"This work presents a class library for manufacturing systems that aims at facilitating the construction of simulation models, allowing reuse and speeding up the modeling process. This library implements a modeling approach that differs from the majority of similar works in this area. It is based on the application of well known manufacturing concepts, like production routings and activities. It allows the creation of new simulations faster than other methodologies, since complex translations from the reality to simulated applications are not necessary. The development of this library was validated by modeling the production line of tractor parts. The production line case study, modeled using both Automod and the proposed class library, allowed a quantitative comparison for the validation of this work. INTRODUCTION The fast dissemination and widely acceptance of simulation methods contributed to the emergence of specialised environments for programming and simulation of manufacturing systems. Simulation is virtually the only methodology at present which is capable to provide accurate performance estimates for manufacturing systems design (Govindaraj et al. 1990). These environments offer tools that allow the efficient analysis from simple to complex systems. Their main limitation is the difficulty to develop new simulation models. Their model building activity usually requires specific information that is related to the internals of the adopted simulation tool. They are usually restricted to highly specialised users, since the systems usually adopt proprietary languages and particular simulation methodologies. On the other hand, the object-oriented paradigm has taken the attention of the scientific community. Properties like encapsulation, inheritance, and reuse have contributed to make the object-oriented technology very popular for manufacturing systems simulation and deployment. The oneto-one mapping between objects in the manufacturing systems being modelled and their abstractions in the simulation model offer conditions for a better modelling. The object-oriented paradigm have been explored to allow the construction of high-fidelity simulation software for supporting the modelling and control of integrated and complex manufacturing systems. The object-oriented paradigm and distributed objects have emerged as some of the most promising technologies since they allow a natural structure for the problem domain of industrial automation systems. This occurs because the industrial components can be easily mapped into the oriented-objects model diagrams using classes and objects. Objects encapsulate the functions of many components, modularizing the description, encouraging reuse, and allowing implementations as well in software as in hardware. There are many commercial simulation environments for manufacturing systems. They offer many features that permit to build powerful simulations in order to develop advanced solutions for new projects through experimenting many alternative configurations. One of these simulation tools is Automod (Auto Simulations, 2002). Automod (Auto Simulations, 1999) is a process-oriented simulator. The simulation method used in Automod is based on moving parts, representing the work in process, on resources, and on transportation entities. To create a simulation using Automod, the designer must control the traffic of parts through the system using a process specification, which indicates the correct pathway that the part must follow. This approach gives to the simulator robust and reliable features, allowing complex model constructions. The major problem of these tools is the difficulty to build new simulation models. The modeling activity in these environments needs a detailed previous study about the tools, generally demanding highly specialised users. Moreover, these environments do not use important modeling concepts like reuse and extensions. Analysing the current state of the art, this fact is unacceptable. An integration between advantages of the commercial simulators and of the new modeling paradigms becomes mandatory. This fact may increase the quality of produced models, since developers only need to worry about the manufacturing problems, without having to worry about particularities of simulation environments. The main goal of this work is the design of a class library for manufacturing systems that allows the development of simulation models to evaluate these systems. Basic components of these systems should be available, and it should be possible to extend them through high-level modeling features found in object-oriented systems, like aggregation and abstraction, aiming at an easier modeling of complex systems, by encouraging reuse and extensions. STATE OF THE ART IN MANUFACTURING SYSTEMS SIMULATION Bertotto (2001) presents a complete study of the state of the art in manufacturing systems simulation. Modeling, control, and simulation methodologies have attracted the attention of researchers and non-governmental consortia to find new ways of developing manufacturing systems. Narayanan (1998) describes relevant efforts in the specification of object-oriented hierarchies applied to manufacturing systems simulation. Among other efforts, it is worth mentioning the application of Distributed Artificial Intelligence (DAI) and multi-agent systems, the ideas proposed by a consortium called Holonic Manufacturing Systems (HMS), and CORBAManufacturing. Park (1997) introduces an object-oriented modeling schema for the design of automated manufacturing systems (MAS) called JR-net. To propose this schema, Park relies on the fact that the modern flexible manufacturing systems are modular and hierarchical structures, built from standard resources. This proposal is formally based on processes and resource definitions, relationships among objects, procedures to build the model, and on the model structure. Its purpose is to shown that the JR-net modeling framework could be used in the same way as a graphical modeling tool for commercial simulators, such as Automod. A CLASS LIBRARY FOR MANUFACTURING SYSTEMS SIMULATION The main feature of this library is its modeling approach, which is different from other efforts in this area. This approach is based on activities and production routings. The production control is established by production routings that own activities and activate the respective resources. This approach makes the design easier, since the current theory of manufacturing systems is based on this kind of concepts. New simulations may be developed faster than in other methodologies, since complex translations between the reality and the simulation models are unnecessary. The class diagram of the proposed library is represented in Figure 1. This library contains a set of classes that supports both the physical and logical modeling of a manufacturing system. The physical modeling is based on classes that represent the manufacturing system entities, like machines, robots, and tasks. The classes that represent the physical resources can be easily extended to represent other equipments that were not yet created. The logical modeling is based on classes, methods, and attributes also similar to real world logic concepts. Class Library Specification This library was created using UML, a standard language for object modeling containing various diagrams. The library is composed by 30 classes, linked by relationships, aggregations, and specializations. The root of the hierarchy is the Production_Line class. This class aggregates two other classes: Resource and Master_Plan. The Resource class is the root of all classes that execute some transformation on manufacturing parts. The Master_Plan class is the root of all classes that logically control the manufacturing system. In this class, the production plan based on client orders is generated. To create the master plan, the Master_Plan class uses services provided by other classes, such as Order, Scheduling_Algorithm, Part, Resource and Daily_Capacity. The user interacts with the model using the Order class, inserting orders to be produced. The Scheduling_Algorithm class implements the production scheduling algorithms that will guide the master plan creation. When the master plan is generated, the production orders should be created to control the production. Each instance of Production_Order will order the production of one part. If, for example, there is an order of 30 parts, there will be 30 instances of Production_Order. This approach makes the production flow easy. Moreover, it is necessary to update the capacity information of the involved resources. Before the creation of the production orders, Production_Order estimates the daily capacity of the resources, using the Daily_Capacity class related to the Resource class. A variation of a production routing was used in the library to control the production in the various resources. The production routing is represented by an aggregation of the Activity class within the Part class. The aggregated Activity classes describe a logical sequence represented by a relationship that shows the next and the previous activity. An activity can only be executed if all the previous activities were executed. This aggregated structure builds a production graph that may control various activities at different levels of the production hierarchy. When an activity is completed, it activates the next instance of the Activity class. If there are no next activities, an answer is returned to the calling instance. This action indicates that the production of the mentioned part is concluded. Each instance of the Activity class must own a default resource to control. Each activity has a relationship to the Resource class that will execute it. To each resource a wait queue, represented by the Queue class, was added. This class holds the scheduled parts if the respective resource is busy. The Position class stores the ph",,2003.0,,18991496,semantic_scholar
784cb6b4c7a6e3f6a2f17584cf0e1e6f07968955,https://www.semanticscholar.org/paper/784cb6b4c7a6e3f6a2f17584cf0e1e6f07968955,Paper Session II-D - Internet Based Training to Support a Changing Workforce,"Changes are now underway at the Kennedy Space Center to re-engineer a smaller but more technically advanced and more productive workforce. A key component of this change is the training to give the workforce the skills needed to implement future space flight programs both effectively and efficiently. The Safety and Mission Assurance Directorate at KSC has developed training classes which are delivered over the Internet for its employees, and are available to all agency employees. The courses developed to date include An Overview of Non-Destructive Evaluation, Introduction to Statistical Process Control, Statistical Process Control, and Radiography. These courses are interactive and can be completed at the optimum pace for the students at any time or location with computer access to the Internet. Overview The re-engineering now underway at the Kennedy Space Center and across all of NASA is a familiar story. The processes that worked so effectively in the past, especially in manned spaceflight applications, are now too expensive to meet the new efficiency goals brought about by today’s budget realities. The new challenge is to maintain or improve effectiveness while dramatically improving efficiency. This means developing new, lower cost techniques in all areas of spaceflight development and operations. New skills must be brought to the workforce to reach these new efficiencies and the means of supplying these skills must itself be accomplished as efficiently as possibly. One area in which technology is delivering increased capability at steadily lower prices is information technology. Information technologies have made training at the desktop cost effective. Desktop training could augment or supplant existing training methods. Interactive learner control enhances training and facilitates Just-In-Time learning and reference. Web Interactive Training (Figure 1) uses time and resources more efficiently than many current training methods in use. The WIT project uses the Web to deliver training directly to the learner’s desktop computers. The primary users of the system are NASA (National Aeronautics and Space Administration) personnel at both KSC (Kennedy Space Center) and other NASA centers. The objective of the project is train a large base of NASA learners; efficiently and effectively, using state of the art technology to enhance learning. Training modules consisting of text, graphics, animation, video, simulations and tests are delivered over the Internet through a Web browser interface. This approach is expected to reduce training costs and associated travel and time-off task costs. The training is available 24 hours a day, seven days a week for learner convenience and follow-up job performance support after the training is completed. The WIT project began in July of 1995 and has continued throughout fiscal year 1996 (October 1995September 1996) and fiscal year 1997 (October 1996September 1997). Four courses have been completed to verify and validate the design, technologies incorporated and developed. These courses include Nondestructive Evaluation (NDE) Overview, Introduction to Statistical Process Control course, Radiography-NDE and Statistical Process Control. The current phase of this project is the development of two advanced courses. One will be Nondestructive Evaluation-Ultrasonics and the other will be Advanced Statistical Process Control-Design of Experiments. The project incorporates state-of-the-art multimedia technologies to meet the defined learning objectives. Instructional Design In order to develop an effective Web-based training system that accomplishes the goals of providing sound instruction over the Web, it is necessary to understand key instructional features that will contribute to the development and deployment of the WIT system (Alexander, 1995). Using multimedia in an effective way on the Web and especially in Web-based training applications is a challenge (Kilby, 1996). The project also defines a functional educational design model that takes into account the advantages and disadvantages of the Web The model for each instructional section generally follows an expansion of the Topic, Task, Test model (Chopping, 1995). For example a section might include: introduction and definitions, key concepts and theoretical foundation, practical application and case studies, an interactive simulation or practice exercise, and testing or evaluation. The WIT system presents information in an interactive and informative way. The learners will engage primarily in guided discovery learning. Learners will have a clear learning objective presented and their path choices will be limited to pertinent information. A structured approach is determined in each course by instructional system designers and subject matter experts (SME’s). The instructional objectives, content, and methodologies are used to determine the best approach for a particular subject area module. The learner will have some flexibility in the depth to which they wish to explore the information, but an acceptable level of proficiency must be met to prove completion of a module. For example, in the Nondestructive Evaluation (NDE) Overview course, learners must take a test with results posted back to a database in order to advance to further sections by the specified path (Figure 5). There is some flexibility built in for the user to explore detailed information on a subject area. Further references and resources are provided as well as more advanced followon modules in a particular NDE method currently under development. Technical Disciplines There are many technical considerations and approaches to this project. The majority of the effort involved advanced HTML scripting, hardware and software setup and design. This effort also includes instructional system design, digital photography, scanning, media conversion, audio and video recording, compression, animation, formatting, scripting, programming, and beta testing. The process includes research and implementation of late-breaking technologies like streaming digital video for topic introductions, CGI interfaces for forms and testing feedback, Shockwave simulation modules, Java and other advanced client/server features. Unique Attributes and Innovations Simulations The simulations developed for the NDE modules are designed to simulate a real-world process in a simple, elegant manner while minimizing the learner’s download time. (Each simulation is approximately 30 kilobytes in size.) These simulations incorporate repetition, learning by example, and positive feedback (Campbell, 1995). Each simulation follows the same general model as the Eddy Current Simulation shown in Figure 4, where the student searches for discontinuities that are randomly scattered throughout a test object. The Eddy Current simulation shows a sample plate on the right and the feedback screen on the left. These simulations were designed and developed using Macromedia Director and its native programming language, Lingo (NASA, In Press). Testing Testing will aid in comprehension of the information presented on NDE and SPC and reinforce the most important points in the section of training presented. Test answers are cataloged in a database to show student progress and adequate completion. Learners are presented with a short multiple-choice quiz. The quiz is randomly generated from a database of questions and is different every time the student takes the quiz. After submitting his or her answers, the student is immediately presented with his or her score, a brief explanation of the answers, and a link to the place in the course where that topic was covered. Feedback and remediation are immediate providing excellent response and reinforcement (NASA, In Press). Learners cannot cheat. The program prevents them from returning to the same quiz and retaking it. The testing database was written in Perl and partially converted to Java. Eventually the program will be entirely converted to Java for ease of future expandability and possible cross-platform server deployment (NASA, In Press). Security and Student Tracking The system is only available to the NASA Centers unless special password authorization is granted by NASA. The courses may be opened up to a broader audience in the future at NASA’s discretion. Current activities include enhancing the learner tracking to include a placeholder for reentering the training space at the same point of exit from the last use. This will avoid unnecessary navigation and the disorientation often associated with large hypertext systems. A good computer managed instruction database to track completion of sections is an essential component of the WIT system. Instructional designers will have a much clearer idea about the effectiveness of the instruction from the answers received. Tracking could be automated to send an electronic mail reminder to individuals who need to finish instruction by a certain date for certification. Performance Support Some of the interactive calculation simulators like the normal distribution calculator shown below (figure 6) provide performance support after the training. The system also contains an electronic version of relevant reference material and procedures used for each discipline. A search engine allows users to pinpoint specific information and get to it in seconds. This makes the modules usable as a reference after the training has been completed. In this way, the same tool that is used for training can be extended to provide reference support to the application of the training on the job. Future Developments Future efforts involve advanced security functions, enhanced student tracking capabilities, additional performance support functions, adaptive learning through dynamic Web pages and objects, integration into centralized NASA training activities, front-end “push” technology, synchronous instructional communication aids (videoconferencing and live bulletin ",,1998.0,,78744263,semantic_scholar
